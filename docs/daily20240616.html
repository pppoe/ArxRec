<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240613.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GGHead: Fast and Generalizable 3D Gaussian Heads", "author": "Tobias Kirschstein and Simon Giebenhain and Jiapeng Tang and Markos Georgopoulos and Matthias Nie\u00dfner", "abstract": "  Learning 3D head priors from large 2D image collections is an important step\ntowards high-quality 3D-aware human modeling. A core requirement is an\nefficient architecture that scales well to large-scale datasets and large image\nresolutions. Unfortunately, existing 3D GANs struggle to scale to generate\nsamples at high resolutions due to their relatively slow train and render\nspeeds, and typically have to rely on 2D superresolution networks at the\nexpense of global 3D consistency. To address these challenges, we propose\nGenerative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian\nSplatting representation within a 3D GAN framework. To generate a 3D\nrepresentation, we employ a powerful 2D CNN generator to predict Gaussian\nattributes in the UV space of a template head mesh. This way, GGHead exploits\nthe regularity of the template's UV layout, substantially facilitating the\nchallenging task of predicting an unstructured set of 3D Gaussians. We further\nimprove the geometric fidelity of the generated 3D representations with a novel\ntotal variation loss on rendered UV coordinates. Intuitively, this\nregularization encourages that neighboring rendered pixels should stem from\nneighboring Gaussians in the template's UV space. Taken together, our pipeline\ncan efficiently generate 3D heads trained only from single-view 2D image\nobservations. Our proposed framework matches the quality of existing 3D head\nGANs on FFHQ while being both substantially faster and fully 3D consistent. As\na result, we demonstrate real-time generation and rendering of high-quality\n3D-consistent heads at $1024^2$ resolution for the first time.\n", "link": "http://arxiv.org/abs/2406.09377v1", "date": "2024-06-13", "relevancy": 3.404, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6936}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6936}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GGHead%3A%20Fast%20and%20Generalizable%203D%20Gaussian%20Heads&body=Title%3A%20GGHead%3A%20Fast%20and%20Generalizable%203D%20Gaussian%20Heads%0AAuthor%3A%20Tobias%20Kirschstein%20and%20Simon%20Giebenhain%20and%20Jiapeng%20Tang%20and%20Markos%20Georgopoulos%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20Learning%203D%20head%20priors%20from%20large%202D%20image%20collections%20is%20an%20important%20step%0Atowards%20high-quality%203D-aware%20human%20modeling.%20A%20core%20requirement%20is%20an%0Aefficient%20architecture%20that%20scales%20well%20to%20large-scale%20datasets%20and%20large%20image%0Aresolutions.%20Unfortunately%2C%20existing%203D%20GANs%20struggle%20to%20scale%20to%20generate%0Asamples%20at%20high%20resolutions%20due%20to%20their%20relatively%20slow%20train%20and%20render%0Aspeeds%2C%20and%20typically%20have%20to%20rely%20on%202D%20superresolution%20networks%20at%20the%0Aexpense%20of%20global%203D%20consistency.%20To%20address%20these%20challenges%2C%20we%20propose%0AGenerative%20Gaussian%20Heads%20%28GGHead%29%2C%20which%20adopts%20the%20recent%203D%20Gaussian%0ASplatting%20representation%20within%20a%203D%20GAN%20framework.%20To%20generate%20a%203D%0Arepresentation%2C%20we%20employ%20a%20powerful%202D%20CNN%20generator%20to%20predict%20Gaussian%0Aattributes%20in%20the%20UV%20space%20of%20a%20template%20head%20mesh.%20This%20way%2C%20GGHead%20exploits%0Athe%20regularity%20of%20the%20template%27s%20UV%20layout%2C%20substantially%20facilitating%20the%0Achallenging%20task%20of%20predicting%20an%20unstructured%20set%20of%203D%20Gaussians.%20We%20further%0Aimprove%20the%20geometric%20fidelity%20of%20the%20generated%203D%20representations%20with%20a%20novel%0Atotal%20variation%20loss%20on%20rendered%20UV%20coordinates.%20Intuitively%2C%20this%0Aregularization%20encourages%20that%20neighboring%20rendered%20pixels%20should%20stem%20from%0Aneighboring%20Gaussians%20in%20the%20template%27s%20UV%20space.%20Taken%20together%2C%20our%20pipeline%0Acan%20efficiently%20generate%203D%20heads%20trained%20only%20from%20single-view%202D%20image%0Aobservations.%20Our%20proposed%20framework%20matches%20the%20quality%20of%20existing%203D%20head%0AGANs%20on%20FFHQ%20while%20being%20both%20substantially%20faster%20and%20fully%203D%20consistent.%20As%0Aa%20result%2C%20we%20demonstrate%20real-time%20generation%20and%20rendering%20of%20high-quality%0A3D-consistent%20heads%20at%20%241024%5E2%24%20resolution%20for%20the%20first%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGGHead%253A%2520Fast%2520and%2520Generalizable%25203D%2520Gaussian%2520Heads%26entry.906535625%3DTobias%2520Kirschstein%2520and%2520Simon%2520Giebenhain%2520and%2520Jiapeng%2520Tang%2520and%2520Markos%2520Georgopoulos%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520Learning%25203D%2520head%2520priors%2520from%2520large%25202D%2520image%2520collections%2520is%2520an%2520important%2520step%250Atowards%2520high-quality%25203D-aware%2520human%2520modeling.%2520A%2520core%2520requirement%2520is%2520an%250Aefficient%2520architecture%2520that%2520scales%2520well%2520to%2520large-scale%2520datasets%2520and%2520large%2520image%250Aresolutions.%2520Unfortunately%252C%2520existing%25203D%2520GANs%2520struggle%2520to%2520scale%2520to%2520generate%250Asamples%2520at%2520high%2520resolutions%2520due%2520to%2520their%2520relatively%2520slow%2520train%2520and%2520render%250Aspeeds%252C%2520and%2520typically%2520have%2520to%2520rely%2520on%25202D%2520superresolution%2520networks%2520at%2520the%250Aexpense%2520of%2520global%25203D%2520consistency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AGenerative%2520Gaussian%2520Heads%2520%2528GGHead%2529%252C%2520which%2520adopts%2520the%2520recent%25203D%2520Gaussian%250ASplatting%2520representation%2520within%2520a%25203D%2520GAN%2520framework.%2520To%2520generate%2520a%25203D%250Arepresentation%252C%2520we%2520employ%2520a%2520powerful%25202D%2520CNN%2520generator%2520to%2520predict%2520Gaussian%250Aattributes%2520in%2520the%2520UV%2520space%2520of%2520a%2520template%2520head%2520mesh.%2520This%2520way%252C%2520GGHead%2520exploits%250Athe%2520regularity%2520of%2520the%2520template%2527s%2520UV%2520layout%252C%2520substantially%2520facilitating%2520the%250Achallenging%2520task%2520of%2520predicting%2520an%2520unstructured%2520set%2520of%25203D%2520Gaussians.%2520We%2520further%250Aimprove%2520the%2520geometric%2520fidelity%2520of%2520the%2520generated%25203D%2520representations%2520with%2520a%2520novel%250Atotal%2520variation%2520loss%2520on%2520rendered%2520UV%2520coordinates.%2520Intuitively%252C%2520this%250Aregularization%2520encourages%2520that%2520neighboring%2520rendered%2520pixels%2520should%2520stem%2520from%250Aneighboring%2520Gaussians%2520in%2520the%2520template%2527s%2520UV%2520space.%2520Taken%2520together%252C%2520our%2520pipeline%250Acan%2520efficiently%2520generate%25203D%2520heads%2520trained%2520only%2520from%2520single-view%25202D%2520image%250Aobservations.%2520Our%2520proposed%2520framework%2520matches%2520the%2520quality%2520of%2520existing%25203D%2520head%250AGANs%2520on%2520FFHQ%2520while%2520being%2520both%2520substantially%2520faster%2520and%2520fully%25203D%2520consistent.%2520As%250Aa%2520result%252C%2520we%2520demonstrate%2520real-time%2520generation%2520and%2520rendering%2520of%2520high-quality%250A3D-consistent%2520heads%2520at%2520%25241024%255E2%2524%2520resolution%2520for%2520the%2520first%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GGHead%3A%20Fast%20and%20Generalizable%203D%20Gaussian%20Heads&entry.906535625=Tobias%20Kirschstein%20and%20Simon%20Giebenhain%20and%20Jiapeng%20Tang%20and%20Markos%20Georgopoulos%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20Learning%203D%20head%20priors%20from%20large%202D%20image%20collections%20is%20an%20important%20step%0Atowards%20high-quality%203D-aware%20human%20modeling.%20A%20core%20requirement%20is%20an%0Aefficient%20architecture%20that%20scales%20well%20to%20large-scale%20datasets%20and%20large%20image%0Aresolutions.%20Unfortunately%2C%20existing%203D%20GANs%20struggle%20to%20scale%20to%20generate%0Asamples%20at%20high%20resolutions%20due%20to%20their%20relatively%20slow%20train%20and%20render%0Aspeeds%2C%20and%20typically%20have%20to%20rely%20on%202D%20superresolution%20networks%20at%20the%0Aexpense%20of%20global%203D%20consistency.%20To%20address%20these%20challenges%2C%20we%20propose%0AGenerative%20Gaussian%20Heads%20%28GGHead%29%2C%20which%20adopts%20the%20recent%203D%20Gaussian%0ASplatting%20representation%20within%20a%203D%20GAN%20framework.%20To%20generate%20a%203D%0Arepresentation%2C%20we%20employ%20a%20powerful%202D%20CNN%20generator%20to%20predict%20Gaussian%0Aattributes%20in%20the%20UV%20space%20of%20a%20template%20head%20mesh.%20This%20way%2C%20GGHead%20exploits%0Athe%20regularity%20of%20the%20template%27s%20UV%20layout%2C%20substantially%20facilitating%20the%0Achallenging%20task%20of%20predicting%20an%20unstructured%20set%20of%203D%20Gaussians.%20We%20further%0Aimprove%20the%20geometric%20fidelity%20of%20the%20generated%203D%20representations%20with%20a%20novel%0Atotal%20variation%20loss%20on%20rendered%20UV%20coordinates.%20Intuitively%2C%20this%0Aregularization%20encourages%20that%20neighboring%20rendered%20pixels%20should%20stem%20from%0Aneighboring%20Gaussians%20in%20the%20template%27s%20UV%20space.%20Taken%20together%2C%20our%20pipeline%0Acan%20efficiently%20generate%203D%20heads%20trained%20only%20from%20single-view%202D%20image%0Aobservations.%20Our%20proposed%20framework%20matches%20the%20quality%20of%20existing%203D%20head%0AGANs%20on%20FFHQ%20while%20being%20both%20substantially%20faster%20and%20fully%203D%20consistent.%20As%0Aa%20result%2C%20we%20demonstrate%20real-time%20generation%20and%20rendering%20of%20high-quality%0A3D-consistent%20heads%20at%20%241024%5E2%24%20resolution%20for%20the%20first%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09377v1&entry.124074799=Read"},
{"title": "Modeling Ambient Scene Dynamics for Free-view Synthesis", "author": "Meng-Li Shih and Jia-Bin Huang and Changil Kim and Rajvi Shah and Johannes Kopf and Chen Gao", "abstract": "  We introduce a novel method for dynamic free-view synthesis of an ambient\nscenes from a monocular capture bringing a immersive quality to the viewing\nexperience. Our method builds upon the recent advancements in 3D Gaussian\nSplatting (3DGS) that can faithfully reconstruct complex static scenes.\nPrevious attempts to extend 3DGS to represent dynamics have been confined to\nbounded scenes or require multi-camera captures, and often fail to generalize\nto unseen motions, limiting their practical application. Our approach overcomes\nthese constraints by leveraging the periodicity of ambient motions to learn the\nmotion trajectory model, coupled with careful regularization. We also propose\nimportant practical strategies to improve the visual quality of the baseline\n3DGS static reconstructions and to improve memory efficiency critical for\nGPU-memory intensive learning. We demonstrate high-quality photorealistic novel\nview synthesis of several ambient natural scenes with intricate textures and\nfine structural elements.\n", "link": "http://arxiv.org/abs/2406.09395v1", "date": "2024-06-13", "relevancy": 3.2068, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6522}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6454}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Ambient%20Scene%20Dynamics%20for%20Free-view%20Synthesis&body=Title%3A%20Modeling%20Ambient%20Scene%20Dynamics%20for%20Free-view%20Synthesis%0AAuthor%3A%20Meng-Li%20Shih%20and%20Jia-Bin%20Huang%20and%20Changil%20Kim%20and%20Rajvi%20Shah%20and%20Johannes%20Kopf%20and%20Chen%20Gao%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20dynamic%20free-view%20synthesis%20of%20an%20ambient%0Ascenes%20from%20a%20monocular%20capture%20bringing%20a%20immersive%20quality%20to%20the%20viewing%0Aexperience.%20Our%20method%20builds%20upon%20the%20recent%20advancements%20in%203D%20Gaussian%0ASplatting%20%283DGS%29%20that%20can%20faithfully%20reconstruct%20complex%20static%20scenes.%0APrevious%20attempts%20to%20extend%203DGS%20to%20represent%20dynamics%20have%20been%20confined%20to%0Abounded%20scenes%20or%20require%20multi-camera%20captures%2C%20and%20often%20fail%20to%20generalize%0Ato%20unseen%20motions%2C%20limiting%20their%20practical%20application.%20Our%20approach%20overcomes%0Athese%20constraints%20by%20leveraging%20the%20periodicity%20of%20ambient%20motions%20to%20learn%20the%0Amotion%20trajectory%20model%2C%20coupled%20with%20careful%20regularization.%20We%20also%20propose%0Aimportant%20practical%20strategies%20to%20improve%20the%20visual%20quality%20of%20the%20baseline%0A3DGS%20static%20reconstructions%20and%20to%20improve%20memory%20efficiency%20critical%20for%0AGPU-memory%20intensive%20learning.%20We%20demonstrate%20high-quality%20photorealistic%20novel%0Aview%20synthesis%20of%20several%20ambient%20natural%20scenes%20with%20intricate%20textures%20and%0Afine%20structural%20elements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Ambient%2520Scene%2520Dynamics%2520for%2520Free-view%2520Synthesis%26entry.906535625%3DMeng-Li%2520Shih%2520and%2520Jia-Bin%2520Huang%2520and%2520Changil%2520Kim%2520and%2520Rajvi%2520Shah%2520and%2520Johannes%2520Kopf%2520and%2520Chen%2520Gao%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520dynamic%2520free-view%2520synthesis%2520of%2520an%2520ambient%250Ascenes%2520from%2520a%2520monocular%2520capture%2520bringing%2520a%2520immersive%2520quality%2520to%2520the%2520viewing%250Aexperience.%2520Our%2520method%2520builds%2520upon%2520the%2520recent%2520advancements%2520in%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520that%2520can%2520faithfully%2520reconstruct%2520complex%2520static%2520scenes.%250APrevious%2520attempts%2520to%2520extend%25203DGS%2520to%2520represent%2520dynamics%2520have%2520been%2520confined%2520to%250Abounded%2520scenes%2520or%2520require%2520multi-camera%2520captures%252C%2520and%2520often%2520fail%2520to%2520generalize%250Ato%2520unseen%2520motions%252C%2520limiting%2520their%2520practical%2520application.%2520Our%2520approach%2520overcomes%250Athese%2520constraints%2520by%2520leveraging%2520the%2520periodicity%2520of%2520ambient%2520motions%2520to%2520learn%2520the%250Amotion%2520trajectory%2520model%252C%2520coupled%2520with%2520careful%2520regularization.%2520We%2520also%2520propose%250Aimportant%2520practical%2520strategies%2520to%2520improve%2520the%2520visual%2520quality%2520of%2520the%2520baseline%250A3DGS%2520static%2520reconstructions%2520and%2520to%2520improve%2520memory%2520efficiency%2520critical%2520for%250AGPU-memory%2520intensive%2520learning.%2520We%2520demonstrate%2520high-quality%2520photorealistic%2520novel%250Aview%2520synthesis%2520of%2520several%2520ambient%2520natural%2520scenes%2520with%2520intricate%2520textures%2520and%250Afine%2520structural%2520elements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Ambient%20Scene%20Dynamics%20for%20Free-view%20Synthesis&entry.906535625=Meng-Li%20Shih%20and%20Jia-Bin%20Huang%20and%20Changil%20Kim%20and%20Rajvi%20Shah%20and%20Johannes%20Kopf%20and%20Chen%20Gao&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20dynamic%20free-view%20synthesis%20of%20an%20ambient%0Ascenes%20from%20a%20monocular%20capture%20bringing%20a%20immersive%20quality%20to%20the%20viewing%0Aexperience.%20Our%20method%20builds%20upon%20the%20recent%20advancements%20in%203D%20Gaussian%0ASplatting%20%283DGS%29%20that%20can%20faithfully%20reconstruct%20complex%20static%20scenes.%0APrevious%20attempts%20to%20extend%203DGS%20to%20represent%20dynamics%20have%20been%20confined%20to%0Abounded%20scenes%20or%20require%20multi-camera%20captures%2C%20and%20often%20fail%20to%20generalize%0Ato%20unseen%20motions%2C%20limiting%20their%20practical%20application.%20Our%20approach%20overcomes%0Athese%20constraints%20by%20leveraging%20the%20periodicity%20of%20ambient%20motions%20to%20learn%20the%0Amotion%20trajectory%20model%2C%20coupled%20with%20careful%20regularization.%20We%20also%20propose%0Aimportant%20practical%20strategies%20to%20improve%20the%20visual%20quality%20of%20the%20baseline%0A3DGS%20static%20reconstructions%20and%20to%20improve%20memory%20efficiency%20critical%20for%0AGPU-memory%20intensive%20learning.%20We%20demonstrate%20high-quality%20photorealistic%20novel%0Aview%20synthesis%20of%20several%20ambient%20natural%20scenes%20with%20intricate%20textures%20and%0Afine%20structural%20elements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09395v1&entry.124074799=Read"},
{"title": "Gaussian Splatting with Localized Points Management", "author": "Haosen Yang and Chenhao Zhang and Wenqing Wang and Marco Volino and Adrian Hilton and Li Zhang and Xiatian Zhu", "abstract": "  Point management is a critical component in optimizing 3D Gaussian Splatting\n(3DGS) models, as the point initiation (e.g., via structure from motion) is\ndistributionally inappropriate. Typically, the Adaptive Density Control (ADC)\nalgorithm is applied, leveraging view-averaged gradient magnitude thresholding\nfor point densification, opacity thresholding for pruning, and regular\nall-points opacity reset. However, we reveal that this strategy is limited in\ntackling intricate/special image regions (e.g., transparent) as it is unable to\nidentify all the 3D zones that require point densification, and lacking an\nappropriate mechanism to handle the ill-conditioned points with negative\nimpacts (occlusion due to false high opacity). To address these limitations, we\npropose a Localized Point Management (LPM) strategy, capable of identifying\nthose error-contributing zones in the highest demand for both point addition\nand geometry calibration. Zone identification is achieved by leveraging the\nunderlying multiview geometry constraints, with the guidance of image rendering\nerrors. We apply point densification in the identified zone, whilst resetting\nthe opacity of those points residing in front of these regions so that a new\nopportunity is created to correct ill-conditioned points. Serving as a\nversatile plugin, LPM can be seamlessly integrated into existing 3D Gaussian\nSplatting models. Experimental evaluation across both static 3D and dynamic 4D\nscenes validate the efficacy of our LPM strategy in boosting a variety of\nexisting 3DGS models both quantitatively and qualitatively. Notably, LPM\nimproves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-art\nrendering quality while retaining real-time speeds, outperforming on\nchallenging datasets such as Tanks & Temples and the Neural 3D Video Dataset.\n", "link": "http://arxiv.org/abs/2406.04251v2", "date": "2024-06-13", "relevancy": 3.1491, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6968}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6562}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20with%20Localized%20Points%20Management&body=Title%3A%20Gaussian%20Splatting%20with%20Localized%20Points%20Management%0AAuthor%3A%20Haosen%20Yang%20and%20Chenhao%20Zhang%20and%20Wenqing%20Wang%20and%20Marco%20Volino%20and%20Adrian%20Hilton%20and%20Li%20Zhang%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20Point%20management%20is%20a%20critical%20component%20in%20optimizing%203D%20Gaussian%20Splatting%0A%283DGS%29%20models%2C%20as%20the%20point%20initiation%20%28e.g.%2C%20via%20structure%20from%20motion%29%20is%0Adistributionally%20inappropriate.%20Typically%2C%20the%20Adaptive%20Density%20Control%20%28ADC%29%0Aalgorithm%20is%20applied%2C%20leveraging%20view-averaged%20gradient%20magnitude%20thresholding%0Afor%20point%20densification%2C%20opacity%20thresholding%20for%20pruning%2C%20and%20regular%0Aall-points%20opacity%20reset.%20However%2C%20we%20reveal%20that%20this%20strategy%20is%20limited%20in%0Atackling%20intricate/special%20image%20regions%20%28e.g.%2C%20transparent%29%20as%20it%20is%20unable%20to%0Aidentify%20all%20the%203D%20zones%20that%20require%20point%20densification%2C%20and%20lacking%20an%0Aappropriate%20mechanism%20to%20handle%20the%20ill-conditioned%20points%20with%20negative%0Aimpacts%20%28occlusion%20due%20to%20false%20high%20opacity%29.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20Localized%20Point%20Management%20%28LPM%29%20strategy%2C%20capable%20of%20identifying%0Athose%20error-contributing%20zones%20in%20the%20highest%20demand%20for%20both%20point%20addition%0Aand%20geometry%20calibration.%20Zone%20identification%20is%20achieved%20by%20leveraging%20the%0Aunderlying%20multiview%20geometry%20constraints%2C%20with%20the%20guidance%20of%20image%20rendering%0Aerrors.%20We%20apply%20point%20densification%20in%20the%20identified%20zone%2C%20whilst%20resetting%0Athe%20opacity%20of%20those%20points%20residing%20in%20front%20of%20these%20regions%20so%20that%20a%20new%0Aopportunity%20is%20created%20to%20correct%20ill-conditioned%20points.%20Serving%20as%20a%0Aversatile%20plugin%2C%20LPM%20can%20be%20seamlessly%20integrated%20into%20existing%203D%20Gaussian%0ASplatting%20models.%20Experimental%20evaluation%20across%20both%20static%203D%20and%20dynamic%204D%0Ascenes%20validate%20the%20efficacy%20of%20our%20LPM%20strategy%20in%20boosting%20a%20variety%20of%0Aexisting%203DGS%20models%20both%20quantitatively%20and%20qualitatively.%20Notably%2C%20LPM%0Aimproves%20both%20vanilla%203DGS%20and%20SpaceTimeGS%20to%20achieve%20state-of-the-art%0Arendering%20quality%20while%20retaining%20real-time%20speeds%2C%20outperforming%20on%0Achallenging%20datasets%20such%20as%20Tanks%20%26%20Temples%20and%20the%20Neural%203D%20Video%20Dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04251v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520with%2520Localized%2520Points%2520Management%26entry.906535625%3DHaosen%2520Yang%2520and%2520Chenhao%2520Zhang%2520and%2520Wenqing%2520Wang%2520and%2520Marco%2520Volino%2520and%2520Adrian%2520Hilton%2520and%2520Li%2520Zhang%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520Point%2520management%2520is%2520a%2520critical%2520component%2520in%2520optimizing%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%2520models%252C%2520as%2520the%2520point%2520initiation%2520%2528e.g.%252C%2520via%2520structure%2520from%2520motion%2529%2520is%250Adistributionally%2520inappropriate.%2520Typically%252C%2520the%2520Adaptive%2520Density%2520Control%2520%2528ADC%2529%250Aalgorithm%2520is%2520applied%252C%2520leveraging%2520view-averaged%2520gradient%2520magnitude%2520thresholding%250Afor%2520point%2520densification%252C%2520opacity%2520thresholding%2520for%2520pruning%252C%2520and%2520regular%250Aall-points%2520opacity%2520reset.%2520However%252C%2520we%2520reveal%2520that%2520this%2520strategy%2520is%2520limited%2520in%250Atackling%2520intricate/special%2520image%2520regions%2520%2528e.g.%252C%2520transparent%2529%2520as%2520it%2520is%2520unable%2520to%250Aidentify%2520all%2520the%25203D%2520zones%2520that%2520require%2520point%2520densification%252C%2520and%2520lacking%2520an%250Aappropriate%2520mechanism%2520to%2520handle%2520the%2520ill-conditioned%2520points%2520with%2520negative%250Aimpacts%2520%2528occlusion%2520due%2520to%2520false%2520high%2520opacity%2529.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520Localized%2520Point%2520Management%2520%2528LPM%2529%2520strategy%252C%2520capable%2520of%2520identifying%250Athose%2520error-contributing%2520zones%2520in%2520the%2520highest%2520demand%2520for%2520both%2520point%2520addition%250Aand%2520geometry%2520calibration.%2520Zone%2520identification%2520is%2520achieved%2520by%2520leveraging%2520the%250Aunderlying%2520multiview%2520geometry%2520constraints%252C%2520with%2520the%2520guidance%2520of%2520image%2520rendering%250Aerrors.%2520We%2520apply%2520point%2520densification%2520in%2520the%2520identified%2520zone%252C%2520whilst%2520resetting%250Athe%2520opacity%2520of%2520those%2520points%2520residing%2520in%2520front%2520of%2520these%2520regions%2520so%2520that%2520a%2520new%250Aopportunity%2520is%2520created%2520to%2520correct%2520ill-conditioned%2520points.%2520Serving%2520as%2520a%250Aversatile%2520plugin%252C%2520LPM%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%25203D%2520Gaussian%250ASplatting%2520models.%2520Experimental%2520evaluation%2520across%2520both%2520static%25203D%2520and%2520dynamic%25204D%250Ascenes%2520validate%2520the%2520efficacy%2520of%2520our%2520LPM%2520strategy%2520in%2520boosting%2520a%2520variety%2520of%250Aexisting%25203DGS%2520models%2520both%2520quantitatively%2520and%2520qualitatively.%2520Notably%252C%2520LPM%250Aimproves%2520both%2520vanilla%25203DGS%2520and%2520SpaceTimeGS%2520to%2520achieve%2520state-of-the-art%250Arendering%2520quality%2520while%2520retaining%2520real-time%2520speeds%252C%2520outperforming%2520on%250Achallenging%2520datasets%2520such%2520as%2520Tanks%2520%2526%2520Temples%2520and%2520the%2520Neural%25203D%2520Video%2520Dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04251v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20with%20Localized%20Points%20Management&entry.906535625=Haosen%20Yang%20and%20Chenhao%20Zhang%20and%20Wenqing%20Wang%20and%20Marco%20Volino%20and%20Adrian%20Hilton%20and%20Li%20Zhang%20and%20Xiatian%20Zhu&entry.1292438233=%20%20Point%20management%20is%20a%20critical%20component%20in%20optimizing%203D%20Gaussian%20Splatting%0A%283DGS%29%20models%2C%20as%20the%20point%20initiation%20%28e.g.%2C%20via%20structure%20from%20motion%29%20is%0Adistributionally%20inappropriate.%20Typically%2C%20the%20Adaptive%20Density%20Control%20%28ADC%29%0Aalgorithm%20is%20applied%2C%20leveraging%20view-averaged%20gradient%20magnitude%20thresholding%0Afor%20point%20densification%2C%20opacity%20thresholding%20for%20pruning%2C%20and%20regular%0Aall-points%20opacity%20reset.%20However%2C%20we%20reveal%20that%20this%20strategy%20is%20limited%20in%0Atackling%20intricate/special%20image%20regions%20%28e.g.%2C%20transparent%29%20as%20it%20is%20unable%20to%0Aidentify%20all%20the%203D%20zones%20that%20require%20point%20densification%2C%20and%20lacking%20an%0Aappropriate%20mechanism%20to%20handle%20the%20ill-conditioned%20points%20with%20negative%0Aimpacts%20%28occlusion%20due%20to%20false%20high%20opacity%29.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20Localized%20Point%20Management%20%28LPM%29%20strategy%2C%20capable%20of%20identifying%0Athose%20error-contributing%20zones%20in%20the%20highest%20demand%20for%20both%20point%20addition%0Aand%20geometry%20calibration.%20Zone%20identification%20is%20achieved%20by%20leveraging%20the%0Aunderlying%20multiview%20geometry%20constraints%2C%20with%20the%20guidance%20of%20image%20rendering%0Aerrors.%20We%20apply%20point%20densification%20in%20the%20identified%20zone%2C%20whilst%20resetting%0Athe%20opacity%20of%20those%20points%20residing%20in%20front%20of%20these%20regions%20so%20that%20a%20new%0Aopportunity%20is%20created%20to%20correct%20ill-conditioned%20points.%20Serving%20as%20a%0Aversatile%20plugin%2C%20LPM%20can%20be%20seamlessly%20integrated%20into%20existing%203D%20Gaussian%0ASplatting%20models.%20Experimental%20evaluation%20across%20both%20static%203D%20and%20dynamic%204D%0Ascenes%20validate%20the%20efficacy%20of%20our%20LPM%20strategy%20in%20boosting%20a%20variety%20of%0Aexisting%203DGS%20models%20both%20quantitatively%20and%20qualitatively.%20Notably%2C%20LPM%0Aimproves%20both%20vanilla%203DGS%20and%20SpaceTimeGS%20to%20achieve%20state-of-the-art%0Arendering%20quality%20while%20retaining%20real-time%20speeds%2C%20outperforming%20on%0Achallenging%20datasets%20such%20as%20Tanks%20%26%20Temples%20and%20the%20Neural%203D%20Video%20Dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04251v2&entry.124074799=Read"},
{"title": "WonderWorld: Interactive 3D Scene Generation from a Single Image", "author": "Hong-Xing Yu and Haoyi Duan and Charles Herrmann and William T. Freeman and Jiajun Wu", "abstract": "  We present WonderWorld, a novel framework for \\emph{interactive} 3D scene\nextrapolation that enables users to explore and shape virtual environments\nbased on a single input image and user-specified text. While significant\nimprovements have been made to the visual quality of scene generation, existing\nmethods are run offline, taking tens of minutes to hours to generate a scene.\nBy leveraging Fast Gaussian Surfels and a guided diffusion-based depth\nestimation method, WonderWorld generates geometrically consistent extrapolation\nwhile significantly reducing computational time. Our framework generates\nconnected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU,\nenabling real-time user interaction and exploration. We demonstrate the\npotential of WonderWorld for applications in virtual reality, gaming, and\ncreative design, where users can quickly generate and navigate immersive,\npotentially infinite virtual worlds from a single image. Our approach\nrepresents a significant advancement in interactive 3D scene generation,\nopening up new possibilities for user-driven content creation and exploration\nin virtual environments. We will release full code and software for\nreproducibility. Project website: https://WonderWorld-2024.github.io/\n", "link": "http://arxiv.org/abs/2406.09394v1", "date": "2024-06-13", "relevancy": 3.122, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6479}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6479}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image&body=Title%3A%20WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Hong-Xing%20Yu%20and%20Haoyi%20Duan%20and%20Charles%20Herrmann%20and%20William%20T.%20Freeman%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20We%20present%20WonderWorld%2C%20a%20novel%20framework%20for%20%5Cemph%7Binteractive%7D%203D%20scene%0Aextrapolation%20that%20enables%20users%20to%20explore%20and%20shape%20virtual%20environments%0Abased%20on%20a%20single%20input%20image%20and%20user-specified%20text.%20While%20significant%0Aimprovements%20have%20been%20made%20to%20the%20visual%20quality%20of%20scene%20generation%2C%20existing%0Amethods%20are%20run%20offline%2C%20taking%20tens%20of%20minutes%20to%20hours%20to%20generate%20a%20scene.%0ABy%20leveraging%20Fast%20Gaussian%20Surfels%20and%20a%20guided%20diffusion-based%20depth%0Aestimation%20method%2C%20WonderWorld%20generates%20geometrically%20consistent%20extrapolation%0Awhile%20significantly%20reducing%20computational%20time.%20Our%20framework%20generates%0Aconnected%20and%20diverse%203D%20scenes%20in%20less%20than%2010%20seconds%20on%20a%20single%20A6000%20GPU%2C%0Aenabling%20real-time%20user%20interaction%20and%20exploration.%20We%20demonstrate%20the%0Apotential%20of%20WonderWorld%20for%20applications%20in%20virtual%20reality%2C%20gaming%2C%20and%0Acreative%20design%2C%20where%20users%20can%20quickly%20generate%20and%20navigate%20immersive%2C%0Apotentially%20infinite%20virtual%20worlds%20from%20a%20single%20image.%20Our%20approach%0Arepresents%20a%20significant%20advancement%20in%20interactive%203D%20scene%20generation%2C%0Aopening%20up%20new%20possibilities%20for%20user-driven%20content%20creation%20and%20exploration%0Ain%20virtual%20environments.%20We%20will%20release%20full%20code%20and%20software%20for%0Areproducibility.%20Project%20website%3A%20https%3A//WonderWorld-2024.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWonderWorld%253A%2520Interactive%25203D%2520Scene%2520Generation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DHong-Xing%2520Yu%2520and%2520Haoyi%2520Duan%2520and%2520Charles%2520Herrmann%2520and%2520William%2520T.%2520Freeman%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520We%2520present%2520WonderWorld%252C%2520a%2520novel%2520framework%2520for%2520%255Cemph%257Binteractive%257D%25203D%2520scene%250Aextrapolation%2520that%2520enables%2520users%2520to%2520explore%2520and%2520shape%2520virtual%2520environments%250Abased%2520on%2520a%2520single%2520input%2520image%2520and%2520user-specified%2520text.%2520While%2520significant%250Aimprovements%2520have%2520been%2520made%2520to%2520the%2520visual%2520quality%2520of%2520scene%2520generation%252C%2520existing%250Amethods%2520are%2520run%2520offline%252C%2520taking%2520tens%2520of%2520minutes%2520to%2520hours%2520to%2520generate%2520a%2520scene.%250ABy%2520leveraging%2520Fast%2520Gaussian%2520Surfels%2520and%2520a%2520guided%2520diffusion-based%2520depth%250Aestimation%2520method%252C%2520WonderWorld%2520generates%2520geometrically%2520consistent%2520extrapolation%250Awhile%2520significantly%2520reducing%2520computational%2520time.%2520Our%2520framework%2520generates%250Aconnected%2520and%2520diverse%25203D%2520scenes%2520in%2520less%2520than%252010%2520seconds%2520on%2520a%2520single%2520A6000%2520GPU%252C%250Aenabling%2520real-time%2520user%2520interaction%2520and%2520exploration.%2520We%2520demonstrate%2520the%250Apotential%2520of%2520WonderWorld%2520for%2520applications%2520in%2520virtual%2520reality%252C%2520gaming%252C%2520and%250Acreative%2520design%252C%2520where%2520users%2520can%2520quickly%2520generate%2520and%2520navigate%2520immersive%252C%250Apotentially%2520infinite%2520virtual%2520worlds%2520from%2520a%2520single%2520image.%2520Our%2520approach%250Arepresents%2520a%2520significant%2520advancement%2520in%2520interactive%25203D%2520scene%2520generation%252C%250Aopening%2520up%2520new%2520possibilities%2520for%2520user-driven%2520content%2520creation%2520and%2520exploration%250Ain%2520virtual%2520environments.%2520We%2520will%2520release%2520full%2520code%2520and%2520software%2520for%250Areproducibility.%2520Project%2520website%253A%2520https%253A//WonderWorld-2024.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image&entry.906535625=Hong-Xing%20Yu%20and%20Haoyi%20Duan%20and%20Charles%20Herrmann%20and%20William%20T.%20Freeman%20and%20Jiajun%20Wu&entry.1292438233=%20%20We%20present%20WonderWorld%2C%20a%20novel%20framework%20for%20%5Cemph%7Binteractive%7D%203D%20scene%0Aextrapolation%20that%20enables%20users%20to%20explore%20and%20shape%20virtual%20environments%0Abased%20on%20a%20single%20input%20image%20and%20user-specified%20text.%20While%20significant%0Aimprovements%20have%20been%20made%20to%20the%20visual%20quality%20of%20scene%20generation%2C%20existing%0Amethods%20are%20run%20offline%2C%20taking%20tens%20of%20minutes%20to%20hours%20to%20generate%20a%20scene.%0ABy%20leveraging%20Fast%20Gaussian%20Surfels%20and%20a%20guided%20diffusion-based%20depth%0Aestimation%20method%2C%20WonderWorld%20generates%20geometrically%20consistent%20extrapolation%0Awhile%20significantly%20reducing%20computational%20time.%20Our%20framework%20generates%0Aconnected%20and%20diverse%203D%20scenes%20in%20less%20than%2010%20seconds%20on%20a%20single%20A6000%20GPU%2C%0Aenabling%20real-time%20user%20interaction%20and%20exploration.%20We%20demonstrate%20the%0Apotential%20of%20WonderWorld%20for%20applications%20in%20virtual%20reality%2C%20gaming%2C%20and%0Acreative%20design%2C%20where%20users%20can%20quickly%20generate%20and%20navigate%20immersive%2C%0Apotentially%20infinite%20virtual%20worlds%20from%20a%20single%20image.%20Our%20approach%0Arepresents%20a%20significant%20advancement%20in%20interactive%203D%20scene%20generation%2C%0Aopening%20up%20new%20possibilities%20for%20user-driven%20content%20creation%20and%20exploration%0Ain%20virtual%20environments.%20We%20will%20release%20full%20code%20and%20software%20for%0Areproducibility.%20Project%20website%3A%20https%3A//WonderWorld-2024.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09394v1&entry.124074799=Read"},
{"title": "4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities", "author": "Roman Bachmann and O\u011fuzhan Fatih Kar and David Mizrahi and Ali Garjani and Mingfei Gao and David Griffiths and Jiaming Hu and Afshin Dehghan and Amir Zamir", "abstract": "  Current multimodal and multitask foundation models like 4M or UnifiedIO show\npromising results, but in practice their out-of-the-box abilities to accept\ndiverse inputs and perform diverse tasks are limited by the (usually rather\nsmall) number of modalities and tasks they are trained on. In this paper, we\nexpand upon the capabilities of them by training a single model on tens of\nhighly diverse modalities and by performing co-training on large-scale\nmultimodal datasets and text corpora. This includes training on several\nsemantic and geometric modalities, feature maps from recent state of the art\nmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAM\nand 4DHumans, and a range of new modalities that allow for novel ways to\ninteract with the model and steer the generation, for example image metadata or\ncolor palettes. A crucial step in this process is performing discrete\ntokenization on various modalities, whether they are image-like, neural network\nfeature maps, vectors, structured data like instance segmentation or human\nposes, or data that can be represented as text. Through this, we expand on the\nout-of-the-box capabilities of multimodal models and specifically show the\npossibility of training one model to solve at least 3x more tasks/modalities\nthan existing ones and doing so without a loss in performance. This enables\nmore fine-grained and controllable multimodal generation capabilities and\nallows us to study the distillation of models trained on diverse data and\nobjectives into a unified model. We successfully scale the training to a three\nbillion parameter model using tens of modalities and different datasets. The\nresulting models and training code are open sourced at 4m.epfl.ch.\n", "link": "http://arxiv.org/abs/2406.09406v1", "date": "2024-06-13", "relevancy": 3.0182, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6413}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5848}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204M-21%3A%20An%20Any-to-Any%20Vision%20Model%20for%20Tens%20of%20Tasks%20and%20Modalities&body=Title%3A%204M-21%3A%20An%20Any-to-Any%20Vision%20Model%20for%20Tens%20of%20Tasks%20and%20Modalities%0AAuthor%3A%20Roman%20Bachmann%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20David%20Mizrahi%20and%20Ali%20Garjani%20and%20Mingfei%20Gao%20and%20David%20Griffiths%20and%20Jiaming%20Hu%20and%20Afshin%20Dehghan%20and%20Amir%20Zamir%0AAbstract%3A%20%20%20Current%20multimodal%20and%20multitask%20foundation%20models%20like%204M%20or%20UnifiedIO%20show%0Apromising%20results%2C%20but%20in%20practice%20their%20out-of-the-box%20abilities%20to%20accept%0Adiverse%20inputs%20and%20perform%20diverse%20tasks%20are%20limited%20by%20the%20%28usually%20rather%0Asmall%29%20number%20of%20modalities%20and%20tasks%20they%20are%20trained%20on.%20In%20this%20paper%2C%20we%0Aexpand%20upon%20the%20capabilities%20of%20them%20by%20training%20a%20single%20model%20on%20tens%20of%0Ahighly%20diverse%20modalities%20and%20by%20performing%20co-training%20on%20large-scale%0Amultimodal%20datasets%20and%20text%20corpora.%20This%20includes%20training%20on%20several%0Asemantic%20and%20geometric%20modalities%2C%20feature%20maps%20from%20recent%20state%20of%20the%20art%0Amodels%20like%20DINOv2%20and%20ImageBind%2C%20pseudo%20labels%20of%20specialist%20models%20like%20SAM%0Aand%204DHumans%2C%20and%20a%20range%20of%20new%20modalities%20that%20allow%20for%20novel%20ways%20to%0Ainteract%20with%20the%20model%20and%20steer%20the%20generation%2C%20for%20example%20image%20metadata%20or%0Acolor%20palettes.%20A%20crucial%20step%20in%20this%20process%20is%20performing%20discrete%0Atokenization%20on%20various%20modalities%2C%20whether%20they%20are%20image-like%2C%20neural%20network%0Afeature%20maps%2C%20vectors%2C%20structured%20data%20like%20instance%20segmentation%20or%20human%0Aposes%2C%20or%20data%20that%20can%20be%20represented%20as%20text.%20Through%20this%2C%20we%20expand%20on%20the%0Aout-of-the-box%20capabilities%20of%20multimodal%20models%20and%20specifically%20show%20the%0Apossibility%20of%20training%20one%20model%20to%20solve%20at%20least%203x%20more%20tasks/modalities%0Athan%20existing%20ones%20and%20doing%20so%20without%20a%20loss%20in%20performance.%20This%20enables%0Amore%20fine-grained%20and%20controllable%20multimodal%20generation%20capabilities%20and%0Aallows%20us%20to%20study%20the%20distillation%20of%20models%20trained%20on%20diverse%20data%20and%0Aobjectives%20into%20a%20unified%20model.%20We%20successfully%20scale%20the%20training%20to%20a%20three%0Abillion%20parameter%20model%20using%20tens%20of%20modalities%20and%20different%20datasets.%20The%0Aresulting%20models%20and%20training%20code%20are%20open%20sourced%20at%204m.epfl.ch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4M-21%253A%2520An%2520Any-to-Any%2520Vision%2520Model%2520for%2520Tens%2520of%2520Tasks%2520and%2520Modalities%26entry.906535625%3DRoman%2520Bachmann%2520and%2520O%25C4%259Fuzhan%2520Fatih%2520Kar%2520and%2520David%2520Mizrahi%2520and%2520Ali%2520Garjani%2520and%2520Mingfei%2520Gao%2520and%2520David%2520Griffiths%2520and%2520Jiaming%2520Hu%2520and%2520Afshin%2520Dehghan%2520and%2520Amir%2520Zamir%26entry.1292438233%3D%2520%2520Current%2520multimodal%2520and%2520multitask%2520foundation%2520models%2520like%25204M%2520or%2520UnifiedIO%2520show%250Apromising%2520results%252C%2520but%2520in%2520practice%2520their%2520out-of-the-box%2520abilities%2520to%2520accept%250Adiverse%2520inputs%2520and%2520perform%2520diverse%2520tasks%2520are%2520limited%2520by%2520the%2520%2528usually%2520rather%250Asmall%2529%2520number%2520of%2520modalities%2520and%2520tasks%2520they%2520are%2520trained%2520on.%2520In%2520this%2520paper%252C%2520we%250Aexpand%2520upon%2520the%2520capabilities%2520of%2520them%2520by%2520training%2520a%2520single%2520model%2520on%2520tens%2520of%250Ahighly%2520diverse%2520modalities%2520and%2520by%2520performing%2520co-training%2520on%2520large-scale%250Amultimodal%2520datasets%2520and%2520text%2520corpora.%2520This%2520includes%2520training%2520on%2520several%250Asemantic%2520and%2520geometric%2520modalities%252C%2520feature%2520maps%2520from%2520recent%2520state%2520of%2520the%2520art%250Amodels%2520like%2520DINOv2%2520and%2520ImageBind%252C%2520pseudo%2520labels%2520of%2520specialist%2520models%2520like%2520SAM%250Aand%25204DHumans%252C%2520and%2520a%2520range%2520of%2520new%2520modalities%2520that%2520allow%2520for%2520novel%2520ways%2520to%250Ainteract%2520with%2520the%2520model%2520and%2520steer%2520the%2520generation%252C%2520for%2520example%2520image%2520metadata%2520or%250Acolor%2520palettes.%2520A%2520crucial%2520step%2520in%2520this%2520process%2520is%2520performing%2520discrete%250Atokenization%2520on%2520various%2520modalities%252C%2520whether%2520they%2520are%2520image-like%252C%2520neural%2520network%250Afeature%2520maps%252C%2520vectors%252C%2520structured%2520data%2520like%2520instance%2520segmentation%2520or%2520human%250Aposes%252C%2520or%2520data%2520that%2520can%2520be%2520represented%2520as%2520text.%2520Through%2520this%252C%2520we%2520expand%2520on%2520the%250Aout-of-the-box%2520capabilities%2520of%2520multimodal%2520models%2520and%2520specifically%2520show%2520the%250Apossibility%2520of%2520training%2520one%2520model%2520to%2520solve%2520at%2520least%25203x%2520more%2520tasks/modalities%250Athan%2520existing%2520ones%2520and%2520doing%2520so%2520without%2520a%2520loss%2520in%2520performance.%2520This%2520enables%250Amore%2520fine-grained%2520and%2520controllable%2520multimodal%2520generation%2520capabilities%2520and%250Aallows%2520us%2520to%2520study%2520the%2520distillation%2520of%2520models%2520trained%2520on%2520diverse%2520data%2520and%250Aobjectives%2520into%2520a%2520unified%2520model.%2520We%2520successfully%2520scale%2520the%2520training%2520to%2520a%2520three%250Abillion%2520parameter%2520model%2520using%2520tens%2520of%2520modalities%2520and%2520different%2520datasets.%2520The%250Aresulting%2520models%2520and%2520training%2520code%2520are%2520open%2520sourced%2520at%25204m.epfl.ch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4M-21%3A%20An%20Any-to-Any%20Vision%20Model%20for%20Tens%20of%20Tasks%20and%20Modalities&entry.906535625=Roman%20Bachmann%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20David%20Mizrahi%20and%20Ali%20Garjani%20and%20Mingfei%20Gao%20and%20David%20Griffiths%20and%20Jiaming%20Hu%20and%20Afshin%20Dehghan%20and%20Amir%20Zamir&entry.1292438233=%20%20Current%20multimodal%20and%20multitask%20foundation%20models%20like%204M%20or%20UnifiedIO%20show%0Apromising%20results%2C%20but%20in%20practice%20their%20out-of-the-box%20abilities%20to%20accept%0Adiverse%20inputs%20and%20perform%20diverse%20tasks%20are%20limited%20by%20the%20%28usually%20rather%0Asmall%29%20number%20of%20modalities%20and%20tasks%20they%20are%20trained%20on.%20In%20this%20paper%2C%20we%0Aexpand%20upon%20the%20capabilities%20of%20them%20by%20training%20a%20single%20model%20on%20tens%20of%0Ahighly%20diverse%20modalities%20and%20by%20performing%20co-training%20on%20large-scale%0Amultimodal%20datasets%20and%20text%20corpora.%20This%20includes%20training%20on%20several%0Asemantic%20and%20geometric%20modalities%2C%20feature%20maps%20from%20recent%20state%20of%20the%20art%0Amodels%20like%20DINOv2%20and%20ImageBind%2C%20pseudo%20labels%20of%20specialist%20models%20like%20SAM%0Aand%204DHumans%2C%20and%20a%20range%20of%20new%20modalities%20that%20allow%20for%20novel%20ways%20to%0Ainteract%20with%20the%20model%20and%20steer%20the%20generation%2C%20for%20example%20image%20metadata%20or%0Acolor%20palettes.%20A%20crucial%20step%20in%20this%20process%20is%20performing%20discrete%0Atokenization%20on%20various%20modalities%2C%20whether%20they%20are%20image-like%2C%20neural%20network%0Afeature%20maps%2C%20vectors%2C%20structured%20data%20like%20instance%20segmentation%20or%20human%0Aposes%2C%20or%20data%20that%20can%20be%20represented%20as%20text.%20Through%20this%2C%20we%20expand%20on%20the%0Aout-of-the-box%20capabilities%20of%20multimodal%20models%20and%20specifically%20show%20the%0Apossibility%20of%20training%20one%20model%20to%20solve%20at%20least%203x%20more%20tasks/modalities%0Athan%20existing%20ones%20and%20doing%20so%20without%20a%20loss%20in%20performance.%20This%20enables%0Amore%20fine-grained%20and%20controllable%20multimodal%20generation%20capabilities%20and%0Aallows%20us%20to%20study%20the%20distillation%20of%20models%20trained%20on%20diverse%20data%20and%0Aobjectives%20into%20a%20unified%20model.%20We%20successfully%20scale%20the%20training%20to%20a%20three%0Abillion%20parameter%20model%20using%20tens%20of%20modalities%20and%20different%20datasets.%20The%0Aresulting%20models%20and%20training%20code%20are%20open%20sourced%20at%204m.epfl.ch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09406v1&entry.124074799=Read"},
{"title": "SimGen: Simulator-conditioned Driving Scene Generation", "author": "Yunsong Zhou and Michael Simon and Zhenghao Peng and Sicheng Mo and Hongzi Zhu and Minyi Guo and Bolei Zhou", "abstract": "  Controllable synthetic data generation can substantially lower the annotation\ncost of training data in autonomous driving research and development. Prior\nworks use diffusion models to generate driving images conditioned on the 3D\nobject layout. However, those models are trained on small-scale datasets like\nnuScenes, which lack appearance and layout diversity. Moreover, the trained\nmodels can only generate images based on the real-world layout data from the\nvalidation set of the same dataset, where overfitting might happen. In this\nwork, we introduce a simulator-conditioned scene generation framework called\nSimGen that can learn to generate diverse driving scenes by mixing data from\nthe simulator and the real world. It uses a novel cascade diffusion pipeline to\naddress challenging sim-to-real gaps and multi-condition conflicts. A driving\nvideo dataset DIVA is collected to enhance the generative diversity of SimGen,\nwhich contains over 147.5 hours of real-world driving videos from 73 locations\nworldwide and simulated driving data from the MetaDrive simulator. SimGen\nachieves superior generation quality and diversity while preserving\ncontrollability based on the text prompt and the layout pulled from a\nsimulator. We further demonstrate the improvements brought by SimGen for\nsynthetic data augmentation on the BEV detection and segmentation task and\nshowcase its capability in safety-critical data generation. Code, data, and\nmodels will be made available.\n", "link": "http://arxiv.org/abs/2406.09386v1", "date": "2024-06-13", "relevancy": 2.9684, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6244}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimGen%3A%20Simulator-conditioned%20Driving%20Scene%20Generation&body=Title%3A%20SimGen%3A%20Simulator-conditioned%20Driving%20Scene%20Generation%0AAuthor%3A%20Yunsong%20Zhou%20and%20Michael%20Simon%20and%20Zhenghao%20Peng%20and%20Sicheng%20Mo%20and%20Hongzi%20Zhu%20and%20Minyi%20Guo%20and%20Bolei%20Zhou%0AAbstract%3A%20%20%20Controllable%20synthetic%20data%20generation%20can%20substantially%20lower%20the%20annotation%0Acost%20of%20training%20data%20in%20autonomous%20driving%20research%20and%20development.%20Prior%0Aworks%20use%20diffusion%20models%20to%20generate%20driving%20images%20conditioned%20on%20the%203D%0Aobject%20layout.%20However%2C%20those%20models%20are%20trained%20on%20small-scale%20datasets%20like%0AnuScenes%2C%20which%20lack%20appearance%20and%20layout%20diversity.%20Moreover%2C%20the%20trained%0Amodels%20can%20only%20generate%20images%20based%20on%20the%20real-world%20layout%20data%20from%20the%0Avalidation%20set%20of%20the%20same%20dataset%2C%20where%20overfitting%20might%20happen.%20In%20this%0Awork%2C%20we%20introduce%20a%20simulator-conditioned%20scene%20generation%20framework%20called%0ASimGen%20that%20can%20learn%20to%20generate%20diverse%20driving%20scenes%20by%20mixing%20data%20from%0Athe%20simulator%20and%20the%20real%20world.%20It%20uses%20a%20novel%20cascade%20diffusion%20pipeline%20to%0Aaddress%20challenging%20sim-to-real%20gaps%20and%20multi-condition%20conflicts.%20A%20driving%0Avideo%20dataset%20DIVA%20is%20collected%20to%20enhance%20the%20generative%20diversity%20of%20SimGen%2C%0Awhich%20contains%20over%20147.5%20hours%20of%20real-world%20driving%20videos%20from%2073%20locations%0Aworldwide%20and%20simulated%20driving%20data%20from%20the%20MetaDrive%20simulator.%20SimGen%0Aachieves%20superior%20generation%20quality%20and%20diversity%20while%20preserving%0Acontrollability%20based%20on%20the%20text%20prompt%20and%20the%20layout%20pulled%20from%20a%0Asimulator.%20We%20further%20demonstrate%20the%20improvements%20brought%20by%20SimGen%20for%0Asynthetic%20data%20augmentation%20on%20the%20BEV%20detection%20and%20segmentation%20task%20and%0Ashowcase%20its%20capability%20in%20safety-critical%20data%20generation.%20Code%2C%20data%2C%20and%0Amodels%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimGen%253A%2520Simulator-conditioned%2520Driving%2520Scene%2520Generation%26entry.906535625%3DYunsong%2520Zhou%2520and%2520Michael%2520Simon%2520and%2520Zhenghao%2520Peng%2520and%2520Sicheng%2520Mo%2520and%2520Hongzi%2520Zhu%2520and%2520Minyi%2520Guo%2520and%2520Bolei%2520Zhou%26entry.1292438233%3D%2520%2520Controllable%2520synthetic%2520data%2520generation%2520can%2520substantially%2520lower%2520the%2520annotation%250Acost%2520of%2520training%2520data%2520in%2520autonomous%2520driving%2520research%2520and%2520development.%2520Prior%250Aworks%2520use%2520diffusion%2520models%2520to%2520generate%2520driving%2520images%2520conditioned%2520on%2520the%25203D%250Aobject%2520layout.%2520However%252C%2520those%2520models%2520are%2520trained%2520on%2520small-scale%2520datasets%2520like%250AnuScenes%252C%2520which%2520lack%2520appearance%2520and%2520layout%2520diversity.%2520Moreover%252C%2520the%2520trained%250Amodels%2520can%2520only%2520generate%2520images%2520based%2520on%2520the%2520real-world%2520layout%2520data%2520from%2520the%250Avalidation%2520set%2520of%2520the%2520same%2520dataset%252C%2520where%2520overfitting%2520might%2520happen.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520simulator-conditioned%2520scene%2520generation%2520framework%2520called%250ASimGen%2520that%2520can%2520learn%2520to%2520generate%2520diverse%2520driving%2520scenes%2520by%2520mixing%2520data%2520from%250Athe%2520simulator%2520and%2520the%2520real%2520world.%2520It%2520uses%2520a%2520novel%2520cascade%2520diffusion%2520pipeline%2520to%250Aaddress%2520challenging%2520sim-to-real%2520gaps%2520and%2520multi-condition%2520conflicts.%2520A%2520driving%250Avideo%2520dataset%2520DIVA%2520is%2520collected%2520to%2520enhance%2520the%2520generative%2520diversity%2520of%2520SimGen%252C%250Awhich%2520contains%2520over%2520147.5%2520hours%2520of%2520real-world%2520driving%2520videos%2520from%252073%2520locations%250Aworldwide%2520and%2520simulated%2520driving%2520data%2520from%2520the%2520MetaDrive%2520simulator.%2520SimGen%250Aachieves%2520superior%2520generation%2520quality%2520and%2520diversity%2520while%2520preserving%250Acontrollability%2520based%2520on%2520the%2520text%2520prompt%2520and%2520the%2520layout%2520pulled%2520from%2520a%250Asimulator.%2520We%2520further%2520demonstrate%2520the%2520improvements%2520brought%2520by%2520SimGen%2520for%250Asynthetic%2520data%2520augmentation%2520on%2520the%2520BEV%2520detection%2520and%2520segmentation%2520task%2520and%250Ashowcase%2520its%2520capability%2520in%2520safety-critical%2520data%2520generation.%2520Code%252C%2520data%252C%2520and%250Amodels%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimGen%3A%20Simulator-conditioned%20Driving%20Scene%20Generation&entry.906535625=Yunsong%20Zhou%20and%20Michael%20Simon%20and%20Zhenghao%20Peng%20and%20Sicheng%20Mo%20and%20Hongzi%20Zhu%20and%20Minyi%20Guo%20and%20Bolei%20Zhou&entry.1292438233=%20%20Controllable%20synthetic%20data%20generation%20can%20substantially%20lower%20the%20annotation%0Acost%20of%20training%20data%20in%20autonomous%20driving%20research%20and%20development.%20Prior%0Aworks%20use%20diffusion%20models%20to%20generate%20driving%20images%20conditioned%20on%20the%203D%0Aobject%20layout.%20However%2C%20those%20models%20are%20trained%20on%20small-scale%20datasets%20like%0AnuScenes%2C%20which%20lack%20appearance%20and%20layout%20diversity.%20Moreover%2C%20the%20trained%0Amodels%20can%20only%20generate%20images%20based%20on%20the%20real-world%20layout%20data%20from%20the%0Avalidation%20set%20of%20the%20same%20dataset%2C%20where%20overfitting%20might%20happen.%20In%20this%0Awork%2C%20we%20introduce%20a%20simulator-conditioned%20scene%20generation%20framework%20called%0ASimGen%20that%20can%20learn%20to%20generate%20diverse%20driving%20scenes%20by%20mixing%20data%20from%0Athe%20simulator%20and%20the%20real%20world.%20It%20uses%20a%20novel%20cascade%20diffusion%20pipeline%20to%0Aaddress%20challenging%20sim-to-real%20gaps%20and%20multi-condition%20conflicts.%20A%20driving%0Avideo%20dataset%20DIVA%20is%20collected%20to%20enhance%20the%20generative%20diversity%20of%20SimGen%2C%0Awhich%20contains%20over%20147.5%20hours%20of%20real-world%20driving%20videos%20from%2073%20locations%0Aworldwide%20and%20simulated%20driving%20data%20from%20the%20MetaDrive%20simulator.%20SimGen%0Aachieves%20superior%20generation%20quality%20and%20diversity%20while%20preserving%0Acontrollability%20based%20on%20the%20text%20prompt%20and%20the%20layout%20pulled%20from%20a%0Asimulator.%20We%20further%20demonstrate%20the%20improvements%20brought%20by%20SimGen%20for%0Asynthetic%20data%20augmentation%20on%20the%20BEV%20detection%20and%20segmentation%20task%20and%0Ashowcase%20its%20capability%20in%20safety-critical%20data%20generation.%20Code%2C%20data%2C%20and%0Amodels%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09386v1&entry.124074799=Read"},
{"title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric\n  Videos", "author": "Changan Chen and Puyuan Peng and Ami Baid and Zihui Xue and Wei-Ning Hsu and David Harwarth and Kristen Grauman", "abstract": "  Generating realistic audio for human interactions is important for many\napplications, such as creating sound effects for films or virtual reality\ngames. Existing approaches implicitly assume total correspondence between the\nvideo and audio during training, yet many sounds happen off-screen and have\nweak to no correspondence with the visuals -- resulting in uncontrolled ambient\nsounds or hallucinations at test time. We propose a novel ambient-aware audio\ngeneration model, AV-LDM. We devise a novel audio-conditioning mechanism to\nlearn to disentangle foreground action sounds from the ambient background\nsounds in in-the-wild training videos. Given a novel silent video, our model\nuses retrieval-augmented generation to create audio that matches the visual\ncontent both semantically and temporally. We train and evaluate our model on\ntwo in-the-wild egocentric video datasets Ego4D and EPIC-KITCHENS. Our model\noutperforms an array of existing methods, allows controllable generation of the\nambient sound, and even shows promise for generalizing to computer graphics\ngame clips. Overall, our work is the first to focus video-to-audio generation\nfaithfully on the observed visual content despite training from uncurated clips\nwith natural background sounds.\n", "link": "http://arxiv.org/abs/2406.09272v1", "date": "2024-06-13", "relevancy": 2.9006, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5639}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos&body=Title%3A%20Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos%0AAuthor%3A%20Changan%20Chen%20and%20Puyuan%20Peng%20and%20Ami%20Baid%20and%20Zihui%20Xue%20and%20Wei-Ning%20Hsu%20and%20David%20Harwarth%20and%20Kristen%20Grauman%0AAbstract%3A%20%20%20Generating%20realistic%20audio%20for%20human%20interactions%20is%20important%20for%20many%0Aapplications%2C%20such%20as%20creating%20sound%20effects%20for%20films%20or%20virtual%20reality%0Agames.%20Existing%20approaches%20implicitly%20assume%20total%20correspondence%20between%20the%0Avideo%20and%20audio%20during%20training%2C%20yet%20many%20sounds%20happen%20off-screen%20and%20have%0Aweak%20to%20no%20correspondence%20with%20the%20visuals%20--%20resulting%20in%20uncontrolled%20ambient%0Asounds%20or%20hallucinations%20at%20test%20time.%20We%20propose%20a%20novel%20ambient-aware%20audio%0Ageneration%20model%2C%20AV-LDM.%20We%20devise%20a%20novel%20audio-conditioning%20mechanism%20to%0Alearn%20to%20disentangle%20foreground%20action%20sounds%20from%20the%20ambient%20background%0Asounds%20in%20in-the-wild%20training%20videos.%20Given%20a%20novel%20silent%20video%2C%20our%20model%0Auses%20retrieval-augmented%20generation%20to%20create%20audio%20that%20matches%20the%20visual%0Acontent%20both%20semantically%20and%20temporally.%20We%20train%20and%20evaluate%20our%20model%20on%0Atwo%20in-the-wild%20egocentric%20video%20datasets%20Ego4D%20and%20EPIC-KITCHENS.%20Our%20model%0Aoutperforms%20an%20array%20of%20existing%20methods%2C%20allows%20controllable%20generation%20of%20the%0Aambient%20sound%2C%20and%20even%20shows%20promise%20for%20generalizing%20to%20computer%20graphics%0Agame%20clips.%20Overall%2C%20our%20work%20is%20the%20first%20to%20focus%20video-to-audio%20generation%0Afaithfully%20on%20the%20observed%20visual%20content%20despite%20training%20from%20uncurated%20clips%0Awith%20natural%20background%20sounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction2Sound%253A%2520Ambient-Aware%2520Generation%2520of%2520Action%2520Sounds%2520from%2520Egocentric%250A%2520%2520Videos%26entry.906535625%3DChangan%2520Chen%2520and%2520Puyuan%2520Peng%2520and%2520Ami%2520Baid%2520and%2520Zihui%2520Xue%2520and%2520Wei-Ning%2520Hsu%2520and%2520David%2520Harwarth%2520and%2520Kristen%2520Grauman%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520audio%2520for%2520human%2520interactions%2520is%2520important%2520for%2520many%250Aapplications%252C%2520such%2520as%2520creating%2520sound%2520effects%2520for%2520films%2520or%2520virtual%2520reality%250Agames.%2520Existing%2520approaches%2520implicitly%2520assume%2520total%2520correspondence%2520between%2520the%250Avideo%2520and%2520audio%2520during%2520training%252C%2520yet%2520many%2520sounds%2520happen%2520off-screen%2520and%2520have%250Aweak%2520to%2520no%2520correspondence%2520with%2520the%2520visuals%2520--%2520resulting%2520in%2520uncontrolled%2520ambient%250Asounds%2520or%2520hallucinations%2520at%2520test%2520time.%2520We%2520propose%2520a%2520novel%2520ambient-aware%2520audio%250Ageneration%2520model%252C%2520AV-LDM.%2520We%2520devise%2520a%2520novel%2520audio-conditioning%2520mechanism%2520to%250Alearn%2520to%2520disentangle%2520foreground%2520action%2520sounds%2520from%2520the%2520ambient%2520background%250Asounds%2520in%2520in-the-wild%2520training%2520videos.%2520Given%2520a%2520novel%2520silent%2520video%252C%2520our%2520model%250Auses%2520retrieval-augmented%2520generation%2520to%2520create%2520audio%2520that%2520matches%2520the%2520visual%250Acontent%2520both%2520semantically%2520and%2520temporally.%2520We%2520train%2520and%2520evaluate%2520our%2520model%2520on%250Atwo%2520in-the-wild%2520egocentric%2520video%2520datasets%2520Ego4D%2520and%2520EPIC-KITCHENS.%2520Our%2520model%250Aoutperforms%2520an%2520array%2520of%2520existing%2520methods%252C%2520allows%2520controllable%2520generation%2520of%2520the%250Aambient%2520sound%252C%2520and%2520even%2520shows%2520promise%2520for%2520generalizing%2520to%2520computer%2520graphics%250Agame%2520clips.%2520Overall%252C%2520our%2520work%2520is%2520the%2520first%2520to%2520focus%2520video-to-audio%2520generation%250Afaithfully%2520on%2520the%2520observed%2520visual%2520content%2520despite%2520training%2520from%2520uncurated%2520clips%250Awith%2520natural%2520background%2520sounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos&entry.906535625=Changan%20Chen%20and%20Puyuan%20Peng%20and%20Ami%20Baid%20and%20Zihui%20Xue%20and%20Wei-Ning%20Hsu%20and%20David%20Harwarth%20and%20Kristen%20Grauman&entry.1292438233=%20%20Generating%20realistic%20audio%20for%20human%20interactions%20is%20important%20for%20many%0Aapplications%2C%20such%20as%20creating%20sound%20effects%20for%20films%20or%20virtual%20reality%0Agames.%20Existing%20approaches%20implicitly%20assume%20total%20correspondence%20between%20the%0Avideo%20and%20audio%20during%20training%2C%20yet%20many%20sounds%20happen%20off-screen%20and%20have%0Aweak%20to%20no%20correspondence%20with%20the%20visuals%20--%20resulting%20in%20uncontrolled%20ambient%0Asounds%20or%20hallucinations%20at%20test%20time.%20We%20propose%20a%20novel%20ambient-aware%20audio%0Ageneration%20model%2C%20AV-LDM.%20We%20devise%20a%20novel%20audio-conditioning%20mechanism%20to%0Alearn%20to%20disentangle%20foreground%20action%20sounds%20from%20the%20ambient%20background%0Asounds%20in%20in-the-wild%20training%20videos.%20Given%20a%20novel%20silent%20video%2C%20our%20model%0Auses%20retrieval-augmented%20generation%20to%20create%20audio%20that%20matches%20the%20visual%0Acontent%20both%20semantically%20and%20temporally.%20We%20train%20and%20evaluate%20our%20model%20on%0Atwo%20in-the-wild%20egocentric%20video%20datasets%20Ego4D%20and%20EPIC-KITCHENS.%20Our%20model%0Aoutperforms%20an%20array%20of%20existing%20methods%2C%20allows%20controllable%20generation%20of%20the%0Aambient%20sound%2C%20and%20even%20shows%20promise%20for%20generalizing%20to%20computer%20graphics%0Agame%20clips.%20Overall%2C%20our%20work%20is%20the%20first%20to%20focus%20video-to-audio%20generation%0Afaithfully%20on%20the%20observed%20visual%20content%20despite%20training%20from%20uncurated%20clips%0Awith%20natural%20background%20sounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09272v1&entry.124074799=Read"},
{"title": "Optimization Efficient Open-World Visual Region Recognition", "author": "Haosen Yang and Chuofan Ma and Bin Wen and Yi Jiang and Zehuan Yuan and Xiatian Zhu", "abstract": "  Understanding the semantics of individual regions or patches of unconstrained\nimages, such as open-world object detection, remains a critical yet challenging\ntask in computer vision. Building on the success of powerful image-level\nvision-language (ViL) foundation models like CLIP, recent efforts have sought\nto harness their capabilities by either training a contrastive model from\nscratch with an extensive collection of region-label pairs or aligning the\noutputs of a detection model with image-level representations of region\nproposals. Despite notable progress, these approaches are plagued by\ncomputationally intensive training requirements, susceptibility to data noise,\nand deficiency in contextual information. To address these limitations, we\nexplore the synergistic potential of off-the-shelf foundation models,\nleveraging their respective strengths in localization and semantics. We\nintroduce a novel, generic, and efficient architecture, named RegionSpot,\ndesigned to integrate position-aware localization knowledge from a localization\nfoundation model (e.g., SAM) with semantic information from a ViL model (e.g.,\nCLIP). To fully exploit pretrained knowledge while minimizing training\noverhead, we keep both foundation models frozen, focusing optimization efforts\nsolely on a lightweight attention-based knowledge integration module. Extensive\nexperiments in open-world object recognition show that our RegionSpot achieves\nsignificant performance gain over prior alternatives, along with substantial\ncomputational savings (e.g., training our model with 3 million data in a single\nday using 8 V100 GPUs). RegionSpot outperforms GLIP-L by 2.9 in mAP on LVIS val\nset, with an even larger margin of 13.1 AP for more challenging and rare\ncategories, and a 2.5 AP increase on ODinW. Furthermore, it exceeds\nGroundingDINO-L by 11.0 AP for rare categories on the LVIS minival set.\n", "link": "http://arxiv.org/abs/2311.01373v2", "date": "2024-06-13", "relevancy": 2.8777, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization%20Efficient%20Open-World%20Visual%20Region%20Recognition&body=Title%3A%20Optimization%20Efficient%20Open-World%20Visual%20Region%20Recognition%0AAuthor%3A%20Haosen%20Yang%20and%20Chuofan%20Ma%20and%20Bin%20Wen%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20Understanding%20the%20semantics%20of%20individual%20regions%20or%20patches%20of%20unconstrained%0Aimages%2C%20such%20as%20open-world%20object%20detection%2C%20remains%20a%20critical%20yet%20challenging%0Atask%20in%20computer%20vision.%20Building%20on%20the%20success%20of%20powerful%20image-level%0Avision-language%20%28ViL%29%20foundation%20models%20like%20CLIP%2C%20recent%20efforts%20have%20sought%0Ato%20harness%20their%20capabilities%20by%20either%20training%20a%20contrastive%20model%20from%0Ascratch%20with%20an%20extensive%20collection%20of%20region-label%20pairs%20or%20aligning%20the%0Aoutputs%20of%20a%20detection%20model%20with%20image-level%20representations%20of%20region%0Aproposals.%20Despite%20notable%20progress%2C%20these%20approaches%20are%20plagued%20by%0Acomputationally%20intensive%20training%20requirements%2C%20susceptibility%20to%20data%20noise%2C%0Aand%20deficiency%20in%20contextual%20information.%20To%20address%20these%20limitations%2C%20we%0Aexplore%20the%20synergistic%20potential%20of%20off-the-shelf%20foundation%20models%2C%0Aleveraging%20their%20respective%20strengths%20in%20localization%20and%20semantics.%20We%0Aintroduce%20a%20novel%2C%20generic%2C%20and%20efficient%20architecture%2C%20named%20RegionSpot%2C%0Adesigned%20to%20integrate%20position-aware%20localization%20knowledge%20from%20a%20localization%0Afoundation%20model%20%28e.g.%2C%20SAM%29%20with%20semantic%20information%20from%20a%20ViL%20model%20%28e.g.%2C%0ACLIP%29.%20To%20fully%20exploit%20pretrained%20knowledge%20while%20minimizing%20training%0Aoverhead%2C%20we%20keep%20both%20foundation%20models%20frozen%2C%20focusing%20optimization%20efforts%0Asolely%20on%20a%20lightweight%20attention-based%20knowledge%20integration%20module.%20Extensive%0Aexperiments%20in%20open-world%20object%20recognition%20show%20that%20our%20RegionSpot%20achieves%0Asignificant%20performance%20gain%20over%20prior%20alternatives%2C%20along%20with%20substantial%0Acomputational%20savings%20%28e.g.%2C%20training%20our%20model%20with%203%20million%20data%20in%20a%20single%0Aday%20using%208%20V100%20GPUs%29.%20RegionSpot%20outperforms%20GLIP-L%20by%202.9%20in%20mAP%20on%20LVIS%20val%0Aset%2C%20with%20an%20even%20larger%20margin%20of%2013.1%20AP%20for%20more%20challenging%20and%20rare%0Acategories%2C%20and%20a%202.5%20AP%20increase%20on%20ODinW.%20Furthermore%2C%20it%20exceeds%0AGroundingDINO-L%20by%2011.0%20AP%20for%20rare%20categories%20on%20the%20LVIS%20minival%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01373v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization%2520Efficient%2520Open-World%2520Visual%2520Region%2520Recognition%26entry.906535625%3DHaosen%2520Yang%2520and%2520Chuofan%2520Ma%2520and%2520Bin%2520Wen%2520and%2520Yi%2520Jiang%2520and%2520Zehuan%2520Yuan%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520Understanding%2520the%2520semantics%2520of%2520individual%2520regions%2520or%2520patches%2520of%2520unconstrained%250Aimages%252C%2520such%2520as%2520open-world%2520object%2520detection%252C%2520remains%2520a%2520critical%2520yet%2520challenging%250Atask%2520in%2520computer%2520vision.%2520Building%2520on%2520the%2520success%2520of%2520powerful%2520image-level%250Avision-language%2520%2528ViL%2529%2520foundation%2520models%2520like%2520CLIP%252C%2520recent%2520efforts%2520have%2520sought%250Ato%2520harness%2520their%2520capabilities%2520by%2520either%2520training%2520a%2520contrastive%2520model%2520from%250Ascratch%2520with%2520an%2520extensive%2520collection%2520of%2520region-label%2520pairs%2520or%2520aligning%2520the%250Aoutputs%2520of%2520a%2520detection%2520model%2520with%2520image-level%2520representations%2520of%2520region%250Aproposals.%2520Despite%2520notable%2520progress%252C%2520these%2520approaches%2520are%2520plagued%2520by%250Acomputationally%2520intensive%2520training%2520requirements%252C%2520susceptibility%2520to%2520data%2520noise%252C%250Aand%2520deficiency%2520in%2520contextual%2520information.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aexplore%2520the%2520synergistic%2520potential%2520of%2520off-the-shelf%2520foundation%2520models%252C%250Aleveraging%2520their%2520respective%2520strengths%2520in%2520localization%2520and%2520semantics.%2520We%250Aintroduce%2520a%2520novel%252C%2520generic%252C%2520and%2520efficient%2520architecture%252C%2520named%2520RegionSpot%252C%250Adesigned%2520to%2520integrate%2520position-aware%2520localization%2520knowledge%2520from%2520a%2520localization%250Afoundation%2520model%2520%2528e.g.%252C%2520SAM%2529%2520with%2520semantic%2520information%2520from%2520a%2520ViL%2520model%2520%2528e.g.%252C%250ACLIP%2529.%2520To%2520fully%2520exploit%2520pretrained%2520knowledge%2520while%2520minimizing%2520training%250Aoverhead%252C%2520we%2520keep%2520both%2520foundation%2520models%2520frozen%252C%2520focusing%2520optimization%2520efforts%250Asolely%2520on%2520a%2520lightweight%2520attention-based%2520knowledge%2520integration%2520module.%2520Extensive%250Aexperiments%2520in%2520open-world%2520object%2520recognition%2520show%2520that%2520our%2520RegionSpot%2520achieves%250Asignificant%2520performance%2520gain%2520over%2520prior%2520alternatives%252C%2520along%2520with%2520substantial%250Acomputational%2520savings%2520%2528e.g.%252C%2520training%2520our%2520model%2520with%25203%2520million%2520data%2520in%2520a%2520single%250Aday%2520using%25208%2520V100%2520GPUs%2529.%2520RegionSpot%2520outperforms%2520GLIP-L%2520by%25202.9%2520in%2520mAP%2520on%2520LVIS%2520val%250Aset%252C%2520with%2520an%2520even%2520larger%2520margin%2520of%252013.1%2520AP%2520for%2520more%2520challenging%2520and%2520rare%250Acategories%252C%2520and%2520a%25202.5%2520AP%2520increase%2520on%2520ODinW.%2520Furthermore%252C%2520it%2520exceeds%250AGroundingDINO-L%2520by%252011.0%2520AP%2520for%2520rare%2520categories%2520on%2520the%2520LVIS%2520minival%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01373v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization%20Efficient%20Open-World%20Visual%20Region%20Recognition&entry.906535625=Haosen%20Yang%20and%20Chuofan%20Ma%20and%20Bin%20Wen%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Xiatian%20Zhu&entry.1292438233=%20%20Understanding%20the%20semantics%20of%20individual%20regions%20or%20patches%20of%20unconstrained%0Aimages%2C%20such%20as%20open-world%20object%20detection%2C%20remains%20a%20critical%20yet%20challenging%0Atask%20in%20computer%20vision.%20Building%20on%20the%20success%20of%20powerful%20image-level%0Avision-language%20%28ViL%29%20foundation%20models%20like%20CLIP%2C%20recent%20efforts%20have%20sought%0Ato%20harness%20their%20capabilities%20by%20either%20training%20a%20contrastive%20model%20from%0Ascratch%20with%20an%20extensive%20collection%20of%20region-label%20pairs%20or%20aligning%20the%0Aoutputs%20of%20a%20detection%20model%20with%20image-level%20representations%20of%20region%0Aproposals.%20Despite%20notable%20progress%2C%20these%20approaches%20are%20plagued%20by%0Acomputationally%20intensive%20training%20requirements%2C%20susceptibility%20to%20data%20noise%2C%0Aand%20deficiency%20in%20contextual%20information.%20To%20address%20these%20limitations%2C%20we%0Aexplore%20the%20synergistic%20potential%20of%20off-the-shelf%20foundation%20models%2C%0Aleveraging%20their%20respective%20strengths%20in%20localization%20and%20semantics.%20We%0Aintroduce%20a%20novel%2C%20generic%2C%20and%20efficient%20architecture%2C%20named%20RegionSpot%2C%0Adesigned%20to%20integrate%20position-aware%20localization%20knowledge%20from%20a%20localization%0Afoundation%20model%20%28e.g.%2C%20SAM%29%20with%20semantic%20information%20from%20a%20ViL%20model%20%28e.g.%2C%0ACLIP%29.%20To%20fully%20exploit%20pretrained%20knowledge%20while%20minimizing%20training%0Aoverhead%2C%20we%20keep%20both%20foundation%20models%20frozen%2C%20focusing%20optimization%20efforts%0Asolely%20on%20a%20lightweight%20attention-based%20knowledge%20integration%20module.%20Extensive%0Aexperiments%20in%20open-world%20object%20recognition%20show%20that%20our%20RegionSpot%20achieves%0Asignificant%20performance%20gain%20over%20prior%20alternatives%2C%20along%20with%20substantial%0Acomputational%20savings%20%28e.g.%2C%20training%20our%20model%20with%203%20million%20data%20in%20a%20single%0Aday%20using%208%20V100%20GPUs%29.%20RegionSpot%20outperforms%20GLIP-L%20by%202.9%20in%20mAP%20on%20LVIS%20val%0Aset%2C%20with%20an%20even%20larger%20margin%20of%2013.1%20AP%20for%20more%20challenging%20and%20rare%0Acategories%2C%20and%20a%202.5%20AP%20increase%20on%20ODinW.%20Furthermore%2C%20it%20exceeds%0AGroundingDINO-L%20by%2011.0%20AP%20for%20rare%20categories%20on%20the%20LVIS%20minival%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01373v2&entry.124074799=Read"},
{"title": "LRM-Zero: Training Large Reconstruction Models with Synthesized Data", "author": "Desai Xie and Sai Bi and Zhixin Shu and Kai Zhang and Zexiang Xu and Yi Zhou and S\u00f6ren Pirk and Arie Kaufman and Xin Sun and Hao Tan", "abstract": "  We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on\nsynthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The\ncore of LRM-Zero is our procedural 3D dataset, Zeroverse, which is\nautomatically synthesized from simple primitive shapes with random texturing\nand augmentations (e.g., height fields, boolean differences, and wireframes).\nUnlike previous 3D datasets (e.g., Objaverse) which are often captured or\ncrafted by humans to approximate real 3D data, Zeroverse completely ignores\nrealistic global semantics but is rich in complex geometric and texture details\nthat are locally similar to or even more intricate than real objects. We\ndemonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,\ncan achieve high visual quality in the reconstruction of real-world objects,\ncompetitive with models trained on Objaverse. We also analyze several critical\ndesign choices of Zeroverse that contribute to LRM-Zero's capability and\ntraining stability. Our work demonstrates that 3D reconstruction, one of the\ncore tasks in 3D vision, can potentially be addressed without the semantics of\nreal-world objects. The Zeroverse's procedural synthesis code and interactive\nvisualization are available at: https://desaixie.github.io/lrm-zero/.\n", "link": "http://arxiv.org/abs/2406.09371v1", "date": "2024-06-13", "relevancy": 2.8745, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5964}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5642}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LRM-Zero%3A%20Training%20Large%20Reconstruction%20Models%20with%20Synthesized%20Data&body=Title%3A%20LRM-Zero%3A%20Training%20Large%20Reconstruction%20Models%20with%20Synthesized%20Data%0AAuthor%3A%20Desai%20Xie%20and%20Sai%20Bi%20and%20Zhixin%20Shu%20and%20Kai%20Zhang%20and%20Zexiang%20Xu%20and%20Yi%20Zhou%20and%20S%C3%B6ren%20Pirk%20and%20Arie%20Kaufman%20and%20Xin%20Sun%20and%20Hao%20Tan%0AAbstract%3A%20%20%20We%20present%20LRM-Zero%2C%20a%20Large%20Reconstruction%20Model%20%28LRM%29%20trained%20entirely%20on%0Asynthesized%203D%20data%2C%20achieving%20high-quality%20sparse-view%203D%20reconstruction.%20The%0Acore%20of%20LRM-Zero%20is%20our%20procedural%203D%20dataset%2C%20Zeroverse%2C%20which%20is%0Aautomatically%20synthesized%20from%20simple%20primitive%20shapes%20with%20random%20texturing%0Aand%20augmentations%20%28e.g.%2C%20height%20fields%2C%20boolean%20differences%2C%20and%20wireframes%29.%0AUnlike%20previous%203D%20datasets%20%28e.g.%2C%20Objaverse%29%20which%20are%20often%20captured%20or%0Acrafted%20by%20humans%20to%20approximate%20real%203D%20data%2C%20Zeroverse%20completely%20ignores%0Arealistic%20global%20semantics%20but%20is%20rich%20in%20complex%20geometric%20and%20texture%20details%0Athat%20are%20locally%20similar%20to%20or%20even%20more%20intricate%20than%20real%20objects.%20We%0Ademonstrate%20that%20our%20LRM-Zero%2C%20trained%20with%20our%20fully%20synthesized%20Zeroverse%2C%0Acan%20achieve%20high%20visual%20quality%20in%20the%20reconstruction%20of%20real-world%20objects%2C%0Acompetitive%20with%20models%20trained%20on%20Objaverse.%20We%20also%20analyze%20several%20critical%0Adesign%20choices%20of%20Zeroverse%20that%20contribute%20to%20LRM-Zero%27s%20capability%20and%0Atraining%20stability.%20Our%20work%20demonstrates%20that%203D%20reconstruction%2C%20one%20of%20the%0Acore%20tasks%20in%203D%20vision%2C%20can%20potentially%20be%20addressed%20without%20the%20semantics%20of%0Areal-world%20objects.%20The%20Zeroverse%27s%20procedural%20synthesis%20code%20and%20interactive%0Avisualization%20are%20available%20at%3A%20https%3A//desaixie.github.io/lrm-zero/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLRM-Zero%253A%2520Training%2520Large%2520Reconstruction%2520Models%2520with%2520Synthesized%2520Data%26entry.906535625%3DDesai%2520Xie%2520and%2520Sai%2520Bi%2520and%2520Zhixin%2520Shu%2520and%2520Kai%2520Zhang%2520and%2520Zexiang%2520Xu%2520and%2520Yi%2520Zhou%2520and%2520S%25C3%25B6ren%2520Pirk%2520and%2520Arie%2520Kaufman%2520and%2520Xin%2520Sun%2520and%2520Hao%2520Tan%26entry.1292438233%3D%2520%2520We%2520present%2520LRM-Zero%252C%2520a%2520Large%2520Reconstruction%2520Model%2520%2528LRM%2529%2520trained%2520entirely%2520on%250Asynthesized%25203D%2520data%252C%2520achieving%2520high-quality%2520sparse-view%25203D%2520reconstruction.%2520The%250Acore%2520of%2520LRM-Zero%2520is%2520our%2520procedural%25203D%2520dataset%252C%2520Zeroverse%252C%2520which%2520is%250Aautomatically%2520synthesized%2520from%2520simple%2520primitive%2520shapes%2520with%2520random%2520texturing%250Aand%2520augmentations%2520%2528e.g.%252C%2520height%2520fields%252C%2520boolean%2520differences%252C%2520and%2520wireframes%2529.%250AUnlike%2520previous%25203D%2520datasets%2520%2528e.g.%252C%2520Objaverse%2529%2520which%2520are%2520often%2520captured%2520or%250Acrafted%2520by%2520humans%2520to%2520approximate%2520real%25203D%2520data%252C%2520Zeroverse%2520completely%2520ignores%250Arealistic%2520global%2520semantics%2520but%2520is%2520rich%2520in%2520complex%2520geometric%2520and%2520texture%2520details%250Athat%2520are%2520locally%2520similar%2520to%2520or%2520even%2520more%2520intricate%2520than%2520real%2520objects.%2520We%250Ademonstrate%2520that%2520our%2520LRM-Zero%252C%2520trained%2520with%2520our%2520fully%2520synthesized%2520Zeroverse%252C%250Acan%2520achieve%2520high%2520visual%2520quality%2520in%2520the%2520reconstruction%2520of%2520real-world%2520objects%252C%250Acompetitive%2520with%2520models%2520trained%2520on%2520Objaverse.%2520We%2520also%2520analyze%2520several%2520critical%250Adesign%2520choices%2520of%2520Zeroverse%2520that%2520contribute%2520to%2520LRM-Zero%2527s%2520capability%2520and%250Atraining%2520stability.%2520Our%2520work%2520demonstrates%2520that%25203D%2520reconstruction%252C%2520one%2520of%2520the%250Acore%2520tasks%2520in%25203D%2520vision%252C%2520can%2520potentially%2520be%2520addressed%2520without%2520the%2520semantics%2520of%250Areal-world%2520objects.%2520The%2520Zeroverse%2527s%2520procedural%2520synthesis%2520code%2520and%2520interactive%250Avisualization%2520are%2520available%2520at%253A%2520https%253A//desaixie.github.io/lrm-zero/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LRM-Zero%3A%20Training%20Large%20Reconstruction%20Models%20with%20Synthesized%20Data&entry.906535625=Desai%20Xie%20and%20Sai%20Bi%20and%20Zhixin%20Shu%20and%20Kai%20Zhang%20and%20Zexiang%20Xu%20and%20Yi%20Zhou%20and%20S%C3%B6ren%20Pirk%20and%20Arie%20Kaufman%20and%20Xin%20Sun%20and%20Hao%20Tan&entry.1292438233=%20%20We%20present%20LRM-Zero%2C%20a%20Large%20Reconstruction%20Model%20%28LRM%29%20trained%20entirely%20on%0Asynthesized%203D%20data%2C%20achieving%20high-quality%20sparse-view%203D%20reconstruction.%20The%0Acore%20of%20LRM-Zero%20is%20our%20procedural%203D%20dataset%2C%20Zeroverse%2C%20which%20is%0Aautomatically%20synthesized%20from%20simple%20primitive%20shapes%20with%20random%20texturing%0Aand%20augmentations%20%28e.g.%2C%20height%20fields%2C%20boolean%20differences%2C%20and%20wireframes%29.%0AUnlike%20previous%203D%20datasets%20%28e.g.%2C%20Objaverse%29%20which%20are%20often%20captured%20or%0Acrafted%20by%20humans%20to%20approximate%20real%203D%20data%2C%20Zeroverse%20completely%20ignores%0Arealistic%20global%20semantics%20but%20is%20rich%20in%20complex%20geometric%20and%20texture%20details%0Athat%20are%20locally%20similar%20to%20or%20even%20more%20intricate%20than%20real%20objects.%20We%0Ademonstrate%20that%20our%20LRM-Zero%2C%20trained%20with%20our%20fully%20synthesized%20Zeroverse%2C%0Acan%20achieve%20high%20visual%20quality%20in%20the%20reconstruction%20of%20real-world%20objects%2C%0Acompetitive%20with%20models%20trained%20on%20Objaverse.%20We%20also%20analyze%20several%20critical%0Adesign%20choices%20of%20Zeroverse%20that%20contribute%20to%20LRM-Zero%27s%20capability%20and%0Atraining%20stability.%20Our%20work%20demonstrates%20that%203D%20reconstruction%2C%20one%20of%20the%0Acore%20tasks%20in%203D%20vision%2C%20can%20potentially%20be%20addressed%20without%20the%20semantics%20of%0Areal-world%20objects.%20The%20Zeroverse%27s%20procedural%20synthesis%20code%20and%20interactive%0Avisualization%20are%20available%20at%3A%20https%3A//desaixie.github.io/lrm-zero/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09371v1&entry.124074799=Read"},
{"title": "OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation", "author": "Junke Wang and Yi Jiang and Zehuan Yuan and Binyue Peng and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Tokenizer, serving as a translator to map the intricate visual data into a\ncompact latent space, lies at the core of visual generative models. Based on\nthe finding that existing tokenizers are tailored to image or video inputs,\nthis paper presents OmniTokenizer, a transformer-based tokenizer for joint\nimage and video tokenization. OmniTokenizer is designed with a spatial-temporal\ndecoupled architecture, which integrates window and causal attention for\nspatial and temporal modeling. To exploit the complementary nature of image and\nvideo data, we further propose a progressive training strategy, where\nOmniTokenizer is first trained on image data on a fixed resolution to develop\nthe spatial encoding capacity and then jointly trained on image and video data\non multiple resolutions to learn the temporal dynamics. OmniTokenizer, for the\nfirst time, handles both image and video inputs within a unified framework and\nproves the possibility of realizing their synergy. Extensive experiments\ndemonstrate that OmniTokenizer achieves state-of-the-art (SOTA) reconstruction\nperformance on various image and video datasets, e.g., 1.11 reconstruction FID\non ImageNet and 42 reconstruction FVD on UCF-101, beating the previous SOTA\nmethods by 13% and 26%, respectively. Additionally, we also show that when\nintegrated with OmniTokenizer, both language model-based approaches and\ndiffusion models can realize advanced visual synthesis performance,\nunderscoring the superiority and versatility of our method. Code is available\nat https://github.com/FoundationVision/OmniTokenizer.\n", "link": "http://arxiv.org/abs/2406.09399v1", "date": "2024-06-13", "relevancy": 2.8464, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5998}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5597}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniTokenizer%3A%20A%20Joint%20Image-Video%20Tokenizer%20for%20Visual%20Generation&body=Title%3A%20OmniTokenizer%3A%20A%20Joint%20Image-Video%20Tokenizer%20for%20Visual%20Generation%0AAuthor%3A%20Junke%20Wang%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Binyue%20Peng%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Tokenizer%2C%20serving%20as%20a%20translator%20to%20map%20the%20intricate%20visual%20data%20into%20a%0Acompact%20latent%20space%2C%20lies%20at%20the%20core%20of%20visual%20generative%20models.%20Based%20on%0Athe%20finding%20that%20existing%20tokenizers%20are%20tailored%20to%20image%20or%20video%20inputs%2C%0Athis%20paper%20presents%20OmniTokenizer%2C%20a%20transformer-based%20tokenizer%20for%20joint%0Aimage%20and%20video%20tokenization.%20OmniTokenizer%20is%20designed%20with%20a%20spatial-temporal%0Adecoupled%20architecture%2C%20which%20integrates%20window%20and%20causal%20attention%20for%0Aspatial%20and%20temporal%20modeling.%20To%20exploit%20the%20complementary%20nature%20of%20image%20and%0Avideo%20data%2C%20we%20further%20propose%20a%20progressive%20training%20strategy%2C%20where%0AOmniTokenizer%20is%20first%20trained%20on%20image%20data%20on%20a%20fixed%20resolution%20to%20develop%0Athe%20spatial%20encoding%20capacity%20and%20then%20jointly%20trained%20on%20image%20and%20video%20data%0Aon%20multiple%20resolutions%20to%20learn%20the%20temporal%20dynamics.%20OmniTokenizer%2C%20for%20the%0Afirst%20time%2C%20handles%20both%20image%20and%20video%20inputs%20within%20a%20unified%20framework%20and%0Aproves%20the%20possibility%20of%20realizing%20their%20synergy.%20Extensive%20experiments%0Ademonstrate%20that%20OmniTokenizer%20achieves%20state-of-the-art%20%28SOTA%29%20reconstruction%0Aperformance%20on%20various%20image%20and%20video%20datasets%2C%20e.g.%2C%201.11%20reconstruction%20FID%0Aon%20ImageNet%20and%2042%20reconstruction%20FVD%20on%20UCF-101%2C%20beating%20the%20previous%20SOTA%0Amethods%20by%2013%25%20and%2026%25%2C%20respectively.%20Additionally%2C%20we%20also%20show%20that%20when%0Aintegrated%20with%20OmniTokenizer%2C%20both%20language%20model-based%20approaches%20and%0Adiffusion%20models%20can%20realize%20advanced%20visual%20synthesis%20performance%2C%0Aunderscoring%20the%20superiority%20and%20versatility%20of%20our%20method.%20Code%20is%20available%0Aat%20https%3A//github.com/FoundationVision/OmniTokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniTokenizer%253A%2520A%2520Joint%2520Image-Video%2520Tokenizer%2520for%2520Visual%2520Generation%26entry.906535625%3DJunke%2520Wang%2520and%2520Yi%2520Jiang%2520and%2520Zehuan%2520Yuan%2520and%2520Binyue%2520Peng%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Tokenizer%252C%2520serving%2520as%2520a%2520translator%2520to%2520map%2520the%2520intricate%2520visual%2520data%2520into%2520a%250Acompact%2520latent%2520space%252C%2520lies%2520at%2520the%2520core%2520of%2520visual%2520generative%2520models.%2520Based%2520on%250Athe%2520finding%2520that%2520existing%2520tokenizers%2520are%2520tailored%2520to%2520image%2520or%2520video%2520inputs%252C%250Athis%2520paper%2520presents%2520OmniTokenizer%252C%2520a%2520transformer-based%2520tokenizer%2520for%2520joint%250Aimage%2520and%2520video%2520tokenization.%2520OmniTokenizer%2520is%2520designed%2520with%2520a%2520spatial-temporal%250Adecoupled%2520architecture%252C%2520which%2520integrates%2520window%2520and%2520causal%2520attention%2520for%250Aspatial%2520and%2520temporal%2520modeling.%2520To%2520exploit%2520the%2520complementary%2520nature%2520of%2520image%2520and%250Avideo%2520data%252C%2520we%2520further%2520propose%2520a%2520progressive%2520training%2520strategy%252C%2520where%250AOmniTokenizer%2520is%2520first%2520trained%2520on%2520image%2520data%2520on%2520a%2520fixed%2520resolution%2520to%2520develop%250Athe%2520spatial%2520encoding%2520capacity%2520and%2520then%2520jointly%2520trained%2520on%2520image%2520and%2520video%2520data%250Aon%2520multiple%2520resolutions%2520to%2520learn%2520the%2520temporal%2520dynamics.%2520OmniTokenizer%252C%2520for%2520the%250Afirst%2520time%252C%2520handles%2520both%2520image%2520and%2520video%2520inputs%2520within%2520a%2520unified%2520framework%2520and%250Aproves%2520the%2520possibility%2520of%2520realizing%2520their%2520synergy.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520OmniTokenizer%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520reconstruction%250Aperformance%2520on%2520various%2520image%2520and%2520video%2520datasets%252C%2520e.g.%252C%25201.11%2520reconstruction%2520FID%250Aon%2520ImageNet%2520and%252042%2520reconstruction%2520FVD%2520on%2520UCF-101%252C%2520beating%2520the%2520previous%2520SOTA%250Amethods%2520by%252013%2525%2520and%252026%2525%252C%2520respectively.%2520Additionally%252C%2520we%2520also%2520show%2520that%2520when%250Aintegrated%2520with%2520OmniTokenizer%252C%2520both%2520language%2520model-based%2520approaches%2520and%250Adiffusion%2520models%2520can%2520realize%2520advanced%2520visual%2520synthesis%2520performance%252C%250Aunderscoring%2520the%2520superiority%2520and%2520versatility%2520of%2520our%2520method.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/FoundationVision/OmniTokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniTokenizer%3A%20A%20Joint%20Image-Video%20Tokenizer%20for%20Visual%20Generation&entry.906535625=Junke%20Wang%20and%20Yi%20Jiang%20and%20Zehuan%20Yuan%20and%20Binyue%20Peng%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Tokenizer%2C%20serving%20as%20a%20translator%20to%20map%20the%20intricate%20visual%20data%20into%20a%0Acompact%20latent%20space%2C%20lies%20at%20the%20core%20of%20visual%20generative%20models.%20Based%20on%0Athe%20finding%20that%20existing%20tokenizers%20are%20tailored%20to%20image%20or%20video%20inputs%2C%0Athis%20paper%20presents%20OmniTokenizer%2C%20a%20transformer-based%20tokenizer%20for%20joint%0Aimage%20and%20video%20tokenization.%20OmniTokenizer%20is%20designed%20with%20a%20spatial-temporal%0Adecoupled%20architecture%2C%20which%20integrates%20window%20and%20causal%20attention%20for%0Aspatial%20and%20temporal%20modeling.%20To%20exploit%20the%20complementary%20nature%20of%20image%20and%0Avideo%20data%2C%20we%20further%20propose%20a%20progressive%20training%20strategy%2C%20where%0AOmniTokenizer%20is%20first%20trained%20on%20image%20data%20on%20a%20fixed%20resolution%20to%20develop%0Athe%20spatial%20encoding%20capacity%20and%20then%20jointly%20trained%20on%20image%20and%20video%20data%0Aon%20multiple%20resolutions%20to%20learn%20the%20temporal%20dynamics.%20OmniTokenizer%2C%20for%20the%0Afirst%20time%2C%20handles%20both%20image%20and%20video%20inputs%20within%20a%20unified%20framework%20and%0Aproves%20the%20possibility%20of%20realizing%20their%20synergy.%20Extensive%20experiments%0Ademonstrate%20that%20OmniTokenizer%20achieves%20state-of-the-art%20%28SOTA%29%20reconstruction%0Aperformance%20on%20various%20image%20and%20video%20datasets%2C%20e.g.%2C%201.11%20reconstruction%20FID%0Aon%20ImageNet%20and%2042%20reconstruction%20FVD%20on%20UCF-101%2C%20beating%20the%20previous%20SOTA%0Amethods%20by%2013%25%20and%2026%25%2C%20respectively.%20Additionally%2C%20we%20also%20show%20that%20when%0Aintegrated%20with%20OmniTokenizer%2C%20both%20language%20model-based%20approaches%20and%0Adiffusion%20models%20can%20realize%20advanced%20visual%20synthesis%20performance%2C%0Aunderscoring%20the%20superiority%20and%20versatility%20of%20our%20method.%20Code%20is%20available%0Aat%20https%3A//github.com/FoundationVision/OmniTokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09399v1&entry.124074799=Read"},
{"title": "EquiPrompt: Debiasing Diffusion Models via Iterative Bootstrapping in\n  Chain of Thoughts", "author": "Zahraa Al Sahili and Ioannis Patras and Matthew Purver", "abstract": "  In the domain of text-to-image generative models, the inadvertent propagation\nof biases inherent in training datasets poses significant ethical challenges,\nparticularly in the generation of socially sensitive content. This paper\nintroduces EquiPrompt, a novel method employing Chain of Thought (CoT)\nreasoning to reduce biases in text-to-image generative models. EquiPrompt uses\niterative bootstrapping and bias-aware exemplar selection to balance creativity\nand ethical responsibility. It integrates iterative reasoning refinement with\ncontrolled evaluation techniques, addressing zero-shot CoT issues in sensitive\ncontexts. Experiments on several generation tasks show EquiPrompt effectively\nlowers bias while maintaining generative quality, advancing ethical AI and\nsocially responsible creative processes.Code will be publically available.\n", "link": "http://arxiv.org/abs/2406.09070v1", "date": "2024-06-13", "relevancy": 2.7706, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5673}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5584}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EquiPrompt%3A%20Debiasing%20Diffusion%20Models%20via%20Iterative%20Bootstrapping%20in%0A%20%20Chain%20of%20Thoughts&body=Title%3A%20EquiPrompt%3A%20Debiasing%20Diffusion%20Models%20via%20Iterative%20Bootstrapping%20in%0A%20%20Chain%20of%20Thoughts%0AAuthor%3A%20Zahraa%20Al%20Sahili%20and%20Ioannis%20Patras%20and%20Matthew%20Purver%0AAbstract%3A%20%20%20In%20the%20domain%20of%20text-to-image%20generative%20models%2C%20the%20inadvertent%20propagation%0Aof%20biases%20inherent%20in%20training%20datasets%20poses%20significant%20ethical%20challenges%2C%0Aparticularly%20in%20the%20generation%20of%20socially%20sensitive%20content.%20This%20paper%0Aintroduces%20EquiPrompt%2C%20a%20novel%20method%20employing%20Chain%20of%20Thought%20%28CoT%29%0Areasoning%20to%20reduce%20biases%20in%20text-to-image%20generative%20models.%20EquiPrompt%20uses%0Aiterative%20bootstrapping%20and%20bias-aware%20exemplar%20selection%20to%20balance%20creativity%0Aand%20ethical%20responsibility.%20It%20integrates%20iterative%20reasoning%20refinement%20with%0Acontrolled%20evaluation%20techniques%2C%20addressing%20zero-shot%20CoT%20issues%20in%20sensitive%0Acontexts.%20Experiments%20on%20several%20generation%20tasks%20show%20EquiPrompt%20effectively%0Alowers%20bias%20while%20maintaining%20generative%20quality%2C%20advancing%20ethical%20AI%20and%0Asocially%20responsible%20creative%20processes.Code%20will%20be%20publically%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquiPrompt%253A%2520Debiasing%2520Diffusion%2520Models%2520via%2520Iterative%2520Bootstrapping%2520in%250A%2520%2520Chain%2520of%2520Thoughts%26entry.906535625%3DZahraa%2520Al%2520Sahili%2520and%2520Ioannis%2520Patras%2520and%2520Matthew%2520Purver%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%2520text-to-image%2520generative%2520models%252C%2520the%2520inadvertent%2520propagation%250Aof%2520biases%2520inherent%2520in%2520training%2520datasets%2520poses%2520significant%2520ethical%2520challenges%252C%250Aparticularly%2520in%2520the%2520generation%2520of%2520socially%2520sensitive%2520content.%2520This%2520paper%250Aintroduces%2520EquiPrompt%252C%2520a%2520novel%2520method%2520employing%2520Chain%2520of%2520Thought%2520%2528CoT%2529%250Areasoning%2520to%2520reduce%2520biases%2520in%2520text-to-image%2520generative%2520models.%2520EquiPrompt%2520uses%250Aiterative%2520bootstrapping%2520and%2520bias-aware%2520exemplar%2520selection%2520to%2520balance%2520creativity%250Aand%2520ethical%2520responsibility.%2520It%2520integrates%2520iterative%2520reasoning%2520refinement%2520with%250Acontrolled%2520evaluation%2520techniques%252C%2520addressing%2520zero-shot%2520CoT%2520issues%2520in%2520sensitive%250Acontexts.%2520Experiments%2520on%2520several%2520generation%2520tasks%2520show%2520EquiPrompt%2520effectively%250Alowers%2520bias%2520while%2520maintaining%2520generative%2520quality%252C%2520advancing%2520ethical%2520AI%2520and%250Asocially%2520responsible%2520creative%2520processes.Code%2520will%2520be%2520publically%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EquiPrompt%3A%20Debiasing%20Diffusion%20Models%20via%20Iterative%20Bootstrapping%20in%0A%20%20Chain%20of%20Thoughts&entry.906535625=Zahraa%20Al%20Sahili%20and%20Ioannis%20Patras%20and%20Matthew%20Purver&entry.1292438233=%20%20In%20the%20domain%20of%20text-to-image%20generative%20models%2C%20the%20inadvertent%20propagation%0Aof%20biases%20inherent%20in%20training%20datasets%20poses%20significant%20ethical%20challenges%2C%0Aparticularly%20in%20the%20generation%20of%20socially%20sensitive%20content.%20This%20paper%0Aintroduces%20EquiPrompt%2C%20a%20novel%20method%20employing%20Chain%20of%20Thought%20%28CoT%29%0Areasoning%20to%20reduce%20biases%20in%20text-to-image%20generative%20models.%20EquiPrompt%20uses%0Aiterative%20bootstrapping%20and%20bias-aware%20exemplar%20selection%20to%20balance%20creativity%0Aand%20ethical%20responsibility.%20It%20integrates%20iterative%20reasoning%20refinement%20with%0Acontrolled%20evaluation%20techniques%2C%20addressing%20zero-shot%20CoT%20issues%20in%20sensitive%0Acontexts.%20Experiments%20on%20several%20generation%20tasks%20show%20EquiPrompt%20effectively%0Alowers%20bias%20while%20maintaining%20generative%20quality%2C%20advancing%20ethical%20AI%20and%0Asocially%20responsible%20creative%20processes.Code%20will%20be%20publically%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09070v1&entry.124074799=Read"},
{"title": "CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking\n  with Event Cameras", "author": "Sachin Shah and Matthew Albert Chan and Haoming Cai and Jingxi Chen and Sakshum Kulshrestha and Chahat Deep Singh and Yiannis Aloimonos and Christopher Metzler", "abstract": "  Point-spread-function (PSF) engineering is a well-established computational\nimaging technique that uses phase masks and other optical elements to embed\nextra information (e.g., depth) into the images captured by conventional CMOS\nimage sensors. To date, however, PSF-engineering has not been applied to\nneuromorphic event cameras; a powerful new image sensing technology that\nresponds to changes in the log-intensity of light.\n  This paper establishes theoretical limits (Cram\\'er Rao bounds) on 3D point\nlocalization and tracking with PSF-engineered event cameras. Using these\nbounds, we first demonstrate that existing Fisher phase masks are already\nnear-optimal for localizing static flashing point sources (e.g., blinking\nfluorescent molecules). We then demonstrate that existing designs are\nsub-optimal for tracking moving point sources and proceed to use our theory to\ndesign optimal phase masks and binary amplitude masks for this task. To\novercome the non-convexity of the design problem, we leverage novel implicit\nneural representation based parameterizations of the phase and amplitude masks.\nWe demonstrate the efficacy of our designs through extensive simulations. We\nalso validate our method with a simple prototype.\n", "link": "http://arxiv.org/abs/2406.09409v1", "date": "2024-06-13", "relevancy": 2.7634, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5788}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5534}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodedEvents%3A%20Optimal%20Point-Spread-Function%20Engineering%20for%203D-Tracking%0A%20%20with%20Event%20Cameras&body=Title%3A%20CodedEvents%3A%20Optimal%20Point-Spread-Function%20Engineering%20for%203D-Tracking%0A%20%20with%20Event%20Cameras%0AAuthor%3A%20Sachin%20Shah%20and%20Matthew%20Albert%20Chan%20and%20Haoming%20Cai%20and%20Jingxi%20Chen%20and%20Sakshum%20Kulshrestha%20and%20Chahat%20Deep%20Singh%20and%20Yiannis%20Aloimonos%20and%20Christopher%20Metzler%0AAbstract%3A%20%20%20Point-spread-function%20%28PSF%29%20engineering%20is%20a%20well-established%20computational%0Aimaging%20technique%20that%20uses%20phase%20masks%20and%20other%20optical%20elements%20to%20embed%0Aextra%20information%20%28e.g.%2C%20depth%29%20into%20the%20images%20captured%20by%20conventional%20CMOS%0Aimage%20sensors.%20To%20date%2C%20however%2C%20PSF-engineering%20has%20not%20been%20applied%20to%0Aneuromorphic%20event%20cameras%3B%20a%20powerful%20new%20image%20sensing%20technology%20that%0Aresponds%20to%20changes%20in%20the%20log-intensity%20of%20light.%0A%20%20This%20paper%20establishes%20theoretical%20limits%20%28Cram%5C%27er%20Rao%20bounds%29%20on%203D%20point%0Alocalization%20and%20tracking%20with%20PSF-engineered%20event%20cameras.%20Using%20these%0Abounds%2C%20we%20first%20demonstrate%20that%20existing%20Fisher%20phase%20masks%20are%20already%0Anear-optimal%20for%20localizing%20static%20flashing%20point%20sources%20%28e.g.%2C%20blinking%0Afluorescent%20molecules%29.%20We%20then%20demonstrate%20that%20existing%20designs%20are%0Asub-optimal%20for%20tracking%20moving%20point%20sources%20and%20proceed%20to%20use%20our%20theory%20to%0Adesign%20optimal%20phase%20masks%20and%20binary%20amplitude%20masks%20for%20this%20task.%20To%0Aovercome%20the%20non-convexity%20of%20the%20design%20problem%2C%20we%20leverage%20novel%20implicit%0Aneural%20representation%20based%20parameterizations%20of%20the%20phase%20and%20amplitude%20masks.%0AWe%20demonstrate%20the%20efficacy%20of%20our%20designs%20through%20extensive%20simulations.%20We%0Aalso%20validate%20our%20method%20with%20a%20simple%20prototype.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodedEvents%253A%2520Optimal%2520Point-Spread-Function%2520Engineering%2520for%25203D-Tracking%250A%2520%2520with%2520Event%2520Cameras%26entry.906535625%3DSachin%2520Shah%2520and%2520Matthew%2520Albert%2520Chan%2520and%2520Haoming%2520Cai%2520and%2520Jingxi%2520Chen%2520and%2520Sakshum%2520Kulshrestha%2520and%2520Chahat%2520Deep%2520Singh%2520and%2520Yiannis%2520Aloimonos%2520and%2520Christopher%2520Metzler%26entry.1292438233%3D%2520%2520Point-spread-function%2520%2528PSF%2529%2520engineering%2520is%2520a%2520well-established%2520computational%250Aimaging%2520technique%2520that%2520uses%2520phase%2520masks%2520and%2520other%2520optical%2520elements%2520to%2520embed%250Aextra%2520information%2520%2528e.g.%252C%2520depth%2529%2520into%2520the%2520images%2520captured%2520by%2520conventional%2520CMOS%250Aimage%2520sensors.%2520To%2520date%252C%2520however%252C%2520PSF-engineering%2520has%2520not%2520been%2520applied%2520to%250Aneuromorphic%2520event%2520cameras%253B%2520a%2520powerful%2520new%2520image%2520sensing%2520technology%2520that%250Aresponds%2520to%2520changes%2520in%2520the%2520log-intensity%2520of%2520light.%250A%2520%2520This%2520paper%2520establishes%2520theoretical%2520limits%2520%2528Cram%255C%2527er%2520Rao%2520bounds%2529%2520on%25203D%2520point%250Alocalization%2520and%2520tracking%2520with%2520PSF-engineered%2520event%2520cameras.%2520Using%2520these%250Abounds%252C%2520we%2520first%2520demonstrate%2520that%2520existing%2520Fisher%2520phase%2520masks%2520are%2520already%250Anear-optimal%2520for%2520localizing%2520static%2520flashing%2520point%2520sources%2520%2528e.g.%252C%2520blinking%250Afluorescent%2520molecules%2529.%2520We%2520then%2520demonstrate%2520that%2520existing%2520designs%2520are%250Asub-optimal%2520for%2520tracking%2520moving%2520point%2520sources%2520and%2520proceed%2520to%2520use%2520our%2520theory%2520to%250Adesign%2520optimal%2520phase%2520masks%2520and%2520binary%2520amplitude%2520masks%2520for%2520this%2520task.%2520To%250Aovercome%2520the%2520non-convexity%2520of%2520the%2520design%2520problem%252C%2520we%2520leverage%2520novel%2520implicit%250Aneural%2520representation%2520based%2520parameterizations%2520of%2520the%2520phase%2520and%2520amplitude%2520masks.%250AWe%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520designs%2520through%2520extensive%2520simulations.%2520We%250Aalso%2520validate%2520our%2520method%2520with%2520a%2520simple%2520prototype.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodedEvents%3A%20Optimal%20Point-Spread-Function%20Engineering%20for%203D-Tracking%0A%20%20with%20Event%20Cameras&entry.906535625=Sachin%20Shah%20and%20Matthew%20Albert%20Chan%20and%20Haoming%20Cai%20and%20Jingxi%20Chen%20and%20Sakshum%20Kulshrestha%20and%20Chahat%20Deep%20Singh%20and%20Yiannis%20Aloimonos%20and%20Christopher%20Metzler&entry.1292438233=%20%20Point-spread-function%20%28PSF%29%20engineering%20is%20a%20well-established%20computational%0Aimaging%20technique%20that%20uses%20phase%20masks%20and%20other%20optical%20elements%20to%20embed%0Aextra%20information%20%28e.g.%2C%20depth%29%20into%20the%20images%20captured%20by%20conventional%20CMOS%0Aimage%20sensors.%20To%20date%2C%20however%2C%20PSF-engineering%20has%20not%20been%20applied%20to%0Aneuromorphic%20event%20cameras%3B%20a%20powerful%20new%20image%20sensing%20technology%20that%0Aresponds%20to%20changes%20in%20the%20log-intensity%20of%20light.%0A%20%20This%20paper%20establishes%20theoretical%20limits%20%28Cram%5C%27er%20Rao%20bounds%29%20on%203D%20point%0Alocalization%20and%20tracking%20with%20PSF-engineered%20event%20cameras.%20Using%20these%0Abounds%2C%20we%20first%20demonstrate%20that%20existing%20Fisher%20phase%20masks%20are%20already%0Anear-optimal%20for%20localizing%20static%20flashing%20point%20sources%20%28e.g.%2C%20blinking%0Afluorescent%20molecules%29.%20We%20then%20demonstrate%20that%20existing%20designs%20are%0Asub-optimal%20for%20tracking%20moving%20point%20sources%20and%20proceed%20to%20use%20our%20theory%20to%0Adesign%20optimal%20phase%20masks%20and%20binary%20amplitude%20masks%20for%20this%20task.%20To%0Aovercome%20the%20non-convexity%20of%20the%20design%20problem%2C%20we%20leverage%20novel%20implicit%0Aneural%20representation%20based%20parameterizations%20of%20the%20phase%20and%20amplitude%20masks.%0AWe%20demonstrate%20the%20efficacy%20of%20our%20designs%20through%20extensive%20simulations.%20We%0Aalso%20validate%20our%20method%20with%20a%20simple%20prototype.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09409v1&entry.124074799=Read"},
{"title": "Exploring the Spectrum of Visio-Linguistic Compositionality and\n  Recognition", "author": "Youngtaek Oh and Pyunghwan Ahn and Jinhyung Kim and Gwangmo Song and Soonyoung Lee and In So Kweon and Junmo Kim", "abstract": "  Vision and language models (VLMs) such as CLIP have showcased remarkable\nzero-shot recognition abilities yet face challenges in visio-linguistic\ncompositionality, particularly in linguistic comprehension and fine-grained\nimage-text alignment. This paper explores the intricate relationship between\ncompositionality and recognition -- two pivotal aspects of VLM capability. We\nconduct a comprehensive evaluation of existing VLMs, covering both pre-training\napproaches aimed at recognition and the fine-tuning methods designed to improve\ncompositionality. Our evaluation employs 12 benchmarks for compositionality,\nalong with 21 zero-shot classification and two retrieval benchmarks for\nrecognition. In our analysis from 274 CLIP model checkpoints, we reveal\npatterns and trade-offs that emerge between compositional understanding and\nrecognition accuracy. Ultimately, this necessitates strategic efforts towards\ndeveloping models that improve both capabilities, as well as the meticulous\nformulation of benchmarks for compositionality. We open our evaluation\nframework at https://github.com/ytaek-oh/vl_compo.\n", "link": "http://arxiv.org/abs/2406.09388v1", "date": "2024-06-13", "relevancy": 2.7378, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.561}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Spectrum%20of%20Visio-Linguistic%20Compositionality%20and%0A%20%20Recognition&body=Title%3A%20Exploring%20the%20Spectrum%20of%20Visio-Linguistic%20Compositionality%20and%0A%20%20Recognition%0AAuthor%3A%20Youngtaek%20Oh%20and%20Pyunghwan%20Ahn%20and%20Jinhyung%20Kim%20and%20Gwangmo%20Song%20and%20Soonyoung%20Lee%20and%20In%20So%20Kweon%20and%20Junmo%20Kim%0AAbstract%3A%20%20%20Vision%20and%20language%20models%20%28VLMs%29%20such%20as%20CLIP%20have%20showcased%20remarkable%0Azero-shot%20recognition%20abilities%20yet%20face%20challenges%20in%20visio-linguistic%0Acompositionality%2C%20particularly%20in%20linguistic%20comprehension%20and%20fine-grained%0Aimage-text%20alignment.%20This%20paper%20explores%20the%20intricate%20relationship%20between%0Acompositionality%20and%20recognition%20--%20two%20pivotal%20aspects%20of%20VLM%20capability.%20We%0Aconduct%20a%20comprehensive%20evaluation%20of%20existing%20VLMs%2C%20covering%20both%20pre-training%0Aapproaches%20aimed%20at%20recognition%20and%20the%20fine-tuning%20methods%20designed%20to%20improve%0Acompositionality.%20Our%20evaluation%20employs%2012%20benchmarks%20for%20compositionality%2C%0Aalong%20with%2021%20zero-shot%20classification%20and%20two%20retrieval%20benchmarks%20for%0Arecognition.%20In%20our%20analysis%20from%20274%20CLIP%20model%20checkpoints%2C%20we%20reveal%0Apatterns%20and%20trade-offs%20that%20emerge%20between%20compositional%20understanding%20and%0Arecognition%20accuracy.%20Ultimately%2C%20this%20necessitates%20strategic%20efforts%20towards%0Adeveloping%20models%20that%20improve%20both%20capabilities%2C%20as%20well%20as%20the%20meticulous%0Aformulation%20of%20benchmarks%20for%20compositionality.%20We%20open%20our%20evaluation%0Aframework%20at%20https%3A//github.com/ytaek-oh/vl_compo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Spectrum%2520of%2520Visio-Linguistic%2520Compositionality%2520and%250A%2520%2520Recognition%26entry.906535625%3DYoungtaek%2520Oh%2520and%2520Pyunghwan%2520Ahn%2520and%2520Jinhyung%2520Kim%2520and%2520Gwangmo%2520Song%2520and%2520Soonyoung%2520Lee%2520and%2520In%2520So%2520Kweon%2520and%2520Junmo%2520Kim%26entry.1292438233%3D%2520%2520Vision%2520and%2520language%2520models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520have%2520showcased%2520remarkable%250Azero-shot%2520recognition%2520abilities%2520yet%2520face%2520challenges%2520in%2520visio-linguistic%250Acompositionality%252C%2520particularly%2520in%2520linguistic%2520comprehension%2520and%2520fine-grained%250Aimage-text%2520alignment.%2520This%2520paper%2520explores%2520the%2520intricate%2520relationship%2520between%250Acompositionality%2520and%2520recognition%2520--%2520two%2520pivotal%2520aspects%2520of%2520VLM%2520capability.%2520We%250Aconduct%2520a%2520comprehensive%2520evaluation%2520of%2520existing%2520VLMs%252C%2520covering%2520both%2520pre-training%250Aapproaches%2520aimed%2520at%2520recognition%2520and%2520the%2520fine-tuning%2520methods%2520designed%2520to%2520improve%250Acompositionality.%2520Our%2520evaluation%2520employs%252012%2520benchmarks%2520for%2520compositionality%252C%250Aalong%2520with%252021%2520zero-shot%2520classification%2520and%2520two%2520retrieval%2520benchmarks%2520for%250Arecognition.%2520In%2520our%2520analysis%2520from%2520274%2520CLIP%2520model%2520checkpoints%252C%2520we%2520reveal%250Apatterns%2520and%2520trade-offs%2520that%2520emerge%2520between%2520compositional%2520understanding%2520and%250Arecognition%2520accuracy.%2520Ultimately%252C%2520this%2520necessitates%2520strategic%2520efforts%2520towards%250Adeveloping%2520models%2520that%2520improve%2520both%2520capabilities%252C%2520as%2520well%2520as%2520the%2520meticulous%250Aformulation%2520of%2520benchmarks%2520for%2520compositionality.%2520We%2520open%2520our%2520evaluation%250Aframework%2520at%2520https%253A//github.com/ytaek-oh/vl_compo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Spectrum%20of%20Visio-Linguistic%20Compositionality%20and%0A%20%20Recognition&entry.906535625=Youngtaek%20Oh%20and%20Pyunghwan%20Ahn%20and%20Jinhyung%20Kim%20and%20Gwangmo%20Song%20and%20Soonyoung%20Lee%20and%20In%20So%20Kweon%20and%20Junmo%20Kim&entry.1292438233=%20%20Vision%20and%20language%20models%20%28VLMs%29%20such%20as%20CLIP%20have%20showcased%20remarkable%0Azero-shot%20recognition%20abilities%20yet%20face%20challenges%20in%20visio-linguistic%0Acompositionality%2C%20particularly%20in%20linguistic%20comprehension%20and%20fine-grained%0Aimage-text%20alignment.%20This%20paper%20explores%20the%20intricate%20relationship%20between%0Acompositionality%20and%20recognition%20--%20two%20pivotal%20aspects%20of%20VLM%20capability.%20We%0Aconduct%20a%20comprehensive%20evaluation%20of%20existing%20VLMs%2C%20covering%20both%20pre-training%0Aapproaches%20aimed%20at%20recognition%20and%20the%20fine-tuning%20methods%20designed%20to%20improve%0Acompositionality.%20Our%20evaluation%20employs%2012%20benchmarks%20for%20compositionality%2C%0Aalong%20with%2021%20zero-shot%20classification%20and%20two%20retrieval%20benchmarks%20for%0Arecognition.%20In%20our%20analysis%20from%20274%20CLIP%20model%20checkpoints%2C%20we%20reveal%0Apatterns%20and%20trade-offs%20that%20emerge%20between%20compositional%20understanding%20and%0Arecognition%20accuracy.%20Ultimately%2C%20this%20necessitates%20strategic%20efforts%20towards%0Adeveloping%20models%20that%20improve%20both%20capabilities%2C%20as%20well%20as%20the%20meticulous%0Aformulation%20of%20benchmarks%20for%20compositionality.%20We%20open%20our%20evaluation%0Aframework%20at%20https%3A//github.com/ytaek-oh/vl_compo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09388v1&entry.124074799=Read"},
{"title": "Adaptive Slot Attention: Object Discovery with Dynamic Slot Number", "author": "Ke Fan and Zechen Bai and Tianjun Xiao and Tong He and Max Horn and Yanwei Fu and Francesco Locatello and Zheng Zhang", "abstract": "  Object-centric learning (OCL) extracts the representation of objects with\nslots, offering an exceptional blend of flexibility and interpretability for\nabstracting low-level perceptual features. A widely adopted method within OCL\nis slot attention, which utilizes attention mechanisms to iteratively refine\nslot representations. However, a major drawback of most object-centric models,\nincluding slot attention, is their reliance on predefining the number of slots.\nThis not only necessitates prior knowledge of the dataset but also overlooks\nthe inherent variability in the number of objects present in each instance. To\novercome this fundamental limitation, we present a novel complexity-aware\nobject auto-encoder framework. Within this framework, we introduce an adaptive\nslot attention (AdaSlot) mechanism that dynamically determines the optimal\nnumber of slots based on the content of the data. This is achieved by proposing\na discrete slot sampling module that is responsible for selecting an\nappropriate number of slots from a candidate list. Furthermore, we introduce a\nmasked slot decoder that suppresses unselected slots during the decoding\nprocess. Our framework, tested extensively on object discovery tasks with\nvarious datasets, shows performance matching or exceeding top fixed-slot\nmodels. Moreover, our analysis substantiates that our method exhibits the\ncapability to dynamically adapt the slot number according to each instance's\ncomplexity, offering the potential for further exploration in slot attention\nresearch. Project will be available at https://kfan21.github.io/AdaSlot/\n", "link": "http://arxiv.org/abs/2406.09196v1", "date": "2024-06-13", "relevancy": 2.7317, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5619}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5453}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Slot%20Attention%3A%20Object%20Discovery%20with%20Dynamic%20Slot%20Number&body=Title%3A%20Adaptive%20Slot%20Attention%3A%20Object%20Discovery%20with%20Dynamic%20Slot%20Number%0AAuthor%3A%20Ke%20Fan%20and%20Zechen%20Bai%20and%20Tianjun%20Xiao%20and%20Tong%20He%20and%20Max%20Horn%20and%20Yanwei%20Fu%20and%20Francesco%20Locatello%20and%20Zheng%20Zhang%0AAbstract%3A%20%20%20Object-centric%20learning%20%28OCL%29%20extracts%20the%20representation%20of%20objects%20with%0Aslots%2C%20offering%20an%20exceptional%20blend%20of%20flexibility%20and%20interpretability%20for%0Aabstracting%20low-level%20perceptual%20features.%20A%20widely%20adopted%20method%20within%20OCL%0Ais%20slot%20attention%2C%20which%20utilizes%20attention%20mechanisms%20to%20iteratively%20refine%0Aslot%20representations.%20However%2C%20a%20major%20drawback%20of%20most%20object-centric%20models%2C%0Aincluding%20slot%20attention%2C%20is%20their%20reliance%20on%20predefining%20the%20number%20of%20slots.%0AThis%20not%20only%20necessitates%20prior%20knowledge%20of%20the%20dataset%20but%20also%20overlooks%0Athe%20inherent%20variability%20in%20the%20number%20of%20objects%20present%20in%20each%20instance.%20To%0Aovercome%20this%20fundamental%20limitation%2C%20we%20present%20a%20novel%20complexity-aware%0Aobject%20auto-encoder%20framework.%20Within%20this%20framework%2C%20we%20introduce%20an%20adaptive%0Aslot%20attention%20%28AdaSlot%29%20mechanism%20that%20dynamically%20determines%20the%20optimal%0Anumber%20of%20slots%20based%20on%20the%20content%20of%20the%20data.%20This%20is%20achieved%20by%20proposing%0Aa%20discrete%20slot%20sampling%20module%20that%20is%20responsible%20for%20selecting%20an%0Aappropriate%20number%20of%20slots%20from%20a%20candidate%20list.%20Furthermore%2C%20we%20introduce%20a%0Amasked%20slot%20decoder%20that%20suppresses%20unselected%20slots%20during%20the%20decoding%0Aprocess.%20Our%20framework%2C%20tested%20extensively%20on%20object%20discovery%20tasks%20with%0Avarious%20datasets%2C%20shows%20performance%20matching%20or%20exceeding%20top%20fixed-slot%0Amodels.%20Moreover%2C%20our%20analysis%20substantiates%20that%20our%20method%20exhibits%20the%0Acapability%20to%20dynamically%20adapt%20the%20slot%20number%20according%20to%20each%20instance%27s%0Acomplexity%2C%20offering%20the%20potential%20for%20further%20exploration%20in%20slot%20attention%0Aresearch.%20Project%20will%20be%20available%20at%20https%3A//kfan21.github.io/AdaSlot/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Slot%2520Attention%253A%2520Object%2520Discovery%2520with%2520Dynamic%2520Slot%2520Number%26entry.906535625%3DKe%2520Fan%2520and%2520Zechen%2520Bai%2520and%2520Tianjun%2520Xiao%2520and%2520Tong%2520He%2520and%2520Max%2520Horn%2520and%2520Yanwei%2520Fu%2520and%2520Francesco%2520Locatello%2520and%2520Zheng%2520Zhang%26entry.1292438233%3D%2520%2520Object-centric%2520learning%2520%2528OCL%2529%2520extracts%2520the%2520representation%2520of%2520objects%2520with%250Aslots%252C%2520offering%2520an%2520exceptional%2520blend%2520of%2520flexibility%2520and%2520interpretability%2520for%250Aabstracting%2520low-level%2520perceptual%2520features.%2520A%2520widely%2520adopted%2520method%2520within%2520OCL%250Ais%2520slot%2520attention%252C%2520which%2520utilizes%2520attention%2520mechanisms%2520to%2520iteratively%2520refine%250Aslot%2520representations.%2520However%252C%2520a%2520major%2520drawback%2520of%2520most%2520object-centric%2520models%252C%250Aincluding%2520slot%2520attention%252C%2520is%2520their%2520reliance%2520on%2520predefining%2520the%2520number%2520of%2520slots.%250AThis%2520not%2520only%2520necessitates%2520prior%2520knowledge%2520of%2520the%2520dataset%2520but%2520also%2520overlooks%250Athe%2520inherent%2520variability%2520in%2520the%2520number%2520of%2520objects%2520present%2520in%2520each%2520instance.%2520To%250Aovercome%2520this%2520fundamental%2520limitation%252C%2520we%2520present%2520a%2520novel%2520complexity-aware%250Aobject%2520auto-encoder%2520framework.%2520Within%2520this%2520framework%252C%2520we%2520introduce%2520an%2520adaptive%250Aslot%2520attention%2520%2528AdaSlot%2529%2520mechanism%2520that%2520dynamically%2520determines%2520the%2520optimal%250Anumber%2520of%2520slots%2520based%2520on%2520the%2520content%2520of%2520the%2520data.%2520This%2520is%2520achieved%2520by%2520proposing%250Aa%2520discrete%2520slot%2520sampling%2520module%2520that%2520is%2520responsible%2520for%2520selecting%2520an%250Aappropriate%2520number%2520of%2520slots%2520from%2520a%2520candidate%2520list.%2520Furthermore%252C%2520we%2520introduce%2520a%250Amasked%2520slot%2520decoder%2520that%2520suppresses%2520unselected%2520slots%2520during%2520the%2520decoding%250Aprocess.%2520Our%2520framework%252C%2520tested%2520extensively%2520on%2520object%2520discovery%2520tasks%2520with%250Avarious%2520datasets%252C%2520shows%2520performance%2520matching%2520or%2520exceeding%2520top%2520fixed-slot%250Amodels.%2520Moreover%252C%2520our%2520analysis%2520substantiates%2520that%2520our%2520method%2520exhibits%2520the%250Acapability%2520to%2520dynamically%2520adapt%2520the%2520slot%2520number%2520according%2520to%2520each%2520instance%2527s%250Acomplexity%252C%2520offering%2520the%2520potential%2520for%2520further%2520exploration%2520in%2520slot%2520attention%250Aresearch.%2520Project%2520will%2520be%2520available%2520at%2520https%253A//kfan21.github.io/AdaSlot/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Slot%20Attention%3A%20Object%20Discovery%20with%20Dynamic%20Slot%20Number&entry.906535625=Ke%20Fan%20and%20Zechen%20Bai%20and%20Tianjun%20Xiao%20and%20Tong%20He%20and%20Max%20Horn%20and%20Yanwei%20Fu%20and%20Francesco%20Locatello%20and%20Zheng%20Zhang&entry.1292438233=%20%20Object-centric%20learning%20%28OCL%29%20extracts%20the%20representation%20of%20objects%20with%0Aslots%2C%20offering%20an%20exceptional%20blend%20of%20flexibility%20and%20interpretability%20for%0Aabstracting%20low-level%20perceptual%20features.%20A%20widely%20adopted%20method%20within%20OCL%0Ais%20slot%20attention%2C%20which%20utilizes%20attention%20mechanisms%20to%20iteratively%20refine%0Aslot%20representations.%20However%2C%20a%20major%20drawback%20of%20most%20object-centric%20models%2C%0Aincluding%20slot%20attention%2C%20is%20their%20reliance%20on%20predefining%20the%20number%20of%20slots.%0AThis%20not%20only%20necessitates%20prior%20knowledge%20of%20the%20dataset%20but%20also%20overlooks%0Athe%20inherent%20variability%20in%20the%20number%20of%20objects%20present%20in%20each%20instance.%20To%0Aovercome%20this%20fundamental%20limitation%2C%20we%20present%20a%20novel%20complexity-aware%0Aobject%20auto-encoder%20framework.%20Within%20this%20framework%2C%20we%20introduce%20an%20adaptive%0Aslot%20attention%20%28AdaSlot%29%20mechanism%20that%20dynamically%20determines%20the%20optimal%0Anumber%20of%20slots%20based%20on%20the%20content%20of%20the%20data.%20This%20is%20achieved%20by%20proposing%0Aa%20discrete%20slot%20sampling%20module%20that%20is%20responsible%20for%20selecting%20an%0Aappropriate%20number%20of%20slots%20from%20a%20candidate%20list.%20Furthermore%2C%20we%20introduce%20a%0Amasked%20slot%20decoder%20that%20suppresses%20unselected%20slots%20during%20the%20decoding%0Aprocess.%20Our%20framework%2C%20tested%20extensively%20on%20object%20discovery%20tasks%20with%0Avarious%20datasets%2C%20shows%20performance%20matching%20or%20exceeding%20top%20fixed-slot%0Amodels.%20Moreover%2C%20our%20analysis%20substantiates%20that%20our%20method%20exhibits%20the%0Acapability%20to%20dynamically%20adapt%20the%20slot%20number%20according%20to%20each%20instance%27s%0Acomplexity%2C%20offering%20the%20potential%20for%20further%20exploration%20in%20slot%20attention%0Aresearch.%20Project%20will%20be%20available%20at%20https%3A//kfan21.github.io/AdaSlot/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09196v1&entry.124074799=Read"},
{"title": "Reducing Task Discrepancy of Text Encoders for Zero-Shot Composed Image\n  Retrieval", "author": "Jaeseok Byun and Seokhyeon Jeong and Wonjae Kim and Sanghyuk Chun and Taesup Moon", "abstract": "  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nreference image and conditioning text, enabling controllable searches. Due to\nthe expensive dataset construction cost for CIR triplets, a zero-shot (ZS) CIR\nsetting has been actively studied to eliminate the need for human-collected\ntriplet datasets. The mainstream of ZS-CIR employs an efficient projection\nmodule that projects a CLIP image embedding to the CLIP text token embedding\nspace, while fixing the CLIP encoders. Using the projected image embedding,\nthese methods generate image-text composed features by using the pre-trained\ntext encoder. However, their CLIP image and text encoders suffer from the task\ndiscrepancy between the pre-training task (text $\\leftrightarrow$ image) and\nthe target CIR task (image + text $\\leftrightarrow$ image). Conceptually, we\nneed expensive triplet samples to reduce the discrepancy, but we use cheap text\ntriplets instead and update the text encoder. To that end, we introduce the\nReducing Task Discrepancy of text encoders for Composed Image Retrieval (RTD),\na plug-and-play training scheme for the text encoder that enhances its\ncapability using a novel target-anchored text contrastive learning. We also\npropose two additional techniques to improve the proposed learning scheme: a\nhard negatives-based refined batch sampling strategy and a sophisticated\nconcatenation scheme. Integrating RTD into the state-of-the-art\nprojection-based ZS-CIR methods significantly improves performance across\nvarious datasets and backbones, demonstrating its efficiency and\ngeneralizability.\n", "link": "http://arxiv.org/abs/2406.09188v1", "date": "2024-06-13", "relevancy": 2.709, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5668}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5344}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20Task%20Discrepancy%20of%20Text%20Encoders%20for%20Zero-Shot%20Composed%20Image%0A%20%20Retrieval&body=Title%3A%20Reducing%20Task%20Discrepancy%20of%20Text%20Encoders%20for%20Zero-Shot%20Composed%20Image%0A%20%20Retrieval%0AAuthor%3A%20Jaeseok%20Byun%20and%20Seokhyeon%20Jeong%20and%20Wonjae%20Kim%20and%20Sanghyuk%20Chun%20and%20Taesup%20Moon%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CIR%29%20aims%20to%20retrieve%20a%20target%20image%20based%20on%20a%0Areference%20image%20and%20conditioning%20text%2C%20enabling%20controllable%20searches.%20Due%20to%0Athe%20expensive%20dataset%20construction%20cost%20for%20CIR%20triplets%2C%20a%20zero-shot%20%28ZS%29%20CIR%0Asetting%20has%20been%20actively%20studied%20to%20eliminate%20the%20need%20for%20human-collected%0Atriplet%20datasets.%20The%20mainstream%20of%20ZS-CIR%20employs%20an%20efficient%20projection%0Amodule%20that%20projects%20a%20CLIP%20image%20embedding%20to%20the%20CLIP%20text%20token%20embedding%0Aspace%2C%20while%20fixing%20the%20CLIP%20encoders.%20Using%20the%20projected%20image%20embedding%2C%0Athese%20methods%20generate%20image-text%20composed%20features%20by%20using%20the%20pre-trained%0Atext%20encoder.%20However%2C%20their%20CLIP%20image%20and%20text%20encoders%20suffer%20from%20the%20task%0Adiscrepancy%20between%20the%20pre-training%20task%20%28text%20%24%5Cleftrightarrow%24%20image%29%20and%0Athe%20target%20CIR%20task%20%28image%20%2B%20text%20%24%5Cleftrightarrow%24%20image%29.%20Conceptually%2C%20we%0Aneed%20expensive%20triplet%20samples%20to%20reduce%20the%20discrepancy%2C%20but%20we%20use%20cheap%20text%0Atriplets%20instead%20and%20update%20the%20text%20encoder.%20To%20that%20end%2C%20we%20introduce%20the%0AReducing%20Task%20Discrepancy%20of%20text%20encoders%20for%20Composed%20Image%20Retrieval%20%28RTD%29%2C%0Aa%20plug-and-play%20training%20scheme%20for%20the%20text%20encoder%20that%20enhances%20its%0Acapability%20using%20a%20novel%20target-anchored%20text%20contrastive%20learning.%20We%20also%0Apropose%20two%20additional%20techniques%20to%20improve%20the%20proposed%20learning%20scheme%3A%20a%0Ahard%20negatives-based%20refined%20batch%20sampling%20strategy%20and%20a%20sophisticated%0Aconcatenation%20scheme.%20Integrating%20RTD%20into%20the%20state-of-the-art%0Aprojection-based%20ZS-CIR%20methods%20significantly%20improves%20performance%20across%0Avarious%20datasets%20and%20backbones%2C%20demonstrating%20its%20efficiency%20and%0Ageneralizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520Task%2520Discrepancy%2520of%2520Text%2520Encoders%2520for%2520Zero-Shot%2520Composed%2520Image%250A%2520%2520Retrieval%26entry.906535625%3DJaeseok%2520Byun%2520and%2520Seokhyeon%2520Jeong%2520and%2520Wonjae%2520Kim%2520and%2520Sanghyuk%2520Chun%2520and%2520Taesup%2520Moon%26entry.1292438233%3D%2520%2520Composed%2520Image%2520Retrieval%2520%2528CIR%2529%2520aims%2520to%2520retrieve%2520a%2520target%2520image%2520based%2520on%2520a%250Areference%2520image%2520and%2520conditioning%2520text%252C%2520enabling%2520controllable%2520searches.%2520Due%2520to%250Athe%2520expensive%2520dataset%2520construction%2520cost%2520for%2520CIR%2520triplets%252C%2520a%2520zero-shot%2520%2528ZS%2529%2520CIR%250Asetting%2520has%2520been%2520actively%2520studied%2520to%2520eliminate%2520the%2520need%2520for%2520human-collected%250Atriplet%2520datasets.%2520The%2520mainstream%2520of%2520ZS-CIR%2520employs%2520an%2520efficient%2520projection%250Amodule%2520that%2520projects%2520a%2520CLIP%2520image%2520embedding%2520to%2520the%2520CLIP%2520text%2520token%2520embedding%250Aspace%252C%2520while%2520fixing%2520the%2520CLIP%2520encoders.%2520Using%2520the%2520projected%2520image%2520embedding%252C%250Athese%2520methods%2520generate%2520image-text%2520composed%2520features%2520by%2520using%2520the%2520pre-trained%250Atext%2520encoder.%2520However%252C%2520their%2520CLIP%2520image%2520and%2520text%2520encoders%2520suffer%2520from%2520the%2520task%250Adiscrepancy%2520between%2520the%2520pre-training%2520task%2520%2528text%2520%2524%255Cleftrightarrow%2524%2520image%2529%2520and%250Athe%2520target%2520CIR%2520task%2520%2528image%2520%252B%2520text%2520%2524%255Cleftrightarrow%2524%2520image%2529.%2520Conceptually%252C%2520we%250Aneed%2520expensive%2520triplet%2520samples%2520to%2520reduce%2520the%2520discrepancy%252C%2520but%2520we%2520use%2520cheap%2520text%250Atriplets%2520instead%2520and%2520update%2520the%2520text%2520encoder.%2520To%2520that%2520end%252C%2520we%2520introduce%2520the%250AReducing%2520Task%2520Discrepancy%2520of%2520text%2520encoders%2520for%2520Composed%2520Image%2520Retrieval%2520%2528RTD%2529%252C%250Aa%2520plug-and-play%2520training%2520scheme%2520for%2520the%2520text%2520encoder%2520that%2520enhances%2520its%250Acapability%2520using%2520a%2520novel%2520target-anchored%2520text%2520contrastive%2520learning.%2520We%2520also%250Apropose%2520two%2520additional%2520techniques%2520to%2520improve%2520the%2520proposed%2520learning%2520scheme%253A%2520a%250Ahard%2520negatives-based%2520refined%2520batch%2520sampling%2520strategy%2520and%2520a%2520sophisticated%250Aconcatenation%2520scheme.%2520Integrating%2520RTD%2520into%2520the%2520state-of-the-art%250Aprojection-based%2520ZS-CIR%2520methods%2520significantly%2520improves%2520performance%2520across%250Avarious%2520datasets%2520and%2520backbones%252C%2520demonstrating%2520its%2520efficiency%2520and%250Ageneralizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20Task%20Discrepancy%20of%20Text%20Encoders%20for%20Zero-Shot%20Composed%20Image%0A%20%20Retrieval&entry.906535625=Jaeseok%20Byun%20and%20Seokhyeon%20Jeong%20and%20Wonjae%20Kim%20and%20Sanghyuk%20Chun%20and%20Taesup%20Moon&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CIR%29%20aims%20to%20retrieve%20a%20target%20image%20based%20on%20a%0Areference%20image%20and%20conditioning%20text%2C%20enabling%20controllable%20searches.%20Due%20to%0Athe%20expensive%20dataset%20construction%20cost%20for%20CIR%20triplets%2C%20a%20zero-shot%20%28ZS%29%20CIR%0Asetting%20has%20been%20actively%20studied%20to%20eliminate%20the%20need%20for%20human-collected%0Atriplet%20datasets.%20The%20mainstream%20of%20ZS-CIR%20employs%20an%20efficient%20projection%0Amodule%20that%20projects%20a%20CLIP%20image%20embedding%20to%20the%20CLIP%20text%20token%20embedding%0Aspace%2C%20while%20fixing%20the%20CLIP%20encoders.%20Using%20the%20projected%20image%20embedding%2C%0Athese%20methods%20generate%20image-text%20composed%20features%20by%20using%20the%20pre-trained%0Atext%20encoder.%20However%2C%20their%20CLIP%20image%20and%20text%20encoders%20suffer%20from%20the%20task%0Adiscrepancy%20between%20the%20pre-training%20task%20%28text%20%24%5Cleftrightarrow%24%20image%29%20and%0Athe%20target%20CIR%20task%20%28image%20%2B%20text%20%24%5Cleftrightarrow%24%20image%29.%20Conceptually%2C%20we%0Aneed%20expensive%20triplet%20samples%20to%20reduce%20the%20discrepancy%2C%20but%20we%20use%20cheap%20text%0Atriplets%20instead%20and%20update%20the%20text%20encoder.%20To%20that%20end%2C%20we%20introduce%20the%0AReducing%20Task%20Discrepancy%20of%20text%20encoders%20for%20Composed%20Image%20Retrieval%20%28RTD%29%2C%0Aa%20plug-and-play%20training%20scheme%20for%20the%20text%20encoder%20that%20enhances%20its%0Acapability%20using%20a%20novel%20target-anchored%20text%20contrastive%20learning.%20We%20also%0Apropose%20two%20additional%20techniques%20to%20improve%20the%20proposed%20learning%20scheme%3A%20a%0Ahard%20negatives-based%20refined%20batch%20sampling%20strategy%20and%20a%20sophisticated%0Aconcatenation%20scheme.%20Integrating%20RTD%20into%20the%20state-of-the-art%0Aprojection-based%20ZS-CIR%20methods%20significantly%20improves%20performance%20across%0Avarious%20datasets%20and%20backbones%2C%20demonstrating%20its%20efficiency%20and%0Ageneralizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09188v1&entry.124074799=Read"},
{"title": "Enhancing Domain Adaptation through Prompt Gradient Alignment", "author": "Hoang Phan and Lam Tran and Quyen Tran and Trung Le", "abstract": "  Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a\ndomain-invariant feature extractor, which may hinder the model from learning\nsufficiently discriminative features. To tackle this, a line of works based on\nprompt learning leverages the power of large-scale pre-trained vision-language\nmodels to learn both domain-invariant and specific features through a set of\ndomain-agnostic and domain-specific learnable prompts. Those studies typically\nenforce invariant constraints on representation, output, or prompt space to\nlearn such prompts. Differently, we cast UDA as a multiple-objective\noptimization problem in which each objective is represented by a domain loss.\nUnder this new framework, we propose aligning per-objective gradients to foster\nconsensus between them. Additionally, to prevent potential overfitting when\nfine-tuning this deep learning architecture, we penalize the norm of these\ngradients. To achieve these goals, we devise a practical gradient update\nprocedure that can work under both single-source and multi-source UDA.\nEmpirically, our method consistently surpasses other prompt-based baselines by\na large margin on different UDA benchmarks\n", "link": "http://arxiv.org/abs/2406.09353v1", "date": "2024-06-13", "relevancy": 2.6849, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5305}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Domain%20Adaptation%20through%20Prompt%20Gradient%20Alignment&body=Title%3A%20Enhancing%20Domain%20Adaptation%20through%20Prompt%20Gradient%20Alignment%0AAuthor%3A%20Hoang%20Phan%20and%20Lam%20Tran%20and%20Quyen%20Tran%20and%20Trung%20Le%0AAbstract%3A%20%20%20Prior%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20methods%20often%20aim%20to%20train%20a%0Adomain-invariant%20feature%20extractor%2C%20which%20may%20hinder%20the%20model%20from%20learning%0Asufficiently%20discriminative%20features.%20To%20tackle%20this%2C%20a%20line%20of%20works%20based%20on%0Aprompt%20learning%20leverages%20the%20power%20of%20large-scale%20pre-trained%20vision-language%0Amodels%20to%20learn%20both%20domain-invariant%20and%20specific%20features%20through%20a%20set%20of%0Adomain-agnostic%20and%20domain-specific%20learnable%20prompts.%20Those%20studies%20typically%0Aenforce%20invariant%20constraints%20on%20representation%2C%20output%2C%20or%20prompt%20space%20to%0Alearn%20such%20prompts.%20Differently%2C%20we%20cast%20UDA%20as%20a%20multiple-objective%0Aoptimization%20problem%20in%20which%20each%20objective%20is%20represented%20by%20a%20domain%20loss.%0AUnder%20this%20new%20framework%2C%20we%20propose%20aligning%20per-objective%20gradients%20to%20foster%0Aconsensus%20between%20them.%20Additionally%2C%20to%20prevent%20potential%20overfitting%20when%0Afine-tuning%20this%20deep%20learning%20architecture%2C%20we%20penalize%20the%20norm%20of%20these%0Agradients.%20To%20achieve%20these%20goals%2C%20we%20devise%20a%20practical%20gradient%20update%0Aprocedure%20that%20can%20work%20under%20both%20single-source%20and%20multi-source%20UDA.%0AEmpirically%2C%20our%20method%20consistently%20surpasses%20other%20prompt-based%20baselines%20by%0Aa%20large%20margin%20on%20different%20UDA%20benchmarks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Domain%2520Adaptation%2520through%2520Prompt%2520Gradient%2520Alignment%26entry.906535625%3DHoang%2520Phan%2520and%2520Lam%2520Tran%2520and%2520Quyen%2520Tran%2520and%2520Trung%2520Le%26entry.1292438233%3D%2520%2520Prior%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520methods%2520often%2520aim%2520to%2520train%2520a%250Adomain-invariant%2520feature%2520extractor%252C%2520which%2520may%2520hinder%2520the%2520model%2520from%2520learning%250Asufficiently%2520discriminative%2520features.%2520To%2520tackle%2520this%252C%2520a%2520line%2520of%2520works%2520based%2520on%250Aprompt%2520learning%2520leverages%2520the%2520power%2520of%2520large-scale%2520pre-trained%2520vision-language%250Amodels%2520to%2520learn%2520both%2520domain-invariant%2520and%2520specific%2520features%2520through%2520a%2520set%2520of%250Adomain-agnostic%2520and%2520domain-specific%2520learnable%2520prompts.%2520Those%2520studies%2520typically%250Aenforce%2520invariant%2520constraints%2520on%2520representation%252C%2520output%252C%2520or%2520prompt%2520space%2520to%250Alearn%2520such%2520prompts.%2520Differently%252C%2520we%2520cast%2520UDA%2520as%2520a%2520multiple-objective%250Aoptimization%2520problem%2520in%2520which%2520each%2520objective%2520is%2520represented%2520by%2520a%2520domain%2520loss.%250AUnder%2520this%2520new%2520framework%252C%2520we%2520propose%2520aligning%2520per-objective%2520gradients%2520to%2520foster%250Aconsensus%2520between%2520them.%2520Additionally%252C%2520to%2520prevent%2520potential%2520overfitting%2520when%250Afine-tuning%2520this%2520deep%2520learning%2520architecture%252C%2520we%2520penalize%2520the%2520norm%2520of%2520these%250Agradients.%2520To%2520achieve%2520these%2520goals%252C%2520we%2520devise%2520a%2520practical%2520gradient%2520update%250Aprocedure%2520that%2520can%2520work%2520under%2520both%2520single-source%2520and%2520multi-source%2520UDA.%250AEmpirically%252C%2520our%2520method%2520consistently%2520surpasses%2520other%2520prompt-based%2520baselines%2520by%250Aa%2520large%2520margin%2520on%2520different%2520UDA%2520benchmarks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Domain%20Adaptation%20through%20Prompt%20Gradient%20Alignment&entry.906535625=Hoang%20Phan%20and%20Lam%20Tran%20and%20Quyen%20Tran%20and%20Trung%20Le&entry.1292438233=%20%20Prior%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20methods%20often%20aim%20to%20train%20a%0Adomain-invariant%20feature%20extractor%2C%20which%20may%20hinder%20the%20model%20from%20learning%0Asufficiently%20discriminative%20features.%20To%20tackle%20this%2C%20a%20line%20of%20works%20based%20on%0Aprompt%20learning%20leverages%20the%20power%20of%20large-scale%20pre-trained%20vision-language%0Amodels%20to%20learn%20both%20domain-invariant%20and%20specific%20features%20through%20a%20set%20of%0Adomain-agnostic%20and%20domain-specific%20learnable%20prompts.%20Those%20studies%20typically%0Aenforce%20invariant%20constraints%20on%20representation%2C%20output%2C%20or%20prompt%20space%20to%0Alearn%20such%20prompts.%20Differently%2C%20we%20cast%20UDA%20as%20a%20multiple-objective%0Aoptimization%20problem%20in%20which%20each%20objective%20is%20represented%20by%20a%20domain%20loss.%0AUnder%20this%20new%20framework%2C%20we%20propose%20aligning%20per-objective%20gradients%20to%20foster%0Aconsensus%20between%20them.%20Additionally%2C%20to%20prevent%20potential%20overfitting%20when%0Afine-tuning%20this%20deep%20learning%20architecture%2C%20we%20penalize%20the%20norm%20of%20these%0Agradients.%20To%20achieve%20these%20goals%2C%20we%20devise%20a%20practical%20gradient%20update%0Aprocedure%20that%20can%20work%20under%20both%20single-source%20and%20multi-source%20UDA.%0AEmpirically%2C%20our%20method%20consistently%20surpasses%20other%20prompt-based%20baselines%20by%0Aa%20large%20margin%20on%20different%20UDA%20benchmarks%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09353v1&entry.124074799=Read"},
{"title": "Self-Training for Sample-Efficient Active Learning for Text\n  Classification with Pre-Trained Language Models", "author": "Christopher Schr\u00f6der and Gerhard Heyer", "abstract": "  Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. Here\nwe investigate how self-training, a semi-supervised approach where a model is\nused to obtain pseudo-labels from the unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Starting with an\nextensive reproduction of four previous self-training approaches, some of which\nare evaluated for the first time in the context of active learning or natural\nlanguage processing, we devise HAST, a new and effective self-training\nstrategy, which is evaluated on four text classification benchmarks, on which\nit outperforms the reproduced self-training approaches and reaches\nclassification results comparable to previous experiments for three out of four\ndatasets, using only 25% of the data.\n", "link": "http://arxiv.org/abs/2406.09206v1", "date": "2024-06-13", "relevancy": 2.6634, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5718}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5335}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Training%20for%20Sample-Efficient%20Active%20Learning%20for%20Text%0A%20%20Classification%20with%20Pre-Trained%20Language%20Models&body=Title%3A%20Self-Training%20for%20Sample-Efficient%20Active%20Learning%20for%20Text%0A%20%20Classification%20with%20Pre-Trained%20Language%20Models%0AAuthor%3A%20Christopher%20Schr%C3%B6der%20and%20Gerhard%20Heyer%0AAbstract%3A%20%20%20Active%20learning%20is%20an%20iterative%20labeling%20process%20that%20is%20used%20to%20obtain%20a%0Asmall%20labeled%20subset%2C%20despite%20the%20absence%20of%20labeled%20data%2C%20thereby%20enabling%20to%0Atrain%20a%20model%20for%20supervised%20tasks%20such%20as%20text%20classification.%20While%20active%0Alearning%20has%20made%20considerable%20progress%20in%20recent%20years%20due%20to%20improvements%0Aprovided%20by%20pre-trained%20language%20models%2C%20there%20is%20untapped%20potential%20in%20the%0Aoften%20neglected%20unlabeled%20portion%20of%20the%20data%2C%20although%20it%20is%20available%20in%0Aconsiderably%20larger%20quantities%20than%20the%20usually%20small%20set%20of%20labeled%20data.%20Here%0Awe%20investigate%20how%20self-training%2C%20a%20semi-supervised%20approach%20where%20a%20model%20is%0Aused%20to%20obtain%20pseudo-labels%20from%20the%20unlabeled%20data%2C%20can%20be%20used%20to%20improve%0Athe%20efficiency%20of%20active%20learning%20for%20text%20classification.%20Starting%20with%20an%0Aextensive%20reproduction%20of%20four%20previous%20self-training%20approaches%2C%20some%20of%20which%0Aare%20evaluated%20for%20the%20first%20time%20in%20the%20context%20of%20active%20learning%20or%20natural%0Alanguage%20processing%2C%20we%20devise%20HAST%2C%20a%20new%20and%20effective%20self-training%0Astrategy%2C%20which%20is%20evaluated%20on%20four%20text%20classification%20benchmarks%2C%20on%20which%0Ait%20outperforms%20the%20reproduced%20self-training%20approaches%20and%20reaches%0Aclassification%20results%20comparable%20to%20previous%20experiments%20for%20three%20out%20of%20four%0Adatasets%2C%20using%20only%2025%25%20of%20the%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Training%2520for%2520Sample-Efficient%2520Active%2520Learning%2520for%2520Text%250A%2520%2520Classification%2520with%2520Pre-Trained%2520Language%2520Models%26entry.906535625%3DChristopher%2520Schr%25C3%25B6der%2520and%2520Gerhard%2520Heyer%26entry.1292438233%3D%2520%2520Active%2520learning%2520is%2520an%2520iterative%2520labeling%2520process%2520that%2520is%2520used%2520to%2520obtain%2520a%250Asmall%2520labeled%2520subset%252C%2520despite%2520the%2520absence%2520of%2520labeled%2520data%252C%2520thereby%2520enabling%2520to%250Atrain%2520a%2520model%2520for%2520supervised%2520tasks%2520such%2520as%2520text%2520classification.%2520While%2520active%250Alearning%2520has%2520made%2520considerable%2520progress%2520in%2520recent%2520years%2520due%2520to%2520improvements%250Aprovided%2520by%2520pre-trained%2520language%2520models%252C%2520there%2520is%2520untapped%2520potential%2520in%2520the%250Aoften%2520neglected%2520unlabeled%2520portion%2520of%2520the%2520data%252C%2520although%2520it%2520is%2520available%2520in%250Aconsiderably%2520larger%2520quantities%2520than%2520the%2520usually%2520small%2520set%2520of%2520labeled%2520data.%2520Here%250Awe%2520investigate%2520how%2520self-training%252C%2520a%2520semi-supervised%2520approach%2520where%2520a%2520model%2520is%250Aused%2520to%2520obtain%2520pseudo-labels%2520from%2520the%2520unlabeled%2520data%252C%2520can%2520be%2520used%2520to%2520improve%250Athe%2520efficiency%2520of%2520active%2520learning%2520for%2520text%2520classification.%2520Starting%2520with%2520an%250Aextensive%2520reproduction%2520of%2520four%2520previous%2520self-training%2520approaches%252C%2520some%2520of%2520which%250Aare%2520evaluated%2520for%2520the%2520first%2520time%2520in%2520the%2520context%2520of%2520active%2520learning%2520or%2520natural%250Alanguage%2520processing%252C%2520we%2520devise%2520HAST%252C%2520a%2520new%2520and%2520effective%2520self-training%250Astrategy%252C%2520which%2520is%2520evaluated%2520on%2520four%2520text%2520classification%2520benchmarks%252C%2520on%2520which%250Ait%2520outperforms%2520the%2520reproduced%2520self-training%2520approaches%2520and%2520reaches%250Aclassification%2520results%2520comparable%2520to%2520previous%2520experiments%2520for%2520three%2520out%2520of%2520four%250Adatasets%252C%2520using%2520only%252025%2525%2520of%2520the%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Training%20for%20Sample-Efficient%20Active%20Learning%20for%20Text%0A%20%20Classification%20with%20Pre-Trained%20Language%20Models&entry.906535625=Christopher%20Schr%C3%B6der%20and%20Gerhard%20Heyer&entry.1292438233=%20%20Active%20learning%20is%20an%20iterative%20labeling%20process%20that%20is%20used%20to%20obtain%20a%0Asmall%20labeled%20subset%2C%20despite%20the%20absence%20of%20labeled%20data%2C%20thereby%20enabling%20to%0Atrain%20a%20model%20for%20supervised%20tasks%20such%20as%20text%20classification.%20While%20active%0Alearning%20has%20made%20considerable%20progress%20in%20recent%20years%20due%20to%20improvements%0Aprovided%20by%20pre-trained%20language%20models%2C%20there%20is%20untapped%20potential%20in%20the%0Aoften%20neglected%20unlabeled%20portion%20of%20the%20data%2C%20although%20it%20is%20available%20in%0Aconsiderably%20larger%20quantities%20than%20the%20usually%20small%20set%20of%20labeled%20data.%20Here%0Awe%20investigate%20how%20self-training%2C%20a%20semi-supervised%20approach%20where%20a%20model%20is%0Aused%20to%20obtain%20pseudo-labels%20from%20the%20unlabeled%20data%2C%20can%20be%20used%20to%20improve%0Athe%20efficiency%20of%20active%20learning%20for%20text%20classification.%20Starting%20with%20an%0Aextensive%20reproduction%20of%20four%20previous%20self-training%20approaches%2C%20some%20of%20which%0Aare%20evaluated%20for%20the%20first%20time%20in%20the%20context%20of%20active%20learning%20or%20natural%0Alanguage%20processing%2C%20we%20devise%20HAST%2C%20a%20new%20and%20effective%20self-training%0Astrategy%2C%20which%20is%20evaluated%20on%20four%20text%20classification%20benchmarks%2C%20on%20which%0Ait%20outperforms%20the%20reproduced%20self-training%20approaches%20and%20reaches%0Aclassification%20results%20comparable%20to%20previous%20experiments%20for%20three%20out%20of%20four%0Adatasets%2C%20using%20only%2025%25%20of%20the%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09206v1&entry.124074799=Read"},
{"title": "OLGA: One-cLass Graph Autoencoder", "author": "M. P. S. G\u00f4lo and J. G. B. M. Junior and D. F. Silva and R. M. Marcacini", "abstract": "  One-class learning (OCL) comprises a set of techniques applied when\nreal-world problems have a single class of interest. The usual procedure for\nOCL is learning a hypersphere that comprises instances of this class and,\nideally, repels unseen instances from any other classes. Besides, several OCL\nalgorithms for graphs have been proposed since graph representation learning\nhas succeeded in various fields. These methods may use a two-step strategy,\ninitially representing the graph and, in a second step, classifying its nodes.\nOn the other hand, end-to-end methods learn the node representations while\nclassifying the nodes in one learning process. We highlight three main gaps in\nthe literature on OCL for graphs: (i) non-customized representations for OCL;\n(ii) the lack of constraints on hypersphere parameters learning; and (iii) the\nmethods' lack of interpretability and visualization. We propose One-cLass Graph\nAutoencoder (OLGA). OLGA is end-to-end and learns the representations for the\ngraph nodes while encapsulating the interest instances by combining two loss\nfunctions. We propose a new hypersphere loss function to encapsulate the\ninterest instances. OLGA combines this new hypersphere loss with the graph\nautoencoder reconstruction loss to improve model learning. OLGA achieved\nstate-of-the-art results and outperformed six other methods with a\nstatistically significant difference from five methods. Moreover, OLGA learns\nlow-dimensional representations maintaining the classification performance with\nan interpretable model representation learning and results.\n", "link": "http://arxiv.org/abs/2406.09131v1", "date": "2024-06-13", "relevancy": 2.6361, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5835}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5242}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OLGA%3A%20One-cLass%20Graph%20Autoencoder&body=Title%3A%20OLGA%3A%20One-cLass%20Graph%20Autoencoder%0AAuthor%3A%20M.%20P.%20S.%20G%C3%B4lo%20and%20J.%20G.%20B.%20M.%20Junior%20and%20D.%20F.%20Silva%20and%20R.%20M.%20Marcacini%0AAbstract%3A%20%20%20One-class%20learning%20%28OCL%29%20comprises%20a%20set%20of%20techniques%20applied%20when%0Areal-world%20problems%20have%20a%20single%20class%20of%20interest.%20The%20usual%20procedure%20for%0AOCL%20is%20learning%20a%20hypersphere%20that%20comprises%20instances%20of%20this%20class%20and%2C%0Aideally%2C%20repels%20unseen%20instances%20from%20any%20other%20classes.%20Besides%2C%20several%20OCL%0Aalgorithms%20for%20graphs%20have%20been%20proposed%20since%20graph%20representation%20learning%0Ahas%20succeeded%20in%20various%20fields.%20These%20methods%20may%20use%20a%20two-step%20strategy%2C%0Ainitially%20representing%20the%20graph%20and%2C%20in%20a%20second%20step%2C%20classifying%20its%20nodes.%0AOn%20the%20other%20hand%2C%20end-to-end%20methods%20learn%20the%20node%20representations%20while%0Aclassifying%20the%20nodes%20in%20one%20learning%20process.%20We%20highlight%20three%20main%20gaps%20in%0Athe%20literature%20on%20OCL%20for%20graphs%3A%20%28i%29%20non-customized%20representations%20for%20OCL%3B%0A%28ii%29%20the%20lack%20of%20constraints%20on%20hypersphere%20parameters%20learning%3B%20and%20%28iii%29%20the%0Amethods%27%20lack%20of%20interpretability%20and%20visualization.%20We%20propose%20One-cLass%20Graph%0AAutoencoder%20%28OLGA%29.%20OLGA%20is%20end-to-end%20and%20learns%20the%20representations%20for%20the%0Agraph%20nodes%20while%20encapsulating%20the%20interest%20instances%20by%20combining%20two%20loss%0Afunctions.%20We%20propose%20a%20new%20hypersphere%20loss%20function%20to%20encapsulate%20the%0Ainterest%20instances.%20OLGA%20combines%20this%20new%20hypersphere%20loss%20with%20the%20graph%0Aautoencoder%20reconstruction%20loss%20to%20improve%20model%20learning.%20OLGA%20achieved%0Astate-of-the-art%20results%20and%20outperformed%20six%20other%20methods%20with%20a%0Astatistically%20significant%20difference%20from%20five%20methods.%20Moreover%2C%20OLGA%20learns%0Alow-dimensional%20representations%20maintaining%20the%20classification%20performance%20with%0Aan%20interpretable%20model%20representation%20learning%20and%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOLGA%253A%2520One-cLass%2520Graph%2520Autoencoder%26entry.906535625%3DM.%2520P.%2520S.%2520G%25C3%25B4lo%2520and%2520J.%2520G.%2520B.%2520M.%2520Junior%2520and%2520D.%2520F.%2520Silva%2520and%2520R.%2520M.%2520Marcacini%26entry.1292438233%3D%2520%2520One-class%2520learning%2520%2528OCL%2529%2520comprises%2520a%2520set%2520of%2520techniques%2520applied%2520when%250Areal-world%2520problems%2520have%2520a%2520single%2520class%2520of%2520interest.%2520The%2520usual%2520procedure%2520for%250AOCL%2520is%2520learning%2520a%2520hypersphere%2520that%2520comprises%2520instances%2520of%2520this%2520class%2520and%252C%250Aideally%252C%2520repels%2520unseen%2520instances%2520from%2520any%2520other%2520classes.%2520Besides%252C%2520several%2520OCL%250Aalgorithms%2520for%2520graphs%2520have%2520been%2520proposed%2520since%2520graph%2520representation%2520learning%250Ahas%2520succeeded%2520in%2520various%2520fields.%2520These%2520methods%2520may%2520use%2520a%2520two-step%2520strategy%252C%250Ainitially%2520representing%2520the%2520graph%2520and%252C%2520in%2520a%2520second%2520step%252C%2520classifying%2520its%2520nodes.%250AOn%2520the%2520other%2520hand%252C%2520end-to-end%2520methods%2520learn%2520the%2520node%2520representations%2520while%250Aclassifying%2520the%2520nodes%2520in%2520one%2520learning%2520process.%2520We%2520highlight%2520three%2520main%2520gaps%2520in%250Athe%2520literature%2520on%2520OCL%2520for%2520graphs%253A%2520%2528i%2529%2520non-customized%2520representations%2520for%2520OCL%253B%250A%2528ii%2529%2520the%2520lack%2520of%2520constraints%2520on%2520hypersphere%2520parameters%2520learning%253B%2520and%2520%2528iii%2529%2520the%250Amethods%2527%2520lack%2520of%2520interpretability%2520and%2520visualization.%2520We%2520propose%2520One-cLass%2520Graph%250AAutoencoder%2520%2528OLGA%2529.%2520OLGA%2520is%2520end-to-end%2520and%2520learns%2520the%2520representations%2520for%2520the%250Agraph%2520nodes%2520while%2520encapsulating%2520the%2520interest%2520instances%2520by%2520combining%2520two%2520loss%250Afunctions.%2520We%2520propose%2520a%2520new%2520hypersphere%2520loss%2520function%2520to%2520encapsulate%2520the%250Ainterest%2520instances.%2520OLGA%2520combines%2520this%2520new%2520hypersphere%2520loss%2520with%2520the%2520graph%250Aautoencoder%2520reconstruction%2520loss%2520to%2520improve%2520model%2520learning.%2520OLGA%2520achieved%250Astate-of-the-art%2520results%2520and%2520outperformed%2520six%2520other%2520methods%2520with%2520a%250Astatistically%2520significant%2520difference%2520from%2520five%2520methods.%2520Moreover%252C%2520OLGA%2520learns%250Alow-dimensional%2520representations%2520maintaining%2520the%2520classification%2520performance%2520with%250Aan%2520interpretable%2520model%2520representation%2520learning%2520and%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OLGA%3A%20One-cLass%20Graph%20Autoencoder&entry.906535625=M.%20P.%20S.%20G%C3%B4lo%20and%20J.%20G.%20B.%20M.%20Junior%20and%20D.%20F.%20Silva%20and%20R.%20M.%20Marcacini&entry.1292438233=%20%20One-class%20learning%20%28OCL%29%20comprises%20a%20set%20of%20techniques%20applied%20when%0Areal-world%20problems%20have%20a%20single%20class%20of%20interest.%20The%20usual%20procedure%20for%0AOCL%20is%20learning%20a%20hypersphere%20that%20comprises%20instances%20of%20this%20class%20and%2C%0Aideally%2C%20repels%20unseen%20instances%20from%20any%20other%20classes.%20Besides%2C%20several%20OCL%0Aalgorithms%20for%20graphs%20have%20been%20proposed%20since%20graph%20representation%20learning%0Ahas%20succeeded%20in%20various%20fields.%20These%20methods%20may%20use%20a%20two-step%20strategy%2C%0Ainitially%20representing%20the%20graph%20and%2C%20in%20a%20second%20step%2C%20classifying%20its%20nodes.%0AOn%20the%20other%20hand%2C%20end-to-end%20methods%20learn%20the%20node%20representations%20while%0Aclassifying%20the%20nodes%20in%20one%20learning%20process.%20We%20highlight%20three%20main%20gaps%20in%0Athe%20literature%20on%20OCL%20for%20graphs%3A%20%28i%29%20non-customized%20representations%20for%20OCL%3B%0A%28ii%29%20the%20lack%20of%20constraints%20on%20hypersphere%20parameters%20learning%3B%20and%20%28iii%29%20the%0Amethods%27%20lack%20of%20interpretability%20and%20visualization.%20We%20propose%20One-cLass%20Graph%0AAutoencoder%20%28OLGA%29.%20OLGA%20is%20end-to-end%20and%20learns%20the%20representations%20for%20the%0Agraph%20nodes%20while%20encapsulating%20the%20interest%20instances%20by%20combining%20two%20loss%0Afunctions.%20We%20propose%20a%20new%20hypersphere%20loss%20function%20to%20encapsulate%20the%0Ainterest%20instances.%20OLGA%20combines%20this%20new%20hypersphere%20loss%20with%20the%20graph%0Aautoencoder%20reconstruction%20loss%20to%20improve%20model%20learning.%20OLGA%20achieved%0Astate-of-the-art%20results%20and%20outperformed%20six%20other%20methods%20with%20a%0Astatistically%20significant%20difference%20from%20five%20methods.%20Moreover%2C%20OLGA%20learns%0Alow-dimensional%20representations%20maintaining%20the%20classification%20performance%20with%0Aan%20interpretable%20model%20representation%20learning%20and%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09131v1&entry.124074799=Read"},
{"title": "SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image\n  Super-Resolution", "author": "Soufiane Belharbi and Mara KM Whitford and Phuong Hoang and Shakeeb Murtaza and Luke McCaffrey and Eric Granger", "abstract": "  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes. Scanning\nconfocal microscopy allows the capture of high-quality images from 3D samples,\nyet suffers from well-known limitations such as photobleaching and\nphototoxicity of specimens caused by intense light exposure, which limits its\nuse in some applications, especially for living cells. Cellular damage can be\nalleviated by changing imaging parameters to reduce light exposure, often at\nthe expense of image quality. Machine/deep learning methods for single-image\nsuper-resolution (SISR) can be applied to restore image quality by upscaling\nlower-resolution (LR) images to produce high-resolution images (HR). These SISR\nmethods have been successfully applied to photo-realistic images due partly to\nthe abundance of publicly available data. In contrast, the lack of publicly\navailable data partly limits their application and success in scanning confocal\nmicroscopy. In this paper, we introduce a large scanning confocal microscopy\ndataset named SR-CACO-2 that is comprised of low- and high-resolution image\npairs marked for three different fluorescent markers. It allows the evaluation\nof performance of SISR methods on three different upscaling levels (X2, X4,\nX8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37),\nand it is composed of 22 tiles that have been translated in the form of 9,937\nimage patches for experiments with SISR methods. Given the new SR-CACO-2\ndataset, we also provide benchmarking results for 15 state-of-the-art methods\nthat are representative of the main SISR families. Results show that these\nmethods have limited success in producing high-resolution textures, indicating\nthat SR-CACO-2 represents a challenging problem. Our dataset, code and\npretrained weights are available: https://github.com/sbelharbi/sr-caco-2.\n", "link": "http://arxiv.org/abs/2406.09168v1", "date": "2024-06-13", "relevancy": 2.5606, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5271}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5047}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SR-CACO-2%3A%20A%20Dataset%20for%20Confocal%20Fluorescence%20Microscopy%20Image%0A%20%20Super-Resolution&body=Title%3A%20SR-CACO-2%3A%20A%20Dataset%20for%20Confocal%20Fluorescence%20Microscopy%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Soufiane%20Belharbi%20and%20Mara%20KM%20Whitford%20and%20Phuong%20Hoang%20and%20Shakeeb%20Murtaza%20and%20Luke%20McCaffrey%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Confocal%20fluorescence%20microscopy%20is%20one%20of%20the%20most%20accessible%20and%20widely%0Aused%20imaging%20techniques%20for%20the%20study%20of%20biological%20processes.%20Scanning%0Aconfocal%20microscopy%20allows%20the%20capture%20of%20high-quality%20images%20from%203D%20samples%2C%0Ayet%20suffers%20from%20well-known%20limitations%20such%20as%20photobleaching%20and%0Aphototoxicity%20of%20specimens%20caused%20by%20intense%20light%20exposure%2C%20which%20limits%20its%0Ause%20in%20some%20applications%2C%20especially%20for%20living%20cells.%20Cellular%20damage%20can%20be%0Aalleviated%20by%20changing%20imaging%20parameters%20to%20reduce%20light%20exposure%2C%20often%20at%0Athe%20expense%20of%20image%20quality.%20Machine/deep%20learning%20methods%20for%20single-image%0Asuper-resolution%20%28SISR%29%20can%20be%20applied%20to%20restore%20image%20quality%20by%20upscaling%0Alower-resolution%20%28LR%29%20images%20to%20produce%20high-resolution%20images%20%28HR%29.%20These%20SISR%0Amethods%20have%20been%20successfully%20applied%20to%20photo-realistic%20images%20due%20partly%20to%0Athe%20abundance%20of%20publicly%20available%20data.%20In%20contrast%2C%20the%20lack%20of%20publicly%0Aavailable%20data%20partly%20limits%20their%20application%20and%20success%20in%20scanning%20confocal%0Amicroscopy.%20In%20this%20paper%2C%20we%20introduce%20a%20large%20scanning%20confocal%20microscopy%0Adataset%20named%20SR-CACO-2%20that%20is%20comprised%20of%20low-%20and%20high-resolution%20image%0Apairs%20marked%20for%20three%20different%20fluorescent%20markers.%20It%20allows%20the%20evaluation%0Aof%20performance%20of%20SISR%20methods%20on%20three%20different%20upscaling%20levels%20%28X2%2C%20X4%2C%0AX8%29.%20SR-CACO-2%20contains%20the%20human%20epithelial%20cell%20line%20Caco-2%20%28ATCC%20HTB-37%29%2C%0Aand%20it%20is%20composed%20of%2022%20tiles%20that%20have%20been%20translated%20in%20the%20form%20of%209%2C937%0Aimage%20patches%20for%20experiments%20with%20SISR%20methods.%20Given%20the%20new%20SR-CACO-2%0Adataset%2C%20we%20also%20provide%20benchmarking%20results%20for%2015%20state-of-the-art%20methods%0Athat%20are%20representative%20of%20the%20main%20SISR%20families.%20Results%20show%20that%20these%0Amethods%20have%20limited%20success%20in%20producing%20high-resolution%20textures%2C%20indicating%0Athat%20SR-CACO-2%20represents%20a%20challenging%20problem.%20Our%20dataset%2C%20code%20and%0Apretrained%20weights%20are%20available%3A%20https%3A//github.com/sbelharbi/sr-caco-2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSR-CACO-2%253A%2520A%2520Dataset%2520for%2520Confocal%2520Fluorescence%2520Microscopy%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DSoufiane%2520Belharbi%2520and%2520Mara%2520KM%2520Whitford%2520and%2520Phuong%2520Hoang%2520and%2520Shakeeb%2520Murtaza%2520and%2520Luke%2520McCaffrey%2520and%2520Eric%2520Granger%26entry.1292438233%3D%2520%2520Confocal%2520fluorescence%2520microscopy%2520is%2520one%2520of%2520the%2520most%2520accessible%2520and%2520widely%250Aused%2520imaging%2520techniques%2520for%2520the%2520study%2520of%2520biological%2520processes.%2520Scanning%250Aconfocal%2520microscopy%2520allows%2520the%2520capture%2520of%2520high-quality%2520images%2520from%25203D%2520samples%252C%250Ayet%2520suffers%2520from%2520well-known%2520limitations%2520such%2520as%2520photobleaching%2520and%250Aphototoxicity%2520of%2520specimens%2520caused%2520by%2520intense%2520light%2520exposure%252C%2520which%2520limits%2520its%250Ause%2520in%2520some%2520applications%252C%2520especially%2520for%2520living%2520cells.%2520Cellular%2520damage%2520can%2520be%250Aalleviated%2520by%2520changing%2520imaging%2520parameters%2520to%2520reduce%2520light%2520exposure%252C%2520often%2520at%250Athe%2520expense%2520of%2520image%2520quality.%2520Machine/deep%2520learning%2520methods%2520for%2520single-image%250Asuper-resolution%2520%2528SISR%2529%2520can%2520be%2520applied%2520to%2520restore%2520image%2520quality%2520by%2520upscaling%250Alower-resolution%2520%2528LR%2529%2520images%2520to%2520produce%2520high-resolution%2520images%2520%2528HR%2529.%2520These%2520SISR%250Amethods%2520have%2520been%2520successfully%2520applied%2520to%2520photo-realistic%2520images%2520due%2520partly%2520to%250Athe%2520abundance%2520of%2520publicly%2520available%2520data.%2520In%2520contrast%252C%2520the%2520lack%2520of%2520publicly%250Aavailable%2520data%2520partly%2520limits%2520their%2520application%2520and%2520success%2520in%2520scanning%2520confocal%250Amicroscopy.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520large%2520scanning%2520confocal%2520microscopy%250Adataset%2520named%2520SR-CACO-2%2520that%2520is%2520comprised%2520of%2520low-%2520and%2520high-resolution%2520image%250Apairs%2520marked%2520for%2520three%2520different%2520fluorescent%2520markers.%2520It%2520allows%2520the%2520evaluation%250Aof%2520performance%2520of%2520SISR%2520methods%2520on%2520three%2520different%2520upscaling%2520levels%2520%2528X2%252C%2520X4%252C%250AX8%2529.%2520SR-CACO-2%2520contains%2520the%2520human%2520epithelial%2520cell%2520line%2520Caco-2%2520%2528ATCC%2520HTB-37%2529%252C%250Aand%2520it%2520is%2520composed%2520of%252022%2520tiles%2520that%2520have%2520been%2520translated%2520in%2520the%2520form%2520of%25209%252C937%250Aimage%2520patches%2520for%2520experiments%2520with%2520SISR%2520methods.%2520Given%2520the%2520new%2520SR-CACO-2%250Adataset%252C%2520we%2520also%2520provide%2520benchmarking%2520results%2520for%252015%2520state-of-the-art%2520methods%250Athat%2520are%2520representative%2520of%2520the%2520main%2520SISR%2520families.%2520Results%2520show%2520that%2520these%250Amethods%2520have%2520limited%2520success%2520in%2520producing%2520high-resolution%2520textures%252C%2520indicating%250Athat%2520SR-CACO-2%2520represents%2520a%2520challenging%2520problem.%2520Our%2520dataset%252C%2520code%2520and%250Apretrained%2520weights%2520are%2520available%253A%2520https%253A//github.com/sbelharbi/sr-caco-2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SR-CACO-2%3A%20A%20Dataset%20for%20Confocal%20Fluorescence%20Microscopy%20Image%0A%20%20Super-Resolution&entry.906535625=Soufiane%20Belharbi%20and%20Mara%20KM%20Whitford%20and%20Phuong%20Hoang%20and%20Shakeeb%20Murtaza%20and%20Luke%20McCaffrey%20and%20Eric%20Granger&entry.1292438233=%20%20Confocal%20fluorescence%20microscopy%20is%20one%20of%20the%20most%20accessible%20and%20widely%0Aused%20imaging%20techniques%20for%20the%20study%20of%20biological%20processes.%20Scanning%0Aconfocal%20microscopy%20allows%20the%20capture%20of%20high-quality%20images%20from%203D%20samples%2C%0Ayet%20suffers%20from%20well-known%20limitations%20such%20as%20photobleaching%20and%0Aphototoxicity%20of%20specimens%20caused%20by%20intense%20light%20exposure%2C%20which%20limits%20its%0Ause%20in%20some%20applications%2C%20especially%20for%20living%20cells.%20Cellular%20damage%20can%20be%0Aalleviated%20by%20changing%20imaging%20parameters%20to%20reduce%20light%20exposure%2C%20often%20at%0Athe%20expense%20of%20image%20quality.%20Machine/deep%20learning%20methods%20for%20single-image%0Asuper-resolution%20%28SISR%29%20can%20be%20applied%20to%20restore%20image%20quality%20by%20upscaling%0Alower-resolution%20%28LR%29%20images%20to%20produce%20high-resolution%20images%20%28HR%29.%20These%20SISR%0Amethods%20have%20been%20successfully%20applied%20to%20photo-realistic%20images%20due%20partly%20to%0Athe%20abundance%20of%20publicly%20available%20data.%20In%20contrast%2C%20the%20lack%20of%20publicly%0Aavailable%20data%20partly%20limits%20their%20application%20and%20success%20in%20scanning%20confocal%0Amicroscopy.%20In%20this%20paper%2C%20we%20introduce%20a%20large%20scanning%20confocal%20microscopy%0Adataset%20named%20SR-CACO-2%20that%20is%20comprised%20of%20low-%20and%20high-resolution%20image%0Apairs%20marked%20for%20three%20different%20fluorescent%20markers.%20It%20allows%20the%20evaluation%0Aof%20performance%20of%20SISR%20methods%20on%20three%20different%20upscaling%20levels%20%28X2%2C%20X4%2C%0AX8%29.%20SR-CACO-2%20contains%20the%20human%20epithelial%20cell%20line%20Caco-2%20%28ATCC%20HTB-37%29%2C%0Aand%20it%20is%20composed%20of%2022%20tiles%20that%20have%20been%20translated%20in%20the%20form%20of%209%2C937%0Aimage%20patches%20for%20experiments%20with%20SISR%20methods.%20Given%20the%20new%20SR-CACO-2%0Adataset%2C%20we%20also%20provide%20benchmarking%20results%20for%2015%20state-of-the-art%20methods%0Athat%20are%20representative%20of%20the%20main%20SISR%20families.%20Results%20show%20that%20these%0Amethods%20have%20limited%20success%20in%20producing%20high-resolution%20textures%2C%20indicating%0Athat%20SR-CACO-2%20represents%20a%20challenging%20problem.%20Our%20dataset%2C%20code%20and%0Apretrained%20weights%20are%20available%3A%20https%3A//github.com/sbelharbi/sr-caco-2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09168v1&entry.124074799=Read"},
{"title": "ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene\n  Editing", "author": "Jun-Kun Chen and Samuel Rota Bul\u00f2 and Norman M\u00fcller and Lorenzo Porzi and Peter Kontschieder and Yu-Xiong Wang", "abstract": "  This paper proposes ConsistDreamer - a novel framework that lifts 2D\ndiffusion models with 3D awareness and 3D consistency, thus enabling\nhigh-fidelity instruction-guided scene editing. To overcome the fundamental\nlimitation of missing 3D consistency in 2D diffusion models, our key insight is\nto introduce three synergetic strategies that augment the input of the 2D\ndiffusion model to become 3D-aware and to explicitly enforce 3D consistency\nduring the training process. Specifically, we design surrounding views as\ncontext-rich input for the 2D diffusion model, and generate 3D-consistent,\nstructured noise instead of image-independent noise. Moreover, we introduce\nself-supervised consistency-enforcing training within the per-scene editing\nprocedure. Extensive evaluation shows that our ConsistDreamer achieves\nstate-of-the-art performance for instruction-guided scene editing across\nvarious scenes and editing instructions, particularly in complicated\nlarge-scale indoor scenes from ScanNet++, with significantly improved sharpness\nand fine-grained textures. Notably, ConsistDreamer stands as the first work\ncapable of successfully editing complex (e.g., plaid/checkered) patterns. Our\nproject page is at immortalco.github.io/ConsistDreamer.\n", "link": "http://arxiv.org/abs/2406.09404v1", "date": "2024-06-13", "relevancy": 2.5458, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6407}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6407}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConsistDreamer%3A%203D-Consistent%202D%20Diffusion%20for%20High-Fidelity%20Scene%0A%20%20Editing&body=Title%3A%20ConsistDreamer%3A%203D-Consistent%202D%20Diffusion%20for%20High-Fidelity%20Scene%0A%20%20Editing%0AAuthor%3A%20Jun-Kun%20Chen%20and%20Samuel%20Rota%20Bul%C3%B2%20and%20Norman%20M%C3%BCller%20and%20Lorenzo%20Porzi%20and%20Peter%20Kontschieder%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20This%20paper%20proposes%20ConsistDreamer%20-%20a%20novel%20framework%20that%20lifts%202D%0Adiffusion%20models%20with%203D%20awareness%20and%203D%20consistency%2C%20thus%20enabling%0Ahigh-fidelity%20instruction-guided%20scene%20editing.%20To%20overcome%20the%20fundamental%0Alimitation%20of%20missing%203D%20consistency%20in%202D%20diffusion%20models%2C%20our%20key%20insight%20is%0Ato%20introduce%20three%20synergetic%20strategies%20that%20augment%20the%20input%20of%20the%202D%0Adiffusion%20model%20to%20become%203D-aware%20and%20to%20explicitly%20enforce%203D%20consistency%0Aduring%20the%20training%20process.%20Specifically%2C%20we%20design%20surrounding%20views%20as%0Acontext-rich%20input%20for%20the%202D%20diffusion%20model%2C%20and%20generate%203D-consistent%2C%0Astructured%20noise%20instead%20of%20image-independent%20noise.%20Moreover%2C%20we%20introduce%0Aself-supervised%20consistency-enforcing%20training%20within%20the%20per-scene%20editing%0Aprocedure.%20Extensive%20evaluation%20shows%20that%20our%20ConsistDreamer%20achieves%0Astate-of-the-art%20performance%20for%20instruction-guided%20scene%20editing%20across%0Avarious%20scenes%20and%20editing%20instructions%2C%20particularly%20in%20complicated%0Alarge-scale%20indoor%20scenes%20from%20ScanNet%2B%2B%2C%20with%20significantly%20improved%20sharpness%0Aand%20fine-grained%20textures.%20Notably%2C%20ConsistDreamer%20stands%20as%20the%20first%20work%0Acapable%20of%20successfully%20editing%20complex%20%28e.g.%2C%20plaid/checkered%29%20patterns.%20Our%0Aproject%20page%20is%20at%20immortalco.github.io/ConsistDreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistDreamer%253A%25203D-Consistent%25202D%2520Diffusion%2520for%2520High-Fidelity%2520Scene%250A%2520%2520Editing%26entry.906535625%3DJun-Kun%2520Chen%2520and%2520Samuel%2520Rota%2520Bul%25C3%25B2%2520and%2520Norman%2520M%25C3%25BCller%2520and%2520Lorenzo%2520Porzi%2520and%2520Peter%2520Kontschieder%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520ConsistDreamer%2520-%2520a%2520novel%2520framework%2520that%2520lifts%25202D%250Adiffusion%2520models%2520with%25203D%2520awareness%2520and%25203D%2520consistency%252C%2520thus%2520enabling%250Ahigh-fidelity%2520instruction-guided%2520scene%2520editing.%2520To%2520overcome%2520the%2520fundamental%250Alimitation%2520of%2520missing%25203D%2520consistency%2520in%25202D%2520diffusion%2520models%252C%2520our%2520key%2520insight%2520is%250Ato%2520introduce%2520three%2520synergetic%2520strategies%2520that%2520augment%2520the%2520input%2520of%2520the%25202D%250Adiffusion%2520model%2520to%2520become%25203D-aware%2520and%2520to%2520explicitly%2520enforce%25203D%2520consistency%250Aduring%2520the%2520training%2520process.%2520Specifically%252C%2520we%2520design%2520surrounding%2520views%2520as%250Acontext-rich%2520input%2520for%2520the%25202D%2520diffusion%2520model%252C%2520and%2520generate%25203D-consistent%252C%250Astructured%2520noise%2520instead%2520of%2520image-independent%2520noise.%2520Moreover%252C%2520we%2520introduce%250Aself-supervised%2520consistency-enforcing%2520training%2520within%2520the%2520per-scene%2520editing%250Aprocedure.%2520Extensive%2520evaluation%2520shows%2520that%2520our%2520ConsistDreamer%2520achieves%250Astate-of-the-art%2520performance%2520for%2520instruction-guided%2520scene%2520editing%2520across%250Avarious%2520scenes%2520and%2520editing%2520instructions%252C%2520particularly%2520in%2520complicated%250Alarge-scale%2520indoor%2520scenes%2520from%2520ScanNet%252B%252B%252C%2520with%2520significantly%2520improved%2520sharpness%250Aand%2520fine-grained%2520textures.%2520Notably%252C%2520ConsistDreamer%2520stands%2520as%2520the%2520first%2520work%250Acapable%2520of%2520successfully%2520editing%2520complex%2520%2528e.g.%252C%2520plaid/checkered%2529%2520patterns.%2520Our%250Aproject%2520page%2520is%2520at%2520immortalco.github.io/ConsistDreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConsistDreamer%3A%203D-Consistent%202D%20Diffusion%20for%20High-Fidelity%20Scene%0A%20%20Editing&entry.906535625=Jun-Kun%20Chen%20and%20Samuel%20Rota%20Bul%C3%B2%20and%20Norman%20M%C3%BCller%20and%20Lorenzo%20Porzi%20and%20Peter%20Kontschieder%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20This%20paper%20proposes%20ConsistDreamer%20-%20a%20novel%20framework%20that%20lifts%202D%0Adiffusion%20models%20with%203D%20awareness%20and%203D%20consistency%2C%20thus%20enabling%0Ahigh-fidelity%20instruction-guided%20scene%20editing.%20To%20overcome%20the%20fundamental%0Alimitation%20of%20missing%203D%20consistency%20in%202D%20diffusion%20models%2C%20our%20key%20insight%20is%0Ato%20introduce%20three%20synergetic%20strategies%20that%20augment%20the%20input%20of%20the%202D%0Adiffusion%20model%20to%20become%203D-aware%20and%20to%20explicitly%20enforce%203D%20consistency%0Aduring%20the%20training%20process.%20Specifically%2C%20we%20design%20surrounding%20views%20as%0Acontext-rich%20input%20for%20the%202D%20diffusion%20model%2C%20and%20generate%203D-consistent%2C%0Astructured%20noise%20instead%20of%20image-independent%20noise.%20Moreover%2C%20we%20introduce%0Aself-supervised%20consistency-enforcing%20training%20within%20the%20per-scene%20editing%0Aprocedure.%20Extensive%20evaluation%20shows%20that%20our%20ConsistDreamer%20achieves%0Astate-of-the-art%20performance%20for%20instruction-guided%20scene%20editing%20across%0Avarious%20scenes%20and%20editing%20instructions%2C%20particularly%20in%20complicated%0Alarge-scale%20indoor%20scenes%20from%20ScanNet%2B%2B%2C%20with%20significantly%20improved%20sharpness%0Aand%20fine-grained%20textures.%20Notably%2C%20ConsistDreamer%20stands%20as%20the%20first%20work%0Acapable%20of%20successfully%20editing%20complex%20%28e.g.%2C%20plaid/checkered%29%20patterns.%20Our%0Aproject%20page%20is%20at%20immortalco.github.io/ConsistDreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09404v1&entry.124074799=Read"},
{"title": "Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D\n  Diffusion", "author": "Linzhan Mou and Jun-Kun Chen and Yu-Xiong Wang", "abstract": "  This paper proposes Instruct 4D-to-4D that achieves 4D awareness and\nspatial-temporal consistency for 2D diffusion models to generate high-quality\ninstruction-guided dynamic scene editing results. Traditional applications of\n2D diffusion models in dynamic scene editing often result in inconsistency,\nprimarily due to their inherent frame-by-frame editing methodology. Addressing\nthe complexities of extending instruction-guided editing to 4D, our key insight\nis to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems:\nachieving temporal consistency in video editing and applying these edits to the\npseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P)\nmodel with an anchor-aware attention module for batch processing and consistent\nediting. Additionally, we integrate optical flow-guided appearance propagation\nin a sliding window fashion for more precise frame-to-frame editing and\nincorporate depth-based projection to manage the extensive data of pseudo-3D\nscenes, followed by iterative editing to achieve convergence. We extensively\nevaluate our approach in various scenes and editing instructions, and\ndemonstrate that it achieves spatially and temporally consistent editing\nresults, with significantly enhanced detail and sharpness over the prior art.\nNotably, Instruct 4D-to-4D is general and applicable to both monocular and\nchallenging multi-camera scenes. Code and more results are available at\nimmortalco.github.io/Instruct-4D-to-4D.\n", "link": "http://arxiv.org/abs/2406.09402v1", "date": "2024-06-13", "relevancy": 2.5246, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6359}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6359}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruct%204D-to-4D%3A%20Editing%204D%20Scenes%20as%20Pseudo-3D%20Scenes%20Using%202D%0A%20%20Diffusion&body=Title%3A%20Instruct%204D-to-4D%3A%20Editing%204D%20Scenes%20as%20Pseudo-3D%20Scenes%20Using%202D%0A%20%20Diffusion%0AAuthor%3A%20Linzhan%20Mou%20and%20Jun-Kun%20Chen%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20This%20paper%20proposes%20Instruct%204D-to-4D%20that%20achieves%204D%20awareness%20and%0Aspatial-temporal%20consistency%20for%202D%20diffusion%20models%20to%20generate%20high-quality%0Ainstruction-guided%20dynamic%20scene%20editing%20results.%20Traditional%20applications%20of%0A2D%20diffusion%20models%20in%20dynamic%20scene%20editing%20often%20result%20in%20inconsistency%2C%0Aprimarily%20due%20to%20their%20inherent%20frame-by-frame%20editing%20methodology.%20Addressing%0Athe%20complexities%20of%20extending%20instruction-guided%20editing%20to%204D%2C%20our%20key%20insight%0Ais%20to%20treat%20a%204D%20scene%20as%20a%20pseudo-3D%20scene%2C%20decoupled%20into%20two%20sub-problems%3A%0Aachieving%20temporal%20consistency%20in%20video%20editing%20and%20applying%20these%20edits%20to%20the%0Apseudo-3D%20scene.%20Following%20this%2C%20we%20first%20enhance%20the%20Instruct-Pix2Pix%20%28IP2P%29%0Amodel%20with%20an%20anchor-aware%20attention%20module%20for%20batch%20processing%20and%20consistent%0Aediting.%20Additionally%2C%20we%20integrate%20optical%20flow-guided%20appearance%20propagation%0Ain%20a%20sliding%20window%20fashion%20for%20more%20precise%20frame-to-frame%20editing%20and%0Aincorporate%20depth-based%20projection%20to%20manage%20the%20extensive%20data%20of%20pseudo-3D%0Ascenes%2C%20followed%20by%20iterative%20editing%20to%20achieve%20convergence.%20We%20extensively%0Aevaluate%20our%20approach%20in%20various%20scenes%20and%20editing%20instructions%2C%20and%0Ademonstrate%20that%20it%20achieves%20spatially%20and%20temporally%20consistent%20editing%0Aresults%2C%20with%20significantly%20enhanced%20detail%20and%20sharpness%20over%20the%20prior%20art.%0ANotably%2C%20Instruct%204D-to-4D%20is%20general%20and%20applicable%20to%20both%20monocular%20and%0Achallenging%20multi-camera%20scenes.%20Code%20and%20more%20results%20are%20available%20at%0Aimmortalco.github.io/Instruct-4D-to-4D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruct%25204D-to-4D%253A%2520Editing%25204D%2520Scenes%2520as%2520Pseudo-3D%2520Scenes%2520Using%25202D%250A%2520%2520Diffusion%26entry.906535625%3DLinzhan%2520Mou%2520and%2520Jun-Kun%2520Chen%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520Instruct%25204D-to-4D%2520that%2520achieves%25204D%2520awareness%2520and%250Aspatial-temporal%2520consistency%2520for%25202D%2520diffusion%2520models%2520to%2520generate%2520high-quality%250Ainstruction-guided%2520dynamic%2520scene%2520editing%2520results.%2520Traditional%2520applications%2520of%250A2D%2520diffusion%2520models%2520in%2520dynamic%2520scene%2520editing%2520often%2520result%2520in%2520inconsistency%252C%250Aprimarily%2520due%2520to%2520their%2520inherent%2520frame-by-frame%2520editing%2520methodology.%2520Addressing%250Athe%2520complexities%2520of%2520extending%2520instruction-guided%2520editing%2520to%25204D%252C%2520our%2520key%2520insight%250Ais%2520to%2520treat%2520a%25204D%2520scene%2520as%2520a%2520pseudo-3D%2520scene%252C%2520decoupled%2520into%2520two%2520sub-problems%253A%250Aachieving%2520temporal%2520consistency%2520in%2520video%2520editing%2520and%2520applying%2520these%2520edits%2520to%2520the%250Apseudo-3D%2520scene.%2520Following%2520this%252C%2520we%2520first%2520enhance%2520the%2520Instruct-Pix2Pix%2520%2528IP2P%2529%250Amodel%2520with%2520an%2520anchor-aware%2520attention%2520module%2520for%2520batch%2520processing%2520and%2520consistent%250Aediting.%2520Additionally%252C%2520we%2520integrate%2520optical%2520flow-guided%2520appearance%2520propagation%250Ain%2520a%2520sliding%2520window%2520fashion%2520for%2520more%2520precise%2520frame-to-frame%2520editing%2520and%250Aincorporate%2520depth-based%2520projection%2520to%2520manage%2520the%2520extensive%2520data%2520of%2520pseudo-3D%250Ascenes%252C%2520followed%2520by%2520iterative%2520editing%2520to%2520achieve%2520convergence.%2520We%2520extensively%250Aevaluate%2520our%2520approach%2520in%2520various%2520scenes%2520and%2520editing%2520instructions%252C%2520and%250Ademonstrate%2520that%2520it%2520achieves%2520spatially%2520and%2520temporally%2520consistent%2520editing%250Aresults%252C%2520with%2520significantly%2520enhanced%2520detail%2520and%2520sharpness%2520over%2520the%2520prior%2520art.%250ANotably%252C%2520Instruct%25204D-to-4D%2520is%2520general%2520and%2520applicable%2520to%2520both%2520monocular%2520and%250Achallenging%2520multi-camera%2520scenes.%2520Code%2520and%2520more%2520results%2520are%2520available%2520at%250Aimmortalco.github.io/Instruct-4D-to-4D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruct%204D-to-4D%3A%20Editing%204D%20Scenes%20as%20Pseudo-3D%20Scenes%20Using%202D%0A%20%20Diffusion&entry.906535625=Linzhan%20Mou%20and%20Jun-Kun%20Chen%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20This%20paper%20proposes%20Instruct%204D-to-4D%20that%20achieves%204D%20awareness%20and%0Aspatial-temporal%20consistency%20for%202D%20diffusion%20models%20to%20generate%20high-quality%0Ainstruction-guided%20dynamic%20scene%20editing%20results.%20Traditional%20applications%20of%0A2D%20diffusion%20models%20in%20dynamic%20scene%20editing%20often%20result%20in%20inconsistency%2C%0Aprimarily%20due%20to%20their%20inherent%20frame-by-frame%20editing%20methodology.%20Addressing%0Athe%20complexities%20of%20extending%20instruction-guided%20editing%20to%204D%2C%20our%20key%20insight%0Ais%20to%20treat%20a%204D%20scene%20as%20a%20pseudo-3D%20scene%2C%20decoupled%20into%20two%20sub-problems%3A%0Aachieving%20temporal%20consistency%20in%20video%20editing%20and%20applying%20these%20edits%20to%20the%0Apseudo-3D%20scene.%20Following%20this%2C%20we%20first%20enhance%20the%20Instruct-Pix2Pix%20%28IP2P%29%0Amodel%20with%20an%20anchor-aware%20attention%20module%20for%20batch%20processing%20and%20consistent%0Aediting.%20Additionally%2C%20we%20integrate%20optical%20flow-guided%20appearance%20propagation%0Ain%20a%20sliding%20window%20fashion%20for%20more%20precise%20frame-to-frame%20editing%20and%0Aincorporate%20depth-based%20projection%20to%20manage%20the%20extensive%20data%20of%20pseudo-3D%0Ascenes%2C%20followed%20by%20iterative%20editing%20to%20achieve%20convergence.%20We%20extensively%0Aevaluate%20our%20approach%20in%20various%20scenes%20and%20editing%20instructions%2C%20and%0Ademonstrate%20that%20it%20achieves%20spatially%20and%20temporally%20consistent%20editing%0Aresults%2C%20with%20significantly%20enhanced%20detail%20and%20sharpness%20over%20the%20prior%20art.%0ANotably%2C%20Instruct%204D-to-4D%20is%20general%20and%20applicable%20to%20both%20monocular%20and%0Achallenging%20multi-camera%20scenes.%20Code%20and%20more%20results%20are%20available%20at%0Aimmortalco.github.io/Instruct-4D-to-4D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09402v1&entry.124074799=Read"},
{"title": "How Much Training Data is Memorized in Overparameterized Autoencoders?\n  An Inverse Problem Perspective on Memorization Evaluation", "author": "Koren Abitbul and Yehuda Dar", "abstract": "  Overparameterized autoencoder models often memorize their training data. For\nimage data, memorization is often examined by using the trained autoencoder to\nrecover missing regions in its training images (that were used only in their\ncomplete forms in the training). In this paper, we propose an inverse problem\nperspective for the study of memorization. Given a degraded training image, we\ndefine the recovery of the original training image as an inverse problem and\nformulate it as an optimization task. In our inverse problem, we use the\ntrained autoencoder to implicitly define a regularizer for the particular\ntraining dataset that we aim to retrieve from. We develop the intricate\noptimization task into a practical method that iteratively applies the trained\nautoencoder and relatively simple computations that estimate and address the\nunknown degradation operator. We evaluate our method for blind inpainting where\nthe goal is to recover training images from degradation of many missing pixels\nin an unknown pattern. We examine various deep autoencoder architectures, such\nas fully connected and U-Net (with various nonlinearities and at diverse train\nloss values), and show that our method significantly outperforms previous\nmemorization-evaluation methods that recover training data from autoencoders.\nImportantly, our method greatly improves the recovery performance also in\nsettings that were previously considered highly challenging, and even\nimpractical, for such recovery and memorization evaluation.\n", "link": "http://arxiv.org/abs/2310.02897v2", "date": "2024-06-13", "relevancy": 2.5246, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5225}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5211}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Much%20Training%20Data%20is%20Memorized%20in%20Overparameterized%20Autoencoders%3F%0A%20%20An%20Inverse%20Problem%20Perspective%20on%20Memorization%20Evaluation&body=Title%3A%20How%20Much%20Training%20Data%20is%20Memorized%20in%20Overparameterized%20Autoencoders%3F%0A%20%20An%20Inverse%20Problem%20Perspective%20on%20Memorization%20Evaluation%0AAuthor%3A%20Koren%20Abitbul%20and%20Yehuda%20Dar%0AAbstract%3A%20%20%20Overparameterized%20autoencoder%20models%20often%20memorize%20their%20training%20data.%20For%0Aimage%20data%2C%20memorization%20is%20often%20examined%20by%20using%20the%20trained%20autoencoder%20to%0Arecover%20missing%20regions%20in%20its%20training%20images%20%28that%20were%20used%20only%20in%20their%0Acomplete%20forms%20in%20the%20training%29.%20In%20this%20paper%2C%20we%20propose%20an%20inverse%20problem%0Aperspective%20for%20the%20study%20of%20memorization.%20Given%20a%20degraded%20training%20image%2C%20we%0Adefine%20the%20recovery%20of%20the%20original%20training%20image%20as%20an%20inverse%20problem%20and%0Aformulate%20it%20as%20an%20optimization%20task.%20In%20our%20inverse%20problem%2C%20we%20use%20the%0Atrained%20autoencoder%20to%20implicitly%20define%20a%20regularizer%20for%20the%20particular%0Atraining%20dataset%20that%20we%20aim%20to%20retrieve%20from.%20We%20develop%20the%20intricate%0Aoptimization%20task%20into%20a%20practical%20method%20that%20iteratively%20applies%20the%20trained%0Aautoencoder%20and%20relatively%20simple%20computations%20that%20estimate%20and%20address%20the%0Aunknown%20degradation%20operator.%20We%20evaluate%20our%20method%20for%20blind%20inpainting%20where%0Athe%20goal%20is%20to%20recover%20training%20images%20from%20degradation%20of%20many%20missing%20pixels%0Ain%20an%20unknown%20pattern.%20We%20examine%20various%20deep%20autoencoder%20architectures%2C%20such%0Aas%20fully%20connected%20and%20U-Net%20%28with%20various%20nonlinearities%20and%20at%20diverse%20train%0Aloss%20values%29%2C%20and%20show%20that%20our%20method%20significantly%20outperforms%20previous%0Amemorization-evaluation%20methods%20that%20recover%20training%20data%20from%20autoencoders.%0AImportantly%2C%20our%20method%20greatly%20improves%20the%20recovery%20performance%20also%20in%0Asettings%20that%20were%20previously%20considered%20highly%20challenging%2C%20and%20even%0Aimpractical%2C%20for%20such%20recovery%20and%20memorization%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02897v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Much%2520Training%2520Data%2520is%2520Memorized%2520in%2520Overparameterized%2520Autoencoders%253F%250A%2520%2520An%2520Inverse%2520Problem%2520Perspective%2520on%2520Memorization%2520Evaluation%26entry.906535625%3DKoren%2520Abitbul%2520and%2520Yehuda%2520Dar%26entry.1292438233%3D%2520%2520Overparameterized%2520autoencoder%2520models%2520often%2520memorize%2520their%2520training%2520data.%2520For%250Aimage%2520data%252C%2520memorization%2520is%2520often%2520examined%2520by%2520using%2520the%2520trained%2520autoencoder%2520to%250Arecover%2520missing%2520regions%2520in%2520its%2520training%2520images%2520%2528that%2520were%2520used%2520only%2520in%2520their%250Acomplete%2520forms%2520in%2520the%2520training%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520inverse%2520problem%250Aperspective%2520for%2520the%2520study%2520of%2520memorization.%2520Given%2520a%2520degraded%2520training%2520image%252C%2520we%250Adefine%2520the%2520recovery%2520of%2520the%2520original%2520training%2520image%2520as%2520an%2520inverse%2520problem%2520and%250Aformulate%2520it%2520as%2520an%2520optimization%2520task.%2520In%2520our%2520inverse%2520problem%252C%2520we%2520use%2520the%250Atrained%2520autoencoder%2520to%2520implicitly%2520define%2520a%2520regularizer%2520for%2520the%2520particular%250Atraining%2520dataset%2520that%2520we%2520aim%2520to%2520retrieve%2520from.%2520We%2520develop%2520the%2520intricate%250Aoptimization%2520task%2520into%2520a%2520practical%2520method%2520that%2520iteratively%2520applies%2520the%2520trained%250Aautoencoder%2520and%2520relatively%2520simple%2520computations%2520that%2520estimate%2520and%2520address%2520the%250Aunknown%2520degradation%2520operator.%2520We%2520evaluate%2520our%2520method%2520for%2520blind%2520inpainting%2520where%250Athe%2520goal%2520is%2520to%2520recover%2520training%2520images%2520from%2520degradation%2520of%2520many%2520missing%2520pixels%250Ain%2520an%2520unknown%2520pattern.%2520We%2520examine%2520various%2520deep%2520autoencoder%2520architectures%252C%2520such%250Aas%2520fully%2520connected%2520and%2520U-Net%2520%2528with%2520various%2520nonlinearities%2520and%2520at%2520diverse%2520train%250Aloss%2520values%2529%252C%2520and%2520show%2520that%2520our%2520method%2520significantly%2520outperforms%2520previous%250Amemorization-evaluation%2520methods%2520that%2520recover%2520training%2520data%2520from%2520autoencoders.%250AImportantly%252C%2520our%2520method%2520greatly%2520improves%2520the%2520recovery%2520performance%2520also%2520in%250Asettings%2520that%2520were%2520previously%2520considered%2520highly%2520challenging%252C%2520and%2520even%250Aimpractical%252C%2520for%2520such%2520recovery%2520and%2520memorization%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02897v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Much%20Training%20Data%20is%20Memorized%20in%20Overparameterized%20Autoencoders%3F%0A%20%20An%20Inverse%20Problem%20Perspective%20on%20Memorization%20Evaluation&entry.906535625=Koren%20Abitbul%20and%20Yehuda%20Dar&entry.1292438233=%20%20Overparameterized%20autoencoder%20models%20often%20memorize%20their%20training%20data.%20For%0Aimage%20data%2C%20memorization%20is%20often%20examined%20by%20using%20the%20trained%20autoencoder%20to%0Arecover%20missing%20regions%20in%20its%20training%20images%20%28that%20were%20used%20only%20in%20their%0Acomplete%20forms%20in%20the%20training%29.%20In%20this%20paper%2C%20we%20propose%20an%20inverse%20problem%0Aperspective%20for%20the%20study%20of%20memorization.%20Given%20a%20degraded%20training%20image%2C%20we%0Adefine%20the%20recovery%20of%20the%20original%20training%20image%20as%20an%20inverse%20problem%20and%0Aformulate%20it%20as%20an%20optimization%20task.%20In%20our%20inverse%20problem%2C%20we%20use%20the%0Atrained%20autoencoder%20to%20implicitly%20define%20a%20regularizer%20for%20the%20particular%0Atraining%20dataset%20that%20we%20aim%20to%20retrieve%20from.%20We%20develop%20the%20intricate%0Aoptimization%20task%20into%20a%20practical%20method%20that%20iteratively%20applies%20the%20trained%0Aautoencoder%20and%20relatively%20simple%20computations%20that%20estimate%20and%20address%20the%0Aunknown%20degradation%20operator.%20We%20evaluate%20our%20method%20for%20blind%20inpainting%20where%0Athe%20goal%20is%20to%20recover%20training%20images%20from%20degradation%20of%20many%20missing%20pixels%0Ain%20an%20unknown%20pattern.%20We%20examine%20various%20deep%20autoencoder%20architectures%2C%20such%0Aas%20fully%20connected%20and%20U-Net%20%28with%20various%20nonlinearities%20and%20at%20diverse%20train%0Aloss%20values%29%2C%20and%20show%20that%20our%20method%20significantly%20outperforms%20previous%0Amemorization-evaluation%20methods%20that%20recover%20training%20data%20from%20autoencoders.%0AImportantly%2C%20our%20method%20greatly%20improves%20the%20recovery%20performance%20also%20in%0Asettings%20that%20were%20previously%20considered%20highly%20challenging%2C%20and%20even%0Aimpractical%2C%20for%20such%20recovery%20and%20memorization%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02897v2&entry.124074799=Read"},
{"title": "Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image\n  Diffusion Models", "author": "Ziyi Wu and Yulia Rubanova and Rishabh Kabra and Drew A. Hudson and Igor Gilitschenski and Yusuf Aytar and Sjoerd van Steenkiste and Kelsey R. Allen and Thomas Kipf", "abstract": "  We address the problem of multi-object 3D pose control in image diffusion\nmodels. Instead of conditioning on a sequence of text tokens, we propose to use\na set of per-object representations, Neural Assets, to control the 3D pose of\nindividual objects in a scene. Neural Assets are obtained by pooling visual\nrepresentations of objects from a reference image, such as a frame in a video,\nand are trained to reconstruct the respective objects in a different image,\ne.g., a later frame in the video. Importantly, we encode object visuals from\nthe reference image while conditioning on object poses from the target frame.\nThis enables learning disentangled appearance and pose features. Combining\nvisual and 3D pose representations in a sequence-of-tokens format allows us to\nkeep the text-to-image architecture of existing models, with Neural Assets in\nplace of text tokens. By fine-tuning a pre-trained text-to-image diffusion\nmodel with this information, our approach enables fine-grained 3D pose and\nplacement control of individual objects in a scene. We further demonstrate that\nNeural Assets can be transferred and recomposed across different scenes. Our\nmodel achieves state-of-the-art multi-object editing results on both synthetic\n3D scene datasets, as well as two real-world video datasets (Objectron, Waymo\nOpen).\n", "link": "http://arxiv.org/abs/2406.09292v1", "date": "2024-06-13", "relevancy": 2.4904, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.625}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.625}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Assets%3A%203D-Aware%20Multi-Object%20Scene%20Synthesis%20with%20Image%0A%20%20Diffusion%20Models&body=Title%3A%20Neural%20Assets%3A%203D-Aware%20Multi-Object%20Scene%20Synthesis%20with%20Image%0A%20%20Diffusion%20Models%0AAuthor%3A%20Ziyi%20Wu%20and%20Yulia%20Rubanova%20and%20Rishabh%20Kabra%20and%20Drew%20A.%20Hudson%20and%20Igor%20Gilitschenski%20and%20Yusuf%20Aytar%20and%20Sjoerd%20van%20Steenkiste%20and%20Kelsey%20R.%20Allen%20and%20Thomas%20Kipf%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20multi-object%203D%20pose%20control%20in%20image%20diffusion%0Amodels.%20Instead%20of%20conditioning%20on%20a%20sequence%20of%20text%20tokens%2C%20we%20propose%20to%20use%0Aa%20set%20of%20per-object%20representations%2C%20Neural%20Assets%2C%20to%20control%20the%203D%20pose%20of%0Aindividual%20objects%20in%20a%20scene.%20Neural%20Assets%20are%20obtained%20by%20pooling%20visual%0Arepresentations%20of%20objects%20from%20a%20reference%20image%2C%20such%20as%20a%20frame%20in%20a%20video%2C%0Aand%20are%20trained%20to%20reconstruct%20the%20respective%20objects%20in%20a%20different%20image%2C%0Ae.g.%2C%20a%20later%20frame%20in%20the%20video.%20Importantly%2C%20we%20encode%20object%20visuals%20from%0Athe%20reference%20image%20while%20conditioning%20on%20object%20poses%20from%20the%20target%20frame.%0AThis%20enables%20learning%20disentangled%20appearance%20and%20pose%20features.%20Combining%0Avisual%20and%203D%20pose%20representations%20in%20a%20sequence-of-tokens%20format%20allows%20us%20to%0Akeep%20the%20text-to-image%20architecture%20of%20existing%20models%2C%20with%20Neural%20Assets%20in%0Aplace%20of%20text%20tokens.%20By%20fine-tuning%20a%20pre-trained%20text-to-image%20diffusion%0Amodel%20with%20this%20information%2C%20our%20approach%20enables%20fine-grained%203D%20pose%20and%0Aplacement%20control%20of%20individual%20objects%20in%20a%20scene.%20We%20further%20demonstrate%20that%0ANeural%20Assets%20can%20be%20transferred%20and%20recomposed%20across%20different%20scenes.%20Our%0Amodel%20achieves%20state-of-the-art%20multi-object%20editing%20results%20on%20both%20synthetic%0A3D%20scene%20datasets%2C%20as%20well%20as%20two%20real-world%20video%20datasets%20%28Objectron%2C%20Waymo%0AOpen%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Assets%253A%25203D-Aware%2520Multi-Object%2520Scene%2520Synthesis%2520with%2520Image%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DZiyi%2520Wu%2520and%2520Yulia%2520Rubanova%2520and%2520Rishabh%2520Kabra%2520and%2520Drew%2520A.%2520Hudson%2520and%2520Igor%2520Gilitschenski%2520and%2520Yusuf%2520Aytar%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Kelsey%2520R.%2520Allen%2520and%2520Thomas%2520Kipf%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520multi-object%25203D%2520pose%2520control%2520in%2520image%2520diffusion%250Amodels.%2520Instead%2520of%2520conditioning%2520on%2520a%2520sequence%2520of%2520text%2520tokens%252C%2520we%2520propose%2520to%2520use%250Aa%2520set%2520of%2520per-object%2520representations%252C%2520Neural%2520Assets%252C%2520to%2520control%2520the%25203D%2520pose%2520of%250Aindividual%2520objects%2520in%2520a%2520scene.%2520Neural%2520Assets%2520are%2520obtained%2520by%2520pooling%2520visual%250Arepresentations%2520of%2520objects%2520from%2520a%2520reference%2520image%252C%2520such%2520as%2520a%2520frame%2520in%2520a%2520video%252C%250Aand%2520are%2520trained%2520to%2520reconstruct%2520the%2520respective%2520objects%2520in%2520a%2520different%2520image%252C%250Ae.g.%252C%2520a%2520later%2520frame%2520in%2520the%2520video.%2520Importantly%252C%2520we%2520encode%2520object%2520visuals%2520from%250Athe%2520reference%2520image%2520while%2520conditioning%2520on%2520object%2520poses%2520from%2520the%2520target%2520frame.%250AThis%2520enables%2520learning%2520disentangled%2520appearance%2520and%2520pose%2520features.%2520Combining%250Avisual%2520and%25203D%2520pose%2520representations%2520in%2520a%2520sequence-of-tokens%2520format%2520allows%2520us%2520to%250Akeep%2520the%2520text-to-image%2520architecture%2520of%2520existing%2520models%252C%2520with%2520Neural%2520Assets%2520in%250Aplace%2520of%2520text%2520tokens.%2520By%2520fine-tuning%2520a%2520pre-trained%2520text-to-image%2520diffusion%250Amodel%2520with%2520this%2520information%252C%2520our%2520approach%2520enables%2520fine-grained%25203D%2520pose%2520and%250Aplacement%2520control%2520of%2520individual%2520objects%2520in%2520a%2520scene.%2520We%2520further%2520demonstrate%2520that%250ANeural%2520Assets%2520can%2520be%2520transferred%2520and%2520recomposed%2520across%2520different%2520scenes.%2520Our%250Amodel%2520achieves%2520state-of-the-art%2520multi-object%2520editing%2520results%2520on%2520both%2520synthetic%250A3D%2520scene%2520datasets%252C%2520as%2520well%2520as%2520two%2520real-world%2520video%2520datasets%2520%2528Objectron%252C%2520Waymo%250AOpen%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Assets%3A%203D-Aware%20Multi-Object%20Scene%20Synthesis%20with%20Image%0A%20%20Diffusion%20Models&entry.906535625=Ziyi%20Wu%20and%20Yulia%20Rubanova%20and%20Rishabh%20Kabra%20and%20Drew%20A.%20Hudson%20and%20Igor%20Gilitschenski%20and%20Yusuf%20Aytar%20and%20Sjoerd%20van%20Steenkiste%20and%20Kelsey%20R.%20Allen%20and%20Thomas%20Kipf&entry.1292438233=%20%20We%20address%20the%20problem%20of%20multi-object%203D%20pose%20control%20in%20image%20diffusion%0Amodels.%20Instead%20of%20conditioning%20on%20a%20sequence%20of%20text%20tokens%2C%20we%20propose%20to%20use%0Aa%20set%20of%20per-object%20representations%2C%20Neural%20Assets%2C%20to%20control%20the%203D%20pose%20of%0Aindividual%20objects%20in%20a%20scene.%20Neural%20Assets%20are%20obtained%20by%20pooling%20visual%0Arepresentations%20of%20objects%20from%20a%20reference%20image%2C%20such%20as%20a%20frame%20in%20a%20video%2C%0Aand%20are%20trained%20to%20reconstruct%20the%20respective%20objects%20in%20a%20different%20image%2C%0Ae.g.%2C%20a%20later%20frame%20in%20the%20video.%20Importantly%2C%20we%20encode%20object%20visuals%20from%0Athe%20reference%20image%20while%20conditioning%20on%20object%20poses%20from%20the%20target%20frame.%0AThis%20enables%20learning%20disentangled%20appearance%20and%20pose%20features.%20Combining%0Avisual%20and%203D%20pose%20representations%20in%20a%20sequence-of-tokens%20format%20allows%20us%20to%0Akeep%20the%20text-to-image%20architecture%20of%20existing%20models%2C%20with%20Neural%20Assets%20in%0Aplace%20of%20text%20tokens.%20By%20fine-tuning%20a%20pre-trained%20text-to-image%20diffusion%0Amodel%20with%20this%20information%2C%20our%20approach%20enables%20fine-grained%203D%20pose%20and%0Aplacement%20control%20of%20individual%20objects%20in%20a%20scene.%20We%20further%20demonstrate%20that%0ANeural%20Assets%20can%20be%20transferred%20and%20recomposed%20across%20different%20scenes.%20Our%0Amodel%20achieves%20state-of-the-art%20multi-object%20editing%20results%20on%20both%20synthetic%0A3D%20scene%20datasets%2C%20as%20well%20as%20two%20real-world%20video%20datasets%20%28Objectron%2C%20Waymo%0AOpen%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09292v1&entry.124074799=Read"},
{"title": "DiffPoGAN: Diffusion Policies with Generative Adversarial Networks for\n  Offline Reinforcement Learning", "author": "Xuemin Hu and Shen Li and Yingfen Xu and Bo Tang and Long Chen", "abstract": "  Offline reinforcement learning (RL) can learn optimal policies from\npre-collected offline datasets without interacting with the environment, but\nthe sampled actions of the agent cannot often cover the action distribution\nunder a given state, resulting in the extrapolation error issue. Recent works\naddress this issue by employing generative adversarial networks (GANs).\nHowever, these methods often suffer from insufficient constraints on policy\nexploration and inaccurate representation of behavior policies. Moreover, the\ngenerator in GANs fails in fooling the discriminator while maximizing the\nexpected returns of a policy. Inspired by the diffusion, a generative model\nwith powerful feature expressiveness, we propose a new offline RL method named\nDiffusion Policies with Generative Adversarial Networks (DiffPoGAN). In this\napproach, the diffusion serves as the policy generator to generate diverse\ndistributions of actions, and a regularization method based on maximum\nlikelihood estimation (MLE) is developed to generate data that approximate the\ndistribution of behavior policies. Besides, we introduce an additional\nregularization term based on the discriminator output to effectively constrain\npolicy exploration for policy improvement. Comprehensive experiments are\nconducted on the datasets for deep data-driven reinforcement learning (D4RL),\nand experimental results show that DiffPoGAN outperforms state-of-the-art\nmethods in offline RL.\n", "link": "http://arxiv.org/abs/2406.09089v1", "date": "2024-06-13", "relevancy": 2.4737, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.498}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4958}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffPoGAN%3A%20Diffusion%20Policies%20with%20Generative%20Adversarial%20Networks%20for%0A%20%20Offline%20Reinforcement%20Learning&body=Title%3A%20DiffPoGAN%3A%20Diffusion%20Policies%20with%20Generative%20Adversarial%20Networks%20for%0A%20%20Offline%20Reinforcement%20Learning%0AAuthor%3A%20Xuemin%20Hu%20and%20Shen%20Li%20and%20Yingfen%20Xu%20and%20Bo%20Tang%20and%20Long%20Chen%0AAbstract%3A%20%20%20Offline%20reinforcement%20learning%20%28RL%29%20can%20learn%20optimal%20policies%20from%0Apre-collected%20offline%20datasets%20without%20interacting%20with%20the%20environment%2C%20but%0Athe%20sampled%20actions%20of%20the%20agent%20cannot%20often%20cover%20the%20action%20distribution%0Aunder%20a%20given%20state%2C%20resulting%20in%20the%20extrapolation%20error%20issue.%20Recent%20works%0Aaddress%20this%20issue%20by%20employing%20generative%20adversarial%20networks%20%28GANs%29.%0AHowever%2C%20these%20methods%20often%20suffer%20from%20insufficient%20constraints%20on%20policy%0Aexploration%20and%20inaccurate%20representation%20of%20behavior%20policies.%20Moreover%2C%20the%0Agenerator%20in%20GANs%20fails%20in%20fooling%20the%20discriminator%20while%20maximizing%20the%0Aexpected%20returns%20of%20a%20policy.%20Inspired%20by%20the%20diffusion%2C%20a%20generative%20model%0Awith%20powerful%20feature%20expressiveness%2C%20we%20propose%20a%20new%20offline%20RL%20method%20named%0ADiffusion%20Policies%20with%20Generative%20Adversarial%20Networks%20%28DiffPoGAN%29.%20In%20this%0Aapproach%2C%20the%20diffusion%20serves%20as%20the%20policy%20generator%20to%20generate%20diverse%0Adistributions%20of%20actions%2C%20and%20a%20regularization%20method%20based%20on%20maximum%0Alikelihood%20estimation%20%28MLE%29%20is%20developed%20to%20generate%20data%20that%20approximate%20the%0Adistribution%20of%20behavior%20policies.%20Besides%2C%20we%20introduce%20an%20additional%0Aregularization%20term%20based%20on%20the%20discriminator%20output%20to%20effectively%20constrain%0Apolicy%20exploration%20for%20policy%20improvement.%20Comprehensive%20experiments%20are%0Aconducted%20on%20the%20datasets%20for%20deep%20data-driven%20reinforcement%20learning%20%28D4RL%29%2C%0Aand%20experimental%20results%20show%20that%20DiffPoGAN%20outperforms%20state-of-the-art%0Amethods%20in%20offline%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffPoGAN%253A%2520Diffusion%2520Policies%2520with%2520Generative%2520Adversarial%2520Networks%2520for%250A%2520%2520Offline%2520Reinforcement%2520Learning%26entry.906535625%3DXuemin%2520Hu%2520and%2520Shen%2520Li%2520and%2520Yingfen%2520Xu%2520and%2520Bo%2520Tang%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Offline%2520reinforcement%2520learning%2520%2528RL%2529%2520can%2520learn%2520optimal%2520policies%2520from%250Apre-collected%2520offline%2520datasets%2520without%2520interacting%2520with%2520the%2520environment%252C%2520but%250Athe%2520sampled%2520actions%2520of%2520the%2520agent%2520cannot%2520often%2520cover%2520the%2520action%2520distribution%250Aunder%2520a%2520given%2520state%252C%2520resulting%2520in%2520the%2520extrapolation%2520error%2520issue.%2520Recent%2520works%250Aaddress%2520this%2520issue%2520by%2520employing%2520generative%2520adversarial%2520networks%2520%2528GANs%2529.%250AHowever%252C%2520these%2520methods%2520often%2520suffer%2520from%2520insufficient%2520constraints%2520on%2520policy%250Aexploration%2520and%2520inaccurate%2520representation%2520of%2520behavior%2520policies.%2520Moreover%252C%2520the%250Agenerator%2520in%2520GANs%2520fails%2520in%2520fooling%2520the%2520discriminator%2520while%2520maximizing%2520the%250Aexpected%2520returns%2520of%2520a%2520policy.%2520Inspired%2520by%2520the%2520diffusion%252C%2520a%2520generative%2520model%250Awith%2520powerful%2520feature%2520expressiveness%252C%2520we%2520propose%2520a%2520new%2520offline%2520RL%2520method%2520named%250ADiffusion%2520Policies%2520with%2520Generative%2520Adversarial%2520Networks%2520%2528DiffPoGAN%2529.%2520In%2520this%250Aapproach%252C%2520the%2520diffusion%2520serves%2520as%2520the%2520policy%2520generator%2520to%2520generate%2520diverse%250Adistributions%2520of%2520actions%252C%2520and%2520a%2520regularization%2520method%2520based%2520on%2520maximum%250Alikelihood%2520estimation%2520%2528MLE%2529%2520is%2520developed%2520to%2520generate%2520data%2520that%2520approximate%2520the%250Adistribution%2520of%2520behavior%2520policies.%2520Besides%252C%2520we%2520introduce%2520an%2520additional%250Aregularization%2520term%2520based%2520on%2520the%2520discriminator%2520output%2520to%2520effectively%2520constrain%250Apolicy%2520exploration%2520for%2520policy%2520improvement.%2520Comprehensive%2520experiments%2520are%250Aconducted%2520on%2520the%2520datasets%2520for%2520deep%2520data-driven%2520reinforcement%2520learning%2520%2528D4RL%2529%252C%250Aand%2520experimental%2520results%2520show%2520that%2520DiffPoGAN%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520offline%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffPoGAN%3A%20Diffusion%20Policies%20with%20Generative%20Adversarial%20Networks%20for%0A%20%20Offline%20Reinforcement%20Learning&entry.906535625=Xuemin%20Hu%20and%20Shen%20Li%20and%20Yingfen%20Xu%20and%20Bo%20Tang%20and%20Long%20Chen&entry.1292438233=%20%20Offline%20reinforcement%20learning%20%28RL%29%20can%20learn%20optimal%20policies%20from%0Apre-collected%20offline%20datasets%20without%20interacting%20with%20the%20environment%2C%20but%0Athe%20sampled%20actions%20of%20the%20agent%20cannot%20often%20cover%20the%20action%20distribution%0Aunder%20a%20given%20state%2C%20resulting%20in%20the%20extrapolation%20error%20issue.%20Recent%20works%0Aaddress%20this%20issue%20by%20employing%20generative%20adversarial%20networks%20%28GANs%29.%0AHowever%2C%20these%20methods%20often%20suffer%20from%20insufficient%20constraints%20on%20policy%0Aexploration%20and%20inaccurate%20representation%20of%20behavior%20policies.%20Moreover%2C%20the%0Agenerator%20in%20GANs%20fails%20in%20fooling%20the%20discriminator%20while%20maximizing%20the%0Aexpected%20returns%20of%20a%20policy.%20Inspired%20by%20the%20diffusion%2C%20a%20generative%20model%0Awith%20powerful%20feature%20expressiveness%2C%20we%20propose%20a%20new%20offline%20RL%20method%20named%0ADiffusion%20Policies%20with%20Generative%20Adversarial%20Networks%20%28DiffPoGAN%29.%20In%20this%0Aapproach%2C%20the%20diffusion%20serves%20as%20the%20policy%20generator%20to%20generate%20diverse%0Adistributions%20of%20actions%2C%20and%20a%20regularization%20method%20based%20on%20maximum%0Alikelihood%20estimation%20%28MLE%29%20is%20developed%20to%20generate%20data%20that%20approximate%20the%0Adistribution%20of%20behavior%20policies.%20Besides%2C%20we%20introduce%20an%20additional%0Aregularization%20term%20based%20on%20the%20discriminator%20output%20to%20effectively%20constrain%0Apolicy%20exploration%20for%20policy%20improvement.%20Comprehensive%20experiments%20are%0Aconducted%20on%20the%20datasets%20for%20deep%20data-driven%20reinforcement%20learning%20%28D4RL%29%2C%0Aand%20experimental%20results%20show%20that%20DiffPoGAN%20outperforms%20state-of-the-art%0Amethods%20in%20offline%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09089v1&entry.124074799=Read"},
{"title": "FacEnhance: Facial Expression Enhancing with Recurrent DDPMs", "author": "Hamza Bouzid and Lahoucine Ballihi", "abstract": "  Facial expressions, vital in non-verbal human communication, have found\napplications in various computer vision fields like virtual reality, gaming,\nand emotional AI assistants. Despite advancements, many facial expression\ngeneration models encounter challenges such as low resolution (e.g., 32x32 or\n64x64 pixels), poor quality, and the absence of background details. In this\npaper, we introduce FacEnhance, a novel diffusion-based approach addressing\nconstraints in existing low-resolution facial expression generation models.\nFacEnhance enhances low-resolution facial expression videos (64x64 pixels) to\nhigher resolutions (192x192 pixels), incorporating background details and\nimproving overall quality. Leveraging conditional denoising within a diffusion\nframework, guided by a background-free low-resolution video and a single\nneutral expression high-resolution image, FacEnhance generates a video\nincorporating the facial expression from the low-resolution video performed by\nthe individual with background from the neutral image. By complementing\nlightweight low-resolution models, FacEnhance strikes a balance between\ncomputational efficiency and desirable image resolution and quality. Extensive\nexperiments on the MUG facial expression database demonstrate the efficacy of\nFacEnhance in enhancing low-resolution model outputs to state-of-the-art\nquality while preserving content and identity consistency. FacEnhance\nrepresents significant progress towards resource-efficient, high-fidelity\nfacial expression generation, Renewing outdated low-resolution methods to\nup-to-date standards.\n", "link": "http://arxiv.org/abs/2406.09040v1", "date": "2024-06-13", "relevancy": 2.4697, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6241}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6132}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FacEnhance%3A%20Facial%20Expression%20Enhancing%20with%20Recurrent%20DDPMs&body=Title%3A%20FacEnhance%3A%20Facial%20Expression%20Enhancing%20with%20Recurrent%20DDPMs%0AAuthor%3A%20Hamza%20Bouzid%20and%20Lahoucine%20Ballihi%0AAbstract%3A%20%20%20Facial%20expressions%2C%20vital%20in%20non-verbal%20human%20communication%2C%20have%20found%0Aapplications%20in%20various%20computer%20vision%20fields%20like%20virtual%20reality%2C%20gaming%2C%0Aand%20emotional%20AI%20assistants.%20Despite%20advancements%2C%20many%20facial%20expression%0Ageneration%20models%20encounter%20challenges%20such%20as%20low%20resolution%20%28e.g.%2C%2032x32%20or%0A64x64%20pixels%29%2C%20poor%20quality%2C%20and%20the%20absence%20of%20background%20details.%20In%20this%0Apaper%2C%20we%20introduce%20FacEnhance%2C%20a%20novel%20diffusion-based%20approach%20addressing%0Aconstraints%20in%20existing%20low-resolution%20facial%20expression%20generation%20models.%0AFacEnhance%20enhances%20low-resolution%20facial%20expression%20videos%20%2864x64%20pixels%29%20to%0Ahigher%20resolutions%20%28192x192%20pixels%29%2C%20incorporating%20background%20details%20and%0Aimproving%20overall%20quality.%20Leveraging%20conditional%20denoising%20within%20a%20diffusion%0Aframework%2C%20guided%20by%20a%20background-free%20low-resolution%20video%20and%20a%20single%0Aneutral%20expression%20high-resolution%20image%2C%20FacEnhance%20generates%20a%20video%0Aincorporating%20the%20facial%20expression%20from%20the%20low-resolution%20video%20performed%20by%0Athe%20individual%20with%20background%20from%20the%20neutral%20image.%20By%20complementing%0Alightweight%20low-resolution%20models%2C%20FacEnhance%20strikes%20a%20balance%20between%0Acomputational%20efficiency%20and%20desirable%20image%20resolution%20and%20quality.%20Extensive%0Aexperiments%20on%20the%20MUG%20facial%20expression%20database%20demonstrate%20the%20efficacy%20of%0AFacEnhance%20in%20enhancing%20low-resolution%20model%20outputs%20to%20state-of-the-art%0Aquality%20while%20preserving%20content%20and%20identity%20consistency.%20FacEnhance%0Arepresents%20significant%20progress%20towards%20resource-efficient%2C%20high-fidelity%0Afacial%20expression%20generation%2C%20Renewing%20outdated%20low-resolution%20methods%20to%0Aup-to-date%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacEnhance%253A%2520Facial%2520Expression%2520Enhancing%2520with%2520Recurrent%2520DDPMs%26entry.906535625%3DHamza%2520Bouzid%2520and%2520Lahoucine%2520Ballihi%26entry.1292438233%3D%2520%2520Facial%2520expressions%252C%2520vital%2520in%2520non-verbal%2520human%2520communication%252C%2520have%2520found%250Aapplications%2520in%2520various%2520computer%2520vision%2520fields%2520like%2520virtual%2520reality%252C%2520gaming%252C%250Aand%2520emotional%2520AI%2520assistants.%2520Despite%2520advancements%252C%2520many%2520facial%2520expression%250Ageneration%2520models%2520encounter%2520challenges%2520such%2520as%2520low%2520resolution%2520%2528e.g.%252C%252032x32%2520or%250A64x64%2520pixels%2529%252C%2520poor%2520quality%252C%2520and%2520the%2520absence%2520of%2520background%2520details.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520FacEnhance%252C%2520a%2520novel%2520diffusion-based%2520approach%2520addressing%250Aconstraints%2520in%2520existing%2520low-resolution%2520facial%2520expression%2520generation%2520models.%250AFacEnhance%2520enhances%2520low-resolution%2520facial%2520expression%2520videos%2520%252864x64%2520pixels%2529%2520to%250Ahigher%2520resolutions%2520%2528192x192%2520pixels%2529%252C%2520incorporating%2520background%2520details%2520and%250Aimproving%2520overall%2520quality.%2520Leveraging%2520conditional%2520denoising%2520within%2520a%2520diffusion%250Aframework%252C%2520guided%2520by%2520a%2520background-free%2520low-resolution%2520video%2520and%2520a%2520single%250Aneutral%2520expression%2520high-resolution%2520image%252C%2520FacEnhance%2520generates%2520a%2520video%250Aincorporating%2520the%2520facial%2520expression%2520from%2520the%2520low-resolution%2520video%2520performed%2520by%250Athe%2520individual%2520with%2520background%2520from%2520the%2520neutral%2520image.%2520By%2520complementing%250Alightweight%2520low-resolution%2520models%252C%2520FacEnhance%2520strikes%2520a%2520balance%2520between%250Acomputational%2520efficiency%2520and%2520desirable%2520image%2520resolution%2520and%2520quality.%2520Extensive%250Aexperiments%2520on%2520the%2520MUG%2520facial%2520expression%2520database%2520demonstrate%2520the%2520efficacy%2520of%250AFacEnhance%2520in%2520enhancing%2520low-resolution%2520model%2520outputs%2520to%2520state-of-the-art%250Aquality%2520while%2520preserving%2520content%2520and%2520identity%2520consistency.%2520FacEnhance%250Arepresents%2520significant%2520progress%2520towards%2520resource-efficient%252C%2520high-fidelity%250Afacial%2520expression%2520generation%252C%2520Renewing%2520outdated%2520low-resolution%2520methods%2520to%250Aup-to-date%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FacEnhance%3A%20Facial%20Expression%20Enhancing%20with%20Recurrent%20DDPMs&entry.906535625=Hamza%20Bouzid%20and%20Lahoucine%20Ballihi&entry.1292438233=%20%20Facial%20expressions%2C%20vital%20in%20non-verbal%20human%20communication%2C%20have%20found%0Aapplications%20in%20various%20computer%20vision%20fields%20like%20virtual%20reality%2C%20gaming%2C%0Aand%20emotional%20AI%20assistants.%20Despite%20advancements%2C%20many%20facial%20expression%0Ageneration%20models%20encounter%20challenges%20such%20as%20low%20resolution%20%28e.g.%2C%2032x32%20or%0A64x64%20pixels%29%2C%20poor%20quality%2C%20and%20the%20absence%20of%20background%20details.%20In%20this%0Apaper%2C%20we%20introduce%20FacEnhance%2C%20a%20novel%20diffusion-based%20approach%20addressing%0Aconstraints%20in%20existing%20low-resolution%20facial%20expression%20generation%20models.%0AFacEnhance%20enhances%20low-resolution%20facial%20expression%20videos%20%2864x64%20pixels%29%20to%0Ahigher%20resolutions%20%28192x192%20pixels%29%2C%20incorporating%20background%20details%20and%0Aimproving%20overall%20quality.%20Leveraging%20conditional%20denoising%20within%20a%20diffusion%0Aframework%2C%20guided%20by%20a%20background-free%20low-resolution%20video%20and%20a%20single%0Aneutral%20expression%20high-resolution%20image%2C%20FacEnhance%20generates%20a%20video%0Aincorporating%20the%20facial%20expression%20from%20the%20low-resolution%20video%20performed%20by%0Athe%20individual%20with%20background%20from%20the%20neutral%20image.%20By%20complementing%0Alightweight%20low-resolution%20models%2C%20FacEnhance%20strikes%20a%20balance%20between%0Acomputational%20efficiency%20and%20desirable%20image%20resolution%20and%20quality.%20Extensive%0Aexperiments%20on%20the%20MUG%20facial%20expression%20database%20demonstrate%20the%20efficacy%20of%0AFacEnhance%20in%20enhancing%20low-resolution%20model%20outputs%20to%20state-of-the-art%0Aquality%20while%20preserving%20content%20and%20identity%20consistency.%20FacEnhance%0Arepresents%20significant%20progress%20towards%20resource-efficient%2C%20high-fidelity%0Afacial%20expression%20generation%2C%20Renewing%20outdated%20low-resolution%20methods%20to%0Aup-to-date%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09040v1&entry.124074799=Read"},
{"title": "A PCA based Keypoint Tracking Approach to Automated Facial Expressions\n  Encoding", "author": "Shivansh Chandra Tripathi and Rahul Garg", "abstract": "  The Facial Action Coding System (FACS) for studying facial expressions is\nmanual and requires significant effort and expertise. This paper explores the\nuse of automated techniques to generate Action Units (AUs) for studying facial\nexpressions. We propose an unsupervised approach based on Principal Component\nAnalysis (PCA) and facial keypoint tracking to generate data-driven AUs called\nPCA AUs using the publicly available DISFA dataset. The PCA AUs comply with the\ndirection of facial muscle movements and are capable of explaining over 92.83\npercent of the variance in other public test datasets (BP4D-Spontaneous and\nCK+), indicating their capability to generalize facial expressions. The PCA AUs\nare also comparable to a keypoint-based equivalence of FACS AUs in terms of\nvariance explained on the test datasets. In conclusion, our research\ndemonstrates the potential of automated techniques to be an alternative to\nmanual FACS labeling which could lead to efficient real-time analysis of facial\nexpressions in psychology and related fields. To promote further research, we\nhave made code repository publicly available.\n", "link": "http://arxiv.org/abs/2406.09017v1", "date": "2024-06-13", "relevancy": 2.4595, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5114}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4821}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20PCA%20based%20Keypoint%20Tracking%20Approach%20to%20Automated%20Facial%20Expressions%0A%20%20Encoding&body=Title%3A%20A%20PCA%20based%20Keypoint%20Tracking%20Approach%20to%20Automated%20Facial%20Expressions%0A%20%20Encoding%0AAuthor%3A%20Shivansh%20Chandra%20Tripathi%20and%20Rahul%20Garg%0AAbstract%3A%20%20%20The%20Facial%20Action%20Coding%20System%20%28FACS%29%20for%20studying%20facial%20expressions%20is%0Amanual%20and%20requires%20significant%20effort%20and%20expertise.%20This%20paper%20explores%20the%0Ause%20of%20automated%20techniques%20to%20generate%20Action%20Units%20%28AUs%29%20for%20studying%20facial%0Aexpressions.%20We%20propose%20an%20unsupervised%20approach%20based%20on%20Principal%20Component%0AAnalysis%20%28PCA%29%20and%20facial%20keypoint%20tracking%20to%20generate%20data-driven%20AUs%20called%0APCA%20AUs%20using%20the%20publicly%20available%20DISFA%20dataset.%20The%20PCA%20AUs%20comply%20with%20the%0Adirection%20of%20facial%20muscle%20movements%20and%20are%20capable%20of%20explaining%20over%2092.83%0Apercent%20of%20the%20variance%20in%20other%20public%20test%20datasets%20%28BP4D-Spontaneous%20and%0ACK%2B%29%2C%20indicating%20their%20capability%20to%20generalize%20facial%20expressions.%20The%20PCA%20AUs%0Aare%20also%20comparable%20to%20a%20keypoint-based%20equivalence%20of%20FACS%20AUs%20in%20terms%20of%0Avariance%20explained%20on%20the%20test%20datasets.%20In%20conclusion%2C%20our%20research%0Ademonstrates%20the%20potential%20of%20automated%20techniques%20to%20be%20an%20alternative%20to%0Amanual%20FACS%20labeling%20which%20could%20lead%20to%20efficient%20real-time%20analysis%20of%20facial%0Aexpressions%20in%20psychology%20and%20related%20fields.%20To%20promote%20further%20research%2C%20we%0Ahave%20made%20code%20repository%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520PCA%2520based%2520Keypoint%2520Tracking%2520Approach%2520to%2520Automated%2520Facial%2520Expressions%250A%2520%2520Encoding%26entry.906535625%3DShivansh%2520Chandra%2520Tripathi%2520and%2520Rahul%2520Garg%26entry.1292438233%3D%2520%2520The%2520Facial%2520Action%2520Coding%2520System%2520%2528FACS%2529%2520for%2520studying%2520facial%2520expressions%2520is%250Amanual%2520and%2520requires%2520significant%2520effort%2520and%2520expertise.%2520This%2520paper%2520explores%2520the%250Ause%2520of%2520automated%2520techniques%2520to%2520generate%2520Action%2520Units%2520%2528AUs%2529%2520for%2520studying%2520facial%250Aexpressions.%2520We%2520propose%2520an%2520unsupervised%2520approach%2520based%2520on%2520Principal%2520Component%250AAnalysis%2520%2528PCA%2529%2520and%2520facial%2520keypoint%2520tracking%2520to%2520generate%2520data-driven%2520AUs%2520called%250APCA%2520AUs%2520using%2520the%2520publicly%2520available%2520DISFA%2520dataset.%2520The%2520PCA%2520AUs%2520comply%2520with%2520the%250Adirection%2520of%2520facial%2520muscle%2520movements%2520and%2520are%2520capable%2520of%2520explaining%2520over%252092.83%250Apercent%2520of%2520the%2520variance%2520in%2520other%2520public%2520test%2520datasets%2520%2528BP4D-Spontaneous%2520and%250ACK%252B%2529%252C%2520indicating%2520their%2520capability%2520to%2520generalize%2520facial%2520expressions.%2520The%2520PCA%2520AUs%250Aare%2520also%2520comparable%2520to%2520a%2520keypoint-based%2520equivalence%2520of%2520FACS%2520AUs%2520in%2520terms%2520of%250Avariance%2520explained%2520on%2520the%2520test%2520datasets.%2520In%2520conclusion%252C%2520our%2520research%250Ademonstrates%2520the%2520potential%2520of%2520automated%2520techniques%2520to%2520be%2520an%2520alternative%2520to%250Amanual%2520FACS%2520labeling%2520which%2520could%2520lead%2520to%2520efficient%2520real-time%2520analysis%2520of%2520facial%250Aexpressions%2520in%2520psychology%2520and%2520related%2520fields.%2520To%2520promote%2520further%2520research%252C%2520we%250Ahave%2520made%2520code%2520repository%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20PCA%20based%20Keypoint%20Tracking%20Approach%20to%20Automated%20Facial%20Expressions%0A%20%20Encoding&entry.906535625=Shivansh%20Chandra%20Tripathi%20and%20Rahul%20Garg&entry.1292438233=%20%20The%20Facial%20Action%20Coding%20System%20%28FACS%29%20for%20studying%20facial%20expressions%20is%0Amanual%20and%20requires%20significant%20effort%20and%20expertise.%20This%20paper%20explores%20the%0Ause%20of%20automated%20techniques%20to%20generate%20Action%20Units%20%28AUs%29%20for%20studying%20facial%0Aexpressions.%20We%20propose%20an%20unsupervised%20approach%20based%20on%20Principal%20Component%0AAnalysis%20%28PCA%29%20and%20facial%20keypoint%20tracking%20to%20generate%20data-driven%20AUs%20called%0APCA%20AUs%20using%20the%20publicly%20available%20DISFA%20dataset.%20The%20PCA%20AUs%20comply%20with%20the%0Adirection%20of%20facial%20muscle%20movements%20and%20are%20capable%20of%20explaining%20over%2092.83%0Apercent%20of%20the%20variance%20in%20other%20public%20test%20datasets%20%28BP4D-Spontaneous%20and%0ACK%2B%29%2C%20indicating%20their%20capability%20to%20generalize%20facial%20expressions.%20The%20PCA%20AUs%0Aare%20also%20comparable%20to%20a%20keypoint-based%20equivalence%20of%20FACS%20AUs%20in%20terms%20of%0Avariance%20explained%20on%20the%20test%20datasets.%20In%20conclusion%2C%20our%20research%0Ademonstrates%20the%20potential%20of%20automated%20techniques%20to%20be%20an%20alternative%20to%0Amanual%20FACS%20labeling%20which%20could%20lead%20to%20efficient%20real-time%20analysis%20of%20facial%0Aexpressions%20in%20psychology%20and%20related%20fields.%20To%20promote%20further%20research%2C%20we%0Ahave%20made%20code%20repository%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09017v1&entry.124074799=Read"},
{"title": "Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN\n  Pipeline applied on PSMA PET/CT Scans", "author": "Stefan P. Hein and Manuel Schultheiss and Andrei Gafita and Raphael Zaum and Farid Yagubbayli and Isabel Rauscher and Matthias Eiber and Franz Pfeiffer and Wolfgang A. Weber", "abstract": "  Assessing tumor response to systemic therapies is one of the main\napplications of PET/CT. Routinely, only a small subset of index lesions out of\nmultiple lesions is analyzed. However, this operator dependent selection may\nbias the results due to possible significant inter-metastatic heterogeneity of\nresponse to therapy. Automated, AI based approaches for lesion tracking hold\npromise in enabling the analysis of many more lesions and thus providing a\nbetter assessment of tumor response. This work introduces a Siamese CNN\napproach for lesion tracking between PET/CT scans. Our approach is applied on\nthe laborious task of tracking a high number of bone lesions in full-body\nbaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles\nof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer\npatients. Data preparation includes lesion segmentation and affine\nregistration. Our algorithm extracts suitable lesion patches and forwards them\ninto a Siamese CNN trained to classify the lesion patch pairs as corresponding\nor non-corresponding lesions. Experiments have been performed with different\ninput patch types and a Siamese network in 2D and 3D. The CNN model\nsuccessfully learned to classify lesion assignments, reaching a lesion tracking\naccuracy of 83 % in its best configuration with an AUC = 0.91. For remaining\nlesions the pipeline accomplished a re-identification rate of 89 %. We proved\nthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT\nscans. Future clinical studies are necessary if this improves the prediction of\nthe outcome of therapies.\n", "link": "http://arxiv.org/abs/2406.09327v1", "date": "2024-06-13", "relevancy": 2.4409, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4972}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4854}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20AI%20Lesion%20Tracking%20in%20PET/CT%20Imaging%3A%20A%20Siamese-based%20CNN%0A%20%20Pipeline%20applied%20on%20PSMA%20PET/CT%20Scans&body=Title%3A%20Towards%20AI%20Lesion%20Tracking%20in%20PET/CT%20Imaging%3A%20A%20Siamese-based%20CNN%0A%20%20Pipeline%20applied%20on%20PSMA%20PET/CT%20Scans%0AAuthor%3A%20Stefan%20P.%20Hein%20and%20Manuel%20Schultheiss%20and%20Andrei%20Gafita%20and%20Raphael%20Zaum%20and%20Farid%20Yagubbayli%20and%20Isabel%20Rauscher%20and%20Matthias%20Eiber%20and%20Franz%20Pfeiffer%20and%20Wolfgang%20A.%20Weber%0AAbstract%3A%20%20%20Assessing%20tumor%20response%20to%20systemic%20therapies%20is%20one%20of%20the%20main%0Aapplications%20of%20PET/CT.%20Routinely%2C%20only%20a%20small%20subset%20of%20index%20lesions%20out%20of%0Amultiple%20lesions%20is%20analyzed.%20However%2C%20this%20operator%20dependent%20selection%20may%0Abias%20the%20results%20due%20to%20possible%20significant%20inter-metastatic%20heterogeneity%20of%0Aresponse%20to%20therapy.%20Automated%2C%20AI%20based%20approaches%20for%20lesion%20tracking%20hold%0Apromise%20in%20enabling%20the%20analysis%20of%20many%20more%20lesions%20and%20thus%20providing%20a%0Abetter%20assessment%20of%20tumor%20response.%20This%20work%20introduces%20a%20Siamese%20CNN%0Aapproach%20for%20lesion%20tracking%20between%20PET/CT%20scans.%20Our%20approach%20is%20applied%20on%0Athe%20laborious%20task%20of%20tracking%20a%20high%20number%20of%20bone%20lesions%20in%20full-body%0Abaseline%20and%20follow-up%20%5B68Ga%5DGa-%20or%20%5B18F%5DF-PSMA%20PET/CT%20scans%20after%20two%20cycles%0Aof%20%5B177Lu%5DLu-PSMA%20therapy%20of%20metastatic%20castration%20resistant%20prostate%20cancer%0Apatients.%20Data%20preparation%20includes%20lesion%20segmentation%20and%20affine%0Aregistration.%20Our%20algorithm%20extracts%20suitable%20lesion%20patches%20and%20forwards%20them%0Ainto%20a%20Siamese%20CNN%20trained%20to%20classify%20the%20lesion%20patch%20pairs%20as%20corresponding%0Aor%20non-corresponding%20lesions.%20Experiments%20have%20been%20performed%20with%20different%0Ainput%20patch%20types%20and%20a%20Siamese%20network%20in%202D%20and%203D.%20The%20CNN%20model%0Asuccessfully%20learned%20to%20classify%20lesion%20assignments%2C%20reaching%20a%20lesion%20tracking%0Aaccuracy%20of%2083%20%25%20in%20its%20best%20configuration%20with%20an%20AUC%20%3D%200.91.%20For%20remaining%0Alesions%20the%20pipeline%20accomplished%20a%20re-identification%20rate%20of%2089%20%25.%20We%20proved%0Athat%20a%20CNN%20may%20facilitate%20the%20tracking%20of%20multiple%20lesions%20in%20PSMA%20PET/CT%0Ascans.%20Future%20clinical%20studies%20are%20necessary%20if%20this%20improves%20the%20prediction%20of%0Athe%20outcome%20of%20therapies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520AI%2520Lesion%2520Tracking%2520in%2520PET/CT%2520Imaging%253A%2520A%2520Siamese-based%2520CNN%250A%2520%2520Pipeline%2520applied%2520on%2520PSMA%2520PET/CT%2520Scans%26entry.906535625%3DStefan%2520P.%2520Hein%2520and%2520Manuel%2520Schultheiss%2520and%2520Andrei%2520Gafita%2520and%2520Raphael%2520Zaum%2520and%2520Farid%2520Yagubbayli%2520and%2520Isabel%2520Rauscher%2520and%2520Matthias%2520Eiber%2520and%2520Franz%2520Pfeiffer%2520and%2520Wolfgang%2520A.%2520Weber%26entry.1292438233%3D%2520%2520Assessing%2520tumor%2520response%2520to%2520systemic%2520therapies%2520is%2520one%2520of%2520the%2520main%250Aapplications%2520of%2520PET/CT.%2520Routinely%252C%2520only%2520a%2520small%2520subset%2520of%2520index%2520lesions%2520out%2520of%250Amultiple%2520lesions%2520is%2520analyzed.%2520However%252C%2520this%2520operator%2520dependent%2520selection%2520may%250Abias%2520the%2520results%2520due%2520to%2520possible%2520significant%2520inter-metastatic%2520heterogeneity%2520of%250Aresponse%2520to%2520therapy.%2520Automated%252C%2520AI%2520based%2520approaches%2520for%2520lesion%2520tracking%2520hold%250Apromise%2520in%2520enabling%2520the%2520analysis%2520of%2520many%2520more%2520lesions%2520and%2520thus%2520providing%2520a%250Abetter%2520assessment%2520of%2520tumor%2520response.%2520This%2520work%2520introduces%2520a%2520Siamese%2520CNN%250Aapproach%2520for%2520lesion%2520tracking%2520between%2520PET/CT%2520scans.%2520Our%2520approach%2520is%2520applied%2520on%250Athe%2520laborious%2520task%2520of%2520tracking%2520a%2520high%2520number%2520of%2520bone%2520lesions%2520in%2520full-body%250Abaseline%2520and%2520follow-up%2520%255B68Ga%255DGa-%2520or%2520%255B18F%255DF-PSMA%2520PET/CT%2520scans%2520after%2520two%2520cycles%250Aof%2520%255B177Lu%255DLu-PSMA%2520therapy%2520of%2520metastatic%2520castration%2520resistant%2520prostate%2520cancer%250Apatients.%2520Data%2520preparation%2520includes%2520lesion%2520segmentation%2520and%2520affine%250Aregistration.%2520Our%2520algorithm%2520extracts%2520suitable%2520lesion%2520patches%2520and%2520forwards%2520them%250Ainto%2520a%2520Siamese%2520CNN%2520trained%2520to%2520classify%2520the%2520lesion%2520patch%2520pairs%2520as%2520corresponding%250Aor%2520non-corresponding%2520lesions.%2520Experiments%2520have%2520been%2520performed%2520with%2520different%250Ainput%2520patch%2520types%2520and%2520a%2520Siamese%2520network%2520in%25202D%2520and%25203D.%2520The%2520CNN%2520model%250Asuccessfully%2520learned%2520to%2520classify%2520lesion%2520assignments%252C%2520reaching%2520a%2520lesion%2520tracking%250Aaccuracy%2520of%252083%2520%2525%2520in%2520its%2520best%2520configuration%2520with%2520an%2520AUC%2520%253D%25200.91.%2520For%2520remaining%250Alesions%2520the%2520pipeline%2520accomplished%2520a%2520re-identification%2520rate%2520of%252089%2520%2525.%2520We%2520proved%250Athat%2520a%2520CNN%2520may%2520facilitate%2520the%2520tracking%2520of%2520multiple%2520lesions%2520in%2520PSMA%2520PET/CT%250Ascans.%2520Future%2520clinical%2520studies%2520are%2520necessary%2520if%2520this%2520improves%2520the%2520prediction%2520of%250Athe%2520outcome%2520of%2520therapies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20AI%20Lesion%20Tracking%20in%20PET/CT%20Imaging%3A%20A%20Siamese-based%20CNN%0A%20%20Pipeline%20applied%20on%20PSMA%20PET/CT%20Scans&entry.906535625=Stefan%20P.%20Hein%20and%20Manuel%20Schultheiss%20and%20Andrei%20Gafita%20and%20Raphael%20Zaum%20and%20Farid%20Yagubbayli%20and%20Isabel%20Rauscher%20and%20Matthias%20Eiber%20and%20Franz%20Pfeiffer%20and%20Wolfgang%20A.%20Weber&entry.1292438233=%20%20Assessing%20tumor%20response%20to%20systemic%20therapies%20is%20one%20of%20the%20main%0Aapplications%20of%20PET/CT.%20Routinely%2C%20only%20a%20small%20subset%20of%20index%20lesions%20out%20of%0Amultiple%20lesions%20is%20analyzed.%20However%2C%20this%20operator%20dependent%20selection%20may%0Abias%20the%20results%20due%20to%20possible%20significant%20inter-metastatic%20heterogeneity%20of%0Aresponse%20to%20therapy.%20Automated%2C%20AI%20based%20approaches%20for%20lesion%20tracking%20hold%0Apromise%20in%20enabling%20the%20analysis%20of%20many%20more%20lesions%20and%20thus%20providing%20a%0Abetter%20assessment%20of%20tumor%20response.%20This%20work%20introduces%20a%20Siamese%20CNN%0Aapproach%20for%20lesion%20tracking%20between%20PET/CT%20scans.%20Our%20approach%20is%20applied%20on%0Athe%20laborious%20task%20of%20tracking%20a%20high%20number%20of%20bone%20lesions%20in%20full-body%0Abaseline%20and%20follow-up%20%5B68Ga%5DGa-%20or%20%5B18F%5DF-PSMA%20PET/CT%20scans%20after%20two%20cycles%0Aof%20%5B177Lu%5DLu-PSMA%20therapy%20of%20metastatic%20castration%20resistant%20prostate%20cancer%0Apatients.%20Data%20preparation%20includes%20lesion%20segmentation%20and%20affine%0Aregistration.%20Our%20algorithm%20extracts%20suitable%20lesion%20patches%20and%20forwards%20them%0Ainto%20a%20Siamese%20CNN%20trained%20to%20classify%20the%20lesion%20patch%20pairs%20as%20corresponding%0Aor%20non-corresponding%20lesions.%20Experiments%20have%20been%20performed%20with%20different%0Ainput%20patch%20types%20and%20a%20Siamese%20network%20in%202D%20and%203D.%20The%20CNN%20model%0Asuccessfully%20learned%20to%20classify%20lesion%20assignments%2C%20reaching%20a%20lesion%20tracking%0Aaccuracy%20of%2083%20%25%20in%20its%20best%20configuration%20with%20an%20AUC%20%3D%200.91.%20For%20remaining%0Alesions%20the%20pipeline%20accomplished%20a%20re-identification%20rate%20of%2089%20%25.%20We%20proved%0Athat%20a%20CNN%20may%20facilitate%20the%20tracking%20of%20multiple%20lesions%20in%20PSMA%20PET/CT%0Ascans.%20Future%20clinical%20studies%20are%20necessary%20if%20this%20improves%20the%20prediction%20of%0Athe%20outcome%20of%20therapies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09327v1&entry.124074799=Read"},
{"title": "Large-Scale Evaluation of Open-Set Image Classification Techniques", "author": "Halil Bisgin and Andres Palechor and Mike Suter and Manuel G\u00fcnther", "abstract": "  The goal for classification is to correctly assign labels to unseen samples.\nHowever, most methods misclassify samples with unseen labels and assign them to\none of the known classes. Open-Set Classification (OSC) algorithms aim to\nmaximize both closed and open-set recognition capabilities. Recent studies\nshowed the utility of such algorithms on small-scale data sets, but limited\nexperimentation makes it difficult to assess their performances in real-world\nproblems. Here, we provide a comprehensive comparison of various OSC\nalgorithms, including training-based (SoftMax, Garbage, EOS) and\npost-processing methods (Maximum SoftMax Scores, Maximum Logit Scores, OpenMax,\nEVM, PROSER), the latter are applied on features from the former. We perform\nour evaluation on three large-scale protocols that mimic real-world challenges,\nwhere we train on known and negative open-set samples, and test on known and\nunknown instances. Our results show that EOS helps to improve performance of\nalmost all post-processing algorithms. Particularly, OpenMax and PROSER are\nable to exploit better-trained networks, demonstrating the utility of hybrid\nmodels. However, while most algorithms work well on negative test samples --\nsamples of open-set classes seen during training -- they tend to perform poorly\nwhen tested on samples of previously unseen unknown classes, especially in\nchallenging conditions.\n", "link": "http://arxiv.org/abs/2406.09112v1", "date": "2024-06-13", "relevancy": 2.3943, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4822}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4821}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Scale%20Evaluation%20of%20Open-Set%20Image%20Classification%20Techniques&body=Title%3A%20Large-Scale%20Evaluation%20of%20Open-Set%20Image%20Classification%20Techniques%0AAuthor%3A%20Halil%20Bisgin%20and%20Andres%20Palechor%20and%20Mike%20Suter%20and%20Manuel%20G%C3%BCnther%0AAbstract%3A%20%20%20The%20goal%20for%20classification%20is%20to%20correctly%20assign%20labels%20to%20unseen%20samples.%0AHowever%2C%20most%20methods%20misclassify%20samples%20with%20unseen%20labels%20and%20assign%20them%20to%0Aone%20of%20the%20known%20classes.%20Open-Set%20Classification%20%28OSC%29%20algorithms%20aim%20to%0Amaximize%20both%20closed%20and%20open-set%20recognition%20capabilities.%20Recent%20studies%0Ashowed%20the%20utility%20of%20such%20algorithms%20on%20small-scale%20data%20sets%2C%20but%20limited%0Aexperimentation%20makes%20it%20difficult%20to%20assess%20their%20performances%20in%20real-world%0Aproblems.%20Here%2C%20we%20provide%20a%20comprehensive%20comparison%20of%20various%20OSC%0Aalgorithms%2C%20including%20training-based%20%28SoftMax%2C%20Garbage%2C%20EOS%29%20and%0Apost-processing%20methods%20%28Maximum%20SoftMax%20Scores%2C%20Maximum%20Logit%20Scores%2C%20OpenMax%2C%0AEVM%2C%20PROSER%29%2C%20the%20latter%20are%20applied%20on%20features%20from%20the%20former.%20We%20perform%0Aour%20evaluation%20on%20three%20large-scale%20protocols%20that%20mimic%20real-world%20challenges%2C%0Awhere%20we%20train%20on%20known%20and%20negative%20open-set%20samples%2C%20and%20test%20on%20known%20and%0Aunknown%20instances.%20Our%20results%20show%20that%20EOS%20helps%20to%20improve%20performance%20of%0Aalmost%20all%20post-processing%20algorithms.%20Particularly%2C%20OpenMax%20and%20PROSER%20are%0Aable%20to%20exploit%20better-trained%20networks%2C%20demonstrating%20the%20utility%20of%20hybrid%0Amodels.%20However%2C%20while%20most%20algorithms%20work%20well%20on%20negative%20test%20samples%20--%0Asamples%20of%20open-set%20classes%20seen%20during%20training%20--%20they%20tend%20to%20perform%20poorly%0Awhen%20tested%20on%20samples%20of%20previously%20unseen%20unknown%20classes%2C%20especially%20in%0Achallenging%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Scale%2520Evaluation%2520of%2520Open-Set%2520Image%2520Classification%2520Techniques%26entry.906535625%3DHalil%2520Bisgin%2520and%2520Andres%2520Palechor%2520and%2520Mike%2520Suter%2520and%2520Manuel%2520G%25C3%25BCnther%26entry.1292438233%3D%2520%2520The%2520goal%2520for%2520classification%2520is%2520to%2520correctly%2520assign%2520labels%2520to%2520unseen%2520samples.%250AHowever%252C%2520most%2520methods%2520misclassify%2520samples%2520with%2520unseen%2520labels%2520and%2520assign%2520them%2520to%250Aone%2520of%2520the%2520known%2520classes.%2520Open-Set%2520Classification%2520%2528OSC%2529%2520algorithms%2520aim%2520to%250Amaximize%2520both%2520closed%2520and%2520open-set%2520recognition%2520capabilities.%2520Recent%2520studies%250Ashowed%2520the%2520utility%2520of%2520such%2520algorithms%2520on%2520small-scale%2520data%2520sets%252C%2520but%2520limited%250Aexperimentation%2520makes%2520it%2520difficult%2520to%2520assess%2520their%2520performances%2520in%2520real-world%250Aproblems.%2520Here%252C%2520we%2520provide%2520a%2520comprehensive%2520comparison%2520of%2520various%2520OSC%250Aalgorithms%252C%2520including%2520training-based%2520%2528SoftMax%252C%2520Garbage%252C%2520EOS%2529%2520and%250Apost-processing%2520methods%2520%2528Maximum%2520SoftMax%2520Scores%252C%2520Maximum%2520Logit%2520Scores%252C%2520OpenMax%252C%250AEVM%252C%2520PROSER%2529%252C%2520the%2520latter%2520are%2520applied%2520on%2520features%2520from%2520the%2520former.%2520We%2520perform%250Aour%2520evaluation%2520on%2520three%2520large-scale%2520protocols%2520that%2520mimic%2520real-world%2520challenges%252C%250Awhere%2520we%2520train%2520on%2520known%2520and%2520negative%2520open-set%2520samples%252C%2520and%2520test%2520on%2520known%2520and%250Aunknown%2520instances.%2520Our%2520results%2520show%2520that%2520EOS%2520helps%2520to%2520improve%2520performance%2520of%250Aalmost%2520all%2520post-processing%2520algorithms.%2520Particularly%252C%2520OpenMax%2520and%2520PROSER%2520are%250Aable%2520to%2520exploit%2520better-trained%2520networks%252C%2520demonstrating%2520the%2520utility%2520of%2520hybrid%250Amodels.%2520However%252C%2520while%2520most%2520algorithms%2520work%2520well%2520on%2520negative%2520test%2520samples%2520--%250Asamples%2520of%2520open-set%2520classes%2520seen%2520during%2520training%2520--%2520they%2520tend%2520to%2520perform%2520poorly%250Awhen%2520tested%2520on%2520samples%2520of%2520previously%2520unseen%2520unknown%2520classes%252C%2520especially%2520in%250Achallenging%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Scale%20Evaluation%20of%20Open-Set%20Image%20Classification%20Techniques&entry.906535625=Halil%20Bisgin%20and%20Andres%20Palechor%20and%20Mike%20Suter%20and%20Manuel%20G%C3%BCnther&entry.1292438233=%20%20The%20goal%20for%20classification%20is%20to%20correctly%20assign%20labels%20to%20unseen%20samples.%0AHowever%2C%20most%20methods%20misclassify%20samples%20with%20unseen%20labels%20and%20assign%20them%20to%0Aone%20of%20the%20known%20classes.%20Open-Set%20Classification%20%28OSC%29%20algorithms%20aim%20to%0Amaximize%20both%20closed%20and%20open-set%20recognition%20capabilities.%20Recent%20studies%0Ashowed%20the%20utility%20of%20such%20algorithms%20on%20small-scale%20data%20sets%2C%20but%20limited%0Aexperimentation%20makes%20it%20difficult%20to%20assess%20their%20performances%20in%20real-world%0Aproblems.%20Here%2C%20we%20provide%20a%20comprehensive%20comparison%20of%20various%20OSC%0Aalgorithms%2C%20including%20training-based%20%28SoftMax%2C%20Garbage%2C%20EOS%29%20and%0Apost-processing%20methods%20%28Maximum%20SoftMax%20Scores%2C%20Maximum%20Logit%20Scores%2C%20OpenMax%2C%0AEVM%2C%20PROSER%29%2C%20the%20latter%20are%20applied%20on%20features%20from%20the%20former.%20We%20perform%0Aour%20evaluation%20on%20three%20large-scale%20protocols%20that%20mimic%20real-world%20challenges%2C%0Awhere%20we%20train%20on%20known%20and%20negative%20open-set%20samples%2C%20and%20test%20on%20known%20and%0Aunknown%20instances.%20Our%20results%20show%20that%20EOS%20helps%20to%20improve%20performance%20of%0Aalmost%20all%20post-processing%20algorithms.%20Particularly%2C%20OpenMax%20and%20PROSER%20are%0Aable%20to%20exploit%20better-trained%20networks%2C%20demonstrating%20the%20utility%20of%20hybrid%0Amodels.%20However%2C%20while%20most%20algorithms%20work%20well%20on%20negative%20test%20samples%20--%0Asamples%20of%20open-set%20classes%20seen%20during%20training%20--%20they%20tend%20to%20perform%20poorly%0Awhen%20tested%20on%20samples%20of%20previously%20unseen%20unknown%20classes%2C%20especially%20in%0Achallenging%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09112v1&entry.124074799=Read"},
{"title": "RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar", "author": "Fangqiang Ding and Xiangyu Wen and Lawrence Zhu and Yiming Li and Chris Xiaoxuan Lu", "abstract": "  3D occupancy-based perception pipeline has significantly advanced autonomous\ndriving by capturing detailed scene descriptions and demonstrating strong\ngeneralizability across various object categories and shapes. Current methods\npredominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These\nmethods are susceptible to adverse weather conditions, limiting the all-weather\ndeployment of self-driving cars. To improve perception robustness, we leverage\nthe recent advances in automotive radars and introduce a novel approach that\nutilizes 4D imaging radar sensors for 3D occupancy prediction. Our method,\nRadarOcc, circumvents the limitations of sparse radar point clouds by directly\nprocessing the 4D radar tensor, thus preserving essential scene details.\nRadarOcc innovatively addresses the challenges associated with the voluminous\nand noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware\nspatial sparsification, and range-wise self-attention mechanisms. To minimize\nthe interpolation errors associated with direct coordinate transformations, we\nalso devise a spherical-based feature encoding followed by\nspherical-to-Cartesian feature aggregation. We benchmark various baseline\nmethods based on distinct modalities on the public K-Radar dataset. The results\ndemonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy\nprediction and promising results even when compared with LiDAR- or camera-based\nmethods. Additionally, we present qualitative evidence of the superior\nperformance of 4D radar in adverse weather conditions and explore the impact of\nkey pipeline components through ablation studies.\n", "link": "http://arxiv.org/abs/2405.14014v3", "date": "2024-06-13", "relevancy": 2.3927, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6011}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5962}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadarOcc%3A%20Robust%203D%20Occupancy%20Prediction%20with%204D%20Imaging%20Radar&body=Title%3A%20RadarOcc%3A%20Robust%203D%20Occupancy%20Prediction%20with%204D%20Imaging%20Radar%0AAuthor%3A%20Fangqiang%20Ding%20and%20Xiangyu%20Wen%20and%20Lawrence%20Zhu%20and%20Yiming%20Li%20and%20Chris%20Xiaoxuan%20Lu%0AAbstract%3A%20%20%203D%20occupancy-based%20perception%20pipeline%20has%20significantly%20advanced%20autonomous%0Adriving%20by%20capturing%20detailed%20scene%20descriptions%20and%20demonstrating%20strong%0Ageneralizability%20across%20various%20object%20categories%20and%20shapes.%20Current%20methods%0Apredominantly%20rely%20on%20LiDAR%20or%20camera%20inputs%20for%203D%20occupancy%20prediction.%20These%0Amethods%20are%20susceptible%20to%20adverse%20weather%20conditions%2C%20limiting%20the%20all-weather%0Adeployment%20of%20self-driving%20cars.%20To%20improve%20perception%20robustness%2C%20we%20leverage%0Athe%20recent%20advances%20in%20automotive%20radars%20and%20introduce%20a%20novel%20approach%20that%0Autilizes%204D%20imaging%20radar%20sensors%20for%203D%20occupancy%20prediction.%20Our%20method%2C%0ARadarOcc%2C%20circumvents%20the%20limitations%20of%20sparse%20radar%20point%20clouds%20by%20directly%0Aprocessing%20the%204D%20radar%20tensor%2C%20thus%20preserving%20essential%20scene%20details.%0ARadarOcc%20innovatively%20addresses%20the%20challenges%20associated%20with%20the%20voluminous%0Aand%20noisy%204D%20radar%20data%20by%20employing%20Doppler%20bins%20descriptors%2C%20sidelobe-aware%0Aspatial%20sparsification%2C%20and%20range-wise%20self-attention%20mechanisms.%20To%20minimize%0Athe%20interpolation%20errors%20associated%20with%20direct%20coordinate%20transformations%2C%20we%0Aalso%20devise%20a%20spherical-based%20feature%20encoding%20followed%20by%0Aspherical-to-Cartesian%20feature%20aggregation.%20We%20benchmark%20various%20baseline%0Amethods%20based%20on%20distinct%20modalities%20on%20the%20public%20K-Radar%20dataset.%20The%20results%0Ademonstrate%20RadarOcc%27s%20state-of-the-art%20performance%20in%20radar-based%203D%20occupancy%0Aprediction%20and%20promising%20results%20even%20when%20compared%20with%20LiDAR-%20or%20camera-based%0Amethods.%20Additionally%2C%20we%20present%20qualitative%20evidence%20of%20the%20superior%0Aperformance%20of%204D%20radar%20in%20adverse%20weather%20conditions%20and%20explore%20the%20impact%20of%0Akey%20pipeline%20components%20through%20ablation%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14014v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadarOcc%253A%2520Robust%25203D%2520Occupancy%2520Prediction%2520with%25204D%2520Imaging%2520Radar%26entry.906535625%3DFangqiang%2520Ding%2520and%2520Xiangyu%2520Wen%2520and%2520Lawrence%2520Zhu%2520and%2520Yiming%2520Li%2520and%2520Chris%2520Xiaoxuan%2520Lu%26entry.1292438233%3D%2520%25203D%2520occupancy-based%2520perception%2520pipeline%2520has%2520significantly%2520advanced%2520autonomous%250Adriving%2520by%2520capturing%2520detailed%2520scene%2520descriptions%2520and%2520demonstrating%2520strong%250Ageneralizability%2520across%2520various%2520object%2520categories%2520and%2520shapes.%2520Current%2520methods%250Apredominantly%2520rely%2520on%2520LiDAR%2520or%2520camera%2520inputs%2520for%25203D%2520occupancy%2520prediction.%2520These%250Amethods%2520are%2520susceptible%2520to%2520adverse%2520weather%2520conditions%252C%2520limiting%2520the%2520all-weather%250Adeployment%2520of%2520self-driving%2520cars.%2520To%2520improve%2520perception%2520robustness%252C%2520we%2520leverage%250Athe%2520recent%2520advances%2520in%2520automotive%2520radars%2520and%2520introduce%2520a%2520novel%2520approach%2520that%250Autilizes%25204D%2520imaging%2520radar%2520sensors%2520for%25203D%2520occupancy%2520prediction.%2520Our%2520method%252C%250ARadarOcc%252C%2520circumvents%2520the%2520limitations%2520of%2520sparse%2520radar%2520point%2520clouds%2520by%2520directly%250Aprocessing%2520the%25204D%2520radar%2520tensor%252C%2520thus%2520preserving%2520essential%2520scene%2520details.%250ARadarOcc%2520innovatively%2520addresses%2520the%2520challenges%2520associated%2520with%2520the%2520voluminous%250Aand%2520noisy%25204D%2520radar%2520data%2520by%2520employing%2520Doppler%2520bins%2520descriptors%252C%2520sidelobe-aware%250Aspatial%2520sparsification%252C%2520and%2520range-wise%2520self-attention%2520mechanisms.%2520To%2520minimize%250Athe%2520interpolation%2520errors%2520associated%2520with%2520direct%2520coordinate%2520transformations%252C%2520we%250Aalso%2520devise%2520a%2520spherical-based%2520feature%2520encoding%2520followed%2520by%250Aspherical-to-Cartesian%2520feature%2520aggregation.%2520We%2520benchmark%2520various%2520baseline%250Amethods%2520based%2520on%2520distinct%2520modalities%2520on%2520the%2520public%2520K-Radar%2520dataset.%2520The%2520results%250Ademonstrate%2520RadarOcc%2527s%2520state-of-the-art%2520performance%2520in%2520radar-based%25203D%2520occupancy%250Aprediction%2520and%2520promising%2520results%2520even%2520when%2520compared%2520with%2520LiDAR-%2520or%2520camera-based%250Amethods.%2520Additionally%252C%2520we%2520present%2520qualitative%2520evidence%2520of%2520the%2520superior%250Aperformance%2520of%25204D%2520radar%2520in%2520adverse%2520weather%2520conditions%2520and%2520explore%2520the%2520impact%2520of%250Akey%2520pipeline%2520components%2520through%2520ablation%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14014v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadarOcc%3A%20Robust%203D%20Occupancy%20Prediction%20with%204D%20Imaging%20Radar&entry.906535625=Fangqiang%20Ding%20and%20Xiangyu%20Wen%20and%20Lawrence%20Zhu%20and%20Yiming%20Li%20and%20Chris%20Xiaoxuan%20Lu&entry.1292438233=%20%203D%20occupancy-based%20perception%20pipeline%20has%20significantly%20advanced%20autonomous%0Adriving%20by%20capturing%20detailed%20scene%20descriptions%20and%20demonstrating%20strong%0Ageneralizability%20across%20various%20object%20categories%20and%20shapes.%20Current%20methods%0Apredominantly%20rely%20on%20LiDAR%20or%20camera%20inputs%20for%203D%20occupancy%20prediction.%20These%0Amethods%20are%20susceptible%20to%20adverse%20weather%20conditions%2C%20limiting%20the%20all-weather%0Adeployment%20of%20self-driving%20cars.%20To%20improve%20perception%20robustness%2C%20we%20leverage%0Athe%20recent%20advances%20in%20automotive%20radars%20and%20introduce%20a%20novel%20approach%20that%0Autilizes%204D%20imaging%20radar%20sensors%20for%203D%20occupancy%20prediction.%20Our%20method%2C%0ARadarOcc%2C%20circumvents%20the%20limitations%20of%20sparse%20radar%20point%20clouds%20by%20directly%0Aprocessing%20the%204D%20radar%20tensor%2C%20thus%20preserving%20essential%20scene%20details.%0ARadarOcc%20innovatively%20addresses%20the%20challenges%20associated%20with%20the%20voluminous%0Aand%20noisy%204D%20radar%20data%20by%20employing%20Doppler%20bins%20descriptors%2C%20sidelobe-aware%0Aspatial%20sparsification%2C%20and%20range-wise%20self-attention%20mechanisms.%20To%20minimize%0Athe%20interpolation%20errors%20associated%20with%20direct%20coordinate%20transformations%2C%20we%0Aalso%20devise%20a%20spherical-based%20feature%20encoding%20followed%20by%0Aspherical-to-Cartesian%20feature%20aggregation.%20We%20benchmark%20various%20baseline%0Amethods%20based%20on%20distinct%20modalities%20on%20the%20public%20K-Radar%20dataset.%20The%20results%0Ademonstrate%20RadarOcc%27s%20state-of-the-art%20performance%20in%20radar-based%203D%20occupancy%0Aprediction%20and%20promising%20results%20even%20when%20compared%20with%20LiDAR-%20or%20camera-based%0Amethods.%20Additionally%2C%20we%20present%20qualitative%20evidence%20of%20the%20superior%0Aperformance%20of%204D%20radar%20in%20adverse%20weather%20conditions%20and%20explore%20the%20impact%20of%0Akey%20pipeline%20components%20through%20ablation%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14014v3&entry.124074799=Read"},
{"title": "A Generative Model for Digital Camera Noise Synthesis", "author": "Mingyang Song and Yang Zhang and Tun\u00e7 O. Ayd\u0131n and Elham Amin Mansour and Christopher Schroers", "abstract": "  Noise synthesis is a challenging low-level vision task aiming to generate\nrealistic noise given a clean image along with the camera settings. To this\nend, we propose an effective generative model which utilizes clean features as\nguidance followed by noise injections into the network. Specifically, our\ngenerator follows a UNet-like structure with skip connections but without\ndownsampling and upsampling layers. Firstly, we extract deep features from a\nclean image as the guidance and concatenate a Gaussian noise map to the\ntransition point between the encoder and decoder as the noise source. Secondly,\nwe propose noise synthesis blocks in the decoder in each of which we inject\nGaussian noise to model the noise characteristics. Thirdly, we propose to\nutilize an additional Style Loss and demonstrate that this allows better noise\ncharacteristics supervision in the generator. Through a number of new\nexperiments, we evaluate the temporal variance and the spatial correlation of\nthe generated noise which we hope can provide meaningful insights for future\nworks. Finally, we show that our proposed approach outperforms existing methods\nfor synthesizing camera noise.\n", "link": "http://arxiv.org/abs/2303.09199v3", "date": "2024-06-13", "relevancy": 2.3764, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6334}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5837}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generative%20Model%20for%20Digital%20Camera%20Noise%20Synthesis&body=Title%3A%20A%20Generative%20Model%20for%20Digital%20Camera%20Noise%20Synthesis%0AAuthor%3A%20Mingyang%20Song%20and%20Yang%20Zhang%20and%20Tun%C3%A7%20O.%20Ayd%C4%B1n%20and%20Elham%20Amin%20Mansour%20and%20Christopher%20Schroers%0AAbstract%3A%20%20%20Noise%20synthesis%20is%20a%20challenging%20low-level%20vision%20task%20aiming%20to%20generate%0Arealistic%20noise%20given%20a%20clean%20image%20along%20with%20the%20camera%20settings.%20To%20this%0Aend%2C%20we%20propose%20an%20effective%20generative%20model%20which%20utilizes%20clean%20features%20as%0Aguidance%20followed%20by%20noise%20injections%20into%20the%20network.%20Specifically%2C%20our%0Agenerator%20follows%20a%20UNet-like%20structure%20with%20skip%20connections%20but%20without%0Adownsampling%20and%20upsampling%20layers.%20Firstly%2C%20we%20extract%20deep%20features%20from%20a%0Aclean%20image%20as%20the%20guidance%20and%20concatenate%20a%20Gaussian%20noise%20map%20to%20the%0Atransition%20point%20between%20the%20encoder%20and%20decoder%20as%20the%20noise%20source.%20Secondly%2C%0Awe%20propose%20noise%20synthesis%20blocks%20in%20the%20decoder%20in%20each%20of%20which%20we%20inject%0AGaussian%20noise%20to%20model%20the%20noise%20characteristics.%20Thirdly%2C%20we%20propose%20to%0Autilize%20an%20additional%20Style%20Loss%20and%20demonstrate%20that%20this%20allows%20better%20noise%0Acharacteristics%20supervision%20in%20the%20generator.%20Through%20a%20number%20of%20new%0Aexperiments%2C%20we%20evaluate%20the%20temporal%20variance%20and%20the%20spatial%20correlation%20of%0Athe%20generated%20noise%20which%20we%20hope%20can%20provide%20meaningful%20insights%20for%20future%0Aworks.%20Finally%2C%20we%20show%20that%20our%20proposed%20approach%20outperforms%20existing%20methods%0Afor%20synthesizing%20camera%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.09199v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generative%2520Model%2520for%2520Digital%2520Camera%2520Noise%2520Synthesis%26entry.906535625%3DMingyang%2520Song%2520and%2520Yang%2520Zhang%2520and%2520Tun%25C3%25A7%2520O.%2520Ayd%25C4%25B1n%2520and%2520Elham%2520Amin%2520Mansour%2520and%2520Christopher%2520Schroers%26entry.1292438233%3D%2520%2520Noise%2520synthesis%2520is%2520a%2520challenging%2520low-level%2520vision%2520task%2520aiming%2520to%2520generate%250Arealistic%2520noise%2520given%2520a%2520clean%2520image%2520along%2520with%2520the%2520camera%2520settings.%2520To%2520this%250Aend%252C%2520we%2520propose%2520an%2520effective%2520generative%2520model%2520which%2520utilizes%2520clean%2520features%2520as%250Aguidance%2520followed%2520by%2520noise%2520injections%2520into%2520the%2520network.%2520Specifically%252C%2520our%250Agenerator%2520follows%2520a%2520UNet-like%2520structure%2520with%2520skip%2520connections%2520but%2520without%250Adownsampling%2520and%2520upsampling%2520layers.%2520Firstly%252C%2520we%2520extract%2520deep%2520features%2520from%2520a%250Aclean%2520image%2520as%2520the%2520guidance%2520and%2520concatenate%2520a%2520Gaussian%2520noise%2520map%2520to%2520the%250Atransition%2520point%2520between%2520the%2520encoder%2520and%2520decoder%2520as%2520the%2520noise%2520source.%2520Secondly%252C%250Awe%2520propose%2520noise%2520synthesis%2520blocks%2520in%2520the%2520decoder%2520in%2520each%2520of%2520which%2520we%2520inject%250AGaussian%2520noise%2520to%2520model%2520the%2520noise%2520characteristics.%2520Thirdly%252C%2520we%2520propose%2520to%250Autilize%2520an%2520additional%2520Style%2520Loss%2520and%2520demonstrate%2520that%2520this%2520allows%2520better%2520noise%250Acharacteristics%2520supervision%2520in%2520the%2520generator.%2520Through%2520a%2520number%2520of%2520new%250Aexperiments%252C%2520we%2520evaluate%2520the%2520temporal%2520variance%2520and%2520the%2520spatial%2520correlation%2520of%250Athe%2520generated%2520noise%2520which%2520we%2520hope%2520can%2520provide%2520meaningful%2520insights%2520for%2520future%250Aworks.%2520Finally%252C%2520we%2520show%2520that%2520our%2520proposed%2520approach%2520outperforms%2520existing%2520methods%250Afor%2520synthesizing%2520camera%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.09199v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generative%20Model%20for%20Digital%20Camera%20Noise%20Synthesis&entry.906535625=Mingyang%20Song%20and%20Yang%20Zhang%20and%20Tun%C3%A7%20O.%20Ayd%C4%B1n%20and%20Elham%20Amin%20Mansour%20and%20Christopher%20Schroers&entry.1292438233=%20%20Noise%20synthesis%20is%20a%20challenging%20low-level%20vision%20task%20aiming%20to%20generate%0Arealistic%20noise%20given%20a%20clean%20image%20along%20with%20the%20camera%20settings.%20To%20this%0Aend%2C%20we%20propose%20an%20effective%20generative%20model%20which%20utilizes%20clean%20features%20as%0Aguidance%20followed%20by%20noise%20injections%20into%20the%20network.%20Specifically%2C%20our%0Agenerator%20follows%20a%20UNet-like%20structure%20with%20skip%20connections%20but%20without%0Adownsampling%20and%20upsampling%20layers.%20Firstly%2C%20we%20extract%20deep%20features%20from%20a%0Aclean%20image%20as%20the%20guidance%20and%20concatenate%20a%20Gaussian%20noise%20map%20to%20the%0Atransition%20point%20between%20the%20encoder%20and%20decoder%20as%20the%20noise%20source.%20Secondly%2C%0Awe%20propose%20noise%20synthesis%20blocks%20in%20the%20decoder%20in%20each%20of%20which%20we%20inject%0AGaussian%20noise%20to%20model%20the%20noise%20characteristics.%20Thirdly%2C%20we%20propose%20to%0Autilize%20an%20additional%20Style%20Loss%20and%20demonstrate%20that%20this%20allows%20better%20noise%0Acharacteristics%20supervision%20in%20the%20generator.%20Through%20a%20number%20of%20new%0Aexperiments%2C%20we%20evaluate%20the%20temporal%20variance%20and%20the%20spatial%20correlation%20of%0Athe%20generated%20noise%20which%20we%20hope%20can%20provide%20meaningful%20insights%20for%20future%0Aworks.%20Finally%2C%20we%20show%20that%20our%20proposed%20approach%20outperforms%20existing%20methods%0Afor%20synthesizing%20camera%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.09199v3&entry.124074799=Read"},
{"title": "ASTRA: Aligning Speech and Text Representations for Asr without Sampling", "author": "Neeraj Gaur and Rohan Agrawal and Gary Wang and Parisa Haghani and Andrew Rosenberg and Bhuvana Ramabhadran", "abstract": "  This paper introduces ASTRA, a novel method for improving Automatic Speech\nRecognition (ASR) through text injection.Unlike prevailing techniques, ASTRA\neliminates the need for sampling to match sequence lengths between speech and\ntext modalities. Instead, it leverages the inherent alignments learned within\nCTC/RNNT models. This approach offers the following two advantages, namely,\navoiding potential misalignment between speech and text features that could\narise from upsampling and eliminating the need for models to accurately predict\nduration of sub-word tokens. This novel formulation of modality (length)\nmatching as a weighted RNNT objective matches the performance of the\nstate-of-the-art duration-based methods on the FLEURS benchmark, while opening\nup other avenues of research in speech processing.\n", "link": "http://arxiv.org/abs/2406.06664v2", "date": "2024-06-13", "relevancy": 2.3618, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4752}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASTRA%3A%20Aligning%20Speech%20and%20Text%20Representations%20for%20Asr%20without%20Sampling&body=Title%3A%20ASTRA%3A%20Aligning%20Speech%20and%20Text%20Representations%20for%20Asr%20without%20Sampling%0AAuthor%3A%20Neeraj%20Gaur%20and%20Rohan%20Agrawal%20and%20Gary%20Wang%20and%20Parisa%20Haghani%20and%20Andrew%20Rosenberg%20and%20Bhuvana%20Ramabhadran%0AAbstract%3A%20%20%20This%20paper%20introduces%20ASTRA%2C%20a%20novel%20method%20for%20improving%20Automatic%20Speech%0ARecognition%20%28ASR%29%20through%20text%20injection.Unlike%20prevailing%20techniques%2C%20ASTRA%0Aeliminates%20the%20need%20for%20sampling%20to%20match%20sequence%20lengths%20between%20speech%20and%0Atext%20modalities.%20Instead%2C%20it%20leverages%20the%20inherent%20alignments%20learned%20within%0ACTC/RNNT%20models.%20This%20approach%20offers%20the%20following%20two%20advantages%2C%20namely%2C%0Aavoiding%20potential%20misalignment%20between%20speech%20and%20text%20features%20that%20could%0Aarise%20from%20upsampling%20and%20eliminating%20the%20need%20for%20models%20to%20accurately%20predict%0Aduration%20of%20sub-word%20tokens.%20This%20novel%20formulation%20of%20modality%20%28length%29%0Amatching%20as%20a%20weighted%20RNNT%20objective%20matches%20the%20performance%20of%20the%0Astate-of-the-art%20duration-based%20methods%20on%20the%20FLEURS%20benchmark%2C%20while%20opening%0Aup%20other%20avenues%20of%20research%20in%20speech%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASTRA%253A%2520Aligning%2520Speech%2520and%2520Text%2520Representations%2520for%2520Asr%2520without%2520Sampling%26entry.906535625%3DNeeraj%2520Gaur%2520and%2520Rohan%2520Agrawal%2520and%2520Gary%2520Wang%2520and%2520Parisa%2520Haghani%2520and%2520Andrew%2520Rosenberg%2520and%2520Bhuvana%2520Ramabhadran%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520ASTRA%252C%2520a%2520novel%2520method%2520for%2520improving%2520Automatic%2520Speech%250ARecognition%2520%2528ASR%2529%2520through%2520text%2520injection.Unlike%2520prevailing%2520techniques%252C%2520ASTRA%250Aeliminates%2520the%2520need%2520for%2520sampling%2520to%2520match%2520sequence%2520lengths%2520between%2520speech%2520and%250Atext%2520modalities.%2520Instead%252C%2520it%2520leverages%2520the%2520inherent%2520alignments%2520learned%2520within%250ACTC/RNNT%2520models.%2520This%2520approach%2520offers%2520the%2520following%2520two%2520advantages%252C%2520namely%252C%250Aavoiding%2520potential%2520misalignment%2520between%2520speech%2520and%2520text%2520features%2520that%2520could%250Aarise%2520from%2520upsampling%2520and%2520eliminating%2520the%2520need%2520for%2520models%2520to%2520accurately%2520predict%250Aduration%2520of%2520sub-word%2520tokens.%2520This%2520novel%2520formulation%2520of%2520modality%2520%2528length%2529%250Amatching%2520as%2520a%2520weighted%2520RNNT%2520objective%2520matches%2520the%2520performance%2520of%2520the%250Astate-of-the-art%2520duration-based%2520methods%2520on%2520the%2520FLEURS%2520benchmark%252C%2520while%2520opening%250Aup%2520other%2520avenues%2520of%2520research%2520in%2520speech%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASTRA%3A%20Aligning%20Speech%20and%20Text%20Representations%20for%20Asr%20without%20Sampling&entry.906535625=Neeraj%20Gaur%20and%20Rohan%20Agrawal%20and%20Gary%20Wang%20and%20Parisa%20Haghani%20and%20Andrew%20Rosenberg%20and%20Bhuvana%20Ramabhadran&entry.1292438233=%20%20This%20paper%20introduces%20ASTRA%2C%20a%20novel%20method%20for%20improving%20Automatic%20Speech%0ARecognition%20%28ASR%29%20through%20text%20injection.Unlike%20prevailing%20techniques%2C%20ASTRA%0Aeliminates%20the%20need%20for%20sampling%20to%20match%20sequence%20lengths%20between%20speech%20and%0Atext%20modalities.%20Instead%2C%20it%20leverages%20the%20inherent%20alignments%20learned%20within%0ACTC/RNNT%20models.%20This%20approach%20offers%20the%20following%20two%20advantages%2C%20namely%2C%0Aavoiding%20potential%20misalignment%20between%20speech%20and%20text%20features%20that%20could%0Aarise%20from%20upsampling%20and%20eliminating%20the%20need%20for%20models%20to%20accurately%20predict%0Aduration%20of%20sub-word%20tokens.%20This%20novel%20formulation%20of%20modality%20%28length%29%0Amatching%20as%20a%20weighted%20RNNT%20objective%20matches%20the%20performance%20of%20the%0Astate-of-the-art%20duration-based%20methods%20on%20the%20FLEURS%20benchmark%2C%20while%20opening%0Aup%20other%20avenues%20of%20research%20in%20speech%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06664v2&entry.124074799=Read"},
{"title": "Sagiri: Low Dynamic Range Image Enhancement with Generative Diffusion\n  Prior", "author": "Baiang Li and Sizhuo Ma and Yanhong Zeng and Xiaogang Xu and Youqing Fang and Zhao Zhang and Jian Wang and Kai Chen", "abstract": "  Capturing High Dynamic Range (HDR) scenery using 8-bit cameras often suffers\nfrom over-/underexposure, loss of fine details due to low bit-depth\ncompression, skewed color distributions, and strong noise in dark areas.\nTraditional LDR image enhancement methods primarily focus on color mapping,\nwhich enhances the visual representation by expanding the image's color range\nand adjusting the brightness. However, these approaches fail to effectively\nrestore content in dynamic range extremes, which are regions with pixel values\nclose to 0 or 255. To address the full scope of challenges in HDR imaging and\nsurpass the limitations of current models, we propose a novel two-stage\napproach. The first stage maps the color and brightness to an appropriate range\nwhile keeping the existing details, and the second stage utilizes a diffusion\nprior to generate content in dynamic range extremes lost during capture. This\ngenerative refinement module can also be used as a plug-and-play module to\nenhance and complement existing LDR enhancement models. The proposed method\nmarkedly improves the quality and details of LDR images, demonstrating superior\nperformance through rigorous experimental validation. The project page is at\nhttps://sagiri0208.github.io\n", "link": "http://arxiv.org/abs/2406.09389v1", "date": "2024-06-13", "relevancy": 2.3495, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6022}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5906}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sagiri%3A%20Low%20Dynamic%20Range%20Image%20Enhancement%20with%20Generative%20Diffusion%0A%20%20Prior&body=Title%3A%20Sagiri%3A%20Low%20Dynamic%20Range%20Image%20Enhancement%20with%20Generative%20Diffusion%0A%20%20Prior%0AAuthor%3A%20Baiang%20Li%20and%20Sizhuo%20Ma%20and%20Yanhong%20Zeng%20and%20Xiaogang%20Xu%20and%20Youqing%20Fang%20and%20Zhao%20Zhang%20and%20Jian%20Wang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Capturing%20High%20Dynamic%20Range%20%28HDR%29%20scenery%20using%208-bit%20cameras%20often%20suffers%0Afrom%20over-/underexposure%2C%20loss%20of%20fine%20details%20due%20to%20low%20bit-depth%0Acompression%2C%20skewed%20color%20distributions%2C%20and%20strong%20noise%20in%20dark%20areas.%0ATraditional%20LDR%20image%20enhancement%20methods%20primarily%20focus%20on%20color%20mapping%2C%0Awhich%20enhances%20the%20visual%20representation%20by%20expanding%20the%20image%27s%20color%20range%0Aand%20adjusting%20the%20brightness.%20However%2C%20these%20approaches%20fail%20to%20effectively%0Arestore%20content%20in%20dynamic%20range%20extremes%2C%20which%20are%20regions%20with%20pixel%20values%0Aclose%20to%200%20or%20255.%20To%20address%20the%20full%20scope%20of%20challenges%20in%20HDR%20imaging%20and%0Asurpass%20the%20limitations%20of%20current%20models%2C%20we%20propose%20a%20novel%20two-stage%0Aapproach.%20The%20first%20stage%20maps%20the%20color%20and%20brightness%20to%20an%20appropriate%20range%0Awhile%20keeping%20the%20existing%20details%2C%20and%20the%20second%20stage%20utilizes%20a%20diffusion%0Aprior%20to%20generate%20content%20in%20dynamic%20range%20extremes%20lost%20during%20capture.%20This%0Agenerative%20refinement%20module%20can%20also%20be%20used%20as%20a%20plug-and-play%20module%20to%0Aenhance%20and%20complement%20existing%20LDR%20enhancement%20models.%20The%20proposed%20method%0Amarkedly%20improves%20the%20quality%20and%20details%20of%20LDR%20images%2C%20demonstrating%20superior%0Aperformance%20through%20rigorous%20experimental%20validation.%20The%20project%20page%20is%20at%0Ahttps%3A//sagiri0208.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSagiri%253A%2520Low%2520Dynamic%2520Range%2520Image%2520Enhancement%2520with%2520Generative%2520Diffusion%250A%2520%2520Prior%26entry.906535625%3DBaiang%2520Li%2520and%2520Sizhuo%2520Ma%2520and%2520Yanhong%2520Zeng%2520and%2520Xiaogang%2520Xu%2520and%2520Youqing%2520Fang%2520and%2520Zhao%2520Zhang%2520and%2520Jian%2520Wang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Capturing%2520High%2520Dynamic%2520Range%2520%2528HDR%2529%2520scenery%2520using%25208-bit%2520cameras%2520often%2520suffers%250Afrom%2520over-/underexposure%252C%2520loss%2520of%2520fine%2520details%2520due%2520to%2520low%2520bit-depth%250Acompression%252C%2520skewed%2520color%2520distributions%252C%2520and%2520strong%2520noise%2520in%2520dark%2520areas.%250ATraditional%2520LDR%2520image%2520enhancement%2520methods%2520primarily%2520focus%2520on%2520color%2520mapping%252C%250Awhich%2520enhances%2520the%2520visual%2520representation%2520by%2520expanding%2520the%2520image%2527s%2520color%2520range%250Aand%2520adjusting%2520the%2520brightness.%2520However%252C%2520these%2520approaches%2520fail%2520to%2520effectively%250Arestore%2520content%2520in%2520dynamic%2520range%2520extremes%252C%2520which%2520are%2520regions%2520with%2520pixel%2520values%250Aclose%2520to%25200%2520or%2520255.%2520To%2520address%2520the%2520full%2520scope%2520of%2520challenges%2520in%2520HDR%2520imaging%2520and%250Asurpass%2520the%2520limitations%2520of%2520current%2520models%252C%2520we%2520propose%2520a%2520novel%2520two-stage%250Aapproach.%2520The%2520first%2520stage%2520maps%2520the%2520color%2520and%2520brightness%2520to%2520an%2520appropriate%2520range%250Awhile%2520keeping%2520the%2520existing%2520details%252C%2520and%2520the%2520second%2520stage%2520utilizes%2520a%2520diffusion%250Aprior%2520to%2520generate%2520content%2520in%2520dynamic%2520range%2520extremes%2520lost%2520during%2520capture.%2520This%250Agenerative%2520refinement%2520module%2520can%2520also%2520be%2520used%2520as%2520a%2520plug-and-play%2520module%2520to%250Aenhance%2520and%2520complement%2520existing%2520LDR%2520enhancement%2520models.%2520The%2520proposed%2520method%250Amarkedly%2520improves%2520the%2520quality%2520and%2520details%2520of%2520LDR%2520images%252C%2520demonstrating%2520superior%250Aperformance%2520through%2520rigorous%2520experimental%2520validation.%2520The%2520project%2520page%2520is%2520at%250Ahttps%253A//sagiri0208.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sagiri%3A%20Low%20Dynamic%20Range%20Image%20Enhancement%20with%20Generative%20Diffusion%0A%20%20Prior&entry.906535625=Baiang%20Li%20and%20Sizhuo%20Ma%20and%20Yanhong%20Zeng%20and%20Xiaogang%20Xu%20and%20Youqing%20Fang%20and%20Zhao%20Zhang%20and%20Jian%20Wang%20and%20Kai%20Chen&entry.1292438233=%20%20Capturing%20High%20Dynamic%20Range%20%28HDR%29%20scenery%20using%208-bit%20cameras%20often%20suffers%0Afrom%20over-/underexposure%2C%20loss%20of%20fine%20details%20due%20to%20low%20bit-depth%0Acompression%2C%20skewed%20color%20distributions%2C%20and%20strong%20noise%20in%20dark%20areas.%0ATraditional%20LDR%20image%20enhancement%20methods%20primarily%20focus%20on%20color%20mapping%2C%0Awhich%20enhances%20the%20visual%20representation%20by%20expanding%20the%20image%27s%20color%20range%0Aand%20adjusting%20the%20brightness.%20However%2C%20these%20approaches%20fail%20to%20effectively%0Arestore%20content%20in%20dynamic%20range%20extremes%2C%20which%20are%20regions%20with%20pixel%20values%0Aclose%20to%200%20or%20255.%20To%20address%20the%20full%20scope%20of%20challenges%20in%20HDR%20imaging%20and%0Asurpass%20the%20limitations%20of%20current%20models%2C%20we%20propose%20a%20novel%20two-stage%0Aapproach.%20The%20first%20stage%20maps%20the%20color%20and%20brightness%20to%20an%20appropriate%20range%0Awhile%20keeping%20the%20existing%20details%2C%20and%20the%20second%20stage%20utilizes%20a%20diffusion%0Aprior%20to%20generate%20content%20in%20dynamic%20range%20extremes%20lost%20during%20capture.%20This%0Agenerative%20refinement%20module%20can%20also%20be%20used%20as%20a%20plug-and-play%20module%20to%0Aenhance%20and%20complement%20existing%20LDR%20enhancement%20models.%20The%20proposed%20method%0Amarkedly%20improves%20the%20quality%20and%20details%20of%20LDR%20images%2C%20demonstrating%20superior%0Aperformance%20through%20rigorous%20experimental%20validation.%20The%20project%20page%20is%20at%0Ahttps%3A//sagiri0208.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09389v1&entry.124074799=Read"},
{"title": "Depth Anything V2", "author": "Lihe Yang and Bingyi Kang and Zilong Huang and Zhen Zhao and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao", "abstract": "  This work presents Depth Anything V2. Without pursuing fancy techniques, we\naim to reveal crucial findings to pave the way towards building a powerful\nmonocular depth estimation model. Notably, compared with V1, this version\nproduces much finer and more robust depth predictions through three key\npractices: 1) replacing all labeled real images with synthetic images, 2)\nscaling up the capacity of our teacher model, and 3) teaching student models\nvia the bridge of large-scale pseudo-labeled real images. Compared with the\nlatest models built on Stable Diffusion, our models are significantly more\nefficient (more than 10x faster) and more accurate. We offer models of\ndifferent scales (ranging from 25M to 1.3B params) to support extensive\nscenarios. Benefiting from their strong generalization capability, we fine-tune\nthem with metric depth labels to obtain our metric depth models. In addition to\nour models, considering the limited diversity and frequent noise in current\ntest sets, we construct a versatile evaluation benchmark with precise\nannotations and diverse scenes to facilitate future research.\n", "link": "http://arxiv.org/abs/2406.09414v1", "date": "2024-06-13", "relevancy": 2.3449, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5871}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5871}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Anything%20V2&body=Title%3A%20Depth%20Anything%20V2%0AAuthor%3A%20Lihe%20Yang%20and%20Bingyi%20Kang%20and%20Zilong%20Huang%20and%20Zhen%20Zhao%20and%20Xiaogang%20Xu%20and%20Jiashi%20Feng%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20This%20work%20presents%20Depth%20Anything%20V2.%20Without%20pursuing%20fancy%20techniques%2C%20we%0Aaim%20to%20reveal%20crucial%20findings%20to%20pave%20the%20way%20towards%20building%20a%20powerful%0Amonocular%20depth%20estimation%20model.%20Notably%2C%20compared%20with%20V1%2C%20this%20version%0Aproduces%20much%20finer%20and%20more%20robust%20depth%20predictions%20through%20three%20key%0Apractices%3A%201%29%20replacing%20all%20labeled%20real%20images%20with%20synthetic%20images%2C%202%29%0Ascaling%20up%20the%20capacity%20of%20our%20teacher%20model%2C%20and%203%29%20teaching%20student%20models%0Avia%20the%20bridge%20of%20large-scale%20pseudo-labeled%20real%20images.%20Compared%20with%20the%0Alatest%20models%20built%20on%20Stable%20Diffusion%2C%20our%20models%20are%20significantly%20more%0Aefficient%20%28more%20than%2010x%20faster%29%20and%20more%20accurate.%20We%20offer%20models%20of%0Adifferent%20scales%20%28ranging%20from%2025M%20to%201.3B%20params%29%20to%20support%20extensive%0Ascenarios.%20Benefiting%20from%20their%20strong%20generalization%20capability%2C%20we%20fine-tune%0Athem%20with%20metric%20depth%20labels%20to%20obtain%20our%20metric%20depth%20models.%20In%20addition%20to%0Aour%20models%2C%20considering%20the%20limited%20diversity%20and%20frequent%20noise%20in%20current%0Atest%20sets%2C%20we%20construct%20a%20versatile%20evaluation%20benchmark%20with%20precise%0Aannotations%20and%20diverse%20scenes%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Anything%2520V2%26entry.906535625%3DLihe%2520Yang%2520and%2520Bingyi%2520Kang%2520and%2520Zilong%2520Huang%2520and%2520Zhen%2520Zhao%2520and%2520Xiaogang%2520Xu%2520and%2520Jiashi%2520Feng%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520Depth%2520Anything%2520V2.%2520Without%2520pursuing%2520fancy%2520techniques%252C%2520we%250Aaim%2520to%2520reveal%2520crucial%2520findings%2520to%2520pave%2520the%2520way%2520towards%2520building%2520a%2520powerful%250Amonocular%2520depth%2520estimation%2520model.%2520Notably%252C%2520compared%2520with%2520V1%252C%2520this%2520version%250Aproduces%2520much%2520finer%2520and%2520more%2520robust%2520depth%2520predictions%2520through%2520three%2520key%250Apractices%253A%25201%2529%2520replacing%2520all%2520labeled%2520real%2520images%2520with%2520synthetic%2520images%252C%25202%2529%250Ascaling%2520up%2520the%2520capacity%2520of%2520our%2520teacher%2520model%252C%2520and%25203%2529%2520teaching%2520student%2520models%250Avia%2520the%2520bridge%2520of%2520large-scale%2520pseudo-labeled%2520real%2520images.%2520Compared%2520with%2520the%250Alatest%2520models%2520built%2520on%2520Stable%2520Diffusion%252C%2520our%2520models%2520are%2520significantly%2520more%250Aefficient%2520%2528more%2520than%252010x%2520faster%2529%2520and%2520more%2520accurate.%2520We%2520offer%2520models%2520of%250Adifferent%2520scales%2520%2528ranging%2520from%252025M%2520to%25201.3B%2520params%2529%2520to%2520support%2520extensive%250Ascenarios.%2520Benefiting%2520from%2520their%2520strong%2520generalization%2520capability%252C%2520we%2520fine-tune%250Athem%2520with%2520metric%2520depth%2520labels%2520to%2520obtain%2520our%2520metric%2520depth%2520models.%2520In%2520addition%2520to%250Aour%2520models%252C%2520considering%2520the%2520limited%2520diversity%2520and%2520frequent%2520noise%2520in%2520current%250Atest%2520sets%252C%2520we%2520construct%2520a%2520versatile%2520evaluation%2520benchmark%2520with%2520precise%250Aannotations%2520and%2520diverse%2520scenes%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Anything%20V2&entry.906535625=Lihe%20Yang%20and%20Bingyi%20Kang%20and%20Zilong%20Huang%20and%20Zhen%20Zhao%20and%20Xiaogang%20Xu%20and%20Jiashi%20Feng%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20This%20work%20presents%20Depth%20Anything%20V2.%20Without%20pursuing%20fancy%20techniques%2C%20we%0Aaim%20to%20reveal%20crucial%20findings%20to%20pave%20the%20way%20towards%20building%20a%20powerful%0Amonocular%20depth%20estimation%20model.%20Notably%2C%20compared%20with%20V1%2C%20this%20version%0Aproduces%20much%20finer%20and%20more%20robust%20depth%20predictions%20through%20three%20key%0Apractices%3A%201%29%20replacing%20all%20labeled%20real%20images%20with%20synthetic%20images%2C%202%29%0Ascaling%20up%20the%20capacity%20of%20our%20teacher%20model%2C%20and%203%29%20teaching%20student%20models%0Avia%20the%20bridge%20of%20large-scale%20pseudo-labeled%20real%20images.%20Compared%20with%20the%0Alatest%20models%20built%20on%20Stable%20Diffusion%2C%20our%20models%20are%20significantly%20more%0Aefficient%20%28more%20than%2010x%20faster%29%20and%20more%20accurate.%20We%20offer%20models%20of%0Adifferent%20scales%20%28ranging%20from%2025M%20to%201.3B%20params%29%20to%20support%20extensive%0Ascenarios.%20Benefiting%20from%20their%20strong%20generalization%20capability%2C%20we%20fine-tune%0Athem%20with%20metric%20depth%20labels%20to%20obtain%20our%20metric%20depth%20models.%20In%20addition%20to%0Aour%20models%2C%20considering%20the%20limited%20diversity%20and%20frequent%20noise%20in%20current%0Atest%20sets%2C%20we%20construct%20a%20versatile%20evaluation%20benchmark%20with%20precise%0Aannotations%20and%20diverse%20scenes%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09414v1&entry.124074799=Read"},
{"title": "A Comprehensive Graph Pooling Benchmark: Effectiveness, Robustness and\n  Generalizability", "author": "Pengyun Wang and Junyu Luo and Yanxin Shen and Siyu Heng and Xiao Luo", "abstract": "  Graph pooling has gained attention for its ability to obtain effective node\nand graph representations for various downstream tasks. Despite the recent\nsurge in graph pooling approaches, there is a lack of standardized experimental\nsettings and fair benchmarks to evaluate their performance. To address this\nissue, we have constructed a comprehensive benchmark that includes 15 graph\npooling methods and 21 different graph datasets. This benchmark systematically\nassesses the performance of graph pooling methods in three dimensions, i.e.,\neffectiveness, robustness, and generalizability. We first evaluate the\nperformance of these graph pooling approaches across different tasks including\ngraph classification, graph regression and node classification. Then, we\ninvestigate their performance under potential noise attacks and\nout-of-distribution shifts in real-world scenarios. We also involve detailed\nefficiency analysis and parameter analysis. Extensive experiments validate the\nstrong capability and applicability of graph pooling approaches in various\nscenarios, which can provide valuable insights and guidance for deep geometric\nlearning research. The source code of our benchmark is available at\nhttps://github.com/goose315/Graph_Pooling_Benchmark.\n", "link": "http://arxiv.org/abs/2406.09031v1", "date": "2024-06-13", "relevancy": 2.3447, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4928}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4618}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Graph%20Pooling%20Benchmark%3A%20Effectiveness%2C%20Robustness%20and%0A%20%20Generalizability&body=Title%3A%20A%20Comprehensive%20Graph%20Pooling%20Benchmark%3A%20Effectiveness%2C%20Robustness%20and%0A%20%20Generalizability%0AAuthor%3A%20Pengyun%20Wang%20and%20Junyu%20Luo%20and%20Yanxin%20Shen%20and%20Siyu%20Heng%20and%20Xiao%20Luo%0AAbstract%3A%20%20%20Graph%20pooling%20has%20gained%20attention%20for%20its%20ability%20to%20obtain%20effective%20node%0Aand%20graph%20representations%20for%20various%20downstream%20tasks.%20Despite%20the%20recent%0Asurge%20in%20graph%20pooling%20approaches%2C%20there%20is%20a%20lack%20of%20standardized%20experimental%0Asettings%20and%20fair%20benchmarks%20to%20evaluate%20their%20performance.%20To%20address%20this%0Aissue%2C%20we%20have%20constructed%20a%20comprehensive%20benchmark%20that%20includes%2015%20graph%0Apooling%20methods%20and%2021%20different%20graph%20datasets.%20This%20benchmark%20systematically%0Aassesses%20the%20performance%20of%20graph%20pooling%20methods%20in%20three%20dimensions%2C%20i.e.%2C%0Aeffectiveness%2C%20robustness%2C%20and%20generalizability.%20We%20first%20evaluate%20the%0Aperformance%20of%20these%20graph%20pooling%20approaches%20across%20different%20tasks%20including%0Agraph%20classification%2C%20graph%20regression%20and%20node%20classification.%20Then%2C%20we%0Ainvestigate%20their%20performance%20under%20potential%20noise%20attacks%20and%0Aout-of-distribution%20shifts%20in%20real-world%20scenarios.%20We%20also%20involve%20detailed%0Aefficiency%20analysis%20and%20parameter%20analysis.%20Extensive%20experiments%20validate%20the%0Astrong%20capability%20and%20applicability%20of%20graph%20pooling%20approaches%20in%20various%0Ascenarios%2C%20which%20can%20provide%20valuable%20insights%20and%20guidance%20for%20deep%20geometric%0Alearning%20research.%20The%20source%20code%20of%20our%20benchmark%20is%20available%20at%0Ahttps%3A//github.com/goose315/Graph_Pooling_Benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Graph%2520Pooling%2520Benchmark%253A%2520Effectiveness%252C%2520Robustness%2520and%250A%2520%2520Generalizability%26entry.906535625%3DPengyun%2520Wang%2520and%2520Junyu%2520Luo%2520and%2520Yanxin%2520Shen%2520and%2520Siyu%2520Heng%2520and%2520Xiao%2520Luo%26entry.1292438233%3D%2520%2520Graph%2520pooling%2520has%2520gained%2520attention%2520for%2520its%2520ability%2520to%2520obtain%2520effective%2520node%250Aand%2520graph%2520representations%2520for%2520various%2520downstream%2520tasks.%2520Despite%2520the%2520recent%250Asurge%2520in%2520graph%2520pooling%2520approaches%252C%2520there%2520is%2520a%2520lack%2520of%2520standardized%2520experimental%250Asettings%2520and%2520fair%2520benchmarks%2520to%2520evaluate%2520their%2520performance.%2520To%2520address%2520this%250Aissue%252C%2520we%2520have%2520constructed%2520a%2520comprehensive%2520benchmark%2520that%2520includes%252015%2520graph%250Apooling%2520methods%2520and%252021%2520different%2520graph%2520datasets.%2520This%2520benchmark%2520systematically%250Aassesses%2520the%2520performance%2520of%2520graph%2520pooling%2520methods%2520in%2520three%2520dimensions%252C%2520i.e.%252C%250Aeffectiveness%252C%2520robustness%252C%2520and%2520generalizability.%2520We%2520first%2520evaluate%2520the%250Aperformance%2520of%2520these%2520graph%2520pooling%2520approaches%2520across%2520different%2520tasks%2520including%250Agraph%2520classification%252C%2520graph%2520regression%2520and%2520node%2520classification.%2520Then%252C%2520we%250Ainvestigate%2520their%2520performance%2520under%2520potential%2520noise%2520attacks%2520and%250Aout-of-distribution%2520shifts%2520in%2520real-world%2520scenarios.%2520We%2520also%2520involve%2520detailed%250Aefficiency%2520analysis%2520and%2520parameter%2520analysis.%2520Extensive%2520experiments%2520validate%2520the%250Astrong%2520capability%2520and%2520applicability%2520of%2520graph%2520pooling%2520approaches%2520in%2520various%250Ascenarios%252C%2520which%2520can%2520provide%2520valuable%2520insights%2520and%2520guidance%2520for%2520deep%2520geometric%250Alearning%2520research.%2520The%2520source%2520code%2520of%2520our%2520benchmark%2520is%2520available%2520at%250Ahttps%253A//github.com/goose315/Graph_Pooling_Benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Graph%20Pooling%20Benchmark%3A%20Effectiveness%2C%20Robustness%20and%0A%20%20Generalizability&entry.906535625=Pengyun%20Wang%20and%20Junyu%20Luo%20and%20Yanxin%20Shen%20and%20Siyu%20Heng%20and%20Xiao%20Luo&entry.1292438233=%20%20Graph%20pooling%20has%20gained%20attention%20for%20its%20ability%20to%20obtain%20effective%20node%0Aand%20graph%20representations%20for%20various%20downstream%20tasks.%20Despite%20the%20recent%0Asurge%20in%20graph%20pooling%20approaches%2C%20there%20is%20a%20lack%20of%20standardized%20experimental%0Asettings%20and%20fair%20benchmarks%20to%20evaluate%20their%20performance.%20To%20address%20this%0Aissue%2C%20we%20have%20constructed%20a%20comprehensive%20benchmark%20that%20includes%2015%20graph%0Apooling%20methods%20and%2021%20different%20graph%20datasets.%20This%20benchmark%20systematically%0Aassesses%20the%20performance%20of%20graph%20pooling%20methods%20in%20three%20dimensions%2C%20i.e.%2C%0Aeffectiveness%2C%20robustness%2C%20and%20generalizability.%20We%20first%20evaluate%20the%0Aperformance%20of%20these%20graph%20pooling%20approaches%20across%20different%20tasks%20including%0Agraph%20classification%2C%20graph%20regression%20and%20node%20classification.%20Then%2C%20we%0Ainvestigate%20their%20performance%20under%20potential%20noise%20attacks%20and%0Aout-of-distribution%20shifts%20in%20real-world%20scenarios.%20We%20also%20involve%20detailed%0Aefficiency%20analysis%20and%20parameter%20analysis.%20Extensive%20experiments%20validate%20the%0Astrong%20capability%20and%20applicability%20of%20graph%20pooling%20approaches%20in%20various%0Ascenarios%2C%20which%20can%20provide%20valuable%20insights%20and%20guidance%20for%20deep%20geometric%0Alearning%20research.%20The%20source%20code%20of%20our%20benchmark%20is%20available%20at%0Ahttps%3A//github.com/goose315/Graph_Pooling_Benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09031v1&entry.124074799=Read"},
{"title": "MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded\n  Language Annotations", "author": "Ruiyuan Lyu and Tai Wang and Jingli Lin and Shuai Yang and Xiaohan Mao and Yilun Chen and Runsen Xu and Haifeng Huang and Chenming Zhu and Dahua Lin and Jiangmiao Pang", "abstract": "  With the emergence of LLMs and their integration with other data modalities,\nmulti-modal 3D perception attracts more attention due to its connectivity to\nthe physical world and makes rapid progress. However, limited by existing\ndatasets, previous works mainly focus on understanding object properties or\ninter-object spatial relationships in a 3D scene. To tackle this problem, this\npaper builds the first largest ever multi-modal 3D scene dataset and benchmark\nwith hierarchical grounded language annotations, MMScan. It is constructed\nbased on a top-down logic, from region to object level, from a single target to\ninter-target relationships, covering holistic aspects of spatial and attribute\nunderstanding. The overall pipeline incorporates powerful VLMs via carefully\ndesigned prompts to initialize the annotations efficiently and further involve\nhumans' correction in the loop to ensure the annotations are natural, correct,\nand comprehensive. Built upon existing 3D scanning data, the resulting\nmulti-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects\nand 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding\nand question-answering benchmarks. We evaluate representative baselines on our\nbenchmarks, analyze their capabilities in different aspects, and showcase the\nkey problems to be addressed in the future. Furthermore, we use this\nhigh-quality dataset to train state-of-the-art 3D visual grounding and LLMs and\nobtain remarkable performance improvement both on existing benchmarks and\nin-the-wild evaluation. Codes, datasets, and benchmarks will be available at\nhttps://github.com/OpenRobotLab/EmbodiedScan.\n", "link": "http://arxiv.org/abs/2406.09401v1", "date": "2024-06-13", "relevancy": 2.3403, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6032}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.579}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMScan%3A%20A%20Multi-Modal%203D%20Scene%20Dataset%20with%20Hierarchical%20Grounded%0A%20%20Language%20Annotations&body=Title%3A%20MMScan%3A%20A%20Multi-Modal%203D%20Scene%20Dataset%20with%20Hierarchical%20Grounded%0A%20%20Language%20Annotations%0AAuthor%3A%20Ruiyuan%20Lyu%20and%20Tai%20Wang%20and%20Jingli%20Lin%20and%20Shuai%20Yang%20and%20Xiaohan%20Mao%20and%20Yilun%20Chen%20and%20Runsen%20Xu%20and%20Haifeng%20Huang%20and%20Chenming%20Zhu%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20LLMs%20and%20their%20integration%20with%20other%20data%20modalities%2C%0Amulti-modal%203D%20perception%20attracts%20more%20attention%20due%20to%20its%20connectivity%20to%0Athe%20physical%20world%20and%20makes%20rapid%20progress.%20However%2C%20limited%20by%20existing%0Adatasets%2C%20previous%20works%20mainly%20focus%20on%20understanding%20object%20properties%20or%0Ainter-object%20spatial%20relationships%20in%20a%203D%20scene.%20To%20tackle%20this%20problem%2C%20this%0Apaper%20builds%20the%20first%20largest%20ever%20multi-modal%203D%20scene%20dataset%20and%20benchmark%0Awith%20hierarchical%20grounded%20language%20annotations%2C%20MMScan.%20It%20is%20constructed%0Abased%20on%20a%20top-down%20logic%2C%20from%20region%20to%20object%20level%2C%20from%20a%20single%20target%20to%0Ainter-target%20relationships%2C%20covering%20holistic%20aspects%20of%20spatial%20and%20attribute%0Aunderstanding.%20The%20overall%20pipeline%20incorporates%20powerful%20VLMs%20via%20carefully%0Adesigned%20prompts%20to%20initialize%20the%20annotations%20efficiently%20and%20further%20involve%0Ahumans%27%20correction%20in%20the%20loop%20to%20ensure%20the%20annotations%20are%20natural%2C%20correct%2C%0Aand%20comprehensive.%20Built%20upon%20existing%203D%20scanning%20data%2C%20the%20resulting%0Amulti-modal%203D%20dataset%20encompasses%201.4M%20meta-annotated%20captions%20on%20109k%20objects%0Aand%207.7k%20regions%20as%20well%20as%20over%203.04M%20diverse%20samples%20for%203D%20visual%20grounding%0Aand%20question-answering%20benchmarks.%20We%20evaluate%20representative%20baselines%20on%20our%0Abenchmarks%2C%20analyze%20their%20capabilities%20in%20different%20aspects%2C%20and%20showcase%20the%0Akey%20problems%20to%20be%20addressed%20in%20the%20future.%20Furthermore%2C%20we%20use%20this%0Ahigh-quality%20dataset%20to%20train%20state-of-the-art%203D%20visual%20grounding%20and%20LLMs%20and%0Aobtain%20remarkable%20performance%20improvement%20both%20on%20existing%20benchmarks%20and%0Ain-the-wild%20evaluation.%20Codes%2C%20datasets%2C%20and%20benchmarks%20will%20be%20available%20at%0Ahttps%3A//github.com/OpenRobotLab/EmbodiedScan.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMScan%253A%2520A%2520Multi-Modal%25203D%2520Scene%2520Dataset%2520with%2520Hierarchical%2520Grounded%250A%2520%2520Language%2520Annotations%26entry.906535625%3DRuiyuan%2520Lyu%2520and%2520Tai%2520Wang%2520and%2520Jingli%2520Lin%2520and%2520Shuai%2520Yang%2520and%2520Xiaohan%2520Mao%2520and%2520Yilun%2520Chen%2520and%2520Runsen%2520Xu%2520and%2520Haifeng%2520Huang%2520and%2520Chenming%2520Zhu%2520and%2520Dahua%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520LLMs%2520and%2520their%2520integration%2520with%2520other%2520data%2520modalities%252C%250Amulti-modal%25203D%2520perception%2520attracts%2520more%2520attention%2520due%2520to%2520its%2520connectivity%2520to%250Athe%2520physical%2520world%2520and%2520makes%2520rapid%2520progress.%2520However%252C%2520limited%2520by%2520existing%250Adatasets%252C%2520previous%2520works%2520mainly%2520focus%2520on%2520understanding%2520object%2520properties%2520or%250Ainter-object%2520spatial%2520relationships%2520in%2520a%25203D%2520scene.%2520To%2520tackle%2520this%2520problem%252C%2520this%250Apaper%2520builds%2520the%2520first%2520largest%2520ever%2520multi-modal%25203D%2520scene%2520dataset%2520and%2520benchmark%250Awith%2520hierarchical%2520grounded%2520language%2520annotations%252C%2520MMScan.%2520It%2520is%2520constructed%250Abased%2520on%2520a%2520top-down%2520logic%252C%2520from%2520region%2520to%2520object%2520level%252C%2520from%2520a%2520single%2520target%2520to%250Ainter-target%2520relationships%252C%2520covering%2520holistic%2520aspects%2520of%2520spatial%2520and%2520attribute%250Aunderstanding.%2520The%2520overall%2520pipeline%2520incorporates%2520powerful%2520VLMs%2520via%2520carefully%250Adesigned%2520prompts%2520to%2520initialize%2520the%2520annotations%2520efficiently%2520and%2520further%2520involve%250Ahumans%2527%2520correction%2520in%2520the%2520loop%2520to%2520ensure%2520the%2520annotations%2520are%2520natural%252C%2520correct%252C%250Aand%2520comprehensive.%2520Built%2520upon%2520existing%25203D%2520scanning%2520data%252C%2520the%2520resulting%250Amulti-modal%25203D%2520dataset%2520encompasses%25201.4M%2520meta-annotated%2520captions%2520on%2520109k%2520objects%250Aand%25207.7k%2520regions%2520as%2520well%2520as%2520over%25203.04M%2520diverse%2520samples%2520for%25203D%2520visual%2520grounding%250Aand%2520question-answering%2520benchmarks.%2520We%2520evaluate%2520representative%2520baselines%2520on%2520our%250Abenchmarks%252C%2520analyze%2520their%2520capabilities%2520in%2520different%2520aspects%252C%2520and%2520showcase%2520the%250Akey%2520problems%2520to%2520be%2520addressed%2520in%2520the%2520future.%2520Furthermore%252C%2520we%2520use%2520this%250Ahigh-quality%2520dataset%2520to%2520train%2520state-of-the-art%25203D%2520visual%2520grounding%2520and%2520LLMs%2520and%250Aobtain%2520remarkable%2520performance%2520improvement%2520both%2520on%2520existing%2520benchmarks%2520and%250Ain-the-wild%2520evaluation.%2520Codes%252C%2520datasets%252C%2520and%2520benchmarks%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/OpenRobotLab/EmbodiedScan.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMScan%3A%20A%20Multi-Modal%203D%20Scene%20Dataset%20with%20Hierarchical%20Grounded%0A%20%20Language%20Annotations&entry.906535625=Ruiyuan%20Lyu%20and%20Tai%20Wang%20and%20Jingli%20Lin%20and%20Shuai%20Yang%20and%20Xiaohan%20Mao%20and%20Yilun%20Chen%20and%20Runsen%20Xu%20and%20Haifeng%20Huang%20and%20Chenming%20Zhu%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20With%20the%20emergence%20of%20LLMs%20and%20their%20integration%20with%20other%20data%20modalities%2C%0Amulti-modal%203D%20perception%20attracts%20more%20attention%20due%20to%20its%20connectivity%20to%0Athe%20physical%20world%20and%20makes%20rapid%20progress.%20However%2C%20limited%20by%20existing%0Adatasets%2C%20previous%20works%20mainly%20focus%20on%20understanding%20object%20properties%20or%0Ainter-object%20spatial%20relationships%20in%20a%203D%20scene.%20To%20tackle%20this%20problem%2C%20this%0Apaper%20builds%20the%20first%20largest%20ever%20multi-modal%203D%20scene%20dataset%20and%20benchmark%0Awith%20hierarchical%20grounded%20language%20annotations%2C%20MMScan.%20It%20is%20constructed%0Abased%20on%20a%20top-down%20logic%2C%20from%20region%20to%20object%20level%2C%20from%20a%20single%20target%20to%0Ainter-target%20relationships%2C%20covering%20holistic%20aspects%20of%20spatial%20and%20attribute%0Aunderstanding.%20The%20overall%20pipeline%20incorporates%20powerful%20VLMs%20via%20carefully%0Adesigned%20prompts%20to%20initialize%20the%20annotations%20efficiently%20and%20further%20involve%0Ahumans%27%20correction%20in%20the%20loop%20to%20ensure%20the%20annotations%20are%20natural%2C%20correct%2C%0Aand%20comprehensive.%20Built%20upon%20existing%203D%20scanning%20data%2C%20the%20resulting%0Amulti-modal%203D%20dataset%20encompasses%201.4M%20meta-annotated%20captions%20on%20109k%20objects%0Aand%207.7k%20regions%20as%20well%20as%20over%203.04M%20diverse%20samples%20for%203D%20visual%20grounding%0Aand%20question-answering%20benchmarks.%20We%20evaluate%20representative%20baselines%20on%20our%0Abenchmarks%2C%20analyze%20their%20capabilities%20in%20different%20aspects%2C%20and%20showcase%20the%0Akey%20problems%20to%20be%20addressed%20in%20the%20future.%20Furthermore%2C%20we%20use%20this%0Ahigh-quality%20dataset%20to%20train%20state-of-the-art%203D%20visual%20grounding%20and%20LLMs%20and%0Aobtain%20remarkable%20performance%20improvement%20both%20on%20existing%20benchmarks%20and%0Ain-the-wild%20evaluation.%20Codes%2C%20datasets%2C%20and%20benchmarks%20will%20be%20available%20at%0Ahttps%3A//github.com/OpenRobotLab/EmbodiedScan.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09401v1&entry.124074799=Read"},
{"title": "Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven\n  Text-to-Image Generation", "author": "Yufan Zhou and Ruiyi Zhang and Kaizhi Zheng and Nanxuan Zhao and Jiuxiang Gu and Zichao Wang and Xin Eric Wang and Tong Sun", "abstract": "  In subject-driven text-to-image generation, recent works have achieved\nsuperior performance by training the model on synthetic datasets containing\nnumerous image pairs. Trained on these datasets, generative models can produce\ntext-aligned images for specific subject from arbitrary testing image in a\nzero-shot manner. They even outperform methods which require additional\nfine-tuning on testing images. However, the cost of creating such datasets is\nprohibitive for most researchers. To generate a single training pair, current\nmethods fine-tune a pre-trained text-to-image model on the subject image to\ncapture fine-grained details, then use the fine-tuned model to create images\nfor the same subject based on creative text prompts. Consequently, constructing\na large-scale dataset with millions of subjects can require hundreds of\nthousands of GPU hours. To tackle this problem, we propose Toffee, an efficient\nmethod to construct datasets for subject-driven editing and generation.\nSpecifically, our dataset construction does not need any subject-level\nfine-tuning. After pre-training two generative models, we are able to generate\ninfinite number of high-quality samples. We construct the first large-scale\ndataset for subject-driven image editing and generation, which contains 5\nmillion image pairs, text prompts, and masks. Our dataset is 5 times the size\nof previous largest dataset, yet our cost is tens of thousands of GPU hours\nlower. To test the proposed dataset, we also propose a model which is capable\nof both subject-driven image editing and generation. By simply training the\nmodel on our proposed dataset, it obtains competitive results, illustrating the\neffectiveness of the proposed dataset construction framework.\n", "link": "http://arxiv.org/abs/2406.09305v1", "date": "2024-06-13", "relevancy": 2.3368, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6099}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5669}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toffee%3A%20Efficient%20Million-Scale%20Dataset%20Construction%20for%20Subject-Driven%0A%20%20Text-to-Image%20Generation&body=Title%3A%20Toffee%3A%20Efficient%20Million-Scale%20Dataset%20Construction%20for%20Subject-Driven%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Yufan%20Zhou%20and%20Ruiyi%20Zhang%20and%20Kaizhi%20Zheng%20and%20Nanxuan%20Zhao%20and%20Jiuxiang%20Gu%20and%20Zichao%20Wang%20and%20Xin%20Eric%20Wang%20and%20Tong%20Sun%0AAbstract%3A%20%20%20In%20subject-driven%20text-to-image%20generation%2C%20recent%20works%20have%20achieved%0Asuperior%20performance%20by%20training%20the%20model%20on%20synthetic%20datasets%20containing%0Anumerous%20image%20pairs.%20Trained%20on%20these%20datasets%2C%20generative%20models%20can%20produce%0Atext-aligned%20images%20for%20specific%20subject%20from%20arbitrary%20testing%20image%20in%20a%0Azero-shot%20manner.%20They%20even%20outperform%20methods%20which%20require%20additional%0Afine-tuning%20on%20testing%20images.%20However%2C%20the%20cost%20of%20creating%20such%20datasets%20is%0Aprohibitive%20for%20most%20researchers.%20To%20generate%20a%20single%20training%20pair%2C%20current%0Amethods%20fine-tune%20a%20pre-trained%20text-to-image%20model%20on%20the%20subject%20image%20to%0Acapture%20fine-grained%20details%2C%20then%20use%20the%20fine-tuned%20model%20to%20create%20images%0Afor%20the%20same%20subject%20based%20on%20creative%20text%20prompts.%20Consequently%2C%20constructing%0Aa%20large-scale%20dataset%20with%20millions%20of%20subjects%20can%20require%20hundreds%20of%0Athousands%20of%20GPU%20hours.%20To%20tackle%20this%20problem%2C%20we%20propose%20Toffee%2C%20an%20efficient%0Amethod%20to%20construct%20datasets%20for%20subject-driven%20editing%20and%20generation.%0ASpecifically%2C%20our%20dataset%20construction%20does%20not%20need%20any%20subject-level%0Afine-tuning.%20After%20pre-training%20two%20generative%20models%2C%20we%20are%20able%20to%20generate%0Ainfinite%20number%20of%20high-quality%20samples.%20We%20construct%20the%20first%20large-scale%0Adataset%20for%20subject-driven%20image%20editing%20and%20generation%2C%20which%20contains%205%0Amillion%20image%20pairs%2C%20text%20prompts%2C%20and%20masks.%20Our%20dataset%20is%205%20times%20the%20size%0Aof%20previous%20largest%20dataset%2C%20yet%20our%20cost%20is%20tens%20of%20thousands%20of%20GPU%20hours%0Alower.%20To%20test%20the%20proposed%20dataset%2C%20we%20also%20propose%20a%20model%20which%20is%20capable%0Aof%20both%20subject-driven%20image%20editing%20and%20generation.%20By%20simply%20training%20the%0Amodel%20on%20our%20proposed%20dataset%2C%20it%20obtains%20competitive%20results%2C%20illustrating%20the%0Aeffectiveness%20of%20the%20proposed%20dataset%20construction%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToffee%253A%2520Efficient%2520Million-Scale%2520Dataset%2520Construction%2520for%2520Subject-Driven%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DYufan%2520Zhou%2520and%2520Ruiyi%2520Zhang%2520and%2520Kaizhi%2520Zheng%2520and%2520Nanxuan%2520Zhao%2520and%2520Jiuxiang%2520Gu%2520and%2520Zichao%2520Wang%2520and%2520Xin%2520Eric%2520Wang%2520and%2520Tong%2520Sun%26entry.1292438233%3D%2520%2520In%2520subject-driven%2520text-to-image%2520generation%252C%2520recent%2520works%2520have%2520achieved%250Asuperior%2520performance%2520by%2520training%2520the%2520model%2520on%2520synthetic%2520datasets%2520containing%250Anumerous%2520image%2520pairs.%2520Trained%2520on%2520these%2520datasets%252C%2520generative%2520models%2520can%2520produce%250Atext-aligned%2520images%2520for%2520specific%2520subject%2520from%2520arbitrary%2520testing%2520image%2520in%2520a%250Azero-shot%2520manner.%2520They%2520even%2520outperform%2520methods%2520which%2520require%2520additional%250Afine-tuning%2520on%2520testing%2520images.%2520However%252C%2520the%2520cost%2520of%2520creating%2520such%2520datasets%2520is%250Aprohibitive%2520for%2520most%2520researchers.%2520To%2520generate%2520a%2520single%2520training%2520pair%252C%2520current%250Amethods%2520fine-tune%2520a%2520pre-trained%2520text-to-image%2520model%2520on%2520the%2520subject%2520image%2520to%250Acapture%2520fine-grained%2520details%252C%2520then%2520use%2520the%2520fine-tuned%2520model%2520to%2520create%2520images%250Afor%2520the%2520same%2520subject%2520based%2520on%2520creative%2520text%2520prompts.%2520Consequently%252C%2520constructing%250Aa%2520large-scale%2520dataset%2520with%2520millions%2520of%2520subjects%2520can%2520require%2520hundreds%2520of%250Athousands%2520of%2520GPU%2520hours.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520Toffee%252C%2520an%2520efficient%250Amethod%2520to%2520construct%2520datasets%2520for%2520subject-driven%2520editing%2520and%2520generation.%250ASpecifically%252C%2520our%2520dataset%2520construction%2520does%2520not%2520need%2520any%2520subject-level%250Afine-tuning.%2520After%2520pre-training%2520two%2520generative%2520models%252C%2520we%2520are%2520able%2520to%2520generate%250Ainfinite%2520number%2520of%2520high-quality%2520samples.%2520We%2520construct%2520the%2520first%2520large-scale%250Adataset%2520for%2520subject-driven%2520image%2520editing%2520and%2520generation%252C%2520which%2520contains%25205%250Amillion%2520image%2520pairs%252C%2520text%2520prompts%252C%2520and%2520masks.%2520Our%2520dataset%2520is%25205%2520times%2520the%2520size%250Aof%2520previous%2520largest%2520dataset%252C%2520yet%2520our%2520cost%2520is%2520tens%2520of%2520thousands%2520of%2520GPU%2520hours%250Alower.%2520To%2520test%2520the%2520proposed%2520dataset%252C%2520we%2520also%2520propose%2520a%2520model%2520which%2520is%2520capable%250Aof%2520both%2520subject-driven%2520image%2520editing%2520and%2520generation.%2520By%2520simply%2520training%2520the%250Amodel%2520on%2520our%2520proposed%2520dataset%252C%2520it%2520obtains%2520competitive%2520results%252C%2520illustrating%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520dataset%2520construction%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toffee%3A%20Efficient%20Million-Scale%20Dataset%20Construction%20for%20Subject-Driven%0A%20%20Text-to-Image%20Generation&entry.906535625=Yufan%20Zhou%20and%20Ruiyi%20Zhang%20and%20Kaizhi%20Zheng%20and%20Nanxuan%20Zhao%20and%20Jiuxiang%20Gu%20and%20Zichao%20Wang%20and%20Xin%20Eric%20Wang%20and%20Tong%20Sun&entry.1292438233=%20%20In%20subject-driven%20text-to-image%20generation%2C%20recent%20works%20have%20achieved%0Asuperior%20performance%20by%20training%20the%20model%20on%20synthetic%20datasets%20containing%0Anumerous%20image%20pairs.%20Trained%20on%20these%20datasets%2C%20generative%20models%20can%20produce%0Atext-aligned%20images%20for%20specific%20subject%20from%20arbitrary%20testing%20image%20in%20a%0Azero-shot%20manner.%20They%20even%20outperform%20methods%20which%20require%20additional%0Afine-tuning%20on%20testing%20images.%20However%2C%20the%20cost%20of%20creating%20such%20datasets%20is%0Aprohibitive%20for%20most%20researchers.%20To%20generate%20a%20single%20training%20pair%2C%20current%0Amethods%20fine-tune%20a%20pre-trained%20text-to-image%20model%20on%20the%20subject%20image%20to%0Acapture%20fine-grained%20details%2C%20then%20use%20the%20fine-tuned%20model%20to%20create%20images%0Afor%20the%20same%20subject%20based%20on%20creative%20text%20prompts.%20Consequently%2C%20constructing%0Aa%20large-scale%20dataset%20with%20millions%20of%20subjects%20can%20require%20hundreds%20of%0Athousands%20of%20GPU%20hours.%20To%20tackle%20this%20problem%2C%20we%20propose%20Toffee%2C%20an%20efficient%0Amethod%20to%20construct%20datasets%20for%20subject-driven%20editing%20and%20generation.%0ASpecifically%2C%20our%20dataset%20construction%20does%20not%20need%20any%20subject-level%0Afine-tuning.%20After%20pre-training%20two%20generative%20models%2C%20we%20are%20able%20to%20generate%0Ainfinite%20number%20of%20high-quality%20samples.%20We%20construct%20the%20first%20large-scale%0Adataset%20for%20subject-driven%20image%20editing%20and%20generation%2C%20which%20contains%205%0Amillion%20image%20pairs%2C%20text%20prompts%2C%20and%20masks.%20Our%20dataset%20is%205%20times%20the%20size%0Aof%20previous%20largest%20dataset%2C%20yet%20our%20cost%20is%20tens%20of%20thousands%20of%20GPU%20hours%0Alower.%20To%20test%20the%20proposed%20dataset%2C%20we%20also%20propose%20a%20model%20which%20is%20capable%0Aof%20both%20subject-driven%20image%20editing%20and%20generation.%20By%20simply%20training%20the%0Amodel%20on%20our%20proposed%20dataset%2C%20it%20obtains%20competitive%20results%2C%20illustrating%20the%0Aeffectiveness%20of%20the%20proposed%20dataset%20construction%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09305v1&entry.124074799=Read"},
{"title": "Improved Stability and Generalization Guarantees of the Decentralized\n  SGD Algorithm", "author": "Batiste Le Bars and Aur\u00e9lien Bellet and Marc Tommasi and Kevin Scaman and Giovanni Neglia", "abstract": "  This paper presents a new generalization error analysis for Decentralized\nStochastic Gradient Descent (D-SGD) based on algorithmic stability. The\nobtained results overhaul a series of recent works that suggested an increased\ninstability due to decentralization and a detrimental impact of\npoorly-connected communication graphs on generalization. On the contrary, we\nshow, for convex, strongly convex and non-convex functions, that D-SGD can\nalways recover generalization bounds analogous to those of classical SGD,\nsuggesting that the choice of graph does not matter. We then argue that this\nresult is coming from a worst-case analysis, and we provide a refined\noptimization-dependent generalization bound for general convex functions. This\nnew bound reveals that the choice of graph can in fact improve the worst-case\nbound in certain regimes, and that surprisingly, a poorly-connected graph can\neven be beneficial for generalization.\n", "link": "http://arxiv.org/abs/2306.02939v4", "date": "2024-06-13", "relevancy": 2.3335, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4711}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4646}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Stability%20and%20Generalization%20Guarantees%20of%20the%20Decentralized%0A%20%20SGD%20Algorithm&body=Title%3A%20Improved%20Stability%20and%20Generalization%20Guarantees%20of%20the%20Decentralized%0A%20%20SGD%20Algorithm%0AAuthor%3A%20Batiste%20Le%20Bars%20and%20Aur%C3%A9lien%20Bellet%20and%20Marc%20Tommasi%20and%20Kevin%20Scaman%20and%20Giovanni%20Neglia%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20generalization%20error%20analysis%20for%20Decentralized%0AStochastic%20Gradient%20Descent%20%28D-SGD%29%20based%20on%20algorithmic%20stability.%20The%0Aobtained%20results%20overhaul%20a%20series%20of%20recent%20works%20that%20suggested%20an%20increased%0Ainstability%20due%20to%20decentralization%20and%20a%20detrimental%20impact%20of%0Apoorly-connected%20communication%20graphs%20on%20generalization.%20On%20the%20contrary%2C%20we%0Ashow%2C%20for%20convex%2C%20strongly%20convex%20and%20non-convex%20functions%2C%20that%20D-SGD%20can%0Aalways%20recover%20generalization%20bounds%20analogous%20to%20those%20of%20classical%20SGD%2C%0Asuggesting%20that%20the%20choice%20of%20graph%20does%20not%20matter.%20We%20then%20argue%20that%20this%0Aresult%20is%20coming%20from%20a%20worst-case%20analysis%2C%20and%20we%20provide%20a%20refined%0Aoptimization-dependent%20generalization%20bound%20for%20general%20convex%20functions.%20This%0Anew%20bound%20reveals%20that%20the%20choice%20of%20graph%20can%20in%20fact%20improve%20the%20worst-case%0Abound%20in%20certain%20regimes%2C%20and%20that%20surprisingly%2C%20a%20poorly-connected%20graph%20can%0Aeven%20be%20beneficial%20for%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02939v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Stability%2520and%2520Generalization%2520Guarantees%2520of%2520the%2520Decentralized%250A%2520%2520SGD%2520Algorithm%26entry.906535625%3DBatiste%2520Le%2520Bars%2520and%2520Aur%25C3%25A9lien%2520Bellet%2520and%2520Marc%2520Tommasi%2520and%2520Kevin%2520Scaman%2520and%2520Giovanni%2520Neglia%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520generalization%2520error%2520analysis%2520for%2520Decentralized%250AStochastic%2520Gradient%2520Descent%2520%2528D-SGD%2529%2520based%2520on%2520algorithmic%2520stability.%2520The%250Aobtained%2520results%2520overhaul%2520a%2520series%2520of%2520recent%2520works%2520that%2520suggested%2520an%2520increased%250Ainstability%2520due%2520to%2520decentralization%2520and%2520a%2520detrimental%2520impact%2520of%250Apoorly-connected%2520communication%2520graphs%2520on%2520generalization.%2520On%2520the%2520contrary%252C%2520we%250Ashow%252C%2520for%2520convex%252C%2520strongly%2520convex%2520and%2520non-convex%2520functions%252C%2520that%2520D-SGD%2520can%250Aalways%2520recover%2520generalization%2520bounds%2520analogous%2520to%2520those%2520of%2520classical%2520SGD%252C%250Asuggesting%2520that%2520the%2520choice%2520of%2520graph%2520does%2520not%2520matter.%2520We%2520then%2520argue%2520that%2520this%250Aresult%2520is%2520coming%2520from%2520a%2520worst-case%2520analysis%252C%2520and%2520we%2520provide%2520a%2520refined%250Aoptimization-dependent%2520generalization%2520bound%2520for%2520general%2520convex%2520functions.%2520This%250Anew%2520bound%2520reveals%2520that%2520the%2520choice%2520of%2520graph%2520can%2520in%2520fact%2520improve%2520the%2520worst-case%250Abound%2520in%2520certain%2520regimes%252C%2520and%2520that%2520surprisingly%252C%2520a%2520poorly-connected%2520graph%2520can%250Aeven%2520be%2520beneficial%2520for%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02939v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Stability%20and%20Generalization%20Guarantees%20of%20the%20Decentralized%0A%20%20SGD%20Algorithm&entry.906535625=Batiste%20Le%20Bars%20and%20Aur%C3%A9lien%20Bellet%20and%20Marc%20Tommasi%20and%20Kevin%20Scaman%20and%20Giovanni%20Neglia&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20generalization%20error%20analysis%20for%20Decentralized%0AStochastic%20Gradient%20Descent%20%28D-SGD%29%20based%20on%20algorithmic%20stability.%20The%0Aobtained%20results%20overhaul%20a%20series%20of%20recent%20works%20that%20suggested%20an%20increased%0Ainstability%20due%20to%20decentralization%20and%20a%20detrimental%20impact%20of%0Apoorly-connected%20communication%20graphs%20on%20generalization.%20On%20the%20contrary%2C%20we%0Ashow%2C%20for%20convex%2C%20strongly%20convex%20and%20non-convex%20functions%2C%20that%20D-SGD%20can%0Aalways%20recover%20generalization%20bounds%20analogous%20to%20those%20of%20classical%20SGD%2C%0Asuggesting%20that%20the%20choice%20of%20graph%20does%20not%20matter.%20We%20then%20argue%20that%20this%0Aresult%20is%20coming%20from%20a%20worst-case%20analysis%2C%20and%20we%20provide%20a%20refined%0Aoptimization-dependent%20generalization%20bound%20for%20general%20convex%20functions.%20This%0Anew%20bound%20reveals%20that%20the%20choice%20of%20graph%20can%20in%20fact%20improve%20the%20worst-case%0Abound%20in%20certain%20regimes%2C%20and%20that%20surprisingly%2C%20a%20poorly-connected%20graph%20can%0Aeven%20be%20beneficial%20for%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02939v4&entry.124074799=Read"},
{"title": "A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products\n  and Graph Coarsening", "author": "Guy Bar-Shalom and Yam Eitan and Fabrizio Frasca and Haggai Maron", "abstract": "  Subgraph Graph Neural Networks (Subgraph GNNs) enhance the expressivity of\nmessage-passing GNNs by representing graphs as sets of subgraphs. They have\nshown impressive performance on several tasks, but their complexity limits\napplications to larger graphs. Previous approaches suggested processing only\nsubsets of subgraphs, selected either randomly or via learnable sampling.\nHowever, they make suboptimal subgraph selections or can only cope with very\nsmall subset sizes, inevitably incurring performance degradation. This paper\nintroduces a new Subgraph GNNs framework to address these issues. We employ a\ngraph coarsening function to cluster nodes into super-nodes with induced\nconnectivity. The product between the coarsened and the original graph reveals\nan implicit structure whereby subgraphs are associated with specific sets of\nnodes. By running generalized message-passing on such graph product, our method\neffectively implements an efficient, yet powerful Subgraph GNN. Controlling the\ncoarsening function enables meaningful selection of any number of subgraphs\nwhile, contrary to previous methods, being fully compatible with standard\ntraining techniques. Notably, we discover that the resulting node feature\ntensor exhibits new, unexplored permutation symmetries. We leverage this\nstructure, characterize the associated linear equivariant layers and\nincorporate them into the layers of our Subgraph GNN architecture. Extensive\nexperiments on multiple graph learning benchmarks demonstrate that our method\nis significantly more flexible than previous approaches, as it can seamlessly\nhandle any number of subgraphs, while consistently outperforming baseline\napproaches.\n", "link": "http://arxiv.org/abs/2406.09291v1", "date": "2024-06-13", "relevancy": 2.3294, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.472}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.469}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Flexible%2C%20Equivariant%20Framework%20for%20Subgraph%20GNNs%20via%20Graph%20Products%0A%20%20and%20Graph%20Coarsening&body=Title%3A%20A%20Flexible%2C%20Equivariant%20Framework%20for%20Subgraph%20GNNs%20via%20Graph%20Products%0A%20%20and%20Graph%20Coarsening%0AAuthor%3A%20Guy%20Bar-Shalom%20and%20Yam%20Eitan%20and%20Fabrizio%20Frasca%20and%20Haggai%20Maron%0AAbstract%3A%20%20%20Subgraph%20Graph%20Neural%20Networks%20%28Subgraph%20GNNs%29%20enhance%20the%20expressivity%20of%0Amessage-passing%20GNNs%20by%20representing%20graphs%20as%20sets%20of%20subgraphs.%20They%20have%0Ashown%20impressive%20performance%20on%20several%20tasks%2C%20but%20their%20complexity%20limits%0Aapplications%20to%20larger%20graphs.%20Previous%20approaches%20suggested%20processing%20only%0Asubsets%20of%20subgraphs%2C%20selected%20either%20randomly%20or%20via%20learnable%20sampling.%0AHowever%2C%20they%20make%20suboptimal%20subgraph%20selections%20or%20can%20only%20cope%20with%20very%0Asmall%20subset%20sizes%2C%20inevitably%20incurring%20performance%20degradation.%20This%20paper%0Aintroduces%20a%20new%20Subgraph%20GNNs%20framework%20to%20address%20these%20issues.%20We%20employ%20a%0Agraph%20coarsening%20function%20to%20cluster%20nodes%20into%20super-nodes%20with%20induced%0Aconnectivity.%20The%20product%20between%20the%20coarsened%20and%20the%20original%20graph%20reveals%0Aan%20implicit%20structure%20whereby%20subgraphs%20are%20associated%20with%20specific%20sets%20of%0Anodes.%20By%20running%20generalized%20message-passing%20on%20such%20graph%20product%2C%20our%20method%0Aeffectively%20implements%20an%20efficient%2C%20yet%20powerful%20Subgraph%20GNN.%20Controlling%20the%0Acoarsening%20function%20enables%20meaningful%20selection%20of%20any%20number%20of%20subgraphs%0Awhile%2C%20contrary%20to%20previous%20methods%2C%20being%20fully%20compatible%20with%20standard%0Atraining%20techniques.%20Notably%2C%20we%20discover%20that%20the%20resulting%20node%20feature%0Atensor%20exhibits%20new%2C%20unexplored%20permutation%20symmetries.%20We%20leverage%20this%0Astructure%2C%20characterize%20the%20associated%20linear%20equivariant%20layers%20and%0Aincorporate%20them%20into%20the%20layers%20of%20our%20Subgraph%20GNN%20architecture.%20Extensive%0Aexperiments%20on%20multiple%20graph%20learning%20benchmarks%20demonstrate%20that%20our%20method%0Ais%20significantly%20more%20flexible%20than%20previous%20approaches%2C%20as%20it%20can%20seamlessly%0Ahandle%20any%20number%20of%20subgraphs%2C%20while%20consistently%20outperforming%20baseline%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Flexible%252C%2520Equivariant%2520Framework%2520for%2520Subgraph%2520GNNs%2520via%2520Graph%2520Products%250A%2520%2520and%2520Graph%2520Coarsening%26entry.906535625%3DGuy%2520Bar-Shalom%2520and%2520Yam%2520Eitan%2520and%2520Fabrizio%2520Frasca%2520and%2520Haggai%2520Maron%26entry.1292438233%3D%2520%2520Subgraph%2520Graph%2520Neural%2520Networks%2520%2528Subgraph%2520GNNs%2529%2520enhance%2520the%2520expressivity%2520of%250Amessage-passing%2520GNNs%2520by%2520representing%2520graphs%2520as%2520sets%2520of%2520subgraphs.%2520They%2520have%250Ashown%2520impressive%2520performance%2520on%2520several%2520tasks%252C%2520but%2520their%2520complexity%2520limits%250Aapplications%2520to%2520larger%2520graphs.%2520Previous%2520approaches%2520suggested%2520processing%2520only%250Asubsets%2520of%2520subgraphs%252C%2520selected%2520either%2520randomly%2520or%2520via%2520learnable%2520sampling.%250AHowever%252C%2520they%2520make%2520suboptimal%2520subgraph%2520selections%2520or%2520can%2520only%2520cope%2520with%2520very%250Asmall%2520subset%2520sizes%252C%2520inevitably%2520incurring%2520performance%2520degradation.%2520This%2520paper%250Aintroduces%2520a%2520new%2520Subgraph%2520GNNs%2520framework%2520to%2520address%2520these%2520issues.%2520We%2520employ%2520a%250Agraph%2520coarsening%2520function%2520to%2520cluster%2520nodes%2520into%2520super-nodes%2520with%2520induced%250Aconnectivity.%2520The%2520product%2520between%2520the%2520coarsened%2520and%2520the%2520original%2520graph%2520reveals%250Aan%2520implicit%2520structure%2520whereby%2520subgraphs%2520are%2520associated%2520with%2520specific%2520sets%2520of%250Anodes.%2520By%2520running%2520generalized%2520message-passing%2520on%2520such%2520graph%2520product%252C%2520our%2520method%250Aeffectively%2520implements%2520an%2520efficient%252C%2520yet%2520powerful%2520Subgraph%2520GNN.%2520Controlling%2520the%250Acoarsening%2520function%2520enables%2520meaningful%2520selection%2520of%2520any%2520number%2520of%2520subgraphs%250Awhile%252C%2520contrary%2520to%2520previous%2520methods%252C%2520being%2520fully%2520compatible%2520with%2520standard%250Atraining%2520techniques.%2520Notably%252C%2520we%2520discover%2520that%2520the%2520resulting%2520node%2520feature%250Atensor%2520exhibits%2520new%252C%2520unexplored%2520permutation%2520symmetries.%2520We%2520leverage%2520this%250Astructure%252C%2520characterize%2520the%2520associated%2520linear%2520equivariant%2520layers%2520and%250Aincorporate%2520them%2520into%2520the%2520layers%2520of%2520our%2520Subgraph%2520GNN%2520architecture.%2520Extensive%250Aexperiments%2520on%2520multiple%2520graph%2520learning%2520benchmarks%2520demonstrate%2520that%2520our%2520method%250Ais%2520significantly%2520more%2520flexible%2520than%2520previous%2520approaches%252C%2520as%2520it%2520can%2520seamlessly%250Ahandle%2520any%2520number%2520of%2520subgraphs%252C%2520while%2520consistently%2520outperforming%2520baseline%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Flexible%2C%20Equivariant%20Framework%20for%20Subgraph%20GNNs%20via%20Graph%20Products%0A%20%20and%20Graph%20Coarsening&entry.906535625=Guy%20Bar-Shalom%20and%20Yam%20Eitan%20and%20Fabrizio%20Frasca%20and%20Haggai%20Maron&entry.1292438233=%20%20Subgraph%20Graph%20Neural%20Networks%20%28Subgraph%20GNNs%29%20enhance%20the%20expressivity%20of%0Amessage-passing%20GNNs%20by%20representing%20graphs%20as%20sets%20of%20subgraphs.%20They%20have%0Ashown%20impressive%20performance%20on%20several%20tasks%2C%20but%20their%20complexity%20limits%0Aapplications%20to%20larger%20graphs.%20Previous%20approaches%20suggested%20processing%20only%0Asubsets%20of%20subgraphs%2C%20selected%20either%20randomly%20or%20via%20learnable%20sampling.%0AHowever%2C%20they%20make%20suboptimal%20subgraph%20selections%20or%20can%20only%20cope%20with%20very%0Asmall%20subset%20sizes%2C%20inevitably%20incurring%20performance%20degradation.%20This%20paper%0Aintroduces%20a%20new%20Subgraph%20GNNs%20framework%20to%20address%20these%20issues.%20We%20employ%20a%0Agraph%20coarsening%20function%20to%20cluster%20nodes%20into%20super-nodes%20with%20induced%0Aconnectivity.%20The%20product%20between%20the%20coarsened%20and%20the%20original%20graph%20reveals%0Aan%20implicit%20structure%20whereby%20subgraphs%20are%20associated%20with%20specific%20sets%20of%0Anodes.%20By%20running%20generalized%20message-passing%20on%20such%20graph%20product%2C%20our%20method%0Aeffectively%20implements%20an%20efficient%2C%20yet%20powerful%20Subgraph%20GNN.%20Controlling%20the%0Acoarsening%20function%20enables%20meaningful%20selection%20of%20any%20number%20of%20subgraphs%0Awhile%2C%20contrary%20to%20previous%20methods%2C%20being%20fully%20compatible%20with%20standard%0Atraining%20techniques.%20Notably%2C%20we%20discover%20that%20the%20resulting%20node%20feature%0Atensor%20exhibits%20new%2C%20unexplored%20permutation%20symmetries.%20We%20leverage%20this%0Astructure%2C%20characterize%20the%20associated%20linear%20equivariant%20layers%20and%0Aincorporate%20them%20into%20the%20layers%20of%20our%20Subgraph%20GNN%20architecture.%20Extensive%0Aexperiments%20on%20multiple%20graph%20learning%20benchmarks%20demonstrate%20that%20our%20method%0Ais%20significantly%20more%20flexible%20than%20previous%20approaches%2C%20as%20it%20can%20seamlessly%0Ahandle%20any%20number%20of%20subgraphs%2C%20while%20consistently%20outperforming%20baseline%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09291v1&entry.124074799=Read"},
{"title": "CLIP-Driven Cloth-Agnostic Feature Learning for Cloth-Changing Person\n  Re-Identification", "author": "Shuang Li and Jiaxu Leng and Guozhang Li and Ji Gan and Haosheng chen and Xinbo Gao", "abstract": "  Contrastive Language-Image Pre-Training (CLIP) has shown impressive\nperformance in short-term Person Re-Identification (ReID) due to its ability to\nextract high-level semantic features of pedestrians, yet its direct application\nto Cloth-Changing Person Re-Identification (CC-ReID) faces challenges due to\nCLIP's image encoder overly focusing on clothes clues. To address this, we\npropose a novel framework called CLIP-Driven Cloth-Agnostic Feature Learning\n(CCAF) for CC-ReID. Accordingly, two modules were custom-designed: the\nInvariant Feature Prompting (IFP) and the Clothes Feature Minimization (CFM).\nThese modules guide the model to extract cloth-agnostic features positively and\nattenuate clothes-related features negatively. Specifically, IFP is designed to\nextract fine-grained semantic features unrelated to clothes from the raw image,\nguided by the cloth-agnostic text prompts. This module first covers the clothes\nin the raw image at the pixel level to obtain the shielding image and then\nutilizes CLIP's knowledge to generate cloth-agnostic text prompts.\nSubsequently, it aligns the raw image-text and the raw image-shielding image in\nthe feature space, emphasizing discriminative clues related to identity but\nunrelated to clothes. Furthermore, CFM is designed to examine and weaken the\nimage encoder's ability to extract clothes features. It first generates text\nprompts corresponding to clothes pixels. Then, guided by these clothes text\nprompts, it iteratively examines and disentangles clothes features from\npedestrian features, ultimately retaining inherent discriminative features.\nExtensive experiments have demonstrated the effectiveness of the proposed CCAF,\nachieving new state-of-the-art performance on several popular CC-ReID\nbenchmarks without any additional inference time.\n", "link": "http://arxiv.org/abs/2406.09198v1", "date": "2024-06-13", "relevancy": 2.3113, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5893}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5776}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-Driven%20Cloth-Agnostic%20Feature%20Learning%20for%20Cloth-Changing%20Person%0A%20%20Re-Identification&body=Title%3A%20CLIP-Driven%20Cloth-Agnostic%20Feature%20Learning%20for%20Cloth-Changing%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Shuang%20Li%20and%20Jiaxu%20Leng%20and%20Guozhang%20Li%20and%20Ji%20Gan%20and%20Haosheng%20chen%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20has%20shown%20impressive%0Aperformance%20in%20short-term%20Person%20Re-Identification%20%28ReID%29%20due%20to%20its%20ability%20to%0Aextract%20high-level%20semantic%20features%20of%20pedestrians%2C%20yet%20its%20direct%20application%0Ato%20Cloth-Changing%20Person%20Re-Identification%20%28CC-ReID%29%20faces%20challenges%20due%20to%0ACLIP%27s%20image%20encoder%20overly%20focusing%20on%20clothes%20clues.%20To%20address%20this%2C%20we%0Apropose%20a%20novel%20framework%20called%20CLIP-Driven%20Cloth-Agnostic%20Feature%20Learning%0A%28CCAF%29%20for%20CC-ReID.%20Accordingly%2C%20two%20modules%20were%20custom-designed%3A%20the%0AInvariant%20Feature%20Prompting%20%28IFP%29%20and%20the%20Clothes%20Feature%20Minimization%20%28CFM%29.%0AThese%20modules%20guide%20the%20model%20to%20extract%20cloth-agnostic%20features%20positively%20and%0Aattenuate%20clothes-related%20features%20negatively.%20Specifically%2C%20IFP%20is%20designed%20to%0Aextract%20fine-grained%20semantic%20features%20unrelated%20to%20clothes%20from%20the%20raw%20image%2C%0Aguided%20by%20the%20cloth-agnostic%20text%20prompts.%20This%20module%20first%20covers%20the%20clothes%0Ain%20the%20raw%20image%20at%20the%20pixel%20level%20to%20obtain%20the%20shielding%20image%20and%20then%0Autilizes%20CLIP%27s%20knowledge%20to%20generate%20cloth-agnostic%20text%20prompts.%0ASubsequently%2C%20it%20aligns%20the%20raw%20image-text%20and%20the%20raw%20image-shielding%20image%20in%0Athe%20feature%20space%2C%20emphasizing%20discriminative%20clues%20related%20to%20identity%20but%0Aunrelated%20to%20clothes.%20Furthermore%2C%20CFM%20is%20designed%20to%20examine%20and%20weaken%20the%0Aimage%20encoder%27s%20ability%20to%20extract%20clothes%20features.%20It%20first%20generates%20text%0Aprompts%20corresponding%20to%20clothes%20pixels.%20Then%2C%20guided%20by%20these%20clothes%20text%0Aprompts%2C%20it%20iteratively%20examines%20and%20disentangles%20clothes%20features%20from%0Apedestrian%20features%2C%20ultimately%20retaining%20inherent%20discriminative%20features.%0AExtensive%20experiments%20have%20demonstrated%20the%20effectiveness%20of%20the%20proposed%20CCAF%2C%0Aachieving%20new%20state-of-the-art%20performance%20on%20several%20popular%20CC-ReID%0Abenchmarks%20without%20any%20additional%20inference%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-Driven%2520Cloth-Agnostic%2520Feature%2520Learning%2520for%2520Cloth-Changing%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DShuang%2520Li%2520and%2520Jiaxu%2520Leng%2520and%2520Guozhang%2520Li%2520and%2520Ji%2520Gan%2520and%2520Haosheng%2520chen%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-Training%2520%2528CLIP%2529%2520has%2520shown%2520impressive%250Aperformance%2520in%2520short-term%2520Person%2520Re-Identification%2520%2528ReID%2529%2520due%2520to%2520its%2520ability%2520to%250Aextract%2520high-level%2520semantic%2520features%2520of%2520pedestrians%252C%2520yet%2520its%2520direct%2520application%250Ato%2520Cloth-Changing%2520Person%2520Re-Identification%2520%2528CC-ReID%2529%2520faces%2520challenges%2520due%2520to%250ACLIP%2527s%2520image%2520encoder%2520overly%2520focusing%2520on%2520clothes%2520clues.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520called%2520CLIP-Driven%2520Cloth-Agnostic%2520Feature%2520Learning%250A%2528CCAF%2529%2520for%2520CC-ReID.%2520Accordingly%252C%2520two%2520modules%2520were%2520custom-designed%253A%2520the%250AInvariant%2520Feature%2520Prompting%2520%2528IFP%2529%2520and%2520the%2520Clothes%2520Feature%2520Minimization%2520%2528CFM%2529.%250AThese%2520modules%2520guide%2520the%2520model%2520to%2520extract%2520cloth-agnostic%2520features%2520positively%2520and%250Aattenuate%2520clothes-related%2520features%2520negatively.%2520Specifically%252C%2520IFP%2520is%2520designed%2520to%250Aextract%2520fine-grained%2520semantic%2520features%2520unrelated%2520to%2520clothes%2520from%2520the%2520raw%2520image%252C%250Aguided%2520by%2520the%2520cloth-agnostic%2520text%2520prompts.%2520This%2520module%2520first%2520covers%2520the%2520clothes%250Ain%2520the%2520raw%2520image%2520at%2520the%2520pixel%2520level%2520to%2520obtain%2520the%2520shielding%2520image%2520and%2520then%250Autilizes%2520CLIP%2527s%2520knowledge%2520to%2520generate%2520cloth-agnostic%2520text%2520prompts.%250ASubsequently%252C%2520it%2520aligns%2520the%2520raw%2520image-text%2520and%2520the%2520raw%2520image-shielding%2520image%2520in%250Athe%2520feature%2520space%252C%2520emphasizing%2520discriminative%2520clues%2520related%2520to%2520identity%2520but%250Aunrelated%2520to%2520clothes.%2520Furthermore%252C%2520CFM%2520is%2520designed%2520to%2520examine%2520and%2520weaken%2520the%250Aimage%2520encoder%2527s%2520ability%2520to%2520extract%2520clothes%2520features.%2520It%2520first%2520generates%2520text%250Aprompts%2520corresponding%2520to%2520clothes%2520pixels.%2520Then%252C%2520guided%2520by%2520these%2520clothes%2520text%250Aprompts%252C%2520it%2520iteratively%2520examines%2520and%2520disentangles%2520clothes%2520features%2520from%250Apedestrian%2520features%252C%2520ultimately%2520retaining%2520inherent%2520discriminative%2520features.%250AExtensive%2520experiments%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520the%2520proposed%2520CCAF%252C%250Aachieving%2520new%2520state-of-the-art%2520performance%2520on%2520several%2520popular%2520CC-ReID%250Abenchmarks%2520without%2520any%2520additional%2520inference%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-Driven%20Cloth-Agnostic%20Feature%20Learning%20for%20Cloth-Changing%20Person%0A%20%20Re-Identification&entry.906535625=Shuang%20Li%20and%20Jiaxu%20Leng%20and%20Guozhang%20Li%20and%20Ji%20Gan%20and%20Haosheng%20chen%20and%20Xinbo%20Gao&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20has%20shown%20impressive%0Aperformance%20in%20short-term%20Person%20Re-Identification%20%28ReID%29%20due%20to%20its%20ability%20to%0Aextract%20high-level%20semantic%20features%20of%20pedestrians%2C%20yet%20its%20direct%20application%0Ato%20Cloth-Changing%20Person%20Re-Identification%20%28CC-ReID%29%20faces%20challenges%20due%20to%0ACLIP%27s%20image%20encoder%20overly%20focusing%20on%20clothes%20clues.%20To%20address%20this%2C%20we%0Apropose%20a%20novel%20framework%20called%20CLIP-Driven%20Cloth-Agnostic%20Feature%20Learning%0A%28CCAF%29%20for%20CC-ReID.%20Accordingly%2C%20two%20modules%20were%20custom-designed%3A%20the%0AInvariant%20Feature%20Prompting%20%28IFP%29%20and%20the%20Clothes%20Feature%20Minimization%20%28CFM%29.%0AThese%20modules%20guide%20the%20model%20to%20extract%20cloth-agnostic%20features%20positively%20and%0Aattenuate%20clothes-related%20features%20negatively.%20Specifically%2C%20IFP%20is%20designed%20to%0Aextract%20fine-grained%20semantic%20features%20unrelated%20to%20clothes%20from%20the%20raw%20image%2C%0Aguided%20by%20the%20cloth-agnostic%20text%20prompts.%20This%20module%20first%20covers%20the%20clothes%0Ain%20the%20raw%20image%20at%20the%20pixel%20level%20to%20obtain%20the%20shielding%20image%20and%20then%0Autilizes%20CLIP%27s%20knowledge%20to%20generate%20cloth-agnostic%20text%20prompts.%0ASubsequently%2C%20it%20aligns%20the%20raw%20image-text%20and%20the%20raw%20image-shielding%20image%20in%0Athe%20feature%20space%2C%20emphasizing%20discriminative%20clues%20related%20to%20identity%20but%0Aunrelated%20to%20clothes.%20Furthermore%2C%20CFM%20is%20designed%20to%20examine%20and%20weaken%20the%0Aimage%20encoder%27s%20ability%20to%20extract%20clothes%20features.%20It%20first%20generates%20text%0Aprompts%20corresponding%20to%20clothes%20pixels.%20Then%2C%20guided%20by%20these%20clothes%20text%0Aprompts%2C%20it%20iteratively%20examines%20and%20disentangles%20clothes%20features%20from%0Apedestrian%20features%2C%20ultimately%20retaining%20inherent%20discriminative%20features.%0AExtensive%20experiments%20have%20demonstrated%20the%20effectiveness%20of%20the%20proposed%20CCAF%2C%0Aachieving%20new%20state-of-the-art%20performance%20on%20several%20popular%20CC-ReID%0Abenchmarks%20without%20any%20additional%20inference%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09198v1&entry.124074799=Read"},
{"title": "Why Warmup the Learning Rate? Underlying Mechanisms and Improvements", "author": "Dayal Singh Kalra and Maissam Barkeshli", "abstract": "  It is common in deep learning to warm up the learning rate $\\eta$, often by a\nlinear schedule between $\\eta_{\\text{init}} = 0$ and a predetermined target\n$\\eta_{\\text{trgt}}$. In this paper, we show through systematic experiments\nusing SGD and Adam that the overwhelming benefit of warmup arises from allowing\nthe network to tolerate larger $\\eta_{\\text{trgt}}$ by forcing the network to\nmore well-conditioned areas of the loss landscape. The ability to handle larger\n$\\eta_{\\text{trgt}}$ makes hyperparameter tuning more robust while improving\nthe final performance. We uncover different regimes of operation during the\nwarmup period, depending on whether training starts off in a progressive\nsharpening or sharpness reduction phase, which in turn depends on the\ninitialization and parameterization. Using these insights, we show how\n$\\eta_{\\text{init}}$ can be properly chosen by utilizing the loss catapult\nmechanism, which saves on the number of warmup steps, in some cases completely\neliminating the need for warmup. We also suggest an initialization for the\nvariance in Adam which provides benefits similar to warmup.\n", "link": "http://arxiv.org/abs/2406.09405v1", "date": "2024-06-13", "relevancy": 2.3072, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4708}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4586}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Warmup%20the%20Learning%20Rate%3F%20Underlying%20Mechanisms%20and%20Improvements&body=Title%3A%20Why%20Warmup%20the%20Learning%20Rate%3F%20Underlying%20Mechanisms%20and%20Improvements%0AAuthor%3A%20Dayal%20Singh%20Kalra%20and%20Maissam%20Barkeshli%0AAbstract%3A%20%20%20It%20is%20common%20in%20deep%20learning%20to%20warm%20up%20the%20learning%20rate%20%24%5Ceta%24%2C%20often%20by%20a%0Alinear%20schedule%20between%20%24%5Ceta_%7B%5Ctext%7Binit%7D%7D%20%3D%200%24%20and%20a%20predetermined%20target%0A%24%5Ceta_%7B%5Ctext%7Btrgt%7D%7D%24.%20In%20this%20paper%2C%20we%20show%20through%20systematic%20experiments%0Ausing%20SGD%20and%20Adam%20that%20the%20overwhelming%20benefit%20of%20warmup%20arises%20from%20allowing%0Athe%20network%20to%20tolerate%20larger%20%24%5Ceta_%7B%5Ctext%7Btrgt%7D%7D%24%20by%20forcing%20the%20network%20to%0Amore%20well-conditioned%20areas%20of%20the%20loss%20landscape.%20The%20ability%20to%20handle%20larger%0A%24%5Ceta_%7B%5Ctext%7Btrgt%7D%7D%24%20makes%20hyperparameter%20tuning%20more%20robust%20while%20improving%0Athe%20final%20performance.%20We%20uncover%20different%20regimes%20of%20operation%20during%20the%0Awarmup%20period%2C%20depending%20on%20whether%20training%20starts%20off%20in%20a%20progressive%0Asharpening%20or%20sharpness%20reduction%20phase%2C%20which%20in%20turn%20depends%20on%20the%0Ainitialization%20and%20parameterization.%20Using%20these%20insights%2C%20we%20show%20how%0A%24%5Ceta_%7B%5Ctext%7Binit%7D%7D%24%20can%20be%20properly%20chosen%20by%20utilizing%20the%20loss%20catapult%0Amechanism%2C%20which%20saves%20on%20the%20number%20of%20warmup%20steps%2C%20in%20some%20cases%20completely%0Aeliminating%20the%20need%20for%20warmup.%20We%20also%20suggest%20an%20initialization%20for%20the%0Avariance%20in%20Adam%20which%20provides%20benefits%20similar%20to%20warmup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Warmup%2520the%2520Learning%2520Rate%253F%2520Underlying%2520Mechanisms%2520and%2520Improvements%26entry.906535625%3DDayal%2520Singh%2520Kalra%2520and%2520Maissam%2520Barkeshli%26entry.1292438233%3D%2520%2520It%2520is%2520common%2520in%2520deep%2520learning%2520to%2520warm%2520up%2520the%2520learning%2520rate%2520%2524%255Ceta%2524%252C%2520often%2520by%2520a%250Alinear%2520schedule%2520between%2520%2524%255Ceta_%257B%255Ctext%257Binit%257D%257D%2520%253D%25200%2524%2520and%2520a%2520predetermined%2520target%250A%2524%255Ceta_%257B%255Ctext%257Btrgt%257D%257D%2524.%2520In%2520this%2520paper%252C%2520we%2520show%2520through%2520systematic%2520experiments%250Ausing%2520SGD%2520and%2520Adam%2520that%2520the%2520overwhelming%2520benefit%2520of%2520warmup%2520arises%2520from%2520allowing%250Athe%2520network%2520to%2520tolerate%2520larger%2520%2524%255Ceta_%257B%255Ctext%257Btrgt%257D%257D%2524%2520by%2520forcing%2520the%2520network%2520to%250Amore%2520well-conditioned%2520areas%2520of%2520the%2520loss%2520landscape.%2520The%2520ability%2520to%2520handle%2520larger%250A%2524%255Ceta_%257B%255Ctext%257Btrgt%257D%257D%2524%2520makes%2520hyperparameter%2520tuning%2520more%2520robust%2520while%2520improving%250Athe%2520final%2520performance.%2520We%2520uncover%2520different%2520regimes%2520of%2520operation%2520during%2520the%250Awarmup%2520period%252C%2520depending%2520on%2520whether%2520training%2520starts%2520off%2520in%2520a%2520progressive%250Asharpening%2520or%2520sharpness%2520reduction%2520phase%252C%2520which%2520in%2520turn%2520depends%2520on%2520the%250Ainitialization%2520and%2520parameterization.%2520Using%2520these%2520insights%252C%2520we%2520show%2520how%250A%2524%255Ceta_%257B%255Ctext%257Binit%257D%257D%2524%2520can%2520be%2520properly%2520chosen%2520by%2520utilizing%2520the%2520loss%2520catapult%250Amechanism%252C%2520which%2520saves%2520on%2520the%2520number%2520of%2520warmup%2520steps%252C%2520in%2520some%2520cases%2520completely%250Aeliminating%2520the%2520need%2520for%2520warmup.%2520We%2520also%2520suggest%2520an%2520initialization%2520for%2520the%250Avariance%2520in%2520Adam%2520which%2520provides%2520benefits%2520similar%2520to%2520warmup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Warmup%20the%20Learning%20Rate%3F%20Underlying%20Mechanisms%20and%20Improvements&entry.906535625=Dayal%20Singh%20Kalra%20and%20Maissam%20Barkeshli&entry.1292438233=%20%20It%20is%20common%20in%20deep%20learning%20to%20warm%20up%20the%20learning%20rate%20%24%5Ceta%24%2C%20often%20by%20a%0Alinear%20schedule%20between%20%24%5Ceta_%7B%5Ctext%7Binit%7D%7D%20%3D%200%24%20and%20a%20predetermined%20target%0A%24%5Ceta_%7B%5Ctext%7Btrgt%7D%7D%24.%20In%20this%20paper%2C%20we%20show%20through%20systematic%20experiments%0Ausing%20SGD%20and%20Adam%20that%20the%20overwhelming%20benefit%20of%20warmup%20arises%20from%20allowing%0Athe%20network%20to%20tolerate%20larger%20%24%5Ceta_%7B%5Ctext%7Btrgt%7D%7D%24%20by%20forcing%20the%20network%20to%0Amore%20well-conditioned%20areas%20of%20the%20loss%20landscape.%20The%20ability%20to%20handle%20larger%0A%24%5Ceta_%7B%5Ctext%7Btrgt%7D%7D%24%20makes%20hyperparameter%20tuning%20more%20robust%20while%20improving%0Athe%20final%20performance.%20We%20uncover%20different%20regimes%20of%20operation%20during%20the%0Awarmup%20period%2C%20depending%20on%20whether%20training%20starts%20off%20in%20a%20progressive%0Asharpening%20or%20sharpness%20reduction%20phase%2C%20which%20in%20turn%20depends%20on%20the%0Ainitialization%20and%20parameterization.%20Using%20these%20insights%2C%20we%20show%20how%0A%24%5Ceta_%7B%5Ctext%7Binit%7D%7D%24%20can%20be%20properly%20chosen%20by%20utilizing%20the%20loss%20catapult%0Amechanism%2C%20which%20saves%20on%20the%20number%20of%20warmup%20steps%2C%20in%20some%20cases%20completely%0Aeliminating%20the%20need%20for%20warmup.%20We%20also%20suggest%20an%20initialization%20for%20the%0Avariance%20in%20Adam%20which%20provides%20benefits%20similar%20to%20warmup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09405v1&entry.124074799=Read"},
{"title": "VideoGPT+: Integrating Image and Video Encoders for Enhanced Video\n  Understanding", "author": "Muhammad Maaz and Hanoona Rasheed and Salman Khan and Fahad Khan", "abstract": "  Building on the advances of language models, Large Multimodal Models (LMMs)\nhave contributed significant improvements in video understanding. While the\ncurrent video LMMs utilize advanced Large Language Models (LLMs), they rely on\neither image or video encoders to process visual inputs, each of which has its\nown limitations. Image encoders excel at capturing rich spatial details from\nframe sequences but lack explicit temporal context, which can be important in\nvideos with intricate action sequences. On the other hand, video encoders\nprovide temporal context but are often limited by computational constraints\nthat lead to processing only sparse frames at lower resolutions, resulting in\nreduced contextual and spatial understanding. To this end, we introduce\nVideoGPT+, which combines the complementary benefits of the image encoder (for\ndetailed spatial understanding) and the video encoder (for global temporal\ncontext modeling). The model processes videos by dividing them into smaller\nsegments and applies an adaptive pooling strategy on features extracted by both\nimage and video encoders. Our architecture showcases improved performance\nacross multiple video benchmarks, including VCGBench, MVBench and Zero-shot\nquestion-answering. Further, we develop 112K video-instruction set using a\nnovel semi-automatic annotation pipeline which further improves the model\nperformance. Additionally, to comprehensively evaluate video LMMs, we present\nVCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports,\nscience, gaming, and surveillance videos. This benchmark with 4,354\nquestion-answer pairs evaluates the generalization of existing LMMs on dense\nvideo captioning, spatial and temporal understanding, and complex reasoning,\nensuring comprehensive assessment across diverse video types and dynamics.\nCode: https://github.com/mbzuai-oryx/VideoGPT-plus.\n", "link": "http://arxiv.org/abs/2406.09418v1", "date": "2024-06-13", "relevancy": 2.3071, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5836}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5744}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoGPT%2B%3A%20Integrating%20Image%20and%20Video%20Encoders%20for%20Enhanced%20Video%0A%20%20Understanding&body=Title%3A%20VideoGPT%2B%3A%20Integrating%20Image%20and%20Video%20Encoders%20for%20Enhanced%20Video%0A%20%20Understanding%0AAuthor%3A%20Muhammad%20Maaz%20and%20Hanoona%20Rasheed%20and%20Salman%20Khan%20and%20Fahad%20Khan%0AAbstract%3A%20%20%20Building%20on%20the%20advances%20of%20language%20models%2C%20Large%20Multimodal%20Models%20%28LMMs%29%0Ahave%20contributed%20significant%20improvements%20in%20video%20understanding.%20While%20the%0Acurrent%20video%20LMMs%20utilize%20advanced%20Large%20Language%20Models%20%28LLMs%29%2C%20they%20rely%20on%0Aeither%20image%20or%20video%20encoders%20to%20process%20visual%20inputs%2C%20each%20of%20which%20has%20its%0Aown%20limitations.%20Image%20encoders%20excel%20at%20capturing%20rich%20spatial%20details%20from%0Aframe%20sequences%20but%20lack%20explicit%20temporal%20context%2C%20which%20can%20be%20important%20in%0Avideos%20with%20intricate%20action%20sequences.%20On%20the%20other%20hand%2C%20video%20encoders%0Aprovide%20temporal%20context%20but%20are%20often%20limited%20by%20computational%20constraints%0Athat%20lead%20to%20processing%20only%20sparse%20frames%20at%20lower%20resolutions%2C%20resulting%20in%0Areduced%20contextual%20and%20spatial%20understanding.%20To%20this%20end%2C%20we%20introduce%0AVideoGPT%2B%2C%20which%20combines%20the%20complementary%20benefits%20of%20the%20image%20encoder%20%28for%0Adetailed%20spatial%20understanding%29%20and%20the%20video%20encoder%20%28for%20global%20temporal%0Acontext%20modeling%29.%20The%20model%20processes%20videos%20by%20dividing%20them%20into%20smaller%0Asegments%20and%20applies%20an%20adaptive%20pooling%20strategy%20on%20features%20extracted%20by%20both%0Aimage%20and%20video%20encoders.%20Our%20architecture%20showcases%20improved%20performance%0Aacross%20multiple%20video%20benchmarks%2C%20including%20VCGBench%2C%20MVBench%20and%20Zero-shot%0Aquestion-answering.%20Further%2C%20we%20develop%20112K%20video-instruction%20set%20using%20a%0Anovel%20semi-automatic%20annotation%20pipeline%20which%20further%20improves%20the%20model%0Aperformance.%20Additionally%2C%20to%20comprehensively%20evaluate%20video%20LMMs%2C%20we%20present%0AVCGBench-Diverse%2C%20covering%2018%20broad%20video%20categories%20such%20as%20lifestyle%2C%20sports%2C%0Ascience%2C%20gaming%2C%20and%20surveillance%20videos.%20This%20benchmark%20with%204%2C354%0Aquestion-answer%20pairs%20evaluates%20the%20generalization%20of%20existing%20LMMs%20on%20dense%0Avideo%20captioning%2C%20spatial%20and%20temporal%20understanding%2C%20and%20complex%20reasoning%2C%0Aensuring%20comprehensive%20assessment%20across%20diverse%20video%20types%20and%20dynamics.%0ACode%3A%20https%3A//github.com/mbzuai-oryx/VideoGPT-plus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoGPT%252B%253A%2520Integrating%2520Image%2520and%2520Video%2520Encoders%2520for%2520Enhanced%2520Video%250A%2520%2520Understanding%26entry.906535625%3DMuhammad%2520Maaz%2520and%2520Hanoona%2520Rasheed%2520and%2520Salman%2520Khan%2520and%2520Fahad%2520Khan%26entry.1292438233%3D%2520%2520Building%2520on%2520the%2520advances%2520of%2520language%2520models%252C%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%250Ahave%2520contributed%2520significant%2520improvements%2520in%2520video%2520understanding.%2520While%2520the%250Acurrent%2520video%2520LMMs%2520utilize%2520advanced%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520they%2520rely%2520on%250Aeither%2520image%2520or%2520video%2520encoders%2520to%2520process%2520visual%2520inputs%252C%2520each%2520of%2520which%2520has%2520its%250Aown%2520limitations.%2520Image%2520encoders%2520excel%2520at%2520capturing%2520rich%2520spatial%2520details%2520from%250Aframe%2520sequences%2520but%2520lack%2520explicit%2520temporal%2520context%252C%2520which%2520can%2520be%2520important%2520in%250Avideos%2520with%2520intricate%2520action%2520sequences.%2520On%2520the%2520other%2520hand%252C%2520video%2520encoders%250Aprovide%2520temporal%2520context%2520but%2520are%2520often%2520limited%2520by%2520computational%2520constraints%250Athat%2520lead%2520to%2520processing%2520only%2520sparse%2520frames%2520at%2520lower%2520resolutions%252C%2520resulting%2520in%250Areduced%2520contextual%2520and%2520spatial%2520understanding.%2520To%2520this%2520end%252C%2520we%2520introduce%250AVideoGPT%252B%252C%2520which%2520combines%2520the%2520complementary%2520benefits%2520of%2520the%2520image%2520encoder%2520%2528for%250Adetailed%2520spatial%2520understanding%2529%2520and%2520the%2520video%2520encoder%2520%2528for%2520global%2520temporal%250Acontext%2520modeling%2529.%2520The%2520model%2520processes%2520videos%2520by%2520dividing%2520them%2520into%2520smaller%250Asegments%2520and%2520applies%2520an%2520adaptive%2520pooling%2520strategy%2520on%2520features%2520extracted%2520by%2520both%250Aimage%2520and%2520video%2520encoders.%2520Our%2520architecture%2520showcases%2520improved%2520performance%250Aacross%2520multiple%2520video%2520benchmarks%252C%2520including%2520VCGBench%252C%2520MVBench%2520and%2520Zero-shot%250Aquestion-answering.%2520Further%252C%2520we%2520develop%2520112K%2520video-instruction%2520set%2520using%2520a%250Anovel%2520semi-automatic%2520annotation%2520pipeline%2520which%2520further%2520improves%2520the%2520model%250Aperformance.%2520Additionally%252C%2520to%2520comprehensively%2520evaluate%2520video%2520LMMs%252C%2520we%2520present%250AVCGBench-Diverse%252C%2520covering%252018%2520broad%2520video%2520categories%2520such%2520as%2520lifestyle%252C%2520sports%252C%250Ascience%252C%2520gaming%252C%2520and%2520surveillance%2520videos.%2520This%2520benchmark%2520with%25204%252C354%250Aquestion-answer%2520pairs%2520evaluates%2520the%2520generalization%2520of%2520existing%2520LMMs%2520on%2520dense%250Avideo%2520captioning%252C%2520spatial%2520and%2520temporal%2520understanding%252C%2520and%2520complex%2520reasoning%252C%250Aensuring%2520comprehensive%2520assessment%2520across%2520diverse%2520video%2520types%2520and%2520dynamics.%250ACode%253A%2520https%253A//github.com/mbzuai-oryx/VideoGPT-plus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGPT%2B%3A%20Integrating%20Image%20and%20Video%20Encoders%20for%20Enhanced%20Video%0A%20%20Understanding&entry.906535625=Muhammad%20Maaz%20and%20Hanoona%20Rasheed%20and%20Salman%20Khan%20and%20Fahad%20Khan&entry.1292438233=%20%20Building%20on%20the%20advances%20of%20language%20models%2C%20Large%20Multimodal%20Models%20%28LMMs%29%0Ahave%20contributed%20significant%20improvements%20in%20video%20understanding.%20While%20the%0Acurrent%20video%20LMMs%20utilize%20advanced%20Large%20Language%20Models%20%28LLMs%29%2C%20they%20rely%20on%0Aeither%20image%20or%20video%20encoders%20to%20process%20visual%20inputs%2C%20each%20of%20which%20has%20its%0Aown%20limitations.%20Image%20encoders%20excel%20at%20capturing%20rich%20spatial%20details%20from%0Aframe%20sequences%20but%20lack%20explicit%20temporal%20context%2C%20which%20can%20be%20important%20in%0Avideos%20with%20intricate%20action%20sequences.%20On%20the%20other%20hand%2C%20video%20encoders%0Aprovide%20temporal%20context%20but%20are%20often%20limited%20by%20computational%20constraints%0Athat%20lead%20to%20processing%20only%20sparse%20frames%20at%20lower%20resolutions%2C%20resulting%20in%0Areduced%20contextual%20and%20spatial%20understanding.%20To%20this%20end%2C%20we%20introduce%0AVideoGPT%2B%2C%20which%20combines%20the%20complementary%20benefits%20of%20the%20image%20encoder%20%28for%0Adetailed%20spatial%20understanding%29%20and%20the%20video%20encoder%20%28for%20global%20temporal%0Acontext%20modeling%29.%20The%20model%20processes%20videos%20by%20dividing%20them%20into%20smaller%0Asegments%20and%20applies%20an%20adaptive%20pooling%20strategy%20on%20features%20extracted%20by%20both%0Aimage%20and%20video%20encoders.%20Our%20architecture%20showcases%20improved%20performance%0Aacross%20multiple%20video%20benchmarks%2C%20including%20VCGBench%2C%20MVBench%20and%20Zero-shot%0Aquestion-answering.%20Further%2C%20we%20develop%20112K%20video-instruction%20set%20using%20a%0Anovel%20semi-automatic%20annotation%20pipeline%20which%20further%20improves%20the%20model%0Aperformance.%20Additionally%2C%20to%20comprehensively%20evaluate%20video%20LMMs%2C%20we%20present%0AVCGBench-Diverse%2C%20covering%2018%20broad%20video%20categories%20such%20as%20lifestyle%2C%20sports%2C%0Ascience%2C%20gaming%2C%20and%20surveillance%20videos.%20This%20benchmark%20with%204%2C354%0Aquestion-answer%20pairs%20evaluates%20the%20generalization%20of%20existing%20LMMs%20on%20dense%0Avideo%20captioning%2C%20spatial%20and%20temporal%20understanding%2C%20and%20complex%20reasoning%2C%0Aensuring%20comprehensive%20assessment%20across%20diverse%20video%20types%20and%20dynamics.%0ACode%3A%20https%3A//github.com/mbzuai-oryx/VideoGPT-plus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09418v1&entry.124074799=Read"},
{"title": "CGP++ : A Modern C++ Implementation of Cartesian Genetic Programming", "author": "Roman Kalkreuth and Thomas Baeck", "abstract": "  The reference implementation of Cartesian Genetic Programming (CGP) was\nwritten in the C programming language. C inherently follows a procedural\nprogramming paradigm, which entails challenges in providing a reusable and\nscalable implementation model for complex structures and methods. Moreover, due\nto the limiting factors of C, the reference implementation of CGP does not\nprovide a generic framework and is therefore restricted to a set of predefined\nevaluation types. Besides the reference implementation, we also observe that\nother existing implementations are limited with respect to the features\nprovided. In this work, we therefore propose the first version of a modern C++\nimplementation of CGP that pursues object-oriented design and generic\nprogramming paradigm to provide an efficient implementation model that can\nfacilitate the discovery of new problem domains and the implementation of\ncomplex advanced methods that have been proposed for CGP over time. With the\nproposal of our new implementation, we aim to generally promote\ninterpretability, accessibility and reproducibility in the field of CGP.\n", "link": "http://arxiv.org/abs/2406.09038v1", "date": "2024-06-13", "relevancy": 2.3035, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4729}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4641}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CGP%2B%2B%20%3A%20A%20Modern%20C%2B%2B%20Implementation%20of%20Cartesian%20Genetic%20Programming&body=Title%3A%20CGP%2B%2B%20%3A%20A%20Modern%20C%2B%2B%20Implementation%20of%20Cartesian%20Genetic%20Programming%0AAuthor%3A%20Roman%20Kalkreuth%20and%20Thomas%20Baeck%0AAbstract%3A%20%20%20The%20reference%20implementation%20of%20Cartesian%20Genetic%20Programming%20%28CGP%29%20was%0Awritten%20in%20the%20C%20programming%20language.%20C%20inherently%20follows%20a%20procedural%0Aprogramming%20paradigm%2C%20which%20entails%20challenges%20in%20providing%20a%20reusable%20and%0Ascalable%20implementation%20model%20for%20complex%20structures%20and%20methods.%20Moreover%2C%20due%0Ato%20the%20limiting%20factors%20of%20C%2C%20the%20reference%20implementation%20of%20CGP%20does%20not%0Aprovide%20a%20generic%20framework%20and%20is%20therefore%20restricted%20to%20a%20set%20of%20predefined%0Aevaluation%20types.%20Besides%20the%20reference%20implementation%2C%20we%20also%20observe%20that%0Aother%20existing%20implementations%20are%20limited%20with%20respect%20to%20the%20features%0Aprovided.%20In%20this%20work%2C%20we%20therefore%20propose%20the%20first%20version%20of%20a%20modern%20C%2B%2B%0Aimplementation%20of%20CGP%20that%20pursues%20object-oriented%20design%20and%20generic%0Aprogramming%20paradigm%20to%20provide%20an%20efficient%20implementation%20model%20that%20can%0Afacilitate%20the%20discovery%20of%20new%20problem%20domains%20and%20the%20implementation%20of%0Acomplex%20advanced%20methods%20that%20have%20been%20proposed%20for%20CGP%20over%20time.%20With%20the%0Aproposal%20of%20our%20new%20implementation%2C%20we%20aim%20to%20generally%20promote%0Ainterpretability%2C%20accessibility%20and%20reproducibility%20in%20the%20field%20of%20CGP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCGP%252B%252B%2520%253A%2520A%2520Modern%2520C%252B%252B%2520Implementation%2520of%2520Cartesian%2520Genetic%2520Programming%26entry.906535625%3DRoman%2520Kalkreuth%2520and%2520Thomas%2520Baeck%26entry.1292438233%3D%2520%2520The%2520reference%2520implementation%2520of%2520Cartesian%2520Genetic%2520Programming%2520%2528CGP%2529%2520was%250Awritten%2520in%2520the%2520C%2520programming%2520language.%2520C%2520inherently%2520follows%2520a%2520procedural%250Aprogramming%2520paradigm%252C%2520which%2520entails%2520challenges%2520in%2520providing%2520a%2520reusable%2520and%250Ascalable%2520implementation%2520model%2520for%2520complex%2520structures%2520and%2520methods.%2520Moreover%252C%2520due%250Ato%2520the%2520limiting%2520factors%2520of%2520C%252C%2520the%2520reference%2520implementation%2520of%2520CGP%2520does%2520not%250Aprovide%2520a%2520generic%2520framework%2520and%2520is%2520therefore%2520restricted%2520to%2520a%2520set%2520of%2520predefined%250Aevaluation%2520types.%2520Besides%2520the%2520reference%2520implementation%252C%2520we%2520also%2520observe%2520that%250Aother%2520existing%2520implementations%2520are%2520limited%2520with%2520respect%2520to%2520the%2520features%250Aprovided.%2520In%2520this%2520work%252C%2520we%2520therefore%2520propose%2520the%2520first%2520version%2520of%2520a%2520modern%2520C%252B%252B%250Aimplementation%2520of%2520CGP%2520that%2520pursues%2520object-oriented%2520design%2520and%2520generic%250Aprogramming%2520paradigm%2520to%2520provide%2520an%2520efficient%2520implementation%2520model%2520that%2520can%250Afacilitate%2520the%2520discovery%2520of%2520new%2520problem%2520domains%2520and%2520the%2520implementation%2520of%250Acomplex%2520advanced%2520methods%2520that%2520have%2520been%2520proposed%2520for%2520CGP%2520over%2520time.%2520With%2520the%250Aproposal%2520of%2520our%2520new%2520implementation%252C%2520we%2520aim%2520to%2520generally%2520promote%250Ainterpretability%252C%2520accessibility%2520and%2520reproducibility%2520in%2520the%2520field%2520of%2520CGP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGP%2B%2B%20%3A%20A%20Modern%20C%2B%2B%20Implementation%20of%20Cartesian%20Genetic%20Programming&entry.906535625=Roman%20Kalkreuth%20and%20Thomas%20Baeck&entry.1292438233=%20%20The%20reference%20implementation%20of%20Cartesian%20Genetic%20Programming%20%28CGP%29%20was%0Awritten%20in%20the%20C%20programming%20language.%20C%20inherently%20follows%20a%20procedural%0Aprogramming%20paradigm%2C%20which%20entails%20challenges%20in%20providing%20a%20reusable%20and%0Ascalable%20implementation%20model%20for%20complex%20structures%20and%20methods.%20Moreover%2C%20due%0Ato%20the%20limiting%20factors%20of%20C%2C%20the%20reference%20implementation%20of%20CGP%20does%20not%0Aprovide%20a%20generic%20framework%20and%20is%20therefore%20restricted%20to%20a%20set%20of%20predefined%0Aevaluation%20types.%20Besides%20the%20reference%20implementation%2C%20we%20also%20observe%20that%0Aother%20existing%20implementations%20are%20limited%20with%20respect%20to%20the%20features%0Aprovided.%20In%20this%20work%2C%20we%20therefore%20propose%20the%20first%20version%20of%20a%20modern%20C%2B%2B%0Aimplementation%20of%20CGP%20that%20pursues%20object-oriented%20design%20and%20generic%0Aprogramming%20paradigm%20to%20provide%20an%20efficient%20implementation%20model%20that%20can%0Afacilitate%20the%20discovery%20of%20new%20problem%20domains%20and%20the%20implementation%20of%0Acomplex%20advanced%20methods%20that%20have%20been%20proposed%20for%20CGP%20over%20time.%20With%20the%0Aproposal%20of%20our%20new%20implementation%2C%20we%20aim%20to%20generally%20promote%0Ainterpretability%2C%20accessibility%20and%20reproducibility%20in%20the%20field%20of%20CGP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09038v1&entry.124074799=Read"},
{"title": "Stepwise Regression and Pre-trained Edge for Robust Stereo Matching", "author": "Weiqing Xiao and Wei Zhao", "abstract": "  Due to the difficulty in obtaining real samples and ground truth, the\ngeneralization performance and the fine-tuned performance are critical for the\nfeasibility of stereo matching methods in real-world applications. However, the\npresence of substantial disparity distributions and density variations across\ndifferent datasets presents significant challenges for the generalization and\nfine-tuning of the model. In this paper, we propose a novel stereo matching\nmethod, called SR-Stereo, which mitigates the distributional differences across\ndifferent datasets by predicting the disparity clips and uses a loss weight\nrelated to the regression target scale to improve the accuracy of the disparity\nclips. Moreover, this stepwise regression architecture can be easily extended\nto existing iteration-based methods to improve the performance without changing\nthe structure. In addition, to mitigate the edge blurring of the fine-tuned\nmodel on sparse ground truth, we propose Domain Adaptation Based on Pre-trained\nEdges (DAPE). Specifically, we use the predicted disparity and RGB image to\nestimate the edge map of the target domain image. The edge map is filtered to\ngenerate edge map background pseudo-labels, which together with the sparse\nground truth disparity on the target domain are used as a supervision to\njointly fine-tune the pre-trained stereo matching model. These proposed methods\nare extensively evaluated on SceneFlow, KITTI, Middbury 2014 and ETH3D. The\nSR-Stereo achieves competitive disparity estimation performance and\nstate-of-the-art cross-domain generalisation performance. Meanwhile, the\nproposed DAPE significantly improves the disparity estimation performance of\nfine-tuned models, especially in the textureless and detail regions.\n", "link": "http://arxiv.org/abs/2406.06953v2", "date": "2024-06-13", "relevancy": 2.2684, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5956}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.564}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stepwise%20Regression%20and%20Pre-trained%20Edge%20for%20Robust%20Stereo%20Matching&body=Title%3A%20Stepwise%20Regression%20and%20Pre-trained%20Edge%20for%20Robust%20Stereo%20Matching%0AAuthor%3A%20Weiqing%20Xiao%20and%20Wei%20Zhao%0AAbstract%3A%20%20%20Due%20to%20the%20difficulty%20in%20obtaining%20real%20samples%20and%20ground%20truth%2C%20the%0Ageneralization%20performance%20and%20the%20fine-tuned%20performance%20are%20critical%20for%20the%0Afeasibility%20of%20stereo%20matching%20methods%20in%20real-world%20applications.%20However%2C%20the%0Apresence%20of%20substantial%20disparity%20distributions%20and%20density%20variations%20across%0Adifferent%20datasets%20presents%20significant%20challenges%20for%20the%20generalization%20and%0Afine-tuning%20of%20the%20model.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20stereo%20matching%0Amethod%2C%20called%20SR-Stereo%2C%20which%20mitigates%20the%20distributional%20differences%20across%0Adifferent%20datasets%20by%20predicting%20the%20disparity%20clips%20and%20uses%20a%20loss%20weight%0Arelated%20to%20the%20regression%20target%20scale%20to%20improve%20the%20accuracy%20of%20the%20disparity%0Aclips.%20Moreover%2C%20this%20stepwise%20regression%20architecture%20can%20be%20easily%20extended%0Ato%20existing%20iteration-based%20methods%20to%20improve%20the%20performance%20without%20changing%0Athe%20structure.%20In%20addition%2C%20to%20mitigate%20the%20edge%20blurring%20of%20the%20fine-tuned%0Amodel%20on%20sparse%20ground%20truth%2C%20we%20propose%20Domain%20Adaptation%20Based%20on%20Pre-trained%0AEdges%20%28DAPE%29.%20Specifically%2C%20we%20use%20the%20predicted%20disparity%20and%20RGB%20image%20to%0Aestimate%20the%20edge%20map%20of%20the%20target%20domain%20image.%20The%20edge%20map%20is%20filtered%20to%0Agenerate%20edge%20map%20background%20pseudo-labels%2C%20which%20together%20with%20the%20sparse%0Aground%20truth%20disparity%20on%20the%20target%20domain%20are%20used%20as%20a%20supervision%20to%0Ajointly%20fine-tune%20the%20pre-trained%20stereo%20matching%20model.%20These%20proposed%20methods%0Aare%20extensively%20evaluated%20on%20SceneFlow%2C%20KITTI%2C%20Middbury%202014%20and%20ETH3D.%20The%0ASR-Stereo%20achieves%20competitive%20disparity%20estimation%20performance%20and%0Astate-of-the-art%20cross-domain%20generalisation%20performance.%20Meanwhile%2C%20the%0Aproposed%20DAPE%20significantly%20improves%20the%20disparity%20estimation%20performance%20of%0Afine-tuned%20models%2C%20especially%20in%20the%20textureless%20and%20detail%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepwise%2520Regression%2520and%2520Pre-trained%2520Edge%2520for%2520Robust%2520Stereo%2520Matching%26entry.906535625%3DWeiqing%2520Xiao%2520and%2520Wei%2520Zhao%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520difficulty%2520in%2520obtaining%2520real%2520samples%2520and%2520ground%2520truth%252C%2520the%250Ageneralization%2520performance%2520and%2520the%2520fine-tuned%2520performance%2520are%2520critical%2520for%2520the%250Afeasibility%2520of%2520stereo%2520matching%2520methods%2520in%2520real-world%2520applications.%2520However%252C%2520the%250Apresence%2520of%2520substantial%2520disparity%2520distributions%2520and%2520density%2520variations%2520across%250Adifferent%2520datasets%2520presents%2520significant%2520challenges%2520for%2520the%2520generalization%2520and%250Afine-tuning%2520of%2520the%2520model.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520stereo%2520matching%250Amethod%252C%2520called%2520SR-Stereo%252C%2520which%2520mitigates%2520the%2520distributional%2520differences%2520across%250Adifferent%2520datasets%2520by%2520predicting%2520the%2520disparity%2520clips%2520and%2520uses%2520a%2520loss%2520weight%250Arelated%2520to%2520the%2520regression%2520target%2520scale%2520to%2520improve%2520the%2520accuracy%2520of%2520the%2520disparity%250Aclips.%2520Moreover%252C%2520this%2520stepwise%2520regression%2520architecture%2520can%2520be%2520easily%2520extended%250Ato%2520existing%2520iteration-based%2520methods%2520to%2520improve%2520the%2520performance%2520without%2520changing%250Athe%2520structure.%2520In%2520addition%252C%2520to%2520mitigate%2520the%2520edge%2520blurring%2520of%2520the%2520fine-tuned%250Amodel%2520on%2520sparse%2520ground%2520truth%252C%2520we%2520propose%2520Domain%2520Adaptation%2520Based%2520on%2520Pre-trained%250AEdges%2520%2528DAPE%2529.%2520Specifically%252C%2520we%2520use%2520the%2520predicted%2520disparity%2520and%2520RGB%2520image%2520to%250Aestimate%2520the%2520edge%2520map%2520of%2520the%2520target%2520domain%2520image.%2520The%2520edge%2520map%2520is%2520filtered%2520to%250Agenerate%2520edge%2520map%2520background%2520pseudo-labels%252C%2520which%2520together%2520with%2520the%2520sparse%250Aground%2520truth%2520disparity%2520on%2520the%2520target%2520domain%2520are%2520used%2520as%2520a%2520supervision%2520to%250Ajointly%2520fine-tune%2520the%2520pre-trained%2520stereo%2520matching%2520model.%2520These%2520proposed%2520methods%250Aare%2520extensively%2520evaluated%2520on%2520SceneFlow%252C%2520KITTI%252C%2520Middbury%25202014%2520and%2520ETH3D.%2520The%250ASR-Stereo%2520achieves%2520competitive%2520disparity%2520estimation%2520performance%2520and%250Astate-of-the-art%2520cross-domain%2520generalisation%2520performance.%2520Meanwhile%252C%2520the%250Aproposed%2520DAPE%2520significantly%2520improves%2520the%2520disparity%2520estimation%2520performance%2520of%250Afine-tuned%2520models%252C%2520especially%2520in%2520the%2520textureless%2520and%2520detail%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stepwise%20Regression%20and%20Pre-trained%20Edge%20for%20Robust%20Stereo%20Matching&entry.906535625=Weiqing%20Xiao%20and%20Wei%20Zhao&entry.1292438233=%20%20Due%20to%20the%20difficulty%20in%20obtaining%20real%20samples%20and%20ground%20truth%2C%20the%0Ageneralization%20performance%20and%20the%20fine-tuned%20performance%20are%20critical%20for%20the%0Afeasibility%20of%20stereo%20matching%20methods%20in%20real-world%20applications.%20However%2C%20the%0Apresence%20of%20substantial%20disparity%20distributions%20and%20density%20variations%20across%0Adifferent%20datasets%20presents%20significant%20challenges%20for%20the%20generalization%20and%0Afine-tuning%20of%20the%20model.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20stereo%20matching%0Amethod%2C%20called%20SR-Stereo%2C%20which%20mitigates%20the%20distributional%20differences%20across%0Adifferent%20datasets%20by%20predicting%20the%20disparity%20clips%20and%20uses%20a%20loss%20weight%0Arelated%20to%20the%20regression%20target%20scale%20to%20improve%20the%20accuracy%20of%20the%20disparity%0Aclips.%20Moreover%2C%20this%20stepwise%20regression%20architecture%20can%20be%20easily%20extended%0Ato%20existing%20iteration-based%20methods%20to%20improve%20the%20performance%20without%20changing%0Athe%20structure.%20In%20addition%2C%20to%20mitigate%20the%20edge%20blurring%20of%20the%20fine-tuned%0Amodel%20on%20sparse%20ground%20truth%2C%20we%20propose%20Domain%20Adaptation%20Based%20on%20Pre-trained%0AEdges%20%28DAPE%29.%20Specifically%2C%20we%20use%20the%20predicted%20disparity%20and%20RGB%20image%20to%0Aestimate%20the%20edge%20map%20of%20the%20target%20domain%20image.%20The%20edge%20map%20is%20filtered%20to%0Agenerate%20edge%20map%20background%20pseudo-labels%2C%20which%20together%20with%20the%20sparse%0Aground%20truth%20disparity%20on%20the%20target%20domain%20are%20used%20as%20a%20supervision%20to%0Ajointly%20fine-tune%20the%20pre-trained%20stereo%20matching%20model.%20These%20proposed%20methods%0Aare%20extensively%20evaluated%20on%20SceneFlow%2C%20KITTI%2C%20Middbury%202014%20and%20ETH3D.%20The%0ASR-Stereo%20achieves%20competitive%20disparity%20estimation%20performance%20and%0Astate-of-the-art%20cross-domain%20generalisation%20performance.%20Meanwhile%2C%20the%0Aproposed%20DAPE%20significantly%20improves%20the%20disparity%20estimation%20performance%20of%0Afine-tuned%20models%2C%20especially%20in%20the%20textureless%20and%20detail%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06953v2&entry.124074799=Read"},
{"title": "How structured are the representations in transformer-based vision\n  encoders? An analysis of multi-object representations in vision-language\n  models", "author": "Tarun Khajuria and Braian Olmiro Dias and Jaan Aru", "abstract": "  Forming and using symbol-like structured representations for reasoning has\nbeen considered essential for generalising over novel inputs. The primary tool\nthat allows generalisation outside training data distribution is the ability to\nabstract away irrelevant information into a compact form relevant to the task.\nAn extreme form of such abstract representations is symbols. Humans make use of\nsymbols to bind information while abstracting away irrelevant parts to utilise\nthe information consistently and meaningfully. This work estimates the state of\nsuch structured representations in vision encoders. Specifically, we evaluate\nimage encoders in large vision-language pre-trained models to address the\nquestion of which desirable properties their representations lack by applying\nthe criteria of symbolic structured reasoning described for LLMs to the image\nmodels. We test the representation space of image encoders like VIT, BLIP,\nCLIP, and FLAVA to characterise the distribution of the object representations\nin these models. In particular, we create decoding tasks using multi-object\nscenes from the COCO dataset, relating the token space to its input content for\nvarious objects in the scene. We use these tasks to characterise the network's\ntoken and layer-wise information modelling. Our analysis highlights that the\nCLS token, used for the downstream task, only focuses on a few objects\nnecessary for the trained downstream task. Still, other individual objects are\nwell-modelled separately by the tokens in the network originating from those\nobjects. We further observed a widespread distribution of scene information.\nThis demonstrates that information is far more entangled in tokens than optimal\nfor representing objects similar to symbols. Given these symbolic properties,\nwe show the network dynamics that cause failure modes of these models on basic\ndownstream tasks in a multi-object scene.\n", "link": "http://arxiv.org/abs/2406.09067v1", "date": "2024-06-13", "relevancy": 2.2463, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5937}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5437}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20structured%20are%20the%20representations%20in%20transformer-based%20vision%0A%20%20encoders%3F%20An%20analysis%20of%20multi-object%20representations%20in%20vision-language%0A%20%20models&body=Title%3A%20How%20structured%20are%20the%20representations%20in%20transformer-based%20vision%0A%20%20encoders%3F%20An%20analysis%20of%20multi-object%20representations%20in%20vision-language%0A%20%20models%0AAuthor%3A%20Tarun%20Khajuria%20and%20Braian%20Olmiro%20Dias%20and%20Jaan%20Aru%0AAbstract%3A%20%20%20Forming%20and%20using%20symbol-like%20structured%20representations%20for%20reasoning%20has%0Abeen%20considered%20essential%20for%20generalising%20over%20novel%20inputs.%20The%20primary%20tool%0Athat%20allows%20generalisation%20outside%20training%20data%20distribution%20is%20the%20ability%20to%0Aabstract%20away%20irrelevant%20information%20into%20a%20compact%20form%20relevant%20to%20the%20task.%0AAn%20extreme%20form%20of%20such%20abstract%20representations%20is%20symbols.%20Humans%20make%20use%20of%0Asymbols%20to%20bind%20information%20while%20abstracting%20away%20irrelevant%20parts%20to%20utilise%0Athe%20information%20consistently%20and%20meaningfully.%20This%20work%20estimates%20the%20state%20of%0Asuch%20structured%20representations%20in%20vision%20encoders.%20Specifically%2C%20we%20evaluate%0Aimage%20encoders%20in%20large%20vision-language%20pre-trained%20models%20to%20address%20the%0Aquestion%20of%20which%20desirable%20properties%20their%20representations%20lack%20by%20applying%0Athe%20criteria%20of%20symbolic%20structured%20reasoning%20described%20for%20LLMs%20to%20the%20image%0Amodels.%20We%20test%20the%20representation%20space%20of%20image%20encoders%20like%20VIT%2C%20BLIP%2C%0ACLIP%2C%20and%20FLAVA%20to%20characterise%20the%20distribution%20of%20the%20object%20representations%0Ain%20these%20models.%20In%20particular%2C%20we%20create%20decoding%20tasks%20using%20multi-object%0Ascenes%20from%20the%20COCO%20dataset%2C%20relating%20the%20token%20space%20to%20its%20input%20content%20for%0Avarious%20objects%20in%20the%20scene.%20We%20use%20these%20tasks%20to%20characterise%20the%20network%27s%0Atoken%20and%20layer-wise%20information%20modelling.%20Our%20analysis%20highlights%20that%20the%0ACLS%20token%2C%20used%20for%20the%20downstream%20task%2C%20only%20focuses%20on%20a%20few%20objects%0Anecessary%20for%20the%20trained%20downstream%20task.%20Still%2C%20other%20individual%20objects%20are%0Awell-modelled%20separately%20by%20the%20tokens%20in%20the%20network%20originating%20from%20those%0Aobjects.%20We%20further%20observed%20a%20widespread%20distribution%20of%20scene%20information.%0AThis%20demonstrates%20that%20information%20is%20far%20more%20entangled%20in%20tokens%20than%20optimal%0Afor%20representing%20objects%20similar%20to%20symbols.%20Given%20these%20symbolic%20properties%2C%0Awe%20show%20the%20network%20dynamics%20that%20cause%20failure%20modes%20of%20these%20models%20on%20basic%0Adownstream%20tasks%20in%20a%20multi-object%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520structured%2520are%2520the%2520representations%2520in%2520transformer-based%2520vision%250A%2520%2520encoders%253F%2520An%2520analysis%2520of%2520multi-object%2520representations%2520in%2520vision-language%250A%2520%2520models%26entry.906535625%3DTarun%2520Khajuria%2520and%2520Braian%2520Olmiro%2520Dias%2520and%2520Jaan%2520Aru%26entry.1292438233%3D%2520%2520Forming%2520and%2520using%2520symbol-like%2520structured%2520representations%2520for%2520reasoning%2520has%250Abeen%2520considered%2520essential%2520for%2520generalising%2520over%2520novel%2520inputs.%2520The%2520primary%2520tool%250Athat%2520allows%2520generalisation%2520outside%2520training%2520data%2520distribution%2520is%2520the%2520ability%2520to%250Aabstract%2520away%2520irrelevant%2520information%2520into%2520a%2520compact%2520form%2520relevant%2520to%2520the%2520task.%250AAn%2520extreme%2520form%2520of%2520such%2520abstract%2520representations%2520is%2520symbols.%2520Humans%2520make%2520use%2520of%250Asymbols%2520to%2520bind%2520information%2520while%2520abstracting%2520away%2520irrelevant%2520parts%2520to%2520utilise%250Athe%2520information%2520consistently%2520and%2520meaningfully.%2520This%2520work%2520estimates%2520the%2520state%2520of%250Asuch%2520structured%2520representations%2520in%2520vision%2520encoders.%2520Specifically%252C%2520we%2520evaluate%250Aimage%2520encoders%2520in%2520large%2520vision-language%2520pre-trained%2520models%2520to%2520address%2520the%250Aquestion%2520of%2520which%2520desirable%2520properties%2520their%2520representations%2520lack%2520by%2520applying%250Athe%2520criteria%2520of%2520symbolic%2520structured%2520reasoning%2520described%2520for%2520LLMs%2520to%2520the%2520image%250Amodels.%2520We%2520test%2520the%2520representation%2520space%2520of%2520image%2520encoders%2520like%2520VIT%252C%2520BLIP%252C%250ACLIP%252C%2520and%2520FLAVA%2520to%2520characterise%2520the%2520distribution%2520of%2520the%2520object%2520representations%250Ain%2520these%2520models.%2520In%2520particular%252C%2520we%2520create%2520decoding%2520tasks%2520using%2520multi-object%250Ascenes%2520from%2520the%2520COCO%2520dataset%252C%2520relating%2520the%2520token%2520space%2520to%2520its%2520input%2520content%2520for%250Avarious%2520objects%2520in%2520the%2520scene.%2520We%2520use%2520these%2520tasks%2520to%2520characterise%2520the%2520network%2527s%250Atoken%2520and%2520layer-wise%2520information%2520modelling.%2520Our%2520analysis%2520highlights%2520that%2520the%250ACLS%2520token%252C%2520used%2520for%2520the%2520downstream%2520task%252C%2520only%2520focuses%2520on%2520a%2520few%2520objects%250Anecessary%2520for%2520the%2520trained%2520downstream%2520task.%2520Still%252C%2520other%2520individual%2520objects%2520are%250Awell-modelled%2520separately%2520by%2520the%2520tokens%2520in%2520the%2520network%2520originating%2520from%2520those%250Aobjects.%2520We%2520further%2520observed%2520a%2520widespread%2520distribution%2520of%2520scene%2520information.%250AThis%2520demonstrates%2520that%2520information%2520is%2520far%2520more%2520entangled%2520in%2520tokens%2520than%2520optimal%250Afor%2520representing%2520objects%2520similar%2520to%2520symbols.%2520Given%2520these%2520symbolic%2520properties%252C%250Awe%2520show%2520the%2520network%2520dynamics%2520that%2520cause%2520failure%2520modes%2520of%2520these%2520models%2520on%2520basic%250Adownstream%2520tasks%2520in%2520a%2520multi-object%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20structured%20are%20the%20representations%20in%20transformer-based%20vision%0A%20%20encoders%3F%20An%20analysis%20of%20multi-object%20representations%20in%20vision-language%0A%20%20models&entry.906535625=Tarun%20Khajuria%20and%20Braian%20Olmiro%20Dias%20and%20Jaan%20Aru&entry.1292438233=%20%20Forming%20and%20using%20symbol-like%20structured%20representations%20for%20reasoning%20has%0Abeen%20considered%20essential%20for%20generalising%20over%20novel%20inputs.%20The%20primary%20tool%0Athat%20allows%20generalisation%20outside%20training%20data%20distribution%20is%20the%20ability%20to%0Aabstract%20away%20irrelevant%20information%20into%20a%20compact%20form%20relevant%20to%20the%20task.%0AAn%20extreme%20form%20of%20such%20abstract%20representations%20is%20symbols.%20Humans%20make%20use%20of%0Asymbols%20to%20bind%20information%20while%20abstracting%20away%20irrelevant%20parts%20to%20utilise%0Athe%20information%20consistently%20and%20meaningfully.%20This%20work%20estimates%20the%20state%20of%0Asuch%20structured%20representations%20in%20vision%20encoders.%20Specifically%2C%20we%20evaluate%0Aimage%20encoders%20in%20large%20vision-language%20pre-trained%20models%20to%20address%20the%0Aquestion%20of%20which%20desirable%20properties%20their%20representations%20lack%20by%20applying%0Athe%20criteria%20of%20symbolic%20structured%20reasoning%20described%20for%20LLMs%20to%20the%20image%0Amodels.%20We%20test%20the%20representation%20space%20of%20image%20encoders%20like%20VIT%2C%20BLIP%2C%0ACLIP%2C%20and%20FLAVA%20to%20characterise%20the%20distribution%20of%20the%20object%20representations%0Ain%20these%20models.%20In%20particular%2C%20we%20create%20decoding%20tasks%20using%20multi-object%0Ascenes%20from%20the%20COCO%20dataset%2C%20relating%20the%20token%20space%20to%20its%20input%20content%20for%0Avarious%20objects%20in%20the%20scene.%20We%20use%20these%20tasks%20to%20characterise%20the%20network%27s%0Atoken%20and%20layer-wise%20information%20modelling.%20Our%20analysis%20highlights%20that%20the%0ACLS%20token%2C%20used%20for%20the%20downstream%20task%2C%20only%20focuses%20on%20a%20few%20objects%0Anecessary%20for%20the%20trained%20downstream%20task.%20Still%2C%20other%20individual%20objects%20are%0Awell-modelled%20separately%20by%20the%20tokens%20in%20the%20network%20originating%20from%20those%0Aobjects.%20We%20further%20observed%20a%20widespread%20distribution%20of%20scene%20information.%0AThis%20demonstrates%20that%20information%20is%20far%20more%20entangled%20in%20tokens%20than%20optimal%0Afor%20representing%20objects%20similar%20to%20symbols.%20Given%20these%20symbolic%20properties%2C%0Awe%20show%20the%20network%20dynamics%20that%20cause%20failure%20modes%20of%20these%20models%20on%20basic%0Adownstream%20tasks%20in%20a%20multi-object%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09067v1&entry.124074799=Read"},
{"title": "Scene Graph Generation in Large-Size VHR Satellite Imagery: A\n  Large-Scale Dataset and A Context-Aware Approach", "author": "Yansheng Li and Linlin Wang and Tingzhu Wang and Xue Yang and Junwei Luo and Qi Wang and Youming Deng and Wenbin Wang and Xian Sun and Haifeng Li and Bo Dang and Yongjun Zhang and Yi Yu and Junchi Yan", "abstract": "  Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting\nintelligent understanding of geospatial scenarios from perception to cognition.\nIn SAI, objects exhibit great variations in scales and aspect ratios, and there\nexist rich relationships between objects (even between spatially disjoint\nobjects), which makes it necessary to holistically conduct SGG in large-size\nvery-high-resolution (VHR) SAI. However, the lack of SGG datasets with\nlarge-size VHR SAI has constrained the advancement of SGG in SAI. Due to the\ncomplexity of large-size VHR SAI, mining triplets <subject, relationship,\nobject> in large-size VHR SAI heavily relies on long-range contextual\nreasoning. Consequently, SGG models designed for small-size natural imagery are\nnot directly applicable to large-size VHR SAI. To address the scarcity of\ndatasets, this paper constructs a large-scale dataset for SGG in large-size VHR\nSAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, named\nRSG, encompassing over 210,000 objects and more than 400,000 triplets. To\nrealize SGG in large-size VHR SAI, we propose a context-aware cascade cognition\n(CAC) framework to understand SAI at three levels: object detection (OBD), pair\npruning and relationship prediction. As a fundamental prerequisite for SGG in\nlarge-size SAI, a holistic multi-class object detection network (HOD-Net) that\ncan flexibly integrate multi-scale contexts is proposed. With the consideration\nthat there exist a huge amount of object pairs in large-size SAI but only a\nminority of object pairs contain meaningful relationships, we design a pair\nproposal generation (PPG) network via adversarial reconstruction to select\nhigh-value pairs. Furthermore, a relationship prediction network with\ncontext-aware messaging (RPCM) is proposed to predict the relationship types of\nthese pairs.\n", "link": "http://arxiv.org/abs/2406.09410v1", "date": "2024-06-13", "relevancy": 2.2392, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5842}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5765}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene%20Graph%20Generation%20in%20Large-Size%20VHR%20Satellite%20Imagery%3A%20A%0A%20%20Large-Scale%20Dataset%20and%20A%20Context-Aware%20Approach&body=Title%3A%20Scene%20Graph%20Generation%20in%20Large-Size%20VHR%20Satellite%20Imagery%3A%20A%0A%20%20Large-Scale%20Dataset%20and%20A%20Context-Aware%20Approach%0AAuthor%3A%20Yansheng%20Li%20and%20Linlin%20Wang%20and%20Tingzhu%20Wang%20and%20Xue%20Yang%20and%20Junwei%20Luo%20and%20Qi%20Wang%20and%20Youming%20Deng%20and%20Wenbin%20Wang%20and%20Xian%20Sun%20and%20Haifeng%20Li%20and%20Bo%20Dang%20and%20Yongjun%20Zhang%20and%20Yi%20Yu%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20Scene%20graph%20generation%20%28SGG%29%20in%20satellite%20imagery%20%28SAI%29%20benefits%20promoting%0Aintelligent%20understanding%20of%20geospatial%20scenarios%20from%20perception%20to%20cognition.%0AIn%20SAI%2C%20objects%20exhibit%20great%20variations%20in%20scales%20and%20aspect%20ratios%2C%20and%20there%0Aexist%20rich%20relationships%20between%20objects%20%28even%20between%20spatially%20disjoint%0Aobjects%29%2C%20which%20makes%20it%20necessary%20to%20holistically%20conduct%20SGG%20in%20large-size%0Avery-high-resolution%20%28VHR%29%20SAI.%20However%2C%20the%20lack%20of%20SGG%20datasets%20with%0Alarge-size%20VHR%20SAI%20has%20constrained%20the%20advancement%20of%20SGG%20in%20SAI.%20Due%20to%20the%0Acomplexity%20of%20large-size%20VHR%20SAI%2C%20mining%20triplets%20%3Csubject%2C%20relationship%2C%0Aobject%3E%20in%20large-size%20VHR%20SAI%20heavily%20relies%20on%20long-range%20contextual%0Areasoning.%20Consequently%2C%20SGG%20models%20designed%20for%20small-size%20natural%20imagery%20are%0Anot%20directly%20applicable%20to%20large-size%20VHR%20SAI.%20To%20address%20the%20scarcity%20of%0Adatasets%2C%20this%20paper%20constructs%20a%20large-scale%20dataset%20for%20SGG%20in%20large-size%20VHR%0ASAI%20with%20image%20sizes%20ranging%20from%20512%20x%20768%20to%2027%2C860%20x%2031%2C096%20pixels%2C%20named%0ARSG%2C%20encompassing%20over%20210%2C000%20objects%20and%20more%20than%20400%2C000%20triplets.%20To%0Arealize%20SGG%20in%20large-size%20VHR%20SAI%2C%20we%20propose%20a%20context-aware%20cascade%20cognition%0A%28CAC%29%20framework%20to%20understand%20SAI%20at%20three%20levels%3A%20object%20detection%20%28OBD%29%2C%20pair%0Apruning%20and%20relationship%20prediction.%20As%20a%20fundamental%20prerequisite%20for%20SGG%20in%0Alarge-size%20SAI%2C%20a%20holistic%20multi-class%20object%20detection%20network%20%28HOD-Net%29%20that%0Acan%20flexibly%20integrate%20multi-scale%20contexts%20is%20proposed.%20With%20the%20consideration%0Athat%20there%20exist%20a%20huge%20amount%20of%20object%20pairs%20in%20large-size%20SAI%20but%20only%20a%0Aminority%20of%20object%20pairs%20contain%20meaningful%20relationships%2C%20we%20design%20a%20pair%0Aproposal%20generation%20%28PPG%29%20network%20via%20adversarial%20reconstruction%20to%20select%0Ahigh-value%20pairs.%20Furthermore%2C%20a%20relationship%20prediction%20network%20with%0Acontext-aware%20messaging%20%28RPCM%29%20is%20proposed%20to%20predict%20the%20relationship%20types%20of%0Athese%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene%2520Graph%2520Generation%2520in%2520Large-Size%2520VHR%2520Satellite%2520Imagery%253A%2520A%250A%2520%2520Large-Scale%2520Dataset%2520and%2520A%2520Context-Aware%2520Approach%26entry.906535625%3DYansheng%2520Li%2520and%2520Linlin%2520Wang%2520and%2520Tingzhu%2520Wang%2520and%2520Xue%2520Yang%2520and%2520Junwei%2520Luo%2520and%2520Qi%2520Wang%2520and%2520Youming%2520Deng%2520and%2520Wenbin%2520Wang%2520and%2520Xian%2520Sun%2520and%2520Haifeng%2520Li%2520and%2520Bo%2520Dang%2520and%2520Yongjun%2520Zhang%2520and%2520Yi%2520Yu%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520Scene%2520graph%2520generation%2520%2528SGG%2529%2520in%2520satellite%2520imagery%2520%2528SAI%2529%2520benefits%2520promoting%250Aintelligent%2520understanding%2520of%2520geospatial%2520scenarios%2520from%2520perception%2520to%2520cognition.%250AIn%2520SAI%252C%2520objects%2520exhibit%2520great%2520variations%2520in%2520scales%2520and%2520aspect%2520ratios%252C%2520and%2520there%250Aexist%2520rich%2520relationships%2520between%2520objects%2520%2528even%2520between%2520spatially%2520disjoint%250Aobjects%2529%252C%2520which%2520makes%2520it%2520necessary%2520to%2520holistically%2520conduct%2520SGG%2520in%2520large-size%250Avery-high-resolution%2520%2528VHR%2529%2520SAI.%2520However%252C%2520the%2520lack%2520of%2520SGG%2520datasets%2520with%250Alarge-size%2520VHR%2520SAI%2520has%2520constrained%2520the%2520advancement%2520of%2520SGG%2520in%2520SAI.%2520Due%2520to%2520the%250Acomplexity%2520of%2520large-size%2520VHR%2520SAI%252C%2520mining%2520triplets%2520%253Csubject%252C%2520relationship%252C%250Aobject%253E%2520in%2520large-size%2520VHR%2520SAI%2520heavily%2520relies%2520on%2520long-range%2520contextual%250Areasoning.%2520Consequently%252C%2520SGG%2520models%2520designed%2520for%2520small-size%2520natural%2520imagery%2520are%250Anot%2520directly%2520applicable%2520to%2520large-size%2520VHR%2520SAI.%2520To%2520address%2520the%2520scarcity%2520of%250Adatasets%252C%2520this%2520paper%2520constructs%2520a%2520large-scale%2520dataset%2520for%2520SGG%2520in%2520large-size%2520VHR%250ASAI%2520with%2520image%2520sizes%2520ranging%2520from%2520512%2520x%2520768%2520to%252027%252C860%2520x%252031%252C096%2520pixels%252C%2520named%250ARSG%252C%2520encompassing%2520over%2520210%252C000%2520objects%2520and%2520more%2520than%2520400%252C000%2520triplets.%2520To%250Arealize%2520SGG%2520in%2520large-size%2520VHR%2520SAI%252C%2520we%2520propose%2520a%2520context-aware%2520cascade%2520cognition%250A%2528CAC%2529%2520framework%2520to%2520understand%2520SAI%2520at%2520three%2520levels%253A%2520object%2520detection%2520%2528OBD%2529%252C%2520pair%250Apruning%2520and%2520relationship%2520prediction.%2520As%2520a%2520fundamental%2520prerequisite%2520for%2520SGG%2520in%250Alarge-size%2520SAI%252C%2520a%2520holistic%2520multi-class%2520object%2520detection%2520network%2520%2528HOD-Net%2529%2520that%250Acan%2520flexibly%2520integrate%2520multi-scale%2520contexts%2520is%2520proposed.%2520With%2520the%2520consideration%250Athat%2520there%2520exist%2520a%2520huge%2520amount%2520of%2520object%2520pairs%2520in%2520large-size%2520SAI%2520but%2520only%2520a%250Aminority%2520of%2520object%2520pairs%2520contain%2520meaningful%2520relationships%252C%2520we%2520design%2520a%2520pair%250Aproposal%2520generation%2520%2528PPG%2529%2520network%2520via%2520adversarial%2520reconstruction%2520to%2520select%250Ahigh-value%2520pairs.%2520Furthermore%252C%2520a%2520relationship%2520prediction%2520network%2520with%250Acontext-aware%2520messaging%2520%2528RPCM%2529%2520is%2520proposed%2520to%2520predict%2520the%2520relationship%2520types%2520of%250Athese%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene%20Graph%20Generation%20in%20Large-Size%20VHR%20Satellite%20Imagery%3A%20A%0A%20%20Large-Scale%20Dataset%20and%20A%20Context-Aware%20Approach&entry.906535625=Yansheng%20Li%20and%20Linlin%20Wang%20and%20Tingzhu%20Wang%20and%20Xue%20Yang%20and%20Junwei%20Luo%20and%20Qi%20Wang%20and%20Youming%20Deng%20and%20Wenbin%20Wang%20and%20Xian%20Sun%20and%20Haifeng%20Li%20and%20Bo%20Dang%20and%20Yongjun%20Zhang%20and%20Yi%20Yu%20and%20Junchi%20Yan&entry.1292438233=%20%20Scene%20graph%20generation%20%28SGG%29%20in%20satellite%20imagery%20%28SAI%29%20benefits%20promoting%0Aintelligent%20understanding%20of%20geospatial%20scenarios%20from%20perception%20to%20cognition.%0AIn%20SAI%2C%20objects%20exhibit%20great%20variations%20in%20scales%20and%20aspect%20ratios%2C%20and%20there%0Aexist%20rich%20relationships%20between%20objects%20%28even%20between%20spatially%20disjoint%0Aobjects%29%2C%20which%20makes%20it%20necessary%20to%20holistically%20conduct%20SGG%20in%20large-size%0Avery-high-resolution%20%28VHR%29%20SAI.%20However%2C%20the%20lack%20of%20SGG%20datasets%20with%0Alarge-size%20VHR%20SAI%20has%20constrained%20the%20advancement%20of%20SGG%20in%20SAI.%20Due%20to%20the%0Acomplexity%20of%20large-size%20VHR%20SAI%2C%20mining%20triplets%20%3Csubject%2C%20relationship%2C%0Aobject%3E%20in%20large-size%20VHR%20SAI%20heavily%20relies%20on%20long-range%20contextual%0Areasoning.%20Consequently%2C%20SGG%20models%20designed%20for%20small-size%20natural%20imagery%20are%0Anot%20directly%20applicable%20to%20large-size%20VHR%20SAI.%20To%20address%20the%20scarcity%20of%0Adatasets%2C%20this%20paper%20constructs%20a%20large-scale%20dataset%20for%20SGG%20in%20large-size%20VHR%0ASAI%20with%20image%20sizes%20ranging%20from%20512%20x%20768%20to%2027%2C860%20x%2031%2C096%20pixels%2C%20named%0ARSG%2C%20encompassing%20over%20210%2C000%20objects%20and%20more%20than%20400%2C000%20triplets.%20To%0Arealize%20SGG%20in%20large-size%20VHR%20SAI%2C%20we%20propose%20a%20context-aware%20cascade%20cognition%0A%28CAC%29%20framework%20to%20understand%20SAI%20at%20three%20levels%3A%20object%20detection%20%28OBD%29%2C%20pair%0Apruning%20and%20relationship%20prediction.%20As%20a%20fundamental%20prerequisite%20for%20SGG%20in%0Alarge-size%20SAI%2C%20a%20holistic%20multi-class%20object%20detection%20network%20%28HOD-Net%29%20that%0Acan%20flexibly%20integrate%20multi-scale%20contexts%20is%20proposed.%20With%20the%20consideration%0Athat%20there%20exist%20a%20huge%20amount%20of%20object%20pairs%20in%20large-size%20SAI%20but%20only%20a%0Aminority%20of%20object%20pairs%20contain%20meaningful%20relationships%2C%20we%20design%20a%20pair%0Aproposal%20generation%20%28PPG%29%20network%20via%20adversarial%20reconstruction%20to%20select%0Ahigh-value%20pairs.%20Furthermore%2C%20a%20relationship%20prediction%20network%20with%0Acontext-aware%20messaging%20%28RPCM%29%20is%20proposed%20to%20predict%20the%20relationship%20types%20of%0Athese%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09410v1&entry.124074799=Read"},
{"title": "Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset", "author": "Yiming Li and Zhiheng Li and Nuo Chen and Moonjun Gong and Zonglin Lyu and Zehong Wang and Peili Jiang and Chen Feng", "abstract": "  Large-scale datasets have fueled recent advancements in AI-based autonomous\nvehicle research. However, these datasets are usually collected from a single\nvehicle's one-time pass of a certain location, lacking multiagent interactions\nor repeated traversals of the same place. Such information could lead to\ntransformative enhancements in autonomous vehicles' perception, prediction, and\nplanning capabilities. To bridge this gap, in collaboration with the\nself-driving company May Mobility, we present the MARS dataset which unifies\nscenarios that enable MultiAgent, multitraveRSal, and multimodal autonomous\nvehicle research. More specifically, MARS is collected with a fleet of\nautonomous vehicles driving within a certain geographical area. Each vehicle\nhas its own route and different vehicles may appear at nearby locations. Each\nvehicle is equipped with a LiDAR and surround-view RGB cameras. We curate two\nsubsets in MARS: one facilitates collaborative driving with multiple vehicles\nsimultaneously present at the same location, and the other enables memory\nretrospection through asynchronous traversals of the same location by multiple\nvehicles. We conduct experiments in place recognition and neural\nreconstruction. More importantly, MARS introduces new research opportunities\nand challenges such as multitraversal 3D reconstruction, multiagent perception,\nand unsupervised object discovery. Our data and codes can be found at\nhttps://ai4ce.github.io/MARS/.\n", "link": "http://arxiv.org/abs/2406.09383v1", "date": "2024-06-13", "relevancy": 2.2296, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5524}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiagent%20Multitraversal%20Multimodal%20Self-Driving%3A%20Open%20MARS%20Dataset&body=Title%3A%20Multiagent%20Multitraversal%20Multimodal%20Self-Driving%3A%20Open%20MARS%20Dataset%0AAuthor%3A%20Yiming%20Li%20and%20Zhiheng%20Li%20and%20Nuo%20Chen%20and%20Moonjun%20Gong%20and%20Zonglin%20Lyu%20and%20Zehong%20Wang%20and%20Peili%20Jiang%20and%20Chen%20Feng%0AAbstract%3A%20%20%20Large-scale%20datasets%20have%20fueled%20recent%20advancements%20in%20AI-based%20autonomous%0Avehicle%20research.%20However%2C%20these%20datasets%20are%20usually%20collected%20from%20a%20single%0Avehicle%27s%20one-time%20pass%20of%20a%20certain%20location%2C%20lacking%20multiagent%20interactions%0Aor%20repeated%20traversals%20of%20the%20same%20place.%20Such%20information%20could%20lead%20to%0Atransformative%20enhancements%20in%20autonomous%20vehicles%27%20perception%2C%20prediction%2C%20and%0Aplanning%20capabilities.%20To%20bridge%20this%20gap%2C%20in%20collaboration%20with%20the%0Aself-driving%20company%20May%20Mobility%2C%20we%20present%20the%20MARS%20dataset%20which%20unifies%0Ascenarios%20that%20enable%20MultiAgent%2C%20multitraveRSal%2C%20and%20multimodal%20autonomous%0Avehicle%20research.%20More%20specifically%2C%20MARS%20is%20collected%20with%20a%20fleet%20of%0Aautonomous%20vehicles%20driving%20within%20a%20certain%20geographical%20area.%20Each%20vehicle%0Ahas%20its%20own%20route%20and%20different%20vehicles%20may%20appear%20at%20nearby%20locations.%20Each%0Avehicle%20is%20equipped%20with%20a%20LiDAR%20and%20surround-view%20RGB%20cameras.%20We%20curate%20two%0Asubsets%20in%20MARS%3A%20one%20facilitates%20collaborative%20driving%20with%20multiple%20vehicles%0Asimultaneously%20present%20at%20the%20same%20location%2C%20and%20the%20other%20enables%20memory%0Aretrospection%20through%20asynchronous%20traversals%20of%20the%20same%20location%20by%20multiple%0Avehicles.%20We%20conduct%20experiments%20in%20place%20recognition%20and%20neural%0Areconstruction.%20More%20importantly%2C%20MARS%20introduces%20new%20research%20opportunities%0Aand%20challenges%20such%20as%20multitraversal%203D%20reconstruction%2C%20multiagent%20perception%2C%0Aand%20unsupervised%20object%20discovery.%20Our%20data%20and%20codes%20can%20be%20found%20at%0Ahttps%3A//ai4ce.github.io/MARS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiagent%2520Multitraversal%2520Multimodal%2520Self-Driving%253A%2520Open%2520MARS%2520Dataset%26entry.906535625%3DYiming%2520Li%2520and%2520Zhiheng%2520Li%2520and%2520Nuo%2520Chen%2520and%2520Moonjun%2520Gong%2520and%2520Zonglin%2520Lyu%2520and%2520Zehong%2520Wang%2520and%2520Peili%2520Jiang%2520and%2520Chen%2520Feng%26entry.1292438233%3D%2520%2520Large-scale%2520datasets%2520have%2520fueled%2520recent%2520advancements%2520in%2520AI-based%2520autonomous%250Avehicle%2520research.%2520However%252C%2520these%2520datasets%2520are%2520usually%2520collected%2520from%2520a%2520single%250Avehicle%2527s%2520one-time%2520pass%2520of%2520a%2520certain%2520location%252C%2520lacking%2520multiagent%2520interactions%250Aor%2520repeated%2520traversals%2520of%2520the%2520same%2520place.%2520Such%2520information%2520could%2520lead%2520to%250Atransformative%2520enhancements%2520in%2520autonomous%2520vehicles%2527%2520perception%252C%2520prediction%252C%2520and%250Aplanning%2520capabilities.%2520To%2520bridge%2520this%2520gap%252C%2520in%2520collaboration%2520with%2520the%250Aself-driving%2520company%2520May%2520Mobility%252C%2520we%2520present%2520the%2520MARS%2520dataset%2520which%2520unifies%250Ascenarios%2520that%2520enable%2520MultiAgent%252C%2520multitraveRSal%252C%2520and%2520multimodal%2520autonomous%250Avehicle%2520research.%2520More%2520specifically%252C%2520MARS%2520is%2520collected%2520with%2520a%2520fleet%2520of%250Aautonomous%2520vehicles%2520driving%2520within%2520a%2520certain%2520geographical%2520area.%2520Each%2520vehicle%250Ahas%2520its%2520own%2520route%2520and%2520different%2520vehicles%2520may%2520appear%2520at%2520nearby%2520locations.%2520Each%250Avehicle%2520is%2520equipped%2520with%2520a%2520LiDAR%2520and%2520surround-view%2520RGB%2520cameras.%2520We%2520curate%2520two%250Asubsets%2520in%2520MARS%253A%2520one%2520facilitates%2520collaborative%2520driving%2520with%2520multiple%2520vehicles%250Asimultaneously%2520present%2520at%2520the%2520same%2520location%252C%2520and%2520the%2520other%2520enables%2520memory%250Aretrospection%2520through%2520asynchronous%2520traversals%2520of%2520the%2520same%2520location%2520by%2520multiple%250Avehicles.%2520We%2520conduct%2520experiments%2520in%2520place%2520recognition%2520and%2520neural%250Areconstruction.%2520More%2520importantly%252C%2520MARS%2520introduces%2520new%2520research%2520opportunities%250Aand%2520challenges%2520such%2520as%2520multitraversal%25203D%2520reconstruction%252C%2520multiagent%2520perception%252C%250Aand%2520unsupervised%2520object%2520discovery.%2520Our%2520data%2520and%2520codes%2520can%2520be%2520found%2520at%250Ahttps%253A//ai4ce.github.io/MARS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiagent%20Multitraversal%20Multimodal%20Self-Driving%3A%20Open%20MARS%20Dataset&entry.906535625=Yiming%20Li%20and%20Zhiheng%20Li%20and%20Nuo%20Chen%20and%20Moonjun%20Gong%20and%20Zonglin%20Lyu%20and%20Zehong%20Wang%20and%20Peili%20Jiang%20and%20Chen%20Feng&entry.1292438233=%20%20Large-scale%20datasets%20have%20fueled%20recent%20advancements%20in%20AI-based%20autonomous%0Avehicle%20research.%20However%2C%20these%20datasets%20are%20usually%20collected%20from%20a%20single%0Avehicle%27s%20one-time%20pass%20of%20a%20certain%20location%2C%20lacking%20multiagent%20interactions%0Aor%20repeated%20traversals%20of%20the%20same%20place.%20Such%20information%20could%20lead%20to%0Atransformative%20enhancements%20in%20autonomous%20vehicles%27%20perception%2C%20prediction%2C%20and%0Aplanning%20capabilities.%20To%20bridge%20this%20gap%2C%20in%20collaboration%20with%20the%0Aself-driving%20company%20May%20Mobility%2C%20we%20present%20the%20MARS%20dataset%20which%20unifies%0Ascenarios%20that%20enable%20MultiAgent%2C%20multitraveRSal%2C%20and%20multimodal%20autonomous%0Avehicle%20research.%20More%20specifically%2C%20MARS%20is%20collected%20with%20a%20fleet%20of%0Aautonomous%20vehicles%20driving%20within%20a%20certain%20geographical%20area.%20Each%20vehicle%0Ahas%20its%20own%20route%20and%20different%20vehicles%20may%20appear%20at%20nearby%20locations.%20Each%0Avehicle%20is%20equipped%20with%20a%20LiDAR%20and%20surround-view%20RGB%20cameras.%20We%20curate%20two%0Asubsets%20in%20MARS%3A%20one%20facilitates%20collaborative%20driving%20with%20multiple%20vehicles%0Asimultaneously%20present%20at%20the%20same%20location%2C%20and%20the%20other%20enables%20memory%0Aretrospection%20through%20asynchronous%20traversals%20of%20the%20same%20location%20by%20multiple%0Avehicles.%20We%20conduct%20experiments%20in%20place%20recognition%20and%20neural%0Areconstruction.%20More%20importantly%2C%20MARS%20introduces%20new%20research%20opportunities%0Aand%20challenges%20such%20as%20multitraversal%203D%20reconstruction%2C%20multiagent%20perception%2C%0Aand%20unsupervised%20object%20discovery.%20Our%20data%20and%20codes%20can%20be%20found%20at%0Ahttps%3A//ai4ce.github.io/MARS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09383v1&entry.124074799=Read"},
{"title": "Needle In A Video Haystack: A Scalable Synthetic Framework for\n  Benchmarking Video MLLMs", "author": "Zijia Zhao and Haoyu Lu and Yuqi Huo and Yifan Du and Tongtian Yue and Longteng Guo and Bingning Wang and Weipeng Chen and Jing Liu", "abstract": "  Video understanding is a crucial next step for multimodal large language\nmodels (MLLMs). To probe specific aspects of video understanding ability,\nexisting video benchmarks typically require careful video selection based on\nthe target capability, along with laborious annotation of query-response pairs\nto match the specific video content. This process is both challenging and\nresource-intensive. In this paper, we propose VideoNIAH (Video Needle In A\nHaystack), a benchmark construction framework through synthetic video\ngeneration. VideoNIAH decouples test video content from their query-responses\nby inserting unrelated image/text 'needles' into original videos. It generates\nannotations solely from these needles, ensuring diversity in video sources and\na variety of query-responses. Additionally, by inserting multiple needles,\nVideoNIAH rigorously evaluates the temporal understanding capabilities of\nmodels. We utilized VideoNIAH to compile a video benchmark VNBench, including\ntasks such as retrieval, ordering, and counting. VNBench can efficiently\nevaluate the fine-grained understanding ability and spatio-temporal modeling\nability of a video model, while also supporting the long-context evaluation.\nAdditionally, we evaluated recent video-centric multimodal large language\nmodels (MLLMs), both open-source and proprietary, providing a comprehensive\nanalysis. We found that although proprietary models have significant advantages\nover open-source models, all existing video models still perform poorly on\nlong-distance dependency tasks. VideoNIAH is a simple yet highly scalable\nbenchmark construction framework, and we believe it will inspire future video\nbenchmark works. The code and data are available at\nhttps://github.com/joez17/VideoNIAH.\n", "link": "http://arxiv.org/abs/2406.09367v1", "date": "2024-06-13", "relevancy": 2.2149, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5682}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5442}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Needle%20In%20A%20Video%20Haystack%3A%20A%20Scalable%20Synthetic%20Framework%20for%0A%20%20Benchmarking%20Video%20MLLMs&body=Title%3A%20Needle%20In%20A%20Video%20Haystack%3A%20A%20Scalable%20Synthetic%20Framework%20for%0A%20%20Benchmarking%20Video%20MLLMs%0AAuthor%3A%20Zijia%20Zhao%20and%20Haoyu%20Lu%20and%20Yuqi%20Huo%20and%20Yifan%20Du%20and%20Tongtian%20Yue%20and%20Longteng%20Guo%20and%20Bingning%20Wang%20and%20Weipeng%20Chen%20and%20Jing%20Liu%0AAbstract%3A%20%20%20Video%20understanding%20is%20a%20crucial%20next%20step%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29.%20To%20probe%20specific%20aspects%20of%20video%20understanding%20ability%2C%0Aexisting%20video%20benchmarks%20typically%20require%20careful%20video%20selection%20based%20on%0Athe%20target%20capability%2C%20along%20with%20laborious%20annotation%20of%20query-response%20pairs%0Ato%20match%20the%20specific%20video%20content.%20This%20process%20is%20both%20challenging%20and%0Aresource-intensive.%20In%20this%20paper%2C%20we%20propose%20VideoNIAH%20%28Video%20Needle%20In%20A%0AHaystack%29%2C%20a%20benchmark%20construction%20framework%20through%20synthetic%20video%0Ageneration.%20VideoNIAH%20decouples%20test%20video%20content%20from%20their%20query-responses%0Aby%20inserting%20unrelated%20image/text%20%27needles%27%20into%20original%20videos.%20It%20generates%0Aannotations%20solely%20from%20these%20needles%2C%20ensuring%20diversity%20in%20video%20sources%20and%0Aa%20variety%20of%20query-responses.%20Additionally%2C%20by%20inserting%20multiple%20needles%2C%0AVideoNIAH%20rigorously%20evaluates%20the%20temporal%20understanding%20capabilities%20of%0Amodels.%20We%20utilized%20VideoNIAH%20to%20compile%20a%20video%20benchmark%20VNBench%2C%20including%0Atasks%20such%20as%20retrieval%2C%20ordering%2C%20and%20counting.%20VNBench%20can%20efficiently%0Aevaluate%20the%20fine-grained%20understanding%20ability%20and%20spatio-temporal%20modeling%0Aability%20of%20a%20video%20model%2C%20while%20also%20supporting%20the%20long-context%20evaluation.%0AAdditionally%2C%20we%20evaluated%20recent%20video-centric%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20both%20open-source%20and%20proprietary%2C%20providing%20a%20comprehensive%0Aanalysis.%20We%20found%20that%20although%20proprietary%20models%20have%20significant%20advantages%0Aover%20open-source%20models%2C%20all%20existing%20video%20models%20still%20perform%20poorly%20on%0Along-distance%20dependency%20tasks.%20VideoNIAH%20is%20a%20simple%20yet%20highly%20scalable%0Abenchmark%20construction%20framework%2C%20and%20we%20believe%20it%20will%20inspire%20future%20video%0Abenchmark%20works.%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/joez17/VideoNIAH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeedle%2520In%2520A%2520Video%2520Haystack%253A%2520A%2520Scalable%2520Synthetic%2520Framework%2520for%250A%2520%2520Benchmarking%2520Video%2520MLLMs%26entry.906535625%3DZijia%2520Zhao%2520and%2520Haoyu%2520Lu%2520and%2520Yuqi%2520Huo%2520and%2520Yifan%2520Du%2520and%2520Tongtian%2520Yue%2520and%2520Longteng%2520Guo%2520and%2520Bingning%2520Wang%2520and%2520Weipeng%2520Chen%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520Video%2520understanding%2520is%2520a%2520crucial%2520next%2520step%2520for%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529.%2520To%2520probe%2520specific%2520aspects%2520of%2520video%2520understanding%2520ability%252C%250Aexisting%2520video%2520benchmarks%2520typically%2520require%2520careful%2520video%2520selection%2520based%2520on%250Athe%2520target%2520capability%252C%2520along%2520with%2520laborious%2520annotation%2520of%2520query-response%2520pairs%250Ato%2520match%2520the%2520specific%2520video%2520content.%2520This%2520process%2520is%2520both%2520challenging%2520and%250Aresource-intensive.%2520In%2520this%2520paper%252C%2520we%2520propose%2520VideoNIAH%2520%2528Video%2520Needle%2520In%2520A%250AHaystack%2529%252C%2520a%2520benchmark%2520construction%2520framework%2520through%2520synthetic%2520video%250Ageneration.%2520VideoNIAH%2520decouples%2520test%2520video%2520content%2520from%2520their%2520query-responses%250Aby%2520inserting%2520unrelated%2520image/text%2520%2527needles%2527%2520into%2520original%2520videos.%2520It%2520generates%250Aannotations%2520solely%2520from%2520these%2520needles%252C%2520ensuring%2520diversity%2520in%2520video%2520sources%2520and%250Aa%2520variety%2520of%2520query-responses.%2520Additionally%252C%2520by%2520inserting%2520multiple%2520needles%252C%250AVideoNIAH%2520rigorously%2520evaluates%2520the%2520temporal%2520understanding%2520capabilities%2520of%250Amodels.%2520We%2520utilized%2520VideoNIAH%2520to%2520compile%2520a%2520video%2520benchmark%2520VNBench%252C%2520including%250Atasks%2520such%2520as%2520retrieval%252C%2520ordering%252C%2520and%2520counting.%2520VNBench%2520can%2520efficiently%250Aevaluate%2520the%2520fine-grained%2520understanding%2520ability%2520and%2520spatio-temporal%2520modeling%250Aability%2520of%2520a%2520video%2520model%252C%2520while%2520also%2520supporting%2520the%2520long-context%2520evaluation.%250AAdditionally%252C%2520we%2520evaluated%2520recent%2520video-centric%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%252C%2520both%2520open-source%2520and%2520proprietary%252C%2520providing%2520a%2520comprehensive%250Aanalysis.%2520We%2520found%2520that%2520although%2520proprietary%2520models%2520have%2520significant%2520advantages%250Aover%2520open-source%2520models%252C%2520all%2520existing%2520video%2520models%2520still%2520perform%2520poorly%2520on%250Along-distance%2520dependency%2520tasks.%2520VideoNIAH%2520is%2520a%2520simple%2520yet%2520highly%2520scalable%250Abenchmark%2520construction%2520framework%252C%2520and%2520we%2520believe%2520it%2520will%2520inspire%2520future%2520video%250Abenchmark%2520works.%2520The%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/joez17/VideoNIAH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Needle%20In%20A%20Video%20Haystack%3A%20A%20Scalable%20Synthetic%20Framework%20for%0A%20%20Benchmarking%20Video%20MLLMs&entry.906535625=Zijia%20Zhao%20and%20Haoyu%20Lu%20and%20Yuqi%20Huo%20and%20Yifan%20Du%20and%20Tongtian%20Yue%20and%20Longteng%20Guo%20and%20Bingning%20Wang%20and%20Weipeng%20Chen%20and%20Jing%20Liu&entry.1292438233=%20%20Video%20understanding%20is%20a%20crucial%20next%20step%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29.%20To%20probe%20specific%20aspects%20of%20video%20understanding%20ability%2C%0Aexisting%20video%20benchmarks%20typically%20require%20careful%20video%20selection%20based%20on%0Athe%20target%20capability%2C%20along%20with%20laborious%20annotation%20of%20query-response%20pairs%0Ato%20match%20the%20specific%20video%20content.%20This%20process%20is%20both%20challenging%20and%0Aresource-intensive.%20In%20this%20paper%2C%20we%20propose%20VideoNIAH%20%28Video%20Needle%20In%20A%0AHaystack%29%2C%20a%20benchmark%20construction%20framework%20through%20synthetic%20video%0Ageneration.%20VideoNIAH%20decouples%20test%20video%20content%20from%20their%20query-responses%0Aby%20inserting%20unrelated%20image/text%20%27needles%27%20into%20original%20videos.%20It%20generates%0Aannotations%20solely%20from%20these%20needles%2C%20ensuring%20diversity%20in%20video%20sources%20and%0Aa%20variety%20of%20query-responses.%20Additionally%2C%20by%20inserting%20multiple%20needles%2C%0AVideoNIAH%20rigorously%20evaluates%20the%20temporal%20understanding%20capabilities%20of%0Amodels.%20We%20utilized%20VideoNIAH%20to%20compile%20a%20video%20benchmark%20VNBench%2C%20including%0Atasks%20such%20as%20retrieval%2C%20ordering%2C%20and%20counting.%20VNBench%20can%20efficiently%0Aevaluate%20the%20fine-grained%20understanding%20ability%20and%20spatio-temporal%20modeling%0Aability%20of%20a%20video%20model%2C%20while%20also%20supporting%20the%20long-context%20evaluation.%0AAdditionally%2C%20we%20evaluated%20recent%20video-centric%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20both%20open-source%20and%20proprietary%2C%20providing%20a%20comprehensive%0Aanalysis.%20We%20found%20that%20although%20proprietary%20models%20have%20significant%20advantages%0Aover%20open-source%20models%2C%20all%20existing%20video%20models%20still%20perform%20poorly%20on%0Along-distance%20dependency%20tasks.%20VideoNIAH%20is%20a%20simple%20yet%20highly%20scalable%0Abenchmark%20construction%20framework%2C%20and%20we%20believe%20it%20will%20inspire%20future%20video%0Abenchmark%20works.%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/joez17/VideoNIAH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09367v1&entry.124074799=Read"},
{"title": "PTA: Enhancing Multimodal Sentiment Analysis through Pipelined\n  Prediction and Translation-based Alignment", "author": "Shezheng Song and Shasha Li and Shan Zhao and Chengyu Wang and Xiaopeng Li and Jie Yu and Qian Wan and Jun Ma and Tianwei Yan and Wentao Ma and Xiaoguang Mao", "abstract": "  Multimodal aspect-based sentiment analysis (MABSA) aims to understand\nopinions in a granular manner, advancing human-computer interaction and other\nfields. Traditionally, MABSA methods use a joint prediction approach to\nidentify aspects and sentiments simultaneously. However, we argue that joint\nmodels are not always superior. Our analysis shows that joint models struggle\nto align relevant text tokens with image patches, leading to misalignment and\nineffective image utilization.\n  In contrast, a pipeline framework first identifies aspects through MATE\n(Multimodal Aspect Term Extraction) and then aligns these aspects with image\npatches for sentiment classification (MASC: Multimodal Aspect-Oriented\nSentiment Classification). This method is better suited for multimodal\nscenarios where effective image use is crucial. We present three key\nobservations: (a) MATE and MASC have different feature requirements, with MATE\nfocusing on token-level features and MASC on sequence-level features; (b) the\naspect identified by MATE is crucial for effective image utilization; and (c)\nimages play a trivial role in previous MABSA methods due to high noise.\n  Based on these observations, we propose a pipeline framework that first\npredicts the aspect and then uses translation-based alignment (TBA) to enhance\nmultimodal semantic consistency for better image utilization. Our method\nachieves state-of-the-art (SOTA) performance on widely used MABSA datasets\nTwitter-15 and Twitter-17. This demonstrates the effectiveness of the pipeline\napproach and its potential to provide valuable insights for future MABSA\nresearch.\n  For reproducibility, the code and checkpoint will be released.\n", "link": "http://arxiv.org/abs/2406.00017v2", "date": "2024-06-13", "relevancy": 2.2123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5627}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PTA%3A%20Enhancing%20Multimodal%20Sentiment%20Analysis%20through%20Pipelined%0A%20%20Prediction%20and%20Translation-based%20Alignment&body=Title%3A%20PTA%3A%20Enhancing%20Multimodal%20Sentiment%20Analysis%20through%20Pipelined%0A%20%20Prediction%20and%20Translation-based%20Alignment%0AAuthor%3A%20Shezheng%20Song%20and%20Shasha%20Li%20and%20Shan%20Zhao%20and%20Chengyu%20Wang%20and%20Xiaopeng%20Li%20and%20Jie%20Yu%20and%20Qian%20Wan%20and%20Jun%20Ma%20and%20Tianwei%20Yan%20and%20Wentao%20Ma%20and%20Xiaoguang%20Mao%0AAbstract%3A%20%20%20Multimodal%20aspect-based%20sentiment%20analysis%20%28MABSA%29%20aims%20to%20understand%0Aopinions%20in%20a%20granular%20manner%2C%20advancing%20human-computer%20interaction%20and%20other%0Afields.%20Traditionally%2C%20MABSA%20methods%20use%20a%20joint%20prediction%20approach%20to%0Aidentify%20aspects%20and%20sentiments%20simultaneously.%20However%2C%20we%20argue%20that%20joint%0Amodels%20are%20not%20always%20superior.%20Our%20analysis%20shows%20that%20joint%20models%20struggle%0Ato%20align%20relevant%20text%20tokens%20with%20image%20patches%2C%20leading%20to%20misalignment%20and%0Aineffective%20image%20utilization.%0A%20%20In%20contrast%2C%20a%20pipeline%20framework%20first%20identifies%20aspects%20through%20MATE%0A%28Multimodal%20Aspect%20Term%20Extraction%29%20and%20then%20aligns%20these%20aspects%20with%20image%0Apatches%20for%20sentiment%20classification%20%28MASC%3A%20Multimodal%20Aspect-Oriented%0ASentiment%20Classification%29.%20This%20method%20is%20better%20suited%20for%20multimodal%0Ascenarios%20where%20effective%20image%20use%20is%20crucial.%20We%20present%20three%20key%0Aobservations%3A%20%28a%29%20MATE%20and%20MASC%20have%20different%20feature%20requirements%2C%20with%20MATE%0Afocusing%20on%20token-level%20features%20and%20MASC%20on%20sequence-level%20features%3B%20%28b%29%20the%0Aaspect%20identified%20by%20MATE%20is%20crucial%20for%20effective%20image%20utilization%3B%20and%20%28c%29%0Aimages%20play%20a%20trivial%20role%20in%20previous%20MABSA%20methods%20due%20to%20high%20noise.%0A%20%20Based%20on%20these%20observations%2C%20we%20propose%20a%20pipeline%20framework%20that%20first%0Apredicts%20the%20aspect%20and%20then%20uses%20translation-based%20alignment%20%28TBA%29%20to%20enhance%0Amultimodal%20semantic%20consistency%20for%20better%20image%20utilization.%20Our%20method%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20widely%20used%20MABSA%20datasets%0ATwitter-15%20and%20Twitter-17.%20This%20demonstrates%20the%20effectiveness%20of%20the%20pipeline%0Aapproach%20and%20its%20potential%20to%20provide%20valuable%20insights%20for%20future%20MABSA%0Aresearch.%0A%20%20For%20reproducibility%2C%20the%20code%20and%20checkpoint%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00017v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPTA%253A%2520Enhancing%2520Multimodal%2520Sentiment%2520Analysis%2520through%2520Pipelined%250A%2520%2520Prediction%2520and%2520Translation-based%2520Alignment%26entry.906535625%3DShezheng%2520Song%2520and%2520Shasha%2520Li%2520and%2520Shan%2520Zhao%2520and%2520Chengyu%2520Wang%2520and%2520Xiaopeng%2520Li%2520and%2520Jie%2520Yu%2520and%2520Qian%2520Wan%2520and%2520Jun%2520Ma%2520and%2520Tianwei%2520Yan%2520and%2520Wentao%2520Ma%2520and%2520Xiaoguang%2520Mao%26entry.1292438233%3D%2520%2520Multimodal%2520aspect-based%2520sentiment%2520analysis%2520%2528MABSA%2529%2520aims%2520to%2520understand%250Aopinions%2520in%2520a%2520granular%2520manner%252C%2520advancing%2520human-computer%2520interaction%2520and%2520other%250Afields.%2520Traditionally%252C%2520MABSA%2520methods%2520use%2520a%2520joint%2520prediction%2520approach%2520to%250Aidentify%2520aspects%2520and%2520sentiments%2520simultaneously.%2520However%252C%2520we%2520argue%2520that%2520joint%250Amodels%2520are%2520not%2520always%2520superior.%2520Our%2520analysis%2520shows%2520that%2520joint%2520models%2520struggle%250Ato%2520align%2520relevant%2520text%2520tokens%2520with%2520image%2520patches%252C%2520leading%2520to%2520misalignment%2520and%250Aineffective%2520image%2520utilization.%250A%2520%2520In%2520contrast%252C%2520a%2520pipeline%2520framework%2520first%2520identifies%2520aspects%2520through%2520MATE%250A%2528Multimodal%2520Aspect%2520Term%2520Extraction%2529%2520and%2520then%2520aligns%2520these%2520aspects%2520with%2520image%250Apatches%2520for%2520sentiment%2520classification%2520%2528MASC%253A%2520Multimodal%2520Aspect-Oriented%250ASentiment%2520Classification%2529.%2520This%2520method%2520is%2520better%2520suited%2520for%2520multimodal%250Ascenarios%2520where%2520effective%2520image%2520use%2520is%2520crucial.%2520We%2520present%2520three%2520key%250Aobservations%253A%2520%2528a%2529%2520MATE%2520and%2520MASC%2520have%2520different%2520feature%2520requirements%252C%2520with%2520MATE%250Afocusing%2520on%2520token-level%2520features%2520and%2520MASC%2520on%2520sequence-level%2520features%253B%2520%2528b%2529%2520the%250Aaspect%2520identified%2520by%2520MATE%2520is%2520crucial%2520for%2520effective%2520image%2520utilization%253B%2520and%2520%2528c%2529%250Aimages%2520play%2520a%2520trivial%2520role%2520in%2520previous%2520MABSA%2520methods%2520due%2520to%2520high%2520noise.%250A%2520%2520Based%2520on%2520these%2520observations%252C%2520we%2520propose%2520a%2520pipeline%2520framework%2520that%2520first%250Apredicts%2520the%2520aspect%2520and%2520then%2520uses%2520translation-based%2520alignment%2520%2528TBA%2529%2520to%2520enhance%250Amultimodal%2520semantic%2520consistency%2520for%2520better%2520image%2520utilization.%2520Our%2520method%250Aachieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520widely%2520used%2520MABSA%2520datasets%250ATwitter-15%2520and%2520Twitter-17.%2520This%2520demonstrates%2520the%2520effectiveness%2520of%2520the%2520pipeline%250Aapproach%2520and%2520its%2520potential%2520to%2520provide%2520valuable%2520insights%2520for%2520future%2520MABSA%250Aresearch.%250A%2520%2520For%2520reproducibility%252C%2520the%2520code%2520and%2520checkpoint%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00017v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PTA%3A%20Enhancing%20Multimodal%20Sentiment%20Analysis%20through%20Pipelined%0A%20%20Prediction%20and%20Translation-based%20Alignment&entry.906535625=Shezheng%20Song%20and%20Shasha%20Li%20and%20Shan%20Zhao%20and%20Chengyu%20Wang%20and%20Xiaopeng%20Li%20and%20Jie%20Yu%20and%20Qian%20Wan%20and%20Jun%20Ma%20and%20Tianwei%20Yan%20and%20Wentao%20Ma%20and%20Xiaoguang%20Mao&entry.1292438233=%20%20Multimodal%20aspect-based%20sentiment%20analysis%20%28MABSA%29%20aims%20to%20understand%0Aopinions%20in%20a%20granular%20manner%2C%20advancing%20human-computer%20interaction%20and%20other%0Afields.%20Traditionally%2C%20MABSA%20methods%20use%20a%20joint%20prediction%20approach%20to%0Aidentify%20aspects%20and%20sentiments%20simultaneously.%20However%2C%20we%20argue%20that%20joint%0Amodels%20are%20not%20always%20superior.%20Our%20analysis%20shows%20that%20joint%20models%20struggle%0Ato%20align%20relevant%20text%20tokens%20with%20image%20patches%2C%20leading%20to%20misalignment%20and%0Aineffective%20image%20utilization.%0A%20%20In%20contrast%2C%20a%20pipeline%20framework%20first%20identifies%20aspects%20through%20MATE%0A%28Multimodal%20Aspect%20Term%20Extraction%29%20and%20then%20aligns%20these%20aspects%20with%20image%0Apatches%20for%20sentiment%20classification%20%28MASC%3A%20Multimodal%20Aspect-Oriented%0ASentiment%20Classification%29.%20This%20method%20is%20better%20suited%20for%20multimodal%0Ascenarios%20where%20effective%20image%20use%20is%20crucial.%20We%20present%20three%20key%0Aobservations%3A%20%28a%29%20MATE%20and%20MASC%20have%20different%20feature%20requirements%2C%20with%20MATE%0Afocusing%20on%20token-level%20features%20and%20MASC%20on%20sequence-level%20features%3B%20%28b%29%20the%0Aaspect%20identified%20by%20MATE%20is%20crucial%20for%20effective%20image%20utilization%3B%20and%20%28c%29%0Aimages%20play%20a%20trivial%20role%20in%20previous%20MABSA%20methods%20due%20to%20high%20noise.%0A%20%20Based%20on%20these%20observations%2C%20we%20propose%20a%20pipeline%20framework%20that%20first%0Apredicts%20the%20aspect%20and%20then%20uses%20translation-based%20alignment%20%28TBA%29%20to%20enhance%0Amultimodal%20semantic%20consistency%20for%20better%20image%20utilization.%20Our%20method%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20widely%20used%20MABSA%20datasets%0ATwitter-15%20and%20Twitter-17.%20This%20demonstrates%20the%20effectiveness%20of%20the%20pipeline%0Aapproach%20and%20its%20potential%20to%20provide%20valuable%20insights%20for%20future%20MABSA%0Aresearch.%0A%20%20For%20reproducibility%2C%20the%20code%20and%20checkpoint%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00017v2&entry.124074799=Read"},
{"title": "Real2Code: Reconstruct Articulated Objects via Code Generation", "author": "Zhao Mandi and Yijia Weng and Dominik Bauer and Shuran Song", "abstract": "  We present Real2Code, a novel approach to reconstructing articulated objects\nvia code generation. Given visual observations of an object, we first\nreconstruct its part geometry using an image segmentation model and a shape\ncompletion model. We then represent the object parts with oriented bounding\nboxes, which are input to a fine-tuned large language model (LLM) to predict\njoint articulation as code. By leveraging pre-trained vision and language\nmodels, our approach scales elegantly with the number of articulated parts, and\ngeneralizes from synthetic training data to real world objects in unstructured\nenvironments. Experimental results demonstrate that Real2Code significantly\noutperforms previous state-of-the-art in reconstruction accuracy, and is the\nfirst approach to extrapolate beyond objects' structural complexity in the\ntraining set, and reconstructs objects with up to 10 articulated parts. When\nincorporated with a stereo reconstruction model, Real2Code also generalizes to\nreal world objects from a handful of multi-view RGB images, without the need\nfor depth or camera information.\n", "link": "http://arxiv.org/abs/2406.08474v2", "date": "2024-06-13", "relevancy": 2.2107, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5565}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5531}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real2Code%3A%20Reconstruct%20Articulated%20Objects%20via%20Code%20Generation&body=Title%3A%20Real2Code%3A%20Reconstruct%20Articulated%20Objects%20via%20Code%20Generation%0AAuthor%3A%20Zhao%20Mandi%20and%20Yijia%20Weng%20and%20Dominik%20Bauer%20and%20Shuran%20Song%0AAbstract%3A%20%20%20We%20present%20Real2Code%2C%20a%20novel%20approach%20to%20reconstructing%20articulated%20objects%0Avia%20code%20generation.%20Given%20visual%20observations%20of%20an%20object%2C%20we%20first%0Areconstruct%20its%20part%20geometry%20using%20an%20image%20segmentation%20model%20and%20a%20shape%0Acompletion%20model.%20We%20then%20represent%20the%20object%20parts%20with%20oriented%20bounding%0Aboxes%2C%20which%20are%20input%20to%20a%20fine-tuned%20large%20language%20model%20%28LLM%29%20to%20predict%0Ajoint%20articulation%20as%20code.%20By%20leveraging%20pre-trained%20vision%20and%20language%0Amodels%2C%20our%20approach%20scales%20elegantly%20with%20the%20number%20of%20articulated%20parts%2C%20and%0Ageneralizes%20from%20synthetic%20training%20data%20to%20real%20world%20objects%20in%20unstructured%0Aenvironments.%20Experimental%20results%20demonstrate%20that%20Real2Code%20significantly%0Aoutperforms%20previous%20state-of-the-art%20in%20reconstruction%20accuracy%2C%20and%20is%20the%0Afirst%20approach%20to%20extrapolate%20beyond%20objects%27%20structural%20complexity%20in%20the%0Atraining%20set%2C%20and%20reconstructs%20objects%20with%20up%20to%2010%20articulated%20parts.%20When%0Aincorporated%20with%20a%20stereo%20reconstruction%20model%2C%20Real2Code%20also%20generalizes%20to%0Areal%20world%20objects%20from%20a%20handful%20of%20multi-view%20RGB%20images%2C%20without%20the%20need%0Afor%20depth%20or%20camera%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08474v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal2Code%253A%2520Reconstruct%2520Articulated%2520Objects%2520via%2520Code%2520Generation%26entry.906535625%3DZhao%2520Mandi%2520and%2520Yijia%2520Weng%2520and%2520Dominik%2520Bauer%2520and%2520Shuran%2520Song%26entry.1292438233%3D%2520%2520We%2520present%2520Real2Code%252C%2520a%2520novel%2520approach%2520to%2520reconstructing%2520articulated%2520objects%250Avia%2520code%2520generation.%2520Given%2520visual%2520observations%2520of%2520an%2520object%252C%2520we%2520first%250Areconstruct%2520its%2520part%2520geometry%2520using%2520an%2520image%2520segmentation%2520model%2520and%2520a%2520shape%250Acompletion%2520model.%2520We%2520then%2520represent%2520the%2520object%2520parts%2520with%2520oriented%2520bounding%250Aboxes%252C%2520which%2520are%2520input%2520to%2520a%2520fine-tuned%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520predict%250Ajoint%2520articulation%2520as%2520code.%2520By%2520leveraging%2520pre-trained%2520vision%2520and%2520language%250Amodels%252C%2520our%2520approach%2520scales%2520elegantly%2520with%2520the%2520number%2520of%2520articulated%2520parts%252C%2520and%250Ageneralizes%2520from%2520synthetic%2520training%2520data%2520to%2520real%2520world%2520objects%2520in%2520unstructured%250Aenvironments.%2520Experimental%2520results%2520demonstrate%2520that%2520Real2Code%2520significantly%250Aoutperforms%2520previous%2520state-of-the-art%2520in%2520reconstruction%2520accuracy%252C%2520and%2520is%2520the%250Afirst%2520approach%2520to%2520extrapolate%2520beyond%2520objects%2527%2520structural%2520complexity%2520in%2520the%250Atraining%2520set%252C%2520and%2520reconstructs%2520objects%2520with%2520up%2520to%252010%2520articulated%2520parts.%2520When%250Aincorporated%2520with%2520a%2520stereo%2520reconstruction%2520model%252C%2520Real2Code%2520also%2520generalizes%2520to%250Areal%2520world%2520objects%2520from%2520a%2520handful%2520of%2520multi-view%2520RGB%2520images%252C%2520without%2520the%2520need%250Afor%2520depth%2520or%2520camera%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08474v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real2Code%3A%20Reconstruct%20Articulated%20Objects%20via%20Code%20Generation&entry.906535625=Zhao%20Mandi%20and%20Yijia%20Weng%20and%20Dominik%20Bauer%20and%20Shuran%20Song&entry.1292438233=%20%20We%20present%20Real2Code%2C%20a%20novel%20approach%20to%20reconstructing%20articulated%20objects%0Avia%20code%20generation.%20Given%20visual%20observations%20of%20an%20object%2C%20we%20first%0Areconstruct%20its%20part%20geometry%20using%20an%20image%20segmentation%20model%20and%20a%20shape%0Acompletion%20model.%20We%20then%20represent%20the%20object%20parts%20with%20oriented%20bounding%0Aboxes%2C%20which%20are%20input%20to%20a%20fine-tuned%20large%20language%20model%20%28LLM%29%20to%20predict%0Ajoint%20articulation%20as%20code.%20By%20leveraging%20pre-trained%20vision%20and%20language%0Amodels%2C%20our%20approach%20scales%20elegantly%20with%20the%20number%20of%20articulated%20parts%2C%20and%0Ageneralizes%20from%20synthetic%20training%20data%20to%20real%20world%20objects%20in%20unstructured%0Aenvironments.%20Experimental%20results%20demonstrate%20that%20Real2Code%20significantly%0Aoutperforms%20previous%20state-of-the-art%20in%20reconstruction%20accuracy%2C%20and%20is%20the%0Afirst%20approach%20to%20extrapolate%20beyond%20objects%27%20structural%20complexity%20in%20the%0Atraining%20set%2C%20and%20reconstructs%20objects%20with%20up%20to%2010%20articulated%20parts.%20When%0Aincorporated%20with%20a%20stereo%20reconstruction%20model%2C%20Real2Code%20also%20generalizes%20to%0Areal%20world%20objects%20from%20a%20handful%20of%20multi-view%20RGB%20images%2C%20without%20the%20need%0Afor%20depth%20or%20camera%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08474v2&entry.124074799=Read"},
{"title": "Multi-modal Learning with Missing Modality via Shared-Specific Feature\n  Modelling", "author": "Hu Wang and Yuanhong Chen and Congbo Ma and Jodie Avery and Louise Hull and Gustavo Carneiro", "abstract": "  The missing modality issue is critical but non-trivial to be solved by\nmulti-modal models. Current methods aiming to handle the missing modality\nproblem in multi-modal tasks, either deal with missing modalities only during\nevaluation or train separate models to handle specific missing modality\nsettings. In addition, these models are designed for specific tasks, so for\nexample, classification models are not easily adapted to segmentation tasks and\nvice versa. In this paper, we propose the Shared-Specific Feature Modelling\n(ShaSpec) method that is considerably simpler and more effective than competing\napproaches that address the issues above. ShaSpec is designed to take advantage\nof all available input modalities during training and evaluation by learning\nshared and specific features to better represent the input data. This is\nachieved from a strategy that relies on auxiliary tasks based on distribution\nalignment and domain classification, in addition to a residual feature fusion\nprocedure. Also, the design simplicity of ShaSpec enables its easy adaptation\nto multiple tasks, such as classification and segmentation. Experiments are\nconducted on both medical image segmentation and computer vision\nclassification, with results indicating that ShaSpec outperforms competing\nmethods by a large margin. For instance, on BraTS2018, ShaSpec improves the\nSOTA by more than 3% for enhancing tumour, 5% for tumour core and 3% for whole\ntumour. The code repository address is https://github.com/billhhh/ShaSpec/.\n", "link": "http://arxiv.org/abs/2307.14126v2", "date": "2024-06-13", "relevancy": 2.2062, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5916}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5651}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Learning%20with%20Missing%20Modality%20via%20Shared-Specific%20Feature%0A%20%20Modelling&body=Title%3A%20Multi-modal%20Learning%20with%20Missing%20Modality%20via%20Shared-Specific%20Feature%0A%20%20Modelling%0AAuthor%3A%20Hu%20Wang%20and%20Yuanhong%20Chen%20and%20Congbo%20Ma%20and%20Jodie%20Avery%20and%20Louise%20Hull%20and%20Gustavo%20Carneiro%0AAbstract%3A%20%20%20The%20missing%20modality%20issue%20is%20critical%20but%20non-trivial%20to%20be%20solved%20by%0Amulti-modal%20models.%20Current%20methods%20aiming%20to%20handle%20the%20missing%20modality%0Aproblem%20in%20multi-modal%20tasks%2C%20either%20deal%20with%20missing%20modalities%20only%20during%0Aevaluation%20or%20train%20separate%20models%20to%20handle%20specific%20missing%20modality%0Asettings.%20In%20addition%2C%20these%20models%20are%20designed%20for%20specific%20tasks%2C%20so%20for%0Aexample%2C%20classification%20models%20are%20not%20easily%20adapted%20to%20segmentation%20tasks%20and%0Avice%20versa.%20In%20this%20paper%2C%20we%20propose%20the%20Shared-Specific%20Feature%20Modelling%0A%28ShaSpec%29%20method%20that%20is%20considerably%20simpler%20and%20more%20effective%20than%20competing%0Aapproaches%20that%20address%20the%20issues%20above.%20ShaSpec%20is%20designed%20to%20take%20advantage%0Aof%20all%20available%20input%20modalities%20during%20training%20and%20evaluation%20by%20learning%0Ashared%20and%20specific%20features%20to%20better%20represent%20the%20input%20data.%20This%20is%0Aachieved%20from%20a%20strategy%20that%20relies%20on%20auxiliary%20tasks%20based%20on%20distribution%0Aalignment%20and%20domain%20classification%2C%20in%20addition%20to%20a%20residual%20feature%20fusion%0Aprocedure.%20Also%2C%20the%20design%20simplicity%20of%20ShaSpec%20enables%20its%20easy%20adaptation%0Ato%20multiple%20tasks%2C%20such%20as%20classification%20and%20segmentation.%20Experiments%20are%0Aconducted%20on%20both%20medical%20image%20segmentation%20and%20computer%20vision%0Aclassification%2C%20with%20results%20indicating%20that%20ShaSpec%20outperforms%20competing%0Amethods%20by%20a%20large%20margin.%20For%20instance%2C%20on%20BraTS2018%2C%20ShaSpec%20improves%20the%0ASOTA%20by%20more%20than%203%25%20for%20enhancing%20tumour%2C%205%25%20for%20tumour%20core%20and%203%25%20for%20whole%0Atumour.%20The%20code%20repository%20address%20is%20https%3A//github.com/billhhh/ShaSpec/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.14126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Learning%2520with%2520Missing%2520Modality%2520via%2520Shared-Specific%2520Feature%250A%2520%2520Modelling%26entry.906535625%3DHu%2520Wang%2520and%2520Yuanhong%2520Chen%2520and%2520Congbo%2520Ma%2520and%2520Jodie%2520Avery%2520and%2520Louise%2520Hull%2520and%2520Gustavo%2520Carneiro%26entry.1292438233%3D%2520%2520The%2520missing%2520modality%2520issue%2520is%2520critical%2520but%2520non-trivial%2520to%2520be%2520solved%2520by%250Amulti-modal%2520models.%2520Current%2520methods%2520aiming%2520to%2520handle%2520the%2520missing%2520modality%250Aproblem%2520in%2520multi-modal%2520tasks%252C%2520either%2520deal%2520with%2520missing%2520modalities%2520only%2520during%250Aevaluation%2520or%2520train%2520separate%2520models%2520to%2520handle%2520specific%2520missing%2520modality%250Asettings.%2520In%2520addition%252C%2520these%2520models%2520are%2520designed%2520for%2520specific%2520tasks%252C%2520so%2520for%250Aexample%252C%2520classification%2520models%2520are%2520not%2520easily%2520adapted%2520to%2520segmentation%2520tasks%2520and%250Avice%2520versa.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Shared-Specific%2520Feature%2520Modelling%250A%2528ShaSpec%2529%2520method%2520that%2520is%2520considerably%2520simpler%2520and%2520more%2520effective%2520than%2520competing%250Aapproaches%2520that%2520address%2520the%2520issues%2520above.%2520ShaSpec%2520is%2520designed%2520to%2520take%2520advantage%250Aof%2520all%2520available%2520input%2520modalities%2520during%2520training%2520and%2520evaluation%2520by%2520learning%250Ashared%2520and%2520specific%2520features%2520to%2520better%2520represent%2520the%2520input%2520data.%2520This%2520is%250Aachieved%2520from%2520a%2520strategy%2520that%2520relies%2520on%2520auxiliary%2520tasks%2520based%2520on%2520distribution%250Aalignment%2520and%2520domain%2520classification%252C%2520in%2520addition%2520to%2520a%2520residual%2520feature%2520fusion%250Aprocedure.%2520Also%252C%2520the%2520design%2520simplicity%2520of%2520ShaSpec%2520enables%2520its%2520easy%2520adaptation%250Ato%2520multiple%2520tasks%252C%2520such%2520as%2520classification%2520and%2520segmentation.%2520Experiments%2520are%250Aconducted%2520on%2520both%2520medical%2520image%2520segmentation%2520and%2520computer%2520vision%250Aclassification%252C%2520with%2520results%2520indicating%2520that%2520ShaSpec%2520outperforms%2520competing%250Amethods%2520by%2520a%2520large%2520margin.%2520For%2520instance%252C%2520on%2520BraTS2018%252C%2520ShaSpec%2520improves%2520the%250ASOTA%2520by%2520more%2520than%25203%2525%2520for%2520enhancing%2520tumour%252C%25205%2525%2520for%2520tumour%2520core%2520and%25203%2525%2520for%2520whole%250Atumour.%2520The%2520code%2520repository%2520address%2520is%2520https%253A//github.com/billhhh/ShaSpec/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.14126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Learning%20with%20Missing%20Modality%20via%20Shared-Specific%20Feature%0A%20%20Modelling&entry.906535625=Hu%20Wang%20and%20Yuanhong%20Chen%20and%20Congbo%20Ma%20and%20Jodie%20Avery%20and%20Louise%20Hull%20and%20Gustavo%20Carneiro&entry.1292438233=%20%20The%20missing%20modality%20issue%20is%20critical%20but%20non-trivial%20to%20be%20solved%20by%0Amulti-modal%20models.%20Current%20methods%20aiming%20to%20handle%20the%20missing%20modality%0Aproblem%20in%20multi-modal%20tasks%2C%20either%20deal%20with%20missing%20modalities%20only%20during%0Aevaluation%20or%20train%20separate%20models%20to%20handle%20specific%20missing%20modality%0Asettings.%20In%20addition%2C%20these%20models%20are%20designed%20for%20specific%20tasks%2C%20so%20for%0Aexample%2C%20classification%20models%20are%20not%20easily%20adapted%20to%20segmentation%20tasks%20and%0Avice%20versa.%20In%20this%20paper%2C%20we%20propose%20the%20Shared-Specific%20Feature%20Modelling%0A%28ShaSpec%29%20method%20that%20is%20considerably%20simpler%20and%20more%20effective%20than%20competing%0Aapproaches%20that%20address%20the%20issues%20above.%20ShaSpec%20is%20designed%20to%20take%20advantage%0Aof%20all%20available%20input%20modalities%20during%20training%20and%20evaluation%20by%20learning%0Ashared%20and%20specific%20features%20to%20better%20represent%20the%20input%20data.%20This%20is%0Aachieved%20from%20a%20strategy%20that%20relies%20on%20auxiliary%20tasks%20based%20on%20distribution%0Aalignment%20and%20domain%20classification%2C%20in%20addition%20to%20a%20residual%20feature%20fusion%0Aprocedure.%20Also%2C%20the%20design%20simplicity%20of%20ShaSpec%20enables%20its%20easy%20adaptation%0Ato%20multiple%20tasks%2C%20such%20as%20classification%20and%20segmentation.%20Experiments%20are%0Aconducted%20on%20both%20medical%20image%20segmentation%20and%20computer%20vision%0Aclassification%2C%20with%20results%20indicating%20that%20ShaSpec%20outperforms%20competing%0Amethods%20by%20a%20large%20margin.%20For%20instance%2C%20on%20BraTS2018%2C%20ShaSpec%20improves%20the%0ASOTA%20by%20more%20than%203%25%20for%20enhancing%20tumour%2C%205%25%20for%20tumour%20core%20and%203%25%20for%20whole%0Atumour.%20The%20code%20repository%20address%20is%20https%3A//github.com/billhhh/ShaSpec/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.14126v2&entry.124074799=Read"},
{"title": "Auto-Vocabulary Segmentation for LiDAR Points", "author": "Weijie Wei and Osman \u00dclger and Fatemeh Karimi Najadasl and Theo Gevers and Martin R. Oswald", "abstract": "  Existing perception methods for autonomous driving fall short of recognizing\nunknown entities not covered in the training data. Open-vocabulary methods\noffer promising capabilities in detecting any object but are limited by\nuser-specified queries representing target classes. We propose AutoVoc3D, a\nframework for automatic object class recognition and open-ended segmentation.\nEvaluation on nuScenes showcases AutoVoc3D's ability to generate precise\nsemantic classes and accurate point-wise segmentation. Moreover, we introduce\nText-Point Semantic Similarity, a new metric to assess the semantic similarity\nbetween text and point cloud without eliminating novel classes.\n", "link": "http://arxiv.org/abs/2406.09126v1", "date": "2024-06-13", "relevancy": 2.2006, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5646}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5433}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Vocabulary%20Segmentation%20for%20LiDAR%20Points&body=Title%3A%20Auto-Vocabulary%20Segmentation%20for%20LiDAR%20Points%0AAuthor%3A%20Weijie%20Wei%20and%20Osman%20%C3%9Clger%20and%20Fatemeh%20Karimi%20Najadasl%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Existing%20perception%20methods%20for%20autonomous%20driving%20fall%20short%20of%20recognizing%0Aunknown%20entities%20not%20covered%20in%20the%20training%20data.%20Open-vocabulary%20methods%0Aoffer%20promising%20capabilities%20in%20detecting%20any%20object%20but%20are%20limited%20by%0Auser-specified%20queries%20representing%20target%20classes.%20We%20propose%20AutoVoc3D%2C%20a%0Aframework%20for%20automatic%20object%20class%20recognition%20and%20open-ended%20segmentation.%0AEvaluation%20on%20nuScenes%20showcases%20AutoVoc3D%27s%20ability%20to%20generate%20precise%0Asemantic%20classes%20and%20accurate%20point-wise%20segmentation.%20Moreover%2C%20we%20introduce%0AText-Point%20Semantic%20Similarity%2C%20a%20new%20metric%20to%20assess%20the%20semantic%20similarity%0Abetween%20text%20and%20point%20cloud%20without%20eliminating%20novel%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Vocabulary%2520Segmentation%2520for%2520LiDAR%2520Points%26entry.906535625%3DWeijie%2520Wei%2520and%2520Osman%2520%25C3%259Clger%2520and%2520Fatemeh%2520Karimi%2520Najadasl%2520and%2520Theo%2520Gevers%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Existing%2520perception%2520methods%2520for%2520autonomous%2520driving%2520fall%2520short%2520of%2520recognizing%250Aunknown%2520entities%2520not%2520covered%2520in%2520the%2520training%2520data.%2520Open-vocabulary%2520methods%250Aoffer%2520promising%2520capabilities%2520in%2520detecting%2520any%2520object%2520but%2520are%2520limited%2520by%250Auser-specified%2520queries%2520representing%2520target%2520classes.%2520We%2520propose%2520AutoVoc3D%252C%2520a%250Aframework%2520for%2520automatic%2520object%2520class%2520recognition%2520and%2520open-ended%2520segmentation.%250AEvaluation%2520on%2520nuScenes%2520showcases%2520AutoVoc3D%2527s%2520ability%2520to%2520generate%2520precise%250Asemantic%2520classes%2520and%2520accurate%2520point-wise%2520segmentation.%2520Moreover%252C%2520we%2520introduce%250AText-Point%2520Semantic%2520Similarity%252C%2520a%2520new%2520metric%2520to%2520assess%2520the%2520semantic%2520similarity%250Abetween%2520text%2520and%2520point%2520cloud%2520without%2520eliminating%2520novel%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Vocabulary%20Segmentation%20for%20LiDAR%20Points&entry.906535625=Weijie%20Wei%20and%20Osman%20%C3%9Clger%20and%20Fatemeh%20Karimi%20Najadasl%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Existing%20perception%20methods%20for%20autonomous%20driving%20fall%20short%20of%20recognizing%0Aunknown%20entities%20not%20covered%20in%20the%20training%20data.%20Open-vocabulary%20methods%0Aoffer%20promising%20capabilities%20in%20detecting%20any%20object%20but%20are%20limited%20by%0Auser-specified%20queries%20representing%20target%20classes.%20We%20propose%20AutoVoc3D%2C%20a%0Aframework%20for%20automatic%20object%20class%20recognition%20and%20open-ended%20segmentation.%0AEvaluation%20on%20nuScenes%20showcases%20AutoVoc3D%27s%20ability%20to%20generate%20precise%0Asemantic%20classes%20and%20accurate%20point-wise%20segmentation.%20Moreover%2C%20we%20introduce%0AText-Point%20Semantic%20Similarity%2C%20a%20new%20metric%20to%20assess%20the%20semantic%20similarity%0Abetween%20text%20and%20point%20cloud%20without%20eliminating%20novel%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09126v1&entry.124074799=Read"},
{"title": "CLIPAway: Harmonizing Focused Embeddings for Removing Objects via\n  Diffusion Models", "author": "Yigit Ekin and Ahmet Burak Yildirim and Erdem Eren Caglar and Aykut Erdem and Erkut Erdem and Aysegul Dundar", "abstract": "  Advanced image editing techniques, particularly inpainting, are essential for\nseamlessly removing unwanted elements while preserving visual integrity.\nTraditional GAN-based methods have achieved notable success, but recent\nadvancements in diffusion models have produced superior results due to their\ntraining on large-scale datasets, enabling the generation of remarkably\nrealistic inpainted images. Despite their strengths, diffusion models often\nstruggle with object removal tasks without explicit guidance, leading to\nunintended hallucinations of the removed object. To address this issue, we\nintroduce CLIPAway, a novel approach leveraging CLIP embeddings to focus on\nbackground regions while excluding foreground elements. CLIPAway enhances\ninpainting accuracy and quality by identifying embeddings that prioritize the\nbackground, thus achieving seamless object removal. Unlike other methods that\nrely on specialized training datasets or costly manual annotations, CLIPAway\nprovides a flexible, plug-and-play solution compatible with various\ndiffusion-based inpainting techniques.\n", "link": "http://arxiv.org/abs/2406.09368v1", "date": "2024-06-13", "relevancy": 2.1998, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5597}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5503}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPAway%3A%20Harmonizing%20Focused%20Embeddings%20for%20Removing%20Objects%20via%0A%20%20Diffusion%20Models&body=Title%3A%20CLIPAway%3A%20Harmonizing%20Focused%20Embeddings%20for%20Removing%20Objects%20via%0A%20%20Diffusion%20Models%0AAuthor%3A%20Yigit%20Ekin%20and%20Ahmet%20Burak%20Yildirim%20and%20Erdem%20Eren%20Caglar%20and%20Aykut%20Erdem%20and%20Erkut%20Erdem%20and%20Aysegul%20Dundar%0AAbstract%3A%20%20%20Advanced%20image%20editing%20techniques%2C%20particularly%20inpainting%2C%20are%20essential%20for%0Aseamlessly%20removing%20unwanted%20elements%20while%20preserving%20visual%20integrity.%0ATraditional%20GAN-based%20methods%20have%20achieved%20notable%20success%2C%20but%20recent%0Aadvancements%20in%20diffusion%20models%20have%20produced%20superior%20results%20due%20to%20their%0Atraining%20on%20large-scale%20datasets%2C%20enabling%20the%20generation%20of%20remarkably%0Arealistic%20inpainted%20images.%20Despite%20their%20strengths%2C%20diffusion%20models%20often%0Astruggle%20with%20object%20removal%20tasks%20without%20explicit%20guidance%2C%20leading%20to%0Aunintended%20hallucinations%20of%20the%20removed%20object.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20CLIPAway%2C%20a%20novel%20approach%20leveraging%20CLIP%20embeddings%20to%20focus%20on%0Abackground%20regions%20while%20excluding%20foreground%20elements.%20CLIPAway%20enhances%0Ainpainting%20accuracy%20and%20quality%20by%20identifying%20embeddings%20that%20prioritize%20the%0Abackground%2C%20thus%20achieving%20seamless%20object%20removal.%20Unlike%20other%20methods%20that%0Arely%20on%20specialized%20training%20datasets%20or%20costly%20manual%20annotations%2C%20CLIPAway%0Aprovides%20a%20flexible%2C%20plug-and-play%20solution%20compatible%20with%20various%0Adiffusion-based%20inpainting%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPAway%253A%2520Harmonizing%2520Focused%2520Embeddings%2520for%2520Removing%2520Objects%2520via%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DYigit%2520Ekin%2520and%2520Ahmet%2520Burak%2520Yildirim%2520and%2520Erdem%2520Eren%2520Caglar%2520and%2520Aykut%2520Erdem%2520and%2520Erkut%2520Erdem%2520and%2520Aysegul%2520Dundar%26entry.1292438233%3D%2520%2520Advanced%2520image%2520editing%2520techniques%252C%2520particularly%2520inpainting%252C%2520are%2520essential%2520for%250Aseamlessly%2520removing%2520unwanted%2520elements%2520while%2520preserving%2520visual%2520integrity.%250ATraditional%2520GAN-based%2520methods%2520have%2520achieved%2520notable%2520success%252C%2520but%2520recent%250Aadvancements%2520in%2520diffusion%2520models%2520have%2520produced%2520superior%2520results%2520due%2520to%2520their%250Atraining%2520on%2520large-scale%2520datasets%252C%2520enabling%2520the%2520generation%2520of%2520remarkably%250Arealistic%2520inpainted%2520images.%2520Despite%2520their%2520strengths%252C%2520diffusion%2520models%2520often%250Astruggle%2520with%2520object%2520removal%2520tasks%2520without%2520explicit%2520guidance%252C%2520leading%2520to%250Aunintended%2520hallucinations%2520of%2520the%2520removed%2520object.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520CLIPAway%252C%2520a%2520novel%2520approach%2520leveraging%2520CLIP%2520embeddings%2520to%2520focus%2520on%250Abackground%2520regions%2520while%2520excluding%2520foreground%2520elements.%2520CLIPAway%2520enhances%250Ainpainting%2520accuracy%2520and%2520quality%2520by%2520identifying%2520embeddings%2520that%2520prioritize%2520the%250Abackground%252C%2520thus%2520achieving%2520seamless%2520object%2520removal.%2520Unlike%2520other%2520methods%2520that%250Arely%2520on%2520specialized%2520training%2520datasets%2520or%2520costly%2520manual%2520annotations%252C%2520CLIPAway%250Aprovides%2520a%2520flexible%252C%2520plug-and-play%2520solution%2520compatible%2520with%2520various%250Adiffusion-based%2520inpainting%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPAway%3A%20Harmonizing%20Focused%20Embeddings%20for%20Removing%20Objects%20via%0A%20%20Diffusion%20Models&entry.906535625=Yigit%20Ekin%20and%20Ahmet%20Burak%20Yildirim%20and%20Erdem%20Eren%20Caglar%20and%20Aykut%20Erdem%20and%20Erkut%20Erdem%20and%20Aysegul%20Dundar&entry.1292438233=%20%20Advanced%20image%20editing%20techniques%2C%20particularly%20inpainting%2C%20are%20essential%20for%0Aseamlessly%20removing%20unwanted%20elements%20while%20preserving%20visual%20integrity.%0ATraditional%20GAN-based%20methods%20have%20achieved%20notable%20success%2C%20but%20recent%0Aadvancements%20in%20diffusion%20models%20have%20produced%20superior%20results%20due%20to%20their%0Atraining%20on%20large-scale%20datasets%2C%20enabling%20the%20generation%20of%20remarkably%0Arealistic%20inpainted%20images.%20Despite%20their%20strengths%2C%20diffusion%20models%20often%0Astruggle%20with%20object%20removal%20tasks%20without%20explicit%20guidance%2C%20leading%20to%0Aunintended%20hallucinations%20of%20the%20removed%20object.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20CLIPAway%2C%20a%20novel%20approach%20leveraging%20CLIP%20embeddings%20to%20focus%20on%0Abackground%20regions%20while%20excluding%20foreground%20elements.%20CLIPAway%20enhances%0Ainpainting%20accuracy%20and%20quality%20by%20identifying%20embeddings%20that%20prioritize%20the%0Abackground%2C%20thus%20achieving%20seamless%20object%20removal.%20Unlike%20other%20methods%20that%0Arely%20on%20specialized%20training%20datasets%20or%20costly%20manual%20annotations%2C%20CLIPAway%0Aprovides%20a%20flexible%2C%20plug-and-play%20solution%20compatible%20with%20various%0Adiffusion-based%20inpainting%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09368v1&entry.124074799=Read"},
{"title": "AdaRevD: Adaptive Patch Exiting Reversible Decoder Pushes the Limit of\n  Image Deblurring", "author": "Xintian Mao and Qingli Li and Yan Wang", "abstract": "  Despite the recent progress in enhancing the efficacy of image deblurring,\nthe limited decoding capability constrains the upper limit of State-Of-The-Art\n(SOTA) methods. This paper proposes a pioneering work, Adaptive Patch Exiting\nReversible Decoder (AdaRevD), to explore their insufficient decoding\ncapability. By inheriting the weights of the well-trained encoder, we refactor\na reversible decoder which scales up the single-decoder training to\nmulti-decoder training while remaining GPU memory-friendly. Meanwhile, we show\nthat our reversible structure gradually disentangles high-level degradation\ndegree and low-level blur pattern (residual of the blur image and its sharp\ncounterpart) from compact degradation representation. Besides, due to the\nspatially-variant motion blur kernels, different blur patches have various\ndeblurring difficulties. We further introduce a classifier to learn the\ndegradation degree of image patches, enabling them to exit at different\nsub-decoders for speedup. Experiments show that our AdaRevD pushes the limit of\nimage deblurring, e.g., achieving 34.60 dB in PSNR on GoPro dataset.\n", "link": "http://arxiv.org/abs/2406.09135v1", "date": "2024-06-13", "relevancy": 2.1959, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5716}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5615}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaRevD%3A%20Adaptive%20Patch%20Exiting%20Reversible%20Decoder%20Pushes%20the%20Limit%20of%0A%20%20Image%20Deblurring&body=Title%3A%20AdaRevD%3A%20Adaptive%20Patch%20Exiting%20Reversible%20Decoder%20Pushes%20the%20Limit%20of%0A%20%20Image%20Deblurring%0AAuthor%3A%20Xintian%20Mao%20and%20Qingli%20Li%20and%20Yan%20Wang%0AAbstract%3A%20%20%20Despite%20the%20recent%20progress%20in%20enhancing%20the%20efficacy%20of%20image%20deblurring%2C%0Athe%20limited%20decoding%20capability%20constrains%20the%20upper%20limit%20of%20State-Of-The-Art%0A%28SOTA%29%20methods.%20This%20paper%20proposes%20a%20pioneering%20work%2C%20Adaptive%20Patch%20Exiting%0AReversible%20Decoder%20%28AdaRevD%29%2C%20to%20explore%20their%20insufficient%20decoding%0Acapability.%20By%20inheriting%20the%20weights%20of%20the%20well-trained%20encoder%2C%20we%20refactor%0Aa%20reversible%20decoder%20which%20scales%20up%20the%20single-decoder%20training%20to%0Amulti-decoder%20training%20while%20remaining%20GPU%20memory-friendly.%20Meanwhile%2C%20we%20show%0Athat%20our%20reversible%20structure%20gradually%20disentangles%20high-level%20degradation%0Adegree%20and%20low-level%20blur%20pattern%20%28residual%20of%20the%20blur%20image%20and%20its%20sharp%0Acounterpart%29%20from%20compact%20degradation%20representation.%20Besides%2C%20due%20to%20the%0Aspatially-variant%20motion%20blur%20kernels%2C%20different%20blur%20patches%20have%20various%0Adeblurring%20difficulties.%20We%20further%20introduce%20a%20classifier%20to%20learn%20the%0Adegradation%20degree%20of%20image%20patches%2C%20enabling%20them%20to%20exit%20at%20different%0Asub-decoders%20for%20speedup.%20Experiments%20show%20that%20our%20AdaRevD%20pushes%20the%20limit%20of%0Aimage%20deblurring%2C%20e.g.%2C%20achieving%2034.60%20dB%20in%20PSNR%20on%20GoPro%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaRevD%253A%2520Adaptive%2520Patch%2520Exiting%2520Reversible%2520Decoder%2520Pushes%2520the%2520Limit%2520of%250A%2520%2520Image%2520Deblurring%26entry.906535625%3DXintian%2520Mao%2520and%2520Qingli%2520Li%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520progress%2520in%2520enhancing%2520the%2520efficacy%2520of%2520image%2520deblurring%252C%250Athe%2520limited%2520decoding%2520capability%2520constrains%2520the%2520upper%2520limit%2520of%2520State-Of-The-Art%250A%2528SOTA%2529%2520methods.%2520This%2520paper%2520proposes%2520a%2520pioneering%2520work%252C%2520Adaptive%2520Patch%2520Exiting%250AReversible%2520Decoder%2520%2528AdaRevD%2529%252C%2520to%2520explore%2520their%2520insufficient%2520decoding%250Acapability.%2520By%2520inheriting%2520the%2520weights%2520of%2520the%2520well-trained%2520encoder%252C%2520we%2520refactor%250Aa%2520reversible%2520decoder%2520which%2520scales%2520up%2520the%2520single-decoder%2520training%2520to%250Amulti-decoder%2520training%2520while%2520remaining%2520GPU%2520memory-friendly.%2520Meanwhile%252C%2520we%2520show%250Athat%2520our%2520reversible%2520structure%2520gradually%2520disentangles%2520high-level%2520degradation%250Adegree%2520and%2520low-level%2520blur%2520pattern%2520%2528residual%2520of%2520the%2520blur%2520image%2520and%2520its%2520sharp%250Acounterpart%2529%2520from%2520compact%2520degradation%2520representation.%2520Besides%252C%2520due%2520to%2520the%250Aspatially-variant%2520motion%2520blur%2520kernels%252C%2520different%2520blur%2520patches%2520have%2520various%250Adeblurring%2520difficulties.%2520We%2520further%2520introduce%2520a%2520classifier%2520to%2520learn%2520the%250Adegradation%2520degree%2520of%2520image%2520patches%252C%2520enabling%2520them%2520to%2520exit%2520at%2520different%250Asub-decoders%2520for%2520speedup.%2520Experiments%2520show%2520that%2520our%2520AdaRevD%2520pushes%2520the%2520limit%2520of%250Aimage%2520deblurring%252C%2520e.g.%252C%2520achieving%252034.60%2520dB%2520in%2520PSNR%2520on%2520GoPro%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaRevD%3A%20Adaptive%20Patch%20Exiting%20Reversible%20Decoder%20Pushes%20the%20Limit%20of%0A%20%20Image%20Deblurring&entry.906535625=Xintian%20Mao%20and%20Qingli%20Li%20and%20Yan%20Wang&entry.1292438233=%20%20Despite%20the%20recent%20progress%20in%20enhancing%20the%20efficacy%20of%20image%20deblurring%2C%0Athe%20limited%20decoding%20capability%20constrains%20the%20upper%20limit%20of%20State-Of-The-Art%0A%28SOTA%29%20methods.%20This%20paper%20proposes%20a%20pioneering%20work%2C%20Adaptive%20Patch%20Exiting%0AReversible%20Decoder%20%28AdaRevD%29%2C%20to%20explore%20their%20insufficient%20decoding%0Acapability.%20By%20inheriting%20the%20weights%20of%20the%20well-trained%20encoder%2C%20we%20refactor%0Aa%20reversible%20decoder%20which%20scales%20up%20the%20single-decoder%20training%20to%0Amulti-decoder%20training%20while%20remaining%20GPU%20memory-friendly.%20Meanwhile%2C%20we%20show%0Athat%20our%20reversible%20structure%20gradually%20disentangles%20high-level%20degradation%0Adegree%20and%20low-level%20blur%20pattern%20%28residual%20of%20the%20blur%20image%20and%20its%20sharp%0Acounterpart%29%20from%20compact%20degradation%20representation.%20Besides%2C%20due%20to%20the%0Aspatially-variant%20motion%20blur%20kernels%2C%20different%20blur%20patches%20have%20various%0Adeblurring%20difficulties.%20We%20further%20introduce%20a%20classifier%20to%20learn%20the%0Adegradation%20degree%20of%20image%20patches%2C%20enabling%20them%20to%20exit%20at%20different%0Asub-decoders%20for%20speedup.%20Experiments%20show%20that%20our%20AdaRevD%20pushes%20the%20limit%20of%0Aimage%20deblurring%2C%20e.g.%2C%20achieving%2034.60%20dB%20in%20PSNR%20on%20GoPro%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09135v1&entry.124074799=Read"},
{"title": "Data Attribution for Text-to-Image Models by Unlearning Synthesized\n  Images", "author": "Sheng-Yu Wang and Aaron Hertzmann and Alexei A. Efros and Jun-Yan Zhu and Richard Zhang", "abstract": "  The goal of data attribution for text-to-image models is to identify the\ntraining images that most influence the generation of a new image. We can\ndefine \"influence\" by saying that, for a given output, if a model is retrained\nfrom scratch without that output's most influential images, the model should\nthen fail to generate that output image. Unfortunately, directly searching for\nthese influential images is computationally infeasible, since it would require\nrepeatedly retraining from scratch. We propose a new approach that efficiently\nidentifies highly-influential images. Specifically, we simulate unlearning the\nsynthesized image, proposing a method to increase the training loss on the\noutput image, without catastrophic forgetting of other, unrelated concepts.\nThen, we find training images that are forgotten by proxy, identifying ones\nwith significant loss deviations after the unlearning process, and label these\nas influential. We evaluate our method with a computationally intensive but\n\"gold-standard\" retraining from scratch and demonstrate our method's advantages\nover previous methods.\n", "link": "http://arxiv.org/abs/2406.09408v1", "date": "2024-06-13", "relevancy": 2.1907, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5787}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5552}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Attribution%20for%20Text-to-Image%20Models%20by%20Unlearning%20Synthesized%0A%20%20Images&body=Title%3A%20Data%20Attribution%20for%20Text-to-Image%20Models%20by%20Unlearning%20Synthesized%0A%20%20Images%0AAuthor%3A%20Sheng-Yu%20Wang%20and%20Aaron%20Hertzmann%20and%20Alexei%20A.%20Efros%20and%20Jun-Yan%20Zhu%20and%20Richard%20Zhang%0AAbstract%3A%20%20%20The%20goal%20of%20data%20attribution%20for%20text-to-image%20models%20is%20to%20identify%20the%0Atraining%20images%20that%20most%20influence%20the%20generation%20of%20a%20new%20image.%20We%20can%0Adefine%20%22influence%22%20by%20saying%20that%2C%20for%20a%20given%20output%2C%20if%20a%20model%20is%20retrained%0Afrom%20scratch%20without%20that%20output%27s%20most%20influential%20images%2C%20the%20model%20should%0Athen%20fail%20to%20generate%20that%20output%20image.%20Unfortunately%2C%20directly%20searching%20for%0Athese%20influential%20images%20is%20computationally%20infeasible%2C%20since%20it%20would%20require%0Arepeatedly%20retraining%20from%20scratch.%20We%20propose%20a%20new%20approach%20that%20efficiently%0Aidentifies%20highly-influential%20images.%20Specifically%2C%20we%20simulate%20unlearning%20the%0Asynthesized%20image%2C%20proposing%20a%20method%20to%20increase%20the%20training%20loss%20on%20the%0Aoutput%20image%2C%20without%20catastrophic%20forgetting%20of%20other%2C%20unrelated%20concepts.%0AThen%2C%20we%20find%20training%20images%20that%20are%20forgotten%20by%20proxy%2C%20identifying%20ones%0Awith%20significant%20loss%20deviations%20after%20the%20unlearning%20process%2C%20and%20label%20these%0Aas%20influential.%20We%20evaluate%20our%20method%20with%20a%20computationally%20intensive%20but%0A%22gold-standard%22%20retraining%20from%20scratch%20and%20demonstrate%20our%20method%27s%20advantages%0Aover%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Attribution%2520for%2520Text-to-Image%2520Models%2520by%2520Unlearning%2520Synthesized%250A%2520%2520Images%26entry.906535625%3DSheng-Yu%2520Wang%2520and%2520Aaron%2520Hertzmann%2520and%2520Alexei%2520A.%2520Efros%2520and%2520Jun-Yan%2520Zhu%2520and%2520Richard%2520Zhang%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520data%2520attribution%2520for%2520text-to-image%2520models%2520is%2520to%2520identify%2520the%250Atraining%2520images%2520that%2520most%2520influence%2520the%2520generation%2520of%2520a%2520new%2520image.%2520We%2520can%250Adefine%2520%2522influence%2522%2520by%2520saying%2520that%252C%2520for%2520a%2520given%2520output%252C%2520if%2520a%2520model%2520is%2520retrained%250Afrom%2520scratch%2520without%2520that%2520output%2527s%2520most%2520influential%2520images%252C%2520the%2520model%2520should%250Athen%2520fail%2520to%2520generate%2520that%2520output%2520image.%2520Unfortunately%252C%2520directly%2520searching%2520for%250Athese%2520influential%2520images%2520is%2520computationally%2520infeasible%252C%2520since%2520it%2520would%2520require%250Arepeatedly%2520retraining%2520from%2520scratch.%2520We%2520propose%2520a%2520new%2520approach%2520that%2520efficiently%250Aidentifies%2520highly-influential%2520images.%2520Specifically%252C%2520we%2520simulate%2520unlearning%2520the%250Asynthesized%2520image%252C%2520proposing%2520a%2520method%2520to%2520increase%2520the%2520training%2520loss%2520on%2520the%250Aoutput%2520image%252C%2520without%2520catastrophic%2520forgetting%2520of%2520other%252C%2520unrelated%2520concepts.%250AThen%252C%2520we%2520find%2520training%2520images%2520that%2520are%2520forgotten%2520by%2520proxy%252C%2520identifying%2520ones%250Awith%2520significant%2520loss%2520deviations%2520after%2520the%2520unlearning%2520process%252C%2520and%2520label%2520these%250Aas%2520influential.%2520We%2520evaluate%2520our%2520method%2520with%2520a%2520computationally%2520intensive%2520but%250A%2522gold-standard%2522%2520retraining%2520from%2520scratch%2520and%2520demonstrate%2520our%2520method%2527s%2520advantages%250Aover%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Attribution%20for%20Text-to-Image%20Models%20by%20Unlearning%20Synthesized%0A%20%20Images&entry.906535625=Sheng-Yu%20Wang%20and%20Aaron%20Hertzmann%20and%20Alexei%20A.%20Efros%20and%20Jun-Yan%20Zhu%20and%20Richard%20Zhang&entry.1292438233=%20%20The%20goal%20of%20data%20attribution%20for%20text-to-image%20models%20is%20to%20identify%20the%0Atraining%20images%20that%20most%20influence%20the%20generation%20of%20a%20new%20image.%20We%20can%0Adefine%20%22influence%22%20by%20saying%20that%2C%20for%20a%20given%20output%2C%20if%20a%20model%20is%20retrained%0Afrom%20scratch%20without%20that%20output%27s%20most%20influential%20images%2C%20the%20model%20should%0Athen%20fail%20to%20generate%20that%20output%20image.%20Unfortunately%2C%20directly%20searching%20for%0Athese%20influential%20images%20is%20computationally%20infeasible%2C%20since%20it%20would%20require%0Arepeatedly%20retraining%20from%20scratch.%20We%20propose%20a%20new%20approach%20that%20efficiently%0Aidentifies%20highly-influential%20images.%20Specifically%2C%20we%20simulate%20unlearning%20the%0Asynthesized%20image%2C%20proposing%20a%20method%20to%20increase%20the%20training%20loss%20on%20the%0Aoutput%20image%2C%20without%20catastrophic%20forgetting%20of%20other%2C%20unrelated%20concepts.%0AThen%2C%20we%20find%20training%20images%20that%20are%20forgotten%20by%20proxy%2C%20identifying%20ones%0Awith%20significant%20loss%20deviations%20after%20the%20unlearning%20process%2C%20and%20label%20these%0Aas%20influential.%20We%20evaluate%20our%20method%20with%20a%20computationally%20intensive%20but%0A%22gold-standard%22%20retraining%20from%20scratch%20and%20demonstrate%20our%20method%27s%20advantages%0Aover%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09408v1&entry.124074799=Read"},
{"title": "Transfer learning with generative models for object detection on limited\n  datasets", "author": "Matteo Paiano and Stefano Martina and Carlotta Giannelli and Filippo Caruso", "abstract": "  The availability of data is limited in some fields, especially for object\ndetection tasks, where it is necessary to have correctly labeled bounding boxes\naround each object. A notable example of such data scarcity is found in the\ndomain of marine biology, where it is useful to develop methods to\nautomatically detect submarine species for environmental monitoring. To address\nthis data limitation, the state-of-the-art machine learning strategies employ\ntwo main approaches. The first involves pretraining models on existing datasets\nbefore generalizing to the specific domain of interest. The second strategy is\nto create synthetic datasets specifically tailored to the target domain using\nmethods like copy-paste techniques or ad-hoc simulators. The first strategy\noften faces a significant domain shift, while the second demands custom\nsolutions crafted for the specific task. In response to these challenges, here\nwe propose a transfer learning framework that is valid for a generic scenario.\nIn this framework, generated images help to improve the performances of an\nobject detector in a few-real data regime. This is achieved through a\ndiffusion-based generative model that was pretrained on large generic datasets.\nWith respect to the state-of-the-art, we find that it is not necessary to fine\ntune the generative model on the specific domain of interest. We believe that\nthis is an important advance because it mitigates the labor-intensive task of\nmanual labeling the images in object detection tasks. We validate our approach\nfocusing on fishes in an underwater environment, and on the more common domain\nof cars in an urban setting. Our method achieves detection performance\ncomparable to models trained on thousands of images, using only a few hundreds\nof input data. Our results pave the way for new generative AI-based protocols\nfor machine learning applications in various domains.\n", "link": "http://arxiv.org/abs/2402.06784v2", "date": "2024-06-13", "relevancy": 2.1881, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5476}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5471}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20learning%20with%20generative%20models%20for%20object%20detection%20on%20limited%0A%20%20datasets&body=Title%3A%20Transfer%20learning%20with%20generative%20models%20for%20object%20detection%20on%20limited%0A%20%20datasets%0AAuthor%3A%20Matteo%20Paiano%20and%20Stefano%20Martina%20and%20Carlotta%20Giannelli%20and%20Filippo%20Caruso%0AAbstract%3A%20%20%20The%20availability%20of%20data%20is%20limited%20in%20some%20fields%2C%20especially%20for%20object%0Adetection%20tasks%2C%20where%20it%20is%20necessary%20to%20have%20correctly%20labeled%20bounding%20boxes%0Aaround%20each%20object.%20A%20notable%20example%20of%20such%20data%20scarcity%20is%20found%20in%20the%0Adomain%20of%20marine%20biology%2C%20where%20it%20is%20useful%20to%20develop%20methods%20to%0Aautomatically%20detect%20submarine%20species%20for%20environmental%20monitoring.%20To%20address%0Athis%20data%20limitation%2C%20the%20state-of-the-art%20machine%20learning%20strategies%20employ%0Atwo%20main%20approaches.%20The%20first%20involves%20pretraining%20models%20on%20existing%20datasets%0Abefore%20generalizing%20to%20the%20specific%20domain%20of%20interest.%20The%20second%20strategy%20is%0Ato%20create%20synthetic%20datasets%20specifically%20tailored%20to%20the%20target%20domain%20using%0Amethods%20like%20copy-paste%20techniques%20or%20ad-hoc%20simulators.%20The%20first%20strategy%0Aoften%20faces%20a%20significant%20domain%20shift%2C%20while%20the%20second%20demands%20custom%0Asolutions%20crafted%20for%20the%20specific%20task.%20In%20response%20to%20these%20challenges%2C%20here%0Awe%20propose%20a%20transfer%20learning%20framework%20that%20is%20valid%20for%20a%20generic%20scenario.%0AIn%20this%20framework%2C%20generated%20images%20help%20to%20improve%20the%20performances%20of%20an%0Aobject%20detector%20in%20a%20few-real%20data%20regime.%20This%20is%20achieved%20through%20a%0Adiffusion-based%20generative%20model%20that%20was%20pretrained%20on%20large%20generic%20datasets.%0AWith%20respect%20to%20the%20state-of-the-art%2C%20we%20find%20that%20it%20is%20not%20necessary%20to%20fine%0Atune%20the%20generative%20model%20on%20the%20specific%20domain%20of%20interest.%20We%20believe%20that%0Athis%20is%20an%20important%20advance%20because%20it%20mitigates%20the%20labor-intensive%20task%20of%0Amanual%20labeling%20the%20images%20in%20object%20detection%20tasks.%20We%20validate%20our%20approach%0Afocusing%20on%20fishes%20in%20an%20underwater%20environment%2C%20and%20on%20the%20more%20common%20domain%0Aof%20cars%20in%20an%20urban%20setting.%20Our%20method%20achieves%20detection%20performance%0Acomparable%20to%20models%20trained%20on%20thousands%20of%20images%2C%20using%20only%20a%20few%20hundreds%0Aof%20input%20data.%20Our%20results%20pave%20the%20way%20for%20new%20generative%20AI-based%20protocols%0Afor%20machine%20learning%20applications%20in%20various%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520learning%2520with%2520generative%2520models%2520for%2520object%2520detection%2520on%2520limited%250A%2520%2520datasets%26entry.906535625%3DMatteo%2520Paiano%2520and%2520Stefano%2520Martina%2520and%2520Carlotta%2520Giannelli%2520and%2520Filippo%2520Caruso%26entry.1292438233%3D%2520%2520The%2520availability%2520of%2520data%2520is%2520limited%2520in%2520some%2520fields%252C%2520especially%2520for%2520object%250Adetection%2520tasks%252C%2520where%2520it%2520is%2520necessary%2520to%2520have%2520correctly%2520labeled%2520bounding%2520boxes%250Aaround%2520each%2520object.%2520A%2520notable%2520example%2520of%2520such%2520data%2520scarcity%2520is%2520found%2520in%2520the%250Adomain%2520of%2520marine%2520biology%252C%2520where%2520it%2520is%2520useful%2520to%2520develop%2520methods%2520to%250Aautomatically%2520detect%2520submarine%2520species%2520for%2520environmental%2520monitoring.%2520To%2520address%250Athis%2520data%2520limitation%252C%2520the%2520state-of-the-art%2520machine%2520learning%2520strategies%2520employ%250Atwo%2520main%2520approaches.%2520The%2520first%2520involves%2520pretraining%2520models%2520on%2520existing%2520datasets%250Abefore%2520generalizing%2520to%2520the%2520specific%2520domain%2520of%2520interest.%2520The%2520second%2520strategy%2520is%250Ato%2520create%2520synthetic%2520datasets%2520specifically%2520tailored%2520to%2520the%2520target%2520domain%2520using%250Amethods%2520like%2520copy-paste%2520techniques%2520or%2520ad-hoc%2520simulators.%2520The%2520first%2520strategy%250Aoften%2520faces%2520a%2520significant%2520domain%2520shift%252C%2520while%2520the%2520second%2520demands%2520custom%250Asolutions%2520crafted%2520for%2520the%2520specific%2520task.%2520In%2520response%2520to%2520these%2520challenges%252C%2520here%250Awe%2520propose%2520a%2520transfer%2520learning%2520framework%2520that%2520is%2520valid%2520for%2520a%2520generic%2520scenario.%250AIn%2520this%2520framework%252C%2520generated%2520images%2520help%2520to%2520improve%2520the%2520performances%2520of%2520an%250Aobject%2520detector%2520in%2520a%2520few-real%2520data%2520regime.%2520This%2520is%2520achieved%2520through%2520a%250Adiffusion-based%2520generative%2520model%2520that%2520was%2520pretrained%2520on%2520large%2520generic%2520datasets.%250AWith%2520respect%2520to%2520the%2520state-of-the-art%252C%2520we%2520find%2520that%2520it%2520is%2520not%2520necessary%2520to%2520fine%250Atune%2520the%2520generative%2520model%2520on%2520the%2520specific%2520domain%2520of%2520interest.%2520We%2520believe%2520that%250Athis%2520is%2520an%2520important%2520advance%2520because%2520it%2520mitigates%2520the%2520labor-intensive%2520task%2520of%250Amanual%2520labeling%2520the%2520images%2520in%2520object%2520detection%2520tasks.%2520We%2520validate%2520our%2520approach%250Afocusing%2520on%2520fishes%2520in%2520an%2520underwater%2520environment%252C%2520and%2520on%2520the%2520more%2520common%2520domain%250Aof%2520cars%2520in%2520an%2520urban%2520setting.%2520Our%2520method%2520achieves%2520detection%2520performance%250Acomparable%2520to%2520models%2520trained%2520on%2520thousands%2520of%2520images%252C%2520using%2520only%2520a%2520few%2520hundreds%250Aof%2520input%2520data.%2520Our%2520results%2520pave%2520the%2520way%2520for%2520new%2520generative%2520AI-based%2520protocols%250Afor%2520machine%2520learning%2520applications%2520in%2520various%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20learning%20with%20generative%20models%20for%20object%20detection%20on%20limited%0A%20%20datasets&entry.906535625=Matteo%20Paiano%20and%20Stefano%20Martina%20and%20Carlotta%20Giannelli%20and%20Filippo%20Caruso&entry.1292438233=%20%20The%20availability%20of%20data%20is%20limited%20in%20some%20fields%2C%20especially%20for%20object%0Adetection%20tasks%2C%20where%20it%20is%20necessary%20to%20have%20correctly%20labeled%20bounding%20boxes%0Aaround%20each%20object.%20A%20notable%20example%20of%20such%20data%20scarcity%20is%20found%20in%20the%0Adomain%20of%20marine%20biology%2C%20where%20it%20is%20useful%20to%20develop%20methods%20to%0Aautomatically%20detect%20submarine%20species%20for%20environmental%20monitoring.%20To%20address%0Athis%20data%20limitation%2C%20the%20state-of-the-art%20machine%20learning%20strategies%20employ%0Atwo%20main%20approaches.%20The%20first%20involves%20pretraining%20models%20on%20existing%20datasets%0Abefore%20generalizing%20to%20the%20specific%20domain%20of%20interest.%20The%20second%20strategy%20is%0Ato%20create%20synthetic%20datasets%20specifically%20tailored%20to%20the%20target%20domain%20using%0Amethods%20like%20copy-paste%20techniques%20or%20ad-hoc%20simulators.%20The%20first%20strategy%0Aoften%20faces%20a%20significant%20domain%20shift%2C%20while%20the%20second%20demands%20custom%0Asolutions%20crafted%20for%20the%20specific%20task.%20In%20response%20to%20these%20challenges%2C%20here%0Awe%20propose%20a%20transfer%20learning%20framework%20that%20is%20valid%20for%20a%20generic%20scenario.%0AIn%20this%20framework%2C%20generated%20images%20help%20to%20improve%20the%20performances%20of%20an%0Aobject%20detector%20in%20a%20few-real%20data%20regime.%20This%20is%20achieved%20through%20a%0Adiffusion-based%20generative%20model%20that%20was%20pretrained%20on%20large%20generic%20datasets.%0AWith%20respect%20to%20the%20state-of-the-art%2C%20we%20find%20that%20it%20is%20not%20necessary%20to%20fine%0Atune%20the%20generative%20model%20on%20the%20specific%20domain%20of%20interest.%20We%20believe%20that%0Athis%20is%20an%20important%20advance%20because%20it%20mitigates%20the%20labor-intensive%20task%20of%0Amanual%20labeling%20the%20images%20in%20object%20detection%20tasks.%20We%20validate%20our%20approach%0Afocusing%20on%20fishes%20in%20an%20underwater%20environment%2C%20and%20on%20the%20more%20common%20domain%0Aof%20cars%20in%20an%20urban%20setting.%20Our%20method%20achieves%20detection%20performance%0Acomparable%20to%20models%20trained%20on%20thousands%20of%20images%2C%20using%20only%20a%20few%20hundreds%0Aof%20input%20data.%20Our%20results%20pave%20the%20way%20for%20new%20generative%20AI-based%20protocols%0Afor%20machine%20learning%20applications%20in%20various%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06784v2&entry.124074799=Read"},
{"title": "Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks\n  and Algorithms", "author": "Miaosen Zhang and Yixuan Wei and Zhen Xing and Yifei Ma and Zuxuan Wu and Ji Li and Zheng Zhang and Qi Dai and Chong Luo and Xin Geng and Baining Guo", "abstract": "  Modern vision models are trained on very large noisy datasets. While these\nmodels acquire strong capabilities, they may not follow the user's intent to\noutput the desired results in certain aspects, e.g., visual aesthetic,\npreferred style, and responsibility. In this paper, we target the realm of\nvisual aesthetics and aim to align vision models with human aesthetic standards\nin a retrieval system. Advanced retrieval systems usually adopt a cascade of\naesthetic models as re-rankers or filters, which are limited to low-level\nfeatures like saturation and perform poorly when stylistic, cultural or\nknowledge contexts are involved. We find that utilizing the reasoning ability\nof large language models (LLMs) to rephrase the search query and extend the\naesthetic expectations can make up for this shortcoming. Based on the above\nfindings, we propose a preference-based reinforcement learning method that\nfine-tunes the vision models to distill the knowledge from both LLMs reasoning\nand the aesthetic models to better align the vision models with human\naesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval\nsystems, we leverage large multi-modality model (LMM) to evaluate the aesthetic\nperformance with their strong abilities. As aesthetic assessment is one of the\nmost subjective tasks, to validate the robustness of LMM, we further propose a\nnovel dataset named HPIR to benchmark the alignment with human aesthetics.\nExperiments demonstrate that our method significantly enhances the aesthetic\nbehaviors of the vision models, under several metrics. We believe the proposed\nalgorithm can be a general practice for aligning vision models with human\nvalues.\n", "link": "http://arxiv.org/abs/2406.09397v1", "date": "2024-06-13", "relevancy": 2.182, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5738}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5422}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Vision%20Models%20with%20Human%20Aesthetics%20in%20Retrieval%3A%20Benchmarks%0A%20%20and%20Algorithms&body=Title%3A%20Aligning%20Vision%20Models%20with%20Human%20Aesthetics%20in%20Retrieval%3A%20Benchmarks%0A%20%20and%20Algorithms%0AAuthor%3A%20Miaosen%20Zhang%20and%20Yixuan%20Wei%20and%20Zhen%20Xing%20and%20Yifei%20Ma%20and%20Zuxuan%20Wu%20and%20Ji%20Li%20and%20Zheng%20Zhang%20and%20Qi%20Dai%20and%20Chong%20Luo%20and%20Xin%20Geng%20and%20Baining%20Guo%0AAbstract%3A%20%20%20Modern%20vision%20models%20are%20trained%20on%20very%20large%20noisy%20datasets.%20While%20these%0Amodels%20acquire%20strong%20capabilities%2C%20they%20may%20not%20follow%20the%20user%27s%20intent%20to%0Aoutput%20the%20desired%20results%20in%20certain%20aspects%2C%20e.g.%2C%20visual%20aesthetic%2C%0Apreferred%20style%2C%20and%20responsibility.%20In%20this%20paper%2C%20we%20target%20the%20realm%20of%0Avisual%20aesthetics%20and%20aim%20to%20align%20vision%20models%20with%20human%20aesthetic%20standards%0Ain%20a%20retrieval%20system.%20Advanced%20retrieval%20systems%20usually%20adopt%20a%20cascade%20of%0Aaesthetic%20models%20as%20re-rankers%20or%20filters%2C%20which%20are%20limited%20to%20low-level%0Afeatures%20like%20saturation%20and%20perform%20poorly%20when%20stylistic%2C%20cultural%20or%0Aknowledge%20contexts%20are%20involved.%20We%20find%20that%20utilizing%20the%20reasoning%20ability%0Aof%20large%20language%20models%20%28LLMs%29%20to%20rephrase%20the%20search%20query%20and%20extend%20the%0Aaesthetic%20expectations%20can%20make%20up%20for%20this%20shortcoming.%20Based%20on%20the%20above%0Afindings%2C%20we%20propose%20a%20preference-based%20reinforcement%20learning%20method%20that%0Afine-tunes%20the%20vision%20models%20to%20distill%20the%20knowledge%20from%20both%20LLMs%20reasoning%0Aand%20the%20aesthetic%20models%20to%20better%20align%20the%20vision%20models%20with%20human%0Aaesthetics.%20Meanwhile%2C%20with%20rare%20benchmarks%20designed%20for%20evaluating%20retrieval%0Asystems%2C%20we%20leverage%20large%20multi-modality%20model%20%28LMM%29%20to%20evaluate%20the%20aesthetic%0Aperformance%20with%20their%20strong%20abilities.%20As%20aesthetic%20assessment%20is%20one%20of%20the%0Amost%20subjective%20tasks%2C%20to%20validate%20the%20robustness%20of%20LMM%2C%20we%20further%20propose%20a%0Anovel%20dataset%20named%20HPIR%20to%20benchmark%20the%20alignment%20with%20human%20aesthetics.%0AExperiments%20demonstrate%20that%20our%20method%20significantly%20enhances%20the%20aesthetic%0Abehaviors%20of%20the%20vision%20models%2C%20under%20several%20metrics.%20We%20believe%20the%20proposed%0Aalgorithm%20can%20be%20a%20general%20practice%20for%20aligning%20vision%20models%20with%20human%0Avalues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Vision%2520Models%2520with%2520Human%2520Aesthetics%2520in%2520Retrieval%253A%2520Benchmarks%250A%2520%2520and%2520Algorithms%26entry.906535625%3DMiaosen%2520Zhang%2520and%2520Yixuan%2520Wei%2520and%2520Zhen%2520Xing%2520and%2520Yifei%2520Ma%2520and%2520Zuxuan%2520Wu%2520and%2520Ji%2520Li%2520and%2520Zheng%2520Zhang%2520and%2520Qi%2520Dai%2520and%2520Chong%2520Luo%2520and%2520Xin%2520Geng%2520and%2520Baining%2520Guo%26entry.1292438233%3D%2520%2520Modern%2520vision%2520models%2520are%2520trained%2520on%2520very%2520large%2520noisy%2520datasets.%2520While%2520these%250Amodels%2520acquire%2520strong%2520capabilities%252C%2520they%2520may%2520not%2520follow%2520the%2520user%2527s%2520intent%2520to%250Aoutput%2520the%2520desired%2520results%2520in%2520certain%2520aspects%252C%2520e.g.%252C%2520visual%2520aesthetic%252C%250Apreferred%2520style%252C%2520and%2520responsibility.%2520In%2520this%2520paper%252C%2520we%2520target%2520the%2520realm%2520of%250Avisual%2520aesthetics%2520and%2520aim%2520to%2520align%2520vision%2520models%2520with%2520human%2520aesthetic%2520standards%250Ain%2520a%2520retrieval%2520system.%2520Advanced%2520retrieval%2520systems%2520usually%2520adopt%2520a%2520cascade%2520of%250Aaesthetic%2520models%2520as%2520re-rankers%2520or%2520filters%252C%2520which%2520are%2520limited%2520to%2520low-level%250Afeatures%2520like%2520saturation%2520and%2520perform%2520poorly%2520when%2520stylistic%252C%2520cultural%2520or%250Aknowledge%2520contexts%2520are%2520involved.%2520We%2520find%2520that%2520utilizing%2520the%2520reasoning%2520ability%250Aof%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520rephrase%2520the%2520search%2520query%2520and%2520extend%2520the%250Aaesthetic%2520expectations%2520can%2520make%2520up%2520for%2520this%2520shortcoming.%2520Based%2520on%2520the%2520above%250Afindings%252C%2520we%2520propose%2520a%2520preference-based%2520reinforcement%2520learning%2520method%2520that%250Afine-tunes%2520the%2520vision%2520models%2520to%2520distill%2520the%2520knowledge%2520from%2520both%2520LLMs%2520reasoning%250Aand%2520the%2520aesthetic%2520models%2520to%2520better%2520align%2520the%2520vision%2520models%2520with%2520human%250Aaesthetics.%2520Meanwhile%252C%2520with%2520rare%2520benchmarks%2520designed%2520for%2520evaluating%2520retrieval%250Asystems%252C%2520we%2520leverage%2520large%2520multi-modality%2520model%2520%2528LMM%2529%2520to%2520evaluate%2520the%2520aesthetic%250Aperformance%2520with%2520their%2520strong%2520abilities.%2520As%2520aesthetic%2520assessment%2520is%2520one%2520of%2520the%250Amost%2520subjective%2520tasks%252C%2520to%2520validate%2520the%2520robustness%2520of%2520LMM%252C%2520we%2520further%2520propose%2520a%250Anovel%2520dataset%2520named%2520HPIR%2520to%2520benchmark%2520the%2520alignment%2520with%2520human%2520aesthetics.%250AExperiments%2520demonstrate%2520that%2520our%2520method%2520significantly%2520enhances%2520the%2520aesthetic%250Abehaviors%2520of%2520the%2520vision%2520models%252C%2520under%2520several%2520metrics.%2520We%2520believe%2520the%2520proposed%250Aalgorithm%2520can%2520be%2520a%2520general%2520practice%2520for%2520aligning%2520vision%2520models%2520with%2520human%250Avalues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Vision%20Models%20with%20Human%20Aesthetics%20in%20Retrieval%3A%20Benchmarks%0A%20%20and%20Algorithms&entry.906535625=Miaosen%20Zhang%20and%20Yixuan%20Wei%20and%20Zhen%20Xing%20and%20Yifei%20Ma%20and%20Zuxuan%20Wu%20and%20Ji%20Li%20and%20Zheng%20Zhang%20and%20Qi%20Dai%20and%20Chong%20Luo%20and%20Xin%20Geng%20and%20Baining%20Guo&entry.1292438233=%20%20Modern%20vision%20models%20are%20trained%20on%20very%20large%20noisy%20datasets.%20While%20these%0Amodels%20acquire%20strong%20capabilities%2C%20they%20may%20not%20follow%20the%20user%27s%20intent%20to%0Aoutput%20the%20desired%20results%20in%20certain%20aspects%2C%20e.g.%2C%20visual%20aesthetic%2C%0Apreferred%20style%2C%20and%20responsibility.%20In%20this%20paper%2C%20we%20target%20the%20realm%20of%0Avisual%20aesthetics%20and%20aim%20to%20align%20vision%20models%20with%20human%20aesthetic%20standards%0Ain%20a%20retrieval%20system.%20Advanced%20retrieval%20systems%20usually%20adopt%20a%20cascade%20of%0Aaesthetic%20models%20as%20re-rankers%20or%20filters%2C%20which%20are%20limited%20to%20low-level%0Afeatures%20like%20saturation%20and%20perform%20poorly%20when%20stylistic%2C%20cultural%20or%0Aknowledge%20contexts%20are%20involved.%20We%20find%20that%20utilizing%20the%20reasoning%20ability%0Aof%20large%20language%20models%20%28LLMs%29%20to%20rephrase%20the%20search%20query%20and%20extend%20the%0Aaesthetic%20expectations%20can%20make%20up%20for%20this%20shortcoming.%20Based%20on%20the%20above%0Afindings%2C%20we%20propose%20a%20preference-based%20reinforcement%20learning%20method%20that%0Afine-tunes%20the%20vision%20models%20to%20distill%20the%20knowledge%20from%20both%20LLMs%20reasoning%0Aand%20the%20aesthetic%20models%20to%20better%20align%20the%20vision%20models%20with%20human%0Aaesthetics.%20Meanwhile%2C%20with%20rare%20benchmarks%20designed%20for%20evaluating%20retrieval%0Asystems%2C%20we%20leverage%20large%20multi-modality%20model%20%28LMM%29%20to%20evaluate%20the%20aesthetic%0Aperformance%20with%20their%20strong%20abilities.%20As%20aesthetic%20assessment%20is%20one%20of%20the%0Amost%20subjective%20tasks%2C%20to%20validate%20the%20robustness%20of%20LMM%2C%20we%20further%20propose%20a%0Anovel%20dataset%20named%20HPIR%20to%20benchmark%20the%20alignment%20with%20human%20aesthetics.%0AExperiments%20demonstrate%20that%20our%20method%20significantly%20enhances%20the%20aesthetic%0Abehaviors%20of%20the%20vision%20models%2C%20under%20several%20metrics.%20We%20believe%20the%20proposed%0Aalgorithm%20can%20be%20a%20general%20practice%20for%20aligning%20vision%20models%20with%20human%0Avalues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09397v1&entry.124074799=Read"},
{"title": "Active Learning for Multilingual Fingerspelling Corpora", "author": "Shuai Wang and Eric Nalisnick", "abstract": "  We apply active learning to help with data scarcity problems in sign\nlanguages. In particular, we perform a novel analysis of the effect of\npre-training. Since many sign languages are linguistic descendants of French\nsign language, they share hand configurations, which pre-training can hopefully\nexploit. We test this hypothesis on American, Chinese, German, and Irish\nfingerspelling corpora. We do observe a benefit from pre-training, but this may\nbe due to visual rather than linguistic similarities\n", "link": "http://arxiv.org/abs/2309.12443v2", "date": "2024-06-13", "relevancy": 2.1761, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4632}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4249}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20for%20Multilingual%20Fingerspelling%20Corpora&body=Title%3A%20Active%20Learning%20for%20Multilingual%20Fingerspelling%20Corpora%0AAuthor%3A%20Shuai%20Wang%20and%20Eric%20Nalisnick%0AAbstract%3A%20%20%20We%20apply%20active%20learning%20to%20help%20with%20data%20scarcity%20problems%20in%20sign%0Alanguages.%20In%20particular%2C%20we%20perform%20a%20novel%20analysis%20of%20the%20effect%20of%0Apre-training.%20Since%20many%20sign%20languages%20are%20linguistic%20descendants%20of%20French%0Asign%20language%2C%20they%20share%20hand%20configurations%2C%20which%20pre-training%20can%20hopefully%0Aexploit.%20We%20test%20this%20hypothesis%20on%20American%2C%20Chinese%2C%20German%2C%20and%20Irish%0Afingerspelling%20corpora.%20We%20do%20observe%20a%20benefit%20from%20pre-training%2C%20but%20this%20may%0Abe%20due%20to%20visual%20rather%20than%20linguistic%20similarities%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12443v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520for%2520Multilingual%2520Fingerspelling%2520Corpora%26entry.906535625%3DShuai%2520Wang%2520and%2520Eric%2520Nalisnick%26entry.1292438233%3D%2520%2520We%2520apply%2520active%2520learning%2520to%2520help%2520with%2520data%2520scarcity%2520problems%2520in%2520sign%250Alanguages.%2520In%2520particular%252C%2520we%2520perform%2520a%2520novel%2520analysis%2520of%2520the%2520effect%2520of%250Apre-training.%2520Since%2520many%2520sign%2520languages%2520are%2520linguistic%2520descendants%2520of%2520French%250Asign%2520language%252C%2520they%2520share%2520hand%2520configurations%252C%2520which%2520pre-training%2520can%2520hopefully%250Aexploit.%2520We%2520test%2520this%2520hypothesis%2520on%2520American%252C%2520Chinese%252C%2520German%252C%2520and%2520Irish%250Afingerspelling%2520corpora.%2520We%2520do%2520observe%2520a%2520benefit%2520from%2520pre-training%252C%2520but%2520this%2520may%250Abe%2520due%2520to%2520visual%2520rather%2520than%2520linguistic%2520similarities%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12443v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20for%20Multilingual%20Fingerspelling%20Corpora&entry.906535625=Shuai%20Wang%20and%20Eric%20Nalisnick&entry.1292438233=%20%20We%20apply%20active%20learning%20to%20help%20with%20data%20scarcity%20problems%20in%20sign%0Alanguages.%20In%20particular%2C%20we%20perform%20a%20novel%20analysis%20of%20the%20effect%20of%0Apre-training.%20Since%20many%20sign%20languages%20are%20linguistic%20descendants%20of%20French%0Asign%20language%2C%20they%20share%20hand%20configurations%2C%20which%20pre-training%20can%20hopefully%0Aexploit.%20We%20test%20this%20hypothesis%20on%20American%2C%20Chinese%2C%20German%2C%20and%20Irish%0Afingerspelling%20corpora.%20We%20do%20observe%20a%20benefit%20from%20pre-training%2C%20but%20this%20may%0Abe%20due%20to%20visual%20rather%20than%20linguistic%20similarities%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12443v2&entry.124074799=Read"},
{"title": "On the Expressibility of the Reconstructional Color Refinement", "author": "V. Arvind and Johannes K\u00f6bler and Oleg Verbitsky", "abstract": "  One of the most basic facts related to the famous Ulam reconstruction\nconjecture is that the connectedness of a graph can be determined by the deck\nof its vertex-deleted subgraphs, which are considered up to isomorphism. We\nstrengthen this result by proving that connectedness can still be determined\nwhen the subgraphs in the deck are given up to equivalence under the color\nrefinement isomorphism test. Consequently, this implies that connectedness is\nrecognizable by Reconstruction Graph Neural Networks, a recently introduced GNN\narchitecture inspired by the reconstruction conjecture (Cotta, Morris, Ribeiro\n2021).\n", "link": "http://arxiv.org/abs/2406.09351v1", "date": "2024-06-13", "relevancy": 2.1758, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4557}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4347}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Expressibility%20of%20the%20Reconstructional%20Color%20Refinement&body=Title%3A%20On%20the%20Expressibility%20of%20the%20Reconstructional%20Color%20Refinement%0AAuthor%3A%20V.%20Arvind%20and%20Johannes%20K%C3%B6bler%20and%20Oleg%20Verbitsky%0AAbstract%3A%20%20%20One%20of%20the%20most%20basic%20facts%20related%20to%20the%20famous%20Ulam%20reconstruction%0Aconjecture%20is%20that%20the%20connectedness%20of%20a%20graph%20can%20be%20determined%20by%20the%20deck%0Aof%20its%20vertex-deleted%20subgraphs%2C%20which%20are%20considered%20up%20to%20isomorphism.%20We%0Astrengthen%20this%20result%20by%20proving%20that%20connectedness%20can%20still%20be%20determined%0Awhen%20the%20subgraphs%20in%20the%20deck%20are%20given%20up%20to%20equivalence%20under%20the%20color%0Arefinement%20isomorphism%20test.%20Consequently%2C%20this%20implies%20that%20connectedness%20is%0Arecognizable%20by%20Reconstruction%20Graph%20Neural%20Networks%2C%20a%20recently%20introduced%20GNN%0Aarchitecture%20inspired%20by%20the%20reconstruction%20conjecture%20%28Cotta%2C%20Morris%2C%20Ribeiro%0A2021%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Expressibility%2520of%2520the%2520Reconstructional%2520Color%2520Refinement%26entry.906535625%3DV.%2520Arvind%2520and%2520Johannes%2520K%25C3%25B6bler%2520and%2520Oleg%2520Verbitsky%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520basic%2520facts%2520related%2520to%2520the%2520famous%2520Ulam%2520reconstruction%250Aconjecture%2520is%2520that%2520the%2520connectedness%2520of%2520a%2520graph%2520can%2520be%2520determined%2520by%2520the%2520deck%250Aof%2520its%2520vertex-deleted%2520subgraphs%252C%2520which%2520are%2520considered%2520up%2520to%2520isomorphism.%2520We%250Astrengthen%2520this%2520result%2520by%2520proving%2520that%2520connectedness%2520can%2520still%2520be%2520determined%250Awhen%2520the%2520subgraphs%2520in%2520the%2520deck%2520are%2520given%2520up%2520to%2520equivalence%2520under%2520the%2520color%250Arefinement%2520isomorphism%2520test.%2520Consequently%252C%2520this%2520implies%2520that%2520connectedness%2520is%250Arecognizable%2520by%2520Reconstruction%2520Graph%2520Neural%2520Networks%252C%2520a%2520recently%2520introduced%2520GNN%250Aarchitecture%2520inspired%2520by%2520the%2520reconstruction%2520conjecture%2520%2528Cotta%252C%2520Morris%252C%2520Ribeiro%250A2021%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Expressibility%20of%20the%20Reconstructional%20Color%20Refinement&entry.906535625=V.%20Arvind%20and%20Johannes%20K%C3%B6bler%20and%20Oleg%20Verbitsky&entry.1292438233=%20%20One%20of%20the%20most%20basic%20facts%20related%20to%20the%20famous%20Ulam%20reconstruction%0Aconjecture%20is%20that%20the%20connectedness%20of%20a%20graph%20can%20be%20determined%20by%20the%20deck%0Aof%20its%20vertex-deleted%20subgraphs%2C%20which%20are%20considered%20up%20to%20isomorphism.%20We%0Astrengthen%20this%20result%20by%20proving%20that%20connectedness%20can%20still%20be%20determined%0Awhen%20the%20subgraphs%20in%20the%20deck%20are%20given%20up%20to%20equivalence%20under%20the%20color%0Arefinement%20isomorphism%20test.%20Consequently%2C%20this%20implies%20that%20connectedness%20is%0Arecognizable%20by%20Reconstruction%20Graph%20Neural%20Networks%2C%20a%20recently%20introduced%20GNN%0Aarchitecture%20inspired%20by%20the%20reconstruction%20conjecture%20%28Cotta%2C%20Morris%2C%20Ribeiro%0A2021%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09351v1&entry.124074799=Read"},
{"title": "See Through Their Minds: Learning Transferable Neural Representation\n  from Cross-Subject fMRI", "author": "Yulong Liu and Yongqiang Ma and Guibo Zhu and Haodong Jing and Nanning Zheng", "abstract": "  Deciphering visual content from functional Magnetic Resonance Imaging (fMRI)\nhelps illuminate the human vision system. However, the scarcity of fMRI data\nand noise hamper brain decoding model performance. Previous approaches\nprimarily employ subject-specific models, sensitive to training sample size. In\nthis paper, we explore a straightforward but overlooked solution to address\ndata scarcity. We propose shallow subject-specific adapters to map\ncross-subject fMRI data into unified representations. Subsequently, a shared\ndeeper decoding model decodes cross-subject features into the target feature\nspace. During training, we leverage both visual and textual supervision for\nmulti-modal brain decoding. Our model integrates a high-level perception\ndecoding pipeline and a pixel-wise reconstruction pipeline guided by high-level\nperceptions, simulating bottom-up and top-down processes in neuroscience.\nEmpirical experiments demonstrate robust neural representation learning across\nsubjects for both pipelines. Moreover, merging high-level and low-level\ninformation improves both low-level and high-level reconstruction metrics.\nAdditionally, we successfully transfer learned general knowledge to new\nsubjects by training new adapters with limited training data. Compared to\nprevious state-of-the-art methods, notably pre-training-based methods (Mind-Vis\nand fMRI-PTE), our approach achieves comparable or superior results across\ndiverse tasks, showing promise as an alternative method for cross-subject fMRI\ndata pre-training. Our code and pre-trained weights will be publicly released\nat https://github.com/YulongBonjour/See_Through_Their_Minds.\n", "link": "http://arxiv.org/abs/2403.06361v2", "date": "2024-06-13", "relevancy": 2.1727, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.574}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5244}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20Through%20Their%20Minds%3A%20Learning%20Transferable%20Neural%20Representation%0A%20%20from%20Cross-Subject%20fMRI&body=Title%3A%20See%20Through%20Their%20Minds%3A%20Learning%20Transferable%20Neural%20Representation%0A%20%20from%20Cross-Subject%20fMRI%0AAuthor%3A%20Yulong%20Liu%20and%20Yongqiang%20Ma%20and%20Guibo%20Zhu%20and%20Haodong%20Jing%20and%20Nanning%20Zheng%0AAbstract%3A%20%20%20Deciphering%20visual%20content%20from%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%0Ahelps%20illuminate%20the%20human%20vision%20system.%20However%2C%20the%20scarcity%20of%20fMRI%20data%0Aand%20noise%20hamper%20brain%20decoding%20model%20performance.%20Previous%20approaches%0Aprimarily%20employ%20subject-specific%20models%2C%20sensitive%20to%20training%20sample%20size.%20In%0Athis%20paper%2C%20we%20explore%20a%20straightforward%20but%20overlooked%20solution%20to%20address%0Adata%20scarcity.%20We%20propose%20shallow%20subject-specific%20adapters%20to%20map%0Across-subject%20fMRI%20data%20into%20unified%20representations.%20Subsequently%2C%20a%20shared%0Adeeper%20decoding%20model%20decodes%20cross-subject%20features%20into%20the%20target%20feature%0Aspace.%20During%20training%2C%20we%20leverage%20both%20visual%20and%20textual%20supervision%20for%0Amulti-modal%20brain%20decoding.%20Our%20model%20integrates%20a%20high-level%20perception%0Adecoding%20pipeline%20and%20a%20pixel-wise%20reconstruction%20pipeline%20guided%20by%20high-level%0Aperceptions%2C%20simulating%20bottom-up%20and%20top-down%20processes%20in%20neuroscience.%0AEmpirical%20experiments%20demonstrate%20robust%20neural%20representation%20learning%20across%0Asubjects%20for%20both%20pipelines.%20Moreover%2C%20merging%20high-level%20and%20low-level%0Ainformation%20improves%20both%20low-level%20and%20high-level%20reconstruction%20metrics.%0AAdditionally%2C%20we%20successfully%20transfer%20learned%20general%20knowledge%20to%20new%0Asubjects%20by%20training%20new%20adapters%20with%20limited%20training%20data.%20Compared%20to%0Aprevious%20state-of-the-art%20methods%2C%20notably%20pre-training-based%20methods%20%28Mind-Vis%0Aand%20fMRI-PTE%29%2C%20our%20approach%20achieves%20comparable%20or%20superior%20results%20across%0Adiverse%20tasks%2C%20showing%20promise%20as%20an%20alternative%20method%20for%20cross-subject%20fMRI%0Adata%20pre-training.%20Our%20code%20and%20pre-trained%20weights%20will%20be%20publicly%20released%0Aat%20https%3A//github.com/YulongBonjour/See_Through_Their_Minds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520Through%2520Their%2520Minds%253A%2520Learning%2520Transferable%2520Neural%2520Representation%250A%2520%2520from%2520Cross-Subject%2520fMRI%26entry.906535625%3DYulong%2520Liu%2520and%2520Yongqiang%2520Ma%2520and%2520Guibo%2520Zhu%2520and%2520Haodong%2520Jing%2520and%2520Nanning%2520Zheng%26entry.1292438233%3D%2520%2520Deciphering%2520visual%2520content%2520from%2520functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%250Ahelps%2520illuminate%2520the%2520human%2520vision%2520system.%2520However%252C%2520the%2520scarcity%2520of%2520fMRI%2520data%250Aand%2520noise%2520hamper%2520brain%2520decoding%2520model%2520performance.%2520Previous%2520approaches%250Aprimarily%2520employ%2520subject-specific%2520models%252C%2520sensitive%2520to%2520training%2520sample%2520size.%2520In%250Athis%2520paper%252C%2520we%2520explore%2520a%2520straightforward%2520but%2520overlooked%2520solution%2520to%2520address%250Adata%2520scarcity.%2520We%2520propose%2520shallow%2520subject-specific%2520adapters%2520to%2520map%250Across-subject%2520fMRI%2520data%2520into%2520unified%2520representations.%2520Subsequently%252C%2520a%2520shared%250Adeeper%2520decoding%2520model%2520decodes%2520cross-subject%2520features%2520into%2520the%2520target%2520feature%250Aspace.%2520During%2520training%252C%2520we%2520leverage%2520both%2520visual%2520and%2520textual%2520supervision%2520for%250Amulti-modal%2520brain%2520decoding.%2520Our%2520model%2520integrates%2520a%2520high-level%2520perception%250Adecoding%2520pipeline%2520and%2520a%2520pixel-wise%2520reconstruction%2520pipeline%2520guided%2520by%2520high-level%250Aperceptions%252C%2520simulating%2520bottom-up%2520and%2520top-down%2520processes%2520in%2520neuroscience.%250AEmpirical%2520experiments%2520demonstrate%2520robust%2520neural%2520representation%2520learning%2520across%250Asubjects%2520for%2520both%2520pipelines.%2520Moreover%252C%2520merging%2520high-level%2520and%2520low-level%250Ainformation%2520improves%2520both%2520low-level%2520and%2520high-level%2520reconstruction%2520metrics.%250AAdditionally%252C%2520we%2520successfully%2520transfer%2520learned%2520general%2520knowledge%2520to%2520new%250Asubjects%2520by%2520training%2520new%2520adapters%2520with%2520limited%2520training%2520data.%2520Compared%2520to%250Aprevious%2520state-of-the-art%2520methods%252C%2520notably%2520pre-training-based%2520methods%2520%2528Mind-Vis%250Aand%2520fMRI-PTE%2529%252C%2520our%2520approach%2520achieves%2520comparable%2520or%2520superior%2520results%2520across%250Adiverse%2520tasks%252C%2520showing%2520promise%2520as%2520an%2520alternative%2520method%2520for%2520cross-subject%2520fMRI%250Adata%2520pre-training.%2520Our%2520code%2520and%2520pre-trained%2520weights%2520will%2520be%2520publicly%2520released%250Aat%2520https%253A//github.com/YulongBonjour/See_Through_Their_Minds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20Through%20Their%20Minds%3A%20Learning%20Transferable%20Neural%20Representation%0A%20%20from%20Cross-Subject%20fMRI&entry.906535625=Yulong%20Liu%20and%20Yongqiang%20Ma%20and%20Guibo%20Zhu%20and%20Haodong%20Jing%20and%20Nanning%20Zheng&entry.1292438233=%20%20Deciphering%20visual%20content%20from%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%0Ahelps%20illuminate%20the%20human%20vision%20system.%20However%2C%20the%20scarcity%20of%20fMRI%20data%0Aand%20noise%20hamper%20brain%20decoding%20model%20performance.%20Previous%20approaches%0Aprimarily%20employ%20subject-specific%20models%2C%20sensitive%20to%20training%20sample%20size.%20In%0Athis%20paper%2C%20we%20explore%20a%20straightforward%20but%20overlooked%20solution%20to%20address%0Adata%20scarcity.%20We%20propose%20shallow%20subject-specific%20adapters%20to%20map%0Across-subject%20fMRI%20data%20into%20unified%20representations.%20Subsequently%2C%20a%20shared%0Adeeper%20decoding%20model%20decodes%20cross-subject%20features%20into%20the%20target%20feature%0Aspace.%20During%20training%2C%20we%20leverage%20both%20visual%20and%20textual%20supervision%20for%0Amulti-modal%20brain%20decoding.%20Our%20model%20integrates%20a%20high-level%20perception%0Adecoding%20pipeline%20and%20a%20pixel-wise%20reconstruction%20pipeline%20guided%20by%20high-level%0Aperceptions%2C%20simulating%20bottom-up%20and%20top-down%20processes%20in%20neuroscience.%0AEmpirical%20experiments%20demonstrate%20robust%20neural%20representation%20learning%20across%0Asubjects%20for%20both%20pipelines.%20Moreover%2C%20merging%20high-level%20and%20low-level%0Ainformation%20improves%20both%20low-level%20and%20high-level%20reconstruction%20metrics.%0AAdditionally%2C%20we%20successfully%20transfer%20learned%20general%20knowledge%20to%20new%0Asubjects%20by%20training%20new%20adapters%20with%20limited%20training%20data.%20Compared%20to%0Aprevious%20state-of-the-art%20methods%2C%20notably%20pre-training-based%20methods%20%28Mind-Vis%0Aand%20fMRI-PTE%29%2C%20our%20approach%20achieves%20comparable%20or%20superior%20results%20across%0Adiverse%20tasks%2C%20showing%20promise%20as%20an%20alternative%20method%20for%20cross-subject%20fMRI%0Adata%20pre-training.%20Our%20code%20and%20pre-trained%20weights%20will%20be%20publicly%20released%0Aat%20https%3A//github.com/YulongBonjour/See_Through_Their_Minds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06361v2&entry.124074799=Read"},
{"title": "Flexible Heteroscedastic Count Regression with Deep Double Poisson\n  Networks", "author": "Spencer Young and Porter Jenkins and Lonchao Da and Jeff Dotson and Hua Wei", "abstract": "  Neural networks that can produce accurate, input-conditional uncertainty\nrepresentations are critical for real-world applications. Recent progress on\nheteroscedastic continuous regression has shown great promise for calibrated\nuncertainty quantification on complex tasks, like image regression. However,\nwhen these methods are applied to discrete regression tasks, such as crowd\ncounting, ratings prediction, or inventory estimation, they tend to produce\npredictive distributions with numerous pathologies. We propose to address these\nissues by training a neural network to output the parameters of a Double\nPoisson distribution, which we call the Deep Double Poisson Network (DDPN). In\ncontrast to existing methods that are trained to minimize Gaussian negative log\nlikelihood (NLL), DDPNs produce a proper probability mass function over\ndiscrete output. Additionally, DDPNs naturally model under-, over-, and\nequi-dispersion, unlike networks trained with the more rigid Poisson and\nNegative Binomial parameterizations. We show DDPNs 1) vastly outperform\nexisting discrete models; 2) meet or exceed the accuracy and flexibility of\nnetworks trained with Gaussian NLL; 3) produce proper predictive distributions\nover discrete counts; and 4) exhibit superior out-of-distribution detection.\nDDPNs can easily be applied to a variety of count regression datasets including\ntabular, image, point cloud, and text data.\n", "link": "http://arxiv.org/abs/2406.09262v1", "date": "2024-06-13", "relevancy": 2.1662, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5655}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5249}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flexible%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%0A%20%20Networks&body=Title%3A%20Flexible%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%0A%20%20Networks%0AAuthor%3A%20Spencer%20Young%20and%20Porter%20Jenkins%20and%20Lonchao%20Da%20and%20Jeff%20Dotson%20and%20Hua%20Wei%0AAbstract%3A%20%20%20Neural%20networks%20that%20can%20produce%20accurate%2C%20input-conditional%20uncertainty%0Arepresentations%20are%20critical%20for%20real-world%20applications.%20Recent%20progress%20on%0Aheteroscedastic%20continuous%20regression%20has%20shown%20great%20promise%20for%20calibrated%0Auncertainty%20quantification%20on%20complex%20tasks%2C%20like%20image%20regression.%20However%2C%0Awhen%20these%20methods%20are%20applied%20to%20discrete%20regression%20tasks%2C%20such%20as%20crowd%0Acounting%2C%20ratings%20prediction%2C%20or%20inventory%20estimation%2C%20they%20tend%20to%20produce%0Apredictive%20distributions%20with%20numerous%20pathologies.%20We%20propose%20to%20address%20these%0Aissues%20by%20training%20a%20neural%20network%20to%20output%20the%20parameters%20of%20a%20Double%0APoisson%20distribution%2C%20which%20we%20call%20the%20Deep%20Double%20Poisson%20Network%20%28DDPN%29.%20In%0Acontrast%20to%20existing%20methods%20that%20are%20trained%20to%20minimize%20Gaussian%20negative%20log%0Alikelihood%20%28NLL%29%2C%20DDPNs%20produce%20a%20proper%20probability%20mass%20function%20over%0Adiscrete%20output.%20Additionally%2C%20DDPNs%20naturally%20model%20under-%2C%20over-%2C%20and%0Aequi-dispersion%2C%20unlike%20networks%20trained%20with%20the%20more%20rigid%20Poisson%20and%0ANegative%20Binomial%20parameterizations.%20We%20show%20DDPNs%201%29%20vastly%20outperform%0Aexisting%20discrete%20models%3B%202%29%20meet%20or%20exceed%20the%20accuracy%20and%20flexibility%20of%0Anetworks%20trained%20with%20Gaussian%20NLL%3B%203%29%20produce%20proper%20predictive%20distributions%0Aover%20discrete%20counts%3B%20and%204%29%20exhibit%20superior%20out-of-distribution%20detection.%0ADDPNs%20can%20easily%20be%20applied%20to%20a%20variety%20of%20count%20regression%20datasets%20including%0Atabular%2C%20image%2C%20point%20cloud%2C%20and%20text%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexible%2520Heteroscedastic%2520Count%2520Regression%2520with%2520Deep%2520Double%2520Poisson%250A%2520%2520Networks%26entry.906535625%3DSpencer%2520Young%2520and%2520Porter%2520Jenkins%2520and%2520Lonchao%2520Da%2520and%2520Jeff%2520Dotson%2520and%2520Hua%2520Wei%26entry.1292438233%3D%2520%2520Neural%2520networks%2520that%2520can%2520produce%2520accurate%252C%2520input-conditional%2520uncertainty%250Arepresentations%2520are%2520critical%2520for%2520real-world%2520applications.%2520Recent%2520progress%2520on%250Aheteroscedastic%2520continuous%2520regression%2520has%2520shown%2520great%2520promise%2520for%2520calibrated%250Auncertainty%2520quantification%2520on%2520complex%2520tasks%252C%2520like%2520image%2520regression.%2520However%252C%250Awhen%2520these%2520methods%2520are%2520applied%2520to%2520discrete%2520regression%2520tasks%252C%2520such%2520as%2520crowd%250Acounting%252C%2520ratings%2520prediction%252C%2520or%2520inventory%2520estimation%252C%2520they%2520tend%2520to%2520produce%250Apredictive%2520distributions%2520with%2520numerous%2520pathologies.%2520We%2520propose%2520to%2520address%2520these%250Aissues%2520by%2520training%2520a%2520neural%2520network%2520to%2520output%2520the%2520parameters%2520of%2520a%2520Double%250APoisson%2520distribution%252C%2520which%2520we%2520call%2520the%2520Deep%2520Double%2520Poisson%2520Network%2520%2528DDPN%2529.%2520In%250Acontrast%2520to%2520existing%2520methods%2520that%2520are%2520trained%2520to%2520minimize%2520Gaussian%2520negative%2520log%250Alikelihood%2520%2528NLL%2529%252C%2520DDPNs%2520produce%2520a%2520proper%2520probability%2520mass%2520function%2520over%250Adiscrete%2520output.%2520Additionally%252C%2520DDPNs%2520naturally%2520model%2520under-%252C%2520over-%252C%2520and%250Aequi-dispersion%252C%2520unlike%2520networks%2520trained%2520with%2520the%2520more%2520rigid%2520Poisson%2520and%250ANegative%2520Binomial%2520parameterizations.%2520We%2520show%2520DDPNs%25201%2529%2520vastly%2520outperform%250Aexisting%2520discrete%2520models%253B%25202%2529%2520meet%2520or%2520exceed%2520the%2520accuracy%2520and%2520flexibility%2520of%250Anetworks%2520trained%2520with%2520Gaussian%2520NLL%253B%25203%2529%2520produce%2520proper%2520predictive%2520distributions%250Aover%2520discrete%2520counts%253B%2520and%25204%2529%2520exhibit%2520superior%2520out-of-distribution%2520detection.%250ADDPNs%2520can%2520easily%2520be%2520applied%2520to%2520a%2520variety%2520of%2520count%2520regression%2520datasets%2520including%250Atabular%252C%2520image%252C%2520point%2520cloud%252C%2520and%2520text%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flexible%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%0A%20%20Networks&entry.906535625=Spencer%20Young%20and%20Porter%20Jenkins%20and%20Lonchao%20Da%20and%20Jeff%20Dotson%20and%20Hua%20Wei&entry.1292438233=%20%20Neural%20networks%20that%20can%20produce%20accurate%2C%20input-conditional%20uncertainty%0Arepresentations%20are%20critical%20for%20real-world%20applications.%20Recent%20progress%20on%0Aheteroscedastic%20continuous%20regression%20has%20shown%20great%20promise%20for%20calibrated%0Auncertainty%20quantification%20on%20complex%20tasks%2C%20like%20image%20regression.%20However%2C%0Awhen%20these%20methods%20are%20applied%20to%20discrete%20regression%20tasks%2C%20such%20as%20crowd%0Acounting%2C%20ratings%20prediction%2C%20or%20inventory%20estimation%2C%20they%20tend%20to%20produce%0Apredictive%20distributions%20with%20numerous%20pathologies.%20We%20propose%20to%20address%20these%0Aissues%20by%20training%20a%20neural%20network%20to%20output%20the%20parameters%20of%20a%20Double%0APoisson%20distribution%2C%20which%20we%20call%20the%20Deep%20Double%20Poisson%20Network%20%28DDPN%29.%20In%0Acontrast%20to%20existing%20methods%20that%20are%20trained%20to%20minimize%20Gaussian%20negative%20log%0Alikelihood%20%28NLL%29%2C%20DDPNs%20produce%20a%20proper%20probability%20mass%20function%20over%0Adiscrete%20output.%20Additionally%2C%20DDPNs%20naturally%20model%20under-%2C%20over-%2C%20and%0Aequi-dispersion%2C%20unlike%20networks%20trained%20with%20the%20more%20rigid%20Poisson%20and%0ANegative%20Binomial%20parameterizations.%20We%20show%20DDPNs%201%29%20vastly%20outperform%0Aexisting%20discrete%20models%3B%202%29%20meet%20or%20exceed%20the%20accuracy%20and%20flexibility%20of%0Anetworks%20trained%20with%20Gaussian%20NLL%3B%203%29%20produce%20proper%20predictive%20distributions%0Aover%20discrete%20counts%3B%20and%204%29%20exhibit%20superior%20out-of-distribution%20detection.%0ADDPNs%20can%20easily%20be%20applied%20to%20a%20variety%20of%20count%20regression%20datasets%20including%0Atabular%2C%20image%2C%20point%20cloud%2C%20and%20text%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09262v1&entry.124074799=Read"},
{"title": "Zero-Shot Learning Over Large Output Spaces : Utilizing Indirect\n  Knowledge Extraction from Large Language Models", "author": "Jinbin Zhang and Nasib Ullah and Rohit Babbar", "abstract": "  Extreme Multi-label Learning (XMC) is a task that allocates the most relevant\nlabels for an instance from a predefined label set. Extreme Zero-shot XMC\n(EZ-XMC) is a special setting of XMC wherein no supervision is provided; only\nthe instances (raw text of the document) and the predetermined label set are\ngiven. The scenario is designed to address cold-start problems in\ncategorization and recommendation. Traditional state-of-the-art methods extract\npseudo labels from the document title or segments. These labels from the\ndocument are used to train a zero-shot bi-encoder model. The main issue with\nthese generated labels is their misalignment with the tagging task. In this\nwork, we propose a framework to train a small bi-encoder model via the feedback\nfrom the large language model (LLM), the bi-encoder model encodes the document\nand labels into embeddings for retrieval. Our approach leverages the zero-shot\nability of LLM to assess the correlation between labels and the document\ninstead of using the low-quality labels extracted from the document itself. Our\nmethod also guarantees fast inference without the involvement of LLM. The\nperformance of our approach outperforms the SOTA methods on various datasets\nwhile retaining a similar training time for large datasets.\n", "link": "http://arxiv.org/abs/2406.09288v1", "date": "2024-06-13", "relevancy": 2.164, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5537}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Learning%20Over%20Large%20Output%20Spaces%20%3A%20Utilizing%20Indirect%0A%20%20Knowledge%20Extraction%20from%20Large%20Language%20Models&body=Title%3A%20Zero-Shot%20Learning%20Over%20Large%20Output%20Spaces%20%3A%20Utilizing%20Indirect%0A%20%20Knowledge%20Extraction%20from%20Large%20Language%20Models%0AAuthor%3A%20Jinbin%20Zhang%20and%20Nasib%20Ullah%20and%20Rohit%20Babbar%0AAbstract%3A%20%20%20Extreme%20Multi-label%20Learning%20%28XMC%29%20is%20a%20task%20that%20allocates%20the%20most%20relevant%0Alabels%20for%20an%20instance%20from%20a%20predefined%20label%20set.%20Extreme%20Zero-shot%20XMC%0A%28EZ-XMC%29%20is%20a%20special%20setting%20of%20XMC%20wherein%20no%20supervision%20is%20provided%3B%20only%0Athe%20instances%20%28raw%20text%20of%20the%20document%29%20and%20the%20predetermined%20label%20set%20are%0Agiven.%20The%20scenario%20is%20designed%20to%20address%20cold-start%20problems%20in%0Acategorization%20and%20recommendation.%20Traditional%20state-of-the-art%20methods%20extract%0Apseudo%20labels%20from%20the%20document%20title%20or%20segments.%20These%20labels%20from%20the%0Adocument%20are%20used%20to%20train%20a%20zero-shot%20bi-encoder%20model.%20The%20main%20issue%20with%0Athese%20generated%20labels%20is%20their%20misalignment%20with%20the%20tagging%20task.%20In%20this%0Awork%2C%20we%20propose%20a%20framework%20to%20train%20a%20small%20bi-encoder%20model%20via%20the%20feedback%0Afrom%20the%20large%20language%20model%20%28LLM%29%2C%20the%20bi-encoder%20model%20encodes%20the%20document%0Aand%20labels%20into%20embeddings%20for%20retrieval.%20Our%20approach%20leverages%20the%20zero-shot%0Aability%20of%20LLM%20to%20assess%20the%20correlation%20between%20labels%20and%20the%20document%0Ainstead%20of%20using%20the%20low-quality%20labels%20extracted%20from%20the%20document%20itself.%20Our%0Amethod%20also%20guarantees%20fast%20inference%20without%20the%20involvement%20of%20LLM.%20The%0Aperformance%20of%20our%20approach%20outperforms%20the%20SOTA%20methods%20on%20various%20datasets%0Awhile%20retaining%20a%20similar%20training%20time%20for%20large%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Learning%2520Over%2520Large%2520Output%2520Spaces%2520%253A%2520Utilizing%2520Indirect%250A%2520%2520Knowledge%2520Extraction%2520from%2520Large%2520Language%2520Models%26entry.906535625%3DJinbin%2520Zhang%2520and%2520Nasib%2520Ullah%2520and%2520Rohit%2520Babbar%26entry.1292438233%3D%2520%2520Extreme%2520Multi-label%2520Learning%2520%2528XMC%2529%2520is%2520a%2520task%2520that%2520allocates%2520the%2520most%2520relevant%250Alabels%2520for%2520an%2520instance%2520from%2520a%2520predefined%2520label%2520set.%2520Extreme%2520Zero-shot%2520XMC%250A%2528EZ-XMC%2529%2520is%2520a%2520special%2520setting%2520of%2520XMC%2520wherein%2520no%2520supervision%2520is%2520provided%253B%2520only%250Athe%2520instances%2520%2528raw%2520text%2520of%2520the%2520document%2529%2520and%2520the%2520predetermined%2520label%2520set%2520are%250Agiven.%2520The%2520scenario%2520is%2520designed%2520to%2520address%2520cold-start%2520problems%2520in%250Acategorization%2520and%2520recommendation.%2520Traditional%2520state-of-the-art%2520methods%2520extract%250Apseudo%2520labels%2520from%2520the%2520document%2520title%2520or%2520segments.%2520These%2520labels%2520from%2520the%250Adocument%2520are%2520used%2520to%2520train%2520a%2520zero-shot%2520bi-encoder%2520model.%2520The%2520main%2520issue%2520with%250Athese%2520generated%2520labels%2520is%2520their%2520misalignment%2520with%2520the%2520tagging%2520task.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520framework%2520to%2520train%2520a%2520small%2520bi-encoder%2520model%2520via%2520the%2520feedback%250Afrom%2520the%2520large%2520language%2520model%2520%2528LLM%2529%252C%2520the%2520bi-encoder%2520model%2520encodes%2520the%2520document%250Aand%2520labels%2520into%2520embeddings%2520for%2520retrieval.%2520Our%2520approach%2520leverages%2520the%2520zero-shot%250Aability%2520of%2520LLM%2520to%2520assess%2520the%2520correlation%2520between%2520labels%2520and%2520the%2520document%250Ainstead%2520of%2520using%2520the%2520low-quality%2520labels%2520extracted%2520from%2520the%2520document%2520itself.%2520Our%250Amethod%2520also%2520guarantees%2520fast%2520inference%2520without%2520the%2520involvement%2520of%2520LLM.%2520The%250Aperformance%2520of%2520our%2520approach%2520outperforms%2520the%2520SOTA%2520methods%2520on%2520various%2520datasets%250Awhile%2520retaining%2520a%2520similar%2520training%2520time%2520for%2520large%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Learning%20Over%20Large%20Output%20Spaces%20%3A%20Utilizing%20Indirect%0A%20%20Knowledge%20Extraction%20from%20Large%20Language%20Models&entry.906535625=Jinbin%20Zhang%20and%20Nasib%20Ullah%20and%20Rohit%20Babbar&entry.1292438233=%20%20Extreme%20Multi-label%20Learning%20%28XMC%29%20is%20a%20task%20that%20allocates%20the%20most%20relevant%0Alabels%20for%20an%20instance%20from%20a%20predefined%20label%20set.%20Extreme%20Zero-shot%20XMC%0A%28EZ-XMC%29%20is%20a%20special%20setting%20of%20XMC%20wherein%20no%20supervision%20is%20provided%3B%20only%0Athe%20instances%20%28raw%20text%20of%20the%20document%29%20and%20the%20predetermined%20label%20set%20are%0Agiven.%20The%20scenario%20is%20designed%20to%20address%20cold-start%20problems%20in%0Acategorization%20and%20recommendation.%20Traditional%20state-of-the-art%20methods%20extract%0Apseudo%20labels%20from%20the%20document%20title%20or%20segments.%20These%20labels%20from%20the%0Adocument%20are%20used%20to%20train%20a%20zero-shot%20bi-encoder%20model.%20The%20main%20issue%20with%0Athese%20generated%20labels%20is%20their%20misalignment%20with%20the%20tagging%20task.%20In%20this%0Awork%2C%20we%20propose%20a%20framework%20to%20train%20a%20small%20bi-encoder%20model%20via%20the%20feedback%0Afrom%20the%20large%20language%20model%20%28LLM%29%2C%20the%20bi-encoder%20model%20encodes%20the%20document%0Aand%20labels%20into%20embeddings%20for%20retrieval.%20Our%20approach%20leverages%20the%20zero-shot%0Aability%20of%20LLM%20to%20assess%20the%20correlation%20between%20labels%20and%20the%20document%0Ainstead%20of%20using%20the%20low-quality%20labels%20extracted%20from%20the%20document%20itself.%20Our%0Amethod%20also%20guarantees%20fast%20inference%20without%20the%20involvement%20of%20LLM.%20The%0Aperformance%20of%20our%20approach%20outperforms%20the%20SOTA%20methods%20on%20various%20datasets%0Awhile%20retaining%20a%20similar%20training%20time%20for%20large%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09288v1&entry.124074799=Read"},
{"title": "A Large-scale Universal Evaluation Benchmark For Face Forgery Detection", "author": "Yijun Bei and Hengrui Lou and Jinsong Geng and Erteng Liu and Lechao Cheng and Jie Song and Mingli Song and Zunlei Feng", "abstract": "  With the rapid development of AI-generated content (AIGC) technology, the\nproduction of realistic fake facial images and videos that deceive human visual\nperception has become possible. Consequently, various face forgery detection\ntechniques have been proposed to identify such fake facial content. However,\nevaluating the effectiveness and generalizability of these detection techniques\nremains a significant challenge. To address this, we have constructed a\nlarge-scale evaluation benchmark called DeepFaceGen, aimed at quantitatively\nassessing the effectiveness of face forgery detection and facilitating the\niterative development of forgery detection technology. DeepFaceGen consists of\n776,990 real face image/video samples and 773,812 face forgery image/video\nsamples, generated using 34 mainstream face generation techniques. During the\nconstruction process, we carefully consider important factors such as content\ndiversity, fairness across ethnicities, and availability of comprehensive\nlabels, in order to ensure the versatility and convenience of DeepFaceGen.\nSubsequently, DeepFaceGen is employed in this study to evaluate and analyze the\nperformance of 13 mainstream face forgery detection techniques from various\nperspectives. Through extensive experimental analysis, we derive significant\nfindings and propose potential directions for future research. The code and\ndataset for DeepFaceGen are available at\nhttps://anonymous.4open.science/r/DeepFaceGen-47D1.\n", "link": "http://arxiv.org/abs/2406.09181v1", "date": "2024-06-13", "relevancy": 2.158, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5488}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5402}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-scale%20Universal%20Evaluation%20Benchmark%20For%20Face%20Forgery%20Detection&body=Title%3A%20A%20Large-scale%20Universal%20Evaluation%20Benchmark%20For%20Face%20Forgery%20Detection%0AAuthor%3A%20Yijun%20Bei%20and%20Hengrui%20Lou%20and%20Jinsong%20Geng%20and%20Erteng%20Liu%20and%20Lechao%20Cheng%20and%20Jie%20Song%20and%20Mingli%20Song%20and%20Zunlei%20Feng%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20AI-generated%20content%20%28AIGC%29%20technology%2C%20the%0Aproduction%20of%20realistic%20fake%20facial%20images%20and%20videos%20that%20deceive%20human%20visual%0Aperception%20has%20become%20possible.%20Consequently%2C%20various%20face%20forgery%20detection%0Atechniques%20have%20been%20proposed%20to%20identify%20such%20fake%20facial%20content.%20However%2C%0Aevaluating%20the%20effectiveness%20and%20generalizability%20of%20these%20detection%20techniques%0Aremains%20a%20significant%20challenge.%20To%20address%20this%2C%20we%20have%20constructed%20a%0Alarge-scale%20evaluation%20benchmark%20called%20DeepFaceGen%2C%20aimed%20at%20quantitatively%0Aassessing%20the%20effectiveness%20of%20face%20forgery%20detection%20and%20facilitating%20the%0Aiterative%20development%20of%20forgery%20detection%20technology.%20DeepFaceGen%20consists%20of%0A776%2C990%20real%20face%20image/video%20samples%20and%20773%2C812%20face%20forgery%20image/video%0Asamples%2C%20generated%20using%2034%20mainstream%20face%20generation%20techniques.%20During%20the%0Aconstruction%20process%2C%20we%20carefully%20consider%20important%20factors%20such%20as%20content%0Adiversity%2C%20fairness%20across%20ethnicities%2C%20and%20availability%20of%20comprehensive%0Alabels%2C%20in%20order%20to%20ensure%20the%20versatility%20and%20convenience%20of%20DeepFaceGen.%0ASubsequently%2C%20DeepFaceGen%20is%20employed%20in%20this%20study%20to%20evaluate%20and%20analyze%20the%0Aperformance%20of%2013%20mainstream%20face%20forgery%20detection%20techniques%20from%20various%0Aperspectives.%20Through%20extensive%20experimental%20analysis%2C%20we%20derive%20significant%0Afindings%20and%20propose%20potential%20directions%20for%20future%20research.%20The%20code%20and%0Adataset%20for%20DeepFaceGen%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/DeepFaceGen-47D1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-scale%2520Universal%2520Evaluation%2520Benchmark%2520For%2520Face%2520Forgery%2520Detection%26entry.906535625%3DYijun%2520Bei%2520and%2520Hengrui%2520Lou%2520and%2520Jinsong%2520Geng%2520and%2520Erteng%2520Liu%2520and%2520Lechao%2520Cheng%2520and%2520Jie%2520Song%2520and%2520Mingli%2520Song%2520and%2520Zunlei%2520Feng%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520AI-generated%2520content%2520%2528AIGC%2529%2520technology%252C%2520the%250Aproduction%2520of%2520realistic%2520fake%2520facial%2520images%2520and%2520videos%2520that%2520deceive%2520human%2520visual%250Aperception%2520has%2520become%2520possible.%2520Consequently%252C%2520various%2520face%2520forgery%2520detection%250Atechniques%2520have%2520been%2520proposed%2520to%2520identify%2520such%2520fake%2520facial%2520content.%2520However%252C%250Aevaluating%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520these%2520detection%2520techniques%250Aremains%2520a%2520significant%2520challenge.%2520To%2520address%2520this%252C%2520we%2520have%2520constructed%2520a%250Alarge-scale%2520evaluation%2520benchmark%2520called%2520DeepFaceGen%252C%2520aimed%2520at%2520quantitatively%250Aassessing%2520the%2520effectiveness%2520of%2520face%2520forgery%2520detection%2520and%2520facilitating%2520the%250Aiterative%2520development%2520of%2520forgery%2520detection%2520technology.%2520DeepFaceGen%2520consists%2520of%250A776%252C990%2520real%2520face%2520image/video%2520samples%2520and%2520773%252C812%2520face%2520forgery%2520image/video%250Asamples%252C%2520generated%2520using%252034%2520mainstream%2520face%2520generation%2520techniques.%2520During%2520the%250Aconstruction%2520process%252C%2520we%2520carefully%2520consider%2520important%2520factors%2520such%2520as%2520content%250Adiversity%252C%2520fairness%2520across%2520ethnicities%252C%2520and%2520availability%2520of%2520comprehensive%250Alabels%252C%2520in%2520order%2520to%2520ensure%2520the%2520versatility%2520and%2520convenience%2520of%2520DeepFaceGen.%250ASubsequently%252C%2520DeepFaceGen%2520is%2520employed%2520in%2520this%2520study%2520to%2520evaluate%2520and%2520analyze%2520the%250Aperformance%2520of%252013%2520mainstream%2520face%2520forgery%2520detection%2520techniques%2520from%2520various%250Aperspectives.%2520Through%2520extensive%2520experimental%2520analysis%252C%2520we%2520derive%2520significant%250Afindings%2520and%2520propose%2520potential%2520directions%2520for%2520future%2520research.%2520The%2520code%2520and%250Adataset%2520for%2520DeepFaceGen%2520are%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/DeepFaceGen-47D1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-scale%20Universal%20Evaluation%20Benchmark%20For%20Face%20Forgery%20Detection&entry.906535625=Yijun%20Bei%20and%20Hengrui%20Lou%20and%20Jinsong%20Geng%20and%20Erteng%20Liu%20and%20Lechao%20Cheng%20and%20Jie%20Song%20and%20Mingli%20Song%20and%20Zunlei%20Feng&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20AI-generated%20content%20%28AIGC%29%20technology%2C%20the%0Aproduction%20of%20realistic%20fake%20facial%20images%20and%20videos%20that%20deceive%20human%20visual%0Aperception%20has%20become%20possible.%20Consequently%2C%20various%20face%20forgery%20detection%0Atechniques%20have%20been%20proposed%20to%20identify%20such%20fake%20facial%20content.%20However%2C%0Aevaluating%20the%20effectiveness%20and%20generalizability%20of%20these%20detection%20techniques%0Aremains%20a%20significant%20challenge.%20To%20address%20this%2C%20we%20have%20constructed%20a%0Alarge-scale%20evaluation%20benchmark%20called%20DeepFaceGen%2C%20aimed%20at%20quantitatively%0Aassessing%20the%20effectiveness%20of%20face%20forgery%20detection%20and%20facilitating%20the%0Aiterative%20development%20of%20forgery%20detection%20technology.%20DeepFaceGen%20consists%20of%0A776%2C990%20real%20face%20image/video%20samples%20and%20773%2C812%20face%20forgery%20image/video%0Asamples%2C%20generated%20using%2034%20mainstream%20face%20generation%20techniques.%20During%20the%0Aconstruction%20process%2C%20we%20carefully%20consider%20important%20factors%20such%20as%20content%0Adiversity%2C%20fairness%20across%20ethnicities%2C%20and%20availability%20of%20comprehensive%0Alabels%2C%20in%20order%20to%20ensure%20the%20versatility%20and%20convenience%20of%20DeepFaceGen.%0ASubsequently%2C%20DeepFaceGen%20is%20employed%20in%20this%20study%20to%20evaluate%20and%20analyze%20the%0Aperformance%20of%2013%20mainstream%20face%20forgery%20detection%20techniques%20from%20various%0Aperspectives.%20Through%20extensive%20experimental%20analysis%2C%20we%20derive%20significant%0Afindings%20and%20propose%20potential%20directions%20for%20future%20research.%20The%20code%20and%0Adataset%20for%20DeepFaceGen%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/DeepFaceGen-47D1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09181v1&entry.124074799=Read"},
{"title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal\n  Language Models", "author": "Yushi Hu and Weijia Shi and Xingyu Fu and Dan Roth and Mari Ostendorf and Luke Zettlemoyer and Noah A Smith and Ranjay Krishna", "abstract": "  Humans draw to facilitate reasoning: we draw auxiliary lines when solving\ngeometry problems; we mark and circle when reasoning on maps; we use sketches\nto amplify our ideas and relieve our limited-capacity working memory. However,\nsuch actions are missing in current multimodal language models (LMs). Current\nchain-of-thought and tool-use paradigms only use text as intermediate reasoning\nsteps. In this work, we introduce Sketchpad, a framework that gives multimodal\nLMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts\nplanning and reasoning according to the visual artifacts it has drawn.\nDifferent from prior work, which uses text-to-image models to enable LMs to\ndraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is\ncloser to human sketching and better facilitates reasoning. Sketchpad can also\nuse specialist vision models during the sketching process (e.g., draw bounding\nboxes with object detection models, draw masks with segmentation models), to\nfurther enhance visual perception and reasoning. We experiment with a wide\nrange of math tasks (including geometry, functions, graphs, and chess) and\ncomplex visual reasoning tasks. Sketchpad substantially improves performance on\nall tasks over strong base models with no sketching, yielding an average gain\nof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a\nnew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial\nreasoning (83.9%), and visual correspondence (80.8%). All codes and data are in\nhttps://visualsketchpad.github.io/.\n", "link": "http://arxiv.org/abs/2406.09403v1", "date": "2024-06-13", "relevancy": 2.1544, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5363}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Sketchpad%3A%20Sketching%20as%20a%20Visual%20Chain%20of%20Thought%20for%20Multimodal%0A%20%20Language%20Models&body=Title%3A%20Visual%20Sketchpad%3A%20Sketching%20as%20a%20Visual%20Chain%20of%20Thought%20for%20Multimodal%0A%20%20Language%20Models%0AAuthor%3A%20Yushi%20Hu%20and%20Weijia%20Shi%20and%20Xingyu%20Fu%20and%20Dan%20Roth%20and%20Mari%20Ostendorf%20and%20Luke%20Zettlemoyer%20and%20Noah%20A%20Smith%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Humans%20draw%20to%20facilitate%20reasoning%3A%20we%20draw%20auxiliary%20lines%20when%20solving%0Ageometry%20problems%3B%20we%20mark%20and%20circle%20when%20reasoning%20on%20maps%3B%20we%20use%20sketches%0Ato%20amplify%20our%20ideas%20and%20relieve%20our%20limited-capacity%20working%20memory.%20However%2C%0Asuch%20actions%20are%20missing%20in%20current%20multimodal%20language%20models%20%28LMs%29.%20Current%0Achain-of-thought%20and%20tool-use%20paradigms%20only%20use%20text%20as%20intermediate%20reasoning%0Asteps.%20In%20this%20work%2C%20we%20introduce%20Sketchpad%2C%20a%20framework%20that%20gives%20multimodal%0ALMs%20a%20visual%20sketchpad%20and%20tools%20to%20draw%20on%20the%20sketchpad.%20The%20LM%20conducts%0Aplanning%20and%20reasoning%20according%20to%20the%20visual%20artifacts%20it%20has%20drawn.%0ADifferent%20from%20prior%20work%2C%20which%20uses%20text-to-image%20models%20to%20enable%20LMs%20to%0Adraw%2C%20Sketchpad%20enables%20LMs%20to%20draw%20with%20lines%2C%20boxes%2C%20marks%2C%20etc.%2C%20which%20is%0Acloser%20to%20human%20sketching%20and%20better%20facilitates%20reasoning.%20Sketchpad%20can%20also%0Ause%20specialist%20vision%20models%20during%20the%20sketching%20process%20%28e.g.%2C%20draw%20bounding%0Aboxes%20with%20object%20detection%20models%2C%20draw%20masks%20with%20segmentation%20models%29%2C%20to%0Afurther%20enhance%20visual%20perception%20and%20reasoning.%20We%20experiment%20with%20a%20wide%0Arange%20of%20math%20tasks%20%28including%20geometry%2C%20functions%2C%20graphs%2C%20and%20chess%29%20and%0Acomplex%20visual%20reasoning%20tasks.%20Sketchpad%20substantially%20improves%20performance%20on%0Aall%20tasks%20over%20strong%20base%20models%20with%20no%20sketching%2C%20yielding%20an%20average%20gain%0Aof%2012.7%25%20on%20math%20tasks%2C%20and%208.6%25%20on%20vision%20tasks.%20GPT-4o%20with%20Sketchpad%20sets%20a%0Anew%20state%20of%20the%20art%20on%20all%20tasks%2C%20including%20V%2ABench%20%2880.3%25%29%2C%20BLINK%20spatial%0Areasoning%20%2883.9%25%29%2C%20and%20visual%20correspondence%20%2880.8%25%29.%20All%20codes%20and%20data%20are%20in%0Ahttps%3A//visualsketchpad.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Sketchpad%253A%2520Sketching%2520as%2520a%2520Visual%2520Chain%2520of%2520Thought%2520for%2520Multimodal%250A%2520%2520Language%2520Models%26entry.906535625%3DYushi%2520Hu%2520and%2520Weijia%2520Shi%2520and%2520Xingyu%2520Fu%2520and%2520Dan%2520Roth%2520and%2520Mari%2520Ostendorf%2520and%2520Luke%2520Zettlemoyer%2520and%2520Noah%2520A%2520Smith%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Humans%2520draw%2520to%2520facilitate%2520reasoning%253A%2520we%2520draw%2520auxiliary%2520lines%2520when%2520solving%250Ageometry%2520problems%253B%2520we%2520mark%2520and%2520circle%2520when%2520reasoning%2520on%2520maps%253B%2520we%2520use%2520sketches%250Ato%2520amplify%2520our%2520ideas%2520and%2520relieve%2520our%2520limited-capacity%2520working%2520memory.%2520However%252C%250Asuch%2520actions%2520are%2520missing%2520in%2520current%2520multimodal%2520language%2520models%2520%2528LMs%2529.%2520Current%250Achain-of-thought%2520and%2520tool-use%2520paradigms%2520only%2520use%2520text%2520as%2520intermediate%2520reasoning%250Asteps.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Sketchpad%252C%2520a%2520framework%2520that%2520gives%2520multimodal%250ALMs%2520a%2520visual%2520sketchpad%2520and%2520tools%2520to%2520draw%2520on%2520the%2520sketchpad.%2520The%2520LM%2520conducts%250Aplanning%2520and%2520reasoning%2520according%2520to%2520the%2520visual%2520artifacts%2520it%2520has%2520drawn.%250ADifferent%2520from%2520prior%2520work%252C%2520which%2520uses%2520text-to-image%2520models%2520to%2520enable%2520LMs%2520to%250Adraw%252C%2520Sketchpad%2520enables%2520LMs%2520to%2520draw%2520with%2520lines%252C%2520boxes%252C%2520marks%252C%2520etc.%252C%2520which%2520is%250Acloser%2520to%2520human%2520sketching%2520and%2520better%2520facilitates%2520reasoning.%2520Sketchpad%2520can%2520also%250Ause%2520specialist%2520vision%2520models%2520during%2520the%2520sketching%2520process%2520%2528e.g.%252C%2520draw%2520bounding%250Aboxes%2520with%2520object%2520detection%2520models%252C%2520draw%2520masks%2520with%2520segmentation%2520models%2529%252C%2520to%250Afurther%2520enhance%2520visual%2520perception%2520and%2520reasoning.%2520We%2520experiment%2520with%2520a%2520wide%250Arange%2520of%2520math%2520tasks%2520%2528including%2520geometry%252C%2520functions%252C%2520graphs%252C%2520and%2520chess%2529%2520and%250Acomplex%2520visual%2520reasoning%2520tasks.%2520Sketchpad%2520substantially%2520improves%2520performance%2520on%250Aall%2520tasks%2520over%2520strong%2520base%2520models%2520with%2520no%2520sketching%252C%2520yielding%2520an%2520average%2520gain%250Aof%252012.7%2525%2520on%2520math%2520tasks%252C%2520and%25208.6%2525%2520on%2520vision%2520tasks.%2520GPT-4o%2520with%2520Sketchpad%2520sets%2520a%250Anew%2520state%2520of%2520the%2520art%2520on%2520all%2520tasks%252C%2520including%2520V%252ABench%2520%252880.3%2525%2529%252C%2520BLINK%2520spatial%250Areasoning%2520%252883.9%2525%2529%252C%2520and%2520visual%2520correspondence%2520%252880.8%2525%2529.%2520All%2520codes%2520and%2520data%2520are%2520in%250Ahttps%253A//visualsketchpad.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Sketchpad%3A%20Sketching%20as%20a%20Visual%20Chain%20of%20Thought%20for%20Multimodal%0A%20%20Language%20Models&entry.906535625=Yushi%20Hu%20and%20Weijia%20Shi%20and%20Xingyu%20Fu%20and%20Dan%20Roth%20and%20Mari%20Ostendorf%20and%20Luke%20Zettlemoyer%20and%20Noah%20A%20Smith%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Humans%20draw%20to%20facilitate%20reasoning%3A%20we%20draw%20auxiliary%20lines%20when%20solving%0Ageometry%20problems%3B%20we%20mark%20and%20circle%20when%20reasoning%20on%20maps%3B%20we%20use%20sketches%0Ato%20amplify%20our%20ideas%20and%20relieve%20our%20limited-capacity%20working%20memory.%20However%2C%0Asuch%20actions%20are%20missing%20in%20current%20multimodal%20language%20models%20%28LMs%29.%20Current%0Achain-of-thought%20and%20tool-use%20paradigms%20only%20use%20text%20as%20intermediate%20reasoning%0Asteps.%20In%20this%20work%2C%20we%20introduce%20Sketchpad%2C%20a%20framework%20that%20gives%20multimodal%0ALMs%20a%20visual%20sketchpad%20and%20tools%20to%20draw%20on%20the%20sketchpad.%20The%20LM%20conducts%0Aplanning%20and%20reasoning%20according%20to%20the%20visual%20artifacts%20it%20has%20drawn.%0ADifferent%20from%20prior%20work%2C%20which%20uses%20text-to-image%20models%20to%20enable%20LMs%20to%0Adraw%2C%20Sketchpad%20enables%20LMs%20to%20draw%20with%20lines%2C%20boxes%2C%20marks%2C%20etc.%2C%20which%20is%0Acloser%20to%20human%20sketching%20and%20better%20facilitates%20reasoning.%20Sketchpad%20can%20also%0Ause%20specialist%20vision%20models%20during%20the%20sketching%20process%20%28e.g.%2C%20draw%20bounding%0Aboxes%20with%20object%20detection%20models%2C%20draw%20masks%20with%20segmentation%20models%29%2C%20to%0Afurther%20enhance%20visual%20perception%20and%20reasoning.%20We%20experiment%20with%20a%20wide%0Arange%20of%20math%20tasks%20%28including%20geometry%2C%20functions%2C%20graphs%2C%20and%20chess%29%20and%0Acomplex%20visual%20reasoning%20tasks.%20Sketchpad%20substantially%20improves%20performance%20on%0Aall%20tasks%20over%20strong%20base%20models%20with%20no%20sketching%2C%20yielding%20an%20average%20gain%0Aof%2012.7%25%20on%20math%20tasks%2C%20and%208.6%25%20on%20vision%20tasks.%20GPT-4o%20with%20Sketchpad%20sets%20a%0Anew%20state%20of%20the%20art%20on%20all%20tasks%2C%20including%20V%2ABench%20%2880.3%25%29%2C%20BLINK%20spatial%0Areasoning%20%2883.9%25%29%2C%20and%20visual%20correspondence%20%2880.8%25%29.%20All%20codes%20and%20data%20are%20in%0Ahttps%3A//visualsketchpad.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09403v1&entry.124074799=Read"},
{"title": "MGRQ: Post-Training Quantization For Vision Transformer With Mixed\n  Granularity Reconstruction", "author": "Lianwei Yang and Zhikai Li and Junrui Xiao and Haisong Gong and Qingyi Gu", "abstract": "  Post-training quantization (PTQ) efficiently compresses vision models, but\nunfortunately, it accompanies a certain degree of accuracy degradation.\nReconstruction methods aim to enhance model performance by narrowing the gap\nbetween the quantized model and the full-precision model, often yielding\npromising results. However, efforts to significantly improve the performance of\nPTQ through reconstruction in the Vision Transformer (ViT) have shown limited\nefficacy. In this paper, we conduct a thorough analysis of the reasons for this\nlimited effectiveness and propose MGRQ (Mixed Granularity Reconstruction\nQuantization) as a solution to address this issue. Unlike previous\nreconstruction schemes, MGRQ introduces a mixed granularity reconstruction\napproach. Specifically, MGRQ enhances the performance of PTQ by introducing\nExtra-Block Global Supervision and Intra-Block Local Supervision, building upon\nOptimized Block-wise Reconstruction. Extra-Block Global Supervision considers\nthe relationship between block outputs and the model's output, aiding\nblock-wise reconstruction through global supervision. Meanwhile, Intra-Block\nLocal Supervision reduces generalization errors by aligning the distribution of\noutputs at each layer within a block. Subsequently, MGRQ is further optimized\nfor reconstruction through Mixed Granularity Loss Fusion. Extensive experiments\nconducted on various ViT models illustrate the effectiveness of MGRQ. Notably,\nMGRQ demonstrates robust performance in low-bit quantization, thereby enhancing\nthe practicality of the quantized model.\n", "link": "http://arxiv.org/abs/2406.09229v1", "date": "2024-06-13", "relevancy": 2.1469, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5732}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5349}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGRQ%3A%20Post-Training%20Quantization%20For%20Vision%20Transformer%20With%20Mixed%0A%20%20Granularity%20Reconstruction&body=Title%3A%20MGRQ%3A%20Post-Training%20Quantization%20For%20Vision%20Transformer%20With%20Mixed%0A%20%20Granularity%20Reconstruction%0AAuthor%3A%20Lianwei%20Yang%20and%20Zhikai%20Li%20and%20Junrui%20Xiao%20and%20Haisong%20Gong%20and%20Qingyi%20Gu%0AAbstract%3A%20%20%20Post-training%20quantization%20%28PTQ%29%20efficiently%20compresses%20vision%20models%2C%20but%0Aunfortunately%2C%20it%20accompanies%20a%20certain%20degree%20of%20accuracy%20degradation.%0AReconstruction%20methods%20aim%20to%20enhance%20model%20performance%20by%20narrowing%20the%20gap%0Abetween%20the%20quantized%20model%20and%20the%20full-precision%20model%2C%20often%20yielding%0Apromising%20results.%20However%2C%20efforts%20to%20significantly%20improve%20the%20performance%20of%0APTQ%20through%20reconstruction%20in%20the%20Vision%20Transformer%20%28ViT%29%20have%20shown%20limited%0Aefficacy.%20In%20this%20paper%2C%20we%20conduct%20a%20thorough%20analysis%20of%20the%20reasons%20for%20this%0Alimited%20effectiveness%20and%20propose%20MGRQ%20%28Mixed%20Granularity%20Reconstruction%0AQuantization%29%20as%20a%20solution%20to%20address%20this%20issue.%20Unlike%20previous%0Areconstruction%20schemes%2C%20MGRQ%20introduces%20a%20mixed%20granularity%20reconstruction%0Aapproach.%20Specifically%2C%20MGRQ%20enhances%20the%20performance%20of%20PTQ%20by%20introducing%0AExtra-Block%20Global%20Supervision%20and%20Intra-Block%20Local%20Supervision%2C%20building%20upon%0AOptimized%20Block-wise%20Reconstruction.%20Extra-Block%20Global%20Supervision%20considers%0Athe%20relationship%20between%20block%20outputs%20and%20the%20model%27s%20output%2C%20aiding%0Ablock-wise%20reconstruction%20through%20global%20supervision.%20Meanwhile%2C%20Intra-Block%0ALocal%20Supervision%20reduces%20generalization%20errors%20by%20aligning%20the%20distribution%20of%0Aoutputs%20at%20each%20layer%20within%20a%20block.%20Subsequently%2C%20MGRQ%20is%20further%20optimized%0Afor%20reconstruction%20through%20Mixed%20Granularity%20Loss%20Fusion.%20Extensive%20experiments%0Aconducted%20on%20various%20ViT%20models%20illustrate%20the%20effectiveness%20of%20MGRQ.%20Notably%2C%0AMGRQ%20demonstrates%20robust%20performance%20in%20low-bit%20quantization%2C%20thereby%20enhancing%0Athe%20practicality%20of%20the%20quantized%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGRQ%253A%2520Post-Training%2520Quantization%2520For%2520Vision%2520Transformer%2520With%2520Mixed%250A%2520%2520Granularity%2520Reconstruction%26entry.906535625%3DLianwei%2520Yang%2520and%2520Zhikai%2520Li%2520and%2520Junrui%2520Xiao%2520and%2520Haisong%2520Gong%2520and%2520Qingyi%2520Gu%26entry.1292438233%3D%2520%2520Post-training%2520quantization%2520%2528PTQ%2529%2520efficiently%2520compresses%2520vision%2520models%252C%2520but%250Aunfortunately%252C%2520it%2520accompanies%2520a%2520certain%2520degree%2520of%2520accuracy%2520degradation.%250AReconstruction%2520methods%2520aim%2520to%2520enhance%2520model%2520performance%2520by%2520narrowing%2520the%2520gap%250Abetween%2520the%2520quantized%2520model%2520and%2520the%2520full-precision%2520model%252C%2520often%2520yielding%250Apromising%2520results.%2520However%252C%2520efforts%2520to%2520significantly%2520improve%2520the%2520performance%2520of%250APTQ%2520through%2520reconstruction%2520in%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%2520have%2520shown%2520limited%250Aefficacy.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520thorough%2520analysis%2520of%2520the%2520reasons%2520for%2520this%250Alimited%2520effectiveness%2520and%2520propose%2520MGRQ%2520%2528Mixed%2520Granularity%2520Reconstruction%250AQuantization%2529%2520as%2520a%2520solution%2520to%2520address%2520this%2520issue.%2520Unlike%2520previous%250Areconstruction%2520schemes%252C%2520MGRQ%2520introduces%2520a%2520mixed%2520granularity%2520reconstruction%250Aapproach.%2520Specifically%252C%2520MGRQ%2520enhances%2520the%2520performance%2520of%2520PTQ%2520by%2520introducing%250AExtra-Block%2520Global%2520Supervision%2520and%2520Intra-Block%2520Local%2520Supervision%252C%2520building%2520upon%250AOptimized%2520Block-wise%2520Reconstruction.%2520Extra-Block%2520Global%2520Supervision%2520considers%250Athe%2520relationship%2520between%2520block%2520outputs%2520and%2520the%2520model%2527s%2520output%252C%2520aiding%250Ablock-wise%2520reconstruction%2520through%2520global%2520supervision.%2520Meanwhile%252C%2520Intra-Block%250ALocal%2520Supervision%2520reduces%2520generalization%2520errors%2520by%2520aligning%2520the%2520distribution%2520of%250Aoutputs%2520at%2520each%2520layer%2520within%2520a%2520block.%2520Subsequently%252C%2520MGRQ%2520is%2520further%2520optimized%250Afor%2520reconstruction%2520through%2520Mixed%2520Granularity%2520Loss%2520Fusion.%2520Extensive%2520experiments%250Aconducted%2520on%2520various%2520ViT%2520models%2520illustrate%2520the%2520effectiveness%2520of%2520MGRQ.%2520Notably%252C%250AMGRQ%2520demonstrates%2520robust%2520performance%2520in%2520low-bit%2520quantization%252C%2520thereby%2520enhancing%250Athe%2520practicality%2520of%2520the%2520quantized%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGRQ%3A%20Post-Training%20Quantization%20For%20Vision%20Transformer%20With%20Mixed%0A%20%20Granularity%20Reconstruction&entry.906535625=Lianwei%20Yang%20and%20Zhikai%20Li%20and%20Junrui%20Xiao%20and%20Haisong%20Gong%20and%20Qingyi%20Gu&entry.1292438233=%20%20Post-training%20quantization%20%28PTQ%29%20efficiently%20compresses%20vision%20models%2C%20but%0Aunfortunately%2C%20it%20accompanies%20a%20certain%20degree%20of%20accuracy%20degradation.%0AReconstruction%20methods%20aim%20to%20enhance%20model%20performance%20by%20narrowing%20the%20gap%0Abetween%20the%20quantized%20model%20and%20the%20full-precision%20model%2C%20often%20yielding%0Apromising%20results.%20However%2C%20efforts%20to%20significantly%20improve%20the%20performance%20of%0APTQ%20through%20reconstruction%20in%20the%20Vision%20Transformer%20%28ViT%29%20have%20shown%20limited%0Aefficacy.%20In%20this%20paper%2C%20we%20conduct%20a%20thorough%20analysis%20of%20the%20reasons%20for%20this%0Alimited%20effectiveness%20and%20propose%20MGRQ%20%28Mixed%20Granularity%20Reconstruction%0AQuantization%29%20as%20a%20solution%20to%20address%20this%20issue.%20Unlike%20previous%0Areconstruction%20schemes%2C%20MGRQ%20introduces%20a%20mixed%20granularity%20reconstruction%0Aapproach.%20Specifically%2C%20MGRQ%20enhances%20the%20performance%20of%20PTQ%20by%20introducing%0AExtra-Block%20Global%20Supervision%20and%20Intra-Block%20Local%20Supervision%2C%20building%20upon%0AOptimized%20Block-wise%20Reconstruction.%20Extra-Block%20Global%20Supervision%20considers%0Athe%20relationship%20between%20block%20outputs%20and%20the%20model%27s%20output%2C%20aiding%0Ablock-wise%20reconstruction%20through%20global%20supervision.%20Meanwhile%2C%20Intra-Block%0ALocal%20Supervision%20reduces%20generalization%20errors%20by%20aligning%20the%20distribution%20of%0Aoutputs%20at%20each%20layer%20within%20a%20block.%20Subsequently%2C%20MGRQ%20is%20further%20optimized%0Afor%20reconstruction%20through%20Mixed%20Granularity%20Loss%20Fusion.%20Extensive%20experiments%0Aconducted%20on%20various%20ViT%20models%20illustrate%20the%20effectiveness%20of%20MGRQ.%20Notably%2C%0AMGRQ%20demonstrates%20robust%20performance%20in%20low-bit%20quantization%2C%20thereby%20enhancing%0Athe%20practicality%20of%20the%20quantized%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09229v1&entry.124074799=Read"},
{"title": "Comparison Visual Instruction Tuning", "author": "Wei Lin and Muhammad Jehanzeb Mirza and Sivan Doveh and Rogerio Feris and Raja Giryes and Sepp Hochreiter and Leonid Karlinsky", "abstract": "  Comparing two images in terms of Commonalities and Differences (CaD) is a\nfundamental human capability that forms the basis of advanced visual reasoning\nand interpretation. It is essential for the generation of detailed and\ncontextually relevant descriptions, performing comparative analysis, novelty\ndetection, and making informed decisions based on visual data. However,\nsurprisingly, little attention has been given to these fundamental concepts in\nthe best current mimic of human visual intelligence - Large Multimodal Models\n(LMMs). We develop and contribute a new two-phase approach CaD-VI for\ncollecting synthetic visual instructions, together with an\ninstruction-following dataset CaD-Inst containing 349K image pairs with CaD\ninstructions collected using CaD-VI. Our approach significantly improves the\nCaD spotting capabilities in LMMs, advancing the SOTA on a diverse set of\nrelated tasks by up to 17.5%. It is also complementary to existing\ndifference-only instruction datasets, allowing automatic targeted refinement of\nthose resources increasing their effectiveness for CaD tuning by up to 10%.\nAdditionally, we propose an evaluation benchmark with 7.5K open-ended QAs to\nassess the CaD understanding abilities of LMMs.\n", "link": "http://arxiv.org/abs/2406.09240v1", "date": "2024-06-13", "relevancy": 2.1465, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5368}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20Visual%20Instruction%20Tuning&body=Title%3A%20Comparison%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Wei%20Lin%20and%20Muhammad%20Jehanzeb%20Mirza%20and%20Sivan%20Doveh%20and%20Rogerio%20Feris%20and%20Raja%20Giryes%20and%20Sepp%20Hochreiter%20and%20Leonid%20Karlinsky%0AAbstract%3A%20%20%20Comparing%20two%20images%20in%20terms%20of%20Commonalities%20and%20Differences%20%28CaD%29%20is%20a%0Afundamental%20human%20capability%20that%20forms%20the%20basis%20of%20advanced%20visual%20reasoning%0Aand%20interpretation.%20It%20is%20essential%20for%20the%20generation%20of%20detailed%20and%0Acontextually%20relevant%20descriptions%2C%20performing%20comparative%20analysis%2C%20novelty%0Adetection%2C%20and%20making%20informed%20decisions%20based%20on%20visual%20data.%20However%2C%0Asurprisingly%2C%20little%20attention%20has%20been%20given%20to%20these%20fundamental%20concepts%20in%0Athe%20best%20current%20mimic%20of%20human%20visual%20intelligence%20-%20Large%20Multimodal%20Models%0A%28LMMs%29.%20We%20develop%20and%20contribute%20a%20new%20two-phase%20approach%20CaD-VI%20for%0Acollecting%20synthetic%20visual%20instructions%2C%20together%20with%20an%0Ainstruction-following%20dataset%20CaD-Inst%20containing%20349K%20image%20pairs%20with%20CaD%0Ainstructions%20collected%20using%20CaD-VI.%20Our%20approach%20significantly%20improves%20the%0ACaD%20spotting%20capabilities%20in%20LMMs%2C%20advancing%20the%20SOTA%20on%20a%20diverse%20set%20of%0Arelated%20tasks%20by%20up%20to%2017.5%25.%20It%20is%20also%20complementary%20to%20existing%0Adifference-only%20instruction%20datasets%2C%20allowing%20automatic%20targeted%20refinement%20of%0Athose%20resources%20increasing%20their%20effectiveness%20for%20CaD%20tuning%20by%20up%20to%2010%25.%0AAdditionally%2C%20we%20propose%20an%20evaluation%20benchmark%20with%207.5K%20open-ended%20QAs%20to%0Aassess%20the%20CaD%20understanding%20abilities%20of%20LMMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520Visual%2520Instruction%2520Tuning%26entry.906535625%3DWei%2520Lin%2520and%2520Muhammad%2520Jehanzeb%2520Mirza%2520and%2520Sivan%2520Doveh%2520and%2520Rogerio%2520Feris%2520and%2520Raja%2520Giryes%2520and%2520Sepp%2520Hochreiter%2520and%2520Leonid%2520Karlinsky%26entry.1292438233%3D%2520%2520Comparing%2520two%2520images%2520in%2520terms%2520of%2520Commonalities%2520and%2520Differences%2520%2528CaD%2529%2520is%2520a%250Afundamental%2520human%2520capability%2520that%2520forms%2520the%2520basis%2520of%2520advanced%2520visual%2520reasoning%250Aand%2520interpretation.%2520It%2520is%2520essential%2520for%2520the%2520generation%2520of%2520detailed%2520and%250Acontextually%2520relevant%2520descriptions%252C%2520performing%2520comparative%2520analysis%252C%2520novelty%250Adetection%252C%2520and%2520making%2520informed%2520decisions%2520based%2520on%2520visual%2520data.%2520However%252C%250Asurprisingly%252C%2520little%2520attention%2520has%2520been%2520given%2520to%2520these%2520fundamental%2520concepts%2520in%250Athe%2520best%2520current%2520mimic%2520of%2520human%2520visual%2520intelligence%2520-%2520Large%2520Multimodal%2520Models%250A%2528LMMs%2529.%2520We%2520develop%2520and%2520contribute%2520a%2520new%2520two-phase%2520approach%2520CaD-VI%2520for%250Acollecting%2520synthetic%2520visual%2520instructions%252C%2520together%2520with%2520an%250Ainstruction-following%2520dataset%2520CaD-Inst%2520containing%2520349K%2520image%2520pairs%2520with%2520CaD%250Ainstructions%2520collected%2520using%2520CaD-VI.%2520Our%2520approach%2520significantly%2520improves%2520the%250ACaD%2520spotting%2520capabilities%2520in%2520LMMs%252C%2520advancing%2520the%2520SOTA%2520on%2520a%2520diverse%2520set%2520of%250Arelated%2520tasks%2520by%2520up%2520to%252017.5%2525.%2520It%2520is%2520also%2520complementary%2520to%2520existing%250Adifference-only%2520instruction%2520datasets%252C%2520allowing%2520automatic%2520targeted%2520refinement%2520of%250Athose%2520resources%2520increasing%2520their%2520effectiveness%2520for%2520CaD%2520tuning%2520by%2520up%2520to%252010%2525.%250AAdditionally%252C%2520we%2520propose%2520an%2520evaluation%2520benchmark%2520with%25207.5K%2520open-ended%2520QAs%2520to%250Aassess%2520the%2520CaD%2520understanding%2520abilities%2520of%2520LMMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20Visual%20Instruction%20Tuning&entry.906535625=Wei%20Lin%20and%20Muhammad%20Jehanzeb%20Mirza%20and%20Sivan%20Doveh%20and%20Rogerio%20Feris%20and%20Raja%20Giryes%20and%20Sepp%20Hochreiter%20and%20Leonid%20Karlinsky&entry.1292438233=%20%20Comparing%20two%20images%20in%20terms%20of%20Commonalities%20and%20Differences%20%28CaD%29%20is%20a%0Afundamental%20human%20capability%20that%20forms%20the%20basis%20of%20advanced%20visual%20reasoning%0Aand%20interpretation.%20It%20is%20essential%20for%20the%20generation%20of%20detailed%20and%0Acontextually%20relevant%20descriptions%2C%20performing%20comparative%20analysis%2C%20novelty%0Adetection%2C%20and%20making%20informed%20decisions%20based%20on%20visual%20data.%20However%2C%0Asurprisingly%2C%20little%20attention%20has%20been%20given%20to%20these%20fundamental%20concepts%20in%0Athe%20best%20current%20mimic%20of%20human%20visual%20intelligence%20-%20Large%20Multimodal%20Models%0A%28LMMs%29.%20We%20develop%20and%20contribute%20a%20new%20two-phase%20approach%20CaD-VI%20for%0Acollecting%20synthetic%20visual%20instructions%2C%20together%20with%20an%0Ainstruction-following%20dataset%20CaD-Inst%20containing%20349K%20image%20pairs%20with%20CaD%0Ainstructions%20collected%20using%20CaD-VI.%20Our%20approach%20significantly%20improves%20the%0ACaD%20spotting%20capabilities%20in%20LMMs%2C%20advancing%20the%20SOTA%20on%20a%20diverse%20set%20of%0Arelated%20tasks%20by%20up%20to%2017.5%25.%20It%20is%20also%20complementary%20to%20existing%0Adifference-only%20instruction%20datasets%2C%20allowing%20automatic%20targeted%20refinement%20of%0Athose%20resources%20increasing%20their%20effectiveness%20for%20CaD%20tuning%20by%20up%20to%2010%25.%0AAdditionally%2C%20we%20propose%20an%20evaluation%20benchmark%20with%207.5K%20open-ended%20QAs%20to%0Aassess%20the%20CaD%20understanding%20abilities%20of%20LMMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09240v1&entry.124074799=Read"},
{"title": "AMSA-UNet: An Asymmetric Multiple Scales U-net Based on Self-attention\n  for Deblurring", "author": "Yingying Wang", "abstract": "  The traditional ingle-scale U-Net often leads to the loss of spatial\ninformation during deblurring, which affects the deblurring accracy.\nAdditionally, due to the convolutional method's limitation in capturing\nlong-range dependencies, the quality of the recovered image is degraded. To\naddress the above problems, an asymmetric multiple scales U-net based on\nself-attention (AMSA-UNet) is proposed to improve the accuracy and\ncomputational complexity. By introducing a multiple-scales U shape\narchitecture, the network can focus on blurry regions at the global level and\nbetter recover image details at the local level. In order to overcome the\nlimitations of traditional convolutional methods in capturing the long-range\ndependencies of information, a self-attention mechanism is introduced into the\ndecoder part of the backbone network, which significantly increases the model's\nreceptive field, enabling it to pay more attention to semantic information of\nthe image, thereby producing more accurate and visually pleasing deblurred\nimages. What's more, a frequency domain-based computation method was introduced\nto reduces the computation amount. The experimental results demonstrate that\nthe proposed method exhibits significant improvements in both accuracy and\nspeed compared to eight excellent methods\n", "link": "http://arxiv.org/abs/2406.09015v1", "date": "2024-06-13", "relevancy": 2.1427, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5478}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5431}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMSA-UNet%3A%20An%20Asymmetric%20Multiple%20Scales%20U-net%20Based%20on%20Self-attention%0A%20%20for%20Deblurring&body=Title%3A%20AMSA-UNet%3A%20An%20Asymmetric%20Multiple%20Scales%20U-net%20Based%20on%20Self-attention%0A%20%20for%20Deblurring%0AAuthor%3A%20Yingying%20Wang%0AAbstract%3A%20%20%20The%20traditional%20ingle-scale%20U-Net%20often%20leads%20to%20the%20loss%20of%20spatial%0Ainformation%20during%20deblurring%2C%20which%20affects%20the%20deblurring%20accracy.%0AAdditionally%2C%20due%20to%20the%20convolutional%20method%27s%20limitation%20in%20capturing%0Along-range%20dependencies%2C%20the%20quality%20of%20the%20recovered%20image%20is%20degraded.%20To%0Aaddress%20the%20above%20problems%2C%20an%20asymmetric%20multiple%20scales%20U-net%20based%20on%0Aself-attention%20%28AMSA-UNet%29%20is%20proposed%20to%20improve%20the%20accuracy%20and%0Acomputational%20complexity.%20By%20introducing%20a%20multiple-scales%20U%20shape%0Aarchitecture%2C%20the%20network%20can%20focus%20on%20blurry%20regions%20at%20the%20global%20level%20and%0Abetter%20recover%20image%20details%20at%20the%20local%20level.%20In%20order%20to%20overcome%20the%0Alimitations%20of%20traditional%20convolutional%20methods%20in%20capturing%20the%20long-range%0Adependencies%20of%20information%2C%20a%20self-attention%20mechanism%20is%20introduced%20into%20the%0Adecoder%20part%20of%20the%20backbone%20network%2C%20which%20significantly%20increases%20the%20model%27s%0Areceptive%20field%2C%20enabling%20it%20to%20pay%20more%20attention%20to%20semantic%20information%20of%0Athe%20image%2C%20thereby%20producing%20more%20accurate%20and%20visually%20pleasing%20deblurred%0Aimages.%20What%27s%20more%2C%20a%20frequency%20domain-based%20computation%20method%20was%20introduced%0Ato%20reduces%20the%20computation%20amount.%20The%20experimental%20results%20demonstrate%20that%0Athe%20proposed%20method%20exhibits%20significant%20improvements%20in%20both%20accuracy%20and%0Aspeed%20compared%20to%20eight%20excellent%20methods%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMSA-UNet%253A%2520An%2520Asymmetric%2520Multiple%2520Scales%2520U-net%2520Based%2520on%2520Self-attention%250A%2520%2520for%2520Deblurring%26entry.906535625%3DYingying%2520Wang%26entry.1292438233%3D%2520%2520The%2520traditional%2520ingle-scale%2520U-Net%2520often%2520leads%2520to%2520the%2520loss%2520of%2520spatial%250Ainformation%2520during%2520deblurring%252C%2520which%2520affects%2520the%2520deblurring%2520accracy.%250AAdditionally%252C%2520due%2520to%2520the%2520convolutional%2520method%2527s%2520limitation%2520in%2520capturing%250Along-range%2520dependencies%252C%2520the%2520quality%2520of%2520the%2520recovered%2520image%2520is%2520degraded.%2520To%250Aaddress%2520the%2520above%2520problems%252C%2520an%2520asymmetric%2520multiple%2520scales%2520U-net%2520based%2520on%250Aself-attention%2520%2528AMSA-UNet%2529%2520is%2520proposed%2520to%2520improve%2520the%2520accuracy%2520and%250Acomputational%2520complexity.%2520By%2520introducing%2520a%2520multiple-scales%2520U%2520shape%250Aarchitecture%252C%2520the%2520network%2520can%2520focus%2520on%2520blurry%2520regions%2520at%2520the%2520global%2520level%2520and%250Abetter%2520recover%2520image%2520details%2520at%2520the%2520local%2520level.%2520In%2520order%2520to%2520overcome%2520the%250Alimitations%2520of%2520traditional%2520convolutional%2520methods%2520in%2520capturing%2520the%2520long-range%250Adependencies%2520of%2520information%252C%2520a%2520self-attention%2520mechanism%2520is%2520introduced%2520into%2520the%250Adecoder%2520part%2520of%2520the%2520backbone%2520network%252C%2520which%2520significantly%2520increases%2520the%2520model%2527s%250Areceptive%2520field%252C%2520enabling%2520it%2520to%2520pay%2520more%2520attention%2520to%2520semantic%2520information%2520of%250Athe%2520image%252C%2520thereby%2520producing%2520more%2520accurate%2520and%2520visually%2520pleasing%2520deblurred%250Aimages.%2520What%2527s%2520more%252C%2520a%2520frequency%2520domain-based%2520computation%2520method%2520was%2520introduced%250Ato%2520reduces%2520the%2520computation%2520amount.%2520The%2520experimental%2520results%2520demonstrate%2520that%250Athe%2520proposed%2520method%2520exhibits%2520significant%2520improvements%2520in%2520both%2520accuracy%2520and%250Aspeed%2520compared%2520to%2520eight%2520excellent%2520methods%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMSA-UNet%3A%20An%20Asymmetric%20Multiple%20Scales%20U-net%20Based%20on%20Self-attention%0A%20%20for%20Deblurring&entry.906535625=Yingying%20Wang&entry.1292438233=%20%20The%20traditional%20ingle-scale%20U-Net%20often%20leads%20to%20the%20loss%20of%20spatial%0Ainformation%20during%20deblurring%2C%20which%20affects%20the%20deblurring%20accracy.%0AAdditionally%2C%20due%20to%20the%20convolutional%20method%27s%20limitation%20in%20capturing%0Along-range%20dependencies%2C%20the%20quality%20of%20the%20recovered%20image%20is%20degraded.%20To%0Aaddress%20the%20above%20problems%2C%20an%20asymmetric%20multiple%20scales%20U-net%20based%20on%0Aself-attention%20%28AMSA-UNet%29%20is%20proposed%20to%20improve%20the%20accuracy%20and%0Acomputational%20complexity.%20By%20introducing%20a%20multiple-scales%20U%20shape%0Aarchitecture%2C%20the%20network%20can%20focus%20on%20blurry%20regions%20at%20the%20global%20level%20and%0Abetter%20recover%20image%20details%20at%20the%20local%20level.%20In%20order%20to%20overcome%20the%0Alimitations%20of%20traditional%20convolutional%20methods%20in%20capturing%20the%20long-range%0Adependencies%20of%20information%2C%20a%20self-attention%20mechanism%20is%20introduced%20into%20the%0Adecoder%20part%20of%20the%20backbone%20network%2C%20which%20significantly%20increases%20the%20model%27s%0Areceptive%20field%2C%20enabling%20it%20to%20pay%20more%20attention%20to%20semantic%20information%20of%0Athe%20image%2C%20thereby%20producing%20more%20accurate%20and%20visually%20pleasing%20deblurred%0Aimages.%20What%27s%20more%2C%20a%20frequency%20domain-based%20computation%20method%20was%20introduced%0Ato%20reduces%20the%20computation%20amount.%20The%20experimental%20results%20demonstrate%20that%0Athe%20proposed%20method%20exhibits%20significant%20improvements%20in%20both%20accuracy%20and%0Aspeed%20compared%20to%20eight%20excellent%20methods%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09015v1&entry.124074799=Read"},
{"title": "Z-GMOT: Zero-shot Generic Multiple Object Tracking", "author": "Kim Hoang Tran and Anh Duy Le Dinh and Tien Phat Nguyen and Thinh Phan and Pha Nguyen and Khoa Luu and Donald Adjeroh and Gianfranco Doretto and Ngan Hoang Le", "abstract": "  Despite recent significant progress, Multi-Object Tracking (MOT) faces\nlimitations such as reliance on prior knowledge and predefined categories and\nstruggles with unseen objects. To address these issues, Generic Multiple Object\nTracking (GMOT) has emerged as an alternative approach, requiring less prior\ninformation. However, current GMOT methods often rely on initial bounding boxes\nand struggle to handle variations in factors such as viewpoint, lighting,\nocclusion, and scale, among others. Our contributions commence with the\nintroduction of the \\textit{Referring GMOT dataset} a collection of videos,\neach accompanied by detailed textual descriptions of their attributes.\nSubsequently, we propose $\\mathtt{Z-GMOT}$, a cutting-edge tracking solution\ncapable of tracking objects from \\textit{never-seen categories} without the\nneed of initial bounding boxes or predefined categories. Within our\n$\\mathtt{Z-GMOT}$ framework, we introduce two novel components: (i)\n$\\mathtt{iGLIP}$, an improved Grounded language-image pretraining, for\naccurately detecting unseen objects with specific characteristics. (ii)\n$\\mathtt{MA-SORT}$, a novel object association approach that adeptly integrates\nmotion and appearance-based matching strategies to tackle the complex task of\ntracking objects with high similarity. Our contributions are benchmarked\nthrough extensive experiments conducted on the Referring GMOT dataset for GMOT\ntask. Additionally, to assess the generalizability of the proposed\n$\\mathtt{Z-GMOT}$, we conduct ablation studies on the DanceTrack and MOT20\ndatasets for the MOT task. Our dataset, code, and models are released at:\nhttps://fsoft-aic.github.io/Z-GMOT.\n", "link": "http://arxiv.org/abs/2305.17648v4", "date": "2024-06-13", "relevancy": 2.1394, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5302}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Z-GMOT%3A%20Zero-shot%20Generic%20Multiple%20Object%20Tracking&body=Title%3A%20Z-GMOT%3A%20Zero-shot%20Generic%20Multiple%20Object%20Tracking%0AAuthor%3A%20Kim%20Hoang%20Tran%20and%20Anh%20Duy%20Le%20Dinh%20and%20Tien%20Phat%20Nguyen%20and%20Thinh%20Phan%20and%20Pha%20Nguyen%20and%20Khoa%20Luu%20and%20Donald%20Adjeroh%20and%20Gianfranco%20Doretto%20and%20Ngan%20Hoang%20Le%0AAbstract%3A%20%20%20Despite%20recent%20significant%20progress%2C%20Multi-Object%20Tracking%20%28MOT%29%20faces%0Alimitations%20such%20as%20reliance%20on%20prior%20knowledge%20and%20predefined%20categories%20and%0Astruggles%20with%20unseen%20objects.%20To%20address%20these%20issues%2C%20Generic%20Multiple%20Object%0ATracking%20%28GMOT%29%20has%20emerged%20as%20an%20alternative%20approach%2C%20requiring%20less%20prior%0Ainformation.%20However%2C%20current%20GMOT%20methods%20often%20rely%20on%20initial%20bounding%20boxes%0Aand%20struggle%20to%20handle%20variations%20in%20factors%20such%20as%20viewpoint%2C%20lighting%2C%0Aocclusion%2C%20and%20scale%2C%20among%20others.%20Our%20contributions%20commence%20with%20the%0Aintroduction%20of%20the%20%5Ctextit%7BReferring%20GMOT%20dataset%7D%20a%20collection%20of%20videos%2C%0Aeach%20accompanied%20by%20detailed%20textual%20descriptions%20of%20their%20attributes.%0ASubsequently%2C%20we%20propose%20%24%5Cmathtt%7BZ-GMOT%7D%24%2C%20a%20cutting-edge%20tracking%20solution%0Acapable%20of%20tracking%20objects%20from%20%5Ctextit%7Bnever-seen%20categories%7D%20without%20the%0Aneed%20of%20initial%20bounding%20boxes%20or%20predefined%20categories.%20Within%20our%0A%24%5Cmathtt%7BZ-GMOT%7D%24%20framework%2C%20we%20introduce%20two%20novel%20components%3A%20%28i%29%0A%24%5Cmathtt%7BiGLIP%7D%24%2C%20an%20improved%20Grounded%20language-image%20pretraining%2C%20for%0Aaccurately%20detecting%20unseen%20objects%20with%20specific%20characteristics.%20%28ii%29%0A%24%5Cmathtt%7BMA-SORT%7D%24%2C%20a%20novel%20object%20association%20approach%20that%20adeptly%20integrates%0Amotion%20and%20appearance-based%20matching%20strategies%20to%20tackle%20the%20complex%20task%20of%0Atracking%20objects%20with%20high%20similarity.%20Our%20contributions%20are%20benchmarked%0Athrough%20extensive%20experiments%20conducted%20on%20the%20Referring%20GMOT%20dataset%20for%20GMOT%0Atask.%20Additionally%2C%20to%20assess%20the%20generalizability%20of%20the%20proposed%0A%24%5Cmathtt%7BZ-GMOT%7D%24%2C%20we%20conduct%20ablation%20studies%20on%20the%20DanceTrack%20and%20MOT20%0Adatasets%20for%20the%20MOT%20task.%20Our%20dataset%2C%20code%2C%20and%20models%20are%20released%20at%3A%0Ahttps%3A//fsoft-aic.github.io/Z-GMOT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17648v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZ-GMOT%253A%2520Zero-shot%2520Generic%2520Multiple%2520Object%2520Tracking%26entry.906535625%3DKim%2520Hoang%2520Tran%2520and%2520Anh%2520Duy%2520Le%2520Dinh%2520and%2520Tien%2520Phat%2520Nguyen%2520and%2520Thinh%2520Phan%2520and%2520Pha%2520Nguyen%2520and%2520Khoa%2520Luu%2520and%2520Donald%2520Adjeroh%2520and%2520Gianfranco%2520Doretto%2520and%2520Ngan%2520Hoang%2520Le%26entry.1292438233%3D%2520%2520Despite%2520recent%2520significant%2520progress%252C%2520Multi-Object%2520Tracking%2520%2528MOT%2529%2520faces%250Alimitations%2520such%2520as%2520reliance%2520on%2520prior%2520knowledge%2520and%2520predefined%2520categories%2520and%250Astruggles%2520with%2520unseen%2520objects.%2520To%2520address%2520these%2520issues%252C%2520Generic%2520Multiple%2520Object%250ATracking%2520%2528GMOT%2529%2520has%2520emerged%2520as%2520an%2520alternative%2520approach%252C%2520requiring%2520less%2520prior%250Ainformation.%2520However%252C%2520current%2520GMOT%2520methods%2520often%2520rely%2520on%2520initial%2520bounding%2520boxes%250Aand%2520struggle%2520to%2520handle%2520variations%2520in%2520factors%2520such%2520as%2520viewpoint%252C%2520lighting%252C%250Aocclusion%252C%2520and%2520scale%252C%2520among%2520others.%2520Our%2520contributions%2520commence%2520with%2520the%250Aintroduction%2520of%2520the%2520%255Ctextit%257BReferring%2520GMOT%2520dataset%257D%2520a%2520collection%2520of%2520videos%252C%250Aeach%2520accompanied%2520by%2520detailed%2520textual%2520descriptions%2520of%2520their%2520attributes.%250ASubsequently%252C%2520we%2520propose%2520%2524%255Cmathtt%257BZ-GMOT%257D%2524%252C%2520a%2520cutting-edge%2520tracking%2520solution%250Acapable%2520of%2520tracking%2520objects%2520from%2520%255Ctextit%257Bnever-seen%2520categories%257D%2520without%2520the%250Aneed%2520of%2520initial%2520bounding%2520boxes%2520or%2520predefined%2520categories.%2520Within%2520our%250A%2524%255Cmathtt%257BZ-GMOT%257D%2524%2520framework%252C%2520we%2520introduce%2520two%2520novel%2520components%253A%2520%2528i%2529%250A%2524%255Cmathtt%257BiGLIP%257D%2524%252C%2520an%2520improved%2520Grounded%2520language-image%2520pretraining%252C%2520for%250Aaccurately%2520detecting%2520unseen%2520objects%2520with%2520specific%2520characteristics.%2520%2528ii%2529%250A%2524%255Cmathtt%257BMA-SORT%257D%2524%252C%2520a%2520novel%2520object%2520association%2520approach%2520that%2520adeptly%2520integrates%250Amotion%2520and%2520appearance-based%2520matching%2520strategies%2520to%2520tackle%2520the%2520complex%2520task%2520of%250Atracking%2520objects%2520with%2520high%2520similarity.%2520Our%2520contributions%2520are%2520benchmarked%250Athrough%2520extensive%2520experiments%2520conducted%2520on%2520the%2520Referring%2520GMOT%2520dataset%2520for%2520GMOT%250Atask.%2520Additionally%252C%2520to%2520assess%2520the%2520generalizability%2520of%2520the%2520proposed%250A%2524%255Cmathtt%257BZ-GMOT%257D%2524%252C%2520we%2520conduct%2520ablation%2520studies%2520on%2520the%2520DanceTrack%2520and%2520MOT20%250Adatasets%2520for%2520the%2520MOT%2520task.%2520Our%2520dataset%252C%2520code%252C%2520and%2520models%2520are%2520released%2520at%253A%250Ahttps%253A//fsoft-aic.github.io/Z-GMOT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.17648v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Z-GMOT%3A%20Zero-shot%20Generic%20Multiple%20Object%20Tracking&entry.906535625=Kim%20Hoang%20Tran%20and%20Anh%20Duy%20Le%20Dinh%20and%20Tien%20Phat%20Nguyen%20and%20Thinh%20Phan%20and%20Pha%20Nguyen%20and%20Khoa%20Luu%20and%20Donald%20Adjeroh%20and%20Gianfranco%20Doretto%20and%20Ngan%20Hoang%20Le&entry.1292438233=%20%20Despite%20recent%20significant%20progress%2C%20Multi-Object%20Tracking%20%28MOT%29%20faces%0Alimitations%20such%20as%20reliance%20on%20prior%20knowledge%20and%20predefined%20categories%20and%0Astruggles%20with%20unseen%20objects.%20To%20address%20these%20issues%2C%20Generic%20Multiple%20Object%0ATracking%20%28GMOT%29%20has%20emerged%20as%20an%20alternative%20approach%2C%20requiring%20less%20prior%0Ainformation.%20However%2C%20current%20GMOT%20methods%20often%20rely%20on%20initial%20bounding%20boxes%0Aand%20struggle%20to%20handle%20variations%20in%20factors%20such%20as%20viewpoint%2C%20lighting%2C%0Aocclusion%2C%20and%20scale%2C%20among%20others.%20Our%20contributions%20commence%20with%20the%0Aintroduction%20of%20the%20%5Ctextit%7BReferring%20GMOT%20dataset%7D%20a%20collection%20of%20videos%2C%0Aeach%20accompanied%20by%20detailed%20textual%20descriptions%20of%20their%20attributes.%0ASubsequently%2C%20we%20propose%20%24%5Cmathtt%7BZ-GMOT%7D%24%2C%20a%20cutting-edge%20tracking%20solution%0Acapable%20of%20tracking%20objects%20from%20%5Ctextit%7Bnever-seen%20categories%7D%20without%20the%0Aneed%20of%20initial%20bounding%20boxes%20or%20predefined%20categories.%20Within%20our%0A%24%5Cmathtt%7BZ-GMOT%7D%24%20framework%2C%20we%20introduce%20two%20novel%20components%3A%20%28i%29%0A%24%5Cmathtt%7BiGLIP%7D%24%2C%20an%20improved%20Grounded%20language-image%20pretraining%2C%20for%0Aaccurately%20detecting%20unseen%20objects%20with%20specific%20characteristics.%20%28ii%29%0A%24%5Cmathtt%7BMA-SORT%7D%24%2C%20a%20novel%20object%20association%20approach%20that%20adeptly%20integrates%0Amotion%20and%20appearance-based%20matching%20strategies%20to%20tackle%20the%20complex%20task%20of%0Atracking%20objects%20with%20high%20similarity.%20Our%20contributions%20are%20benchmarked%0Athrough%20extensive%20experiments%20conducted%20on%20the%20Referring%20GMOT%20dataset%20for%20GMOT%0Atask.%20Additionally%2C%20to%20assess%20the%20generalizability%20of%20the%20proposed%0A%24%5Cmathtt%7BZ-GMOT%7D%24%2C%20we%20conduct%20ablation%20studies%20on%20the%20DanceTrack%20and%20MOT20%0Adatasets%20for%20the%20MOT%20task.%20Our%20dataset%2C%20code%2C%20and%20models%20are%20released%20at%3A%0Ahttps%3A//fsoft-aic.github.io/Z-GMOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17648v4&entry.124074799=Read"},
{"title": "Getting More for Less: Using Weak Labels and AV-Mixup for Robust\n  Audio-Visual Speaker Verification", "author": "Anith Selvakumar and Homa Fashandi", "abstract": "  Distance Metric Learning (DML) has typically dominated the audio-visual\nspeaker verification problem space, owing to strong performance in new and\nunseen classes. In our work, we explored multitask learning techniques to\nfurther enhance DML, and show that an auxiliary task with even weak labels can\nincrease the quality of the learned speaker representation without increasing\nmodel complexity during inference. We also extend the Generalized End-to-End\nLoss (GE2E) to multimodal inputs and demonstrate that it can achieve\ncompetitive performance in an audio-visual space. Finally, we introduce\nAV-Mixup, a multimodal augmentation technique during training time that has\nshown to reduce speaker overfit. Our network achieves state of the art\nperformance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal\nError Rate (EER) on the VoxCeleb1-O/E/H test sets, which is to our knowledge,\nthe best published results on VoxCeleb1-E and VoxCeleb1-H.\n", "link": "http://arxiv.org/abs/2309.07115v2", "date": "2024-06-13", "relevancy": 2.1346, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.567}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5422}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Getting%20More%20for%20Less%3A%20Using%20Weak%20Labels%20and%20AV-Mixup%20for%20Robust%0A%20%20Audio-Visual%20Speaker%20Verification&body=Title%3A%20Getting%20More%20for%20Less%3A%20Using%20Weak%20Labels%20and%20AV-Mixup%20for%20Robust%0A%20%20Audio-Visual%20Speaker%20Verification%0AAuthor%3A%20Anith%20Selvakumar%20and%20Homa%20Fashandi%0AAbstract%3A%20%20%20Distance%20Metric%20Learning%20%28DML%29%20has%20typically%20dominated%20the%20audio-visual%0Aspeaker%20verification%20problem%20space%2C%20owing%20to%20strong%20performance%20in%20new%20and%0Aunseen%20classes.%20In%20our%20work%2C%20we%20explored%20multitask%20learning%20techniques%20to%0Afurther%20enhance%20DML%2C%20and%20show%20that%20an%20auxiliary%20task%20with%20even%20weak%20labels%20can%0Aincrease%20the%20quality%20of%20the%20learned%20speaker%20representation%20without%20increasing%0Amodel%20complexity%20during%20inference.%20We%20also%20extend%20the%20Generalized%20End-to-End%0ALoss%20%28GE2E%29%20to%20multimodal%20inputs%20and%20demonstrate%20that%20it%20can%20achieve%0Acompetitive%20performance%20in%20an%20audio-visual%20space.%20Finally%2C%20we%20introduce%0AAV-Mixup%2C%20a%20multimodal%20augmentation%20technique%20during%20training%20time%20that%20has%0Ashown%20to%20reduce%20speaker%20overfit.%20Our%20network%20achieves%20state%20of%20the%20art%0Aperformance%20for%20speaker%20verification%2C%20reporting%200.244%25%2C%200.252%25%2C%200.441%25%20Equal%0AError%20Rate%20%28EER%29%20on%20the%20VoxCeleb1-O/E/H%20test%20sets%2C%20which%20is%20to%20our%20knowledge%2C%0Athe%20best%20published%20results%20on%20VoxCeleb1-E%20and%20VoxCeleb1-H.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGetting%2520More%2520for%2520Less%253A%2520Using%2520Weak%2520Labels%2520and%2520AV-Mixup%2520for%2520Robust%250A%2520%2520Audio-Visual%2520Speaker%2520Verification%26entry.906535625%3DAnith%2520Selvakumar%2520and%2520Homa%2520Fashandi%26entry.1292438233%3D%2520%2520Distance%2520Metric%2520Learning%2520%2528DML%2529%2520has%2520typically%2520dominated%2520the%2520audio-visual%250Aspeaker%2520verification%2520problem%2520space%252C%2520owing%2520to%2520strong%2520performance%2520in%2520new%2520and%250Aunseen%2520classes.%2520In%2520our%2520work%252C%2520we%2520explored%2520multitask%2520learning%2520techniques%2520to%250Afurther%2520enhance%2520DML%252C%2520and%2520show%2520that%2520an%2520auxiliary%2520task%2520with%2520even%2520weak%2520labels%2520can%250Aincrease%2520the%2520quality%2520of%2520the%2520learned%2520speaker%2520representation%2520without%2520increasing%250Amodel%2520complexity%2520during%2520inference.%2520We%2520also%2520extend%2520the%2520Generalized%2520End-to-End%250ALoss%2520%2528GE2E%2529%2520to%2520multimodal%2520inputs%2520and%2520demonstrate%2520that%2520it%2520can%2520achieve%250Acompetitive%2520performance%2520in%2520an%2520audio-visual%2520space.%2520Finally%252C%2520we%2520introduce%250AAV-Mixup%252C%2520a%2520multimodal%2520augmentation%2520technique%2520during%2520training%2520time%2520that%2520has%250Ashown%2520to%2520reduce%2520speaker%2520overfit.%2520Our%2520network%2520achieves%2520state%2520of%2520the%2520art%250Aperformance%2520for%2520speaker%2520verification%252C%2520reporting%25200.244%2525%252C%25200.252%2525%252C%25200.441%2525%2520Equal%250AError%2520Rate%2520%2528EER%2529%2520on%2520the%2520VoxCeleb1-O/E/H%2520test%2520sets%252C%2520which%2520is%2520to%2520our%2520knowledge%252C%250Athe%2520best%2520published%2520results%2520on%2520VoxCeleb1-E%2520and%2520VoxCeleb1-H.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.07115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Getting%20More%20for%20Less%3A%20Using%20Weak%20Labels%20and%20AV-Mixup%20for%20Robust%0A%20%20Audio-Visual%20Speaker%20Verification&entry.906535625=Anith%20Selvakumar%20and%20Homa%20Fashandi&entry.1292438233=%20%20Distance%20Metric%20Learning%20%28DML%29%20has%20typically%20dominated%20the%20audio-visual%0Aspeaker%20verification%20problem%20space%2C%20owing%20to%20strong%20performance%20in%20new%20and%0Aunseen%20classes.%20In%20our%20work%2C%20we%20explored%20multitask%20learning%20techniques%20to%0Afurther%20enhance%20DML%2C%20and%20show%20that%20an%20auxiliary%20task%20with%20even%20weak%20labels%20can%0Aincrease%20the%20quality%20of%20the%20learned%20speaker%20representation%20without%20increasing%0Amodel%20complexity%20during%20inference.%20We%20also%20extend%20the%20Generalized%20End-to-End%0ALoss%20%28GE2E%29%20to%20multimodal%20inputs%20and%20demonstrate%20that%20it%20can%20achieve%0Acompetitive%20performance%20in%20an%20audio-visual%20space.%20Finally%2C%20we%20introduce%0AAV-Mixup%2C%20a%20multimodal%20augmentation%20technique%20during%20training%20time%20that%20has%0Ashown%20to%20reduce%20speaker%20overfit.%20Our%20network%20achieves%20state%20of%20the%20art%0Aperformance%20for%20speaker%20verification%2C%20reporting%200.244%25%2C%200.252%25%2C%200.441%25%20Equal%0AError%20Rate%20%28EER%29%20on%20the%20VoxCeleb1-O/E/H%20test%20sets%2C%20which%20is%20to%20our%20knowledge%2C%0Athe%20best%20published%20results%20on%20VoxCeleb1-E%20and%20VoxCeleb1-H.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07115v2&entry.124074799=Read"},
{"title": "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models", "author": "Samar Fares and Klea Ziu and Toluwani Aremu and Nikita Durasov and Martin Tak\u00e1\u010d and Pascal Fua and Karthik Nandakumar and Ivan Laptev", "abstract": "  Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.\n", "link": "http://arxiv.org/abs/2406.09250v1", "date": "2024-06-13", "relevancy": 2.1343, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5245}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MirrorCheck%3A%20Efficient%20Adversarial%20Defense%20for%20Vision-Language%20Models&body=Title%3A%20MirrorCheck%3A%20Efficient%20Adversarial%20Defense%20for%20Vision-Language%20Models%0AAuthor%3A%20Samar%20Fares%20and%20Klea%20Ziu%20and%20Toluwani%20Aremu%20and%20Nikita%20Durasov%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Pascal%20Fua%20and%20Karthik%20Nandakumar%20and%20Ivan%20Laptev%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20are%20becoming%20increasingly%20vulnerable%20to%0Aadversarial%20attacks%20as%20various%20novel%20attack%20strategies%20are%20being%20proposed%0Aagainst%20these%20models.%20While%20existing%20defenses%20excel%20in%20unimodal%20contexts%2C%20they%0Acurrently%20fall%20short%20in%20safeguarding%20VLMs%20against%20adversarial%20threats.%20To%0Amitigate%20this%20vulnerability%2C%20we%20propose%20a%20novel%2C%20yet%20elegantly%20simple%20approach%0Afor%20detecting%20adversarial%20samples%20in%20VLMs.%20Our%20method%20leverages%20Text-to-Image%0A%28T2I%29%20models%20to%20generate%20images%20based%20on%20captions%20produced%20by%20target%20VLMs.%0ASubsequently%2C%20we%20calculate%20the%20similarities%20of%20the%20embeddings%20of%20both%20input%20and%0Agenerated%20images%20in%20the%20feature%20space%20to%20identify%20adversarial%20samples.%0AEmpirical%20evaluations%20conducted%20on%20different%20datasets%20validate%20the%20efficacy%20of%0Aour%20approach%2C%20outperforming%20baseline%20methods%20adapted%20from%20image%20classification%0Adomains.%20Furthermore%2C%20we%20extend%20our%20methodology%20to%20classification%20tasks%2C%0Ashowcasing%20its%20adaptability%20and%20model-agnostic%20nature.%20Theoretical%20analyses%20and%0Aempirical%20findings%20also%20show%20the%20resilience%20of%20our%20approach%20against%20adaptive%0Aattacks%2C%20positioning%20it%20as%20an%20excellent%20defense%20mechanism%20for%20real-world%0Adeployment%20against%20adversarial%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMirrorCheck%253A%2520Efficient%2520Adversarial%2520Defense%2520for%2520Vision-Language%2520Models%26entry.906535625%3DSamar%2520Fares%2520and%2520Klea%2520Ziu%2520and%2520Toluwani%2520Aremu%2520and%2520Nikita%2520Durasov%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%2520and%2520Pascal%2520Fua%2520and%2520Karthik%2520Nandakumar%2520and%2520Ivan%2520Laptev%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%2520becoming%2520increasingly%2520vulnerable%2520to%250Aadversarial%2520attacks%2520as%2520various%2520novel%2520attack%2520strategies%2520are%2520being%2520proposed%250Aagainst%2520these%2520models.%2520While%2520existing%2520defenses%2520excel%2520in%2520unimodal%2520contexts%252C%2520they%250Acurrently%2520fall%2520short%2520in%2520safeguarding%2520VLMs%2520against%2520adversarial%2520threats.%2520To%250Amitigate%2520this%2520vulnerability%252C%2520we%2520propose%2520a%2520novel%252C%2520yet%2520elegantly%2520simple%2520approach%250Afor%2520detecting%2520adversarial%2520samples%2520in%2520VLMs.%2520Our%2520method%2520leverages%2520Text-to-Image%250A%2528T2I%2529%2520models%2520to%2520generate%2520images%2520based%2520on%2520captions%2520produced%2520by%2520target%2520VLMs.%250ASubsequently%252C%2520we%2520calculate%2520the%2520similarities%2520of%2520the%2520embeddings%2520of%2520both%2520input%2520and%250Agenerated%2520images%2520in%2520the%2520feature%2520space%2520to%2520identify%2520adversarial%2520samples.%250AEmpirical%2520evaluations%2520conducted%2520on%2520different%2520datasets%2520validate%2520the%2520efficacy%2520of%250Aour%2520approach%252C%2520outperforming%2520baseline%2520methods%2520adapted%2520from%2520image%2520classification%250Adomains.%2520Furthermore%252C%2520we%2520extend%2520our%2520methodology%2520to%2520classification%2520tasks%252C%250Ashowcasing%2520its%2520adaptability%2520and%2520model-agnostic%2520nature.%2520Theoretical%2520analyses%2520and%250Aempirical%2520findings%2520also%2520show%2520the%2520resilience%2520of%2520our%2520approach%2520against%2520adaptive%250Aattacks%252C%2520positioning%2520it%2520as%2520an%2520excellent%2520defense%2520mechanism%2520for%2520real-world%250Adeployment%2520against%2520adversarial%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MirrorCheck%3A%20Efficient%20Adversarial%20Defense%20for%20Vision-Language%20Models&entry.906535625=Samar%20Fares%20and%20Klea%20Ziu%20and%20Toluwani%20Aremu%20and%20Nikita%20Durasov%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Pascal%20Fua%20and%20Karthik%20Nandakumar%20and%20Ivan%20Laptev&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20are%20becoming%20increasingly%20vulnerable%20to%0Aadversarial%20attacks%20as%20various%20novel%20attack%20strategies%20are%20being%20proposed%0Aagainst%20these%20models.%20While%20existing%20defenses%20excel%20in%20unimodal%20contexts%2C%20they%0Acurrently%20fall%20short%20in%20safeguarding%20VLMs%20against%20adversarial%20threats.%20To%0Amitigate%20this%20vulnerability%2C%20we%20propose%20a%20novel%2C%20yet%20elegantly%20simple%20approach%0Afor%20detecting%20adversarial%20samples%20in%20VLMs.%20Our%20method%20leverages%20Text-to-Image%0A%28T2I%29%20models%20to%20generate%20images%20based%20on%20captions%20produced%20by%20target%20VLMs.%0ASubsequently%2C%20we%20calculate%20the%20similarities%20of%20the%20embeddings%20of%20both%20input%20and%0Agenerated%20images%20in%20the%20feature%20space%20to%20identify%20adversarial%20samples.%0AEmpirical%20evaluations%20conducted%20on%20different%20datasets%20validate%20the%20efficacy%20of%0Aour%20approach%2C%20outperforming%20baseline%20methods%20adapted%20from%20image%20classification%0Adomains.%20Furthermore%2C%20we%20extend%20our%20methodology%20to%20classification%20tasks%2C%0Ashowcasing%20its%20adaptability%20and%20model-agnostic%20nature.%20Theoretical%20analyses%20and%0Aempirical%20findings%20also%20show%20the%20resilience%20of%20our%20approach%20against%20adaptive%0Aattacks%2C%20positioning%20it%20as%20an%20excellent%20defense%20mechanism%20for%20real-world%0Adeployment%20against%20adversarial%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09250v1&entry.124074799=Read"},
{"title": "Advanced Feature Manipulation for Enhanced Change Detection Leveraging\n  Natural Language Models", "author": "Zhenglin Li and Yangchen Huang and Mengran Zhu and Jingyu Zhang and JingHao Chang and Houze Liu", "abstract": "  Change detection is a fundamental task in computer vision that processes a\nbi-temporal image pair to differentiate between semantically altered and\nunaltered regions. Large language models (LLMs) have been utilized in various\ndomains for their exceptional feature extraction capabilities and have shown\npromise in numerous downstream applications. In this study, we harness the\npower of a pre-trained LLM, extracting feature maps from extensive datasets,\nand employ an auxiliary network to detect changes. Unlike existing LLM-based\nchange detection methods that solely focus on deriving high-quality feature\nmaps, our approach emphasizes the manipulation of these feature maps to enhance\nsemantic relevance.\n", "link": "http://arxiv.org/abs/2403.15943v2", "date": "2024-06-13", "relevancy": 2.1295, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5437}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Feature%20Manipulation%20for%20Enhanced%20Change%20Detection%20Leveraging%0A%20%20Natural%20Language%20Models&body=Title%3A%20Advanced%20Feature%20Manipulation%20for%20Enhanced%20Change%20Detection%20Leveraging%0A%20%20Natural%20Language%20Models%0AAuthor%3A%20Zhenglin%20Li%20and%20Yangchen%20Huang%20and%20Mengran%20Zhu%20and%20Jingyu%20Zhang%20and%20JingHao%20Chang%20and%20Houze%20Liu%0AAbstract%3A%20%20%20Change%20detection%20is%20a%20fundamental%20task%20in%20computer%20vision%20that%20processes%20a%0Abi-temporal%20image%20pair%20to%20differentiate%20between%20semantically%20altered%20and%0Aunaltered%20regions.%20Large%20language%20models%20%28LLMs%29%20have%20been%20utilized%20in%20various%0Adomains%20for%20their%20exceptional%20feature%20extraction%20capabilities%20and%20have%20shown%0Apromise%20in%20numerous%20downstream%20applications.%20In%20this%20study%2C%20we%20harness%20the%0Apower%20of%20a%20pre-trained%20LLM%2C%20extracting%20feature%20maps%20from%20extensive%20datasets%2C%0Aand%20employ%20an%20auxiliary%20network%20to%20detect%20changes.%20Unlike%20existing%20LLM-based%0Achange%20detection%20methods%20that%20solely%20focus%20on%20deriving%20high-quality%20feature%0Amaps%2C%20our%20approach%20emphasizes%20the%20manipulation%20of%20these%20feature%20maps%20to%20enhance%0Asemantic%20relevance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15943v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Feature%2520Manipulation%2520for%2520Enhanced%2520Change%2520Detection%2520Leveraging%250A%2520%2520Natural%2520Language%2520Models%26entry.906535625%3DZhenglin%2520Li%2520and%2520Yangchen%2520Huang%2520and%2520Mengran%2520Zhu%2520and%2520Jingyu%2520Zhang%2520and%2520JingHao%2520Chang%2520and%2520Houze%2520Liu%26entry.1292438233%3D%2520%2520Change%2520detection%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision%2520that%2520processes%2520a%250Abi-temporal%2520image%2520pair%2520to%2520differentiate%2520between%2520semantically%2520altered%2520and%250Aunaltered%2520regions.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520utilized%2520in%2520various%250Adomains%2520for%2520their%2520exceptional%2520feature%2520extraction%2520capabilities%2520and%2520have%2520shown%250Apromise%2520in%2520numerous%2520downstream%2520applications.%2520In%2520this%2520study%252C%2520we%2520harness%2520the%250Apower%2520of%2520a%2520pre-trained%2520LLM%252C%2520extracting%2520feature%2520maps%2520from%2520extensive%2520datasets%252C%250Aand%2520employ%2520an%2520auxiliary%2520network%2520to%2520detect%2520changes.%2520Unlike%2520existing%2520LLM-based%250Achange%2520detection%2520methods%2520that%2520solely%2520focus%2520on%2520deriving%2520high-quality%2520feature%250Amaps%252C%2520our%2520approach%2520emphasizes%2520the%2520manipulation%2520of%2520these%2520feature%2520maps%2520to%2520enhance%250Asemantic%2520relevance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15943v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Feature%20Manipulation%20for%20Enhanced%20Change%20Detection%20Leveraging%0A%20%20Natural%20Language%20Models&entry.906535625=Zhenglin%20Li%20and%20Yangchen%20Huang%20and%20Mengran%20Zhu%20and%20Jingyu%20Zhang%20and%20JingHao%20Chang%20and%20Houze%20Liu&entry.1292438233=%20%20Change%20detection%20is%20a%20fundamental%20task%20in%20computer%20vision%20that%20processes%20a%0Abi-temporal%20image%20pair%20to%20differentiate%20between%20semantically%20altered%20and%0Aunaltered%20regions.%20Large%20language%20models%20%28LLMs%29%20have%20been%20utilized%20in%20various%0Adomains%20for%20their%20exceptional%20feature%20extraction%20capabilities%20and%20have%20shown%0Apromise%20in%20numerous%20downstream%20applications.%20In%20this%20study%2C%20we%20harness%20the%0Apower%20of%20a%20pre-trained%20LLM%2C%20extracting%20feature%20maps%20from%20extensive%20datasets%2C%0Aand%20employ%20an%20auxiliary%20network%20to%20detect%20changes.%20Unlike%20existing%20LLM-based%0Achange%20detection%20methods%20that%20solely%20focus%20on%20deriving%20high-quality%20feature%0Amaps%2C%20our%20approach%20emphasizes%20the%20manipulation%20of%20these%20feature%20maps%20to%20enhance%0Asemantic%20relevance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15943v2&entry.124074799=Read"},
{"title": "Steganalysis on Digital Watermarking: Is Your Defense Truly Impervious?", "author": "Pei Yang and Hai Ci and Yiren Song and Mike Zheng Shou", "abstract": "  Digital watermarking techniques are crucial for copyright protection and\nsource identification of images, especially in the era of generative AI models.\nHowever, many existing watermarking methods, particularly content-agnostic\napproaches that embed fixed patterns regardless of image content, are\nvulnerable to steganalysis attacks that can extract and remove the watermark\nwith minimal perceptual distortion. In this work, we categorize watermarking\nalgorithms into content-adaptive and content-agnostic ones, and demonstrate how\naveraging a collection of watermarked images could reveal the underlying\nwatermark pattern. We then leverage this extracted pattern for effective\nwatermark removal under both graybox and blackbox settings, even when the\ncollection contains multiple watermark patterns. For some algorithms like\nTree-Ring watermarks, the extracted pattern can also forge convincing\nwatermarks on clean images. Our quantitative and qualitative evaluations across\ntwelve watermarking methods highlight the threat posed by steganalysis to\ncontent-agnostic watermarks and the importance of designing watermarking\ntechniques resilient to such analytical attacks. We propose security guidelines\ncalling for using content-adaptive watermarking strategies and performing\nsecurity evaluation against steganalysis. We also suggest multi-key assignments\nas potential mitigations against steganalysis vulnerabilities.\n", "link": "http://arxiv.org/abs/2406.09026v1", "date": "2024-06-13", "relevancy": 2.1283, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4452}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4207}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steganalysis%20on%20Digital%20Watermarking%3A%20Is%20Your%20Defense%20Truly%20Impervious%3F&body=Title%3A%20Steganalysis%20on%20Digital%20Watermarking%3A%20Is%20Your%20Defense%20Truly%20Impervious%3F%0AAuthor%3A%20Pei%20Yang%20and%20Hai%20Ci%20and%20Yiren%20Song%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Digital%20watermarking%20techniques%20are%20crucial%20for%20copyright%20protection%20and%0Asource%20identification%20of%20images%2C%20especially%20in%20the%20era%20of%20generative%20AI%20models.%0AHowever%2C%20many%20existing%20watermarking%20methods%2C%20particularly%20content-agnostic%0Aapproaches%20that%20embed%20fixed%20patterns%20regardless%20of%20image%20content%2C%20are%0Avulnerable%20to%20steganalysis%20attacks%20that%20can%20extract%20and%20remove%20the%20watermark%0Awith%20minimal%20perceptual%20distortion.%20In%20this%20work%2C%20we%20categorize%20watermarking%0Aalgorithms%20into%20content-adaptive%20and%20content-agnostic%20ones%2C%20and%20demonstrate%20how%0Aaveraging%20a%20collection%20of%20watermarked%20images%20could%20reveal%20the%20underlying%0Awatermark%20pattern.%20We%20then%20leverage%20this%20extracted%20pattern%20for%20effective%0Awatermark%20removal%20under%20both%20graybox%20and%20blackbox%20settings%2C%20even%20when%20the%0Acollection%20contains%20multiple%20watermark%20patterns.%20For%20some%20algorithms%20like%0ATree-Ring%20watermarks%2C%20the%20extracted%20pattern%20can%20also%20forge%20convincing%0Awatermarks%20on%20clean%20images.%20Our%20quantitative%20and%20qualitative%20evaluations%20across%0Atwelve%20watermarking%20methods%20highlight%20the%20threat%20posed%20by%20steganalysis%20to%0Acontent-agnostic%20watermarks%20and%20the%20importance%20of%20designing%20watermarking%0Atechniques%20resilient%20to%20such%20analytical%20attacks.%20We%20propose%20security%20guidelines%0Acalling%20for%20using%20content-adaptive%20watermarking%20strategies%20and%20performing%0Asecurity%20evaluation%20against%20steganalysis.%20We%20also%20suggest%20multi-key%20assignments%0Aas%20potential%20mitigations%20against%20steganalysis%20vulnerabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteganalysis%2520on%2520Digital%2520Watermarking%253A%2520Is%2520Your%2520Defense%2520Truly%2520Impervious%253F%26entry.906535625%3DPei%2520Yang%2520and%2520Hai%2520Ci%2520and%2520Yiren%2520Song%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Digital%2520watermarking%2520techniques%2520are%2520crucial%2520for%2520copyright%2520protection%2520and%250Asource%2520identification%2520of%2520images%252C%2520especially%2520in%2520the%2520era%2520of%2520generative%2520AI%2520models.%250AHowever%252C%2520many%2520existing%2520watermarking%2520methods%252C%2520particularly%2520content-agnostic%250Aapproaches%2520that%2520embed%2520fixed%2520patterns%2520regardless%2520of%2520image%2520content%252C%2520are%250Avulnerable%2520to%2520steganalysis%2520attacks%2520that%2520can%2520extract%2520and%2520remove%2520the%2520watermark%250Awith%2520minimal%2520perceptual%2520distortion.%2520In%2520this%2520work%252C%2520we%2520categorize%2520watermarking%250Aalgorithms%2520into%2520content-adaptive%2520and%2520content-agnostic%2520ones%252C%2520and%2520demonstrate%2520how%250Aaveraging%2520a%2520collection%2520of%2520watermarked%2520images%2520could%2520reveal%2520the%2520underlying%250Awatermark%2520pattern.%2520We%2520then%2520leverage%2520this%2520extracted%2520pattern%2520for%2520effective%250Awatermark%2520removal%2520under%2520both%2520graybox%2520and%2520blackbox%2520settings%252C%2520even%2520when%2520the%250Acollection%2520contains%2520multiple%2520watermark%2520patterns.%2520For%2520some%2520algorithms%2520like%250ATree-Ring%2520watermarks%252C%2520the%2520extracted%2520pattern%2520can%2520also%2520forge%2520convincing%250Awatermarks%2520on%2520clean%2520images.%2520Our%2520quantitative%2520and%2520qualitative%2520evaluations%2520across%250Atwelve%2520watermarking%2520methods%2520highlight%2520the%2520threat%2520posed%2520by%2520steganalysis%2520to%250Acontent-agnostic%2520watermarks%2520and%2520the%2520importance%2520of%2520designing%2520watermarking%250Atechniques%2520resilient%2520to%2520such%2520analytical%2520attacks.%2520We%2520propose%2520security%2520guidelines%250Acalling%2520for%2520using%2520content-adaptive%2520watermarking%2520strategies%2520and%2520performing%250Asecurity%2520evaluation%2520against%2520steganalysis.%2520We%2520also%2520suggest%2520multi-key%2520assignments%250Aas%2520potential%2520mitigations%2520against%2520steganalysis%2520vulnerabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steganalysis%20on%20Digital%20Watermarking%3A%20Is%20Your%20Defense%20Truly%20Impervious%3F&entry.906535625=Pei%20Yang%20and%20Hai%20Ci%20and%20Yiren%20Song%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Digital%20watermarking%20techniques%20are%20crucial%20for%20copyright%20protection%20and%0Asource%20identification%20of%20images%2C%20especially%20in%20the%20era%20of%20generative%20AI%20models.%0AHowever%2C%20many%20existing%20watermarking%20methods%2C%20particularly%20content-agnostic%0Aapproaches%20that%20embed%20fixed%20patterns%20regardless%20of%20image%20content%2C%20are%0Avulnerable%20to%20steganalysis%20attacks%20that%20can%20extract%20and%20remove%20the%20watermark%0Awith%20minimal%20perceptual%20distortion.%20In%20this%20work%2C%20we%20categorize%20watermarking%0Aalgorithms%20into%20content-adaptive%20and%20content-agnostic%20ones%2C%20and%20demonstrate%20how%0Aaveraging%20a%20collection%20of%20watermarked%20images%20could%20reveal%20the%20underlying%0Awatermark%20pattern.%20We%20then%20leverage%20this%20extracted%20pattern%20for%20effective%0Awatermark%20removal%20under%20both%20graybox%20and%20blackbox%20settings%2C%20even%20when%20the%0Acollection%20contains%20multiple%20watermark%20patterns.%20For%20some%20algorithms%20like%0ATree-Ring%20watermarks%2C%20the%20extracted%20pattern%20can%20also%20forge%20convincing%0Awatermarks%20on%20clean%20images.%20Our%20quantitative%20and%20qualitative%20evaluations%20across%0Atwelve%20watermarking%20methods%20highlight%20the%20threat%20posed%20by%20steganalysis%20to%0Acontent-agnostic%20watermarks%20and%20the%20importance%20of%20designing%20watermarking%0Atechniques%20resilient%20to%20such%20analytical%20attacks.%20We%20propose%20security%20guidelines%0Acalling%20for%20using%20content-adaptive%20watermarking%20strategies%20and%20performing%0Asecurity%20evaluation%20against%20steganalysis.%20We%20also%20suggest%20multi-key%20assignments%0Aas%20potential%20mitigations%20against%20steganalysis%20vulnerabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09026v1&entry.124074799=Read"},
{"title": "Demystifying the Physics of Deep Reinforcement Learning-Based Autonomous\n  Vehicle Decision-Making", "author": "Hanxi Wan and Pei Li and Arpan Kusari", "abstract": "  With the advent of universal function approximators in the domain of\nreinforcement learning, the number of practical applications leveraging deep\nreinforcement learning (DRL) has exploded. Decision-making in autonomous\nvehicles (AVs) has emerged as a chief application among them, taking the sensor\ndata or the higher-order kinematic variables as the input and providing a\ndiscrete choice or continuous control output. There has been a continuous\neffort to understand the black-box nature of the DRL models, but so far, there\nhasn't been any discussion (to the best of authors' knowledge) about how the\nmodels learn the physical process. This presents an overwhelming limitation\nthat restricts the real-world deployment of DRL in AVs. Therefore, in this\nresearch work, we try to decode the knowledge learnt by the attention-based DRL\nframework about the physical process. We use a continuous proximal policy\noptimization-based DRL algorithm as the baseline model and add a multi-head\nattention framework in an open-source AV simulation environment. We provide\nsome analytical techniques for discussing the interpretability of the trained\nmodels in terms of explainability and causality for spatial and temporal\ncorrelations. We show that the weights in the first head encode the positions\nof the neighboring vehicles while the second head focuses on the leader vehicle\nexclusively. Also, the ego vehicle's action is causally dependent on the\nvehicles in the target lane spatially and temporally. Through these findings,\nwe reliably show that these techniques can help practitioners decipher the\nresults of the DRL algorithms.\n", "link": "http://arxiv.org/abs/2403.11432v2", "date": "2024-06-13", "relevancy": 2.1212, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5842}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5208}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20the%20Physics%20of%20Deep%20Reinforcement%20Learning-Based%20Autonomous%0A%20%20Vehicle%20Decision-Making&body=Title%3A%20Demystifying%20the%20Physics%20of%20Deep%20Reinforcement%20Learning-Based%20Autonomous%0A%20%20Vehicle%20Decision-Making%0AAuthor%3A%20Hanxi%20Wan%20and%20Pei%20Li%20and%20Arpan%20Kusari%0AAbstract%3A%20%20%20With%20the%20advent%20of%20universal%20function%20approximators%20in%20the%20domain%20of%0Areinforcement%20learning%2C%20the%20number%20of%20practical%20applications%20leveraging%20deep%0Areinforcement%20learning%20%28DRL%29%20has%20exploded.%20Decision-making%20in%20autonomous%0Avehicles%20%28AVs%29%20has%20emerged%20as%20a%20chief%20application%20among%20them%2C%20taking%20the%20sensor%0Adata%20or%20the%20higher-order%20kinematic%20variables%20as%20the%20input%20and%20providing%20a%0Adiscrete%20choice%20or%20continuous%20control%20output.%20There%20has%20been%20a%20continuous%0Aeffort%20to%20understand%20the%20black-box%20nature%20of%20the%20DRL%20models%2C%20but%20so%20far%2C%20there%0Ahasn%27t%20been%20any%20discussion%20%28to%20the%20best%20of%20authors%27%20knowledge%29%20about%20how%20the%0Amodels%20learn%20the%20physical%20process.%20This%20presents%20an%20overwhelming%20limitation%0Athat%20restricts%20the%20real-world%20deployment%20of%20DRL%20in%20AVs.%20Therefore%2C%20in%20this%0Aresearch%20work%2C%20we%20try%20to%20decode%20the%20knowledge%20learnt%20by%20the%20attention-based%20DRL%0Aframework%20about%20the%20physical%20process.%20We%20use%20a%20continuous%20proximal%20policy%0Aoptimization-based%20DRL%20algorithm%20as%20the%20baseline%20model%20and%20add%20a%20multi-head%0Aattention%20framework%20in%20an%20open-source%20AV%20simulation%20environment.%20We%20provide%0Asome%20analytical%20techniques%20for%20discussing%20the%20interpretability%20of%20the%20trained%0Amodels%20in%20terms%20of%20explainability%20and%20causality%20for%20spatial%20and%20temporal%0Acorrelations.%20We%20show%20that%20the%20weights%20in%20the%20first%20head%20encode%20the%20positions%0Aof%20the%20neighboring%20vehicles%20while%20the%20second%20head%20focuses%20on%20the%20leader%20vehicle%0Aexclusively.%20Also%2C%20the%20ego%20vehicle%27s%20action%20is%20causally%20dependent%20on%20the%0Avehicles%20in%20the%20target%20lane%20spatially%20and%20temporally.%20Through%20these%20findings%2C%0Awe%20reliably%20show%20that%20these%20techniques%20can%20help%20practitioners%20decipher%20the%0Aresults%20of%20the%20DRL%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520the%2520Physics%2520of%2520Deep%2520Reinforcement%2520Learning-Based%2520Autonomous%250A%2520%2520Vehicle%2520Decision-Making%26entry.906535625%3DHanxi%2520Wan%2520and%2520Pei%2520Li%2520and%2520Arpan%2520Kusari%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520universal%2520function%2520approximators%2520in%2520the%2520domain%2520of%250Areinforcement%2520learning%252C%2520the%2520number%2520of%2520practical%2520applications%2520leveraging%2520deep%250Areinforcement%2520learning%2520%2528DRL%2529%2520has%2520exploded.%2520Decision-making%2520in%2520autonomous%250Avehicles%2520%2528AVs%2529%2520has%2520emerged%2520as%2520a%2520chief%2520application%2520among%2520them%252C%2520taking%2520the%2520sensor%250Adata%2520or%2520the%2520higher-order%2520kinematic%2520variables%2520as%2520the%2520input%2520and%2520providing%2520a%250Adiscrete%2520choice%2520or%2520continuous%2520control%2520output.%2520There%2520has%2520been%2520a%2520continuous%250Aeffort%2520to%2520understand%2520the%2520black-box%2520nature%2520of%2520the%2520DRL%2520models%252C%2520but%2520so%2520far%252C%2520there%250Ahasn%2527t%2520been%2520any%2520discussion%2520%2528to%2520the%2520best%2520of%2520authors%2527%2520knowledge%2529%2520about%2520how%2520the%250Amodels%2520learn%2520the%2520physical%2520process.%2520This%2520presents%2520an%2520overwhelming%2520limitation%250Athat%2520restricts%2520the%2520real-world%2520deployment%2520of%2520DRL%2520in%2520AVs.%2520Therefore%252C%2520in%2520this%250Aresearch%2520work%252C%2520we%2520try%2520to%2520decode%2520the%2520knowledge%2520learnt%2520by%2520the%2520attention-based%2520DRL%250Aframework%2520about%2520the%2520physical%2520process.%2520We%2520use%2520a%2520continuous%2520proximal%2520policy%250Aoptimization-based%2520DRL%2520algorithm%2520as%2520the%2520baseline%2520model%2520and%2520add%2520a%2520multi-head%250Aattention%2520framework%2520in%2520an%2520open-source%2520AV%2520simulation%2520environment.%2520We%2520provide%250Asome%2520analytical%2520techniques%2520for%2520discussing%2520the%2520interpretability%2520of%2520the%2520trained%250Amodels%2520in%2520terms%2520of%2520explainability%2520and%2520causality%2520for%2520spatial%2520and%2520temporal%250Acorrelations.%2520We%2520show%2520that%2520the%2520weights%2520in%2520the%2520first%2520head%2520encode%2520the%2520positions%250Aof%2520the%2520neighboring%2520vehicles%2520while%2520the%2520second%2520head%2520focuses%2520on%2520the%2520leader%2520vehicle%250Aexclusively.%2520Also%252C%2520the%2520ego%2520vehicle%2527s%2520action%2520is%2520causally%2520dependent%2520on%2520the%250Avehicles%2520in%2520the%2520target%2520lane%2520spatially%2520and%2520temporally.%2520Through%2520these%2520findings%252C%250Awe%2520reliably%2520show%2520that%2520these%2520techniques%2520can%2520help%2520practitioners%2520decipher%2520the%250Aresults%2520of%2520the%2520DRL%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20the%20Physics%20of%20Deep%20Reinforcement%20Learning-Based%20Autonomous%0A%20%20Vehicle%20Decision-Making&entry.906535625=Hanxi%20Wan%20and%20Pei%20Li%20and%20Arpan%20Kusari&entry.1292438233=%20%20With%20the%20advent%20of%20universal%20function%20approximators%20in%20the%20domain%20of%0Areinforcement%20learning%2C%20the%20number%20of%20practical%20applications%20leveraging%20deep%0Areinforcement%20learning%20%28DRL%29%20has%20exploded.%20Decision-making%20in%20autonomous%0Avehicles%20%28AVs%29%20has%20emerged%20as%20a%20chief%20application%20among%20them%2C%20taking%20the%20sensor%0Adata%20or%20the%20higher-order%20kinematic%20variables%20as%20the%20input%20and%20providing%20a%0Adiscrete%20choice%20or%20continuous%20control%20output.%20There%20has%20been%20a%20continuous%0Aeffort%20to%20understand%20the%20black-box%20nature%20of%20the%20DRL%20models%2C%20but%20so%20far%2C%20there%0Ahasn%27t%20been%20any%20discussion%20%28to%20the%20best%20of%20authors%27%20knowledge%29%20about%20how%20the%0Amodels%20learn%20the%20physical%20process.%20This%20presents%20an%20overwhelming%20limitation%0Athat%20restricts%20the%20real-world%20deployment%20of%20DRL%20in%20AVs.%20Therefore%2C%20in%20this%0Aresearch%20work%2C%20we%20try%20to%20decode%20the%20knowledge%20learnt%20by%20the%20attention-based%20DRL%0Aframework%20about%20the%20physical%20process.%20We%20use%20a%20continuous%20proximal%20policy%0Aoptimization-based%20DRL%20algorithm%20as%20the%20baseline%20model%20and%20add%20a%20multi-head%0Aattention%20framework%20in%20an%20open-source%20AV%20simulation%20environment.%20We%20provide%0Asome%20analytical%20techniques%20for%20discussing%20the%20interpretability%20of%20the%20trained%0Amodels%20in%20terms%20of%20explainability%20and%20causality%20for%20spatial%20and%20temporal%0Acorrelations.%20We%20show%20that%20the%20weights%20in%20the%20first%20head%20encode%20the%20positions%0Aof%20the%20neighboring%20vehicles%20while%20the%20second%20head%20focuses%20on%20the%20leader%20vehicle%0Aexclusively.%20Also%2C%20the%20ego%20vehicle%27s%20action%20is%20causally%20dependent%20on%20the%0Avehicles%20in%20the%20target%20lane%20spatially%20and%20temporally.%20Through%20these%20findings%2C%0Awe%20reliably%20show%20that%20these%20techniques%20can%20help%20practitioners%20decipher%20the%0Aresults%20of%20the%20DRL%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11432v2&entry.124074799=Read"},
{"title": "OpenVLA: An Open-Source Vision-Language-Action Model", "author": "Moo Jin Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn", "abstract": "  Large policies pretrained on a combination of Internet-scale vision-language\ndata and diverse robot demonstrations have the potential to change how we teach\nrobots new skills: rather than training new behaviors from scratch, we can\nfine-tune such vision-language-action (VLA) models to obtain robust,\ngeneralizable policies for visuomotor control. Yet, widespread adoption of VLAs\nfor robotics has been challenging as 1) existing VLAs are largely closed and\ninaccessible to the public, and 2) prior work fails to explore methods for\nefficiently fine-tuning VLAs for new tasks, a key component for adoption.\nAddressing these challenges, we introduce OpenVLA, a 7B-parameter open-source\nVLA trained on a diverse collection of 970k real-world robot demonstrations.\nOpenVLA builds on a Llama 2 language model combined with a visual encoder that\nfuses pretrained features from DINOv2 and SigLIP. As a product of the added\ndata diversity and new model components, OpenVLA demonstrates strong results\nfor generalist manipulation, outperforming closed models such as RT-2-X (55B)\nby 16.5% in absolute task success rate across 29 tasks and multiple robot\nembodiments, with 7x fewer parameters. We further show that we can effectively\nfine-tune OpenVLA for new settings, with especially strong generalization\nresults in multi-task environments involving multiple objects and strong\nlanguage grounding abilities, and outperform expressive from-scratch imitation\nlearning methods such as Diffusion Policy by 20.4%. We also explore compute\nefficiency; as a separate contribution, we show that OpenVLA can be fine-tuned\non consumer GPUs via modern low-rank adaptation methods and served efficiently\nvia quantization without a hit to downstream success rate. Finally, we release\nmodel checkpoints, fine-tuning notebooks, and our PyTorch codebase with\nbuilt-in support for training VLAs at scale on Open X-Embodiment datasets.\n", "link": "http://arxiv.org/abs/2406.09246v1", "date": "2024-06-13", "relevancy": 2.1138, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5354}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5318}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenVLA%3A%20An%20Open-Source%20Vision-Language-Action%20Model&body=Title%3A%20OpenVLA%3A%20An%20Open-Source%20Vision-Language-Action%20Model%0AAuthor%3A%20Moo%20Jin%20Kim%20and%20Karl%20Pertsch%20and%20Siddharth%20Karamcheti%20and%20Ted%20Xiao%20and%20Ashwin%20Balakrishna%20and%20Suraj%20Nair%20and%20Rafael%20Rafailov%20and%20Ethan%20Foster%20and%20Grace%20Lam%20and%20Pannag%20Sanketi%20and%20Quan%20Vuong%20and%20Thomas%20Kollar%20and%20Benjamin%20Burchfiel%20and%20Russ%20Tedrake%20and%20Dorsa%20Sadigh%20and%20Sergey%20Levine%20and%20Percy%20Liang%20and%20Chelsea%20Finn%0AAbstract%3A%20%20%20Large%20policies%20pretrained%20on%20a%20combination%20of%20Internet-scale%20vision-language%0Adata%20and%20diverse%20robot%20demonstrations%20have%20the%20potential%20to%20change%20how%20we%20teach%0Arobots%20new%20skills%3A%20rather%20than%20training%20new%20behaviors%20from%20scratch%2C%20we%20can%0Afine-tune%20such%20vision-language-action%20%28VLA%29%20models%20to%20obtain%20robust%2C%0Ageneralizable%20policies%20for%20visuomotor%20control.%20Yet%2C%20widespread%20adoption%20of%20VLAs%0Afor%20robotics%20has%20been%20challenging%20as%201%29%20existing%20VLAs%20are%20largely%20closed%20and%0Ainaccessible%20to%20the%20public%2C%20and%202%29%20prior%20work%20fails%20to%20explore%20methods%20for%0Aefficiently%20fine-tuning%20VLAs%20for%20new%20tasks%2C%20a%20key%20component%20for%20adoption.%0AAddressing%20these%20challenges%2C%20we%20introduce%20OpenVLA%2C%20a%207B-parameter%20open-source%0AVLA%20trained%20on%20a%20diverse%20collection%20of%20970k%20real-world%20robot%20demonstrations.%0AOpenVLA%20builds%20on%20a%20Llama%202%20language%20model%20combined%20with%20a%20visual%20encoder%20that%0Afuses%20pretrained%20features%20from%20DINOv2%20and%20SigLIP.%20As%20a%20product%20of%20the%20added%0Adata%20diversity%20and%20new%20model%20components%2C%20OpenVLA%20demonstrates%20strong%20results%0Afor%20generalist%20manipulation%2C%20outperforming%20closed%20models%20such%20as%20RT-2-X%20%2855B%29%0Aby%2016.5%25%20in%20absolute%20task%20success%20rate%20across%2029%20tasks%20and%20multiple%20robot%0Aembodiments%2C%20with%207x%20fewer%20parameters.%20We%20further%20show%20that%20we%20can%20effectively%0Afine-tune%20OpenVLA%20for%20new%20settings%2C%20with%20especially%20strong%20generalization%0Aresults%20in%20multi-task%20environments%20involving%20multiple%20objects%20and%20strong%0Alanguage%20grounding%20abilities%2C%20and%20outperform%20expressive%20from-scratch%20imitation%0Alearning%20methods%20such%20as%20Diffusion%20Policy%20by%2020.4%25.%20We%20also%20explore%20compute%0Aefficiency%3B%20as%20a%20separate%20contribution%2C%20we%20show%20that%20OpenVLA%20can%20be%20fine-tuned%0Aon%20consumer%20GPUs%20via%20modern%20low-rank%20adaptation%20methods%20and%20served%20efficiently%0Avia%20quantization%20without%20a%20hit%20to%20downstream%20success%20rate.%20Finally%2C%20we%20release%0Amodel%20checkpoints%2C%20fine-tuning%20notebooks%2C%20and%20our%20PyTorch%20codebase%20with%0Abuilt-in%20support%20for%20training%20VLAs%20at%20scale%20on%20Open%20X-Embodiment%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenVLA%253A%2520An%2520Open-Source%2520Vision-Language-Action%2520Model%26entry.906535625%3DMoo%2520Jin%2520Kim%2520and%2520Karl%2520Pertsch%2520and%2520Siddharth%2520Karamcheti%2520and%2520Ted%2520Xiao%2520and%2520Ashwin%2520Balakrishna%2520and%2520Suraj%2520Nair%2520and%2520Rafael%2520Rafailov%2520and%2520Ethan%2520Foster%2520and%2520Grace%2520Lam%2520and%2520Pannag%2520Sanketi%2520and%2520Quan%2520Vuong%2520and%2520Thomas%2520Kollar%2520and%2520Benjamin%2520Burchfiel%2520and%2520Russ%2520Tedrake%2520and%2520Dorsa%2520Sadigh%2520and%2520Sergey%2520Levine%2520and%2520Percy%2520Liang%2520and%2520Chelsea%2520Finn%26entry.1292438233%3D%2520%2520Large%2520policies%2520pretrained%2520on%2520a%2520combination%2520of%2520Internet-scale%2520vision-language%250Adata%2520and%2520diverse%2520robot%2520demonstrations%2520have%2520the%2520potential%2520to%2520change%2520how%2520we%2520teach%250Arobots%2520new%2520skills%253A%2520rather%2520than%2520training%2520new%2520behaviors%2520from%2520scratch%252C%2520we%2520can%250Afine-tune%2520such%2520vision-language-action%2520%2528VLA%2529%2520models%2520to%2520obtain%2520robust%252C%250Ageneralizable%2520policies%2520for%2520visuomotor%2520control.%2520Yet%252C%2520widespread%2520adoption%2520of%2520VLAs%250Afor%2520robotics%2520has%2520been%2520challenging%2520as%25201%2529%2520existing%2520VLAs%2520are%2520largely%2520closed%2520and%250Ainaccessible%2520to%2520the%2520public%252C%2520and%25202%2529%2520prior%2520work%2520fails%2520to%2520explore%2520methods%2520for%250Aefficiently%2520fine-tuning%2520VLAs%2520for%2520new%2520tasks%252C%2520a%2520key%2520component%2520for%2520adoption.%250AAddressing%2520these%2520challenges%252C%2520we%2520introduce%2520OpenVLA%252C%2520a%25207B-parameter%2520open-source%250AVLA%2520trained%2520on%2520a%2520diverse%2520collection%2520of%2520970k%2520real-world%2520robot%2520demonstrations.%250AOpenVLA%2520builds%2520on%2520a%2520Llama%25202%2520language%2520model%2520combined%2520with%2520a%2520visual%2520encoder%2520that%250Afuses%2520pretrained%2520features%2520from%2520DINOv2%2520and%2520SigLIP.%2520As%2520a%2520product%2520of%2520the%2520added%250Adata%2520diversity%2520and%2520new%2520model%2520components%252C%2520OpenVLA%2520demonstrates%2520strong%2520results%250Afor%2520generalist%2520manipulation%252C%2520outperforming%2520closed%2520models%2520such%2520as%2520RT-2-X%2520%252855B%2529%250Aby%252016.5%2525%2520in%2520absolute%2520task%2520success%2520rate%2520across%252029%2520tasks%2520and%2520multiple%2520robot%250Aembodiments%252C%2520with%25207x%2520fewer%2520parameters.%2520We%2520further%2520show%2520that%2520we%2520can%2520effectively%250Afine-tune%2520OpenVLA%2520for%2520new%2520settings%252C%2520with%2520especially%2520strong%2520generalization%250Aresults%2520in%2520multi-task%2520environments%2520involving%2520multiple%2520objects%2520and%2520strong%250Alanguage%2520grounding%2520abilities%252C%2520and%2520outperform%2520expressive%2520from-scratch%2520imitation%250Alearning%2520methods%2520such%2520as%2520Diffusion%2520Policy%2520by%252020.4%2525.%2520We%2520also%2520explore%2520compute%250Aefficiency%253B%2520as%2520a%2520separate%2520contribution%252C%2520we%2520show%2520that%2520OpenVLA%2520can%2520be%2520fine-tuned%250Aon%2520consumer%2520GPUs%2520via%2520modern%2520low-rank%2520adaptation%2520methods%2520and%2520served%2520efficiently%250Avia%2520quantization%2520without%2520a%2520hit%2520to%2520downstream%2520success%2520rate.%2520Finally%252C%2520we%2520release%250Amodel%2520checkpoints%252C%2520fine-tuning%2520notebooks%252C%2520and%2520our%2520PyTorch%2520codebase%2520with%250Abuilt-in%2520support%2520for%2520training%2520VLAs%2520at%2520scale%2520on%2520Open%2520X-Embodiment%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenVLA%3A%20An%20Open-Source%20Vision-Language-Action%20Model&entry.906535625=Moo%20Jin%20Kim%20and%20Karl%20Pertsch%20and%20Siddharth%20Karamcheti%20and%20Ted%20Xiao%20and%20Ashwin%20Balakrishna%20and%20Suraj%20Nair%20and%20Rafael%20Rafailov%20and%20Ethan%20Foster%20and%20Grace%20Lam%20and%20Pannag%20Sanketi%20and%20Quan%20Vuong%20and%20Thomas%20Kollar%20and%20Benjamin%20Burchfiel%20and%20Russ%20Tedrake%20and%20Dorsa%20Sadigh%20and%20Sergey%20Levine%20and%20Percy%20Liang%20and%20Chelsea%20Finn&entry.1292438233=%20%20Large%20policies%20pretrained%20on%20a%20combination%20of%20Internet-scale%20vision-language%0Adata%20and%20diverse%20robot%20demonstrations%20have%20the%20potential%20to%20change%20how%20we%20teach%0Arobots%20new%20skills%3A%20rather%20than%20training%20new%20behaviors%20from%20scratch%2C%20we%20can%0Afine-tune%20such%20vision-language-action%20%28VLA%29%20models%20to%20obtain%20robust%2C%0Ageneralizable%20policies%20for%20visuomotor%20control.%20Yet%2C%20widespread%20adoption%20of%20VLAs%0Afor%20robotics%20has%20been%20challenging%20as%201%29%20existing%20VLAs%20are%20largely%20closed%20and%0Ainaccessible%20to%20the%20public%2C%20and%202%29%20prior%20work%20fails%20to%20explore%20methods%20for%0Aefficiently%20fine-tuning%20VLAs%20for%20new%20tasks%2C%20a%20key%20component%20for%20adoption.%0AAddressing%20these%20challenges%2C%20we%20introduce%20OpenVLA%2C%20a%207B-parameter%20open-source%0AVLA%20trained%20on%20a%20diverse%20collection%20of%20970k%20real-world%20robot%20demonstrations.%0AOpenVLA%20builds%20on%20a%20Llama%202%20language%20model%20combined%20with%20a%20visual%20encoder%20that%0Afuses%20pretrained%20features%20from%20DINOv2%20and%20SigLIP.%20As%20a%20product%20of%20the%20added%0Adata%20diversity%20and%20new%20model%20components%2C%20OpenVLA%20demonstrates%20strong%20results%0Afor%20generalist%20manipulation%2C%20outperforming%20closed%20models%20such%20as%20RT-2-X%20%2855B%29%0Aby%2016.5%25%20in%20absolute%20task%20success%20rate%20across%2029%20tasks%20and%20multiple%20robot%0Aembodiments%2C%20with%207x%20fewer%20parameters.%20We%20further%20show%20that%20we%20can%20effectively%0Afine-tune%20OpenVLA%20for%20new%20settings%2C%20with%20especially%20strong%20generalization%0Aresults%20in%20multi-task%20environments%20involving%20multiple%20objects%20and%20strong%0Alanguage%20grounding%20abilities%2C%20and%20outperform%20expressive%20from-scratch%20imitation%0Alearning%20methods%20such%20as%20Diffusion%20Policy%20by%2020.4%25.%20We%20also%20explore%20compute%0Aefficiency%3B%20as%20a%20separate%20contribution%2C%20we%20show%20that%20OpenVLA%20can%20be%20fine-tuned%0Aon%20consumer%20GPUs%20via%20modern%20low-rank%20adaptation%20methods%20and%20served%20efficiently%0Avia%20quantization%20without%20a%20hit%20to%20downstream%20success%20rate.%20Finally%2C%20we%20release%0Amodel%20checkpoints%2C%20fine-tuning%20notebooks%2C%20and%20our%20PyTorch%20codebase%20with%0Abuilt-in%20support%20for%20training%20VLAs%20at%20scale%20on%20Open%20X-Embodiment%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09246v1&entry.124074799=Read"},
{"title": "HyperFields: Towards Zero-Shot Generation of NeRFs from Text", "author": "Sudarshan Babu and Richard Liu and Avery Zhou and Michael Maire and Greg Shakhnarovich and Rana Hanocka", "abstract": "  We introduce HyperFields, a method for generating text-conditioned Neural\nRadiance Fields (NeRFs) with a single forward pass and (optionally) some\nfine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns\na smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF\ndistillation training, which distills scenes encoded in individual NeRFs into\none dynamic hypernetwork. These techniques enable a single network to fit over\na hundred unique scenes. We further demonstrate that HyperFields learns a more\ngeneral map between text and NeRFs, and consequently is capable of predicting\nnovel in-distribution and out-of-distribution scenes -- either zero-shot or\nwith a few finetuning steps. Finetuning HyperFields benefits from accelerated\nconvergence thanks to the learned general map, and is capable of synthesizing\nnovel scenes 5 to 10 times faster than existing neural optimization-based\nmethods. Our ablation experiments show that both the dynamic architecture and\nNeRF distillation are critical to the expressivity of HyperFields.\n", "link": "http://arxiv.org/abs/2310.17075v3", "date": "2024-06-13", "relevancy": 2.1055, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5433}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5293}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperFields%3A%20Towards%20Zero-Shot%20Generation%20of%20NeRFs%20from%20Text&body=Title%3A%20HyperFields%3A%20Towards%20Zero-Shot%20Generation%20of%20NeRFs%20from%20Text%0AAuthor%3A%20Sudarshan%20Babu%20and%20Richard%20Liu%20and%20Avery%20Zhou%20and%20Michael%20Maire%20and%20Greg%20Shakhnarovich%20and%20Rana%20Hanocka%0AAbstract%3A%20%20%20We%20introduce%20HyperFields%2C%20a%20method%20for%20generating%20text-conditioned%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20with%20a%20single%20forward%20pass%20and%20%28optionally%29%20some%0Afine-tuning.%20Key%20to%20our%20approach%20are%3A%20%28i%29%20a%20dynamic%20hypernetwork%2C%20which%20learns%0Aa%20smooth%20mapping%20from%20text%20token%20embeddings%20to%20the%20space%20of%20NeRFs%3B%20%28ii%29%20NeRF%0Adistillation%20training%2C%20which%20distills%20scenes%20encoded%20in%20individual%20NeRFs%20into%0Aone%20dynamic%20hypernetwork.%20These%20techniques%20enable%20a%20single%20network%20to%20fit%20over%0Aa%20hundred%20unique%20scenes.%20We%20further%20demonstrate%20that%20HyperFields%20learns%20a%20more%0Ageneral%20map%20between%20text%20and%20NeRFs%2C%20and%20consequently%20is%20capable%20of%20predicting%0Anovel%20in-distribution%20and%20out-of-distribution%20scenes%20--%20either%20zero-shot%20or%0Awith%20a%20few%20finetuning%20steps.%20Finetuning%20HyperFields%20benefits%20from%20accelerated%0Aconvergence%20thanks%20to%20the%20learned%20general%20map%2C%20and%20is%20capable%20of%20synthesizing%0Anovel%20scenes%205%20to%2010%20times%20faster%20than%20existing%20neural%20optimization-based%0Amethods.%20Our%20ablation%20experiments%20show%20that%20both%20the%20dynamic%20architecture%20and%0ANeRF%20distillation%20are%20critical%20to%20the%20expressivity%20of%20HyperFields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17075v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperFields%253A%2520Towards%2520Zero-Shot%2520Generation%2520of%2520NeRFs%2520from%2520Text%26entry.906535625%3DSudarshan%2520Babu%2520and%2520Richard%2520Liu%2520and%2520Avery%2520Zhou%2520and%2520Michael%2520Maire%2520and%2520Greg%2520Shakhnarovich%2520and%2520Rana%2520Hanocka%26entry.1292438233%3D%2520%2520We%2520introduce%2520HyperFields%252C%2520a%2520method%2520for%2520generating%2520text-conditioned%2520Neural%250ARadiance%2520Fields%2520%2528NeRFs%2529%2520with%2520a%2520single%2520forward%2520pass%2520and%2520%2528optionally%2529%2520some%250Afine-tuning.%2520Key%2520to%2520our%2520approach%2520are%253A%2520%2528i%2529%2520a%2520dynamic%2520hypernetwork%252C%2520which%2520learns%250Aa%2520smooth%2520mapping%2520from%2520text%2520token%2520embeddings%2520to%2520the%2520space%2520of%2520NeRFs%253B%2520%2528ii%2529%2520NeRF%250Adistillation%2520training%252C%2520which%2520distills%2520scenes%2520encoded%2520in%2520individual%2520NeRFs%2520into%250Aone%2520dynamic%2520hypernetwork.%2520These%2520techniques%2520enable%2520a%2520single%2520network%2520to%2520fit%2520over%250Aa%2520hundred%2520unique%2520scenes.%2520We%2520further%2520demonstrate%2520that%2520HyperFields%2520learns%2520a%2520more%250Ageneral%2520map%2520between%2520text%2520and%2520NeRFs%252C%2520and%2520consequently%2520is%2520capable%2520of%2520predicting%250Anovel%2520in-distribution%2520and%2520out-of-distribution%2520scenes%2520--%2520either%2520zero-shot%2520or%250Awith%2520a%2520few%2520finetuning%2520steps.%2520Finetuning%2520HyperFields%2520benefits%2520from%2520accelerated%250Aconvergence%2520thanks%2520to%2520the%2520learned%2520general%2520map%252C%2520and%2520is%2520capable%2520of%2520synthesizing%250Anovel%2520scenes%25205%2520to%252010%2520times%2520faster%2520than%2520existing%2520neural%2520optimization-based%250Amethods.%2520Our%2520ablation%2520experiments%2520show%2520that%2520both%2520the%2520dynamic%2520architecture%2520and%250ANeRF%2520distillation%2520are%2520critical%2520to%2520the%2520expressivity%2520of%2520HyperFields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17075v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperFields%3A%20Towards%20Zero-Shot%20Generation%20of%20NeRFs%20from%20Text&entry.906535625=Sudarshan%20Babu%20and%20Richard%20Liu%20and%20Avery%20Zhou%20and%20Michael%20Maire%20and%20Greg%20Shakhnarovich%20and%20Rana%20Hanocka&entry.1292438233=%20%20We%20introduce%20HyperFields%2C%20a%20method%20for%20generating%20text-conditioned%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20with%20a%20single%20forward%20pass%20and%20%28optionally%29%20some%0Afine-tuning.%20Key%20to%20our%20approach%20are%3A%20%28i%29%20a%20dynamic%20hypernetwork%2C%20which%20learns%0Aa%20smooth%20mapping%20from%20text%20token%20embeddings%20to%20the%20space%20of%20NeRFs%3B%20%28ii%29%20NeRF%0Adistillation%20training%2C%20which%20distills%20scenes%20encoded%20in%20individual%20NeRFs%20into%0Aone%20dynamic%20hypernetwork.%20These%20techniques%20enable%20a%20single%20network%20to%20fit%20over%0Aa%20hundred%20unique%20scenes.%20We%20further%20demonstrate%20that%20HyperFields%20learns%20a%20more%0Ageneral%20map%20between%20text%20and%20NeRFs%2C%20and%20consequently%20is%20capable%20of%20predicting%0Anovel%20in-distribution%20and%20out-of-distribution%20scenes%20--%20either%20zero-shot%20or%0Awith%20a%20few%20finetuning%20steps.%20Finetuning%20HyperFields%20benefits%20from%20accelerated%0Aconvergence%20thanks%20to%20the%20learned%20general%20map%2C%20and%20is%20capable%20of%20synthesizing%0Anovel%20scenes%205%20to%2010%20times%20faster%20than%20existing%20neural%20optimization-based%0Amethods.%20Our%20ablation%20experiments%20show%20that%20both%20the%20dynamic%20architecture%20and%0ANeRF%20distillation%20are%20critical%20to%20the%20expressivity%20of%20HyperFields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17075v3&entry.124074799=Read"},
{"title": "Yo'LLaVA: Your Personalized Language and Vision Assistant", "author": "Thao Nguyen and Haotian Liu and Yuheng Li and Mu Cai and Utkarsh Ojha and Yong Jae Lee", "abstract": "  Large Multimodal Models (LMMs) have shown remarkable capabilities across a\nvariety of tasks (e.g., image captioning, visual question answering). While\nbroad, their knowledge remains generic (e.g., recognizing a dog), and they are\nunable to handle personalized subjects (e.g., recognizing a user's pet dog).\nHuman reasoning, in contrast, typically operates within the context of specific\nsubjects in our surroundings. For example, one might ask, \"What should I buy\nfor my dog's birthday?\"; as opposed to a generic inquiry about \"What should I\nbuy for a dog's birthday?\". Similarly, when looking at a friend's image, the\ninterest lies in seeing their activities (e.g., \"my friend is holding a cat\"),\nrather than merely observing generic human actions (e.g., \"a man is holding a\ncat\"). In this paper, we introduce the novel task of personalizing LMMs, so\nthat they can have conversations about a specific subject. We propose Yo'LLaVA,\nwhich learns to embed a personalized subject into a set of latent tokens given\na handful of example images of the subject. Our qualitative and quantitative\nanalyses reveal that Yo'LLaVA can learn the concept more efficiently using\nfewer tokens and more effectively encode the visual attributes compared to\nstrong prompting baselines (e.g., LLaVA).\n", "link": "http://arxiv.org/abs/2406.09400v1", "date": "2024-06-13", "relevancy": 2.1029, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5299}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5233}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yo%27LLaVA%3A%20Your%20Personalized%20Language%20and%20Vision%20Assistant&body=Title%3A%20Yo%27LLaVA%3A%20Your%20Personalized%20Language%20and%20Vision%20Assistant%0AAuthor%3A%20Thao%20Nguyen%20and%20Haotian%20Liu%20and%20Yuheng%20Li%20and%20Mu%20Cai%20and%20Utkarsh%20Ojha%20and%20Yong%20Jae%20Lee%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20remarkable%20capabilities%20across%20a%0Avariety%20of%20tasks%20%28e.g.%2C%20image%20captioning%2C%20visual%20question%20answering%29.%20While%0Abroad%2C%20their%20knowledge%20remains%20generic%20%28e.g.%2C%20recognizing%20a%20dog%29%2C%20and%20they%20are%0Aunable%20to%20handle%20personalized%20subjects%20%28e.g.%2C%20recognizing%20a%20user%27s%20pet%20dog%29.%0AHuman%20reasoning%2C%20in%20contrast%2C%20typically%20operates%20within%20the%20context%20of%20specific%0Asubjects%20in%20our%20surroundings.%20For%20example%2C%20one%20might%20ask%2C%20%22What%20should%20I%20buy%0Afor%20my%20dog%27s%20birthday%3F%22%3B%20as%20opposed%20to%20a%20generic%20inquiry%20about%20%22What%20should%20I%0Abuy%20for%20a%20dog%27s%20birthday%3F%22.%20Similarly%2C%20when%20looking%20at%20a%20friend%27s%20image%2C%20the%0Ainterest%20lies%20in%20seeing%20their%20activities%20%28e.g.%2C%20%22my%20friend%20is%20holding%20a%20cat%22%29%2C%0Arather%20than%20merely%20observing%20generic%20human%20actions%20%28e.g.%2C%20%22a%20man%20is%20holding%20a%0Acat%22%29.%20In%20this%20paper%2C%20we%20introduce%20the%20novel%20task%20of%20personalizing%20LMMs%2C%20so%0Athat%20they%20can%20have%20conversations%20about%20a%20specific%20subject.%20We%20propose%20Yo%27LLaVA%2C%0Awhich%20learns%20to%20embed%20a%20personalized%20subject%20into%20a%20set%20of%20latent%20tokens%20given%0Aa%20handful%20of%20example%20images%20of%20the%20subject.%20Our%20qualitative%20and%20quantitative%0Aanalyses%20reveal%20that%20Yo%27LLaVA%20can%20learn%20the%20concept%20more%20efficiently%20using%0Afewer%20tokens%20and%20more%20effectively%20encode%20the%20visual%20attributes%20compared%20to%0Astrong%20prompting%20baselines%20%28e.g.%2C%20LLaVA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYo%2527LLaVA%253A%2520Your%2520Personalized%2520Language%2520and%2520Vision%2520Assistant%26entry.906535625%3DThao%2520Nguyen%2520and%2520Haotian%2520Liu%2520and%2520Yuheng%2520Li%2520and%2520Mu%2520Cai%2520and%2520Utkarsh%2520Ojha%2520and%2520Yong%2520Jae%2520Lee%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520across%2520a%250Avariety%2520of%2520tasks%2520%2528e.g.%252C%2520image%2520captioning%252C%2520visual%2520question%2520answering%2529.%2520While%250Abroad%252C%2520their%2520knowledge%2520remains%2520generic%2520%2528e.g.%252C%2520recognizing%2520a%2520dog%2529%252C%2520and%2520they%2520are%250Aunable%2520to%2520handle%2520personalized%2520subjects%2520%2528e.g.%252C%2520recognizing%2520a%2520user%2527s%2520pet%2520dog%2529.%250AHuman%2520reasoning%252C%2520in%2520contrast%252C%2520typically%2520operates%2520within%2520the%2520context%2520of%2520specific%250Asubjects%2520in%2520our%2520surroundings.%2520For%2520example%252C%2520one%2520might%2520ask%252C%2520%2522What%2520should%2520I%2520buy%250Afor%2520my%2520dog%2527s%2520birthday%253F%2522%253B%2520as%2520opposed%2520to%2520a%2520generic%2520inquiry%2520about%2520%2522What%2520should%2520I%250Abuy%2520for%2520a%2520dog%2527s%2520birthday%253F%2522.%2520Similarly%252C%2520when%2520looking%2520at%2520a%2520friend%2527s%2520image%252C%2520the%250Ainterest%2520lies%2520in%2520seeing%2520their%2520activities%2520%2528e.g.%252C%2520%2522my%2520friend%2520is%2520holding%2520a%2520cat%2522%2529%252C%250Arather%2520than%2520merely%2520observing%2520generic%2520human%2520actions%2520%2528e.g.%252C%2520%2522a%2520man%2520is%2520holding%2520a%250Acat%2522%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520novel%2520task%2520of%2520personalizing%2520LMMs%252C%2520so%250Athat%2520they%2520can%2520have%2520conversations%2520about%2520a%2520specific%2520subject.%2520We%2520propose%2520Yo%2527LLaVA%252C%250Awhich%2520learns%2520to%2520embed%2520a%2520personalized%2520subject%2520into%2520a%2520set%2520of%2520latent%2520tokens%2520given%250Aa%2520handful%2520of%2520example%2520images%2520of%2520the%2520subject.%2520Our%2520qualitative%2520and%2520quantitative%250Aanalyses%2520reveal%2520that%2520Yo%2527LLaVA%2520can%2520learn%2520the%2520concept%2520more%2520efficiently%2520using%250Afewer%2520tokens%2520and%2520more%2520effectively%2520encode%2520the%2520visual%2520attributes%2520compared%2520to%250Astrong%2520prompting%2520baselines%2520%2528e.g.%252C%2520LLaVA%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yo%27LLaVA%3A%20Your%20Personalized%20Language%20and%20Vision%20Assistant&entry.906535625=Thao%20Nguyen%20and%20Haotian%20Liu%20and%20Yuheng%20Li%20and%20Mu%20Cai%20and%20Utkarsh%20Ojha%20and%20Yong%20Jae%20Lee&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20remarkable%20capabilities%20across%20a%0Avariety%20of%20tasks%20%28e.g.%2C%20image%20captioning%2C%20visual%20question%20answering%29.%20While%0Abroad%2C%20their%20knowledge%20remains%20generic%20%28e.g.%2C%20recognizing%20a%20dog%29%2C%20and%20they%20are%0Aunable%20to%20handle%20personalized%20subjects%20%28e.g.%2C%20recognizing%20a%20user%27s%20pet%20dog%29.%0AHuman%20reasoning%2C%20in%20contrast%2C%20typically%20operates%20within%20the%20context%20of%20specific%0Asubjects%20in%20our%20surroundings.%20For%20example%2C%20one%20might%20ask%2C%20%22What%20should%20I%20buy%0Afor%20my%20dog%27s%20birthday%3F%22%3B%20as%20opposed%20to%20a%20generic%20inquiry%20about%20%22What%20should%20I%0Abuy%20for%20a%20dog%27s%20birthday%3F%22.%20Similarly%2C%20when%20looking%20at%20a%20friend%27s%20image%2C%20the%0Ainterest%20lies%20in%20seeing%20their%20activities%20%28e.g.%2C%20%22my%20friend%20is%20holding%20a%20cat%22%29%2C%0Arather%20than%20merely%20observing%20generic%20human%20actions%20%28e.g.%2C%20%22a%20man%20is%20holding%20a%0Acat%22%29.%20In%20this%20paper%2C%20we%20introduce%20the%20novel%20task%20of%20personalizing%20LMMs%2C%20so%0Athat%20they%20can%20have%20conversations%20about%20a%20specific%20subject.%20We%20propose%20Yo%27LLaVA%2C%0Awhich%20learns%20to%20embed%20a%20personalized%20subject%20into%20a%20set%20of%20latent%20tokens%20given%0Aa%20handful%20of%20example%20images%20of%20the%20subject.%20Our%20qualitative%20and%20quantitative%0Aanalyses%20reveal%20that%20Yo%27LLaVA%20can%20learn%20the%20concept%20more%20efficiently%20using%0Afewer%20tokens%20and%20more%20effectively%20encode%20the%20visual%20attributes%20compared%20to%0Astrong%20prompting%20baselines%20%28e.g.%2C%20LLaVA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09400v1&entry.124074799=Read"},
{"title": "Towards an Improved Understanding and Utilization of Maximum Manifold\n  Capacity Representations", "author": "Rylan Schaeffer and Victor Lecomte and Dhruv Bhandarkar Pai and Andres Carranza and Berivan Isik and Alyssa Unell and Mikail Khona and Thomas Yerxa and Yann LeCun and SueYeon Chung and Andrey Gromov and Ravid Shwartz-Ziv and Sanmi Koyejo", "abstract": "  Maximum Manifold Capacity Representations (MMCR) is a recent multi-view\nself-supervised learning (MVSSL) method that matches or surpasses other leading\nMVSSL methods. MMCR is intriguing because it does not fit neatly into any of\nthe commonplace MVSSL lineages, instead originating from a statistical\nmechanical perspective on the linear separability of data manifolds. In this\npaper, we seek to improve our understanding and our utilization of MMCR. To\nbetter understand MMCR, we leverage tools from high dimensional probability to\ndemonstrate that MMCR incentivizes alignment and uniformity of learned\nembeddings. We then leverage tools from information theory to show that such\nembeddings maximize a well-known lower bound on mutual information between\nviews, thereby connecting the geometric perspective of MMCR to the\ninformation-theoretic perspective commonly discussed in MVSSL. To better\nutilize MMCR, we mathematically predict and experimentally confirm\nnon-monotonic changes in the pretraining loss akin to double descent but with\nrespect to atypical hyperparameters. We also discover compute scaling laws that\nenable predicting the pretraining loss as a function of gradients steps, batch\nsize, embedding dimension and number of views. We then show that MMCR,\noriginally applied to image data, is performant on multimodal image-text data.\nBy more deeply understanding the theoretical and empirical behavior of MMCR,\nour work reveals insights on improving MVSSL methods.\n", "link": "http://arxiv.org/abs/2406.09366v1", "date": "2024-06-13", "relevancy": 2.0951, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5075}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20an%20Improved%20Understanding%20and%20Utilization%20of%20Maximum%20Manifold%0A%20%20Capacity%20Representations&body=Title%3A%20Towards%20an%20Improved%20Understanding%20and%20Utilization%20of%20Maximum%20Manifold%0A%20%20Capacity%20Representations%0AAuthor%3A%20Rylan%20Schaeffer%20and%20Victor%20Lecomte%20and%20Dhruv%20Bhandarkar%20Pai%20and%20Andres%20Carranza%20and%20Berivan%20Isik%20and%20Alyssa%20Unell%20and%20Mikail%20Khona%20and%20Thomas%20Yerxa%20and%20Yann%20LeCun%20and%20SueYeon%20Chung%20and%20Andrey%20Gromov%20and%20Ravid%20Shwartz-Ziv%20and%20Sanmi%20Koyejo%0AAbstract%3A%20%20%20Maximum%20Manifold%20Capacity%20Representations%20%28MMCR%29%20is%20a%20recent%20multi-view%0Aself-supervised%20learning%20%28MVSSL%29%20method%20that%20matches%20or%20surpasses%20other%20leading%0AMVSSL%20methods.%20MMCR%20is%20intriguing%20because%20it%20does%20not%20fit%20neatly%20into%20any%20of%0Athe%20commonplace%20MVSSL%20lineages%2C%20instead%20originating%20from%20a%20statistical%0Amechanical%20perspective%20on%20the%20linear%20separability%20of%20data%20manifolds.%20In%20this%0Apaper%2C%20we%20seek%20to%20improve%20our%20understanding%20and%20our%20utilization%20of%20MMCR.%20To%0Abetter%20understand%20MMCR%2C%20we%20leverage%20tools%20from%20high%20dimensional%20probability%20to%0Ademonstrate%20that%20MMCR%20incentivizes%20alignment%20and%20uniformity%20of%20learned%0Aembeddings.%20We%20then%20leverage%20tools%20from%20information%20theory%20to%20show%20that%20such%0Aembeddings%20maximize%20a%20well-known%20lower%20bound%20on%20mutual%20information%20between%0Aviews%2C%20thereby%20connecting%20the%20geometric%20perspective%20of%20MMCR%20to%20the%0Ainformation-theoretic%20perspective%20commonly%20discussed%20in%20MVSSL.%20To%20better%0Autilize%20MMCR%2C%20we%20mathematically%20predict%20and%20experimentally%20confirm%0Anon-monotonic%20changes%20in%20the%20pretraining%20loss%20akin%20to%20double%20descent%20but%20with%0Arespect%20to%20atypical%20hyperparameters.%20We%20also%20discover%20compute%20scaling%20laws%20that%0Aenable%20predicting%20the%20pretraining%20loss%20as%20a%20function%20of%20gradients%20steps%2C%20batch%0Asize%2C%20embedding%20dimension%20and%20number%20of%20views.%20We%20then%20show%20that%20MMCR%2C%0Aoriginally%20applied%20to%20image%20data%2C%20is%20performant%20on%20multimodal%20image-text%20data.%0ABy%20more%20deeply%20understanding%20the%20theoretical%20and%20empirical%20behavior%20of%20MMCR%2C%0Aour%20work%20reveals%20insights%20on%20improving%20MVSSL%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520an%2520Improved%2520Understanding%2520and%2520Utilization%2520of%2520Maximum%2520Manifold%250A%2520%2520Capacity%2520Representations%26entry.906535625%3DRylan%2520Schaeffer%2520and%2520Victor%2520Lecomte%2520and%2520Dhruv%2520Bhandarkar%2520Pai%2520and%2520Andres%2520Carranza%2520and%2520Berivan%2520Isik%2520and%2520Alyssa%2520Unell%2520and%2520Mikail%2520Khona%2520and%2520Thomas%2520Yerxa%2520and%2520Yann%2520LeCun%2520and%2520SueYeon%2520Chung%2520and%2520Andrey%2520Gromov%2520and%2520Ravid%2520Shwartz-Ziv%2520and%2520Sanmi%2520Koyejo%26entry.1292438233%3D%2520%2520Maximum%2520Manifold%2520Capacity%2520Representations%2520%2528MMCR%2529%2520is%2520a%2520recent%2520multi-view%250Aself-supervised%2520learning%2520%2528MVSSL%2529%2520method%2520that%2520matches%2520or%2520surpasses%2520other%2520leading%250AMVSSL%2520methods.%2520MMCR%2520is%2520intriguing%2520because%2520it%2520does%2520not%2520fit%2520neatly%2520into%2520any%2520of%250Athe%2520commonplace%2520MVSSL%2520lineages%252C%2520instead%2520originating%2520from%2520a%2520statistical%250Amechanical%2520perspective%2520on%2520the%2520linear%2520separability%2520of%2520data%2520manifolds.%2520In%2520this%250Apaper%252C%2520we%2520seek%2520to%2520improve%2520our%2520understanding%2520and%2520our%2520utilization%2520of%2520MMCR.%2520To%250Abetter%2520understand%2520MMCR%252C%2520we%2520leverage%2520tools%2520from%2520high%2520dimensional%2520probability%2520to%250Ademonstrate%2520that%2520MMCR%2520incentivizes%2520alignment%2520and%2520uniformity%2520of%2520learned%250Aembeddings.%2520We%2520then%2520leverage%2520tools%2520from%2520information%2520theory%2520to%2520show%2520that%2520such%250Aembeddings%2520maximize%2520a%2520well-known%2520lower%2520bound%2520on%2520mutual%2520information%2520between%250Aviews%252C%2520thereby%2520connecting%2520the%2520geometric%2520perspective%2520of%2520MMCR%2520to%2520the%250Ainformation-theoretic%2520perspective%2520commonly%2520discussed%2520in%2520MVSSL.%2520To%2520better%250Autilize%2520MMCR%252C%2520we%2520mathematically%2520predict%2520and%2520experimentally%2520confirm%250Anon-monotonic%2520changes%2520in%2520the%2520pretraining%2520loss%2520akin%2520to%2520double%2520descent%2520but%2520with%250Arespect%2520to%2520atypical%2520hyperparameters.%2520We%2520also%2520discover%2520compute%2520scaling%2520laws%2520that%250Aenable%2520predicting%2520the%2520pretraining%2520loss%2520as%2520a%2520function%2520of%2520gradients%2520steps%252C%2520batch%250Asize%252C%2520embedding%2520dimension%2520and%2520number%2520of%2520views.%2520We%2520then%2520show%2520that%2520MMCR%252C%250Aoriginally%2520applied%2520to%2520image%2520data%252C%2520is%2520performant%2520on%2520multimodal%2520image-text%2520data.%250ABy%2520more%2520deeply%2520understanding%2520the%2520theoretical%2520and%2520empirical%2520behavior%2520of%2520MMCR%252C%250Aour%2520work%2520reveals%2520insights%2520on%2520improving%2520MVSSL%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20an%20Improved%20Understanding%20and%20Utilization%20of%20Maximum%20Manifold%0A%20%20Capacity%20Representations&entry.906535625=Rylan%20Schaeffer%20and%20Victor%20Lecomte%20and%20Dhruv%20Bhandarkar%20Pai%20and%20Andres%20Carranza%20and%20Berivan%20Isik%20and%20Alyssa%20Unell%20and%20Mikail%20Khona%20and%20Thomas%20Yerxa%20and%20Yann%20LeCun%20and%20SueYeon%20Chung%20and%20Andrey%20Gromov%20and%20Ravid%20Shwartz-Ziv%20and%20Sanmi%20Koyejo&entry.1292438233=%20%20Maximum%20Manifold%20Capacity%20Representations%20%28MMCR%29%20is%20a%20recent%20multi-view%0Aself-supervised%20learning%20%28MVSSL%29%20method%20that%20matches%20or%20surpasses%20other%20leading%0AMVSSL%20methods.%20MMCR%20is%20intriguing%20because%20it%20does%20not%20fit%20neatly%20into%20any%20of%0Athe%20commonplace%20MVSSL%20lineages%2C%20instead%20originating%20from%20a%20statistical%0Amechanical%20perspective%20on%20the%20linear%20separability%20of%20data%20manifolds.%20In%20this%0Apaper%2C%20we%20seek%20to%20improve%20our%20understanding%20and%20our%20utilization%20of%20MMCR.%20To%0Abetter%20understand%20MMCR%2C%20we%20leverage%20tools%20from%20high%20dimensional%20probability%20to%0Ademonstrate%20that%20MMCR%20incentivizes%20alignment%20and%20uniformity%20of%20learned%0Aembeddings.%20We%20then%20leverage%20tools%20from%20information%20theory%20to%20show%20that%20such%0Aembeddings%20maximize%20a%20well-known%20lower%20bound%20on%20mutual%20information%20between%0Aviews%2C%20thereby%20connecting%20the%20geometric%20perspective%20of%20MMCR%20to%20the%0Ainformation-theoretic%20perspective%20commonly%20discussed%20in%20MVSSL.%20To%20better%0Autilize%20MMCR%2C%20we%20mathematically%20predict%20and%20experimentally%20confirm%0Anon-monotonic%20changes%20in%20the%20pretraining%20loss%20akin%20to%20double%20descent%20but%20with%0Arespect%20to%20atypical%20hyperparameters.%20We%20also%20discover%20compute%20scaling%20laws%20that%0Aenable%20predicting%20the%20pretraining%20loss%20as%20a%20function%20of%20gradients%20steps%2C%20batch%0Asize%2C%20embedding%20dimension%20and%20number%20of%20views.%20We%20then%20show%20that%20MMCR%2C%0Aoriginally%20applied%20to%20image%20data%2C%20is%20performant%20on%20multimodal%20image-text%20data.%0ABy%20more%20deeply%20understanding%20the%20theoretical%20and%20empirical%20behavior%20of%20MMCR%2C%0Aour%20work%20reveals%20insights%20on%20improving%20MVSSL%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09366v1&entry.124074799=Read"},
{"title": "Adaptive Temporal Motion Guided Graph Convolution Network for\n  Micro-expression Recognition", "author": "Fengyuan Zhang and Zhaopei Huang and Xinjie Zhang and Qin Jin", "abstract": "  Micro-expressions serve as essential cues for understanding individuals'\ngenuine emotional states. Recognizing micro-expressions attracts increasing\nresearch attention due to its various applications in fields such as business\nnegotiation and psychotherapy. However, the intricate and transient nature of\nmicro-expressions poses a significant challenge to their accurate recognition.\nMost existing works either neglect temporal dependencies or suffer from\nredundancy issues in clip-level recognition. In this work, we propose a novel\nframework for micro-expression recognition, named the Adaptive Temporal Motion\nGuided Graph Convolution Network (ATM-GCN). Our framework excels at capturing\ntemporal dependencies between frames across the entire clip, thereby enhancing\nmicro-expression recognition at the clip level. Specifically, the integration\nof Adaptive Temporal Motion layers empowers our method to aggregate global and\nlocal motion features inherent in micro-expressions. Experimental results\ndemonstrate that ATM-GCN not only surpasses existing state-of-the-art methods,\nparticularly on the Composite dataset, but also achieves superior performance\non the latest micro-expression dataset CAS(ME)$^3$.\n", "link": "http://arxiv.org/abs/2406.08997v1", "date": "2024-06-13", "relevancy": 2.0922, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Temporal%20Motion%20Guided%20Graph%20Convolution%20Network%20for%0A%20%20Micro-expression%20Recognition&body=Title%3A%20Adaptive%20Temporal%20Motion%20Guided%20Graph%20Convolution%20Network%20for%0A%20%20Micro-expression%20Recognition%0AAuthor%3A%20Fengyuan%20Zhang%20and%20Zhaopei%20Huang%20and%20Xinjie%20Zhang%20and%20Qin%20Jin%0AAbstract%3A%20%20%20Micro-expressions%20serve%20as%20essential%20cues%20for%20understanding%20individuals%27%0Agenuine%20emotional%20states.%20Recognizing%20micro-expressions%20attracts%20increasing%0Aresearch%20attention%20due%20to%20its%20various%20applications%20in%20fields%20such%20as%20business%0Anegotiation%20and%20psychotherapy.%20However%2C%20the%20intricate%20and%20transient%20nature%20of%0Amicro-expressions%20poses%20a%20significant%20challenge%20to%20their%20accurate%20recognition.%0AMost%20existing%20works%20either%20neglect%20temporal%20dependencies%20or%20suffer%20from%0Aredundancy%20issues%20in%20clip-level%20recognition.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aframework%20for%20micro-expression%20recognition%2C%20named%20the%20Adaptive%20Temporal%20Motion%0AGuided%20Graph%20Convolution%20Network%20%28ATM-GCN%29.%20Our%20framework%20excels%20at%20capturing%0Atemporal%20dependencies%20between%20frames%20across%20the%20entire%20clip%2C%20thereby%20enhancing%0Amicro-expression%20recognition%20at%20the%20clip%20level.%20Specifically%2C%20the%20integration%0Aof%20Adaptive%20Temporal%20Motion%20layers%20empowers%20our%20method%20to%20aggregate%20global%20and%0Alocal%20motion%20features%20inherent%20in%20micro-expressions.%20Experimental%20results%0Ademonstrate%20that%20ATM-GCN%20not%20only%20surpasses%20existing%20state-of-the-art%20methods%2C%0Aparticularly%20on%20the%20Composite%20dataset%2C%20but%20also%20achieves%20superior%20performance%0Aon%20the%20latest%20micro-expression%20dataset%20CAS%28ME%29%24%5E3%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Temporal%2520Motion%2520Guided%2520Graph%2520Convolution%2520Network%2520for%250A%2520%2520Micro-expression%2520Recognition%26entry.906535625%3DFengyuan%2520Zhang%2520and%2520Zhaopei%2520Huang%2520and%2520Xinjie%2520Zhang%2520and%2520Qin%2520Jin%26entry.1292438233%3D%2520%2520Micro-expressions%2520serve%2520as%2520essential%2520cues%2520for%2520understanding%2520individuals%2527%250Agenuine%2520emotional%2520states.%2520Recognizing%2520micro-expressions%2520attracts%2520increasing%250Aresearch%2520attention%2520due%2520to%2520its%2520various%2520applications%2520in%2520fields%2520such%2520as%2520business%250Anegotiation%2520and%2520psychotherapy.%2520However%252C%2520the%2520intricate%2520and%2520transient%2520nature%2520of%250Amicro-expressions%2520poses%2520a%2520significant%2520challenge%2520to%2520their%2520accurate%2520recognition.%250AMost%2520existing%2520works%2520either%2520neglect%2520temporal%2520dependencies%2520or%2520suffer%2520from%250Aredundancy%2520issues%2520in%2520clip-level%2520recognition.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520for%2520micro-expression%2520recognition%252C%2520named%2520the%2520Adaptive%2520Temporal%2520Motion%250AGuided%2520Graph%2520Convolution%2520Network%2520%2528ATM-GCN%2529.%2520Our%2520framework%2520excels%2520at%2520capturing%250Atemporal%2520dependencies%2520between%2520frames%2520across%2520the%2520entire%2520clip%252C%2520thereby%2520enhancing%250Amicro-expression%2520recognition%2520at%2520the%2520clip%2520level.%2520Specifically%252C%2520the%2520integration%250Aof%2520Adaptive%2520Temporal%2520Motion%2520layers%2520empowers%2520our%2520method%2520to%2520aggregate%2520global%2520and%250Alocal%2520motion%2520features%2520inherent%2520in%2520micro-expressions.%2520Experimental%2520results%250Ademonstrate%2520that%2520ATM-GCN%2520not%2520only%2520surpasses%2520existing%2520state-of-the-art%2520methods%252C%250Aparticularly%2520on%2520the%2520Composite%2520dataset%252C%2520but%2520also%2520achieves%2520superior%2520performance%250Aon%2520the%2520latest%2520micro-expression%2520dataset%2520CAS%2528ME%2529%2524%255E3%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Temporal%20Motion%20Guided%20Graph%20Convolution%20Network%20for%0A%20%20Micro-expression%20Recognition&entry.906535625=Fengyuan%20Zhang%20and%20Zhaopei%20Huang%20and%20Xinjie%20Zhang%20and%20Qin%20Jin&entry.1292438233=%20%20Micro-expressions%20serve%20as%20essential%20cues%20for%20understanding%20individuals%27%0Agenuine%20emotional%20states.%20Recognizing%20micro-expressions%20attracts%20increasing%0Aresearch%20attention%20due%20to%20its%20various%20applications%20in%20fields%20such%20as%20business%0Anegotiation%20and%20psychotherapy.%20However%2C%20the%20intricate%20and%20transient%20nature%20of%0Amicro-expressions%20poses%20a%20significant%20challenge%20to%20their%20accurate%20recognition.%0AMost%20existing%20works%20either%20neglect%20temporal%20dependencies%20or%20suffer%20from%0Aredundancy%20issues%20in%20clip-level%20recognition.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aframework%20for%20micro-expression%20recognition%2C%20named%20the%20Adaptive%20Temporal%20Motion%0AGuided%20Graph%20Convolution%20Network%20%28ATM-GCN%29.%20Our%20framework%20excels%20at%20capturing%0Atemporal%20dependencies%20between%20frames%20across%20the%20entire%20clip%2C%20thereby%20enhancing%0Amicro-expression%20recognition%20at%20the%20clip%20level.%20Specifically%2C%20the%20integration%0Aof%20Adaptive%20Temporal%20Motion%20layers%20empowers%20our%20method%20to%20aggregate%20global%20and%0Alocal%20motion%20features%20inherent%20in%20micro-expressions.%20Experimental%20results%0Ademonstrate%20that%20ATM-GCN%20not%20only%20surpasses%20existing%20state-of-the-art%20methods%2C%0Aparticularly%20on%20the%20Composite%20dataset%2C%20but%20also%20achieves%20superior%20performance%0Aon%20the%20latest%20micro-expression%20dataset%20CAS%28ME%29%24%5E3%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08997v1&entry.124074799=Read"},
{"title": "You Don't Need Data-Augmentation in Self-Supervised Learning", "author": "Th\u00e9o Moutakanni and Maxime Oquab and Marc Szafraniec and Maria Vakalopoulou and Piotr Bojanowski", "abstract": "  Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has\nled to outstanding performances. All instantiations of this paradigm were\ntrained using strong and well-established hand-crafted data augmentations,\nleading to the general belief that they are required for the proper training\nand performance of such models. On the other hand, generative\nreconstruction-based models such as BEIT and MAE or Joint-Embedding Predictive\nArchitectures such as I-JEPA have shown strong performance without using data\naugmentations except masking. In this work, we challenge the importance of\ninvariance and data-augmentation in JEAs at scale. By running a case-study on a\nrecent SSL foundation model - DINOv2 - we show that strong image\nrepresentations can be obtained with JEAs and only cropping without resizing\nprovided the training data is large enough, reaching state-of-the-art results\nand using the least amount of augmentation in the literature. Through this\nstudy, we also discuss the impact of compute constraints on the outcomes of\nexperimental deep learning research, showing that they can lead to very\ndifferent conclusions.\n", "link": "http://arxiv.org/abs/2406.09294v1", "date": "2024-06-13", "relevancy": 2.0919, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5381}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5187}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Don%27t%20Need%20Data-Augmentation%20in%20Self-Supervised%20Learning&body=Title%3A%20You%20Don%27t%20Need%20Data-Augmentation%20in%20Self-Supervised%20Learning%0AAuthor%3A%20Th%C3%A9o%20Moutakanni%20and%20Maxime%20Oquab%20and%20Marc%20Szafraniec%20and%20Maria%20Vakalopoulou%20and%20Piotr%20Bojanowski%0AAbstract%3A%20%20%20Self-Supervised%20learning%20%28SSL%29%20with%20Joint-Embedding%20Architectures%20%28JEA%29%20has%0Aled%20to%20outstanding%20performances.%20All%20instantiations%20of%20this%20paradigm%20were%0Atrained%20using%20strong%20and%20well-established%20hand-crafted%20data%20augmentations%2C%0Aleading%20to%20the%20general%20belief%20that%20they%20are%20required%20for%20the%20proper%20training%0Aand%20performance%20of%20such%20models.%20On%20the%20other%20hand%2C%20generative%0Areconstruction-based%20models%20such%20as%20BEIT%20and%20MAE%20or%20Joint-Embedding%20Predictive%0AArchitectures%20such%20as%20I-JEPA%20have%20shown%20strong%20performance%20without%20using%20data%0Aaugmentations%20except%20masking.%20In%20this%20work%2C%20we%20challenge%20the%20importance%20of%0Ainvariance%20and%20data-augmentation%20in%20JEAs%20at%20scale.%20By%20running%20a%20case-study%20on%20a%0Arecent%20SSL%20foundation%20model%20-%20DINOv2%20-%20we%20show%20that%20strong%20image%0Arepresentations%20can%20be%20obtained%20with%20JEAs%20and%20only%20cropping%20without%20resizing%0Aprovided%20the%20training%20data%20is%20large%20enough%2C%20reaching%20state-of-the-art%20results%0Aand%20using%20the%20least%20amount%20of%20augmentation%20in%20the%20literature.%20Through%20this%0Astudy%2C%20we%20also%20discuss%20the%20impact%20of%20compute%20constraints%20on%20the%20outcomes%20of%0Aexperimental%20deep%20learning%20research%2C%20showing%20that%20they%20can%20lead%20to%20very%0Adifferent%20conclusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Don%2527t%2520Need%2520Data-Augmentation%2520in%2520Self-Supervised%2520Learning%26entry.906535625%3DTh%25C3%25A9o%2520Moutakanni%2520and%2520Maxime%2520Oquab%2520and%2520Marc%2520Szafraniec%2520and%2520Maria%2520Vakalopoulou%2520and%2520Piotr%2520Bojanowski%26entry.1292438233%3D%2520%2520Self-Supervised%2520learning%2520%2528SSL%2529%2520with%2520Joint-Embedding%2520Architectures%2520%2528JEA%2529%2520has%250Aled%2520to%2520outstanding%2520performances.%2520All%2520instantiations%2520of%2520this%2520paradigm%2520were%250Atrained%2520using%2520strong%2520and%2520well-established%2520hand-crafted%2520data%2520augmentations%252C%250Aleading%2520to%2520the%2520general%2520belief%2520that%2520they%2520are%2520required%2520for%2520the%2520proper%2520training%250Aand%2520performance%2520of%2520such%2520models.%2520On%2520the%2520other%2520hand%252C%2520generative%250Areconstruction-based%2520models%2520such%2520as%2520BEIT%2520and%2520MAE%2520or%2520Joint-Embedding%2520Predictive%250AArchitectures%2520such%2520as%2520I-JEPA%2520have%2520shown%2520strong%2520performance%2520without%2520using%2520data%250Aaugmentations%2520except%2520masking.%2520In%2520this%2520work%252C%2520we%2520challenge%2520the%2520importance%2520of%250Ainvariance%2520and%2520data-augmentation%2520in%2520JEAs%2520at%2520scale.%2520By%2520running%2520a%2520case-study%2520on%2520a%250Arecent%2520SSL%2520foundation%2520model%2520-%2520DINOv2%2520-%2520we%2520show%2520that%2520strong%2520image%250Arepresentations%2520can%2520be%2520obtained%2520with%2520JEAs%2520and%2520only%2520cropping%2520without%2520resizing%250Aprovided%2520the%2520training%2520data%2520is%2520large%2520enough%252C%2520reaching%2520state-of-the-art%2520results%250Aand%2520using%2520the%2520least%2520amount%2520of%2520augmentation%2520in%2520the%2520literature.%2520Through%2520this%250Astudy%252C%2520we%2520also%2520discuss%2520the%2520impact%2520of%2520compute%2520constraints%2520on%2520the%2520outcomes%2520of%250Aexperimental%2520deep%2520learning%2520research%252C%2520showing%2520that%2520they%2520can%2520lead%2520to%2520very%250Adifferent%2520conclusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Don%27t%20Need%20Data-Augmentation%20in%20Self-Supervised%20Learning&entry.906535625=Th%C3%A9o%20Moutakanni%20and%20Maxime%20Oquab%20and%20Marc%20Szafraniec%20and%20Maria%20Vakalopoulou%20and%20Piotr%20Bojanowski&entry.1292438233=%20%20Self-Supervised%20learning%20%28SSL%29%20with%20Joint-Embedding%20Architectures%20%28JEA%29%20has%0Aled%20to%20outstanding%20performances.%20All%20instantiations%20of%20this%20paradigm%20were%0Atrained%20using%20strong%20and%20well-established%20hand-crafted%20data%20augmentations%2C%0Aleading%20to%20the%20general%20belief%20that%20they%20are%20required%20for%20the%20proper%20training%0Aand%20performance%20of%20such%20models.%20On%20the%20other%20hand%2C%20generative%0Areconstruction-based%20models%20such%20as%20BEIT%20and%20MAE%20or%20Joint-Embedding%20Predictive%0AArchitectures%20such%20as%20I-JEPA%20have%20shown%20strong%20performance%20without%20using%20data%0Aaugmentations%20except%20masking.%20In%20this%20work%2C%20we%20challenge%20the%20importance%20of%0Ainvariance%20and%20data-augmentation%20in%20JEAs%20at%20scale.%20By%20running%20a%20case-study%20on%20a%0Arecent%20SSL%20foundation%20model%20-%20DINOv2%20-%20we%20show%20that%20strong%20image%0Arepresentations%20can%20be%20obtained%20with%20JEAs%20and%20only%20cropping%20without%20resizing%0Aprovided%20the%20training%20data%20is%20large%20enough%2C%20reaching%20state-of-the-art%20results%0Aand%20using%20the%20least%20amount%20of%20augmentation%20in%20the%20literature.%20Through%20this%0Astudy%2C%20we%20also%20discuss%20the%20impact%20of%20compute%20constraints%20on%20the%20outcomes%20of%0Aexperimental%20deep%20learning%20research%2C%20showing%20that%20they%20can%20lead%20to%20very%0Adifferent%20conclusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09294v1&entry.124074799=Read"},
{"title": "Schur's Positive-Definite Network: Deep Learning in the SPD cone with\n  structure", "author": "Can Pouliquen and Mathurin Massias and Titouan Vayer", "abstract": "  Estimating matrices in the symmetric positive-definite (SPD) cone is of\ninterest for many applications ranging from computer vision to graph learning.\nWhile there exist various convex optimization-based estimators, they remain\nlimited in expressivity due to their model-based approach. The success of deep\nlearning has thus led many to use neural networks to learn to estimate SPD\nmatrices in a data-driven fashion. For learning structured outputs, one\npromising strategy involves architectures designed by unrolling iterative\nalgorithms, which potentially benefit from inductive bias properties. However,\ndesigning correct unrolled architectures for SPD learning is difficult: they\neither do not guarantee that their output has all the desired properties, rely\non heavy computations, or are overly restrained to specific matrices which\nhinders their expressivity. In this paper, we propose a novel and generic\nlearning module with guaranteed SPD outputs called SpodNet, that also enables\nlearning a larger class of functions than existing approaches. Notably, it\nsolves the challenging task of learning jointly SPD and sparse matrices. Our\nexperiments demonstrate the versatility of SpodNet layers.\n", "link": "http://arxiv.org/abs/2406.09023v1", "date": "2024-06-13", "relevancy": 2.0867, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5233}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5219}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Schur%27s%20Positive-Definite%20Network%3A%20Deep%20Learning%20in%20the%20SPD%20cone%20with%0A%20%20structure&body=Title%3A%20Schur%27s%20Positive-Definite%20Network%3A%20Deep%20Learning%20in%20the%20SPD%20cone%20with%0A%20%20structure%0AAuthor%3A%20Can%20Pouliquen%20and%20Mathurin%20Massias%20and%20Titouan%20Vayer%0AAbstract%3A%20%20%20Estimating%20matrices%20in%20the%20symmetric%20positive-definite%20%28SPD%29%20cone%20is%20of%0Ainterest%20for%20many%20applications%20ranging%20from%20computer%20vision%20to%20graph%20learning.%0AWhile%20there%20exist%20various%20convex%20optimization-based%20estimators%2C%20they%20remain%0Alimited%20in%20expressivity%20due%20to%20their%20model-based%20approach.%20The%20success%20of%20deep%0Alearning%20has%20thus%20led%20many%20to%20use%20neural%20networks%20to%20learn%20to%20estimate%20SPD%0Amatrices%20in%20a%20data-driven%20fashion.%20For%20learning%20structured%20outputs%2C%20one%0Apromising%20strategy%20involves%20architectures%20designed%20by%20unrolling%20iterative%0Aalgorithms%2C%20which%20potentially%20benefit%20from%20inductive%20bias%20properties.%20However%2C%0Adesigning%20correct%20unrolled%20architectures%20for%20SPD%20learning%20is%20difficult%3A%20they%0Aeither%20do%20not%20guarantee%20that%20their%20output%20has%20all%20the%20desired%20properties%2C%20rely%0Aon%20heavy%20computations%2C%20or%20are%20overly%20restrained%20to%20specific%20matrices%20which%0Ahinders%20their%20expressivity.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20generic%0Alearning%20module%20with%20guaranteed%20SPD%20outputs%20called%20SpodNet%2C%20that%20also%20enables%0Alearning%20a%20larger%20class%20of%20functions%20than%20existing%20approaches.%20Notably%2C%20it%0Asolves%20the%20challenging%20task%20of%20learning%20jointly%20SPD%20and%20sparse%20matrices.%20Our%0Aexperiments%20demonstrate%20the%20versatility%20of%20SpodNet%20layers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSchur%2527s%2520Positive-Definite%2520Network%253A%2520Deep%2520Learning%2520in%2520the%2520SPD%2520cone%2520with%250A%2520%2520structure%26entry.906535625%3DCan%2520Pouliquen%2520and%2520Mathurin%2520Massias%2520and%2520Titouan%2520Vayer%26entry.1292438233%3D%2520%2520Estimating%2520matrices%2520in%2520the%2520symmetric%2520positive-definite%2520%2528SPD%2529%2520cone%2520is%2520of%250Ainterest%2520for%2520many%2520applications%2520ranging%2520from%2520computer%2520vision%2520to%2520graph%2520learning.%250AWhile%2520there%2520exist%2520various%2520convex%2520optimization-based%2520estimators%252C%2520they%2520remain%250Alimited%2520in%2520expressivity%2520due%2520to%2520their%2520model-based%2520approach.%2520The%2520success%2520of%2520deep%250Alearning%2520has%2520thus%2520led%2520many%2520to%2520use%2520neural%2520networks%2520to%2520learn%2520to%2520estimate%2520SPD%250Amatrices%2520in%2520a%2520data-driven%2520fashion.%2520For%2520learning%2520structured%2520outputs%252C%2520one%250Apromising%2520strategy%2520involves%2520architectures%2520designed%2520by%2520unrolling%2520iterative%250Aalgorithms%252C%2520which%2520potentially%2520benefit%2520from%2520inductive%2520bias%2520properties.%2520However%252C%250Adesigning%2520correct%2520unrolled%2520architectures%2520for%2520SPD%2520learning%2520is%2520difficult%253A%2520they%250Aeither%2520do%2520not%2520guarantee%2520that%2520their%2520output%2520has%2520all%2520the%2520desired%2520properties%252C%2520rely%250Aon%2520heavy%2520computations%252C%2520or%2520are%2520overly%2520restrained%2520to%2520specific%2520matrices%2520which%250Ahinders%2520their%2520expressivity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520and%2520generic%250Alearning%2520module%2520with%2520guaranteed%2520SPD%2520outputs%2520called%2520SpodNet%252C%2520that%2520also%2520enables%250Alearning%2520a%2520larger%2520class%2520of%2520functions%2520than%2520existing%2520approaches.%2520Notably%252C%2520it%250Asolves%2520the%2520challenging%2520task%2520of%2520learning%2520jointly%2520SPD%2520and%2520sparse%2520matrices.%2520Our%250Aexperiments%2520demonstrate%2520the%2520versatility%2520of%2520SpodNet%2520layers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Schur%27s%20Positive-Definite%20Network%3A%20Deep%20Learning%20in%20the%20SPD%20cone%20with%0A%20%20structure&entry.906535625=Can%20Pouliquen%20and%20Mathurin%20Massias%20and%20Titouan%20Vayer&entry.1292438233=%20%20Estimating%20matrices%20in%20the%20symmetric%20positive-definite%20%28SPD%29%20cone%20is%20of%0Ainterest%20for%20many%20applications%20ranging%20from%20computer%20vision%20to%20graph%20learning.%0AWhile%20there%20exist%20various%20convex%20optimization-based%20estimators%2C%20they%20remain%0Alimited%20in%20expressivity%20due%20to%20their%20model-based%20approach.%20The%20success%20of%20deep%0Alearning%20has%20thus%20led%20many%20to%20use%20neural%20networks%20to%20learn%20to%20estimate%20SPD%0Amatrices%20in%20a%20data-driven%20fashion.%20For%20learning%20structured%20outputs%2C%20one%0Apromising%20strategy%20involves%20architectures%20designed%20by%20unrolling%20iterative%0Aalgorithms%2C%20which%20potentially%20benefit%20from%20inductive%20bias%20properties.%20However%2C%0Adesigning%20correct%20unrolled%20architectures%20for%20SPD%20learning%20is%20difficult%3A%20they%0Aeither%20do%20not%20guarantee%20that%20their%20output%20has%20all%20the%20desired%20properties%2C%20rely%0Aon%20heavy%20computations%2C%20or%20are%20overly%20restrained%20to%20specific%20matrices%20which%0Ahinders%20their%20expressivity.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20generic%0Alearning%20module%20with%20guaranteed%20SPD%20outputs%20called%20SpodNet%2C%20that%20also%20enables%0Alearning%20a%20larger%20class%20of%20functions%20than%20existing%20approaches.%20Notably%2C%20it%0Asolves%20the%20challenging%20task%20of%20learning%20jointly%20SPD%20and%20sparse%20matrices.%20Our%0Aexperiments%20demonstrate%20the%20versatility%20of%20SpodNet%20layers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09023v1&entry.124074799=Read"},
{"title": "Generative AI-based Prompt Evolution Engineering Design Optimization\n  With Vision-Language Model", "author": "Melvin Wong and Thiago Rios and Stefan Menzel and Yew Soon Ong", "abstract": "  Engineering design optimization requires an efficient combination of a 3D\nshape representation, an optimization algorithm, and a design performance\nevaluation method, which is often computationally expensive. We present a\nprompt evolution design optimization (PEDO) framework contextualized in a\nvehicle design scenario that leverages a vision-language model for penalizing\nimpractical car designs synthesized by a generative model. The backbone of our\nframework is an evolutionary strategy coupled with an optimization objective\nfunction that comprises a physics-based solver and a vision-language model for\npractical or functional guidance in the generated car designs. In the prompt\nevolutionary search, the optimizer iteratively generates a population of text\nprompts, which embed user specifications on the aerodynamic performance and\nvisual preferences of the 3D car designs. Then, in addition to the\ncomputational fluid dynamics simulations, the pre-trained vision-language model\nis used to penalize impractical designs and, thus, foster the evolutionary\nalgorithm to seek more viable designs. Our investigations on a car design\noptimization problem show a wide spread of potential car designs generated at\nthe early phase of the search, which indicates a good diversity of designs in\nthe initial populations, and an increase of over 20\\% in the probability of\ngenerating practical designs compared to a baseline framework without using a\nvision-language model. Visual inspection of the designs against the performance\nresults demonstrates prompt evolution as a very promising paradigm for finding\nnovel designs with good optimization performance while providing ease of use in\nspecifying design specifications and preferences via a natural language\ninterface.\n", "link": "http://arxiv.org/abs/2406.09143v1", "date": "2024-06-13", "relevancy": 2.0855, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5358}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5178}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI-based%20Prompt%20Evolution%20Engineering%20Design%20Optimization%0A%20%20With%20Vision-Language%20Model&body=Title%3A%20Generative%20AI-based%20Prompt%20Evolution%20Engineering%20Design%20Optimization%0A%20%20With%20Vision-Language%20Model%0AAuthor%3A%20Melvin%20Wong%20and%20Thiago%20Rios%20and%20Stefan%20Menzel%20and%20Yew%20Soon%20Ong%0AAbstract%3A%20%20%20Engineering%20design%20optimization%20requires%20an%20efficient%20combination%20of%20a%203D%0Ashape%20representation%2C%20an%20optimization%20algorithm%2C%20and%20a%20design%20performance%0Aevaluation%20method%2C%20which%20is%20often%20computationally%20expensive.%20We%20present%20a%0Aprompt%20evolution%20design%20optimization%20%28PEDO%29%20framework%20contextualized%20in%20a%0Avehicle%20design%20scenario%20that%20leverages%20a%20vision-language%20model%20for%20penalizing%0Aimpractical%20car%20designs%20synthesized%20by%20a%20generative%20model.%20The%20backbone%20of%20our%0Aframework%20is%20an%20evolutionary%20strategy%20coupled%20with%20an%20optimization%20objective%0Afunction%20that%20comprises%20a%20physics-based%20solver%20and%20a%20vision-language%20model%20for%0Apractical%20or%20functional%20guidance%20in%20the%20generated%20car%20designs.%20In%20the%20prompt%0Aevolutionary%20search%2C%20the%20optimizer%20iteratively%20generates%20a%20population%20of%20text%0Aprompts%2C%20which%20embed%20user%20specifications%20on%20the%20aerodynamic%20performance%20and%0Avisual%20preferences%20of%20the%203D%20car%20designs.%20Then%2C%20in%20addition%20to%20the%0Acomputational%20fluid%20dynamics%20simulations%2C%20the%20pre-trained%20vision-language%20model%0Ais%20used%20to%20penalize%20impractical%20designs%20and%2C%20thus%2C%20foster%20the%20evolutionary%0Aalgorithm%20to%20seek%20more%20viable%20designs.%20Our%20investigations%20on%20a%20car%20design%0Aoptimization%20problem%20show%20a%20wide%20spread%20of%20potential%20car%20designs%20generated%20at%0Athe%20early%20phase%20of%20the%20search%2C%20which%20indicates%20a%20good%20diversity%20of%20designs%20in%0Athe%20initial%20populations%2C%20and%20an%20increase%20of%20over%2020%5C%25%20in%20the%20probability%20of%0Agenerating%20practical%20designs%20compared%20to%20a%20baseline%20framework%20without%20using%20a%0Avision-language%20model.%20Visual%20inspection%20of%20the%20designs%20against%20the%20performance%0Aresults%20demonstrates%20prompt%20evolution%20as%20a%20very%20promising%20paradigm%20for%20finding%0Anovel%20designs%20with%20good%20optimization%20performance%20while%20providing%20ease%20of%20use%20in%0Aspecifying%20design%20specifications%20and%20preferences%20via%20a%20natural%20language%0Ainterface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI-based%2520Prompt%2520Evolution%2520Engineering%2520Design%2520Optimization%250A%2520%2520With%2520Vision-Language%2520Model%26entry.906535625%3DMelvin%2520Wong%2520and%2520Thiago%2520Rios%2520and%2520Stefan%2520Menzel%2520and%2520Yew%2520Soon%2520Ong%26entry.1292438233%3D%2520%2520Engineering%2520design%2520optimization%2520requires%2520an%2520efficient%2520combination%2520of%2520a%25203D%250Ashape%2520representation%252C%2520an%2520optimization%2520algorithm%252C%2520and%2520a%2520design%2520performance%250Aevaluation%2520method%252C%2520which%2520is%2520often%2520computationally%2520expensive.%2520We%2520present%2520a%250Aprompt%2520evolution%2520design%2520optimization%2520%2528PEDO%2529%2520framework%2520contextualized%2520in%2520a%250Avehicle%2520design%2520scenario%2520that%2520leverages%2520a%2520vision-language%2520model%2520for%2520penalizing%250Aimpractical%2520car%2520designs%2520synthesized%2520by%2520a%2520generative%2520model.%2520The%2520backbone%2520of%2520our%250Aframework%2520is%2520an%2520evolutionary%2520strategy%2520coupled%2520with%2520an%2520optimization%2520objective%250Afunction%2520that%2520comprises%2520a%2520physics-based%2520solver%2520and%2520a%2520vision-language%2520model%2520for%250Apractical%2520or%2520functional%2520guidance%2520in%2520the%2520generated%2520car%2520designs.%2520In%2520the%2520prompt%250Aevolutionary%2520search%252C%2520the%2520optimizer%2520iteratively%2520generates%2520a%2520population%2520of%2520text%250Aprompts%252C%2520which%2520embed%2520user%2520specifications%2520on%2520the%2520aerodynamic%2520performance%2520and%250Avisual%2520preferences%2520of%2520the%25203D%2520car%2520designs.%2520Then%252C%2520in%2520addition%2520to%2520the%250Acomputational%2520fluid%2520dynamics%2520simulations%252C%2520the%2520pre-trained%2520vision-language%2520model%250Ais%2520used%2520to%2520penalize%2520impractical%2520designs%2520and%252C%2520thus%252C%2520foster%2520the%2520evolutionary%250Aalgorithm%2520to%2520seek%2520more%2520viable%2520designs.%2520Our%2520investigations%2520on%2520a%2520car%2520design%250Aoptimization%2520problem%2520show%2520a%2520wide%2520spread%2520of%2520potential%2520car%2520designs%2520generated%2520at%250Athe%2520early%2520phase%2520of%2520the%2520search%252C%2520which%2520indicates%2520a%2520good%2520diversity%2520of%2520designs%2520in%250Athe%2520initial%2520populations%252C%2520and%2520an%2520increase%2520of%2520over%252020%255C%2525%2520in%2520the%2520probability%2520of%250Agenerating%2520practical%2520designs%2520compared%2520to%2520a%2520baseline%2520framework%2520without%2520using%2520a%250Avision-language%2520model.%2520Visual%2520inspection%2520of%2520the%2520designs%2520against%2520the%2520performance%250Aresults%2520demonstrates%2520prompt%2520evolution%2520as%2520a%2520very%2520promising%2520paradigm%2520for%2520finding%250Anovel%2520designs%2520with%2520good%2520optimization%2520performance%2520while%2520providing%2520ease%2520of%2520use%2520in%250Aspecifying%2520design%2520specifications%2520and%2520preferences%2520via%2520a%2520natural%2520language%250Ainterface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI-based%20Prompt%20Evolution%20Engineering%20Design%20Optimization%0A%20%20With%20Vision-Language%20Model&entry.906535625=Melvin%20Wong%20and%20Thiago%20Rios%20and%20Stefan%20Menzel%20and%20Yew%20Soon%20Ong&entry.1292438233=%20%20Engineering%20design%20optimization%20requires%20an%20efficient%20combination%20of%20a%203D%0Ashape%20representation%2C%20an%20optimization%20algorithm%2C%20and%20a%20design%20performance%0Aevaluation%20method%2C%20which%20is%20often%20computationally%20expensive.%20We%20present%20a%0Aprompt%20evolution%20design%20optimization%20%28PEDO%29%20framework%20contextualized%20in%20a%0Avehicle%20design%20scenario%20that%20leverages%20a%20vision-language%20model%20for%20penalizing%0Aimpractical%20car%20designs%20synthesized%20by%20a%20generative%20model.%20The%20backbone%20of%20our%0Aframework%20is%20an%20evolutionary%20strategy%20coupled%20with%20an%20optimization%20objective%0Afunction%20that%20comprises%20a%20physics-based%20solver%20and%20a%20vision-language%20model%20for%0Apractical%20or%20functional%20guidance%20in%20the%20generated%20car%20designs.%20In%20the%20prompt%0Aevolutionary%20search%2C%20the%20optimizer%20iteratively%20generates%20a%20population%20of%20text%0Aprompts%2C%20which%20embed%20user%20specifications%20on%20the%20aerodynamic%20performance%20and%0Avisual%20preferences%20of%20the%203D%20car%20designs.%20Then%2C%20in%20addition%20to%20the%0Acomputational%20fluid%20dynamics%20simulations%2C%20the%20pre-trained%20vision-language%20model%0Ais%20used%20to%20penalize%20impractical%20designs%20and%2C%20thus%2C%20foster%20the%20evolutionary%0Aalgorithm%20to%20seek%20more%20viable%20designs.%20Our%20investigations%20on%20a%20car%20design%0Aoptimization%20problem%20show%20a%20wide%20spread%20of%20potential%20car%20designs%20generated%20at%0Athe%20early%20phase%20of%20the%20search%2C%20which%20indicates%20a%20good%20diversity%20of%20designs%20in%0Athe%20initial%20populations%2C%20and%20an%20increase%20of%20over%2020%5C%25%20in%20the%20probability%20of%0Agenerating%20practical%20designs%20compared%20to%20a%20baseline%20framework%20without%20using%20a%0Avision-language%20model.%20Visual%20inspection%20of%20the%20designs%20against%20the%20performance%0Aresults%20demonstrates%20prompt%20evolution%20as%20a%20very%20promising%20paradigm%20for%20finding%0Anovel%20designs%20with%20good%20optimization%20performance%20while%20providing%20ease%20of%20use%20in%0Aspecifying%20design%20specifications%20and%20preferences%20via%20a%20natural%20language%0Ainterface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09143v1&entry.124074799=Read"},
{"title": "Dodo: Dynamic Contextual Compression for Decoder-only LMs", "author": "Guanghui Qin and Corby Rosset and Ethan C. Chau and Nikhil Rao and Benjamin Van Durme", "abstract": "  Transformer-based language models (LMs) are inefficient in long contexts. We\npropose Dodo, a solution for context compression. Instead of one vector per\ntoken in a standard transformer model, Dodo represents text with a dynamic\nnumber of hidden states at each layer, reducing the cost of self-attention to a\nfraction of typical time and space. Moreover, off-the-shelf models such as\nLLaMA can be adapted to Dodo by efficient parameter tuning methods such as\nLoRA. In use, Dodo can act as either an autoregressive LM or a context\ncompressor for downstream tasks. We demonstrate through experiments in language\nmodeling, question answering, and summarization that Dodo retains capabilities\nin these tasks, while drastically reducing the overhead during decoding. For\nexample, in the autoencoding task, Dodo shrinks context at a 20x compression\nratio with a BLEU score of 98% for reconstruction, achieving nearly lossless\nencoding.\n", "link": "http://arxiv.org/abs/2310.02409v2", "date": "2024-06-13", "relevancy": 2.0762, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5346}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dodo%3A%20Dynamic%20Contextual%20Compression%20for%20Decoder-only%20LMs&body=Title%3A%20Dodo%3A%20Dynamic%20Contextual%20Compression%20for%20Decoder-only%20LMs%0AAuthor%3A%20Guanghui%20Qin%20and%20Corby%20Rosset%20and%20Ethan%20C.%20Chau%20and%20Nikhil%20Rao%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20Transformer-based%20language%20models%20%28LMs%29%20are%20inefficient%20in%20long%20contexts.%20We%0Apropose%20Dodo%2C%20a%20solution%20for%20context%20compression.%20Instead%20of%20one%20vector%20per%0Atoken%20in%20a%20standard%20transformer%20model%2C%20Dodo%20represents%20text%20with%20a%20dynamic%0Anumber%20of%20hidden%20states%20at%20each%20layer%2C%20reducing%20the%20cost%20of%20self-attention%20to%20a%0Afraction%20of%20typical%20time%20and%20space.%20Moreover%2C%20off-the-shelf%20models%20such%20as%0ALLaMA%20can%20be%20adapted%20to%20Dodo%20by%20efficient%20parameter%20tuning%20methods%20such%20as%0ALoRA.%20In%20use%2C%20Dodo%20can%20act%20as%20either%20an%20autoregressive%20LM%20or%20a%20context%0Acompressor%20for%20downstream%20tasks.%20We%20demonstrate%20through%20experiments%20in%20language%0Amodeling%2C%20question%20answering%2C%20and%20summarization%20that%20Dodo%20retains%20capabilities%0Ain%20these%20tasks%2C%20while%20drastically%20reducing%20the%20overhead%20during%20decoding.%20For%0Aexample%2C%20in%20the%20autoencoding%20task%2C%20Dodo%20shrinks%20context%20at%20a%2020x%20compression%0Aratio%20with%20a%20BLEU%20score%20of%2098%25%20for%20reconstruction%2C%20achieving%20nearly%20lossless%0Aencoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02409v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDodo%253A%2520Dynamic%2520Contextual%2520Compression%2520for%2520Decoder-only%2520LMs%26entry.906535625%3DGuanghui%2520Qin%2520and%2520Corby%2520Rosset%2520and%2520Ethan%2520C.%2520Chau%2520and%2520Nikhil%2520Rao%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520Transformer-based%2520language%2520models%2520%2528LMs%2529%2520are%2520inefficient%2520in%2520long%2520contexts.%2520We%250Apropose%2520Dodo%252C%2520a%2520solution%2520for%2520context%2520compression.%2520Instead%2520of%2520one%2520vector%2520per%250Atoken%2520in%2520a%2520standard%2520transformer%2520model%252C%2520Dodo%2520represents%2520text%2520with%2520a%2520dynamic%250Anumber%2520of%2520hidden%2520states%2520at%2520each%2520layer%252C%2520reducing%2520the%2520cost%2520of%2520self-attention%2520to%2520a%250Afraction%2520of%2520typical%2520time%2520and%2520space.%2520Moreover%252C%2520off-the-shelf%2520models%2520such%2520as%250ALLaMA%2520can%2520be%2520adapted%2520to%2520Dodo%2520by%2520efficient%2520parameter%2520tuning%2520methods%2520such%2520as%250ALoRA.%2520In%2520use%252C%2520Dodo%2520can%2520act%2520as%2520either%2520an%2520autoregressive%2520LM%2520or%2520a%2520context%250Acompressor%2520for%2520downstream%2520tasks.%2520We%2520demonstrate%2520through%2520experiments%2520in%2520language%250Amodeling%252C%2520question%2520answering%252C%2520and%2520summarization%2520that%2520Dodo%2520retains%2520capabilities%250Ain%2520these%2520tasks%252C%2520while%2520drastically%2520reducing%2520the%2520overhead%2520during%2520decoding.%2520For%250Aexample%252C%2520in%2520the%2520autoencoding%2520task%252C%2520Dodo%2520shrinks%2520context%2520at%2520a%252020x%2520compression%250Aratio%2520with%2520a%2520BLEU%2520score%2520of%252098%2525%2520for%2520reconstruction%252C%2520achieving%2520nearly%2520lossless%250Aencoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02409v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dodo%3A%20Dynamic%20Contextual%20Compression%20for%20Decoder-only%20LMs&entry.906535625=Guanghui%20Qin%20and%20Corby%20Rosset%20and%20Ethan%20C.%20Chau%20and%20Nikhil%20Rao%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20Transformer-based%20language%20models%20%28LMs%29%20are%20inefficient%20in%20long%20contexts.%20We%0Apropose%20Dodo%2C%20a%20solution%20for%20context%20compression.%20Instead%20of%20one%20vector%20per%0Atoken%20in%20a%20standard%20transformer%20model%2C%20Dodo%20represents%20text%20with%20a%20dynamic%0Anumber%20of%20hidden%20states%20at%20each%20layer%2C%20reducing%20the%20cost%20of%20self-attention%20to%20a%0Afraction%20of%20typical%20time%20and%20space.%20Moreover%2C%20off-the-shelf%20models%20such%20as%0ALLaMA%20can%20be%20adapted%20to%20Dodo%20by%20efficient%20parameter%20tuning%20methods%20such%20as%0ALoRA.%20In%20use%2C%20Dodo%20can%20act%20as%20either%20an%20autoregressive%20LM%20or%20a%20context%0Acompressor%20for%20downstream%20tasks.%20We%20demonstrate%20through%20experiments%20in%20language%0Amodeling%2C%20question%20answering%2C%20and%20summarization%20that%20Dodo%20retains%20capabilities%0Ain%20these%20tasks%2C%20while%20drastically%20reducing%20the%20overhead%20during%20decoding.%20For%0Aexample%2C%20in%20the%20autoencoding%20task%2C%20Dodo%20shrinks%20context%20at%20a%2020x%20compression%0Aratio%20with%20a%20BLEU%20score%20of%2098%25%20for%20reconstruction%2C%20achieving%20nearly%20lossless%0Aencoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02409v2&entry.124074799=Read"},
{"title": "Deep Sketched Output Kernel Regression for Structured Prediction", "author": "Tamim El Ahmad and Junjie Yang and Pierre Laforgue and Florence d'Alch\u00e9-Buc", "abstract": "  By leveraging the kernel trick in the output space, kernel-induced losses\nprovide a principled way to define structured output prediction tasks for a\nwide variety of output modalities. In particular, they have been successfully\nused in the context of surrogate non-parametric regression, where the kernel\ntrick is typically exploited in the input space as well. However, when inputs\nare images or texts, more expressive models such as deep neural networks seem\nmore suited than non-parametric methods. In this work, we tackle the question\nof how to train neural networks to solve structured output prediction tasks,\nwhile still benefiting from the versatility and relevance of kernel-induced\nlosses. We design a novel family of deep neural architectures, whose last layer\npredicts in a data-dependent finite-dimensional subspace of the\ninfinite-dimensional output feature space deriving from the kernel-induced\nloss. This subspace is chosen as the span of the eigenfunctions of a\nrandomly-approximated version of the empirical kernel covariance operator.\nInterestingly, this approach unlocks the use of gradient descent algorithms\n(and consequently of any neural architecture) for structured prediction.\nExperiments on synthetic tasks as well as real-world supervised graph\nprediction problems show the relevance of our method.\n", "link": "http://arxiv.org/abs/2406.09253v1", "date": "2024-06-13", "relevancy": 2.068, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5316}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Sketched%20Output%20Kernel%20Regression%20for%20Structured%20Prediction&body=Title%3A%20Deep%20Sketched%20Output%20Kernel%20Regression%20for%20Structured%20Prediction%0AAuthor%3A%20Tamim%20El%20Ahmad%20and%20Junjie%20Yang%20and%20Pierre%20Laforgue%20and%20Florence%20d%27Alch%C3%A9-Buc%0AAbstract%3A%20%20%20By%20leveraging%20the%20kernel%20trick%20in%20the%20output%20space%2C%20kernel-induced%20losses%0Aprovide%20a%20principled%20way%20to%20define%20structured%20output%20prediction%20tasks%20for%20a%0Awide%20variety%20of%20output%20modalities.%20In%20particular%2C%20they%20have%20been%20successfully%0Aused%20in%20the%20context%20of%20surrogate%20non-parametric%20regression%2C%20where%20the%20kernel%0Atrick%20is%20typically%20exploited%20in%20the%20input%20space%20as%20well.%20However%2C%20when%20inputs%0Aare%20images%20or%20texts%2C%20more%20expressive%20models%20such%20as%20deep%20neural%20networks%20seem%0Amore%20suited%20than%20non-parametric%20methods.%20In%20this%20work%2C%20we%20tackle%20the%20question%0Aof%20how%20to%20train%20neural%20networks%20to%20solve%20structured%20output%20prediction%20tasks%2C%0Awhile%20still%20benefiting%20from%20the%20versatility%20and%20relevance%20of%20kernel-induced%0Alosses.%20We%20design%20a%20novel%20family%20of%20deep%20neural%20architectures%2C%20whose%20last%20layer%0Apredicts%20in%20a%20data-dependent%20finite-dimensional%20subspace%20of%20the%0Ainfinite-dimensional%20output%20feature%20space%20deriving%20from%20the%20kernel-induced%0Aloss.%20This%20subspace%20is%20chosen%20as%20the%20span%20of%20the%20eigenfunctions%20of%20a%0Arandomly-approximated%20version%20of%20the%20empirical%20kernel%20covariance%20operator.%0AInterestingly%2C%20this%20approach%20unlocks%20the%20use%20of%20gradient%20descent%20algorithms%0A%28and%20consequently%20of%20any%20neural%20architecture%29%20for%20structured%20prediction.%0AExperiments%20on%20synthetic%20tasks%20as%20well%20as%20real-world%20supervised%20graph%0Aprediction%20problems%20show%20the%20relevance%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Sketched%2520Output%2520Kernel%2520Regression%2520for%2520Structured%2520Prediction%26entry.906535625%3DTamim%2520El%2520Ahmad%2520and%2520Junjie%2520Yang%2520and%2520Pierre%2520Laforgue%2520and%2520Florence%2520d%2527Alch%25C3%25A9-Buc%26entry.1292438233%3D%2520%2520By%2520leveraging%2520the%2520kernel%2520trick%2520in%2520the%2520output%2520space%252C%2520kernel-induced%2520losses%250Aprovide%2520a%2520principled%2520way%2520to%2520define%2520structured%2520output%2520prediction%2520tasks%2520for%2520a%250Awide%2520variety%2520of%2520output%2520modalities.%2520In%2520particular%252C%2520they%2520have%2520been%2520successfully%250Aused%2520in%2520the%2520context%2520of%2520surrogate%2520non-parametric%2520regression%252C%2520where%2520the%2520kernel%250Atrick%2520is%2520typically%2520exploited%2520in%2520the%2520input%2520space%2520as%2520well.%2520However%252C%2520when%2520inputs%250Aare%2520images%2520or%2520texts%252C%2520more%2520expressive%2520models%2520such%2520as%2520deep%2520neural%2520networks%2520seem%250Amore%2520suited%2520than%2520non-parametric%2520methods.%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520question%250Aof%2520how%2520to%2520train%2520neural%2520networks%2520to%2520solve%2520structured%2520output%2520prediction%2520tasks%252C%250Awhile%2520still%2520benefiting%2520from%2520the%2520versatility%2520and%2520relevance%2520of%2520kernel-induced%250Alosses.%2520We%2520design%2520a%2520novel%2520family%2520of%2520deep%2520neural%2520architectures%252C%2520whose%2520last%2520layer%250Apredicts%2520in%2520a%2520data-dependent%2520finite-dimensional%2520subspace%2520of%2520the%250Ainfinite-dimensional%2520output%2520feature%2520space%2520deriving%2520from%2520the%2520kernel-induced%250Aloss.%2520This%2520subspace%2520is%2520chosen%2520as%2520the%2520span%2520of%2520the%2520eigenfunctions%2520of%2520a%250Arandomly-approximated%2520version%2520of%2520the%2520empirical%2520kernel%2520covariance%2520operator.%250AInterestingly%252C%2520this%2520approach%2520unlocks%2520the%2520use%2520of%2520gradient%2520descent%2520algorithms%250A%2528and%2520consequently%2520of%2520any%2520neural%2520architecture%2529%2520for%2520structured%2520prediction.%250AExperiments%2520on%2520synthetic%2520tasks%2520as%2520well%2520as%2520real-world%2520supervised%2520graph%250Aprediction%2520problems%2520show%2520the%2520relevance%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Sketched%20Output%20Kernel%20Regression%20for%20Structured%20Prediction&entry.906535625=Tamim%20El%20Ahmad%20and%20Junjie%20Yang%20and%20Pierre%20Laforgue%20and%20Florence%20d%27Alch%C3%A9-Buc&entry.1292438233=%20%20By%20leveraging%20the%20kernel%20trick%20in%20the%20output%20space%2C%20kernel-induced%20losses%0Aprovide%20a%20principled%20way%20to%20define%20structured%20output%20prediction%20tasks%20for%20a%0Awide%20variety%20of%20output%20modalities.%20In%20particular%2C%20they%20have%20been%20successfully%0Aused%20in%20the%20context%20of%20surrogate%20non-parametric%20regression%2C%20where%20the%20kernel%0Atrick%20is%20typically%20exploited%20in%20the%20input%20space%20as%20well.%20However%2C%20when%20inputs%0Aare%20images%20or%20texts%2C%20more%20expressive%20models%20such%20as%20deep%20neural%20networks%20seem%0Amore%20suited%20than%20non-parametric%20methods.%20In%20this%20work%2C%20we%20tackle%20the%20question%0Aof%20how%20to%20train%20neural%20networks%20to%20solve%20structured%20output%20prediction%20tasks%2C%0Awhile%20still%20benefiting%20from%20the%20versatility%20and%20relevance%20of%20kernel-induced%0Alosses.%20We%20design%20a%20novel%20family%20of%20deep%20neural%20architectures%2C%20whose%20last%20layer%0Apredicts%20in%20a%20data-dependent%20finite-dimensional%20subspace%20of%20the%0Ainfinite-dimensional%20output%20feature%20space%20deriving%20from%20the%20kernel-induced%0Aloss.%20This%20subspace%20is%20chosen%20as%20the%20span%20of%20the%20eigenfunctions%20of%20a%0Arandomly-approximated%20version%20of%20the%20empirical%20kernel%20covariance%20operator.%0AInterestingly%2C%20this%20approach%20unlocks%20the%20use%20of%20gradient%20descent%20algorithms%0A%28and%20consequently%20of%20any%20neural%20architecture%29%20for%20structured%20prediction.%0AExperiments%20on%20synthetic%20tasks%20as%20well%20as%20real-world%20supervised%20graph%0Aprediction%20problems%20show%20the%20relevance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09253v1&entry.124074799=Read"},
{"title": "Effects of Multimodal Explanations for Autonomous Driving on Driving\n  Performance, Cognitive Load, Expertise, Confidence, and Trust", "author": "Robert Kaufman and Jean Costa and Everlyne Kimani", "abstract": "  Advances in autonomous driving provide an opportunity for AI-assisted driving\ninstruction that directly addresses the critical need for human driving\nimprovement. How should an AI instructor convey information to promote\nlearning? In a pre-post experiment (n = 41), we tested the impact of an AI\nCoach's explanatory communications modeled after performance driving expert\ninstructions. Participants were divided into four (4) groups to assess two (2)\ndimensions of the AI coach's explanations: information type ('what' and\n'why'-type explanations) and presentation modality (auditory and visual). We\ncompare how different explanatory techniques impact driving performance,\ncognitive load, confidence, expertise, and trust via observational learning.\nThrough interview, we delineate participant learning processes. Results show AI\ncoaching can effectively teach performance driving skills to novices. We find\nthe type and modality of information influences performance outcomes.\nDifferences in how successfully participants learned are attributed to how\ninformation directs attention, mitigates uncertainty, and influences overload\nexperienced by participants. Results suggest efficient, modality-appropriate\nexplanations should be opted for when designing effective HMI communications\nthat can instruct without overwhelming. Further, results support the need to\nalign communications with human learning and cognitive processes. We provide\neight design implications for future autonomous vehicle HMI and AI coach\ndesign.\n", "link": "http://arxiv.org/abs/2401.04206v4", "date": "2024-06-13", "relevancy": 2.0642, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5221}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5132}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effects%20of%20Multimodal%20Explanations%20for%20Autonomous%20Driving%20on%20Driving%0A%20%20Performance%2C%20Cognitive%20Load%2C%20Expertise%2C%20Confidence%2C%20and%20Trust&body=Title%3A%20Effects%20of%20Multimodal%20Explanations%20for%20Autonomous%20Driving%20on%20Driving%0A%20%20Performance%2C%20Cognitive%20Load%2C%20Expertise%2C%20Confidence%2C%20and%20Trust%0AAuthor%3A%20Robert%20Kaufman%20and%20Jean%20Costa%20and%20Everlyne%20Kimani%0AAbstract%3A%20%20%20Advances%20in%20autonomous%20driving%20provide%20an%20opportunity%20for%20AI-assisted%20driving%0Ainstruction%20that%20directly%20addresses%20the%20critical%20need%20for%20human%20driving%0Aimprovement.%20How%20should%20an%20AI%20instructor%20convey%20information%20to%20promote%0Alearning%3F%20In%20a%20pre-post%20experiment%20%28n%20%3D%2041%29%2C%20we%20tested%20the%20impact%20of%20an%20AI%0ACoach%27s%20explanatory%20communications%20modeled%20after%20performance%20driving%20expert%0Ainstructions.%20Participants%20were%20divided%20into%20four%20%284%29%20groups%20to%20assess%20two%20%282%29%0Adimensions%20of%20the%20AI%20coach%27s%20explanations%3A%20information%20type%20%28%27what%27%20and%0A%27why%27-type%20explanations%29%20and%20presentation%20modality%20%28auditory%20and%20visual%29.%20We%0Acompare%20how%20different%20explanatory%20techniques%20impact%20driving%20performance%2C%0Acognitive%20load%2C%20confidence%2C%20expertise%2C%20and%20trust%20via%20observational%20learning.%0AThrough%20interview%2C%20we%20delineate%20participant%20learning%20processes.%20Results%20show%20AI%0Acoaching%20can%20effectively%20teach%20performance%20driving%20skills%20to%20novices.%20We%20find%0Athe%20type%20and%20modality%20of%20information%20influences%20performance%20outcomes.%0ADifferences%20in%20how%20successfully%20participants%20learned%20are%20attributed%20to%20how%0Ainformation%20directs%20attention%2C%20mitigates%20uncertainty%2C%20and%20influences%20overload%0Aexperienced%20by%20participants.%20Results%20suggest%20efficient%2C%20modality-appropriate%0Aexplanations%20should%20be%20opted%20for%20when%20designing%20effective%20HMI%20communications%0Athat%20can%20instruct%20without%20overwhelming.%20Further%2C%20results%20support%20the%20need%20to%0Aalign%20communications%20with%20human%20learning%20and%20cognitive%20processes.%20We%20provide%0Aeight%20design%20implications%20for%20future%20autonomous%20vehicle%20HMI%20and%20AI%20coach%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04206v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffects%2520of%2520Multimodal%2520Explanations%2520for%2520Autonomous%2520Driving%2520on%2520Driving%250A%2520%2520Performance%252C%2520Cognitive%2520Load%252C%2520Expertise%252C%2520Confidence%252C%2520and%2520Trust%26entry.906535625%3DRobert%2520Kaufman%2520and%2520Jean%2520Costa%2520and%2520Everlyne%2520Kimani%26entry.1292438233%3D%2520%2520Advances%2520in%2520autonomous%2520driving%2520provide%2520an%2520opportunity%2520for%2520AI-assisted%2520driving%250Ainstruction%2520that%2520directly%2520addresses%2520the%2520critical%2520need%2520for%2520human%2520driving%250Aimprovement.%2520How%2520should%2520an%2520AI%2520instructor%2520convey%2520information%2520to%2520promote%250Alearning%253F%2520In%2520a%2520pre-post%2520experiment%2520%2528n%2520%253D%252041%2529%252C%2520we%2520tested%2520the%2520impact%2520of%2520an%2520AI%250ACoach%2527s%2520explanatory%2520communications%2520modeled%2520after%2520performance%2520driving%2520expert%250Ainstructions.%2520Participants%2520were%2520divided%2520into%2520four%2520%25284%2529%2520groups%2520to%2520assess%2520two%2520%25282%2529%250Adimensions%2520of%2520the%2520AI%2520coach%2527s%2520explanations%253A%2520information%2520type%2520%2528%2527what%2527%2520and%250A%2527why%2527-type%2520explanations%2529%2520and%2520presentation%2520modality%2520%2528auditory%2520and%2520visual%2529.%2520We%250Acompare%2520how%2520different%2520explanatory%2520techniques%2520impact%2520driving%2520performance%252C%250Acognitive%2520load%252C%2520confidence%252C%2520expertise%252C%2520and%2520trust%2520via%2520observational%2520learning.%250AThrough%2520interview%252C%2520we%2520delineate%2520participant%2520learning%2520processes.%2520Results%2520show%2520AI%250Acoaching%2520can%2520effectively%2520teach%2520performance%2520driving%2520skills%2520to%2520novices.%2520We%2520find%250Athe%2520type%2520and%2520modality%2520of%2520information%2520influences%2520performance%2520outcomes.%250ADifferences%2520in%2520how%2520successfully%2520participants%2520learned%2520are%2520attributed%2520to%2520how%250Ainformation%2520directs%2520attention%252C%2520mitigates%2520uncertainty%252C%2520and%2520influences%2520overload%250Aexperienced%2520by%2520participants.%2520Results%2520suggest%2520efficient%252C%2520modality-appropriate%250Aexplanations%2520should%2520be%2520opted%2520for%2520when%2520designing%2520effective%2520HMI%2520communications%250Athat%2520can%2520instruct%2520without%2520overwhelming.%2520Further%252C%2520results%2520support%2520the%2520need%2520to%250Aalign%2520communications%2520with%2520human%2520learning%2520and%2520cognitive%2520processes.%2520We%2520provide%250Aeight%2520design%2520implications%2520for%2520future%2520autonomous%2520vehicle%2520HMI%2520and%2520AI%2520coach%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04206v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effects%20of%20Multimodal%20Explanations%20for%20Autonomous%20Driving%20on%20Driving%0A%20%20Performance%2C%20Cognitive%20Load%2C%20Expertise%2C%20Confidence%2C%20and%20Trust&entry.906535625=Robert%20Kaufman%20and%20Jean%20Costa%20and%20Everlyne%20Kimani&entry.1292438233=%20%20Advances%20in%20autonomous%20driving%20provide%20an%20opportunity%20for%20AI-assisted%20driving%0Ainstruction%20that%20directly%20addresses%20the%20critical%20need%20for%20human%20driving%0Aimprovement.%20How%20should%20an%20AI%20instructor%20convey%20information%20to%20promote%0Alearning%3F%20In%20a%20pre-post%20experiment%20%28n%20%3D%2041%29%2C%20we%20tested%20the%20impact%20of%20an%20AI%0ACoach%27s%20explanatory%20communications%20modeled%20after%20performance%20driving%20expert%0Ainstructions.%20Participants%20were%20divided%20into%20four%20%284%29%20groups%20to%20assess%20two%20%282%29%0Adimensions%20of%20the%20AI%20coach%27s%20explanations%3A%20information%20type%20%28%27what%27%20and%0A%27why%27-type%20explanations%29%20and%20presentation%20modality%20%28auditory%20and%20visual%29.%20We%0Acompare%20how%20different%20explanatory%20techniques%20impact%20driving%20performance%2C%0Acognitive%20load%2C%20confidence%2C%20expertise%2C%20and%20trust%20via%20observational%20learning.%0AThrough%20interview%2C%20we%20delineate%20participant%20learning%20processes.%20Results%20show%20AI%0Acoaching%20can%20effectively%20teach%20performance%20driving%20skills%20to%20novices.%20We%20find%0Athe%20type%20and%20modality%20of%20information%20influences%20performance%20outcomes.%0ADifferences%20in%20how%20successfully%20participants%20learned%20are%20attributed%20to%20how%0Ainformation%20directs%20attention%2C%20mitigates%20uncertainty%2C%20and%20influences%20overload%0Aexperienced%20by%20participants.%20Results%20suggest%20efficient%2C%20modality-appropriate%0Aexplanations%20should%20be%20opted%20for%20when%20designing%20effective%20HMI%20communications%0Athat%20can%20instruct%20without%20overwhelming.%20Further%2C%20results%20support%20the%20need%20to%0Aalign%20communications%20with%20human%20learning%20and%20cognitive%20processes.%20We%20provide%0Aeight%20design%20implications%20for%20future%20autonomous%20vehicle%20HMI%20and%20AI%20coach%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04206v4&entry.124074799=Read"},
{"title": "Personalized Product Assortment with Real-time 3D Perception and\n  Bayesian Payoff Estimation", "author": "Porter Jenkins and Michael Selander and J. Stockton Jenkins and Andrew Merrill and Kyle Armstrong", "abstract": "  Product assortment selection is a critical challenge facing physical\nretailers. Effectively aligning inventory with the preferences of shoppers can\nincrease sales and decrease out-of-stocks. However, in real-world settings the\nproblem is challenging due to the combinatorial explosion of product assortment\npossibilities. Consumer preferences are typically heterogeneous across space\nand time, making inventory-preference alignment challenging. Additionally,\nexisting strategies rely on syndicated data, which tends to be aggregated, low\nresolution, and suffer from high latency. To solve these challenges, we\nintroduce a real-time recommendation system, which we call EdgeRec3D. Our\nsystem utilizes recent advances in 3D computer vision for perception and\nautomatic, fine grained sales estimation. These perceptual components run on\nthe edge of the network and facilitate real-time reward signals. Additionally,\nwe develop a Bayesian payoff model to account for noisy estimates from 3D LIDAR\ndata. We rely on spatial clustering to allow the system to adapt to\nheterogeneous consumer preferences, and a graph-based candidate generation\nalgorithm to address the combinatorial search problem. We test our system in\nreal-world stores across two, 6-8 week A/B tests with beverage products and\ndemonstrate a 35% and 27% increase in sales respectively. Finally, we monitor\nthe deployed system for a period of 28 weeks with an observational study and\nshow a 9.4% increase in sales.\n", "link": "http://arxiv.org/abs/2406.07769v2", "date": "2024-06-13", "relevancy": 2.0633, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.545}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.512}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Product%20Assortment%20with%20Real-time%203D%20Perception%20and%0A%20%20Bayesian%20Payoff%20Estimation&body=Title%3A%20Personalized%20Product%20Assortment%20with%20Real-time%203D%20Perception%20and%0A%20%20Bayesian%20Payoff%20Estimation%0AAuthor%3A%20Porter%20Jenkins%20and%20Michael%20Selander%20and%20J.%20Stockton%20Jenkins%20and%20Andrew%20Merrill%20and%20Kyle%20Armstrong%0AAbstract%3A%20%20%20Product%20assortment%20selection%20is%20a%20critical%20challenge%20facing%20physical%0Aretailers.%20Effectively%20aligning%20inventory%20with%20the%20preferences%20of%20shoppers%20can%0Aincrease%20sales%20and%20decrease%20out-of-stocks.%20However%2C%20in%20real-world%20settings%20the%0Aproblem%20is%20challenging%20due%20to%20the%20combinatorial%20explosion%20of%20product%20assortment%0Apossibilities.%20Consumer%20preferences%20are%20typically%20heterogeneous%20across%20space%0Aand%20time%2C%20making%20inventory-preference%20alignment%20challenging.%20Additionally%2C%0Aexisting%20strategies%20rely%20on%20syndicated%20data%2C%20which%20tends%20to%20be%20aggregated%2C%20low%0Aresolution%2C%20and%20suffer%20from%20high%20latency.%20To%20solve%20these%20challenges%2C%20we%0Aintroduce%20a%20real-time%20recommendation%20system%2C%20which%20we%20call%20EdgeRec3D.%20Our%0Asystem%20utilizes%20recent%20advances%20in%203D%20computer%20vision%20for%20perception%20and%0Aautomatic%2C%20fine%20grained%20sales%20estimation.%20These%20perceptual%20components%20run%20on%0Athe%20edge%20of%20the%20network%20and%20facilitate%20real-time%20reward%20signals.%20Additionally%2C%0Awe%20develop%20a%20Bayesian%20payoff%20model%20to%20account%20for%20noisy%20estimates%20from%203D%20LIDAR%0Adata.%20We%20rely%20on%20spatial%20clustering%20to%20allow%20the%20system%20to%20adapt%20to%0Aheterogeneous%20consumer%20preferences%2C%20and%20a%20graph-based%20candidate%20generation%0Aalgorithm%20to%20address%20the%20combinatorial%20search%20problem.%20We%20test%20our%20system%20in%0Areal-world%20stores%20across%20two%2C%206-8%20week%20A/B%20tests%20with%20beverage%20products%20and%0Ademonstrate%20a%2035%25%20and%2027%25%20increase%20in%20sales%20respectively.%20Finally%2C%20we%20monitor%0Athe%20deployed%20system%20for%20a%20period%20of%2028%20weeks%20with%20an%20observational%20study%20and%0Ashow%20a%209.4%25%20increase%20in%20sales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Product%2520Assortment%2520with%2520Real-time%25203D%2520Perception%2520and%250A%2520%2520Bayesian%2520Payoff%2520Estimation%26entry.906535625%3DPorter%2520Jenkins%2520and%2520Michael%2520Selander%2520and%2520J.%2520Stockton%2520Jenkins%2520and%2520Andrew%2520Merrill%2520and%2520Kyle%2520Armstrong%26entry.1292438233%3D%2520%2520Product%2520assortment%2520selection%2520is%2520a%2520critical%2520challenge%2520facing%2520physical%250Aretailers.%2520Effectively%2520aligning%2520inventory%2520with%2520the%2520preferences%2520of%2520shoppers%2520can%250Aincrease%2520sales%2520and%2520decrease%2520out-of-stocks.%2520However%252C%2520in%2520real-world%2520settings%2520the%250Aproblem%2520is%2520challenging%2520due%2520to%2520the%2520combinatorial%2520explosion%2520of%2520product%2520assortment%250Apossibilities.%2520Consumer%2520preferences%2520are%2520typically%2520heterogeneous%2520across%2520space%250Aand%2520time%252C%2520making%2520inventory-preference%2520alignment%2520challenging.%2520Additionally%252C%250Aexisting%2520strategies%2520rely%2520on%2520syndicated%2520data%252C%2520which%2520tends%2520to%2520be%2520aggregated%252C%2520low%250Aresolution%252C%2520and%2520suffer%2520from%2520high%2520latency.%2520To%2520solve%2520these%2520challenges%252C%2520we%250Aintroduce%2520a%2520real-time%2520recommendation%2520system%252C%2520which%2520we%2520call%2520EdgeRec3D.%2520Our%250Asystem%2520utilizes%2520recent%2520advances%2520in%25203D%2520computer%2520vision%2520for%2520perception%2520and%250Aautomatic%252C%2520fine%2520grained%2520sales%2520estimation.%2520These%2520perceptual%2520components%2520run%2520on%250Athe%2520edge%2520of%2520the%2520network%2520and%2520facilitate%2520real-time%2520reward%2520signals.%2520Additionally%252C%250Awe%2520develop%2520a%2520Bayesian%2520payoff%2520model%2520to%2520account%2520for%2520noisy%2520estimates%2520from%25203D%2520LIDAR%250Adata.%2520We%2520rely%2520on%2520spatial%2520clustering%2520to%2520allow%2520the%2520system%2520to%2520adapt%2520to%250Aheterogeneous%2520consumer%2520preferences%252C%2520and%2520a%2520graph-based%2520candidate%2520generation%250Aalgorithm%2520to%2520address%2520the%2520combinatorial%2520search%2520problem.%2520We%2520test%2520our%2520system%2520in%250Areal-world%2520stores%2520across%2520two%252C%25206-8%2520week%2520A/B%2520tests%2520with%2520beverage%2520products%2520and%250Ademonstrate%2520a%252035%2525%2520and%252027%2525%2520increase%2520in%2520sales%2520respectively.%2520Finally%252C%2520we%2520monitor%250Athe%2520deployed%2520system%2520for%2520a%2520period%2520of%252028%2520weeks%2520with%2520an%2520observational%2520study%2520and%250Ashow%2520a%25209.4%2525%2520increase%2520in%2520sales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Product%20Assortment%20with%20Real-time%203D%20Perception%20and%0A%20%20Bayesian%20Payoff%20Estimation&entry.906535625=Porter%20Jenkins%20and%20Michael%20Selander%20and%20J.%20Stockton%20Jenkins%20and%20Andrew%20Merrill%20and%20Kyle%20Armstrong&entry.1292438233=%20%20Product%20assortment%20selection%20is%20a%20critical%20challenge%20facing%20physical%0Aretailers.%20Effectively%20aligning%20inventory%20with%20the%20preferences%20of%20shoppers%20can%0Aincrease%20sales%20and%20decrease%20out-of-stocks.%20However%2C%20in%20real-world%20settings%20the%0Aproblem%20is%20challenging%20due%20to%20the%20combinatorial%20explosion%20of%20product%20assortment%0Apossibilities.%20Consumer%20preferences%20are%20typically%20heterogeneous%20across%20space%0Aand%20time%2C%20making%20inventory-preference%20alignment%20challenging.%20Additionally%2C%0Aexisting%20strategies%20rely%20on%20syndicated%20data%2C%20which%20tends%20to%20be%20aggregated%2C%20low%0Aresolution%2C%20and%20suffer%20from%20high%20latency.%20To%20solve%20these%20challenges%2C%20we%0Aintroduce%20a%20real-time%20recommendation%20system%2C%20which%20we%20call%20EdgeRec3D.%20Our%0Asystem%20utilizes%20recent%20advances%20in%203D%20computer%20vision%20for%20perception%20and%0Aautomatic%2C%20fine%20grained%20sales%20estimation.%20These%20perceptual%20components%20run%20on%0Athe%20edge%20of%20the%20network%20and%20facilitate%20real-time%20reward%20signals.%20Additionally%2C%0Awe%20develop%20a%20Bayesian%20payoff%20model%20to%20account%20for%20noisy%20estimates%20from%203D%20LIDAR%0Adata.%20We%20rely%20on%20spatial%20clustering%20to%20allow%20the%20system%20to%20adapt%20to%0Aheterogeneous%20consumer%20preferences%2C%20and%20a%20graph-based%20candidate%20generation%0Aalgorithm%20to%20address%20the%20combinatorial%20search%20problem.%20We%20test%20our%20system%20in%0Areal-world%20stores%20across%20two%2C%206-8%20week%20A/B%20tests%20with%20beverage%20products%20and%0Ademonstrate%20a%2035%25%20and%2027%25%20increase%20in%20sales%20respectively.%20Finally%2C%20we%20monitor%0Athe%20deployed%20system%20for%20a%20period%20of%2028%20weeks%20with%20an%20observational%20study%20and%0Ashow%20a%209.4%25%20increase%20in%20sales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07769v2&entry.124074799=Read"},
{"title": "Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video\n  QA", "author": "Jongwoo Park and Kanchana Ranasinghe and Kumara Kahatapitiya and Wonjeong Ryoo and Donghyun Kim and Michael S. Ryoo", "abstract": "  Long-form videos that span across wide temporal intervals are highly\ninformation redundant and contain multiple distinct events or entities that are\noften loosely-related. Therefore, when performing long-form video question\nanswering (LVQA),all information necessary to generate a correct response can\noften be contained within a small subset of frames. Recent literature explore\nthe use of large language models (LLMs) in LVQA benchmarks, achieving\nexceptional performance, while relying on vision language models (VLMs) to\nconvert all visual content within videos into natural language. Such VLMs often\nindependently caption a large number of frames uniformly sampled from long\nvideos, which is not efficient and can mostly be redundant. Questioning these\ndecision choices, we explore optimal strategies for key-frame selection and\nsequence-aware captioning, that can significantly reduce these redundancies. We\npropose two novel approaches that improve each of aspects, namely Hierarchical\nKeyframe Selector and Sequential Visual LLM. Our resulting framework termed\nLVNet achieves state-of-the-art performance across three benchmark LVQA\ndatasets. Our code will be released publicly.\n", "link": "http://arxiv.org/abs/2406.09396v1", "date": "2024-06-13", "relevancy": 2.0631, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5354}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5141}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Too%20Many%20Frames%2C%20not%20all%20Useful%3AEfficient%20Strategies%20for%20Long-Form%20Video%0A%20%20QA&body=Title%3A%20Too%20Many%20Frames%2C%20not%20all%20Useful%3AEfficient%20Strategies%20for%20Long-Form%20Video%0A%20%20QA%0AAuthor%3A%20Jongwoo%20Park%20and%20Kanchana%20Ranasinghe%20and%20Kumara%20Kahatapitiya%20and%20Wonjeong%20Ryoo%20and%20Donghyun%20Kim%20and%20Michael%20S.%20Ryoo%0AAbstract%3A%20%20%20Long-form%20videos%20that%20span%20across%20wide%20temporal%20intervals%20are%20highly%0Ainformation%20redundant%20and%20contain%20multiple%20distinct%20events%20or%20entities%20that%20are%0Aoften%20loosely-related.%20Therefore%2C%20when%20performing%20long-form%20video%20question%0Aanswering%20%28LVQA%29%2Call%20information%20necessary%20to%20generate%20a%20correct%20response%20can%0Aoften%20be%20contained%20within%20a%20small%20subset%20of%20frames.%20Recent%20literature%20explore%0Athe%20use%20of%20large%20language%20models%20%28LLMs%29%20in%20LVQA%20benchmarks%2C%20achieving%0Aexceptional%20performance%2C%20while%20relying%20on%20vision%20language%20models%20%28VLMs%29%20to%0Aconvert%20all%20visual%20content%20within%20videos%20into%20natural%20language.%20Such%20VLMs%20often%0Aindependently%20caption%20a%20large%20number%20of%20frames%20uniformly%20sampled%20from%20long%0Avideos%2C%20which%20is%20not%20efficient%20and%20can%20mostly%20be%20redundant.%20Questioning%20these%0Adecision%20choices%2C%20we%20explore%20optimal%20strategies%20for%20key-frame%20selection%20and%0Asequence-aware%20captioning%2C%20that%20can%20significantly%20reduce%20these%20redundancies.%20We%0Apropose%20two%20novel%20approaches%20that%20improve%20each%20of%20aspects%2C%20namely%20Hierarchical%0AKeyframe%20Selector%20and%20Sequential%20Visual%20LLM.%20Our%20resulting%20framework%20termed%0ALVNet%20achieves%20state-of-the-art%20performance%20across%20three%20benchmark%20LVQA%0Adatasets.%20Our%20code%20will%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToo%2520Many%2520Frames%252C%2520not%2520all%2520Useful%253AEfficient%2520Strategies%2520for%2520Long-Form%2520Video%250A%2520%2520QA%26entry.906535625%3DJongwoo%2520Park%2520and%2520Kanchana%2520Ranasinghe%2520and%2520Kumara%2520Kahatapitiya%2520and%2520Wonjeong%2520Ryoo%2520and%2520Donghyun%2520Kim%2520and%2520Michael%2520S.%2520Ryoo%26entry.1292438233%3D%2520%2520Long-form%2520videos%2520that%2520span%2520across%2520wide%2520temporal%2520intervals%2520are%2520highly%250Ainformation%2520redundant%2520and%2520contain%2520multiple%2520distinct%2520events%2520or%2520entities%2520that%2520are%250Aoften%2520loosely-related.%2520Therefore%252C%2520when%2520performing%2520long-form%2520video%2520question%250Aanswering%2520%2528LVQA%2529%252Call%2520information%2520necessary%2520to%2520generate%2520a%2520correct%2520response%2520can%250Aoften%2520be%2520contained%2520within%2520a%2520small%2520subset%2520of%2520frames.%2520Recent%2520literature%2520explore%250Athe%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520LVQA%2520benchmarks%252C%2520achieving%250Aexceptional%2520performance%252C%2520while%2520relying%2520on%2520vision%2520language%2520models%2520%2528VLMs%2529%2520to%250Aconvert%2520all%2520visual%2520content%2520within%2520videos%2520into%2520natural%2520language.%2520Such%2520VLMs%2520often%250Aindependently%2520caption%2520a%2520large%2520number%2520of%2520frames%2520uniformly%2520sampled%2520from%2520long%250Avideos%252C%2520which%2520is%2520not%2520efficient%2520and%2520can%2520mostly%2520be%2520redundant.%2520Questioning%2520these%250Adecision%2520choices%252C%2520we%2520explore%2520optimal%2520strategies%2520for%2520key-frame%2520selection%2520and%250Asequence-aware%2520captioning%252C%2520that%2520can%2520significantly%2520reduce%2520these%2520redundancies.%2520We%250Apropose%2520two%2520novel%2520approaches%2520that%2520improve%2520each%2520of%2520aspects%252C%2520namely%2520Hierarchical%250AKeyframe%2520Selector%2520and%2520Sequential%2520Visual%2520LLM.%2520Our%2520resulting%2520framework%2520termed%250ALVNet%2520achieves%2520state-of-the-art%2520performance%2520across%2520three%2520benchmark%2520LVQA%250Adatasets.%2520Our%2520code%2520will%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Too%20Many%20Frames%2C%20not%20all%20Useful%3AEfficient%20Strategies%20for%20Long-Form%20Video%0A%20%20QA&entry.906535625=Jongwoo%20Park%20and%20Kanchana%20Ranasinghe%20and%20Kumara%20Kahatapitiya%20and%20Wonjeong%20Ryoo%20and%20Donghyun%20Kim%20and%20Michael%20S.%20Ryoo&entry.1292438233=%20%20Long-form%20videos%20that%20span%20across%20wide%20temporal%20intervals%20are%20highly%0Ainformation%20redundant%20and%20contain%20multiple%20distinct%20events%20or%20entities%20that%20are%0Aoften%20loosely-related.%20Therefore%2C%20when%20performing%20long-form%20video%20question%0Aanswering%20%28LVQA%29%2Call%20information%20necessary%20to%20generate%20a%20correct%20response%20can%0Aoften%20be%20contained%20within%20a%20small%20subset%20of%20frames.%20Recent%20literature%20explore%0Athe%20use%20of%20large%20language%20models%20%28LLMs%29%20in%20LVQA%20benchmarks%2C%20achieving%0Aexceptional%20performance%2C%20while%20relying%20on%20vision%20language%20models%20%28VLMs%29%20to%0Aconvert%20all%20visual%20content%20within%20videos%20into%20natural%20language.%20Such%20VLMs%20often%0Aindependently%20caption%20a%20large%20number%20of%20frames%20uniformly%20sampled%20from%20long%0Avideos%2C%20which%20is%20not%20efficient%20and%20can%20mostly%20be%20redundant.%20Questioning%20these%0Adecision%20choices%2C%20we%20explore%20optimal%20strategies%20for%20key-frame%20selection%20and%0Asequence-aware%20captioning%2C%20that%20can%20significantly%20reduce%20these%20redundancies.%20We%0Apropose%20two%20novel%20approaches%20that%20improve%20each%20of%20aspects%2C%20namely%20Hierarchical%0AKeyframe%20Selector%20and%20Sequential%20Visual%20LLM.%20Our%20resulting%20framework%20termed%0ALVNet%20achieves%20state-of-the-art%20performance%20across%20three%20benchmark%20LVQA%0Adatasets.%20Our%20code%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09396v1&entry.124074799=Read"},
{"title": "Optimizing Visual Question Answering Models for Driving: Bridging the\n  Gap Between Human and Machine Attention Patterns", "author": "Kaavya Rekanar and Martin Hayes and Ganesh Sistu and Ciaran Eising", "abstract": "  Visual Question Answering (VQA) models play a critical role in enhancing the\nperception capabilities of autonomous driving systems by allowing vehicles to\nanalyze visual inputs alongside textual queries, fostering natural interaction\nand trust between the vehicle and its occupants or other road users. This study\ninvestigates the attention patterns of humans compared to a VQA model when\nanswering driving-related questions, revealing disparities in the objects\nobserved. We propose an approach integrating filters to optimize the model's\nattention mechanisms, prioritizing relevant objects and improving accuracy.\nUtilizing the LXMERT model for a case study, we compare attention patterns of\nthe pre-trained and Filter Integrated models, alongside human answers using\nimages from the NuImages dataset, gaining insights into feature prioritization.\nWe evaluated the models using a Subjective scoring framework which shows that\nthe integration of the feature encoder filter has enhanced the performance of\nthe VQA model by refining its attention mechanisms.\n", "link": "http://arxiv.org/abs/2406.09203v1", "date": "2024-06-13", "relevancy": 2.0532, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5128}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Visual%20Question%20Answering%20Models%20for%20Driving%3A%20Bridging%20the%0A%20%20Gap%20Between%20Human%20and%20Machine%20Attention%20Patterns&body=Title%3A%20Optimizing%20Visual%20Question%20Answering%20Models%20for%20Driving%3A%20Bridging%20the%0A%20%20Gap%20Between%20Human%20and%20Machine%20Attention%20Patterns%0AAuthor%3A%20Kaavya%20Rekanar%20and%20Martin%20Hayes%20and%20Ganesh%20Sistu%20and%20Ciaran%20Eising%0AAbstract%3A%20%20%20Visual%20Question%20Answering%20%28VQA%29%20models%20play%20a%20critical%20role%20in%20enhancing%20the%0Aperception%20capabilities%20of%20autonomous%20driving%20systems%20by%20allowing%20vehicles%20to%0Aanalyze%20visual%20inputs%20alongside%20textual%20queries%2C%20fostering%20natural%20interaction%0Aand%20trust%20between%20the%20vehicle%20and%20its%20occupants%20or%20other%20road%20users.%20This%20study%0Ainvestigates%20the%20attention%20patterns%20of%20humans%20compared%20to%20a%20VQA%20model%20when%0Aanswering%20driving-related%20questions%2C%20revealing%20disparities%20in%20the%20objects%0Aobserved.%20We%20propose%20an%20approach%20integrating%20filters%20to%20optimize%20the%20model%27s%0Aattention%20mechanisms%2C%20prioritizing%20relevant%20objects%20and%20improving%20accuracy.%0AUtilizing%20the%20LXMERT%20model%20for%20a%20case%20study%2C%20we%20compare%20attention%20patterns%20of%0Athe%20pre-trained%20and%20Filter%20Integrated%20models%2C%20alongside%20human%20answers%20using%0Aimages%20from%20the%20NuImages%20dataset%2C%20gaining%20insights%20into%20feature%20prioritization.%0AWe%20evaluated%20the%20models%20using%20a%20Subjective%20scoring%20framework%20which%20shows%20that%0Athe%20integration%20of%20the%20feature%20encoder%20filter%20has%20enhanced%20the%20performance%20of%0Athe%20VQA%20model%20by%20refining%20its%20attention%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Visual%2520Question%2520Answering%2520Models%2520for%2520Driving%253A%2520Bridging%2520the%250A%2520%2520Gap%2520Between%2520Human%2520and%2520Machine%2520Attention%2520Patterns%26entry.906535625%3DKaavya%2520Rekanar%2520and%2520Martin%2520Hayes%2520and%2520Ganesh%2520Sistu%2520and%2520Ciaran%2520Eising%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520models%2520play%2520a%2520critical%2520role%2520in%2520enhancing%2520the%250Aperception%2520capabilities%2520of%2520autonomous%2520driving%2520systems%2520by%2520allowing%2520vehicles%2520to%250Aanalyze%2520visual%2520inputs%2520alongside%2520textual%2520queries%252C%2520fostering%2520natural%2520interaction%250Aand%2520trust%2520between%2520the%2520vehicle%2520and%2520its%2520occupants%2520or%2520other%2520road%2520users.%2520This%2520study%250Ainvestigates%2520the%2520attention%2520patterns%2520of%2520humans%2520compared%2520to%2520a%2520VQA%2520model%2520when%250Aanswering%2520driving-related%2520questions%252C%2520revealing%2520disparities%2520in%2520the%2520objects%250Aobserved.%2520We%2520propose%2520an%2520approach%2520integrating%2520filters%2520to%2520optimize%2520the%2520model%2527s%250Aattention%2520mechanisms%252C%2520prioritizing%2520relevant%2520objects%2520and%2520improving%2520accuracy.%250AUtilizing%2520the%2520LXMERT%2520model%2520for%2520a%2520case%2520study%252C%2520we%2520compare%2520attention%2520patterns%2520of%250Athe%2520pre-trained%2520and%2520Filter%2520Integrated%2520models%252C%2520alongside%2520human%2520answers%2520using%250Aimages%2520from%2520the%2520NuImages%2520dataset%252C%2520gaining%2520insights%2520into%2520feature%2520prioritization.%250AWe%2520evaluated%2520the%2520models%2520using%2520a%2520Subjective%2520scoring%2520framework%2520which%2520shows%2520that%250Athe%2520integration%2520of%2520the%2520feature%2520encoder%2520filter%2520has%2520enhanced%2520the%2520performance%2520of%250Athe%2520VQA%2520model%2520by%2520refining%2520its%2520attention%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Visual%20Question%20Answering%20Models%20for%20Driving%3A%20Bridging%20the%0A%20%20Gap%20Between%20Human%20and%20Machine%20Attention%20Patterns&entry.906535625=Kaavya%20Rekanar%20and%20Martin%20Hayes%20and%20Ganesh%20Sistu%20and%20Ciaran%20Eising&entry.1292438233=%20%20Visual%20Question%20Answering%20%28VQA%29%20models%20play%20a%20critical%20role%20in%20enhancing%20the%0Aperception%20capabilities%20of%20autonomous%20driving%20systems%20by%20allowing%20vehicles%20to%0Aanalyze%20visual%20inputs%20alongside%20textual%20queries%2C%20fostering%20natural%20interaction%0Aand%20trust%20between%20the%20vehicle%20and%20its%20occupants%20or%20other%20road%20users.%20This%20study%0Ainvestigates%20the%20attention%20patterns%20of%20humans%20compared%20to%20a%20VQA%20model%20when%0Aanswering%20driving-related%20questions%2C%20revealing%20disparities%20in%20the%20objects%0Aobserved.%20We%20propose%20an%20approach%20integrating%20filters%20to%20optimize%20the%20model%27s%0Aattention%20mechanisms%2C%20prioritizing%20relevant%20objects%20and%20improving%20accuracy.%0AUtilizing%20the%20LXMERT%20model%20for%20a%20case%20study%2C%20we%20compare%20attention%20patterns%20of%0Athe%20pre-trained%20and%20Filter%20Integrated%20models%2C%20alongside%20human%20answers%20using%0Aimages%20from%20the%20NuImages%20dataset%2C%20gaining%20insights%20into%20feature%20prioritization.%0AWe%20evaluated%20the%20models%20using%20a%20Subjective%20scoring%20framework%20which%20shows%20that%0Athe%20integration%20of%20the%20feature%20encoder%20filter%20has%20enhanced%20the%20performance%20of%0Athe%20VQA%20model%20by%20refining%20its%20attention%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09203v1&entry.124074799=Read"},
{"title": "Deep Transformer Network for Monocular Pose Estimation of Ship-Based UAV", "author": "Maneesha Wickramasuriya and Taeyoung Lee and Murray Snyder", "abstract": "  This paper introduces a deep transformer network for estimating the relative\n6D pose of a Unmanned Aerial Vehicle (UAV) with respect to a ship using\nmonocular images. A synthetic dataset of ship images is created and annotated\nwith 2D keypoints of multiple ship parts. A Transformer Neural Network model is\ntrained to detect these keypoints and estimate the 6D pose of each part. The\nestimates are integrated using Bayesian fusion. The model is tested on\nsynthetic data and in-situ flight experiments, demonstrating robustness and\naccuracy in various lighting conditions. The position estimation error is\napproximately 0.8\\% and 1.0\\% of the distance to the ship for the synthetic\ndata and the flight experiments, respectively. The method has potential\napplications for ship-based autonomous UAV landing and navigation.\n", "link": "http://arxiv.org/abs/2406.09260v1", "date": "2024-06-13", "relevancy": 2.0501, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.51}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Transformer%20Network%20for%20Monocular%20Pose%20Estimation%20of%20Ship-Based%20UAV&body=Title%3A%20Deep%20Transformer%20Network%20for%20Monocular%20Pose%20Estimation%20of%20Ship-Based%20UAV%0AAuthor%3A%20Maneesha%20Wickramasuriya%20and%20Taeyoung%20Lee%20and%20Murray%20Snyder%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20deep%20transformer%20network%20for%20estimating%20the%20relative%0A6D%20pose%20of%20a%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20with%20respect%20to%20a%20ship%20using%0Amonocular%20images.%20A%20synthetic%20dataset%20of%20ship%20images%20is%20created%20and%20annotated%0Awith%202D%20keypoints%20of%20multiple%20ship%20parts.%20A%20Transformer%20Neural%20Network%20model%20is%0Atrained%20to%20detect%20these%20keypoints%20and%20estimate%20the%206D%20pose%20of%20each%20part.%20The%0Aestimates%20are%20integrated%20using%20Bayesian%20fusion.%20The%20model%20is%20tested%20on%0Asynthetic%20data%20and%20in-situ%20flight%20experiments%2C%20demonstrating%20robustness%20and%0Aaccuracy%20in%20various%20lighting%20conditions.%20The%20position%20estimation%20error%20is%0Aapproximately%200.8%5C%25%20and%201.0%5C%25%20of%20the%20distance%20to%20the%20ship%20for%20the%20synthetic%0Adata%20and%20the%20flight%20experiments%2C%20respectively.%20The%20method%20has%20potential%0Aapplications%20for%20ship-based%20autonomous%20UAV%20landing%20and%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Transformer%2520Network%2520for%2520Monocular%2520Pose%2520Estimation%2520of%2520Ship-Based%2520UAV%26entry.906535625%3DManeesha%2520Wickramasuriya%2520and%2520Taeyoung%2520Lee%2520and%2520Murray%2520Snyder%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520deep%2520transformer%2520network%2520for%2520estimating%2520the%2520relative%250A6D%2520pose%2520of%2520a%2520Unmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520with%2520respect%2520to%2520a%2520ship%2520using%250Amonocular%2520images.%2520A%2520synthetic%2520dataset%2520of%2520ship%2520images%2520is%2520created%2520and%2520annotated%250Awith%25202D%2520keypoints%2520of%2520multiple%2520ship%2520parts.%2520A%2520Transformer%2520Neural%2520Network%2520model%2520is%250Atrained%2520to%2520detect%2520these%2520keypoints%2520and%2520estimate%2520the%25206D%2520pose%2520of%2520each%2520part.%2520The%250Aestimates%2520are%2520integrated%2520using%2520Bayesian%2520fusion.%2520The%2520model%2520is%2520tested%2520on%250Asynthetic%2520data%2520and%2520in-situ%2520flight%2520experiments%252C%2520demonstrating%2520robustness%2520and%250Aaccuracy%2520in%2520various%2520lighting%2520conditions.%2520The%2520position%2520estimation%2520error%2520is%250Aapproximately%25200.8%255C%2525%2520and%25201.0%255C%2525%2520of%2520the%2520distance%2520to%2520the%2520ship%2520for%2520the%2520synthetic%250Adata%2520and%2520the%2520flight%2520experiments%252C%2520respectively.%2520The%2520method%2520has%2520potential%250Aapplications%2520for%2520ship-based%2520autonomous%2520UAV%2520landing%2520and%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Transformer%20Network%20for%20Monocular%20Pose%20Estimation%20of%20Ship-Based%20UAV&entry.906535625=Maneesha%20Wickramasuriya%20and%20Taeyoung%20Lee%20and%20Murray%20Snyder&entry.1292438233=%20%20This%20paper%20introduces%20a%20deep%20transformer%20network%20for%20estimating%20the%20relative%0A6D%20pose%20of%20a%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20with%20respect%20to%20a%20ship%20using%0Amonocular%20images.%20A%20synthetic%20dataset%20of%20ship%20images%20is%20created%20and%20annotated%0Awith%202D%20keypoints%20of%20multiple%20ship%20parts.%20A%20Transformer%20Neural%20Network%20model%20is%0Atrained%20to%20detect%20these%20keypoints%20and%20estimate%20the%206D%20pose%20of%20each%20part.%20The%0Aestimates%20are%20integrated%20using%20Bayesian%20fusion.%20The%20model%20is%20tested%20on%0Asynthetic%20data%20and%20in-situ%20flight%20experiments%2C%20demonstrating%20robustness%20and%0Aaccuracy%20in%20various%20lighting%20conditions.%20The%20position%20estimation%20error%20is%0Aapproximately%200.8%5C%25%20and%201.0%5C%25%20of%20the%20distance%20to%20the%20ship%20for%20the%20synthetic%0Adata%20and%20the%20flight%20experiments%2C%20respectively.%20The%20method%20has%20potential%0Aapplications%20for%20ship-based%20autonomous%20UAV%20landing%20and%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09260v1&entry.124074799=Read"},
{"title": "Memory-Efficient Sparse Pyramid Attention Networks for Whole Slide Image\n  Analysis", "author": "Weiyi Wu and Chongyang Gao and Xinwen Xu and Siting Li and Jiang Gui", "abstract": "  Whole Slide Images (WSIs) are crucial for modern pathological diagnosis, yet\ntheir gigapixel-scale resolutions and sparse informative regions pose\nsignificant computational challenges. Traditional dense attention mechanisms,\nwidely used in computer vision and natural language processing, are impractical\nfor WSI analysis due to the substantial data scale and the redundant processing\nof uninformative areas. To address these challenges, we propose\nMemory-Efficient Sparse Pyramid Attention Networks with Shifted Windows (SPAN),\ndrawing inspiration from state-of-the-art sparse attention techniques in other\ndomains. SPAN introduces a sparse pyramid attention architecture that\nhierarchically focuses on informative regions within the WSI, aiming to reduce\nmemory overhead while preserving critical features. Additionally, the\nincorporation of shifted windows enables the model to capture long-range\ncontextual dependencies essential for accurate classification. We evaluated\nSPAN on multiple public WSI datasets, observing its competitive performance.\nUnlike existing methods that often struggle to model spatial and contextual\ninformation due to memory constraints, our approach enables the accurate\nmodeling of these crucial features. Our study also highlights the importance of\nkey design elements in attention mechanisms, such as the shifted-window scheme\nand the hierarchical structure, which contribute substantially to the\neffectiveness of SPAN in WSI analysis. The potential of SPAN for\nmemory-efficient and effective analysis of WSI data is thus demonstrated, and\nthe code will be made publicly available following the publication of this\nwork.\n", "link": "http://arxiv.org/abs/2406.09333v1", "date": "2024-06-13", "relevancy": 2.0492, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5147}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.511}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-Efficient%20Sparse%20Pyramid%20Attention%20Networks%20for%20Whole%20Slide%20Image%0A%20%20Analysis&body=Title%3A%20Memory-Efficient%20Sparse%20Pyramid%20Attention%20Networks%20for%20Whole%20Slide%20Image%0A%20%20Analysis%0AAuthor%3A%20Weiyi%20Wu%20and%20Chongyang%20Gao%20and%20Xinwen%20Xu%20and%20Siting%20Li%20and%20Jiang%20Gui%0AAbstract%3A%20%20%20Whole%20Slide%20Images%20%28WSIs%29%20are%20crucial%20for%20modern%20pathological%20diagnosis%2C%20yet%0Atheir%20gigapixel-scale%20resolutions%20and%20sparse%20informative%20regions%20pose%0Asignificant%20computational%20challenges.%20Traditional%20dense%20attention%20mechanisms%2C%0Awidely%20used%20in%20computer%20vision%20and%20natural%20language%20processing%2C%20are%20impractical%0Afor%20WSI%20analysis%20due%20to%20the%20substantial%20data%20scale%20and%20the%20redundant%20processing%0Aof%20uninformative%20areas.%20To%20address%20these%20challenges%2C%20we%20propose%0AMemory-Efficient%20Sparse%20Pyramid%20Attention%20Networks%20with%20Shifted%20Windows%20%28SPAN%29%2C%0Adrawing%20inspiration%20from%20state-of-the-art%20sparse%20attention%20techniques%20in%20other%0Adomains.%20SPAN%20introduces%20a%20sparse%20pyramid%20attention%20architecture%20that%0Ahierarchically%20focuses%20on%20informative%20regions%20within%20the%20WSI%2C%20aiming%20to%20reduce%0Amemory%20overhead%20while%20preserving%20critical%20features.%20Additionally%2C%20the%0Aincorporation%20of%20shifted%20windows%20enables%20the%20model%20to%20capture%20long-range%0Acontextual%20dependencies%20essential%20for%20accurate%20classification.%20We%20evaluated%0ASPAN%20on%20multiple%20public%20WSI%20datasets%2C%20observing%20its%20competitive%20performance.%0AUnlike%20existing%20methods%20that%20often%20struggle%20to%20model%20spatial%20and%20contextual%0Ainformation%20due%20to%20memory%20constraints%2C%20our%20approach%20enables%20the%20accurate%0Amodeling%20of%20these%20crucial%20features.%20Our%20study%20also%20highlights%20the%20importance%20of%0Akey%20design%20elements%20in%20attention%20mechanisms%2C%20such%20as%20the%20shifted-window%20scheme%0Aand%20the%20hierarchical%20structure%2C%20which%20contribute%20substantially%20to%20the%0Aeffectiveness%20of%20SPAN%20in%20WSI%20analysis.%20The%20potential%20of%20SPAN%20for%0Amemory-efficient%20and%20effective%20analysis%20of%20WSI%20data%20is%20thus%20demonstrated%2C%20and%0Athe%20code%20will%20be%20made%20publicly%20available%20following%20the%20publication%20of%20this%0Awork.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-Efficient%2520Sparse%2520Pyramid%2520Attention%2520Networks%2520for%2520Whole%2520Slide%2520Image%250A%2520%2520Analysis%26entry.906535625%3DWeiyi%2520Wu%2520and%2520Chongyang%2520Gao%2520and%2520Xinwen%2520Xu%2520and%2520Siting%2520Li%2520and%2520Jiang%2520Gui%26entry.1292438233%3D%2520%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520are%2520crucial%2520for%2520modern%2520pathological%2520diagnosis%252C%2520yet%250Atheir%2520gigapixel-scale%2520resolutions%2520and%2520sparse%2520informative%2520regions%2520pose%250Asignificant%2520computational%2520challenges.%2520Traditional%2520dense%2520attention%2520mechanisms%252C%250Awidely%2520used%2520in%2520computer%2520vision%2520and%2520natural%2520language%2520processing%252C%2520are%2520impractical%250Afor%2520WSI%2520analysis%2520due%2520to%2520the%2520substantial%2520data%2520scale%2520and%2520the%2520redundant%2520processing%250Aof%2520uninformative%2520areas.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AMemory-Efficient%2520Sparse%2520Pyramid%2520Attention%2520Networks%2520with%2520Shifted%2520Windows%2520%2528SPAN%2529%252C%250Adrawing%2520inspiration%2520from%2520state-of-the-art%2520sparse%2520attention%2520techniques%2520in%2520other%250Adomains.%2520SPAN%2520introduces%2520a%2520sparse%2520pyramid%2520attention%2520architecture%2520that%250Ahierarchically%2520focuses%2520on%2520informative%2520regions%2520within%2520the%2520WSI%252C%2520aiming%2520to%2520reduce%250Amemory%2520overhead%2520while%2520preserving%2520critical%2520features.%2520Additionally%252C%2520the%250Aincorporation%2520of%2520shifted%2520windows%2520enables%2520the%2520model%2520to%2520capture%2520long-range%250Acontextual%2520dependencies%2520essential%2520for%2520accurate%2520classification.%2520We%2520evaluated%250ASPAN%2520on%2520multiple%2520public%2520WSI%2520datasets%252C%2520observing%2520its%2520competitive%2520performance.%250AUnlike%2520existing%2520methods%2520that%2520often%2520struggle%2520to%2520model%2520spatial%2520and%2520contextual%250Ainformation%2520due%2520to%2520memory%2520constraints%252C%2520our%2520approach%2520enables%2520the%2520accurate%250Amodeling%2520of%2520these%2520crucial%2520features.%2520Our%2520study%2520also%2520highlights%2520the%2520importance%2520of%250Akey%2520design%2520elements%2520in%2520attention%2520mechanisms%252C%2520such%2520as%2520the%2520shifted-window%2520scheme%250Aand%2520the%2520hierarchical%2520structure%252C%2520which%2520contribute%2520substantially%2520to%2520the%250Aeffectiveness%2520of%2520SPAN%2520in%2520WSI%2520analysis.%2520The%2520potential%2520of%2520SPAN%2520for%250Amemory-efficient%2520and%2520effective%2520analysis%2520of%2520WSI%2520data%2520is%2520thus%2520demonstrated%252C%2520and%250Athe%2520code%2520will%2520be%2520made%2520publicly%2520available%2520following%2520the%2520publication%2520of%2520this%250Awork.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-Efficient%20Sparse%20Pyramid%20Attention%20Networks%20for%20Whole%20Slide%20Image%0A%20%20Analysis&entry.906535625=Weiyi%20Wu%20and%20Chongyang%20Gao%20and%20Xinwen%20Xu%20and%20Siting%20Li%20and%20Jiang%20Gui&entry.1292438233=%20%20Whole%20Slide%20Images%20%28WSIs%29%20are%20crucial%20for%20modern%20pathological%20diagnosis%2C%20yet%0Atheir%20gigapixel-scale%20resolutions%20and%20sparse%20informative%20regions%20pose%0Asignificant%20computational%20challenges.%20Traditional%20dense%20attention%20mechanisms%2C%0Awidely%20used%20in%20computer%20vision%20and%20natural%20language%20processing%2C%20are%20impractical%0Afor%20WSI%20analysis%20due%20to%20the%20substantial%20data%20scale%20and%20the%20redundant%20processing%0Aof%20uninformative%20areas.%20To%20address%20these%20challenges%2C%20we%20propose%0AMemory-Efficient%20Sparse%20Pyramid%20Attention%20Networks%20with%20Shifted%20Windows%20%28SPAN%29%2C%0Adrawing%20inspiration%20from%20state-of-the-art%20sparse%20attention%20techniques%20in%20other%0Adomains.%20SPAN%20introduces%20a%20sparse%20pyramid%20attention%20architecture%20that%0Ahierarchically%20focuses%20on%20informative%20regions%20within%20the%20WSI%2C%20aiming%20to%20reduce%0Amemory%20overhead%20while%20preserving%20critical%20features.%20Additionally%2C%20the%0Aincorporation%20of%20shifted%20windows%20enables%20the%20model%20to%20capture%20long-range%0Acontextual%20dependencies%20essential%20for%20accurate%20classification.%20We%20evaluated%0ASPAN%20on%20multiple%20public%20WSI%20datasets%2C%20observing%20its%20competitive%20performance.%0AUnlike%20existing%20methods%20that%20often%20struggle%20to%20model%20spatial%20and%20contextual%0Ainformation%20due%20to%20memory%20constraints%2C%20our%20approach%20enables%20the%20accurate%0Amodeling%20of%20these%20crucial%20features.%20Our%20study%20also%20highlights%20the%20importance%20of%0Akey%20design%20elements%20in%20attention%20mechanisms%2C%20such%20as%20the%20shifted-window%20scheme%0Aand%20the%20hierarchical%20structure%2C%20which%20contribute%20substantially%20to%20the%0Aeffectiveness%20of%20SPAN%20in%20WSI%20analysis.%20The%20potential%20of%20SPAN%20for%0Amemory-efficient%20and%20effective%20analysis%20of%20WSI%20data%20is%20thus%20demonstrated%2C%20and%0Athe%20code%20will%20be%20made%20publicly%20available%20following%20the%20publication%20of%20this%0Awork.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09333v1&entry.124074799=Read"},
{"title": "Parameter-Efficient Active Learning for Foundational models", "author": "Athmanarayanan Lakshmi Narayanan and Ranganath Krishnan and Amrutha Machireddy and Mahesh Subedar", "abstract": "  Foundational vision transformer models have shown impressive few shot\nperformance on many vision tasks. This research presents a novel investigation\ninto the application of parameter efficient fine-tuning methods within an\nactive learning (AL) framework, to advance the sampling selection process in\nextremely budget constrained classification tasks. The focus on image datasets,\nknown for their out-of-distribution characteristics, adds a layer of complexity\nand relevance to our study. Through a detailed evaluation, we illustrate the\nimproved AL performance on these challenging datasets, highlighting the\nstrategic advantage of merging parameter efficient fine tuning methods with\nfoundation models. This contributes to the broader discourse on optimizing AL\nstrategies, presenting a promising avenue for future exploration in leveraging\nfoundation models for efficient and effective data annotation in specialized\ndomains.\n", "link": "http://arxiv.org/abs/2406.09296v1", "date": "2024-06-13", "relevancy": 2.0485, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5072}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20Active%20Learning%20for%20Foundational%20models&body=Title%3A%20Parameter-Efficient%20Active%20Learning%20for%20Foundational%20models%0AAuthor%3A%20Athmanarayanan%20Lakshmi%20Narayanan%20and%20Ranganath%20Krishnan%20and%20Amrutha%20Machireddy%20and%20Mahesh%20Subedar%0AAbstract%3A%20%20%20Foundational%20vision%20transformer%20models%20have%20shown%20impressive%20few%20shot%0Aperformance%20on%20many%20vision%20tasks.%20This%20research%20presents%20a%20novel%20investigation%0Ainto%20the%20application%20of%20parameter%20efficient%20fine-tuning%20methods%20within%20an%0Aactive%20learning%20%28AL%29%20framework%2C%20to%20advance%20the%20sampling%20selection%20process%20in%0Aextremely%20budget%20constrained%20classification%20tasks.%20The%20focus%20on%20image%20datasets%2C%0Aknown%20for%20their%20out-of-distribution%20characteristics%2C%20adds%20a%20layer%20of%20complexity%0Aand%20relevance%20to%20our%20study.%20Through%20a%20detailed%20evaluation%2C%20we%20illustrate%20the%0Aimproved%20AL%20performance%20on%20these%20challenging%20datasets%2C%20highlighting%20the%0Astrategic%20advantage%20of%20merging%20parameter%20efficient%20fine%20tuning%20methods%20with%0Afoundation%20models.%20This%20contributes%20to%20the%20broader%20discourse%20on%20optimizing%20AL%0Astrategies%2C%20presenting%20a%20promising%20avenue%20for%20future%20exploration%20in%20leveraging%0Afoundation%20models%20for%20efficient%20and%20effective%20data%20annotation%20in%20specialized%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520Active%2520Learning%2520for%2520Foundational%2520models%26entry.906535625%3DAthmanarayanan%2520Lakshmi%2520Narayanan%2520and%2520Ranganath%2520Krishnan%2520and%2520Amrutha%2520Machireddy%2520and%2520Mahesh%2520Subedar%26entry.1292438233%3D%2520%2520Foundational%2520vision%2520transformer%2520models%2520have%2520shown%2520impressive%2520few%2520shot%250Aperformance%2520on%2520many%2520vision%2520tasks.%2520This%2520research%2520presents%2520a%2520novel%2520investigation%250Ainto%2520the%2520application%2520of%2520parameter%2520efficient%2520fine-tuning%2520methods%2520within%2520an%250Aactive%2520learning%2520%2528AL%2529%2520framework%252C%2520to%2520advance%2520the%2520sampling%2520selection%2520process%2520in%250Aextremely%2520budget%2520constrained%2520classification%2520tasks.%2520The%2520focus%2520on%2520image%2520datasets%252C%250Aknown%2520for%2520their%2520out-of-distribution%2520characteristics%252C%2520adds%2520a%2520layer%2520of%2520complexity%250Aand%2520relevance%2520to%2520our%2520study.%2520Through%2520a%2520detailed%2520evaluation%252C%2520we%2520illustrate%2520the%250Aimproved%2520AL%2520performance%2520on%2520these%2520challenging%2520datasets%252C%2520highlighting%2520the%250Astrategic%2520advantage%2520of%2520merging%2520parameter%2520efficient%2520fine%2520tuning%2520methods%2520with%250Afoundation%2520models.%2520This%2520contributes%2520to%2520the%2520broader%2520discourse%2520on%2520optimizing%2520AL%250Astrategies%252C%2520presenting%2520a%2520promising%2520avenue%2520for%2520future%2520exploration%2520in%2520leveraging%250Afoundation%2520models%2520for%2520efficient%2520and%2520effective%2520data%2520annotation%2520in%2520specialized%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20Active%20Learning%20for%20Foundational%20models&entry.906535625=Athmanarayanan%20Lakshmi%20Narayanan%20and%20Ranganath%20Krishnan%20and%20Amrutha%20Machireddy%20and%20Mahesh%20Subedar&entry.1292438233=%20%20Foundational%20vision%20transformer%20models%20have%20shown%20impressive%20few%20shot%0Aperformance%20on%20many%20vision%20tasks.%20This%20research%20presents%20a%20novel%20investigation%0Ainto%20the%20application%20of%20parameter%20efficient%20fine-tuning%20methods%20within%20an%0Aactive%20learning%20%28AL%29%20framework%2C%20to%20advance%20the%20sampling%20selection%20process%20in%0Aextremely%20budget%20constrained%20classification%20tasks.%20The%20focus%20on%20image%20datasets%2C%0Aknown%20for%20their%20out-of-distribution%20characteristics%2C%20adds%20a%20layer%20of%20complexity%0Aand%20relevance%20to%20our%20study.%20Through%20a%20detailed%20evaluation%2C%20we%20illustrate%20the%0Aimproved%20AL%20performance%20on%20these%20challenging%20datasets%2C%20highlighting%20the%0Astrategic%20advantage%20of%20merging%20parameter%20efficient%20fine%20tuning%20methods%20with%0Afoundation%20models.%20This%20contributes%20to%20the%20broader%20discourse%20on%20optimizing%20AL%0Astrategies%2C%20presenting%20a%20promising%20avenue%20for%20future%20exploration%20in%20leveraging%0Afoundation%20models%20for%20efficient%20and%20effective%20data%20annotation%20in%20specialized%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09296v1&entry.124074799=Read"},
{"title": "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large\n  Vision-Language Models", "author": "Yuhang Wu and Wenmeng Yu and Yean Cheng and Yan Wang and Xiaohan Zhang and Jiazheng Xu and Ming Ding and Yuxiao Dong", "abstract": "  Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, a comprehensive alignment\nbenchmark specifically designed for emerging Chinese VLMs. This benchmark is\nmeticulously curated from real-world scenarios and Chinese Internet sources,\nencompassing thirteen specific tasks across three categories, and includes both\nsingle-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite\nstrategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer\npairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a\nrule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we\nreport the performance of representative VLMs on AlignMMBench, offering\ninsights into the capabilities and limitations of different VLM architectures.\nAll evaluation codes and data are available on https://alignmmbench.github.io.\n", "link": "http://arxiv.org/abs/2406.09295v1", "date": "2024-06-13", "relevancy": 2.0468, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4913}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignMMBench%3A%20Evaluating%20Chinese%20Multimodal%20Alignment%20in%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20AlignMMBench%3A%20Evaluating%20Chinese%20Multimodal%20Alignment%20in%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Yuhang%20Wu%20and%20Wenmeng%20Yu%20and%20Yean%20Cheng%20and%20Yan%20Wang%20and%20Xiaohan%20Zhang%20and%20Jiazheng%20Xu%20and%20Ming%20Ding%20and%20Yuxiao%20Dong%0AAbstract%3A%20%20%20Evaluating%20the%20alignment%20capabilities%20of%20large%20Vision-Language%20Models%20%28VLMs%29%0Ais%20essential%20for%20determining%20their%20effectiveness%20as%20helpful%20assistants.%0AHowever%2C%20existing%20benchmarks%20primarily%20focus%20on%20basic%20abilities%20using%20nonverbal%0Amethods%2C%20such%20as%20yes-no%20and%20multiple-choice%20questions.%20In%20this%20paper%2C%20we%0Aaddress%20this%20gap%20by%20introducing%20AlignMMBench%2C%20a%20comprehensive%20alignment%0Abenchmark%20specifically%20designed%20for%20emerging%20Chinese%20VLMs.%20This%20benchmark%20is%0Ameticulously%20curated%20from%20real-world%20scenarios%20and%20Chinese%20Internet%20sources%2C%0Aencompassing%20thirteen%20specific%20tasks%20across%20three%20categories%2C%20and%20includes%20both%0Asingle-turn%20and%20multi-turn%20dialogue%20scenarios.%20Incorporating%20a%20prompt%20rewrite%0Astrategy%2C%20AlignMMBench%20encompasses%201%2C054%20images%20and%204%2C978%20question-answer%0Apairs.%20To%20facilitate%20the%20evaluation%20pipeline%2C%20we%20propose%20CritiqueVLM%2C%20a%0Arule-calibrated%20evaluator%20that%20exceeds%20GPT-4%27s%20evaluation%20ability.%20Finally%2C%20we%0Areport%20the%20performance%20of%20representative%20VLMs%20on%20AlignMMBench%2C%20offering%0Ainsights%20into%20the%20capabilities%20and%20limitations%20of%20different%20VLM%20architectures.%0AAll%20evaluation%20codes%20and%20data%20are%20available%20on%20https%3A//alignmmbench.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignMMBench%253A%2520Evaluating%2520Chinese%2520Multimodal%2520Alignment%2520in%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DYuhang%2520Wu%2520and%2520Wenmeng%2520Yu%2520and%2520Yean%2520Cheng%2520and%2520Yan%2520Wang%2520and%2520Xiaohan%2520Zhang%2520and%2520Jiazheng%2520Xu%2520and%2520Ming%2520Ding%2520and%2520Yuxiao%2520Dong%26entry.1292438233%3D%2520%2520Evaluating%2520the%2520alignment%2520capabilities%2520of%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%250Ais%2520essential%2520for%2520determining%2520their%2520effectiveness%2520as%2520helpful%2520assistants.%250AHowever%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%2520basic%2520abilities%2520using%2520nonverbal%250Amethods%252C%2520such%2520as%2520yes-no%2520and%2520multiple-choice%2520questions.%2520In%2520this%2520paper%252C%2520we%250Aaddress%2520this%2520gap%2520by%2520introducing%2520AlignMMBench%252C%2520a%2520comprehensive%2520alignment%250Abenchmark%2520specifically%2520designed%2520for%2520emerging%2520Chinese%2520VLMs.%2520This%2520benchmark%2520is%250Ameticulously%2520curated%2520from%2520real-world%2520scenarios%2520and%2520Chinese%2520Internet%2520sources%252C%250Aencompassing%2520thirteen%2520specific%2520tasks%2520across%2520three%2520categories%252C%2520and%2520includes%2520both%250Asingle-turn%2520and%2520multi-turn%2520dialogue%2520scenarios.%2520Incorporating%2520a%2520prompt%2520rewrite%250Astrategy%252C%2520AlignMMBench%2520encompasses%25201%252C054%2520images%2520and%25204%252C978%2520question-answer%250Apairs.%2520To%2520facilitate%2520the%2520evaluation%2520pipeline%252C%2520we%2520propose%2520CritiqueVLM%252C%2520a%250Arule-calibrated%2520evaluator%2520that%2520exceeds%2520GPT-4%2527s%2520evaluation%2520ability.%2520Finally%252C%2520we%250Areport%2520the%2520performance%2520of%2520representative%2520VLMs%2520on%2520AlignMMBench%252C%2520offering%250Ainsights%2520into%2520the%2520capabilities%2520and%2520limitations%2520of%2520different%2520VLM%2520architectures.%250AAll%2520evaluation%2520codes%2520and%2520data%2520are%2520available%2520on%2520https%253A//alignmmbench.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignMMBench%3A%20Evaluating%20Chinese%20Multimodal%20Alignment%20in%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Yuhang%20Wu%20and%20Wenmeng%20Yu%20and%20Yean%20Cheng%20and%20Yan%20Wang%20and%20Xiaohan%20Zhang%20and%20Jiazheng%20Xu%20and%20Ming%20Ding%20and%20Yuxiao%20Dong&entry.1292438233=%20%20Evaluating%20the%20alignment%20capabilities%20of%20large%20Vision-Language%20Models%20%28VLMs%29%0Ais%20essential%20for%20determining%20their%20effectiveness%20as%20helpful%20assistants.%0AHowever%2C%20existing%20benchmarks%20primarily%20focus%20on%20basic%20abilities%20using%20nonverbal%0Amethods%2C%20such%20as%20yes-no%20and%20multiple-choice%20questions.%20In%20this%20paper%2C%20we%0Aaddress%20this%20gap%20by%20introducing%20AlignMMBench%2C%20a%20comprehensive%20alignment%0Abenchmark%20specifically%20designed%20for%20emerging%20Chinese%20VLMs.%20This%20benchmark%20is%0Ameticulously%20curated%20from%20real-world%20scenarios%20and%20Chinese%20Internet%20sources%2C%0Aencompassing%20thirteen%20specific%20tasks%20across%20three%20categories%2C%20and%20includes%20both%0Asingle-turn%20and%20multi-turn%20dialogue%20scenarios.%20Incorporating%20a%20prompt%20rewrite%0Astrategy%2C%20AlignMMBench%20encompasses%201%2C054%20images%20and%204%2C978%20question-answer%0Apairs.%20To%20facilitate%20the%20evaluation%20pipeline%2C%20we%20propose%20CritiqueVLM%2C%20a%0Arule-calibrated%20evaluator%20that%20exceeds%20GPT-4%27s%20evaluation%20ability.%20Finally%2C%20we%0Areport%20the%20performance%20of%20representative%20VLMs%20on%20AlignMMBench%2C%20offering%0Ainsights%20into%20the%20capabilities%20and%20limitations%20of%20different%20VLM%20architectures.%0AAll%20evaluation%20codes%20and%20data%20are%20available%20on%20https%3A//alignmmbench.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09295v1&entry.124074799=Read"},
{"title": "Thoracic Surgery Video Analysis for Surgical Phase Recognition", "author": "Syed Abdul Mateen and Niharika Malvia and Syed Abdul Khader and Danny Wang and Deepti Srinivasan and Chi-Fu Jeffrey Yang and Lana Schumacher and Sandeep Manjanna", "abstract": "  This paper presents an approach for surgical phase recognition using video\ndata, aiming to provide a comprehensive understanding of surgical procedures\nfor automated workflow analysis. The advent of robotic surgery, digitized\noperating rooms, and the generation of vast amounts of data have opened doors\nfor the application of machine learning and computer vision in the analysis of\nsurgical videos. Among these advancements, Surgical Phase Recognition(SPR)\nstands out as an emerging technology that has the potential to recognize and\nassess the ongoing surgical scenario, summarize the surgery, evaluate surgical\nskills, offer surgical decision support, and facilitate medical training. In\nthis paper, we analyse and evaluate both frame-based and video clipping-based\nphase recognition on thoracic surgery dataset consisting of 11 classes of\nphases. Specifically, we utilize ImageNet ViT for image-based classification\nand VideoMAE as the baseline model for video-based classification. We show that\nMasked Video Distillation(MVD) exhibits superior performance, achieving a top-1\naccuracy of 72.9%, compared to 52.31% achieved by ImageNet ViT. These findings\nunderscore the efficacy of video-based classifiers over their image-based\ncounterparts in surgical phase recognition tasks.\n", "link": "http://arxiv.org/abs/2406.09185v1", "date": "2024-06-13", "relevancy": 1.8924, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4639}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thoracic%20Surgery%20Video%20Analysis%20for%20Surgical%20Phase%20Recognition&body=Title%3A%20Thoracic%20Surgery%20Video%20Analysis%20for%20Surgical%20Phase%20Recognition%0AAuthor%3A%20Syed%20Abdul%20Mateen%20and%20Niharika%20Malvia%20and%20Syed%20Abdul%20Khader%20and%20Danny%20Wang%20and%20Deepti%20Srinivasan%20and%20Chi-Fu%20Jeffrey%20Yang%20and%20Lana%20Schumacher%20and%20Sandeep%20Manjanna%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20approach%20for%20surgical%20phase%20recognition%20using%20video%0Adata%2C%20aiming%20to%20provide%20a%20comprehensive%20understanding%20of%20surgical%20procedures%0Afor%20automated%20workflow%20analysis.%20The%20advent%20of%20robotic%20surgery%2C%20digitized%0Aoperating%20rooms%2C%20and%20the%20generation%20of%20vast%20amounts%20of%20data%20have%20opened%20doors%0Afor%20the%20application%20of%20machine%20learning%20and%20computer%20vision%20in%20the%20analysis%20of%0Asurgical%20videos.%20Among%20these%20advancements%2C%20Surgical%20Phase%20Recognition%28SPR%29%0Astands%20out%20as%20an%20emerging%20technology%20that%20has%20the%20potential%20to%20recognize%20and%0Aassess%20the%20ongoing%20surgical%20scenario%2C%20summarize%20the%20surgery%2C%20evaluate%20surgical%0Askills%2C%20offer%20surgical%20decision%20support%2C%20and%20facilitate%20medical%20training.%20In%0Athis%20paper%2C%20we%20analyse%20and%20evaluate%20both%20frame-based%20and%20video%20clipping-based%0Aphase%20recognition%20on%20thoracic%20surgery%20dataset%20consisting%20of%2011%20classes%20of%0Aphases.%20Specifically%2C%20we%20utilize%20ImageNet%20ViT%20for%20image-based%20classification%0Aand%20VideoMAE%20as%20the%20baseline%20model%20for%20video-based%20classification.%20We%20show%20that%0AMasked%20Video%20Distillation%28MVD%29%20exhibits%20superior%20performance%2C%20achieving%20a%20top-1%0Aaccuracy%20of%2072.9%25%2C%20compared%20to%2052.31%25%20achieved%20by%20ImageNet%20ViT.%20These%20findings%0Aunderscore%20the%20efficacy%20of%20video-based%20classifiers%20over%20their%20image-based%0Acounterparts%20in%20surgical%20phase%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThoracic%2520Surgery%2520Video%2520Analysis%2520for%2520Surgical%2520Phase%2520Recognition%26entry.906535625%3DSyed%2520Abdul%2520Mateen%2520and%2520Niharika%2520Malvia%2520and%2520Syed%2520Abdul%2520Khader%2520and%2520Danny%2520Wang%2520and%2520Deepti%2520Srinivasan%2520and%2520Chi-Fu%2520Jeffrey%2520Yang%2520and%2520Lana%2520Schumacher%2520and%2520Sandeep%2520Manjanna%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520approach%2520for%2520surgical%2520phase%2520recognition%2520using%2520video%250Adata%252C%2520aiming%2520to%2520provide%2520a%2520comprehensive%2520understanding%2520of%2520surgical%2520procedures%250Afor%2520automated%2520workflow%2520analysis.%2520The%2520advent%2520of%2520robotic%2520surgery%252C%2520digitized%250Aoperating%2520rooms%252C%2520and%2520the%2520generation%2520of%2520vast%2520amounts%2520of%2520data%2520have%2520opened%2520doors%250Afor%2520the%2520application%2520of%2520machine%2520learning%2520and%2520computer%2520vision%2520in%2520the%2520analysis%2520of%250Asurgical%2520videos.%2520Among%2520these%2520advancements%252C%2520Surgical%2520Phase%2520Recognition%2528SPR%2529%250Astands%2520out%2520as%2520an%2520emerging%2520technology%2520that%2520has%2520the%2520potential%2520to%2520recognize%2520and%250Aassess%2520the%2520ongoing%2520surgical%2520scenario%252C%2520summarize%2520the%2520surgery%252C%2520evaluate%2520surgical%250Askills%252C%2520offer%2520surgical%2520decision%2520support%252C%2520and%2520facilitate%2520medical%2520training.%2520In%250Athis%2520paper%252C%2520we%2520analyse%2520and%2520evaluate%2520both%2520frame-based%2520and%2520video%2520clipping-based%250Aphase%2520recognition%2520on%2520thoracic%2520surgery%2520dataset%2520consisting%2520of%252011%2520classes%2520of%250Aphases.%2520Specifically%252C%2520we%2520utilize%2520ImageNet%2520ViT%2520for%2520image-based%2520classification%250Aand%2520VideoMAE%2520as%2520the%2520baseline%2520model%2520for%2520video-based%2520classification.%2520We%2520show%2520that%250AMasked%2520Video%2520Distillation%2528MVD%2529%2520exhibits%2520superior%2520performance%252C%2520achieving%2520a%2520top-1%250Aaccuracy%2520of%252072.9%2525%252C%2520compared%2520to%252052.31%2525%2520achieved%2520by%2520ImageNet%2520ViT.%2520These%2520findings%250Aunderscore%2520the%2520efficacy%2520of%2520video-based%2520classifiers%2520over%2520their%2520image-based%250Acounterparts%2520in%2520surgical%2520phase%2520recognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thoracic%20Surgery%20Video%20Analysis%20for%20Surgical%20Phase%20Recognition&entry.906535625=Syed%20Abdul%20Mateen%20and%20Niharika%20Malvia%20and%20Syed%20Abdul%20Khader%20and%20Danny%20Wang%20and%20Deepti%20Srinivasan%20and%20Chi-Fu%20Jeffrey%20Yang%20and%20Lana%20Schumacher%20and%20Sandeep%20Manjanna&entry.1292438233=%20%20This%20paper%20presents%20an%20approach%20for%20surgical%20phase%20recognition%20using%20video%0Adata%2C%20aiming%20to%20provide%20a%20comprehensive%20understanding%20of%20surgical%20procedures%0Afor%20automated%20workflow%20analysis.%20The%20advent%20of%20robotic%20surgery%2C%20digitized%0Aoperating%20rooms%2C%20and%20the%20generation%20of%20vast%20amounts%20of%20data%20have%20opened%20doors%0Afor%20the%20application%20of%20machine%20learning%20and%20computer%20vision%20in%20the%20analysis%20of%0Asurgical%20videos.%20Among%20these%20advancements%2C%20Surgical%20Phase%20Recognition%28SPR%29%0Astands%20out%20as%20an%20emerging%20technology%20that%20has%20the%20potential%20to%20recognize%20and%0Aassess%20the%20ongoing%20surgical%20scenario%2C%20summarize%20the%20surgery%2C%20evaluate%20surgical%0Askills%2C%20offer%20surgical%20decision%20support%2C%20and%20facilitate%20medical%20training.%20In%0Athis%20paper%2C%20we%20analyse%20and%20evaluate%20both%20frame-based%20and%20video%20clipping-based%0Aphase%20recognition%20on%20thoracic%20surgery%20dataset%20consisting%20of%2011%20classes%20of%0Aphases.%20Specifically%2C%20we%20utilize%20ImageNet%20ViT%20for%20image-based%20classification%0Aand%20VideoMAE%20as%20the%20baseline%20model%20for%20video-based%20classification.%20We%20show%20that%0AMasked%20Video%20Distillation%28MVD%29%20exhibits%20superior%20performance%2C%20achieving%20a%20top-1%0Aaccuracy%20of%2072.9%25%2C%20compared%20to%2052.31%25%20achieved%20by%20ImageNet%20ViT.%20These%20findings%0Aunderscore%20the%20efficacy%20of%20video-based%20classifiers%20over%20their%20image-based%0Acounterparts%20in%20surgical%20phase%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09185v1&entry.124074799=Read"},
{"title": "Multimodal Learning Without Labeled Multimodal Data: Guarantees and\n  Applications", "author": "Paul Pu Liang and Chun Kai Ling and Yun Cheng and Alex Obolenskiy and Yudong Liu and Rohan Pandey and Alex Wilf and Louis-Philippe Morency and Ruslan Salakhutdinov", "abstract": "  In many machine learning systems that jointly learn from multiple modalities,\na core research question is to understand the nature of multimodal\ninteractions: how modalities combine to provide new task-relevant information\nthat was not present in either alone. We study this challenge of interaction\nquantification in a semi-supervised setting with only labeled unimodal data and\nnaturally co-occurring multimodal data (e.g., unlabeled images and captions,\nvideo and corresponding audio) but when labeling them is time-consuming. Using\na precise information-theoretic definition of interactions, our key\ncontribution is the derivation of lower and upper bounds to quantify the amount\nof multimodal interactions in this semi-supervised setting. We propose two\nlower bounds: one based on the shared information between modalities and the\nother based on disagreement between separately trained unimodal classifiers,\nand derive an upper bound through connections to approximate algorithms for\nmin-entropy couplings. We validate these estimated bounds and show how they\naccurately track true interactions. Finally, we show how these theoretical\nresults can be used to estimate multimodal model performance, guide data\ncollection, and select appropriate multimodal models for various tasks.\n", "link": "http://arxiv.org/abs/2306.04539v2", "date": "2024-06-13", "relevancy": 1.6592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5557}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Learning%20Without%20Labeled%20Multimodal%20Data%3A%20Guarantees%20and%0A%20%20Applications&body=Title%3A%20Multimodal%20Learning%20Without%20Labeled%20Multimodal%20Data%3A%20Guarantees%20and%0A%20%20Applications%0AAuthor%3A%20Paul%20Pu%20Liang%20and%20Chun%20Kai%20Ling%20and%20Yun%20Cheng%20and%20Alex%20Obolenskiy%20and%20Yudong%20Liu%20and%20Rohan%20Pandey%20and%20Alex%20Wilf%20and%20Louis-Philippe%20Morency%20and%20Ruslan%20Salakhutdinov%0AAbstract%3A%20%20%20In%20many%20machine%20learning%20systems%20that%20jointly%20learn%20from%20multiple%20modalities%2C%0Aa%20core%20research%20question%20is%20to%20understand%20the%20nature%20of%20multimodal%0Ainteractions%3A%20how%20modalities%20combine%20to%20provide%20new%20task-relevant%20information%0Athat%20was%20not%20present%20in%20either%20alone.%20We%20study%20this%20challenge%20of%20interaction%0Aquantification%20in%20a%20semi-supervised%20setting%20with%20only%20labeled%20unimodal%20data%20and%0Anaturally%20co-occurring%20multimodal%20data%20%28e.g.%2C%20unlabeled%20images%20and%20captions%2C%0Avideo%20and%20corresponding%20audio%29%20but%20when%20labeling%20them%20is%20time-consuming.%20Using%0Aa%20precise%20information-theoretic%20definition%20of%20interactions%2C%20our%20key%0Acontribution%20is%20the%20derivation%20of%20lower%20and%20upper%20bounds%20to%20quantify%20the%20amount%0Aof%20multimodal%20interactions%20in%20this%20semi-supervised%20setting.%20We%20propose%20two%0Alower%20bounds%3A%20one%20based%20on%20the%20shared%20information%20between%20modalities%20and%20the%0Aother%20based%20on%20disagreement%20between%20separately%20trained%20unimodal%20classifiers%2C%0Aand%20derive%20an%20upper%20bound%20through%20connections%20to%20approximate%20algorithms%20for%0Amin-entropy%20couplings.%20We%20validate%20these%20estimated%20bounds%20and%20show%20how%20they%0Aaccurately%20track%20true%20interactions.%20Finally%2C%20we%20show%20how%20these%20theoretical%0Aresults%20can%20be%20used%20to%20estimate%20multimodal%20model%20performance%2C%20guide%20data%0Acollection%2C%20and%20select%20appropriate%20multimodal%20models%20for%20various%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Learning%2520Without%2520Labeled%2520Multimodal%2520Data%253A%2520Guarantees%2520and%250A%2520%2520Applications%26entry.906535625%3DPaul%2520Pu%2520Liang%2520and%2520Chun%2520Kai%2520Ling%2520and%2520Yun%2520Cheng%2520and%2520Alex%2520Obolenskiy%2520and%2520Yudong%2520Liu%2520and%2520Rohan%2520Pandey%2520and%2520Alex%2520Wilf%2520and%2520Louis-Philippe%2520Morency%2520and%2520Ruslan%2520Salakhutdinov%26entry.1292438233%3D%2520%2520In%2520many%2520machine%2520learning%2520systems%2520that%2520jointly%2520learn%2520from%2520multiple%2520modalities%252C%250Aa%2520core%2520research%2520question%2520is%2520to%2520understand%2520the%2520nature%2520of%2520multimodal%250Ainteractions%253A%2520how%2520modalities%2520combine%2520to%2520provide%2520new%2520task-relevant%2520information%250Athat%2520was%2520not%2520present%2520in%2520either%2520alone.%2520We%2520study%2520this%2520challenge%2520of%2520interaction%250Aquantification%2520in%2520a%2520semi-supervised%2520setting%2520with%2520only%2520labeled%2520unimodal%2520data%2520and%250Anaturally%2520co-occurring%2520multimodal%2520data%2520%2528e.g.%252C%2520unlabeled%2520images%2520and%2520captions%252C%250Avideo%2520and%2520corresponding%2520audio%2529%2520but%2520when%2520labeling%2520them%2520is%2520time-consuming.%2520Using%250Aa%2520precise%2520information-theoretic%2520definition%2520of%2520interactions%252C%2520our%2520key%250Acontribution%2520is%2520the%2520derivation%2520of%2520lower%2520and%2520upper%2520bounds%2520to%2520quantify%2520the%2520amount%250Aof%2520multimodal%2520interactions%2520in%2520this%2520semi-supervised%2520setting.%2520We%2520propose%2520two%250Alower%2520bounds%253A%2520one%2520based%2520on%2520the%2520shared%2520information%2520between%2520modalities%2520and%2520the%250Aother%2520based%2520on%2520disagreement%2520between%2520separately%2520trained%2520unimodal%2520classifiers%252C%250Aand%2520derive%2520an%2520upper%2520bound%2520through%2520connections%2520to%2520approximate%2520algorithms%2520for%250Amin-entropy%2520couplings.%2520We%2520validate%2520these%2520estimated%2520bounds%2520and%2520show%2520how%2520they%250Aaccurately%2520track%2520true%2520interactions.%2520Finally%252C%2520we%2520show%2520how%2520these%2520theoretical%250Aresults%2520can%2520be%2520used%2520to%2520estimate%2520multimodal%2520model%2520performance%252C%2520guide%2520data%250Acollection%252C%2520and%2520select%2520appropriate%2520multimodal%2520models%2520for%2520various%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Learning%20Without%20Labeled%20Multimodal%20Data%3A%20Guarantees%20and%0A%20%20Applications&entry.906535625=Paul%20Pu%20Liang%20and%20Chun%20Kai%20Ling%20and%20Yun%20Cheng%20and%20Alex%20Obolenskiy%20and%20Yudong%20Liu%20and%20Rohan%20Pandey%20and%20Alex%20Wilf%20and%20Louis-Philippe%20Morency%20and%20Ruslan%20Salakhutdinov&entry.1292438233=%20%20In%20many%20machine%20learning%20systems%20that%20jointly%20learn%20from%20multiple%20modalities%2C%0Aa%20core%20research%20question%20is%20to%20understand%20the%20nature%20of%20multimodal%0Ainteractions%3A%20how%20modalities%20combine%20to%20provide%20new%20task-relevant%20information%0Athat%20was%20not%20present%20in%20either%20alone.%20We%20study%20this%20challenge%20of%20interaction%0Aquantification%20in%20a%20semi-supervised%20setting%20with%20only%20labeled%20unimodal%20data%20and%0Anaturally%20co-occurring%20multimodal%20data%20%28e.g.%2C%20unlabeled%20images%20and%20captions%2C%0Avideo%20and%20corresponding%20audio%29%20but%20when%20labeling%20them%20is%20time-consuming.%20Using%0Aa%20precise%20information-theoretic%20definition%20of%20interactions%2C%20our%20key%0Acontribution%20is%20the%20derivation%20of%20lower%20and%20upper%20bounds%20to%20quantify%20the%20amount%0Aof%20multimodal%20interactions%20in%20this%20semi-supervised%20setting.%20We%20propose%20two%0Alower%20bounds%3A%20one%20based%20on%20the%20shared%20information%20between%20modalities%20and%20the%0Aother%20based%20on%20disagreement%20between%20separately%20trained%20unimodal%20classifiers%2C%0Aand%20derive%20an%20upper%20bound%20through%20connections%20to%20approximate%20algorithms%20for%0Amin-entropy%20couplings.%20We%20validate%20these%20estimated%20bounds%20and%20show%20how%20they%0Aaccurately%20track%20true%20interactions.%20Finally%2C%20we%20show%20how%20these%20theoretical%0Aresults%20can%20be%20used%20to%20estimate%20multimodal%20model%20performance%2C%20guide%20data%0Acollection%2C%20and%20select%20appropriate%20multimodal%20models%20for%20various%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04539v2&entry.124074799=Read"},
{"title": "Data-dependent and Oracle Bounds on Forgetting in Continual Learning", "author": "Lior Friedman and Ron Meir", "abstract": "  In continual learning, knowledge must be preserved and re-used between tasks,\nmaintaining good transfer to future tasks and minimizing forgetting of\npreviously learned ones. While several practical algorithms have been devised\nfor this setting, there have been few theoretical works aiming to quantify and\nbound the degree of Forgetting in general settings. We provide both\ndata-dependent and oracle upper bounds that apply regardless of model and\nalgorithm choice, as well as bounds for Gibbs posteriors. We derive an\nalgorithm inspired by our bounds and demonstrate empirically that our approach\nyields improved forward and backward transfer.\n", "link": "http://arxiv.org/abs/2406.09370v1", "date": "2024-06-13", "relevancy": 1.82, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4602}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4579}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-dependent%20and%20Oracle%20Bounds%20on%20Forgetting%20in%20Continual%20Learning&body=Title%3A%20Data-dependent%20and%20Oracle%20Bounds%20on%20Forgetting%20in%20Continual%20Learning%0AAuthor%3A%20Lior%20Friedman%20and%20Ron%20Meir%0AAbstract%3A%20%20%20In%20continual%20learning%2C%20knowledge%20must%20be%20preserved%20and%20re-used%20between%20tasks%2C%0Amaintaining%20good%20transfer%20to%20future%20tasks%20and%20minimizing%20forgetting%20of%0Apreviously%20learned%20ones.%20While%20several%20practical%20algorithms%20have%20been%20devised%0Afor%20this%20setting%2C%20there%20have%20been%20few%20theoretical%20works%20aiming%20to%20quantify%20and%0Abound%20the%20degree%20of%20Forgetting%20in%20general%20settings.%20We%20provide%20both%0Adata-dependent%20and%20oracle%20upper%20bounds%20that%20apply%20regardless%20of%20model%20and%0Aalgorithm%20choice%2C%20as%20well%20as%20bounds%20for%20Gibbs%20posteriors.%20We%20derive%20an%0Aalgorithm%20inspired%20by%20our%20bounds%20and%20demonstrate%20empirically%20that%20our%20approach%0Ayields%20improved%20forward%20and%20backward%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-dependent%2520and%2520Oracle%2520Bounds%2520on%2520Forgetting%2520in%2520Continual%2520Learning%26entry.906535625%3DLior%2520Friedman%2520and%2520Ron%2520Meir%26entry.1292438233%3D%2520%2520In%2520continual%2520learning%252C%2520knowledge%2520must%2520be%2520preserved%2520and%2520re-used%2520between%2520tasks%252C%250Amaintaining%2520good%2520transfer%2520to%2520future%2520tasks%2520and%2520minimizing%2520forgetting%2520of%250Apreviously%2520learned%2520ones.%2520While%2520several%2520practical%2520algorithms%2520have%2520been%2520devised%250Afor%2520this%2520setting%252C%2520there%2520have%2520been%2520few%2520theoretical%2520works%2520aiming%2520to%2520quantify%2520and%250Abound%2520the%2520degree%2520of%2520Forgetting%2520in%2520general%2520settings.%2520We%2520provide%2520both%250Adata-dependent%2520and%2520oracle%2520upper%2520bounds%2520that%2520apply%2520regardless%2520of%2520model%2520and%250Aalgorithm%2520choice%252C%2520as%2520well%2520as%2520bounds%2520for%2520Gibbs%2520posteriors.%2520We%2520derive%2520an%250Aalgorithm%2520inspired%2520by%2520our%2520bounds%2520and%2520demonstrate%2520empirically%2520that%2520our%2520approach%250Ayields%2520improved%2520forward%2520and%2520backward%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-dependent%20and%20Oracle%20Bounds%20on%20Forgetting%20in%20Continual%20Learning&entry.906535625=Lior%20Friedman%20and%20Ron%20Meir&entry.1292438233=%20%20In%20continual%20learning%2C%20knowledge%20must%20be%20preserved%20and%20re-used%20between%20tasks%2C%0Amaintaining%20good%20transfer%20to%20future%20tasks%20and%20minimizing%20forgetting%20of%0Apreviously%20learned%20ones.%20While%20several%20practical%20algorithms%20have%20been%20devised%0Afor%20this%20setting%2C%20there%20have%20been%20few%20theoretical%20works%20aiming%20to%20quantify%20and%0Abound%20the%20degree%20of%20Forgetting%20in%20general%20settings.%20We%20provide%20both%0Adata-dependent%20and%20oracle%20upper%20bounds%20that%20apply%20regardless%20of%20model%20and%0Aalgorithm%20choice%2C%20as%20well%20as%20bounds%20for%20Gibbs%20posteriors.%20We%20derive%20an%0Aalgorithm%20inspired%20by%20our%20bounds%20and%20demonstrate%20empirically%20that%20our%20approach%0Ayields%20improved%20forward%20and%20backward%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09370v1&entry.124074799=Read"},
{"title": "LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities\n  of Living", "author": "Rajatsubhra Chakraborty and Arkaprava Sinha and Dominick Reilly and Manish Kumar Govind and Pu Wang and Francois Bremond and Srijan Das", "abstract": "  Large Language Vision Models (LLVMs) have demonstrated effectiveness in\nprocessing internet videos, yet they struggle with the visually perplexing\ndynamics present in Activities of Daily Living (ADL) due to limited pertinent\ndatasets and models tailored to relevant cues. To this end, we propose a\nframework for curating ADL multiview datasets to fine-tune LLVMs, resulting in\nthe creation of ADL-X, comprising 100K RGB video-instruction pairs, language\ndescriptions, 3D skeletons, and action-conditioned object trajectories. We\nintroduce LLAVIDAL, an LLVM capable of incorporating 3D poses and relevant\nobject trajectories to understand the intricate spatiotemporal relationships\nwithin ADLs. Furthermore, we present a novel benchmark, ADLMCQ, for quantifying\nLLVM effectiveness in ADL scenarios. When trained on ADL-X, LLAVIDAL\nconsistently achieves state-of-the-art performance across all ADL evaluation\nmetrics. Qualitative analysis reveals LLAVIDAL's temporal reasoning\ncapabilities in understanding ADL. The link to the dataset is provided at:\nhttps://adl-x.github.io/\n", "link": "http://arxiv.org/abs/2406.09390v1", "date": "2024-06-13", "relevancy": 1.6798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5741}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5528}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLAVIDAL%3A%20Benchmarking%20Large%20Language%20Vision%20Models%20for%20Daily%20Activities%0A%20%20of%20Living&body=Title%3A%20LLAVIDAL%3A%20Benchmarking%20Large%20Language%20Vision%20Models%20for%20Daily%20Activities%0A%20%20of%20Living%0AAuthor%3A%20Rajatsubhra%20Chakraborty%20and%20Arkaprava%20Sinha%20and%20Dominick%20Reilly%20and%20Manish%20Kumar%20Govind%20and%20Pu%20Wang%20and%20Francois%20Bremond%20and%20Srijan%20Das%0AAbstract%3A%20%20%20Large%20Language%20Vision%20Models%20%28LLVMs%29%20have%20demonstrated%20effectiveness%20in%0Aprocessing%20internet%20videos%2C%20yet%20they%20struggle%20with%20the%20visually%20perplexing%0Adynamics%20present%20in%20Activities%20of%20Daily%20Living%20%28ADL%29%20due%20to%20limited%20pertinent%0Adatasets%20and%20models%20tailored%20to%20relevant%20cues.%20To%20this%20end%2C%20we%20propose%20a%0Aframework%20for%20curating%20ADL%20multiview%20datasets%20to%20fine-tune%20LLVMs%2C%20resulting%20in%0Athe%20creation%20of%20ADL-X%2C%20comprising%20100K%20RGB%20video-instruction%20pairs%2C%20language%0Adescriptions%2C%203D%20skeletons%2C%20and%20action-conditioned%20object%20trajectories.%20We%0Aintroduce%20LLAVIDAL%2C%20an%20LLVM%20capable%20of%20incorporating%203D%20poses%20and%20relevant%0Aobject%20trajectories%20to%20understand%20the%20intricate%20spatiotemporal%20relationships%0Awithin%20ADLs.%20Furthermore%2C%20we%20present%20a%20novel%20benchmark%2C%20ADLMCQ%2C%20for%20quantifying%0ALLVM%20effectiveness%20in%20ADL%20scenarios.%20When%20trained%20on%20ADL-X%2C%20LLAVIDAL%0Aconsistently%20achieves%20state-of-the-art%20performance%20across%20all%20ADL%20evaluation%0Ametrics.%20Qualitative%20analysis%20reveals%20LLAVIDAL%27s%20temporal%20reasoning%0Acapabilities%20in%20understanding%20ADL.%20The%20link%20to%20the%20dataset%20is%20provided%20at%3A%0Ahttps%3A//adl-x.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLAVIDAL%253A%2520Benchmarking%2520Large%2520Language%2520Vision%2520Models%2520for%2520Daily%2520Activities%250A%2520%2520of%2520Living%26entry.906535625%3DRajatsubhra%2520Chakraborty%2520and%2520Arkaprava%2520Sinha%2520and%2520Dominick%2520Reilly%2520and%2520Manish%2520Kumar%2520Govind%2520and%2520Pu%2520Wang%2520and%2520Francois%2520Bremond%2520and%2520Srijan%2520Das%26entry.1292438233%3D%2520%2520Large%2520Language%2520Vision%2520Models%2520%2528LLVMs%2529%2520have%2520demonstrated%2520effectiveness%2520in%250Aprocessing%2520internet%2520videos%252C%2520yet%2520they%2520struggle%2520with%2520the%2520visually%2520perplexing%250Adynamics%2520present%2520in%2520Activities%2520of%2520Daily%2520Living%2520%2528ADL%2529%2520due%2520to%2520limited%2520pertinent%250Adatasets%2520and%2520models%2520tailored%2520to%2520relevant%2520cues.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Aframework%2520for%2520curating%2520ADL%2520multiview%2520datasets%2520to%2520fine-tune%2520LLVMs%252C%2520resulting%2520in%250Athe%2520creation%2520of%2520ADL-X%252C%2520comprising%2520100K%2520RGB%2520video-instruction%2520pairs%252C%2520language%250Adescriptions%252C%25203D%2520skeletons%252C%2520and%2520action-conditioned%2520object%2520trajectories.%2520We%250Aintroduce%2520LLAVIDAL%252C%2520an%2520LLVM%2520capable%2520of%2520incorporating%25203D%2520poses%2520and%2520relevant%250Aobject%2520trajectories%2520to%2520understand%2520the%2520intricate%2520spatiotemporal%2520relationships%250Awithin%2520ADLs.%2520Furthermore%252C%2520we%2520present%2520a%2520novel%2520benchmark%252C%2520ADLMCQ%252C%2520for%2520quantifying%250ALLVM%2520effectiveness%2520in%2520ADL%2520scenarios.%2520When%2520trained%2520on%2520ADL-X%252C%2520LLAVIDAL%250Aconsistently%2520achieves%2520state-of-the-art%2520performance%2520across%2520all%2520ADL%2520evaluation%250Ametrics.%2520Qualitative%2520analysis%2520reveals%2520LLAVIDAL%2527s%2520temporal%2520reasoning%250Acapabilities%2520in%2520understanding%2520ADL.%2520The%2520link%2520to%2520the%2520dataset%2520is%2520provided%2520at%253A%250Ahttps%253A//adl-x.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLAVIDAL%3A%20Benchmarking%20Large%20Language%20Vision%20Models%20for%20Daily%20Activities%0A%20%20of%20Living&entry.906535625=Rajatsubhra%20Chakraborty%20and%20Arkaprava%20Sinha%20and%20Dominick%20Reilly%20and%20Manish%20Kumar%20Govind%20and%20Pu%20Wang%20and%20Francois%20Bremond%20and%20Srijan%20Das&entry.1292438233=%20%20Large%20Language%20Vision%20Models%20%28LLVMs%29%20have%20demonstrated%20effectiveness%20in%0Aprocessing%20internet%20videos%2C%20yet%20they%20struggle%20with%20the%20visually%20perplexing%0Adynamics%20present%20in%20Activities%20of%20Daily%20Living%20%28ADL%29%20due%20to%20limited%20pertinent%0Adatasets%20and%20models%20tailored%20to%20relevant%20cues.%20To%20this%20end%2C%20we%20propose%20a%0Aframework%20for%20curating%20ADL%20multiview%20datasets%20to%20fine-tune%20LLVMs%2C%20resulting%20in%0Athe%20creation%20of%20ADL-X%2C%20comprising%20100K%20RGB%20video-instruction%20pairs%2C%20language%0Adescriptions%2C%203D%20skeletons%2C%20and%20action-conditioned%20object%20trajectories.%20We%0Aintroduce%20LLAVIDAL%2C%20an%20LLVM%20capable%20of%20incorporating%203D%20poses%20and%20relevant%0Aobject%20trajectories%20to%20understand%20the%20intricate%20spatiotemporal%20relationships%0Awithin%20ADLs.%20Furthermore%2C%20we%20present%20a%20novel%20benchmark%2C%20ADLMCQ%2C%20for%20quantifying%0ALLVM%20effectiveness%20in%20ADL%20scenarios.%20When%20trained%20on%20ADL-X%2C%20LLAVIDAL%0Aconsistently%20achieves%20state-of-the-art%20performance%20across%20all%20ADL%20evaluation%0Ametrics.%20Qualitative%20analysis%20reveals%20LLAVIDAL%27s%20temporal%20reasoning%0Acapabilities%20in%20understanding%20ADL.%20The%20link%20to%20the%20dataset%20is%20provided%20at%3A%0Ahttps%3A//adl-x.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09390v1&entry.124074799=Read"},
{"title": "Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking\n  Scores", "author": "\u00c1lvaro Ciudad and Adri\u00e1n Morales-Pastor and Laura Malo and Isaac Filella-Merc\u00e8 and Victor Guallar and Alexis Molina", "abstract": "  In this study, we present ScoreFormer, a novel graph transformer model\ndesigned to accurately predict molecular docking scores, thereby optimizing\nhigh-throughput virtual screening (HTVS) in drug discovery. The architecture\nintegrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk\nPositional Encodings (LRWPE), enhancing the model's ability to understand\ncomplex molecular structures and their relationship with their respective\ndocking scores. This approach significantly surpasses traditional HTVS methods\nand recent Graph Neural Network (GNN) models in both recovery and efficiency\ndue to a wider coverage of the chemical space and enhanced performance. Our\nresults demonstrate that ScoreFormer achieves competitive performance in\ndocking score prediction and offers a substantial 1.65-fold reduction in\ninference time compared to existing models. We evaluated ScoreFormer across\nmultiple datasets under various conditions, confirming its robustness and\nreliability in identifying potential drug candidates rapidly.\n", "link": "http://arxiv.org/abs/2406.09346v1", "date": "2024-06-13", "relevancy": 1.3682, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4678}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4653}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scoreformer%3A%20A%20Surrogate%20Model%20For%20Large-Scale%20Prediction%20of%20Docking%0A%20%20Scores&body=Title%3A%20Scoreformer%3A%20A%20Surrogate%20Model%20For%20Large-Scale%20Prediction%20of%20Docking%0A%20%20Scores%0AAuthor%3A%20%C3%81lvaro%20Ciudad%20and%20Adri%C3%A1n%20Morales-Pastor%20and%20Laura%20Malo%20and%20Isaac%20Filella-Merc%C3%A8%20and%20Victor%20Guallar%20and%20Alexis%20Molina%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20present%20ScoreFormer%2C%20a%20novel%20graph%20transformer%20model%0Adesigned%20to%20accurately%20predict%20molecular%20docking%20scores%2C%20thereby%20optimizing%0Ahigh-throughput%20virtual%20screening%20%28HTVS%29%20in%20drug%20discovery.%20The%20architecture%0Aintegrates%20Principal%20Neighborhood%20Aggregation%20%28PNA%29%20and%20Learnable%20Random%20Walk%0APositional%20Encodings%20%28LRWPE%29%2C%20enhancing%20the%20model%27s%20ability%20to%20understand%0Acomplex%20molecular%20structures%20and%20their%20relationship%20with%20their%20respective%0Adocking%20scores.%20This%20approach%20significantly%20surpasses%20traditional%20HTVS%20methods%0Aand%20recent%20Graph%20Neural%20Network%20%28GNN%29%20models%20in%20both%20recovery%20and%20efficiency%0Adue%20to%20a%20wider%20coverage%20of%20the%20chemical%20space%20and%20enhanced%20performance.%20Our%0Aresults%20demonstrate%20that%20ScoreFormer%20achieves%20competitive%20performance%20in%0Adocking%20score%20prediction%20and%20offers%20a%20substantial%201.65-fold%20reduction%20in%0Ainference%20time%20compared%20to%20existing%20models.%20We%20evaluated%20ScoreFormer%20across%0Amultiple%20datasets%20under%20various%20conditions%2C%20confirming%20its%20robustness%20and%0Areliability%20in%20identifying%20potential%20drug%20candidates%20rapidly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScoreformer%253A%2520A%2520Surrogate%2520Model%2520For%2520Large-Scale%2520Prediction%2520of%2520Docking%250A%2520%2520Scores%26entry.906535625%3D%25C3%2581lvaro%2520Ciudad%2520and%2520Adri%25C3%25A1n%2520Morales-Pastor%2520and%2520Laura%2520Malo%2520and%2520Isaac%2520Filella-Merc%25C3%25A8%2520and%2520Victor%2520Guallar%2520and%2520Alexis%2520Molina%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520present%2520ScoreFormer%252C%2520a%2520novel%2520graph%2520transformer%2520model%250Adesigned%2520to%2520accurately%2520predict%2520molecular%2520docking%2520scores%252C%2520thereby%2520optimizing%250Ahigh-throughput%2520virtual%2520screening%2520%2528HTVS%2529%2520in%2520drug%2520discovery.%2520The%2520architecture%250Aintegrates%2520Principal%2520Neighborhood%2520Aggregation%2520%2528PNA%2529%2520and%2520Learnable%2520Random%2520Walk%250APositional%2520Encodings%2520%2528LRWPE%2529%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520understand%250Acomplex%2520molecular%2520structures%2520and%2520their%2520relationship%2520with%2520their%2520respective%250Adocking%2520scores.%2520This%2520approach%2520significantly%2520surpasses%2520traditional%2520HTVS%2520methods%250Aand%2520recent%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520models%2520in%2520both%2520recovery%2520and%2520efficiency%250Adue%2520to%2520a%2520wider%2520coverage%2520of%2520the%2520chemical%2520space%2520and%2520enhanced%2520performance.%2520Our%250Aresults%2520demonstrate%2520that%2520ScoreFormer%2520achieves%2520competitive%2520performance%2520in%250Adocking%2520score%2520prediction%2520and%2520offers%2520a%2520substantial%25201.65-fold%2520reduction%2520in%250Ainference%2520time%2520compared%2520to%2520existing%2520models.%2520We%2520evaluated%2520ScoreFormer%2520across%250Amultiple%2520datasets%2520under%2520various%2520conditions%252C%2520confirming%2520its%2520robustness%2520and%250Areliability%2520in%2520identifying%2520potential%2520drug%2520candidates%2520rapidly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scoreformer%3A%20A%20Surrogate%20Model%20For%20Large-Scale%20Prediction%20of%20Docking%0A%20%20Scores&entry.906535625=%C3%81lvaro%20Ciudad%20and%20Adri%C3%A1n%20Morales-Pastor%20and%20Laura%20Malo%20and%20Isaac%20Filella-Merc%C3%A8%20and%20Victor%20Guallar%20and%20Alexis%20Molina&entry.1292438233=%20%20In%20this%20study%2C%20we%20present%20ScoreFormer%2C%20a%20novel%20graph%20transformer%20model%0Adesigned%20to%20accurately%20predict%20molecular%20docking%20scores%2C%20thereby%20optimizing%0Ahigh-throughput%20virtual%20screening%20%28HTVS%29%20in%20drug%20discovery.%20The%20architecture%0Aintegrates%20Principal%20Neighborhood%20Aggregation%20%28PNA%29%20and%20Learnable%20Random%20Walk%0APositional%20Encodings%20%28LRWPE%29%2C%20enhancing%20the%20model%27s%20ability%20to%20understand%0Acomplex%20molecular%20structures%20and%20their%20relationship%20with%20their%20respective%0Adocking%20scores.%20This%20approach%20significantly%20surpasses%20traditional%20HTVS%20methods%0Aand%20recent%20Graph%20Neural%20Network%20%28GNN%29%20models%20in%20both%20recovery%20and%20efficiency%0Adue%20to%20a%20wider%20coverage%20of%20the%20chemical%20space%20and%20enhanced%20performance.%20Our%0Aresults%20demonstrate%20that%20ScoreFormer%20achieves%20competitive%20performance%20in%0Adocking%20score%20prediction%20and%20offers%20a%20substantial%201.65-fold%20reduction%20in%0Ainference%20time%20compared%20to%20existing%20models.%20We%20evaluated%20ScoreFormer%20across%0Amultiple%20datasets%20under%20various%20conditions%2C%20confirming%20its%20robustness%20and%0Areliability%20in%20identifying%20potential%20drug%20candidates%20rapidly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09346v1&entry.124074799=Read"},
{"title": "VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark", "author": "Han Huang and Haitian Zhong and Tao Yu and Qiang Liu and Shu Wu and Liang Wang and Tieniu Tan", "abstract": "  Recently, knowledge editing on large language models (LLMs) has received\nconsiderable attention. Compared to this, editing Large Vision-Language Models\n(LVLMs) faces extra challenges from diverse data modalities and complicated\nmodel components, and data for LVLMs editing are limited. The existing LVLM\nediting benchmark, which comprises three metrics (Reliability, Locality, and\nGenerality), falls short in the quality of synthesized evaluation images and\ncannot assess whether models apply edited knowledge in relevant content.\nTherefore, we employ more reliable data collection methods to construct a new\nLarge $\\textbf{V}$ision-$\\textbf{L}$anguage Model $\\textbf{K}$nowledge\n$\\textbf{E}$diting $\\textbf{B}$enchmark, $\\textbf{VLKEB}$, and extend the\nPortability metric for more comprehensive evaluation. Leveraging a multi-modal\nknowledge graph, our image data are bound with knowledge entities. This can be\nfurther used to extract entity-related knowledge, which constitutes the base of\nediting data. We conduct experiments of different editing methods on five\nLVLMs, and thoroughly analyze how do they impact the models. The results reveal\nstrengths and deficiencies of these methods and hopefully provide insights for\nfuture research. The codes and dataset are available at:\n$\\href{https://github.com/VLKEB/VLKEB}{\\text{https://github.com/VLKEB/VLKEB}}$.\n", "link": "http://arxiv.org/abs/2403.07350v2", "date": "2024-06-13", "relevancy": 1.4767, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5261}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5169}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLKEB%3A%20A%20Large%20Vision-Language%20Model%20Knowledge%20Editing%20Benchmark&body=Title%3A%20VLKEB%3A%20A%20Large%20Vision-Language%20Model%20Knowledge%20Editing%20Benchmark%0AAuthor%3A%20Han%20Huang%20and%20Haitian%20Zhong%20and%20Tao%20Yu%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20Recently%2C%20knowledge%20editing%20on%20large%20language%20models%20%28LLMs%29%20has%20received%0Aconsiderable%20attention.%20Compared%20to%20this%2C%20editing%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20faces%20extra%20challenges%20from%20diverse%20data%20modalities%20and%20complicated%0Amodel%20components%2C%20and%20data%20for%20LVLMs%20editing%20are%20limited.%20The%20existing%20LVLM%0Aediting%20benchmark%2C%20which%20comprises%20three%20metrics%20%28Reliability%2C%20Locality%2C%20and%0AGenerality%29%2C%20falls%20short%20in%20the%20quality%20of%20synthesized%20evaluation%20images%20and%0Acannot%20assess%20whether%20models%20apply%20edited%20knowledge%20in%20relevant%20content.%0ATherefore%2C%20we%20employ%20more%20reliable%20data%20collection%20methods%20to%20construct%20a%20new%0ALarge%20%24%5Ctextbf%7BV%7D%24ision-%24%5Ctextbf%7BL%7D%24anguage%20Model%20%24%5Ctextbf%7BK%7D%24nowledge%0A%24%5Ctextbf%7BE%7D%24diting%20%24%5Ctextbf%7BB%7D%24enchmark%2C%20%24%5Ctextbf%7BVLKEB%7D%24%2C%20and%20extend%20the%0APortability%20metric%20for%20more%20comprehensive%20evaluation.%20Leveraging%20a%20multi-modal%0Aknowledge%20graph%2C%20our%20image%20data%20are%20bound%20with%20knowledge%20entities.%20This%20can%20be%0Afurther%20used%20to%20extract%20entity-related%20knowledge%2C%20which%20constitutes%20the%20base%20of%0Aediting%20data.%20We%20conduct%20experiments%20of%20different%20editing%20methods%20on%20five%0ALVLMs%2C%20and%20thoroughly%20analyze%20how%20do%20they%20impact%20the%20models.%20The%20results%20reveal%0Astrengths%20and%20deficiencies%20of%20these%20methods%20and%20hopefully%20provide%20insights%20for%0Afuture%20research.%20The%20codes%20and%20dataset%20are%20available%20at%3A%0A%24%5Chref%7Bhttps%3A//github.com/VLKEB/VLKEB%7D%7B%5Ctext%7Bhttps%3A//github.com/VLKEB/VLKEB%7D%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07350v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLKEB%253A%2520A%2520Large%2520Vision-Language%2520Model%2520Knowledge%2520Editing%2520Benchmark%26entry.906535625%3DHan%2520Huang%2520and%2520Haitian%2520Zhong%2520and%2520Tao%2520Yu%2520and%2520Qiang%2520Liu%2520and%2520Shu%2520Wu%2520and%2520Liang%2520Wang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520Recently%252C%2520knowledge%2520editing%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520received%250Aconsiderable%2520attention.%2520Compared%2520to%2520this%252C%2520editing%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520faces%2520extra%2520challenges%2520from%2520diverse%2520data%2520modalities%2520and%2520complicated%250Amodel%2520components%252C%2520and%2520data%2520for%2520LVLMs%2520editing%2520are%2520limited.%2520The%2520existing%2520LVLM%250Aediting%2520benchmark%252C%2520which%2520comprises%2520three%2520metrics%2520%2528Reliability%252C%2520Locality%252C%2520and%250AGenerality%2529%252C%2520falls%2520short%2520in%2520the%2520quality%2520of%2520synthesized%2520evaluation%2520images%2520and%250Acannot%2520assess%2520whether%2520models%2520apply%2520edited%2520knowledge%2520in%2520relevant%2520content.%250ATherefore%252C%2520we%2520employ%2520more%2520reliable%2520data%2520collection%2520methods%2520to%2520construct%2520a%2520new%250ALarge%2520%2524%255Ctextbf%257BV%257D%2524ision-%2524%255Ctextbf%257BL%257D%2524anguage%2520Model%2520%2524%255Ctextbf%257BK%257D%2524nowledge%250A%2524%255Ctextbf%257BE%257D%2524diting%2520%2524%255Ctextbf%257BB%257D%2524enchmark%252C%2520%2524%255Ctextbf%257BVLKEB%257D%2524%252C%2520and%2520extend%2520the%250APortability%2520metric%2520for%2520more%2520comprehensive%2520evaluation.%2520Leveraging%2520a%2520multi-modal%250Aknowledge%2520graph%252C%2520our%2520image%2520data%2520are%2520bound%2520with%2520knowledge%2520entities.%2520This%2520can%2520be%250Afurther%2520used%2520to%2520extract%2520entity-related%2520knowledge%252C%2520which%2520constitutes%2520the%2520base%2520of%250Aediting%2520data.%2520We%2520conduct%2520experiments%2520of%2520different%2520editing%2520methods%2520on%2520five%250ALVLMs%252C%2520and%2520thoroughly%2520analyze%2520how%2520do%2520they%2520impact%2520the%2520models.%2520The%2520results%2520reveal%250Astrengths%2520and%2520deficiencies%2520of%2520these%2520methods%2520and%2520hopefully%2520provide%2520insights%2520for%250Afuture%2520research.%2520The%2520codes%2520and%2520dataset%2520are%2520available%2520at%253A%250A%2524%255Chref%257Bhttps%253A//github.com/VLKEB/VLKEB%257D%257B%255Ctext%257Bhttps%253A//github.com/VLKEB/VLKEB%257D%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07350v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLKEB%3A%20A%20Large%20Vision-Language%20Model%20Knowledge%20Editing%20Benchmark&entry.906535625=Han%20Huang%20and%20Haitian%20Zhong%20and%20Tao%20Yu%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Liang%20Wang%20and%20Tieniu%20Tan&entry.1292438233=%20%20Recently%2C%20knowledge%20editing%20on%20large%20language%20models%20%28LLMs%29%20has%20received%0Aconsiderable%20attention.%20Compared%20to%20this%2C%20editing%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20faces%20extra%20challenges%20from%20diverse%20data%20modalities%20and%20complicated%0Amodel%20components%2C%20and%20data%20for%20LVLMs%20editing%20are%20limited.%20The%20existing%20LVLM%0Aediting%20benchmark%2C%20which%20comprises%20three%20metrics%20%28Reliability%2C%20Locality%2C%20and%0AGenerality%29%2C%20falls%20short%20in%20the%20quality%20of%20synthesized%20evaluation%20images%20and%0Acannot%20assess%20whether%20models%20apply%20edited%20knowledge%20in%20relevant%20content.%0ATherefore%2C%20we%20employ%20more%20reliable%20data%20collection%20methods%20to%20construct%20a%20new%0ALarge%20%24%5Ctextbf%7BV%7D%24ision-%24%5Ctextbf%7BL%7D%24anguage%20Model%20%24%5Ctextbf%7BK%7D%24nowledge%0A%24%5Ctextbf%7BE%7D%24diting%20%24%5Ctextbf%7BB%7D%24enchmark%2C%20%24%5Ctextbf%7BVLKEB%7D%24%2C%20and%20extend%20the%0APortability%20metric%20for%20more%20comprehensive%20evaluation.%20Leveraging%20a%20multi-modal%0Aknowledge%20graph%2C%20our%20image%20data%20are%20bound%20with%20knowledge%20entities.%20This%20can%20be%0Afurther%20used%20to%20extract%20entity-related%20knowledge%2C%20which%20constitutes%20the%20base%20of%0Aediting%20data.%20We%20conduct%20experiments%20of%20different%20editing%20methods%20on%20five%0ALVLMs%2C%20and%20thoroughly%20analyze%20how%20do%20they%20impact%20the%20models.%20The%20results%20reveal%0Astrengths%20and%20deficiencies%20of%20these%20methods%20and%20hopefully%20provide%20insights%20for%0Afuture%20research.%20The%20codes%20and%20dataset%20are%20available%20at%3A%0A%24%5Chref%7Bhttps%3A//github.com/VLKEB/VLKEB%7D%7B%5Ctext%7Bhttps%3A//github.com/VLKEB/VLKEB%7D%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07350v2&entry.124074799=Read"},
{"title": "Understanding Jailbreak Success: A Study of Latent Space Dynamics in\n  Large Language Models", "author": "Sarah Ball and Frauke Kreuter and Nina Rimsky", "abstract": "  Conversational Large Language Models are trained to refuse to answer harmful\nquestions. However, emergent jailbreaking techniques can still elicit unsafe\noutputs, presenting an ongoing challenge for model alignment. To better\nunderstand how different jailbreak types circumvent safeguards, this paper\nanalyses model activations on different jailbreak inputs. We find that it is\npossible to extract a jailbreak vector from a single class of jailbreaks that\nworks to mitigate jailbreak effectiveness from other classes. This may indicate\nthat different kinds of effective jailbreaks operate via similar internal\nmechanisms. We investigate a potential common mechanism of harmfulness feature\nsuppression, and provide evidence for its existence by looking at the\nharmfulness vector component. These findings offer actionable insights for\ndeveloping more robust jailbreak countermeasures and lay the groundwork for a\ndeeper, mechanistic understanding of jailbreak dynamics in language models.\n", "link": "http://arxiv.org/abs/2406.09289v1", "date": "2024-06-13", "relevancy": 1.243, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.418}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.415}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Jailbreak%20Success%3A%20A%20Study%20of%20Latent%20Space%20Dynamics%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Understanding%20Jailbreak%20Success%3A%20A%20Study%20of%20Latent%20Space%20Dynamics%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Sarah%20Ball%20and%20Frauke%20Kreuter%20and%20Nina%20Rimsky%0AAbstract%3A%20%20%20Conversational%20Large%20Language%20Models%20are%20trained%20to%20refuse%20to%20answer%20harmful%0Aquestions.%20However%2C%20emergent%20jailbreaking%20techniques%20can%20still%20elicit%20unsafe%0Aoutputs%2C%20presenting%20an%20ongoing%20challenge%20for%20model%20alignment.%20To%20better%0Aunderstand%20how%20different%20jailbreak%20types%20circumvent%20safeguards%2C%20this%20paper%0Aanalyses%20model%20activations%20on%20different%20jailbreak%20inputs.%20We%20find%20that%20it%20is%0Apossible%20to%20extract%20a%20jailbreak%20vector%20from%20a%20single%20class%20of%20jailbreaks%20that%0Aworks%20to%20mitigate%20jailbreak%20effectiveness%20from%20other%20classes.%20This%20may%20indicate%0Athat%20different%20kinds%20of%20effective%20jailbreaks%20operate%20via%20similar%20internal%0Amechanisms.%20We%20investigate%20a%20potential%20common%20mechanism%20of%20harmfulness%20feature%0Asuppression%2C%20and%20provide%20evidence%20for%20its%20existence%20by%20looking%20at%20the%0Aharmfulness%20vector%20component.%20These%20findings%20offer%20actionable%20insights%20for%0Adeveloping%20more%20robust%20jailbreak%20countermeasures%20and%20lay%20the%20groundwork%20for%20a%0Adeeper%2C%20mechanistic%20understanding%20of%20jailbreak%20dynamics%20in%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Jailbreak%2520Success%253A%2520A%2520Study%2520of%2520Latent%2520Space%2520Dynamics%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DSarah%2520Ball%2520and%2520Frauke%2520Kreuter%2520and%2520Nina%2520Rimsky%26entry.1292438233%3D%2520%2520Conversational%2520Large%2520Language%2520Models%2520are%2520trained%2520to%2520refuse%2520to%2520answer%2520harmful%250Aquestions.%2520However%252C%2520emergent%2520jailbreaking%2520techniques%2520can%2520still%2520elicit%2520unsafe%250Aoutputs%252C%2520presenting%2520an%2520ongoing%2520challenge%2520for%2520model%2520alignment.%2520To%2520better%250Aunderstand%2520how%2520different%2520jailbreak%2520types%2520circumvent%2520safeguards%252C%2520this%2520paper%250Aanalyses%2520model%2520activations%2520on%2520different%2520jailbreak%2520inputs.%2520We%2520find%2520that%2520it%2520is%250Apossible%2520to%2520extract%2520a%2520jailbreak%2520vector%2520from%2520a%2520single%2520class%2520of%2520jailbreaks%2520that%250Aworks%2520to%2520mitigate%2520jailbreak%2520effectiveness%2520from%2520other%2520classes.%2520This%2520may%2520indicate%250Athat%2520different%2520kinds%2520of%2520effective%2520jailbreaks%2520operate%2520via%2520similar%2520internal%250Amechanisms.%2520We%2520investigate%2520a%2520potential%2520common%2520mechanism%2520of%2520harmfulness%2520feature%250Asuppression%252C%2520and%2520provide%2520evidence%2520for%2520its%2520existence%2520by%2520looking%2520at%2520the%250Aharmfulness%2520vector%2520component.%2520These%2520findings%2520offer%2520actionable%2520insights%2520for%250Adeveloping%2520more%2520robust%2520jailbreak%2520countermeasures%2520and%2520lay%2520the%2520groundwork%2520for%2520a%250Adeeper%252C%2520mechanistic%2520understanding%2520of%2520jailbreak%2520dynamics%2520in%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Jailbreak%20Success%3A%20A%20Study%20of%20Latent%20Space%20Dynamics%20in%0A%20%20Large%20Language%20Models&entry.906535625=Sarah%20Ball%20and%20Frauke%20Kreuter%20and%20Nina%20Rimsky&entry.1292438233=%20%20Conversational%20Large%20Language%20Models%20are%20trained%20to%20refuse%20to%20answer%20harmful%0Aquestions.%20However%2C%20emergent%20jailbreaking%20techniques%20can%20still%20elicit%20unsafe%0Aoutputs%2C%20presenting%20an%20ongoing%20challenge%20for%20model%20alignment.%20To%20better%0Aunderstand%20how%20different%20jailbreak%20types%20circumvent%20safeguards%2C%20this%20paper%0Aanalyses%20model%20activations%20on%20different%20jailbreak%20inputs.%20We%20find%20that%20it%20is%0Apossible%20to%20extract%20a%20jailbreak%20vector%20from%20a%20single%20class%20of%20jailbreaks%20that%0Aworks%20to%20mitigate%20jailbreak%20effectiveness%20from%20other%20classes.%20This%20may%20indicate%0Athat%20different%20kinds%20of%20effective%20jailbreaks%20operate%20via%20similar%20internal%0Amechanisms.%20We%20investigate%20a%20potential%20common%20mechanism%20of%20harmfulness%20feature%0Asuppression%2C%20and%20provide%20evidence%20for%20its%20existence%20by%20looking%20at%20the%0Aharmfulness%20vector%20component.%20These%20findings%20offer%20actionable%20insights%20for%0Adeveloping%20more%20robust%20jailbreak%20countermeasures%20and%20lay%20the%20groundwork%20for%20a%0Adeeper%2C%20mechanistic%20understanding%20of%20jailbreak%20dynamics%20in%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09289v1&entry.124074799=Read"},
{"title": "Neural networks in non-metric spaces", "author": "Luca Galimberti", "abstract": "  Leveraging the infinite dimensional neural network architecture we proposed\nin arXiv:2109.13512v4 and which can process inputs from Fr\\'echet spaces, and\nusing the universal approximation property shown therein, we now largely extend\nthe scope of this architecture by proving several universal approximation\ntheorems for a vast class of input and output spaces. More precisely, the input\nspace $\\mathfrak X$ is allowed to be a general topological space satisfying\nonly a mild condition (\"quasi-Polish\"), and the output space can be either\nanother quasi-Polish space $\\mathfrak Y$ or a topological vector space $E$.\nSimilarly to arXiv:2109.13512v4, we show furthermore that our neural network\narchitectures can be projected down to \"finite dimensional\" subspaces with any\ndesirable accuracy, thus obtaining approximating networks that are easy to\nimplement and allow for fast computation and fitting. The resulting neural\nnetwork architecture is therefore applicable for prediction tasks based on\nfunctional data. To the best of our knowledge, this is the first result which\ndeals with such a wide class of input/output spaces and simultaneously\nguarantees the numerical feasibility of the ensuing architectures. Finally, we\nprove an obstruction result which indicates that the category of quasi-Polish\nspaces is in a certain sense the correct category to work with if one aims at\nconstructing approximating architectures on infinite-dimensional spaces\n$\\mathfrak X$ which, at the same time, have sufficient expressive power to\napproximate continuous functions on $\\mathfrak X$, are specified by a finite\nnumber of parameters only and are \"stable\" with respect to these parameters.\n", "link": "http://arxiv.org/abs/2406.09310v1", "date": "2024-06-13", "relevancy": 1.8733, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4705}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20networks%20in%20non-metric%20spaces&body=Title%3A%20Neural%20networks%20in%20non-metric%20spaces%0AAuthor%3A%20Luca%20Galimberti%0AAbstract%3A%20%20%20Leveraging%20the%20infinite%20dimensional%20neural%20network%20architecture%20we%20proposed%0Ain%20arXiv%3A2109.13512v4%20and%20which%20can%20process%20inputs%20from%20Fr%5C%27echet%20spaces%2C%20and%0Ausing%20the%20universal%20approximation%20property%20shown%20therein%2C%20we%20now%20largely%20extend%0Athe%20scope%20of%20this%20architecture%20by%20proving%20several%20universal%20approximation%0Atheorems%20for%20a%20vast%20class%20of%20input%20and%20output%20spaces.%20More%20precisely%2C%20the%20input%0Aspace%20%24%5Cmathfrak%20X%24%20is%20allowed%20to%20be%20a%20general%20topological%20space%20satisfying%0Aonly%20a%20mild%20condition%20%28%22quasi-Polish%22%29%2C%20and%20the%20output%20space%20can%20be%20either%0Aanother%20quasi-Polish%20space%20%24%5Cmathfrak%20Y%24%20or%20a%20topological%20vector%20space%20%24E%24.%0ASimilarly%20to%20arXiv%3A2109.13512v4%2C%20we%20show%20furthermore%20that%20our%20neural%20network%0Aarchitectures%20can%20be%20projected%20down%20to%20%22finite%20dimensional%22%20subspaces%20with%20any%0Adesirable%20accuracy%2C%20thus%20obtaining%20approximating%20networks%20that%20are%20easy%20to%0Aimplement%20and%20allow%20for%20fast%20computation%20and%20fitting.%20The%20resulting%20neural%0Anetwork%20architecture%20is%20therefore%20applicable%20for%20prediction%20tasks%20based%20on%0Afunctional%20data.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20result%20which%0Adeals%20with%20such%20a%20wide%20class%20of%20input/output%20spaces%20and%20simultaneously%0Aguarantees%20the%20numerical%20feasibility%20of%20the%20ensuing%20architectures.%20Finally%2C%20we%0Aprove%20an%20obstruction%20result%20which%20indicates%20that%20the%20category%20of%20quasi-Polish%0Aspaces%20is%20in%20a%20certain%20sense%20the%20correct%20category%20to%20work%20with%20if%20one%20aims%20at%0Aconstructing%20approximating%20architectures%20on%20infinite-dimensional%20spaces%0A%24%5Cmathfrak%20X%24%20which%2C%20at%20the%20same%20time%2C%20have%20sufficient%20expressive%20power%20to%0Aapproximate%20continuous%20functions%20on%20%24%5Cmathfrak%20X%24%2C%20are%20specified%20by%20a%20finite%0Anumber%20of%20parameters%20only%20and%20are%20%22stable%22%20with%20respect%20to%20these%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520networks%2520in%2520non-metric%2520spaces%26entry.906535625%3DLuca%2520Galimberti%26entry.1292438233%3D%2520%2520Leveraging%2520the%2520infinite%2520dimensional%2520neural%2520network%2520architecture%2520we%2520proposed%250Ain%2520arXiv%253A2109.13512v4%2520and%2520which%2520can%2520process%2520inputs%2520from%2520Fr%255C%2527echet%2520spaces%252C%2520and%250Ausing%2520the%2520universal%2520approximation%2520property%2520shown%2520therein%252C%2520we%2520now%2520largely%2520extend%250Athe%2520scope%2520of%2520this%2520architecture%2520by%2520proving%2520several%2520universal%2520approximation%250Atheorems%2520for%2520a%2520vast%2520class%2520of%2520input%2520and%2520output%2520spaces.%2520More%2520precisely%252C%2520the%2520input%250Aspace%2520%2524%255Cmathfrak%2520X%2524%2520is%2520allowed%2520to%2520be%2520a%2520general%2520topological%2520space%2520satisfying%250Aonly%2520a%2520mild%2520condition%2520%2528%2522quasi-Polish%2522%2529%252C%2520and%2520the%2520output%2520space%2520can%2520be%2520either%250Aanother%2520quasi-Polish%2520space%2520%2524%255Cmathfrak%2520Y%2524%2520or%2520a%2520topological%2520vector%2520space%2520%2524E%2524.%250ASimilarly%2520to%2520arXiv%253A2109.13512v4%252C%2520we%2520show%2520furthermore%2520that%2520our%2520neural%2520network%250Aarchitectures%2520can%2520be%2520projected%2520down%2520to%2520%2522finite%2520dimensional%2522%2520subspaces%2520with%2520any%250Adesirable%2520accuracy%252C%2520thus%2520obtaining%2520approximating%2520networks%2520that%2520are%2520easy%2520to%250Aimplement%2520and%2520allow%2520for%2520fast%2520computation%2520and%2520fitting.%2520The%2520resulting%2520neural%250Anetwork%2520architecture%2520is%2520therefore%2520applicable%2520for%2520prediction%2520tasks%2520based%2520on%250Afunctional%2520data.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520result%2520which%250Adeals%2520with%2520such%2520a%2520wide%2520class%2520of%2520input/output%2520spaces%2520and%2520simultaneously%250Aguarantees%2520the%2520numerical%2520feasibility%2520of%2520the%2520ensuing%2520architectures.%2520Finally%252C%2520we%250Aprove%2520an%2520obstruction%2520result%2520which%2520indicates%2520that%2520the%2520category%2520of%2520quasi-Polish%250Aspaces%2520is%2520in%2520a%2520certain%2520sense%2520the%2520correct%2520category%2520to%2520work%2520with%2520if%2520one%2520aims%2520at%250Aconstructing%2520approximating%2520architectures%2520on%2520infinite-dimensional%2520spaces%250A%2524%255Cmathfrak%2520X%2524%2520which%252C%2520at%2520the%2520same%2520time%252C%2520have%2520sufficient%2520expressive%2520power%2520to%250Aapproximate%2520continuous%2520functions%2520on%2520%2524%255Cmathfrak%2520X%2524%252C%2520are%2520specified%2520by%2520a%2520finite%250Anumber%2520of%2520parameters%2520only%2520and%2520are%2520%2522stable%2522%2520with%2520respect%2520to%2520these%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20networks%20in%20non-metric%20spaces&entry.906535625=Luca%20Galimberti&entry.1292438233=%20%20Leveraging%20the%20infinite%20dimensional%20neural%20network%20architecture%20we%20proposed%0Ain%20arXiv%3A2109.13512v4%20and%20which%20can%20process%20inputs%20from%20Fr%5C%27echet%20spaces%2C%20and%0Ausing%20the%20universal%20approximation%20property%20shown%20therein%2C%20we%20now%20largely%20extend%0Athe%20scope%20of%20this%20architecture%20by%20proving%20several%20universal%20approximation%0Atheorems%20for%20a%20vast%20class%20of%20input%20and%20output%20spaces.%20More%20precisely%2C%20the%20input%0Aspace%20%24%5Cmathfrak%20X%24%20is%20allowed%20to%20be%20a%20general%20topological%20space%20satisfying%0Aonly%20a%20mild%20condition%20%28%22quasi-Polish%22%29%2C%20and%20the%20output%20space%20can%20be%20either%0Aanother%20quasi-Polish%20space%20%24%5Cmathfrak%20Y%24%20or%20a%20topological%20vector%20space%20%24E%24.%0ASimilarly%20to%20arXiv%3A2109.13512v4%2C%20we%20show%20furthermore%20that%20our%20neural%20network%0Aarchitectures%20can%20be%20projected%20down%20to%20%22finite%20dimensional%22%20subspaces%20with%20any%0Adesirable%20accuracy%2C%20thus%20obtaining%20approximating%20networks%20that%20are%20easy%20to%0Aimplement%20and%20allow%20for%20fast%20computation%20and%20fitting.%20The%20resulting%20neural%0Anetwork%20architecture%20is%20therefore%20applicable%20for%20prediction%20tasks%20based%20on%0Afunctional%20data.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20result%20which%0Adeals%20with%20such%20a%20wide%20class%20of%20input/output%20spaces%20and%20simultaneously%0Aguarantees%20the%20numerical%20feasibility%20of%20the%20ensuing%20architectures.%20Finally%2C%20we%0Aprove%20an%20obstruction%20result%20which%20indicates%20that%20the%20category%20of%20quasi-Polish%0Aspaces%20is%20in%20a%20certain%20sense%20the%20correct%20category%20to%20work%20with%20if%20one%20aims%20at%0Aconstructing%20approximating%20architectures%20on%20infinite-dimensional%20spaces%0A%24%5Cmathfrak%20X%24%20which%2C%20at%20the%20same%20time%2C%20have%20sufficient%20expressive%20power%20to%0Aapproximate%20continuous%20functions%20on%20%24%5Cmathfrak%20X%24%2C%20are%20specified%20by%20a%20finite%0Anumber%20of%20parameters%20only%20and%20are%20%22stable%22%20with%20respect%20to%20these%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09310v1&entry.124074799=Read"},
{"title": "Is Programming by Example solved by LLMs?", "author": "Wen-Ding Li and Kevin Ellis", "abstract": "  Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n`solved' PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.\n", "link": "http://arxiv.org/abs/2406.08316v2", "date": "2024-06-13", "relevancy": 1.289, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4386}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4268}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Programming%20by%20Example%20solved%20by%20LLMs%3F&body=Title%3A%20Is%20Programming%20by%20Example%20solved%20by%20LLMs%3F%0AAuthor%3A%20Wen-Ding%20Li%20and%20Kevin%20Ellis%0AAbstract%3A%20%20%20Programming-by-Examples%20%28PBE%29%20aims%20to%20generate%20an%20algorithm%20from%20input-output%0Aexamples.%20Such%20systems%20are%20practically%20and%20theoretically%20important%3A%20from%20an%0Aend-user%20perspective%2C%20they%20are%20deployed%20to%20millions%20of%20people%2C%20and%20from%20an%20AI%0Aperspective%2C%20PBE%20corresponds%20to%20a%20very%20general%20form%20of%20few-shot%20inductive%0Ainference.%20Given%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20code-generation%0Atasks%2C%20we%20investigate%20here%20the%20extent%20to%20which%20LLMs%20can%20be%20said%20to%20have%0A%60solved%27%20PBE.%20We%20experiment%20on%20classic%20domains%20such%20as%20lists%20and%20strings%2C%20and%0Aan%20uncommon%20graphics%20programming%20domain%20not%20well%20represented%20in%20typical%0Apretraining%20data.%20We%20find%20that%20pretrained%20models%20are%20not%20effective%20at%20PBE%2C%20but%0Athat%20they%20can%20be%20fine-tuned%20for%20much%20higher%20performance%2C%20provided%20the%20test%0Aproblems%20are%20in-distribution.%20We%20analyze%20empirically%20what%20causes%20these%20models%0Ato%20succeed%20and%20fail%2C%20and%20take%20steps%20toward%20understanding%20how%20to%20achieve%20better%0Aout-of-distribution%20generalization.%20Collectively%20these%20results%20suggest%20that%0ALLMs%20make%20strong%20progress%20toward%20solving%20the%20typical%20suite%20of%20PBE%20tasks%2C%0Apotentially%20increasing%20the%20flexibility%20and%20applicability%20of%20PBE%20systems%2C%20while%0Aalso%20identifying%20ways%20in%20which%20LLMs%20still%20fall%20short.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Programming%2520by%2520Example%2520solved%2520by%2520LLMs%253F%26entry.906535625%3DWen-Ding%2520Li%2520and%2520Kevin%2520Ellis%26entry.1292438233%3D%2520%2520Programming-by-Examples%2520%2528PBE%2529%2520aims%2520to%2520generate%2520an%2520algorithm%2520from%2520input-output%250Aexamples.%2520Such%2520systems%2520are%2520practically%2520and%2520theoretically%2520important%253A%2520from%2520an%250Aend-user%2520perspective%252C%2520they%2520are%2520deployed%2520to%2520millions%2520of%2520people%252C%2520and%2520from%2520an%2520AI%250Aperspective%252C%2520PBE%2520corresponds%2520to%2520a%2520very%2520general%2520form%2520of%2520few-shot%2520inductive%250Ainference.%2520Given%2520the%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520code-generation%250Atasks%252C%2520we%2520investigate%2520here%2520the%2520extent%2520to%2520which%2520LLMs%2520can%2520be%2520said%2520to%2520have%250A%2560solved%2527%2520PBE.%2520We%2520experiment%2520on%2520classic%2520domains%2520such%2520as%2520lists%2520and%2520strings%252C%2520and%250Aan%2520uncommon%2520graphics%2520programming%2520domain%2520not%2520well%2520represented%2520in%2520typical%250Apretraining%2520data.%2520We%2520find%2520that%2520pretrained%2520models%2520are%2520not%2520effective%2520at%2520PBE%252C%2520but%250Athat%2520they%2520can%2520be%2520fine-tuned%2520for%2520much%2520higher%2520performance%252C%2520provided%2520the%2520test%250Aproblems%2520are%2520in-distribution.%2520We%2520analyze%2520empirically%2520what%2520causes%2520these%2520models%250Ato%2520succeed%2520and%2520fail%252C%2520and%2520take%2520steps%2520toward%2520understanding%2520how%2520to%2520achieve%2520better%250Aout-of-distribution%2520generalization.%2520Collectively%2520these%2520results%2520suggest%2520that%250ALLMs%2520make%2520strong%2520progress%2520toward%2520solving%2520the%2520typical%2520suite%2520of%2520PBE%2520tasks%252C%250Apotentially%2520increasing%2520the%2520flexibility%2520and%2520applicability%2520of%2520PBE%2520systems%252C%2520while%250Aalso%2520identifying%2520ways%2520in%2520which%2520LLMs%2520still%2520fall%2520short.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Programming%20by%20Example%20solved%20by%20LLMs%3F&entry.906535625=Wen-Ding%20Li%20and%20Kevin%20Ellis&entry.1292438233=%20%20Programming-by-Examples%20%28PBE%29%20aims%20to%20generate%20an%20algorithm%20from%20input-output%0Aexamples.%20Such%20systems%20are%20practically%20and%20theoretically%20important%3A%20from%20an%0Aend-user%20perspective%2C%20they%20are%20deployed%20to%20millions%20of%20people%2C%20and%20from%20an%20AI%0Aperspective%2C%20PBE%20corresponds%20to%20a%20very%20general%20form%20of%20few-shot%20inductive%0Ainference.%20Given%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20code-generation%0Atasks%2C%20we%20investigate%20here%20the%20extent%20to%20which%20LLMs%20can%20be%20said%20to%20have%0A%60solved%27%20PBE.%20We%20experiment%20on%20classic%20domains%20such%20as%20lists%20and%20strings%2C%20and%0Aan%20uncommon%20graphics%20programming%20domain%20not%20well%20represented%20in%20typical%0Apretraining%20data.%20We%20find%20that%20pretrained%20models%20are%20not%20effective%20at%20PBE%2C%20but%0Athat%20they%20can%20be%20fine-tuned%20for%20much%20higher%20performance%2C%20provided%20the%20test%0Aproblems%20are%20in-distribution.%20We%20analyze%20empirically%20what%20causes%20these%20models%0Ato%20succeed%20and%20fail%2C%20and%20take%20steps%20toward%20understanding%20how%20to%20achieve%20better%0Aout-of-distribution%20generalization.%20Collectively%20these%20results%20suggest%20that%0ALLMs%20make%20strong%20progress%20toward%20solving%20the%20typical%20suite%20of%20PBE%20tasks%2C%0Apotentially%20increasing%20the%20flexibility%20and%20applicability%20of%20PBE%20systems%2C%20while%0Aalso%20identifying%20ways%20in%20which%20LLMs%20still%20fall%20short.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08316v2&entry.124074799=Read"},
{"title": "DIAS: A Dataset and Benchmark for Intracranial Artery Segmentation in\n  DSA sequences", "author": "Wentao Liu and Tong Tian and Lemeng Wang and Weijin Xu and Lei Li and Haoyuan Li and Wenyi Zhao and Siyu Tian and Xipeng Pan and Huihua Yang and Feng Gao and Yiming Deng and Xin Yang and Ruisheng Su", "abstract": "  The automated segmentation of Intracranial Arteries (IA) in Digital\nSubtraction Angiography (DSA) plays a crucial role in the quantification of\nvascular morphology, significantly contributing to computer-assisted stroke\nresearch and clinical practice. Current research primarily focuses on the\nsegmentation of single-frame DSA using proprietary datasets. However, these\nmethods face challenges due to the inherent limitation of single-frame DSA,\nwhich only partially displays vascular contrast, thereby hindering accurate\nvascular structure representation. In this work, we introduce DIAS, a dataset\nspecifically developed for IA segmentation in DSA sequences. We establish a\ncomprehensive benchmark for evaluating DIAS, covering full, weak, and\nsemi-supervised segmentation methods. Specifically, we propose the vessel\nsequence segmentation network, in which the sequence feature extraction module\neffectively captures spatiotemporal representations of intravascular contrast,\nachieving intracranial artery segmentation in 2D+Time DSA sequences. For\nweakly-supervised IA segmentation, we propose a novel scribble learning-based\nimage segmentation framework, which, under the guidance of scribble labels,\nemploys cross pseudo-supervision and consistency regularization to improve the\nperformance of the segmentation network. Furthermore, we introduce the random\npatch-based self-training framework, aimed at alleviating the performance\nconstraints encountered in IA segmentation due to the limited availability of\nannotated DSA data. Our extensive experiments on the DIAS dataset demonstrate\nthe effectiveness of these methods as potential baselines for future research\nand clinical applications. The dataset and code are publicly available at\nhttps://doi.org/10.5281/zenodo.11396520 and https://github.com/lseventeen/DIAS.\n", "link": "http://arxiv.org/abs/2306.12153v4", "date": "2024-06-13", "relevancy": 1.8459, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4731}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4711}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIAS%3A%20A%20Dataset%20and%20Benchmark%20for%20Intracranial%20Artery%20Segmentation%20in%0A%20%20DSA%20sequences&body=Title%3A%20DIAS%3A%20A%20Dataset%20and%20Benchmark%20for%20Intracranial%20Artery%20Segmentation%20in%0A%20%20DSA%20sequences%0AAuthor%3A%20Wentao%20Liu%20and%20Tong%20Tian%20and%20Lemeng%20Wang%20and%20Weijin%20Xu%20and%20Lei%20Li%20and%20Haoyuan%20Li%20and%20Wenyi%20Zhao%20and%20Siyu%20Tian%20and%20Xipeng%20Pan%20and%20Huihua%20Yang%20and%20Feng%20Gao%20and%20Yiming%20Deng%20and%20Xin%20Yang%20and%20Ruisheng%20Su%0AAbstract%3A%20%20%20The%20automated%20segmentation%20of%20Intracranial%20Arteries%20%28IA%29%20in%20Digital%0ASubtraction%20Angiography%20%28DSA%29%20plays%20a%20crucial%20role%20in%20the%20quantification%20of%0Avascular%20morphology%2C%20significantly%20contributing%20to%20computer-assisted%20stroke%0Aresearch%20and%20clinical%20practice.%20Current%20research%20primarily%20focuses%20on%20the%0Asegmentation%20of%20single-frame%20DSA%20using%20proprietary%20datasets.%20However%2C%20these%0Amethods%20face%20challenges%20due%20to%20the%20inherent%20limitation%20of%20single-frame%20DSA%2C%0Awhich%20only%20partially%20displays%20vascular%20contrast%2C%20thereby%20hindering%20accurate%0Avascular%20structure%20representation.%20In%20this%20work%2C%20we%20introduce%20DIAS%2C%20a%20dataset%0Aspecifically%20developed%20for%20IA%20segmentation%20in%20DSA%20sequences.%20We%20establish%20a%0Acomprehensive%20benchmark%20for%20evaluating%20DIAS%2C%20covering%20full%2C%20weak%2C%20and%0Asemi-supervised%20segmentation%20methods.%20Specifically%2C%20we%20propose%20the%20vessel%0Asequence%20segmentation%20network%2C%20in%20which%20the%20sequence%20feature%20extraction%20module%0Aeffectively%20captures%20spatiotemporal%20representations%20of%20intravascular%20contrast%2C%0Aachieving%20intracranial%20artery%20segmentation%20in%202D%2BTime%20DSA%20sequences.%20For%0Aweakly-supervised%20IA%20segmentation%2C%20we%20propose%20a%20novel%20scribble%20learning-based%0Aimage%20segmentation%20framework%2C%20which%2C%20under%20the%20guidance%20of%20scribble%20labels%2C%0Aemploys%20cross%20pseudo-supervision%20and%20consistency%20regularization%20to%20improve%20the%0Aperformance%20of%20the%20segmentation%20network.%20Furthermore%2C%20we%20introduce%20the%20random%0Apatch-based%20self-training%20framework%2C%20aimed%20at%20alleviating%20the%20performance%0Aconstraints%20encountered%20in%20IA%20segmentation%20due%20to%20the%20limited%20availability%20of%0Aannotated%20DSA%20data.%20Our%20extensive%20experiments%20on%20the%20DIAS%20dataset%20demonstrate%0Athe%20effectiveness%20of%20these%20methods%20as%20potential%20baselines%20for%20future%20research%0Aand%20clinical%20applications.%20The%20dataset%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//doi.org/10.5281/zenodo.11396520%20and%20https%3A//github.com/lseventeen/DIAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.12153v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIAS%253A%2520A%2520Dataset%2520and%2520Benchmark%2520for%2520Intracranial%2520Artery%2520Segmentation%2520in%250A%2520%2520DSA%2520sequences%26entry.906535625%3DWentao%2520Liu%2520and%2520Tong%2520Tian%2520and%2520Lemeng%2520Wang%2520and%2520Weijin%2520Xu%2520and%2520Lei%2520Li%2520and%2520Haoyuan%2520Li%2520and%2520Wenyi%2520Zhao%2520and%2520Siyu%2520Tian%2520and%2520Xipeng%2520Pan%2520and%2520Huihua%2520Yang%2520and%2520Feng%2520Gao%2520and%2520Yiming%2520Deng%2520and%2520Xin%2520Yang%2520and%2520Ruisheng%2520Su%26entry.1292438233%3D%2520%2520The%2520automated%2520segmentation%2520of%2520Intracranial%2520Arteries%2520%2528IA%2529%2520in%2520Digital%250ASubtraction%2520Angiography%2520%2528DSA%2529%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520quantification%2520of%250Avascular%2520morphology%252C%2520significantly%2520contributing%2520to%2520computer-assisted%2520stroke%250Aresearch%2520and%2520clinical%2520practice.%2520Current%2520research%2520primarily%2520focuses%2520on%2520the%250Asegmentation%2520of%2520single-frame%2520DSA%2520using%2520proprietary%2520datasets.%2520However%252C%2520these%250Amethods%2520face%2520challenges%2520due%2520to%2520the%2520inherent%2520limitation%2520of%2520single-frame%2520DSA%252C%250Awhich%2520only%2520partially%2520displays%2520vascular%2520contrast%252C%2520thereby%2520hindering%2520accurate%250Avascular%2520structure%2520representation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DIAS%252C%2520a%2520dataset%250Aspecifically%2520developed%2520for%2520IA%2520segmentation%2520in%2520DSA%2520sequences.%2520We%2520establish%2520a%250Acomprehensive%2520benchmark%2520for%2520evaluating%2520DIAS%252C%2520covering%2520full%252C%2520weak%252C%2520and%250Asemi-supervised%2520segmentation%2520methods.%2520Specifically%252C%2520we%2520propose%2520the%2520vessel%250Asequence%2520segmentation%2520network%252C%2520in%2520which%2520the%2520sequence%2520feature%2520extraction%2520module%250Aeffectively%2520captures%2520spatiotemporal%2520representations%2520of%2520intravascular%2520contrast%252C%250Aachieving%2520intracranial%2520artery%2520segmentation%2520in%25202D%252BTime%2520DSA%2520sequences.%2520For%250Aweakly-supervised%2520IA%2520segmentation%252C%2520we%2520propose%2520a%2520novel%2520scribble%2520learning-based%250Aimage%2520segmentation%2520framework%252C%2520which%252C%2520under%2520the%2520guidance%2520of%2520scribble%2520labels%252C%250Aemploys%2520cross%2520pseudo-supervision%2520and%2520consistency%2520regularization%2520to%2520improve%2520the%250Aperformance%2520of%2520the%2520segmentation%2520network.%2520Furthermore%252C%2520we%2520introduce%2520the%2520random%250Apatch-based%2520self-training%2520framework%252C%2520aimed%2520at%2520alleviating%2520the%2520performance%250Aconstraints%2520encountered%2520in%2520IA%2520segmentation%2520due%2520to%2520the%2520limited%2520availability%2520of%250Aannotated%2520DSA%2520data.%2520Our%2520extensive%2520experiments%2520on%2520the%2520DIAS%2520dataset%2520demonstrate%250Athe%2520effectiveness%2520of%2520these%2520methods%2520as%2520potential%2520baselines%2520for%2520future%2520research%250Aand%2520clinical%2520applications.%2520The%2520dataset%2520and%2520code%2520are%2520publicly%2520available%2520at%250Ahttps%253A//doi.org/10.5281/zenodo.11396520%2520and%2520https%253A//github.com/lseventeen/DIAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.12153v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIAS%3A%20A%20Dataset%20and%20Benchmark%20for%20Intracranial%20Artery%20Segmentation%20in%0A%20%20DSA%20sequences&entry.906535625=Wentao%20Liu%20and%20Tong%20Tian%20and%20Lemeng%20Wang%20and%20Weijin%20Xu%20and%20Lei%20Li%20and%20Haoyuan%20Li%20and%20Wenyi%20Zhao%20and%20Siyu%20Tian%20and%20Xipeng%20Pan%20and%20Huihua%20Yang%20and%20Feng%20Gao%20and%20Yiming%20Deng%20and%20Xin%20Yang%20and%20Ruisheng%20Su&entry.1292438233=%20%20The%20automated%20segmentation%20of%20Intracranial%20Arteries%20%28IA%29%20in%20Digital%0ASubtraction%20Angiography%20%28DSA%29%20plays%20a%20crucial%20role%20in%20the%20quantification%20of%0Avascular%20morphology%2C%20significantly%20contributing%20to%20computer-assisted%20stroke%0Aresearch%20and%20clinical%20practice.%20Current%20research%20primarily%20focuses%20on%20the%0Asegmentation%20of%20single-frame%20DSA%20using%20proprietary%20datasets.%20However%2C%20these%0Amethods%20face%20challenges%20due%20to%20the%20inherent%20limitation%20of%20single-frame%20DSA%2C%0Awhich%20only%20partially%20displays%20vascular%20contrast%2C%20thereby%20hindering%20accurate%0Avascular%20structure%20representation.%20In%20this%20work%2C%20we%20introduce%20DIAS%2C%20a%20dataset%0Aspecifically%20developed%20for%20IA%20segmentation%20in%20DSA%20sequences.%20We%20establish%20a%0Acomprehensive%20benchmark%20for%20evaluating%20DIAS%2C%20covering%20full%2C%20weak%2C%20and%0Asemi-supervised%20segmentation%20methods.%20Specifically%2C%20we%20propose%20the%20vessel%0Asequence%20segmentation%20network%2C%20in%20which%20the%20sequence%20feature%20extraction%20module%0Aeffectively%20captures%20spatiotemporal%20representations%20of%20intravascular%20contrast%2C%0Aachieving%20intracranial%20artery%20segmentation%20in%202D%2BTime%20DSA%20sequences.%20For%0Aweakly-supervised%20IA%20segmentation%2C%20we%20propose%20a%20novel%20scribble%20learning-based%0Aimage%20segmentation%20framework%2C%20which%2C%20under%20the%20guidance%20of%20scribble%20labels%2C%0Aemploys%20cross%20pseudo-supervision%20and%20consistency%20regularization%20to%20improve%20the%0Aperformance%20of%20the%20segmentation%20network.%20Furthermore%2C%20we%20introduce%20the%20random%0Apatch-based%20self-training%20framework%2C%20aimed%20at%20alleviating%20the%20performance%0Aconstraints%20encountered%20in%20IA%20segmentation%20due%20to%20the%20limited%20availability%20of%0Aannotated%20DSA%20data.%20Our%20extensive%20experiments%20on%20the%20DIAS%20dataset%20demonstrate%0Athe%20effectiveness%20of%20these%20methods%20as%20potential%20baselines%20for%20future%20research%0Aand%20clinical%20applications.%20The%20dataset%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//doi.org/10.5281/zenodo.11396520%20and%20https%3A//github.com/lseventeen/DIAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.12153v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


