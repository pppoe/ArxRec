<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241003.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Variational Bayes Gaussian Splatting", "author": "Toon Van de Maele and Ozan Catal and Alexander Tschantz and Christopher L. Buckley and Tim Verbelen", "abstract": "  Recently, 3D Gaussian Splatting has emerged as a promising approach for\nmodeling 3D scenes using mixtures of Gaussians. The predominant optimization\nmethod for these models relies on backpropagating gradients through a\ndifferentiable rendering pipeline, which struggles with catastrophic forgetting\nwhen dealing with continuous streams of data. To address this limitation, we\npropose Variational Bayes Gaussian Splatting (VBGS), a novel approach that\nframes training a Gaussian splat as variational inference over model\nparameters. By leveraging the conjugacy properties of multivariate Gaussians,\nwe derive a closed-form variational update rule, allowing efficient updates\nfrom partial, sequential observations without the need for replay buffers. Our\nexperiments show that VBGS not only matches state-of-the-art performance on\nstatic datasets, but also enables continual learning from sequentially streamed\n2D and 3D data, drastically improving performance in this setting.\n", "link": "http://arxiv.org/abs/2410.03592v1", "date": "2024-10-04", "relevancy": 3.21, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6693}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6519}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Bayes%20Gaussian%20Splatting&body=Title%3A%20Variational%20Bayes%20Gaussian%20Splatting%0AAuthor%3A%20Toon%20Van%20de%20Maele%20and%20Ozan%20Catal%20and%20Alexander%20Tschantz%20and%20Christopher%20L.%20Buckley%20and%20Tim%20Verbelen%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20promising%20approach%20for%0Amodeling%203D%20scenes%20using%20mixtures%20of%20Gaussians.%20The%20predominant%20optimization%0Amethod%20for%20these%20models%20relies%20on%20backpropagating%20gradients%20through%20a%0Adifferentiable%20rendering%20pipeline%2C%20which%20struggles%20with%20catastrophic%20forgetting%0Awhen%20dealing%20with%20continuous%20streams%20of%20data.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Variational%20Bayes%20Gaussian%20Splatting%20%28VBGS%29%2C%20a%20novel%20approach%20that%0Aframes%20training%20a%20Gaussian%20splat%20as%20variational%20inference%20over%20model%0Aparameters.%20By%20leveraging%20the%20conjugacy%20properties%20of%20multivariate%20Gaussians%2C%0Awe%20derive%20a%20closed-form%20variational%20update%20rule%2C%20allowing%20efficient%20updates%0Afrom%20partial%2C%20sequential%20observations%20without%20the%20need%20for%20replay%20buffers.%20Our%0Aexperiments%20show%20that%20VBGS%20not%20only%20matches%20state-of-the-art%20performance%20on%0Astatic%20datasets%2C%20but%20also%20enables%20continual%20learning%20from%20sequentially%20streamed%0A2D%20and%203D%20data%2C%20drastically%20improving%20performance%20in%20this%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Bayes%2520Gaussian%2520Splatting%26entry.906535625%3DToon%2520Van%2520de%2520Maele%2520and%2520Ozan%2520Catal%2520and%2520Alexander%2520Tschantz%2520and%2520Christopher%2520L.%2520Buckley%2520and%2520Tim%2520Verbelen%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%250Amodeling%25203D%2520scenes%2520using%2520mixtures%2520of%2520Gaussians.%2520The%2520predominant%2520optimization%250Amethod%2520for%2520these%2520models%2520relies%2520on%2520backpropagating%2520gradients%2520through%2520a%250Adifferentiable%2520rendering%2520pipeline%252C%2520which%2520struggles%2520with%2520catastrophic%2520forgetting%250Awhen%2520dealing%2520with%2520continuous%2520streams%2520of%2520data.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520Variational%2520Bayes%2520Gaussian%2520Splatting%2520%2528VBGS%2529%252C%2520a%2520novel%2520approach%2520that%250Aframes%2520training%2520a%2520Gaussian%2520splat%2520as%2520variational%2520inference%2520over%2520model%250Aparameters.%2520By%2520leveraging%2520the%2520conjugacy%2520properties%2520of%2520multivariate%2520Gaussians%252C%250Awe%2520derive%2520a%2520closed-form%2520variational%2520update%2520rule%252C%2520allowing%2520efficient%2520updates%250Afrom%2520partial%252C%2520sequential%2520observations%2520without%2520the%2520need%2520for%2520replay%2520buffers.%2520Our%250Aexperiments%2520show%2520that%2520VBGS%2520not%2520only%2520matches%2520state-of-the-art%2520performance%2520on%250Astatic%2520datasets%252C%2520but%2520also%2520enables%2520continual%2520learning%2520from%2520sequentially%2520streamed%250A2D%2520and%25203D%2520data%252C%2520drastically%2520improving%2520performance%2520in%2520this%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Bayes%20Gaussian%20Splatting&entry.906535625=Toon%20Van%20de%20Maele%20and%20Ozan%20Catal%20and%20Alexander%20Tschantz%20and%20Christopher%20L.%20Buckley%20and%20Tim%20Verbelen&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20has%20emerged%20as%20a%20promising%20approach%20for%0Amodeling%203D%20scenes%20using%20mixtures%20of%20Gaussians.%20The%20predominant%20optimization%0Amethod%20for%20these%20models%20relies%20on%20backpropagating%20gradients%20through%20a%0Adifferentiable%20rendering%20pipeline%2C%20which%20struggles%20with%20catastrophic%20forgetting%0Awhen%20dealing%20with%20continuous%20streams%20of%20data.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Variational%20Bayes%20Gaussian%20Splatting%20%28VBGS%29%2C%20a%20novel%20approach%20that%0Aframes%20training%20a%20Gaussian%20splat%20as%20variational%20inference%20over%20model%0Aparameters.%20By%20leveraging%20the%20conjugacy%20properties%20of%20multivariate%20Gaussians%2C%0Awe%20derive%20a%20closed-form%20variational%20update%20rule%2C%20allowing%20efficient%20updates%0Afrom%20partial%2C%20sequential%20observations%20without%20the%20need%20for%20replay%20buffers.%20Our%0Aexperiments%20show%20that%20VBGS%20not%20only%20matches%20state-of-the-art%20performance%20on%0Astatic%20datasets%2C%20but%20also%20enables%20continual%20learning%20from%20sequentially%20streamed%0A2D%20and%203D%20data%2C%20drastically%20improving%20performance%20in%20this%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03592v1&entry.124074799=Read"},
{"title": "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP\n  Models", "author": "Jiapeng Wang and Chengyu Wang and Kunzhe Huang and Jun Huang and Lianwen Jin", "abstract": "  Contrastive Language-Image Pre-training (CLIP) has been widely studied and\napplied in numerous applications. However, the emphasis on brief summary texts\nduring pre-training prevents CLIP from understanding long descriptions. This\nissue is particularly acute regarding videos given that videos often contain\nabundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra\nLength) model, which aims to unleash the long-description understanding\ncapability of video CLIP models. Firstly, we establish an automatic data\ncollection system and gather a large-scale VILD pre-training dataset with VIdeo\nand Long-Description pairs. Then, we propose Text-similarity-guided Primary\nComponent Matching (TPCM) to better learn the distribution of feature space\nwhile expanding the long description capability. We also introduce two new\ntasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware\nDescription Ranking (HDR) for further understanding improvement. Finally, we\nconstruct a Long Video Description Ranking (LVDR) benchmark for evaluating the\nlong-description capability more comprehensively. Extensive experimental\nresults on widely-used text-video retrieval benchmarks with both short and long\ndescriptions and our LVDR benchmark can fully demonstrate the effectiveness of\nour method.\n", "link": "http://arxiv.org/abs/2410.00741v2", "date": "2024-10-04", "relevancy": 3.0878, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoCLIP-XL%3A%20Advancing%20Long%20Description%20Understanding%20for%20Video%20CLIP%0A%20%20Models&body=Title%3A%20VideoCLIP-XL%3A%20Advancing%20Long%20Description%20Understanding%20for%20Video%20CLIP%0A%20%20Models%0AAuthor%3A%20Jiapeng%20Wang%20and%20Chengyu%20Wang%20and%20Kunzhe%20Huang%20and%20Jun%20Huang%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20been%20widely%20studied%20and%0Aapplied%20in%20numerous%20applications.%20However%2C%20the%20emphasis%20on%20brief%20summary%20texts%0Aduring%20pre-training%20prevents%20CLIP%20from%20understanding%20long%20descriptions.%20This%0Aissue%20is%20particularly%20acute%20regarding%20videos%20given%20that%20videos%20often%20contain%0Aabundant%20detailed%20contents.%20In%20this%20paper%2C%20we%20propose%20the%20VideoCLIP-XL%20%28eXtra%0ALength%29%20model%2C%20which%20aims%20to%20unleash%20the%20long-description%20understanding%0Acapability%20of%20video%20CLIP%20models.%20Firstly%2C%20we%20establish%20an%20automatic%20data%0Acollection%20system%20and%20gather%20a%20large-scale%20VILD%20pre-training%20dataset%20with%20VIdeo%0Aand%20Long-Description%20pairs.%20Then%2C%20we%20propose%20Text-similarity-guided%20Primary%0AComponent%20Matching%20%28TPCM%29%20to%20better%20learn%20the%20distribution%20of%20feature%20space%0Awhile%20expanding%20the%20long%20description%20capability.%20We%20also%20introduce%20two%20new%0Atasks%20namely%20Detail-aware%20Description%20Ranking%20%28DDR%29%20and%20Hallucination-aware%0ADescription%20Ranking%20%28HDR%29%20for%20further%20understanding%20improvement.%20Finally%2C%20we%0Aconstruct%20a%20Long%20Video%20Description%20Ranking%20%28LVDR%29%20benchmark%20for%20evaluating%20the%0Along-description%20capability%20more%20comprehensively.%20Extensive%20experimental%0Aresults%20on%20widely-used%20text-video%20retrieval%20benchmarks%20with%20both%20short%20and%20long%0Adescriptions%20and%20our%20LVDR%20benchmark%20can%20fully%20demonstrate%20the%20effectiveness%20of%0Aour%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoCLIP-XL%253A%2520Advancing%2520Long%2520Description%2520Understanding%2520for%2520Video%2520CLIP%250A%2520%2520Models%26entry.906535625%3DJiapeng%2520Wang%2520and%2520Chengyu%2520Wang%2520and%2520Kunzhe%2520Huang%2520and%2520Jun%2520Huang%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520been%2520widely%2520studied%2520and%250Aapplied%2520in%2520numerous%2520applications.%2520However%252C%2520the%2520emphasis%2520on%2520brief%2520summary%2520texts%250Aduring%2520pre-training%2520prevents%2520CLIP%2520from%2520understanding%2520long%2520descriptions.%2520This%250Aissue%2520is%2520particularly%2520acute%2520regarding%2520videos%2520given%2520that%2520videos%2520often%2520contain%250Aabundant%2520detailed%2520contents.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520VideoCLIP-XL%2520%2528eXtra%250ALength%2529%2520model%252C%2520which%2520aims%2520to%2520unleash%2520the%2520long-description%2520understanding%250Acapability%2520of%2520video%2520CLIP%2520models.%2520Firstly%252C%2520we%2520establish%2520an%2520automatic%2520data%250Acollection%2520system%2520and%2520gather%2520a%2520large-scale%2520VILD%2520pre-training%2520dataset%2520with%2520VIdeo%250Aand%2520Long-Description%2520pairs.%2520Then%252C%2520we%2520propose%2520Text-similarity-guided%2520Primary%250AComponent%2520Matching%2520%2528TPCM%2529%2520to%2520better%2520learn%2520the%2520distribution%2520of%2520feature%2520space%250Awhile%2520expanding%2520the%2520long%2520description%2520capability.%2520We%2520also%2520introduce%2520two%2520new%250Atasks%2520namely%2520Detail-aware%2520Description%2520Ranking%2520%2528DDR%2529%2520and%2520Hallucination-aware%250ADescription%2520Ranking%2520%2528HDR%2529%2520for%2520further%2520understanding%2520improvement.%2520Finally%252C%2520we%250Aconstruct%2520a%2520Long%2520Video%2520Description%2520Ranking%2520%2528LVDR%2529%2520benchmark%2520for%2520evaluating%2520the%250Along-description%2520capability%2520more%2520comprehensively.%2520Extensive%2520experimental%250Aresults%2520on%2520widely-used%2520text-video%2520retrieval%2520benchmarks%2520with%2520both%2520short%2520and%2520long%250Adescriptions%2520and%2520our%2520LVDR%2520benchmark%2520can%2520fully%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoCLIP-XL%3A%20Advancing%20Long%20Description%20Understanding%20for%20Video%20CLIP%0A%20%20Models&entry.906535625=Jiapeng%20Wang%20and%20Chengyu%20Wang%20and%20Kunzhe%20Huang%20and%20Jun%20Huang%20and%20Lianwen%20Jin&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20been%20widely%20studied%20and%0Aapplied%20in%20numerous%20applications.%20However%2C%20the%20emphasis%20on%20brief%20summary%20texts%0Aduring%20pre-training%20prevents%20CLIP%20from%20understanding%20long%20descriptions.%20This%0Aissue%20is%20particularly%20acute%20regarding%20videos%20given%20that%20videos%20often%20contain%0Aabundant%20detailed%20contents.%20In%20this%20paper%2C%20we%20propose%20the%20VideoCLIP-XL%20%28eXtra%0ALength%29%20model%2C%20which%20aims%20to%20unleash%20the%20long-description%20understanding%0Acapability%20of%20video%20CLIP%20models.%20Firstly%2C%20we%20establish%20an%20automatic%20data%0Acollection%20system%20and%20gather%20a%20large-scale%20VILD%20pre-training%20dataset%20with%20VIdeo%0Aand%20Long-Description%20pairs.%20Then%2C%20we%20propose%20Text-similarity-guided%20Primary%0AComponent%20Matching%20%28TPCM%29%20to%20better%20learn%20the%20distribution%20of%20feature%20space%0Awhile%20expanding%20the%20long%20description%20capability.%20We%20also%20introduce%20two%20new%0Atasks%20namely%20Detail-aware%20Description%20Ranking%20%28DDR%29%20and%20Hallucination-aware%0ADescription%20Ranking%20%28HDR%29%20for%20further%20understanding%20improvement.%20Finally%2C%20we%0Aconstruct%20a%20Long%20Video%20Description%20Ranking%20%28LVDR%29%20benchmark%20for%20evaluating%20the%0Along-description%20capability%20more%20comprehensively.%20Extensive%20experimental%0Aresults%20on%20widely-used%20text-video%20retrieval%20benchmarks%20with%20both%20short%20and%20long%0Adescriptions%20and%20our%20LVDR%20benchmark%20can%20fully%20demonstrate%20the%20effectiveness%20of%0Aour%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00741v2&entry.124074799=Read"},
{"title": "Img2CAD: Conditioned 3D CAD Model Generation from Single Image with\n  Structured Visual Geometry", "author": "Tianrun Chen and Chunan Yu and Yuanqi Hu and Jing Li and Tao Xu and Runlong Cao and Lanyun Zhu and Ying Zang and Yong Zhang and Zejian Li and Linyun Sun", "abstract": "  In this paper, we propose Img2CAD, the first approach to our knowledge that\nuses 2D image inputs to generate CAD models with editable parameters. Unlike\nexisting AI methods for 3D model generation using text or image inputs often\nrely on mesh-based representations, which are incompatible with CAD tools and\nlack editability and fine control, Img2CAD enables seamless integration between\nAI-based 3D reconstruction and CAD software. We have identified an innovative\nintermediate representation called Structured Visual Geometry (SVG),\ncharacterized by vectorized wireframes extracted from objects. This\nrepresentation significantly enhances the performance of generating conditioned\nCAD models. Additionally, we introduce two new datasets to further support\nresearch in this area: ABC-mono, the largest known dataset comprising over\n200,000 3D CAD models with rendered images, and KOCAD, the first dataset\nfeaturing real-world captured objects alongside their ground truth CAD models,\nsupporting further research in conditioned CAD model generation.\n", "link": "http://arxiv.org/abs/2410.03417v1", "date": "2024-10-04", "relevancy": 3.0793, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6331}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6331}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Img2CAD%3A%20Conditioned%203D%20CAD%20Model%20Generation%20from%20Single%20Image%20with%0A%20%20Structured%20Visual%20Geometry&body=Title%3A%20Img2CAD%3A%20Conditioned%203D%20CAD%20Model%20Generation%20from%20Single%20Image%20with%0A%20%20Structured%20Visual%20Geometry%0AAuthor%3A%20Tianrun%20Chen%20and%20Chunan%20Yu%20and%20Yuanqi%20Hu%20and%20Jing%20Li%20and%20Tao%20Xu%20and%20Runlong%20Cao%20and%20Lanyun%20Zhu%20and%20Ying%20Zang%20and%20Yong%20Zhang%20and%20Zejian%20Li%20and%20Linyun%20Sun%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Img2CAD%2C%20the%20first%20approach%20to%20our%20knowledge%20that%0Auses%202D%20image%20inputs%20to%20generate%20CAD%20models%20with%20editable%20parameters.%20Unlike%0Aexisting%20AI%20methods%20for%203D%20model%20generation%20using%20text%20or%20image%20inputs%20often%0Arely%20on%20mesh-based%20representations%2C%20which%20are%20incompatible%20with%20CAD%20tools%20and%0Alack%20editability%20and%20fine%20control%2C%20Img2CAD%20enables%20seamless%20integration%20between%0AAI-based%203D%20reconstruction%20and%20CAD%20software.%20We%20have%20identified%20an%20innovative%0Aintermediate%20representation%20called%20Structured%20Visual%20Geometry%20%28SVG%29%2C%0Acharacterized%20by%20vectorized%20wireframes%20extracted%20from%20objects.%20This%0Arepresentation%20significantly%20enhances%20the%20performance%20of%20generating%20conditioned%0ACAD%20models.%20Additionally%2C%20we%20introduce%20two%20new%20datasets%20to%20further%20support%0Aresearch%20in%20this%20area%3A%20ABC-mono%2C%20the%20largest%20known%20dataset%20comprising%20over%0A200%2C000%203D%20CAD%20models%20with%20rendered%20images%2C%20and%20KOCAD%2C%20the%20first%20dataset%0Afeaturing%20real-world%20captured%20objects%20alongside%20their%20ground%20truth%20CAD%20models%2C%0Asupporting%20further%20research%20in%20conditioned%20CAD%20model%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImg2CAD%253A%2520Conditioned%25203D%2520CAD%2520Model%2520Generation%2520from%2520Single%2520Image%2520with%250A%2520%2520Structured%2520Visual%2520Geometry%26entry.906535625%3DTianrun%2520Chen%2520and%2520Chunan%2520Yu%2520and%2520Yuanqi%2520Hu%2520and%2520Jing%2520Li%2520and%2520Tao%2520Xu%2520and%2520Runlong%2520Cao%2520and%2520Lanyun%2520Zhu%2520and%2520Ying%2520Zang%2520and%2520Yong%2520Zhang%2520and%2520Zejian%2520Li%2520and%2520Linyun%2520Sun%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Img2CAD%252C%2520the%2520first%2520approach%2520to%2520our%2520knowledge%2520that%250Auses%25202D%2520image%2520inputs%2520to%2520generate%2520CAD%2520models%2520with%2520editable%2520parameters.%2520Unlike%250Aexisting%2520AI%2520methods%2520for%25203D%2520model%2520generation%2520using%2520text%2520or%2520image%2520inputs%2520often%250Arely%2520on%2520mesh-based%2520representations%252C%2520which%2520are%2520incompatible%2520with%2520CAD%2520tools%2520and%250Alack%2520editability%2520and%2520fine%2520control%252C%2520Img2CAD%2520enables%2520seamless%2520integration%2520between%250AAI-based%25203D%2520reconstruction%2520and%2520CAD%2520software.%2520We%2520have%2520identified%2520an%2520innovative%250Aintermediate%2520representation%2520called%2520Structured%2520Visual%2520Geometry%2520%2528SVG%2529%252C%250Acharacterized%2520by%2520vectorized%2520wireframes%2520extracted%2520from%2520objects.%2520This%250Arepresentation%2520significantly%2520enhances%2520the%2520performance%2520of%2520generating%2520conditioned%250ACAD%2520models.%2520Additionally%252C%2520we%2520introduce%2520two%2520new%2520datasets%2520to%2520further%2520support%250Aresearch%2520in%2520this%2520area%253A%2520ABC-mono%252C%2520the%2520largest%2520known%2520dataset%2520comprising%2520over%250A200%252C000%25203D%2520CAD%2520models%2520with%2520rendered%2520images%252C%2520and%2520KOCAD%252C%2520the%2520first%2520dataset%250Afeaturing%2520real-world%2520captured%2520objects%2520alongside%2520their%2520ground%2520truth%2520CAD%2520models%252C%250Asupporting%2520further%2520research%2520in%2520conditioned%2520CAD%2520model%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Img2CAD%3A%20Conditioned%203D%20CAD%20Model%20Generation%20from%20Single%20Image%20with%0A%20%20Structured%20Visual%20Geometry&entry.906535625=Tianrun%20Chen%20and%20Chunan%20Yu%20and%20Yuanqi%20Hu%20and%20Jing%20Li%20and%20Tao%20Xu%20and%20Runlong%20Cao%20and%20Lanyun%20Zhu%20and%20Ying%20Zang%20and%20Yong%20Zhang%20and%20Zejian%20Li%20and%20Linyun%20Sun&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Img2CAD%2C%20the%20first%20approach%20to%20our%20knowledge%20that%0Auses%202D%20image%20inputs%20to%20generate%20CAD%20models%20with%20editable%20parameters.%20Unlike%0Aexisting%20AI%20methods%20for%203D%20model%20generation%20using%20text%20or%20image%20inputs%20often%0Arely%20on%20mesh-based%20representations%2C%20which%20are%20incompatible%20with%20CAD%20tools%20and%0Alack%20editability%20and%20fine%20control%2C%20Img2CAD%20enables%20seamless%20integration%20between%0AAI-based%203D%20reconstruction%20and%20CAD%20software.%20We%20have%20identified%20an%20innovative%0Aintermediate%20representation%20called%20Structured%20Visual%20Geometry%20%28SVG%29%2C%0Acharacterized%20by%20vectorized%20wireframes%20extracted%20from%20objects.%20This%0Arepresentation%20significantly%20enhances%20the%20performance%20of%20generating%20conditioned%0ACAD%20models.%20Additionally%2C%20we%20introduce%20two%20new%20datasets%20to%20further%20support%0Aresearch%20in%20this%20area%3A%20ABC-mono%2C%20the%20largest%20known%20dataset%20comprising%20over%0A200%2C000%203D%20CAD%20models%20with%20rendered%20images%2C%20and%20KOCAD%2C%20the%20first%20dataset%0Afeaturing%20real-world%20captured%20objects%20alongside%20their%20ground%20truth%20CAD%20models%2C%0Asupporting%20further%20research%20in%20conditioned%20CAD%20model%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03417v1&entry.124074799=Read"},
{"title": "Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language\n  Models", "author": "Tinghui Zhu and Qin Liu and Fei Wang and Zhengzhong Tu and Muhao Chen", "abstract": "  Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.\n", "link": "http://arxiv.org/abs/2410.03659v1", "date": "2024-10-04", "relevancy": 2.9639, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6107}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20Cross-Modality%20Knowledge%20Conflict%20in%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20Unraveling%20Cross-Modality%20Knowledge%20Conflict%20in%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Tinghui%20Zhu%20and%20Qin%20Liu%20and%20Fei%20Wang%20and%20Zhengzhong%20Tu%20and%20Muhao%20Chen%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20for%20capturing%20and%20reasoning%20over%20multimodal%20inputs.%20However%2C%20these%0Amodels%20are%20prone%20to%20parametric%20knowledge%20conflicts%2C%20which%20arise%20from%0Ainconsistencies%20of%20represented%20knowledge%20between%20their%20vision%20and%20language%0Acomponents.%20In%20this%20paper%2C%20we%20formally%20define%20the%20problem%20of%0A%24%5Ctextbf%7Bcross-modality%20parametric%20knowledge%20conflict%7D%24%20and%20present%20a%0Asystematic%20approach%20to%20detect%2C%20interpret%2C%20and%20mitigate%20them.%20We%20introduce%20a%0Apipeline%20that%20identifies%20conflicts%20between%20visual%20and%20textual%20answers%2C%20showing%0Aa%20persistently%20high%20conflict%20rate%20across%20modalities%20in%20recent%20LVLMs%20regardless%0Aof%20the%20model%20size.%20We%20further%20investigate%20how%20these%20conflicts%20interfere%20with%0Athe%20inference%20process%20and%20propose%20a%20contrastive%20metric%20to%20discern%20the%0Aconflicting%20samples%20from%20the%20others.%20Building%20on%20these%20insights%2C%20we%20develop%20a%0Anovel%20dynamic%20contrastive%20decoding%20method%20that%20removes%20undesirable%20logits%0Ainferred%20from%20the%20less%20confident%20modality%20components%20based%20on%20answer%0Aconfidence.%20For%20models%20that%20do%20not%20provide%20logits%2C%20we%20also%20introduce%20two%0Aprompt-based%20strategies%20to%20mitigate%20the%20conflicts.%20Our%20methods%20achieve%0Apromising%20improvements%20in%20accuracy%20on%20both%20the%20ViQuAE%20and%20InfoSeek%20datasets.%0ASpecifically%2C%20using%20LLaVA-34B%2C%20our%20proposed%20dynamic%20contrastive%20decoding%0Aimproves%20an%20average%20accuracy%20of%202.24%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520Cross-Modality%2520Knowledge%2520Conflict%2520in%2520Large%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DTinghui%2520Zhu%2520and%2520Qin%2520Liu%2520and%2520Fei%2520Wang%2520and%2520Zhengzhong%2520Tu%2520and%2520Muhao%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520impressive%250Acapabilities%2520for%2520capturing%2520and%2520reasoning%2520over%2520multimodal%2520inputs.%2520However%252C%2520these%250Amodels%2520are%2520prone%2520to%2520parametric%2520knowledge%2520conflicts%252C%2520which%2520arise%2520from%250Ainconsistencies%2520of%2520represented%2520knowledge%2520between%2520their%2520vision%2520and%2520language%250Acomponents.%2520In%2520this%2520paper%252C%2520we%2520formally%2520define%2520the%2520problem%2520of%250A%2524%255Ctextbf%257Bcross-modality%2520parametric%2520knowledge%2520conflict%257D%2524%2520and%2520present%2520a%250Asystematic%2520approach%2520to%2520detect%252C%2520interpret%252C%2520and%2520mitigate%2520them.%2520We%2520introduce%2520a%250Apipeline%2520that%2520identifies%2520conflicts%2520between%2520visual%2520and%2520textual%2520answers%252C%2520showing%250Aa%2520persistently%2520high%2520conflict%2520rate%2520across%2520modalities%2520in%2520recent%2520LVLMs%2520regardless%250Aof%2520the%2520model%2520size.%2520We%2520further%2520investigate%2520how%2520these%2520conflicts%2520interfere%2520with%250Athe%2520inference%2520process%2520and%2520propose%2520a%2520contrastive%2520metric%2520to%2520discern%2520the%250Aconflicting%2520samples%2520from%2520the%2520others.%2520Building%2520on%2520these%2520insights%252C%2520we%2520develop%2520a%250Anovel%2520dynamic%2520contrastive%2520decoding%2520method%2520that%2520removes%2520undesirable%2520logits%250Ainferred%2520from%2520the%2520less%2520confident%2520modality%2520components%2520based%2520on%2520answer%250Aconfidence.%2520For%2520models%2520that%2520do%2520not%2520provide%2520logits%252C%2520we%2520also%2520introduce%2520two%250Aprompt-based%2520strategies%2520to%2520mitigate%2520the%2520conflicts.%2520Our%2520methods%2520achieve%250Apromising%2520improvements%2520in%2520accuracy%2520on%2520both%2520the%2520ViQuAE%2520and%2520InfoSeek%2520datasets.%250ASpecifically%252C%2520using%2520LLaVA-34B%252C%2520our%2520proposed%2520dynamic%2520contrastive%2520decoding%250Aimproves%2520an%2520average%2520accuracy%2520of%25202.24%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20Cross-Modality%20Knowledge%20Conflict%20in%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Tinghui%20Zhu%20and%20Qin%20Liu%20and%20Fei%20Wang%20and%20Zhengzhong%20Tu%20and%20Muhao%20Chen&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20impressive%0Acapabilities%20for%20capturing%20and%20reasoning%20over%20multimodal%20inputs.%20However%2C%20these%0Amodels%20are%20prone%20to%20parametric%20knowledge%20conflicts%2C%20which%20arise%20from%0Ainconsistencies%20of%20represented%20knowledge%20between%20their%20vision%20and%20language%0Acomponents.%20In%20this%20paper%2C%20we%20formally%20define%20the%20problem%20of%0A%24%5Ctextbf%7Bcross-modality%20parametric%20knowledge%20conflict%7D%24%20and%20present%20a%0Asystematic%20approach%20to%20detect%2C%20interpret%2C%20and%20mitigate%20them.%20We%20introduce%20a%0Apipeline%20that%20identifies%20conflicts%20between%20visual%20and%20textual%20answers%2C%20showing%0Aa%20persistently%20high%20conflict%20rate%20across%20modalities%20in%20recent%20LVLMs%20regardless%0Aof%20the%20model%20size.%20We%20further%20investigate%20how%20these%20conflicts%20interfere%20with%0Athe%20inference%20process%20and%20propose%20a%20contrastive%20metric%20to%20discern%20the%0Aconflicting%20samples%20from%20the%20others.%20Building%20on%20these%20insights%2C%20we%20develop%20a%0Anovel%20dynamic%20contrastive%20decoding%20method%20that%20removes%20undesirable%20logits%0Ainferred%20from%20the%20less%20confident%20modality%20components%20based%20on%20answer%0Aconfidence.%20For%20models%20that%20do%20not%20provide%20logits%2C%20we%20also%20introduce%20two%0Aprompt-based%20strategies%20to%20mitigate%20the%20conflicts.%20Our%20methods%20achieve%0Apromising%20improvements%20in%20accuracy%20on%20both%20the%20ViQuAE%20and%20InfoSeek%20datasets.%0ASpecifically%2C%20using%20LLaVA-34B%2C%20our%20proposed%20dynamic%20contrastive%20decoding%0Aimproves%20an%20average%20accuracy%20of%202.24%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03659v1&entry.124074799=Read"},
{"title": "Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need", "author": "Xianlong Wang and Minghui Li and Wei Liu and Hangtao Zhang and Shengshan Hu and Yechao Zhang and Ziqi Zhou and Hai Jin", "abstract": "  Traditional unlearnable strategies have been proposed to prevent unauthorized\nusers from training on the 2D image data. With more 3D point cloud data\ncontaining sensitivity information, unauthorized usage of this new type data\nhas also become a serious concern. To address this, we propose the first\nintegral unlearnable framework for 3D point clouds including two processes: (i)\nwe propose an unlearnable data protection scheme, involving a class-wise\nsetting established by a category-adaptive allocation strategy and\nmulti-transformations assigned to samples; (ii) we propose a data restoration\nscheme that utilizes class-wise inverse matrix transformation, thus enabling\nauthorized-only training for unlearnable data. This restoration process is a\npractical issue overlooked in most existing unlearnable literature, \\ie, even\nauthorized users struggle to gain knowledge from 3D unlearnable data. Both\ntheoretical and empirical results (including 6 datasets, 16 models, and 2\ntasks) demonstrate the effectiveness of our proposed unlearnable framework. Our\ncode is available at \\url{https://github.com/CGCL-codes/UnlearnablePC}\n", "link": "http://arxiv.org/abs/2410.03644v1", "date": "2024-10-04", "relevancy": 2.8193, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5891}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5658}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlearnable%203D%20Point%20Clouds%3A%20Class-wise%20Transformation%20Is%20All%20You%20Need&body=Title%3A%20Unlearnable%203D%20Point%20Clouds%3A%20Class-wise%20Transformation%20Is%20All%20You%20Need%0AAuthor%3A%20Xianlong%20Wang%20and%20Minghui%20Li%20and%20Wei%20Liu%20and%20Hangtao%20Zhang%20and%20Shengshan%20Hu%20and%20Yechao%20Zhang%20and%20Ziqi%20Zhou%20and%20Hai%20Jin%0AAbstract%3A%20%20%20Traditional%20unlearnable%20strategies%20have%20been%20proposed%20to%20prevent%20unauthorized%0Ausers%20from%20training%20on%20the%202D%20image%20data.%20With%20more%203D%20point%20cloud%20data%0Acontaining%20sensitivity%20information%2C%20unauthorized%20usage%20of%20this%20new%20type%20data%0Ahas%20also%20become%20a%20serious%20concern.%20To%20address%20this%2C%20we%20propose%20the%20first%0Aintegral%20unlearnable%20framework%20for%203D%20point%20clouds%20including%20two%20processes%3A%20%28i%29%0Awe%20propose%20an%20unlearnable%20data%20protection%20scheme%2C%20involving%20a%20class-wise%0Asetting%20established%20by%20a%20category-adaptive%20allocation%20strategy%20and%0Amulti-transformations%20assigned%20to%20samples%3B%20%28ii%29%20we%20propose%20a%20data%20restoration%0Ascheme%20that%20utilizes%20class-wise%20inverse%20matrix%20transformation%2C%20thus%20enabling%0Aauthorized-only%20training%20for%20unlearnable%20data.%20This%20restoration%20process%20is%20a%0Apractical%20issue%20overlooked%20in%20most%20existing%20unlearnable%20literature%2C%20%5Cie%2C%20even%0Aauthorized%20users%20struggle%20to%20gain%20knowledge%20from%203D%20unlearnable%20data.%20Both%0Atheoretical%20and%20empirical%20results%20%28including%206%20datasets%2C%2016%20models%2C%20and%202%0Atasks%29%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20unlearnable%20framework.%20Our%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/CGCL-codes/UnlearnablePC%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlearnable%25203D%2520Point%2520Clouds%253A%2520Class-wise%2520Transformation%2520Is%2520All%2520You%2520Need%26entry.906535625%3DXianlong%2520Wang%2520and%2520Minghui%2520Li%2520and%2520Wei%2520Liu%2520and%2520Hangtao%2520Zhang%2520and%2520Shengshan%2520Hu%2520and%2520Yechao%2520Zhang%2520and%2520Ziqi%2520Zhou%2520and%2520Hai%2520Jin%26entry.1292438233%3D%2520%2520Traditional%2520unlearnable%2520strategies%2520have%2520been%2520proposed%2520to%2520prevent%2520unauthorized%250Ausers%2520from%2520training%2520on%2520the%25202D%2520image%2520data.%2520With%2520more%25203D%2520point%2520cloud%2520data%250Acontaining%2520sensitivity%2520information%252C%2520unauthorized%2520usage%2520of%2520this%2520new%2520type%2520data%250Ahas%2520also%2520become%2520a%2520serious%2520concern.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520first%250Aintegral%2520unlearnable%2520framework%2520for%25203D%2520point%2520clouds%2520including%2520two%2520processes%253A%2520%2528i%2529%250Awe%2520propose%2520an%2520unlearnable%2520data%2520protection%2520scheme%252C%2520involving%2520a%2520class-wise%250Asetting%2520established%2520by%2520a%2520category-adaptive%2520allocation%2520strategy%2520and%250Amulti-transformations%2520assigned%2520to%2520samples%253B%2520%2528ii%2529%2520we%2520propose%2520a%2520data%2520restoration%250Ascheme%2520that%2520utilizes%2520class-wise%2520inverse%2520matrix%2520transformation%252C%2520thus%2520enabling%250Aauthorized-only%2520training%2520for%2520unlearnable%2520data.%2520This%2520restoration%2520process%2520is%2520a%250Apractical%2520issue%2520overlooked%2520in%2520most%2520existing%2520unlearnable%2520literature%252C%2520%255Cie%252C%2520even%250Aauthorized%2520users%2520struggle%2520to%2520gain%2520knowledge%2520from%25203D%2520unlearnable%2520data.%2520Both%250Atheoretical%2520and%2520empirical%2520results%2520%2528including%25206%2520datasets%252C%252016%2520models%252C%2520and%25202%250Atasks%2529%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520unlearnable%2520framework.%2520Our%250Acode%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/CGCL-codes/UnlearnablePC%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlearnable%203D%20Point%20Clouds%3A%20Class-wise%20Transformation%20Is%20All%20You%20Need&entry.906535625=Xianlong%20Wang%20and%20Minghui%20Li%20and%20Wei%20Liu%20and%20Hangtao%20Zhang%20and%20Shengshan%20Hu%20and%20Yechao%20Zhang%20and%20Ziqi%20Zhou%20and%20Hai%20Jin&entry.1292438233=%20%20Traditional%20unlearnable%20strategies%20have%20been%20proposed%20to%20prevent%20unauthorized%0Ausers%20from%20training%20on%20the%202D%20image%20data.%20With%20more%203D%20point%20cloud%20data%0Acontaining%20sensitivity%20information%2C%20unauthorized%20usage%20of%20this%20new%20type%20data%0Ahas%20also%20become%20a%20serious%20concern.%20To%20address%20this%2C%20we%20propose%20the%20first%0Aintegral%20unlearnable%20framework%20for%203D%20point%20clouds%20including%20two%20processes%3A%20%28i%29%0Awe%20propose%20an%20unlearnable%20data%20protection%20scheme%2C%20involving%20a%20class-wise%0Asetting%20established%20by%20a%20category-adaptive%20allocation%20strategy%20and%0Amulti-transformations%20assigned%20to%20samples%3B%20%28ii%29%20we%20propose%20a%20data%20restoration%0Ascheme%20that%20utilizes%20class-wise%20inverse%20matrix%20transformation%2C%20thus%20enabling%0Aauthorized-only%20training%20for%20unlearnable%20data.%20This%20restoration%20process%20is%20a%0Apractical%20issue%20overlooked%20in%20most%20existing%20unlearnable%20literature%2C%20%5Cie%2C%20even%0Aauthorized%20users%20struggle%20to%20gain%20knowledge%20from%203D%20unlearnable%20data.%20Both%0Atheoretical%20and%20empirical%20results%20%28including%206%20datasets%2C%2016%20models%2C%20and%202%0Atasks%29%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20unlearnable%20framework.%20Our%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/CGCL-codes/UnlearnablePC%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03644v1&entry.124074799=Read"},
{"title": "Bayesian Unsupervised Disentanglement of Anatomy and Geometry for Deep\n  Groupwise Image Registration", "author": "Xinzhe Luo and Xin Wang and Linda Shapiro and Chun Yuan and Jianfeng Feng and Xiahai Zhuang", "abstract": "  This article presents a general Bayesian learning framework for multi-modal\ngroupwise image registration. The method builds on probabilistic modelling of\nthe image generative process, where the underlying common anatomy and geometric\nvariations of the observed images are explicitly disentangled as latent\nvariables. Therefore, groupwise image registration is achieved via hierarchical\nBayesian inference. We propose a novel hierarchical variational auto-encoding\narchitecture to realise the inference procedure of the latent variables, where\nthe registration parameters can be explicitly estimated in a mathematically\ninterpretable fashion. Remarkably, this new paradigm learns groupwise image\nregistration in an unsupervised closed-loop self-reconstruction process,\nsparing the burden of designing complex image-based similarity measures. The\ncomputationally efficient disentangled network architecture is also inherently\nscalable and flexible, allowing for groupwise registration on large-scale image\ngroups with variable sizes. Furthermore, the inferred structural\nrepresentations from multi-modal images via disentanglement learning are\ncapable of capturing the latent anatomy of the observations with visual\nsemantics. Extensive experiments were conducted to validate the proposed\nframework, including four different datasets from cardiac, brain, and abdominal\nmedical images. The results have demonstrated the superiority of our method\nover conventional similarity-based approaches in terms of accuracy, efficiency,\nscalability, and interpretability.\n", "link": "http://arxiv.org/abs/2401.02141v2", "date": "2024-10-04", "relevancy": 2.7967, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5732}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5561}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Unsupervised%20Disentanglement%20of%20Anatomy%20and%20Geometry%20for%20Deep%0A%20%20Groupwise%20Image%20Registration&body=Title%3A%20Bayesian%20Unsupervised%20Disentanglement%20of%20Anatomy%20and%20Geometry%20for%20Deep%0A%20%20Groupwise%20Image%20Registration%0AAuthor%3A%20Xinzhe%20Luo%20and%20Xin%20Wang%20and%20Linda%20Shapiro%20and%20Chun%20Yuan%20and%20Jianfeng%20Feng%20and%20Xiahai%20Zhuang%0AAbstract%3A%20%20%20This%20article%20presents%20a%20general%20Bayesian%20learning%20framework%20for%20multi-modal%0Agroupwise%20image%20registration.%20The%20method%20builds%20on%20probabilistic%20modelling%20of%0Athe%20image%20generative%20process%2C%20where%20the%20underlying%20common%20anatomy%20and%20geometric%0Avariations%20of%20the%20observed%20images%20are%20explicitly%20disentangled%20as%20latent%0Avariables.%20Therefore%2C%20groupwise%20image%20registration%20is%20achieved%20via%20hierarchical%0ABayesian%20inference.%20We%20propose%20a%20novel%20hierarchical%20variational%20auto-encoding%0Aarchitecture%20to%20realise%20the%20inference%20procedure%20of%20the%20latent%20variables%2C%20where%0Athe%20registration%20parameters%20can%20be%20explicitly%20estimated%20in%20a%20mathematically%0Ainterpretable%20fashion.%20Remarkably%2C%20this%20new%20paradigm%20learns%20groupwise%20image%0Aregistration%20in%20an%20unsupervised%20closed-loop%20self-reconstruction%20process%2C%0Asparing%20the%20burden%20of%20designing%20complex%20image-based%20similarity%20measures.%20The%0Acomputationally%20efficient%20disentangled%20network%20architecture%20is%20also%20inherently%0Ascalable%20and%20flexible%2C%20allowing%20for%20groupwise%20registration%20on%20large-scale%20image%0Agroups%20with%20variable%20sizes.%20Furthermore%2C%20the%20inferred%20structural%0Arepresentations%20from%20multi-modal%20images%20via%20disentanglement%20learning%20are%0Acapable%20of%20capturing%20the%20latent%20anatomy%20of%20the%20observations%20with%20visual%0Asemantics.%20Extensive%20experiments%20were%20conducted%20to%20validate%20the%20proposed%0Aframework%2C%20including%20four%20different%20datasets%20from%20cardiac%2C%20brain%2C%20and%20abdominal%0Amedical%20images.%20The%20results%20have%20demonstrated%20the%20superiority%20of%20our%20method%0Aover%20conventional%20similarity-based%20approaches%20in%20terms%20of%20accuracy%2C%20efficiency%2C%0Ascalability%2C%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Unsupervised%2520Disentanglement%2520of%2520Anatomy%2520and%2520Geometry%2520for%2520Deep%250A%2520%2520Groupwise%2520Image%2520Registration%26entry.906535625%3DXinzhe%2520Luo%2520and%2520Xin%2520Wang%2520and%2520Linda%2520Shapiro%2520and%2520Chun%2520Yuan%2520and%2520Jianfeng%2520Feng%2520and%2520Xiahai%2520Zhuang%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520a%2520general%2520Bayesian%2520learning%2520framework%2520for%2520multi-modal%250Agroupwise%2520image%2520registration.%2520The%2520method%2520builds%2520on%2520probabilistic%2520modelling%2520of%250Athe%2520image%2520generative%2520process%252C%2520where%2520the%2520underlying%2520common%2520anatomy%2520and%2520geometric%250Avariations%2520of%2520the%2520observed%2520images%2520are%2520explicitly%2520disentangled%2520as%2520latent%250Avariables.%2520Therefore%252C%2520groupwise%2520image%2520registration%2520is%2520achieved%2520via%2520hierarchical%250ABayesian%2520inference.%2520We%2520propose%2520a%2520novel%2520hierarchical%2520variational%2520auto-encoding%250Aarchitecture%2520to%2520realise%2520the%2520inference%2520procedure%2520of%2520the%2520latent%2520variables%252C%2520where%250Athe%2520registration%2520parameters%2520can%2520be%2520explicitly%2520estimated%2520in%2520a%2520mathematically%250Ainterpretable%2520fashion.%2520Remarkably%252C%2520this%2520new%2520paradigm%2520learns%2520groupwise%2520image%250Aregistration%2520in%2520an%2520unsupervised%2520closed-loop%2520self-reconstruction%2520process%252C%250Asparing%2520the%2520burden%2520of%2520designing%2520complex%2520image-based%2520similarity%2520measures.%2520The%250Acomputationally%2520efficient%2520disentangled%2520network%2520architecture%2520is%2520also%2520inherently%250Ascalable%2520and%2520flexible%252C%2520allowing%2520for%2520groupwise%2520registration%2520on%2520large-scale%2520image%250Agroups%2520with%2520variable%2520sizes.%2520Furthermore%252C%2520the%2520inferred%2520structural%250Arepresentations%2520from%2520multi-modal%2520images%2520via%2520disentanglement%2520learning%2520are%250Acapable%2520of%2520capturing%2520the%2520latent%2520anatomy%2520of%2520the%2520observations%2520with%2520visual%250Asemantics.%2520Extensive%2520experiments%2520were%2520conducted%2520to%2520validate%2520the%2520proposed%250Aframework%252C%2520including%2520four%2520different%2520datasets%2520from%2520cardiac%252C%2520brain%252C%2520and%2520abdominal%250Amedical%2520images.%2520The%2520results%2520have%2520demonstrated%2520the%2520superiority%2520of%2520our%2520method%250Aover%2520conventional%2520similarity-based%2520approaches%2520in%2520terms%2520of%2520accuracy%252C%2520efficiency%252C%250Ascalability%252C%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Unsupervised%20Disentanglement%20of%20Anatomy%20and%20Geometry%20for%20Deep%0A%20%20Groupwise%20Image%20Registration&entry.906535625=Xinzhe%20Luo%20and%20Xin%20Wang%20and%20Linda%20Shapiro%20and%20Chun%20Yuan%20and%20Jianfeng%20Feng%20and%20Xiahai%20Zhuang&entry.1292438233=%20%20This%20article%20presents%20a%20general%20Bayesian%20learning%20framework%20for%20multi-modal%0Agroupwise%20image%20registration.%20The%20method%20builds%20on%20probabilistic%20modelling%20of%0Athe%20image%20generative%20process%2C%20where%20the%20underlying%20common%20anatomy%20and%20geometric%0Avariations%20of%20the%20observed%20images%20are%20explicitly%20disentangled%20as%20latent%0Avariables.%20Therefore%2C%20groupwise%20image%20registration%20is%20achieved%20via%20hierarchical%0ABayesian%20inference.%20We%20propose%20a%20novel%20hierarchical%20variational%20auto-encoding%0Aarchitecture%20to%20realise%20the%20inference%20procedure%20of%20the%20latent%20variables%2C%20where%0Athe%20registration%20parameters%20can%20be%20explicitly%20estimated%20in%20a%20mathematically%0Ainterpretable%20fashion.%20Remarkably%2C%20this%20new%20paradigm%20learns%20groupwise%20image%0Aregistration%20in%20an%20unsupervised%20closed-loop%20self-reconstruction%20process%2C%0Asparing%20the%20burden%20of%20designing%20complex%20image-based%20similarity%20measures.%20The%0Acomputationally%20efficient%20disentangled%20network%20architecture%20is%20also%20inherently%0Ascalable%20and%20flexible%2C%20allowing%20for%20groupwise%20registration%20on%20large-scale%20image%0Agroups%20with%20variable%20sizes.%20Furthermore%2C%20the%20inferred%20structural%0Arepresentations%20from%20multi-modal%20images%20via%20disentanglement%20learning%20are%0Acapable%20of%20capturing%20the%20latent%20anatomy%20of%20the%20observations%20with%20visual%0Asemantics.%20Extensive%20experiments%20were%20conducted%20to%20validate%20the%20proposed%0Aframework%2C%20including%20four%20different%20datasets%20from%20cardiac%2C%20brain%2C%20and%20abdominal%0Amedical%20images.%20The%20results%20have%20demonstrated%20the%20superiority%20of%20our%20method%0Aover%20conventional%20similarity-based%20approaches%20in%20terms%20of%20accuracy%2C%20efficiency%2C%0Ascalability%2C%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02141v2&entry.124074799=Read"},
{"title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video\n  Large Language Models", "author": "Haibo Wang and Zhiyang Xu and Yu Cheng and Shizhe Diao and Yufan Zhou and Yixin Cao and Qifan Wang and Weifeng Ge and Lifu Huang", "abstract": "  Video Large Language Models (Video-LLMs) have demonstrated remarkable\ncapabilities in coarse-grained video understanding, however, they struggle with\nfine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,\na novel Video-LLM adept at perceiving and reasoning over specific video moments\nin a fine-grained manner. We identify that current Video-LLMs have limitations\nfor fine-grained video understanding since they lack effective temporal\nmodeling and timestamp representation. In light of this, we sharpen our model\nby incorporating (1) an additional temporal stream to encode the relationships\nbetween frames and (2) discrete temporal tokens enriched with specific time\nknowledge to represent timestamps. To optimize the training of\nGrounded-VideoLLM, we employ a multi-stage training scheme, beginning with\nsimple video-captioning tasks and progressively introducing video temporal\ngrounding tasks of increasing complexity. To further enhance\nGrounded-VideoLLM's temporal reasoning capability, we also curate a grounded\nVideoQA dataset by an automatic annotation pipeline. Extensive experiments\ndemonstrate that Grounded-VideoLLM not only excels in fine-grained grounding\ntasks such as temporal sentence grounding, dense video captioning, and grounded\nVideoQA, but also shows great potential as a versatile video assistant for\ngeneral video understanding.\n", "link": "http://arxiv.org/abs/2410.03290v1", "date": "2024-10-04", "relevancy": 2.7504, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounded-VideoLLM%3A%20Sharpening%20Fine-grained%20Temporal%20Grounding%20in%20Video%0A%20%20Large%20Language%20Models&body=Title%3A%20Grounded-VideoLLM%3A%20Sharpening%20Fine-grained%20Temporal%20Grounding%20in%20Video%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Haibo%20Wang%20and%20Zhiyang%20Xu%20and%20Yu%20Cheng%20and%20Shizhe%20Diao%20and%20Yufan%20Zhou%20and%20Yixin%20Cao%20and%20Qifan%20Wang%20and%20Weifeng%20Ge%20and%20Lifu%20Huang%0AAbstract%3A%20%20%20Video%20Large%20Language%20Models%20%28Video-LLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20coarse-grained%20video%20understanding%2C%20however%2C%20they%20struggle%20with%0Afine-grained%20temporal%20grounding.%20In%20this%20paper%2C%20we%20introduce%20Grounded-VideoLLM%2C%0Aa%20novel%20Video-LLM%20adept%20at%20perceiving%20and%20reasoning%20over%20specific%20video%20moments%0Ain%20a%20fine-grained%20manner.%20We%20identify%20that%20current%20Video-LLMs%20have%20limitations%0Afor%20fine-grained%20video%20understanding%20since%20they%20lack%20effective%20temporal%0Amodeling%20and%20timestamp%20representation.%20In%20light%20of%20this%2C%20we%20sharpen%20our%20model%0Aby%20incorporating%20%281%29%20an%20additional%20temporal%20stream%20to%20encode%20the%20relationships%0Abetween%20frames%20and%20%282%29%20discrete%20temporal%20tokens%20enriched%20with%20specific%20time%0Aknowledge%20to%20represent%20timestamps.%20To%20optimize%20the%20training%20of%0AGrounded-VideoLLM%2C%20we%20employ%20a%20multi-stage%20training%20scheme%2C%20beginning%20with%0Asimple%20video-captioning%20tasks%20and%20progressively%20introducing%20video%20temporal%0Agrounding%20tasks%20of%20increasing%20complexity.%20To%20further%20enhance%0AGrounded-VideoLLM%27s%20temporal%20reasoning%20capability%2C%20we%20also%20curate%20a%20grounded%0AVideoQA%20dataset%20by%20an%20automatic%20annotation%20pipeline.%20Extensive%20experiments%0Ademonstrate%20that%20Grounded-VideoLLM%20not%20only%20excels%20in%20fine-grained%20grounding%0Atasks%20such%20as%20temporal%20sentence%20grounding%2C%20dense%20video%20captioning%2C%20and%20grounded%0AVideoQA%2C%20but%20also%20shows%20great%20potential%20as%20a%20versatile%20video%20assistant%20for%0Ageneral%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounded-VideoLLM%253A%2520Sharpening%2520Fine-grained%2520Temporal%2520Grounding%2520in%2520Video%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DHaibo%2520Wang%2520and%2520Zhiyang%2520Xu%2520and%2520Yu%2520Cheng%2520and%2520Shizhe%2520Diao%2520and%2520Yufan%2520Zhou%2520and%2520Yixin%2520Cao%2520and%2520Qifan%2520Wang%2520and%2520Weifeng%2520Ge%2520and%2520Lifu%2520Huang%26entry.1292438233%3D%2520%2520Video%2520Large%2520Language%2520Models%2520%2528Video-LLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520coarse-grained%2520video%2520understanding%252C%2520however%252C%2520they%2520struggle%2520with%250Afine-grained%2520temporal%2520grounding.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Grounded-VideoLLM%252C%250Aa%2520novel%2520Video-LLM%2520adept%2520at%2520perceiving%2520and%2520reasoning%2520over%2520specific%2520video%2520moments%250Ain%2520a%2520fine-grained%2520manner.%2520We%2520identify%2520that%2520current%2520Video-LLMs%2520have%2520limitations%250Afor%2520fine-grained%2520video%2520understanding%2520since%2520they%2520lack%2520effective%2520temporal%250Amodeling%2520and%2520timestamp%2520representation.%2520In%2520light%2520of%2520this%252C%2520we%2520sharpen%2520our%2520model%250Aby%2520incorporating%2520%25281%2529%2520an%2520additional%2520temporal%2520stream%2520to%2520encode%2520the%2520relationships%250Abetween%2520frames%2520and%2520%25282%2529%2520discrete%2520temporal%2520tokens%2520enriched%2520with%2520specific%2520time%250Aknowledge%2520to%2520represent%2520timestamps.%2520To%2520optimize%2520the%2520training%2520of%250AGrounded-VideoLLM%252C%2520we%2520employ%2520a%2520multi-stage%2520training%2520scheme%252C%2520beginning%2520with%250Asimple%2520video-captioning%2520tasks%2520and%2520progressively%2520introducing%2520video%2520temporal%250Agrounding%2520tasks%2520of%2520increasing%2520complexity.%2520To%2520further%2520enhance%250AGrounded-VideoLLM%2527s%2520temporal%2520reasoning%2520capability%252C%2520we%2520also%2520curate%2520a%2520grounded%250AVideoQA%2520dataset%2520by%2520an%2520automatic%2520annotation%2520pipeline.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Grounded-VideoLLM%2520not%2520only%2520excels%2520in%2520fine-grained%2520grounding%250Atasks%2520such%2520as%2520temporal%2520sentence%2520grounding%252C%2520dense%2520video%2520captioning%252C%2520and%2520grounded%250AVideoQA%252C%2520but%2520also%2520shows%2520great%2520potential%2520as%2520a%2520versatile%2520video%2520assistant%2520for%250Ageneral%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounded-VideoLLM%3A%20Sharpening%20Fine-grained%20Temporal%20Grounding%20in%20Video%0A%20%20Large%20Language%20Models&entry.906535625=Haibo%20Wang%20and%20Zhiyang%20Xu%20and%20Yu%20Cheng%20and%20Shizhe%20Diao%20and%20Yufan%20Zhou%20and%20Yixin%20Cao%20and%20Qifan%20Wang%20and%20Weifeng%20Ge%20and%20Lifu%20Huang&entry.1292438233=%20%20Video%20Large%20Language%20Models%20%28Video-LLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20in%20coarse-grained%20video%20understanding%2C%20however%2C%20they%20struggle%20with%0Afine-grained%20temporal%20grounding.%20In%20this%20paper%2C%20we%20introduce%20Grounded-VideoLLM%2C%0Aa%20novel%20Video-LLM%20adept%20at%20perceiving%20and%20reasoning%20over%20specific%20video%20moments%0Ain%20a%20fine-grained%20manner.%20We%20identify%20that%20current%20Video-LLMs%20have%20limitations%0Afor%20fine-grained%20video%20understanding%20since%20they%20lack%20effective%20temporal%0Amodeling%20and%20timestamp%20representation.%20In%20light%20of%20this%2C%20we%20sharpen%20our%20model%0Aby%20incorporating%20%281%29%20an%20additional%20temporal%20stream%20to%20encode%20the%20relationships%0Abetween%20frames%20and%20%282%29%20discrete%20temporal%20tokens%20enriched%20with%20specific%20time%0Aknowledge%20to%20represent%20timestamps.%20To%20optimize%20the%20training%20of%0AGrounded-VideoLLM%2C%20we%20employ%20a%20multi-stage%20training%20scheme%2C%20beginning%20with%0Asimple%20video-captioning%20tasks%20and%20progressively%20introducing%20video%20temporal%0Agrounding%20tasks%20of%20increasing%20complexity.%20To%20further%20enhance%0AGrounded-VideoLLM%27s%20temporal%20reasoning%20capability%2C%20we%20also%20curate%20a%20grounded%0AVideoQA%20dataset%20by%20an%20automatic%20annotation%20pipeline.%20Extensive%20experiments%0Ademonstrate%20that%20Grounded-VideoLLM%20not%20only%20excels%20in%20fine-grained%20grounding%0Atasks%20such%20as%20temporal%20sentence%20grounding%2C%20dense%20video%20captioning%2C%20and%20grounded%0AVideoQA%2C%20but%20also%20shows%20great%20potential%20as%20a%20versatile%20video%20assistant%20for%0Ageneral%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03290v1&entry.124074799=Read"},
{"title": "Dessie: Disentanglement for Articulated 3D Horse Shape and Pose\n  Estimation from Images", "author": "Ci Li and Yi Yang and Zehang Weng and Elin Hernlund and Silvia Zuffi and Hedvig Kjellstr\u00f6m", "abstract": "  In recent years, 3D parametric animal models have been developed to aid in\nestimating 3D shape and pose from images and video. While progress has been\nmade for humans, it's more challenging for animals due to limited annotated\ndata. To address this, we introduce the first method using synthetic data\ngeneration and disentanglement to learn to regress 3D shape and pose. Focusing\non horses, we use text-based texture generation and a synthetic data pipeline\nto create varied shapes, poses, and appearances, learning disentangled spaces.\nOur method, Dessie, surpasses existing 3D horse reconstruction methods and\ngeneralizes to other large animals like zebras, cows, and deer. See the project\nwebsite at: \\url{https://celiali.github.io/Dessie/}.\n", "link": "http://arxiv.org/abs/2410.03438v1", "date": "2024-10-04", "relevancy": 2.7497, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5505}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5497}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dessie%3A%20Disentanglement%20for%20Articulated%203D%20Horse%20Shape%20and%20Pose%0A%20%20Estimation%20from%20Images&body=Title%3A%20Dessie%3A%20Disentanglement%20for%20Articulated%203D%20Horse%20Shape%20and%20Pose%0A%20%20Estimation%20from%20Images%0AAuthor%3A%20Ci%20Li%20and%20Yi%20Yang%20and%20Zehang%20Weng%20and%20Elin%20Hernlund%20and%20Silvia%20Zuffi%20and%20Hedvig%20Kjellstr%C3%B6m%0AAbstract%3A%20%20%20In%20recent%20years%2C%203D%20parametric%20animal%20models%20have%20been%20developed%20to%20aid%20in%0Aestimating%203D%20shape%20and%20pose%20from%20images%20and%20video.%20While%20progress%20has%20been%0Amade%20for%20humans%2C%20it%27s%20more%20challenging%20for%20animals%20due%20to%20limited%20annotated%0Adata.%20To%20address%20this%2C%20we%20introduce%20the%20first%20method%20using%20synthetic%20data%0Ageneration%20and%20disentanglement%20to%20learn%20to%20regress%203D%20shape%20and%20pose.%20Focusing%0Aon%20horses%2C%20we%20use%20text-based%20texture%20generation%20and%20a%20synthetic%20data%20pipeline%0Ato%20create%20varied%20shapes%2C%20poses%2C%20and%20appearances%2C%20learning%20disentangled%20spaces.%0AOur%20method%2C%20Dessie%2C%20surpasses%20existing%203D%20horse%20reconstruction%20methods%20and%0Ageneralizes%20to%20other%20large%20animals%20like%20zebras%2C%20cows%2C%20and%20deer.%20See%20the%20project%0Awebsite%20at%3A%20%5Curl%7Bhttps%3A//celiali.github.io/Dessie/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDessie%253A%2520Disentanglement%2520for%2520Articulated%25203D%2520Horse%2520Shape%2520and%2520Pose%250A%2520%2520Estimation%2520from%2520Images%26entry.906535625%3DCi%2520Li%2520and%2520Yi%2520Yang%2520and%2520Zehang%2520Weng%2520and%2520Elin%2520Hernlund%2520and%2520Silvia%2520Zuffi%2520and%2520Hedvig%2520Kjellstr%25C3%25B6m%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%25203D%2520parametric%2520animal%2520models%2520have%2520been%2520developed%2520to%2520aid%2520in%250Aestimating%25203D%2520shape%2520and%2520pose%2520from%2520images%2520and%2520video.%2520While%2520progress%2520has%2520been%250Amade%2520for%2520humans%252C%2520it%2527s%2520more%2520challenging%2520for%2520animals%2520due%2520to%2520limited%2520annotated%250Adata.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520first%2520method%2520using%2520synthetic%2520data%250Ageneration%2520and%2520disentanglement%2520to%2520learn%2520to%2520regress%25203D%2520shape%2520and%2520pose.%2520Focusing%250Aon%2520horses%252C%2520we%2520use%2520text-based%2520texture%2520generation%2520and%2520a%2520synthetic%2520data%2520pipeline%250Ato%2520create%2520varied%2520shapes%252C%2520poses%252C%2520and%2520appearances%252C%2520learning%2520disentangled%2520spaces.%250AOur%2520method%252C%2520Dessie%252C%2520surpasses%2520existing%25203D%2520horse%2520reconstruction%2520methods%2520and%250Ageneralizes%2520to%2520other%2520large%2520animals%2520like%2520zebras%252C%2520cows%252C%2520and%2520deer.%2520See%2520the%2520project%250Awebsite%2520at%253A%2520%255Curl%257Bhttps%253A//celiali.github.io/Dessie/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dessie%3A%20Disentanglement%20for%20Articulated%203D%20Horse%20Shape%20and%20Pose%0A%20%20Estimation%20from%20Images&entry.906535625=Ci%20Li%20and%20Yi%20Yang%20and%20Zehang%20Weng%20and%20Elin%20Hernlund%20and%20Silvia%20Zuffi%20and%20Hedvig%20Kjellstr%C3%B6m&entry.1292438233=%20%20In%20recent%20years%2C%203D%20parametric%20animal%20models%20have%20been%20developed%20to%20aid%20in%0Aestimating%203D%20shape%20and%20pose%20from%20images%20and%20video.%20While%20progress%20has%20been%0Amade%20for%20humans%2C%20it%27s%20more%20challenging%20for%20animals%20due%20to%20limited%20annotated%0Adata.%20To%20address%20this%2C%20we%20introduce%20the%20first%20method%20using%20synthetic%20data%0Ageneration%20and%20disentanglement%20to%20learn%20to%20regress%203D%20shape%20and%20pose.%20Focusing%0Aon%20horses%2C%20we%20use%20text-based%20texture%20generation%20and%20a%20synthetic%20data%20pipeline%0Ato%20create%20varied%20shapes%2C%20poses%2C%20and%20appearances%2C%20learning%20disentangled%20spaces.%0AOur%20method%2C%20Dessie%2C%20surpasses%20existing%203D%20horse%20reconstruction%20methods%20and%0Ageneralizes%20to%20other%20large%20animals%20like%20zebras%2C%20cows%2C%20and%20deer.%20See%20the%20project%0Awebsite%20at%3A%20%5Curl%7Bhttps%3A//celiali.github.io/Dessie/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03438v1&entry.124074799=Read"},
{"title": "Look Twice Before You Answer: Memory-Space Visual Retracing for\n  Hallucination Mitigation in Multimodal Large Language Models", "author": "Xin Zou and Yizhou Wang and Yibo Yan and Sirui Huang and Kening Zheng and Junkai Chen and Chang Tang and Xuming Hu", "abstract": "  Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) are susceptible to hallucinations, especially assertively fabricating\ncontent not present in the visual inputs. To address the aforementioned\nchallenge, we follow a common cognitive process - when one's initial memory of\ncritical on-sight details fades, it is intuitive to look at them a second time\nto seek a factual and accurate answer. Therefore, we introduce Memory-space\nVisual Retracing (MemVR), a novel hallucination mitigation paradigm that\nwithout the need for external knowledge retrieval or additional fine-tuning. In\nparticular, we treat visual prompts as supplementary evidence to be reinjected\ninto MLLMs via Feed Forward Network (FFN) as key-value memory, when the model\nis uncertain or even amnesic about question-relevant visual memories.\nComprehensive experimental evaluations demonstrate that MemVR significantly\nmitigates hallucination issues across various MLLMs and excels in general\nbenchmarks without incurring added time overhead, thus emphasizing its\npotential for widespread applicability.\n", "link": "http://arxiv.org/abs/2410.03577v1", "date": "2024-10-04", "relevancy": 2.7106, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Twice%20Before%20You%20Answer%3A%20Memory-Space%20Visual%20Retracing%20for%0A%20%20Hallucination%20Mitigation%20in%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Look%20Twice%20Before%20You%20Answer%3A%20Memory-Space%20Visual%20Retracing%20for%0A%20%20Hallucination%20Mitigation%20in%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Xin%20Zou%20and%20Yizhou%20Wang%20and%20Yibo%20Yan%20and%20Sirui%20Huang%20and%20Kening%20Zheng%20and%20Junkai%20Chen%20and%20Chang%20Tang%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20Despite%20their%20impressive%20capabilities%2C%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20are%20susceptible%20to%20hallucinations%2C%20especially%20assertively%20fabricating%0Acontent%20not%20present%20in%20the%20visual%20inputs.%20To%20address%20the%20aforementioned%0Achallenge%2C%20we%20follow%20a%20common%20cognitive%20process%20-%20when%20one%27s%20initial%20memory%20of%0Acritical%20on-sight%20details%20fades%2C%20it%20is%20intuitive%20to%20look%20at%20them%20a%20second%20time%0Ato%20seek%20a%20factual%20and%20accurate%20answer.%20Therefore%2C%20we%20introduce%20Memory-space%0AVisual%20Retracing%20%28MemVR%29%2C%20a%20novel%20hallucination%20mitigation%20paradigm%20that%0Awithout%20the%20need%20for%20external%20knowledge%20retrieval%20or%20additional%20fine-tuning.%20In%0Aparticular%2C%20we%20treat%20visual%20prompts%20as%20supplementary%20evidence%20to%20be%20reinjected%0Ainto%20MLLMs%20via%20Feed%20Forward%20Network%20%28FFN%29%20as%20key-value%20memory%2C%20when%20the%20model%0Ais%20uncertain%20or%20even%20amnesic%20about%20question-relevant%20visual%20memories.%0AComprehensive%20experimental%20evaluations%20demonstrate%20that%20MemVR%20significantly%0Amitigates%20hallucination%20issues%20across%20various%20MLLMs%20and%20excels%20in%20general%0Abenchmarks%20without%20incurring%20added%20time%20overhead%2C%20thus%20emphasizing%20its%0Apotential%20for%20widespread%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Twice%2520Before%2520You%2520Answer%253A%2520Memory-Space%2520Visual%2520Retracing%2520for%250A%2520%2520Hallucination%2520Mitigation%2520in%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DXin%2520Zou%2520and%2520Yizhou%2520Wang%2520and%2520Yibo%2520Yan%2520and%2520Sirui%2520Huang%2520and%2520Kening%2520Zheng%2520and%2520Junkai%2520Chen%2520and%2520Chang%2520Tang%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520Despite%2520their%2520impressive%2520capabilities%252C%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520are%2520susceptible%2520to%2520hallucinations%252C%2520especially%2520assertively%2520fabricating%250Acontent%2520not%2520present%2520in%2520the%2520visual%2520inputs.%2520To%2520address%2520the%2520aforementioned%250Achallenge%252C%2520we%2520follow%2520a%2520common%2520cognitive%2520process%2520-%2520when%2520one%2527s%2520initial%2520memory%2520of%250Acritical%2520on-sight%2520details%2520fades%252C%2520it%2520is%2520intuitive%2520to%2520look%2520at%2520them%2520a%2520second%2520time%250Ato%2520seek%2520a%2520factual%2520and%2520accurate%2520answer.%2520Therefore%252C%2520we%2520introduce%2520Memory-space%250AVisual%2520Retracing%2520%2528MemVR%2529%252C%2520a%2520novel%2520hallucination%2520mitigation%2520paradigm%2520that%250Awithout%2520the%2520need%2520for%2520external%2520knowledge%2520retrieval%2520or%2520additional%2520fine-tuning.%2520In%250Aparticular%252C%2520we%2520treat%2520visual%2520prompts%2520as%2520supplementary%2520evidence%2520to%2520be%2520reinjected%250Ainto%2520MLLMs%2520via%2520Feed%2520Forward%2520Network%2520%2528FFN%2529%2520as%2520key-value%2520memory%252C%2520when%2520the%2520model%250Ais%2520uncertain%2520or%2520even%2520amnesic%2520about%2520question-relevant%2520visual%2520memories.%250AComprehensive%2520experimental%2520evaluations%2520demonstrate%2520that%2520MemVR%2520significantly%250Amitigates%2520hallucination%2520issues%2520across%2520various%2520MLLMs%2520and%2520excels%2520in%2520general%250Abenchmarks%2520without%2520incurring%2520added%2520time%2520overhead%252C%2520thus%2520emphasizing%2520its%250Apotential%2520for%2520widespread%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Twice%20Before%20You%20Answer%3A%20Memory-Space%20Visual%20Retracing%20for%0A%20%20Hallucination%20Mitigation%20in%20Multimodal%20Large%20Language%20Models&entry.906535625=Xin%20Zou%20and%20Yizhou%20Wang%20and%20Yibo%20Yan%20and%20Sirui%20Huang%20and%20Kening%20Zheng%20and%20Junkai%20Chen%20and%20Chang%20Tang%20and%20Xuming%20Hu&entry.1292438233=%20%20Despite%20their%20impressive%20capabilities%2C%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20are%20susceptible%20to%20hallucinations%2C%20especially%20assertively%20fabricating%0Acontent%20not%20present%20in%20the%20visual%20inputs.%20To%20address%20the%20aforementioned%0Achallenge%2C%20we%20follow%20a%20common%20cognitive%20process%20-%20when%20one%27s%20initial%20memory%20of%0Acritical%20on-sight%20details%20fades%2C%20it%20is%20intuitive%20to%20look%20at%20them%20a%20second%20time%0Ato%20seek%20a%20factual%20and%20accurate%20answer.%20Therefore%2C%20we%20introduce%20Memory-space%0AVisual%20Retracing%20%28MemVR%29%2C%20a%20novel%20hallucination%20mitigation%20paradigm%20that%0Awithout%20the%20need%20for%20external%20knowledge%20retrieval%20or%20additional%20fine-tuning.%20In%0Aparticular%2C%20we%20treat%20visual%20prompts%20as%20supplementary%20evidence%20to%20be%20reinjected%0Ainto%20MLLMs%20via%20Feed%20Forward%20Network%20%28FFN%29%20as%20key-value%20memory%2C%20when%20the%20model%0Ais%20uncertain%20or%20even%20amnesic%20about%20question-relevant%20visual%20memories.%0AComprehensive%20experimental%20evaluations%20demonstrate%20that%20MemVR%20significantly%0Amitigates%20hallucination%20issues%20across%20various%20MLLMs%20and%20excels%20in%20general%0Abenchmarks%20without%20incurring%20added%20time%20overhead%2C%20thus%20emphasizing%20its%0Apotential%20for%20widespread%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03577v1&entry.124074799=Read"},
{"title": "Self-Training for Sample-Efficient Active Learning for Text\n  Classification with Pre-Trained Language Models", "author": "Christopher Schr\u00f6der and Gerhard Heyer", "abstract": "  Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. In\nthis work, we investigate how self-training, a semi-supervised approach that\nuses a model to obtain pseudo-labels for unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Building on a\ncomprehensive reproduction of four previous self-training approaches, some of\nwhich are evaluated for the first time in the context of active learning or\nnatural language processing, we introduce HAST, a new and effective\nself-training strategy, which is evaluated on four text classification\nbenchmarks. Our results show that it outperforms the reproduced self-training\napproaches and reaches classification results comparable to previous\nexperiments for three out of four datasets, using as little as 25% of the data.\nThe code is publicly available at\nhttps://github.com/chschroeder/self-training-for-sample-efficient-active-learning .\n", "link": "http://arxiv.org/abs/2406.09206v2", "date": "2024-10-04", "relevancy": 2.7004, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5791}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Training%20for%20Sample-Efficient%20Active%20Learning%20for%20Text%0A%20%20Classification%20with%20Pre-Trained%20Language%20Models&body=Title%3A%20Self-Training%20for%20Sample-Efficient%20Active%20Learning%20for%20Text%0A%20%20Classification%20with%20Pre-Trained%20Language%20Models%0AAuthor%3A%20Christopher%20Schr%C3%B6der%20and%20Gerhard%20Heyer%0AAbstract%3A%20%20%20Active%20learning%20is%20an%20iterative%20labeling%20process%20that%20is%20used%20to%20obtain%20a%0Asmall%20labeled%20subset%2C%20despite%20the%20absence%20of%20labeled%20data%2C%20thereby%20enabling%20to%0Atrain%20a%20model%20for%20supervised%20tasks%20such%20as%20text%20classification.%20While%20active%0Alearning%20has%20made%20considerable%20progress%20in%20recent%20years%20due%20to%20improvements%0Aprovided%20by%20pre-trained%20language%20models%2C%20there%20is%20untapped%20potential%20in%20the%0Aoften%20neglected%20unlabeled%20portion%20of%20the%20data%2C%20although%20it%20is%20available%20in%0Aconsiderably%20larger%20quantities%20than%20the%20usually%20small%20set%20of%20labeled%20data.%20In%0Athis%20work%2C%20we%20investigate%20how%20self-training%2C%20a%20semi-supervised%20approach%20that%0Auses%20a%20model%20to%20obtain%20pseudo-labels%20for%20unlabeled%20data%2C%20can%20be%20used%20to%20improve%0Athe%20efficiency%20of%20active%20learning%20for%20text%20classification.%20Building%20on%20a%0Acomprehensive%20reproduction%20of%20four%20previous%20self-training%20approaches%2C%20some%20of%0Awhich%20are%20evaluated%20for%20the%20first%20time%20in%20the%20context%20of%20active%20learning%20or%0Anatural%20language%20processing%2C%20we%20introduce%20HAST%2C%20a%20new%20and%20effective%0Aself-training%20strategy%2C%20which%20is%20evaluated%20on%20four%20text%20classification%0Abenchmarks.%20Our%20results%20show%20that%20it%20outperforms%20the%20reproduced%20self-training%0Aapproaches%20and%20reaches%20classification%20results%20comparable%20to%20previous%0Aexperiments%20for%20three%20out%20of%20four%20datasets%2C%20using%20as%20little%20as%2025%25%20of%20the%20data.%0AThe%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/chschroeder/self-training-for-sample-efficient-active-learning%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Training%2520for%2520Sample-Efficient%2520Active%2520Learning%2520for%2520Text%250A%2520%2520Classification%2520with%2520Pre-Trained%2520Language%2520Models%26entry.906535625%3DChristopher%2520Schr%25C3%25B6der%2520and%2520Gerhard%2520Heyer%26entry.1292438233%3D%2520%2520Active%2520learning%2520is%2520an%2520iterative%2520labeling%2520process%2520that%2520is%2520used%2520to%2520obtain%2520a%250Asmall%2520labeled%2520subset%252C%2520despite%2520the%2520absence%2520of%2520labeled%2520data%252C%2520thereby%2520enabling%2520to%250Atrain%2520a%2520model%2520for%2520supervised%2520tasks%2520such%2520as%2520text%2520classification.%2520While%2520active%250Alearning%2520has%2520made%2520considerable%2520progress%2520in%2520recent%2520years%2520due%2520to%2520improvements%250Aprovided%2520by%2520pre-trained%2520language%2520models%252C%2520there%2520is%2520untapped%2520potential%2520in%2520the%250Aoften%2520neglected%2520unlabeled%2520portion%2520of%2520the%2520data%252C%2520although%2520it%2520is%2520available%2520in%250Aconsiderably%2520larger%2520quantities%2520than%2520the%2520usually%2520small%2520set%2520of%2520labeled%2520data.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520how%2520self-training%252C%2520a%2520semi-supervised%2520approach%2520that%250Auses%2520a%2520model%2520to%2520obtain%2520pseudo-labels%2520for%2520unlabeled%2520data%252C%2520can%2520be%2520used%2520to%2520improve%250Athe%2520efficiency%2520of%2520active%2520learning%2520for%2520text%2520classification.%2520Building%2520on%2520a%250Acomprehensive%2520reproduction%2520of%2520four%2520previous%2520self-training%2520approaches%252C%2520some%2520of%250Awhich%2520are%2520evaluated%2520for%2520the%2520first%2520time%2520in%2520the%2520context%2520of%2520active%2520learning%2520or%250Anatural%2520language%2520processing%252C%2520we%2520introduce%2520HAST%252C%2520a%2520new%2520and%2520effective%250Aself-training%2520strategy%252C%2520which%2520is%2520evaluated%2520on%2520four%2520text%2520classification%250Abenchmarks.%2520Our%2520results%2520show%2520that%2520it%2520outperforms%2520the%2520reproduced%2520self-training%250Aapproaches%2520and%2520reaches%2520classification%2520results%2520comparable%2520to%2520previous%250Aexperiments%2520for%2520three%2520out%2520of%2520four%2520datasets%252C%2520using%2520as%2520little%2520as%252025%2525%2520of%2520the%2520data.%250AThe%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/chschroeder/self-training-for-sample-efficient-active-learning%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Training%20for%20Sample-Efficient%20Active%20Learning%20for%20Text%0A%20%20Classification%20with%20Pre-Trained%20Language%20Models&entry.906535625=Christopher%20Schr%C3%B6der%20and%20Gerhard%20Heyer&entry.1292438233=%20%20Active%20learning%20is%20an%20iterative%20labeling%20process%20that%20is%20used%20to%20obtain%20a%0Asmall%20labeled%20subset%2C%20despite%20the%20absence%20of%20labeled%20data%2C%20thereby%20enabling%20to%0Atrain%20a%20model%20for%20supervised%20tasks%20such%20as%20text%20classification.%20While%20active%0Alearning%20has%20made%20considerable%20progress%20in%20recent%20years%20due%20to%20improvements%0Aprovided%20by%20pre-trained%20language%20models%2C%20there%20is%20untapped%20potential%20in%20the%0Aoften%20neglected%20unlabeled%20portion%20of%20the%20data%2C%20although%20it%20is%20available%20in%0Aconsiderably%20larger%20quantities%20than%20the%20usually%20small%20set%20of%20labeled%20data.%20In%0Athis%20work%2C%20we%20investigate%20how%20self-training%2C%20a%20semi-supervised%20approach%20that%0Auses%20a%20model%20to%20obtain%20pseudo-labels%20for%20unlabeled%20data%2C%20can%20be%20used%20to%20improve%0Athe%20efficiency%20of%20active%20learning%20for%20text%20classification.%20Building%20on%20a%0Acomprehensive%20reproduction%20of%20four%20previous%20self-training%20approaches%2C%20some%20of%0Awhich%20are%20evaluated%20for%20the%20first%20time%20in%20the%20context%20of%20active%20learning%20or%0Anatural%20language%20processing%2C%20we%20introduce%20HAST%2C%20a%20new%20and%20effective%0Aself-training%20strategy%2C%20which%20is%20evaluated%20on%20four%20text%20classification%0Abenchmarks.%20Our%20results%20show%20that%20it%20outperforms%20the%20reproduced%20self-training%0Aapproaches%20and%20reaches%20classification%20results%20comparable%20to%20previous%0Aexperiments%20for%20three%20out%20of%20four%20datasets%2C%20using%20as%20little%20as%2025%25%20of%20the%20data.%0AThe%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/chschroeder/self-training-for-sample-efficient-active-learning%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09206v2&entry.124074799=Read"},
{"title": "Semi-Supervised Manifold Learning with Complexity Decoupled Chart\n  Autoencoders", "author": "Stefan C. Schonsheck and Scott Mahan and Timo Klock and Alexander Cloninger and Rongjie Lai", "abstract": "  Autoencoding is a popular method in representation learning. Conventional\nautoencoders employ symmetric encoding-decoding procedures and a simple\nEuclidean latent space to detect hidden low-dimensional structures in an\nunsupervised way. Some modern approaches to novel data generation such as\ngenerative adversarial networks askew this symmetry, but still employ a pair of\nmassive networks--one to generate the image and another to judge the images\nquality based on priors learned from a training set. This work introduces a\nchart autoencoder with an asymmetric encoding-decoding process that can\nincorporate additional semi-supervised information such as class labels.\nBesides enhancing the capability for handling data with complicated topological\nand geometric structures, the proposed model can successfully differentiate\nnearby but disjoint manifolds and intersecting manifolds with only a small\namount of supervision. Moreover, this model only requires a low-complexity\nencoding operation, such as a locally defined linear projection. We discuss the\napproximation power of such networks and derive a bound that essentially\ndepends on the intrinsic dimension of the data manifold rather than the\ndimension of ambient space. Next we incorporate bounds for the sampling rate of\ntraining data need to faithfully represent a given data manifold. We present\nnumerical experiments that verify that the proposed model can effectively\nmanage data with multi-class nearby but disjoint manifolds of different\nclasses, overlapping manifolds, and manifolds with non-trivial topology.\nFinally, we conclude with some experiments on computer vision and molecular\ndynamics problems which showcase the efficacy of our methods on real-world\ndata.\n", "link": "http://arxiv.org/abs/2208.10570v2", "date": "2024-10-04", "relevancy": 2.6712, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5725}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Manifold%20Learning%20with%20Complexity%20Decoupled%20Chart%0A%20%20Autoencoders&body=Title%3A%20Semi-Supervised%20Manifold%20Learning%20with%20Complexity%20Decoupled%20Chart%0A%20%20Autoencoders%0AAuthor%3A%20Stefan%20C.%20Schonsheck%20and%20Scott%20Mahan%20and%20Timo%20Klock%20and%20Alexander%20Cloninger%20and%20Rongjie%20Lai%0AAbstract%3A%20%20%20Autoencoding%20is%20a%20popular%20method%20in%20representation%20learning.%20Conventional%0Aautoencoders%20employ%20symmetric%20encoding-decoding%20procedures%20and%20a%20simple%0AEuclidean%20latent%20space%20to%20detect%20hidden%20low-dimensional%20structures%20in%20an%0Aunsupervised%20way.%20Some%20modern%20approaches%20to%20novel%20data%20generation%20such%20as%0Agenerative%20adversarial%20networks%20askew%20this%20symmetry%2C%20but%20still%20employ%20a%20pair%20of%0Amassive%20networks--one%20to%20generate%20the%20image%20and%20another%20to%20judge%20the%20images%0Aquality%20based%20on%20priors%20learned%20from%20a%20training%20set.%20This%20work%20introduces%20a%0Achart%20autoencoder%20with%20an%20asymmetric%20encoding-decoding%20process%20that%20can%0Aincorporate%20additional%20semi-supervised%20information%20such%20as%20class%20labels.%0ABesides%20enhancing%20the%20capability%20for%20handling%20data%20with%20complicated%20topological%0Aand%20geometric%20structures%2C%20the%20proposed%20model%20can%20successfully%20differentiate%0Anearby%20but%20disjoint%20manifolds%20and%20intersecting%20manifolds%20with%20only%20a%20small%0Aamount%20of%20supervision.%20Moreover%2C%20this%20model%20only%20requires%20a%20low-complexity%0Aencoding%20operation%2C%20such%20as%20a%20locally%20defined%20linear%20projection.%20We%20discuss%20the%0Aapproximation%20power%20of%20such%20networks%20and%20derive%20a%20bound%20that%20essentially%0Adepends%20on%20the%20intrinsic%20dimension%20of%20the%20data%20manifold%20rather%20than%20the%0Adimension%20of%20ambient%20space.%20Next%20we%20incorporate%20bounds%20for%20the%20sampling%20rate%20of%0Atraining%20data%20need%20to%20faithfully%20represent%20a%20given%20data%20manifold.%20We%20present%0Anumerical%20experiments%20that%20verify%20that%20the%20proposed%20model%20can%20effectively%0Amanage%20data%20with%20multi-class%20nearby%20but%20disjoint%20manifolds%20of%20different%0Aclasses%2C%20overlapping%20manifolds%2C%20and%20manifolds%20with%20non-trivial%20topology.%0AFinally%2C%20we%20conclude%20with%20some%20experiments%20on%20computer%20vision%20and%20molecular%0Adynamics%20problems%20which%20showcase%20the%20efficacy%20of%20our%20methods%20on%20real-world%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.10570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Manifold%2520Learning%2520with%2520Complexity%2520Decoupled%2520Chart%250A%2520%2520Autoencoders%26entry.906535625%3DStefan%2520C.%2520Schonsheck%2520and%2520Scott%2520Mahan%2520and%2520Timo%2520Klock%2520and%2520Alexander%2520Cloninger%2520and%2520Rongjie%2520Lai%26entry.1292438233%3D%2520%2520Autoencoding%2520is%2520a%2520popular%2520method%2520in%2520representation%2520learning.%2520Conventional%250Aautoencoders%2520employ%2520symmetric%2520encoding-decoding%2520procedures%2520and%2520a%2520simple%250AEuclidean%2520latent%2520space%2520to%2520detect%2520hidden%2520low-dimensional%2520structures%2520in%2520an%250Aunsupervised%2520way.%2520Some%2520modern%2520approaches%2520to%2520novel%2520data%2520generation%2520such%2520as%250Agenerative%2520adversarial%2520networks%2520askew%2520this%2520symmetry%252C%2520but%2520still%2520employ%2520a%2520pair%2520of%250Amassive%2520networks--one%2520to%2520generate%2520the%2520image%2520and%2520another%2520to%2520judge%2520the%2520images%250Aquality%2520based%2520on%2520priors%2520learned%2520from%2520a%2520training%2520set.%2520This%2520work%2520introduces%2520a%250Achart%2520autoencoder%2520with%2520an%2520asymmetric%2520encoding-decoding%2520process%2520that%2520can%250Aincorporate%2520additional%2520semi-supervised%2520information%2520such%2520as%2520class%2520labels.%250ABesides%2520enhancing%2520the%2520capability%2520for%2520handling%2520data%2520with%2520complicated%2520topological%250Aand%2520geometric%2520structures%252C%2520the%2520proposed%2520model%2520can%2520successfully%2520differentiate%250Anearby%2520but%2520disjoint%2520manifolds%2520and%2520intersecting%2520manifolds%2520with%2520only%2520a%2520small%250Aamount%2520of%2520supervision.%2520Moreover%252C%2520this%2520model%2520only%2520requires%2520a%2520low-complexity%250Aencoding%2520operation%252C%2520such%2520as%2520a%2520locally%2520defined%2520linear%2520projection.%2520We%2520discuss%2520the%250Aapproximation%2520power%2520of%2520such%2520networks%2520and%2520derive%2520a%2520bound%2520that%2520essentially%250Adepends%2520on%2520the%2520intrinsic%2520dimension%2520of%2520the%2520data%2520manifold%2520rather%2520than%2520the%250Adimension%2520of%2520ambient%2520space.%2520Next%2520we%2520incorporate%2520bounds%2520for%2520the%2520sampling%2520rate%2520of%250Atraining%2520data%2520need%2520to%2520faithfully%2520represent%2520a%2520given%2520data%2520manifold.%2520We%2520present%250Anumerical%2520experiments%2520that%2520verify%2520that%2520the%2520proposed%2520model%2520can%2520effectively%250Amanage%2520data%2520with%2520multi-class%2520nearby%2520but%2520disjoint%2520manifolds%2520of%2520different%250Aclasses%252C%2520overlapping%2520manifolds%252C%2520and%2520manifolds%2520with%2520non-trivial%2520topology.%250AFinally%252C%2520we%2520conclude%2520with%2520some%2520experiments%2520on%2520computer%2520vision%2520and%2520molecular%250Adynamics%2520problems%2520which%2520showcase%2520the%2520efficacy%2520of%2520our%2520methods%2520on%2520real-world%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.10570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Manifold%20Learning%20with%20Complexity%20Decoupled%20Chart%0A%20%20Autoencoders&entry.906535625=Stefan%20C.%20Schonsheck%20and%20Scott%20Mahan%20and%20Timo%20Klock%20and%20Alexander%20Cloninger%20and%20Rongjie%20Lai&entry.1292438233=%20%20Autoencoding%20is%20a%20popular%20method%20in%20representation%20learning.%20Conventional%0Aautoencoders%20employ%20symmetric%20encoding-decoding%20procedures%20and%20a%20simple%0AEuclidean%20latent%20space%20to%20detect%20hidden%20low-dimensional%20structures%20in%20an%0Aunsupervised%20way.%20Some%20modern%20approaches%20to%20novel%20data%20generation%20such%20as%0Agenerative%20adversarial%20networks%20askew%20this%20symmetry%2C%20but%20still%20employ%20a%20pair%20of%0Amassive%20networks--one%20to%20generate%20the%20image%20and%20another%20to%20judge%20the%20images%0Aquality%20based%20on%20priors%20learned%20from%20a%20training%20set.%20This%20work%20introduces%20a%0Achart%20autoencoder%20with%20an%20asymmetric%20encoding-decoding%20process%20that%20can%0Aincorporate%20additional%20semi-supervised%20information%20such%20as%20class%20labels.%0ABesides%20enhancing%20the%20capability%20for%20handling%20data%20with%20complicated%20topological%0Aand%20geometric%20structures%2C%20the%20proposed%20model%20can%20successfully%20differentiate%0Anearby%20but%20disjoint%20manifolds%20and%20intersecting%20manifolds%20with%20only%20a%20small%0Aamount%20of%20supervision.%20Moreover%2C%20this%20model%20only%20requires%20a%20low-complexity%0Aencoding%20operation%2C%20such%20as%20a%20locally%20defined%20linear%20projection.%20We%20discuss%20the%0Aapproximation%20power%20of%20such%20networks%20and%20derive%20a%20bound%20that%20essentially%0Adepends%20on%20the%20intrinsic%20dimension%20of%20the%20data%20manifold%20rather%20than%20the%0Adimension%20of%20ambient%20space.%20Next%20we%20incorporate%20bounds%20for%20the%20sampling%20rate%20of%0Atraining%20data%20need%20to%20faithfully%20represent%20a%20given%20data%20manifold.%20We%20present%0Anumerical%20experiments%20that%20verify%20that%20the%20proposed%20model%20can%20effectively%0Amanage%20data%20with%20multi-class%20nearby%20but%20disjoint%20manifolds%20of%20different%0Aclasses%2C%20overlapping%20manifolds%2C%20and%20manifolds%20with%20non-trivial%20topology.%0AFinally%2C%20we%20conclude%20with%20some%20experiments%20on%20computer%20vision%20and%20molecular%0Adynamics%20problems%20which%20showcase%20the%20efficacy%20of%20our%20methods%20on%20real-world%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.10570v2&entry.124074799=Read"},
{"title": "Infrared Small Target Detection in Satellite Videos: A New Dataset and A\n  Novel Recurrent Feature Refinement Framework", "author": "Xinyi Ying and Li Liu and Zaipin Lin and Yangsi Shi and Yingqian Wang and Ruojing Li and Xu Cao and Boyang Li and Shilin Zhou", "abstract": "  Multi-frame infrared small target (MIRST) detection in satellite videos is a\nlong-standing, fundamental yet challenging task for decades, and the challenges\ncan be summarized as: First, extremely small target size, highly complex\nclutters & noises, various satellite motions result in limited feature\nrepresentation, high false alarms, and difficult motion analyses. Second, the\nlack of large-scale public available MIRST dataset in satellite videos greatly\nhinders the algorithm development. To address the aforementioned challenges, in\nthis paper, we first build a large-scale dataset for MIRST detection in\nsatellite videos (namely IRSatVideo-LEO), and then develop a recurrent feature\nrefinement (RFR) framework as the baseline method. Specifically, IRSatVideo-LEO\nis a semi-simulated dataset with synthesized satellite motion, target\nappearance, trajectory and intensity, which can provide a standard toolbox for\nsatellite video generation and a reliable evaluation platform to facilitate the\nalgorithm development. For baseline method, RFR is proposed to be equipped with\nexisting powerful CNN-based methods for long-term temporal dependency\nexploitation and integrated motion compensation & MIRST detection.\nSpecifically, a pyramid deformable alignment (PDA) module and a\ntemporal-spatial-frequency modulation (TSFM) module are proposed to achieve\neffective and efficient feature alignment, propagation, aggregation and\nrefinement. Extensive experiments have been conducted to demonstrate the\neffectiveness and superiority of our scheme. The comparative results show that\nResUNet equipped with RFR outperforms the state-of-the-art MIRST detection\nmethods. Dataset and code are released at https://github.com/XinyiYing/RFR.\n", "link": "http://arxiv.org/abs/2409.12448v2", "date": "2024-10-04", "relevancy": 2.6449, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5373}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5336}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infrared%20Small%20Target%20Detection%20in%20Satellite%20Videos%3A%20A%20New%20Dataset%20and%20A%0A%20%20Novel%20Recurrent%20Feature%20Refinement%20Framework&body=Title%3A%20Infrared%20Small%20Target%20Detection%20in%20Satellite%20Videos%3A%20A%20New%20Dataset%20and%20A%0A%20%20Novel%20Recurrent%20Feature%20Refinement%20Framework%0AAuthor%3A%20Xinyi%20Ying%20and%20Li%20Liu%20and%20Zaipin%20Lin%20and%20Yangsi%20Shi%20and%20Yingqian%20Wang%20and%20Ruojing%20Li%20and%20Xu%20Cao%20and%20Boyang%20Li%20and%20Shilin%20Zhou%0AAbstract%3A%20%20%20Multi-frame%20infrared%20small%20target%20%28MIRST%29%20detection%20in%20satellite%20videos%20is%20a%0Along-standing%2C%20fundamental%20yet%20challenging%20task%20for%20decades%2C%20and%20the%20challenges%0Acan%20be%20summarized%20as%3A%20First%2C%20extremely%20small%20target%20size%2C%20highly%20complex%0Aclutters%20%26%20noises%2C%20various%20satellite%20motions%20result%20in%20limited%20feature%0Arepresentation%2C%20high%20false%20alarms%2C%20and%20difficult%20motion%20analyses.%20Second%2C%20the%0Alack%20of%20large-scale%20public%20available%20MIRST%20dataset%20in%20satellite%20videos%20greatly%0Ahinders%20the%20algorithm%20development.%20To%20address%20the%20aforementioned%20challenges%2C%20in%0Athis%20paper%2C%20we%20first%20build%20a%20large-scale%20dataset%20for%20MIRST%20detection%20in%0Asatellite%20videos%20%28namely%20IRSatVideo-LEO%29%2C%20and%20then%20develop%20a%20recurrent%20feature%0Arefinement%20%28RFR%29%20framework%20as%20the%20baseline%20method.%20Specifically%2C%20IRSatVideo-LEO%0Ais%20a%20semi-simulated%20dataset%20with%20synthesized%20satellite%20motion%2C%20target%0Aappearance%2C%20trajectory%20and%20intensity%2C%20which%20can%20provide%20a%20standard%20toolbox%20for%0Asatellite%20video%20generation%20and%20a%20reliable%20evaluation%20platform%20to%20facilitate%20the%0Aalgorithm%20development.%20For%20baseline%20method%2C%20RFR%20is%20proposed%20to%20be%20equipped%20with%0Aexisting%20powerful%20CNN-based%20methods%20for%20long-term%20temporal%20dependency%0Aexploitation%20and%20integrated%20motion%20compensation%20%26%20MIRST%20detection.%0ASpecifically%2C%20a%20pyramid%20deformable%20alignment%20%28PDA%29%20module%20and%20a%0Atemporal-spatial-frequency%20modulation%20%28TSFM%29%20module%20are%20proposed%20to%20achieve%0Aeffective%20and%20efficient%20feature%20alignment%2C%20propagation%2C%20aggregation%20and%0Arefinement.%20Extensive%20experiments%20have%20been%20conducted%20to%20demonstrate%20the%0Aeffectiveness%20and%20superiority%20of%20our%20scheme.%20The%20comparative%20results%20show%20that%0AResUNet%20equipped%20with%20RFR%20outperforms%20the%20state-of-the-art%20MIRST%20detection%0Amethods.%20Dataset%20and%20code%20are%20released%20at%20https%3A//github.com/XinyiYing/RFR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12448v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfrared%2520Small%2520Target%2520Detection%2520in%2520Satellite%2520Videos%253A%2520A%2520New%2520Dataset%2520and%2520A%250A%2520%2520Novel%2520Recurrent%2520Feature%2520Refinement%2520Framework%26entry.906535625%3DXinyi%2520Ying%2520and%2520Li%2520Liu%2520and%2520Zaipin%2520Lin%2520and%2520Yangsi%2520Shi%2520and%2520Yingqian%2520Wang%2520and%2520Ruojing%2520Li%2520and%2520Xu%2520Cao%2520and%2520Boyang%2520Li%2520and%2520Shilin%2520Zhou%26entry.1292438233%3D%2520%2520Multi-frame%2520infrared%2520small%2520target%2520%2528MIRST%2529%2520detection%2520in%2520satellite%2520videos%2520is%2520a%250Along-standing%252C%2520fundamental%2520yet%2520challenging%2520task%2520for%2520decades%252C%2520and%2520the%2520challenges%250Acan%2520be%2520summarized%2520as%253A%2520First%252C%2520extremely%2520small%2520target%2520size%252C%2520highly%2520complex%250Aclutters%2520%2526%2520noises%252C%2520various%2520satellite%2520motions%2520result%2520in%2520limited%2520feature%250Arepresentation%252C%2520high%2520false%2520alarms%252C%2520and%2520difficult%2520motion%2520analyses.%2520Second%252C%2520the%250Alack%2520of%2520large-scale%2520public%2520available%2520MIRST%2520dataset%2520in%2520satellite%2520videos%2520greatly%250Ahinders%2520the%2520algorithm%2520development.%2520To%2520address%2520the%2520aforementioned%2520challenges%252C%2520in%250Athis%2520paper%252C%2520we%2520first%2520build%2520a%2520large-scale%2520dataset%2520for%2520MIRST%2520detection%2520in%250Asatellite%2520videos%2520%2528namely%2520IRSatVideo-LEO%2529%252C%2520and%2520then%2520develop%2520a%2520recurrent%2520feature%250Arefinement%2520%2528RFR%2529%2520framework%2520as%2520the%2520baseline%2520method.%2520Specifically%252C%2520IRSatVideo-LEO%250Ais%2520a%2520semi-simulated%2520dataset%2520with%2520synthesized%2520satellite%2520motion%252C%2520target%250Aappearance%252C%2520trajectory%2520and%2520intensity%252C%2520which%2520can%2520provide%2520a%2520standard%2520toolbox%2520for%250Asatellite%2520video%2520generation%2520and%2520a%2520reliable%2520evaluation%2520platform%2520to%2520facilitate%2520the%250Aalgorithm%2520development.%2520For%2520baseline%2520method%252C%2520RFR%2520is%2520proposed%2520to%2520be%2520equipped%2520with%250Aexisting%2520powerful%2520CNN-based%2520methods%2520for%2520long-term%2520temporal%2520dependency%250Aexploitation%2520and%2520integrated%2520motion%2520compensation%2520%2526%2520MIRST%2520detection.%250ASpecifically%252C%2520a%2520pyramid%2520deformable%2520alignment%2520%2528PDA%2529%2520module%2520and%2520a%250Atemporal-spatial-frequency%2520modulation%2520%2528TSFM%2529%2520module%2520are%2520proposed%2520to%2520achieve%250Aeffective%2520and%2520efficient%2520feature%2520alignment%252C%2520propagation%252C%2520aggregation%2520and%250Arefinement.%2520Extensive%2520experiments%2520have%2520been%2520conducted%2520to%2520demonstrate%2520the%250Aeffectiveness%2520and%2520superiority%2520of%2520our%2520scheme.%2520The%2520comparative%2520results%2520show%2520that%250AResUNet%2520equipped%2520with%2520RFR%2520outperforms%2520the%2520state-of-the-art%2520MIRST%2520detection%250Amethods.%2520Dataset%2520and%2520code%2520are%2520released%2520at%2520https%253A//github.com/XinyiYing/RFR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12448v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infrared%20Small%20Target%20Detection%20in%20Satellite%20Videos%3A%20A%20New%20Dataset%20and%20A%0A%20%20Novel%20Recurrent%20Feature%20Refinement%20Framework&entry.906535625=Xinyi%20Ying%20and%20Li%20Liu%20and%20Zaipin%20Lin%20and%20Yangsi%20Shi%20and%20Yingqian%20Wang%20and%20Ruojing%20Li%20and%20Xu%20Cao%20and%20Boyang%20Li%20and%20Shilin%20Zhou&entry.1292438233=%20%20Multi-frame%20infrared%20small%20target%20%28MIRST%29%20detection%20in%20satellite%20videos%20is%20a%0Along-standing%2C%20fundamental%20yet%20challenging%20task%20for%20decades%2C%20and%20the%20challenges%0Acan%20be%20summarized%20as%3A%20First%2C%20extremely%20small%20target%20size%2C%20highly%20complex%0Aclutters%20%26%20noises%2C%20various%20satellite%20motions%20result%20in%20limited%20feature%0Arepresentation%2C%20high%20false%20alarms%2C%20and%20difficult%20motion%20analyses.%20Second%2C%20the%0Alack%20of%20large-scale%20public%20available%20MIRST%20dataset%20in%20satellite%20videos%20greatly%0Ahinders%20the%20algorithm%20development.%20To%20address%20the%20aforementioned%20challenges%2C%20in%0Athis%20paper%2C%20we%20first%20build%20a%20large-scale%20dataset%20for%20MIRST%20detection%20in%0Asatellite%20videos%20%28namely%20IRSatVideo-LEO%29%2C%20and%20then%20develop%20a%20recurrent%20feature%0Arefinement%20%28RFR%29%20framework%20as%20the%20baseline%20method.%20Specifically%2C%20IRSatVideo-LEO%0Ais%20a%20semi-simulated%20dataset%20with%20synthesized%20satellite%20motion%2C%20target%0Aappearance%2C%20trajectory%20and%20intensity%2C%20which%20can%20provide%20a%20standard%20toolbox%20for%0Asatellite%20video%20generation%20and%20a%20reliable%20evaluation%20platform%20to%20facilitate%20the%0Aalgorithm%20development.%20For%20baseline%20method%2C%20RFR%20is%20proposed%20to%20be%20equipped%20with%0Aexisting%20powerful%20CNN-based%20methods%20for%20long-term%20temporal%20dependency%0Aexploitation%20and%20integrated%20motion%20compensation%20%26%20MIRST%20detection.%0ASpecifically%2C%20a%20pyramid%20deformable%20alignment%20%28PDA%29%20module%20and%20a%0Atemporal-spatial-frequency%20modulation%20%28TSFM%29%20module%20are%20proposed%20to%20achieve%0Aeffective%20and%20efficient%20feature%20alignment%2C%20propagation%2C%20aggregation%20and%0Arefinement.%20Extensive%20experiments%20have%20been%20conducted%20to%20demonstrate%20the%0Aeffectiveness%20and%20superiority%20of%20our%20scheme.%20The%20comparative%20results%20show%20that%0AResUNet%20equipped%20with%20RFR%20outperforms%20the%20state-of-the-art%20MIRST%20detection%0Amethods.%20Dataset%20and%20code%20are%20released%20at%20https%3A//github.com/XinyiYing/RFR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12448v2&entry.124074799=Read"},
{"title": "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into\n  Consistency and Robustness", "author": "Srija Mukhopadhyay and Adnan Qidwai and Aparna Garimella and Pritika Ramu and Vivek Gupta and Dan Roth", "abstract": "  Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.\n", "link": "http://arxiv.org/abs/2407.11229v2", "date": "2024-10-04", "relevancy": 2.615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20the%20Truth%3A%20Do%20VLMs%20really%20Understand%20Charts%3F%20A%20Deep%20Dive%20into%0A%20%20Consistency%20and%20Robustness&body=Title%3A%20Unraveling%20the%20Truth%3A%20Do%20VLMs%20really%20Understand%20Charts%3F%20A%20Deep%20Dive%20into%0A%20%20Consistency%20and%20Robustness%0AAuthor%3A%20Srija%20Mukhopadhyay%20and%20Adnan%20Qidwai%20and%20Aparna%20Garimella%20and%20Pritika%20Ramu%20and%20Vivek%20Gupta%20and%20Dan%20Roth%0AAbstract%3A%20%20%20Chart%20question%20answering%20%28CQA%29%20is%20a%20crucial%20area%20of%20Visual%20Language%0AUnderstanding.%20However%2C%20the%20robustness%20and%20consistency%20of%20current%20Visual%0ALanguage%20Models%20%28VLMs%29%20in%20this%20field%20remain%20under-explored.%20This%20paper%0Aevaluates%20state-of-the-art%20VLMs%20on%20comprehensive%20datasets%2C%20developed%0Aspecifically%20for%20this%20study%2C%20encompassing%20diverse%20question%20categories%20and%20chart%0Aformats.%20We%20investigate%20two%20key%20aspects%3A%201%29%20the%20models%27%20ability%20to%20handle%0Avarying%20levels%20of%20chart%20and%20question%20complexity%2C%20and%202%29%20their%20robustness%20across%0Adifferent%20visual%20representations%20of%20the%20same%20underlying%20data.%20Our%20analysis%0Areveals%20significant%20performance%20variations%20based%20on%20question%20and%20chart%20types%2C%0Ahighlighting%20both%20strengths%20and%20weaknesses%20of%20current%20models.%20Additionally%2C%20we%0Aidentify%20areas%20for%20improvement%20and%20propose%20future%20research%20directions%20to%20build%0Amore%20robust%20and%20reliable%20CQA%20systems.%20This%20study%20sheds%20light%20on%20the%20limitations%0Aof%20current%20models%20and%20paves%20the%20way%20for%20future%20advancements%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520the%2520Truth%253A%2520Do%2520VLMs%2520really%2520Understand%2520Charts%253F%2520A%2520Deep%2520Dive%2520into%250A%2520%2520Consistency%2520and%2520Robustness%26entry.906535625%3DSrija%2520Mukhopadhyay%2520and%2520Adnan%2520Qidwai%2520and%2520Aparna%2520Garimella%2520and%2520Pritika%2520Ramu%2520and%2520Vivek%2520Gupta%2520and%2520Dan%2520Roth%26entry.1292438233%3D%2520%2520Chart%2520question%2520answering%2520%2528CQA%2529%2520is%2520a%2520crucial%2520area%2520of%2520Visual%2520Language%250AUnderstanding.%2520However%252C%2520the%2520robustness%2520and%2520consistency%2520of%2520current%2520Visual%250ALanguage%2520Models%2520%2528VLMs%2529%2520in%2520this%2520field%2520remain%2520under-explored.%2520This%2520paper%250Aevaluates%2520state-of-the-art%2520VLMs%2520on%2520comprehensive%2520datasets%252C%2520developed%250Aspecifically%2520for%2520this%2520study%252C%2520encompassing%2520diverse%2520question%2520categories%2520and%2520chart%250Aformats.%2520We%2520investigate%2520two%2520key%2520aspects%253A%25201%2529%2520the%2520models%2527%2520ability%2520to%2520handle%250Avarying%2520levels%2520of%2520chart%2520and%2520question%2520complexity%252C%2520and%25202%2529%2520their%2520robustness%2520across%250Adifferent%2520visual%2520representations%2520of%2520the%2520same%2520underlying%2520data.%2520Our%2520analysis%250Areveals%2520significant%2520performance%2520variations%2520based%2520on%2520question%2520and%2520chart%2520types%252C%250Ahighlighting%2520both%2520strengths%2520and%2520weaknesses%2520of%2520current%2520models.%2520Additionally%252C%2520we%250Aidentify%2520areas%2520for%2520improvement%2520and%2520propose%2520future%2520research%2520directions%2520to%2520build%250Amore%2520robust%2520and%2520reliable%2520CQA%2520systems.%2520This%2520study%2520sheds%2520light%2520on%2520the%2520limitations%250Aof%2520current%2520models%2520and%2520paves%2520the%2520way%2520for%2520future%2520advancements%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20the%20Truth%3A%20Do%20VLMs%20really%20Understand%20Charts%3F%20A%20Deep%20Dive%20into%0A%20%20Consistency%20and%20Robustness&entry.906535625=Srija%20Mukhopadhyay%20and%20Adnan%20Qidwai%20and%20Aparna%20Garimella%20and%20Pritika%20Ramu%20and%20Vivek%20Gupta%20and%20Dan%20Roth&entry.1292438233=%20%20Chart%20question%20answering%20%28CQA%29%20is%20a%20crucial%20area%20of%20Visual%20Language%0AUnderstanding.%20However%2C%20the%20robustness%20and%20consistency%20of%20current%20Visual%0ALanguage%20Models%20%28VLMs%29%20in%20this%20field%20remain%20under-explored.%20This%20paper%0Aevaluates%20state-of-the-art%20VLMs%20on%20comprehensive%20datasets%2C%20developed%0Aspecifically%20for%20this%20study%2C%20encompassing%20diverse%20question%20categories%20and%20chart%0Aformats.%20We%20investigate%20two%20key%20aspects%3A%201%29%20the%20models%27%20ability%20to%20handle%0Avarying%20levels%20of%20chart%20and%20question%20complexity%2C%20and%202%29%20their%20robustness%20across%0Adifferent%20visual%20representations%20of%20the%20same%20underlying%20data.%20Our%20analysis%0Areveals%20significant%20performance%20variations%20based%20on%20question%20and%20chart%20types%2C%0Ahighlighting%20both%20strengths%20and%20weaknesses%20of%20current%20models.%20Additionally%2C%20we%0Aidentify%20areas%20for%20improvement%20and%20propose%20future%20research%20directions%20to%20build%0Amore%20robust%20and%20reliable%20CQA%20systems.%20This%20study%20sheds%20light%20on%20the%20limitations%0Aof%20current%20models%20and%20paves%20the%20way%20for%20future%20advancements%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11229v2&entry.124074799=Read"},
{"title": "Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for\n  Text-to-Any-Task", "author": "Jing Wang and Ao Ma and Jiasong Feng and Dawei Leng and Yuhui Yin and Xiaodan Liang", "abstract": "  The global self-attention mechanism in diffusion transformers involves\nredundant computation due to the sparse and redundant nature of visual\ninformation, and the attention map of tokens within a spatial window shows\nsignificant similarity. To address this redundancy, we propose the\nProxy-Tokenized Diffusion Transformer (PT-DiT), which employs sparse\nrepresentative token attention (where the number of representative tokens is\nmuch smaller than the total number of tokens) to model global visual\ninformation efficiently. Specifically, within each transformer block, we\ncompute an averaging token from each spatial-temporal window to serve as a\nproxy token for that region. The global semantics are captured through the\nself-attention of these proxy tokens and then injected into all latent tokens\nvia cross-attention. Simultaneously, we introduce window and shift window\nattention to address the limitations in detail modeling caused by the sparse\nattention mechanism. Building on the well-designed PT-DiT, we further develop\nthe Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV\ntasks. Experimental results show that PT-DiT achieves competitive performance\nwhile reducing the computational complexity in both image and video generation\ntasks (e.g., a 49% reduction compared to DiT and a 34% reduction compared to\nPixArt-$\\alpha$). The visual exhibition and source code of Qihoo-T2X is\navailable at https://360cvgroup.github.io/Qihoo-T2X/.\n", "link": "http://arxiv.org/abs/2409.04005v2", "date": "2024-10-04", "relevancy": 2.5824, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6815}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6384}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qihoo-T2X%3A%20An%20Efficient%20Proxy-Tokenized%20Diffusion%20Transformer%20for%0A%20%20Text-to-Any-Task&body=Title%3A%20Qihoo-T2X%3A%20An%20Efficient%20Proxy-Tokenized%20Diffusion%20Transformer%20for%0A%20%20Text-to-Any-Task%0AAuthor%3A%20Jing%20Wang%20and%20Ao%20Ma%20and%20Jiasong%20Feng%20and%20Dawei%20Leng%20and%20Yuhui%20Yin%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20The%20global%20self-attention%20mechanism%20in%20diffusion%20transformers%20involves%0Aredundant%20computation%20due%20to%20the%20sparse%20and%20redundant%20nature%20of%20visual%0Ainformation%2C%20and%20the%20attention%20map%20of%20tokens%20within%20a%20spatial%20window%20shows%0Asignificant%20similarity.%20To%20address%20this%20redundancy%2C%20we%20propose%20the%0AProxy-Tokenized%20Diffusion%20Transformer%20%28PT-DiT%29%2C%20which%20employs%20sparse%0Arepresentative%20token%20attention%20%28where%20the%20number%20of%20representative%20tokens%20is%0Amuch%20smaller%20than%20the%20total%20number%20of%20tokens%29%20to%20model%20global%20visual%0Ainformation%20efficiently.%20Specifically%2C%20within%20each%20transformer%20block%2C%20we%0Acompute%20an%20averaging%20token%20from%20each%20spatial-temporal%20window%20to%20serve%20as%20a%0Aproxy%20token%20for%20that%20region.%20The%20global%20semantics%20are%20captured%20through%20the%0Aself-attention%20of%20these%20proxy%20tokens%20and%20then%20injected%20into%20all%20latent%20tokens%0Avia%20cross-attention.%20Simultaneously%2C%20we%20introduce%20window%20and%20shift%20window%0Aattention%20to%20address%20the%20limitations%20in%20detail%20modeling%20caused%20by%20the%20sparse%0Aattention%20mechanism.%20Building%20on%20the%20well-designed%20PT-DiT%2C%20we%20further%20develop%0Athe%20Qihoo-T2X%20family%2C%20which%20includes%20a%20variety%20of%20models%20for%20T2I%2C%20T2V%2C%20and%20T2MV%0Atasks.%20Experimental%20results%20show%20that%20PT-DiT%20achieves%20competitive%20performance%0Awhile%20reducing%20the%20computational%20complexity%20in%20both%20image%20and%20video%20generation%0Atasks%20%28e.g.%2C%20a%2049%25%20reduction%20compared%20to%20DiT%20and%20a%2034%25%20reduction%20compared%20to%0APixArt-%24%5Calpha%24%29.%20The%20visual%20exhibition%20and%20source%20code%20of%20Qihoo-T2X%20is%0Aavailable%20at%20https%3A//360cvgroup.github.io/Qihoo-T2X/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQihoo-T2X%253A%2520An%2520Efficient%2520Proxy-Tokenized%2520Diffusion%2520Transformer%2520for%250A%2520%2520Text-to-Any-Task%26entry.906535625%3DJing%2520Wang%2520and%2520Ao%2520Ma%2520and%2520Jiasong%2520Feng%2520and%2520Dawei%2520Leng%2520and%2520Yuhui%2520Yin%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520The%2520global%2520self-attention%2520mechanism%2520in%2520diffusion%2520transformers%2520involves%250Aredundant%2520computation%2520due%2520to%2520the%2520sparse%2520and%2520redundant%2520nature%2520of%2520visual%250Ainformation%252C%2520and%2520the%2520attention%2520map%2520of%2520tokens%2520within%2520a%2520spatial%2520window%2520shows%250Asignificant%2520similarity.%2520To%2520address%2520this%2520redundancy%252C%2520we%2520propose%2520the%250AProxy-Tokenized%2520Diffusion%2520Transformer%2520%2528PT-DiT%2529%252C%2520which%2520employs%2520sparse%250Arepresentative%2520token%2520attention%2520%2528where%2520the%2520number%2520of%2520representative%2520tokens%2520is%250Amuch%2520smaller%2520than%2520the%2520total%2520number%2520of%2520tokens%2529%2520to%2520model%2520global%2520visual%250Ainformation%2520efficiently.%2520Specifically%252C%2520within%2520each%2520transformer%2520block%252C%2520we%250Acompute%2520an%2520averaging%2520token%2520from%2520each%2520spatial-temporal%2520window%2520to%2520serve%2520as%2520a%250Aproxy%2520token%2520for%2520that%2520region.%2520The%2520global%2520semantics%2520are%2520captured%2520through%2520the%250Aself-attention%2520of%2520these%2520proxy%2520tokens%2520and%2520then%2520injected%2520into%2520all%2520latent%2520tokens%250Avia%2520cross-attention.%2520Simultaneously%252C%2520we%2520introduce%2520window%2520and%2520shift%2520window%250Aattention%2520to%2520address%2520the%2520limitations%2520in%2520detail%2520modeling%2520caused%2520by%2520the%2520sparse%250Aattention%2520mechanism.%2520Building%2520on%2520the%2520well-designed%2520PT-DiT%252C%2520we%2520further%2520develop%250Athe%2520Qihoo-T2X%2520family%252C%2520which%2520includes%2520a%2520variety%2520of%2520models%2520for%2520T2I%252C%2520T2V%252C%2520and%2520T2MV%250Atasks.%2520Experimental%2520results%2520show%2520that%2520PT-DiT%2520achieves%2520competitive%2520performance%250Awhile%2520reducing%2520the%2520computational%2520complexity%2520in%2520both%2520image%2520and%2520video%2520generation%250Atasks%2520%2528e.g.%252C%2520a%252049%2525%2520reduction%2520compared%2520to%2520DiT%2520and%2520a%252034%2525%2520reduction%2520compared%2520to%250APixArt-%2524%255Calpha%2524%2529.%2520The%2520visual%2520exhibition%2520and%2520source%2520code%2520of%2520Qihoo-T2X%2520is%250Aavailable%2520at%2520https%253A//360cvgroup.github.io/Qihoo-T2X/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qihoo-T2X%3A%20An%20Efficient%20Proxy-Tokenized%20Diffusion%20Transformer%20for%0A%20%20Text-to-Any-Task&entry.906535625=Jing%20Wang%20and%20Ao%20Ma%20and%20Jiasong%20Feng%20and%20Dawei%20Leng%20and%20Yuhui%20Yin%20and%20Xiaodan%20Liang&entry.1292438233=%20%20The%20global%20self-attention%20mechanism%20in%20diffusion%20transformers%20involves%0Aredundant%20computation%20due%20to%20the%20sparse%20and%20redundant%20nature%20of%20visual%0Ainformation%2C%20and%20the%20attention%20map%20of%20tokens%20within%20a%20spatial%20window%20shows%0Asignificant%20similarity.%20To%20address%20this%20redundancy%2C%20we%20propose%20the%0AProxy-Tokenized%20Diffusion%20Transformer%20%28PT-DiT%29%2C%20which%20employs%20sparse%0Arepresentative%20token%20attention%20%28where%20the%20number%20of%20representative%20tokens%20is%0Amuch%20smaller%20than%20the%20total%20number%20of%20tokens%29%20to%20model%20global%20visual%0Ainformation%20efficiently.%20Specifically%2C%20within%20each%20transformer%20block%2C%20we%0Acompute%20an%20averaging%20token%20from%20each%20spatial-temporal%20window%20to%20serve%20as%20a%0Aproxy%20token%20for%20that%20region.%20The%20global%20semantics%20are%20captured%20through%20the%0Aself-attention%20of%20these%20proxy%20tokens%20and%20then%20injected%20into%20all%20latent%20tokens%0Avia%20cross-attention.%20Simultaneously%2C%20we%20introduce%20window%20and%20shift%20window%0Aattention%20to%20address%20the%20limitations%20in%20detail%20modeling%20caused%20by%20the%20sparse%0Aattention%20mechanism.%20Building%20on%20the%20well-designed%20PT-DiT%2C%20we%20further%20develop%0Athe%20Qihoo-T2X%20family%2C%20which%20includes%20a%20variety%20of%20models%20for%20T2I%2C%20T2V%2C%20and%20T2MV%0Atasks.%20Experimental%20results%20show%20that%20PT-DiT%20achieves%20competitive%20performance%0Awhile%20reducing%20the%20computational%20complexity%20in%20both%20image%20and%20video%20generation%0Atasks%20%28e.g.%2C%20a%2049%25%20reduction%20compared%20to%20DiT%20and%20a%2034%25%20reduction%20compared%20to%0APixArt-%24%5Calpha%24%29.%20The%20visual%20exhibition%20and%20source%20code%20of%20Qihoo-T2X%20is%0Aavailable%20at%20https%3A//360cvgroup.github.io/Qihoo-T2X/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04005v2&entry.124074799=Read"},
{"title": "MultiContrievers: Analysis of Dense Retrieval Representations", "author": "Seraphina Goldfarb-Tarrant and Pedro Rodriguez and Jane Dwivedi-Yu and Patrick Lewis", "abstract": "  Dense retrievers compress source documents into (possibly lossy) vector\nrepresentations, yet there is little analysis of what information is lost\nversus preserved, and how it affects downstream tasks. We conduct the first\nanalysis of the information captured by dense retrievers compared to the\nlanguage models they are based on (e.g., BERT versus Contriever). We use 25\nMultiBert checkpoints as randomized initialisations to train MultiContrievers,\na set of 25 contriever models. We test whether specific pieces of information\n-- such as gender and occupation -- can be extracted from contriever vectors of\nwikipedia-like documents. We measure this extractability via information\ntheoretic probing. We then examine the relationship of extractability to\nperformance and gender bias, as well as the sensitivity of these results to\nmany random initialisations and data shuffles. We find that (1) contriever\nmodels have significantly increased extractability, but extractability usually\ncorrelates poorly with benchmark performance 2) gender bias is present, but is\nnot caused by the contriever representations 3) there is high sensitivity to\nboth random initialisation and to data shuffle, suggesting that future\nretrieval research should test across a wider spread of both.\n", "link": "http://arxiv.org/abs/2402.15925v2", "date": "2024-10-04", "relevancy": 2.543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiContrievers%3A%20Analysis%20of%20Dense%20Retrieval%20Representations&body=Title%3A%20MultiContrievers%3A%20Analysis%20of%20Dense%20Retrieval%20Representations%0AAuthor%3A%20Seraphina%20Goldfarb-Tarrant%20and%20Pedro%20Rodriguez%20and%20Jane%20Dwivedi-Yu%20and%20Patrick%20Lewis%0AAbstract%3A%20%20%20Dense%20retrievers%20compress%20source%20documents%20into%20%28possibly%20lossy%29%20vector%0Arepresentations%2C%20yet%20there%20is%20little%20analysis%20of%20what%20information%20is%20lost%0Aversus%20preserved%2C%20and%20how%20it%20affects%20downstream%20tasks.%20We%20conduct%20the%20first%0Aanalysis%20of%20the%20information%20captured%20by%20dense%20retrievers%20compared%20to%20the%0Alanguage%20models%20they%20are%20based%20on%20%28e.g.%2C%20BERT%20versus%20Contriever%29.%20We%20use%2025%0AMultiBert%20checkpoints%20as%20randomized%20initialisations%20to%20train%20MultiContrievers%2C%0Aa%20set%20of%2025%20contriever%20models.%20We%20test%20whether%20specific%20pieces%20of%20information%0A--%20such%20as%20gender%20and%20occupation%20--%20can%20be%20extracted%20from%20contriever%20vectors%20of%0Awikipedia-like%20documents.%20We%20measure%20this%20extractability%20via%20information%0Atheoretic%20probing.%20We%20then%20examine%20the%20relationship%20of%20extractability%20to%0Aperformance%20and%20gender%20bias%2C%20as%20well%20as%20the%20sensitivity%20of%20these%20results%20to%0Amany%20random%20initialisations%20and%20data%20shuffles.%20We%20find%20that%20%281%29%20contriever%0Amodels%20have%20significantly%20increased%20extractability%2C%20but%20extractability%20usually%0Acorrelates%20poorly%20with%20benchmark%20performance%202%29%20gender%20bias%20is%20present%2C%20but%20is%0Anot%20caused%20by%20the%20contriever%20representations%203%29%20there%20is%20high%20sensitivity%20to%0Aboth%20random%20initialisation%20and%20to%20data%20shuffle%2C%20suggesting%20that%20future%0Aretrieval%20research%20should%20test%20across%20a%20wider%20spread%20of%20both.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiContrievers%253A%2520Analysis%2520of%2520Dense%2520Retrieval%2520Representations%26entry.906535625%3DSeraphina%2520Goldfarb-Tarrant%2520and%2520Pedro%2520Rodriguez%2520and%2520Jane%2520Dwivedi-Yu%2520and%2520Patrick%2520Lewis%26entry.1292438233%3D%2520%2520Dense%2520retrievers%2520compress%2520source%2520documents%2520into%2520%2528possibly%2520lossy%2529%2520vector%250Arepresentations%252C%2520yet%2520there%2520is%2520little%2520analysis%2520of%2520what%2520information%2520is%2520lost%250Aversus%2520preserved%252C%2520and%2520how%2520it%2520affects%2520downstream%2520tasks.%2520We%2520conduct%2520the%2520first%250Aanalysis%2520of%2520the%2520information%2520captured%2520by%2520dense%2520retrievers%2520compared%2520to%2520the%250Alanguage%2520models%2520they%2520are%2520based%2520on%2520%2528e.g.%252C%2520BERT%2520versus%2520Contriever%2529.%2520We%2520use%252025%250AMultiBert%2520checkpoints%2520as%2520randomized%2520initialisations%2520to%2520train%2520MultiContrievers%252C%250Aa%2520set%2520of%252025%2520contriever%2520models.%2520We%2520test%2520whether%2520specific%2520pieces%2520of%2520information%250A--%2520such%2520as%2520gender%2520and%2520occupation%2520--%2520can%2520be%2520extracted%2520from%2520contriever%2520vectors%2520of%250Awikipedia-like%2520documents.%2520We%2520measure%2520this%2520extractability%2520via%2520information%250Atheoretic%2520probing.%2520We%2520then%2520examine%2520the%2520relationship%2520of%2520extractability%2520to%250Aperformance%2520and%2520gender%2520bias%252C%2520as%2520well%2520as%2520the%2520sensitivity%2520of%2520these%2520results%2520to%250Amany%2520random%2520initialisations%2520and%2520data%2520shuffles.%2520We%2520find%2520that%2520%25281%2529%2520contriever%250Amodels%2520have%2520significantly%2520increased%2520extractability%252C%2520but%2520extractability%2520usually%250Acorrelates%2520poorly%2520with%2520benchmark%2520performance%25202%2529%2520gender%2520bias%2520is%2520present%252C%2520but%2520is%250Anot%2520caused%2520by%2520the%2520contriever%2520representations%25203%2529%2520there%2520is%2520high%2520sensitivity%2520to%250Aboth%2520random%2520initialisation%2520and%2520to%2520data%2520shuffle%252C%2520suggesting%2520that%2520future%250Aretrieval%2520research%2520should%2520test%2520across%2520a%2520wider%2520spread%2520of%2520both.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiContrievers%3A%20Analysis%20of%20Dense%20Retrieval%20Representations&entry.906535625=Seraphina%20Goldfarb-Tarrant%20and%20Pedro%20Rodriguez%20and%20Jane%20Dwivedi-Yu%20and%20Patrick%20Lewis&entry.1292438233=%20%20Dense%20retrievers%20compress%20source%20documents%20into%20%28possibly%20lossy%29%20vector%0Arepresentations%2C%20yet%20there%20is%20little%20analysis%20of%20what%20information%20is%20lost%0Aversus%20preserved%2C%20and%20how%20it%20affects%20downstream%20tasks.%20We%20conduct%20the%20first%0Aanalysis%20of%20the%20information%20captured%20by%20dense%20retrievers%20compared%20to%20the%0Alanguage%20models%20they%20are%20based%20on%20%28e.g.%2C%20BERT%20versus%20Contriever%29.%20We%20use%2025%0AMultiBert%20checkpoints%20as%20randomized%20initialisations%20to%20train%20MultiContrievers%2C%0Aa%20set%20of%2025%20contriever%20models.%20We%20test%20whether%20specific%20pieces%20of%20information%0A--%20such%20as%20gender%20and%20occupation%20--%20can%20be%20extracted%20from%20contriever%20vectors%20of%0Awikipedia-like%20documents.%20We%20measure%20this%20extractability%20via%20information%0Atheoretic%20probing.%20We%20then%20examine%20the%20relationship%20of%20extractability%20to%0Aperformance%20and%20gender%20bias%2C%20as%20well%20as%20the%20sensitivity%20of%20these%20results%20to%0Amany%20random%20initialisations%20and%20data%20shuffles.%20We%20find%20that%20%281%29%20contriever%0Amodels%20have%20significantly%20increased%20extractability%2C%20but%20extractability%20usually%0Acorrelates%20poorly%20with%20benchmark%20performance%202%29%20gender%20bias%20is%20present%2C%20but%20is%0Anot%20caused%20by%20the%20contriever%20representations%203%29%20there%20is%20high%20sensitivity%20to%0Aboth%20random%20initialisation%20and%20to%20data%20shuffle%2C%20suggesting%20that%20future%0Aretrieval%20research%20should%20test%20across%20a%20wider%20spread%20of%20both.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15925v2&entry.124074799=Read"},
{"title": "Training Over a Distribution of Hyperparameters for Enhanced Performance\n  and Adaptability on Imbalanced Classification", "author": "Kelsey Lieberman and Swarna Kamlam Ravindran and Shuai Yuan and Carlo Tomasi", "abstract": "  Although binary classification is a well-studied problem, training reliable\nclassifiers under severe class imbalance remains a challenge. Recent techniques\nmitigate the ill effects of imbalance on training by modifying the loss\nfunctions or optimization methods. We observe that different hyperparameter\nvalues on these loss functions perform better at different recall values. We\npropose to exploit this fact by training one model over a distribution of\nhyperparameter values--instead of a single value--via Loss Conditional Training\n(LCT). Experiments show that training over a distribution of hyperparameters\nnot only approximates the performance of several models but actually improves\nthe overall performance of models on both CIFAR and real medical imaging\napplications, such as melanoma and diabetic retinopathy detection. Furthermore,\ntraining models with LCT is more efficient because some hyperparameter tuning\ncan be conducted after training to meet individual needs without needing to\nretrain from scratch.\n", "link": "http://arxiv.org/abs/2410.03588v1", "date": "2024-10-04", "relevancy": 2.5424, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5224}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Over%20a%20Distribution%20of%20Hyperparameters%20for%20Enhanced%20Performance%0A%20%20and%20Adaptability%20on%20Imbalanced%20Classification&body=Title%3A%20Training%20Over%20a%20Distribution%20of%20Hyperparameters%20for%20Enhanced%20Performance%0A%20%20and%20Adaptability%20on%20Imbalanced%20Classification%0AAuthor%3A%20Kelsey%20Lieberman%20and%20Swarna%20Kamlam%20Ravindran%20and%20Shuai%20Yuan%20and%20Carlo%20Tomasi%0AAbstract%3A%20%20%20Although%20binary%20classification%20is%20a%20well-studied%20problem%2C%20training%20reliable%0Aclassifiers%20under%20severe%20class%20imbalance%20remains%20a%20challenge.%20Recent%20techniques%0Amitigate%20the%20ill%20effects%20of%20imbalance%20on%20training%20by%20modifying%20the%20loss%0Afunctions%20or%20optimization%20methods.%20We%20observe%20that%20different%20hyperparameter%0Avalues%20on%20these%20loss%20functions%20perform%20better%20at%20different%20recall%20values.%20We%0Apropose%20to%20exploit%20this%20fact%20by%20training%20one%20model%20over%20a%20distribution%20of%0Ahyperparameter%20values--instead%20of%20a%20single%20value--via%20Loss%20Conditional%20Training%0A%28LCT%29.%20Experiments%20show%20that%20training%20over%20a%20distribution%20of%20hyperparameters%0Anot%20only%20approximates%20the%20performance%20of%20several%20models%20but%20actually%20improves%0Athe%20overall%20performance%20of%20models%20on%20both%20CIFAR%20and%20real%20medical%20imaging%0Aapplications%2C%20such%20as%20melanoma%20and%20diabetic%20retinopathy%20detection.%20Furthermore%2C%0Atraining%20models%20with%20LCT%20is%20more%20efficient%20because%20some%20hyperparameter%20tuning%0Acan%20be%20conducted%20after%20training%20to%20meet%20individual%20needs%20without%20needing%20to%0Aretrain%20from%20scratch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Over%2520a%2520Distribution%2520of%2520Hyperparameters%2520for%2520Enhanced%2520Performance%250A%2520%2520and%2520Adaptability%2520on%2520Imbalanced%2520Classification%26entry.906535625%3DKelsey%2520Lieberman%2520and%2520Swarna%2520Kamlam%2520Ravindran%2520and%2520Shuai%2520Yuan%2520and%2520Carlo%2520Tomasi%26entry.1292438233%3D%2520%2520Although%2520binary%2520classification%2520is%2520a%2520well-studied%2520problem%252C%2520training%2520reliable%250Aclassifiers%2520under%2520severe%2520class%2520imbalance%2520remains%2520a%2520challenge.%2520Recent%2520techniques%250Amitigate%2520the%2520ill%2520effects%2520of%2520imbalance%2520on%2520training%2520by%2520modifying%2520the%2520loss%250Afunctions%2520or%2520optimization%2520methods.%2520We%2520observe%2520that%2520different%2520hyperparameter%250Avalues%2520on%2520these%2520loss%2520functions%2520perform%2520better%2520at%2520different%2520recall%2520values.%2520We%250Apropose%2520to%2520exploit%2520this%2520fact%2520by%2520training%2520one%2520model%2520over%2520a%2520distribution%2520of%250Ahyperparameter%2520values--instead%2520of%2520a%2520single%2520value--via%2520Loss%2520Conditional%2520Training%250A%2528LCT%2529.%2520Experiments%2520show%2520that%2520training%2520over%2520a%2520distribution%2520of%2520hyperparameters%250Anot%2520only%2520approximates%2520the%2520performance%2520of%2520several%2520models%2520but%2520actually%2520improves%250Athe%2520overall%2520performance%2520of%2520models%2520on%2520both%2520CIFAR%2520and%2520real%2520medical%2520imaging%250Aapplications%252C%2520such%2520as%2520melanoma%2520and%2520diabetic%2520retinopathy%2520detection.%2520Furthermore%252C%250Atraining%2520models%2520with%2520LCT%2520is%2520more%2520efficient%2520because%2520some%2520hyperparameter%2520tuning%250Acan%2520be%2520conducted%2520after%2520training%2520to%2520meet%2520individual%2520needs%2520without%2520needing%2520to%250Aretrain%2520from%2520scratch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Over%20a%20Distribution%20of%20Hyperparameters%20for%20Enhanced%20Performance%0A%20%20and%20Adaptability%20on%20Imbalanced%20Classification&entry.906535625=Kelsey%20Lieberman%20and%20Swarna%20Kamlam%20Ravindran%20and%20Shuai%20Yuan%20and%20Carlo%20Tomasi&entry.1292438233=%20%20Although%20binary%20classification%20is%20a%20well-studied%20problem%2C%20training%20reliable%0Aclassifiers%20under%20severe%20class%20imbalance%20remains%20a%20challenge.%20Recent%20techniques%0Amitigate%20the%20ill%20effects%20of%20imbalance%20on%20training%20by%20modifying%20the%20loss%0Afunctions%20or%20optimization%20methods.%20We%20observe%20that%20different%20hyperparameter%0Avalues%20on%20these%20loss%20functions%20perform%20better%20at%20different%20recall%20values.%20We%0Apropose%20to%20exploit%20this%20fact%20by%20training%20one%20model%20over%20a%20distribution%20of%0Ahyperparameter%20values--instead%20of%20a%20single%20value--via%20Loss%20Conditional%20Training%0A%28LCT%29.%20Experiments%20show%20that%20training%20over%20a%20distribution%20of%20hyperparameters%0Anot%20only%20approximates%20the%20performance%20of%20several%20models%20but%20actually%20improves%0Athe%20overall%20performance%20of%20models%20on%20both%20CIFAR%20and%20real%20medical%20imaging%0Aapplications%2C%20such%20as%20melanoma%20and%20diabetic%20retinopathy%20detection.%20Furthermore%2C%0Atraining%20models%20with%20LCT%20is%20more%20efficient%20because%20some%20hyperparameter%20tuning%0Acan%20be%20conducted%20after%20training%20to%20meet%20individual%20needs%20without%20needing%20to%0Aretrain%20from%20scratch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03588v1&entry.124074799=Read"},
{"title": "Comparative Analysis and Ensemble Enhancement of Leading CNN\n  Architectures for Breast Cancer Classification", "author": "Gary Murphy and Raghubir Singh", "abstract": "  This study introduces a novel and accurate approach to breast cancer\nclassification using histopathology images. It systematically compares leading\nConvolutional Neural Network (CNN) models across varying image datasets,\nidentifies their optimal hyperparameters, and ranks them based on\nclassification efficacy. To maximize classification accuracy for each model we\nexplore, the effects of data augmentation, alternative fully-connected layers,\nmodel training hyperparameter settings, and, the advantages of retraining\nmodels versus using pre-trained weights. Our methodology includes several\noriginal concepts, including serializing generated datasets to ensure\nconsistent data conditions across training runs and significantly reducing\ntraining duration. Combined with automated curation of results, this enabled\nthe exploration of over 2,000 training permutations -- such a comprehensive\ncomparison is as yet unprecedented. Our findings establish the settings\nrequired to achieve exceptional classification accuracy for standalone CNN\nmodels and rank them by model efficacy. Based on these results, we propose\nensemble architectures that stack three high-performing standalone CNN models\ntogether with diverse classifiers, resulting in improved classification\naccuracy. The ability to systematically run so many model permutations to get\nthe best outcomes gives rise to very high quality results, including 99.75% for\nBreakHis x40 and BreakHis x200 and 95.18% for the Bach datasets when split into\ntrain, validation and test datasets. The Bach Online blind challenge, yielded\n89% using this approach. Whilst this study is based on breast cancer\nhistopathology image datasets, the methodology is equally applicable to other\nmedical image datasets.\n", "link": "http://arxiv.org/abs/2410.03333v1", "date": "2024-10-04", "relevancy": 2.5236, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5244}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20and%20Ensemble%20Enhancement%20of%20Leading%20CNN%0A%20%20Architectures%20for%20Breast%20Cancer%20Classification&body=Title%3A%20Comparative%20Analysis%20and%20Ensemble%20Enhancement%20of%20Leading%20CNN%0A%20%20Architectures%20for%20Breast%20Cancer%20Classification%0AAuthor%3A%20Gary%20Murphy%20and%20Raghubir%20Singh%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20novel%20and%20accurate%20approach%20to%20breast%20cancer%0Aclassification%20using%20histopathology%20images.%20It%20systematically%20compares%20leading%0AConvolutional%20Neural%20Network%20%28CNN%29%20models%20across%20varying%20image%20datasets%2C%0Aidentifies%20their%20optimal%20hyperparameters%2C%20and%20ranks%20them%20based%20on%0Aclassification%20efficacy.%20To%20maximize%20classification%20accuracy%20for%20each%20model%20we%0Aexplore%2C%20the%20effects%20of%20data%20augmentation%2C%20alternative%20fully-connected%20layers%2C%0Amodel%20training%20hyperparameter%20settings%2C%20and%2C%20the%20advantages%20of%20retraining%0Amodels%20versus%20using%20pre-trained%20weights.%20Our%20methodology%20includes%20several%0Aoriginal%20concepts%2C%20including%20serializing%20generated%20datasets%20to%20ensure%0Aconsistent%20data%20conditions%20across%20training%20runs%20and%20significantly%20reducing%0Atraining%20duration.%20Combined%20with%20automated%20curation%20of%20results%2C%20this%20enabled%0Athe%20exploration%20of%20over%202%2C000%20training%20permutations%20--%20such%20a%20comprehensive%0Acomparison%20is%20as%20yet%20unprecedented.%20Our%20findings%20establish%20the%20settings%0Arequired%20to%20achieve%20exceptional%20classification%20accuracy%20for%20standalone%20CNN%0Amodels%20and%20rank%20them%20by%20model%20efficacy.%20Based%20on%20these%20results%2C%20we%20propose%0Aensemble%20architectures%20that%20stack%20three%20high-performing%20standalone%20CNN%20models%0Atogether%20with%20diverse%20classifiers%2C%20resulting%20in%20improved%20classification%0Aaccuracy.%20The%20ability%20to%20systematically%20run%20so%20many%20model%20permutations%20to%20get%0Athe%20best%20outcomes%20gives%20rise%20to%20very%20high%20quality%20results%2C%20including%2099.75%25%20for%0ABreakHis%20x40%20and%20BreakHis%20x200%20and%2095.18%25%20for%20the%20Bach%20datasets%20when%20split%20into%0Atrain%2C%20validation%20and%20test%20datasets.%20The%20Bach%20Online%20blind%20challenge%2C%20yielded%0A89%25%20using%20this%20approach.%20Whilst%20this%20study%20is%20based%20on%20breast%20cancer%0Ahistopathology%20image%20datasets%2C%20the%20methodology%20is%20equally%20applicable%20to%20other%0Amedical%20image%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520and%2520Ensemble%2520Enhancement%2520of%2520Leading%2520CNN%250A%2520%2520Architectures%2520for%2520Breast%2520Cancer%2520Classification%26entry.906535625%3DGary%2520Murphy%2520and%2520Raghubir%2520Singh%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520novel%2520and%2520accurate%2520approach%2520to%2520breast%2520cancer%250Aclassification%2520using%2520histopathology%2520images.%2520It%2520systematically%2520compares%2520leading%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529%2520models%2520across%2520varying%2520image%2520datasets%252C%250Aidentifies%2520their%2520optimal%2520hyperparameters%252C%2520and%2520ranks%2520them%2520based%2520on%250Aclassification%2520efficacy.%2520To%2520maximize%2520classification%2520accuracy%2520for%2520each%2520model%2520we%250Aexplore%252C%2520the%2520effects%2520of%2520data%2520augmentation%252C%2520alternative%2520fully-connected%2520layers%252C%250Amodel%2520training%2520hyperparameter%2520settings%252C%2520and%252C%2520the%2520advantages%2520of%2520retraining%250Amodels%2520versus%2520using%2520pre-trained%2520weights.%2520Our%2520methodology%2520includes%2520several%250Aoriginal%2520concepts%252C%2520including%2520serializing%2520generated%2520datasets%2520to%2520ensure%250Aconsistent%2520data%2520conditions%2520across%2520training%2520runs%2520and%2520significantly%2520reducing%250Atraining%2520duration.%2520Combined%2520with%2520automated%2520curation%2520of%2520results%252C%2520this%2520enabled%250Athe%2520exploration%2520of%2520over%25202%252C000%2520training%2520permutations%2520--%2520such%2520a%2520comprehensive%250Acomparison%2520is%2520as%2520yet%2520unprecedented.%2520Our%2520findings%2520establish%2520the%2520settings%250Arequired%2520to%2520achieve%2520exceptional%2520classification%2520accuracy%2520for%2520standalone%2520CNN%250Amodels%2520and%2520rank%2520them%2520by%2520model%2520efficacy.%2520Based%2520on%2520these%2520results%252C%2520we%2520propose%250Aensemble%2520architectures%2520that%2520stack%2520three%2520high-performing%2520standalone%2520CNN%2520models%250Atogether%2520with%2520diverse%2520classifiers%252C%2520resulting%2520in%2520improved%2520classification%250Aaccuracy.%2520The%2520ability%2520to%2520systematically%2520run%2520so%2520many%2520model%2520permutations%2520to%2520get%250Athe%2520best%2520outcomes%2520gives%2520rise%2520to%2520very%2520high%2520quality%2520results%252C%2520including%252099.75%2525%2520for%250ABreakHis%2520x40%2520and%2520BreakHis%2520x200%2520and%252095.18%2525%2520for%2520the%2520Bach%2520datasets%2520when%2520split%2520into%250Atrain%252C%2520validation%2520and%2520test%2520datasets.%2520The%2520Bach%2520Online%2520blind%2520challenge%252C%2520yielded%250A89%2525%2520using%2520this%2520approach.%2520Whilst%2520this%2520study%2520is%2520based%2520on%2520breast%2520cancer%250Ahistopathology%2520image%2520datasets%252C%2520the%2520methodology%2520is%2520equally%2520applicable%2520to%2520other%250Amedical%2520image%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20and%20Ensemble%20Enhancement%20of%20Leading%20CNN%0A%20%20Architectures%20for%20Breast%20Cancer%20Classification&entry.906535625=Gary%20Murphy%20and%20Raghubir%20Singh&entry.1292438233=%20%20This%20study%20introduces%20a%20novel%20and%20accurate%20approach%20to%20breast%20cancer%0Aclassification%20using%20histopathology%20images.%20It%20systematically%20compares%20leading%0AConvolutional%20Neural%20Network%20%28CNN%29%20models%20across%20varying%20image%20datasets%2C%0Aidentifies%20their%20optimal%20hyperparameters%2C%20and%20ranks%20them%20based%20on%0Aclassification%20efficacy.%20To%20maximize%20classification%20accuracy%20for%20each%20model%20we%0Aexplore%2C%20the%20effects%20of%20data%20augmentation%2C%20alternative%20fully-connected%20layers%2C%0Amodel%20training%20hyperparameter%20settings%2C%20and%2C%20the%20advantages%20of%20retraining%0Amodels%20versus%20using%20pre-trained%20weights.%20Our%20methodology%20includes%20several%0Aoriginal%20concepts%2C%20including%20serializing%20generated%20datasets%20to%20ensure%0Aconsistent%20data%20conditions%20across%20training%20runs%20and%20significantly%20reducing%0Atraining%20duration.%20Combined%20with%20automated%20curation%20of%20results%2C%20this%20enabled%0Athe%20exploration%20of%20over%202%2C000%20training%20permutations%20--%20such%20a%20comprehensive%0Acomparison%20is%20as%20yet%20unprecedented.%20Our%20findings%20establish%20the%20settings%0Arequired%20to%20achieve%20exceptional%20classification%20accuracy%20for%20standalone%20CNN%0Amodels%20and%20rank%20them%20by%20model%20efficacy.%20Based%20on%20these%20results%2C%20we%20propose%0Aensemble%20architectures%20that%20stack%20three%20high-performing%20standalone%20CNN%20models%0Atogether%20with%20diverse%20classifiers%2C%20resulting%20in%20improved%20classification%0Aaccuracy.%20The%20ability%20to%20systematically%20run%20so%20many%20model%20permutations%20to%20get%0Athe%20best%20outcomes%20gives%20rise%20to%20very%20high%20quality%20results%2C%20including%2099.75%25%20for%0ABreakHis%20x40%20and%20BreakHis%20x200%20and%2095.18%25%20for%20the%20Bach%20datasets%20when%20split%20into%0Atrain%2C%20validation%20and%20test%20datasets.%20The%20Bach%20Online%20blind%20challenge%2C%20yielded%0A89%25%20using%20this%20approach.%20Whilst%20this%20study%20is%20based%20on%20breast%20cancer%0Ahistopathology%20image%20datasets%2C%20the%20methodology%20is%20equally%20applicable%20to%20other%0Amedical%20image%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03333v1&entry.124074799=Read"},
{"title": "HyperCMR: Enhanced Multi-Contrast CMR Reconstruction with Eagle Loss", "author": "Ruru Xu and Caner \u00d6zer and Ilkay Oksuz", "abstract": "  Accelerating image acquisition for cardiac magnetic resonance imaging (CMRI)\nis a critical task. CMRxRecon2024 challenge aims to set the state of the art\nfor multi-contrast CMR reconstruction. This paper presents HyperCMR, a novel\nframework designed to accelerate the reconstruction of multi-contrast cardiac\nmagnetic resonance (CMR) images. HyperCMR enhances the existing PromptMR model\nby incorporating advanced loss functions, notably the innovative Eagle Loss,\nwhich is specifically designed to recover missing high-frequency information in\nundersampled k-space. Extensive experiments conducted on the CMRxRecon2024\nchallenge dataset demonstrate that HyperCMR consistently outperforms the\nbaseline across multiple evaluation metrics, achieving superior SSIM and PSNR\nscores.\n", "link": "http://arxiv.org/abs/2410.03624v1", "date": "2024-10-04", "relevancy": 2.5164, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5102}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5102}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperCMR%3A%20Enhanced%20Multi-Contrast%20CMR%20Reconstruction%20with%20Eagle%20Loss&body=Title%3A%20HyperCMR%3A%20Enhanced%20Multi-Contrast%20CMR%20Reconstruction%20with%20Eagle%20Loss%0AAuthor%3A%20Ruru%20Xu%20and%20Caner%20%C3%96zer%20and%20Ilkay%20Oksuz%0AAbstract%3A%20%20%20Accelerating%20image%20acquisition%20for%20cardiac%20magnetic%20resonance%20imaging%20%28CMRI%29%0Ais%20a%20critical%20task.%20CMRxRecon2024%20challenge%20aims%20to%20set%20the%20state%20of%20the%20art%0Afor%20multi-contrast%20CMR%20reconstruction.%20This%20paper%20presents%20HyperCMR%2C%20a%20novel%0Aframework%20designed%20to%20accelerate%20the%20reconstruction%20of%20multi-contrast%20cardiac%0Amagnetic%20resonance%20%28CMR%29%20images.%20HyperCMR%20enhances%20the%20existing%20PromptMR%20model%0Aby%20incorporating%20advanced%20loss%20functions%2C%20notably%20the%20innovative%20Eagle%20Loss%2C%0Awhich%20is%20specifically%20designed%20to%20recover%20missing%20high-frequency%20information%20in%0Aundersampled%20k-space.%20Extensive%20experiments%20conducted%20on%20the%20CMRxRecon2024%0Achallenge%20dataset%20demonstrate%20that%20HyperCMR%20consistently%20outperforms%20the%0Abaseline%20across%20multiple%20evaluation%20metrics%2C%20achieving%20superior%20SSIM%20and%20PSNR%0Ascores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperCMR%253A%2520Enhanced%2520Multi-Contrast%2520CMR%2520Reconstruction%2520with%2520Eagle%2520Loss%26entry.906535625%3DRuru%2520Xu%2520and%2520Caner%2520%25C3%2596zer%2520and%2520Ilkay%2520Oksuz%26entry.1292438233%3D%2520%2520Accelerating%2520image%2520acquisition%2520for%2520cardiac%2520magnetic%2520resonance%2520imaging%2520%2528CMRI%2529%250Ais%2520a%2520critical%2520task.%2520CMRxRecon2024%2520challenge%2520aims%2520to%2520set%2520the%2520state%2520of%2520the%2520art%250Afor%2520multi-contrast%2520CMR%2520reconstruction.%2520This%2520paper%2520presents%2520HyperCMR%252C%2520a%2520novel%250Aframework%2520designed%2520to%2520accelerate%2520the%2520reconstruction%2520of%2520multi-contrast%2520cardiac%250Amagnetic%2520resonance%2520%2528CMR%2529%2520images.%2520HyperCMR%2520enhances%2520the%2520existing%2520PromptMR%2520model%250Aby%2520incorporating%2520advanced%2520loss%2520functions%252C%2520notably%2520the%2520innovative%2520Eagle%2520Loss%252C%250Awhich%2520is%2520specifically%2520designed%2520to%2520recover%2520missing%2520high-frequency%2520information%2520in%250Aundersampled%2520k-space.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520CMRxRecon2024%250Achallenge%2520dataset%2520demonstrate%2520that%2520HyperCMR%2520consistently%2520outperforms%2520the%250Abaseline%2520across%2520multiple%2520evaluation%2520metrics%252C%2520achieving%2520superior%2520SSIM%2520and%2520PSNR%250Ascores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperCMR%3A%20Enhanced%20Multi-Contrast%20CMR%20Reconstruction%20with%20Eagle%20Loss&entry.906535625=Ruru%20Xu%20and%20Caner%20%C3%96zer%20and%20Ilkay%20Oksuz&entry.1292438233=%20%20Accelerating%20image%20acquisition%20for%20cardiac%20magnetic%20resonance%20imaging%20%28CMRI%29%0Ais%20a%20critical%20task.%20CMRxRecon2024%20challenge%20aims%20to%20set%20the%20state%20of%20the%20art%0Afor%20multi-contrast%20CMR%20reconstruction.%20This%20paper%20presents%20HyperCMR%2C%20a%20novel%0Aframework%20designed%20to%20accelerate%20the%20reconstruction%20of%20multi-contrast%20cardiac%0Amagnetic%20resonance%20%28CMR%29%20images.%20HyperCMR%20enhances%20the%20existing%20PromptMR%20model%0Aby%20incorporating%20advanced%20loss%20functions%2C%20notably%20the%20innovative%20Eagle%20Loss%2C%0Awhich%20is%20specifically%20designed%20to%20recover%20missing%20high-frequency%20information%20in%0Aundersampled%20k-space.%20Extensive%20experiments%20conducted%20on%20the%20CMRxRecon2024%0Achallenge%20dataset%20demonstrate%20that%20HyperCMR%20consistently%20outperforms%20the%0Abaseline%20across%20multiple%20evaluation%20metrics%2C%20achieving%20superior%20SSIM%20and%20PSNR%0Ascores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03624v1&entry.124074799=Read"},
{"title": "The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed\n  Learning", "author": "Youssef Allouah and Rachid Guerraoui and Nirupam Gupta and Ahmed Jellouli and Geovani Rizk and John Stephan", "abstract": "  Byzantine-resilient distributed machine learning seeks to achieve robust\nlearning performance in the presence of misbehaving or adversarial workers.\nWhile state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD)\nmethods were proven theoretically optimal, their empirical success has often\nrelied on pre-aggregation gradient clipping. However, the currently considered\nstatic clipping strategy exhibits mixed results: improving robustness against\nsome attacks while being ineffective or detrimental against others. We address\nthis gap by proposing a principled adaptive clipping strategy, termed Adaptive\nRobust Clipping (ARC). We show that ARC consistently enhances the empirical\nrobustness of SOTA Robust-DGD methods, while preserving the theoretical\nrobustness guarantees. Our analysis shows that ARC provably improves the\nasymptotic convergence guarantee of Robust-DGD in the case when the model is\nwell-initialized. We validate this theoretical insight through an exhaustive\nset of experiments on benchmark image classification tasks. We observe that the\nimprovement induced by ARC is more pronounced in highly heterogeneous and\nadversarial settings.\n", "link": "http://arxiv.org/abs/2405.14432v3", "date": "2024-10-04", "relevancy": 2.512, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5164}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4959}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Vital%20Role%20of%20Gradient%20Clipping%20in%20Byzantine-Resilient%20Distributed%0A%20%20Learning&body=Title%3A%20The%20Vital%20Role%20of%20Gradient%20Clipping%20in%20Byzantine-Resilient%20Distributed%0A%20%20Learning%0AAuthor%3A%20Youssef%20Allouah%20and%20Rachid%20Guerraoui%20and%20Nirupam%20Gupta%20and%20Ahmed%20Jellouli%20and%20Geovani%20Rizk%20and%20John%20Stephan%0AAbstract%3A%20%20%20Byzantine-resilient%20distributed%20machine%20learning%20seeks%20to%20achieve%20robust%0Alearning%20performance%20in%20the%20presence%20of%20misbehaving%20or%20adversarial%20workers.%0AWhile%20state-of-the-art%20%28SOTA%29%20robust%20distributed%20gradient%20descent%20%28Robust-DGD%29%0Amethods%20were%20proven%20theoretically%20optimal%2C%20their%20empirical%20success%20has%20often%0Arelied%20on%20pre-aggregation%20gradient%20clipping.%20However%2C%20the%20currently%20considered%0Astatic%20clipping%20strategy%20exhibits%20mixed%20results%3A%20improving%20robustness%20against%0Asome%20attacks%20while%20being%20ineffective%20or%20detrimental%20against%20others.%20We%20address%0Athis%20gap%20by%20proposing%20a%20principled%20adaptive%20clipping%20strategy%2C%20termed%20Adaptive%0ARobust%20Clipping%20%28ARC%29.%20We%20show%20that%20ARC%20consistently%20enhances%20the%20empirical%0Arobustness%20of%20SOTA%20Robust-DGD%20methods%2C%20while%20preserving%20the%20theoretical%0Arobustness%20guarantees.%20Our%20analysis%20shows%20that%20ARC%20provably%20improves%20the%0Aasymptotic%20convergence%20guarantee%20of%20Robust-DGD%20in%20the%20case%20when%20the%20model%20is%0Awell-initialized.%20We%20validate%20this%20theoretical%20insight%20through%20an%20exhaustive%0Aset%20of%20experiments%20on%20benchmark%20image%20classification%20tasks.%20We%20observe%20that%20the%0Aimprovement%20induced%20by%20ARC%20is%20more%20pronounced%20in%20highly%20heterogeneous%20and%0Aadversarial%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14432v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Vital%2520Role%2520of%2520Gradient%2520Clipping%2520in%2520Byzantine-Resilient%2520Distributed%250A%2520%2520Learning%26entry.906535625%3DYoussef%2520Allouah%2520and%2520Rachid%2520Guerraoui%2520and%2520Nirupam%2520Gupta%2520and%2520Ahmed%2520Jellouli%2520and%2520Geovani%2520Rizk%2520and%2520John%2520Stephan%26entry.1292438233%3D%2520%2520Byzantine-resilient%2520distributed%2520machine%2520learning%2520seeks%2520to%2520achieve%2520robust%250Alearning%2520performance%2520in%2520the%2520presence%2520of%2520misbehaving%2520or%2520adversarial%2520workers.%250AWhile%2520state-of-the-art%2520%2528SOTA%2529%2520robust%2520distributed%2520gradient%2520descent%2520%2528Robust-DGD%2529%250Amethods%2520were%2520proven%2520theoretically%2520optimal%252C%2520their%2520empirical%2520success%2520has%2520often%250Arelied%2520on%2520pre-aggregation%2520gradient%2520clipping.%2520However%252C%2520the%2520currently%2520considered%250Astatic%2520clipping%2520strategy%2520exhibits%2520mixed%2520results%253A%2520improving%2520robustness%2520against%250Asome%2520attacks%2520while%2520being%2520ineffective%2520or%2520detrimental%2520against%2520others.%2520We%2520address%250Athis%2520gap%2520by%2520proposing%2520a%2520principled%2520adaptive%2520clipping%2520strategy%252C%2520termed%2520Adaptive%250ARobust%2520Clipping%2520%2528ARC%2529.%2520We%2520show%2520that%2520ARC%2520consistently%2520enhances%2520the%2520empirical%250Arobustness%2520of%2520SOTA%2520Robust-DGD%2520methods%252C%2520while%2520preserving%2520the%2520theoretical%250Arobustness%2520guarantees.%2520Our%2520analysis%2520shows%2520that%2520ARC%2520provably%2520improves%2520the%250Aasymptotic%2520convergence%2520guarantee%2520of%2520Robust-DGD%2520in%2520the%2520case%2520when%2520the%2520model%2520is%250Awell-initialized.%2520We%2520validate%2520this%2520theoretical%2520insight%2520through%2520an%2520exhaustive%250Aset%2520of%2520experiments%2520on%2520benchmark%2520image%2520classification%2520tasks.%2520We%2520observe%2520that%2520the%250Aimprovement%2520induced%2520by%2520ARC%2520is%2520more%2520pronounced%2520in%2520highly%2520heterogeneous%2520and%250Aadversarial%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14432v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Vital%20Role%20of%20Gradient%20Clipping%20in%20Byzantine-Resilient%20Distributed%0A%20%20Learning&entry.906535625=Youssef%20Allouah%20and%20Rachid%20Guerraoui%20and%20Nirupam%20Gupta%20and%20Ahmed%20Jellouli%20and%20Geovani%20Rizk%20and%20John%20Stephan&entry.1292438233=%20%20Byzantine-resilient%20distributed%20machine%20learning%20seeks%20to%20achieve%20robust%0Alearning%20performance%20in%20the%20presence%20of%20misbehaving%20or%20adversarial%20workers.%0AWhile%20state-of-the-art%20%28SOTA%29%20robust%20distributed%20gradient%20descent%20%28Robust-DGD%29%0Amethods%20were%20proven%20theoretically%20optimal%2C%20their%20empirical%20success%20has%20often%0Arelied%20on%20pre-aggregation%20gradient%20clipping.%20However%2C%20the%20currently%20considered%0Astatic%20clipping%20strategy%20exhibits%20mixed%20results%3A%20improving%20robustness%20against%0Asome%20attacks%20while%20being%20ineffective%20or%20detrimental%20against%20others.%20We%20address%0Athis%20gap%20by%20proposing%20a%20principled%20adaptive%20clipping%20strategy%2C%20termed%20Adaptive%0ARobust%20Clipping%20%28ARC%29.%20We%20show%20that%20ARC%20consistently%20enhances%20the%20empirical%0Arobustness%20of%20SOTA%20Robust-DGD%20methods%2C%20while%20preserving%20the%20theoretical%0Arobustness%20guarantees.%20Our%20analysis%20shows%20that%20ARC%20provably%20improves%20the%0Aasymptotic%20convergence%20guarantee%20of%20Robust-DGD%20in%20the%20case%20when%20the%20model%20is%0Awell-initialized.%20We%20validate%20this%20theoretical%20insight%20through%20an%20exhaustive%0Aset%20of%20experiments%20on%20benchmark%20image%20classification%20tasks.%20We%20observe%20that%20the%0Aimprovement%20induced%20by%20ARC%20is%20more%20pronounced%20in%20highly%20heterogeneous%20and%0Aadversarial%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14432v3&entry.124074799=Read"},
{"title": "Enhancing Autonomous Navigation by Imaging Hidden Objects using\n  Single-Photon LiDAR", "author": "Aaron Young and Nevindu M. Batagoda and Harry Zhang and Akshat Dave and Adithya Pediredla and Dan Negrut and Ramesh Raskar", "abstract": "  Robust autonomous navigation in environments with limited visibility remains\na critical challenge in robotics. We present a novel approach that leverages\nNon-Line-of-Sight (NLOS) sensing using single-photon LiDAR to improve\nvisibility and enhance autonomous navigation. Our method enables mobile robots\nto \"see around corners\" by utilizing multi-bounce light information,\neffectively expanding their perceptual range without additional infrastructure.\nWe propose a three-module pipeline: (1) Sensing, which captures multi-bounce\nhistograms using SPAD-based LiDAR; (2) Perception, which estimates occupancy\nmaps of hidden regions from these histograms using a convolutional neural\nnetwork; and (3) Control, which allows a robot to follow safe paths based on\nthe estimated occupancy. We evaluate our approach through simulations and\nreal-world experiments on a mobile robot navigating an L-shaped corridor with\nhidden obstacles. Our work represents the first experimental demonstration of\nNLOS imaging for autonomous navigation, paving the way for safer and more\nefficient robotic systems operating in complex environments. We also contribute\na novel dynamics-integrated transient rendering framework for simulating NLOS\nscenarios, facilitating future research in this domain.\n", "link": "http://arxiv.org/abs/2410.03555v1", "date": "2024-10-04", "relevancy": 2.5103, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6437}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6176}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Autonomous%20Navigation%20by%20Imaging%20Hidden%20Objects%20using%0A%20%20Single-Photon%20LiDAR&body=Title%3A%20Enhancing%20Autonomous%20Navigation%20by%20Imaging%20Hidden%20Objects%20using%0A%20%20Single-Photon%20LiDAR%0AAuthor%3A%20Aaron%20Young%20and%20Nevindu%20M.%20Batagoda%20and%20Harry%20Zhang%20and%20Akshat%20Dave%20and%20Adithya%20Pediredla%20and%20Dan%20Negrut%20and%20Ramesh%20Raskar%0AAbstract%3A%20%20%20Robust%20autonomous%20navigation%20in%20environments%20with%20limited%20visibility%20remains%0Aa%20critical%20challenge%20in%20robotics.%20We%20present%20a%20novel%20approach%20that%20leverages%0ANon-Line-of-Sight%20%28NLOS%29%20sensing%20using%20single-photon%20LiDAR%20to%20improve%0Avisibility%20and%20enhance%20autonomous%20navigation.%20Our%20method%20enables%20mobile%20robots%0Ato%20%22see%20around%20corners%22%20by%20utilizing%20multi-bounce%20light%20information%2C%0Aeffectively%20expanding%20their%20perceptual%20range%20without%20additional%20infrastructure.%0AWe%20propose%20a%20three-module%20pipeline%3A%20%281%29%20Sensing%2C%20which%20captures%20multi-bounce%0Ahistograms%20using%20SPAD-based%20LiDAR%3B%20%282%29%20Perception%2C%20which%20estimates%20occupancy%0Amaps%20of%20hidden%20regions%20from%20these%20histograms%20using%20a%20convolutional%20neural%0Anetwork%3B%20and%20%283%29%20Control%2C%20which%20allows%20a%20robot%20to%20follow%20safe%20paths%20based%20on%0Athe%20estimated%20occupancy.%20We%20evaluate%20our%20approach%20through%20simulations%20and%0Areal-world%20experiments%20on%20a%20mobile%20robot%20navigating%20an%20L-shaped%20corridor%20with%0Ahidden%20obstacles.%20Our%20work%20represents%20the%20first%20experimental%20demonstration%20of%0ANLOS%20imaging%20for%20autonomous%20navigation%2C%20paving%20the%20way%20for%20safer%20and%20more%0Aefficient%20robotic%20systems%20operating%20in%20complex%20environments.%20We%20also%20contribute%0Aa%20novel%20dynamics-integrated%20transient%20rendering%20framework%20for%20simulating%20NLOS%0Ascenarios%2C%20facilitating%20future%20research%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Autonomous%2520Navigation%2520by%2520Imaging%2520Hidden%2520Objects%2520using%250A%2520%2520Single-Photon%2520LiDAR%26entry.906535625%3DAaron%2520Young%2520and%2520Nevindu%2520M.%2520Batagoda%2520and%2520Harry%2520Zhang%2520and%2520Akshat%2520Dave%2520and%2520Adithya%2520Pediredla%2520and%2520Dan%2520Negrut%2520and%2520Ramesh%2520Raskar%26entry.1292438233%3D%2520%2520Robust%2520autonomous%2520navigation%2520in%2520environments%2520with%2520limited%2520visibility%2520remains%250Aa%2520critical%2520challenge%2520in%2520robotics.%2520We%2520present%2520a%2520novel%2520approach%2520that%2520leverages%250ANon-Line-of-Sight%2520%2528NLOS%2529%2520sensing%2520using%2520single-photon%2520LiDAR%2520to%2520improve%250Avisibility%2520and%2520enhance%2520autonomous%2520navigation.%2520Our%2520method%2520enables%2520mobile%2520robots%250Ato%2520%2522see%2520around%2520corners%2522%2520by%2520utilizing%2520multi-bounce%2520light%2520information%252C%250Aeffectively%2520expanding%2520their%2520perceptual%2520range%2520without%2520additional%2520infrastructure.%250AWe%2520propose%2520a%2520three-module%2520pipeline%253A%2520%25281%2529%2520Sensing%252C%2520which%2520captures%2520multi-bounce%250Ahistograms%2520using%2520SPAD-based%2520LiDAR%253B%2520%25282%2529%2520Perception%252C%2520which%2520estimates%2520occupancy%250Amaps%2520of%2520hidden%2520regions%2520from%2520these%2520histograms%2520using%2520a%2520convolutional%2520neural%250Anetwork%253B%2520and%2520%25283%2529%2520Control%252C%2520which%2520allows%2520a%2520robot%2520to%2520follow%2520safe%2520paths%2520based%2520on%250Athe%2520estimated%2520occupancy.%2520We%2520evaluate%2520our%2520approach%2520through%2520simulations%2520and%250Areal-world%2520experiments%2520on%2520a%2520mobile%2520robot%2520navigating%2520an%2520L-shaped%2520corridor%2520with%250Ahidden%2520obstacles.%2520Our%2520work%2520represents%2520the%2520first%2520experimental%2520demonstration%2520of%250ANLOS%2520imaging%2520for%2520autonomous%2520navigation%252C%2520paving%2520the%2520way%2520for%2520safer%2520and%2520more%250Aefficient%2520robotic%2520systems%2520operating%2520in%2520complex%2520environments.%2520We%2520also%2520contribute%250Aa%2520novel%2520dynamics-integrated%2520transient%2520rendering%2520framework%2520for%2520simulating%2520NLOS%250Ascenarios%252C%2520facilitating%2520future%2520research%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Autonomous%20Navigation%20by%20Imaging%20Hidden%20Objects%20using%0A%20%20Single-Photon%20LiDAR&entry.906535625=Aaron%20Young%20and%20Nevindu%20M.%20Batagoda%20and%20Harry%20Zhang%20and%20Akshat%20Dave%20and%20Adithya%20Pediredla%20and%20Dan%20Negrut%20and%20Ramesh%20Raskar&entry.1292438233=%20%20Robust%20autonomous%20navigation%20in%20environments%20with%20limited%20visibility%20remains%0Aa%20critical%20challenge%20in%20robotics.%20We%20present%20a%20novel%20approach%20that%20leverages%0ANon-Line-of-Sight%20%28NLOS%29%20sensing%20using%20single-photon%20LiDAR%20to%20improve%0Avisibility%20and%20enhance%20autonomous%20navigation.%20Our%20method%20enables%20mobile%20robots%0Ato%20%22see%20around%20corners%22%20by%20utilizing%20multi-bounce%20light%20information%2C%0Aeffectively%20expanding%20their%20perceptual%20range%20without%20additional%20infrastructure.%0AWe%20propose%20a%20three-module%20pipeline%3A%20%281%29%20Sensing%2C%20which%20captures%20multi-bounce%0Ahistograms%20using%20SPAD-based%20LiDAR%3B%20%282%29%20Perception%2C%20which%20estimates%20occupancy%0Amaps%20of%20hidden%20regions%20from%20these%20histograms%20using%20a%20convolutional%20neural%0Anetwork%3B%20and%20%283%29%20Control%2C%20which%20allows%20a%20robot%20to%20follow%20safe%20paths%20based%20on%0Athe%20estimated%20occupancy.%20We%20evaluate%20our%20approach%20through%20simulations%20and%0Areal-world%20experiments%20on%20a%20mobile%20robot%20navigating%20an%20L-shaped%20corridor%20with%0Ahidden%20obstacles.%20Our%20work%20represents%20the%20first%20experimental%20demonstration%20of%0ANLOS%20imaging%20for%20autonomous%20navigation%2C%20paving%20the%20way%20for%20safer%20and%20more%0Aefficient%20robotic%20systems%20operating%20in%20complex%20environments.%20We%20also%20contribute%0Aa%20novel%20dynamics-integrated%20transient%20rendering%20framework%20for%20simulating%20NLOS%0Ascenarios%2C%20facilitating%20future%20research%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03555v1&entry.124074799=Read"},
{"title": "Conditional Enzyme Generation Using Protein Language Models with\n  Adapters", "author": "Jason Yang and Aadyot Bhatnagar and Jeffrey A. Ruffolo and Ali Madani", "abstract": "  The conditional generation of proteins with desired functions and/or\nproperties is a key goal for generative models. Existing methods based on\nprompting of language models can generate proteins conditioned on a target\nfunctionality, such as a desired enzyme family. However, these methods are\nlimited to simple, tokenized conditioning and have not been shown to generalize\nto unseen functions. In this study, we propose ProCALM (Protein Conditionally\nAdapted Language Model), an approach for the conditional generation of proteins\nusing adapters to protein language models. Our specific implementation of\nProCALM involves finetuning ProGen2 to incorporate conditioning representations\nof enzyme function and taxonomy. ProCALM matches existing methods at\nconditionally generating sequences from target enzyme families. Impressively,\nit can also generate within the joint distribution of enzymatic function and\ntaxonomy, and it can generalize to rare and unseen enzyme families and\ntaxonomies. Overall, ProCALM is a flexible and computationally efficient\napproach, and we expect that it can be extended to a wide range of generative\nlanguage models.\n", "link": "http://arxiv.org/abs/2410.03634v1", "date": "2024-10-04", "relevancy": 2.4629, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Enzyme%20Generation%20Using%20Protein%20Language%20Models%20with%0A%20%20Adapters&body=Title%3A%20Conditional%20Enzyme%20Generation%20Using%20Protein%20Language%20Models%20with%0A%20%20Adapters%0AAuthor%3A%20Jason%20Yang%20and%20Aadyot%20Bhatnagar%20and%20Jeffrey%20A.%20Ruffolo%20and%20Ali%20Madani%0AAbstract%3A%20%20%20The%20conditional%20generation%20of%20proteins%20with%20desired%20functions%20and/or%0Aproperties%20is%20a%20key%20goal%20for%20generative%20models.%20Existing%20methods%20based%20on%0Aprompting%20of%20language%20models%20can%20generate%20proteins%20conditioned%20on%20a%20target%0Afunctionality%2C%20such%20as%20a%20desired%20enzyme%20family.%20However%2C%20these%20methods%20are%0Alimited%20to%20simple%2C%20tokenized%20conditioning%20and%20have%20not%20been%20shown%20to%20generalize%0Ato%20unseen%20functions.%20In%20this%20study%2C%20we%20propose%20ProCALM%20%28Protein%20Conditionally%0AAdapted%20Language%20Model%29%2C%20an%20approach%20for%20the%20conditional%20generation%20of%20proteins%0Ausing%20adapters%20to%20protein%20language%20models.%20Our%20specific%20implementation%20of%0AProCALM%20involves%20finetuning%20ProGen2%20to%20incorporate%20conditioning%20representations%0Aof%20enzyme%20function%20and%20taxonomy.%20ProCALM%20matches%20existing%20methods%20at%0Aconditionally%20generating%20sequences%20from%20target%20enzyme%20families.%20Impressively%2C%0Ait%20can%20also%20generate%20within%20the%20joint%20distribution%20of%20enzymatic%20function%20and%0Ataxonomy%2C%20and%20it%20can%20generalize%20to%20rare%20and%20unseen%20enzyme%20families%20and%0Ataxonomies.%20Overall%2C%20ProCALM%20is%20a%20flexible%20and%20computationally%20efficient%0Aapproach%2C%20and%20we%20expect%20that%20it%20can%20be%20extended%20to%20a%20wide%20range%20of%20generative%0Alanguage%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Enzyme%2520Generation%2520Using%2520Protein%2520Language%2520Models%2520with%250A%2520%2520Adapters%26entry.906535625%3DJason%2520Yang%2520and%2520Aadyot%2520Bhatnagar%2520and%2520Jeffrey%2520A.%2520Ruffolo%2520and%2520Ali%2520Madani%26entry.1292438233%3D%2520%2520The%2520conditional%2520generation%2520of%2520proteins%2520with%2520desired%2520functions%2520and/or%250Aproperties%2520is%2520a%2520key%2520goal%2520for%2520generative%2520models.%2520Existing%2520methods%2520based%2520on%250Aprompting%2520of%2520language%2520models%2520can%2520generate%2520proteins%2520conditioned%2520on%2520a%2520target%250Afunctionality%252C%2520such%2520as%2520a%2520desired%2520enzyme%2520family.%2520However%252C%2520these%2520methods%2520are%250Alimited%2520to%2520simple%252C%2520tokenized%2520conditioning%2520and%2520have%2520not%2520been%2520shown%2520to%2520generalize%250Ato%2520unseen%2520functions.%2520In%2520this%2520study%252C%2520we%2520propose%2520ProCALM%2520%2528Protein%2520Conditionally%250AAdapted%2520Language%2520Model%2529%252C%2520an%2520approach%2520for%2520the%2520conditional%2520generation%2520of%2520proteins%250Ausing%2520adapters%2520to%2520protein%2520language%2520models.%2520Our%2520specific%2520implementation%2520of%250AProCALM%2520involves%2520finetuning%2520ProGen2%2520to%2520incorporate%2520conditioning%2520representations%250Aof%2520enzyme%2520function%2520and%2520taxonomy.%2520ProCALM%2520matches%2520existing%2520methods%2520at%250Aconditionally%2520generating%2520sequences%2520from%2520target%2520enzyme%2520families.%2520Impressively%252C%250Ait%2520can%2520also%2520generate%2520within%2520the%2520joint%2520distribution%2520of%2520enzymatic%2520function%2520and%250Ataxonomy%252C%2520and%2520it%2520can%2520generalize%2520to%2520rare%2520and%2520unseen%2520enzyme%2520families%2520and%250Ataxonomies.%2520Overall%252C%2520ProCALM%2520is%2520a%2520flexible%2520and%2520computationally%2520efficient%250Aapproach%252C%2520and%2520we%2520expect%2520that%2520it%2520can%2520be%2520extended%2520to%2520a%2520wide%2520range%2520of%2520generative%250Alanguage%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Enzyme%20Generation%20Using%20Protein%20Language%20Models%20with%0A%20%20Adapters&entry.906535625=Jason%20Yang%20and%20Aadyot%20Bhatnagar%20and%20Jeffrey%20A.%20Ruffolo%20and%20Ali%20Madani&entry.1292438233=%20%20The%20conditional%20generation%20of%20proteins%20with%20desired%20functions%20and/or%0Aproperties%20is%20a%20key%20goal%20for%20generative%20models.%20Existing%20methods%20based%20on%0Aprompting%20of%20language%20models%20can%20generate%20proteins%20conditioned%20on%20a%20target%0Afunctionality%2C%20such%20as%20a%20desired%20enzyme%20family.%20However%2C%20these%20methods%20are%0Alimited%20to%20simple%2C%20tokenized%20conditioning%20and%20have%20not%20been%20shown%20to%20generalize%0Ato%20unseen%20functions.%20In%20this%20study%2C%20we%20propose%20ProCALM%20%28Protein%20Conditionally%0AAdapted%20Language%20Model%29%2C%20an%20approach%20for%20the%20conditional%20generation%20of%20proteins%0Ausing%20adapters%20to%20protein%20language%20models.%20Our%20specific%20implementation%20of%0AProCALM%20involves%20finetuning%20ProGen2%20to%20incorporate%20conditioning%20representations%0Aof%20enzyme%20function%20and%20taxonomy.%20ProCALM%20matches%20existing%20methods%20at%0Aconditionally%20generating%20sequences%20from%20target%20enzyme%20families.%20Impressively%2C%0Ait%20can%20also%20generate%20within%20the%20joint%20distribution%20of%20enzymatic%20function%20and%0Ataxonomy%2C%20and%20it%20can%20generalize%20to%20rare%20and%20unseen%20enzyme%20families%20and%0Ataxonomies.%20Overall%2C%20ProCALM%20is%20a%20flexible%20and%20computationally%20efficient%0Aapproach%2C%20and%20we%20expect%20that%20it%20can%20be%20extended%20to%20a%20wide%20range%20of%20generative%0Alanguage%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03634v1&entry.124074799=Read"},
{"title": "Learning 3D Perception from Others' Predictions", "author": "Jinsu Yoo and Zhenyang Feng and Tai-Yu Pan and Yihong Sun and Cheng Perng Phoo and Xiangyu Chen and Mark Campbell and Kilian Q. Weinberger and Bharath Hariharan and Wei-Lun Chao", "abstract": "  Accurate 3D object detection in real-world environments requires a huge\namount of annotated data with high quality. Acquiring such data is tedious and\nexpensive, and often needs repeated effort when a new sensor is adopted or when\nthe detector is deployed in a new environment. We investigate a new scenario to\nconstruct 3D object detectors: learning from the predictions of a nearby unit\nthat is equipped with an accurate detector. For example, when a self-driving\ncar enters a new area, it may learn from other traffic participants whose\ndetectors have been optimized for that area. This setting is label-efficient,\nsensor-agnostic, and communication-efficient: nearby units only need to share\nthe predictions with the ego agent (e.g., car). Naively using the received\npredictions as ground-truths to train the detector for the ego car, however,\nleads to inferior performance. We systematically study the problem and identify\nviewpoint mismatches and mislocalization (due to synchronization and GPS\nerrors) as the main causes, which unavoidably result in false positives, false\nnegatives, and inaccurate pseudo labels. We propose a distance-based\ncurriculum, first learning from closer units with similar viewpoints and\nsubsequently improving the quality of other units' predictions via\nself-training. We further demonstrate that an effective pseudo label refinement\nmodule can be trained with a handful of annotated data, largely reducing the\ndata quantity necessary to train an object detector. We validate our approach\non the recently released real-world collaborative driving dataset, using\nreference cars' predictions as pseudo labels for the ego car. Extensive\nexperiments including several scenarios (e.g., different sensors, detectors,\nand domains) demonstrate the effectiveness of our approach toward\nlabel-efficient learning of 3D perception from other units' predictions.\n", "link": "http://arxiv.org/abs/2410.02646v2", "date": "2024-10-04", "relevancy": 2.4594, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6605}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6208}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%203D%20Perception%20from%20Others%27%20Predictions&body=Title%3A%20Learning%203D%20Perception%20from%20Others%27%20Predictions%0AAuthor%3A%20Jinsu%20Yoo%20and%20Zhenyang%20Feng%20and%20Tai-Yu%20Pan%20and%20Yihong%20Sun%20and%20Cheng%20Perng%20Phoo%20and%20Xiangyu%20Chen%20and%20Mark%20Campbell%20and%20Kilian%20Q.%20Weinberger%20and%20Bharath%20Hariharan%20and%20Wei-Lun%20Chao%0AAbstract%3A%20%20%20Accurate%203D%20object%20detection%20in%20real-world%20environments%20requires%20a%20huge%0Aamount%20of%20annotated%20data%20with%20high%20quality.%20Acquiring%20such%20data%20is%20tedious%20and%0Aexpensive%2C%20and%20often%20needs%20repeated%20effort%20when%20a%20new%20sensor%20is%20adopted%20or%20when%0Athe%20detector%20is%20deployed%20in%20a%20new%20environment.%20We%20investigate%20a%20new%20scenario%20to%0Aconstruct%203D%20object%20detectors%3A%20learning%20from%20the%20predictions%20of%20a%20nearby%20unit%0Athat%20is%20equipped%20with%20an%20accurate%20detector.%20For%20example%2C%20when%20a%20self-driving%0Acar%20enters%20a%20new%20area%2C%20it%20may%20learn%20from%20other%20traffic%20participants%20whose%0Adetectors%20have%20been%20optimized%20for%20that%20area.%20This%20setting%20is%20label-efficient%2C%0Asensor-agnostic%2C%20and%20communication-efficient%3A%20nearby%20units%20only%20need%20to%20share%0Athe%20predictions%20with%20the%20ego%20agent%20%28e.g.%2C%20car%29.%20Naively%20using%20the%20received%0Apredictions%20as%20ground-truths%20to%20train%20the%20detector%20for%20the%20ego%20car%2C%20however%2C%0Aleads%20to%20inferior%20performance.%20We%20systematically%20study%20the%20problem%20and%20identify%0Aviewpoint%20mismatches%20and%20mislocalization%20%28due%20to%20synchronization%20and%20GPS%0Aerrors%29%20as%20the%20main%20causes%2C%20which%20unavoidably%20result%20in%20false%20positives%2C%20false%0Anegatives%2C%20and%20inaccurate%20pseudo%20labels.%20We%20propose%20a%20distance-based%0Acurriculum%2C%20first%20learning%20from%20closer%20units%20with%20similar%20viewpoints%20and%0Asubsequently%20improving%20the%20quality%20of%20other%20units%27%20predictions%20via%0Aself-training.%20We%20further%20demonstrate%20that%20an%20effective%20pseudo%20label%20refinement%0Amodule%20can%20be%20trained%20with%20a%20handful%20of%20annotated%20data%2C%20largely%20reducing%20the%0Adata%20quantity%20necessary%20to%20train%20an%20object%20detector.%20We%20validate%20our%20approach%0Aon%20the%20recently%20released%20real-world%20collaborative%20driving%20dataset%2C%20using%0Areference%20cars%27%20predictions%20as%20pseudo%20labels%20for%20the%20ego%20car.%20Extensive%0Aexperiments%20including%20several%20scenarios%20%28e.g.%2C%20different%20sensors%2C%20detectors%2C%0Aand%20domains%29%20demonstrate%20the%20effectiveness%20of%20our%20approach%20toward%0Alabel-efficient%20learning%20of%203D%20perception%20from%20other%20units%27%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02646v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%25203D%2520Perception%2520from%2520Others%2527%2520Predictions%26entry.906535625%3DJinsu%2520Yoo%2520and%2520Zhenyang%2520Feng%2520and%2520Tai-Yu%2520Pan%2520and%2520Yihong%2520Sun%2520and%2520Cheng%2520Perng%2520Phoo%2520and%2520Xiangyu%2520Chen%2520and%2520Mark%2520Campbell%2520and%2520Kilian%2520Q.%2520Weinberger%2520and%2520Bharath%2520Hariharan%2520and%2520Wei-Lun%2520Chao%26entry.1292438233%3D%2520%2520Accurate%25203D%2520object%2520detection%2520in%2520real-world%2520environments%2520requires%2520a%2520huge%250Aamount%2520of%2520annotated%2520data%2520with%2520high%2520quality.%2520Acquiring%2520such%2520data%2520is%2520tedious%2520and%250Aexpensive%252C%2520and%2520often%2520needs%2520repeated%2520effort%2520when%2520a%2520new%2520sensor%2520is%2520adopted%2520or%2520when%250Athe%2520detector%2520is%2520deployed%2520in%2520a%2520new%2520environment.%2520We%2520investigate%2520a%2520new%2520scenario%2520to%250Aconstruct%25203D%2520object%2520detectors%253A%2520learning%2520from%2520the%2520predictions%2520of%2520a%2520nearby%2520unit%250Athat%2520is%2520equipped%2520with%2520an%2520accurate%2520detector.%2520For%2520example%252C%2520when%2520a%2520self-driving%250Acar%2520enters%2520a%2520new%2520area%252C%2520it%2520may%2520learn%2520from%2520other%2520traffic%2520participants%2520whose%250Adetectors%2520have%2520been%2520optimized%2520for%2520that%2520area.%2520This%2520setting%2520is%2520label-efficient%252C%250Asensor-agnostic%252C%2520and%2520communication-efficient%253A%2520nearby%2520units%2520only%2520need%2520to%2520share%250Athe%2520predictions%2520with%2520the%2520ego%2520agent%2520%2528e.g.%252C%2520car%2529.%2520Naively%2520using%2520the%2520received%250Apredictions%2520as%2520ground-truths%2520to%2520train%2520the%2520detector%2520for%2520the%2520ego%2520car%252C%2520however%252C%250Aleads%2520to%2520inferior%2520performance.%2520We%2520systematically%2520study%2520the%2520problem%2520and%2520identify%250Aviewpoint%2520mismatches%2520and%2520mislocalization%2520%2528due%2520to%2520synchronization%2520and%2520GPS%250Aerrors%2529%2520as%2520the%2520main%2520causes%252C%2520which%2520unavoidably%2520result%2520in%2520false%2520positives%252C%2520false%250Anegatives%252C%2520and%2520inaccurate%2520pseudo%2520labels.%2520We%2520propose%2520a%2520distance-based%250Acurriculum%252C%2520first%2520learning%2520from%2520closer%2520units%2520with%2520similar%2520viewpoints%2520and%250Asubsequently%2520improving%2520the%2520quality%2520of%2520other%2520units%2527%2520predictions%2520via%250Aself-training.%2520We%2520further%2520demonstrate%2520that%2520an%2520effective%2520pseudo%2520label%2520refinement%250Amodule%2520can%2520be%2520trained%2520with%2520a%2520handful%2520of%2520annotated%2520data%252C%2520largely%2520reducing%2520the%250Adata%2520quantity%2520necessary%2520to%2520train%2520an%2520object%2520detector.%2520We%2520validate%2520our%2520approach%250Aon%2520the%2520recently%2520released%2520real-world%2520collaborative%2520driving%2520dataset%252C%2520using%250Areference%2520cars%2527%2520predictions%2520as%2520pseudo%2520labels%2520for%2520the%2520ego%2520car.%2520Extensive%250Aexperiments%2520including%2520several%2520scenarios%2520%2528e.g.%252C%2520different%2520sensors%252C%2520detectors%252C%250Aand%2520domains%2529%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520toward%250Alabel-efficient%2520learning%2520of%25203D%2520perception%2520from%2520other%2520units%2527%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02646v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%203D%20Perception%20from%20Others%27%20Predictions&entry.906535625=Jinsu%20Yoo%20and%20Zhenyang%20Feng%20and%20Tai-Yu%20Pan%20and%20Yihong%20Sun%20and%20Cheng%20Perng%20Phoo%20and%20Xiangyu%20Chen%20and%20Mark%20Campbell%20and%20Kilian%20Q.%20Weinberger%20and%20Bharath%20Hariharan%20and%20Wei-Lun%20Chao&entry.1292438233=%20%20Accurate%203D%20object%20detection%20in%20real-world%20environments%20requires%20a%20huge%0Aamount%20of%20annotated%20data%20with%20high%20quality.%20Acquiring%20such%20data%20is%20tedious%20and%0Aexpensive%2C%20and%20often%20needs%20repeated%20effort%20when%20a%20new%20sensor%20is%20adopted%20or%20when%0Athe%20detector%20is%20deployed%20in%20a%20new%20environment.%20We%20investigate%20a%20new%20scenario%20to%0Aconstruct%203D%20object%20detectors%3A%20learning%20from%20the%20predictions%20of%20a%20nearby%20unit%0Athat%20is%20equipped%20with%20an%20accurate%20detector.%20For%20example%2C%20when%20a%20self-driving%0Acar%20enters%20a%20new%20area%2C%20it%20may%20learn%20from%20other%20traffic%20participants%20whose%0Adetectors%20have%20been%20optimized%20for%20that%20area.%20This%20setting%20is%20label-efficient%2C%0Asensor-agnostic%2C%20and%20communication-efficient%3A%20nearby%20units%20only%20need%20to%20share%0Athe%20predictions%20with%20the%20ego%20agent%20%28e.g.%2C%20car%29.%20Naively%20using%20the%20received%0Apredictions%20as%20ground-truths%20to%20train%20the%20detector%20for%20the%20ego%20car%2C%20however%2C%0Aleads%20to%20inferior%20performance.%20We%20systematically%20study%20the%20problem%20and%20identify%0Aviewpoint%20mismatches%20and%20mislocalization%20%28due%20to%20synchronization%20and%20GPS%0Aerrors%29%20as%20the%20main%20causes%2C%20which%20unavoidably%20result%20in%20false%20positives%2C%20false%0Anegatives%2C%20and%20inaccurate%20pseudo%20labels.%20We%20propose%20a%20distance-based%0Acurriculum%2C%20first%20learning%20from%20closer%20units%20with%20similar%20viewpoints%20and%0Asubsequently%20improving%20the%20quality%20of%20other%20units%27%20predictions%20via%0Aself-training.%20We%20further%20demonstrate%20that%20an%20effective%20pseudo%20label%20refinement%0Amodule%20can%20be%20trained%20with%20a%20handful%20of%20annotated%20data%2C%20largely%20reducing%20the%0Adata%20quantity%20necessary%20to%20train%20an%20object%20detector.%20We%20validate%20our%20approach%0Aon%20the%20recently%20released%20real-world%20collaborative%20driving%20dataset%2C%20using%0Areference%20cars%27%20predictions%20as%20pseudo%20labels%20for%20the%20ego%20car.%20Extensive%0Aexperiments%20including%20several%20scenarios%20%28e.g.%2C%20different%20sensors%2C%20detectors%2C%0Aand%20domains%29%20demonstrate%20the%20effectiveness%20of%20our%20approach%20toward%0Alabel-efficient%20learning%20of%203D%20perception%20from%20other%20units%27%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02646v2&entry.124074799=Read"},
{"title": "A Survey on Time-Series Pre-Trained Models", "author": "Qianli Ma and Zhen Liu and Zhenjing Zheng and Ziyang Huang and Siying Zhu and Zhongzhong Yu and James T. Kwok", "abstract": "  Time-Series Mining (TSM) is an important research area since it shows great\npotential in practical applications. Deep learning models that rely on massive\nlabeled data have been utilized for TSM successfully. However, constructing a\nlarge-scale well-labeled dataset is difficult due to data annotation costs.\nRecently, pre-trained models have gradually attracted attention in the time\nseries domain due to their remarkable performance in computer vision and\nnatural language processing. In this survey, we provide a comprehensive review\nof Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding,\napplying, and studying TS-PTMs. Specifically, we first briefly introduce the\ntypical deep learning models employed in TSM. Then, we give an overview of\nTS-PTMs according to the pre-training techniques. The main categories we\nexplore include supervised, unsupervised, and self-supervised TS-PTMs. Further,\nextensive experiments involving 27 methods, 434 datasets, and 679 transfer\nlearning scenarios are conducted to analyze the advantages and disadvantages of\ntransfer learning strategies, Transformer-based models, and representative\nTS-PTMs. Finally, we point out some potential directions of TS-PTMs for future\nwork.\n", "link": "http://arxiv.org/abs/2305.10716v2", "date": "2024-10-04", "relevancy": 2.4537, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Time-Series%20Pre-Trained%20Models&body=Title%3A%20A%20Survey%20on%20Time-Series%20Pre-Trained%20Models%0AAuthor%3A%20Qianli%20Ma%20and%20Zhen%20Liu%20and%20Zhenjing%20Zheng%20and%20Ziyang%20Huang%20and%20Siying%20Zhu%20and%20Zhongzhong%20Yu%20and%20James%20T.%20Kwok%0AAbstract%3A%20%20%20Time-Series%20Mining%20%28TSM%29%20is%20an%20important%20research%20area%20since%20it%20shows%20great%0Apotential%20in%20practical%20applications.%20Deep%20learning%20models%20that%20rely%20on%20massive%0Alabeled%20data%20have%20been%20utilized%20for%20TSM%20successfully.%20However%2C%20constructing%20a%0Alarge-scale%20well-labeled%20dataset%20is%20difficult%20due%20to%20data%20annotation%20costs.%0ARecently%2C%20pre-trained%20models%20have%20gradually%20attracted%20attention%20in%20the%20time%0Aseries%20domain%20due%20to%20their%20remarkable%20performance%20in%20computer%20vision%20and%0Anatural%20language%20processing.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%0Aof%20Time-Series%20Pre-Trained%20Models%20%28TS-PTMs%29%2C%20aiming%20to%20guide%20the%20understanding%2C%0Aapplying%2C%20and%20studying%20TS-PTMs.%20Specifically%2C%20we%20first%20briefly%20introduce%20the%0Atypical%20deep%20learning%20models%20employed%20in%20TSM.%20Then%2C%20we%20give%20an%20overview%20of%0ATS-PTMs%20according%20to%20the%20pre-training%20techniques.%20The%20main%20categories%20we%0Aexplore%20include%20supervised%2C%20unsupervised%2C%20and%20self-supervised%20TS-PTMs.%20Further%2C%0Aextensive%20experiments%20involving%2027%20methods%2C%20434%20datasets%2C%20and%20679%20transfer%0Alearning%20scenarios%20are%20conducted%20to%20analyze%20the%20advantages%20and%20disadvantages%20of%0Atransfer%20learning%20strategies%2C%20Transformer-based%20models%2C%20and%20representative%0ATS-PTMs.%20Finally%2C%20we%20point%20out%20some%20potential%20directions%20of%20TS-PTMs%20for%20future%0Awork.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Time-Series%2520Pre-Trained%2520Models%26entry.906535625%3DQianli%2520Ma%2520and%2520Zhen%2520Liu%2520and%2520Zhenjing%2520Zheng%2520and%2520Ziyang%2520Huang%2520and%2520Siying%2520Zhu%2520and%2520Zhongzhong%2520Yu%2520and%2520James%2520T.%2520Kwok%26entry.1292438233%3D%2520%2520Time-Series%2520Mining%2520%2528TSM%2529%2520is%2520an%2520important%2520research%2520area%2520since%2520it%2520shows%2520great%250Apotential%2520in%2520practical%2520applications.%2520Deep%2520learning%2520models%2520that%2520rely%2520on%2520massive%250Alabeled%2520data%2520have%2520been%2520utilized%2520for%2520TSM%2520successfully.%2520However%252C%2520constructing%2520a%250Alarge-scale%2520well-labeled%2520dataset%2520is%2520difficult%2520due%2520to%2520data%2520annotation%2520costs.%250ARecently%252C%2520pre-trained%2520models%2520have%2520gradually%2520attracted%2520attention%2520in%2520the%2520time%250Aseries%2520domain%2520due%2520to%2520their%2520remarkable%2520performance%2520in%2520computer%2520vision%2520and%250Anatural%2520language%2520processing.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520review%250Aof%2520Time-Series%2520Pre-Trained%2520Models%2520%2528TS-PTMs%2529%252C%2520aiming%2520to%2520guide%2520the%2520understanding%252C%250Aapplying%252C%2520and%2520studying%2520TS-PTMs.%2520Specifically%252C%2520we%2520first%2520briefly%2520introduce%2520the%250Atypical%2520deep%2520learning%2520models%2520employed%2520in%2520TSM.%2520Then%252C%2520we%2520give%2520an%2520overview%2520of%250ATS-PTMs%2520according%2520to%2520the%2520pre-training%2520techniques.%2520The%2520main%2520categories%2520we%250Aexplore%2520include%2520supervised%252C%2520unsupervised%252C%2520and%2520self-supervised%2520TS-PTMs.%2520Further%252C%250Aextensive%2520experiments%2520involving%252027%2520methods%252C%2520434%2520datasets%252C%2520and%2520679%2520transfer%250Alearning%2520scenarios%2520are%2520conducted%2520to%2520analyze%2520the%2520advantages%2520and%2520disadvantages%2520of%250Atransfer%2520learning%2520strategies%252C%2520Transformer-based%2520models%252C%2520and%2520representative%250ATS-PTMs.%2520Finally%252C%2520we%2520point%2520out%2520some%2520potential%2520directions%2520of%2520TS-PTMs%2520for%2520future%250Awork.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.10716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Time-Series%20Pre-Trained%20Models&entry.906535625=Qianli%20Ma%20and%20Zhen%20Liu%20and%20Zhenjing%20Zheng%20and%20Ziyang%20Huang%20and%20Siying%20Zhu%20and%20Zhongzhong%20Yu%20and%20James%20T.%20Kwok&entry.1292438233=%20%20Time-Series%20Mining%20%28TSM%29%20is%20an%20important%20research%20area%20since%20it%20shows%20great%0Apotential%20in%20practical%20applications.%20Deep%20learning%20models%20that%20rely%20on%20massive%0Alabeled%20data%20have%20been%20utilized%20for%20TSM%20successfully.%20However%2C%20constructing%20a%0Alarge-scale%20well-labeled%20dataset%20is%20difficult%20due%20to%20data%20annotation%20costs.%0ARecently%2C%20pre-trained%20models%20have%20gradually%20attracted%20attention%20in%20the%20time%0Aseries%20domain%20due%20to%20their%20remarkable%20performance%20in%20computer%20vision%20and%0Anatural%20language%20processing.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%0Aof%20Time-Series%20Pre-Trained%20Models%20%28TS-PTMs%29%2C%20aiming%20to%20guide%20the%20understanding%2C%0Aapplying%2C%20and%20studying%20TS-PTMs.%20Specifically%2C%20we%20first%20briefly%20introduce%20the%0Atypical%20deep%20learning%20models%20employed%20in%20TSM.%20Then%2C%20we%20give%20an%20overview%20of%0ATS-PTMs%20according%20to%20the%20pre-training%20techniques.%20The%20main%20categories%20we%0Aexplore%20include%20supervised%2C%20unsupervised%2C%20and%20self-supervised%20TS-PTMs.%20Further%2C%0Aextensive%20experiments%20involving%2027%20methods%2C%20434%20datasets%2C%20and%20679%20transfer%0Alearning%20scenarios%20are%20conducted%20to%20analyze%20the%20advantages%20and%20disadvantages%20of%0Atransfer%20learning%20strategies%2C%20Transformer-based%20models%2C%20and%20representative%0ATS-PTMs.%20Finally%2C%20we%20point%20out%20some%20potential%20directions%20of%20TS-PTMs%20for%20future%0Awork.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10716v2&entry.124074799=Read"},
{"title": "Consensus Knowledge Graph Learning via Multi-view Sparse Low Rank Block\n  Model", "author": "Tianxi Cai and Dong Xia and Luwan Zhang and Doudou Zhou", "abstract": "  Network analysis has been a powerful tool to unveil relationships and\ninteractions among a large number of objects. Yet its effectiveness in\naccurately identifying important node-node interactions is challenged by the\nrapidly growing network size, with data being collected at an unprecedented\ngranularity and scale. Common wisdom to overcome such high dimensionality is\ncollapsing nodes into smaller groups and conducting connectivity analysis on\nthe group level. Dividing efforts into two phases inevitably opens a gap in\nconsistency and drives down efficiency. Consensus learning emerges as a new\nnormal for common knowledge discovery with multiple data sources available. In\nthis paper, we propose a unified multi-view sparse low-rank block model (msLBM)\nframework, which enables simultaneous grouping and connectivity analysis by\ncombining multiple data sources. The msLBM framework efficiently represents\noverlapping information across large scale concepts and accommodates different\ntypes of heterogeneity across sources. Both features are desirable when\nanalyzing high dimensional electronic health record (EHR) datasets from\nmultiple health systems. An estimating procedure based on the alternating\nminimization algorithm is proposed. Our theoretical results demonstrate that a\nconsensus knowledge graph can be more accurately learned by leveraging\nmulti-source datasets, and statistically optimal rates can be achieved under\nmild conditions. Applications to the real world EHR data suggest that our\nproposed msLBM algorithm can more reliably reveal network structure among\nclinical concepts by effectively combining summary level EHR data from multiple\nhealth systems.\n", "link": "http://arxiv.org/abs/2209.13762v2", "date": "2024-10-04", "relevancy": 2.4413, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consensus%20Knowledge%20Graph%20Learning%20via%20Multi-view%20Sparse%20Low%20Rank%20Block%0A%20%20Model&body=Title%3A%20Consensus%20Knowledge%20Graph%20Learning%20via%20Multi-view%20Sparse%20Low%20Rank%20Block%0A%20%20Model%0AAuthor%3A%20Tianxi%20Cai%20and%20Dong%20Xia%20and%20Luwan%20Zhang%20and%20Doudou%20Zhou%0AAbstract%3A%20%20%20Network%20analysis%20has%20been%20a%20powerful%20tool%20to%20unveil%20relationships%20and%0Ainteractions%20among%20a%20large%20number%20of%20objects.%20Yet%20its%20effectiveness%20in%0Aaccurately%20identifying%20important%20node-node%20interactions%20is%20challenged%20by%20the%0Arapidly%20growing%20network%20size%2C%20with%20data%20being%20collected%20at%20an%20unprecedented%0Agranularity%20and%20scale.%20Common%20wisdom%20to%20overcome%20such%20high%20dimensionality%20is%0Acollapsing%20nodes%20into%20smaller%20groups%20and%20conducting%20connectivity%20analysis%20on%0Athe%20group%20level.%20Dividing%20efforts%20into%20two%20phases%20inevitably%20opens%20a%20gap%20in%0Aconsistency%20and%20drives%20down%20efficiency.%20Consensus%20learning%20emerges%20as%20a%20new%0Anormal%20for%20common%20knowledge%20discovery%20with%20multiple%20data%20sources%20available.%20In%0Athis%20paper%2C%20we%20propose%20a%20unified%20multi-view%20sparse%20low-rank%20block%20model%20%28msLBM%29%0Aframework%2C%20which%20enables%20simultaneous%20grouping%20and%20connectivity%20analysis%20by%0Acombining%20multiple%20data%20sources.%20The%20msLBM%20framework%20efficiently%20represents%0Aoverlapping%20information%20across%20large%20scale%20concepts%20and%20accommodates%20different%0Atypes%20of%20heterogeneity%20across%20sources.%20Both%20features%20are%20desirable%20when%0Aanalyzing%20high%20dimensional%20electronic%20health%20record%20%28EHR%29%20datasets%20from%0Amultiple%20health%20systems.%20An%20estimating%20procedure%20based%20on%20the%20alternating%0Aminimization%20algorithm%20is%20proposed.%20Our%20theoretical%20results%20demonstrate%20that%20a%0Aconsensus%20knowledge%20graph%20can%20be%20more%20accurately%20learned%20by%20leveraging%0Amulti-source%20datasets%2C%20and%20statistically%20optimal%20rates%20can%20be%20achieved%20under%0Amild%20conditions.%20Applications%20to%20the%20real%20world%20EHR%20data%20suggest%20that%20our%0Aproposed%20msLBM%20algorithm%20can%20more%20reliably%20reveal%20network%20structure%20among%0Aclinical%20concepts%20by%20effectively%20combining%20summary%20level%20EHR%20data%20from%20multiple%0Ahealth%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.13762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsensus%2520Knowledge%2520Graph%2520Learning%2520via%2520Multi-view%2520Sparse%2520Low%2520Rank%2520Block%250A%2520%2520Model%26entry.906535625%3DTianxi%2520Cai%2520and%2520Dong%2520Xia%2520and%2520Luwan%2520Zhang%2520and%2520Doudou%2520Zhou%26entry.1292438233%3D%2520%2520Network%2520analysis%2520has%2520been%2520a%2520powerful%2520tool%2520to%2520unveil%2520relationships%2520and%250Ainteractions%2520among%2520a%2520large%2520number%2520of%2520objects.%2520Yet%2520its%2520effectiveness%2520in%250Aaccurately%2520identifying%2520important%2520node-node%2520interactions%2520is%2520challenged%2520by%2520the%250Arapidly%2520growing%2520network%2520size%252C%2520with%2520data%2520being%2520collected%2520at%2520an%2520unprecedented%250Agranularity%2520and%2520scale.%2520Common%2520wisdom%2520to%2520overcome%2520such%2520high%2520dimensionality%2520is%250Acollapsing%2520nodes%2520into%2520smaller%2520groups%2520and%2520conducting%2520connectivity%2520analysis%2520on%250Athe%2520group%2520level.%2520Dividing%2520efforts%2520into%2520two%2520phases%2520inevitably%2520opens%2520a%2520gap%2520in%250Aconsistency%2520and%2520drives%2520down%2520efficiency.%2520Consensus%2520learning%2520emerges%2520as%2520a%2520new%250Anormal%2520for%2520common%2520knowledge%2520discovery%2520with%2520multiple%2520data%2520sources%2520available.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520unified%2520multi-view%2520sparse%2520low-rank%2520block%2520model%2520%2528msLBM%2529%250Aframework%252C%2520which%2520enables%2520simultaneous%2520grouping%2520and%2520connectivity%2520analysis%2520by%250Acombining%2520multiple%2520data%2520sources.%2520The%2520msLBM%2520framework%2520efficiently%2520represents%250Aoverlapping%2520information%2520across%2520large%2520scale%2520concepts%2520and%2520accommodates%2520different%250Atypes%2520of%2520heterogeneity%2520across%2520sources.%2520Both%2520features%2520are%2520desirable%2520when%250Aanalyzing%2520high%2520dimensional%2520electronic%2520health%2520record%2520%2528EHR%2529%2520datasets%2520from%250Amultiple%2520health%2520systems.%2520An%2520estimating%2520procedure%2520based%2520on%2520the%2520alternating%250Aminimization%2520algorithm%2520is%2520proposed.%2520Our%2520theoretical%2520results%2520demonstrate%2520that%2520a%250Aconsensus%2520knowledge%2520graph%2520can%2520be%2520more%2520accurately%2520learned%2520by%2520leveraging%250Amulti-source%2520datasets%252C%2520and%2520statistically%2520optimal%2520rates%2520can%2520be%2520achieved%2520under%250Amild%2520conditions.%2520Applications%2520to%2520the%2520real%2520world%2520EHR%2520data%2520suggest%2520that%2520our%250Aproposed%2520msLBM%2520algorithm%2520can%2520more%2520reliably%2520reveal%2520network%2520structure%2520among%250Aclinical%2520concepts%2520by%2520effectively%2520combining%2520summary%2520level%2520EHR%2520data%2520from%2520multiple%250Ahealth%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.13762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consensus%20Knowledge%20Graph%20Learning%20via%20Multi-view%20Sparse%20Low%20Rank%20Block%0A%20%20Model&entry.906535625=Tianxi%20Cai%20and%20Dong%20Xia%20and%20Luwan%20Zhang%20and%20Doudou%20Zhou&entry.1292438233=%20%20Network%20analysis%20has%20been%20a%20powerful%20tool%20to%20unveil%20relationships%20and%0Ainteractions%20among%20a%20large%20number%20of%20objects.%20Yet%20its%20effectiveness%20in%0Aaccurately%20identifying%20important%20node-node%20interactions%20is%20challenged%20by%20the%0Arapidly%20growing%20network%20size%2C%20with%20data%20being%20collected%20at%20an%20unprecedented%0Agranularity%20and%20scale.%20Common%20wisdom%20to%20overcome%20such%20high%20dimensionality%20is%0Acollapsing%20nodes%20into%20smaller%20groups%20and%20conducting%20connectivity%20analysis%20on%0Athe%20group%20level.%20Dividing%20efforts%20into%20two%20phases%20inevitably%20opens%20a%20gap%20in%0Aconsistency%20and%20drives%20down%20efficiency.%20Consensus%20learning%20emerges%20as%20a%20new%0Anormal%20for%20common%20knowledge%20discovery%20with%20multiple%20data%20sources%20available.%20In%0Athis%20paper%2C%20we%20propose%20a%20unified%20multi-view%20sparse%20low-rank%20block%20model%20%28msLBM%29%0Aframework%2C%20which%20enables%20simultaneous%20grouping%20and%20connectivity%20analysis%20by%0Acombining%20multiple%20data%20sources.%20The%20msLBM%20framework%20efficiently%20represents%0Aoverlapping%20information%20across%20large%20scale%20concepts%20and%20accommodates%20different%0Atypes%20of%20heterogeneity%20across%20sources.%20Both%20features%20are%20desirable%20when%0Aanalyzing%20high%20dimensional%20electronic%20health%20record%20%28EHR%29%20datasets%20from%0Amultiple%20health%20systems.%20An%20estimating%20procedure%20based%20on%20the%20alternating%0Aminimization%20algorithm%20is%20proposed.%20Our%20theoretical%20results%20demonstrate%20that%20a%0Aconsensus%20knowledge%20graph%20can%20be%20more%20accurately%20learned%20by%20leveraging%0Amulti-source%20datasets%2C%20and%20statistically%20optimal%20rates%20can%20be%20achieved%20under%0Amild%20conditions.%20Applications%20to%20the%20real%20world%20EHR%20data%20suggest%20that%20our%0Aproposed%20msLBM%20algorithm%20can%20more%20reliably%20reveal%20network%20structure%20among%0Aclinical%20concepts%20by%20effectively%20combining%20summary%20level%20EHR%20data%20from%20multiple%0Ahealth%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.13762v2&entry.124074799=Read"},
{"title": "Visual-O1: Understanding Ambiguous Instructions via Multi-modal\n  Multi-turn Chain-of-thoughts Reasoning", "author": "Minheng Ni and Yutao Fan and Lei Zhang and Wangmeng Zuo", "abstract": "  As large-scale models evolve, language instructions are increasingly utilized\nin multi-modal tasks. Due to human language habits, these instructions often\ncontain ambiguities in real-world scenarios, necessitating the integration of\nvisual context or common sense for accurate interpretation. However, even\nhighly intelligent large models exhibit significant performance limitations on\nambiguous instructions, where weak reasoning abilities of disambiguation can\nlead to catastrophic errors. To address this issue, this paper proposes\nVisual-O1, a multi-modal multi-turn chain-of-thought reasoning framework. It\nsimulates human multi-modal multi-turn reasoning, providing instantial\nexperience for highly intelligent models or empirical experience for generally\nintelligent models to understand ambiguous instructions. Unlike traditional\nmethods that require models to possess high intelligence to understand long\ntexts or perform lengthy complex reasoning, our framework does not\nsignificantly increase computational overhead and is more general and\neffective, even for generally intelligent models. Experiments show that our\nmethod not only significantly enhances the performance of models of different\nintelligence levels on ambiguous instructions but also improves their\nperformance on general datasets. Our work highlights the potential of\nartificial intelligence to work like humans in real-world scenarios with\nuncertainty and ambiguity. We will release our data and code.\n", "link": "http://arxiv.org/abs/2410.03321v1", "date": "2024-10-04", "relevancy": 2.438, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6114}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6114}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-O1%3A%20Understanding%20Ambiguous%20Instructions%20via%20Multi-modal%0A%20%20Multi-turn%20Chain-of-thoughts%20Reasoning&body=Title%3A%20Visual-O1%3A%20Understanding%20Ambiguous%20Instructions%20via%20Multi-modal%0A%20%20Multi-turn%20Chain-of-thoughts%20Reasoning%0AAuthor%3A%20Minheng%20Ni%20and%20Yutao%20Fan%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20As%20large-scale%20models%20evolve%2C%20language%20instructions%20are%20increasingly%20utilized%0Ain%20multi-modal%20tasks.%20Due%20to%20human%20language%20habits%2C%20these%20instructions%20often%0Acontain%20ambiguities%20in%20real-world%20scenarios%2C%20necessitating%20the%20integration%20of%0Avisual%20context%20or%20common%20sense%20for%20accurate%20interpretation.%20However%2C%20even%0Ahighly%20intelligent%20large%20models%20exhibit%20significant%20performance%20limitations%20on%0Aambiguous%20instructions%2C%20where%20weak%20reasoning%20abilities%20of%20disambiguation%20can%0Alead%20to%20catastrophic%20errors.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%0AVisual-O1%2C%20a%20multi-modal%20multi-turn%20chain-of-thought%20reasoning%20framework.%20It%0Asimulates%20human%20multi-modal%20multi-turn%20reasoning%2C%20providing%20instantial%0Aexperience%20for%20highly%20intelligent%20models%20or%20empirical%20experience%20for%20generally%0Aintelligent%20models%20to%20understand%20ambiguous%20instructions.%20Unlike%20traditional%0Amethods%20that%20require%20models%20to%20possess%20high%20intelligence%20to%20understand%20long%0Atexts%20or%20perform%20lengthy%20complex%20reasoning%2C%20our%20framework%20does%20not%0Asignificantly%20increase%20computational%20overhead%20and%20is%20more%20general%20and%0Aeffective%2C%20even%20for%20generally%20intelligent%20models.%20Experiments%20show%20that%20our%0Amethod%20not%20only%20significantly%20enhances%20the%20performance%20of%20models%20of%20different%0Aintelligence%20levels%20on%20ambiguous%20instructions%20but%20also%20improves%20their%0Aperformance%20on%20general%20datasets.%20Our%20work%20highlights%20the%20potential%20of%0Aartificial%20intelligence%20to%20work%20like%20humans%20in%20real-world%20scenarios%20with%0Auncertainty%20and%20ambiguity.%20We%20will%20release%20our%20data%20and%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-O1%253A%2520Understanding%2520Ambiguous%2520Instructions%2520via%2520Multi-modal%250A%2520%2520Multi-turn%2520Chain-of-thoughts%2520Reasoning%26entry.906535625%3DMinheng%2520Ni%2520and%2520Yutao%2520Fan%2520and%2520Lei%2520Zhang%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520As%2520large-scale%2520models%2520evolve%252C%2520language%2520instructions%2520are%2520increasingly%2520utilized%250Ain%2520multi-modal%2520tasks.%2520Due%2520to%2520human%2520language%2520habits%252C%2520these%2520instructions%2520often%250Acontain%2520ambiguities%2520in%2520real-world%2520scenarios%252C%2520necessitating%2520the%2520integration%2520of%250Avisual%2520context%2520or%2520common%2520sense%2520for%2520accurate%2520interpretation.%2520However%252C%2520even%250Ahighly%2520intelligent%2520large%2520models%2520exhibit%2520significant%2520performance%2520limitations%2520on%250Aambiguous%2520instructions%252C%2520where%2520weak%2520reasoning%2520abilities%2520of%2520disambiguation%2520can%250Alead%2520to%2520catastrophic%2520errors.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%250AVisual-O1%252C%2520a%2520multi-modal%2520multi-turn%2520chain-of-thought%2520reasoning%2520framework.%2520It%250Asimulates%2520human%2520multi-modal%2520multi-turn%2520reasoning%252C%2520providing%2520instantial%250Aexperience%2520for%2520highly%2520intelligent%2520models%2520or%2520empirical%2520experience%2520for%2520generally%250Aintelligent%2520models%2520to%2520understand%2520ambiguous%2520instructions.%2520Unlike%2520traditional%250Amethods%2520that%2520require%2520models%2520to%2520possess%2520high%2520intelligence%2520to%2520understand%2520long%250Atexts%2520or%2520perform%2520lengthy%2520complex%2520reasoning%252C%2520our%2520framework%2520does%2520not%250Asignificantly%2520increase%2520computational%2520overhead%2520and%2520is%2520more%2520general%2520and%250Aeffective%252C%2520even%2520for%2520generally%2520intelligent%2520models.%2520Experiments%2520show%2520that%2520our%250Amethod%2520not%2520only%2520significantly%2520enhances%2520the%2520performance%2520of%2520models%2520of%2520different%250Aintelligence%2520levels%2520on%2520ambiguous%2520instructions%2520but%2520also%2520improves%2520their%250Aperformance%2520on%2520general%2520datasets.%2520Our%2520work%2520highlights%2520the%2520potential%2520of%250Aartificial%2520intelligence%2520to%2520work%2520like%2520humans%2520in%2520real-world%2520scenarios%2520with%250Auncertainty%2520and%2520ambiguity.%2520We%2520will%2520release%2520our%2520data%2520and%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-O1%3A%20Understanding%20Ambiguous%20Instructions%20via%20Multi-modal%0A%20%20Multi-turn%20Chain-of-thoughts%20Reasoning&entry.906535625=Minheng%20Ni%20and%20Yutao%20Fan%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20As%20large-scale%20models%20evolve%2C%20language%20instructions%20are%20increasingly%20utilized%0Ain%20multi-modal%20tasks.%20Due%20to%20human%20language%20habits%2C%20these%20instructions%20often%0Acontain%20ambiguities%20in%20real-world%20scenarios%2C%20necessitating%20the%20integration%20of%0Avisual%20context%20or%20common%20sense%20for%20accurate%20interpretation.%20However%2C%20even%0Ahighly%20intelligent%20large%20models%20exhibit%20significant%20performance%20limitations%20on%0Aambiguous%20instructions%2C%20where%20weak%20reasoning%20abilities%20of%20disambiguation%20can%0Alead%20to%20catastrophic%20errors.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%0AVisual-O1%2C%20a%20multi-modal%20multi-turn%20chain-of-thought%20reasoning%20framework.%20It%0Asimulates%20human%20multi-modal%20multi-turn%20reasoning%2C%20providing%20instantial%0Aexperience%20for%20highly%20intelligent%20models%20or%20empirical%20experience%20for%20generally%0Aintelligent%20models%20to%20understand%20ambiguous%20instructions.%20Unlike%20traditional%0Amethods%20that%20require%20models%20to%20possess%20high%20intelligence%20to%20understand%20long%0Atexts%20or%20perform%20lengthy%20complex%20reasoning%2C%20our%20framework%20does%20not%0Asignificantly%20increase%20computational%20overhead%20and%20is%20more%20general%20and%0Aeffective%2C%20even%20for%20generally%20intelligent%20models.%20Experiments%20show%20that%20our%0Amethod%20not%20only%20significantly%20enhances%20the%20performance%20of%20models%20of%20different%0Aintelligence%20levels%20on%20ambiguous%20instructions%20but%20also%20improves%20their%0Aperformance%20on%20general%20datasets.%20Our%20work%20highlights%20the%20potential%20of%0Aartificial%20intelligence%20to%20work%20like%20humans%20in%20real-world%20scenarios%20with%0Auncertainty%20and%20ambiguity.%20We%20will%20release%20our%20data%20and%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03321v1&entry.124074799=Read"},
{"title": "FusionDTI: Fine-grained Binding Discovery with Token-level Fusion for\n  Drug-Target Interaction", "author": "Zhaohan Meng and Zaiqiao Meng and Ke Yuan and Iadh Ounis", "abstract": "  Predicting drug-target interaction (DTI) is critical in the drug discovery\nprocess. Despite remarkable advances in recent DTI models through the\nintegration of representations from diverse drug and target encoders, such\nmodels often struggle to capture the fine-grained interactions between drugs\nand protein, i.e. the binding of specific drug atoms (or substructures) and key\namino acids of proteins, which is crucial for understanding the binding\nmechanisms and optimising drug design. To address this issue, this paper\nintroduces a novel model, called FusionDTI, which uses a token-level Fusion\nmodule to effectively learn fine-grained information for Drug-Target\nInteraction. In particular, our FusionDTI model uses the SELFIES representation\nof drugs to mitigate sequence fragment invalidation and incorporates the\nstructure-aware (SA) vocabulary of target proteins to address the limitation of\namino acid sequences in structural information, additionally leveraging\npre-trained language models extensively trained on large-scale biomedical\ndatasets as encoders to capture the complex information of drugs and targets.\nExperiments on three well-known benchmark datasets show that our proposed\nFusionDTI model achieves the best performance in DTI prediction compared with\nseven existing state-of-the-art baselines. Furthermore, our case study\nindicates that FusionDTI could highlight the potential binding sites, enhancing\nthe explainability of the DTI prediction.\n", "link": "http://arxiv.org/abs/2406.01651v2", "date": "2024-10-04", "relevancy": 2.433, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5054}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FusionDTI%3A%20Fine-grained%20Binding%20Discovery%20with%20Token-level%20Fusion%20for%0A%20%20Drug-Target%20Interaction&body=Title%3A%20FusionDTI%3A%20Fine-grained%20Binding%20Discovery%20with%20Token-level%20Fusion%20for%0A%20%20Drug-Target%20Interaction%0AAuthor%3A%20Zhaohan%20Meng%20and%20Zaiqiao%20Meng%20and%20Ke%20Yuan%20and%20Iadh%20Ounis%0AAbstract%3A%20%20%20Predicting%20drug-target%20interaction%20%28DTI%29%20is%20critical%20in%20the%20drug%20discovery%0Aprocess.%20Despite%20remarkable%20advances%20in%20recent%20DTI%20models%20through%20the%0Aintegration%20of%20representations%20from%20diverse%20drug%20and%20target%20encoders%2C%20such%0Amodels%20often%20struggle%20to%20capture%20the%20fine-grained%20interactions%20between%20drugs%0Aand%20protein%2C%20i.e.%20the%20binding%20of%20specific%20drug%20atoms%20%28or%20substructures%29%20and%20key%0Aamino%20acids%20of%20proteins%2C%20which%20is%20crucial%20for%20understanding%20the%20binding%0Amechanisms%20and%20optimising%20drug%20design.%20To%20address%20this%20issue%2C%20this%20paper%0Aintroduces%20a%20novel%20model%2C%20called%20FusionDTI%2C%20which%20uses%20a%20token-level%20Fusion%0Amodule%20to%20effectively%20learn%20fine-grained%20information%20for%20Drug-Target%0AInteraction.%20In%20particular%2C%20our%20FusionDTI%20model%20uses%20the%20SELFIES%20representation%0Aof%20drugs%20to%20mitigate%20sequence%20fragment%20invalidation%20and%20incorporates%20the%0Astructure-aware%20%28SA%29%20vocabulary%20of%20target%20proteins%20to%20address%20the%20limitation%20of%0Aamino%20acid%20sequences%20in%20structural%20information%2C%20additionally%20leveraging%0Apre-trained%20language%20models%20extensively%20trained%20on%20large-scale%20biomedical%0Adatasets%20as%20encoders%20to%20capture%20the%20complex%20information%20of%20drugs%20and%20targets.%0AExperiments%20on%20three%20well-known%20benchmark%20datasets%20show%20that%20our%20proposed%0AFusionDTI%20model%20achieves%20the%20best%20performance%20in%20DTI%20prediction%20compared%20with%0Aseven%20existing%20state-of-the-art%20baselines.%20Furthermore%2C%20our%20case%20study%0Aindicates%20that%20FusionDTI%20could%20highlight%20the%20potential%20binding%20sites%2C%20enhancing%0Athe%20explainability%20of%20the%20DTI%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01651v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusionDTI%253A%2520Fine-grained%2520Binding%2520Discovery%2520with%2520Token-level%2520Fusion%2520for%250A%2520%2520Drug-Target%2520Interaction%26entry.906535625%3DZhaohan%2520Meng%2520and%2520Zaiqiao%2520Meng%2520and%2520Ke%2520Yuan%2520and%2520Iadh%2520Ounis%26entry.1292438233%3D%2520%2520Predicting%2520drug-target%2520interaction%2520%2528DTI%2529%2520is%2520critical%2520in%2520the%2520drug%2520discovery%250Aprocess.%2520Despite%2520remarkable%2520advances%2520in%2520recent%2520DTI%2520models%2520through%2520the%250Aintegration%2520of%2520representations%2520from%2520diverse%2520drug%2520and%2520target%2520encoders%252C%2520such%250Amodels%2520often%2520struggle%2520to%2520capture%2520the%2520fine-grained%2520interactions%2520between%2520drugs%250Aand%2520protein%252C%2520i.e.%2520the%2520binding%2520of%2520specific%2520drug%2520atoms%2520%2528or%2520substructures%2529%2520and%2520key%250Aamino%2520acids%2520of%2520proteins%252C%2520which%2520is%2520crucial%2520for%2520understanding%2520the%2520binding%250Amechanisms%2520and%2520optimising%2520drug%2520design.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520model%252C%2520called%2520FusionDTI%252C%2520which%2520uses%2520a%2520token-level%2520Fusion%250Amodule%2520to%2520effectively%2520learn%2520fine-grained%2520information%2520for%2520Drug-Target%250AInteraction.%2520In%2520particular%252C%2520our%2520FusionDTI%2520model%2520uses%2520the%2520SELFIES%2520representation%250Aof%2520drugs%2520to%2520mitigate%2520sequence%2520fragment%2520invalidation%2520and%2520incorporates%2520the%250Astructure-aware%2520%2528SA%2529%2520vocabulary%2520of%2520target%2520proteins%2520to%2520address%2520the%2520limitation%2520of%250Aamino%2520acid%2520sequences%2520in%2520structural%2520information%252C%2520additionally%2520leveraging%250Apre-trained%2520language%2520models%2520extensively%2520trained%2520on%2520large-scale%2520biomedical%250Adatasets%2520as%2520encoders%2520to%2520capture%2520the%2520complex%2520information%2520of%2520drugs%2520and%2520targets.%250AExperiments%2520on%2520three%2520well-known%2520benchmark%2520datasets%2520show%2520that%2520our%2520proposed%250AFusionDTI%2520model%2520achieves%2520the%2520best%2520performance%2520in%2520DTI%2520prediction%2520compared%2520with%250Aseven%2520existing%2520state-of-the-art%2520baselines.%2520Furthermore%252C%2520our%2520case%2520study%250Aindicates%2520that%2520FusionDTI%2520could%2520highlight%2520the%2520potential%2520binding%2520sites%252C%2520enhancing%250Athe%2520explainability%2520of%2520the%2520DTI%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01651v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionDTI%3A%20Fine-grained%20Binding%20Discovery%20with%20Token-level%20Fusion%20for%0A%20%20Drug-Target%20Interaction&entry.906535625=Zhaohan%20Meng%20and%20Zaiqiao%20Meng%20and%20Ke%20Yuan%20and%20Iadh%20Ounis&entry.1292438233=%20%20Predicting%20drug-target%20interaction%20%28DTI%29%20is%20critical%20in%20the%20drug%20discovery%0Aprocess.%20Despite%20remarkable%20advances%20in%20recent%20DTI%20models%20through%20the%0Aintegration%20of%20representations%20from%20diverse%20drug%20and%20target%20encoders%2C%20such%0Amodels%20often%20struggle%20to%20capture%20the%20fine-grained%20interactions%20between%20drugs%0Aand%20protein%2C%20i.e.%20the%20binding%20of%20specific%20drug%20atoms%20%28or%20substructures%29%20and%20key%0Aamino%20acids%20of%20proteins%2C%20which%20is%20crucial%20for%20understanding%20the%20binding%0Amechanisms%20and%20optimising%20drug%20design.%20To%20address%20this%20issue%2C%20this%20paper%0Aintroduces%20a%20novel%20model%2C%20called%20FusionDTI%2C%20which%20uses%20a%20token-level%20Fusion%0Amodule%20to%20effectively%20learn%20fine-grained%20information%20for%20Drug-Target%0AInteraction.%20In%20particular%2C%20our%20FusionDTI%20model%20uses%20the%20SELFIES%20representation%0Aof%20drugs%20to%20mitigate%20sequence%20fragment%20invalidation%20and%20incorporates%20the%0Astructure-aware%20%28SA%29%20vocabulary%20of%20target%20proteins%20to%20address%20the%20limitation%20of%0Aamino%20acid%20sequences%20in%20structural%20information%2C%20additionally%20leveraging%0Apre-trained%20language%20models%20extensively%20trained%20on%20large-scale%20biomedical%0Adatasets%20as%20encoders%20to%20capture%20the%20complex%20information%20of%20drugs%20and%20targets.%0AExperiments%20on%20three%20well-known%20benchmark%20datasets%20show%20that%20our%20proposed%0AFusionDTI%20model%20achieves%20the%20best%20performance%20in%20DTI%20prediction%20compared%20with%0Aseven%20existing%20state-of-the-art%20baselines.%20Furthermore%2C%20our%20case%20study%0Aindicates%20that%20FusionDTI%20could%20highlight%20the%20potential%20binding%20sites%2C%20enhancing%0Athe%20explainability%20of%20the%20DTI%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01651v2&entry.124074799=Read"},
{"title": "EmojiHeroVR: A Study on Facial Expression Recognition under Partial\n  Occlusion from Head-Mounted Displays", "author": "Thorben Ortmann and Qi Wang and Larissa Putzar", "abstract": "  Emotion recognition promotes the evaluation and enhancement of Virtual\nReality (VR) experiences by providing emotional feedback and enabling advanced\npersonalization. However, facial expressions are rarely used to recognize\nusers' emotions, as Head-Mounted Displays (HMDs) occlude the upper half of the\nface. To address this issue, we conducted a study with 37 participants who\nplayed our novel affective VR game EmojiHeroVR. The collected database,\nEmoHeVRDB (EmojiHeroVR Database), includes 3,556 labeled facial images of 1,778\nreenacted emotions. For each labeled image, we also provide 29 additional\nframes recorded directly before and after the labeled image to facilitate\ndynamic Facial Expression Recognition (FER). Additionally, EmoHeVRDB includes\ndata on the activations of 63 facial expressions captured via the Meta Quest\nPro VR headset for each frame. Leveraging our database, we conducted a baseline\nevaluation on the static FER classification task with six basic emotions and\nneutral using the EfficientNet-B0 architecture. The best model achieved an\naccuracy of 69.84% on the test set, indicating that FER under HMD occlusion is\nfeasible but significantly more challenging than conventional FER.\n", "link": "http://arxiv.org/abs/2410.03331v1", "date": "2024-10-04", "relevancy": 2.4256, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4997}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4778}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmojiHeroVR%3A%20A%20Study%20on%20Facial%20Expression%20Recognition%20under%20Partial%0A%20%20Occlusion%20from%20Head-Mounted%20Displays&body=Title%3A%20EmojiHeroVR%3A%20A%20Study%20on%20Facial%20Expression%20Recognition%20under%20Partial%0A%20%20Occlusion%20from%20Head-Mounted%20Displays%0AAuthor%3A%20Thorben%20Ortmann%20and%20Qi%20Wang%20and%20Larissa%20Putzar%0AAbstract%3A%20%20%20Emotion%20recognition%20promotes%20the%20evaluation%20and%20enhancement%20of%20Virtual%0AReality%20%28VR%29%20experiences%20by%20providing%20emotional%20feedback%20and%20enabling%20advanced%0Apersonalization.%20However%2C%20facial%20expressions%20are%20rarely%20used%20to%20recognize%0Ausers%27%20emotions%2C%20as%20Head-Mounted%20Displays%20%28HMDs%29%20occlude%20the%20upper%20half%20of%20the%0Aface.%20To%20address%20this%20issue%2C%20we%20conducted%20a%20study%20with%2037%20participants%20who%0Aplayed%20our%20novel%20affective%20VR%20game%20EmojiHeroVR.%20The%20collected%20database%2C%0AEmoHeVRDB%20%28EmojiHeroVR%20Database%29%2C%20includes%203%2C556%20labeled%20facial%20images%20of%201%2C778%0Areenacted%20emotions.%20For%20each%20labeled%20image%2C%20we%20also%20provide%2029%20additional%0Aframes%20recorded%20directly%20before%20and%20after%20the%20labeled%20image%20to%20facilitate%0Adynamic%20Facial%20Expression%20Recognition%20%28FER%29.%20Additionally%2C%20EmoHeVRDB%20includes%0Adata%20on%20the%20activations%20of%2063%20facial%20expressions%20captured%20via%20the%20Meta%20Quest%0APro%20VR%20headset%20for%20each%20frame.%20Leveraging%20our%20database%2C%20we%20conducted%20a%20baseline%0Aevaluation%20on%20the%20static%20FER%20classification%20task%20with%20six%20basic%20emotions%20and%0Aneutral%20using%20the%20EfficientNet-B0%20architecture.%20The%20best%20model%20achieved%20an%0Aaccuracy%20of%2069.84%25%20on%20the%20test%20set%2C%20indicating%20that%20FER%20under%20HMD%20occlusion%20is%0Afeasible%20but%20significantly%20more%20challenging%20than%20conventional%20FER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmojiHeroVR%253A%2520A%2520Study%2520on%2520Facial%2520Expression%2520Recognition%2520under%2520Partial%250A%2520%2520Occlusion%2520from%2520Head-Mounted%2520Displays%26entry.906535625%3DThorben%2520Ortmann%2520and%2520Qi%2520Wang%2520and%2520Larissa%2520Putzar%26entry.1292438233%3D%2520%2520Emotion%2520recognition%2520promotes%2520the%2520evaluation%2520and%2520enhancement%2520of%2520Virtual%250AReality%2520%2528VR%2529%2520experiences%2520by%2520providing%2520emotional%2520feedback%2520and%2520enabling%2520advanced%250Apersonalization.%2520However%252C%2520facial%2520expressions%2520are%2520rarely%2520used%2520to%2520recognize%250Ausers%2527%2520emotions%252C%2520as%2520Head-Mounted%2520Displays%2520%2528HMDs%2529%2520occlude%2520the%2520upper%2520half%2520of%2520the%250Aface.%2520To%2520address%2520this%2520issue%252C%2520we%2520conducted%2520a%2520study%2520with%252037%2520participants%2520who%250Aplayed%2520our%2520novel%2520affective%2520VR%2520game%2520EmojiHeroVR.%2520The%2520collected%2520database%252C%250AEmoHeVRDB%2520%2528EmojiHeroVR%2520Database%2529%252C%2520includes%25203%252C556%2520labeled%2520facial%2520images%2520of%25201%252C778%250Areenacted%2520emotions.%2520For%2520each%2520labeled%2520image%252C%2520we%2520also%2520provide%252029%2520additional%250Aframes%2520recorded%2520directly%2520before%2520and%2520after%2520the%2520labeled%2520image%2520to%2520facilitate%250Adynamic%2520Facial%2520Expression%2520Recognition%2520%2528FER%2529.%2520Additionally%252C%2520EmoHeVRDB%2520includes%250Adata%2520on%2520the%2520activations%2520of%252063%2520facial%2520expressions%2520captured%2520via%2520the%2520Meta%2520Quest%250APro%2520VR%2520headset%2520for%2520each%2520frame.%2520Leveraging%2520our%2520database%252C%2520we%2520conducted%2520a%2520baseline%250Aevaluation%2520on%2520the%2520static%2520FER%2520classification%2520task%2520with%2520six%2520basic%2520emotions%2520and%250Aneutral%2520using%2520the%2520EfficientNet-B0%2520architecture.%2520The%2520best%2520model%2520achieved%2520an%250Aaccuracy%2520of%252069.84%2525%2520on%2520the%2520test%2520set%252C%2520indicating%2520that%2520FER%2520under%2520HMD%2520occlusion%2520is%250Afeasible%2520but%2520significantly%2520more%2520challenging%2520than%2520conventional%2520FER.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmojiHeroVR%3A%20A%20Study%20on%20Facial%20Expression%20Recognition%20under%20Partial%0A%20%20Occlusion%20from%20Head-Mounted%20Displays&entry.906535625=Thorben%20Ortmann%20and%20Qi%20Wang%20and%20Larissa%20Putzar&entry.1292438233=%20%20Emotion%20recognition%20promotes%20the%20evaluation%20and%20enhancement%20of%20Virtual%0AReality%20%28VR%29%20experiences%20by%20providing%20emotional%20feedback%20and%20enabling%20advanced%0Apersonalization.%20However%2C%20facial%20expressions%20are%20rarely%20used%20to%20recognize%0Ausers%27%20emotions%2C%20as%20Head-Mounted%20Displays%20%28HMDs%29%20occlude%20the%20upper%20half%20of%20the%0Aface.%20To%20address%20this%20issue%2C%20we%20conducted%20a%20study%20with%2037%20participants%20who%0Aplayed%20our%20novel%20affective%20VR%20game%20EmojiHeroVR.%20The%20collected%20database%2C%0AEmoHeVRDB%20%28EmojiHeroVR%20Database%29%2C%20includes%203%2C556%20labeled%20facial%20images%20of%201%2C778%0Areenacted%20emotions.%20For%20each%20labeled%20image%2C%20we%20also%20provide%2029%20additional%0Aframes%20recorded%20directly%20before%20and%20after%20the%20labeled%20image%20to%20facilitate%0Adynamic%20Facial%20Expression%20Recognition%20%28FER%29.%20Additionally%2C%20EmoHeVRDB%20includes%0Adata%20on%20the%20activations%20of%2063%20facial%20expressions%20captured%20via%20the%20Meta%20Quest%0APro%20VR%20headset%20for%20each%20frame.%20Leveraging%20our%20database%2C%20we%20conducted%20a%20baseline%0Aevaluation%20on%20the%20static%20FER%20classification%20task%20with%20six%20basic%20emotions%20and%0Aneutral%20using%20the%20EfficientNet-B0%20architecture.%20The%20best%20model%20achieved%20an%0Aaccuracy%20of%2069.84%25%20on%20the%20test%20set%2C%20indicating%20that%20FER%20under%20HMD%20occlusion%20is%0Afeasible%20but%20significantly%20more%20challenging%20than%20conventional%20FER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03331v1&entry.124074799=Read"},
{"title": "GAP-RL: Grasps As Points for RL Towards Dynamic Object Grasping", "author": "Pengwei Xie and Siang Chen and Qianrun Chen and Wei Tang and Dingchang Hu and Yixiang Dai and Rui Chen and Guijin Wang", "abstract": "  Dynamic grasping of moving objects in complex, continuous motion scenarios\nremains challenging. Reinforcement Learning (RL) has been applied in various\nrobotic manipulation tasks, benefiting from its closed-loop property. However,\nexisting RL-based methods do not fully explore the potential for enhancing\nvisual representations. In this letter, we propose a novel framework called\nGrasps As Points for RL (GAP-RL) to effectively and reliably grasp moving\nobjects. By implementing a fast region-based grasp detector, we build a Grasp\nEncoder by transforming 6D grasp poses into Gaussian points and extracting\ngrasp features as a higher-level abstraction than the original object point\nfeatures. Additionally, we develop a Graspable Region Explorer for real-world\ndeployment, which searches for consistent graspable regions, enabling smoother\ngrasp generation and stable policy execution. To assess the performance fairly,\nwe construct a simulated dynamic grasping benchmark involving objects with\nvarious complex motions. Experiment results demonstrate that our method\neffectively generalizes to novel objects and unseen dynamic motions compared to\nother baselines. Real-world experiments further validate the framework's\nsim-to-real transferability.\n", "link": "http://arxiv.org/abs/2410.03509v1", "date": "2024-10-04", "relevancy": 2.4197, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6368}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6113}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAP-RL%3A%20Grasps%20As%20Points%20for%20RL%20Towards%20Dynamic%20Object%20Grasping&body=Title%3A%20GAP-RL%3A%20Grasps%20As%20Points%20for%20RL%20Towards%20Dynamic%20Object%20Grasping%0AAuthor%3A%20Pengwei%20Xie%20and%20Siang%20Chen%20and%20Qianrun%20Chen%20and%20Wei%20Tang%20and%20Dingchang%20Hu%20and%20Yixiang%20Dai%20and%20Rui%20Chen%20and%20Guijin%20Wang%0AAbstract%3A%20%20%20Dynamic%20grasping%20of%20moving%20objects%20in%20complex%2C%20continuous%20motion%20scenarios%0Aremains%20challenging.%20Reinforcement%20Learning%20%28RL%29%20has%20been%20applied%20in%20various%0Arobotic%20manipulation%20tasks%2C%20benefiting%20from%20its%20closed-loop%20property.%20However%2C%0Aexisting%20RL-based%20methods%20do%20not%20fully%20explore%20the%20potential%20for%20enhancing%0Avisual%20representations.%20In%20this%20letter%2C%20we%20propose%20a%20novel%20framework%20called%0AGrasps%20As%20Points%20for%20RL%20%28GAP-RL%29%20to%20effectively%20and%20reliably%20grasp%20moving%0Aobjects.%20By%20implementing%20a%20fast%20region-based%20grasp%20detector%2C%20we%20build%20a%20Grasp%0AEncoder%20by%20transforming%206D%20grasp%20poses%20into%20Gaussian%20points%20and%20extracting%0Agrasp%20features%20as%20a%20higher-level%20abstraction%20than%20the%20original%20object%20point%0Afeatures.%20Additionally%2C%20we%20develop%20a%20Graspable%20Region%20Explorer%20for%20real-world%0Adeployment%2C%20which%20searches%20for%20consistent%20graspable%20regions%2C%20enabling%20smoother%0Agrasp%20generation%20and%20stable%20policy%20execution.%20To%20assess%20the%20performance%20fairly%2C%0Awe%20construct%20a%20simulated%20dynamic%20grasping%20benchmark%20involving%20objects%20with%0Avarious%20complex%20motions.%20Experiment%20results%20demonstrate%20that%20our%20method%0Aeffectively%20generalizes%20to%20novel%20objects%20and%20unseen%20dynamic%20motions%20compared%20to%0Aother%20baselines.%20Real-world%20experiments%20further%20validate%20the%20framework%27s%0Asim-to-real%20transferability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAP-RL%253A%2520Grasps%2520As%2520Points%2520for%2520RL%2520Towards%2520Dynamic%2520Object%2520Grasping%26entry.906535625%3DPengwei%2520Xie%2520and%2520Siang%2520Chen%2520and%2520Qianrun%2520Chen%2520and%2520Wei%2520Tang%2520and%2520Dingchang%2520Hu%2520and%2520Yixiang%2520Dai%2520and%2520Rui%2520Chen%2520and%2520Guijin%2520Wang%26entry.1292438233%3D%2520%2520Dynamic%2520grasping%2520of%2520moving%2520objects%2520in%2520complex%252C%2520continuous%2520motion%2520scenarios%250Aremains%2520challenging.%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520been%2520applied%2520in%2520various%250Arobotic%2520manipulation%2520tasks%252C%2520benefiting%2520from%2520its%2520closed-loop%2520property.%2520However%252C%250Aexisting%2520RL-based%2520methods%2520do%2520not%2520fully%2520explore%2520the%2520potential%2520for%2520enhancing%250Avisual%2520representations.%2520In%2520this%2520letter%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%250AGrasps%2520As%2520Points%2520for%2520RL%2520%2528GAP-RL%2529%2520to%2520effectively%2520and%2520reliably%2520grasp%2520moving%250Aobjects.%2520By%2520implementing%2520a%2520fast%2520region-based%2520grasp%2520detector%252C%2520we%2520build%2520a%2520Grasp%250AEncoder%2520by%2520transforming%25206D%2520grasp%2520poses%2520into%2520Gaussian%2520points%2520and%2520extracting%250Agrasp%2520features%2520as%2520a%2520higher-level%2520abstraction%2520than%2520the%2520original%2520object%2520point%250Afeatures.%2520Additionally%252C%2520we%2520develop%2520a%2520Graspable%2520Region%2520Explorer%2520for%2520real-world%250Adeployment%252C%2520which%2520searches%2520for%2520consistent%2520graspable%2520regions%252C%2520enabling%2520smoother%250Agrasp%2520generation%2520and%2520stable%2520policy%2520execution.%2520To%2520assess%2520the%2520performance%2520fairly%252C%250Awe%2520construct%2520a%2520simulated%2520dynamic%2520grasping%2520benchmark%2520involving%2520objects%2520with%250Avarious%2520complex%2520motions.%2520Experiment%2520results%2520demonstrate%2520that%2520our%2520method%250Aeffectively%2520generalizes%2520to%2520novel%2520objects%2520and%2520unseen%2520dynamic%2520motions%2520compared%2520to%250Aother%2520baselines.%2520Real-world%2520experiments%2520further%2520validate%2520the%2520framework%2527s%250Asim-to-real%2520transferability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAP-RL%3A%20Grasps%20As%20Points%20for%20RL%20Towards%20Dynamic%20Object%20Grasping&entry.906535625=Pengwei%20Xie%20and%20Siang%20Chen%20and%20Qianrun%20Chen%20and%20Wei%20Tang%20and%20Dingchang%20Hu%20and%20Yixiang%20Dai%20and%20Rui%20Chen%20and%20Guijin%20Wang&entry.1292438233=%20%20Dynamic%20grasping%20of%20moving%20objects%20in%20complex%2C%20continuous%20motion%20scenarios%0Aremains%20challenging.%20Reinforcement%20Learning%20%28RL%29%20has%20been%20applied%20in%20various%0Arobotic%20manipulation%20tasks%2C%20benefiting%20from%20its%20closed-loop%20property.%20However%2C%0Aexisting%20RL-based%20methods%20do%20not%20fully%20explore%20the%20potential%20for%20enhancing%0Avisual%20representations.%20In%20this%20letter%2C%20we%20propose%20a%20novel%20framework%20called%0AGrasps%20As%20Points%20for%20RL%20%28GAP-RL%29%20to%20effectively%20and%20reliably%20grasp%20moving%0Aobjects.%20By%20implementing%20a%20fast%20region-based%20grasp%20detector%2C%20we%20build%20a%20Grasp%0AEncoder%20by%20transforming%206D%20grasp%20poses%20into%20Gaussian%20points%20and%20extracting%0Agrasp%20features%20as%20a%20higher-level%20abstraction%20than%20the%20original%20object%20point%0Afeatures.%20Additionally%2C%20we%20develop%20a%20Graspable%20Region%20Explorer%20for%20real-world%0Adeployment%2C%20which%20searches%20for%20consistent%20graspable%20regions%2C%20enabling%20smoother%0Agrasp%20generation%20and%20stable%20policy%20execution.%20To%20assess%20the%20performance%20fairly%2C%0Awe%20construct%20a%20simulated%20dynamic%20grasping%20benchmark%20involving%20objects%20with%0Avarious%20complex%20motions.%20Experiment%20results%20demonstrate%20that%20our%20method%0Aeffectively%20generalizes%20to%20novel%20objects%20and%20unseen%20dynamic%20motions%20compared%20to%0Aother%20baselines.%20Real-world%20experiments%20further%20validate%20the%20framework%27s%0Asim-to-real%20transferability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03509v1&entry.124074799=Read"},
{"title": "VEDIT: Latent Prediction Architecture For Procedural Video\n  Representation Learning", "author": "Han Lin and Tushar Nagarajan and Nicolas Ballas and Mido Assran and Mojtaba Komeili and Mohit Bansal and Koustuv Sinha", "abstract": "  Procedural video representation learning is an active research area where the\nobjective is to learn an agent which can anticipate and forecast the future\ngiven the present video input, typically in conjunction with textual\nannotations. Prior works often rely on large-scale pretraining of visual\nencoders and prediction models with language supervision. However, the\nnecessity and effectiveness of extending compute intensive pretraining to learn\nvideo clip sequences with noisy text supervision have not yet been fully\nvalidated by previous works. In this work, we show that a strong off-the-shelf\nfrozen pretrained visual encoder, along with a well designed prediction model,\ncan achieve state-of-the-art (SoTA) performance in forecasting and procedural\nplanning without the need for pretraining the prediction model, nor requiring\nadditional supervision from language or ASR. Instead of learning\nrepresentations from pixel space, our method utilizes the latent embedding\nspace of publicly available vision encoders. By conditioning on frozen\nclip-level embeddings from observed steps to predict the actions of unseen\nsteps, our prediction model is able to learn robust representations for\nforecasting through iterative denoising - leveraging the recent advances in\ndiffusion transformers (Peebles & Xie, 2023). Empirical studies over a total of\nfive procedural learning tasks across four datasets (NIV, CrossTask, COIN and\nEgo4D-v2) show that our model advances the strong baselines in long-horizon\naction anticipation (+2.6% in Verb ED@20, +3.1% in Noun ED@20), and\nsignificantly improves the SoTA in step forecasting (+5.0%), task\nclassification (+3.8%), and procedure planning tasks (up to +2.28% in success\nrate, +3.39% in mAcc, and +0.90% in mIoU).\n", "link": "http://arxiv.org/abs/2410.03478v1", "date": "2024-10-04", "relevancy": 2.4031, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VEDIT%3A%20Latent%20Prediction%20Architecture%20For%20Procedural%20Video%0A%20%20Representation%20Learning&body=Title%3A%20VEDIT%3A%20Latent%20Prediction%20Architecture%20For%20Procedural%20Video%0A%20%20Representation%20Learning%0AAuthor%3A%20Han%20Lin%20and%20Tushar%20Nagarajan%20and%20Nicolas%20Ballas%20and%20Mido%20Assran%20and%20Mojtaba%20Komeili%20and%20Mohit%20Bansal%20and%20Koustuv%20Sinha%0AAbstract%3A%20%20%20Procedural%20video%20representation%20learning%20is%20an%20active%20research%20area%20where%20the%0Aobjective%20is%20to%20learn%20an%20agent%20which%20can%20anticipate%20and%20forecast%20the%20future%0Agiven%20the%20present%20video%20input%2C%20typically%20in%20conjunction%20with%20textual%0Aannotations.%20Prior%20works%20often%20rely%20on%20large-scale%20pretraining%20of%20visual%0Aencoders%20and%20prediction%20models%20with%20language%20supervision.%20However%2C%20the%0Anecessity%20and%20effectiveness%20of%20extending%20compute%20intensive%20pretraining%20to%20learn%0Avideo%20clip%20sequences%20with%20noisy%20text%20supervision%20have%20not%20yet%20been%20fully%0Avalidated%20by%20previous%20works.%20In%20this%20work%2C%20we%20show%20that%20a%20strong%20off-the-shelf%0Afrozen%20pretrained%20visual%20encoder%2C%20along%20with%20a%20well%20designed%20prediction%20model%2C%0Acan%20achieve%20state-of-the-art%20%28SoTA%29%20performance%20in%20forecasting%20and%20procedural%0Aplanning%20without%20the%20need%20for%20pretraining%20the%20prediction%20model%2C%20nor%20requiring%0Aadditional%20supervision%20from%20language%20or%20ASR.%20Instead%20of%20learning%0Arepresentations%20from%20pixel%20space%2C%20our%20method%20utilizes%20the%20latent%20embedding%0Aspace%20of%20publicly%20available%20vision%20encoders.%20By%20conditioning%20on%20frozen%0Aclip-level%20embeddings%20from%20observed%20steps%20to%20predict%20the%20actions%20of%20unseen%0Asteps%2C%20our%20prediction%20model%20is%20able%20to%20learn%20robust%20representations%20for%0Aforecasting%20through%20iterative%20denoising%20-%20leveraging%20the%20recent%20advances%20in%0Adiffusion%20transformers%20%28Peebles%20%26%20Xie%2C%202023%29.%20Empirical%20studies%20over%20a%20total%20of%0Afive%20procedural%20learning%20tasks%20across%20four%20datasets%20%28NIV%2C%20CrossTask%2C%20COIN%20and%0AEgo4D-v2%29%20show%20that%20our%20model%20advances%20the%20strong%20baselines%20in%20long-horizon%0Aaction%20anticipation%20%28%2B2.6%25%20in%20Verb%20ED%4020%2C%20%2B3.1%25%20in%20Noun%20ED%4020%29%2C%20and%0Asignificantly%20improves%20the%20SoTA%20in%20step%20forecasting%20%28%2B5.0%25%29%2C%20task%0Aclassification%20%28%2B3.8%25%29%2C%20and%20procedure%20planning%20tasks%20%28up%20to%20%2B2.28%25%20in%20success%0Arate%2C%20%2B3.39%25%20in%20mAcc%2C%20and%20%2B0.90%25%20in%20mIoU%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVEDIT%253A%2520Latent%2520Prediction%2520Architecture%2520For%2520Procedural%2520Video%250A%2520%2520Representation%2520Learning%26entry.906535625%3DHan%2520Lin%2520and%2520Tushar%2520Nagarajan%2520and%2520Nicolas%2520Ballas%2520and%2520Mido%2520Assran%2520and%2520Mojtaba%2520Komeili%2520and%2520Mohit%2520Bansal%2520and%2520Koustuv%2520Sinha%26entry.1292438233%3D%2520%2520Procedural%2520video%2520representation%2520learning%2520is%2520an%2520active%2520research%2520area%2520where%2520the%250Aobjective%2520is%2520to%2520learn%2520an%2520agent%2520which%2520can%2520anticipate%2520and%2520forecast%2520the%2520future%250Agiven%2520the%2520present%2520video%2520input%252C%2520typically%2520in%2520conjunction%2520with%2520textual%250Aannotations.%2520Prior%2520works%2520often%2520rely%2520on%2520large-scale%2520pretraining%2520of%2520visual%250Aencoders%2520and%2520prediction%2520models%2520with%2520language%2520supervision.%2520However%252C%2520the%250Anecessity%2520and%2520effectiveness%2520of%2520extending%2520compute%2520intensive%2520pretraining%2520to%2520learn%250Avideo%2520clip%2520sequences%2520with%2520noisy%2520text%2520supervision%2520have%2520not%2520yet%2520been%2520fully%250Avalidated%2520by%2520previous%2520works.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520a%2520strong%2520off-the-shelf%250Afrozen%2520pretrained%2520visual%2520encoder%252C%2520along%2520with%2520a%2520well%2520designed%2520prediction%2520model%252C%250Acan%2520achieve%2520state-of-the-art%2520%2528SoTA%2529%2520performance%2520in%2520forecasting%2520and%2520procedural%250Aplanning%2520without%2520the%2520need%2520for%2520pretraining%2520the%2520prediction%2520model%252C%2520nor%2520requiring%250Aadditional%2520supervision%2520from%2520language%2520or%2520ASR.%2520Instead%2520of%2520learning%250Arepresentations%2520from%2520pixel%2520space%252C%2520our%2520method%2520utilizes%2520the%2520latent%2520embedding%250Aspace%2520of%2520publicly%2520available%2520vision%2520encoders.%2520By%2520conditioning%2520on%2520frozen%250Aclip-level%2520embeddings%2520from%2520observed%2520steps%2520to%2520predict%2520the%2520actions%2520of%2520unseen%250Asteps%252C%2520our%2520prediction%2520model%2520is%2520able%2520to%2520learn%2520robust%2520representations%2520for%250Aforecasting%2520through%2520iterative%2520denoising%2520-%2520leveraging%2520the%2520recent%2520advances%2520in%250Adiffusion%2520transformers%2520%2528Peebles%2520%2526%2520Xie%252C%25202023%2529.%2520Empirical%2520studies%2520over%2520a%2520total%2520of%250Afive%2520procedural%2520learning%2520tasks%2520across%2520four%2520datasets%2520%2528NIV%252C%2520CrossTask%252C%2520COIN%2520and%250AEgo4D-v2%2529%2520show%2520that%2520our%2520model%2520advances%2520the%2520strong%2520baselines%2520in%2520long-horizon%250Aaction%2520anticipation%2520%2528%252B2.6%2525%2520in%2520Verb%2520ED%254020%252C%2520%252B3.1%2525%2520in%2520Noun%2520ED%254020%2529%252C%2520and%250Asignificantly%2520improves%2520the%2520SoTA%2520in%2520step%2520forecasting%2520%2528%252B5.0%2525%2529%252C%2520task%250Aclassification%2520%2528%252B3.8%2525%2529%252C%2520and%2520procedure%2520planning%2520tasks%2520%2528up%2520to%2520%252B2.28%2525%2520in%2520success%250Arate%252C%2520%252B3.39%2525%2520in%2520mAcc%252C%2520and%2520%252B0.90%2525%2520in%2520mIoU%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VEDIT%3A%20Latent%20Prediction%20Architecture%20For%20Procedural%20Video%0A%20%20Representation%20Learning&entry.906535625=Han%20Lin%20and%20Tushar%20Nagarajan%20and%20Nicolas%20Ballas%20and%20Mido%20Assran%20and%20Mojtaba%20Komeili%20and%20Mohit%20Bansal%20and%20Koustuv%20Sinha&entry.1292438233=%20%20Procedural%20video%20representation%20learning%20is%20an%20active%20research%20area%20where%20the%0Aobjective%20is%20to%20learn%20an%20agent%20which%20can%20anticipate%20and%20forecast%20the%20future%0Agiven%20the%20present%20video%20input%2C%20typically%20in%20conjunction%20with%20textual%0Aannotations.%20Prior%20works%20often%20rely%20on%20large-scale%20pretraining%20of%20visual%0Aencoders%20and%20prediction%20models%20with%20language%20supervision.%20However%2C%20the%0Anecessity%20and%20effectiveness%20of%20extending%20compute%20intensive%20pretraining%20to%20learn%0Avideo%20clip%20sequences%20with%20noisy%20text%20supervision%20have%20not%20yet%20been%20fully%0Avalidated%20by%20previous%20works.%20In%20this%20work%2C%20we%20show%20that%20a%20strong%20off-the-shelf%0Afrozen%20pretrained%20visual%20encoder%2C%20along%20with%20a%20well%20designed%20prediction%20model%2C%0Acan%20achieve%20state-of-the-art%20%28SoTA%29%20performance%20in%20forecasting%20and%20procedural%0Aplanning%20without%20the%20need%20for%20pretraining%20the%20prediction%20model%2C%20nor%20requiring%0Aadditional%20supervision%20from%20language%20or%20ASR.%20Instead%20of%20learning%0Arepresentations%20from%20pixel%20space%2C%20our%20method%20utilizes%20the%20latent%20embedding%0Aspace%20of%20publicly%20available%20vision%20encoders.%20By%20conditioning%20on%20frozen%0Aclip-level%20embeddings%20from%20observed%20steps%20to%20predict%20the%20actions%20of%20unseen%0Asteps%2C%20our%20prediction%20model%20is%20able%20to%20learn%20robust%20representations%20for%0Aforecasting%20through%20iterative%20denoising%20-%20leveraging%20the%20recent%20advances%20in%0Adiffusion%20transformers%20%28Peebles%20%26%20Xie%2C%202023%29.%20Empirical%20studies%20over%20a%20total%20of%0Afive%20procedural%20learning%20tasks%20across%20four%20datasets%20%28NIV%2C%20CrossTask%2C%20COIN%20and%0AEgo4D-v2%29%20show%20that%20our%20model%20advances%20the%20strong%20baselines%20in%20long-horizon%0Aaction%20anticipation%20%28%2B2.6%25%20in%20Verb%20ED%4020%2C%20%2B3.1%25%20in%20Noun%20ED%4020%29%2C%20and%0Asignificantly%20improves%20the%20SoTA%20in%20step%20forecasting%20%28%2B5.0%25%29%2C%20task%0Aclassification%20%28%2B3.8%25%29%2C%20and%20procedure%20planning%20tasks%20%28up%20to%20%2B2.28%25%20in%20success%0Arate%2C%20%2B3.39%25%20in%20mAcc%2C%20and%20%2B0.90%25%20in%20mIoU%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03478v1&entry.124074799=Read"},
{"title": "Distributed Networked Multi-task Learning", "author": "Lingzhou Hong and Alfredo Garcia", "abstract": "  We consider a distributed multi-task learning scheme that accounts for\nmultiple linear model estimation tasks with heterogeneous and/or correlated\ndata streams. We assume that nodes can be partitioned into groups corresponding\nto different learning tasks and communicate according to a directed network\ntopology. Each node estimates a linear model asynchronously and is subject to\nlocal (within-group) regularization and global (across groups) regularization\nterms targeting noise reduction and generalization performance improvement\nrespectively. We provide a finite-time characterization of convergence of the\nestimators and task relation and illustrate the scheme's general applicability\nin two examples: random field temperature estimation and modeling student\nperformance from different academic districts.\n", "link": "http://arxiv.org/abs/2410.03403v1", "date": "2024-10-04", "relevancy": 2.3997, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4751}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Networked%20Multi-task%20Learning&body=Title%3A%20Distributed%20Networked%20Multi-task%20Learning%0AAuthor%3A%20Lingzhou%20Hong%20and%20Alfredo%20Garcia%0AAbstract%3A%20%20%20We%20consider%20a%20distributed%20multi-task%20learning%20scheme%20that%20accounts%20for%0Amultiple%20linear%20model%20estimation%20tasks%20with%20heterogeneous%20and/or%20correlated%0Adata%20streams.%20We%20assume%20that%20nodes%20can%20be%20partitioned%20into%20groups%20corresponding%0Ato%20different%20learning%20tasks%20and%20communicate%20according%20to%20a%20directed%20network%0Atopology.%20Each%20node%20estimates%20a%20linear%20model%20asynchronously%20and%20is%20subject%20to%0Alocal%20%28within-group%29%20regularization%20and%20global%20%28across%20groups%29%20regularization%0Aterms%20targeting%20noise%20reduction%20and%20generalization%20performance%20improvement%0Arespectively.%20We%20provide%20a%20finite-time%20characterization%20of%20convergence%20of%20the%0Aestimators%20and%20task%20relation%20and%20illustrate%20the%20scheme%27s%20general%20applicability%0Ain%20two%20examples%3A%20random%20field%20temperature%20estimation%20and%20modeling%20student%0Aperformance%20from%20different%20academic%20districts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Networked%2520Multi-task%2520Learning%26entry.906535625%3DLingzhou%2520Hong%2520and%2520Alfredo%2520Garcia%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520distributed%2520multi-task%2520learning%2520scheme%2520that%2520accounts%2520for%250Amultiple%2520linear%2520model%2520estimation%2520tasks%2520with%2520heterogeneous%2520and/or%2520correlated%250Adata%2520streams.%2520We%2520assume%2520that%2520nodes%2520can%2520be%2520partitioned%2520into%2520groups%2520corresponding%250Ato%2520different%2520learning%2520tasks%2520and%2520communicate%2520according%2520to%2520a%2520directed%2520network%250Atopology.%2520Each%2520node%2520estimates%2520a%2520linear%2520model%2520asynchronously%2520and%2520is%2520subject%2520to%250Alocal%2520%2528within-group%2529%2520regularization%2520and%2520global%2520%2528across%2520groups%2529%2520regularization%250Aterms%2520targeting%2520noise%2520reduction%2520and%2520generalization%2520performance%2520improvement%250Arespectively.%2520We%2520provide%2520a%2520finite-time%2520characterization%2520of%2520convergence%2520of%2520the%250Aestimators%2520and%2520task%2520relation%2520and%2520illustrate%2520the%2520scheme%2527s%2520general%2520applicability%250Ain%2520two%2520examples%253A%2520random%2520field%2520temperature%2520estimation%2520and%2520modeling%2520student%250Aperformance%2520from%2520different%2520academic%2520districts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Networked%20Multi-task%20Learning&entry.906535625=Lingzhou%20Hong%20and%20Alfredo%20Garcia&entry.1292438233=%20%20We%20consider%20a%20distributed%20multi-task%20learning%20scheme%20that%20accounts%20for%0Amultiple%20linear%20model%20estimation%20tasks%20with%20heterogeneous%20and/or%20correlated%0Adata%20streams.%20We%20assume%20that%20nodes%20can%20be%20partitioned%20into%20groups%20corresponding%0Ato%20different%20learning%20tasks%20and%20communicate%20according%20to%20a%20directed%20network%0Atopology.%20Each%20node%20estimates%20a%20linear%20model%20asynchronously%20and%20is%20subject%20to%0Alocal%20%28within-group%29%20regularization%20and%20global%20%28across%20groups%29%20regularization%0Aterms%20targeting%20noise%20reduction%20and%20generalization%20performance%20improvement%0Arespectively.%20We%20provide%20a%20finite-time%20characterization%20of%20convergence%20of%20the%0Aestimators%20and%20task%20relation%20and%20illustrate%20the%20scheme%27s%20general%20applicability%0Ain%20two%20examples%3A%20random%20field%20temperature%20estimation%20and%20modeling%20student%0Aperformance%20from%20different%20academic%20districts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03403v1&entry.124074799=Read"},
{"title": "One2set + Large Language Model: Best Partners for Keyphrase Generation", "author": "Liangying Shao and Liang Zhang and Minlong Peng and Guoqi Ma and Hao Yue and Mingming Sun and Jinsong Su", "abstract": "  Keyphrase generation (KPG) aims to automatically generate a collection of\nphrases representing the core concepts of a given document. The dominant\nparadigms in KPG include one2seq and one2set. Recently, there has been\nincreasing interest in applying large language models (LLMs) to KPG. Our\npreliminary experiments reveal that it is challenging for a single model to\nexcel in both recall and precision. Further analysis shows that: 1) the one2set\nparadigm owns the advantage of high recall, but suffers from improper\nassignments of supervision signals during training; 2) LLMs are powerful in\nkeyphrase selection, but existing selection methods often make redundant\nselections. Given these observations, we introduce a generate-then-select\nframework decomposing KPG into two steps, where we adopt a one2set-based model\nas generator to produce candidates and then use an LLM as selector to select\nkeyphrases from these candidates. Particularly, we make two important\nimprovements on our generator and selector: 1) we design an Optimal\nTransport-based assignment strategy to address the above improper assignments;\n2) we model the keyphrase selection as a sequence labeling task to alleviate\nredundant selections. Experimental results on multiple benchmark datasets show\nthat our framework significantly surpasses state-of-the-art models, especially\nin absent keyphrase prediction.\n", "link": "http://arxiv.org/abs/2410.03421v1", "date": "2024-10-04", "relevancy": 2.3846, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One2set%20%2B%20Large%20Language%20Model%3A%20Best%20Partners%20for%20Keyphrase%20Generation&body=Title%3A%20One2set%20%2B%20Large%20Language%20Model%3A%20Best%20Partners%20for%20Keyphrase%20Generation%0AAuthor%3A%20Liangying%20Shao%20and%20Liang%20Zhang%20and%20Minlong%20Peng%20and%20Guoqi%20Ma%20and%20Hao%20Yue%20and%20Mingming%20Sun%20and%20Jinsong%20Su%0AAbstract%3A%20%20%20Keyphrase%20generation%20%28KPG%29%20aims%20to%20automatically%20generate%20a%20collection%20of%0Aphrases%20representing%20the%20core%20concepts%20of%20a%20given%20document.%20The%20dominant%0Aparadigms%20in%20KPG%20include%20one2seq%20and%20one2set.%20Recently%2C%20there%20has%20been%0Aincreasing%20interest%20in%20applying%20large%20language%20models%20%28LLMs%29%20to%20KPG.%20Our%0Apreliminary%20experiments%20reveal%20that%20it%20is%20challenging%20for%20a%20single%20model%20to%0Aexcel%20in%20both%20recall%20and%20precision.%20Further%20analysis%20shows%20that%3A%201%29%20the%20one2set%0Aparadigm%20owns%20the%20advantage%20of%20high%20recall%2C%20but%20suffers%20from%20improper%0Aassignments%20of%20supervision%20signals%20during%20training%3B%202%29%20LLMs%20are%20powerful%20in%0Akeyphrase%20selection%2C%20but%20existing%20selection%20methods%20often%20make%20redundant%0Aselections.%20Given%20these%20observations%2C%20we%20introduce%20a%20generate-then-select%0Aframework%20decomposing%20KPG%20into%20two%20steps%2C%20where%20we%20adopt%20a%20one2set-based%20model%0Aas%20generator%20to%20produce%20candidates%20and%20then%20use%20an%20LLM%20as%20selector%20to%20select%0Akeyphrases%20from%20these%20candidates.%20Particularly%2C%20we%20make%20two%20important%0Aimprovements%20on%20our%20generator%20and%20selector%3A%201%29%20we%20design%20an%20Optimal%0ATransport-based%20assignment%20strategy%20to%20address%20the%20above%20improper%20assignments%3B%0A2%29%20we%20model%20the%20keyphrase%20selection%20as%20a%20sequence%20labeling%20task%20to%20alleviate%0Aredundant%20selections.%20Experimental%20results%20on%20multiple%20benchmark%20datasets%20show%0Athat%20our%20framework%20significantly%20surpasses%20state-of-the-art%20models%2C%20especially%0Ain%20absent%20keyphrase%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne2set%2520%252B%2520Large%2520Language%2520Model%253A%2520Best%2520Partners%2520for%2520Keyphrase%2520Generation%26entry.906535625%3DLiangying%2520Shao%2520and%2520Liang%2520Zhang%2520and%2520Minlong%2520Peng%2520and%2520Guoqi%2520Ma%2520and%2520Hao%2520Yue%2520and%2520Mingming%2520Sun%2520and%2520Jinsong%2520Su%26entry.1292438233%3D%2520%2520Keyphrase%2520generation%2520%2528KPG%2529%2520aims%2520to%2520automatically%2520generate%2520a%2520collection%2520of%250Aphrases%2520representing%2520the%2520core%2520concepts%2520of%2520a%2520given%2520document.%2520The%2520dominant%250Aparadigms%2520in%2520KPG%2520include%2520one2seq%2520and%2520one2set.%2520Recently%252C%2520there%2520has%2520been%250Aincreasing%2520interest%2520in%2520applying%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520KPG.%2520Our%250Apreliminary%2520experiments%2520reveal%2520that%2520it%2520is%2520challenging%2520for%2520a%2520single%2520model%2520to%250Aexcel%2520in%2520both%2520recall%2520and%2520precision.%2520Further%2520analysis%2520shows%2520that%253A%25201%2529%2520the%2520one2set%250Aparadigm%2520owns%2520the%2520advantage%2520of%2520high%2520recall%252C%2520but%2520suffers%2520from%2520improper%250Aassignments%2520of%2520supervision%2520signals%2520during%2520training%253B%25202%2529%2520LLMs%2520are%2520powerful%2520in%250Akeyphrase%2520selection%252C%2520but%2520existing%2520selection%2520methods%2520often%2520make%2520redundant%250Aselections.%2520Given%2520these%2520observations%252C%2520we%2520introduce%2520a%2520generate-then-select%250Aframework%2520decomposing%2520KPG%2520into%2520two%2520steps%252C%2520where%2520we%2520adopt%2520a%2520one2set-based%2520model%250Aas%2520generator%2520to%2520produce%2520candidates%2520and%2520then%2520use%2520an%2520LLM%2520as%2520selector%2520to%2520select%250Akeyphrases%2520from%2520these%2520candidates.%2520Particularly%252C%2520we%2520make%2520two%2520important%250Aimprovements%2520on%2520our%2520generator%2520and%2520selector%253A%25201%2529%2520we%2520design%2520an%2520Optimal%250ATransport-based%2520assignment%2520strategy%2520to%2520address%2520the%2520above%2520improper%2520assignments%253B%250A2%2529%2520we%2520model%2520the%2520keyphrase%2520selection%2520as%2520a%2520sequence%2520labeling%2520task%2520to%2520alleviate%250Aredundant%2520selections.%2520Experimental%2520results%2520on%2520multiple%2520benchmark%2520datasets%2520show%250Athat%2520our%2520framework%2520significantly%2520surpasses%2520state-of-the-art%2520models%252C%2520especially%250Ain%2520absent%2520keyphrase%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One2set%20%2B%20Large%20Language%20Model%3A%20Best%20Partners%20for%20Keyphrase%20Generation&entry.906535625=Liangying%20Shao%20and%20Liang%20Zhang%20and%20Minlong%20Peng%20and%20Guoqi%20Ma%20and%20Hao%20Yue%20and%20Mingming%20Sun%20and%20Jinsong%20Su&entry.1292438233=%20%20Keyphrase%20generation%20%28KPG%29%20aims%20to%20automatically%20generate%20a%20collection%20of%0Aphrases%20representing%20the%20core%20concepts%20of%20a%20given%20document.%20The%20dominant%0Aparadigms%20in%20KPG%20include%20one2seq%20and%20one2set.%20Recently%2C%20there%20has%20been%0Aincreasing%20interest%20in%20applying%20large%20language%20models%20%28LLMs%29%20to%20KPG.%20Our%0Apreliminary%20experiments%20reveal%20that%20it%20is%20challenging%20for%20a%20single%20model%20to%0Aexcel%20in%20both%20recall%20and%20precision.%20Further%20analysis%20shows%20that%3A%201%29%20the%20one2set%0Aparadigm%20owns%20the%20advantage%20of%20high%20recall%2C%20but%20suffers%20from%20improper%0Aassignments%20of%20supervision%20signals%20during%20training%3B%202%29%20LLMs%20are%20powerful%20in%0Akeyphrase%20selection%2C%20but%20existing%20selection%20methods%20often%20make%20redundant%0Aselections.%20Given%20these%20observations%2C%20we%20introduce%20a%20generate-then-select%0Aframework%20decomposing%20KPG%20into%20two%20steps%2C%20where%20we%20adopt%20a%20one2set-based%20model%0Aas%20generator%20to%20produce%20candidates%20and%20then%20use%20an%20LLM%20as%20selector%20to%20select%0Akeyphrases%20from%20these%20candidates.%20Particularly%2C%20we%20make%20two%20important%0Aimprovements%20on%20our%20generator%20and%20selector%3A%201%29%20we%20design%20an%20Optimal%0ATransport-based%20assignment%20strategy%20to%20address%20the%20above%20improper%20assignments%3B%0A2%29%20we%20model%20the%20keyphrase%20selection%20as%20a%20sequence%20labeling%20task%20to%20alleviate%0Aredundant%20selections.%20Experimental%20results%20on%20multiple%20benchmark%20datasets%20show%0Athat%20our%20framework%20significantly%20surpasses%20state-of-the-art%20models%2C%20especially%0Ain%20absent%20keyphrase%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03421v1&entry.124074799=Read"},
{"title": "Truncated Kernel Stochastic Gradient Descent on Spheres", "author": "JinHui Bai and Lei Shi", "abstract": "  Inspired by the structure of spherical harmonics, we propose the truncated\nkernel stochastic gradient descent (T-kernel SGD) algorithm with a least-square\nloss function for spherical data fitting. T-kernel SGD employs a \"truncation\"\noperation, enabling the application of series-based kernels function in\nstochastic gradient descent, thereby avoiding the difficulties of finding\nsuitable closed-form kernel functions in high-dimensional spaces.\n  In contrast to traditional kernel SGD, T-kernel SGD is more effective in\nbalancing bias and variance by dynamically adjusting the hypothesis space\nduring iterations. The most significant advantage of the proposed algorithm is\nthat it can achieve theoretically optimal convergence rates using a constant\nstep size (independent of the sample size) while overcoming the inherent\nsaturation problem of kernel SGD. Additionally, we leverage the structure of\nspherical polynomials to derive an equivalent T-kernel SGD, significantly\nreducing storage and computational costs compared to kernel SGD. Typically,\nT-kernel SGD requires only $\\mathcal{O}(n^{1+\\frac{d}{d-1}\\epsilon})$\ncomputational complexity and $\\mathcal{O}(n^{\\frac{d}{d-1}\\epsilon})$ storage\nto achieve optimal rates for the d-dimensional sphere, where\n$0<\\epsilon<\\frac{1}{2}$ can be arbitrarily small if the optimal fitting or the\nunderlying space possesses sufficient regularity. This regularity is determined\nby the smoothness parameter of the objective function and the decaying rate of\nthe eigenvalues of the integral operator associated with the kernel function,\nboth of which reflect the difficulty of the estimation problem. Our main\nresults quantitatively characterize how this prior information influences the\nconvergence of T-kernel SGD. The numerical experiments further validate the\ntheoretical findings presented in this paper.\n", "link": "http://arxiv.org/abs/2410.01570v2", "date": "2024-10-04", "relevancy": 2.3721, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4849}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4708}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20on%20Spheres&body=Title%3A%20Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20on%20Spheres%0AAuthor%3A%20JinHui%20Bai%20and%20Lei%20Shi%0AAbstract%3A%20%20%20Inspired%20by%20the%20structure%20of%20spherical%20harmonics%2C%20we%20propose%20the%20truncated%0Akernel%20stochastic%20gradient%20descent%20%28T-kernel%20SGD%29%20algorithm%20with%20a%20least-square%0Aloss%20function%20for%20spherical%20data%20fitting.%20T-kernel%20SGD%20employs%20a%20%22truncation%22%0Aoperation%2C%20enabling%20the%20application%20of%20series-based%20kernels%20function%20in%0Astochastic%20gradient%20descent%2C%20thereby%20avoiding%20the%20difficulties%20of%20finding%0Asuitable%20closed-form%20kernel%20functions%20in%20high-dimensional%20spaces.%0A%20%20In%20contrast%20to%20traditional%20kernel%20SGD%2C%20T-kernel%20SGD%20is%20more%20effective%20in%0Abalancing%20bias%20and%20variance%20by%20dynamically%20adjusting%20the%20hypothesis%20space%0Aduring%20iterations.%20The%20most%20significant%20advantage%20of%20the%20proposed%20algorithm%20is%0Athat%20it%20can%20achieve%20theoretically%20optimal%20convergence%20rates%20using%20a%20constant%0Astep%20size%20%28independent%20of%20the%20sample%20size%29%20while%20overcoming%20the%20inherent%0Asaturation%20problem%20of%20kernel%20SGD.%20Additionally%2C%20we%20leverage%20the%20structure%20of%0Aspherical%20polynomials%20to%20derive%20an%20equivalent%20T-kernel%20SGD%2C%20significantly%0Areducing%20storage%20and%20computational%20costs%20compared%20to%20kernel%20SGD.%20Typically%2C%0AT-kernel%20SGD%20requires%20only%20%24%5Cmathcal%7BO%7D%28n%5E%7B1%2B%5Cfrac%7Bd%7D%7Bd-1%7D%5Cepsilon%7D%29%24%0Acomputational%20complexity%20and%20%24%5Cmathcal%7BO%7D%28n%5E%7B%5Cfrac%7Bd%7D%7Bd-1%7D%5Cepsilon%7D%29%24%20storage%0Ato%20achieve%20optimal%20rates%20for%20the%20d-dimensional%20sphere%2C%20where%0A%240%3C%5Cepsilon%3C%5Cfrac%7B1%7D%7B2%7D%24%20can%20be%20arbitrarily%20small%20if%20the%20optimal%20fitting%20or%20the%0Aunderlying%20space%20possesses%20sufficient%20regularity.%20This%20regularity%20is%20determined%0Aby%20the%20smoothness%20parameter%20of%20the%20objective%20function%20and%20the%20decaying%20rate%20of%0Athe%20eigenvalues%20of%20the%20integral%20operator%20associated%20with%20the%20kernel%20function%2C%0Aboth%20of%20which%20reflect%20the%20difficulty%20of%20the%20estimation%20problem.%20Our%20main%0Aresults%20quantitatively%20characterize%20how%20this%20prior%20information%20influences%20the%0Aconvergence%20of%20T-kernel%20SGD.%20The%20numerical%20experiments%20further%20validate%20the%0Atheoretical%20findings%20presented%20in%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruncated%2520Kernel%2520Stochastic%2520Gradient%2520Descent%2520on%2520Spheres%26entry.906535625%3DJinHui%2520Bai%2520and%2520Lei%2520Shi%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520structure%2520of%2520spherical%2520harmonics%252C%2520we%2520propose%2520the%2520truncated%250Akernel%2520stochastic%2520gradient%2520descent%2520%2528T-kernel%2520SGD%2529%2520algorithm%2520with%2520a%2520least-square%250Aloss%2520function%2520for%2520spherical%2520data%2520fitting.%2520T-kernel%2520SGD%2520employs%2520a%2520%2522truncation%2522%250Aoperation%252C%2520enabling%2520the%2520application%2520of%2520series-based%2520kernels%2520function%2520in%250Astochastic%2520gradient%2520descent%252C%2520thereby%2520avoiding%2520the%2520difficulties%2520of%2520finding%250Asuitable%2520closed-form%2520kernel%2520functions%2520in%2520high-dimensional%2520spaces.%250A%2520%2520In%2520contrast%2520to%2520traditional%2520kernel%2520SGD%252C%2520T-kernel%2520SGD%2520is%2520more%2520effective%2520in%250Abalancing%2520bias%2520and%2520variance%2520by%2520dynamically%2520adjusting%2520the%2520hypothesis%2520space%250Aduring%2520iterations.%2520The%2520most%2520significant%2520advantage%2520of%2520the%2520proposed%2520algorithm%2520is%250Athat%2520it%2520can%2520achieve%2520theoretically%2520optimal%2520convergence%2520rates%2520using%2520a%2520constant%250Astep%2520size%2520%2528independent%2520of%2520the%2520sample%2520size%2529%2520while%2520overcoming%2520the%2520inherent%250Asaturation%2520problem%2520of%2520kernel%2520SGD.%2520Additionally%252C%2520we%2520leverage%2520the%2520structure%2520of%250Aspherical%2520polynomials%2520to%2520derive%2520an%2520equivalent%2520T-kernel%2520SGD%252C%2520significantly%250Areducing%2520storage%2520and%2520computational%2520costs%2520compared%2520to%2520kernel%2520SGD.%2520Typically%252C%250AT-kernel%2520SGD%2520requires%2520only%2520%2524%255Cmathcal%257BO%257D%2528n%255E%257B1%252B%255Cfrac%257Bd%257D%257Bd-1%257D%255Cepsilon%257D%2529%2524%250Acomputational%2520complexity%2520and%2520%2524%255Cmathcal%257BO%257D%2528n%255E%257B%255Cfrac%257Bd%257D%257Bd-1%257D%255Cepsilon%257D%2529%2524%2520storage%250Ato%2520achieve%2520optimal%2520rates%2520for%2520the%2520d-dimensional%2520sphere%252C%2520where%250A%25240%253C%255Cepsilon%253C%255Cfrac%257B1%257D%257B2%257D%2524%2520can%2520be%2520arbitrarily%2520small%2520if%2520the%2520optimal%2520fitting%2520or%2520the%250Aunderlying%2520space%2520possesses%2520sufficient%2520regularity.%2520This%2520regularity%2520is%2520determined%250Aby%2520the%2520smoothness%2520parameter%2520of%2520the%2520objective%2520function%2520and%2520the%2520decaying%2520rate%2520of%250Athe%2520eigenvalues%2520of%2520the%2520integral%2520operator%2520associated%2520with%2520the%2520kernel%2520function%252C%250Aboth%2520of%2520which%2520reflect%2520the%2520difficulty%2520of%2520the%2520estimation%2520problem.%2520Our%2520main%250Aresults%2520quantitatively%2520characterize%2520how%2520this%2520prior%2520information%2520influences%2520the%250Aconvergence%2520of%2520T-kernel%2520SGD.%2520The%2520numerical%2520experiments%2520further%2520validate%2520the%250Atheoretical%2520findings%2520presented%2520in%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20on%20Spheres&entry.906535625=JinHui%20Bai%20and%20Lei%20Shi&entry.1292438233=%20%20Inspired%20by%20the%20structure%20of%20spherical%20harmonics%2C%20we%20propose%20the%20truncated%0Akernel%20stochastic%20gradient%20descent%20%28T-kernel%20SGD%29%20algorithm%20with%20a%20least-square%0Aloss%20function%20for%20spherical%20data%20fitting.%20T-kernel%20SGD%20employs%20a%20%22truncation%22%0Aoperation%2C%20enabling%20the%20application%20of%20series-based%20kernels%20function%20in%0Astochastic%20gradient%20descent%2C%20thereby%20avoiding%20the%20difficulties%20of%20finding%0Asuitable%20closed-form%20kernel%20functions%20in%20high-dimensional%20spaces.%0A%20%20In%20contrast%20to%20traditional%20kernel%20SGD%2C%20T-kernel%20SGD%20is%20more%20effective%20in%0Abalancing%20bias%20and%20variance%20by%20dynamically%20adjusting%20the%20hypothesis%20space%0Aduring%20iterations.%20The%20most%20significant%20advantage%20of%20the%20proposed%20algorithm%20is%0Athat%20it%20can%20achieve%20theoretically%20optimal%20convergence%20rates%20using%20a%20constant%0Astep%20size%20%28independent%20of%20the%20sample%20size%29%20while%20overcoming%20the%20inherent%0Asaturation%20problem%20of%20kernel%20SGD.%20Additionally%2C%20we%20leverage%20the%20structure%20of%0Aspherical%20polynomials%20to%20derive%20an%20equivalent%20T-kernel%20SGD%2C%20significantly%0Areducing%20storage%20and%20computational%20costs%20compared%20to%20kernel%20SGD.%20Typically%2C%0AT-kernel%20SGD%20requires%20only%20%24%5Cmathcal%7BO%7D%28n%5E%7B1%2B%5Cfrac%7Bd%7D%7Bd-1%7D%5Cepsilon%7D%29%24%0Acomputational%20complexity%20and%20%24%5Cmathcal%7BO%7D%28n%5E%7B%5Cfrac%7Bd%7D%7Bd-1%7D%5Cepsilon%7D%29%24%20storage%0Ato%20achieve%20optimal%20rates%20for%20the%20d-dimensional%20sphere%2C%20where%0A%240%3C%5Cepsilon%3C%5Cfrac%7B1%7D%7B2%7D%24%20can%20be%20arbitrarily%20small%20if%20the%20optimal%20fitting%20or%20the%0Aunderlying%20space%20possesses%20sufficient%20regularity.%20This%20regularity%20is%20determined%0Aby%20the%20smoothness%20parameter%20of%20the%20objective%20function%20and%20the%20decaying%20rate%20of%0Athe%20eigenvalues%20of%20the%20integral%20operator%20associated%20with%20the%20kernel%20function%2C%0Aboth%20of%20which%20reflect%20the%20difficulty%20of%20the%20estimation%20problem.%20Our%20main%0Aresults%20quantitatively%20characterize%20how%20this%20prior%20information%20influences%20the%0Aconvergence%20of%20T-kernel%20SGD.%20The%20numerical%20experiments%20further%20validate%20the%0Atheoretical%20findings%20presented%20in%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01570v2&entry.124074799=Read"},
{"title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?", "author": "Hao Zhao and Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion", "abstract": "  In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.\n", "link": "http://arxiv.org/abs/2405.19874v2", "date": "2024-10-04", "relevancy": 2.3673, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20In-Context%20Learning%20Sufficient%20for%20Instruction%20Following%20in%20LLMs%3F&body=Title%3A%20Is%20In-Context%20Learning%20Sufficient%20for%20Instruction%20Following%20in%20LLMs%3F%0AAuthor%3A%20Hao%20Zhao%20and%20Maksym%20Andriushchenko%20and%20Francesco%20Croce%20and%20Nicolas%20Flammarion%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20allows%20LLMs%20to%20learn%20from%20examples%20without%20changing%0Atheir%20weights%3A%20this%20is%20a%20particularly%20promising%20capability%20for%20long-context%0ALLMs%20that%20can%20potentially%20learn%20from%20many%20examples.%20Recently%2C%20Lin%20et%20al.%20%282024%29%0Aproposed%20URIAL%2C%20a%20method%20using%20only%20three%20in-context%20examples%20to%20align%20base%0ALLMs%2C%20achieving%20non-trivial%20instruction%20following%20performance.%20In%20this%20work%2C%20we%0Ashow%20that%2C%20while%20effective%2C%20ICL%20alignment%20with%20URIAL%20still%20underperforms%0Acompared%20to%20instruction%20fine-tuning%20on%20the%20established%20benchmark%20MT-Bench%2C%0Aespecially%20with%20more%20capable%20base%20LLMs.%20We%20then%20uncover%20the%20most%20relevant%0Aelements%20for%20successful%20in-context%20alignment%2C%20finding%20the%20crucial%20role%20of%20the%0Adecoding%20parameters.%20Based%20on%20these%20insights%2C%20we%20show%20that%20the%20approach%20of%0AURIAL%20can%20indeed%20be%20improved%20by%20adding%20high-quality%2C%20potentially%20carefully%0Aselected%20via%20greedy%20search%2C%20demonstrations%20in%20context%2C%20getting%20closer%20to%20the%0Aperformance%20of%20instruct%20models.%20Finally%2C%20we%20provide%20the%20first%2C%20to%20our%0Aknowledge%2C%20systematic%20comparison%20of%20ICL%20and%20instruction%20fine-tuning%20%28IFT%29%20for%0Ainstruction%20following%20in%20the%20low%20data%20regime%2C%20where%20ICL%20can%20be%20a%20viable%0Aalternative%20to%20IFT.%20Overall%2C%20our%20work%20advances%20the%20understanding%20of%20ICL%20as%20an%0Aalignment%20technique%20and%20its%20relationship%20to%20IFT.%20We%20provide%20our%20code%20at%0Ahttps%3A//github.com/tml-epfl/icl-alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19874v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520In-Context%2520Learning%2520Sufficient%2520for%2520Instruction%2520Following%2520in%2520LLMs%253F%26entry.906535625%3DHao%2520Zhao%2520and%2520Maksym%2520Andriushchenko%2520and%2520Francesco%2520Croce%2520and%2520Nicolas%2520Flammarion%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520allows%2520LLMs%2520to%2520learn%2520from%2520examples%2520without%2520changing%250Atheir%2520weights%253A%2520this%2520is%2520a%2520particularly%2520promising%2520capability%2520for%2520long-context%250ALLMs%2520that%2520can%2520potentially%2520learn%2520from%2520many%2520examples.%2520Recently%252C%2520Lin%2520et%2520al.%2520%25282024%2529%250Aproposed%2520URIAL%252C%2520a%2520method%2520using%2520only%2520three%2520in-context%2520examples%2520to%2520align%2520base%250ALLMs%252C%2520achieving%2520non-trivial%2520instruction%2520following%2520performance.%2520In%2520this%2520work%252C%2520we%250Ashow%2520that%252C%2520while%2520effective%252C%2520ICL%2520alignment%2520with%2520URIAL%2520still%2520underperforms%250Acompared%2520to%2520instruction%2520fine-tuning%2520on%2520the%2520established%2520benchmark%2520MT-Bench%252C%250Aespecially%2520with%2520more%2520capable%2520base%2520LLMs.%2520We%2520then%2520uncover%2520the%2520most%2520relevant%250Aelements%2520for%2520successful%2520in-context%2520alignment%252C%2520finding%2520the%2520crucial%2520role%2520of%2520the%250Adecoding%2520parameters.%2520Based%2520on%2520these%2520insights%252C%2520we%2520show%2520that%2520the%2520approach%2520of%250AURIAL%2520can%2520indeed%2520be%2520improved%2520by%2520adding%2520high-quality%252C%2520potentially%2520carefully%250Aselected%2520via%2520greedy%2520search%252C%2520demonstrations%2520in%2520context%252C%2520getting%2520closer%2520to%2520the%250Aperformance%2520of%2520instruct%2520models.%2520Finally%252C%2520we%2520provide%2520the%2520first%252C%2520to%2520our%250Aknowledge%252C%2520systematic%2520comparison%2520of%2520ICL%2520and%2520instruction%2520fine-tuning%2520%2528IFT%2529%2520for%250Ainstruction%2520following%2520in%2520the%2520low%2520data%2520regime%252C%2520where%2520ICL%2520can%2520be%2520a%2520viable%250Aalternative%2520to%2520IFT.%2520Overall%252C%2520our%2520work%2520advances%2520the%2520understanding%2520of%2520ICL%2520as%2520an%250Aalignment%2520technique%2520and%2520its%2520relationship%2520to%2520IFT.%2520We%2520provide%2520our%2520code%2520at%250Ahttps%253A//github.com/tml-epfl/icl-alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19874v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20In-Context%20Learning%20Sufficient%20for%20Instruction%20Following%20in%20LLMs%3F&entry.906535625=Hao%20Zhao%20and%20Maksym%20Andriushchenko%20and%20Francesco%20Croce%20and%20Nicolas%20Flammarion&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20allows%20LLMs%20to%20learn%20from%20examples%20without%20changing%0Atheir%20weights%3A%20this%20is%20a%20particularly%20promising%20capability%20for%20long-context%0ALLMs%20that%20can%20potentially%20learn%20from%20many%20examples.%20Recently%2C%20Lin%20et%20al.%20%282024%29%0Aproposed%20URIAL%2C%20a%20method%20using%20only%20three%20in-context%20examples%20to%20align%20base%0ALLMs%2C%20achieving%20non-trivial%20instruction%20following%20performance.%20In%20this%20work%2C%20we%0Ashow%20that%2C%20while%20effective%2C%20ICL%20alignment%20with%20URIAL%20still%20underperforms%0Acompared%20to%20instruction%20fine-tuning%20on%20the%20established%20benchmark%20MT-Bench%2C%0Aespecially%20with%20more%20capable%20base%20LLMs.%20We%20then%20uncover%20the%20most%20relevant%0Aelements%20for%20successful%20in-context%20alignment%2C%20finding%20the%20crucial%20role%20of%20the%0Adecoding%20parameters.%20Based%20on%20these%20insights%2C%20we%20show%20that%20the%20approach%20of%0AURIAL%20can%20indeed%20be%20improved%20by%20adding%20high-quality%2C%20potentially%20carefully%0Aselected%20via%20greedy%20search%2C%20demonstrations%20in%20context%2C%20getting%20closer%20to%20the%0Aperformance%20of%20instruct%20models.%20Finally%2C%20we%20provide%20the%20first%2C%20to%20our%0Aknowledge%2C%20systematic%20comparison%20of%20ICL%20and%20instruction%20fine-tuning%20%28IFT%29%20for%0Ainstruction%20following%20in%20the%20low%20data%20regime%2C%20where%20ICL%20can%20be%20a%20viable%0Aalternative%20to%20IFT.%20Overall%2C%20our%20work%20advances%20the%20understanding%20of%20ICL%20as%20an%0Aalignment%20technique%20and%20its%20relationship%20to%20IFT.%20We%20provide%20our%20code%20at%0Ahttps%3A//github.com/tml-epfl/icl-alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19874v2&entry.124074799=Read"},
{"title": "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale\n  Extraction", "author": "Han Jiang and Junwen Duan and Zhe Qu and Jianxin Wang", "abstract": "  Unsupervised rationale extraction aims to extract text snippets to support\nmodel predictions without explicit rationale annotation. Researchers have made\nmany efforts to solve this task. Previous works often encode each aspect\nindependently, which may limit their ability to capture meaningful internal\ncorrelations between aspects. While there has been significant work on\nmitigating spurious correlations, our approach focuses on leveraging the\nbeneficial internal correlations to improve multi-aspect rationale extraction.\nIn this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain\nand predict multiple aspects simultaneously. Concretely, we propose a\nMulti-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to\nencode multiple text chunks simultaneously. Furthermore, multiple special\ntokens are prepended in front of the text with each corresponding to one\ncertain aspect. Finally, multi-task training is deployed to reduce the training\noverhead. Experimental results on two unsupervised rationale extraction\nbenchmarks show that MARE achieves state-of-the-art performance. Ablation\nstudies further demonstrate the effectiveness of our method. Our codes have\nbeen available at https://github.com/CSU-NLP-Group/MARE.\n", "link": "http://arxiv.org/abs/2410.03531v1", "date": "2024-10-04", "relevancy": 2.3583, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARE%3A%20Multi-Aspect%20Rationale%20Extractor%20on%20Unsupervised%20Rationale%0A%20%20Extraction&body=Title%3A%20MARE%3A%20Multi-Aspect%20Rationale%20Extractor%20on%20Unsupervised%20Rationale%0A%20%20Extraction%0AAuthor%3A%20Han%20Jiang%20and%20Junwen%20Duan%20and%20Zhe%20Qu%20and%20Jianxin%20Wang%0AAbstract%3A%20%20%20Unsupervised%20rationale%20extraction%20aims%20to%20extract%20text%20snippets%20to%20support%0Amodel%20predictions%20without%20explicit%20rationale%20annotation.%20Researchers%20have%20made%0Amany%20efforts%20to%20solve%20this%20task.%20Previous%20works%20often%20encode%20each%20aspect%0Aindependently%2C%20which%20may%20limit%20their%20ability%20to%20capture%20meaningful%20internal%0Acorrelations%20between%20aspects.%20While%20there%20has%20been%20significant%20work%20on%0Amitigating%20spurious%20correlations%2C%20our%20approach%20focuses%20on%20leveraging%20the%0Abeneficial%20internal%20correlations%20to%20improve%20multi-aspect%20rationale%20extraction.%0AIn%20this%20paper%2C%20we%20propose%20a%20Multi-Aspect%20Rationale%20Extractor%20%28MARE%29%20to%20explain%0Aand%20predict%20multiple%20aspects%20simultaneously.%20Concretely%2C%20we%20propose%20a%0AMulti-Aspect%20Multi-Head%20Attention%20%28MAMHA%29%20mechanism%20based%20on%20hard%20deletion%20to%0Aencode%20multiple%20text%20chunks%20simultaneously.%20Furthermore%2C%20multiple%20special%0Atokens%20are%20prepended%20in%20front%20of%20the%20text%20with%20each%20corresponding%20to%20one%0Acertain%20aspect.%20Finally%2C%20multi-task%20training%20is%20deployed%20to%20reduce%20the%20training%0Aoverhead.%20Experimental%20results%20on%20two%20unsupervised%20rationale%20extraction%0Abenchmarks%20show%20that%20MARE%20achieves%20state-of-the-art%20performance.%20Ablation%0Astudies%20further%20demonstrate%20the%20effectiveness%20of%20our%20method.%20Our%20codes%20have%0Abeen%20available%20at%20https%3A//github.com/CSU-NLP-Group/MARE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARE%253A%2520Multi-Aspect%2520Rationale%2520Extractor%2520on%2520Unsupervised%2520Rationale%250A%2520%2520Extraction%26entry.906535625%3DHan%2520Jiang%2520and%2520Junwen%2520Duan%2520and%2520Zhe%2520Qu%2520and%2520Jianxin%2520Wang%26entry.1292438233%3D%2520%2520Unsupervised%2520rationale%2520extraction%2520aims%2520to%2520extract%2520text%2520snippets%2520to%2520support%250Amodel%2520predictions%2520without%2520explicit%2520rationale%2520annotation.%2520Researchers%2520have%2520made%250Amany%2520efforts%2520to%2520solve%2520this%2520task.%2520Previous%2520works%2520often%2520encode%2520each%2520aspect%250Aindependently%252C%2520which%2520may%2520limit%2520their%2520ability%2520to%2520capture%2520meaningful%2520internal%250Acorrelations%2520between%2520aspects.%2520While%2520there%2520has%2520been%2520significant%2520work%2520on%250Amitigating%2520spurious%2520correlations%252C%2520our%2520approach%2520focuses%2520on%2520leveraging%2520the%250Abeneficial%2520internal%2520correlations%2520to%2520improve%2520multi-aspect%2520rationale%2520extraction.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520Multi-Aspect%2520Rationale%2520Extractor%2520%2528MARE%2529%2520to%2520explain%250Aand%2520predict%2520multiple%2520aspects%2520simultaneously.%2520Concretely%252C%2520we%2520propose%2520a%250AMulti-Aspect%2520Multi-Head%2520Attention%2520%2528MAMHA%2529%2520mechanism%2520based%2520on%2520hard%2520deletion%2520to%250Aencode%2520multiple%2520text%2520chunks%2520simultaneously.%2520Furthermore%252C%2520multiple%2520special%250Atokens%2520are%2520prepended%2520in%2520front%2520of%2520the%2520text%2520with%2520each%2520corresponding%2520to%2520one%250Acertain%2520aspect.%2520Finally%252C%2520multi-task%2520training%2520is%2520deployed%2520to%2520reduce%2520the%2520training%250Aoverhead.%2520Experimental%2520results%2520on%2520two%2520unsupervised%2520rationale%2520extraction%250Abenchmarks%2520show%2520that%2520MARE%2520achieves%2520state-of-the-art%2520performance.%2520Ablation%250Astudies%2520further%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%2520Our%2520codes%2520have%250Abeen%2520available%2520at%2520https%253A//github.com/CSU-NLP-Group/MARE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARE%3A%20Multi-Aspect%20Rationale%20Extractor%20on%20Unsupervised%20Rationale%0A%20%20Extraction&entry.906535625=Han%20Jiang%20and%20Junwen%20Duan%20and%20Zhe%20Qu%20and%20Jianxin%20Wang&entry.1292438233=%20%20Unsupervised%20rationale%20extraction%20aims%20to%20extract%20text%20snippets%20to%20support%0Amodel%20predictions%20without%20explicit%20rationale%20annotation.%20Researchers%20have%20made%0Amany%20efforts%20to%20solve%20this%20task.%20Previous%20works%20often%20encode%20each%20aspect%0Aindependently%2C%20which%20may%20limit%20their%20ability%20to%20capture%20meaningful%20internal%0Acorrelations%20between%20aspects.%20While%20there%20has%20been%20significant%20work%20on%0Amitigating%20spurious%20correlations%2C%20our%20approach%20focuses%20on%20leveraging%20the%0Abeneficial%20internal%20correlations%20to%20improve%20multi-aspect%20rationale%20extraction.%0AIn%20this%20paper%2C%20we%20propose%20a%20Multi-Aspect%20Rationale%20Extractor%20%28MARE%29%20to%20explain%0Aand%20predict%20multiple%20aspects%20simultaneously.%20Concretely%2C%20we%20propose%20a%0AMulti-Aspect%20Multi-Head%20Attention%20%28MAMHA%29%20mechanism%20based%20on%20hard%20deletion%20to%0Aencode%20multiple%20text%20chunks%20simultaneously.%20Furthermore%2C%20multiple%20special%0Atokens%20are%20prepended%20in%20front%20of%20the%20text%20with%20each%20corresponding%20to%20one%0Acertain%20aspect.%20Finally%2C%20multi-task%20training%20is%20deployed%20to%20reduce%20the%20training%0Aoverhead.%20Experimental%20results%20on%20two%20unsupervised%20rationale%20extraction%0Abenchmarks%20show%20that%20MARE%20achieves%20state-of-the-art%20performance.%20Ablation%0Astudies%20further%20demonstrate%20the%20effectiveness%20of%20our%20method.%20Our%20codes%20have%0Abeen%20available%20at%20https%3A//github.com/CSU-NLP-Group/MARE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03531v1&entry.124074799=Read"},
{"title": "RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State\n  Estimation Drift Mitigation on LiDAR PointCloud", "author": "Mohamed Nagy and Naoufel Werghi and Bilal Hassan and Jorge Dias and Majid Khonji", "abstract": "  This work addresses limitations in recent 3D tracking-by-detection methods,\nfocusing on identifying legitimate trajectories and addressing state estimation\ndrift in Kalman filters. Current methods rely heavily on threshold-based\nfiltering of false positive detections using detection scores to prevent ghost\ntrajectories. However, this approach is inadequate for distant and partially\noccluded objects, where detection scores tend to drop, potentially leading to\nfalse positives exceeding the threshold. Additionally, the literature generally\ntreats detections as precise localizations of objects. Our research reveals\nthat noise in detections impacts localization information, causing trajectory\ndrift for occluded objects and hindering recovery. To this end, we propose a\nnovel online track validity mechanism that temporally distinguishes between\nlegitimate and ghost tracks, along with a multi-stage observational gating\nprocess for incoming observations. This mechanism significantly improves\ntracking performance, with a $6.28\\%$ in HOTA and a $17.87\\%$ increase in MOTA.\nWe also introduce a refinement to the Kalman filter that enhances noise\nmitigation in trajectory drift, leading to more robust state estimation for\noccluded objects. Our framework, RobMOT, outperforms state-of-the-art methods,\nincluding deep learning approaches, across various detectors, achieving up to a\n$4\\%$ margin in HOTA and $6\\%$ in MOTA. RobMOT excels under challenging\nconditions, such as prolonged occlusions and tracking distant objects, with up\nto a 59\\% improvement in processing latency.\n", "link": "http://arxiv.org/abs/2405.11536v3", "date": "2024-10-04", "relevancy": 2.3577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6091}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.603}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobMOT%3A%20Robust%203D%20Multi-Object%20Tracking%20by%20Observational%20Noise%20and%20State%0A%20%20Estimation%20Drift%20Mitigation%20on%20LiDAR%20PointCloud&body=Title%3A%20RobMOT%3A%20Robust%203D%20Multi-Object%20Tracking%20by%20Observational%20Noise%20and%20State%0A%20%20Estimation%20Drift%20Mitigation%20on%20LiDAR%20PointCloud%0AAuthor%3A%20Mohamed%20Nagy%20and%20Naoufel%20Werghi%20and%20Bilal%20Hassan%20and%20Jorge%20Dias%20and%20Majid%20Khonji%0AAbstract%3A%20%20%20This%20work%20addresses%20limitations%20in%20recent%203D%20tracking-by-detection%20methods%2C%0Afocusing%20on%20identifying%20legitimate%20trajectories%20and%20addressing%20state%20estimation%0Adrift%20in%20Kalman%20filters.%20Current%20methods%20rely%20heavily%20on%20threshold-based%0Afiltering%20of%20false%20positive%20detections%20using%20detection%20scores%20to%20prevent%20ghost%0Atrajectories.%20However%2C%20this%20approach%20is%20inadequate%20for%20distant%20and%20partially%0Aoccluded%20objects%2C%20where%20detection%20scores%20tend%20to%20drop%2C%20potentially%20leading%20to%0Afalse%20positives%20exceeding%20the%20threshold.%20Additionally%2C%20the%20literature%20generally%0Atreats%20detections%20as%20precise%20localizations%20of%20objects.%20Our%20research%20reveals%0Athat%20noise%20in%20detections%20impacts%20localization%20information%2C%20causing%20trajectory%0Adrift%20for%20occluded%20objects%20and%20hindering%20recovery.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20online%20track%20validity%20mechanism%20that%20temporally%20distinguishes%20between%0Alegitimate%20and%20ghost%20tracks%2C%20along%20with%20a%20multi-stage%20observational%20gating%0Aprocess%20for%20incoming%20observations.%20This%20mechanism%20significantly%20improves%0Atracking%20performance%2C%20with%20a%20%246.28%5C%25%24%20in%20HOTA%20and%20a%20%2417.87%5C%25%24%20increase%20in%20MOTA.%0AWe%20also%20introduce%20a%20refinement%20to%20the%20Kalman%20filter%20that%20enhances%20noise%0Amitigation%20in%20trajectory%20drift%2C%20leading%20to%20more%20robust%20state%20estimation%20for%0Aoccluded%20objects.%20Our%20framework%2C%20RobMOT%2C%20outperforms%20state-of-the-art%20methods%2C%0Aincluding%20deep%20learning%20approaches%2C%20across%20various%20detectors%2C%20achieving%20up%20to%20a%0A%244%5C%25%24%20margin%20in%20HOTA%20and%20%246%5C%25%24%20in%20MOTA.%20RobMOT%20excels%20under%20challenging%0Aconditions%2C%20such%20as%20prolonged%20occlusions%20and%20tracking%20distant%20objects%2C%20with%20up%0Ato%20a%2059%5C%25%20improvement%20in%20processing%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11536v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobMOT%253A%2520Robust%25203D%2520Multi-Object%2520Tracking%2520by%2520Observational%2520Noise%2520and%2520State%250A%2520%2520Estimation%2520Drift%2520Mitigation%2520on%2520LiDAR%2520PointCloud%26entry.906535625%3DMohamed%2520Nagy%2520and%2520Naoufel%2520Werghi%2520and%2520Bilal%2520Hassan%2520and%2520Jorge%2520Dias%2520and%2520Majid%2520Khonji%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520limitations%2520in%2520recent%25203D%2520tracking-by-detection%2520methods%252C%250Afocusing%2520on%2520identifying%2520legitimate%2520trajectories%2520and%2520addressing%2520state%2520estimation%250Adrift%2520in%2520Kalman%2520filters.%2520Current%2520methods%2520rely%2520heavily%2520on%2520threshold-based%250Afiltering%2520of%2520false%2520positive%2520detections%2520using%2520detection%2520scores%2520to%2520prevent%2520ghost%250Atrajectories.%2520However%252C%2520this%2520approach%2520is%2520inadequate%2520for%2520distant%2520and%2520partially%250Aoccluded%2520objects%252C%2520where%2520detection%2520scores%2520tend%2520to%2520drop%252C%2520potentially%2520leading%2520to%250Afalse%2520positives%2520exceeding%2520the%2520threshold.%2520Additionally%252C%2520the%2520literature%2520generally%250Atreats%2520detections%2520as%2520precise%2520localizations%2520of%2520objects.%2520Our%2520research%2520reveals%250Athat%2520noise%2520in%2520detections%2520impacts%2520localization%2520information%252C%2520causing%2520trajectory%250Adrift%2520for%2520occluded%2520objects%2520and%2520hindering%2520recovery.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Anovel%2520online%2520track%2520validity%2520mechanism%2520that%2520temporally%2520distinguishes%2520between%250Alegitimate%2520and%2520ghost%2520tracks%252C%2520along%2520with%2520a%2520multi-stage%2520observational%2520gating%250Aprocess%2520for%2520incoming%2520observations.%2520This%2520mechanism%2520significantly%2520improves%250Atracking%2520performance%252C%2520with%2520a%2520%25246.28%255C%2525%2524%2520in%2520HOTA%2520and%2520a%2520%252417.87%255C%2525%2524%2520increase%2520in%2520MOTA.%250AWe%2520also%2520introduce%2520a%2520refinement%2520to%2520the%2520Kalman%2520filter%2520that%2520enhances%2520noise%250Amitigation%2520in%2520trajectory%2520drift%252C%2520leading%2520to%2520more%2520robust%2520state%2520estimation%2520for%250Aoccluded%2520objects.%2520Our%2520framework%252C%2520RobMOT%252C%2520outperforms%2520state-of-the-art%2520methods%252C%250Aincluding%2520deep%2520learning%2520approaches%252C%2520across%2520various%2520detectors%252C%2520achieving%2520up%2520to%2520a%250A%25244%255C%2525%2524%2520margin%2520in%2520HOTA%2520and%2520%25246%255C%2525%2524%2520in%2520MOTA.%2520RobMOT%2520excels%2520under%2520challenging%250Aconditions%252C%2520such%2520as%2520prolonged%2520occlusions%2520and%2520tracking%2520distant%2520objects%252C%2520with%2520up%250Ato%2520a%252059%255C%2525%2520improvement%2520in%2520processing%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11536v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobMOT%3A%20Robust%203D%20Multi-Object%20Tracking%20by%20Observational%20Noise%20and%20State%0A%20%20Estimation%20Drift%20Mitigation%20on%20LiDAR%20PointCloud&entry.906535625=Mohamed%20Nagy%20and%20Naoufel%20Werghi%20and%20Bilal%20Hassan%20and%20Jorge%20Dias%20and%20Majid%20Khonji&entry.1292438233=%20%20This%20work%20addresses%20limitations%20in%20recent%203D%20tracking-by-detection%20methods%2C%0Afocusing%20on%20identifying%20legitimate%20trajectories%20and%20addressing%20state%20estimation%0Adrift%20in%20Kalman%20filters.%20Current%20methods%20rely%20heavily%20on%20threshold-based%0Afiltering%20of%20false%20positive%20detections%20using%20detection%20scores%20to%20prevent%20ghost%0Atrajectories.%20However%2C%20this%20approach%20is%20inadequate%20for%20distant%20and%20partially%0Aoccluded%20objects%2C%20where%20detection%20scores%20tend%20to%20drop%2C%20potentially%20leading%20to%0Afalse%20positives%20exceeding%20the%20threshold.%20Additionally%2C%20the%20literature%20generally%0Atreats%20detections%20as%20precise%20localizations%20of%20objects.%20Our%20research%20reveals%0Athat%20noise%20in%20detections%20impacts%20localization%20information%2C%20causing%20trajectory%0Adrift%20for%20occluded%20objects%20and%20hindering%20recovery.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20online%20track%20validity%20mechanism%20that%20temporally%20distinguishes%20between%0Alegitimate%20and%20ghost%20tracks%2C%20along%20with%20a%20multi-stage%20observational%20gating%0Aprocess%20for%20incoming%20observations.%20This%20mechanism%20significantly%20improves%0Atracking%20performance%2C%20with%20a%20%246.28%5C%25%24%20in%20HOTA%20and%20a%20%2417.87%5C%25%24%20increase%20in%20MOTA.%0AWe%20also%20introduce%20a%20refinement%20to%20the%20Kalman%20filter%20that%20enhances%20noise%0Amitigation%20in%20trajectory%20drift%2C%20leading%20to%20more%20robust%20state%20estimation%20for%0Aoccluded%20objects.%20Our%20framework%2C%20RobMOT%2C%20outperforms%20state-of-the-art%20methods%2C%0Aincluding%20deep%20learning%20approaches%2C%20across%20various%20detectors%2C%20achieving%20up%20to%20a%0A%244%5C%25%24%20margin%20in%20HOTA%20and%20%246%5C%25%24%20in%20MOTA.%20RobMOT%20excels%20under%20challenging%0Aconditions%2C%20such%20as%20prolonged%20occlusions%20and%20tracking%20distant%20objects%2C%20with%20up%0Ato%20a%2059%5C%25%20improvement%20in%20processing%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11536v3&entry.124074799=Read"},
{"title": "Rapid and Precise Topological Comparison with Merge Tree Neural Networks", "author": "Yu Qin and Brittany Terese Fasy and Carola Wenk and Brian Summa", "abstract": "  Merge trees are a valuable tool in the scientific visualization of scalar\nfields; however, current methods for merge tree comparisons are computationally\nexpensive, primarily due to the exhaustive matching between tree nodes. To\naddress this challenge, we introduce the Merge Tree Neural Network (MTNN), a\nlearned neural network model designed for merge tree comparison. The MTNN\nenables rapid and high-quality similarity computation. We first demonstrate how\nto train graph neural networks, which emerged as effective encoders for graphs,\nin order to produce embeddings of merge trees in vector spaces for efficient\nsimilarity comparison. Next, we formulate the novel MTNN model that further\nimproves the similarity comparisons by integrating the tree and node embeddings\nwith a new topological attention mechanism. We demonstrate the effectiveness of\nour model on real-world data in different domains and examine our model's\ngeneralizability across various datasets. Our experimental analysis\ndemonstrates our approach's superiority in accuracy and efficiency. In\nparticular, we speed up the prior state-of-the-art by more than $100\\times$ on\nthe benchmark datasets while maintaining an error rate below $0.1\\%$.\n", "link": "http://arxiv.org/abs/2404.05879v3", "date": "2024-10-04", "relevancy": 2.3273, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4638}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20and%20Precise%20Topological%20Comparison%20with%20Merge%20Tree%20Neural%20Networks&body=Title%3A%20Rapid%20and%20Precise%20Topological%20Comparison%20with%20Merge%20Tree%20Neural%20Networks%0AAuthor%3A%20Yu%20Qin%20and%20Brittany%20Terese%20Fasy%20and%20Carola%20Wenk%20and%20Brian%20Summa%0AAbstract%3A%20%20%20Merge%20trees%20are%20a%20valuable%20tool%20in%20the%20scientific%20visualization%20of%20scalar%0Afields%3B%20however%2C%20current%20methods%20for%20merge%20tree%20comparisons%20are%20computationally%0Aexpensive%2C%20primarily%20due%20to%20the%20exhaustive%20matching%20between%20tree%20nodes.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20the%20Merge%20Tree%20Neural%20Network%20%28MTNN%29%2C%20a%0Alearned%20neural%20network%20model%20designed%20for%20merge%20tree%20comparison.%20The%20MTNN%0Aenables%20rapid%20and%20high-quality%20similarity%20computation.%20We%20first%20demonstrate%20how%0Ato%20train%20graph%20neural%20networks%2C%20which%20emerged%20as%20effective%20encoders%20for%20graphs%2C%0Ain%20order%20to%20produce%20embeddings%20of%20merge%20trees%20in%20vector%20spaces%20for%20efficient%0Asimilarity%20comparison.%20Next%2C%20we%20formulate%20the%20novel%20MTNN%20model%20that%20further%0Aimproves%20the%20similarity%20comparisons%20by%20integrating%20the%20tree%20and%20node%20embeddings%0Awith%20a%20new%20topological%20attention%20mechanism.%20We%20demonstrate%20the%20effectiveness%20of%0Aour%20model%20on%20real-world%20data%20in%20different%20domains%20and%20examine%20our%20model%27s%0Ageneralizability%20across%20various%20datasets.%20Our%20experimental%20analysis%0Ademonstrates%20our%20approach%27s%20superiority%20in%20accuracy%20and%20efficiency.%20In%0Aparticular%2C%20we%20speed%20up%20the%20prior%20state-of-the-art%20by%20more%20than%20%24100%5Ctimes%24%20on%0Athe%20benchmark%20datasets%20while%20maintaining%20an%20error%20rate%20below%20%240.1%5C%25%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05879v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520and%2520Precise%2520Topological%2520Comparison%2520with%2520Merge%2520Tree%2520Neural%2520Networks%26entry.906535625%3DYu%2520Qin%2520and%2520Brittany%2520Terese%2520Fasy%2520and%2520Carola%2520Wenk%2520and%2520Brian%2520Summa%26entry.1292438233%3D%2520%2520Merge%2520trees%2520are%2520a%2520valuable%2520tool%2520in%2520the%2520scientific%2520visualization%2520of%2520scalar%250Afields%253B%2520however%252C%2520current%2520methods%2520for%2520merge%2520tree%2520comparisons%2520are%2520computationally%250Aexpensive%252C%2520primarily%2520due%2520to%2520the%2520exhaustive%2520matching%2520between%2520tree%2520nodes.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520the%2520Merge%2520Tree%2520Neural%2520Network%2520%2528MTNN%2529%252C%2520a%250Alearned%2520neural%2520network%2520model%2520designed%2520for%2520merge%2520tree%2520comparison.%2520The%2520MTNN%250Aenables%2520rapid%2520and%2520high-quality%2520similarity%2520computation.%2520We%2520first%2520demonstrate%2520how%250Ato%2520train%2520graph%2520neural%2520networks%252C%2520which%2520emerged%2520as%2520effective%2520encoders%2520for%2520graphs%252C%250Ain%2520order%2520to%2520produce%2520embeddings%2520of%2520merge%2520trees%2520in%2520vector%2520spaces%2520for%2520efficient%250Asimilarity%2520comparison.%2520Next%252C%2520we%2520formulate%2520the%2520novel%2520MTNN%2520model%2520that%2520further%250Aimproves%2520the%2520similarity%2520comparisons%2520by%2520integrating%2520the%2520tree%2520and%2520node%2520embeddings%250Awith%2520a%2520new%2520topological%2520attention%2520mechanism.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520model%2520on%2520real-world%2520data%2520in%2520different%2520domains%2520and%2520examine%2520our%2520model%2527s%250Ageneralizability%2520across%2520various%2520datasets.%2520Our%2520experimental%2520analysis%250Ademonstrates%2520our%2520approach%2527s%2520superiority%2520in%2520accuracy%2520and%2520efficiency.%2520In%250Aparticular%252C%2520we%2520speed%2520up%2520the%2520prior%2520state-of-the-art%2520by%2520more%2520than%2520%2524100%255Ctimes%2524%2520on%250Athe%2520benchmark%2520datasets%2520while%2520maintaining%2520an%2520error%2520rate%2520below%2520%25240.1%255C%2525%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05879v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20and%20Precise%20Topological%20Comparison%20with%20Merge%20Tree%20Neural%20Networks&entry.906535625=Yu%20Qin%20and%20Brittany%20Terese%20Fasy%20and%20Carola%20Wenk%20and%20Brian%20Summa&entry.1292438233=%20%20Merge%20trees%20are%20a%20valuable%20tool%20in%20the%20scientific%20visualization%20of%20scalar%0Afields%3B%20however%2C%20current%20methods%20for%20merge%20tree%20comparisons%20are%20computationally%0Aexpensive%2C%20primarily%20due%20to%20the%20exhaustive%20matching%20between%20tree%20nodes.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20the%20Merge%20Tree%20Neural%20Network%20%28MTNN%29%2C%20a%0Alearned%20neural%20network%20model%20designed%20for%20merge%20tree%20comparison.%20The%20MTNN%0Aenables%20rapid%20and%20high-quality%20similarity%20computation.%20We%20first%20demonstrate%20how%0Ato%20train%20graph%20neural%20networks%2C%20which%20emerged%20as%20effective%20encoders%20for%20graphs%2C%0Ain%20order%20to%20produce%20embeddings%20of%20merge%20trees%20in%20vector%20spaces%20for%20efficient%0Asimilarity%20comparison.%20Next%2C%20we%20formulate%20the%20novel%20MTNN%20model%20that%20further%0Aimproves%20the%20similarity%20comparisons%20by%20integrating%20the%20tree%20and%20node%20embeddings%0Awith%20a%20new%20topological%20attention%20mechanism.%20We%20demonstrate%20the%20effectiveness%20of%0Aour%20model%20on%20real-world%20data%20in%20different%20domains%20and%20examine%20our%20model%27s%0Ageneralizability%20across%20various%20datasets.%20Our%20experimental%20analysis%0Ademonstrates%20our%20approach%27s%20superiority%20in%20accuracy%20and%20efficiency.%20In%0Aparticular%2C%20we%20speed%20up%20the%20prior%20state-of-the-art%20by%20more%20than%20%24100%5Ctimes%24%20on%0Athe%20benchmark%20datasets%20while%20maintaining%20an%20error%20rate%20below%20%240.1%5C%25%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05879v3&entry.124074799=Read"},
{"title": "Towards Linguistically-Aware and Language-Independent Tokenization for\n  Large Language Models (LLMs)", "author": "Abrar Rahman and Garry Bowlin and Binit Mohanty and Sean McGunigal", "abstract": "  This paper presents a comprehensive study on the tokenization techniques\nemployed by state-of-the-art large language models (LLMs) and their\nimplications on the cost and availability of services across different\nlanguages, especially low resource languages. The analysis considers multiple\nLLMs, including GPT-4 (using cl100k_base embeddings), GPT-3 (with p50k_base\nembeddings), and DaVinci (employing r50k_base embeddings), as well as the\nwidely used BERT base tokenizer. The study evaluates the tokenization\nvariability observed across these models and investigates the challenges of\nlinguistic representation in subword tokenization. The research underscores the\nimportance of fostering linguistically-aware development practices, especially\nfor languages that are traditionally under-resourced. Moreover, this paper\nintroduces case studies that highlight the real-world implications of\ntokenization choices, particularly in the context of electronic health record\n(EHR) systems. This research aims to promote generalizable Internationalization\n(I18N) practices in the development of AI services in this domain and beyond,\nwith a strong emphasis on inclusivity, particularly for languages traditionally\nunderrepresented in AI applications.\n", "link": "http://arxiv.org/abs/2410.03568v1", "date": "2024-10-04", "relevancy": 2.3092, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Linguistically-Aware%20and%20Language-Independent%20Tokenization%20for%0A%20%20Large%20Language%20Models%20%28LLMs%29&body=Title%3A%20Towards%20Linguistically-Aware%20and%20Language-Independent%20Tokenization%20for%0A%20%20Large%20Language%20Models%20%28LLMs%29%0AAuthor%3A%20Abrar%20Rahman%20and%20Garry%20Bowlin%20and%20Binit%20Mohanty%20and%20Sean%20McGunigal%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20study%20on%20the%20tokenization%20techniques%0Aemployed%20by%20state-of-the-art%20large%20language%20models%20%28LLMs%29%20and%20their%0Aimplications%20on%20the%20cost%20and%20availability%20of%20services%20across%20different%0Alanguages%2C%20especially%20low%20resource%20languages.%20The%20analysis%20considers%20multiple%0ALLMs%2C%20including%20GPT-4%20%28using%20cl100k_base%20embeddings%29%2C%20GPT-3%20%28with%20p50k_base%0Aembeddings%29%2C%20and%20DaVinci%20%28employing%20r50k_base%20embeddings%29%2C%20as%20well%20as%20the%0Awidely%20used%20BERT%20base%20tokenizer.%20The%20study%20evaluates%20the%20tokenization%0Avariability%20observed%20across%20these%20models%20and%20investigates%20the%20challenges%20of%0Alinguistic%20representation%20in%20subword%20tokenization.%20The%20research%20underscores%20the%0Aimportance%20of%20fostering%20linguistically-aware%20development%20practices%2C%20especially%0Afor%20languages%20that%20are%20traditionally%20under-resourced.%20Moreover%2C%20this%20paper%0Aintroduces%20case%20studies%20that%20highlight%20the%20real-world%20implications%20of%0Atokenization%20choices%2C%20particularly%20in%20the%20context%20of%20electronic%20health%20record%0A%28EHR%29%20systems.%20This%20research%20aims%20to%20promote%20generalizable%20Internationalization%0A%28I18N%29%20practices%20in%20the%20development%20of%20AI%20services%20in%20this%20domain%20and%20beyond%2C%0Awith%20a%20strong%20emphasis%20on%20inclusivity%2C%20particularly%20for%20languages%20traditionally%0Aunderrepresented%20in%20AI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Linguistically-Aware%2520and%2520Language-Independent%2520Tokenization%2520for%250A%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%26entry.906535625%3DAbrar%2520Rahman%2520and%2520Garry%2520Bowlin%2520and%2520Binit%2520Mohanty%2520and%2520Sean%2520McGunigal%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520study%2520on%2520the%2520tokenization%2520techniques%250Aemployed%2520by%2520state-of-the-art%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520their%250Aimplications%2520on%2520the%2520cost%2520and%2520availability%2520of%2520services%2520across%2520different%250Alanguages%252C%2520especially%2520low%2520resource%2520languages.%2520The%2520analysis%2520considers%2520multiple%250ALLMs%252C%2520including%2520GPT-4%2520%2528using%2520cl100k_base%2520embeddings%2529%252C%2520GPT-3%2520%2528with%2520p50k_base%250Aembeddings%2529%252C%2520and%2520DaVinci%2520%2528employing%2520r50k_base%2520embeddings%2529%252C%2520as%2520well%2520as%2520the%250Awidely%2520used%2520BERT%2520base%2520tokenizer.%2520The%2520study%2520evaluates%2520the%2520tokenization%250Avariability%2520observed%2520across%2520these%2520models%2520and%2520investigates%2520the%2520challenges%2520of%250Alinguistic%2520representation%2520in%2520subword%2520tokenization.%2520The%2520research%2520underscores%2520the%250Aimportance%2520of%2520fostering%2520linguistically-aware%2520development%2520practices%252C%2520especially%250Afor%2520languages%2520that%2520are%2520traditionally%2520under-resourced.%2520Moreover%252C%2520this%2520paper%250Aintroduces%2520case%2520studies%2520that%2520highlight%2520the%2520real-world%2520implications%2520of%250Atokenization%2520choices%252C%2520particularly%2520in%2520the%2520context%2520of%2520electronic%2520health%2520record%250A%2528EHR%2529%2520systems.%2520This%2520research%2520aims%2520to%2520promote%2520generalizable%2520Internationalization%250A%2528I18N%2529%2520practices%2520in%2520the%2520development%2520of%2520AI%2520services%2520in%2520this%2520domain%2520and%2520beyond%252C%250Awith%2520a%2520strong%2520emphasis%2520on%2520inclusivity%252C%2520particularly%2520for%2520languages%2520traditionally%250Aunderrepresented%2520in%2520AI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Linguistically-Aware%20and%20Language-Independent%20Tokenization%20for%0A%20%20Large%20Language%20Models%20%28LLMs%29&entry.906535625=Abrar%20Rahman%20and%20Garry%20Bowlin%20and%20Binit%20Mohanty%20and%20Sean%20McGunigal&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20study%20on%20the%20tokenization%20techniques%0Aemployed%20by%20state-of-the-art%20large%20language%20models%20%28LLMs%29%20and%20their%0Aimplications%20on%20the%20cost%20and%20availability%20of%20services%20across%20different%0Alanguages%2C%20especially%20low%20resource%20languages.%20The%20analysis%20considers%20multiple%0ALLMs%2C%20including%20GPT-4%20%28using%20cl100k_base%20embeddings%29%2C%20GPT-3%20%28with%20p50k_base%0Aembeddings%29%2C%20and%20DaVinci%20%28employing%20r50k_base%20embeddings%29%2C%20as%20well%20as%20the%0Awidely%20used%20BERT%20base%20tokenizer.%20The%20study%20evaluates%20the%20tokenization%0Avariability%20observed%20across%20these%20models%20and%20investigates%20the%20challenges%20of%0Alinguistic%20representation%20in%20subword%20tokenization.%20The%20research%20underscores%20the%0Aimportance%20of%20fostering%20linguistically-aware%20development%20practices%2C%20especially%0Afor%20languages%20that%20are%20traditionally%20under-resourced.%20Moreover%2C%20this%20paper%0Aintroduces%20case%20studies%20that%20highlight%20the%20real-world%20implications%20of%0Atokenization%20choices%2C%20particularly%20in%20the%20context%20of%20electronic%20health%20record%0A%28EHR%29%20systems.%20This%20research%20aims%20to%20promote%20generalizable%20Internationalization%0A%28I18N%29%20practices%20in%20the%20development%20of%20AI%20services%20in%20this%20domain%20and%20beyond%2C%0Awith%20a%20strong%20emphasis%20on%20inclusivity%2C%20particularly%20for%20languages%20traditionally%0Aunderrepresented%20in%20AI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03568v1&entry.124074799=Read"},
{"title": "Lost in Tracking: Uncertainty-guided Cardiac Cine MRI Segmentation at\n  Right Ventricle Base", "author": "Yidong Zhao and Yi Zhang and Orlando Simonetti and Yuchi Han and Qian Tao", "abstract": "  Accurate biventricular segmentation of cardiac magnetic resonance (CMR) cine\nimages is essential for the clinical evaluation of heart function. However,\ncompared to left ventricle (LV), right ventricle (RV) segmentation is still\nmore challenging and less reproducible. Degenerate performance frequently\noccurs at the RV base, where the in-plane anatomical structures are complex\n(with atria, valve, and aorta) and vary due to the strong interplanar motion.\nIn this work, we propose to address the currently unsolved issues in CMR\nsegmentation, specifically at the RV base, with two strategies: first, we\ncomplemented the public resource by reannotating the RV base in the ACDC\ndataset, with refined delineation of the right ventricle outflow tract (RVOT),\nunder the guidance of an expert cardiologist. Second, we proposed a novel dual\nencoder U-Net architecture that leverages temporal incoherence to inform the\nsegmentation when interplanar motions occur. The inter-planar motion is\ncharacterized by loss-of-tracking, via Bayesian uncertainty of a\nmotion-tracking model. Our experiments showed that our method significantly\nimproved RV base segmentation taking into account temporal incoherence.\nFurthermore, we investigated the reproducibility of deep learning-based\nsegmentation and showed that the combination of consistent annotation and loss\nof tracking could enhance the reproducibility of RV segmentation, potentially\nfacilitating a large number of clinical studies focusing on RV.\n", "link": "http://arxiv.org/abs/2410.03320v1", "date": "2024-10-04", "relevancy": 2.2957, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6006}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5699}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Tracking%3A%20Uncertainty-guided%20Cardiac%20Cine%20MRI%20Segmentation%20at%0A%20%20Right%20Ventricle%20Base&body=Title%3A%20Lost%20in%20Tracking%3A%20Uncertainty-guided%20Cardiac%20Cine%20MRI%20Segmentation%20at%0A%20%20Right%20Ventricle%20Base%0AAuthor%3A%20Yidong%20Zhao%20and%20Yi%20Zhang%20and%20Orlando%20Simonetti%20and%20Yuchi%20Han%20and%20Qian%20Tao%0AAbstract%3A%20%20%20Accurate%20biventricular%20segmentation%20of%20cardiac%20magnetic%20resonance%20%28CMR%29%20cine%0Aimages%20is%20essential%20for%20the%20clinical%20evaluation%20of%20heart%20function.%20However%2C%0Acompared%20to%20left%20ventricle%20%28LV%29%2C%20right%20ventricle%20%28RV%29%20segmentation%20is%20still%0Amore%20challenging%20and%20less%20reproducible.%20Degenerate%20performance%20frequently%0Aoccurs%20at%20the%20RV%20base%2C%20where%20the%20in-plane%20anatomical%20structures%20are%20complex%0A%28with%20atria%2C%20valve%2C%20and%20aorta%29%20and%20vary%20due%20to%20the%20strong%20interplanar%20motion.%0AIn%20this%20work%2C%20we%20propose%20to%20address%20the%20currently%20unsolved%20issues%20in%20CMR%0Asegmentation%2C%20specifically%20at%20the%20RV%20base%2C%20with%20two%20strategies%3A%20first%2C%20we%0Acomplemented%20the%20public%20resource%20by%20reannotating%20the%20RV%20base%20in%20the%20ACDC%0Adataset%2C%20with%20refined%20delineation%20of%20the%20right%20ventricle%20outflow%20tract%20%28RVOT%29%2C%0Aunder%20the%20guidance%20of%20an%20expert%20cardiologist.%20Second%2C%20we%20proposed%20a%20novel%20dual%0Aencoder%20U-Net%20architecture%20that%20leverages%20temporal%20incoherence%20to%20inform%20the%0Asegmentation%20when%20interplanar%20motions%20occur.%20The%20inter-planar%20motion%20is%0Acharacterized%20by%20loss-of-tracking%2C%20via%20Bayesian%20uncertainty%20of%20a%0Amotion-tracking%20model.%20Our%20experiments%20showed%20that%20our%20method%20significantly%0Aimproved%20RV%20base%20segmentation%20taking%20into%20account%20temporal%20incoherence.%0AFurthermore%2C%20we%20investigated%20the%20reproducibility%20of%20deep%20learning-based%0Asegmentation%20and%20showed%20that%20the%20combination%20of%20consistent%20annotation%20and%20loss%0Aof%20tracking%20could%20enhance%20the%20reproducibility%20of%20RV%20segmentation%2C%20potentially%0Afacilitating%20a%20large%20number%20of%20clinical%20studies%20focusing%20on%20RV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Tracking%253A%2520Uncertainty-guided%2520Cardiac%2520Cine%2520MRI%2520Segmentation%2520at%250A%2520%2520Right%2520Ventricle%2520Base%26entry.906535625%3DYidong%2520Zhao%2520and%2520Yi%2520Zhang%2520and%2520Orlando%2520Simonetti%2520and%2520Yuchi%2520Han%2520and%2520Qian%2520Tao%26entry.1292438233%3D%2520%2520Accurate%2520biventricular%2520segmentation%2520of%2520cardiac%2520magnetic%2520resonance%2520%2528CMR%2529%2520cine%250Aimages%2520is%2520essential%2520for%2520the%2520clinical%2520evaluation%2520of%2520heart%2520function.%2520However%252C%250Acompared%2520to%2520left%2520ventricle%2520%2528LV%2529%252C%2520right%2520ventricle%2520%2528RV%2529%2520segmentation%2520is%2520still%250Amore%2520challenging%2520and%2520less%2520reproducible.%2520Degenerate%2520performance%2520frequently%250Aoccurs%2520at%2520the%2520RV%2520base%252C%2520where%2520the%2520in-plane%2520anatomical%2520structures%2520are%2520complex%250A%2528with%2520atria%252C%2520valve%252C%2520and%2520aorta%2529%2520and%2520vary%2520due%2520to%2520the%2520strong%2520interplanar%2520motion.%250AIn%2520this%2520work%252C%2520we%2520propose%2520to%2520address%2520the%2520currently%2520unsolved%2520issues%2520in%2520CMR%250Asegmentation%252C%2520specifically%2520at%2520the%2520RV%2520base%252C%2520with%2520two%2520strategies%253A%2520first%252C%2520we%250Acomplemented%2520the%2520public%2520resource%2520by%2520reannotating%2520the%2520RV%2520base%2520in%2520the%2520ACDC%250Adataset%252C%2520with%2520refined%2520delineation%2520of%2520the%2520right%2520ventricle%2520outflow%2520tract%2520%2528RVOT%2529%252C%250Aunder%2520the%2520guidance%2520of%2520an%2520expert%2520cardiologist.%2520Second%252C%2520we%2520proposed%2520a%2520novel%2520dual%250Aencoder%2520U-Net%2520architecture%2520that%2520leverages%2520temporal%2520incoherence%2520to%2520inform%2520the%250Asegmentation%2520when%2520interplanar%2520motions%2520occur.%2520The%2520inter-planar%2520motion%2520is%250Acharacterized%2520by%2520loss-of-tracking%252C%2520via%2520Bayesian%2520uncertainty%2520of%2520a%250Amotion-tracking%2520model.%2520Our%2520experiments%2520showed%2520that%2520our%2520method%2520significantly%250Aimproved%2520RV%2520base%2520segmentation%2520taking%2520into%2520account%2520temporal%2520incoherence.%250AFurthermore%252C%2520we%2520investigated%2520the%2520reproducibility%2520of%2520deep%2520learning-based%250Asegmentation%2520and%2520showed%2520that%2520the%2520combination%2520of%2520consistent%2520annotation%2520and%2520loss%250Aof%2520tracking%2520could%2520enhance%2520the%2520reproducibility%2520of%2520RV%2520segmentation%252C%2520potentially%250Afacilitating%2520a%2520large%2520number%2520of%2520clinical%2520studies%2520focusing%2520on%2520RV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Tracking%3A%20Uncertainty-guided%20Cardiac%20Cine%20MRI%20Segmentation%20at%0A%20%20Right%20Ventricle%20Base&entry.906535625=Yidong%20Zhao%20and%20Yi%20Zhang%20and%20Orlando%20Simonetti%20and%20Yuchi%20Han%20and%20Qian%20Tao&entry.1292438233=%20%20Accurate%20biventricular%20segmentation%20of%20cardiac%20magnetic%20resonance%20%28CMR%29%20cine%0Aimages%20is%20essential%20for%20the%20clinical%20evaluation%20of%20heart%20function.%20However%2C%0Acompared%20to%20left%20ventricle%20%28LV%29%2C%20right%20ventricle%20%28RV%29%20segmentation%20is%20still%0Amore%20challenging%20and%20less%20reproducible.%20Degenerate%20performance%20frequently%0Aoccurs%20at%20the%20RV%20base%2C%20where%20the%20in-plane%20anatomical%20structures%20are%20complex%0A%28with%20atria%2C%20valve%2C%20and%20aorta%29%20and%20vary%20due%20to%20the%20strong%20interplanar%20motion.%0AIn%20this%20work%2C%20we%20propose%20to%20address%20the%20currently%20unsolved%20issues%20in%20CMR%0Asegmentation%2C%20specifically%20at%20the%20RV%20base%2C%20with%20two%20strategies%3A%20first%2C%20we%0Acomplemented%20the%20public%20resource%20by%20reannotating%20the%20RV%20base%20in%20the%20ACDC%0Adataset%2C%20with%20refined%20delineation%20of%20the%20right%20ventricle%20outflow%20tract%20%28RVOT%29%2C%0Aunder%20the%20guidance%20of%20an%20expert%20cardiologist.%20Second%2C%20we%20proposed%20a%20novel%20dual%0Aencoder%20U-Net%20architecture%20that%20leverages%20temporal%20incoherence%20to%20inform%20the%0Asegmentation%20when%20interplanar%20motions%20occur.%20The%20inter-planar%20motion%20is%0Acharacterized%20by%20loss-of-tracking%2C%20via%20Bayesian%20uncertainty%20of%20a%0Amotion-tracking%20model.%20Our%20experiments%20showed%20that%20our%20method%20significantly%0Aimproved%20RV%20base%20segmentation%20taking%20into%20account%20temporal%20incoherence.%0AFurthermore%2C%20we%20investigated%20the%20reproducibility%20of%20deep%20learning-based%0Asegmentation%20and%20showed%20that%20the%20combination%20of%20consistent%20annotation%20and%20loss%0Aof%20tracking%20could%20enhance%20the%20reproducibility%20of%20RV%20segmentation%2C%20potentially%0Afacilitating%20a%20large%20number%20of%20clinical%20studies%20focusing%20on%20RV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03320v1&entry.124074799=Read"},
{"title": "Influence-oriented Personalized Federated Learning", "author": "Yue Tan and Guodong Long and Jing Jiang and Chengqi Zhang", "abstract": "  Traditional federated learning (FL) methods often rely on fixed weighting for\nparameter aggregation, neglecting the mutual influence by others. Hence, their\neffectiveness in heterogeneous data contexts is limited. To address this\nproblem, we propose an influence-oriented federated learning framework, namely\nFedC^2I, which quantitatively measures Client-level and Class-level Influence\nto realize adaptive parameter aggregation for each client. Our core idea is to\nexplicitly model the inter-client influence within an FL system via the\nwell-crafted influence vector and influence matrix. The influence vector\nquantifies client-level influence, enables clients to selectively acquire\nknowledge from others, and guides the aggregation of feature representation\nlayers. Meanwhile, the influence matrix captures class-level influence in a\nmore fine-grained manner to achieve personalized classifier aggregation. We\nevaluate the performance of FedC^2I against existing federated learning methods\nunder non-IID settings and the results demonstrate the superiority of our\nmethod.\n", "link": "http://arxiv.org/abs/2410.03315v1", "date": "2024-10-04", "relevancy": 2.285, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.468}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4534}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Influence-oriented%20Personalized%20Federated%20Learning&body=Title%3A%20Influence-oriented%20Personalized%20Federated%20Learning%0AAuthor%3A%20Yue%20Tan%20and%20Guodong%20Long%20and%20Jing%20Jiang%20and%20Chengqi%20Zhang%0AAbstract%3A%20%20%20Traditional%20federated%20learning%20%28FL%29%20methods%20often%20rely%20on%20fixed%20weighting%20for%0Aparameter%20aggregation%2C%20neglecting%20the%20mutual%20influence%20by%20others.%20Hence%2C%20their%0Aeffectiveness%20in%20heterogeneous%20data%20contexts%20is%20limited.%20To%20address%20this%0Aproblem%2C%20we%20propose%20an%20influence-oriented%20federated%20learning%20framework%2C%20namely%0AFedC%5E2I%2C%20which%20quantitatively%20measures%20Client-level%20and%20Class-level%20Influence%0Ato%20realize%20adaptive%20parameter%20aggregation%20for%20each%20client.%20Our%20core%20idea%20is%20to%0Aexplicitly%20model%20the%20inter-client%20influence%20within%20an%20FL%20system%20via%20the%0Awell-crafted%20influence%20vector%20and%20influence%20matrix.%20The%20influence%20vector%0Aquantifies%20client-level%20influence%2C%20enables%20clients%20to%20selectively%20acquire%0Aknowledge%20from%20others%2C%20and%20guides%20the%20aggregation%20of%20feature%20representation%0Alayers.%20Meanwhile%2C%20the%20influence%20matrix%20captures%20class-level%20influence%20in%20a%0Amore%20fine-grained%20manner%20to%20achieve%20personalized%20classifier%20aggregation.%20We%0Aevaluate%20the%20performance%20of%20FedC%5E2I%20against%20existing%20federated%20learning%20methods%0Aunder%20non-IID%20settings%20and%20the%20results%20demonstrate%20the%20superiority%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfluence-oriented%2520Personalized%2520Federated%2520Learning%26entry.906535625%3DYue%2520Tan%2520and%2520Guodong%2520Long%2520and%2520Jing%2520Jiang%2520and%2520Chengqi%2520Zhang%26entry.1292438233%3D%2520%2520Traditional%2520federated%2520learning%2520%2528FL%2529%2520methods%2520often%2520rely%2520on%2520fixed%2520weighting%2520for%250Aparameter%2520aggregation%252C%2520neglecting%2520the%2520mutual%2520influence%2520by%2520others.%2520Hence%252C%2520their%250Aeffectiveness%2520in%2520heterogeneous%2520data%2520contexts%2520is%2520limited.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520propose%2520an%2520influence-oriented%2520federated%2520learning%2520framework%252C%2520namely%250AFedC%255E2I%252C%2520which%2520quantitatively%2520measures%2520Client-level%2520and%2520Class-level%2520Influence%250Ato%2520realize%2520adaptive%2520parameter%2520aggregation%2520for%2520each%2520client.%2520Our%2520core%2520idea%2520is%2520to%250Aexplicitly%2520model%2520the%2520inter-client%2520influence%2520within%2520an%2520FL%2520system%2520via%2520the%250Awell-crafted%2520influence%2520vector%2520and%2520influence%2520matrix.%2520The%2520influence%2520vector%250Aquantifies%2520client-level%2520influence%252C%2520enables%2520clients%2520to%2520selectively%2520acquire%250Aknowledge%2520from%2520others%252C%2520and%2520guides%2520the%2520aggregation%2520of%2520feature%2520representation%250Alayers.%2520Meanwhile%252C%2520the%2520influence%2520matrix%2520captures%2520class-level%2520influence%2520in%2520a%250Amore%2520fine-grained%2520manner%2520to%2520achieve%2520personalized%2520classifier%2520aggregation.%2520We%250Aevaluate%2520the%2520performance%2520of%2520FedC%255E2I%2520against%2520existing%2520federated%2520learning%2520methods%250Aunder%2520non-IID%2520settings%2520and%2520the%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influence-oriented%20Personalized%20Federated%20Learning&entry.906535625=Yue%20Tan%20and%20Guodong%20Long%20and%20Jing%20Jiang%20and%20Chengqi%20Zhang&entry.1292438233=%20%20Traditional%20federated%20learning%20%28FL%29%20methods%20often%20rely%20on%20fixed%20weighting%20for%0Aparameter%20aggregation%2C%20neglecting%20the%20mutual%20influence%20by%20others.%20Hence%2C%20their%0Aeffectiveness%20in%20heterogeneous%20data%20contexts%20is%20limited.%20To%20address%20this%0Aproblem%2C%20we%20propose%20an%20influence-oriented%20federated%20learning%20framework%2C%20namely%0AFedC%5E2I%2C%20which%20quantitatively%20measures%20Client-level%20and%20Class-level%20Influence%0Ato%20realize%20adaptive%20parameter%20aggregation%20for%20each%20client.%20Our%20core%20idea%20is%20to%0Aexplicitly%20model%20the%20inter-client%20influence%20within%20an%20FL%20system%20via%20the%0Awell-crafted%20influence%20vector%20and%20influence%20matrix.%20The%20influence%20vector%0Aquantifies%20client-level%20influence%2C%20enables%20clients%20to%20selectively%20acquire%0Aknowledge%20from%20others%2C%20and%20guides%20the%20aggregation%20of%20feature%20representation%0Alayers.%20Meanwhile%2C%20the%20influence%20matrix%20captures%20class-level%20influence%20in%20a%0Amore%20fine-grained%20manner%20to%20achieve%20personalized%20classifier%20aggregation.%20We%0Aevaluate%20the%20performance%20of%20FedC%5E2I%20against%20existing%20federated%20learning%20methods%0Aunder%20non-IID%20settings%20and%20the%20results%20demonstrate%20the%20superiority%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03315v1&entry.124074799=Read"},
{"title": "Cayley Graph Propagation", "author": "JJ Wilson and Maya Bechler-Speicher and Petar Veli\u010dkovi\u0107", "abstract": "  In spite of the plethora of success stories with graph neural networks (GNNs)\non modelling graph-structured data, they are notoriously vulnerable to\nover-squashing, whereby tasks necessitate the mixing of information between\ndistance pairs of nodes. To address this problem, prior work suggests rewiring\nthe graph structure to improve information flow. Alternatively, a significant\nbody of research has dedicated itself to discovering and precomputing\nbottleneck-free graph structures to ameliorate over-squashing. One well\nregarded family of bottleneck-free graphs within the mathematical community are\nexpander graphs, with prior work$\\unicode{x2014}$Expander Graph Propagation\n(EGP)$\\unicode{x2014}$proposing the use of a well-known expander graph\nfamily$\\unicode{x2014}$the Cayley graphs of the $\\mathrm{SL}(2,\\mathbb{Z}_n)$\nspecial linear group$\\unicode{x2014}$as a computational template for GNNs.\nHowever, in EGP the computational graphs used are truncated to align with a\ngiven input graph. In this work, we show that truncation is detrimental to the\ncoveted expansion properties. Instead, we propose CGP, a method to propagate\ninformation over a complete Cayley graph structure, thereby ensuring it is\nbottleneck-free to better alleviate over-squashing. Our empirical evidence\nacross several real-world datasets not only shows that CGP recovers significant\nimprovements as compared to EGP, but it is also akin to or outperforms\ncomputationally complex graph rewiring techniques.\n", "link": "http://arxiv.org/abs/2410.03424v1", "date": "2024-10-04", "relevancy": 2.2838, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4561}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cayley%20Graph%20Propagation&body=Title%3A%20Cayley%20Graph%20Propagation%0AAuthor%3A%20JJ%20Wilson%20and%20Maya%20Bechler-Speicher%20and%20Petar%20Veli%C4%8Dkovi%C4%87%0AAbstract%3A%20%20%20In%20spite%20of%20the%20plethora%20of%20success%20stories%20with%20graph%20neural%20networks%20%28GNNs%29%0Aon%20modelling%20graph-structured%20data%2C%20they%20are%20notoriously%20vulnerable%20to%0Aover-squashing%2C%20whereby%20tasks%20necessitate%20the%20mixing%20of%20information%20between%0Adistance%20pairs%20of%20nodes.%20To%20address%20this%20problem%2C%20prior%20work%20suggests%20rewiring%0Athe%20graph%20structure%20to%20improve%20information%20flow.%20Alternatively%2C%20a%20significant%0Abody%20of%20research%20has%20dedicated%20itself%20to%20discovering%20and%20precomputing%0Abottleneck-free%20graph%20structures%20to%20ameliorate%20over-squashing.%20One%20well%0Aregarded%20family%20of%20bottleneck-free%20graphs%20within%20the%20mathematical%20community%20are%0Aexpander%20graphs%2C%20with%20prior%20work%24%5Cunicode%7Bx2014%7D%24Expander%20Graph%20Propagation%0A%28EGP%29%24%5Cunicode%7Bx2014%7D%24proposing%20the%20use%20of%20a%20well-known%20expander%20graph%0Afamily%24%5Cunicode%7Bx2014%7D%24the%20Cayley%20graphs%20of%20the%20%24%5Cmathrm%7BSL%7D%282%2C%5Cmathbb%7BZ%7D_n%29%24%0Aspecial%20linear%20group%24%5Cunicode%7Bx2014%7D%24as%20a%20computational%20template%20for%20GNNs.%0AHowever%2C%20in%20EGP%20the%20computational%20graphs%20used%20are%20truncated%20to%20align%20with%20a%0Agiven%20input%20graph.%20In%20this%20work%2C%20we%20show%20that%20truncation%20is%20detrimental%20to%20the%0Acoveted%20expansion%20properties.%20Instead%2C%20we%20propose%20CGP%2C%20a%20method%20to%20propagate%0Ainformation%20over%20a%20complete%20Cayley%20graph%20structure%2C%20thereby%20ensuring%20it%20is%0Abottleneck-free%20to%20better%20alleviate%20over-squashing.%20Our%20empirical%20evidence%0Aacross%20several%20real-world%20datasets%20not%20only%20shows%20that%20CGP%20recovers%20significant%0Aimprovements%20as%20compared%20to%20EGP%2C%20but%20it%20is%20also%20akin%20to%20or%20outperforms%0Acomputationally%20complex%20graph%20rewiring%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCayley%2520Graph%2520Propagation%26entry.906535625%3DJJ%2520Wilson%2520and%2520Maya%2520Bechler-Speicher%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%26entry.1292438233%3D%2520%2520In%2520spite%2520of%2520the%2520plethora%2520of%2520success%2520stories%2520with%2520graph%2520neural%2520networks%2520%2528GNNs%2529%250Aon%2520modelling%2520graph-structured%2520data%252C%2520they%2520are%2520notoriously%2520vulnerable%2520to%250Aover-squashing%252C%2520whereby%2520tasks%2520necessitate%2520the%2520mixing%2520of%2520information%2520between%250Adistance%2520pairs%2520of%2520nodes.%2520To%2520address%2520this%2520problem%252C%2520prior%2520work%2520suggests%2520rewiring%250Athe%2520graph%2520structure%2520to%2520improve%2520information%2520flow.%2520Alternatively%252C%2520a%2520significant%250Abody%2520of%2520research%2520has%2520dedicated%2520itself%2520to%2520discovering%2520and%2520precomputing%250Abottleneck-free%2520graph%2520structures%2520to%2520ameliorate%2520over-squashing.%2520One%2520well%250Aregarded%2520family%2520of%2520bottleneck-free%2520graphs%2520within%2520the%2520mathematical%2520community%2520are%250Aexpander%2520graphs%252C%2520with%2520prior%2520work%2524%255Cunicode%257Bx2014%257D%2524Expander%2520Graph%2520Propagation%250A%2528EGP%2529%2524%255Cunicode%257Bx2014%257D%2524proposing%2520the%2520use%2520of%2520a%2520well-known%2520expander%2520graph%250Afamily%2524%255Cunicode%257Bx2014%257D%2524the%2520Cayley%2520graphs%2520of%2520the%2520%2524%255Cmathrm%257BSL%257D%25282%252C%255Cmathbb%257BZ%257D_n%2529%2524%250Aspecial%2520linear%2520group%2524%255Cunicode%257Bx2014%257D%2524as%2520a%2520computational%2520template%2520for%2520GNNs.%250AHowever%252C%2520in%2520EGP%2520the%2520computational%2520graphs%2520used%2520are%2520truncated%2520to%2520align%2520with%2520a%250Agiven%2520input%2520graph.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520truncation%2520is%2520detrimental%2520to%2520the%250Acoveted%2520expansion%2520properties.%2520Instead%252C%2520we%2520propose%2520CGP%252C%2520a%2520method%2520to%2520propagate%250Ainformation%2520over%2520a%2520complete%2520Cayley%2520graph%2520structure%252C%2520thereby%2520ensuring%2520it%2520is%250Abottleneck-free%2520to%2520better%2520alleviate%2520over-squashing.%2520Our%2520empirical%2520evidence%250Aacross%2520several%2520real-world%2520datasets%2520not%2520only%2520shows%2520that%2520CGP%2520recovers%2520significant%250Aimprovements%2520as%2520compared%2520to%2520EGP%252C%2520but%2520it%2520is%2520also%2520akin%2520to%2520or%2520outperforms%250Acomputationally%2520complex%2520graph%2520rewiring%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cayley%20Graph%20Propagation&entry.906535625=JJ%20Wilson%20and%20Maya%20Bechler-Speicher%20and%20Petar%20Veli%C4%8Dkovi%C4%87&entry.1292438233=%20%20In%20spite%20of%20the%20plethora%20of%20success%20stories%20with%20graph%20neural%20networks%20%28GNNs%29%0Aon%20modelling%20graph-structured%20data%2C%20they%20are%20notoriously%20vulnerable%20to%0Aover-squashing%2C%20whereby%20tasks%20necessitate%20the%20mixing%20of%20information%20between%0Adistance%20pairs%20of%20nodes.%20To%20address%20this%20problem%2C%20prior%20work%20suggests%20rewiring%0Athe%20graph%20structure%20to%20improve%20information%20flow.%20Alternatively%2C%20a%20significant%0Abody%20of%20research%20has%20dedicated%20itself%20to%20discovering%20and%20precomputing%0Abottleneck-free%20graph%20structures%20to%20ameliorate%20over-squashing.%20One%20well%0Aregarded%20family%20of%20bottleneck-free%20graphs%20within%20the%20mathematical%20community%20are%0Aexpander%20graphs%2C%20with%20prior%20work%24%5Cunicode%7Bx2014%7D%24Expander%20Graph%20Propagation%0A%28EGP%29%24%5Cunicode%7Bx2014%7D%24proposing%20the%20use%20of%20a%20well-known%20expander%20graph%0Afamily%24%5Cunicode%7Bx2014%7D%24the%20Cayley%20graphs%20of%20the%20%24%5Cmathrm%7BSL%7D%282%2C%5Cmathbb%7BZ%7D_n%29%24%0Aspecial%20linear%20group%24%5Cunicode%7Bx2014%7D%24as%20a%20computational%20template%20for%20GNNs.%0AHowever%2C%20in%20EGP%20the%20computational%20graphs%20used%20are%20truncated%20to%20align%20with%20a%0Agiven%20input%20graph.%20In%20this%20work%2C%20we%20show%20that%20truncation%20is%20detrimental%20to%20the%0Acoveted%20expansion%20properties.%20Instead%2C%20we%20propose%20CGP%2C%20a%20method%20to%20propagate%0Ainformation%20over%20a%20complete%20Cayley%20graph%20structure%2C%20thereby%20ensuring%20it%20is%0Abottleneck-free%20to%20better%20alleviate%20over-squashing.%20Our%20empirical%20evidence%0Aacross%20several%20real-world%20datasets%20not%20only%20shows%20that%20CGP%20recovers%20significant%0Aimprovements%20as%20compared%20to%20EGP%2C%20but%20it%20is%20also%20akin%20to%20or%20outperforms%0Acomputationally%20complex%20graph%20rewiring%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03424v1&entry.124074799=Read"},
{"title": "SiMilarity-Enhanced Homophily for Multi-View Heterophilous Graph\n  Clustering", "author": "Jianpeng Chen and Yawen Ling and Yazhou Ren and Zichen Wen and Tianyi Wu and Shufei Zhang and Lifang He", "abstract": "  With the increasing prevalence of graph-structured data, multi-view graph\nclustering has been widely used in various downstream applications. Existing\napproaches primarily rely on a unified message passing mechanism, which\nsignificantly enhances clustering performance. Nevertheless, this mechanism\nlimits its applicability to heterophilous situations, as it is fundamentally\npredicated on the assumption of homophily, i.e., the connected nodes often\nbelong to the same class. In reality, this assumption does not always hold; a\nmoderately or even mildly homophilous graph is more common than a fully\nhomophilous one due to inevitable heterophilous information in the graph. To\naddress this issue, in this paper, we propose a novel SiMilarity-enhanced\nHomophily for Multi-view Heterophilous Graph Clustering (SMHGC) approach. By\nanalyzing the relationship between similarity and graph homophily, we propose\nto enhance the homophily by introducing three similarity terms, i.e., neighbor\npattern similarity, node feature similarity, and multi-view global similarity,\nin a label-free manner. Then, a consensus-based inter- and intra-view fusion\nparadigm is proposed to fuse the improved homophilous graph from different\nviews and utilize them for clustering. The state-of-the-art experimental\nresults on both multi-view heterophilous and homophilous datasets collectively\ndemonstrate the strong capacity of similarity for unsupervised multi-view\nheterophilous graph learning. Additionally, the consistent performance across\nsemi-synthetic datasets with varying levels of homophily serves as further\nevidence of SMHGC's resilience to heterophily.\n", "link": "http://arxiv.org/abs/2410.03596v1", "date": "2024-10-04", "relevancy": 2.2703, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4728}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SiMilarity-Enhanced%20Homophily%20for%20Multi-View%20Heterophilous%20Graph%0A%20%20Clustering&body=Title%3A%20SiMilarity-Enhanced%20Homophily%20for%20Multi-View%20Heterophilous%20Graph%0A%20%20Clustering%0AAuthor%3A%20Jianpeng%20Chen%20and%20Yawen%20Ling%20and%20Yazhou%20Ren%20and%20Zichen%20Wen%20and%20Tianyi%20Wu%20and%20Shufei%20Zhang%20and%20Lifang%20He%0AAbstract%3A%20%20%20With%20the%20increasing%20prevalence%20of%20graph-structured%20data%2C%20multi-view%20graph%0Aclustering%20has%20been%20widely%20used%20in%20various%20downstream%20applications.%20Existing%0Aapproaches%20primarily%20rely%20on%20a%20unified%20message%20passing%20mechanism%2C%20which%0Asignificantly%20enhances%20clustering%20performance.%20Nevertheless%2C%20this%20mechanism%0Alimits%20its%20applicability%20to%20heterophilous%20situations%2C%20as%20it%20is%20fundamentally%0Apredicated%20on%20the%20assumption%20of%20homophily%2C%20i.e.%2C%20the%20connected%20nodes%20often%0Abelong%20to%20the%20same%20class.%20In%20reality%2C%20this%20assumption%20does%20not%20always%20hold%3B%20a%0Amoderately%20or%20even%20mildly%20homophilous%20graph%20is%20more%20common%20than%20a%20fully%0Ahomophilous%20one%20due%20to%20inevitable%20heterophilous%20information%20in%20the%20graph.%20To%0Aaddress%20this%20issue%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20SiMilarity-enhanced%0AHomophily%20for%20Multi-view%20Heterophilous%20Graph%20Clustering%20%28SMHGC%29%20approach.%20By%0Aanalyzing%20the%20relationship%20between%20similarity%20and%20graph%20homophily%2C%20we%20propose%0Ato%20enhance%20the%20homophily%20by%20introducing%20three%20similarity%20terms%2C%20i.e.%2C%20neighbor%0Apattern%20similarity%2C%20node%20feature%20similarity%2C%20and%20multi-view%20global%20similarity%2C%0Ain%20a%20label-free%20manner.%20Then%2C%20a%20consensus-based%20inter-%20and%20intra-view%20fusion%0Aparadigm%20is%20proposed%20to%20fuse%20the%20improved%20homophilous%20graph%20from%20different%0Aviews%20and%20utilize%20them%20for%20clustering.%20The%20state-of-the-art%20experimental%0Aresults%20on%20both%20multi-view%20heterophilous%20and%20homophilous%20datasets%20collectively%0Ademonstrate%20the%20strong%20capacity%20of%20similarity%20for%20unsupervised%20multi-view%0Aheterophilous%20graph%20learning.%20Additionally%2C%20the%20consistent%20performance%20across%0Asemi-synthetic%20datasets%20with%20varying%20levels%20of%20homophily%20serves%20as%20further%0Aevidence%20of%20SMHGC%27s%20resilience%20to%20heterophily.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSiMilarity-Enhanced%2520Homophily%2520for%2520Multi-View%2520Heterophilous%2520Graph%250A%2520%2520Clustering%26entry.906535625%3DJianpeng%2520Chen%2520and%2520Yawen%2520Ling%2520and%2520Yazhou%2520Ren%2520and%2520Zichen%2520Wen%2520and%2520Tianyi%2520Wu%2520and%2520Shufei%2520Zhang%2520and%2520Lifang%2520He%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520prevalence%2520of%2520graph-structured%2520data%252C%2520multi-view%2520graph%250Aclustering%2520has%2520been%2520widely%2520used%2520in%2520various%2520downstream%2520applications.%2520Existing%250Aapproaches%2520primarily%2520rely%2520on%2520a%2520unified%2520message%2520passing%2520mechanism%252C%2520which%250Asignificantly%2520enhances%2520clustering%2520performance.%2520Nevertheless%252C%2520this%2520mechanism%250Alimits%2520its%2520applicability%2520to%2520heterophilous%2520situations%252C%2520as%2520it%2520is%2520fundamentally%250Apredicated%2520on%2520the%2520assumption%2520of%2520homophily%252C%2520i.e.%252C%2520the%2520connected%2520nodes%2520often%250Abelong%2520to%2520the%2520same%2520class.%2520In%2520reality%252C%2520this%2520assumption%2520does%2520not%2520always%2520hold%253B%2520a%250Amoderately%2520or%2520even%2520mildly%2520homophilous%2520graph%2520is%2520more%2520common%2520than%2520a%2520fully%250Ahomophilous%2520one%2520due%2520to%2520inevitable%2520heterophilous%2520information%2520in%2520the%2520graph.%2520To%250Aaddress%2520this%2520issue%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520SiMilarity-enhanced%250AHomophily%2520for%2520Multi-view%2520Heterophilous%2520Graph%2520Clustering%2520%2528SMHGC%2529%2520approach.%2520By%250Aanalyzing%2520the%2520relationship%2520between%2520similarity%2520and%2520graph%2520homophily%252C%2520we%2520propose%250Ato%2520enhance%2520the%2520homophily%2520by%2520introducing%2520three%2520similarity%2520terms%252C%2520i.e.%252C%2520neighbor%250Apattern%2520similarity%252C%2520node%2520feature%2520similarity%252C%2520and%2520multi-view%2520global%2520similarity%252C%250Ain%2520a%2520label-free%2520manner.%2520Then%252C%2520a%2520consensus-based%2520inter-%2520and%2520intra-view%2520fusion%250Aparadigm%2520is%2520proposed%2520to%2520fuse%2520the%2520improved%2520homophilous%2520graph%2520from%2520different%250Aviews%2520and%2520utilize%2520them%2520for%2520clustering.%2520The%2520state-of-the-art%2520experimental%250Aresults%2520on%2520both%2520multi-view%2520heterophilous%2520and%2520homophilous%2520datasets%2520collectively%250Ademonstrate%2520the%2520strong%2520capacity%2520of%2520similarity%2520for%2520unsupervised%2520multi-view%250Aheterophilous%2520graph%2520learning.%2520Additionally%252C%2520the%2520consistent%2520performance%2520across%250Asemi-synthetic%2520datasets%2520with%2520varying%2520levels%2520of%2520homophily%2520serves%2520as%2520further%250Aevidence%2520of%2520SMHGC%2527s%2520resilience%2520to%2520heterophily.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SiMilarity-Enhanced%20Homophily%20for%20Multi-View%20Heterophilous%20Graph%0A%20%20Clustering&entry.906535625=Jianpeng%20Chen%20and%20Yawen%20Ling%20and%20Yazhou%20Ren%20and%20Zichen%20Wen%20and%20Tianyi%20Wu%20and%20Shufei%20Zhang%20and%20Lifang%20He&entry.1292438233=%20%20With%20the%20increasing%20prevalence%20of%20graph-structured%20data%2C%20multi-view%20graph%0Aclustering%20has%20been%20widely%20used%20in%20various%20downstream%20applications.%20Existing%0Aapproaches%20primarily%20rely%20on%20a%20unified%20message%20passing%20mechanism%2C%20which%0Asignificantly%20enhances%20clustering%20performance.%20Nevertheless%2C%20this%20mechanism%0Alimits%20its%20applicability%20to%20heterophilous%20situations%2C%20as%20it%20is%20fundamentally%0Apredicated%20on%20the%20assumption%20of%20homophily%2C%20i.e.%2C%20the%20connected%20nodes%20often%0Abelong%20to%20the%20same%20class.%20In%20reality%2C%20this%20assumption%20does%20not%20always%20hold%3B%20a%0Amoderately%20or%20even%20mildly%20homophilous%20graph%20is%20more%20common%20than%20a%20fully%0Ahomophilous%20one%20due%20to%20inevitable%20heterophilous%20information%20in%20the%20graph.%20To%0Aaddress%20this%20issue%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20SiMilarity-enhanced%0AHomophily%20for%20Multi-view%20Heterophilous%20Graph%20Clustering%20%28SMHGC%29%20approach.%20By%0Aanalyzing%20the%20relationship%20between%20similarity%20and%20graph%20homophily%2C%20we%20propose%0Ato%20enhance%20the%20homophily%20by%20introducing%20three%20similarity%20terms%2C%20i.e.%2C%20neighbor%0Apattern%20similarity%2C%20node%20feature%20similarity%2C%20and%20multi-view%20global%20similarity%2C%0Ain%20a%20label-free%20manner.%20Then%2C%20a%20consensus-based%20inter-%20and%20intra-view%20fusion%0Aparadigm%20is%20proposed%20to%20fuse%20the%20improved%20homophilous%20graph%20from%20different%0Aviews%20and%20utilize%20them%20for%20clustering.%20The%20state-of-the-art%20experimental%0Aresults%20on%20both%20multi-view%20heterophilous%20and%20homophilous%20datasets%20collectively%0Ademonstrate%20the%20strong%20capacity%20of%20similarity%20for%20unsupervised%20multi-view%0Aheterophilous%20graph%20learning.%20Additionally%2C%20the%20consistent%20performance%20across%0Asemi-synthetic%20datasets%20with%20varying%20levels%20of%20homophily%20serves%20as%20further%0Aevidence%20of%20SMHGC%27s%20resilience%20to%20heterophily.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03596v1&entry.124074799=Read"},
{"title": "Branches: A Fast Dynamic Programming and Branch & Bound Algorithm for\n  Optimal Decision Trees", "author": "Ayman Chaouki and Jesse Read and Albert Bifet", "abstract": "  Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine\nLearning, yet it poses a formidable optimisation challenge. Despite numerous\nefforts dating back to the early 1990's, practical algorithms have only\nrecently emerged, primarily leveraging Dynamic Programming (DP) and Branch &\nBound (B&B) techniques. These methods fall into two categories: algorithms like\nDL8.5, MurTree and STreeD utilise an efficient DP strategy but lack effective\nbounds for pruning the search space; while algorithms like OSDT and GOSDT\nemploy more efficient pruning bounds but at the expense of a less refined DP\nstrategy. We introduce Branches, a new algorithm that combines the strengths of\nboth approaches. Using DP and B&B with a novel analytical bound for efficient\npruning, Branches offers both speed and sparsity optimisation. Unlike other\nmethods, it also handles non-binary features. Theoretical analysis shows its\nlower complexity compared to existing methods, and empirical results confirm\nthat Branches outperforms the state-of-the-art in speed, iterations, and\noptimality.\n", "link": "http://arxiv.org/abs/2406.02175v3", "date": "2024-10-04", "relevancy": 2.2688, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.498}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4339}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees&body=Title%3A%20Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees%0AAuthor%3A%20Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet%0AAbstract%3A%20%20%20Decision%20Tree%20%28DT%29%20Learning%20is%20a%20fundamental%20problem%20in%20Interpretable%20Machine%0ALearning%2C%20yet%20it%20poses%20a%20formidable%20optimisation%20challenge.%20Despite%20numerous%0Aefforts%20dating%20back%20to%20the%20early%201990%27s%2C%20practical%20algorithms%20have%20only%0Arecently%20emerged%2C%20primarily%20leveraging%20Dynamic%20Programming%20%28DP%29%20and%20Branch%20%26%0ABound%20%28B%26B%29%20techniques.%20These%20methods%20fall%20into%20two%20categories%3A%20algorithms%20like%0ADL8.5%2C%20MurTree%20and%20STreeD%20utilise%20an%20efficient%20DP%20strategy%20but%20lack%20effective%0Abounds%20for%20pruning%20the%20search%20space%3B%20while%20algorithms%20like%20OSDT%20and%20GOSDT%0Aemploy%20more%20efficient%20pruning%20bounds%20but%20at%20the%20expense%20of%20a%20less%20refined%20DP%0Astrategy.%20We%20introduce%20Branches%2C%20a%20new%20algorithm%20that%20combines%20the%20strengths%20of%0Aboth%20approaches.%20Using%20DP%20and%20B%26B%20with%20a%20novel%20analytical%20bound%20for%20efficient%0Apruning%2C%20Branches%20offers%20both%20speed%20and%20sparsity%20optimisation.%20Unlike%20other%0Amethods%2C%20it%20also%20handles%20non-binary%20features.%20Theoretical%20analysis%20shows%20its%0Alower%20complexity%20compared%20to%20existing%20methods%2C%20and%20empirical%20results%20confirm%0Athat%20Branches%20outperforms%20the%20state-of-the-art%20in%20speed%2C%20iterations%2C%20and%0Aoptimality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02175v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranches%253A%2520A%2520Fast%2520Dynamic%2520Programming%2520and%2520Branch%2520%2526%2520Bound%2520Algorithm%2520for%250A%2520%2520Optimal%2520Decision%2520Trees%26entry.906535625%3DAyman%2520Chaouki%2520and%2520Jesse%2520Read%2520and%2520Albert%2520Bifet%26entry.1292438233%3D%2520%2520Decision%2520Tree%2520%2528DT%2529%2520Learning%2520is%2520a%2520fundamental%2520problem%2520in%2520Interpretable%2520Machine%250ALearning%252C%2520yet%2520it%2520poses%2520a%2520formidable%2520optimisation%2520challenge.%2520Despite%2520numerous%250Aefforts%2520dating%2520back%2520to%2520the%2520early%25201990%2527s%252C%2520practical%2520algorithms%2520have%2520only%250Arecently%2520emerged%252C%2520primarily%2520leveraging%2520Dynamic%2520Programming%2520%2528DP%2529%2520and%2520Branch%2520%2526%250ABound%2520%2528B%2526B%2529%2520techniques.%2520These%2520methods%2520fall%2520into%2520two%2520categories%253A%2520algorithms%2520like%250ADL8.5%252C%2520MurTree%2520and%2520STreeD%2520utilise%2520an%2520efficient%2520DP%2520strategy%2520but%2520lack%2520effective%250Abounds%2520for%2520pruning%2520the%2520search%2520space%253B%2520while%2520algorithms%2520like%2520OSDT%2520and%2520GOSDT%250Aemploy%2520more%2520efficient%2520pruning%2520bounds%2520but%2520at%2520the%2520expense%2520of%2520a%2520less%2520refined%2520DP%250Astrategy.%2520We%2520introduce%2520Branches%252C%2520a%2520new%2520algorithm%2520that%2520combines%2520the%2520strengths%2520of%250Aboth%2520approaches.%2520Using%2520DP%2520and%2520B%2526B%2520with%2520a%2520novel%2520analytical%2520bound%2520for%2520efficient%250Apruning%252C%2520Branches%2520offers%2520both%2520speed%2520and%2520sparsity%2520optimisation.%2520Unlike%2520other%250Amethods%252C%2520it%2520also%2520handles%2520non-binary%2520features.%2520Theoretical%2520analysis%2520shows%2520its%250Alower%2520complexity%2520compared%2520to%2520existing%2520methods%252C%2520and%2520empirical%2520results%2520confirm%250Athat%2520Branches%2520outperforms%2520the%2520state-of-the-art%2520in%2520speed%252C%2520iterations%252C%2520and%250Aoptimality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02175v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Branches%3A%20A%20Fast%20Dynamic%20Programming%20and%20Branch%20%26%20Bound%20Algorithm%20for%0A%20%20Optimal%20Decision%20Trees&entry.906535625=Ayman%20Chaouki%20and%20Jesse%20Read%20and%20Albert%20Bifet&entry.1292438233=%20%20Decision%20Tree%20%28DT%29%20Learning%20is%20a%20fundamental%20problem%20in%20Interpretable%20Machine%0ALearning%2C%20yet%20it%20poses%20a%20formidable%20optimisation%20challenge.%20Despite%20numerous%0Aefforts%20dating%20back%20to%20the%20early%201990%27s%2C%20practical%20algorithms%20have%20only%0Arecently%20emerged%2C%20primarily%20leveraging%20Dynamic%20Programming%20%28DP%29%20and%20Branch%20%26%0ABound%20%28B%26B%29%20techniques.%20These%20methods%20fall%20into%20two%20categories%3A%20algorithms%20like%0ADL8.5%2C%20MurTree%20and%20STreeD%20utilise%20an%20efficient%20DP%20strategy%20but%20lack%20effective%0Abounds%20for%20pruning%20the%20search%20space%3B%20while%20algorithms%20like%20OSDT%20and%20GOSDT%0Aemploy%20more%20efficient%20pruning%20bounds%20but%20at%20the%20expense%20of%20a%20less%20refined%20DP%0Astrategy.%20We%20introduce%20Branches%2C%20a%20new%20algorithm%20that%20combines%20the%20strengths%20of%0Aboth%20approaches.%20Using%20DP%20and%20B%26B%20with%20a%20novel%20analytical%20bound%20for%20efficient%0Apruning%2C%20Branches%20offers%20both%20speed%20and%20sparsity%20optimisation.%20Unlike%20other%0Amethods%2C%20it%20also%20handles%20non-binary%20features.%20Theoretical%20analysis%20shows%20its%0Alower%20complexity%20compared%20to%20existing%20methods%2C%20and%20empirical%20results%20confirm%0Athat%20Branches%20outperforms%20the%20state-of-the-art%20in%20speed%2C%20iterations%2C%20and%0Aoptimality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02175v3&entry.124074799=Read"},
{"title": "A Multi-model Approach for Video Data Retrieval in Autonomous Vehicle\n  Development", "author": "Jesper Knapp and Klas Moberg and Yuchuan Jin and Simin Sun and Miroslaw Staron", "abstract": "  Autonomous driving software generates enormous amounts of data every second,\nwhich software development organizations save for future analysis and testing\nin the form of logs. However, given the vast size of this data, locating\nspecific scenarios within a collection of vehicle logs can be challenging.\nWriting the correct SQL queries to find these scenarios requires engineers to\nhave a strong background in SQL and the specific databases in question, further\ncomplicating the search process. This paper presents and evaluates a pipeline\nthat allows searching for specific scenarios in log collections using natural\nlanguage descriptions instead of SQL. The generated descriptions were evaluated\nby engineers working with vehicle logs at the Zenseact on a scale from 1 to 5.\nOur approach achieved a mean score of 3.3, demonstrating the potential of using\na multi-model architecture to improve the software development workflow. We\nalso present an interface that can visualize the query process and visualize\nthe results.\n", "link": "http://arxiv.org/abs/2410.03580v1", "date": "2024-10-04", "relevancy": 2.2637, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-model%20Approach%20for%20Video%20Data%20Retrieval%20in%20Autonomous%20Vehicle%0A%20%20Development&body=Title%3A%20A%20Multi-model%20Approach%20for%20Video%20Data%20Retrieval%20in%20Autonomous%20Vehicle%0A%20%20Development%0AAuthor%3A%20Jesper%20Knapp%20and%20Klas%20Moberg%20and%20Yuchuan%20Jin%20and%20Simin%20Sun%20and%20Miroslaw%20Staron%0AAbstract%3A%20%20%20Autonomous%20driving%20software%20generates%20enormous%20amounts%20of%20data%20every%20second%2C%0Awhich%20software%20development%20organizations%20save%20for%20future%20analysis%20and%20testing%0Ain%20the%20form%20of%20logs.%20However%2C%20given%20the%20vast%20size%20of%20this%20data%2C%20locating%0Aspecific%20scenarios%20within%20a%20collection%20of%20vehicle%20logs%20can%20be%20challenging.%0AWriting%20the%20correct%20SQL%20queries%20to%20find%20these%20scenarios%20requires%20engineers%20to%0Ahave%20a%20strong%20background%20in%20SQL%20and%20the%20specific%20databases%20in%20question%2C%20further%0Acomplicating%20the%20search%20process.%20This%20paper%20presents%20and%20evaluates%20a%20pipeline%0Athat%20allows%20searching%20for%20specific%20scenarios%20in%20log%20collections%20using%20natural%0Alanguage%20descriptions%20instead%20of%20SQL.%20The%20generated%20descriptions%20were%20evaluated%0Aby%20engineers%20working%20with%20vehicle%20logs%20at%20the%20Zenseact%20on%20a%20scale%20from%201%20to%205.%0AOur%20approach%20achieved%20a%20mean%20score%20of%203.3%2C%20demonstrating%20the%20potential%20of%20using%0Aa%20multi-model%20architecture%20to%20improve%20the%20software%20development%20workflow.%20We%0Aalso%20present%20an%20interface%20that%20can%20visualize%20the%20query%20process%20and%20visualize%0Athe%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-model%2520Approach%2520for%2520Video%2520Data%2520Retrieval%2520in%2520Autonomous%2520Vehicle%250A%2520%2520Development%26entry.906535625%3DJesper%2520Knapp%2520and%2520Klas%2520Moberg%2520and%2520Yuchuan%2520Jin%2520and%2520Simin%2520Sun%2520and%2520Miroslaw%2520Staron%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520software%2520generates%2520enormous%2520amounts%2520of%2520data%2520every%2520second%252C%250Awhich%2520software%2520development%2520organizations%2520save%2520for%2520future%2520analysis%2520and%2520testing%250Ain%2520the%2520form%2520of%2520logs.%2520However%252C%2520given%2520the%2520vast%2520size%2520of%2520this%2520data%252C%2520locating%250Aspecific%2520scenarios%2520within%2520a%2520collection%2520of%2520vehicle%2520logs%2520can%2520be%2520challenging.%250AWriting%2520the%2520correct%2520SQL%2520queries%2520to%2520find%2520these%2520scenarios%2520requires%2520engineers%2520to%250Ahave%2520a%2520strong%2520background%2520in%2520SQL%2520and%2520the%2520specific%2520databases%2520in%2520question%252C%2520further%250Acomplicating%2520the%2520search%2520process.%2520This%2520paper%2520presents%2520and%2520evaluates%2520a%2520pipeline%250Athat%2520allows%2520searching%2520for%2520specific%2520scenarios%2520in%2520log%2520collections%2520using%2520natural%250Alanguage%2520descriptions%2520instead%2520of%2520SQL.%2520The%2520generated%2520descriptions%2520were%2520evaluated%250Aby%2520engineers%2520working%2520with%2520vehicle%2520logs%2520at%2520the%2520Zenseact%2520on%2520a%2520scale%2520from%25201%2520to%25205.%250AOur%2520approach%2520achieved%2520a%2520mean%2520score%2520of%25203.3%252C%2520demonstrating%2520the%2520potential%2520of%2520using%250Aa%2520multi-model%2520architecture%2520to%2520improve%2520the%2520software%2520development%2520workflow.%2520We%250Aalso%2520present%2520an%2520interface%2520that%2520can%2520visualize%2520the%2520query%2520process%2520and%2520visualize%250Athe%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-model%20Approach%20for%20Video%20Data%20Retrieval%20in%20Autonomous%20Vehicle%0A%20%20Development&entry.906535625=Jesper%20Knapp%20and%20Klas%20Moberg%20and%20Yuchuan%20Jin%20and%20Simin%20Sun%20and%20Miroslaw%20Staron&entry.1292438233=%20%20Autonomous%20driving%20software%20generates%20enormous%20amounts%20of%20data%20every%20second%2C%0Awhich%20software%20development%20organizations%20save%20for%20future%20analysis%20and%20testing%0Ain%20the%20form%20of%20logs.%20However%2C%20given%20the%20vast%20size%20of%20this%20data%2C%20locating%0Aspecific%20scenarios%20within%20a%20collection%20of%20vehicle%20logs%20can%20be%20challenging.%0AWriting%20the%20correct%20SQL%20queries%20to%20find%20these%20scenarios%20requires%20engineers%20to%0Ahave%20a%20strong%20background%20in%20SQL%20and%20the%20specific%20databases%20in%20question%2C%20further%0Acomplicating%20the%20search%20process.%20This%20paper%20presents%20and%20evaluates%20a%20pipeline%0Athat%20allows%20searching%20for%20specific%20scenarios%20in%20log%20collections%20using%20natural%0Alanguage%20descriptions%20instead%20of%20SQL.%20The%20generated%20descriptions%20were%20evaluated%0Aby%20engineers%20working%20with%20vehicle%20logs%20at%20the%20Zenseact%20on%20a%20scale%20from%201%20to%205.%0AOur%20approach%20achieved%20a%20mean%20score%20of%203.3%2C%20demonstrating%20the%20potential%20of%20using%0Aa%20multi-model%20architecture%20to%20improve%20the%20software%20development%20workflow.%20We%0Aalso%20present%20an%20interface%20that%20can%20visualize%20the%20query%20process%20and%20visualize%0Athe%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03580v1&entry.124074799=Read"},
{"title": "Quo Vadis, Motion Generation? From Large Language Models to Large Motion\n  Models", "author": "Ye Wang and Sipeng Zheng and Bin Cao and Qianshan Wei and Qin Jin and Zongqing Lu", "abstract": "  Inspired by the recent success of LLMs, the field of human motion\nunderstanding has increasingly shifted towards the development of large motion\nmodels. Despite some progress, current state-of-the-art works remain far from\nachieving truly generalist models, largely due to the lack of large-scale,\nhigh-quality motion data. To address this, we present MotionBase, the first\nmillion-level motion generation benchmark, offering 15 times the data volume of\nthe previous largest dataset, and featuring multimodal data with hierarchically\ndetailed text descriptions. By leveraging this vast dataset, our large motion\nmodel demonstrates strong performance across a broad range of motions,\nincluding unseen ones. Through systematic investigation, we underscore the\nimportance of scaling both data and model size, with synthetic data and pseudo\nlabels playing a crucial role in mitigating data acquisition costs. Moreover,\nour research reveals the limitations of existing evaluation metrics,\nparticularly in handling out-of-domain text instructions -- an issue that has\nlong been overlooked. In addition to these, we introduce a novel 2D lookup-free\napproach for motion tokenization, which preserves motion information and\nexpands codebook capacity, further enhancing the representative ability of\nlarge motion models. The release of MotionBase and the insights gained from\nthis study are expected to pave the way for the development of more powerful\nand versatile motion generation models.\n", "link": "http://arxiv.org/abs/2410.03311v1", "date": "2024-10-04", "relevancy": 2.2395, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quo%20Vadis%2C%20Motion%20Generation%3F%20From%20Large%20Language%20Models%20to%20Large%20Motion%0A%20%20Models&body=Title%3A%20Quo%20Vadis%2C%20Motion%20Generation%3F%20From%20Large%20Language%20Models%20to%20Large%20Motion%0A%20%20Models%0AAuthor%3A%20Ye%20Wang%20and%20Sipeng%20Zheng%20and%20Bin%20Cao%20and%20Qianshan%20Wei%20and%20Qin%20Jin%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20Inspired%20by%20the%20recent%20success%20of%20LLMs%2C%20the%20field%20of%20human%20motion%0Aunderstanding%20has%20increasingly%20shifted%20towards%20the%20development%20of%20large%20motion%0Amodels.%20Despite%20some%20progress%2C%20current%20state-of-the-art%20works%20remain%20far%20from%0Aachieving%20truly%20generalist%20models%2C%20largely%20due%20to%20the%20lack%20of%20large-scale%2C%0Ahigh-quality%20motion%20data.%20To%20address%20this%2C%20we%20present%20MotionBase%2C%20the%20first%0Amillion-level%20motion%20generation%20benchmark%2C%20offering%2015%20times%20the%20data%20volume%20of%0Athe%20previous%20largest%20dataset%2C%20and%20featuring%20multimodal%20data%20with%20hierarchically%0Adetailed%20text%20descriptions.%20By%20leveraging%20this%20vast%20dataset%2C%20our%20large%20motion%0Amodel%20demonstrates%20strong%20performance%20across%20a%20broad%20range%20of%20motions%2C%0Aincluding%20unseen%20ones.%20Through%20systematic%20investigation%2C%20we%20underscore%20the%0Aimportance%20of%20scaling%20both%20data%20and%20model%20size%2C%20with%20synthetic%20data%20and%20pseudo%0Alabels%20playing%20a%20crucial%20role%20in%20mitigating%20data%20acquisition%20costs.%20Moreover%2C%0Aour%20research%20reveals%20the%20limitations%20of%20existing%20evaluation%20metrics%2C%0Aparticularly%20in%20handling%20out-of-domain%20text%20instructions%20--%20an%20issue%20that%20has%0Along%20been%20overlooked.%20In%20addition%20to%20these%2C%20we%20introduce%20a%20novel%202D%20lookup-free%0Aapproach%20for%20motion%20tokenization%2C%20which%20preserves%20motion%20information%20and%0Aexpands%20codebook%20capacity%2C%20further%20enhancing%20the%20representative%20ability%20of%0Alarge%20motion%20models.%20The%20release%20of%20MotionBase%20and%20the%20insights%20gained%20from%0Athis%20study%20are%20expected%20to%20pave%20the%20way%20for%20the%20development%20of%20more%20powerful%0Aand%20versatile%20motion%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuo%2520Vadis%252C%2520Motion%2520Generation%253F%2520From%2520Large%2520Language%2520Models%2520to%2520Large%2520Motion%250A%2520%2520Models%26entry.906535625%3DYe%2520Wang%2520and%2520Sipeng%2520Zheng%2520and%2520Bin%2520Cao%2520and%2520Qianshan%2520Wei%2520and%2520Qin%2520Jin%2520and%2520Zongqing%2520Lu%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520LLMs%252C%2520the%2520field%2520of%2520human%2520motion%250Aunderstanding%2520has%2520increasingly%2520shifted%2520towards%2520the%2520development%2520of%2520large%2520motion%250Amodels.%2520Despite%2520some%2520progress%252C%2520current%2520state-of-the-art%2520works%2520remain%2520far%2520from%250Aachieving%2520truly%2520generalist%2520models%252C%2520largely%2520due%2520to%2520the%2520lack%2520of%2520large-scale%252C%250Ahigh-quality%2520motion%2520data.%2520To%2520address%2520this%252C%2520we%2520present%2520MotionBase%252C%2520the%2520first%250Amillion-level%2520motion%2520generation%2520benchmark%252C%2520offering%252015%2520times%2520the%2520data%2520volume%2520of%250Athe%2520previous%2520largest%2520dataset%252C%2520and%2520featuring%2520multimodal%2520data%2520with%2520hierarchically%250Adetailed%2520text%2520descriptions.%2520By%2520leveraging%2520this%2520vast%2520dataset%252C%2520our%2520large%2520motion%250Amodel%2520demonstrates%2520strong%2520performance%2520across%2520a%2520broad%2520range%2520of%2520motions%252C%250Aincluding%2520unseen%2520ones.%2520Through%2520systematic%2520investigation%252C%2520we%2520underscore%2520the%250Aimportance%2520of%2520scaling%2520both%2520data%2520and%2520model%2520size%252C%2520with%2520synthetic%2520data%2520and%2520pseudo%250Alabels%2520playing%2520a%2520crucial%2520role%2520in%2520mitigating%2520data%2520acquisition%2520costs.%2520Moreover%252C%250Aour%2520research%2520reveals%2520the%2520limitations%2520of%2520existing%2520evaluation%2520metrics%252C%250Aparticularly%2520in%2520handling%2520out-of-domain%2520text%2520instructions%2520--%2520an%2520issue%2520that%2520has%250Along%2520been%2520overlooked.%2520In%2520addition%2520to%2520these%252C%2520we%2520introduce%2520a%2520novel%25202D%2520lookup-free%250Aapproach%2520for%2520motion%2520tokenization%252C%2520which%2520preserves%2520motion%2520information%2520and%250Aexpands%2520codebook%2520capacity%252C%2520further%2520enhancing%2520the%2520representative%2520ability%2520of%250Alarge%2520motion%2520models.%2520The%2520release%2520of%2520MotionBase%2520and%2520the%2520insights%2520gained%2520from%250Athis%2520study%2520are%2520expected%2520to%2520pave%2520the%2520way%2520for%2520the%2520development%2520of%2520more%2520powerful%250Aand%2520versatile%2520motion%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quo%20Vadis%2C%20Motion%20Generation%3F%20From%20Large%20Language%20Models%20to%20Large%20Motion%0A%20%20Models&entry.906535625=Ye%20Wang%20and%20Sipeng%20Zheng%20and%20Bin%20Cao%20and%20Qianshan%20Wei%20and%20Qin%20Jin%20and%20Zongqing%20Lu&entry.1292438233=%20%20Inspired%20by%20the%20recent%20success%20of%20LLMs%2C%20the%20field%20of%20human%20motion%0Aunderstanding%20has%20increasingly%20shifted%20towards%20the%20development%20of%20large%20motion%0Amodels.%20Despite%20some%20progress%2C%20current%20state-of-the-art%20works%20remain%20far%20from%0Aachieving%20truly%20generalist%20models%2C%20largely%20due%20to%20the%20lack%20of%20large-scale%2C%0Ahigh-quality%20motion%20data.%20To%20address%20this%2C%20we%20present%20MotionBase%2C%20the%20first%0Amillion-level%20motion%20generation%20benchmark%2C%20offering%2015%20times%20the%20data%20volume%20of%0Athe%20previous%20largest%20dataset%2C%20and%20featuring%20multimodal%20data%20with%20hierarchically%0Adetailed%20text%20descriptions.%20By%20leveraging%20this%20vast%20dataset%2C%20our%20large%20motion%0Amodel%20demonstrates%20strong%20performance%20across%20a%20broad%20range%20of%20motions%2C%0Aincluding%20unseen%20ones.%20Through%20systematic%20investigation%2C%20we%20underscore%20the%0Aimportance%20of%20scaling%20both%20data%20and%20model%20size%2C%20with%20synthetic%20data%20and%20pseudo%0Alabels%20playing%20a%20crucial%20role%20in%20mitigating%20data%20acquisition%20costs.%20Moreover%2C%0Aour%20research%20reveals%20the%20limitations%20of%20existing%20evaluation%20metrics%2C%0Aparticularly%20in%20handling%20out-of-domain%20text%20instructions%20--%20an%20issue%20that%20has%0Along%20been%20overlooked.%20In%20addition%20to%20these%2C%20we%20introduce%20a%20novel%202D%20lookup-free%0Aapproach%20for%20motion%20tokenization%2C%20which%20preserves%20motion%20information%20and%0Aexpands%20codebook%20capacity%2C%20further%20enhancing%20the%20representative%20ability%20of%0Alarge%20motion%20models.%20The%20release%20of%20MotionBase%20and%20the%20insights%20gained%20from%0Athis%20study%20are%20expected%20to%20pave%20the%20way%20for%20the%20development%20of%20more%20powerful%0Aand%20versatile%20motion%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03311v1&entry.124074799=Read"},
{"title": "Classification-Denoising Networks", "author": "Louis Thiry and Florentin Guth", "abstract": "  Image classification and denoising suffer from complementary issues of lack\nof robustness or partially ignoring conditioning information. We argue that\nthey can be alleviated by unifying both tasks through a model of the joint\nprobability of (noisy) images and class labels. Classification is performed\nwith a forward pass followed by conditioning. Using the Tweedie-Miyasawa\nformula, we evaluate the denoising function with the score, which can be\ncomputed by marginalization and back-propagation. The training objective is\nthen a combination of cross-entropy loss and denoising score matching loss\nintegrated over noise levels. Numerical experiments on CIFAR-10 and ImageNet\nshow competitive classification and denoising performance compared to reference\ndeep convolutional classifiers/denoisers, and significantly improves efficiency\ncompared to previous joint approaches. Our model shows an increased robustness\nto adversarial perturbations compared to a standard discriminative classifier,\nand allows for a novel interpretation of adversarial gradients as a difference\nof denoisers.\n", "link": "http://arxiv.org/abs/2410.03505v1", "date": "2024-10-04", "relevancy": 2.2387, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5661}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5579}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification-Denoising%20Networks&body=Title%3A%20Classification-Denoising%20Networks%0AAuthor%3A%20Louis%20Thiry%20and%20Florentin%20Guth%0AAbstract%3A%20%20%20Image%20classification%20and%20denoising%20suffer%20from%20complementary%20issues%20of%20lack%0Aof%20robustness%20or%20partially%20ignoring%20conditioning%20information.%20We%20argue%20that%0Athey%20can%20be%20alleviated%20by%20unifying%20both%20tasks%20through%20a%20model%20of%20the%20joint%0Aprobability%20of%20%28noisy%29%20images%20and%20class%20labels.%20Classification%20is%20performed%0Awith%20a%20forward%20pass%20followed%20by%20conditioning.%20Using%20the%20Tweedie-Miyasawa%0Aformula%2C%20we%20evaluate%20the%20denoising%20function%20with%20the%20score%2C%20which%20can%20be%0Acomputed%20by%20marginalization%20and%20back-propagation.%20The%20training%20objective%20is%0Athen%20a%20combination%20of%20cross-entropy%20loss%20and%20denoising%20score%20matching%20loss%0Aintegrated%20over%20noise%20levels.%20Numerical%20experiments%20on%20CIFAR-10%20and%20ImageNet%0Ashow%20competitive%20classification%20and%20denoising%20performance%20compared%20to%20reference%0Adeep%20convolutional%20classifiers/denoisers%2C%20and%20significantly%20improves%20efficiency%0Acompared%20to%20previous%20joint%20approaches.%20Our%20model%20shows%20an%20increased%20robustness%0Ato%20adversarial%20perturbations%20compared%20to%20a%20standard%20discriminative%20classifier%2C%0Aand%20allows%20for%20a%20novel%20interpretation%20of%20adversarial%20gradients%20as%20a%20difference%0Aof%20denoisers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification-Denoising%2520Networks%26entry.906535625%3DLouis%2520Thiry%2520and%2520Florentin%2520Guth%26entry.1292438233%3D%2520%2520Image%2520classification%2520and%2520denoising%2520suffer%2520from%2520complementary%2520issues%2520of%2520lack%250Aof%2520robustness%2520or%2520partially%2520ignoring%2520conditioning%2520information.%2520We%2520argue%2520that%250Athey%2520can%2520be%2520alleviated%2520by%2520unifying%2520both%2520tasks%2520through%2520a%2520model%2520of%2520the%2520joint%250Aprobability%2520of%2520%2528noisy%2529%2520images%2520and%2520class%2520labels.%2520Classification%2520is%2520performed%250Awith%2520a%2520forward%2520pass%2520followed%2520by%2520conditioning.%2520Using%2520the%2520Tweedie-Miyasawa%250Aformula%252C%2520we%2520evaluate%2520the%2520denoising%2520function%2520with%2520the%2520score%252C%2520which%2520can%2520be%250Acomputed%2520by%2520marginalization%2520and%2520back-propagation.%2520The%2520training%2520objective%2520is%250Athen%2520a%2520combination%2520of%2520cross-entropy%2520loss%2520and%2520denoising%2520score%2520matching%2520loss%250Aintegrated%2520over%2520noise%2520levels.%2520Numerical%2520experiments%2520on%2520CIFAR-10%2520and%2520ImageNet%250Ashow%2520competitive%2520classification%2520and%2520denoising%2520performance%2520compared%2520to%2520reference%250Adeep%2520convolutional%2520classifiers/denoisers%252C%2520and%2520significantly%2520improves%2520efficiency%250Acompared%2520to%2520previous%2520joint%2520approaches.%2520Our%2520model%2520shows%2520an%2520increased%2520robustness%250Ato%2520adversarial%2520perturbations%2520compared%2520to%2520a%2520standard%2520discriminative%2520classifier%252C%250Aand%2520allows%2520for%2520a%2520novel%2520interpretation%2520of%2520adversarial%2520gradients%2520as%2520a%2520difference%250Aof%2520denoisers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification-Denoising%20Networks&entry.906535625=Louis%20Thiry%20and%20Florentin%20Guth&entry.1292438233=%20%20Image%20classification%20and%20denoising%20suffer%20from%20complementary%20issues%20of%20lack%0Aof%20robustness%20or%20partially%20ignoring%20conditioning%20information.%20We%20argue%20that%0Athey%20can%20be%20alleviated%20by%20unifying%20both%20tasks%20through%20a%20model%20of%20the%20joint%0Aprobability%20of%20%28noisy%29%20images%20and%20class%20labels.%20Classification%20is%20performed%0Awith%20a%20forward%20pass%20followed%20by%20conditioning.%20Using%20the%20Tweedie-Miyasawa%0Aformula%2C%20we%20evaluate%20the%20denoising%20function%20with%20the%20score%2C%20which%20can%20be%0Acomputed%20by%20marginalization%20and%20back-propagation.%20The%20training%20objective%20is%0Athen%20a%20combination%20of%20cross-entropy%20loss%20and%20denoising%20score%20matching%20loss%0Aintegrated%20over%20noise%20levels.%20Numerical%20experiments%20on%20CIFAR-10%20and%20ImageNet%0Ashow%20competitive%20classification%20and%20denoising%20performance%20compared%20to%20reference%0Adeep%20convolutional%20classifiers/denoisers%2C%20and%20significantly%20improves%20efficiency%0Acompared%20to%20previous%20joint%20approaches.%20Our%20model%20shows%20an%20increased%20robustness%0Ato%20adversarial%20perturbations%20compared%20to%20a%20standard%20discriminative%20classifier%2C%0Aand%20allows%20for%20a%20novel%20interpretation%20of%20adversarial%20gradients%20as%20a%20difference%0Aof%20denoisers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03505v1&entry.124074799=Read"},
{"title": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval Augmented Generation", "author": "Tobias Leemann and Periklis Petridis and Giuseppe Vietri and Dionysis Manousakas and Aaron Roth and Sergul Aydore", "abstract": "  While retrieval augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. One common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10 % of their computational cost.\n", "link": "http://arxiv.org/abs/2410.03461v1", "date": "2024-10-04", "relevancy": 2.2314, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5715}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5518}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-GDA%3A%20Automatic%20Domain%20Adaptation%20for%20Efficient%20Grounding%0A%20%20Verification%20in%20Retrieval%20Augmented%20Generation&body=Title%3A%20Auto-GDA%3A%20Automatic%20Domain%20Adaptation%20for%20Efficient%20Grounding%0A%20%20Verification%20in%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Tobias%20Leemann%20and%20Periklis%20Petridis%20and%20Giuseppe%20Vietri%20and%20Dionysis%20Manousakas%20and%20Aaron%20Roth%20and%20Sergul%20Aydore%0AAbstract%3A%20%20%20While%20retrieval%20augmented%20generation%20%28RAG%29%20has%20been%20shown%20to%20enhance%0Afactuality%20of%20large%20language%20model%20%28LLM%29%20outputs%2C%20LLMs%20still%20suffer%20from%0Ahallucination%2C%20generating%20incorrect%20or%20irrelevant%20information.%20One%20common%0Adetection%20strategy%20involves%20prompting%20the%20LLM%20again%20to%20assess%20whether%20its%0Aresponse%20is%20grounded%20in%20the%20retrieved%20evidence%2C%20but%20this%20approach%20is%20costly.%0AAlternatively%2C%20lightweight%20natural%20language%20inference%20%28NLI%29%20models%20for%0Aefficient%20grounding%20verification%20can%20be%20used%20at%20inference%20time.%20While%20existing%0Apre-trained%20NLI%20models%20offer%20potential%20solutions%2C%20their%20performance%20remains%0Asubpar%20compared%20to%20larger%20models%20on%20realistic%20RAG%20inputs.%20RAG%20inputs%20are%20more%0Acomplex%20than%20most%20datasets%20used%20for%20training%20NLI%20models%20and%20have%0Acharacteristics%20specific%20to%20the%20underlying%20knowledge%20base%2C%20requiring%20adaptation%0Aof%20the%20NLI%20models%20to%20a%20specific%20target%20domain.%20Additionally%2C%20the%20lack%20of%0Alabeled%20instances%20in%20the%20target%20domain%20makes%20supervised%20domain%20adaptation%2C%0Ae.g.%2C%20through%20fine-tuning%2C%20infeasible.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Automatic%20Generative%20Domain%20Adaptation%20%28Auto-GDA%29.%20Our%20framework%0Aenables%20unsupervised%20domain%20adaptation%20through%20synthetic%20data%20generation.%0AUnlike%20previous%20methods%20that%20rely%20on%20handcrafted%20filtering%20and%20augmentation%0Astrategies%2C%20Auto-GDA%20employs%20an%20iterative%20process%20to%20continuously%20improve%20the%0Aquality%20of%20generated%20samples%20using%20weak%20labels%20from%20less%20efficient%20teacher%0Amodels%20and%20discrete%20optimization%20to%20select%20the%20most%20promising%20augmented%0Asamples.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Awith%20models%20fine-tuned%20on%20synthetic%20data%20using%20Auto-GDA%20often%20surpassing%20the%0Aperformance%20of%20the%20teacher%20model%20and%20reaching%20the%20performance%20level%20of%20LLMs%20at%0A10%20%25%20of%20their%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-GDA%253A%2520Automatic%2520Domain%2520Adaptation%2520for%2520Efficient%2520Grounding%250A%2520%2520Verification%2520in%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DTobias%2520Leemann%2520and%2520Periklis%2520Petridis%2520and%2520Giuseppe%2520Vietri%2520and%2520Dionysis%2520Manousakas%2520and%2520Aaron%2520Roth%2520and%2520Sergul%2520Aydore%26entry.1292438233%3D%2520%2520While%2520retrieval%2520augmented%2520generation%2520%2528RAG%2529%2520has%2520been%2520shown%2520to%2520enhance%250Afactuality%2520of%2520large%2520language%2520model%2520%2528LLM%2529%2520outputs%252C%2520LLMs%2520still%2520suffer%2520from%250Ahallucination%252C%2520generating%2520incorrect%2520or%2520irrelevant%2520information.%2520One%2520common%250Adetection%2520strategy%2520involves%2520prompting%2520the%2520LLM%2520again%2520to%2520assess%2520whether%2520its%250Aresponse%2520is%2520grounded%2520in%2520the%2520retrieved%2520evidence%252C%2520but%2520this%2520approach%2520is%2520costly.%250AAlternatively%252C%2520lightweight%2520natural%2520language%2520inference%2520%2528NLI%2529%2520models%2520for%250Aefficient%2520grounding%2520verification%2520can%2520be%2520used%2520at%2520inference%2520time.%2520While%2520existing%250Apre-trained%2520NLI%2520models%2520offer%2520potential%2520solutions%252C%2520their%2520performance%2520remains%250Asubpar%2520compared%2520to%2520larger%2520models%2520on%2520realistic%2520RAG%2520inputs.%2520RAG%2520inputs%2520are%2520more%250Acomplex%2520than%2520most%2520datasets%2520used%2520for%2520training%2520NLI%2520models%2520and%2520have%250Acharacteristics%2520specific%2520to%2520the%2520underlying%2520knowledge%2520base%252C%2520requiring%2520adaptation%250Aof%2520the%2520NLI%2520models%2520to%2520a%2520specific%2520target%2520domain.%2520Additionally%252C%2520the%2520lack%2520of%250Alabeled%2520instances%2520in%2520the%2520target%2520domain%2520makes%2520supervised%2520domain%2520adaptation%252C%250Ae.g.%252C%2520through%2520fine-tuning%252C%2520infeasible.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520Automatic%2520Generative%2520Domain%2520Adaptation%2520%2528Auto-GDA%2529.%2520Our%2520framework%250Aenables%2520unsupervised%2520domain%2520adaptation%2520through%2520synthetic%2520data%2520generation.%250AUnlike%2520previous%2520methods%2520that%2520rely%2520on%2520handcrafted%2520filtering%2520and%2520augmentation%250Astrategies%252C%2520Auto-GDA%2520employs%2520an%2520iterative%2520process%2520to%2520continuously%2520improve%2520the%250Aquality%2520of%2520generated%2520samples%2520using%2520weak%2520labels%2520from%2520less%2520efficient%2520teacher%250Amodels%2520and%2520discrete%2520optimization%2520to%2520select%2520the%2520most%2520promising%2520augmented%250Asamples.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%250Awith%2520models%2520fine-tuned%2520on%2520synthetic%2520data%2520using%2520Auto-GDA%2520often%2520surpassing%2520the%250Aperformance%2520of%2520the%2520teacher%2520model%2520and%2520reaching%2520the%2520performance%2520level%2520of%2520LLMs%2520at%250A10%2520%2525%2520of%2520their%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-GDA%3A%20Automatic%20Domain%20Adaptation%20for%20Efficient%20Grounding%0A%20%20Verification%20in%20Retrieval%20Augmented%20Generation&entry.906535625=Tobias%20Leemann%20and%20Periklis%20Petridis%20and%20Giuseppe%20Vietri%20and%20Dionysis%20Manousakas%20and%20Aaron%20Roth%20and%20Sergul%20Aydore&entry.1292438233=%20%20While%20retrieval%20augmented%20generation%20%28RAG%29%20has%20been%20shown%20to%20enhance%0Afactuality%20of%20large%20language%20model%20%28LLM%29%20outputs%2C%20LLMs%20still%20suffer%20from%0Ahallucination%2C%20generating%20incorrect%20or%20irrelevant%20information.%20One%20common%0Adetection%20strategy%20involves%20prompting%20the%20LLM%20again%20to%20assess%20whether%20its%0Aresponse%20is%20grounded%20in%20the%20retrieved%20evidence%2C%20but%20this%20approach%20is%20costly.%0AAlternatively%2C%20lightweight%20natural%20language%20inference%20%28NLI%29%20models%20for%0Aefficient%20grounding%20verification%20can%20be%20used%20at%20inference%20time.%20While%20existing%0Apre-trained%20NLI%20models%20offer%20potential%20solutions%2C%20their%20performance%20remains%0Asubpar%20compared%20to%20larger%20models%20on%20realistic%20RAG%20inputs.%20RAG%20inputs%20are%20more%0Acomplex%20than%20most%20datasets%20used%20for%20training%20NLI%20models%20and%20have%0Acharacteristics%20specific%20to%20the%20underlying%20knowledge%20base%2C%20requiring%20adaptation%0Aof%20the%20NLI%20models%20to%20a%20specific%20target%20domain.%20Additionally%2C%20the%20lack%20of%0Alabeled%20instances%20in%20the%20target%20domain%20makes%20supervised%20domain%20adaptation%2C%0Ae.g.%2C%20through%20fine-tuning%2C%20infeasible.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Automatic%20Generative%20Domain%20Adaptation%20%28Auto-GDA%29.%20Our%20framework%0Aenables%20unsupervised%20domain%20adaptation%20through%20synthetic%20data%20generation.%0AUnlike%20previous%20methods%20that%20rely%20on%20handcrafted%20filtering%20and%20augmentation%0Astrategies%2C%20Auto-GDA%20employs%20an%20iterative%20process%20to%20continuously%20improve%20the%0Aquality%20of%20generated%20samples%20using%20weak%20labels%20from%20less%20efficient%20teacher%0Amodels%20and%20discrete%20optimization%20to%20select%20the%20most%20promising%20augmented%0Asamples.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%0Awith%20models%20fine-tuned%20on%20synthetic%20data%20using%20Auto-GDA%20often%20surpassing%20the%0Aperformance%20of%20the%20teacher%20model%20and%20reaching%20the%20performance%20level%20of%20LLMs%20at%0A10%20%25%20of%20their%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03461v1&entry.124074799=Read"},
{"title": "Aircraft Radar Altimeter Interference Mitigation Through a CNN-Layer\n  Only Denoising Autoencoder Architecture", "author": "Samuel B. Brown and Stephen Young and Adam Wagenknecht and Daniel Jakubisin and Charles E. Thornton and Aaron Orndorff and William C. Headley", "abstract": "  Denoising autoencoders for signal processing applications have been shown to\nexperience significant difficulty in learning to reconstruct radio frequency\ncommunication signals, particularly in the large sample regime. In\ncommunication systems, this challenge is primarily due to the need to\nreconstruct the modulated data stream which is generally highly stochastic in\nnature. In this work, we take advantage of this limitation by using the\ndenoising autoencoder to instead remove interfering radio frequency\ncommunication signals while reconstructing highly structured FMCW radar\nsignals. More specifically, in this work we show that a CNN-layer only\nautoencoder architecture can be utilized to improve the accuracy of a radar\naltimeter's ranging estimate even in severe interference environments\nconsisting of a multitude of interference signals. This is demonstrated through\ncomprehensive performance analysis of an end-to-end FMCW radar altimeter\nsimulation with and without the convolutional layer-only autoencoder. The\nproposed approach significantly improves interference mitigation in the\npresence of both narrow-band tone interference as well as wideband QPSK\ninterference in terms of range RMS error, number of false altitude reports, and\nthe peak-to-sidelobe ratio of the resulting range profile. FMCW radar signals\nof up to 40,000 IQ samples can be reliably reconstructed.\n", "link": "http://arxiv.org/abs/2410.03423v1", "date": "2024-10-04", "relevancy": 2.2285, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4681}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4451}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aircraft%20Radar%20Altimeter%20Interference%20Mitigation%20Through%20a%20CNN-Layer%0A%20%20Only%20Denoising%20Autoencoder%20Architecture&body=Title%3A%20Aircraft%20Radar%20Altimeter%20Interference%20Mitigation%20Through%20a%20CNN-Layer%0A%20%20Only%20Denoising%20Autoencoder%20Architecture%0AAuthor%3A%20Samuel%20B.%20Brown%20and%20Stephen%20Young%20and%20Adam%20Wagenknecht%20and%20Daniel%20Jakubisin%20and%20Charles%20E.%20Thornton%20and%20Aaron%20Orndorff%20and%20William%20C.%20Headley%0AAbstract%3A%20%20%20Denoising%20autoencoders%20for%20signal%20processing%20applications%20have%20been%20shown%20to%0Aexperience%20significant%20difficulty%20in%20learning%20to%20reconstruct%20radio%20frequency%0Acommunication%20signals%2C%20particularly%20in%20the%20large%20sample%20regime.%20In%0Acommunication%20systems%2C%20this%20challenge%20is%20primarily%20due%20to%20the%20need%20to%0Areconstruct%20the%20modulated%20data%20stream%20which%20is%20generally%20highly%20stochastic%20in%0Anature.%20In%20this%20work%2C%20we%20take%20advantage%20of%20this%20limitation%20by%20using%20the%0Adenoising%20autoencoder%20to%20instead%20remove%20interfering%20radio%20frequency%0Acommunication%20signals%20while%20reconstructing%20highly%20structured%20FMCW%20radar%0Asignals.%20More%20specifically%2C%20in%20this%20work%20we%20show%20that%20a%20CNN-layer%20only%0Aautoencoder%20architecture%20can%20be%20utilized%20to%20improve%20the%20accuracy%20of%20a%20radar%0Aaltimeter%27s%20ranging%20estimate%20even%20in%20severe%20interference%20environments%0Aconsisting%20of%20a%20multitude%20of%20interference%20signals.%20This%20is%20demonstrated%20through%0Acomprehensive%20performance%20analysis%20of%20an%20end-to-end%20FMCW%20radar%20altimeter%0Asimulation%20with%20and%20without%20the%20convolutional%20layer-only%20autoencoder.%20The%0Aproposed%20approach%20significantly%20improves%20interference%20mitigation%20in%20the%0Apresence%20of%20both%20narrow-band%20tone%20interference%20as%20well%20as%20wideband%20QPSK%0Ainterference%20in%20terms%20of%20range%20RMS%20error%2C%20number%20of%20false%20altitude%20reports%2C%20and%0Athe%20peak-to-sidelobe%20ratio%20of%20the%20resulting%20range%20profile.%20FMCW%20radar%20signals%0Aof%20up%20to%2040%2C000%20IQ%20samples%20can%20be%20reliably%20reconstructed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAircraft%2520Radar%2520Altimeter%2520Interference%2520Mitigation%2520Through%2520a%2520CNN-Layer%250A%2520%2520Only%2520Denoising%2520Autoencoder%2520Architecture%26entry.906535625%3DSamuel%2520B.%2520Brown%2520and%2520Stephen%2520Young%2520and%2520Adam%2520Wagenknecht%2520and%2520Daniel%2520Jakubisin%2520and%2520Charles%2520E.%2520Thornton%2520and%2520Aaron%2520Orndorff%2520and%2520William%2520C.%2520Headley%26entry.1292438233%3D%2520%2520Denoising%2520autoencoders%2520for%2520signal%2520processing%2520applications%2520have%2520been%2520shown%2520to%250Aexperience%2520significant%2520difficulty%2520in%2520learning%2520to%2520reconstruct%2520radio%2520frequency%250Acommunication%2520signals%252C%2520particularly%2520in%2520the%2520large%2520sample%2520regime.%2520In%250Acommunication%2520systems%252C%2520this%2520challenge%2520is%2520primarily%2520due%2520to%2520the%2520need%2520to%250Areconstruct%2520the%2520modulated%2520data%2520stream%2520which%2520is%2520generally%2520highly%2520stochastic%2520in%250Anature.%2520In%2520this%2520work%252C%2520we%2520take%2520advantage%2520of%2520this%2520limitation%2520by%2520using%2520the%250Adenoising%2520autoencoder%2520to%2520instead%2520remove%2520interfering%2520radio%2520frequency%250Acommunication%2520signals%2520while%2520reconstructing%2520highly%2520structured%2520FMCW%2520radar%250Asignals.%2520More%2520specifically%252C%2520in%2520this%2520work%2520we%2520show%2520that%2520a%2520CNN-layer%2520only%250Aautoencoder%2520architecture%2520can%2520be%2520utilized%2520to%2520improve%2520the%2520accuracy%2520of%2520a%2520radar%250Aaltimeter%2527s%2520ranging%2520estimate%2520even%2520in%2520severe%2520interference%2520environments%250Aconsisting%2520of%2520a%2520multitude%2520of%2520interference%2520signals.%2520This%2520is%2520demonstrated%2520through%250Acomprehensive%2520performance%2520analysis%2520of%2520an%2520end-to-end%2520FMCW%2520radar%2520altimeter%250Asimulation%2520with%2520and%2520without%2520the%2520convolutional%2520layer-only%2520autoencoder.%2520The%250Aproposed%2520approach%2520significantly%2520improves%2520interference%2520mitigation%2520in%2520the%250Apresence%2520of%2520both%2520narrow-band%2520tone%2520interference%2520as%2520well%2520as%2520wideband%2520QPSK%250Ainterference%2520in%2520terms%2520of%2520range%2520RMS%2520error%252C%2520number%2520of%2520false%2520altitude%2520reports%252C%2520and%250Athe%2520peak-to-sidelobe%2520ratio%2520of%2520the%2520resulting%2520range%2520profile.%2520FMCW%2520radar%2520signals%250Aof%2520up%2520to%252040%252C000%2520IQ%2520samples%2520can%2520be%2520reliably%2520reconstructed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aircraft%20Radar%20Altimeter%20Interference%20Mitigation%20Through%20a%20CNN-Layer%0A%20%20Only%20Denoising%20Autoencoder%20Architecture&entry.906535625=Samuel%20B.%20Brown%20and%20Stephen%20Young%20and%20Adam%20Wagenknecht%20and%20Daniel%20Jakubisin%20and%20Charles%20E.%20Thornton%20and%20Aaron%20Orndorff%20and%20William%20C.%20Headley&entry.1292438233=%20%20Denoising%20autoencoders%20for%20signal%20processing%20applications%20have%20been%20shown%20to%0Aexperience%20significant%20difficulty%20in%20learning%20to%20reconstruct%20radio%20frequency%0Acommunication%20signals%2C%20particularly%20in%20the%20large%20sample%20regime.%20In%0Acommunication%20systems%2C%20this%20challenge%20is%20primarily%20due%20to%20the%20need%20to%0Areconstruct%20the%20modulated%20data%20stream%20which%20is%20generally%20highly%20stochastic%20in%0Anature.%20In%20this%20work%2C%20we%20take%20advantage%20of%20this%20limitation%20by%20using%20the%0Adenoising%20autoencoder%20to%20instead%20remove%20interfering%20radio%20frequency%0Acommunication%20signals%20while%20reconstructing%20highly%20structured%20FMCW%20radar%0Asignals.%20More%20specifically%2C%20in%20this%20work%20we%20show%20that%20a%20CNN-layer%20only%0Aautoencoder%20architecture%20can%20be%20utilized%20to%20improve%20the%20accuracy%20of%20a%20radar%0Aaltimeter%27s%20ranging%20estimate%20even%20in%20severe%20interference%20environments%0Aconsisting%20of%20a%20multitude%20of%20interference%20signals.%20This%20is%20demonstrated%20through%0Acomprehensive%20performance%20analysis%20of%20an%20end-to-end%20FMCW%20radar%20altimeter%0Asimulation%20with%20and%20without%20the%20convolutional%20layer-only%20autoencoder.%20The%0Aproposed%20approach%20significantly%20improves%20interference%20mitigation%20in%20the%0Apresence%20of%20both%20narrow-band%20tone%20interference%20as%20well%20as%20wideband%20QPSK%0Ainterference%20in%20terms%20of%20range%20RMS%20error%2C%20number%20of%20false%20altitude%20reports%2C%20and%0Athe%20peak-to-sidelobe%20ratio%20of%20the%20resulting%20range%20profile.%20FMCW%20radar%20signals%0Aof%20up%20to%2040%2C000%20IQ%20samples%20can%20be%20reliably%20reconstructed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03423v1&entry.124074799=Read"},
{"title": "LANTERN: Accelerating Visual Autoregressive Models with Relaxed\n  Speculative Decoding", "author": "Doohyuk Jang and Sihwan Park and June Yong Yang and Yeonsung Jung and Jihun Yun and Souvik Kundu and Sung-Yub Kim and Eunho Yang", "abstract": "  Auto-Regressive (AR) models have recently gained prominence in image\ngeneration, often matching or even surpassing the performance of diffusion\nmodels. However, one major limitation of AR models is their sequential nature,\nwhich processes tokens one at a time, slowing down generation compared to\nmodels like GANs or diffusion-based methods that operate more efficiently.\nWhile speculative decoding has proven effective for accelerating LLMs by\ngenerating multiple tokens in a single forward, its application in visual AR\nmodels remains largely unexplored. In this work, we identify a challenge in\nthis setting, which we term \\textit{token selection ambiguity}, wherein visual\nAR models frequently assign uniformly low probabilities to tokens, hampering\nthe performance of speculative decoding. To overcome this challenge, we propose\na relaxed acceptance condition referred to as LANTERN that leverages the\ninterchangeability of tokens in latent space. This relaxation restores the\neffectiveness of speculative decoding in visual AR models by enabling more\nflexible use of candidate tokens that would otherwise be prematurely rejected.\nFurthermore, by incorporating a total variation distance bound, we ensure that\nthese speed gains are achieved without significantly compromising image quality\nor semantic coherence. Experimental results demonstrate the efficacy of our\nmethod in providing a substantial speed-up over speculative decoding. In\nspecific, compared to a na\\\"ive application of the state-of-the-art speculative\ndecoding, LANTERN increases speed-ups by $\\mathbf{1.75}\\times$ and\n$\\mathbf{1.76}\\times$, as compared to greedy decoding and random sampling,\nrespectively, when applied to LlamaGen, a contemporary visual AR model.\n", "link": "http://arxiv.org/abs/2410.03355v1", "date": "2024-10-04", "relevancy": 2.2222, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6034}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LANTERN%3A%20Accelerating%20Visual%20Autoregressive%20Models%20with%20Relaxed%0A%20%20Speculative%20Decoding&body=Title%3A%20LANTERN%3A%20Accelerating%20Visual%20Autoregressive%20Models%20with%20Relaxed%0A%20%20Speculative%20Decoding%0AAuthor%3A%20Doohyuk%20Jang%20and%20Sihwan%20Park%20and%20June%20Yong%20Yang%20and%20Yeonsung%20Jung%20and%20Jihun%20Yun%20and%20Souvik%20Kundu%20and%20Sung-Yub%20Kim%20and%20Eunho%20Yang%0AAbstract%3A%20%20%20Auto-Regressive%20%28AR%29%20models%20have%20recently%20gained%20prominence%20in%20image%0Ageneration%2C%20often%20matching%20or%20even%20surpassing%20the%20performance%20of%20diffusion%0Amodels.%20However%2C%20one%20major%20limitation%20of%20AR%20models%20is%20their%20sequential%20nature%2C%0Awhich%20processes%20tokens%20one%20at%20a%20time%2C%20slowing%20down%20generation%20compared%20to%0Amodels%20like%20GANs%20or%20diffusion-based%20methods%20that%20operate%20more%20efficiently.%0AWhile%20speculative%20decoding%20has%20proven%20effective%20for%20accelerating%20LLMs%20by%0Agenerating%20multiple%20tokens%20in%20a%20single%20forward%2C%20its%20application%20in%20visual%20AR%0Amodels%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20identify%20a%20challenge%20in%0Athis%20setting%2C%20which%20we%20term%20%5Ctextit%7Btoken%20selection%20ambiguity%7D%2C%20wherein%20visual%0AAR%20models%20frequently%20assign%20uniformly%20low%20probabilities%20to%20tokens%2C%20hampering%0Athe%20performance%20of%20speculative%20decoding.%20To%20overcome%20this%20challenge%2C%20we%20propose%0Aa%20relaxed%20acceptance%20condition%20referred%20to%20as%20LANTERN%20that%20leverages%20the%0Ainterchangeability%20of%20tokens%20in%20latent%20space.%20This%20relaxation%20restores%20the%0Aeffectiveness%20of%20speculative%20decoding%20in%20visual%20AR%20models%20by%20enabling%20more%0Aflexible%20use%20of%20candidate%20tokens%20that%20would%20otherwise%20be%20prematurely%20rejected.%0AFurthermore%2C%20by%20incorporating%20a%20total%20variation%20distance%20bound%2C%20we%20ensure%20that%0Athese%20speed%20gains%20are%20achieved%20without%20significantly%20compromising%20image%20quality%0Aor%20semantic%20coherence.%20Experimental%20results%20demonstrate%20the%20efficacy%20of%20our%0Amethod%20in%20providing%20a%20substantial%20speed-up%20over%20speculative%20decoding.%20In%0Aspecific%2C%20compared%20to%20a%20na%5C%22ive%20application%20of%20the%20state-of-the-art%20speculative%0Adecoding%2C%20LANTERN%20increases%20speed-ups%20by%20%24%5Cmathbf%7B1.75%7D%5Ctimes%24%20and%0A%24%5Cmathbf%7B1.76%7D%5Ctimes%24%2C%20as%20compared%20to%20greedy%20decoding%20and%20random%20sampling%2C%0Arespectively%2C%20when%20applied%20to%20LlamaGen%2C%20a%20contemporary%20visual%20AR%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLANTERN%253A%2520Accelerating%2520Visual%2520Autoregressive%2520Models%2520with%2520Relaxed%250A%2520%2520Speculative%2520Decoding%26entry.906535625%3DDoohyuk%2520Jang%2520and%2520Sihwan%2520Park%2520and%2520June%2520Yong%2520Yang%2520and%2520Yeonsung%2520Jung%2520and%2520Jihun%2520Yun%2520and%2520Souvik%2520Kundu%2520and%2520Sung-Yub%2520Kim%2520and%2520Eunho%2520Yang%26entry.1292438233%3D%2520%2520Auto-Regressive%2520%2528AR%2529%2520models%2520have%2520recently%2520gained%2520prominence%2520in%2520image%250Ageneration%252C%2520often%2520matching%2520or%2520even%2520surpassing%2520the%2520performance%2520of%2520diffusion%250Amodels.%2520However%252C%2520one%2520major%2520limitation%2520of%2520AR%2520models%2520is%2520their%2520sequential%2520nature%252C%250Awhich%2520processes%2520tokens%2520one%2520at%2520a%2520time%252C%2520slowing%2520down%2520generation%2520compared%2520to%250Amodels%2520like%2520GANs%2520or%2520diffusion-based%2520methods%2520that%2520operate%2520more%2520efficiently.%250AWhile%2520speculative%2520decoding%2520has%2520proven%2520effective%2520for%2520accelerating%2520LLMs%2520by%250Agenerating%2520multiple%2520tokens%2520in%2520a%2520single%2520forward%252C%2520its%2520application%2520in%2520visual%2520AR%250Amodels%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520identify%2520a%2520challenge%2520in%250Athis%2520setting%252C%2520which%2520we%2520term%2520%255Ctextit%257Btoken%2520selection%2520ambiguity%257D%252C%2520wherein%2520visual%250AAR%2520models%2520frequently%2520assign%2520uniformly%2520low%2520probabilities%2520to%2520tokens%252C%2520hampering%250Athe%2520performance%2520of%2520speculative%2520decoding.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%250Aa%2520relaxed%2520acceptance%2520condition%2520referred%2520to%2520as%2520LANTERN%2520that%2520leverages%2520the%250Ainterchangeability%2520of%2520tokens%2520in%2520latent%2520space.%2520This%2520relaxation%2520restores%2520the%250Aeffectiveness%2520of%2520speculative%2520decoding%2520in%2520visual%2520AR%2520models%2520by%2520enabling%2520more%250Aflexible%2520use%2520of%2520candidate%2520tokens%2520that%2520would%2520otherwise%2520be%2520prematurely%2520rejected.%250AFurthermore%252C%2520by%2520incorporating%2520a%2520total%2520variation%2520distance%2520bound%252C%2520we%2520ensure%2520that%250Athese%2520speed%2520gains%2520are%2520achieved%2520without%2520significantly%2520compromising%2520image%2520quality%250Aor%2520semantic%2520coherence.%2520Experimental%2520results%2520demonstrate%2520the%2520efficacy%2520of%2520our%250Amethod%2520in%2520providing%2520a%2520substantial%2520speed-up%2520over%2520speculative%2520decoding.%2520In%250Aspecific%252C%2520compared%2520to%2520a%2520na%255C%2522ive%2520application%2520of%2520the%2520state-of-the-art%2520speculative%250Adecoding%252C%2520LANTERN%2520increases%2520speed-ups%2520by%2520%2524%255Cmathbf%257B1.75%257D%255Ctimes%2524%2520and%250A%2524%255Cmathbf%257B1.76%257D%255Ctimes%2524%252C%2520as%2520compared%2520to%2520greedy%2520decoding%2520and%2520random%2520sampling%252C%250Arespectively%252C%2520when%2520applied%2520to%2520LlamaGen%252C%2520a%2520contemporary%2520visual%2520AR%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LANTERN%3A%20Accelerating%20Visual%20Autoregressive%20Models%20with%20Relaxed%0A%20%20Speculative%20Decoding&entry.906535625=Doohyuk%20Jang%20and%20Sihwan%20Park%20and%20June%20Yong%20Yang%20and%20Yeonsung%20Jung%20and%20Jihun%20Yun%20and%20Souvik%20Kundu%20and%20Sung-Yub%20Kim%20and%20Eunho%20Yang&entry.1292438233=%20%20Auto-Regressive%20%28AR%29%20models%20have%20recently%20gained%20prominence%20in%20image%0Ageneration%2C%20often%20matching%20or%20even%20surpassing%20the%20performance%20of%20diffusion%0Amodels.%20However%2C%20one%20major%20limitation%20of%20AR%20models%20is%20their%20sequential%20nature%2C%0Awhich%20processes%20tokens%20one%20at%20a%20time%2C%20slowing%20down%20generation%20compared%20to%0Amodels%20like%20GANs%20or%20diffusion-based%20methods%20that%20operate%20more%20efficiently.%0AWhile%20speculative%20decoding%20has%20proven%20effective%20for%20accelerating%20LLMs%20by%0Agenerating%20multiple%20tokens%20in%20a%20single%20forward%2C%20its%20application%20in%20visual%20AR%0Amodels%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20identify%20a%20challenge%20in%0Athis%20setting%2C%20which%20we%20term%20%5Ctextit%7Btoken%20selection%20ambiguity%7D%2C%20wherein%20visual%0AAR%20models%20frequently%20assign%20uniformly%20low%20probabilities%20to%20tokens%2C%20hampering%0Athe%20performance%20of%20speculative%20decoding.%20To%20overcome%20this%20challenge%2C%20we%20propose%0Aa%20relaxed%20acceptance%20condition%20referred%20to%20as%20LANTERN%20that%20leverages%20the%0Ainterchangeability%20of%20tokens%20in%20latent%20space.%20This%20relaxation%20restores%20the%0Aeffectiveness%20of%20speculative%20decoding%20in%20visual%20AR%20models%20by%20enabling%20more%0Aflexible%20use%20of%20candidate%20tokens%20that%20would%20otherwise%20be%20prematurely%20rejected.%0AFurthermore%2C%20by%20incorporating%20a%20total%20variation%20distance%20bound%2C%20we%20ensure%20that%0Athese%20speed%20gains%20are%20achieved%20without%20significantly%20compromising%20image%20quality%0Aor%20semantic%20coherence.%20Experimental%20results%20demonstrate%20the%20efficacy%20of%20our%0Amethod%20in%20providing%20a%20substantial%20speed-up%20over%20speculative%20decoding.%20In%0Aspecific%2C%20compared%20to%20a%20na%5C%22ive%20application%20of%20the%20state-of-the-art%20speculative%0Adecoding%2C%20LANTERN%20increases%20speed-ups%20by%20%24%5Cmathbf%7B1.75%7D%5Ctimes%24%20and%0A%24%5Cmathbf%7B1.76%7D%5Ctimes%24%2C%20as%20compared%20to%20greedy%20decoding%20and%20random%20sampling%2C%0Arespectively%2C%20when%20applied%20to%20LlamaGen%2C%20a%20contemporary%20visual%20AR%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03355v1&entry.124074799=Read"},
{"title": "Semi-Supervised Fine-Tuning of Vision Foundation Models with\n  Content-Style Decomposition", "author": "Mariia Drozdova and Vitaliy Kinakh and Yury Belousov and Erica Lastufka and Slava Voloshynovskiy", "abstract": "  In this paper, we present a semi-supervised fine-tuning approach designed to\nimprove the performance of pre-trained foundation models on downstream tasks\nwith limited labeled data. By leveraging content-style decomposition within an\ninformation-theoretic framework, our method enhances the latent representations\nof pre-trained vision foundation models, aligning them more effectively with\nspecific task objectives and addressing the problem of distribution shift. We\nevaluate our approach on multiple datasets, including MNIST, its augmented\nvariations (with yellow and white stripes), CIFAR-10, SVHN, and GalaxyMNIST.\nThe experiments show improvements over supervised finetuning baseline of\npre-trained models, particularly in low-labeled data regimes, across both\nfrozen and trainable backbones for the majority of the tested datasets.\n", "link": "http://arxiv.org/abs/2410.02069v2", "date": "2024-10-04", "relevancy": 2.206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Fine-Tuning%20of%20Vision%20Foundation%20Models%20with%0A%20%20Content-Style%20Decomposition&body=Title%3A%20Semi-Supervised%20Fine-Tuning%20of%20Vision%20Foundation%20Models%20with%0A%20%20Content-Style%20Decomposition%0AAuthor%3A%20Mariia%20Drozdova%20and%20Vitaliy%20Kinakh%20and%20Yury%20Belousov%20and%20Erica%20Lastufka%20and%20Slava%20Voloshynovskiy%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20semi-supervised%20fine-tuning%20approach%20designed%20to%0Aimprove%20the%20performance%20of%20pre-trained%20foundation%20models%20on%20downstream%20tasks%0Awith%20limited%20labeled%20data.%20By%20leveraging%20content-style%20decomposition%20within%20an%0Ainformation-theoretic%20framework%2C%20our%20method%20enhances%20the%20latent%20representations%0Aof%20pre-trained%20vision%20foundation%20models%2C%20aligning%20them%20more%20effectively%20with%0Aspecific%20task%20objectives%20and%20addressing%20the%20problem%20of%20distribution%20shift.%20We%0Aevaluate%20our%20approach%20on%20multiple%20datasets%2C%20including%20MNIST%2C%20its%20augmented%0Avariations%20%28with%20yellow%20and%20white%20stripes%29%2C%20CIFAR-10%2C%20SVHN%2C%20and%20GalaxyMNIST.%0AThe%20experiments%20show%20improvements%20over%20supervised%20finetuning%20baseline%20of%0Apre-trained%20models%2C%20particularly%20in%20low-labeled%20data%20regimes%2C%20across%20both%0Afrozen%20and%20trainable%20backbones%20for%20the%20majority%20of%20the%20tested%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Fine-Tuning%2520of%2520Vision%2520Foundation%2520Models%2520with%250A%2520%2520Content-Style%2520Decomposition%26entry.906535625%3DMariia%2520Drozdova%2520and%2520Vitaliy%2520Kinakh%2520and%2520Yury%2520Belousov%2520and%2520Erica%2520Lastufka%2520and%2520Slava%2520Voloshynovskiy%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520semi-supervised%2520fine-tuning%2520approach%2520designed%2520to%250Aimprove%2520the%2520performance%2520of%2520pre-trained%2520foundation%2520models%2520on%2520downstream%2520tasks%250Awith%2520limited%2520labeled%2520data.%2520By%2520leveraging%2520content-style%2520decomposition%2520within%2520an%250Ainformation-theoretic%2520framework%252C%2520our%2520method%2520enhances%2520the%2520latent%2520representations%250Aof%2520pre-trained%2520vision%2520foundation%2520models%252C%2520aligning%2520them%2520more%2520effectively%2520with%250Aspecific%2520task%2520objectives%2520and%2520addressing%2520the%2520problem%2520of%2520distribution%2520shift.%2520We%250Aevaluate%2520our%2520approach%2520on%2520multiple%2520datasets%252C%2520including%2520MNIST%252C%2520its%2520augmented%250Avariations%2520%2528with%2520yellow%2520and%2520white%2520stripes%2529%252C%2520CIFAR-10%252C%2520SVHN%252C%2520and%2520GalaxyMNIST.%250AThe%2520experiments%2520show%2520improvements%2520over%2520supervised%2520finetuning%2520baseline%2520of%250Apre-trained%2520models%252C%2520particularly%2520in%2520low-labeled%2520data%2520regimes%252C%2520across%2520both%250Afrozen%2520and%2520trainable%2520backbones%2520for%2520the%2520majority%2520of%2520the%2520tested%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Fine-Tuning%20of%20Vision%20Foundation%20Models%20with%0A%20%20Content-Style%20Decomposition&entry.906535625=Mariia%20Drozdova%20and%20Vitaliy%20Kinakh%20and%20Yury%20Belousov%20and%20Erica%20Lastufka%20and%20Slava%20Voloshynovskiy&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20semi-supervised%20fine-tuning%20approach%20designed%20to%0Aimprove%20the%20performance%20of%20pre-trained%20foundation%20models%20on%20downstream%20tasks%0Awith%20limited%20labeled%20data.%20By%20leveraging%20content-style%20decomposition%20within%20an%0Ainformation-theoretic%20framework%2C%20our%20method%20enhances%20the%20latent%20representations%0Aof%20pre-trained%20vision%20foundation%20models%2C%20aligning%20them%20more%20effectively%20with%0Aspecific%20task%20objectives%20and%20addressing%20the%20problem%20of%20distribution%20shift.%20We%0Aevaluate%20our%20approach%20on%20multiple%20datasets%2C%20including%20MNIST%2C%20its%20augmented%0Avariations%20%28with%20yellow%20and%20white%20stripes%29%2C%20CIFAR-10%2C%20SVHN%2C%20and%20GalaxyMNIST.%0AThe%20experiments%20show%20improvements%20over%20supervised%20finetuning%20baseline%20of%0Apre-trained%20models%2C%20particularly%20in%20low-labeled%20data%20regimes%2C%20across%20both%0Afrozen%20and%20trainable%20backbones%20for%20the%20majority%20of%20the%20tested%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02069v2&entry.124074799=Read"},
{"title": "Images Speak Volumes: User-Centric Assessment of Image Generation for\n  Accessible Communication", "author": "Miriam Ansch\u00fctz and Tringa Sylaj and Georg Groh", "abstract": "  Explanatory images play a pivotal role in accessible and easy-to-read (E2R)\ntexts. However, the images available in online databases are not tailored\ntoward the respective texts, and the creation of customized images is\nexpensive. In this large-scale study, we investigated whether text-to-image\ngeneration models can close this gap by providing customizable images quickly\nand easily. We benchmarked seven, four open- and three closed-source, image\ngeneration models and provide an extensive evaluation of the resulting images.\nIn addition, we performed a user study with people from the E2R target group to\nexamine whether the images met their requirements. We find that some of the\nmodels show remarkable performance, but none of the models are ready to be used\nat a larger scale without human supervision. Our research is an important step\ntoward facilitating the creation of accessible information for E2R creators and\ntailoring accessible images to the target group's needs.\n", "link": "http://arxiv.org/abs/2410.03430v1", "date": "2024-10-04", "relevancy": 2.2058, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5882}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.526}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Images%20Speak%20Volumes%3A%20User-Centric%20Assessment%20of%20Image%20Generation%20for%0A%20%20Accessible%20Communication&body=Title%3A%20Images%20Speak%20Volumes%3A%20User-Centric%20Assessment%20of%20Image%20Generation%20for%0A%20%20Accessible%20Communication%0AAuthor%3A%20Miriam%20Ansch%C3%BCtz%20and%20Tringa%20Sylaj%20and%20Georg%20Groh%0AAbstract%3A%20%20%20Explanatory%20images%20play%20a%20pivotal%20role%20in%20accessible%20and%20easy-to-read%20%28E2R%29%0Atexts.%20However%2C%20the%20images%20available%20in%20online%20databases%20are%20not%20tailored%0Atoward%20the%20respective%20texts%2C%20and%20the%20creation%20of%20customized%20images%20is%0Aexpensive.%20In%20this%20large-scale%20study%2C%20we%20investigated%20whether%20text-to-image%0Ageneration%20models%20can%20close%20this%20gap%20by%20providing%20customizable%20images%20quickly%0Aand%20easily.%20We%20benchmarked%20seven%2C%20four%20open-%20and%20three%20closed-source%2C%20image%0Ageneration%20models%20and%20provide%20an%20extensive%20evaluation%20of%20the%20resulting%20images.%0AIn%20addition%2C%20we%20performed%20a%20user%20study%20with%20people%20from%20the%20E2R%20target%20group%20to%0Aexamine%20whether%20the%20images%20met%20their%20requirements.%20We%20find%20that%20some%20of%20the%0Amodels%20show%20remarkable%20performance%2C%20but%20none%20of%20the%20models%20are%20ready%20to%20be%20used%0Aat%20a%20larger%20scale%20without%20human%20supervision.%20Our%20research%20is%20an%20important%20step%0Atoward%20facilitating%20the%20creation%20of%20accessible%20information%20for%20E2R%20creators%20and%0Atailoring%20accessible%20images%20to%20the%20target%20group%27s%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImages%2520Speak%2520Volumes%253A%2520User-Centric%2520Assessment%2520of%2520Image%2520Generation%2520for%250A%2520%2520Accessible%2520Communication%26entry.906535625%3DMiriam%2520Ansch%25C3%25BCtz%2520and%2520Tringa%2520Sylaj%2520and%2520Georg%2520Groh%26entry.1292438233%3D%2520%2520Explanatory%2520images%2520play%2520a%2520pivotal%2520role%2520in%2520accessible%2520and%2520easy-to-read%2520%2528E2R%2529%250Atexts.%2520However%252C%2520the%2520images%2520available%2520in%2520online%2520databases%2520are%2520not%2520tailored%250Atoward%2520the%2520respective%2520texts%252C%2520and%2520the%2520creation%2520of%2520customized%2520images%2520is%250Aexpensive.%2520In%2520this%2520large-scale%2520study%252C%2520we%2520investigated%2520whether%2520text-to-image%250Ageneration%2520models%2520can%2520close%2520this%2520gap%2520by%2520providing%2520customizable%2520images%2520quickly%250Aand%2520easily.%2520We%2520benchmarked%2520seven%252C%2520four%2520open-%2520and%2520three%2520closed-source%252C%2520image%250Ageneration%2520models%2520and%2520provide%2520an%2520extensive%2520evaluation%2520of%2520the%2520resulting%2520images.%250AIn%2520addition%252C%2520we%2520performed%2520a%2520user%2520study%2520with%2520people%2520from%2520the%2520E2R%2520target%2520group%2520to%250Aexamine%2520whether%2520the%2520images%2520met%2520their%2520requirements.%2520We%2520find%2520that%2520some%2520of%2520the%250Amodels%2520show%2520remarkable%2520performance%252C%2520but%2520none%2520of%2520the%2520models%2520are%2520ready%2520to%2520be%2520used%250Aat%2520a%2520larger%2520scale%2520without%2520human%2520supervision.%2520Our%2520research%2520is%2520an%2520important%2520step%250Atoward%2520facilitating%2520the%2520creation%2520of%2520accessible%2520information%2520for%2520E2R%2520creators%2520and%250Atailoring%2520accessible%2520images%2520to%2520the%2520target%2520group%2527s%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Images%20Speak%20Volumes%3A%20User-Centric%20Assessment%20of%20Image%20Generation%20for%0A%20%20Accessible%20Communication&entry.906535625=Miriam%20Ansch%C3%BCtz%20and%20Tringa%20Sylaj%20and%20Georg%20Groh&entry.1292438233=%20%20Explanatory%20images%20play%20a%20pivotal%20role%20in%20accessible%20and%20easy-to-read%20%28E2R%29%0Atexts.%20However%2C%20the%20images%20available%20in%20online%20databases%20are%20not%20tailored%0Atoward%20the%20respective%20texts%2C%20and%20the%20creation%20of%20customized%20images%20is%0Aexpensive.%20In%20this%20large-scale%20study%2C%20we%20investigated%20whether%20text-to-image%0Ageneration%20models%20can%20close%20this%20gap%20by%20providing%20customizable%20images%20quickly%0Aand%20easily.%20We%20benchmarked%20seven%2C%20four%20open-%20and%20three%20closed-source%2C%20image%0Ageneration%20models%20and%20provide%20an%20extensive%20evaluation%20of%20the%20resulting%20images.%0AIn%20addition%2C%20we%20performed%20a%20user%20study%20with%20people%20from%20the%20E2R%20target%20group%20to%0Aexamine%20whether%20the%20images%20met%20their%20requirements.%20We%20find%20that%20some%20of%20the%0Amodels%20show%20remarkable%20performance%2C%20but%20none%20of%20the%20models%20are%20ready%20to%20be%20used%0Aat%20a%20larger%20scale%20without%20human%20supervision.%20Our%20research%20is%20an%20important%20step%0Atoward%20facilitating%20the%20creation%20of%20accessible%20information%20for%20E2R%20creators%20and%0Atailoring%20accessible%20images%20to%20the%20target%20group%27s%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03430v1&entry.124074799=Read"},
{"title": "EBES: Easy Benchmarking for Event Sequences", "author": "Dmitry Osin and Igor Udovichenko and Viktor Moskvoretskii and Egor Shvetsov and Evgeny Burnaev", "abstract": "  Event sequences, characterized by irregular sampling intervals and a mix of\ncategorical and numerical features, are common data structures in various\nreal-world domains such as healthcare, finance, and user interaction logs.\nDespite advances in temporal data modeling techniques, there is no standardized\nbenchmarks for evaluating their performance on event sequences. This\ncomplicates result comparison across different papers due to varying evaluation\nprotocols, potentially misleading progress in this field. We introduce EBES, a\ncomprehensive benchmarking tool with standardized evaluation scenarios and\nprotocols, focusing on regression and classification problems with\nsequence-level targets. Our library simplifies benchmarking, dataset addition,\nand method integration through a unified interface. It includes a novel\nsynthetic dataset and provides preprocessed real-world datasets, including the\nlargest publicly available banking dataset. Our results provide an in-depth\nanalysis of datasets, identifying some as unsuitable for model comparison. We\ninvestigate the importance of modeling temporal and sequential components, as\nwell as the robustness and scaling properties of the models. These findings\nhighlight potential directions for future research. Our benchmark aim is to\nfacilitate reproducible research, expediting progress and increasing real-world\nimpacts.\n", "link": "http://arxiv.org/abs/2410.03399v1", "date": "2024-10-04", "relevancy": 2.2025, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4278}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EBES%3A%20Easy%20Benchmarking%20for%20Event%20Sequences&body=Title%3A%20EBES%3A%20Easy%20Benchmarking%20for%20Event%20Sequences%0AAuthor%3A%20Dmitry%20Osin%20and%20Igor%20Udovichenko%20and%20Viktor%20Moskvoretskii%20and%20Egor%20Shvetsov%20and%20Evgeny%20Burnaev%0AAbstract%3A%20%20%20Event%20sequences%2C%20characterized%20by%20irregular%20sampling%20intervals%20and%20a%20mix%20of%0Acategorical%20and%20numerical%20features%2C%20are%20common%20data%20structures%20in%20various%0Areal-world%20domains%20such%20as%20healthcare%2C%20finance%2C%20and%20user%20interaction%20logs.%0ADespite%20advances%20in%20temporal%20data%20modeling%20techniques%2C%20there%20is%20no%20standardized%0Abenchmarks%20for%20evaluating%20their%20performance%20on%20event%20sequences.%20This%0Acomplicates%20result%20comparison%20across%20different%20papers%20due%20to%20varying%20evaluation%0Aprotocols%2C%20potentially%20misleading%20progress%20in%20this%20field.%20We%20introduce%20EBES%2C%20a%0Acomprehensive%20benchmarking%20tool%20with%20standardized%20evaluation%20scenarios%20and%0Aprotocols%2C%20focusing%20on%20regression%20and%20classification%20problems%20with%0Asequence-level%20targets.%20Our%20library%20simplifies%20benchmarking%2C%20dataset%20addition%2C%0Aand%20method%20integration%20through%20a%20unified%20interface.%20It%20includes%20a%20novel%0Asynthetic%20dataset%20and%20provides%20preprocessed%20real-world%20datasets%2C%20including%20the%0Alargest%20publicly%20available%20banking%20dataset.%20Our%20results%20provide%20an%20in-depth%0Aanalysis%20of%20datasets%2C%20identifying%20some%20as%20unsuitable%20for%20model%20comparison.%20We%0Ainvestigate%20the%20importance%20of%20modeling%20temporal%20and%20sequential%20components%2C%20as%0Awell%20as%20the%20robustness%20and%20scaling%20properties%20of%20the%20models.%20These%20findings%0Ahighlight%20potential%20directions%20for%20future%20research.%20Our%20benchmark%20aim%20is%20to%0Afacilitate%20reproducible%20research%2C%20expediting%20progress%20and%20increasing%20real-world%0Aimpacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEBES%253A%2520Easy%2520Benchmarking%2520for%2520Event%2520Sequences%26entry.906535625%3DDmitry%2520Osin%2520and%2520Igor%2520Udovichenko%2520and%2520Viktor%2520Moskvoretskii%2520and%2520Egor%2520Shvetsov%2520and%2520Evgeny%2520Burnaev%26entry.1292438233%3D%2520%2520Event%2520sequences%252C%2520characterized%2520by%2520irregular%2520sampling%2520intervals%2520and%2520a%2520mix%2520of%250Acategorical%2520and%2520numerical%2520features%252C%2520are%2520common%2520data%2520structures%2520in%2520various%250Areal-world%2520domains%2520such%2520as%2520healthcare%252C%2520finance%252C%2520and%2520user%2520interaction%2520logs.%250ADespite%2520advances%2520in%2520temporal%2520data%2520modeling%2520techniques%252C%2520there%2520is%2520no%2520standardized%250Abenchmarks%2520for%2520evaluating%2520their%2520performance%2520on%2520event%2520sequences.%2520This%250Acomplicates%2520result%2520comparison%2520across%2520different%2520papers%2520due%2520to%2520varying%2520evaluation%250Aprotocols%252C%2520potentially%2520misleading%2520progress%2520in%2520this%2520field.%2520We%2520introduce%2520EBES%252C%2520a%250Acomprehensive%2520benchmarking%2520tool%2520with%2520standardized%2520evaluation%2520scenarios%2520and%250Aprotocols%252C%2520focusing%2520on%2520regression%2520and%2520classification%2520problems%2520with%250Asequence-level%2520targets.%2520Our%2520library%2520simplifies%2520benchmarking%252C%2520dataset%2520addition%252C%250Aand%2520method%2520integration%2520through%2520a%2520unified%2520interface.%2520It%2520includes%2520a%2520novel%250Asynthetic%2520dataset%2520and%2520provides%2520preprocessed%2520real-world%2520datasets%252C%2520including%2520the%250Alargest%2520publicly%2520available%2520banking%2520dataset.%2520Our%2520results%2520provide%2520an%2520in-depth%250Aanalysis%2520of%2520datasets%252C%2520identifying%2520some%2520as%2520unsuitable%2520for%2520model%2520comparison.%2520We%250Ainvestigate%2520the%2520importance%2520of%2520modeling%2520temporal%2520and%2520sequential%2520components%252C%2520as%250Awell%2520as%2520the%2520robustness%2520and%2520scaling%2520properties%2520of%2520the%2520models.%2520These%2520findings%250Ahighlight%2520potential%2520directions%2520for%2520future%2520research.%2520Our%2520benchmark%2520aim%2520is%2520to%250Afacilitate%2520reproducible%2520research%252C%2520expediting%2520progress%2520and%2520increasing%2520real-world%250Aimpacts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EBES%3A%20Easy%20Benchmarking%20for%20Event%20Sequences&entry.906535625=Dmitry%20Osin%20and%20Igor%20Udovichenko%20and%20Viktor%20Moskvoretskii%20and%20Egor%20Shvetsov%20and%20Evgeny%20Burnaev&entry.1292438233=%20%20Event%20sequences%2C%20characterized%20by%20irregular%20sampling%20intervals%20and%20a%20mix%20of%0Acategorical%20and%20numerical%20features%2C%20are%20common%20data%20structures%20in%20various%0Areal-world%20domains%20such%20as%20healthcare%2C%20finance%2C%20and%20user%20interaction%20logs.%0ADespite%20advances%20in%20temporal%20data%20modeling%20techniques%2C%20there%20is%20no%20standardized%0Abenchmarks%20for%20evaluating%20their%20performance%20on%20event%20sequences.%20This%0Acomplicates%20result%20comparison%20across%20different%20papers%20due%20to%20varying%20evaluation%0Aprotocols%2C%20potentially%20misleading%20progress%20in%20this%20field.%20We%20introduce%20EBES%2C%20a%0Acomprehensive%20benchmarking%20tool%20with%20standardized%20evaluation%20scenarios%20and%0Aprotocols%2C%20focusing%20on%20regression%20and%20classification%20problems%20with%0Asequence-level%20targets.%20Our%20library%20simplifies%20benchmarking%2C%20dataset%20addition%2C%0Aand%20method%20integration%20through%20a%20unified%20interface.%20It%20includes%20a%20novel%0Asynthetic%20dataset%20and%20provides%20preprocessed%20real-world%20datasets%2C%20including%20the%0Alargest%20publicly%20available%20banking%20dataset.%20Our%20results%20provide%20an%20in-depth%0Aanalysis%20of%20datasets%2C%20identifying%20some%20as%20unsuitable%20for%20model%20comparison.%20We%0Ainvestigate%20the%20importance%20of%20modeling%20temporal%20and%20sequential%20components%2C%20as%0Awell%20as%20the%20robustness%20and%20scaling%20properties%20of%20the%20models.%20These%20findings%0Ahighlight%20potential%20directions%20for%20future%20research.%20Our%20benchmark%20aim%20is%20to%0Afacilitate%20reproducible%20research%2C%20expediting%20progress%20and%20increasing%20real-world%0Aimpacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03399v1&entry.124074799=Read"},
{"title": "Estimating Body and Hand Motion in an Ego-sensed World", "author": "Brent Yi and Vickie Ye and Maya Zheng and Lea M\u00fcller and Georgios Pavlakos and Yi Ma and Jitendra Malik and Angjoo Kanazawa", "abstract": "  We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture the wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve the hands: the resulting kinematic and temporal\nconstraints result in over 40% lower hand estimation errors compared to noisy\nmonocular estimates. Project page: https://egoallo.github.io/\n", "link": "http://arxiv.org/abs/2410.03665v1", "date": "2024-10-04", "relevancy": 2.1969, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5739}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5326}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Body%20and%20Hand%20Motion%20in%20an%20Ego-sensed%20World&body=Title%3A%20Estimating%20Body%20and%20Hand%20Motion%20in%20an%20Ego-sensed%20World%0AAuthor%3A%20Brent%20Yi%20and%20Vickie%20Ye%20and%20Maya%20Zheng%20and%20Lea%20M%C3%BCller%20and%20Georgios%20Pavlakos%20and%20Yi%20Ma%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa%0AAbstract%3A%20%20%20We%20present%20EgoAllo%2C%20a%20system%20for%20human%20motion%20estimation%20from%20a%20head-mounted%0Adevice.%20Using%20only%20egocentric%20SLAM%20poses%20and%20images%2C%20EgoAllo%20guides%20sampling%0Afrom%20a%20conditional%20diffusion%20model%20to%20estimate%203D%20body%20pose%2C%20height%2C%20and%20hand%0Aparameters%20that%20capture%20the%20wearer%27s%20actions%20in%20the%20allocentric%20coordinate%0Aframe%20of%20the%20scene.%20To%20achieve%20this%2C%20our%20key%20insight%20is%20in%20representation%3A%20we%0Apropose%20spatial%20and%20temporal%20invariance%20criteria%20for%20improving%20model%0Aperformance%2C%20from%20which%20we%20derive%20a%20head%20motion%20conditioning%20parameterization%0Athat%20improves%20estimation%20by%20up%20to%2018%25.%20We%20also%20show%20how%20the%20bodies%20estimated%20by%0Aour%20system%20can%20improve%20the%20hands%3A%20the%20resulting%20kinematic%20and%20temporal%0Aconstraints%20result%20in%20over%2040%25%20lower%20hand%20estimation%20errors%20compared%20to%20noisy%0Amonocular%20estimates.%20Project%20page%3A%20https%3A//egoallo.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Body%2520and%2520Hand%2520Motion%2520in%2520an%2520Ego-sensed%2520World%26entry.906535625%3DBrent%2520Yi%2520and%2520Vickie%2520Ye%2520and%2520Maya%2520Zheng%2520and%2520Lea%2520M%25C3%25BCller%2520and%2520Georgios%2520Pavlakos%2520and%2520Yi%2520Ma%2520and%2520Jitendra%2520Malik%2520and%2520Angjoo%2520Kanazawa%26entry.1292438233%3D%2520%2520We%2520present%2520EgoAllo%252C%2520a%2520system%2520for%2520human%2520motion%2520estimation%2520from%2520a%2520head-mounted%250Adevice.%2520Using%2520only%2520egocentric%2520SLAM%2520poses%2520and%2520images%252C%2520EgoAllo%2520guides%2520sampling%250Afrom%2520a%2520conditional%2520diffusion%2520model%2520to%2520estimate%25203D%2520body%2520pose%252C%2520height%252C%2520and%2520hand%250Aparameters%2520that%2520capture%2520the%2520wearer%2527s%2520actions%2520in%2520the%2520allocentric%2520coordinate%250Aframe%2520of%2520the%2520scene.%2520To%2520achieve%2520this%252C%2520our%2520key%2520insight%2520is%2520in%2520representation%253A%2520we%250Apropose%2520spatial%2520and%2520temporal%2520invariance%2520criteria%2520for%2520improving%2520model%250Aperformance%252C%2520from%2520which%2520we%2520derive%2520a%2520head%2520motion%2520conditioning%2520parameterization%250Athat%2520improves%2520estimation%2520by%2520up%2520to%252018%2525.%2520We%2520also%2520show%2520how%2520the%2520bodies%2520estimated%2520by%250Aour%2520system%2520can%2520improve%2520the%2520hands%253A%2520the%2520resulting%2520kinematic%2520and%2520temporal%250Aconstraints%2520result%2520in%2520over%252040%2525%2520lower%2520hand%2520estimation%2520errors%2520compared%2520to%2520noisy%250Amonocular%2520estimates.%2520Project%2520page%253A%2520https%253A//egoallo.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Body%20and%20Hand%20Motion%20in%20an%20Ego-sensed%20World&entry.906535625=Brent%20Yi%20and%20Vickie%20Ye%20and%20Maya%20Zheng%20and%20Lea%20M%C3%BCller%20and%20Georgios%20Pavlakos%20and%20Yi%20Ma%20and%20Jitendra%20Malik%20and%20Angjoo%20Kanazawa&entry.1292438233=%20%20We%20present%20EgoAllo%2C%20a%20system%20for%20human%20motion%20estimation%20from%20a%20head-mounted%0Adevice.%20Using%20only%20egocentric%20SLAM%20poses%20and%20images%2C%20EgoAllo%20guides%20sampling%0Afrom%20a%20conditional%20diffusion%20model%20to%20estimate%203D%20body%20pose%2C%20height%2C%20and%20hand%0Aparameters%20that%20capture%20the%20wearer%27s%20actions%20in%20the%20allocentric%20coordinate%0Aframe%20of%20the%20scene.%20To%20achieve%20this%2C%20our%20key%20insight%20is%20in%20representation%3A%20we%0Apropose%20spatial%20and%20temporal%20invariance%20criteria%20for%20improving%20model%0Aperformance%2C%20from%20which%20we%20derive%20a%20head%20motion%20conditioning%20parameterization%0Athat%20improves%20estimation%20by%20up%20to%2018%25.%20We%20also%20show%20how%20the%20bodies%20estimated%20by%0Aour%20system%20can%20improve%20the%20hands%3A%20the%20resulting%20kinematic%20and%20temporal%0Aconstraints%20result%20in%20over%2040%25%20lower%20hand%20estimation%20errors%20compared%20to%20noisy%0Amonocular%20estimates.%20Project%20page%3A%20https%3A//egoallo.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03665v1&entry.124074799=Read"},
{"title": "Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition", "author": "Zixuan Wang and Yu-Wing Tai and Chi-Keung Tang", "abstract": "  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions, and calls the agent for audio generation. Consequently,\nAudio-Agent generates high-quality audio that is closely aligned with the\nprovided text or video while also supporting variable-length generation. For\nvideo-to-audio (VTA) tasks, most existing methods require training a timestamp\ndetector to synchronize video events with generated audio, a process that can\nbe tedious and time-consuming. We propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions to bridge video and audio modality. Thus our\nframework provides a comprehensive solution for both TTA and VTA tasks without\nsubstantial computational overhead in training.\n", "link": "http://arxiv.org/abs/2410.03335v1", "date": "2024-10-04", "relevancy": 2.1893, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5691}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5489}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Agent%3A%20Leveraging%20LLMs%20For%20Audio%20Generation%2C%20Editing%20and%0A%20%20Composition&body=Title%3A%20Audio-Agent%3A%20Leveraging%20LLMs%20For%20Audio%20Generation%2C%20Editing%20and%0A%20%20Composition%0AAuthor%3A%20Zixuan%20Wang%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang%0AAbstract%3A%20%20%20We%20introduce%20Audio-Agent%2C%20a%20multimodal%20framework%20for%20audio%20generation%2C%0Aediting%20and%20composition%20based%20on%20text%20or%20video%20inputs.%20Conventional%20approaches%0Afor%20text-to-audio%20%28TTA%29%20tasks%20often%20make%20single-pass%20inferences%20from%20text%0Adescriptions.%20While%20straightforward%2C%20this%20design%20struggles%20to%20produce%0Ahigh-quality%20audio%20when%20given%20complex%20text%20conditions.%20In%20our%20method%2C%20we%0Autilize%20a%20pre-trained%20TTA%20diffusion%20network%20as%20the%20audio%20generation%20agent%20to%0Awork%20in%20tandem%20with%20GPT-4%2C%20which%20decomposes%20the%20text%20condition%20into%20atomic%2C%0Aspecific%20instructions%2C%20and%20calls%20the%20agent%20for%20audio%20generation.%20Consequently%2C%0AAudio-Agent%20generates%20high-quality%20audio%20that%20is%20closely%20aligned%20with%20the%0Aprovided%20text%20or%20video%20while%20also%20supporting%20variable-length%20generation.%20For%0Avideo-to-audio%20%28VTA%29%20tasks%2C%20most%20existing%20methods%20require%20training%20a%20timestamp%0Adetector%20to%20synchronize%20video%20events%20with%20generated%20audio%2C%20a%20process%20that%20can%0Abe%20tedious%20and%20time-consuming.%20We%20propose%20a%20simpler%20approach%20by%20fine-tuning%20a%0Apre-trained%20Large%20Language%20Model%20%28LLM%29%2C%20e.g.%2C%20Gemma2-2B-it%2C%20to%20obtain%20both%0Asemantic%20and%20temporal%20conditions%20to%20bridge%20video%20and%20audio%20modality.%20Thus%20our%0Aframework%20provides%20a%20comprehensive%20solution%20for%20both%20TTA%20and%20VTA%20tasks%20without%0Asubstantial%20computational%20overhead%20in%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Agent%253A%2520Leveraging%2520LLMs%2520For%2520Audio%2520Generation%252C%2520Editing%2520and%250A%2520%2520Composition%26entry.906535625%3DZixuan%2520Wang%2520and%2520Yu-Wing%2520Tai%2520and%2520Chi-Keung%2520Tang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Audio-Agent%252C%2520a%2520multimodal%2520framework%2520for%2520audio%2520generation%252C%250Aediting%2520and%2520composition%2520based%2520on%2520text%2520or%2520video%2520inputs.%2520Conventional%2520approaches%250Afor%2520text-to-audio%2520%2528TTA%2529%2520tasks%2520often%2520make%2520single-pass%2520inferences%2520from%2520text%250Adescriptions.%2520While%2520straightforward%252C%2520this%2520design%2520struggles%2520to%2520produce%250Ahigh-quality%2520audio%2520when%2520given%2520complex%2520text%2520conditions.%2520In%2520our%2520method%252C%2520we%250Autilize%2520a%2520pre-trained%2520TTA%2520diffusion%2520network%2520as%2520the%2520audio%2520generation%2520agent%2520to%250Awork%2520in%2520tandem%2520with%2520GPT-4%252C%2520which%2520decomposes%2520the%2520text%2520condition%2520into%2520atomic%252C%250Aspecific%2520instructions%252C%2520and%2520calls%2520the%2520agent%2520for%2520audio%2520generation.%2520Consequently%252C%250AAudio-Agent%2520generates%2520high-quality%2520audio%2520that%2520is%2520closely%2520aligned%2520with%2520the%250Aprovided%2520text%2520or%2520video%2520while%2520also%2520supporting%2520variable-length%2520generation.%2520For%250Avideo-to-audio%2520%2528VTA%2529%2520tasks%252C%2520most%2520existing%2520methods%2520require%2520training%2520a%2520timestamp%250Adetector%2520to%2520synchronize%2520video%2520events%2520with%2520generated%2520audio%252C%2520a%2520process%2520that%2520can%250Abe%2520tedious%2520and%2520time-consuming.%2520We%2520propose%2520a%2520simpler%2520approach%2520by%2520fine-tuning%2520a%250Apre-trained%2520Large%2520Language%2520Model%2520%2528LLM%2529%252C%2520e.g.%252C%2520Gemma2-2B-it%252C%2520to%2520obtain%2520both%250Asemantic%2520and%2520temporal%2520conditions%2520to%2520bridge%2520video%2520and%2520audio%2520modality.%2520Thus%2520our%250Aframework%2520provides%2520a%2520comprehensive%2520solution%2520for%2520both%2520TTA%2520and%2520VTA%2520tasks%2520without%250Asubstantial%2520computational%2520overhead%2520in%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Agent%3A%20Leveraging%20LLMs%20For%20Audio%20Generation%2C%20Editing%20and%0A%20%20Composition&entry.906535625=Zixuan%20Wang%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang&entry.1292438233=%20%20We%20introduce%20Audio-Agent%2C%20a%20multimodal%20framework%20for%20audio%20generation%2C%0Aediting%20and%20composition%20based%20on%20text%20or%20video%20inputs.%20Conventional%20approaches%0Afor%20text-to-audio%20%28TTA%29%20tasks%20often%20make%20single-pass%20inferences%20from%20text%0Adescriptions.%20While%20straightforward%2C%20this%20design%20struggles%20to%20produce%0Ahigh-quality%20audio%20when%20given%20complex%20text%20conditions.%20In%20our%20method%2C%20we%0Autilize%20a%20pre-trained%20TTA%20diffusion%20network%20as%20the%20audio%20generation%20agent%20to%0Awork%20in%20tandem%20with%20GPT-4%2C%20which%20decomposes%20the%20text%20condition%20into%20atomic%2C%0Aspecific%20instructions%2C%20and%20calls%20the%20agent%20for%20audio%20generation.%20Consequently%2C%0AAudio-Agent%20generates%20high-quality%20audio%20that%20is%20closely%20aligned%20with%20the%0Aprovided%20text%20or%20video%20while%20also%20supporting%20variable-length%20generation.%20For%0Avideo-to-audio%20%28VTA%29%20tasks%2C%20most%20existing%20methods%20require%20training%20a%20timestamp%0Adetector%20to%20synchronize%20video%20events%20with%20generated%20audio%2C%20a%20process%20that%20can%0Abe%20tedious%20and%20time-consuming.%20We%20propose%20a%20simpler%20approach%20by%20fine-tuning%20a%0Apre-trained%20Large%20Language%20Model%20%28LLM%29%2C%20e.g.%2C%20Gemma2-2B-it%2C%20to%20obtain%20both%0Asemantic%20and%20temporal%20conditions%20to%20bridge%20video%20and%20audio%20modality.%20Thus%20our%0Aframework%20provides%20a%20comprehensive%20solution%20for%20both%20TTA%20and%20VTA%20tasks%20without%0Asubstantial%20computational%20overhead%20in%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03335v1&entry.124074799=Read"},
{"title": "Decoupling Layout from Glyph in Online Chinese Handwriting Generation", "author": "Min-Si Ren and Yan-Ming Zhang and Yi Chen", "abstract": "  Text plays a crucial role in the transmission of human civilization, and\nteaching machines to generate online handwritten text in various styles\npresents an interesting and significant challenge. However, most prior work has\nconcentrated on generating individual Chinese fonts, leaving {complete text\nline generation largely unexplored}. In this paper, we identify that text lines\ncan naturally be divided into two components: layout and glyphs. Based on this\ndivision, we designed a text line layout generator coupled with a\ndiffusion-based stylized font synthesizer to address this challenge\nhierarchically. More concretely, the layout generator performs in-context-like\nlearning based on the text content and the provided style references to\ngenerate positions for each glyph autoregressively. Meanwhile, the font\nsynthesizer which consists of a character embedding dictionary, a multi-scale\ncalligraphy style encoder, and a 1D U-Net based diffusion denoiser will\ngenerate each font on its position while imitating the calligraphy style\nextracted from the given style references. Qualitative and quantitative\nexperiments on the CASIA-OLHWDB demonstrate that our method is capable of\ngenerating structurally correct and indistinguishable imitation samples.\n", "link": "http://arxiv.org/abs/2410.02309v2", "date": "2024-10-04", "relevancy": 2.1892, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5683}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5334}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Layout%20from%20Glyph%20in%20Online%20Chinese%20Handwriting%20Generation&body=Title%3A%20Decoupling%20Layout%20from%20Glyph%20in%20Online%20Chinese%20Handwriting%20Generation%0AAuthor%3A%20Min-Si%20Ren%20and%20Yan-Ming%20Zhang%20and%20Yi%20Chen%0AAbstract%3A%20%20%20Text%20plays%20a%20crucial%20role%20in%20the%20transmission%20of%20human%20civilization%2C%20and%0Ateaching%20machines%20to%20generate%20online%20handwritten%20text%20in%20various%20styles%0Apresents%20an%20interesting%20and%20significant%20challenge.%20However%2C%20most%20prior%20work%20has%0Aconcentrated%20on%20generating%20individual%20Chinese%20fonts%2C%20leaving%20%7Bcomplete%20text%0Aline%20generation%20largely%20unexplored%7D.%20In%20this%20paper%2C%20we%20identify%20that%20text%20lines%0Acan%20naturally%20be%20divided%20into%20two%20components%3A%20layout%20and%20glyphs.%20Based%20on%20this%0Adivision%2C%20we%20designed%20a%20text%20line%20layout%20generator%20coupled%20with%20a%0Adiffusion-based%20stylized%20font%20synthesizer%20to%20address%20this%20challenge%0Ahierarchically.%20More%20concretely%2C%20the%20layout%20generator%20performs%20in-context-like%0Alearning%20based%20on%20the%20text%20content%20and%20the%20provided%20style%20references%20to%0Agenerate%20positions%20for%20each%20glyph%20autoregressively.%20Meanwhile%2C%20the%20font%0Asynthesizer%20which%20consists%20of%20a%20character%20embedding%20dictionary%2C%20a%20multi-scale%0Acalligraphy%20style%20encoder%2C%20and%20a%201D%20U-Net%20based%20diffusion%20denoiser%20will%0Agenerate%20each%20font%20on%20its%20position%20while%20imitating%20the%20calligraphy%20style%0Aextracted%20from%20the%20given%20style%20references.%20Qualitative%20and%20quantitative%0Aexperiments%20on%20the%20CASIA-OLHWDB%20demonstrate%20that%20our%20method%20is%20capable%20of%0Agenerating%20structurally%20correct%20and%20indistinguishable%20imitation%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Layout%2520from%2520Glyph%2520in%2520Online%2520Chinese%2520Handwriting%2520Generation%26entry.906535625%3DMin-Si%2520Ren%2520and%2520Yan-Ming%2520Zhang%2520and%2520Yi%2520Chen%26entry.1292438233%3D%2520%2520Text%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520transmission%2520of%2520human%2520civilization%252C%2520and%250Ateaching%2520machines%2520to%2520generate%2520online%2520handwritten%2520text%2520in%2520various%2520styles%250Apresents%2520an%2520interesting%2520and%2520significant%2520challenge.%2520However%252C%2520most%2520prior%2520work%2520has%250Aconcentrated%2520on%2520generating%2520individual%2520Chinese%2520fonts%252C%2520leaving%2520%257Bcomplete%2520text%250Aline%2520generation%2520largely%2520unexplored%257D.%2520In%2520this%2520paper%252C%2520we%2520identify%2520that%2520text%2520lines%250Acan%2520naturally%2520be%2520divided%2520into%2520two%2520components%253A%2520layout%2520and%2520glyphs.%2520Based%2520on%2520this%250Adivision%252C%2520we%2520designed%2520a%2520text%2520line%2520layout%2520generator%2520coupled%2520with%2520a%250Adiffusion-based%2520stylized%2520font%2520synthesizer%2520to%2520address%2520this%2520challenge%250Ahierarchically.%2520More%2520concretely%252C%2520the%2520layout%2520generator%2520performs%2520in-context-like%250Alearning%2520based%2520on%2520the%2520text%2520content%2520and%2520the%2520provided%2520style%2520references%2520to%250Agenerate%2520positions%2520for%2520each%2520glyph%2520autoregressively.%2520Meanwhile%252C%2520the%2520font%250Asynthesizer%2520which%2520consists%2520of%2520a%2520character%2520embedding%2520dictionary%252C%2520a%2520multi-scale%250Acalligraphy%2520style%2520encoder%252C%2520and%2520a%25201D%2520U-Net%2520based%2520diffusion%2520denoiser%2520will%250Agenerate%2520each%2520font%2520on%2520its%2520position%2520while%2520imitating%2520the%2520calligraphy%2520style%250Aextracted%2520from%2520the%2520given%2520style%2520references.%2520Qualitative%2520and%2520quantitative%250Aexperiments%2520on%2520the%2520CASIA-OLHWDB%2520demonstrate%2520that%2520our%2520method%2520is%2520capable%2520of%250Agenerating%2520structurally%2520correct%2520and%2520indistinguishable%2520imitation%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Layout%20from%20Glyph%20in%20Online%20Chinese%20Handwriting%20Generation&entry.906535625=Min-Si%20Ren%20and%20Yan-Ming%20Zhang%20and%20Yi%20Chen&entry.1292438233=%20%20Text%20plays%20a%20crucial%20role%20in%20the%20transmission%20of%20human%20civilization%2C%20and%0Ateaching%20machines%20to%20generate%20online%20handwritten%20text%20in%20various%20styles%0Apresents%20an%20interesting%20and%20significant%20challenge.%20However%2C%20most%20prior%20work%20has%0Aconcentrated%20on%20generating%20individual%20Chinese%20fonts%2C%20leaving%20%7Bcomplete%20text%0Aline%20generation%20largely%20unexplored%7D.%20In%20this%20paper%2C%20we%20identify%20that%20text%20lines%0Acan%20naturally%20be%20divided%20into%20two%20components%3A%20layout%20and%20glyphs.%20Based%20on%20this%0Adivision%2C%20we%20designed%20a%20text%20line%20layout%20generator%20coupled%20with%20a%0Adiffusion-based%20stylized%20font%20synthesizer%20to%20address%20this%20challenge%0Ahierarchically.%20More%20concretely%2C%20the%20layout%20generator%20performs%20in-context-like%0Alearning%20based%20on%20the%20text%20content%20and%20the%20provided%20style%20references%20to%0Agenerate%20positions%20for%20each%20glyph%20autoregressively.%20Meanwhile%2C%20the%20font%0Asynthesizer%20which%20consists%20of%20a%20character%20embedding%20dictionary%2C%20a%20multi-scale%0Acalligraphy%20style%20encoder%2C%20and%20a%201D%20U-Net%20based%20diffusion%20denoiser%20will%0Agenerate%20each%20font%20on%20its%20position%20while%20imitating%20the%20calligraphy%20style%0Aextracted%20from%20the%20given%20style%20references.%20Qualitative%20and%20quantitative%0Aexperiments%20on%20the%20CASIA-OLHWDB%20demonstrate%20that%20our%20method%20is%20capable%20of%0Agenerating%20structurally%20correct%20and%20indistinguishable%20imitation%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02309v2&entry.124074799=Read"},
{"title": "Video Instruction Tuning With Synthetic Data", "author": "Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li", "abstract": "  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n", "link": "http://arxiv.org/abs/2410.02713v2", "date": "2024-10-04", "relevancy": 2.1875, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5622}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5548}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Instruction%20Tuning%20With%20Synthetic%20Data&body=Title%3A%20Video%20Instruction%20Tuning%20With%20Synthetic%20Data%0AAuthor%3A%20Yuanhan%20Zhang%20and%20Jinming%20Wu%20and%20Wei%20Li%20and%20Bo%20Li%20and%20Zejun%20Ma%20and%20Ziwei%20Liu%20and%20Chunyuan%20Li%0AAbstract%3A%20%20%20The%20development%20of%20video%20large%20multimodal%20models%20%28LMMs%29%20has%20been%20hindered%20by%0Athe%20difficulty%20of%20curating%20large%20amounts%20of%20high-quality%20raw%20data%20from%20the%20web.%0ATo%20address%20this%2C%20we%20propose%20an%20alternative%20approach%20by%20creating%20a%20high-quality%0Asynthetic%20dataset%20specifically%20for%20video%20instruction-following%2C%20namely%0ALLaVA-Video-178K.%20This%20dataset%20includes%20key%20tasks%20such%20as%20detailed%20captioning%2C%0Aopen-ended%20question-answering%20%28QA%29%2C%20and%20multiple-choice%20QA.%20By%20training%20on%20this%0Adataset%2C%20in%20combination%20with%20existing%20visual%20instruction%20tuning%20data%2C%20we%0Aintroduce%20LLaVA-Video%2C%20a%20new%20video%20LMM.%20Our%20experiments%20demonstrate%20that%0ALLaVA-Video%20achieves%20strong%20performance%20across%20various%20video%20benchmarks%2C%0Ahighlighting%20the%20effectiveness%20of%20our%20dataset.%20We%20plan%20to%20release%20the%20dataset%2C%0Aits%20generation%20pipeline%2C%20and%20the%20model%20checkpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Instruction%2520Tuning%2520With%2520Synthetic%2520Data%26entry.906535625%3DYuanhan%2520Zhang%2520and%2520Jinming%2520Wu%2520and%2520Wei%2520Li%2520and%2520Bo%2520Li%2520and%2520Zejun%2520Ma%2520and%2520Ziwei%2520Liu%2520and%2520Chunyuan%2520Li%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520video%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520has%2520been%2520hindered%2520by%250Athe%2520difficulty%2520of%2520curating%2520large%2520amounts%2520of%2520high-quality%2520raw%2520data%2520from%2520the%2520web.%250ATo%2520address%2520this%252C%2520we%2520propose%2520an%2520alternative%2520approach%2520by%2520creating%2520a%2520high-quality%250Asynthetic%2520dataset%2520specifically%2520for%2520video%2520instruction-following%252C%2520namely%250ALLaVA-Video-178K.%2520This%2520dataset%2520includes%2520key%2520tasks%2520such%2520as%2520detailed%2520captioning%252C%250Aopen-ended%2520question-answering%2520%2528QA%2529%252C%2520and%2520multiple-choice%2520QA.%2520By%2520training%2520on%2520this%250Adataset%252C%2520in%2520combination%2520with%2520existing%2520visual%2520instruction%2520tuning%2520data%252C%2520we%250Aintroduce%2520LLaVA-Video%252C%2520a%2520new%2520video%2520LMM.%2520Our%2520experiments%2520demonstrate%2520that%250ALLaVA-Video%2520achieves%2520strong%2520performance%2520across%2520various%2520video%2520benchmarks%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520our%2520dataset.%2520We%2520plan%2520to%2520release%2520the%2520dataset%252C%250Aits%2520generation%2520pipeline%252C%2520and%2520the%2520model%2520checkpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Instruction%20Tuning%20With%20Synthetic%20Data&entry.906535625=Yuanhan%20Zhang%20and%20Jinming%20Wu%20and%20Wei%20Li%20and%20Bo%20Li%20and%20Zejun%20Ma%20and%20Ziwei%20Liu%20and%20Chunyuan%20Li&entry.1292438233=%20%20The%20development%20of%20video%20large%20multimodal%20models%20%28LMMs%29%20has%20been%20hindered%20by%0Athe%20difficulty%20of%20curating%20large%20amounts%20of%20high-quality%20raw%20data%20from%20the%20web.%0ATo%20address%20this%2C%20we%20propose%20an%20alternative%20approach%20by%20creating%20a%20high-quality%0Asynthetic%20dataset%20specifically%20for%20video%20instruction-following%2C%20namely%0ALLaVA-Video-178K.%20This%20dataset%20includes%20key%20tasks%20such%20as%20detailed%20captioning%2C%0Aopen-ended%20question-answering%20%28QA%29%2C%20and%20multiple-choice%20QA.%20By%20training%20on%20this%0Adataset%2C%20in%20combination%20with%20existing%20visual%20instruction%20tuning%20data%2C%20we%0Aintroduce%20LLaVA-Video%2C%20a%20new%20video%20LMM.%20Our%20experiments%20demonstrate%20that%0ALLaVA-Video%20achieves%20strong%20performance%20across%20various%20video%20benchmarks%2C%0Ahighlighting%20the%20effectiveness%20of%20our%20dataset.%20We%20plan%20to%20release%20the%20dataset%2C%0Aits%20generation%20pipeline%2C%20and%20the%20model%20checkpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02713v2&entry.124074799=Read"},
{"title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large\n  Language Models Decoding", "author": "Lifu Tu and Semih Yavuz and Jin Qu and Jiacheng Xu and Rui Meng and Caiming Xiong and Yingbo Zhou", "abstract": "  Large Language Models (LLMs) have demonstrated a powerful ability for text\ngeneration. However, achieving optimal results with a given prompt or\ninstruction can be challenging, especially for billion-sized models.\nAdditionally, undesired behaviors such as toxicity or hallucinations can\nmanifest. While much larger models (e.g., ChatGPT) may demonstrate strength in\nmitigating these issues, there is still no guarantee of complete prevention. In\nthis work, we propose formalizing text generation as a future-constrained\ngeneration problem to minimize undesirable behaviors and enforce faithfulness\nto instructions. The estimation of future constraint satisfaction, accomplished\nusing LLMs, guides the text generation process. Our extensive experiments\ndemonstrate the effectiveness of the proposed approach across three distinct\ntext generation tasks: keyword-constrained generation (Lin et al., 2020),\ntoxicity reduction (Gehman et al., 2020), and factual correctness in\nquestion-answering (Gao et al., 2023).\n", "link": "http://arxiv.org/abs/2312.06149v4", "date": "2024-10-04", "relevancy": 2.1826, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5602}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5362}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Anticipatory%20Text%20Generation%3A%20A%20Constrained%20Approach%20for%20Large%0A%20%20Language%20Models%20Decoding&body=Title%3A%20Unlocking%20Anticipatory%20Text%20Generation%3A%20A%20Constrained%20Approach%20for%20Large%0A%20%20Language%20Models%20Decoding%0AAuthor%3A%20Lifu%20Tu%20and%20Semih%20Yavuz%20and%20Jin%20Qu%20and%20Jiacheng%20Xu%20and%20Rui%20Meng%20and%20Caiming%20Xiong%20and%20Yingbo%20Zhou%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20a%20powerful%20ability%20for%20text%0Ageneration.%20However%2C%20achieving%20optimal%20results%20with%20a%20given%20prompt%20or%0Ainstruction%20can%20be%20challenging%2C%20especially%20for%20billion-sized%20models.%0AAdditionally%2C%20undesired%20behaviors%20such%20as%20toxicity%20or%20hallucinations%20can%0Amanifest.%20While%20much%20larger%20models%20%28e.g.%2C%20ChatGPT%29%20may%20demonstrate%20strength%20in%0Amitigating%20these%20issues%2C%20there%20is%20still%20no%20guarantee%20of%20complete%20prevention.%20In%0Athis%20work%2C%20we%20propose%20formalizing%20text%20generation%20as%20a%20future-constrained%0Ageneration%20problem%20to%20minimize%20undesirable%20behaviors%20and%20enforce%20faithfulness%0Ato%20instructions.%20The%20estimation%20of%20future%20constraint%20satisfaction%2C%20accomplished%0Ausing%20LLMs%2C%20guides%20the%20text%20generation%20process.%20Our%20extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20across%20three%20distinct%0Atext%20generation%20tasks%3A%20keyword-constrained%20generation%20%28Lin%20et%20al.%2C%202020%29%2C%0Atoxicity%20reduction%20%28Gehman%20et%20al.%2C%202020%29%2C%20and%20factual%20correctness%20in%0Aquestion-answering%20%28Gao%20et%20al.%2C%202023%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06149v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Anticipatory%2520Text%2520Generation%253A%2520A%2520Constrained%2520Approach%2520for%2520Large%250A%2520%2520Language%2520Models%2520Decoding%26entry.906535625%3DLifu%2520Tu%2520and%2520Semih%2520Yavuz%2520and%2520Jin%2520Qu%2520and%2520Jiacheng%2520Xu%2520and%2520Rui%2520Meng%2520and%2520Caiming%2520Xiong%2520and%2520Yingbo%2520Zhou%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520a%2520powerful%2520ability%2520for%2520text%250Ageneration.%2520However%252C%2520achieving%2520optimal%2520results%2520with%2520a%2520given%2520prompt%2520or%250Ainstruction%2520can%2520be%2520challenging%252C%2520especially%2520for%2520billion-sized%2520models.%250AAdditionally%252C%2520undesired%2520behaviors%2520such%2520as%2520toxicity%2520or%2520hallucinations%2520can%250Amanifest.%2520While%2520much%2520larger%2520models%2520%2528e.g.%252C%2520ChatGPT%2529%2520may%2520demonstrate%2520strength%2520in%250Amitigating%2520these%2520issues%252C%2520there%2520is%2520still%2520no%2520guarantee%2520of%2520complete%2520prevention.%2520In%250Athis%2520work%252C%2520we%2520propose%2520formalizing%2520text%2520generation%2520as%2520a%2520future-constrained%250Ageneration%2520problem%2520to%2520minimize%2520undesirable%2520behaviors%2520and%2520enforce%2520faithfulness%250Ato%2520instructions.%2520The%2520estimation%2520of%2520future%2520constraint%2520satisfaction%252C%2520accomplished%250Ausing%2520LLMs%252C%2520guides%2520the%2520text%2520generation%2520process.%2520Our%2520extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520across%2520three%2520distinct%250Atext%2520generation%2520tasks%253A%2520keyword-constrained%2520generation%2520%2528Lin%2520et%2520al.%252C%25202020%2529%252C%250Atoxicity%2520reduction%2520%2528Gehman%2520et%2520al.%252C%25202020%2529%252C%2520and%2520factual%2520correctness%2520in%250Aquestion-answering%2520%2528Gao%2520et%2520al.%252C%25202023%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06149v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Anticipatory%20Text%20Generation%3A%20A%20Constrained%20Approach%20for%20Large%0A%20%20Language%20Models%20Decoding&entry.906535625=Lifu%20Tu%20and%20Semih%20Yavuz%20and%20Jin%20Qu%20and%20Jiacheng%20Xu%20and%20Rui%20Meng%20and%20Caiming%20Xiong%20and%20Yingbo%20Zhou&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20a%20powerful%20ability%20for%20text%0Ageneration.%20However%2C%20achieving%20optimal%20results%20with%20a%20given%20prompt%20or%0Ainstruction%20can%20be%20challenging%2C%20especially%20for%20billion-sized%20models.%0AAdditionally%2C%20undesired%20behaviors%20such%20as%20toxicity%20or%20hallucinations%20can%0Amanifest.%20While%20much%20larger%20models%20%28e.g.%2C%20ChatGPT%29%20may%20demonstrate%20strength%20in%0Amitigating%20these%20issues%2C%20there%20is%20still%20no%20guarantee%20of%20complete%20prevention.%20In%0Athis%20work%2C%20we%20propose%20formalizing%20text%20generation%20as%20a%20future-constrained%0Ageneration%20problem%20to%20minimize%20undesirable%20behaviors%20and%20enforce%20faithfulness%0Ato%20instructions.%20The%20estimation%20of%20future%20constraint%20satisfaction%2C%20accomplished%0Ausing%20LLMs%2C%20guides%20the%20text%20generation%20process.%20Our%20extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20across%20three%20distinct%0Atext%20generation%20tasks%3A%20keyword-constrained%20generation%20%28Lin%20et%20al.%2C%202020%29%2C%0Atoxicity%20reduction%20%28Gehman%20et%20al.%2C%202020%29%2C%20and%20factual%20correctness%20in%0Aquestion-answering%20%28Gao%20et%20al.%2C%202023%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06149v4&entry.124074799=Read"},
{"title": "Action Selection Learning for Multi-label Multi-view Action Recognition", "author": "Trung Thanh Nguyen and Yasutomo Kawanishi and Takahiro Komamizu and Ichiro Ide", "abstract": "  Multi-label multi-view action recognition aims to recognize multiple\nconcurrent or sequential actions from untrimmed videos captured by multiple\ncameras. Existing work has focused on multi-view action recognition in a narrow\narea with strong labels available, where the onset and offset of each action\nare labeled at the frame-level. This study focuses on real-world scenarios\nwhere cameras are distributed to capture a wide-range area with only weak\nlabels available at the video-level. We propose the method named MultiASL\n(Multi-view Action Selection Learning), which leverages action selection\nlearning to enhance view fusion by selecting the most useful information from\ndifferent viewpoints. The proposed method includes a Multi-view\nSpatial-Temporal Transformer video encoder to extract spatial and temporal\nfeatures from multi-viewpoint videos. Action Selection Learning is employed at\nthe frame-level, using pseudo ground-truth obtained from weak labels at the\nvideo-level, to identify the most relevant frames for action recognition.\nExperiments in a real-world office environment using the MM-Office dataset\ndemonstrate the superior performance of the proposed method compared to\nexisting methods.\n", "link": "http://arxiv.org/abs/2410.03302v1", "date": "2024-10-04", "relevancy": 2.1776, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5507}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action%20Selection%20Learning%20for%20Multi-label%20Multi-view%20Action%20Recognition&body=Title%3A%20Action%20Selection%20Learning%20for%20Multi-label%20Multi-view%20Action%20Recognition%0AAuthor%3A%20Trung%20Thanh%20Nguyen%20and%20Yasutomo%20Kawanishi%20and%20Takahiro%20Komamizu%20and%20Ichiro%20Ide%0AAbstract%3A%20%20%20Multi-label%20multi-view%20action%20recognition%20aims%20to%20recognize%20multiple%0Aconcurrent%20or%20sequential%20actions%20from%20untrimmed%20videos%20captured%20by%20multiple%0Acameras.%20Existing%20work%20has%20focused%20on%20multi-view%20action%20recognition%20in%20a%20narrow%0Aarea%20with%20strong%20labels%20available%2C%20where%20the%20onset%20and%20offset%20of%20each%20action%0Aare%20labeled%20at%20the%20frame-level.%20This%20study%20focuses%20on%20real-world%20scenarios%0Awhere%20cameras%20are%20distributed%20to%20capture%20a%20wide-range%20area%20with%20only%20weak%0Alabels%20available%20at%20the%20video-level.%20We%20propose%20the%20method%20named%20MultiASL%0A%28Multi-view%20Action%20Selection%20Learning%29%2C%20which%20leverages%20action%20selection%0Alearning%20to%20enhance%20view%20fusion%20by%20selecting%20the%20most%20useful%20information%20from%0Adifferent%20viewpoints.%20The%20proposed%20method%20includes%20a%20Multi-view%0ASpatial-Temporal%20Transformer%20video%20encoder%20to%20extract%20spatial%20and%20temporal%0Afeatures%20from%20multi-viewpoint%20videos.%20Action%20Selection%20Learning%20is%20employed%20at%0Athe%20frame-level%2C%20using%20pseudo%20ground-truth%20obtained%20from%20weak%20labels%20at%20the%0Avideo-level%2C%20to%20identify%20the%20most%20relevant%20frames%20for%20action%20recognition.%0AExperiments%20in%20a%20real-world%20office%20environment%20using%20the%20MM-Office%20dataset%0Ademonstrate%20the%20superior%20performance%20of%20the%20proposed%20method%20compared%20to%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction%2520Selection%2520Learning%2520for%2520Multi-label%2520Multi-view%2520Action%2520Recognition%26entry.906535625%3DTrung%2520Thanh%2520Nguyen%2520and%2520Yasutomo%2520Kawanishi%2520and%2520Takahiro%2520Komamizu%2520and%2520Ichiro%2520Ide%26entry.1292438233%3D%2520%2520Multi-label%2520multi-view%2520action%2520recognition%2520aims%2520to%2520recognize%2520multiple%250Aconcurrent%2520or%2520sequential%2520actions%2520from%2520untrimmed%2520videos%2520captured%2520by%2520multiple%250Acameras.%2520Existing%2520work%2520has%2520focused%2520on%2520multi-view%2520action%2520recognition%2520in%2520a%2520narrow%250Aarea%2520with%2520strong%2520labels%2520available%252C%2520where%2520the%2520onset%2520and%2520offset%2520of%2520each%2520action%250Aare%2520labeled%2520at%2520the%2520frame-level.%2520This%2520study%2520focuses%2520on%2520real-world%2520scenarios%250Awhere%2520cameras%2520are%2520distributed%2520to%2520capture%2520a%2520wide-range%2520area%2520with%2520only%2520weak%250Alabels%2520available%2520at%2520the%2520video-level.%2520We%2520propose%2520the%2520method%2520named%2520MultiASL%250A%2528Multi-view%2520Action%2520Selection%2520Learning%2529%252C%2520which%2520leverages%2520action%2520selection%250Alearning%2520to%2520enhance%2520view%2520fusion%2520by%2520selecting%2520the%2520most%2520useful%2520information%2520from%250Adifferent%2520viewpoints.%2520The%2520proposed%2520method%2520includes%2520a%2520Multi-view%250ASpatial-Temporal%2520Transformer%2520video%2520encoder%2520to%2520extract%2520spatial%2520and%2520temporal%250Afeatures%2520from%2520multi-viewpoint%2520videos.%2520Action%2520Selection%2520Learning%2520is%2520employed%2520at%250Athe%2520frame-level%252C%2520using%2520pseudo%2520ground-truth%2520obtained%2520from%2520weak%2520labels%2520at%2520the%250Avideo-level%252C%2520to%2520identify%2520the%2520most%2520relevant%2520frames%2520for%2520action%2520recognition.%250AExperiments%2520in%2520a%2520real-world%2520office%2520environment%2520using%2520the%2520MM-Office%2520dataset%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%2520method%2520compared%2520to%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action%20Selection%20Learning%20for%20Multi-label%20Multi-view%20Action%20Recognition&entry.906535625=Trung%20Thanh%20Nguyen%20and%20Yasutomo%20Kawanishi%20and%20Takahiro%20Komamizu%20and%20Ichiro%20Ide&entry.1292438233=%20%20Multi-label%20multi-view%20action%20recognition%20aims%20to%20recognize%20multiple%0Aconcurrent%20or%20sequential%20actions%20from%20untrimmed%20videos%20captured%20by%20multiple%0Acameras.%20Existing%20work%20has%20focused%20on%20multi-view%20action%20recognition%20in%20a%20narrow%0Aarea%20with%20strong%20labels%20available%2C%20where%20the%20onset%20and%20offset%20of%20each%20action%0Aare%20labeled%20at%20the%20frame-level.%20This%20study%20focuses%20on%20real-world%20scenarios%0Awhere%20cameras%20are%20distributed%20to%20capture%20a%20wide-range%20area%20with%20only%20weak%0Alabels%20available%20at%20the%20video-level.%20We%20propose%20the%20method%20named%20MultiASL%0A%28Multi-view%20Action%20Selection%20Learning%29%2C%20which%20leverages%20action%20selection%0Alearning%20to%20enhance%20view%20fusion%20by%20selecting%20the%20most%20useful%20information%20from%0Adifferent%20viewpoints.%20The%20proposed%20method%20includes%20a%20Multi-view%0ASpatial-Temporal%20Transformer%20video%20encoder%20to%20extract%20spatial%20and%20temporal%0Afeatures%20from%20multi-viewpoint%20videos.%20Action%20Selection%20Learning%20is%20employed%20at%0Athe%20frame-level%2C%20using%20pseudo%20ground-truth%20obtained%20from%20weak%20labels%20at%20the%0Avideo-level%2C%20to%20identify%20the%20most%20relevant%20frames%20for%20action%20recognition.%0AExperiments%20in%20a%20real-world%20office%20environment%20using%20the%20MM-Office%20dataset%0Ademonstrate%20the%20superior%20performance%20of%20the%20proposed%20method%20compared%20to%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03302v1&entry.124074799=Read"},
{"title": "An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable\n  Radiology Report Generation", "author": "Ahmed Abdulaal and Hugo Fry and Nina Monta\u00f1a-Brown and Ayodeji Ijishakin and Jack Gao and Stephanie Hyland and Daniel C. Alexander and Daniel C. Castro", "abstract": "  Radiological services are experiencing unprecedented demand, leading to\nincreased interest in automating radiology report generation. Existing\nVision-Language Models (VLMs) suffer from hallucinations, lack\ninterpretability, and require expensive fine-tuning. We introduce SAE-Rad,\nwhich uses sparse autoencoders (SAEs) to decompose latent representations from\na pre-trained vision transformer into human-interpretable features. Our hybrid\narchitecture combines state-of-the-art SAE advancements, achieving accurate\nlatent reconstructions while maintaining sparsity. Using an off-the-shelf\nlanguage model, we distil ground-truth reports into radiological descriptions\nfor each SAE feature, which we then compile into a full report for each image,\neliminating the need for fine-tuning large models for this task. To the best of\nour knowledge, SAE-Rad represents the first instance of using mechanistic\ninterpretability techniques explicitly for a downstream multi-modal reasoning\ntask. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific\nmetrics compared to state-of-the-art models while using significantly fewer\ncomputational resources for training. Qualitative analysis reveals that SAE-Rad\nlearns meaningful visual concepts and generates reports aligning closely with\nexpert interpretations. Our results suggest that SAEs can enhance multimodal\nreasoning in healthcare, providing a more interpretable alternative to existing\nVLMs.\n", "link": "http://arxiv.org/abs/2410.03334v1", "date": "2024-10-04", "relevancy": 2.1719, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20X-Ray%20Is%20Worth%2015%20Features%3A%20Sparse%20Autoencoders%20for%20Interpretable%0A%20%20Radiology%20Report%20Generation&body=Title%3A%20An%20X-Ray%20Is%20Worth%2015%20Features%3A%20Sparse%20Autoencoders%20for%20Interpretable%0A%20%20Radiology%20Report%20Generation%0AAuthor%3A%20Ahmed%20Abdulaal%20and%20Hugo%20Fry%20and%20Nina%20Monta%C3%B1a-Brown%20and%20Ayodeji%20Ijishakin%20and%20Jack%20Gao%20and%20Stephanie%20Hyland%20and%20Daniel%20C.%20Alexander%20and%20Daniel%20C.%20Castro%0AAbstract%3A%20%20%20Radiological%20services%20are%20experiencing%20unprecedented%20demand%2C%20leading%20to%0Aincreased%20interest%20in%20automating%20radiology%20report%20generation.%20Existing%0AVision-Language%20Models%20%28VLMs%29%20suffer%20from%20hallucinations%2C%20lack%0Ainterpretability%2C%20and%20require%20expensive%20fine-tuning.%20We%20introduce%20SAE-Rad%2C%0Awhich%20uses%20sparse%20autoencoders%20%28SAEs%29%20to%20decompose%20latent%20representations%20from%0Aa%20pre-trained%20vision%20transformer%20into%20human-interpretable%20features.%20Our%20hybrid%0Aarchitecture%20combines%20state-of-the-art%20SAE%20advancements%2C%20achieving%20accurate%0Alatent%20reconstructions%20while%20maintaining%20sparsity.%20Using%20an%20off-the-shelf%0Alanguage%20model%2C%20we%20distil%20ground-truth%20reports%20into%20radiological%20descriptions%0Afor%20each%20SAE%20feature%2C%20which%20we%20then%20compile%20into%20a%20full%20report%20for%20each%20image%2C%0Aeliminating%20the%20need%20for%20fine-tuning%20large%20models%20for%20this%20task.%20To%20the%20best%20of%0Aour%20knowledge%2C%20SAE-Rad%20represents%20the%20first%20instance%20of%20using%20mechanistic%0Ainterpretability%20techniques%20explicitly%20for%20a%20downstream%20multi-modal%20reasoning%0Atask.%20On%20the%20MIMIC-CXR%20dataset%2C%20SAE-Rad%20achieves%20competitive%20radiology-specific%0Ametrics%20compared%20to%20state-of-the-art%20models%20while%20using%20significantly%20fewer%0Acomputational%20resources%20for%20training.%20Qualitative%20analysis%20reveals%20that%20SAE-Rad%0Alearns%20meaningful%20visual%20concepts%20and%20generates%20reports%20aligning%20closely%20with%0Aexpert%20interpretations.%20Our%20results%20suggest%20that%20SAEs%20can%20enhance%20multimodal%0Areasoning%20in%20healthcare%2C%20providing%20a%20more%20interpretable%20alternative%20to%20existing%0AVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520X-Ray%2520Is%2520Worth%252015%2520Features%253A%2520Sparse%2520Autoencoders%2520for%2520Interpretable%250A%2520%2520Radiology%2520Report%2520Generation%26entry.906535625%3DAhmed%2520Abdulaal%2520and%2520Hugo%2520Fry%2520and%2520Nina%2520Monta%25C3%25B1a-Brown%2520and%2520Ayodeji%2520Ijishakin%2520and%2520Jack%2520Gao%2520and%2520Stephanie%2520Hyland%2520and%2520Daniel%2520C.%2520Alexander%2520and%2520Daniel%2520C.%2520Castro%26entry.1292438233%3D%2520%2520Radiological%2520services%2520are%2520experiencing%2520unprecedented%2520demand%252C%2520leading%2520to%250Aincreased%2520interest%2520in%2520automating%2520radiology%2520report%2520generation.%2520Existing%250AVision-Language%2520Models%2520%2528VLMs%2529%2520suffer%2520from%2520hallucinations%252C%2520lack%250Ainterpretability%252C%2520and%2520require%2520expensive%2520fine-tuning.%2520We%2520introduce%2520SAE-Rad%252C%250Awhich%2520uses%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520to%2520decompose%2520latent%2520representations%2520from%250Aa%2520pre-trained%2520vision%2520transformer%2520into%2520human-interpretable%2520features.%2520Our%2520hybrid%250Aarchitecture%2520combines%2520state-of-the-art%2520SAE%2520advancements%252C%2520achieving%2520accurate%250Alatent%2520reconstructions%2520while%2520maintaining%2520sparsity.%2520Using%2520an%2520off-the-shelf%250Alanguage%2520model%252C%2520we%2520distil%2520ground-truth%2520reports%2520into%2520radiological%2520descriptions%250Afor%2520each%2520SAE%2520feature%252C%2520which%2520we%2520then%2520compile%2520into%2520a%2520full%2520report%2520for%2520each%2520image%252C%250Aeliminating%2520the%2520need%2520for%2520fine-tuning%2520large%2520models%2520for%2520this%2520task.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520SAE-Rad%2520represents%2520the%2520first%2520instance%2520of%2520using%2520mechanistic%250Ainterpretability%2520techniques%2520explicitly%2520for%2520a%2520downstream%2520multi-modal%2520reasoning%250Atask.%2520On%2520the%2520MIMIC-CXR%2520dataset%252C%2520SAE-Rad%2520achieves%2520competitive%2520radiology-specific%250Ametrics%2520compared%2520to%2520state-of-the-art%2520models%2520while%2520using%2520significantly%2520fewer%250Acomputational%2520resources%2520for%2520training.%2520Qualitative%2520analysis%2520reveals%2520that%2520SAE-Rad%250Alearns%2520meaningful%2520visual%2520concepts%2520and%2520generates%2520reports%2520aligning%2520closely%2520with%250Aexpert%2520interpretations.%2520Our%2520results%2520suggest%2520that%2520SAEs%2520can%2520enhance%2520multimodal%250Areasoning%2520in%2520healthcare%252C%2520providing%2520a%2520more%2520interpretable%2520alternative%2520to%2520existing%250AVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20X-Ray%20Is%20Worth%2015%20Features%3A%20Sparse%20Autoencoders%20for%20Interpretable%0A%20%20Radiology%20Report%20Generation&entry.906535625=Ahmed%20Abdulaal%20and%20Hugo%20Fry%20and%20Nina%20Monta%C3%B1a-Brown%20and%20Ayodeji%20Ijishakin%20and%20Jack%20Gao%20and%20Stephanie%20Hyland%20and%20Daniel%20C.%20Alexander%20and%20Daniel%20C.%20Castro&entry.1292438233=%20%20Radiological%20services%20are%20experiencing%20unprecedented%20demand%2C%20leading%20to%0Aincreased%20interest%20in%20automating%20radiology%20report%20generation.%20Existing%0AVision-Language%20Models%20%28VLMs%29%20suffer%20from%20hallucinations%2C%20lack%0Ainterpretability%2C%20and%20require%20expensive%20fine-tuning.%20We%20introduce%20SAE-Rad%2C%0Awhich%20uses%20sparse%20autoencoders%20%28SAEs%29%20to%20decompose%20latent%20representations%20from%0Aa%20pre-trained%20vision%20transformer%20into%20human-interpretable%20features.%20Our%20hybrid%0Aarchitecture%20combines%20state-of-the-art%20SAE%20advancements%2C%20achieving%20accurate%0Alatent%20reconstructions%20while%20maintaining%20sparsity.%20Using%20an%20off-the-shelf%0Alanguage%20model%2C%20we%20distil%20ground-truth%20reports%20into%20radiological%20descriptions%0Afor%20each%20SAE%20feature%2C%20which%20we%20then%20compile%20into%20a%20full%20report%20for%20each%20image%2C%0Aeliminating%20the%20need%20for%20fine-tuning%20large%20models%20for%20this%20task.%20To%20the%20best%20of%0Aour%20knowledge%2C%20SAE-Rad%20represents%20the%20first%20instance%20of%20using%20mechanistic%0Ainterpretability%20techniques%20explicitly%20for%20a%20downstream%20multi-modal%20reasoning%0Atask.%20On%20the%20MIMIC-CXR%20dataset%2C%20SAE-Rad%20achieves%20competitive%20radiology-specific%0Ametrics%20compared%20to%20state-of-the-art%20models%20while%20using%20significantly%20fewer%0Acomputational%20resources%20for%20training.%20Qualitative%20analysis%20reveals%20that%20SAE-Rad%0Alearns%20meaningful%20visual%20concepts%20and%20generates%20reports%20aligning%20closely%20with%0Aexpert%20interpretations.%20Our%20results%20suggest%20that%20SAEs%20can%20enhance%20multimodal%0Areasoning%20in%20healthcare%2C%20providing%20a%20more%20interpretable%20alternative%20to%20existing%0AVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03334v1&entry.124074799=Read"},
{"title": "C-MELT: Contrastive Enhanced Masked Auto-Encoders for ECG-Language\n  Pre-Training", "author": "Manh Pham and Aaqib Saeed and Dong Ma", "abstract": "  Accurate interpretation of Electrocardiogram (ECG) signals is pivotal for\ndiagnosing cardiovascular diseases. Integrating ECG signals with their\naccompanying textual reports holds immense potential to enhance clinical\ndiagnostics through the combination of physiological data and qualitative\ninsights. However, this integration faces significant challenges due to\ninherent modality disparities and the scarcity of labeled data for robust\ncross-modal learning. To address these obstacles, we propose C-MELT, a novel\nframework that pre-trains ECG and text data using a contrastive masked\nauto-encoder architecture. C-MELT uniquely combines the strengths of generative\nwith enhanced discriminative capabilities to achieve robust cross-modal\nrepresentations. This is accomplished through masked modality modeling,\nspecialized loss functions, and an improved negative sampling strategy tailored\nfor cross-modal alignment. Extensive experiments on five public datasets across\ndiverse downstream tasks demonstrate that C-MELT significantly outperforms\nexisting methods, achieving 15% and 2% increases in linear probing and\nzero-shot performance over state-of-the-art models, respectively. These results\nhighlight the effectiveness of C-MELT, underscoring its potential to advance\nautomated clinical diagnostics through multi-modal representations.\n", "link": "http://arxiv.org/abs/2410.02131v2", "date": "2024-10-04", "relevancy": 2.1711, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5636}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5469}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-MELT%3A%20Contrastive%20Enhanced%20Masked%20Auto-Encoders%20for%20ECG-Language%0A%20%20Pre-Training&body=Title%3A%20C-MELT%3A%20Contrastive%20Enhanced%20Masked%20Auto-Encoders%20for%20ECG-Language%0A%20%20Pre-Training%0AAuthor%3A%20Manh%20Pham%20and%20Aaqib%20Saeed%20and%20Dong%20Ma%0AAbstract%3A%20%20%20Accurate%20interpretation%20of%20Electrocardiogram%20%28ECG%29%20signals%20is%20pivotal%20for%0Adiagnosing%20cardiovascular%20diseases.%20Integrating%20ECG%20signals%20with%20their%0Aaccompanying%20textual%20reports%20holds%20immense%20potential%20to%20enhance%20clinical%0Adiagnostics%20through%20the%20combination%20of%20physiological%20data%20and%20qualitative%0Ainsights.%20However%2C%20this%20integration%20faces%20significant%20challenges%20due%20to%0Ainherent%20modality%20disparities%20and%20the%20scarcity%20of%20labeled%20data%20for%20robust%0Across-modal%20learning.%20To%20address%20these%20obstacles%2C%20we%20propose%20C-MELT%2C%20a%20novel%0Aframework%20that%20pre-trains%20ECG%20and%20text%20data%20using%20a%20contrastive%20masked%0Aauto-encoder%20architecture.%20C-MELT%20uniquely%20combines%20the%20strengths%20of%20generative%0Awith%20enhanced%20discriminative%20capabilities%20to%20achieve%20robust%20cross-modal%0Arepresentations.%20This%20is%20accomplished%20through%20masked%20modality%20modeling%2C%0Aspecialized%20loss%20functions%2C%20and%20an%20improved%20negative%20sampling%20strategy%20tailored%0Afor%20cross-modal%20alignment.%20Extensive%20experiments%20on%20five%20public%20datasets%20across%0Adiverse%20downstream%20tasks%20demonstrate%20that%20C-MELT%20significantly%20outperforms%0Aexisting%20methods%2C%20achieving%2015%25%20and%202%25%20increases%20in%20linear%20probing%20and%0Azero-shot%20performance%20over%20state-of-the-art%20models%2C%20respectively.%20These%20results%0Ahighlight%20the%20effectiveness%20of%20C-MELT%2C%20underscoring%20its%20potential%20to%20advance%0Aautomated%20clinical%20diagnostics%20through%20multi-modal%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-MELT%253A%2520Contrastive%2520Enhanced%2520Masked%2520Auto-Encoders%2520for%2520ECG-Language%250A%2520%2520Pre-Training%26entry.906535625%3DManh%2520Pham%2520and%2520Aaqib%2520Saeed%2520and%2520Dong%2520Ma%26entry.1292438233%3D%2520%2520Accurate%2520interpretation%2520of%2520Electrocardiogram%2520%2528ECG%2529%2520signals%2520is%2520pivotal%2520for%250Adiagnosing%2520cardiovascular%2520diseases.%2520Integrating%2520ECG%2520signals%2520with%2520their%250Aaccompanying%2520textual%2520reports%2520holds%2520immense%2520potential%2520to%2520enhance%2520clinical%250Adiagnostics%2520through%2520the%2520combination%2520of%2520physiological%2520data%2520and%2520qualitative%250Ainsights.%2520However%252C%2520this%2520integration%2520faces%2520significant%2520challenges%2520due%2520to%250Ainherent%2520modality%2520disparities%2520and%2520the%2520scarcity%2520of%2520labeled%2520data%2520for%2520robust%250Across-modal%2520learning.%2520To%2520address%2520these%2520obstacles%252C%2520we%2520propose%2520C-MELT%252C%2520a%2520novel%250Aframework%2520that%2520pre-trains%2520ECG%2520and%2520text%2520data%2520using%2520a%2520contrastive%2520masked%250Aauto-encoder%2520architecture.%2520C-MELT%2520uniquely%2520combines%2520the%2520strengths%2520of%2520generative%250Awith%2520enhanced%2520discriminative%2520capabilities%2520to%2520achieve%2520robust%2520cross-modal%250Arepresentations.%2520This%2520is%2520accomplished%2520through%2520masked%2520modality%2520modeling%252C%250Aspecialized%2520loss%2520functions%252C%2520and%2520an%2520improved%2520negative%2520sampling%2520strategy%2520tailored%250Afor%2520cross-modal%2520alignment.%2520Extensive%2520experiments%2520on%2520five%2520public%2520datasets%2520across%250Adiverse%2520downstream%2520tasks%2520demonstrate%2520that%2520C-MELT%2520significantly%2520outperforms%250Aexisting%2520methods%252C%2520achieving%252015%2525%2520and%25202%2525%2520increases%2520in%2520linear%2520probing%2520and%250Azero-shot%2520performance%2520over%2520state-of-the-art%2520models%252C%2520respectively.%2520These%2520results%250Ahighlight%2520the%2520effectiveness%2520of%2520C-MELT%252C%2520underscoring%2520its%2520potential%2520to%2520advance%250Aautomated%2520clinical%2520diagnostics%2520through%2520multi-modal%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-MELT%3A%20Contrastive%20Enhanced%20Masked%20Auto-Encoders%20for%20ECG-Language%0A%20%20Pre-Training&entry.906535625=Manh%20Pham%20and%20Aaqib%20Saeed%20and%20Dong%20Ma&entry.1292438233=%20%20Accurate%20interpretation%20of%20Electrocardiogram%20%28ECG%29%20signals%20is%20pivotal%20for%0Adiagnosing%20cardiovascular%20diseases.%20Integrating%20ECG%20signals%20with%20their%0Aaccompanying%20textual%20reports%20holds%20immense%20potential%20to%20enhance%20clinical%0Adiagnostics%20through%20the%20combination%20of%20physiological%20data%20and%20qualitative%0Ainsights.%20However%2C%20this%20integration%20faces%20significant%20challenges%20due%20to%0Ainherent%20modality%20disparities%20and%20the%20scarcity%20of%20labeled%20data%20for%20robust%0Across-modal%20learning.%20To%20address%20these%20obstacles%2C%20we%20propose%20C-MELT%2C%20a%20novel%0Aframework%20that%20pre-trains%20ECG%20and%20text%20data%20using%20a%20contrastive%20masked%0Aauto-encoder%20architecture.%20C-MELT%20uniquely%20combines%20the%20strengths%20of%20generative%0Awith%20enhanced%20discriminative%20capabilities%20to%20achieve%20robust%20cross-modal%0Arepresentations.%20This%20is%20accomplished%20through%20masked%20modality%20modeling%2C%0Aspecialized%20loss%20functions%2C%20and%20an%20improved%20negative%20sampling%20strategy%20tailored%0Afor%20cross-modal%20alignment.%20Extensive%20experiments%20on%20five%20public%20datasets%20across%0Adiverse%20downstream%20tasks%20demonstrate%20that%20C-MELT%20significantly%20outperforms%0Aexisting%20methods%2C%20achieving%2015%25%20and%202%25%20increases%20in%20linear%20probing%20and%0Azero-shot%20performance%20over%20state-of-the-art%20models%2C%20respectively.%20These%20results%0Ahighlight%20the%20effectiveness%20of%20C-MELT%2C%20underscoring%20its%20potential%20to%20advance%0Aautomated%20clinical%20diagnostics%20through%20multi-modal%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02131v2&entry.124074799=Read"},
{"title": "Linear combinations of Gaussian latents in generative models:\n  interpolation and beyond", "author": "Erik Bodin and Carl Henrik Ek and Henry Moss", "abstract": "  Sampling from generative models has become a crucial tool for applications\nlike data synthesis and augmentation. Diffusion, Flow Matching and Continuous\nNormalizing Flows have shown effectiveness across various modalities, and rely\non Gaussian latent variables for generation. For search-based or creative\napplications that require additional control over the generation process, it\nhas become common to manipulate the latent variable directly. However, existing\napproaches for performing such manipulations (e.g. interpolation or forming\nlow-dimensional representations) only work well in special cases or are network\nor data-modality specific. We propose Combination of Gaussian variables (COG)\nas a general purpose interpolation method that is easy to implement yet\noutperforms recent sophisticated methods. Moreover, COG naturally addresses the\nbroader task of forming general linear combinations of latent variables,\nallowing the construction of subspaces of the latent space, dramatically\nsimplifying the creation of expressive low-dimensional spaces of\nhigh-dimensional objects.\n", "link": "http://arxiv.org/abs/2408.08558v2", "date": "2024-10-04", "relevancy": 2.1686, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6214}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5352}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20combinations%20of%20Gaussian%20latents%20in%20generative%20models%3A%0A%20%20interpolation%20and%20beyond&body=Title%3A%20Linear%20combinations%20of%20Gaussian%20latents%20in%20generative%20models%3A%0A%20%20interpolation%20and%20beyond%0AAuthor%3A%20Erik%20Bodin%20and%20Carl%20Henrik%20Ek%20and%20Henry%20Moss%0AAbstract%3A%20%20%20Sampling%20from%20generative%20models%20has%20become%20a%20crucial%20tool%20for%20applications%0Alike%20data%20synthesis%20and%20augmentation.%20Diffusion%2C%20Flow%20Matching%20and%20Continuous%0ANormalizing%20Flows%20have%20shown%20effectiveness%20across%20various%20modalities%2C%20and%20rely%0Aon%20Gaussian%20latent%20variables%20for%20generation.%20For%20search-based%20or%20creative%0Aapplications%20that%20require%20additional%20control%20over%20the%20generation%20process%2C%20it%0Ahas%20become%20common%20to%20manipulate%20the%20latent%20variable%20directly.%20However%2C%20existing%0Aapproaches%20for%20performing%20such%20manipulations%20%28e.g.%20interpolation%20or%20forming%0Alow-dimensional%20representations%29%20only%20work%20well%20in%20special%20cases%20or%20are%20network%0Aor%20data-modality%20specific.%20We%20propose%20Combination%20of%20Gaussian%20variables%20%28COG%29%0Aas%20a%20general%20purpose%20interpolation%20method%20that%20is%20easy%20to%20implement%20yet%0Aoutperforms%20recent%20sophisticated%20methods.%20Moreover%2C%20COG%20naturally%20addresses%20the%0Abroader%20task%20of%20forming%20general%20linear%20combinations%20of%20latent%20variables%2C%0Aallowing%20the%20construction%20of%20subspaces%20of%20the%20latent%20space%2C%20dramatically%0Asimplifying%20the%20creation%20of%20expressive%20low-dimensional%20spaces%20of%0Ahigh-dimensional%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08558v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520combinations%2520of%2520Gaussian%2520latents%2520in%2520generative%2520models%253A%250A%2520%2520interpolation%2520and%2520beyond%26entry.906535625%3DErik%2520Bodin%2520and%2520Carl%2520Henrik%2520Ek%2520and%2520Henry%2520Moss%26entry.1292438233%3D%2520%2520Sampling%2520from%2520generative%2520models%2520has%2520become%2520a%2520crucial%2520tool%2520for%2520applications%250Alike%2520data%2520synthesis%2520and%2520augmentation.%2520Diffusion%252C%2520Flow%2520Matching%2520and%2520Continuous%250ANormalizing%2520Flows%2520have%2520shown%2520effectiveness%2520across%2520various%2520modalities%252C%2520and%2520rely%250Aon%2520Gaussian%2520latent%2520variables%2520for%2520generation.%2520For%2520search-based%2520or%2520creative%250Aapplications%2520that%2520require%2520additional%2520control%2520over%2520the%2520generation%2520process%252C%2520it%250Ahas%2520become%2520common%2520to%2520manipulate%2520the%2520latent%2520variable%2520directly.%2520However%252C%2520existing%250Aapproaches%2520for%2520performing%2520such%2520manipulations%2520%2528e.g.%2520interpolation%2520or%2520forming%250Alow-dimensional%2520representations%2529%2520only%2520work%2520well%2520in%2520special%2520cases%2520or%2520are%2520network%250Aor%2520data-modality%2520specific.%2520We%2520propose%2520Combination%2520of%2520Gaussian%2520variables%2520%2528COG%2529%250Aas%2520a%2520general%2520purpose%2520interpolation%2520method%2520that%2520is%2520easy%2520to%2520implement%2520yet%250Aoutperforms%2520recent%2520sophisticated%2520methods.%2520Moreover%252C%2520COG%2520naturally%2520addresses%2520the%250Abroader%2520task%2520of%2520forming%2520general%2520linear%2520combinations%2520of%2520latent%2520variables%252C%250Aallowing%2520the%2520construction%2520of%2520subspaces%2520of%2520the%2520latent%2520space%252C%2520dramatically%250Asimplifying%2520the%2520creation%2520of%2520expressive%2520low-dimensional%2520spaces%2520of%250Ahigh-dimensional%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08558v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20combinations%20of%20Gaussian%20latents%20in%20generative%20models%3A%0A%20%20interpolation%20and%20beyond&entry.906535625=Erik%20Bodin%20and%20Carl%20Henrik%20Ek%20and%20Henry%20Moss&entry.1292438233=%20%20Sampling%20from%20generative%20models%20has%20become%20a%20crucial%20tool%20for%20applications%0Alike%20data%20synthesis%20and%20augmentation.%20Diffusion%2C%20Flow%20Matching%20and%20Continuous%0ANormalizing%20Flows%20have%20shown%20effectiveness%20across%20various%20modalities%2C%20and%20rely%0Aon%20Gaussian%20latent%20variables%20for%20generation.%20For%20search-based%20or%20creative%0Aapplications%20that%20require%20additional%20control%20over%20the%20generation%20process%2C%20it%0Ahas%20become%20common%20to%20manipulate%20the%20latent%20variable%20directly.%20However%2C%20existing%0Aapproaches%20for%20performing%20such%20manipulations%20%28e.g.%20interpolation%20or%20forming%0Alow-dimensional%20representations%29%20only%20work%20well%20in%20special%20cases%20or%20are%20network%0Aor%20data-modality%20specific.%20We%20propose%20Combination%20of%20Gaussian%20variables%20%28COG%29%0Aas%20a%20general%20purpose%20interpolation%20method%20that%20is%20easy%20to%20implement%20yet%0Aoutperforms%20recent%20sophisticated%20methods.%20Moreover%2C%20COG%20naturally%20addresses%20the%0Abroader%20task%20of%20forming%20general%20linear%20combinations%20of%20latent%20variables%2C%0Aallowing%20the%20construction%20of%20subspaces%20of%20the%20latent%20space%2C%20dramatically%0Asimplifying%20the%20creation%20of%20expressive%20low-dimensional%20spaces%20of%0Ahigh-dimensional%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08558v2&entry.124074799=Read"},
{"title": "LInK: Learning Joint Representations of Design and Performance Spaces\n  through Contrastive Learning for Mechanism Synthesis", "author": "Amin Heyrani Nobari and Akash Srivastava and Dan Gutfreund and Kai Xu and Faez Ahmed", "abstract": "  In this paper, we introduce LInK, a novel framework that integrates\ncontrastive learning of performance and design space with optimization\ntechniques for solving complex inverse problems in engineering design with\ndiscrete and continuous variables. We focus on the path synthesis problem for\nplanar linkage mechanisms. By leveraging a multimodal and\ntransformation-invariant contrastive learning framework, LInK learns a joint\nrepresentation that captures complex physics and design representations of\nmechanisms, enabling rapid retrieval from a vast dataset of over 10 million\nmechanisms. This approach improves precision through the warm start of a\nhierarchical unconstrained nonlinear optimization algorithm, combining the\nrobustness of traditional optimization with the speed and adaptability of\nmodern deep learning methods. Our results on an existing benchmark demonstrate\nthat LInK outperforms existing methods with 28 times less error compared to a\nstate of the art approach while taking 20 times less time on an existing\nbenchmark. Moreover, we introduce a significantly more challenging benchmark,\nnamed LINK ABC, which involves synthesizing linkages that trace the\ntrajectories of English capital alphabets, an inverse design benchmark task\nthat existing methods struggle with due to large nonlinearities and tiny\nfeasible space. Our results demonstrate that LInK not only advances the field\nof mechanism design but also broadens the applicability of contrastive learning\nand optimization to other areas of engineering. The code and data are publicly\navailable at https://github.com/ahnobari/LInK.\n", "link": "http://arxiv.org/abs/2405.20592v2", "date": "2024-10-04", "relevancy": 2.1491, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LInK%3A%20Learning%20Joint%20Representations%20of%20Design%20and%20Performance%20Spaces%0A%20%20through%20Contrastive%20Learning%20for%20Mechanism%20Synthesis&body=Title%3A%20LInK%3A%20Learning%20Joint%20Representations%20of%20Design%20and%20Performance%20Spaces%0A%20%20through%20Contrastive%20Learning%20for%20Mechanism%20Synthesis%0AAuthor%3A%20Amin%20Heyrani%20Nobari%20and%20Akash%20Srivastava%20and%20Dan%20Gutfreund%20and%20Kai%20Xu%20and%20Faez%20Ahmed%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20LInK%2C%20a%20novel%20framework%20that%20integrates%0Acontrastive%20learning%20of%20performance%20and%20design%20space%20with%20optimization%0Atechniques%20for%20solving%20complex%20inverse%20problems%20in%20engineering%20design%20with%0Adiscrete%20and%20continuous%20variables.%20We%20focus%20on%20the%20path%20synthesis%20problem%20for%0Aplanar%20linkage%20mechanisms.%20By%20leveraging%20a%20multimodal%20and%0Atransformation-invariant%20contrastive%20learning%20framework%2C%20LInK%20learns%20a%20joint%0Arepresentation%20that%20captures%20complex%20physics%20and%20design%20representations%20of%0Amechanisms%2C%20enabling%20rapid%20retrieval%20from%20a%20vast%20dataset%20of%20over%2010%20million%0Amechanisms.%20This%20approach%20improves%20precision%20through%20the%20warm%20start%20of%20a%0Ahierarchical%20unconstrained%20nonlinear%20optimization%20algorithm%2C%20combining%20the%0Arobustness%20of%20traditional%20optimization%20with%20the%20speed%20and%20adaptability%20of%0Amodern%20deep%20learning%20methods.%20Our%20results%20on%20an%20existing%20benchmark%20demonstrate%0Athat%20LInK%20outperforms%20existing%20methods%20with%2028%20times%20less%20error%20compared%20to%20a%0Astate%20of%20the%20art%20approach%20while%20taking%2020%20times%20less%20time%20on%20an%20existing%0Abenchmark.%20Moreover%2C%20we%20introduce%20a%20significantly%20more%20challenging%20benchmark%2C%0Anamed%20LINK%20ABC%2C%20which%20involves%20synthesizing%20linkages%20that%20trace%20the%0Atrajectories%20of%20English%20capital%20alphabets%2C%20an%20inverse%20design%20benchmark%20task%0Athat%20existing%20methods%20struggle%20with%20due%20to%20large%20nonlinearities%20and%20tiny%0Afeasible%20space.%20Our%20results%20demonstrate%20that%20LInK%20not%20only%20advances%20the%20field%0Aof%20mechanism%20design%20but%20also%20broadens%20the%20applicability%20of%20contrastive%20learning%0Aand%20optimization%20to%20other%20areas%20of%20engineering.%20The%20code%20and%20data%20are%20publicly%0Aavailable%20at%20https%3A//github.com/ahnobari/LInK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLInK%253A%2520Learning%2520Joint%2520Representations%2520of%2520Design%2520and%2520Performance%2520Spaces%250A%2520%2520through%2520Contrastive%2520Learning%2520for%2520Mechanism%2520Synthesis%26entry.906535625%3DAmin%2520Heyrani%2520Nobari%2520and%2520Akash%2520Srivastava%2520and%2520Dan%2520Gutfreund%2520and%2520Kai%2520Xu%2520and%2520Faez%2520Ahmed%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LInK%252C%2520a%2520novel%2520framework%2520that%2520integrates%250Acontrastive%2520learning%2520of%2520performance%2520and%2520design%2520space%2520with%2520optimization%250Atechniques%2520for%2520solving%2520complex%2520inverse%2520problems%2520in%2520engineering%2520design%2520with%250Adiscrete%2520and%2520continuous%2520variables.%2520We%2520focus%2520on%2520the%2520path%2520synthesis%2520problem%2520for%250Aplanar%2520linkage%2520mechanisms.%2520By%2520leveraging%2520a%2520multimodal%2520and%250Atransformation-invariant%2520contrastive%2520learning%2520framework%252C%2520LInK%2520learns%2520a%2520joint%250Arepresentation%2520that%2520captures%2520complex%2520physics%2520and%2520design%2520representations%2520of%250Amechanisms%252C%2520enabling%2520rapid%2520retrieval%2520from%2520a%2520vast%2520dataset%2520of%2520over%252010%2520million%250Amechanisms.%2520This%2520approach%2520improves%2520precision%2520through%2520the%2520warm%2520start%2520of%2520a%250Ahierarchical%2520unconstrained%2520nonlinear%2520optimization%2520algorithm%252C%2520combining%2520the%250Arobustness%2520of%2520traditional%2520optimization%2520with%2520the%2520speed%2520and%2520adaptability%2520of%250Amodern%2520deep%2520learning%2520methods.%2520Our%2520results%2520on%2520an%2520existing%2520benchmark%2520demonstrate%250Athat%2520LInK%2520outperforms%2520existing%2520methods%2520with%252028%2520times%2520less%2520error%2520compared%2520to%2520a%250Astate%2520of%2520the%2520art%2520approach%2520while%2520taking%252020%2520times%2520less%2520time%2520on%2520an%2520existing%250Abenchmark.%2520Moreover%252C%2520we%2520introduce%2520a%2520significantly%2520more%2520challenging%2520benchmark%252C%250Anamed%2520LINK%2520ABC%252C%2520which%2520involves%2520synthesizing%2520linkages%2520that%2520trace%2520the%250Atrajectories%2520of%2520English%2520capital%2520alphabets%252C%2520an%2520inverse%2520design%2520benchmark%2520task%250Athat%2520existing%2520methods%2520struggle%2520with%2520due%2520to%2520large%2520nonlinearities%2520and%2520tiny%250Afeasible%2520space.%2520Our%2520results%2520demonstrate%2520that%2520LInK%2520not%2520only%2520advances%2520the%2520field%250Aof%2520mechanism%2520design%2520but%2520also%2520broadens%2520the%2520applicability%2520of%2520contrastive%2520learning%250Aand%2520optimization%2520to%2520other%2520areas%2520of%2520engineering.%2520The%2520code%2520and%2520data%2520are%2520publicly%250Aavailable%2520at%2520https%253A//github.com/ahnobari/LInK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LInK%3A%20Learning%20Joint%20Representations%20of%20Design%20and%20Performance%20Spaces%0A%20%20through%20Contrastive%20Learning%20for%20Mechanism%20Synthesis&entry.906535625=Amin%20Heyrani%20Nobari%20and%20Akash%20Srivastava%20and%20Dan%20Gutfreund%20and%20Kai%20Xu%20and%20Faez%20Ahmed&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20LInK%2C%20a%20novel%20framework%20that%20integrates%0Acontrastive%20learning%20of%20performance%20and%20design%20space%20with%20optimization%0Atechniques%20for%20solving%20complex%20inverse%20problems%20in%20engineering%20design%20with%0Adiscrete%20and%20continuous%20variables.%20We%20focus%20on%20the%20path%20synthesis%20problem%20for%0Aplanar%20linkage%20mechanisms.%20By%20leveraging%20a%20multimodal%20and%0Atransformation-invariant%20contrastive%20learning%20framework%2C%20LInK%20learns%20a%20joint%0Arepresentation%20that%20captures%20complex%20physics%20and%20design%20representations%20of%0Amechanisms%2C%20enabling%20rapid%20retrieval%20from%20a%20vast%20dataset%20of%20over%2010%20million%0Amechanisms.%20This%20approach%20improves%20precision%20through%20the%20warm%20start%20of%20a%0Ahierarchical%20unconstrained%20nonlinear%20optimization%20algorithm%2C%20combining%20the%0Arobustness%20of%20traditional%20optimization%20with%20the%20speed%20and%20adaptability%20of%0Amodern%20deep%20learning%20methods.%20Our%20results%20on%20an%20existing%20benchmark%20demonstrate%0Athat%20LInK%20outperforms%20existing%20methods%20with%2028%20times%20less%20error%20compared%20to%20a%0Astate%20of%20the%20art%20approach%20while%20taking%2020%20times%20less%20time%20on%20an%20existing%0Abenchmark.%20Moreover%2C%20we%20introduce%20a%20significantly%20more%20challenging%20benchmark%2C%0Anamed%20LINK%20ABC%2C%20which%20involves%20synthesizing%20linkages%20that%20trace%20the%0Atrajectories%20of%20English%20capital%20alphabets%2C%20an%20inverse%20design%20benchmark%20task%0Athat%20existing%20methods%20struggle%20with%20due%20to%20large%20nonlinearities%20and%20tiny%0Afeasible%20space.%20Our%20results%20demonstrate%20that%20LInK%20not%20only%20advances%20the%20field%0Aof%20mechanism%20design%20but%20also%20broadens%20the%20applicability%20of%20contrastive%20learning%0Aand%20optimization%20to%20other%20areas%20of%20engineering.%20The%20code%20and%20data%20are%20publicly%0Aavailable%20at%20https%3A//github.com/ahnobari/LInK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20592v2&entry.124074799=Read"},
{"title": "T-FAKE: Synthesizing Thermal Images for Facial Landmarking", "author": "Philipp Flotho and Moritz Piening and Anna Kukleva and Gabriele Steidl", "abstract": "  Facial analysis is a key component in a wide range of applications such as\nsecurity, autonomous driving, entertainment, and healthcare. Despite the\navailability of various facial RGB datasets, the thermal modality, which plays\na crucial role in life sciences, medicine, and biometrics, has been largely\noverlooked. To address this gap, we introduce the T-FAKE dataset, a new\nlarge-scale synthetic thermal dataset with sparse and dense landmarks. To\nfacilitate the creation of the dataset, we propose a novel RGB2Thermal loss\nfunction, which enables the transfer of thermal style to RGB faces. By\nutilizing the Wasserstein distance between thermal and RGB patches and the\nstatistical analysis of clinical temperature distributions on faces, we ensure\nthat the generated thermal images closely resemble real samples. Using\nRGB2Thermal style transfer based on our RGB2Thermal loss function, we create\nthe T-FAKE dataset, a large-scale synthetic thermal dataset of faces.\nLeveraging our novel T-FAKE dataset, probabilistic landmark prediction, and\nlabel adaptation networks, we demonstrate significant improvements in landmark\ndetection methods on thermal images across different landmark conventions. Our\nmodels show excellent performance with both sparse 70-point landmarks and dense\n478-point landmark annotations. Our code and models are available at\nhttps://github.com/phflot/tfake.\n", "link": "http://arxiv.org/abs/2408.15127v2", "date": "2024-10-04", "relevancy": 2.1442, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5418}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5326}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking&body=Title%3A%20T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking%0AAuthor%3A%20Philipp%20Flotho%20and%20Moritz%20Piening%20and%20Anna%20Kukleva%20and%20Gabriele%20Steidl%0AAbstract%3A%20%20%20Facial%20analysis%20is%20a%20key%20component%20in%20a%20wide%20range%20of%20applications%20such%20as%0Asecurity%2C%20autonomous%20driving%2C%20entertainment%2C%20and%20healthcare.%20Despite%20the%0Aavailability%20of%20various%20facial%20RGB%20datasets%2C%20the%20thermal%20modality%2C%20which%20plays%0Aa%20crucial%20role%20in%20life%20sciences%2C%20medicine%2C%20and%20biometrics%2C%20has%20been%20largely%0Aoverlooked.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20T-FAKE%20dataset%2C%20a%20new%0Alarge-scale%20synthetic%20thermal%20dataset%20with%20sparse%20and%20dense%20landmarks.%20To%0Afacilitate%20the%20creation%20of%20the%20dataset%2C%20we%20propose%20a%20novel%20RGB2Thermal%20loss%0Afunction%2C%20which%20enables%20the%20transfer%20of%20thermal%20style%20to%20RGB%20faces.%20By%0Autilizing%20the%20Wasserstein%20distance%20between%20thermal%20and%20RGB%20patches%20and%20the%0Astatistical%20analysis%20of%20clinical%20temperature%20distributions%20on%20faces%2C%20we%20ensure%0Athat%20the%20generated%20thermal%20images%20closely%20resemble%20real%20samples.%20Using%0ARGB2Thermal%20style%20transfer%20based%20on%20our%20RGB2Thermal%20loss%20function%2C%20we%20create%0Athe%20T-FAKE%20dataset%2C%20a%20large-scale%20synthetic%20thermal%20dataset%20of%20faces.%0ALeveraging%20our%20novel%20T-FAKE%20dataset%2C%20probabilistic%20landmark%20prediction%2C%20and%0Alabel%20adaptation%20networks%2C%20we%20demonstrate%20significant%20improvements%20in%20landmark%0Adetection%20methods%20on%20thermal%20images%20across%20different%20landmark%20conventions.%20Our%0Amodels%20show%20excellent%20performance%20with%20both%20sparse%2070-point%20landmarks%20and%20dense%0A478-point%20landmark%20annotations.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/phflot/tfake.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-FAKE%253A%2520Synthesizing%2520Thermal%2520Images%2520for%2520Facial%2520Landmarking%26entry.906535625%3DPhilipp%2520Flotho%2520and%2520Moritz%2520Piening%2520and%2520Anna%2520Kukleva%2520and%2520Gabriele%2520Steidl%26entry.1292438233%3D%2520%2520Facial%2520analysis%2520is%2520a%2520key%2520component%2520in%2520a%2520wide%2520range%2520of%2520applications%2520such%2520as%250Asecurity%252C%2520autonomous%2520driving%252C%2520entertainment%252C%2520and%2520healthcare.%2520Despite%2520the%250Aavailability%2520of%2520various%2520facial%2520RGB%2520datasets%252C%2520the%2520thermal%2520modality%252C%2520which%2520plays%250Aa%2520crucial%2520role%2520in%2520life%2520sciences%252C%2520medicine%252C%2520and%2520biometrics%252C%2520has%2520been%2520largely%250Aoverlooked.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520T-FAKE%2520dataset%252C%2520a%2520new%250Alarge-scale%2520synthetic%2520thermal%2520dataset%2520with%2520sparse%2520and%2520dense%2520landmarks.%2520To%250Afacilitate%2520the%2520creation%2520of%2520the%2520dataset%252C%2520we%2520propose%2520a%2520novel%2520RGB2Thermal%2520loss%250Afunction%252C%2520which%2520enables%2520the%2520transfer%2520of%2520thermal%2520style%2520to%2520RGB%2520faces.%2520By%250Autilizing%2520the%2520Wasserstein%2520distance%2520between%2520thermal%2520and%2520RGB%2520patches%2520and%2520the%250Astatistical%2520analysis%2520of%2520clinical%2520temperature%2520distributions%2520on%2520faces%252C%2520we%2520ensure%250Athat%2520the%2520generated%2520thermal%2520images%2520closely%2520resemble%2520real%2520samples.%2520Using%250ARGB2Thermal%2520style%2520transfer%2520based%2520on%2520our%2520RGB2Thermal%2520loss%2520function%252C%2520we%2520create%250Athe%2520T-FAKE%2520dataset%252C%2520a%2520large-scale%2520synthetic%2520thermal%2520dataset%2520of%2520faces.%250ALeveraging%2520our%2520novel%2520T-FAKE%2520dataset%252C%2520probabilistic%2520landmark%2520prediction%252C%2520and%250Alabel%2520adaptation%2520networks%252C%2520we%2520demonstrate%2520significant%2520improvements%2520in%2520landmark%250Adetection%2520methods%2520on%2520thermal%2520images%2520across%2520different%2520landmark%2520conventions.%2520Our%250Amodels%2520show%2520excellent%2520performance%2520with%2520both%2520sparse%252070-point%2520landmarks%2520and%2520dense%250A478-point%2520landmark%2520annotations.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/phflot/tfake.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking&entry.906535625=Philipp%20Flotho%20and%20Moritz%20Piening%20and%20Anna%20Kukleva%20and%20Gabriele%20Steidl&entry.1292438233=%20%20Facial%20analysis%20is%20a%20key%20component%20in%20a%20wide%20range%20of%20applications%20such%20as%0Asecurity%2C%20autonomous%20driving%2C%20entertainment%2C%20and%20healthcare.%20Despite%20the%0Aavailability%20of%20various%20facial%20RGB%20datasets%2C%20the%20thermal%20modality%2C%20which%20plays%0Aa%20crucial%20role%20in%20life%20sciences%2C%20medicine%2C%20and%20biometrics%2C%20has%20been%20largely%0Aoverlooked.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20T-FAKE%20dataset%2C%20a%20new%0Alarge-scale%20synthetic%20thermal%20dataset%20with%20sparse%20and%20dense%20landmarks.%20To%0Afacilitate%20the%20creation%20of%20the%20dataset%2C%20we%20propose%20a%20novel%20RGB2Thermal%20loss%0Afunction%2C%20which%20enables%20the%20transfer%20of%20thermal%20style%20to%20RGB%20faces.%20By%0Autilizing%20the%20Wasserstein%20distance%20between%20thermal%20and%20RGB%20patches%20and%20the%0Astatistical%20analysis%20of%20clinical%20temperature%20distributions%20on%20faces%2C%20we%20ensure%0Athat%20the%20generated%20thermal%20images%20closely%20resemble%20real%20samples.%20Using%0ARGB2Thermal%20style%20transfer%20based%20on%20our%20RGB2Thermal%20loss%20function%2C%20we%20create%0Athe%20T-FAKE%20dataset%2C%20a%20large-scale%20synthetic%20thermal%20dataset%20of%20faces.%0ALeveraging%20our%20novel%20T-FAKE%20dataset%2C%20probabilistic%20landmark%20prediction%2C%20and%0Alabel%20adaptation%20networks%2C%20we%20demonstrate%20significant%20improvements%20in%20landmark%0Adetection%20methods%20on%20thermal%20images%20across%20different%20landmark%20conventions.%20Our%0Amodels%20show%20excellent%20performance%20with%20both%20sparse%2070-point%20landmarks%20and%20dense%0A478-point%20landmark%20annotations.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/phflot/tfake.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15127v2&entry.124074799=Read"},
{"title": "A General Framework for Producing Interpretable Semantic Text Embeddings", "author": "Yiqun Sun and Qiang Huang and Yixuan Tang and Anthony K. H. Tung and Jun Yu", "abstract": "  Semantic text embedding is essential to many tasks in Natural Language\nProcessing (NLP). While black-box models are capable of generating high-quality\nembeddings, their lack of interpretability limits their use in tasks that\ndemand transparency. Recent approaches have improved interpretability by\nleveraging domain-expert-crafted or LLM-generated questions, but these methods\nrely heavily on expert input or well-prompt design, which restricts their\ngeneralizability and ability to generate discriminative questions across a wide\nrange of tasks. To address these challenges, we introduce \\algo{CQG-MBQA}\n(Contrastive Question Generation - Multi-task Binary Question Answering), a\ngeneral framework for producing interpretable semantic text embeddings across\ndiverse tasks. Our framework systematically generates highly discriminative,\nlow cognitive load yes/no questions through the \\algo{CQG} method and answers\nthem efficiently with the \\algo{MBQA} model, resulting in interpretable\nembeddings in a cost-effective manner. We validate the effectiveness and\ninterpretability of \\algo{CQG-MBQA} through extensive experiments and ablation\nstudies, demonstrating that it delivers embedding quality comparable to many\nadvanced black-box models while maintaining inherently interpretability.\nAdditionally, \\algo{CQG-MBQA} outperforms other interpretable text embedding\nmethods across various downstream tasks.\n", "link": "http://arxiv.org/abs/2410.03435v1", "date": "2024-10-04", "relevancy": 2.1377, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5395}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20General%20Framework%20for%20Producing%20Interpretable%20Semantic%20Text%20Embeddings&body=Title%3A%20A%20General%20Framework%20for%20Producing%20Interpretable%20Semantic%20Text%20Embeddings%0AAuthor%3A%20Yiqun%20Sun%20and%20Qiang%20Huang%20and%20Yixuan%20Tang%20and%20Anthony%20K.%20H.%20Tung%20and%20Jun%20Yu%0AAbstract%3A%20%20%20Semantic%20text%20embedding%20is%20essential%20to%20many%20tasks%20in%20Natural%20Language%0AProcessing%20%28NLP%29.%20While%20black-box%20models%20are%20capable%20of%20generating%20high-quality%0Aembeddings%2C%20their%20lack%20of%20interpretability%20limits%20their%20use%20in%20tasks%20that%0Ademand%20transparency.%20Recent%20approaches%20have%20improved%20interpretability%20by%0Aleveraging%20domain-expert-crafted%20or%20LLM-generated%20questions%2C%20but%20these%20methods%0Arely%20heavily%20on%20expert%20input%20or%20well-prompt%20design%2C%20which%20restricts%20their%0Ageneralizability%20and%20ability%20to%20generate%20discriminative%20questions%20across%20a%20wide%0Arange%20of%20tasks.%20To%20address%20these%20challenges%2C%20we%20introduce%20%5Calgo%7BCQG-MBQA%7D%0A%28Contrastive%20Question%20Generation%20-%20Multi-task%20Binary%20Question%20Answering%29%2C%20a%0Ageneral%20framework%20for%20producing%20interpretable%20semantic%20text%20embeddings%20across%0Adiverse%20tasks.%20Our%20framework%20systematically%20generates%20highly%20discriminative%2C%0Alow%20cognitive%20load%20yes/no%20questions%20through%20the%20%5Calgo%7BCQG%7D%20method%20and%20answers%0Athem%20efficiently%20with%20the%20%5Calgo%7BMBQA%7D%20model%2C%20resulting%20in%20interpretable%0Aembeddings%20in%20a%20cost-effective%20manner.%20We%20validate%20the%20effectiveness%20and%0Ainterpretability%20of%20%5Calgo%7BCQG-MBQA%7D%20through%20extensive%20experiments%20and%20ablation%0Astudies%2C%20demonstrating%20that%20it%20delivers%20embedding%20quality%20comparable%20to%20many%0Aadvanced%20black-box%20models%20while%20maintaining%20inherently%20interpretability.%0AAdditionally%2C%20%5Calgo%7BCQG-MBQA%7D%20outperforms%20other%20interpretable%20text%20embedding%0Amethods%20across%20various%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520General%2520Framework%2520for%2520Producing%2520Interpretable%2520Semantic%2520Text%2520Embeddings%26entry.906535625%3DYiqun%2520Sun%2520and%2520Qiang%2520Huang%2520and%2520Yixuan%2520Tang%2520and%2520Anthony%2520K.%2520H.%2520Tung%2520and%2520Jun%2520Yu%26entry.1292438233%3D%2520%2520Semantic%2520text%2520embedding%2520is%2520essential%2520to%2520many%2520tasks%2520in%2520Natural%2520Language%250AProcessing%2520%2528NLP%2529.%2520While%2520black-box%2520models%2520are%2520capable%2520of%2520generating%2520high-quality%250Aembeddings%252C%2520their%2520lack%2520of%2520interpretability%2520limits%2520their%2520use%2520in%2520tasks%2520that%250Ademand%2520transparency.%2520Recent%2520approaches%2520have%2520improved%2520interpretability%2520by%250Aleveraging%2520domain-expert-crafted%2520or%2520LLM-generated%2520questions%252C%2520but%2520these%2520methods%250Arely%2520heavily%2520on%2520expert%2520input%2520or%2520well-prompt%2520design%252C%2520which%2520restricts%2520their%250Ageneralizability%2520and%2520ability%2520to%2520generate%2520discriminative%2520questions%2520across%2520a%2520wide%250Arange%2520of%2520tasks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520%255Calgo%257BCQG-MBQA%257D%250A%2528Contrastive%2520Question%2520Generation%2520-%2520Multi-task%2520Binary%2520Question%2520Answering%2529%252C%2520a%250Ageneral%2520framework%2520for%2520producing%2520interpretable%2520semantic%2520text%2520embeddings%2520across%250Adiverse%2520tasks.%2520Our%2520framework%2520systematically%2520generates%2520highly%2520discriminative%252C%250Alow%2520cognitive%2520load%2520yes/no%2520questions%2520through%2520the%2520%255Calgo%257BCQG%257D%2520method%2520and%2520answers%250Athem%2520efficiently%2520with%2520the%2520%255Calgo%257BMBQA%257D%2520model%252C%2520resulting%2520in%2520interpretable%250Aembeddings%2520in%2520a%2520cost-effective%2520manner.%2520We%2520validate%2520the%2520effectiveness%2520and%250Ainterpretability%2520of%2520%255Calgo%257BCQG-MBQA%257D%2520through%2520extensive%2520experiments%2520and%2520ablation%250Astudies%252C%2520demonstrating%2520that%2520it%2520delivers%2520embedding%2520quality%2520comparable%2520to%2520many%250Aadvanced%2520black-box%2520models%2520while%2520maintaining%2520inherently%2520interpretability.%250AAdditionally%252C%2520%255Calgo%257BCQG-MBQA%257D%2520outperforms%2520other%2520interpretable%2520text%2520embedding%250Amethods%2520across%2520various%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20General%20Framework%20for%20Producing%20Interpretable%20Semantic%20Text%20Embeddings&entry.906535625=Yiqun%20Sun%20and%20Qiang%20Huang%20and%20Yixuan%20Tang%20and%20Anthony%20K.%20H.%20Tung%20and%20Jun%20Yu&entry.1292438233=%20%20Semantic%20text%20embedding%20is%20essential%20to%20many%20tasks%20in%20Natural%20Language%0AProcessing%20%28NLP%29.%20While%20black-box%20models%20are%20capable%20of%20generating%20high-quality%0Aembeddings%2C%20their%20lack%20of%20interpretability%20limits%20their%20use%20in%20tasks%20that%0Ademand%20transparency.%20Recent%20approaches%20have%20improved%20interpretability%20by%0Aleveraging%20domain-expert-crafted%20or%20LLM-generated%20questions%2C%20but%20these%20methods%0Arely%20heavily%20on%20expert%20input%20or%20well-prompt%20design%2C%20which%20restricts%20their%0Ageneralizability%20and%20ability%20to%20generate%20discriminative%20questions%20across%20a%20wide%0Arange%20of%20tasks.%20To%20address%20these%20challenges%2C%20we%20introduce%20%5Calgo%7BCQG-MBQA%7D%0A%28Contrastive%20Question%20Generation%20-%20Multi-task%20Binary%20Question%20Answering%29%2C%20a%0Ageneral%20framework%20for%20producing%20interpretable%20semantic%20text%20embeddings%20across%0Adiverse%20tasks.%20Our%20framework%20systematically%20generates%20highly%20discriminative%2C%0Alow%20cognitive%20load%20yes/no%20questions%20through%20the%20%5Calgo%7BCQG%7D%20method%20and%20answers%0Athem%20efficiently%20with%20the%20%5Calgo%7BMBQA%7D%20model%2C%20resulting%20in%20interpretable%0Aembeddings%20in%20a%20cost-effective%20manner.%20We%20validate%20the%20effectiveness%20and%0Ainterpretability%20of%20%5Calgo%7BCQG-MBQA%7D%20through%20extensive%20experiments%20and%20ablation%0Astudies%2C%20demonstrating%20that%20it%20delivers%20embedding%20quality%20comparable%20to%20many%0Aadvanced%20black-box%20models%20while%20maintaining%20inherently%20interpretability.%0AAdditionally%2C%20%5Calgo%7BCQG-MBQA%7D%20outperforms%20other%20interpretable%20text%20embedding%0Amethods%20across%20various%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03435v1&entry.124074799=Read"},
{"title": "NRGBoost: Energy-Based Generative Boosted Trees", "author": "Jo\u00e3o Bravo", "abstract": "  Despite the rise to dominance of deep learning in unstructured data domains,\ntree-based methods such as Random Forests (RF) and Gradient Boosted Decision\nTrees (GBDT) are still the workhorses for handling discriminative tasks on\ntabular data. We explore generative extensions of these popular algorithms with\na focus on explicitly modeling the data density (up to a normalization\nconstant), thus enabling other applications besides sampling. As our main\ncontribution we propose an energy-based generative boosting algorithm that is\nanalogous to the second order boosting implemented in popular packages like\nXGBoost. We show that, despite producing a generative model capable of handling\ninference tasks over any input variable, our proposed algorithm can achieve\nsimilar discriminative performance to GBDT on a number of real world tabular\ndatasets, outperforming alternative generative approaches. At the same time, we\nshow that it is also competitive with neural network based models for sampling.\n", "link": "http://arxiv.org/abs/2410.03535v1", "date": "2024-10-04", "relevancy": 2.1371, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5692}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5141}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NRGBoost%3A%20Energy-Based%20Generative%20Boosted%20Trees&body=Title%3A%20NRGBoost%3A%20Energy-Based%20Generative%20Boosted%20Trees%0AAuthor%3A%20Jo%C3%A3o%20Bravo%0AAbstract%3A%20%20%20Despite%20the%20rise%20to%20dominance%20of%20deep%20learning%20in%20unstructured%20data%20domains%2C%0Atree-based%20methods%20such%20as%20Random%20Forests%20%28RF%29%20and%20Gradient%20Boosted%20Decision%0ATrees%20%28GBDT%29%20are%20still%20the%20workhorses%20for%20handling%20discriminative%20tasks%20on%0Atabular%20data.%20We%20explore%20generative%20extensions%20of%20these%20popular%20algorithms%20with%0Aa%20focus%20on%20explicitly%20modeling%20the%20data%20density%20%28up%20to%20a%20normalization%0Aconstant%29%2C%20thus%20enabling%20other%20applications%20besides%20sampling.%20As%20our%20main%0Acontribution%20we%20propose%20an%20energy-based%20generative%20boosting%20algorithm%20that%20is%0Aanalogous%20to%20the%20second%20order%20boosting%20implemented%20in%20popular%20packages%20like%0AXGBoost.%20We%20show%20that%2C%20despite%20producing%20a%20generative%20model%20capable%20of%20handling%0Ainference%20tasks%20over%20any%20input%20variable%2C%20our%20proposed%20algorithm%20can%20achieve%0Asimilar%20discriminative%20performance%20to%20GBDT%20on%20a%20number%20of%20real%20world%20tabular%0Adatasets%2C%20outperforming%20alternative%20generative%20approaches.%20At%20the%20same%20time%2C%20we%0Ashow%20that%20it%20is%20also%20competitive%20with%20neural%20network%20based%20models%20for%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNRGBoost%253A%2520Energy-Based%2520Generative%2520Boosted%2520Trees%26entry.906535625%3DJo%25C3%25A3o%2520Bravo%26entry.1292438233%3D%2520%2520Despite%2520the%2520rise%2520to%2520dominance%2520of%2520deep%2520learning%2520in%2520unstructured%2520data%2520domains%252C%250Atree-based%2520methods%2520such%2520as%2520Random%2520Forests%2520%2528RF%2529%2520and%2520Gradient%2520Boosted%2520Decision%250ATrees%2520%2528GBDT%2529%2520are%2520still%2520the%2520workhorses%2520for%2520handling%2520discriminative%2520tasks%2520on%250Atabular%2520data.%2520We%2520explore%2520generative%2520extensions%2520of%2520these%2520popular%2520algorithms%2520with%250Aa%2520focus%2520on%2520explicitly%2520modeling%2520the%2520data%2520density%2520%2528up%2520to%2520a%2520normalization%250Aconstant%2529%252C%2520thus%2520enabling%2520other%2520applications%2520besides%2520sampling.%2520As%2520our%2520main%250Acontribution%2520we%2520propose%2520an%2520energy-based%2520generative%2520boosting%2520algorithm%2520that%2520is%250Aanalogous%2520to%2520the%2520second%2520order%2520boosting%2520implemented%2520in%2520popular%2520packages%2520like%250AXGBoost.%2520We%2520show%2520that%252C%2520despite%2520producing%2520a%2520generative%2520model%2520capable%2520of%2520handling%250Ainference%2520tasks%2520over%2520any%2520input%2520variable%252C%2520our%2520proposed%2520algorithm%2520can%2520achieve%250Asimilar%2520discriminative%2520performance%2520to%2520GBDT%2520on%2520a%2520number%2520of%2520real%2520world%2520tabular%250Adatasets%252C%2520outperforming%2520alternative%2520generative%2520approaches.%2520At%2520the%2520same%2520time%252C%2520we%250Ashow%2520that%2520it%2520is%2520also%2520competitive%2520with%2520neural%2520network%2520based%2520models%2520for%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NRGBoost%3A%20Energy-Based%20Generative%20Boosted%20Trees&entry.906535625=Jo%C3%A3o%20Bravo&entry.1292438233=%20%20Despite%20the%20rise%20to%20dominance%20of%20deep%20learning%20in%20unstructured%20data%20domains%2C%0Atree-based%20methods%20such%20as%20Random%20Forests%20%28RF%29%20and%20Gradient%20Boosted%20Decision%0ATrees%20%28GBDT%29%20are%20still%20the%20workhorses%20for%20handling%20discriminative%20tasks%20on%0Atabular%20data.%20We%20explore%20generative%20extensions%20of%20these%20popular%20algorithms%20with%0Aa%20focus%20on%20explicitly%20modeling%20the%20data%20density%20%28up%20to%20a%20normalization%0Aconstant%29%2C%20thus%20enabling%20other%20applications%20besides%20sampling.%20As%20our%20main%0Acontribution%20we%20propose%20an%20energy-based%20generative%20boosting%20algorithm%20that%20is%0Aanalogous%20to%20the%20second%20order%20boosting%20implemented%20in%20popular%20packages%20like%0AXGBoost.%20We%20show%20that%2C%20despite%20producing%20a%20generative%20model%20capable%20of%20handling%0Ainference%20tasks%20over%20any%20input%20variable%2C%20our%20proposed%20algorithm%20can%20achieve%0Asimilar%20discriminative%20performance%20to%20GBDT%20on%20a%20number%20of%20real%20world%20tabular%0Adatasets%2C%20outperforming%20alternative%20generative%20approaches.%20At%20the%20same%20time%2C%20we%0Ashow%20that%20it%20is%20also%20competitive%20with%20neural%20network%20based%20models%20for%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03535v1&entry.124074799=Read"},
{"title": "Does SpatioTemporal information benefit Two video summarization\n  benchmarks?", "author": "Aashutosh Ganesh and Mirela Popa and Daan Odijk and Nava Tintarev", "abstract": "  An important aspect of summarizing videos is understanding the temporal\ncontext behind each part of the video to grasp what is and is not important.\nVideo summarization models have in recent years modeled spatio-temporal\nrelationships to represent this information. These models achieved\nstate-of-the-art correlation scores on important benchmark datasets. However,\nwhat has not been reviewed is whether spatio-temporal relationships are even\nrequired to achieve state-of-the-art results. Previous work in activity\nrecognition has found biases, by prioritizing static cues such as scenes or\nobjects, over motion information. In this paper we inquire if similar spurious\nrelationships might influence the task of video summarization. To do so, we\nanalyse the role that temporal information plays on existing benchmark\ndatasets. We first estimate a baseline with temporally invariant models to see\nhow well such models rank on benchmark datasets (TVSum and SumMe). We then\ndisrupt the temporal order of the videos to investigate the impact it has on\nexisting state-of-the-art models. One of our findings is that the temporally\ninvariant models achieve competitive correlation scores that are close to the\nhuman baselines on the TVSum dataset. We also demonstrate that existing models\nare not affected by temporal perturbations. Furthermore, with certain\ndisruption strategies that shuffle fixed time segments, we can actually improve\ntheir correlation scores. With these results, we find that spatio-temporal\nrelationship play a minor role and we raise the question whether these\nbenchmarks adequately model the task of video summarization. Code available at:\nhttps://github.com/AashGan/TemporalPerturbSum\n", "link": "http://arxiv.org/abs/2410.03323v1", "date": "2024-10-04", "relevancy": 2.1257, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5493}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20SpatioTemporal%20information%20benefit%20Two%20video%20summarization%0A%20%20benchmarks%3F&body=Title%3A%20Does%20SpatioTemporal%20information%20benefit%20Two%20video%20summarization%0A%20%20benchmarks%3F%0AAuthor%3A%20Aashutosh%20Ganesh%20and%20Mirela%20Popa%20and%20Daan%20Odijk%20and%20Nava%20Tintarev%0AAbstract%3A%20%20%20An%20important%20aspect%20of%20summarizing%20videos%20is%20understanding%20the%20temporal%0Acontext%20behind%20each%20part%20of%20the%20video%20to%20grasp%20what%20is%20and%20is%20not%20important.%0AVideo%20summarization%20models%20have%20in%20recent%20years%20modeled%20spatio-temporal%0Arelationships%20to%20represent%20this%20information.%20These%20models%20achieved%0Astate-of-the-art%20correlation%20scores%20on%20important%20benchmark%20datasets.%20However%2C%0Awhat%20has%20not%20been%20reviewed%20is%20whether%20spatio-temporal%20relationships%20are%20even%0Arequired%20to%20achieve%20state-of-the-art%20results.%20Previous%20work%20in%20activity%0Arecognition%20has%20found%20biases%2C%20by%20prioritizing%20static%20cues%20such%20as%20scenes%20or%0Aobjects%2C%20over%20motion%20information.%20In%20this%20paper%20we%20inquire%20if%20similar%20spurious%0Arelationships%20might%20influence%20the%20task%20of%20video%20summarization.%20To%20do%20so%2C%20we%0Aanalyse%20the%20role%20that%20temporal%20information%20plays%20on%20existing%20benchmark%0Adatasets.%20We%20first%20estimate%20a%20baseline%20with%20temporally%20invariant%20models%20to%20see%0Ahow%20well%20such%20models%20rank%20on%20benchmark%20datasets%20%28TVSum%20and%20SumMe%29.%20We%20then%0Adisrupt%20the%20temporal%20order%20of%20the%20videos%20to%20investigate%20the%20impact%20it%20has%20on%0Aexisting%20state-of-the-art%20models.%20One%20of%20our%20findings%20is%20that%20the%20temporally%0Ainvariant%20models%20achieve%20competitive%20correlation%20scores%20that%20are%20close%20to%20the%0Ahuman%20baselines%20on%20the%20TVSum%20dataset.%20We%20also%20demonstrate%20that%20existing%20models%0Aare%20not%20affected%20by%20temporal%20perturbations.%20Furthermore%2C%20with%20certain%0Adisruption%20strategies%20that%20shuffle%20fixed%20time%20segments%2C%20we%20can%20actually%20improve%0Atheir%20correlation%20scores.%20With%20these%20results%2C%20we%20find%20that%20spatio-temporal%0Arelationship%20play%20a%20minor%20role%20and%20we%20raise%20the%20question%20whether%20these%0Abenchmarks%20adequately%20model%20the%20task%20of%20video%20summarization.%20Code%20available%20at%3A%0Ahttps%3A//github.com/AashGan/TemporalPerturbSum%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520SpatioTemporal%2520information%2520benefit%2520Two%2520video%2520summarization%250A%2520%2520benchmarks%253F%26entry.906535625%3DAashutosh%2520Ganesh%2520and%2520Mirela%2520Popa%2520and%2520Daan%2520Odijk%2520and%2520Nava%2520Tintarev%26entry.1292438233%3D%2520%2520An%2520important%2520aspect%2520of%2520summarizing%2520videos%2520is%2520understanding%2520the%2520temporal%250Acontext%2520behind%2520each%2520part%2520of%2520the%2520video%2520to%2520grasp%2520what%2520is%2520and%2520is%2520not%2520important.%250AVideo%2520summarization%2520models%2520have%2520in%2520recent%2520years%2520modeled%2520spatio-temporal%250Arelationships%2520to%2520represent%2520this%2520information.%2520These%2520models%2520achieved%250Astate-of-the-art%2520correlation%2520scores%2520on%2520important%2520benchmark%2520datasets.%2520However%252C%250Awhat%2520has%2520not%2520been%2520reviewed%2520is%2520whether%2520spatio-temporal%2520relationships%2520are%2520even%250Arequired%2520to%2520achieve%2520state-of-the-art%2520results.%2520Previous%2520work%2520in%2520activity%250Arecognition%2520has%2520found%2520biases%252C%2520by%2520prioritizing%2520static%2520cues%2520such%2520as%2520scenes%2520or%250Aobjects%252C%2520over%2520motion%2520information.%2520In%2520this%2520paper%2520we%2520inquire%2520if%2520similar%2520spurious%250Arelationships%2520might%2520influence%2520the%2520task%2520of%2520video%2520summarization.%2520To%2520do%2520so%252C%2520we%250Aanalyse%2520the%2520role%2520that%2520temporal%2520information%2520plays%2520on%2520existing%2520benchmark%250Adatasets.%2520We%2520first%2520estimate%2520a%2520baseline%2520with%2520temporally%2520invariant%2520models%2520to%2520see%250Ahow%2520well%2520such%2520models%2520rank%2520on%2520benchmark%2520datasets%2520%2528TVSum%2520and%2520SumMe%2529.%2520We%2520then%250Adisrupt%2520the%2520temporal%2520order%2520of%2520the%2520videos%2520to%2520investigate%2520the%2520impact%2520it%2520has%2520on%250Aexisting%2520state-of-the-art%2520models.%2520One%2520of%2520our%2520findings%2520is%2520that%2520the%2520temporally%250Ainvariant%2520models%2520achieve%2520competitive%2520correlation%2520scores%2520that%2520are%2520close%2520to%2520the%250Ahuman%2520baselines%2520on%2520the%2520TVSum%2520dataset.%2520We%2520also%2520demonstrate%2520that%2520existing%2520models%250Aare%2520not%2520affected%2520by%2520temporal%2520perturbations.%2520Furthermore%252C%2520with%2520certain%250Adisruption%2520strategies%2520that%2520shuffle%2520fixed%2520time%2520segments%252C%2520we%2520can%2520actually%2520improve%250Atheir%2520correlation%2520scores.%2520With%2520these%2520results%252C%2520we%2520find%2520that%2520spatio-temporal%250Arelationship%2520play%2520a%2520minor%2520role%2520and%2520we%2520raise%2520the%2520question%2520whether%2520these%250Abenchmarks%2520adequately%2520model%2520the%2520task%2520of%2520video%2520summarization.%2520Code%2520available%2520at%253A%250Ahttps%253A//github.com/AashGan/TemporalPerturbSum%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20SpatioTemporal%20information%20benefit%20Two%20video%20summarization%0A%20%20benchmarks%3F&entry.906535625=Aashutosh%20Ganesh%20and%20Mirela%20Popa%20and%20Daan%20Odijk%20and%20Nava%20Tintarev&entry.1292438233=%20%20An%20important%20aspect%20of%20summarizing%20videos%20is%20understanding%20the%20temporal%0Acontext%20behind%20each%20part%20of%20the%20video%20to%20grasp%20what%20is%20and%20is%20not%20important.%0AVideo%20summarization%20models%20have%20in%20recent%20years%20modeled%20spatio-temporal%0Arelationships%20to%20represent%20this%20information.%20These%20models%20achieved%0Astate-of-the-art%20correlation%20scores%20on%20important%20benchmark%20datasets.%20However%2C%0Awhat%20has%20not%20been%20reviewed%20is%20whether%20spatio-temporal%20relationships%20are%20even%0Arequired%20to%20achieve%20state-of-the-art%20results.%20Previous%20work%20in%20activity%0Arecognition%20has%20found%20biases%2C%20by%20prioritizing%20static%20cues%20such%20as%20scenes%20or%0Aobjects%2C%20over%20motion%20information.%20In%20this%20paper%20we%20inquire%20if%20similar%20spurious%0Arelationships%20might%20influence%20the%20task%20of%20video%20summarization.%20To%20do%20so%2C%20we%0Aanalyse%20the%20role%20that%20temporal%20information%20plays%20on%20existing%20benchmark%0Adatasets.%20We%20first%20estimate%20a%20baseline%20with%20temporally%20invariant%20models%20to%20see%0Ahow%20well%20such%20models%20rank%20on%20benchmark%20datasets%20%28TVSum%20and%20SumMe%29.%20We%20then%0Adisrupt%20the%20temporal%20order%20of%20the%20videos%20to%20investigate%20the%20impact%20it%20has%20on%0Aexisting%20state-of-the-art%20models.%20One%20of%20our%20findings%20is%20that%20the%20temporally%0Ainvariant%20models%20achieve%20competitive%20correlation%20scores%20that%20are%20close%20to%20the%0Ahuman%20baselines%20on%20the%20TVSum%20dataset.%20We%20also%20demonstrate%20that%20existing%20models%0Aare%20not%20affected%20by%20temporal%20perturbations.%20Furthermore%2C%20with%20certain%0Adisruption%20strategies%20that%20shuffle%20fixed%20time%20segments%2C%20we%20can%20actually%20improve%0Atheir%20correlation%20scores.%20With%20these%20results%2C%20we%20find%20that%20spatio-temporal%0Arelationship%20play%20a%20minor%20role%20and%20we%20raise%20the%20question%20whether%20these%0Abenchmarks%20adequately%20model%20the%20task%20of%20video%20summarization.%20Code%20available%20at%3A%0Ahttps%3A//github.com/AashGan/TemporalPerturbSum%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03323v1&entry.124074799=Read"},
{"title": "Comparison of Reservoir Computing topologies using the Recurrent Kernel\n  approach", "author": "Giuseppe Alessio D'Inverno and Jonathan Dong", "abstract": "  Reservoir Computing (RC) has become popular in recent years thanks to its\nfast and efficient computational capabilities. Standard RC has been shown to be\nequivalent in the asymptotic limit to Recurrent Kernels, which helps in\nanalyzing its expressive power. However, many well-established RC paradigms,\nsuch as Leaky RC, Sparse RC, and Deep RC, are yet to be systematically analyzed\nin such a way. We define the Recurrent Kernel limit of all these RC topologies\nand conduct a convergence study for a wide range of activation functions and\nhyperparameters. Our findings provide new insights into various aspects of\nReservoir Computing. First, we demonstrate that there is an optimal sparsity\nlevel which grows with the reservoir size. Furthermore, our analysis suggests\nthat Deep RC should use reservoir layers of decreasing sizes. Finally, we\nperform a benchmark demonstrating the efficiency of Structured Reservoir\nComputing compared to vanilla and Sparse Reservoir Computing.\n", "link": "http://arxiv.org/abs/2401.14557v3", "date": "2024-10-04", "relevancy": 2.1213, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4272}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20Reservoir%20Computing%20topologies%20using%20the%20Recurrent%20Kernel%0A%20%20approach&body=Title%3A%20Comparison%20of%20Reservoir%20Computing%20topologies%20using%20the%20Recurrent%20Kernel%0A%20%20approach%0AAuthor%3A%20Giuseppe%20Alessio%20D%27Inverno%20and%20Jonathan%20Dong%0AAbstract%3A%20%20%20Reservoir%20Computing%20%28RC%29%20has%20become%20popular%20in%20recent%20years%20thanks%20to%20its%0Afast%20and%20efficient%20computational%20capabilities.%20Standard%20RC%20has%20been%20shown%20to%20be%0Aequivalent%20in%20the%20asymptotic%20limit%20to%20Recurrent%20Kernels%2C%20which%20helps%20in%0Aanalyzing%20its%20expressive%20power.%20However%2C%20many%20well-established%20RC%20paradigms%2C%0Asuch%20as%20Leaky%20RC%2C%20Sparse%20RC%2C%20and%20Deep%20RC%2C%20are%20yet%20to%20be%20systematically%20analyzed%0Ain%20such%20a%20way.%20We%20define%20the%20Recurrent%20Kernel%20limit%20of%20all%20these%20RC%20topologies%0Aand%20conduct%20a%20convergence%20study%20for%20a%20wide%20range%20of%20activation%20functions%20and%0Ahyperparameters.%20Our%20findings%20provide%20new%20insights%20into%20various%20aspects%20of%0AReservoir%20Computing.%20First%2C%20we%20demonstrate%20that%20there%20is%20an%20optimal%20sparsity%0Alevel%20which%20grows%20with%20the%20reservoir%20size.%20Furthermore%2C%20our%20analysis%20suggests%0Athat%20Deep%20RC%20should%20use%20reservoir%20layers%20of%20decreasing%20sizes.%20Finally%2C%20we%0Aperform%20a%20benchmark%20demonstrating%20the%20efficiency%20of%20Structured%20Reservoir%0AComputing%20compared%20to%20vanilla%20and%20Sparse%20Reservoir%20Computing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14557v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520Reservoir%2520Computing%2520topologies%2520using%2520the%2520Recurrent%2520Kernel%250A%2520%2520approach%26entry.906535625%3DGiuseppe%2520Alessio%2520D%2527Inverno%2520and%2520Jonathan%2520Dong%26entry.1292438233%3D%2520%2520Reservoir%2520Computing%2520%2528RC%2529%2520has%2520become%2520popular%2520in%2520recent%2520years%2520thanks%2520to%2520its%250Afast%2520and%2520efficient%2520computational%2520capabilities.%2520Standard%2520RC%2520has%2520been%2520shown%2520to%2520be%250Aequivalent%2520in%2520the%2520asymptotic%2520limit%2520to%2520Recurrent%2520Kernels%252C%2520which%2520helps%2520in%250Aanalyzing%2520its%2520expressive%2520power.%2520However%252C%2520many%2520well-established%2520RC%2520paradigms%252C%250Asuch%2520as%2520Leaky%2520RC%252C%2520Sparse%2520RC%252C%2520and%2520Deep%2520RC%252C%2520are%2520yet%2520to%2520be%2520systematically%2520analyzed%250Ain%2520such%2520a%2520way.%2520We%2520define%2520the%2520Recurrent%2520Kernel%2520limit%2520of%2520all%2520these%2520RC%2520topologies%250Aand%2520conduct%2520a%2520convergence%2520study%2520for%2520a%2520wide%2520range%2520of%2520activation%2520functions%2520and%250Ahyperparameters.%2520Our%2520findings%2520provide%2520new%2520insights%2520into%2520various%2520aspects%2520of%250AReservoir%2520Computing.%2520First%252C%2520we%2520demonstrate%2520that%2520there%2520is%2520an%2520optimal%2520sparsity%250Alevel%2520which%2520grows%2520with%2520the%2520reservoir%2520size.%2520Furthermore%252C%2520our%2520analysis%2520suggests%250Athat%2520Deep%2520RC%2520should%2520use%2520reservoir%2520layers%2520of%2520decreasing%2520sizes.%2520Finally%252C%2520we%250Aperform%2520a%2520benchmark%2520demonstrating%2520the%2520efficiency%2520of%2520Structured%2520Reservoir%250AComputing%2520compared%2520to%2520vanilla%2520and%2520Sparse%2520Reservoir%2520Computing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14557v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20Reservoir%20Computing%20topologies%20using%20the%20Recurrent%20Kernel%0A%20%20approach&entry.906535625=Giuseppe%20Alessio%20D%27Inverno%20and%20Jonathan%20Dong&entry.1292438233=%20%20Reservoir%20Computing%20%28RC%29%20has%20become%20popular%20in%20recent%20years%20thanks%20to%20its%0Afast%20and%20efficient%20computational%20capabilities.%20Standard%20RC%20has%20been%20shown%20to%20be%0Aequivalent%20in%20the%20asymptotic%20limit%20to%20Recurrent%20Kernels%2C%20which%20helps%20in%0Aanalyzing%20its%20expressive%20power.%20However%2C%20many%20well-established%20RC%20paradigms%2C%0Asuch%20as%20Leaky%20RC%2C%20Sparse%20RC%2C%20and%20Deep%20RC%2C%20are%20yet%20to%20be%20systematically%20analyzed%0Ain%20such%20a%20way.%20We%20define%20the%20Recurrent%20Kernel%20limit%20of%20all%20these%20RC%20topologies%0Aand%20conduct%20a%20convergence%20study%20for%20a%20wide%20range%20of%20activation%20functions%20and%0Ahyperparameters.%20Our%20findings%20provide%20new%20insights%20into%20various%20aspects%20of%0AReservoir%20Computing.%20First%2C%20we%20demonstrate%20that%20there%20is%20an%20optimal%20sparsity%0Alevel%20which%20grows%20with%20the%20reservoir%20size.%20Furthermore%2C%20our%20analysis%20suggests%0Athat%20Deep%20RC%20should%20use%20reservoir%20layers%20of%20decreasing%20sizes.%20Finally%2C%20we%0Aperform%20a%20benchmark%20demonstrating%20the%20efficiency%20of%20Structured%20Reservoir%0AComputing%20compared%20to%20vanilla%20and%20Sparse%20Reservoir%20Computing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14557v3&entry.124074799=Read"},
{"title": "Generative Semantic Communication for Text-to-Speech Synthesis", "author": "Jiahao Zheng and Jinke Ren and Peng Xu and Zhihao Yuan and Jie Xu and Fangxin Wang and Gui Gui and Shuguang Cui", "abstract": "  Semantic communication is a promising technology to improve communication\nefficiency by transmitting only the semantic information of the source data.\nHowever, traditional semantic communication methods primarily focus on data\nreconstruction tasks, which may not be efficient for emerging generative tasks\nsuch as text-to-speech (TTS) synthesis. To address this limitation, this paper\ndevelops a novel generative semantic communication framework for TTS synthesis,\nleveraging generative artificial intelligence technologies. Firstly, we utilize\na pre-trained large speech model called WavLM and the residual vector\nquantization method to construct two semantic knowledge bases (KBs) at the\ntransmitter and receiver, respectively. The KB at the transmitter enables\neffective semantic extraction, while the KB at the receiver facilitates\nlifelike speech synthesis. Then, we employ a transformer encoder and a\ndiffusion model to achieve efficient semantic coding without introducing\nsignificant communication overhead. Finally, numerical results demonstrate that\nour framework achieves much higher fidelity for the generated speech than four\nbaselines, in both cases with additive white Gaussian noise channel and\nRayleigh fading channel.\n", "link": "http://arxiv.org/abs/2410.03459v1", "date": "2024-10-04", "relevancy": 2.1131, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.568}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5255}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Semantic%20Communication%20for%20Text-to-Speech%20Synthesis&body=Title%3A%20Generative%20Semantic%20Communication%20for%20Text-to-Speech%20Synthesis%0AAuthor%3A%20Jiahao%20Zheng%20and%20Jinke%20Ren%20and%20Peng%20Xu%20and%20Zhihao%20Yuan%20and%20Jie%20Xu%20and%20Fangxin%20Wang%20and%20Gui%20Gui%20and%20Shuguang%20Cui%0AAbstract%3A%20%20%20Semantic%20communication%20is%20a%20promising%20technology%20to%20improve%20communication%0Aefficiency%20by%20transmitting%20only%20the%20semantic%20information%20of%20the%20source%20data.%0AHowever%2C%20traditional%20semantic%20communication%20methods%20primarily%20focus%20on%20data%0Areconstruction%20tasks%2C%20which%20may%20not%20be%20efficient%20for%20emerging%20generative%20tasks%0Asuch%20as%20text-to-speech%20%28TTS%29%20synthesis.%20To%20address%20this%20limitation%2C%20this%20paper%0Adevelops%20a%20novel%20generative%20semantic%20communication%20framework%20for%20TTS%20synthesis%2C%0Aleveraging%20generative%20artificial%20intelligence%20technologies.%20Firstly%2C%20we%20utilize%0Aa%20pre-trained%20large%20speech%20model%20called%20WavLM%20and%20the%20residual%20vector%0Aquantization%20method%20to%20construct%20two%20semantic%20knowledge%20bases%20%28KBs%29%20at%20the%0Atransmitter%20and%20receiver%2C%20respectively.%20The%20KB%20at%20the%20transmitter%20enables%0Aeffective%20semantic%20extraction%2C%20while%20the%20KB%20at%20the%20receiver%20facilitates%0Alifelike%20speech%20synthesis.%20Then%2C%20we%20employ%20a%20transformer%20encoder%20and%20a%0Adiffusion%20model%20to%20achieve%20efficient%20semantic%20coding%20without%20introducing%0Asignificant%20communication%20overhead.%20Finally%2C%20numerical%20results%20demonstrate%20that%0Aour%20framework%20achieves%20much%20higher%20fidelity%20for%20the%20generated%20speech%20than%20four%0Abaselines%2C%20in%20both%20cases%20with%20additive%20white%20Gaussian%20noise%20channel%20and%0ARayleigh%20fading%20channel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Semantic%2520Communication%2520for%2520Text-to-Speech%2520Synthesis%26entry.906535625%3DJiahao%2520Zheng%2520and%2520Jinke%2520Ren%2520and%2520Peng%2520Xu%2520and%2520Zhihao%2520Yuan%2520and%2520Jie%2520Xu%2520and%2520Fangxin%2520Wang%2520and%2520Gui%2520Gui%2520and%2520Shuguang%2520Cui%26entry.1292438233%3D%2520%2520Semantic%2520communication%2520is%2520a%2520promising%2520technology%2520to%2520improve%2520communication%250Aefficiency%2520by%2520transmitting%2520only%2520the%2520semantic%2520information%2520of%2520the%2520source%2520data.%250AHowever%252C%2520traditional%2520semantic%2520communication%2520methods%2520primarily%2520focus%2520on%2520data%250Areconstruction%2520tasks%252C%2520which%2520may%2520not%2520be%2520efficient%2520for%2520emerging%2520generative%2520tasks%250Asuch%2520as%2520text-to-speech%2520%2528TTS%2529%2520synthesis.%2520To%2520address%2520this%2520limitation%252C%2520this%2520paper%250Adevelops%2520a%2520novel%2520generative%2520semantic%2520communication%2520framework%2520for%2520TTS%2520synthesis%252C%250Aleveraging%2520generative%2520artificial%2520intelligence%2520technologies.%2520Firstly%252C%2520we%2520utilize%250Aa%2520pre-trained%2520large%2520speech%2520model%2520called%2520WavLM%2520and%2520the%2520residual%2520vector%250Aquantization%2520method%2520to%2520construct%2520two%2520semantic%2520knowledge%2520bases%2520%2528KBs%2529%2520at%2520the%250Atransmitter%2520and%2520receiver%252C%2520respectively.%2520The%2520KB%2520at%2520the%2520transmitter%2520enables%250Aeffective%2520semantic%2520extraction%252C%2520while%2520the%2520KB%2520at%2520the%2520receiver%2520facilitates%250Alifelike%2520speech%2520synthesis.%2520Then%252C%2520we%2520employ%2520a%2520transformer%2520encoder%2520and%2520a%250Adiffusion%2520model%2520to%2520achieve%2520efficient%2520semantic%2520coding%2520without%2520introducing%250Asignificant%2520communication%2520overhead.%2520Finally%252C%2520numerical%2520results%2520demonstrate%2520that%250Aour%2520framework%2520achieves%2520much%2520higher%2520fidelity%2520for%2520the%2520generated%2520speech%2520than%2520four%250Abaselines%252C%2520in%2520both%2520cases%2520with%2520additive%2520white%2520Gaussian%2520noise%2520channel%2520and%250ARayleigh%2520fading%2520channel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Semantic%20Communication%20for%20Text-to-Speech%20Synthesis&entry.906535625=Jiahao%20Zheng%20and%20Jinke%20Ren%20and%20Peng%20Xu%20and%20Zhihao%20Yuan%20and%20Jie%20Xu%20and%20Fangxin%20Wang%20and%20Gui%20Gui%20and%20Shuguang%20Cui&entry.1292438233=%20%20Semantic%20communication%20is%20a%20promising%20technology%20to%20improve%20communication%0Aefficiency%20by%20transmitting%20only%20the%20semantic%20information%20of%20the%20source%20data.%0AHowever%2C%20traditional%20semantic%20communication%20methods%20primarily%20focus%20on%20data%0Areconstruction%20tasks%2C%20which%20may%20not%20be%20efficient%20for%20emerging%20generative%20tasks%0Asuch%20as%20text-to-speech%20%28TTS%29%20synthesis.%20To%20address%20this%20limitation%2C%20this%20paper%0Adevelops%20a%20novel%20generative%20semantic%20communication%20framework%20for%20TTS%20synthesis%2C%0Aleveraging%20generative%20artificial%20intelligence%20technologies.%20Firstly%2C%20we%20utilize%0Aa%20pre-trained%20large%20speech%20model%20called%20WavLM%20and%20the%20residual%20vector%0Aquantization%20method%20to%20construct%20two%20semantic%20knowledge%20bases%20%28KBs%29%20at%20the%0Atransmitter%20and%20receiver%2C%20respectively.%20The%20KB%20at%20the%20transmitter%20enables%0Aeffective%20semantic%20extraction%2C%20while%20the%20KB%20at%20the%20receiver%20facilitates%0Alifelike%20speech%20synthesis.%20Then%2C%20we%20employ%20a%20transformer%20encoder%20and%20a%0Adiffusion%20model%20to%20achieve%20efficient%20semantic%20coding%20without%20introducing%0Asignificant%20communication%20overhead.%20Finally%2C%20numerical%20results%20demonstrate%20that%0Aour%20framework%20achieves%20much%20higher%20fidelity%20for%20the%20generated%20speech%20than%20four%0Abaselines%2C%20in%20both%20cases%20with%20additive%20white%20Gaussian%20noise%20channel%20and%0ARayleigh%20fading%20channel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03459v1&entry.124074799=Read"},
{"title": "Selective Test-Time Adaptation for Unsupervised Anomaly Detection using\n  Neural Implicit Representations", "author": "Sameer Ambekar and Julia A. Schnabel and Cosmin Bereca", "abstract": "  Deep learning models in medical imaging often encounter challenges when\nadapting to new clinical settings unseen during training. Test-time adaptation\noffers a promising approach to optimize models for these unseen domains, yet\nits application in anomaly detection (AD) remains largely unexplored. AD aims\nto efficiently identify deviations from normative distributions; however, full\nadaptation, including pathological shifts, may inadvertently learn the\nanomalies it intends to detect. We introduce a novel concept of\n\\emph{selective} test-time adaptation that utilizes the inherent\ncharacteristics of deep pre-trained features to adapt \\emph{selectively} in a\nzero-shot manner to any test image from an unseen domain. This approach employs\na model-agnostic, lightweight multi-layer perceptron for neural implicit\nrepresentations, enabling the adaptation of outputs from any\nreconstruction-based AD method without altering the source-trained model.\nRigorous validation in brain AD demonstrated that our strategy substantially\nenhances detection accuracy for multiple conditions and different target\ndistributions. Specifically, our method improves the detection rates by up to\n78\\% for enlarged ventricles and 24\\% for edemas.\n", "link": "http://arxiv.org/abs/2410.03306v1", "date": "2024-10-04", "relevancy": 2.1106, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5324}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5324}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Test-Time%20Adaptation%20for%20Unsupervised%20Anomaly%20Detection%20using%0A%20%20Neural%20Implicit%20Representations&body=Title%3A%20Selective%20Test-Time%20Adaptation%20for%20Unsupervised%20Anomaly%20Detection%20using%0A%20%20Neural%20Implicit%20Representations%0AAuthor%3A%20Sameer%20Ambekar%20and%20Julia%20A.%20Schnabel%20and%20Cosmin%20Bereca%0AAbstract%3A%20%20%20Deep%20learning%20models%20in%20medical%20imaging%20often%20encounter%20challenges%20when%0Aadapting%20to%20new%20clinical%20settings%20unseen%20during%20training.%20Test-time%20adaptation%0Aoffers%20a%20promising%20approach%20to%20optimize%20models%20for%20these%20unseen%20domains%2C%20yet%0Aits%20application%20in%20anomaly%20detection%20%28AD%29%20remains%20largely%20unexplored.%20AD%20aims%0Ato%20efficiently%20identify%20deviations%20from%20normative%20distributions%3B%20however%2C%20full%0Aadaptation%2C%20including%20pathological%20shifts%2C%20may%20inadvertently%20learn%20the%0Aanomalies%20it%20intends%20to%20detect.%20We%20introduce%20a%20novel%20concept%20of%0A%5Cemph%7Bselective%7D%20test-time%20adaptation%20that%20utilizes%20the%20inherent%0Acharacteristics%20of%20deep%20pre-trained%20features%20to%20adapt%20%5Cemph%7Bselectively%7D%20in%20a%0Azero-shot%20manner%20to%20any%20test%20image%20from%20an%20unseen%20domain.%20This%20approach%20employs%0Aa%20model-agnostic%2C%20lightweight%20multi-layer%20perceptron%20for%20neural%20implicit%0Arepresentations%2C%20enabling%20the%20adaptation%20of%20outputs%20from%20any%0Areconstruction-based%20AD%20method%20without%20altering%20the%20source-trained%20model.%0ARigorous%20validation%20in%20brain%20AD%20demonstrated%20that%20our%20strategy%20substantially%0Aenhances%20detection%20accuracy%20for%20multiple%20conditions%20and%20different%20target%0Adistributions.%20Specifically%2C%20our%20method%20improves%20the%20detection%20rates%20by%20up%20to%0A78%5C%25%20for%20enlarged%20ventricles%20and%2024%5C%25%20for%20edemas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Test-Time%2520Adaptation%2520for%2520Unsupervised%2520Anomaly%2520Detection%2520using%250A%2520%2520Neural%2520Implicit%2520Representations%26entry.906535625%3DSameer%2520Ambekar%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Cosmin%2520Bereca%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520in%2520medical%2520imaging%2520often%2520encounter%2520challenges%2520when%250Aadapting%2520to%2520new%2520clinical%2520settings%2520unseen%2520during%2520training.%2520Test-time%2520adaptation%250Aoffers%2520a%2520promising%2520approach%2520to%2520optimize%2520models%2520for%2520these%2520unseen%2520domains%252C%2520yet%250Aits%2520application%2520in%2520anomaly%2520detection%2520%2528AD%2529%2520remains%2520largely%2520unexplored.%2520AD%2520aims%250Ato%2520efficiently%2520identify%2520deviations%2520from%2520normative%2520distributions%253B%2520however%252C%2520full%250Aadaptation%252C%2520including%2520pathological%2520shifts%252C%2520may%2520inadvertently%2520learn%2520the%250Aanomalies%2520it%2520intends%2520to%2520detect.%2520We%2520introduce%2520a%2520novel%2520concept%2520of%250A%255Cemph%257Bselective%257D%2520test-time%2520adaptation%2520that%2520utilizes%2520the%2520inherent%250Acharacteristics%2520of%2520deep%2520pre-trained%2520features%2520to%2520adapt%2520%255Cemph%257Bselectively%257D%2520in%2520a%250Azero-shot%2520manner%2520to%2520any%2520test%2520image%2520from%2520an%2520unseen%2520domain.%2520This%2520approach%2520employs%250Aa%2520model-agnostic%252C%2520lightweight%2520multi-layer%2520perceptron%2520for%2520neural%2520implicit%250Arepresentations%252C%2520enabling%2520the%2520adaptation%2520of%2520outputs%2520from%2520any%250Areconstruction-based%2520AD%2520method%2520without%2520altering%2520the%2520source-trained%2520model.%250ARigorous%2520validation%2520in%2520brain%2520AD%2520demonstrated%2520that%2520our%2520strategy%2520substantially%250Aenhances%2520detection%2520accuracy%2520for%2520multiple%2520conditions%2520and%2520different%2520target%250Adistributions.%2520Specifically%252C%2520our%2520method%2520improves%2520the%2520detection%2520rates%2520by%2520up%2520to%250A78%255C%2525%2520for%2520enlarged%2520ventricles%2520and%252024%255C%2525%2520for%2520edemas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Test-Time%20Adaptation%20for%20Unsupervised%20Anomaly%20Detection%20using%0A%20%20Neural%20Implicit%20Representations&entry.906535625=Sameer%20Ambekar%20and%20Julia%20A.%20Schnabel%20and%20Cosmin%20Bereca&entry.1292438233=%20%20Deep%20learning%20models%20in%20medical%20imaging%20often%20encounter%20challenges%20when%0Aadapting%20to%20new%20clinical%20settings%20unseen%20during%20training.%20Test-time%20adaptation%0Aoffers%20a%20promising%20approach%20to%20optimize%20models%20for%20these%20unseen%20domains%2C%20yet%0Aits%20application%20in%20anomaly%20detection%20%28AD%29%20remains%20largely%20unexplored.%20AD%20aims%0Ato%20efficiently%20identify%20deviations%20from%20normative%20distributions%3B%20however%2C%20full%0Aadaptation%2C%20including%20pathological%20shifts%2C%20may%20inadvertently%20learn%20the%0Aanomalies%20it%20intends%20to%20detect.%20We%20introduce%20a%20novel%20concept%20of%0A%5Cemph%7Bselective%7D%20test-time%20adaptation%20that%20utilizes%20the%20inherent%0Acharacteristics%20of%20deep%20pre-trained%20features%20to%20adapt%20%5Cemph%7Bselectively%7D%20in%20a%0Azero-shot%20manner%20to%20any%20test%20image%20from%20an%20unseen%20domain.%20This%20approach%20employs%0Aa%20model-agnostic%2C%20lightweight%20multi-layer%20perceptron%20for%20neural%20implicit%0Arepresentations%2C%20enabling%20the%20adaptation%20of%20outputs%20from%20any%0Areconstruction-based%20AD%20method%20without%20altering%20the%20source-trained%20model.%0ARigorous%20validation%20in%20brain%20AD%20demonstrated%20that%20our%20strategy%20substantially%0Aenhances%20detection%20accuracy%20for%20multiple%20conditions%20and%20different%20target%0Adistributions.%20Specifically%2C%20our%20method%20improves%20the%20detection%20rates%20by%20up%20to%0A78%5C%25%20for%20enlarged%20ventricles%20and%2024%5C%25%20for%20edemas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03306v1&entry.124074799=Read"},
{"title": "Navigable Graphs for High-Dimensional Nearest Neighbor Search:\n  Constructions and Limits", "author": "Haya Diwan and Jinrui Gou and Cameron Musco and Christopher Musco and Torsten Suel", "abstract": "  There has been significant recent interest in graph-based nearest neighbor\nsearch methods, many of which are centered on the construction of navigable\ngraphs over high-dimensional point sets. A graph is navigable if we can\nsuccessfully move from any starting node to any target node using a greedy\nrouting strategy where we always move to the neighbor that is closest to the\ndestination according to a given distance function. The complete graph is\nnavigable for any point set, but the important question for applications is if\nsparser graphs can be constructed. While this question is fairly well\nunderstood in low-dimensions, we establish some of the first upper and lower\nbounds for high-dimensional point sets. First, we give a simple and efficient\nway to construct a navigable graph with average degree $O(\\sqrt{n \\log n })$\nfor any set of $n$ points, in any dimension, for any distance function. We\ncompliment this result with a nearly matching lower bound: even under the\nEuclidean metric in $O(\\log n)$ dimensions, a random point set has no navigable\ngraph with average degree $O(n^{\\alpha})$ for any $\\alpha < 1/2$. Our lower\nbound relies on sharp anti-concentration bounds for binomial random variables,\nwhich we use to show that the near-neighborhoods of a set of random points do\nnot overlap significantly, forcing any navigable graph to have many edges.\n", "link": "http://arxiv.org/abs/2405.18680v2", "date": "2024-10-04", "relevancy": 2.0988, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4235}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4194}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigable%20Graphs%20for%20High-Dimensional%20Nearest%20Neighbor%20Search%3A%0A%20%20Constructions%20and%20Limits&body=Title%3A%20Navigable%20Graphs%20for%20High-Dimensional%20Nearest%20Neighbor%20Search%3A%0A%20%20Constructions%20and%20Limits%0AAuthor%3A%20Haya%20Diwan%20and%20Jinrui%20Gou%20and%20Cameron%20Musco%20and%20Christopher%20Musco%20and%20Torsten%20Suel%0AAbstract%3A%20%20%20There%20has%20been%20significant%20recent%20interest%20in%20graph-based%20nearest%20neighbor%0Asearch%20methods%2C%20many%20of%20which%20are%20centered%20on%20the%20construction%20of%20navigable%0Agraphs%20over%20high-dimensional%20point%20sets.%20A%20graph%20is%20navigable%20if%20we%20can%0Asuccessfully%20move%20from%20any%20starting%20node%20to%20any%20target%20node%20using%20a%20greedy%0Arouting%20strategy%20where%20we%20always%20move%20to%20the%20neighbor%20that%20is%20closest%20to%20the%0Adestination%20according%20to%20a%20given%20distance%20function.%20The%20complete%20graph%20is%0Anavigable%20for%20any%20point%20set%2C%20but%20the%20important%20question%20for%20applications%20is%20if%0Asparser%20graphs%20can%20be%20constructed.%20While%20this%20question%20is%20fairly%20well%0Aunderstood%20in%20low-dimensions%2C%20we%20establish%20some%20of%20the%20first%20upper%20and%20lower%0Abounds%20for%20high-dimensional%20point%20sets.%20First%2C%20we%20give%20a%20simple%20and%20efficient%0Away%20to%20construct%20a%20navigable%20graph%20with%20average%20degree%20%24O%28%5Csqrt%7Bn%20%5Clog%20n%20%7D%29%24%0Afor%20any%20set%20of%20%24n%24%20points%2C%20in%20any%20dimension%2C%20for%20any%20distance%20function.%20We%0Acompliment%20this%20result%20with%20a%20nearly%20matching%20lower%20bound%3A%20even%20under%20the%0AEuclidean%20metric%20in%20%24O%28%5Clog%20n%29%24%20dimensions%2C%20a%20random%20point%20set%20has%20no%20navigable%0Agraph%20with%20average%20degree%20%24O%28n%5E%7B%5Calpha%7D%29%24%20for%20any%20%24%5Calpha%20%3C%201/2%24.%20Our%20lower%0Abound%20relies%20on%20sharp%20anti-concentration%20bounds%20for%20binomial%20random%20variables%2C%0Awhich%20we%20use%20to%20show%20that%20the%20near-neighborhoods%20of%20a%20set%20of%20random%20points%20do%0Anot%20overlap%20significantly%2C%20forcing%20any%20navigable%20graph%20to%20have%20many%20edges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18680v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigable%2520Graphs%2520for%2520High-Dimensional%2520Nearest%2520Neighbor%2520Search%253A%250A%2520%2520Constructions%2520and%2520Limits%26entry.906535625%3DHaya%2520Diwan%2520and%2520Jinrui%2520Gou%2520and%2520Cameron%2520Musco%2520and%2520Christopher%2520Musco%2520and%2520Torsten%2520Suel%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520significant%2520recent%2520interest%2520in%2520graph-based%2520nearest%2520neighbor%250Asearch%2520methods%252C%2520many%2520of%2520which%2520are%2520centered%2520on%2520the%2520construction%2520of%2520navigable%250Agraphs%2520over%2520high-dimensional%2520point%2520sets.%2520A%2520graph%2520is%2520navigable%2520if%2520we%2520can%250Asuccessfully%2520move%2520from%2520any%2520starting%2520node%2520to%2520any%2520target%2520node%2520using%2520a%2520greedy%250Arouting%2520strategy%2520where%2520we%2520always%2520move%2520to%2520the%2520neighbor%2520that%2520is%2520closest%2520to%2520the%250Adestination%2520according%2520to%2520a%2520given%2520distance%2520function.%2520The%2520complete%2520graph%2520is%250Anavigable%2520for%2520any%2520point%2520set%252C%2520but%2520the%2520important%2520question%2520for%2520applications%2520is%2520if%250Asparser%2520graphs%2520can%2520be%2520constructed.%2520While%2520this%2520question%2520is%2520fairly%2520well%250Aunderstood%2520in%2520low-dimensions%252C%2520we%2520establish%2520some%2520of%2520the%2520first%2520upper%2520and%2520lower%250Abounds%2520for%2520high-dimensional%2520point%2520sets.%2520First%252C%2520we%2520give%2520a%2520simple%2520and%2520efficient%250Away%2520to%2520construct%2520a%2520navigable%2520graph%2520with%2520average%2520degree%2520%2524O%2528%255Csqrt%257Bn%2520%255Clog%2520n%2520%257D%2529%2524%250Afor%2520any%2520set%2520of%2520%2524n%2524%2520points%252C%2520in%2520any%2520dimension%252C%2520for%2520any%2520distance%2520function.%2520We%250Acompliment%2520this%2520result%2520with%2520a%2520nearly%2520matching%2520lower%2520bound%253A%2520even%2520under%2520the%250AEuclidean%2520metric%2520in%2520%2524O%2528%255Clog%2520n%2529%2524%2520dimensions%252C%2520a%2520random%2520point%2520set%2520has%2520no%2520navigable%250Agraph%2520with%2520average%2520degree%2520%2524O%2528n%255E%257B%255Calpha%257D%2529%2524%2520for%2520any%2520%2524%255Calpha%2520%253C%25201/2%2524.%2520Our%2520lower%250Abound%2520relies%2520on%2520sharp%2520anti-concentration%2520bounds%2520for%2520binomial%2520random%2520variables%252C%250Awhich%2520we%2520use%2520to%2520show%2520that%2520the%2520near-neighborhoods%2520of%2520a%2520set%2520of%2520random%2520points%2520do%250Anot%2520overlap%2520significantly%252C%2520forcing%2520any%2520navigable%2520graph%2520to%2520have%2520many%2520edges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18680v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigable%20Graphs%20for%20High-Dimensional%20Nearest%20Neighbor%20Search%3A%0A%20%20Constructions%20and%20Limits&entry.906535625=Haya%20Diwan%20and%20Jinrui%20Gou%20and%20Cameron%20Musco%20and%20Christopher%20Musco%20and%20Torsten%20Suel&entry.1292438233=%20%20There%20has%20been%20significant%20recent%20interest%20in%20graph-based%20nearest%20neighbor%0Asearch%20methods%2C%20many%20of%20which%20are%20centered%20on%20the%20construction%20of%20navigable%0Agraphs%20over%20high-dimensional%20point%20sets.%20A%20graph%20is%20navigable%20if%20we%20can%0Asuccessfully%20move%20from%20any%20starting%20node%20to%20any%20target%20node%20using%20a%20greedy%0Arouting%20strategy%20where%20we%20always%20move%20to%20the%20neighbor%20that%20is%20closest%20to%20the%0Adestination%20according%20to%20a%20given%20distance%20function.%20The%20complete%20graph%20is%0Anavigable%20for%20any%20point%20set%2C%20but%20the%20important%20question%20for%20applications%20is%20if%0Asparser%20graphs%20can%20be%20constructed.%20While%20this%20question%20is%20fairly%20well%0Aunderstood%20in%20low-dimensions%2C%20we%20establish%20some%20of%20the%20first%20upper%20and%20lower%0Abounds%20for%20high-dimensional%20point%20sets.%20First%2C%20we%20give%20a%20simple%20and%20efficient%0Away%20to%20construct%20a%20navigable%20graph%20with%20average%20degree%20%24O%28%5Csqrt%7Bn%20%5Clog%20n%20%7D%29%24%0Afor%20any%20set%20of%20%24n%24%20points%2C%20in%20any%20dimension%2C%20for%20any%20distance%20function.%20We%0Acompliment%20this%20result%20with%20a%20nearly%20matching%20lower%20bound%3A%20even%20under%20the%0AEuclidean%20metric%20in%20%24O%28%5Clog%20n%29%24%20dimensions%2C%20a%20random%20point%20set%20has%20no%20navigable%0Agraph%20with%20average%20degree%20%24O%28n%5E%7B%5Calpha%7D%29%24%20for%20any%20%24%5Calpha%20%3C%201/2%24.%20Our%20lower%0Abound%20relies%20on%20sharp%20anti-concentration%20bounds%20for%20binomial%20random%20variables%2C%0Awhich%20we%20use%20to%20show%20that%20the%20near-neighborhoods%20of%20a%20set%20of%20random%20points%20do%0Anot%20overlap%20significantly%2C%20forcing%20any%20navigable%20graph%20to%20have%20many%20edges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18680v2&entry.124074799=Read"},
{"title": "Conformal confidence sets for biomedical image segmentation", "author": "Samuel Davenport", "abstract": "  We develop confidence sets which provide spatial uncertainty guarantees for\nthe output of a black-box machine learning model designed for image\nsegmentation. To do so we adapt conformal inference to the imaging setting,\nobtaining thresholds on a calibration dataset based on the distribution of the\nmaximum of the transformed logit scores within and outside of the ground truth\nmasks. We prove that these confidence sets, when applied to new predictions of\nthe model, are guaranteed to contain the true unknown segmented mask with\ndesired probability. We show that learning appropriate score transformations on\na learning dataset before performing calibration is crucial for optimizing\nperformance. We illustrate and validate our approach on a polpys tumor dataset.\nTo do so we obtain the logit scores from a deep neural network trained for\npolpys segmentation and show that using distance transformed scores to obtain\nouter confidence sets and the original scores for inner confidence sets enables\ntight bounds on tumor location whilst controlling the false coverage rate.\n", "link": "http://arxiv.org/abs/2410.03406v1", "date": "2024-10-04", "relevancy": 2.0923, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5471}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5238}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20confidence%20sets%20for%20biomedical%20image%20segmentation&body=Title%3A%20Conformal%20confidence%20sets%20for%20biomedical%20image%20segmentation%0AAuthor%3A%20Samuel%20Davenport%0AAbstract%3A%20%20%20We%20develop%20confidence%20sets%20which%20provide%20spatial%20uncertainty%20guarantees%20for%0Athe%20output%20of%20a%20black-box%20machine%20learning%20model%20designed%20for%20image%0Asegmentation.%20To%20do%20so%20we%20adapt%20conformal%20inference%20to%20the%20imaging%20setting%2C%0Aobtaining%20thresholds%20on%20a%20calibration%20dataset%20based%20on%20the%20distribution%20of%20the%0Amaximum%20of%20the%20transformed%20logit%20scores%20within%20and%20outside%20of%20the%20ground%20truth%0Amasks.%20We%20prove%20that%20these%20confidence%20sets%2C%20when%20applied%20to%20new%20predictions%20of%0Athe%20model%2C%20are%20guaranteed%20to%20contain%20the%20true%20unknown%20segmented%20mask%20with%0Adesired%20probability.%20We%20show%20that%20learning%20appropriate%20score%20transformations%20on%0Aa%20learning%20dataset%20before%20performing%20calibration%20is%20crucial%20for%20optimizing%0Aperformance.%20We%20illustrate%20and%20validate%20our%20approach%20on%20a%20polpys%20tumor%20dataset.%0ATo%20do%20so%20we%20obtain%20the%20logit%20scores%20from%20a%20deep%20neural%20network%20trained%20for%0Apolpys%20segmentation%20and%20show%20that%20using%20distance%20transformed%20scores%20to%20obtain%0Aouter%20confidence%20sets%20and%20the%20original%20scores%20for%20inner%20confidence%20sets%20enables%0Atight%20bounds%20on%20tumor%20location%20whilst%20controlling%20the%20false%20coverage%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520confidence%2520sets%2520for%2520biomedical%2520image%2520segmentation%26entry.906535625%3DSamuel%2520Davenport%26entry.1292438233%3D%2520%2520We%2520develop%2520confidence%2520sets%2520which%2520provide%2520spatial%2520uncertainty%2520guarantees%2520for%250Athe%2520output%2520of%2520a%2520black-box%2520machine%2520learning%2520model%2520designed%2520for%2520image%250Asegmentation.%2520To%2520do%2520so%2520we%2520adapt%2520conformal%2520inference%2520to%2520the%2520imaging%2520setting%252C%250Aobtaining%2520thresholds%2520on%2520a%2520calibration%2520dataset%2520based%2520on%2520the%2520distribution%2520of%2520the%250Amaximum%2520of%2520the%2520transformed%2520logit%2520scores%2520within%2520and%2520outside%2520of%2520the%2520ground%2520truth%250Amasks.%2520We%2520prove%2520that%2520these%2520confidence%2520sets%252C%2520when%2520applied%2520to%2520new%2520predictions%2520of%250Athe%2520model%252C%2520are%2520guaranteed%2520to%2520contain%2520the%2520true%2520unknown%2520segmented%2520mask%2520with%250Adesired%2520probability.%2520We%2520show%2520that%2520learning%2520appropriate%2520score%2520transformations%2520on%250Aa%2520learning%2520dataset%2520before%2520performing%2520calibration%2520is%2520crucial%2520for%2520optimizing%250Aperformance.%2520We%2520illustrate%2520and%2520validate%2520our%2520approach%2520on%2520a%2520polpys%2520tumor%2520dataset.%250ATo%2520do%2520so%2520we%2520obtain%2520the%2520logit%2520scores%2520from%2520a%2520deep%2520neural%2520network%2520trained%2520for%250Apolpys%2520segmentation%2520and%2520show%2520that%2520using%2520distance%2520transformed%2520scores%2520to%2520obtain%250Aouter%2520confidence%2520sets%2520and%2520the%2520original%2520scores%2520for%2520inner%2520confidence%2520sets%2520enables%250Atight%2520bounds%2520on%2520tumor%2520location%2520whilst%2520controlling%2520the%2520false%2520coverage%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20confidence%20sets%20for%20biomedical%20image%20segmentation&entry.906535625=Samuel%20Davenport&entry.1292438233=%20%20We%20develop%20confidence%20sets%20which%20provide%20spatial%20uncertainty%20guarantees%20for%0Athe%20output%20of%20a%20black-box%20machine%20learning%20model%20designed%20for%20image%0Asegmentation.%20To%20do%20so%20we%20adapt%20conformal%20inference%20to%20the%20imaging%20setting%2C%0Aobtaining%20thresholds%20on%20a%20calibration%20dataset%20based%20on%20the%20distribution%20of%20the%0Amaximum%20of%20the%20transformed%20logit%20scores%20within%20and%20outside%20of%20the%20ground%20truth%0Amasks.%20We%20prove%20that%20these%20confidence%20sets%2C%20when%20applied%20to%20new%20predictions%20of%0Athe%20model%2C%20are%20guaranteed%20to%20contain%20the%20true%20unknown%20segmented%20mask%20with%0Adesired%20probability.%20We%20show%20that%20learning%20appropriate%20score%20transformations%20on%0Aa%20learning%20dataset%20before%20performing%20calibration%20is%20crucial%20for%20optimizing%0Aperformance.%20We%20illustrate%20and%20validate%20our%20approach%20on%20a%20polpys%20tumor%20dataset.%0ATo%20do%20so%20we%20obtain%20the%20logit%20scores%20from%20a%20deep%20neural%20network%20trained%20for%0Apolpys%20segmentation%20and%20show%20that%20using%20distance%20transformed%20scores%20to%20obtain%0Aouter%20confidence%20sets%20and%20the%20original%20scores%20for%20inner%20confidence%20sets%20enables%0Atight%20bounds%20on%20tumor%20location%20whilst%20controlling%20the%20false%20coverage%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03406v1&entry.124074799=Read"},
{"title": "Applying the Lower-Biased Teacher Model in Semi-Supervised Object\n  Detection", "author": "Shuang Wang", "abstract": "  I present the Lower Biased Teacher model, an enhancement of the Unbiased\nTeacher model, specifically tailored for semi-supervised object detection\ntasks. The primary innovation of this model is the integration of a\nlocalization loss into the teacher model, which significantly improves the\naccuracy of pseudo-label generation. By addressing key issues such as class\nimbalance and the precision of bounding boxes, the Lower Biased Teacher model\ndemonstrates superior performance in object detection tasks. Extensive\nexperiments on multiple semi-supervised object detection datasets show that the\nLower Biased Teacher model not only reduces the pseudo-labeling bias caused by\nclass imbalances but also mitigates errors arising from incorrect bounding\nboxes. As a result, the model achieves higher mAP scores and more reliable\ndetection outcomes compared to existing methods. This research underscores the\nimportance of accurate pseudo-label generation and provides a robust framework\nfor future advancements in semi-supervised learning for object detection.\n", "link": "http://arxiv.org/abs/2409.19703v2", "date": "2024-10-04", "relevancy": 2.0844, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5448}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5242}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20the%20Lower-Biased%20Teacher%20Model%20in%20Semi-Supervised%20Object%0A%20%20Detection&body=Title%3A%20Applying%20the%20Lower-Biased%20Teacher%20Model%20in%20Semi-Supervised%20Object%0A%20%20Detection%0AAuthor%3A%20Shuang%20Wang%0AAbstract%3A%20%20%20I%20present%20the%20Lower%20Biased%20Teacher%20model%2C%20an%20enhancement%20of%20the%20Unbiased%0ATeacher%20model%2C%20specifically%20tailored%20for%20semi-supervised%20object%20detection%0Atasks.%20The%20primary%20innovation%20of%20this%20model%20is%20the%20integration%20of%20a%0Alocalization%20loss%20into%20the%20teacher%20model%2C%20which%20significantly%20improves%20the%0Aaccuracy%20of%20pseudo-label%20generation.%20By%20addressing%20key%20issues%20such%20as%20class%0Aimbalance%20and%20the%20precision%20of%20bounding%20boxes%2C%20the%20Lower%20Biased%20Teacher%20model%0Ademonstrates%20superior%20performance%20in%20object%20detection%20tasks.%20Extensive%0Aexperiments%20on%20multiple%20semi-supervised%20object%20detection%20datasets%20show%20that%20the%0ALower%20Biased%20Teacher%20model%20not%20only%20reduces%20the%20pseudo-labeling%20bias%20caused%20by%0Aclass%20imbalances%20but%20also%20mitigates%20errors%20arising%20from%20incorrect%20bounding%0Aboxes.%20As%20a%20result%2C%20the%20model%20achieves%20higher%20mAP%20scores%20and%20more%20reliable%0Adetection%20outcomes%20compared%20to%20existing%20methods.%20This%20research%20underscores%20the%0Aimportance%20of%20accurate%20pseudo-label%20generation%20and%20provides%20a%20robust%20framework%0Afor%20future%20advancements%20in%20semi-supervised%20learning%20for%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520the%2520Lower-Biased%2520Teacher%2520Model%2520in%2520Semi-Supervised%2520Object%250A%2520%2520Detection%26entry.906535625%3DShuang%2520Wang%26entry.1292438233%3D%2520%2520I%2520present%2520the%2520Lower%2520Biased%2520Teacher%2520model%252C%2520an%2520enhancement%2520of%2520the%2520Unbiased%250ATeacher%2520model%252C%2520specifically%2520tailored%2520for%2520semi-supervised%2520object%2520detection%250Atasks.%2520The%2520primary%2520innovation%2520of%2520this%2520model%2520is%2520the%2520integration%2520of%2520a%250Alocalization%2520loss%2520into%2520the%2520teacher%2520model%252C%2520which%2520significantly%2520improves%2520the%250Aaccuracy%2520of%2520pseudo-label%2520generation.%2520By%2520addressing%2520key%2520issues%2520such%2520as%2520class%250Aimbalance%2520and%2520the%2520precision%2520of%2520bounding%2520boxes%252C%2520the%2520Lower%2520Biased%2520Teacher%2520model%250Ademonstrates%2520superior%2520performance%2520in%2520object%2520detection%2520tasks.%2520Extensive%250Aexperiments%2520on%2520multiple%2520semi-supervised%2520object%2520detection%2520datasets%2520show%2520that%2520the%250ALower%2520Biased%2520Teacher%2520model%2520not%2520only%2520reduces%2520the%2520pseudo-labeling%2520bias%2520caused%2520by%250Aclass%2520imbalances%2520but%2520also%2520mitigates%2520errors%2520arising%2520from%2520incorrect%2520bounding%250Aboxes.%2520As%2520a%2520result%252C%2520the%2520model%2520achieves%2520higher%2520mAP%2520scores%2520and%2520more%2520reliable%250Adetection%2520outcomes%2520compared%2520to%2520existing%2520methods.%2520This%2520research%2520underscores%2520the%250Aimportance%2520of%2520accurate%2520pseudo-label%2520generation%2520and%2520provides%2520a%2520robust%2520framework%250Afor%2520future%2520advancements%2520in%2520semi-supervised%2520learning%2520for%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20the%20Lower-Biased%20Teacher%20Model%20in%20Semi-Supervised%20Object%0A%20%20Detection&entry.906535625=Shuang%20Wang&entry.1292438233=%20%20I%20present%20the%20Lower%20Biased%20Teacher%20model%2C%20an%20enhancement%20of%20the%20Unbiased%0ATeacher%20model%2C%20specifically%20tailored%20for%20semi-supervised%20object%20detection%0Atasks.%20The%20primary%20innovation%20of%20this%20model%20is%20the%20integration%20of%20a%0Alocalization%20loss%20into%20the%20teacher%20model%2C%20which%20significantly%20improves%20the%0Aaccuracy%20of%20pseudo-label%20generation.%20By%20addressing%20key%20issues%20such%20as%20class%0Aimbalance%20and%20the%20precision%20of%20bounding%20boxes%2C%20the%20Lower%20Biased%20Teacher%20model%0Ademonstrates%20superior%20performance%20in%20object%20detection%20tasks.%20Extensive%0Aexperiments%20on%20multiple%20semi-supervised%20object%20detection%20datasets%20show%20that%20the%0ALower%20Biased%20Teacher%20model%20not%20only%20reduces%20the%20pseudo-labeling%20bias%20caused%20by%0Aclass%20imbalances%20but%20also%20mitigates%20errors%20arising%20from%20incorrect%20bounding%0Aboxes.%20As%20a%20result%2C%20the%20model%20achieves%20higher%20mAP%20scores%20and%20more%20reliable%0Adetection%20outcomes%20compared%20to%20existing%20methods.%20This%20research%20underscores%20the%0Aimportance%20of%20accurate%20pseudo-label%20generation%20and%20provides%20a%20robust%20framework%0Afor%20future%20advancements%20in%20semi-supervised%20learning%20for%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19703v2&entry.124074799=Read"},
{"title": "Collaborative and Efficient Personalization with Mixtures of Adaptors", "author": "Abdulla Jasem Almansoori and Samuel Horv\u00e1th and Martin Tak\u00e1\u010d", "abstract": "  Non-iid data is prevalent in real-world federated learning problems. Data\nheterogeneity can come in different types in terms of distribution shifts. In\nthis work, we are interested in the heterogeneity that comes from concept\nshifts, i.e., shifts in the prediction across clients. In particular, we\nconsider multi-task learning, where we want the model to adapt to the task of\nthe client. We propose a parameter-efficient framework to tackle this issue,\nwhere each client learns to mix between parameter-efficient adaptors according\nto its task. We use Low-Rank Adaptors (LoRAs) as the backbone and extend its\nconcept to other types of layers. We call our framework Federated Low-Rank\nAdaptive Learning (FLoRAL). This framework is not an algorithm but rather a\nmodel parameterization for a multi-task learning objective, so it can work on\ntop of any algorithm that optimizes this objective, which includes many\nalgorithms from the literature. FLoRAL is memory-efficient, and clients are\npersonalized with small states (e.g., one number per adaptor) as the adaptors\nthemselves are federated. Hence, personalization is--in this sense--federated\nas well. Even though clients can personalize more freely by training an adaptor\nlocally, we show that collaborative and efficient training of adaptors is\npossible and performs better. We also show that FLoRAL can outperform an\nensemble of full models with optimal cluster assignment, which demonstrates the\nbenefits of federated personalization and the robustness of FLoRAL to\noverfitting. We show promising experimental results on synthetic datasets,\nreal-world federated multi-task problems such as MNIST, CIFAR-10, and\nCIFAR-100. We also provide a theoretical analysis of local SGD on a relaxed\nobjective and discuss the effects of aggregation mismatch on convergence.\n", "link": "http://arxiv.org/abs/2410.03497v1", "date": "2024-10-04", "relevancy": 2.0806, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5394}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5082}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20and%20Efficient%20Personalization%20with%20Mixtures%20of%20Adaptors&body=Title%3A%20Collaborative%20and%20Efficient%20Personalization%20with%20Mixtures%20of%20Adaptors%0AAuthor%3A%20Abdulla%20Jasem%20Almansoori%20and%20Samuel%20Horv%C3%A1th%20and%20Martin%20Tak%C3%A1%C4%8D%0AAbstract%3A%20%20%20Non-iid%20data%20is%20prevalent%20in%20real-world%20federated%20learning%20problems.%20Data%0Aheterogeneity%20can%20come%20in%20different%20types%20in%20terms%20of%20distribution%20shifts.%20In%0Athis%20work%2C%20we%20are%20interested%20in%20the%20heterogeneity%20that%20comes%20from%20concept%0Ashifts%2C%20i.e.%2C%20shifts%20in%20the%20prediction%20across%20clients.%20In%20particular%2C%20we%0Aconsider%20multi-task%20learning%2C%20where%20we%20want%20the%20model%20to%20adapt%20to%20the%20task%20of%0Athe%20client.%20We%20propose%20a%20parameter-efficient%20framework%20to%20tackle%20this%20issue%2C%0Awhere%20each%20client%20learns%20to%20mix%20between%20parameter-efficient%20adaptors%20according%0Ato%20its%20task.%20We%20use%20Low-Rank%20Adaptors%20%28LoRAs%29%20as%20the%20backbone%20and%20extend%20its%0Aconcept%20to%20other%20types%20of%20layers.%20We%20call%20our%20framework%20Federated%20Low-Rank%0AAdaptive%20Learning%20%28FLoRAL%29.%20This%20framework%20is%20not%20an%20algorithm%20but%20rather%20a%0Amodel%20parameterization%20for%20a%20multi-task%20learning%20objective%2C%20so%20it%20can%20work%20on%0Atop%20of%20any%20algorithm%20that%20optimizes%20this%20objective%2C%20which%20includes%20many%0Aalgorithms%20from%20the%20literature.%20FLoRAL%20is%20memory-efficient%2C%20and%20clients%20are%0Apersonalized%20with%20small%20states%20%28e.g.%2C%20one%20number%20per%20adaptor%29%20as%20the%20adaptors%0Athemselves%20are%20federated.%20Hence%2C%20personalization%20is--in%20this%20sense--federated%0Aas%20well.%20Even%20though%20clients%20can%20personalize%20more%20freely%20by%20training%20an%20adaptor%0Alocally%2C%20we%20show%20that%20collaborative%20and%20efficient%20training%20of%20adaptors%20is%0Apossible%20and%20performs%20better.%20We%20also%20show%20that%20FLoRAL%20can%20outperform%20an%0Aensemble%20of%20full%20models%20with%20optimal%20cluster%20assignment%2C%20which%20demonstrates%20the%0Abenefits%20of%20federated%20personalization%20and%20the%20robustness%20of%20FLoRAL%20to%0Aoverfitting.%20We%20show%20promising%20experimental%20results%20on%20synthetic%20datasets%2C%0Areal-world%20federated%20multi-task%20problems%20such%20as%20MNIST%2C%20CIFAR-10%2C%20and%0ACIFAR-100.%20We%20also%20provide%20a%20theoretical%20analysis%20of%20local%20SGD%20on%20a%20relaxed%0Aobjective%20and%20discuss%20the%20effects%20of%20aggregation%20mismatch%20on%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520and%2520Efficient%2520Personalization%2520with%2520Mixtures%2520of%2520Adaptors%26entry.906535625%3DAbdulla%2520Jasem%2520Almansoori%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%26entry.1292438233%3D%2520%2520Non-iid%2520data%2520is%2520prevalent%2520in%2520real-world%2520federated%2520learning%2520problems.%2520Data%250Aheterogeneity%2520can%2520come%2520in%2520different%2520types%2520in%2520terms%2520of%2520distribution%2520shifts.%2520In%250Athis%2520work%252C%2520we%2520are%2520interested%2520in%2520the%2520heterogeneity%2520that%2520comes%2520from%2520concept%250Ashifts%252C%2520i.e.%252C%2520shifts%2520in%2520the%2520prediction%2520across%2520clients.%2520In%2520particular%252C%2520we%250Aconsider%2520multi-task%2520learning%252C%2520where%2520we%2520want%2520the%2520model%2520to%2520adapt%2520to%2520the%2520task%2520of%250Athe%2520client.%2520We%2520propose%2520a%2520parameter-efficient%2520framework%2520to%2520tackle%2520this%2520issue%252C%250Awhere%2520each%2520client%2520learns%2520to%2520mix%2520between%2520parameter-efficient%2520adaptors%2520according%250Ato%2520its%2520task.%2520We%2520use%2520Low-Rank%2520Adaptors%2520%2528LoRAs%2529%2520as%2520the%2520backbone%2520and%2520extend%2520its%250Aconcept%2520to%2520other%2520types%2520of%2520layers.%2520We%2520call%2520our%2520framework%2520Federated%2520Low-Rank%250AAdaptive%2520Learning%2520%2528FLoRAL%2529.%2520This%2520framework%2520is%2520not%2520an%2520algorithm%2520but%2520rather%2520a%250Amodel%2520parameterization%2520for%2520a%2520multi-task%2520learning%2520objective%252C%2520so%2520it%2520can%2520work%2520on%250Atop%2520of%2520any%2520algorithm%2520that%2520optimizes%2520this%2520objective%252C%2520which%2520includes%2520many%250Aalgorithms%2520from%2520the%2520literature.%2520FLoRAL%2520is%2520memory-efficient%252C%2520and%2520clients%2520are%250Apersonalized%2520with%2520small%2520states%2520%2528e.g.%252C%2520one%2520number%2520per%2520adaptor%2529%2520as%2520the%2520adaptors%250Athemselves%2520are%2520federated.%2520Hence%252C%2520personalization%2520is--in%2520this%2520sense--federated%250Aas%2520well.%2520Even%2520though%2520clients%2520can%2520personalize%2520more%2520freely%2520by%2520training%2520an%2520adaptor%250Alocally%252C%2520we%2520show%2520that%2520collaborative%2520and%2520efficient%2520training%2520of%2520adaptors%2520is%250Apossible%2520and%2520performs%2520better.%2520We%2520also%2520show%2520that%2520FLoRAL%2520can%2520outperform%2520an%250Aensemble%2520of%2520full%2520models%2520with%2520optimal%2520cluster%2520assignment%252C%2520which%2520demonstrates%2520the%250Abenefits%2520of%2520federated%2520personalization%2520and%2520the%2520robustness%2520of%2520FLoRAL%2520to%250Aoverfitting.%2520We%2520show%2520promising%2520experimental%2520results%2520on%2520synthetic%2520datasets%252C%250Areal-world%2520federated%2520multi-task%2520problems%2520such%2520as%2520MNIST%252C%2520CIFAR-10%252C%2520and%250ACIFAR-100.%2520We%2520also%2520provide%2520a%2520theoretical%2520analysis%2520of%2520local%2520SGD%2520on%2520a%2520relaxed%250Aobjective%2520and%2520discuss%2520the%2520effects%2520of%2520aggregation%2520mismatch%2520on%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20and%20Efficient%20Personalization%20with%20Mixtures%20of%20Adaptors&entry.906535625=Abdulla%20Jasem%20Almansoori%20and%20Samuel%20Horv%C3%A1th%20and%20Martin%20Tak%C3%A1%C4%8D&entry.1292438233=%20%20Non-iid%20data%20is%20prevalent%20in%20real-world%20federated%20learning%20problems.%20Data%0Aheterogeneity%20can%20come%20in%20different%20types%20in%20terms%20of%20distribution%20shifts.%20In%0Athis%20work%2C%20we%20are%20interested%20in%20the%20heterogeneity%20that%20comes%20from%20concept%0Ashifts%2C%20i.e.%2C%20shifts%20in%20the%20prediction%20across%20clients.%20In%20particular%2C%20we%0Aconsider%20multi-task%20learning%2C%20where%20we%20want%20the%20model%20to%20adapt%20to%20the%20task%20of%0Athe%20client.%20We%20propose%20a%20parameter-efficient%20framework%20to%20tackle%20this%20issue%2C%0Awhere%20each%20client%20learns%20to%20mix%20between%20parameter-efficient%20adaptors%20according%0Ato%20its%20task.%20We%20use%20Low-Rank%20Adaptors%20%28LoRAs%29%20as%20the%20backbone%20and%20extend%20its%0Aconcept%20to%20other%20types%20of%20layers.%20We%20call%20our%20framework%20Federated%20Low-Rank%0AAdaptive%20Learning%20%28FLoRAL%29.%20This%20framework%20is%20not%20an%20algorithm%20but%20rather%20a%0Amodel%20parameterization%20for%20a%20multi-task%20learning%20objective%2C%20so%20it%20can%20work%20on%0Atop%20of%20any%20algorithm%20that%20optimizes%20this%20objective%2C%20which%20includes%20many%0Aalgorithms%20from%20the%20literature.%20FLoRAL%20is%20memory-efficient%2C%20and%20clients%20are%0Apersonalized%20with%20small%20states%20%28e.g.%2C%20one%20number%20per%20adaptor%29%20as%20the%20adaptors%0Athemselves%20are%20federated.%20Hence%2C%20personalization%20is--in%20this%20sense--federated%0Aas%20well.%20Even%20though%20clients%20can%20personalize%20more%20freely%20by%20training%20an%20adaptor%0Alocally%2C%20we%20show%20that%20collaborative%20and%20efficient%20training%20of%20adaptors%20is%0Apossible%20and%20performs%20better.%20We%20also%20show%20that%20FLoRAL%20can%20outperform%20an%0Aensemble%20of%20full%20models%20with%20optimal%20cluster%20assignment%2C%20which%20demonstrates%20the%0Abenefits%20of%20federated%20personalization%20and%20the%20robustness%20of%20FLoRAL%20to%0Aoverfitting.%20We%20show%20promising%20experimental%20results%20on%20synthetic%20datasets%2C%0Areal-world%20federated%20multi-task%20problems%20such%20as%20MNIST%2C%20CIFAR-10%2C%20and%0ACIFAR-100.%20We%20also%20provide%20a%20theoretical%20analysis%20of%20local%20SGD%20on%20a%20relaxed%0Aobjective%20and%20discuss%20the%20effects%20of%20aggregation%20mismatch%20on%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03497v1&entry.124074799=Read"},
{"title": "Exploring the Benefit of Activation Sparsity in Pre-training", "author": "Zhengyan Zhang and Chaojun Xiao and Qiujieli Qin and Yankai Lin and Zhiyuan Zeng and Xu Han and Zhiyuan Liu and Ruobing Xie and Maosong Sun and Jie Zhou", "abstract": "  Pre-trained Transformers inherently possess the characteristic of sparse\nactivation, where only a small fraction of the neurons are activated for each\ntoken. While sparse activation has been explored through post-training methods,\nits potential in pre-training remains untapped. In this work, we first study\nhow activation properties change during pre-training. Our examination reveals\nthat Transformers exhibit sparse activation throughout the majority of the\npre-training process while the activation correlation keeps evolving as\ntraining progresses. Leveraging this observation, we propose Switchable\nSparse-Dense Learning (SSD). SSD adaptively switches between the\nMixtures-of-Experts (MoE) based sparse training and the conventional dense\ntraining during the pre-training process, leveraging the efficiency of sparse\ntraining and avoiding the static activation correlation of sparse training.\nCompared to dense training, SSD achieves comparable performance with identical\nmodel size and reduces pre-training costs. Moreover, the models trained with\nSSD can be directly used as MoE models for sparse inference and achieve the\nsame performance as dense models with up to $2\\times$ faster inference speed.\nCodes are available at https://github.com/thunlp/moefication.\n", "link": "http://arxiv.org/abs/2410.03440v1", "date": "2024-10-04", "relevancy": 2.0748, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5311}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5165}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Benefit%20of%20Activation%20Sparsity%20in%20Pre-training&body=Title%3A%20Exploring%20the%20Benefit%20of%20Activation%20Sparsity%20in%20Pre-training%0AAuthor%3A%20Zhengyan%20Zhang%20and%20Chaojun%20Xiao%20and%20Qiujieli%20Qin%20and%20Yankai%20Lin%20and%20Zhiyuan%20Zeng%20and%20Xu%20Han%20and%20Zhiyuan%20Liu%20and%20Ruobing%20Xie%20and%20Maosong%20Sun%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Pre-trained%20Transformers%20inherently%20possess%20the%20characteristic%20of%20sparse%0Aactivation%2C%20where%20only%20a%20small%20fraction%20of%20the%20neurons%20are%20activated%20for%20each%0Atoken.%20While%20sparse%20activation%20has%20been%20explored%20through%20post-training%20methods%2C%0Aits%20potential%20in%20pre-training%20remains%20untapped.%20In%20this%20work%2C%20we%20first%20study%0Ahow%20activation%20properties%20change%20during%20pre-training.%20Our%20examination%20reveals%0Athat%20Transformers%20exhibit%20sparse%20activation%20throughout%20the%20majority%20of%20the%0Apre-training%20process%20while%20the%20activation%20correlation%20keeps%20evolving%20as%0Atraining%20progresses.%20Leveraging%20this%20observation%2C%20we%20propose%20Switchable%0ASparse-Dense%20Learning%20%28SSD%29.%20SSD%20adaptively%20switches%20between%20the%0AMixtures-of-Experts%20%28MoE%29%20based%20sparse%20training%20and%20the%20conventional%20dense%0Atraining%20during%20the%20pre-training%20process%2C%20leveraging%20the%20efficiency%20of%20sparse%0Atraining%20and%20avoiding%20the%20static%20activation%20correlation%20of%20sparse%20training.%0ACompared%20to%20dense%20training%2C%20SSD%20achieves%20comparable%20performance%20with%20identical%0Amodel%20size%20and%20reduces%20pre-training%20costs.%20Moreover%2C%20the%20models%20trained%20with%0ASSD%20can%20be%20directly%20used%20as%20MoE%20models%20for%20sparse%20inference%20and%20achieve%20the%0Asame%20performance%20as%20dense%20models%20with%20up%20to%20%242%5Ctimes%24%20faster%20inference%20speed.%0ACodes%20are%20available%20at%20https%3A//github.com/thunlp/moefication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Benefit%2520of%2520Activation%2520Sparsity%2520in%2520Pre-training%26entry.906535625%3DZhengyan%2520Zhang%2520and%2520Chaojun%2520Xiao%2520and%2520Qiujieli%2520Qin%2520and%2520Yankai%2520Lin%2520and%2520Zhiyuan%2520Zeng%2520and%2520Xu%2520Han%2520and%2520Zhiyuan%2520Liu%2520and%2520Ruobing%2520Xie%2520and%2520Maosong%2520Sun%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Pre-trained%2520Transformers%2520inherently%2520possess%2520the%2520characteristic%2520of%2520sparse%250Aactivation%252C%2520where%2520only%2520a%2520small%2520fraction%2520of%2520the%2520neurons%2520are%2520activated%2520for%2520each%250Atoken.%2520While%2520sparse%2520activation%2520has%2520been%2520explored%2520through%2520post-training%2520methods%252C%250Aits%2520potential%2520in%2520pre-training%2520remains%2520untapped.%2520In%2520this%2520work%252C%2520we%2520first%2520study%250Ahow%2520activation%2520properties%2520change%2520during%2520pre-training.%2520Our%2520examination%2520reveals%250Athat%2520Transformers%2520exhibit%2520sparse%2520activation%2520throughout%2520the%2520majority%2520of%2520the%250Apre-training%2520process%2520while%2520the%2520activation%2520correlation%2520keeps%2520evolving%2520as%250Atraining%2520progresses.%2520Leveraging%2520this%2520observation%252C%2520we%2520propose%2520Switchable%250ASparse-Dense%2520Learning%2520%2528SSD%2529.%2520SSD%2520adaptively%2520switches%2520between%2520the%250AMixtures-of-Experts%2520%2528MoE%2529%2520based%2520sparse%2520training%2520and%2520the%2520conventional%2520dense%250Atraining%2520during%2520the%2520pre-training%2520process%252C%2520leveraging%2520the%2520efficiency%2520of%2520sparse%250Atraining%2520and%2520avoiding%2520the%2520static%2520activation%2520correlation%2520of%2520sparse%2520training.%250ACompared%2520to%2520dense%2520training%252C%2520SSD%2520achieves%2520comparable%2520performance%2520with%2520identical%250Amodel%2520size%2520and%2520reduces%2520pre-training%2520costs.%2520Moreover%252C%2520the%2520models%2520trained%2520with%250ASSD%2520can%2520be%2520directly%2520used%2520as%2520MoE%2520models%2520for%2520sparse%2520inference%2520and%2520achieve%2520the%250Asame%2520performance%2520as%2520dense%2520models%2520with%2520up%2520to%2520%25242%255Ctimes%2524%2520faster%2520inference%2520speed.%250ACodes%2520are%2520available%2520at%2520https%253A//github.com/thunlp/moefication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Benefit%20of%20Activation%20Sparsity%20in%20Pre-training&entry.906535625=Zhengyan%20Zhang%20and%20Chaojun%20Xiao%20and%20Qiujieli%20Qin%20and%20Yankai%20Lin%20and%20Zhiyuan%20Zeng%20and%20Xu%20Han%20and%20Zhiyuan%20Liu%20and%20Ruobing%20Xie%20and%20Maosong%20Sun%20and%20Jie%20Zhou&entry.1292438233=%20%20Pre-trained%20Transformers%20inherently%20possess%20the%20characteristic%20of%20sparse%0Aactivation%2C%20where%20only%20a%20small%20fraction%20of%20the%20neurons%20are%20activated%20for%20each%0Atoken.%20While%20sparse%20activation%20has%20been%20explored%20through%20post-training%20methods%2C%0Aits%20potential%20in%20pre-training%20remains%20untapped.%20In%20this%20work%2C%20we%20first%20study%0Ahow%20activation%20properties%20change%20during%20pre-training.%20Our%20examination%20reveals%0Athat%20Transformers%20exhibit%20sparse%20activation%20throughout%20the%20majority%20of%20the%0Apre-training%20process%20while%20the%20activation%20correlation%20keeps%20evolving%20as%0Atraining%20progresses.%20Leveraging%20this%20observation%2C%20we%20propose%20Switchable%0ASparse-Dense%20Learning%20%28SSD%29.%20SSD%20adaptively%20switches%20between%20the%0AMixtures-of-Experts%20%28MoE%29%20based%20sparse%20training%20and%20the%20conventional%20dense%0Atraining%20during%20the%20pre-training%20process%2C%20leveraging%20the%20efficiency%20of%20sparse%0Atraining%20and%20avoiding%20the%20static%20activation%20correlation%20of%20sparse%20training.%0ACompared%20to%20dense%20training%2C%20SSD%20achieves%20comparable%20performance%20with%20identical%0Amodel%20size%20and%20reduces%20pre-training%20costs.%20Moreover%2C%20the%20models%20trained%20with%0ASSD%20can%20be%20directly%20used%20as%20MoE%20models%20for%20sparse%20inference%20and%20achieve%20the%0Asame%20performance%20as%20dense%20models%20with%20up%20to%20%242%5Ctimes%24%20faster%20inference%20speed.%0ACodes%20are%20available%20at%20https%3A//github.com/thunlp/moefication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03440v1&entry.124074799=Read"},
{"title": "Deep Model Interpretation with Limited Data : A Coreset-based Approach", "author": "Hamed Behzadi-Khormouji and Jos\u00e9 Oramas", "abstract": "  Model Interpretation aims at the extraction of insights from the internals of\na trained model. A common approach to address this task is the characterization\nof relevant features internally encoded in the model that are critical for its\nproper operation. Despite recent progress of these methods, they come with the\nweakness of being computationally expensive due to the dense evaluation of\ndatasets that they require. As a consequence, research on the design of these\nmethods have focused on smaller data subsets which may led to reduced insights.\nTo address these computational costs, we propose a coreset-based interpretation\nframework that utilizes coreset selection methods to sample a representative\nsubset of the large dataset for the interpretation task. Towards this goal, we\npropose a similarity-based evaluation protocol to assess the robustness of\nmodel interpretation methods towards the amount data they take as input.\nExperiments considering several interpretation methods, DNN models, and coreset\nselection methods show the effectiveness of the proposed framework.\n", "link": "http://arxiv.org/abs/2410.00524v2", "date": "2024-10-04", "relevancy": 2.0729, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Model%20Interpretation%20with%20Limited%20Data%20%3A%20A%20Coreset-based%20Approach&body=Title%3A%20Deep%20Model%20Interpretation%20with%20Limited%20Data%20%3A%20A%20Coreset-based%20Approach%0AAuthor%3A%20Hamed%20Behzadi-Khormouji%20and%20Jos%C3%A9%20Oramas%0AAbstract%3A%20%20%20Model%20Interpretation%20aims%20at%20the%20extraction%20of%20insights%20from%20the%20internals%20of%0Aa%20trained%20model.%20A%20common%20approach%20to%20address%20this%20task%20is%20the%20characterization%0Aof%20relevant%20features%20internally%20encoded%20in%20the%20model%20that%20are%20critical%20for%20its%0Aproper%20operation.%20Despite%20recent%20progress%20of%20these%20methods%2C%20they%20come%20with%20the%0Aweakness%20of%20being%20computationally%20expensive%20due%20to%20the%20dense%20evaluation%20of%0Adatasets%20that%20they%20require.%20As%20a%20consequence%2C%20research%20on%20the%20design%20of%20these%0Amethods%20have%20focused%20on%20smaller%20data%20subsets%20which%20may%20led%20to%20reduced%20insights.%0ATo%20address%20these%20computational%20costs%2C%20we%20propose%20a%20coreset-based%20interpretation%0Aframework%20that%20utilizes%20coreset%20selection%20methods%20to%20sample%20a%20representative%0Asubset%20of%20the%20large%20dataset%20for%20the%20interpretation%20task.%20Towards%20this%20goal%2C%20we%0Apropose%20a%20similarity-based%20evaluation%20protocol%20to%20assess%20the%20robustness%20of%0Amodel%20interpretation%20methods%20towards%20the%20amount%20data%20they%20take%20as%20input.%0AExperiments%20considering%20several%20interpretation%20methods%2C%20DNN%20models%2C%20and%20coreset%0Aselection%20methods%20show%20the%20effectiveness%20of%20the%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00524v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Model%2520Interpretation%2520with%2520Limited%2520Data%2520%253A%2520A%2520Coreset-based%2520Approach%26entry.906535625%3DHamed%2520Behzadi-Khormouji%2520and%2520Jos%25C3%25A9%2520Oramas%26entry.1292438233%3D%2520%2520Model%2520Interpretation%2520aims%2520at%2520the%2520extraction%2520of%2520insights%2520from%2520the%2520internals%2520of%250Aa%2520trained%2520model.%2520A%2520common%2520approach%2520to%2520address%2520this%2520task%2520is%2520the%2520characterization%250Aof%2520relevant%2520features%2520internally%2520encoded%2520in%2520the%2520model%2520that%2520are%2520critical%2520for%2520its%250Aproper%2520operation.%2520Despite%2520recent%2520progress%2520of%2520these%2520methods%252C%2520they%2520come%2520with%2520the%250Aweakness%2520of%2520being%2520computationally%2520expensive%2520due%2520to%2520the%2520dense%2520evaluation%2520of%250Adatasets%2520that%2520they%2520require.%2520As%2520a%2520consequence%252C%2520research%2520on%2520the%2520design%2520of%2520these%250Amethods%2520have%2520focused%2520on%2520smaller%2520data%2520subsets%2520which%2520may%2520led%2520to%2520reduced%2520insights.%250ATo%2520address%2520these%2520computational%2520costs%252C%2520we%2520propose%2520a%2520coreset-based%2520interpretation%250Aframework%2520that%2520utilizes%2520coreset%2520selection%2520methods%2520to%2520sample%2520a%2520representative%250Asubset%2520of%2520the%2520large%2520dataset%2520for%2520the%2520interpretation%2520task.%2520Towards%2520this%2520goal%252C%2520we%250Apropose%2520a%2520similarity-based%2520evaluation%2520protocol%2520to%2520assess%2520the%2520robustness%2520of%250Amodel%2520interpretation%2520methods%2520towards%2520the%2520amount%2520data%2520they%2520take%2520as%2520input.%250AExperiments%2520considering%2520several%2520interpretation%2520methods%252C%2520DNN%2520models%252C%2520and%2520coreset%250Aselection%2520methods%2520show%2520the%2520effectiveness%2520of%2520the%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00524v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Model%20Interpretation%20with%20Limited%20Data%20%3A%20A%20Coreset-based%20Approach&entry.906535625=Hamed%20Behzadi-Khormouji%20and%20Jos%C3%A9%20Oramas&entry.1292438233=%20%20Model%20Interpretation%20aims%20at%20the%20extraction%20of%20insights%20from%20the%20internals%20of%0Aa%20trained%20model.%20A%20common%20approach%20to%20address%20this%20task%20is%20the%20characterization%0Aof%20relevant%20features%20internally%20encoded%20in%20the%20model%20that%20are%20critical%20for%20its%0Aproper%20operation.%20Despite%20recent%20progress%20of%20these%20methods%2C%20they%20come%20with%20the%0Aweakness%20of%20being%20computationally%20expensive%20due%20to%20the%20dense%20evaluation%20of%0Adatasets%20that%20they%20require.%20As%20a%20consequence%2C%20research%20on%20the%20design%20of%20these%0Amethods%20have%20focused%20on%20smaller%20data%20subsets%20which%20may%20led%20to%20reduced%20insights.%0ATo%20address%20these%20computational%20costs%2C%20we%20propose%20a%20coreset-based%20interpretation%0Aframework%20that%20utilizes%20coreset%20selection%20methods%20to%20sample%20a%20representative%0Asubset%20of%20the%20large%20dataset%20for%20the%20interpretation%20task.%20Towards%20this%20goal%2C%20we%0Apropose%20a%20similarity-based%20evaluation%20protocol%20to%20assess%20the%20robustness%20of%0Amodel%20interpretation%20methods%20towards%20the%20amount%20data%20they%20take%20as%20input.%0AExperiments%20considering%20several%20interpretation%20methods%2C%20DNN%20models%2C%20and%20coreset%0Aselection%20methods%20show%20the%20effectiveness%20of%20the%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00524v2&entry.124074799=Read"},
{"title": "Understanding Reasoning in Chain-of-Thought from the Hopfieldian View", "author": "Lijie Hu and Liang Liu and Shu Yang and Xin Chen and Zhen Tan and Muhammad Asif Ali and Mengdi Li and Di Wang", "abstract": "  Large Language Models have demonstrated remarkable abilities across various\ntasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to\nenhance reasoning capabilities. However, existing research primarily focuses on\nimproving performance, lacking a comprehensive framework to explain and\nunderstand the fundamental factors behind CoT's success. To bridge this gap, we\nintroduce a novel perspective grounded in the Hopfieldian view of cognition in\ncognitive neuroscience. We establish a connection between CoT reasoning and key\ncognitive elements such as stimuli, actions, neural populations, and\nrepresentation spaces. From our view, we can understand the reasoning process\nas the movement between these representation spaces. Building on this insight,\nwe develop a method for localizing reasoning errors in the response of CoTs.\nMoreover, we propose the Representation-of-Thought (RoT) framework, which\nleverages the robustness of low-dimensional representation spaces to enhance\nthe robustness of the reasoning process in CoTs. Experimental results\ndemonstrate that RoT improves the robustness and interpretability of CoT\nreasoning while offering fine-grained control over the reasoning process.\n", "link": "http://arxiv.org/abs/2410.03595v1", "date": "2024-10-04", "relevancy": 2.0685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Reasoning%20in%20Chain-of-Thought%20from%20the%20Hopfieldian%20View&body=Title%3A%20Understanding%20Reasoning%20in%20Chain-of-Thought%20from%20the%20Hopfieldian%20View%0AAuthor%3A%20Lijie%20Hu%20and%20Liang%20Liu%20and%20Shu%20Yang%20and%20Xin%20Chen%20and%20Zhen%20Tan%20and%20Muhammad%20Asif%20Ali%20and%20Mengdi%20Li%20and%20Di%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20demonstrated%20remarkable%20abilities%20across%20various%0Atasks%2C%20with%20Chain-of-Thought%20%28CoT%29%20prompting%20emerging%20as%20a%20key%20technique%20to%0Aenhance%20reasoning%20capabilities.%20However%2C%20existing%20research%20primarily%20focuses%20on%0Aimproving%20performance%2C%20lacking%20a%20comprehensive%20framework%20to%20explain%20and%0Aunderstand%20the%20fundamental%20factors%20behind%20CoT%27s%20success.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20a%20novel%20perspective%20grounded%20in%20the%20Hopfieldian%20view%20of%20cognition%20in%0Acognitive%20neuroscience.%20We%20establish%20a%20connection%20between%20CoT%20reasoning%20and%20key%0Acognitive%20elements%20such%20as%20stimuli%2C%20actions%2C%20neural%20populations%2C%20and%0Arepresentation%20spaces.%20From%20our%20view%2C%20we%20can%20understand%20the%20reasoning%20process%0Aas%20the%20movement%20between%20these%20representation%20spaces.%20Building%20on%20this%20insight%2C%0Awe%20develop%20a%20method%20for%20localizing%20reasoning%20errors%20in%20the%20response%20of%20CoTs.%0AMoreover%2C%20we%20propose%20the%20Representation-of-Thought%20%28RoT%29%20framework%2C%20which%0Aleverages%20the%20robustness%20of%20low-dimensional%20representation%20spaces%20to%20enhance%0Athe%20robustness%20of%20the%20reasoning%20process%20in%20CoTs.%20Experimental%20results%0Ademonstrate%20that%20RoT%20improves%20the%20robustness%20and%20interpretability%20of%20CoT%0Areasoning%20while%20offering%20fine-grained%20control%20over%20the%20reasoning%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Reasoning%2520in%2520Chain-of-Thought%2520from%2520the%2520Hopfieldian%2520View%26entry.906535625%3DLijie%2520Hu%2520and%2520Liang%2520Liu%2520and%2520Shu%2520Yang%2520and%2520Xin%2520Chen%2520and%2520Zhen%2520Tan%2520and%2520Muhammad%2520Asif%2520Ali%2520and%2520Mengdi%2520Li%2520and%2520Di%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520demonstrated%2520remarkable%2520abilities%2520across%2520various%250Atasks%252C%2520with%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520emerging%2520as%2520a%2520key%2520technique%2520to%250Aenhance%2520reasoning%2520capabilities.%2520However%252C%2520existing%2520research%2520primarily%2520focuses%2520on%250Aimproving%2520performance%252C%2520lacking%2520a%2520comprehensive%2520framework%2520to%2520explain%2520and%250Aunderstand%2520the%2520fundamental%2520factors%2520behind%2520CoT%2527s%2520success.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520a%2520novel%2520perspective%2520grounded%2520in%2520the%2520Hopfieldian%2520view%2520of%2520cognition%2520in%250Acognitive%2520neuroscience.%2520We%2520establish%2520a%2520connection%2520between%2520CoT%2520reasoning%2520and%2520key%250Acognitive%2520elements%2520such%2520as%2520stimuli%252C%2520actions%252C%2520neural%2520populations%252C%2520and%250Arepresentation%2520spaces.%2520From%2520our%2520view%252C%2520we%2520can%2520understand%2520the%2520reasoning%2520process%250Aas%2520the%2520movement%2520between%2520these%2520representation%2520spaces.%2520Building%2520on%2520this%2520insight%252C%250Awe%2520develop%2520a%2520method%2520for%2520localizing%2520reasoning%2520errors%2520in%2520the%2520response%2520of%2520CoTs.%250AMoreover%252C%2520we%2520propose%2520the%2520Representation-of-Thought%2520%2528RoT%2529%2520framework%252C%2520which%250Aleverages%2520the%2520robustness%2520of%2520low-dimensional%2520representation%2520spaces%2520to%2520enhance%250Athe%2520robustness%2520of%2520the%2520reasoning%2520process%2520in%2520CoTs.%2520Experimental%2520results%250Ademonstrate%2520that%2520RoT%2520improves%2520the%2520robustness%2520and%2520interpretability%2520of%2520CoT%250Areasoning%2520while%2520offering%2520fine-grained%2520control%2520over%2520the%2520reasoning%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Reasoning%20in%20Chain-of-Thought%20from%20the%20Hopfieldian%20View&entry.906535625=Lijie%20Hu%20and%20Liang%20Liu%20and%20Shu%20Yang%20and%20Xin%20Chen%20and%20Zhen%20Tan%20and%20Muhammad%20Asif%20Ali%20and%20Mengdi%20Li%20and%20Di%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20have%20demonstrated%20remarkable%20abilities%20across%20various%0Atasks%2C%20with%20Chain-of-Thought%20%28CoT%29%20prompting%20emerging%20as%20a%20key%20technique%20to%0Aenhance%20reasoning%20capabilities.%20However%2C%20existing%20research%20primarily%20focuses%20on%0Aimproving%20performance%2C%20lacking%20a%20comprehensive%20framework%20to%20explain%20and%0Aunderstand%20the%20fundamental%20factors%20behind%20CoT%27s%20success.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20a%20novel%20perspective%20grounded%20in%20the%20Hopfieldian%20view%20of%20cognition%20in%0Acognitive%20neuroscience.%20We%20establish%20a%20connection%20between%20CoT%20reasoning%20and%20key%0Acognitive%20elements%20such%20as%20stimuli%2C%20actions%2C%20neural%20populations%2C%20and%0Arepresentation%20spaces.%20From%20our%20view%2C%20we%20can%20understand%20the%20reasoning%20process%0Aas%20the%20movement%20between%20these%20representation%20spaces.%20Building%20on%20this%20insight%2C%0Awe%20develop%20a%20method%20for%20localizing%20reasoning%20errors%20in%20the%20response%20of%20CoTs.%0AMoreover%2C%20we%20propose%20the%20Representation-of-Thought%20%28RoT%29%20framework%2C%20which%0Aleverages%20the%20robustness%20of%20low-dimensional%20representation%20spaces%20to%20enhance%0Athe%20robustness%20of%20the%20reasoning%20process%20in%20CoTs.%20Experimental%20results%0Ademonstrate%20that%20RoT%20improves%20the%20robustness%20and%20interpretability%20of%20CoT%0Areasoning%20while%20offering%20fine-grained%20control%20over%20the%20reasoning%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03595v1&entry.124074799=Read"},
{"title": "Training on more Reachable Tasks for Generalisation in Reinforcement\n  Learning", "author": "Max Weltevrede and Caroline Horsch and Matthijs T. J. Spaan and Wendelin B\u00f6hmer", "abstract": "  In multi-task reinforcement learning, agents train on a fixed set of tasks\nand have to generalise to new ones. Recent work has shown that increased\nexploration improves this generalisation, but it remains unclear why exactly\nthat is. In this paper, we introduce the concept of reachability in multi-task\nreinforcement learning and show that an initial exploration phase increases the\nnumber of reachable tasks the agent is trained on. This, and not the increased\nexploration, is responsible for the improved generalisation, even to\nunreachable tasks. Inspired by this, we propose a novel method Explore-Go that\nimplements such an exploration phase at the beginning of each episode.\nExplore-Go only modifies the way experience is collected and can be used with\nmost existing on-policy or off-policy reinforcement learning algorithms. We\ndemonstrate the effectiveness of our method when combined with some popular\nalgorithms and show an increase in generalisation performance across several\nenvironments.\n", "link": "http://arxiv.org/abs/2410.03565v1", "date": "2024-10-04", "relevancy": 2.0535, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5157}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5146}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20on%20more%20Reachable%20Tasks%20for%20Generalisation%20in%20Reinforcement%0A%20%20Learning&body=Title%3A%20Training%20on%20more%20Reachable%20Tasks%20for%20Generalisation%20in%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Max%20Weltevrede%20and%20Caroline%20Horsch%20and%20Matthijs%20T.%20J.%20Spaan%20and%20Wendelin%20B%C3%B6hmer%0AAbstract%3A%20%20%20In%20multi-task%20reinforcement%20learning%2C%20agents%20train%20on%20a%20fixed%20set%20of%20tasks%0Aand%20have%20to%20generalise%20to%20new%20ones.%20Recent%20work%20has%20shown%20that%20increased%0Aexploration%20improves%20this%20generalisation%2C%20but%20it%20remains%20unclear%20why%20exactly%0Athat%20is.%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20reachability%20in%20multi-task%0Areinforcement%20learning%20and%20show%20that%20an%20initial%20exploration%20phase%20increases%20the%0Anumber%20of%20reachable%20tasks%20the%20agent%20is%20trained%20on.%20This%2C%20and%20not%20the%20increased%0Aexploration%2C%20is%20responsible%20for%20the%20improved%20generalisation%2C%20even%20to%0Aunreachable%20tasks.%20Inspired%20by%20this%2C%20we%20propose%20a%20novel%20method%20Explore-Go%20that%0Aimplements%20such%20an%20exploration%20phase%20at%20the%20beginning%20of%20each%20episode.%0AExplore-Go%20only%20modifies%20the%20way%20experience%20is%20collected%20and%20can%20be%20used%20with%0Amost%20existing%20on-policy%20or%20off-policy%20reinforcement%20learning%20algorithms.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20when%20combined%20with%20some%20popular%0Aalgorithms%20and%20show%20an%20increase%20in%20generalisation%20performance%20across%20several%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520on%2520more%2520Reachable%2520Tasks%2520for%2520Generalisation%2520in%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DMax%2520Weltevrede%2520and%2520Caroline%2520Horsch%2520and%2520Matthijs%2520T.%2520J.%2520Spaan%2520and%2520Wendelin%2520B%25C3%25B6hmer%26entry.1292438233%3D%2520%2520In%2520multi-task%2520reinforcement%2520learning%252C%2520agents%2520train%2520on%2520a%2520fixed%2520set%2520of%2520tasks%250Aand%2520have%2520to%2520generalise%2520to%2520new%2520ones.%2520Recent%2520work%2520has%2520shown%2520that%2520increased%250Aexploration%2520improves%2520this%2520generalisation%252C%2520but%2520it%2520remains%2520unclear%2520why%2520exactly%250Athat%2520is.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520concept%2520of%2520reachability%2520in%2520multi-task%250Areinforcement%2520learning%2520and%2520show%2520that%2520an%2520initial%2520exploration%2520phase%2520increases%2520the%250Anumber%2520of%2520reachable%2520tasks%2520the%2520agent%2520is%2520trained%2520on.%2520This%252C%2520and%2520not%2520the%2520increased%250Aexploration%252C%2520is%2520responsible%2520for%2520the%2520improved%2520generalisation%252C%2520even%2520to%250Aunreachable%2520tasks.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520novel%2520method%2520Explore-Go%2520that%250Aimplements%2520such%2520an%2520exploration%2520phase%2520at%2520the%2520beginning%2520of%2520each%2520episode.%250AExplore-Go%2520only%2520modifies%2520the%2520way%2520experience%2520is%2520collected%2520and%2520can%2520be%2520used%2520with%250Amost%2520existing%2520on-policy%2520or%2520off-policy%2520reinforcement%2520learning%2520algorithms.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520when%2520combined%2520with%2520some%2520popular%250Aalgorithms%2520and%2520show%2520an%2520increase%2520in%2520generalisation%2520performance%2520across%2520several%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20on%20more%20Reachable%20Tasks%20for%20Generalisation%20in%20Reinforcement%0A%20%20Learning&entry.906535625=Max%20Weltevrede%20and%20Caroline%20Horsch%20and%20Matthijs%20T.%20J.%20Spaan%20and%20Wendelin%20B%C3%B6hmer&entry.1292438233=%20%20In%20multi-task%20reinforcement%20learning%2C%20agents%20train%20on%20a%20fixed%20set%20of%20tasks%0Aand%20have%20to%20generalise%20to%20new%20ones.%20Recent%20work%20has%20shown%20that%20increased%0Aexploration%20improves%20this%20generalisation%2C%20but%20it%20remains%20unclear%20why%20exactly%0Athat%20is.%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20reachability%20in%20multi-task%0Areinforcement%20learning%20and%20show%20that%20an%20initial%20exploration%20phase%20increases%20the%0Anumber%20of%20reachable%20tasks%20the%20agent%20is%20trained%20on.%20This%2C%20and%20not%20the%20increased%0Aexploration%2C%20is%20responsible%20for%20the%20improved%20generalisation%2C%20even%20to%0Aunreachable%20tasks.%20Inspired%20by%20this%2C%20we%20propose%20a%20novel%20method%20Explore-Go%20that%0Aimplements%20such%20an%20exploration%20phase%20at%20the%20beginning%20of%20each%20episode.%0AExplore-Go%20only%20modifies%20the%20way%20experience%20is%20collected%20and%20can%20be%20used%20with%0Amost%20existing%20on-policy%20or%20off-policy%20reinforcement%20learning%20algorithms.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20when%20combined%20with%20some%20popular%0Aalgorithms%20and%20show%20an%20increase%20in%20generalisation%20performance%20across%20several%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03565v1&entry.124074799=Read"},
{"title": "Evidence-based Match-status-Aware Gait Recognition for Out-of-Gallery\n  Gait Identification", "author": "Heming Du and Chen Liu and Ming Wang and Lincheng Li and Shunli Zhang and Xin Yu", "abstract": "  Existing gait recognition methods typically identify individuals based on the\nsimilarity between probe and gallery samples. However, these methods often\nneglect the fact that the gallery may not contain identities corresponding to\nthe probes, leading to incorrect recognition.To identify Out-of-Gallery (OOG)\ngait queries, we propose an Evidence-based Match-status-Aware Gait Recognition\n(EMA-GR) framework. Inspired by Evidential Deep Learning (EDL), EMA-GR is\ndesigned to quantify the uncertainty associated with the match status of\nrecognition. Thus, EMA-GR identifies whether the probe has a counterpart in the\ngallery. Specifically, we adopt an evidence collector to gather match status\nevidence from a recognition result pair and parameterize a Dirichlet\ndistribution over the gathered evidence, following the Dempster-Shafer Theory\nof Evidence (DST). We measure the uncertainty and predict the match status of\nthe recognition results, and thus determine whether the probe is an OOG\nquery.To the best of our knowledge, our method is the first attempt to tackle\nOOG queries in gait recognition. Moreover, EMA-GR is agnostic against gait\nrecognition methods and improves the robustness against OOG queries. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non datasets with OOG queries, and can also generalize well to other\nidentity-retrieval tasks. Importantly, our method surpasses existing\nstate-of-the-art methods by a substantial margin, achieving a 51.26%\nimprovement when the OOG query rate is around 50% on OUMVLP.\n", "link": "http://arxiv.org/abs/2211.08007v4", "date": "2024-10-04", "relevancy": 2.0394, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5362}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.51}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evidence-based%20Match-status-Aware%20Gait%20Recognition%20for%20Out-of-Gallery%0A%20%20Gait%20Identification&body=Title%3A%20Evidence-based%20Match-status-Aware%20Gait%20Recognition%20for%20Out-of-Gallery%0A%20%20Gait%20Identification%0AAuthor%3A%20Heming%20Du%20and%20Chen%20Liu%20and%20Ming%20Wang%20and%20Lincheng%20Li%20and%20Shunli%20Zhang%20and%20Xin%20Yu%0AAbstract%3A%20%20%20Existing%20gait%20recognition%20methods%20typically%20identify%20individuals%20based%20on%20the%0Asimilarity%20between%20probe%20and%20gallery%20samples.%20However%2C%20these%20methods%20often%0Aneglect%20the%20fact%20that%20the%20gallery%20may%20not%20contain%20identities%20corresponding%20to%0Athe%20probes%2C%20leading%20to%20incorrect%20recognition.To%20identify%20Out-of-Gallery%20%28OOG%29%0Agait%20queries%2C%20we%20propose%20an%20Evidence-based%20Match-status-Aware%20Gait%20Recognition%0A%28EMA-GR%29%20framework.%20Inspired%20by%20Evidential%20Deep%20Learning%20%28EDL%29%2C%20EMA-GR%20is%0Adesigned%20to%20quantify%20the%20uncertainty%20associated%20with%20the%20match%20status%20of%0Arecognition.%20Thus%2C%20EMA-GR%20identifies%20whether%20the%20probe%20has%20a%20counterpart%20in%20the%0Agallery.%20Specifically%2C%20we%20adopt%20an%20evidence%20collector%20to%20gather%20match%20status%0Aevidence%20from%20a%20recognition%20result%20pair%20and%20parameterize%20a%20Dirichlet%0Adistribution%20over%20the%20gathered%20evidence%2C%20following%20the%20Dempster-Shafer%20Theory%0Aof%20Evidence%20%28DST%29.%20We%20measure%20the%20uncertainty%20and%20predict%20the%20match%20status%20of%0Athe%20recognition%20results%2C%20and%20thus%20determine%20whether%20the%20probe%20is%20an%20OOG%0Aquery.To%20the%20best%20of%20our%20knowledge%2C%20our%20method%20is%20the%20first%20attempt%20to%20tackle%0AOOG%20queries%20in%20gait%20recognition.%20Moreover%2C%20EMA-GR%20is%20agnostic%20against%20gait%0Arecognition%20methods%20and%20improves%20the%20robustness%20against%20OOG%20queries.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Aon%20datasets%20with%20OOG%20queries%2C%20and%20can%20also%20generalize%20well%20to%20other%0Aidentity-retrieval%20tasks.%20Importantly%2C%20our%20method%20surpasses%20existing%0Astate-of-the-art%20methods%20by%20a%20substantial%20margin%2C%20achieving%20a%2051.26%25%0Aimprovement%20when%20the%20OOG%20query%20rate%20is%20around%2050%25%20on%20OUMVLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.08007v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvidence-based%2520Match-status-Aware%2520Gait%2520Recognition%2520for%2520Out-of-Gallery%250A%2520%2520Gait%2520Identification%26entry.906535625%3DHeming%2520Du%2520and%2520Chen%2520Liu%2520and%2520Ming%2520Wang%2520and%2520Lincheng%2520Li%2520and%2520Shunli%2520Zhang%2520and%2520Xin%2520Yu%26entry.1292438233%3D%2520%2520Existing%2520gait%2520recognition%2520methods%2520typically%2520identify%2520individuals%2520based%2520on%2520the%250Asimilarity%2520between%2520probe%2520and%2520gallery%2520samples.%2520However%252C%2520these%2520methods%2520often%250Aneglect%2520the%2520fact%2520that%2520the%2520gallery%2520may%2520not%2520contain%2520identities%2520corresponding%2520to%250Athe%2520probes%252C%2520leading%2520to%2520incorrect%2520recognition.To%2520identify%2520Out-of-Gallery%2520%2528OOG%2529%250Agait%2520queries%252C%2520we%2520propose%2520an%2520Evidence-based%2520Match-status-Aware%2520Gait%2520Recognition%250A%2528EMA-GR%2529%2520framework.%2520Inspired%2520by%2520Evidential%2520Deep%2520Learning%2520%2528EDL%2529%252C%2520EMA-GR%2520is%250Adesigned%2520to%2520quantify%2520the%2520uncertainty%2520associated%2520with%2520the%2520match%2520status%2520of%250Arecognition.%2520Thus%252C%2520EMA-GR%2520identifies%2520whether%2520the%2520probe%2520has%2520a%2520counterpart%2520in%2520the%250Agallery.%2520Specifically%252C%2520we%2520adopt%2520an%2520evidence%2520collector%2520to%2520gather%2520match%2520status%250Aevidence%2520from%2520a%2520recognition%2520result%2520pair%2520and%2520parameterize%2520a%2520Dirichlet%250Adistribution%2520over%2520the%2520gathered%2520evidence%252C%2520following%2520the%2520Dempster-Shafer%2520Theory%250Aof%2520Evidence%2520%2528DST%2529.%2520We%2520measure%2520the%2520uncertainty%2520and%2520predict%2520the%2520match%2520status%2520of%250Athe%2520recognition%2520results%252C%2520and%2520thus%2520determine%2520whether%2520the%2520probe%2520is%2520an%2520OOG%250Aquery.To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520method%2520is%2520the%2520first%2520attempt%2520to%2520tackle%250AOOG%2520queries%2520in%2520gait%2520recognition.%2520Moreover%252C%2520EMA-GR%2520is%2520agnostic%2520against%2520gait%250Arecognition%2520methods%2520and%2520improves%2520the%2520robustness%2520against%2520OOG%2520queries.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%250Aon%2520datasets%2520with%2520OOG%2520queries%252C%2520and%2520can%2520also%2520generalize%2520well%2520to%2520other%250Aidentity-retrieval%2520tasks.%2520Importantly%252C%2520our%2520method%2520surpasses%2520existing%250Astate-of-the-art%2520methods%2520by%2520a%2520substantial%2520margin%252C%2520achieving%2520a%252051.26%2525%250Aimprovement%2520when%2520the%2520OOG%2520query%2520rate%2520is%2520around%252050%2525%2520on%2520OUMVLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.08007v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evidence-based%20Match-status-Aware%20Gait%20Recognition%20for%20Out-of-Gallery%0A%20%20Gait%20Identification&entry.906535625=Heming%20Du%20and%20Chen%20Liu%20and%20Ming%20Wang%20and%20Lincheng%20Li%20and%20Shunli%20Zhang%20and%20Xin%20Yu&entry.1292438233=%20%20Existing%20gait%20recognition%20methods%20typically%20identify%20individuals%20based%20on%20the%0Asimilarity%20between%20probe%20and%20gallery%20samples.%20However%2C%20these%20methods%20often%0Aneglect%20the%20fact%20that%20the%20gallery%20may%20not%20contain%20identities%20corresponding%20to%0Athe%20probes%2C%20leading%20to%20incorrect%20recognition.To%20identify%20Out-of-Gallery%20%28OOG%29%0Agait%20queries%2C%20we%20propose%20an%20Evidence-based%20Match-status-Aware%20Gait%20Recognition%0A%28EMA-GR%29%20framework.%20Inspired%20by%20Evidential%20Deep%20Learning%20%28EDL%29%2C%20EMA-GR%20is%0Adesigned%20to%20quantify%20the%20uncertainty%20associated%20with%20the%20match%20status%20of%0Arecognition.%20Thus%2C%20EMA-GR%20identifies%20whether%20the%20probe%20has%20a%20counterpart%20in%20the%0Agallery.%20Specifically%2C%20we%20adopt%20an%20evidence%20collector%20to%20gather%20match%20status%0Aevidence%20from%20a%20recognition%20result%20pair%20and%20parameterize%20a%20Dirichlet%0Adistribution%20over%20the%20gathered%20evidence%2C%20following%20the%20Dempster-Shafer%20Theory%0Aof%20Evidence%20%28DST%29.%20We%20measure%20the%20uncertainty%20and%20predict%20the%20match%20status%20of%0Athe%20recognition%20results%2C%20and%20thus%20determine%20whether%20the%20probe%20is%20an%20OOG%0Aquery.To%20the%20best%20of%20our%20knowledge%2C%20our%20method%20is%20the%20first%20attempt%20to%20tackle%0AOOG%20queries%20in%20gait%20recognition.%20Moreover%2C%20EMA-GR%20is%20agnostic%20against%20gait%0Arecognition%20methods%20and%20improves%20the%20robustness%20against%20OOG%20queries.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Aon%20datasets%20with%20OOG%20queries%2C%20and%20can%20also%20generalize%20well%20to%20other%0Aidentity-retrieval%20tasks.%20Importantly%2C%20our%20method%20surpasses%20existing%0Astate-of-the-art%20methods%20by%20a%20substantial%20margin%2C%20achieving%20a%2051.26%25%0Aimprovement%20when%20the%20OOG%20query%20rate%20is%20around%2050%25%20on%20OUMVLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.08007v4&entry.124074799=Read"},
{"title": "Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG", "author": "William Merrill and Noah A. Smith and Yanai Elazar", "abstract": "  How novel are texts generated by language models (LMs) relative to their\ntraining corpora? In this work, we investigate the extent to which modern LMs\ngenerate $n$-grams from their training data, evaluating both (i) the\nprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the\nproportion of $n$-grams generated by an LM that did not appear in the training\ndata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search\nover a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a\nnovel search tool inspired by indexing of genomic data. We compare the novelty\nof LM-generated text to human-written text and explore factors that affect\ngeneration novelty, focusing on the Pythia models. We find that, for $n > 4$,\nLM-generated text is less novel than human-written text, though it is more\nnovel for smaller $n$. Larger LMs and more constrained decoding strategies both\ndecrease novelty. Finally, we show that LMs complete $n$-grams with lower loss\nif they are more frequent in the training data. Overall, our results reveal\nfactors influencing the novelty of LM-generated text, and we release Rusty-DAWG\nto facilitate further pretraining data research.\n", "link": "http://arxiv.org/abs/2406.13069v3", "date": "2024-10-04", "relevancy": 2.0356, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4153}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4074}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20%24n%24-Gram%20Novelty%20of%20Language%20Models%20Using%20Rusty-DAWG&body=Title%3A%20Evaluating%20%24n%24-Gram%20Novelty%20of%20Language%20Models%20Using%20Rusty-DAWG%0AAuthor%3A%20William%20Merrill%20and%20Noah%20A.%20Smith%20and%20Yanai%20Elazar%0AAbstract%3A%20%20%20How%20novel%20are%20texts%20generated%20by%20language%20models%20%28LMs%29%20relative%20to%20their%0Atraining%20corpora%3F%20In%20this%20work%2C%20we%20investigate%20the%20extent%20to%20which%20modern%20LMs%0Agenerate%20%24n%24-grams%20from%20their%20training%20data%2C%20evaluating%20both%20%28i%29%20the%0Aprobability%20LMs%20assign%20to%20complete%20training%20%24n%24-grams%20and%20%28ii%29%20%24n%24-novelty%2C%20the%0Aproportion%20of%20%24n%24-grams%20generated%20by%20an%20LM%20that%20did%20not%20appear%20in%20the%20training%0Adata%20%28for%20arbitrarily%20large%20%24n%24%29.%20To%20enable%20arbitrary-length%20%24n%24-gram%20search%0Aover%20a%20corpus%20in%20constant%20time%20w.r.t.%20corpus%20size%2C%20we%20develop%20Rusty-DAWG%2C%20a%0Anovel%20search%20tool%20inspired%20by%20indexing%20of%20genomic%20data.%20We%20compare%20the%20novelty%0Aof%20LM-generated%20text%20to%20human-written%20text%20and%20explore%20factors%20that%20affect%0Ageneration%20novelty%2C%20focusing%20on%20the%20Pythia%20models.%20We%20find%20that%2C%20for%20%24n%20%3E%204%24%2C%0ALM-generated%20text%20is%20less%20novel%20than%20human-written%20text%2C%20though%20it%20is%20more%0Anovel%20for%20smaller%20%24n%24.%20Larger%20LMs%20and%20more%20constrained%20decoding%20strategies%20both%0Adecrease%20novelty.%20Finally%2C%20we%20show%20that%20LMs%20complete%20%24n%24-grams%20with%20lower%20loss%0Aif%20they%20are%20more%20frequent%20in%20the%20training%20data.%20Overall%2C%20our%20results%20reveal%0Afactors%20influencing%20the%20novelty%20of%20LM-generated%20text%2C%20and%20we%20release%20Rusty-DAWG%0Ato%20facilitate%20further%20pretraining%20data%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13069v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520%2524n%2524-Gram%2520Novelty%2520of%2520Language%2520Models%2520Using%2520Rusty-DAWG%26entry.906535625%3DWilliam%2520Merrill%2520and%2520Noah%2520A.%2520Smith%2520and%2520Yanai%2520Elazar%26entry.1292438233%3D%2520%2520How%2520novel%2520are%2520texts%2520generated%2520by%2520language%2520models%2520%2528LMs%2529%2520relative%2520to%2520their%250Atraining%2520corpora%253F%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520modern%2520LMs%250Agenerate%2520%2524n%2524-grams%2520from%2520their%2520training%2520data%252C%2520evaluating%2520both%2520%2528i%2529%2520the%250Aprobability%2520LMs%2520assign%2520to%2520complete%2520training%2520%2524n%2524-grams%2520and%2520%2528ii%2529%2520%2524n%2524-novelty%252C%2520the%250Aproportion%2520of%2520%2524n%2524-grams%2520generated%2520by%2520an%2520LM%2520that%2520did%2520not%2520appear%2520in%2520the%2520training%250Adata%2520%2528for%2520arbitrarily%2520large%2520%2524n%2524%2529.%2520To%2520enable%2520arbitrary-length%2520%2524n%2524-gram%2520search%250Aover%2520a%2520corpus%2520in%2520constant%2520time%2520w.r.t.%2520corpus%2520size%252C%2520we%2520develop%2520Rusty-DAWG%252C%2520a%250Anovel%2520search%2520tool%2520inspired%2520by%2520indexing%2520of%2520genomic%2520data.%2520We%2520compare%2520the%2520novelty%250Aof%2520LM-generated%2520text%2520to%2520human-written%2520text%2520and%2520explore%2520factors%2520that%2520affect%250Ageneration%2520novelty%252C%2520focusing%2520on%2520the%2520Pythia%2520models.%2520We%2520find%2520that%252C%2520for%2520%2524n%2520%253E%25204%2524%252C%250ALM-generated%2520text%2520is%2520less%2520novel%2520than%2520human-written%2520text%252C%2520though%2520it%2520is%2520more%250Anovel%2520for%2520smaller%2520%2524n%2524.%2520Larger%2520LMs%2520and%2520more%2520constrained%2520decoding%2520strategies%2520both%250Adecrease%2520novelty.%2520Finally%252C%2520we%2520show%2520that%2520LMs%2520complete%2520%2524n%2524-grams%2520with%2520lower%2520loss%250Aif%2520they%2520are%2520more%2520frequent%2520in%2520the%2520training%2520data.%2520Overall%252C%2520our%2520results%2520reveal%250Afactors%2520influencing%2520the%2520novelty%2520of%2520LM-generated%2520text%252C%2520and%2520we%2520release%2520Rusty-DAWG%250Ato%2520facilitate%2520further%2520pretraining%2520data%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13069v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20%24n%24-Gram%20Novelty%20of%20Language%20Models%20Using%20Rusty-DAWG&entry.906535625=William%20Merrill%20and%20Noah%20A.%20Smith%20and%20Yanai%20Elazar&entry.1292438233=%20%20How%20novel%20are%20texts%20generated%20by%20language%20models%20%28LMs%29%20relative%20to%20their%0Atraining%20corpora%3F%20In%20this%20work%2C%20we%20investigate%20the%20extent%20to%20which%20modern%20LMs%0Agenerate%20%24n%24-grams%20from%20their%20training%20data%2C%20evaluating%20both%20%28i%29%20the%0Aprobability%20LMs%20assign%20to%20complete%20training%20%24n%24-grams%20and%20%28ii%29%20%24n%24-novelty%2C%20the%0Aproportion%20of%20%24n%24-grams%20generated%20by%20an%20LM%20that%20did%20not%20appear%20in%20the%20training%0Adata%20%28for%20arbitrarily%20large%20%24n%24%29.%20To%20enable%20arbitrary-length%20%24n%24-gram%20search%0Aover%20a%20corpus%20in%20constant%20time%20w.r.t.%20corpus%20size%2C%20we%20develop%20Rusty-DAWG%2C%20a%0Anovel%20search%20tool%20inspired%20by%20indexing%20of%20genomic%20data.%20We%20compare%20the%20novelty%0Aof%20LM-generated%20text%20to%20human-written%20text%20and%20explore%20factors%20that%20affect%0Ageneration%20novelty%2C%20focusing%20on%20the%20Pythia%20models.%20We%20find%20that%2C%20for%20%24n%20%3E%204%24%2C%0ALM-generated%20text%20is%20less%20novel%20than%20human-written%20text%2C%20though%20it%20is%20more%0Anovel%20for%20smaller%20%24n%24.%20Larger%20LMs%20and%20more%20constrained%20decoding%20strategies%20both%0Adecrease%20novelty.%20Finally%2C%20we%20show%20that%20LMs%20complete%20%24n%24-grams%20with%20lower%20loss%0Aif%20they%20are%20more%20frequent%20in%20the%20training%20data.%20Overall%2C%20our%20results%20reveal%0Afactors%20influencing%20the%20novelty%20of%20LM-generated%20text%2C%20and%20we%20release%20Rusty-DAWG%0Ato%20facilitate%20further%20pretraining%20data%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13069v3&entry.124074799=Read"},
{"title": "Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs", "author": "Louis Serrano and Armand Kassa\u00ef Koupa\u00ef and Thomas X Wang and Pierre Erbacher and Patrick Gallinari", "abstract": "  Solving time-dependent parametric partial differential equations (PDEs) is\nchallenging, as models must adapt to variations in parameters such as\ncoefficients, forcing terms, and boundary conditions. Data-driven neural\nsolvers either train on data sampled from the PDE parameters distribution in\nthe hope that the model generalizes to new instances or rely on gradient-based\nadaptation and meta-learning to implicitly encode the dynamics from\nobservations. This often comes with increased inference complexity. Inspired by\nthe in-context learning capabilities of large language models (LLMs), we\nintroduce Zebra, a novel generative auto-regressive transformer designed to\nsolve parametric PDEs without requiring gradient adaptation at inference. By\nleveraging in-context information during both pre-training and inference, Zebra\ndynamically adapts to new tasks by conditioning on input sequences that\nincorporate context trajectories or preceding states. This approach enables\nZebra to flexibly handle arbitrarily sized context inputs and supports\nuncertainty quantification through the sampling of multiple solution\ntrajectories. We evaluate Zebra across a variety of challenging PDE scenarios,\ndemonstrating its adaptability, robustness, and superior performance compared\nto existing approaches.\n", "link": "http://arxiv.org/abs/2410.03437v1", "date": "2024-10-04", "relevancy": 2.0327, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5267}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zebra%3A%20In-Context%20and%20Generative%20Pretraining%20for%20Solving%20Parametric%20PDEs&body=Title%3A%20Zebra%3A%20In-Context%20and%20Generative%20Pretraining%20for%20Solving%20Parametric%20PDEs%0AAuthor%3A%20Louis%20Serrano%20and%20Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Thomas%20X%20Wang%20and%20Pierre%20Erbacher%20and%20Patrick%20Gallinari%0AAbstract%3A%20%20%20Solving%20time-dependent%20parametric%20partial%20differential%20equations%20%28PDEs%29%20is%0Achallenging%2C%20as%20models%20must%20adapt%20to%20variations%20in%20parameters%20such%20as%0Acoefficients%2C%20forcing%20terms%2C%20and%20boundary%20conditions.%20Data-driven%20neural%0Asolvers%20either%20train%20on%20data%20sampled%20from%20the%20PDE%20parameters%20distribution%20in%0Athe%20hope%20that%20the%20model%20generalizes%20to%20new%20instances%20or%20rely%20on%20gradient-based%0Aadaptation%20and%20meta-learning%20to%20implicitly%20encode%20the%20dynamics%20from%0Aobservations.%20This%20often%20comes%20with%20increased%20inference%20complexity.%20Inspired%20by%0Athe%20in-context%20learning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%2C%20we%0Aintroduce%20Zebra%2C%20a%20novel%20generative%20auto-regressive%20transformer%20designed%20to%0Asolve%20parametric%20PDEs%20without%20requiring%20gradient%20adaptation%20at%20inference.%20By%0Aleveraging%20in-context%20information%20during%20both%20pre-training%20and%20inference%2C%20Zebra%0Adynamically%20adapts%20to%20new%20tasks%20by%20conditioning%20on%20input%20sequences%20that%0Aincorporate%20context%20trajectories%20or%20preceding%20states.%20This%20approach%20enables%0AZebra%20to%20flexibly%20handle%20arbitrarily%20sized%20context%20inputs%20and%20supports%0Auncertainty%20quantification%20through%20the%20sampling%20of%20multiple%20solution%0Atrajectories.%20We%20evaluate%20Zebra%20across%20a%20variety%20of%20challenging%20PDE%20scenarios%2C%0Ademonstrating%20its%20adaptability%2C%20robustness%2C%20and%20superior%20performance%20compared%0Ato%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZebra%253A%2520In-Context%2520and%2520Generative%2520Pretraining%2520for%2520Solving%2520Parametric%2520PDEs%26entry.906535625%3DLouis%2520Serrano%2520and%2520Armand%2520Kassa%25C3%25AF%2520Koupa%25C3%25AF%2520and%2520Thomas%2520X%2520Wang%2520and%2520Pierre%2520Erbacher%2520and%2520Patrick%2520Gallinari%26entry.1292438233%3D%2520%2520Solving%2520time-dependent%2520parametric%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520is%250Achallenging%252C%2520as%2520models%2520must%2520adapt%2520to%2520variations%2520in%2520parameters%2520such%2520as%250Acoefficients%252C%2520forcing%2520terms%252C%2520and%2520boundary%2520conditions.%2520Data-driven%2520neural%250Asolvers%2520either%2520train%2520on%2520data%2520sampled%2520from%2520the%2520PDE%2520parameters%2520distribution%2520in%250Athe%2520hope%2520that%2520the%2520model%2520generalizes%2520to%2520new%2520instances%2520or%2520rely%2520on%2520gradient-based%250Aadaptation%2520and%2520meta-learning%2520to%2520implicitly%2520encode%2520the%2520dynamics%2520from%250Aobservations.%2520This%2520often%2520comes%2520with%2520increased%2520inference%2520complexity.%2520Inspired%2520by%250Athe%2520in-context%2520learning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520we%250Aintroduce%2520Zebra%252C%2520a%2520novel%2520generative%2520auto-regressive%2520transformer%2520designed%2520to%250Asolve%2520parametric%2520PDEs%2520without%2520requiring%2520gradient%2520adaptation%2520at%2520inference.%2520By%250Aleveraging%2520in-context%2520information%2520during%2520both%2520pre-training%2520and%2520inference%252C%2520Zebra%250Adynamically%2520adapts%2520to%2520new%2520tasks%2520by%2520conditioning%2520on%2520input%2520sequences%2520that%250Aincorporate%2520context%2520trajectories%2520or%2520preceding%2520states.%2520This%2520approach%2520enables%250AZebra%2520to%2520flexibly%2520handle%2520arbitrarily%2520sized%2520context%2520inputs%2520and%2520supports%250Auncertainty%2520quantification%2520through%2520the%2520sampling%2520of%2520multiple%2520solution%250Atrajectories.%2520We%2520evaluate%2520Zebra%2520across%2520a%2520variety%2520of%2520challenging%2520PDE%2520scenarios%252C%250Ademonstrating%2520its%2520adaptability%252C%2520robustness%252C%2520and%2520superior%2520performance%2520compared%250Ato%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zebra%3A%20In-Context%20and%20Generative%20Pretraining%20for%20Solving%20Parametric%20PDEs&entry.906535625=Louis%20Serrano%20and%20Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Thomas%20X%20Wang%20and%20Pierre%20Erbacher%20and%20Patrick%20Gallinari&entry.1292438233=%20%20Solving%20time-dependent%20parametric%20partial%20differential%20equations%20%28PDEs%29%20is%0Achallenging%2C%20as%20models%20must%20adapt%20to%20variations%20in%20parameters%20such%20as%0Acoefficients%2C%20forcing%20terms%2C%20and%20boundary%20conditions.%20Data-driven%20neural%0Asolvers%20either%20train%20on%20data%20sampled%20from%20the%20PDE%20parameters%20distribution%20in%0Athe%20hope%20that%20the%20model%20generalizes%20to%20new%20instances%20or%20rely%20on%20gradient-based%0Aadaptation%20and%20meta-learning%20to%20implicitly%20encode%20the%20dynamics%20from%0Aobservations.%20This%20often%20comes%20with%20increased%20inference%20complexity.%20Inspired%20by%0Athe%20in-context%20learning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%2C%20we%0Aintroduce%20Zebra%2C%20a%20novel%20generative%20auto-regressive%20transformer%20designed%20to%0Asolve%20parametric%20PDEs%20without%20requiring%20gradient%20adaptation%20at%20inference.%20By%0Aleveraging%20in-context%20information%20during%20both%20pre-training%20and%20inference%2C%20Zebra%0Adynamically%20adapts%20to%20new%20tasks%20by%20conditioning%20on%20input%20sequences%20that%0Aincorporate%20context%20trajectories%20or%20preceding%20states.%20This%20approach%20enables%0AZebra%20to%20flexibly%20handle%20arbitrarily%20sized%20context%20inputs%20and%20supports%0Auncertainty%20quantification%20through%20the%20sampling%20of%20multiple%20solution%0Atrajectories.%20We%20evaluate%20Zebra%20across%20a%20variety%20of%20challenging%20PDE%20scenarios%2C%0Ademonstrating%20its%20adaptability%2C%20robustness%2C%20and%20superior%20performance%20compared%0Ato%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03437v1&entry.124074799=Read"},
{"title": "GraphCroc: Cross-Correlation Autoencoder for Graph Structural\n  Reconstruction", "author": "Shijin Duan and Ruyi Ding and Jiaxing He and Aidong Adam Ding and Yunsi Fei and Xiaolin Xu", "abstract": "  Graph-structured data is integral to many applications, prompting the\ndevelopment of various graph representation methods. Graph autoencoders (GAEs),\nin particular, reconstruct graph structures from node embeddings. Current GAE\nmodels primarily utilize self-correlation to represent graph structures and\nfocus on node-level tasks, often overlooking multi-graph scenarios. Our\ntheoretical analysis indicates that self-correlation generally falls short in\naccurately representing specific graph features such as islands, symmetrical\nstructures, and directional edges, particularly in smaller or multiple graph\ncontexts. To address these limitations, we introduce a cross-correlation\nmechanism that significantly enhances the GAE representational capabilities.\nAdditionally, we propose GraphCroc, a new GAE that supports flexible encoder\narchitectures tailored for various downstream tasks and ensures robust\nstructural reconstruction, through a mirrored encoding-decoding process. This\nmodel also tackles the challenge of representation bias during optimization by\nimplementing a loss-balancing strategy. Both theoretical analysis and numerical\nevaluations demonstrate that our methodology significantly outperforms existing\nself-correlation-based GAEs in graph structure reconstruction.\n", "link": "http://arxiv.org/abs/2410.03396v1", "date": "2024-10-04", "relevancy": 2.0299, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.522}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4989}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphCroc%3A%20Cross-Correlation%20Autoencoder%20for%20Graph%20Structural%0A%20%20Reconstruction&body=Title%3A%20GraphCroc%3A%20Cross-Correlation%20Autoencoder%20for%20Graph%20Structural%0A%20%20Reconstruction%0AAuthor%3A%20Shijin%20Duan%20and%20Ruyi%20Ding%20and%20Jiaxing%20He%20and%20Aidong%20Adam%20Ding%20and%20Yunsi%20Fei%20and%20Xiaolin%20Xu%0AAbstract%3A%20%20%20Graph-structured%20data%20is%20integral%20to%20many%20applications%2C%20prompting%20the%0Adevelopment%20of%20various%20graph%20representation%20methods.%20Graph%20autoencoders%20%28GAEs%29%2C%0Ain%20particular%2C%20reconstruct%20graph%20structures%20from%20node%20embeddings.%20Current%20GAE%0Amodels%20primarily%20utilize%20self-correlation%20to%20represent%20graph%20structures%20and%0Afocus%20on%20node-level%20tasks%2C%20often%20overlooking%20multi-graph%20scenarios.%20Our%0Atheoretical%20analysis%20indicates%20that%20self-correlation%20generally%20falls%20short%20in%0Aaccurately%20representing%20specific%20graph%20features%20such%20as%20islands%2C%20symmetrical%0Astructures%2C%20and%20directional%20edges%2C%20particularly%20in%20smaller%20or%20multiple%20graph%0Acontexts.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20cross-correlation%0Amechanism%20that%20significantly%20enhances%20the%20GAE%20representational%20capabilities.%0AAdditionally%2C%20we%20propose%20GraphCroc%2C%20a%20new%20GAE%20that%20supports%20flexible%20encoder%0Aarchitectures%20tailored%20for%20various%20downstream%20tasks%20and%20ensures%20robust%0Astructural%20reconstruction%2C%20through%20a%20mirrored%20encoding-decoding%20process.%20This%0Amodel%20also%20tackles%20the%20challenge%20of%20representation%20bias%20during%20optimization%20by%0Aimplementing%20a%20loss-balancing%20strategy.%20Both%20theoretical%20analysis%20and%20numerical%0Aevaluations%20demonstrate%20that%20our%20methodology%20significantly%20outperforms%20existing%0Aself-correlation-based%20GAEs%20in%20graph%20structure%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphCroc%253A%2520Cross-Correlation%2520Autoencoder%2520for%2520Graph%2520Structural%250A%2520%2520Reconstruction%26entry.906535625%3DShijin%2520Duan%2520and%2520Ruyi%2520Ding%2520and%2520Jiaxing%2520He%2520and%2520Aidong%2520Adam%2520Ding%2520and%2520Yunsi%2520Fei%2520and%2520Xiaolin%2520Xu%26entry.1292438233%3D%2520%2520Graph-structured%2520data%2520is%2520integral%2520to%2520many%2520applications%252C%2520prompting%2520the%250Adevelopment%2520of%2520various%2520graph%2520representation%2520methods.%2520Graph%2520autoencoders%2520%2528GAEs%2529%252C%250Ain%2520particular%252C%2520reconstruct%2520graph%2520structures%2520from%2520node%2520embeddings.%2520Current%2520GAE%250Amodels%2520primarily%2520utilize%2520self-correlation%2520to%2520represent%2520graph%2520structures%2520and%250Afocus%2520on%2520node-level%2520tasks%252C%2520often%2520overlooking%2520multi-graph%2520scenarios.%2520Our%250Atheoretical%2520analysis%2520indicates%2520that%2520self-correlation%2520generally%2520falls%2520short%2520in%250Aaccurately%2520representing%2520specific%2520graph%2520features%2520such%2520as%2520islands%252C%2520symmetrical%250Astructures%252C%2520and%2520directional%2520edges%252C%2520particularly%2520in%2520smaller%2520or%2520multiple%2520graph%250Acontexts.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520a%2520cross-correlation%250Amechanism%2520that%2520significantly%2520enhances%2520the%2520GAE%2520representational%2520capabilities.%250AAdditionally%252C%2520we%2520propose%2520GraphCroc%252C%2520a%2520new%2520GAE%2520that%2520supports%2520flexible%2520encoder%250Aarchitectures%2520tailored%2520for%2520various%2520downstream%2520tasks%2520and%2520ensures%2520robust%250Astructural%2520reconstruction%252C%2520through%2520a%2520mirrored%2520encoding-decoding%2520process.%2520This%250Amodel%2520also%2520tackles%2520the%2520challenge%2520of%2520representation%2520bias%2520during%2520optimization%2520by%250Aimplementing%2520a%2520loss-balancing%2520strategy.%2520Both%2520theoretical%2520analysis%2520and%2520numerical%250Aevaluations%2520demonstrate%2520that%2520our%2520methodology%2520significantly%2520outperforms%2520existing%250Aself-correlation-based%2520GAEs%2520in%2520graph%2520structure%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphCroc%3A%20Cross-Correlation%20Autoencoder%20for%20Graph%20Structural%0A%20%20Reconstruction&entry.906535625=Shijin%20Duan%20and%20Ruyi%20Ding%20and%20Jiaxing%20He%20and%20Aidong%20Adam%20Ding%20and%20Yunsi%20Fei%20and%20Xiaolin%20Xu&entry.1292438233=%20%20Graph-structured%20data%20is%20integral%20to%20many%20applications%2C%20prompting%20the%0Adevelopment%20of%20various%20graph%20representation%20methods.%20Graph%20autoencoders%20%28GAEs%29%2C%0Ain%20particular%2C%20reconstruct%20graph%20structures%20from%20node%20embeddings.%20Current%20GAE%0Amodels%20primarily%20utilize%20self-correlation%20to%20represent%20graph%20structures%20and%0Afocus%20on%20node-level%20tasks%2C%20often%20overlooking%20multi-graph%20scenarios.%20Our%0Atheoretical%20analysis%20indicates%20that%20self-correlation%20generally%20falls%20short%20in%0Aaccurately%20representing%20specific%20graph%20features%20such%20as%20islands%2C%20symmetrical%0Astructures%2C%20and%20directional%20edges%2C%20particularly%20in%20smaller%20or%20multiple%20graph%0Acontexts.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20cross-correlation%0Amechanism%20that%20significantly%20enhances%20the%20GAE%20representational%20capabilities.%0AAdditionally%2C%20we%20propose%20GraphCroc%2C%20a%20new%20GAE%20that%20supports%20flexible%20encoder%0Aarchitectures%20tailored%20for%20various%20downstream%20tasks%20and%20ensures%20robust%0Astructural%20reconstruction%2C%20through%20a%20mirrored%20encoding-decoding%20process.%20This%0Amodel%20also%20tackles%20the%20challenge%20of%20representation%20bias%20during%20optimization%20by%0Aimplementing%20a%20loss-balancing%20strategy.%20Both%20theoretical%20analysis%20and%20numerical%0Aevaluations%20demonstrate%20that%20our%20methodology%20significantly%20outperforms%20existing%0Aself-correlation-based%20GAEs%20in%20graph%20structure%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03396v1&entry.124074799=Read"},
{"title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces", "author": "Yihuai Hong and Lei Yu and Haiqin Yang and Shauli Ravfogel and Mor Geva", "abstract": "  The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors.\n", "link": "http://arxiv.org/abs/2406.11614v2", "date": "2024-10-04", "relevancy": 2.0287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Evaluation%20of%20Unlearning%20Using%20Parametric%20Knowledge%20Traces&body=Title%3A%20Intrinsic%20Evaluation%20of%20Unlearning%20Using%20Parametric%20Knowledge%20Traces%0AAuthor%3A%20Yihuai%20Hong%20and%20Lei%20Yu%20and%20Haiqin%20Yang%20and%20Shauli%20Ravfogel%20and%20Mor%20Geva%0AAbstract%3A%20%20%20The%20task%20of%20%22unlearning%22%20certain%20concepts%20in%20large%20language%20models%20%28LLMs%29%20has%0Aattracted%20immense%20attention%20recently%2C%20due%20to%20its%20importance%20in%20mitigating%0Aundesirable%20model%20behaviours%2C%20such%20as%20the%20generation%20of%20harmful%2C%20private%2C%20or%0Aincorrect%20information.%20Current%20protocols%20to%20evaluate%20unlearning%20methods%20largely%0Arely%20on%20behavioral%20tests%2C%20without%20monitoring%20the%20presence%20of%20unlearned%0Aknowledge%20within%20the%20model%27s%20parameters.%20This%20residual%20knowledge%20can%20be%0Aadversarially%20exploited%20to%20recover%20the%20erased%20information%20post-unlearning.%20We%0Aargue%20that%20unlearning%20should%20also%20be%20evaluated%20internally%2C%20by%20considering%0Achanges%20in%20the%20parametric%20knowledge%20traces%20of%20the%20unlearned%20concepts.%20To%20this%0Aend%2C%20we%20propose%20a%20general%20evaluation%20methodology%20that%20leverages%20vocabulary%0Aprojections%20to%20inspect%20concepts%20encoded%20in%20model%20parameters.%20We%20use%20this%0Aapproach%20to%20localize%20%22concept%20vectors%22%20-%20parameter%20vectors%20that%20encode%20concrete%0Aconcepts%20-%20and%20construct%20ConceptVectors%2C%20a%20benchmark%20dataset%20containing%0Ahundreds%20of%20common%20concepts%20and%20their%20parametric%20knowledge%20traces%20within%20two%0Aopen-source%20LLMs.%20Evaluation%20on%20ConceptVectors%20shows%20that%20existing%20unlearning%0Amethods%20minimally%20impact%20concept%20vectors%20and%20mostly%20suppress%20them%20during%0Ainference%2C%20while%20directly%20ablating%20these%20vectors%20demonstrably%20removes%20the%0Aassociated%20knowledge%20and%20significantly%20reduces%20the%20model%27s%20susceptibility%20to%0Aadversarial%20manipulation.%20Our%20results%20highlight%20limitations%20in%20behavioral-based%0Aunlearning%20evaluations%20and%20call%20for%20future%20work%20to%20include%20parameter-based%0Aevaluations.%20To%20support%20this%2C%20we%20release%20our%20code%20and%20benchmark%20at%0Ahttps%3A//github.com/yihuaihong/ConceptVectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Evaluation%2520of%2520Unlearning%2520Using%2520Parametric%2520Knowledge%2520Traces%26entry.906535625%3DYihuai%2520Hong%2520and%2520Lei%2520Yu%2520and%2520Haiqin%2520Yang%2520and%2520Shauli%2520Ravfogel%2520and%2520Mor%2520Geva%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520%2522unlearning%2522%2520certain%2520concepts%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%250Aattracted%2520immense%2520attention%2520recently%252C%2520due%2520to%2520its%2520importance%2520in%2520mitigating%250Aundesirable%2520model%2520behaviours%252C%2520such%2520as%2520the%2520generation%2520of%2520harmful%252C%2520private%252C%2520or%250Aincorrect%2520information.%2520Current%2520protocols%2520to%2520evaluate%2520unlearning%2520methods%2520largely%250Arely%2520on%2520behavioral%2520tests%252C%2520without%2520monitoring%2520the%2520presence%2520of%2520unlearned%250Aknowledge%2520within%2520the%2520model%2527s%2520parameters.%2520This%2520residual%2520knowledge%2520can%2520be%250Aadversarially%2520exploited%2520to%2520recover%2520the%2520erased%2520information%2520post-unlearning.%2520We%250Aargue%2520that%2520unlearning%2520should%2520also%2520be%2520evaluated%2520internally%252C%2520by%2520considering%250Achanges%2520in%2520the%2520parametric%2520knowledge%2520traces%2520of%2520the%2520unlearned%2520concepts.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520general%2520evaluation%2520methodology%2520that%2520leverages%2520vocabulary%250Aprojections%2520to%2520inspect%2520concepts%2520encoded%2520in%2520model%2520parameters.%2520We%2520use%2520this%250Aapproach%2520to%2520localize%2520%2522concept%2520vectors%2522%2520-%2520parameter%2520vectors%2520that%2520encode%2520concrete%250Aconcepts%2520-%2520and%2520construct%2520ConceptVectors%252C%2520a%2520benchmark%2520dataset%2520containing%250Ahundreds%2520of%2520common%2520concepts%2520and%2520their%2520parametric%2520knowledge%2520traces%2520within%2520two%250Aopen-source%2520LLMs.%2520Evaluation%2520on%2520ConceptVectors%2520shows%2520that%2520existing%2520unlearning%250Amethods%2520minimally%2520impact%2520concept%2520vectors%2520and%2520mostly%2520suppress%2520them%2520during%250Ainference%252C%2520while%2520directly%2520ablating%2520these%2520vectors%2520demonstrably%2520removes%2520the%250Aassociated%2520knowledge%2520and%2520significantly%2520reduces%2520the%2520model%2527s%2520susceptibility%2520to%250Aadversarial%2520manipulation.%2520Our%2520results%2520highlight%2520limitations%2520in%2520behavioral-based%250Aunlearning%2520evaluations%2520and%2520call%2520for%2520future%2520work%2520to%2520include%2520parameter-based%250Aevaluations.%2520To%2520support%2520this%252C%2520we%2520release%2520our%2520code%2520and%2520benchmark%2520at%250Ahttps%253A//github.com/yihuaihong/ConceptVectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Evaluation%20of%20Unlearning%20Using%20Parametric%20Knowledge%20Traces&entry.906535625=Yihuai%20Hong%20and%20Lei%20Yu%20and%20Haiqin%20Yang%20and%20Shauli%20Ravfogel%20and%20Mor%20Geva&entry.1292438233=%20%20The%20task%20of%20%22unlearning%22%20certain%20concepts%20in%20large%20language%20models%20%28LLMs%29%20has%0Aattracted%20immense%20attention%20recently%2C%20due%20to%20its%20importance%20in%20mitigating%0Aundesirable%20model%20behaviours%2C%20such%20as%20the%20generation%20of%20harmful%2C%20private%2C%20or%0Aincorrect%20information.%20Current%20protocols%20to%20evaluate%20unlearning%20methods%20largely%0Arely%20on%20behavioral%20tests%2C%20without%20monitoring%20the%20presence%20of%20unlearned%0Aknowledge%20within%20the%20model%27s%20parameters.%20This%20residual%20knowledge%20can%20be%0Aadversarially%20exploited%20to%20recover%20the%20erased%20information%20post-unlearning.%20We%0Aargue%20that%20unlearning%20should%20also%20be%20evaluated%20internally%2C%20by%20considering%0Achanges%20in%20the%20parametric%20knowledge%20traces%20of%20the%20unlearned%20concepts.%20To%20this%0Aend%2C%20we%20propose%20a%20general%20evaluation%20methodology%20that%20leverages%20vocabulary%0Aprojections%20to%20inspect%20concepts%20encoded%20in%20model%20parameters.%20We%20use%20this%0Aapproach%20to%20localize%20%22concept%20vectors%22%20-%20parameter%20vectors%20that%20encode%20concrete%0Aconcepts%20-%20and%20construct%20ConceptVectors%2C%20a%20benchmark%20dataset%20containing%0Ahundreds%20of%20common%20concepts%20and%20their%20parametric%20knowledge%20traces%20within%20two%0Aopen-source%20LLMs.%20Evaluation%20on%20ConceptVectors%20shows%20that%20existing%20unlearning%0Amethods%20minimally%20impact%20concept%20vectors%20and%20mostly%20suppress%20them%20during%0Ainference%2C%20while%20directly%20ablating%20these%20vectors%20demonstrably%20removes%20the%0Aassociated%20knowledge%20and%20significantly%20reduces%20the%20model%27s%20susceptibility%20to%0Aadversarial%20manipulation.%20Our%20results%20highlight%20limitations%20in%20behavioral-based%0Aunlearning%20evaluations%20and%20call%20for%20future%20work%20to%20include%20parameter-based%0Aevaluations.%20To%20support%20this%2C%20we%20release%20our%20code%20and%20benchmark%20at%0Ahttps%3A//github.com/yihuaihong/ConceptVectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11614v2&entry.124074799=Read"},
{"title": "DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video\n  Vessel Segmentation", "author": "Chun-Hung Wu and Shih-Hong Chen and Chih-Yao Hu and Hsin-Yu Wu and Kai-Hsin Chen and Yu-You Chen and Chih-Hai Su and Chih-Kuo Lee and Yu-Lun Liu", "abstract": "  This paper presents Deformable Neural Vessel Representations (DeNVeR), an\nunsupervised approach for vessel segmentation in X-ray videos without annotated\nground truth. DeNVeR uses optical flow and layer separation, enhancing\nsegmentation accuracy and adaptability through test-time training. A key\ncomponent of our research is the introduction of the XACV dataset, the first\nX-ray angiography coronary video dataset with high-quality, manually labeled\nsegmentation ground truth. Our evaluation demonstrates that DeNVeR outperforms\ncurrent state-of-the-art methods in vessel segmentation. This paper marks an\nadvance in medical imaging, providing a robust, data-efficient tool for disease\ndiagnosis and treatment planning and setting a new standard for future research\nin video vessel segmentation. See our project page for video results at\nhttps://kirito878.github.io/DeNVeR/.\n", "link": "http://arxiv.org/abs/2406.01591v2", "date": "2024-10-04", "relevancy": 2.0223, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5169}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4989}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeNVeR%3A%20Deformable%20Neural%20Vessel%20Representations%20for%20Unsupervised%20Video%0A%20%20Vessel%20Segmentation&body=Title%3A%20DeNVeR%3A%20Deformable%20Neural%20Vessel%20Representations%20for%20Unsupervised%20Video%0A%20%20Vessel%20Segmentation%0AAuthor%3A%20Chun-Hung%20Wu%20and%20Shih-Hong%20Chen%20and%20Chih-Yao%20Hu%20and%20Hsin-Yu%20Wu%20and%20Kai-Hsin%20Chen%20and%20Yu-You%20Chen%20and%20Chih-Hai%20Su%20and%20Chih-Kuo%20Lee%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20This%20paper%20presents%20Deformable%20Neural%20Vessel%20Representations%20%28DeNVeR%29%2C%20an%0Aunsupervised%20approach%20for%20vessel%20segmentation%20in%20X-ray%20videos%20without%20annotated%0Aground%20truth.%20DeNVeR%20uses%20optical%20flow%20and%20layer%20separation%2C%20enhancing%0Asegmentation%20accuracy%20and%20adaptability%20through%20test-time%20training.%20A%20key%0Acomponent%20of%20our%20research%20is%20the%20introduction%20of%20the%20XACV%20dataset%2C%20the%20first%0AX-ray%20angiography%20coronary%20video%20dataset%20with%20high-quality%2C%20manually%20labeled%0Asegmentation%20ground%20truth.%20Our%20evaluation%20demonstrates%20that%20DeNVeR%20outperforms%0Acurrent%20state-of-the-art%20methods%20in%20vessel%20segmentation.%20This%20paper%20marks%20an%0Aadvance%20in%20medical%20imaging%2C%20providing%20a%20robust%2C%20data-efficient%20tool%20for%20disease%0Adiagnosis%20and%20treatment%20planning%20and%20setting%20a%20new%20standard%20for%20future%20research%0Ain%20video%20vessel%20segmentation.%20See%20our%20project%20page%20for%20video%20results%20at%0Ahttps%3A//kirito878.github.io/DeNVeR/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01591v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeNVeR%253A%2520Deformable%2520Neural%2520Vessel%2520Representations%2520for%2520Unsupervised%2520Video%250A%2520%2520Vessel%2520Segmentation%26entry.906535625%3DChun-Hung%2520Wu%2520and%2520Shih-Hong%2520Chen%2520and%2520Chih-Yao%2520Hu%2520and%2520Hsin-Yu%2520Wu%2520and%2520Kai-Hsin%2520Chen%2520and%2520Yu-You%2520Chen%2520and%2520Chih-Hai%2520Su%2520and%2520Chih-Kuo%2520Lee%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Deformable%2520Neural%2520Vessel%2520Representations%2520%2528DeNVeR%2529%252C%2520an%250Aunsupervised%2520approach%2520for%2520vessel%2520segmentation%2520in%2520X-ray%2520videos%2520without%2520annotated%250Aground%2520truth.%2520DeNVeR%2520uses%2520optical%2520flow%2520and%2520layer%2520separation%252C%2520enhancing%250Asegmentation%2520accuracy%2520and%2520adaptability%2520through%2520test-time%2520training.%2520A%2520key%250Acomponent%2520of%2520our%2520research%2520is%2520the%2520introduction%2520of%2520the%2520XACV%2520dataset%252C%2520the%2520first%250AX-ray%2520angiography%2520coronary%2520video%2520dataset%2520with%2520high-quality%252C%2520manually%2520labeled%250Asegmentation%2520ground%2520truth.%2520Our%2520evaluation%2520demonstrates%2520that%2520DeNVeR%2520outperforms%250Acurrent%2520state-of-the-art%2520methods%2520in%2520vessel%2520segmentation.%2520This%2520paper%2520marks%2520an%250Aadvance%2520in%2520medical%2520imaging%252C%2520providing%2520a%2520robust%252C%2520data-efficient%2520tool%2520for%2520disease%250Adiagnosis%2520and%2520treatment%2520planning%2520and%2520setting%2520a%2520new%2520standard%2520for%2520future%2520research%250Ain%2520video%2520vessel%2520segmentation.%2520See%2520our%2520project%2520page%2520for%2520video%2520results%2520at%250Ahttps%253A//kirito878.github.io/DeNVeR/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01591v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeNVeR%3A%20Deformable%20Neural%20Vessel%20Representations%20for%20Unsupervised%20Video%0A%20%20Vessel%20Segmentation&entry.906535625=Chun-Hung%20Wu%20and%20Shih-Hong%20Chen%20and%20Chih-Yao%20Hu%20and%20Hsin-Yu%20Wu%20and%20Kai-Hsin%20Chen%20and%20Yu-You%20Chen%20and%20Chih-Hai%20Su%20and%20Chih-Kuo%20Lee%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20This%20paper%20presents%20Deformable%20Neural%20Vessel%20Representations%20%28DeNVeR%29%2C%20an%0Aunsupervised%20approach%20for%20vessel%20segmentation%20in%20X-ray%20videos%20without%20annotated%0Aground%20truth.%20DeNVeR%20uses%20optical%20flow%20and%20layer%20separation%2C%20enhancing%0Asegmentation%20accuracy%20and%20adaptability%20through%20test-time%20training.%20A%20key%0Acomponent%20of%20our%20research%20is%20the%20introduction%20of%20the%20XACV%20dataset%2C%20the%20first%0AX-ray%20angiography%20coronary%20video%20dataset%20with%20high-quality%2C%20manually%20labeled%0Asegmentation%20ground%20truth.%20Our%20evaluation%20demonstrates%20that%20DeNVeR%20outperforms%0Acurrent%20state-of-the-art%20methods%20in%20vessel%20segmentation.%20This%20paper%20marks%20an%0Aadvance%20in%20medical%20imaging%2C%20providing%20a%20robust%2C%20data-efficient%20tool%20for%20disease%0Adiagnosis%20and%20treatment%20planning%20and%20setting%20a%20new%20standard%20for%20future%20research%0Ain%20video%20vessel%20segmentation.%20See%20our%20project%20page%20for%20video%20results%20at%0Ahttps%3A//kirito878.github.io/DeNVeR/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01591v2&entry.124074799=Read"},
{"title": "Language Model Empowered Spatio-Temporal Forecasting via Physics-Aware\n  Reprogramming", "author": "Hao Wang and Jindong Han and Wei Fan and Hao Liu", "abstract": "  Spatio-temporal forecasting is pivotal in numerous real-world applications,\nincluding transportation planning, energy management, and climate monitoring.\nIn this work, we aim to harness the reasoning and generalization abilities of\nPre-trained Language Models (PLMs) for more effective spatio-temporal\nforecasting, particularly in data-scarce scenarios. However, recent studies\nuncover that PLMs, which are primarily trained on textual data, often falter\nwhen tasked with modeling the intricate correlations in numerical time series,\nthereby limiting their effectiveness in comprehending spatio-temporal data. To\nbridge the gap, we propose RePST, a physics-aware PLM reprogramming framework\ntailored for spatio-temporal forecasting. Specifically, we first propose a\nphysics-aware decomposer that adaptively disentangles spatially correlated time\nseries into interpretable sub-components, which facilitates PLM to understand\nsophisticated spatio-temporal dynamics via a divide-and-conquer strategy.\nMoreover, we propose a selective discrete reprogramming scheme, which\nintroduces an expanded spatio-temporal vocabulary space to project\nspatio-temporal series into discrete representations. This scheme minimizes the\ninformation loss during reprogramming and enriches the representations derived\nby PLMs. Extensive experiments on real-world datasets show that the proposed\nRePST outperforms twelve state-of-the-art baseline methods, particularly in\ndata-scarce scenarios, highlighting the effectiveness and superior\ngeneralization capabilities of PLMs for spatio-temporal forecasting.\n", "link": "http://arxiv.org/abs/2408.14505v2", "date": "2024-10-04", "relevancy": 2.0178, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Model%20Empowered%20Spatio-Temporal%20Forecasting%20via%20Physics-Aware%0A%20%20Reprogramming&body=Title%3A%20Language%20Model%20Empowered%20Spatio-Temporal%20Forecasting%20via%20Physics-Aware%0A%20%20Reprogramming%0AAuthor%3A%20Hao%20Wang%20and%20Jindong%20Han%20and%20Wei%20Fan%20and%20Hao%20Liu%0AAbstract%3A%20%20%20Spatio-temporal%20forecasting%20is%20pivotal%20in%20numerous%20real-world%20applications%2C%0Aincluding%20transportation%20planning%2C%20energy%20management%2C%20and%20climate%20monitoring.%0AIn%20this%20work%2C%20we%20aim%20to%20harness%20the%20reasoning%20and%20generalization%20abilities%20of%0APre-trained%20Language%20Models%20%28PLMs%29%20for%20more%20effective%20spatio-temporal%0Aforecasting%2C%20particularly%20in%20data-scarce%20scenarios.%20However%2C%20recent%20studies%0Auncover%20that%20PLMs%2C%20which%20are%20primarily%20trained%20on%20textual%20data%2C%20often%20falter%0Awhen%20tasked%20with%20modeling%20the%20intricate%20correlations%20in%20numerical%20time%20series%2C%0Athereby%20limiting%20their%20effectiveness%20in%20comprehending%20spatio-temporal%20data.%20To%0Abridge%20the%20gap%2C%20we%20propose%20RePST%2C%20a%20physics-aware%20PLM%20reprogramming%20framework%0Atailored%20for%20spatio-temporal%20forecasting.%20Specifically%2C%20we%20first%20propose%20a%0Aphysics-aware%20decomposer%20that%20adaptively%20disentangles%20spatially%20correlated%20time%0Aseries%20into%20interpretable%20sub-components%2C%20which%20facilitates%20PLM%20to%20understand%0Asophisticated%20spatio-temporal%20dynamics%20via%20a%20divide-and-conquer%20strategy.%0AMoreover%2C%20we%20propose%20a%20selective%20discrete%20reprogramming%20scheme%2C%20which%0Aintroduces%20an%20expanded%20spatio-temporal%20vocabulary%20space%20to%20project%0Aspatio-temporal%20series%20into%20discrete%20representations.%20This%20scheme%20minimizes%20the%0Ainformation%20loss%20during%20reprogramming%20and%20enriches%20the%20representations%20derived%0Aby%20PLMs.%20Extensive%20experiments%20on%20real-world%20datasets%20show%20that%20the%20proposed%0ARePST%20outperforms%20twelve%20state-of-the-art%20baseline%20methods%2C%20particularly%20in%0Adata-scarce%20scenarios%2C%20highlighting%20the%20effectiveness%20and%20superior%0Ageneralization%20capabilities%20of%20PLMs%20for%20spatio-temporal%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Model%2520Empowered%2520Spatio-Temporal%2520Forecasting%2520via%2520Physics-Aware%250A%2520%2520Reprogramming%26entry.906535625%3DHao%2520Wang%2520and%2520Jindong%2520Han%2520and%2520Wei%2520Fan%2520and%2520Hao%2520Liu%26entry.1292438233%3D%2520%2520Spatio-temporal%2520forecasting%2520is%2520pivotal%2520in%2520numerous%2520real-world%2520applications%252C%250Aincluding%2520transportation%2520planning%252C%2520energy%2520management%252C%2520and%2520climate%2520monitoring.%250AIn%2520this%2520work%252C%2520we%2520aim%2520to%2520harness%2520the%2520reasoning%2520and%2520generalization%2520abilities%2520of%250APre-trained%2520Language%2520Models%2520%2528PLMs%2529%2520for%2520more%2520effective%2520spatio-temporal%250Aforecasting%252C%2520particularly%2520in%2520data-scarce%2520scenarios.%2520However%252C%2520recent%2520studies%250Auncover%2520that%2520PLMs%252C%2520which%2520are%2520primarily%2520trained%2520on%2520textual%2520data%252C%2520often%2520falter%250Awhen%2520tasked%2520with%2520modeling%2520the%2520intricate%2520correlations%2520in%2520numerical%2520time%2520series%252C%250Athereby%2520limiting%2520their%2520effectiveness%2520in%2520comprehending%2520spatio-temporal%2520data.%2520To%250Abridge%2520the%2520gap%252C%2520we%2520propose%2520RePST%252C%2520a%2520physics-aware%2520PLM%2520reprogramming%2520framework%250Atailored%2520for%2520spatio-temporal%2520forecasting.%2520Specifically%252C%2520we%2520first%2520propose%2520a%250Aphysics-aware%2520decomposer%2520that%2520adaptively%2520disentangles%2520spatially%2520correlated%2520time%250Aseries%2520into%2520interpretable%2520sub-components%252C%2520which%2520facilitates%2520PLM%2520to%2520understand%250Asophisticated%2520spatio-temporal%2520dynamics%2520via%2520a%2520divide-and-conquer%2520strategy.%250AMoreover%252C%2520we%2520propose%2520a%2520selective%2520discrete%2520reprogramming%2520scheme%252C%2520which%250Aintroduces%2520an%2520expanded%2520spatio-temporal%2520vocabulary%2520space%2520to%2520project%250Aspatio-temporal%2520series%2520into%2520discrete%2520representations.%2520This%2520scheme%2520minimizes%2520the%250Ainformation%2520loss%2520during%2520reprogramming%2520and%2520enriches%2520the%2520representations%2520derived%250Aby%2520PLMs.%2520Extensive%2520experiments%2520on%2520real-world%2520datasets%2520show%2520that%2520the%2520proposed%250ARePST%2520outperforms%2520twelve%2520state-of-the-art%2520baseline%2520methods%252C%2520particularly%2520in%250Adata-scarce%2520scenarios%252C%2520highlighting%2520the%2520effectiveness%2520and%2520superior%250Ageneralization%2520capabilities%2520of%2520PLMs%2520for%2520spatio-temporal%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Model%20Empowered%20Spatio-Temporal%20Forecasting%20via%20Physics-Aware%0A%20%20Reprogramming&entry.906535625=Hao%20Wang%20and%20Jindong%20Han%20and%20Wei%20Fan%20and%20Hao%20Liu&entry.1292438233=%20%20Spatio-temporal%20forecasting%20is%20pivotal%20in%20numerous%20real-world%20applications%2C%0Aincluding%20transportation%20planning%2C%20energy%20management%2C%20and%20climate%20monitoring.%0AIn%20this%20work%2C%20we%20aim%20to%20harness%20the%20reasoning%20and%20generalization%20abilities%20of%0APre-trained%20Language%20Models%20%28PLMs%29%20for%20more%20effective%20spatio-temporal%0Aforecasting%2C%20particularly%20in%20data-scarce%20scenarios.%20However%2C%20recent%20studies%0Auncover%20that%20PLMs%2C%20which%20are%20primarily%20trained%20on%20textual%20data%2C%20often%20falter%0Awhen%20tasked%20with%20modeling%20the%20intricate%20correlations%20in%20numerical%20time%20series%2C%0Athereby%20limiting%20their%20effectiveness%20in%20comprehending%20spatio-temporal%20data.%20To%0Abridge%20the%20gap%2C%20we%20propose%20RePST%2C%20a%20physics-aware%20PLM%20reprogramming%20framework%0Atailored%20for%20spatio-temporal%20forecasting.%20Specifically%2C%20we%20first%20propose%20a%0Aphysics-aware%20decomposer%20that%20adaptively%20disentangles%20spatially%20correlated%20time%0Aseries%20into%20interpretable%20sub-components%2C%20which%20facilitates%20PLM%20to%20understand%0Asophisticated%20spatio-temporal%20dynamics%20via%20a%20divide-and-conquer%20strategy.%0AMoreover%2C%20we%20propose%20a%20selective%20discrete%20reprogramming%20scheme%2C%20which%0Aintroduces%20an%20expanded%20spatio-temporal%20vocabulary%20space%20to%20project%0Aspatio-temporal%20series%20into%20discrete%20representations.%20This%20scheme%20minimizes%20the%0Ainformation%20loss%20during%20reprogramming%20and%20enriches%20the%20representations%20derived%0Aby%20PLMs.%20Extensive%20experiments%20on%20real-world%20datasets%20show%20that%20the%20proposed%0ARePST%20outperforms%20twelve%20state-of-the-art%20baseline%20methods%2C%20particularly%20in%0Adata-scarce%20scenarios%2C%20highlighting%20the%20effectiveness%20and%20superior%0Ageneralization%20capabilities%20of%20PLMs%20for%20spatio-temporal%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14505v2&entry.124074799=Read"},
{"title": "To Know or Not To Know? Analyzing Self-Consistency of Large Language\n  Models under Ambiguity", "author": "Anastasiia Sedova and Robert Litschko and Diego Frassinelli and Benjamin Roth and Barbara Plank", "abstract": "  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. This paper focuses\non entity type ambiguity, analyzing the proficiency and consistency of\nstate-of-the-art LLMs in applying factual knowledge when prompted with\nambiguous entities. To do so, we propose an evaluation protocol that\ndisentangles knowing from applying knowledge, and test state-of-the-art LLMs on\n49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing\nthe correct entity reading, achieving an average accuracy of only 85%, and as\nlow as 75% with underspecified prompts. The results also reveal systematic\ndiscrepancies in LLM behavior, showing that while the models may possess\nknowledge, they struggle to apply it consistently, exhibit biases toward\npreferred readings, and display self-inconsistencies. This highlights the need\nto address entity ambiguity in the future for more trustworthy LLMs.\n", "link": "http://arxiv.org/abs/2407.17125v3", "date": "2024-10-04", "relevancy": 2.0136, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5191}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Know%20or%20Not%20To%20Know%3F%20Analyzing%20Self-Consistency%20of%20Large%20Language%0A%20%20Models%20under%20Ambiguity&body=Title%3A%20To%20Know%20or%20Not%20To%20Know%3F%20Analyzing%20Self-Consistency%20of%20Large%20Language%0A%20%20Models%20under%20Ambiguity%0AAuthor%3A%20Anastasiia%20Sedova%20and%20Robert%20Litschko%20and%20Diego%20Frassinelli%20and%20Benjamin%20Roth%20and%20Barbara%20Plank%0AAbstract%3A%20%20%20One%20of%20the%20major%20aspects%20contributing%20to%20the%20striking%20performance%20of%20large%0Alanguage%20models%20%28LLMs%29%20is%20the%20vast%20amount%20of%20factual%20knowledge%20accumulated%0Aduring%20pre-training.%20Yet%2C%20many%20LLMs%20suffer%20from%20self-inconsistency%2C%20which%0Araises%20doubts%20about%20their%20trustworthiness%20and%20reliability.%20This%20paper%20focuses%0Aon%20entity%20type%20ambiguity%2C%20analyzing%20the%20proficiency%20and%20consistency%20of%0Astate-of-the-art%20LLMs%20in%20applying%20factual%20knowledge%20when%20prompted%20with%0Aambiguous%20entities.%20To%20do%20so%2C%20we%20propose%20an%20evaluation%20protocol%20that%0Adisentangles%20knowing%20from%20applying%20knowledge%2C%20and%20test%20state-of-the-art%20LLMs%20on%0A49%20ambiguous%20entities.%20Our%20experiments%20reveal%20that%20LLMs%20struggle%20with%20choosing%0Athe%20correct%20entity%20reading%2C%20achieving%20an%20average%20accuracy%20of%20only%2085%25%2C%20and%20as%0Alow%20as%2075%25%20with%20underspecified%20prompts.%20The%20results%20also%20reveal%20systematic%0Adiscrepancies%20in%20LLM%20behavior%2C%20showing%20that%20while%20the%20models%20may%20possess%0Aknowledge%2C%20they%20struggle%20to%20apply%20it%20consistently%2C%20exhibit%20biases%20toward%0Apreferred%20readings%2C%20and%20display%20self-inconsistencies.%20This%20highlights%20the%20need%0Ato%20address%20entity%20ambiguity%20in%20the%20future%20for%20more%20trustworthy%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17125v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Know%2520or%2520Not%2520To%2520Know%253F%2520Analyzing%2520Self-Consistency%2520of%2520Large%2520Language%250A%2520%2520Models%2520under%2520Ambiguity%26entry.906535625%3DAnastasiia%2520Sedova%2520and%2520Robert%2520Litschko%2520and%2520Diego%2520Frassinelli%2520and%2520Benjamin%2520Roth%2520and%2520Barbara%2520Plank%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520major%2520aspects%2520contributing%2520to%2520the%2520striking%2520performance%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520is%2520the%2520vast%2520amount%2520of%2520factual%2520knowledge%2520accumulated%250Aduring%2520pre-training.%2520Yet%252C%2520many%2520LLMs%2520suffer%2520from%2520self-inconsistency%252C%2520which%250Araises%2520doubts%2520about%2520their%2520trustworthiness%2520and%2520reliability.%2520This%2520paper%2520focuses%250Aon%2520entity%2520type%2520ambiguity%252C%2520analyzing%2520the%2520proficiency%2520and%2520consistency%2520of%250Astate-of-the-art%2520LLMs%2520in%2520applying%2520factual%2520knowledge%2520when%2520prompted%2520with%250Aambiguous%2520entities.%2520To%2520do%2520so%252C%2520we%2520propose%2520an%2520evaluation%2520protocol%2520that%250Adisentangles%2520knowing%2520from%2520applying%2520knowledge%252C%2520and%2520test%2520state-of-the-art%2520LLMs%2520on%250A49%2520ambiguous%2520entities.%2520Our%2520experiments%2520reveal%2520that%2520LLMs%2520struggle%2520with%2520choosing%250Athe%2520correct%2520entity%2520reading%252C%2520achieving%2520an%2520average%2520accuracy%2520of%2520only%252085%2525%252C%2520and%2520as%250Alow%2520as%252075%2525%2520with%2520underspecified%2520prompts.%2520The%2520results%2520also%2520reveal%2520systematic%250Adiscrepancies%2520in%2520LLM%2520behavior%252C%2520showing%2520that%2520while%2520the%2520models%2520may%2520possess%250Aknowledge%252C%2520they%2520struggle%2520to%2520apply%2520it%2520consistently%252C%2520exhibit%2520biases%2520toward%250Apreferred%2520readings%252C%2520and%2520display%2520self-inconsistencies.%2520This%2520highlights%2520the%2520need%250Ato%2520address%2520entity%2520ambiguity%2520in%2520the%2520future%2520for%2520more%2520trustworthy%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17125v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Know%20or%20Not%20To%20Know%3F%20Analyzing%20Self-Consistency%20of%20Large%20Language%0A%20%20Models%20under%20Ambiguity&entry.906535625=Anastasiia%20Sedova%20and%20Robert%20Litschko%20and%20Diego%20Frassinelli%20and%20Benjamin%20Roth%20and%20Barbara%20Plank&entry.1292438233=%20%20One%20of%20the%20major%20aspects%20contributing%20to%20the%20striking%20performance%20of%20large%0Alanguage%20models%20%28LLMs%29%20is%20the%20vast%20amount%20of%20factual%20knowledge%20accumulated%0Aduring%20pre-training.%20Yet%2C%20many%20LLMs%20suffer%20from%20self-inconsistency%2C%20which%0Araises%20doubts%20about%20their%20trustworthiness%20and%20reliability.%20This%20paper%20focuses%0Aon%20entity%20type%20ambiguity%2C%20analyzing%20the%20proficiency%20and%20consistency%20of%0Astate-of-the-art%20LLMs%20in%20applying%20factual%20knowledge%20when%20prompted%20with%0Aambiguous%20entities.%20To%20do%20so%2C%20we%20propose%20an%20evaluation%20protocol%20that%0Adisentangles%20knowing%20from%20applying%20knowledge%2C%20and%20test%20state-of-the-art%20LLMs%20on%0A49%20ambiguous%20entities.%20Our%20experiments%20reveal%20that%20LLMs%20struggle%20with%20choosing%0Athe%20correct%20entity%20reading%2C%20achieving%20an%20average%20accuracy%20of%20only%2085%25%2C%20and%20as%0Alow%20as%2075%25%20with%20underspecified%20prompts.%20The%20results%20also%20reveal%20systematic%0Adiscrepancies%20in%20LLM%20behavior%2C%20showing%20that%20while%20the%20models%20may%20possess%0Aknowledge%2C%20they%20struggle%20to%20apply%20it%20consistently%2C%20exhibit%20biases%20toward%0Apreferred%20readings%2C%20and%20display%20self-inconsistencies.%20This%20highlights%20the%20need%0Ato%20address%20entity%20ambiguity%20in%20the%20future%20for%20more%20trustworthy%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17125v3&entry.124074799=Read"},
{"title": "Improving Online Bagging for Complex Imbalanced Data Stream", "author": "Bartosz Przybyl and Jerzy Stefanowski", "abstract": "  Learning classifiers from imbalanced and concept drifting data streams is\nstill a challenge. Most of the current proposals focus on taking into account\nchanges in the global imbalance ratio only and ignore the local difficulty\nfactors, such as the minority class decomposition into sub-concepts and the\npresence of unsafe types of examples (borderline or rare ones). As the above\nfactors present in the stream may deteriorate the performance of popular online\nclassifiers, we propose extensions of resampling online bagging, namely\nNeighbourhood Undersampling or Oversampling Online Bagging to take better\naccount of the presence of unsafe minority examples. The performed\ncomputational experiments with synthetic complex imbalanced data streams have\nshown their advantage over earlier variants of online bagging resampling\nensembles.\n", "link": "http://arxiv.org/abs/2410.03519v1", "date": "2024-10-04", "relevancy": 2.0103, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5417}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4769}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Online%20Bagging%20for%20Complex%20Imbalanced%20Data%20Stream&body=Title%3A%20Improving%20Online%20Bagging%20for%20Complex%20Imbalanced%20Data%20Stream%0AAuthor%3A%20Bartosz%20Przybyl%20and%20Jerzy%20Stefanowski%0AAbstract%3A%20%20%20Learning%20classifiers%20from%20imbalanced%20and%20concept%20drifting%20data%20streams%20is%0Astill%20a%20challenge.%20Most%20of%20the%20current%20proposals%20focus%20on%20taking%20into%20account%0Achanges%20in%20the%20global%20imbalance%20ratio%20only%20and%20ignore%20the%20local%20difficulty%0Afactors%2C%20such%20as%20the%20minority%20class%20decomposition%20into%20sub-concepts%20and%20the%0Apresence%20of%20unsafe%20types%20of%20examples%20%28borderline%20or%20rare%20ones%29.%20As%20the%20above%0Afactors%20present%20in%20the%20stream%20may%20deteriorate%20the%20performance%20of%20popular%20online%0Aclassifiers%2C%20we%20propose%20extensions%20of%20resampling%20online%20bagging%2C%20namely%0ANeighbourhood%20Undersampling%20or%20Oversampling%20Online%20Bagging%20to%20take%20better%0Aaccount%20of%20the%20presence%20of%20unsafe%20minority%20examples.%20The%20performed%0Acomputational%20experiments%20with%20synthetic%20complex%20imbalanced%20data%20streams%20have%0Ashown%20their%20advantage%20over%20earlier%20variants%20of%20online%20bagging%20resampling%0Aensembles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Online%2520Bagging%2520for%2520Complex%2520Imbalanced%2520Data%2520Stream%26entry.906535625%3DBartosz%2520Przybyl%2520and%2520Jerzy%2520Stefanowski%26entry.1292438233%3D%2520%2520Learning%2520classifiers%2520from%2520imbalanced%2520and%2520concept%2520drifting%2520data%2520streams%2520is%250Astill%2520a%2520challenge.%2520Most%2520of%2520the%2520current%2520proposals%2520focus%2520on%2520taking%2520into%2520account%250Achanges%2520in%2520the%2520global%2520imbalance%2520ratio%2520only%2520and%2520ignore%2520the%2520local%2520difficulty%250Afactors%252C%2520such%2520as%2520the%2520minority%2520class%2520decomposition%2520into%2520sub-concepts%2520and%2520the%250Apresence%2520of%2520unsafe%2520types%2520of%2520examples%2520%2528borderline%2520or%2520rare%2520ones%2529.%2520As%2520the%2520above%250Afactors%2520present%2520in%2520the%2520stream%2520may%2520deteriorate%2520the%2520performance%2520of%2520popular%2520online%250Aclassifiers%252C%2520we%2520propose%2520extensions%2520of%2520resampling%2520online%2520bagging%252C%2520namely%250ANeighbourhood%2520Undersampling%2520or%2520Oversampling%2520Online%2520Bagging%2520to%2520take%2520better%250Aaccount%2520of%2520the%2520presence%2520of%2520unsafe%2520minority%2520examples.%2520The%2520performed%250Acomputational%2520experiments%2520with%2520synthetic%2520complex%2520imbalanced%2520data%2520streams%2520have%250Ashown%2520their%2520advantage%2520over%2520earlier%2520variants%2520of%2520online%2520bagging%2520resampling%250Aensembles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Online%20Bagging%20for%20Complex%20Imbalanced%20Data%20Stream&entry.906535625=Bartosz%20Przybyl%20and%20Jerzy%20Stefanowski&entry.1292438233=%20%20Learning%20classifiers%20from%20imbalanced%20and%20concept%20drifting%20data%20streams%20is%0Astill%20a%20challenge.%20Most%20of%20the%20current%20proposals%20focus%20on%20taking%20into%20account%0Achanges%20in%20the%20global%20imbalance%20ratio%20only%20and%20ignore%20the%20local%20difficulty%0Afactors%2C%20such%20as%20the%20minority%20class%20decomposition%20into%20sub-concepts%20and%20the%0Apresence%20of%20unsafe%20types%20of%20examples%20%28borderline%20or%20rare%20ones%29.%20As%20the%20above%0Afactors%20present%20in%20the%20stream%20may%20deteriorate%20the%20performance%20of%20popular%20online%0Aclassifiers%2C%20we%20propose%20extensions%20of%20resampling%20online%20bagging%2C%20namely%0ANeighbourhood%20Undersampling%20or%20Oversampling%20Online%20Bagging%20to%20take%20better%0Aaccount%20of%20the%20presence%20of%20unsafe%20minority%20examples.%20The%20performed%0Acomputational%20experiments%20with%20synthetic%20complex%20imbalanced%20data%20streams%20have%0Ashown%20their%20advantage%20over%20earlier%20variants%20of%20online%20bagging%20resampling%0Aensembles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03519v1&entry.124074799=Read"},
{"title": "Towards Efficient Hyperdimensional Computing Using Photonics", "author": "Farbin Fayza and Cansu Demirkiran and Hanning Chen and Che-Kai Liu and Avi Mohan and Hamza Errahmouni and Sanggeon Yun and Mohsen Imani and David Zhang and Darius Bunandar and Ajay Joshi", "abstract": "  Over the past few years, silicon photonics-based computing has emerged as a\npromising alternative to CMOS-based computing for Deep Neural Networks (DNN).\nUnfortunately, the non-linear operations and the high-precision requirements of\nDNNs make it extremely challenging to design efficient silicon photonics-based\nsystems for DNN inference and training. Hyperdimensional Computing (HDC) is an\nemerging, brain-inspired machine learning technique that enjoys several\nadvantages over existing DNNs, including being lightweight, requiring\nlow-precision operands, and being robust to noise introduced by the\nnonidealities in the hardware. For HDC, computing in-memory (CiM) approaches\nhave been widely used, as CiM reduces the data transfer cost if the operands\ncan fit into the memory. However, inefficient multi-bit operations, high write\nlatency, and low endurance make CiM ill-suited for HDC. On the other hand, the\nexisting electro-photonic DNN accelerators are inefficient for HDC because they\nare specifically optimized for matrix multiplication in DNNs and consume a lot\nof power with high-precision data converters.\n  In this paper, we argue that photonic computing and HDC complement each other\nbetter than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC,\nthe first-ever electro-photonic accelerator for HDC training and inference,\nsupporting the basic, record-based, and graph encoding schemes. Evaluating with\npopular datasets, we show that our accelerator can achieve two to five orders\nof magnitude lower EDP than the state-of-the-art electro-photonic DNN\naccelerators for implementing HDC training and inference. PhotoHDC also\nachieves four orders of magnitude lower energy-delay product than CiM-based\naccelerators for both HDC training and inference.\n", "link": "http://arxiv.org/abs/2311.17801v2", "date": "2024-10-04", "relevancy": 2.0096, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5468}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Hyperdimensional%20Computing%20Using%20Photonics&body=Title%3A%20Towards%20Efficient%20Hyperdimensional%20Computing%20Using%20Photonics%0AAuthor%3A%20Farbin%20Fayza%20and%20Cansu%20Demirkiran%20and%20Hanning%20Chen%20and%20Che-Kai%20Liu%20and%20Avi%20Mohan%20and%20Hamza%20Errahmouni%20and%20Sanggeon%20Yun%20and%20Mohsen%20Imani%20and%20David%20Zhang%20and%20Darius%20Bunandar%20and%20Ajay%20Joshi%0AAbstract%3A%20%20%20Over%20the%20past%20few%20years%2C%20silicon%20photonics-based%20computing%20has%20emerged%20as%20a%0Apromising%20alternative%20to%20CMOS-based%20computing%20for%20Deep%20Neural%20Networks%20%28DNN%29.%0AUnfortunately%2C%20the%20non-linear%20operations%20and%20the%20high-precision%20requirements%20of%0ADNNs%20make%20it%20extremely%20challenging%20to%20design%20efficient%20silicon%20photonics-based%0Asystems%20for%20DNN%20inference%20and%20training.%20Hyperdimensional%20Computing%20%28HDC%29%20is%20an%0Aemerging%2C%20brain-inspired%20machine%20learning%20technique%20that%20enjoys%20several%0Aadvantages%20over%20existing%20DNNs%2C%20including%20being%20lightweight%2C%20requiring%0Alow-precision%20operands%2C%20and%20being%20robust%20to%20noise%20introduced%20by%20the%0Anonidealities%20in%20the%20hardware.%20For%20HDC%2C%20computing%20in-memory%20%28CiM%29%20approaches%0Ahave%20been%20widely%20used%2C%20as%20CiM%20reduces%20the%20data%20transfer%20cost%20if%20the%20operands%0Acan%20fit%20into%20the%20memory.%20However%2C%20inefficient%20multi-bit%20operations%2C%20high%20write%0Alatency%2C%20and%20low%20endurance%20make%20CiM%20ill-suited%20for%20HDC.%20On%20the%20other%20hand%2C%20the%0Aexisting%20electro-photonic%20DNN%20accelerators%20are%20inefficient%20for%20HDC%20because%20they%0Aare%20specifically%20optimized%20for%20matrix%20multiplication%20in%20DNNs%20and%20consume%20a%20lot%0Aof%20power%20with%20high-precision%20data%20converters.%0A%20%20In%20this%20paper%2C%20we%20argue%20that%20photonic%20computing%20and%20HDC%20complement%20each%20other%0Abetter%20than%20photonic%20computing%20and%20DNNs%2C%20or%20CiM%20and%20HDC.%20We%20propose%20PhotoHDC%2C%0Athe%20first-ever%20electro-photonic%20accelerator%20for%20HDC%20training%20and%20inference%2C%0Asupporting%20the%20basic%2C%20record-based%2C%20and%20graph%20encoding%20schemes.%20Evaluating%20with%0Apopular%20datasets%2C%20we%20show%20that%20our%20accelerator%20can%20achieve%20two%20to%20five%20orders%0Aof%20magnitude%20lower%20EDP%20than%20the%20state-of-the-art%20electro-photonic%20DNN%0Aaccelerators%20for%20implementing%20HDC%20training%20and%20inference.%20PhotoHDC%20also%0Aachieves%20four%20orders%20of%20magnitude%20lower%20energy-delay%20product%20than%20CiM-based%0Aaccelerators%20for%20both%20HDC%20training%20and%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520Hyperdimensional%2520Computing%2520Using%2520Photonics%26entry.906535625%3DFarbin%2520Fayza%2520and%2520Cansu%2520Demirkiran%2520and%2520Hanning%2520Chen%2520and%2520Che-Kai%2520Liu%2520and%2520Avi%2520Mohan%2520and%2520Hamza%2520Errahmouni%2520and%2520Sanggeon%2520Yun%2520and%2520Mohsen%2520Imani%2520and%2520David%2520Zhang%2520and%2520Darius%2520Bunandar%2520and%2520Ajay%2520Joshi%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520few%2520years%252C%2520silicon%2520photonics-based%2520computing%2520has%2520emerged%2520as%2520a%250Apromising%2520alternative%2520to%2520CMOS-based%2520computing%2520for%2520Deep%2520Neural%2520Networks%2520%2528DNN%2529.%250AUnfortunately%252C%2520the%2520non-linear%2520operations%2520and%2520the%2520high-precision%2520requirements%2520of%250ADNNs%2520make%2520it%2520extremely%2520challenging%2520to%2520design%2520efficient%2520silicon%2520photonics-based%250Asystems%2520for%2520DNN%2520inference%2520and%2520training.%2520Hyperdimensional%2520Computing%2520%2528HDC%2529%2520is%2520an%250Aemerging%252C%2520brain-inspired%2520machine%2520learning%2520technique%2520that%2520enjoys%2520several%250Aadvantages%2520over%2520existing%2520DNNs%252C%2520including%2520being%2520lightweight%252C%2520requiring%250Alow-precision%2520operands%252C%2520and%2520being%2520robust%2520to%2520noise%2520introduced%2520by%2520the%250Anonidealities%2520in%2520the%2520hardware.%2520For%2520HDC%252C%2520computing%2520in-memory%2520%2528CiM%2529%2520approaches%250Ahave%2520been%2520widely%2520used%252C%2520as%2520CiM%2520reduces%2520the%2520data%2520transfer%2520cost%2520if%2520the%2520operands%250Acan%2520fit%2520into%2520the%2520memory.%2520However%252C%2520inefficient%2520multi-bit%2520operations%252C%2520high%2520write%250Alatency%252C%2520and%2520low%2520endurance%2520make%2520CiM%2520ill-suited%2520for%2520HDC.%2520On%2520the%2520other%2520hand%252C%2520the%250Aexisting%2520electro-photonic%2520DNN%2520accelerators%2520are%2520inefficient%2520for%2520HDC%2520because%2520they%250Aare%2520specifically%2520optimized%2520for%2520matrix%2520multiplication%2520in%2520DNNs%2520and%2520consume%2520a%2520lot%250Aof%2520power%2520with%2520high-precision%2520data%2520converters.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520photonic%2520computing%2520and%2520HDC%2520complement%2520each%2520other%250Abetter%2520than%2520photonic%2520computing%2520and%2520DNNs%252C%2520or%2520CiM%2520and%2520HDC.%2520We%2520propose%2520PhotoHDC%252C%250Athe%2520first-ever%2520electro-photonic%2520accelerator%2520for%2520HDC%2520training%2520and%2520inference%252C%250Asupporting%2520the%2520basic%252C%2520record-based%252C%2520and%2520graph%2520encoding%2520schemes.%2520Evaluating%2520with%250Apopular%2520datasets%252C%2520we%2520show%2520that%2520our%2520accelerator%2520can%2520achieve%2520two%2520to%2520five%2520orders%250Aof%2520magnitude%2520lower%2520EDP%2520than%2520the%2520state-of-the-art%2520electro-photonic%2520DNN%250Aaccelerators%2520for%2520implementing%2520HDC%2520training%2520and%2520inference.%2520PhotoHDC%2520also%250Aachieves%2520four%2520orders%2520of%2520magnitude%2520lower%2520energy-delay%2520product%2520than%2520CiM-based%250Aaccelerators%2520for%2520both%2520HDC%2520training%2520and%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Hyperdimensional%20Computing%20Using%20Photonics&entry.906535625=Farbin%20Fayza%20and%20Cansu%20Demirkiran%20and%20Hanning%20Chen%20and%20Che-Kai%20Liu%20and%20Avi%20Mohan%20and%20Hamza%20Errahmouni%20and%20Sanggeon%20Yun%20and%20Mohsen%20Imani%20and%20David%20Zhang%20and%20Darius%20Bunandar%20and%20Ajay%20Joshi&entry.1292438233=%20%20Over%20the%20past%20few%20years%2C%20silicon%20photonics-based%20computing%20has%20emerged%20as%20a%0Apromising%20alternative%20to%20CMOS-based%20computing%20for%20Deep%20Neural%20Networks%20%28DNN%29.%0AUnfortunately%2C%20the%20non-linear%20operations%20and%20the%20high-precision%20requirements%20of%0ADNNs%20make%20it%20extremely%20challenging%20to%20design%20efficient%20silicon%20photonics-based%0Asystems%20for%20DNN%20inference%20and%20training.%20Hyperdimensional%20Computing%20%28HDC%29%20is%20an%0Aemerging%2C%20brain-inspired%20machine%20learning%20technique%20that%20enjoys%20several%0Aadvantages%20over%20existing%20DNNs%2C%20including%20being%20lightweight%2C%20requiring%0Alow-precision%20operands%2C%20and%20being%20robust%20to%20noise%20introduced%20by%20the%0Anonidealities%20in%20the%20hardware.%20For%20HDC%2C%20computing%20in-memory%20%28CiM%29%20approaches%0Ahave%20been%20widely%20used%2C%20as%20CiM%20reduces%20the%20data%20transfer%20cost%20if%20the%20operands%0Acan%20fit%20into%20the%20memory.%20However%2C%20inefficient%20multi-bit%20operations%2C%20high%20write%0Alatency%2C%20and%20low%20endurance%20make%20CiM%20ill-suited%20for%20HDC.%20On%20the%20other%20hand%2C%20the%0Aexisting%20electro-photonic%20DNN%20accelerators%20are%20inefficient%20for%20HDC%20because%20they%0Aare%20specifically%20optimized%20for%20matrix%20multiplication%20in%20DNNs%20and%20consume%20a%20lot%0Aof%20power%20with%20high-precision%20data%20converters.%0A%20%20In%20this%20paper%2C%20we%20argue%20that%20photonic%20computing%20and%20HDC%20complement%20each%20other%0Abetter%20than%20photonic%20computing%20and%20DNNs%2C%20or%20CiM%20and%20HDC.%20We%20propose%20PhotoHDC%2C%0Athe%20first-ever%20electro-photonic%20accelerator%20for%20HDC%20training%20and%20inference%2C%0Asupporting%20the%20basic%2C%20record-based%2C%20and%20graph%20encoding%20schemes.%20Evaluating%20with%0Apopular%20datasets%2C%20we%20show%20that%20our%20accelerator%20can%20achieve%20two%20to%20five%20orders%0Aof%20magnitude%20lower%20EDP%20than%20the%20state-of-the-art%20electro-photonic%20DNN%0Aaccelerators%20for%20implementing%20HDC%20training%20and%20inference.%20PhotoHDC%20also%0Aachieves%20four%20orders%20of%20magnitude%20lower%20energy-delay%20product%20than%20CiM-based%0Aaccelerators%20for%20both%20HDC%20training%20and%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17801v2&entry.124074799=Read"},
{"title": "Searching for Efficient Linear Layers over a Continuous Space of\n  Structured Matrices", "author": "Andres Potapczynski and Shikai Qiu and Marc Finzi and Christopher Ferri and Zixi Chen and Micah Goldblum and Bayan Bruss and Christopher De Sa and Andrew Gordon Wilson", "abstract": "  Dense linear layers are the dominant computational bottleneck in large neural\nnetworks, presenting a critical need for more efficient alternatives. Previous\nefforts focused on a small number of hand-crafted structured matrices and\nneglected to investigate whether these structures can surpass dense layers in\nterms of compute-optimal scaling laws when both the model size and training\nexamples are optimally allocated. In this work, we present a unifying framework\nthat enables searching among all linear operators expressible via an Einstein\nsummation. This framework encompasses many previously proposed structures, such\nas low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch,\nalong with many novel structures. To analyze the framework, we develop a\ntaxonomy of all such operators based on their computational and algebraic\nproperties and show that differences in the compute-optimal scaling laws are\nmostly governed by a small number of variables that we introduce. Namely, a\nsmall $\\omega$ (which measures parameter sharing) and large $\\psi$ (which\nmeasures the rank) reliably led to better scaling laws. Guided by the insight\nthat full-rank structures that maximize parameters per unit of compute perform\nthe best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture\nobtained by sparsifying computation in the BTT structure. In contrast to the\nstandard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE\nin every single linear layer of the model, including the projection matrices in\nthe attention blocks. We find BTT-MoE provides a substantial compute-efficiency\ngain over dense layers and standard MoE.\n", "link": "http://arxiv.org/abs/2410.02117v2", "date": "2024-10-04", "relevancy": 2.0059, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5057}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Searching%20for%20Efficient%20Linear%20Layers%20over%20a%20Continuous%20Space%20of%0A%20%20Structured%20Matrices&body=Title%3A%20Searching%20for%20Efficient%20Linear%20Layers%20over%20a%20Continuous%20Space%20of%0A%20%20Structured%20Matrices%0AAuthor%3A%20Andres%20Potapczynski%20and%20Shikai%20Qiu%20and%20Marc%20Finzi%20and%20Christopher%20Ferri%20and%20Zixi%20Chen%20and%20Micah%20Goldblum%20and%20Bayan%20Bruss%20and%20Christopher%20De%20Sa%20and%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20Dense%20linear%20layers%20are%20the%20dominant%20computational%20bottleneck%20in%20large%20neural%0Anetworks%2C%20presenting%20a%20critical%20need%20for%20more%20efficient%20alternatives.%20Previous%0Aefforts%20focused%20on%20a%20small%20number%20of%20hand-crafted%20structured%20matrices%20and%0Aneglected%20to%20investigate%20whether%20these%20structures%20can%20surpass%20dense%20layers%20in%0Aterms%20of%20compute-optimal%20scaling%20laws%20when%20both%20the%20model%20size%20and%20training%0Aexamples%20are%20optimally%20allocated.%20In%20this%20work%2C%20we%20present%20a%20unifying%20framework%0Athat%20enables%20searching%20among%20all%20linear%20operators%20expressible%20via%20an%20Einstein%0Asummation.%20This%20framework%20encompasses%20many%20previously%20proposed%20structures%2C%20such%0Aas%20low-rank%2C%20Kronecker%2C%20Tensor-Train%2C%20Block%20Tensor-Train%20%28BTT%29%2C%20and%20Monarch%2C%0Aalong%20with%20many%20novel%20structures.%20To%20analyze%20the%20framework%2C%20we%20develop%20a%0Ataxonomy%20of%20all%20such%20operators%20based%20on%20their%20computational%20and%20algebraic%0Aproperties%20and%20show%20that%20differences%20in%20the%20compute-optimal%20scaling%20laws%20are%0Amostly%20governed%20by%20a%20small%20number%20of%20variables%20that%20we%20introduce.%20Namely%2C%20a%0Asmall%20%24%5Comega%24%20%28which%20measures%20parameter%20sharing%29%20and%20large%20%24%5Cpsi%24%20%28which%0Ameasures%20the%20rank%29%20reliably%20led%20to%20better%20scaling%20laws.%20Guided%20by%20the%20insight%0Athat%20full-rank%20structures%20that%20maximize%20parameters%20per%20unit%20of%20compute%20perform%0Athe%20best%2C%20we%20propose%20BTT-MoE%2C%20a%20novel%20Mixture-of-Experts%20%28MoE%29%20architecture%0Aobtained%20by%20sparsifying%20computation%20in%20the%20BTT%20structure.%20In%20contrast%20to%20the%0Astandard%20sparse%20MoE%20for%20each%20entire%20feed-forward%20network%2C%20BTT-MoE%20learns%20an%20MoE%0Ain%20every%20single%20linear%20layer%20of%20the%20model%2C%20including%20the%20projection%20matrices%20in%0Athe%20attention%20blocks.%20We%20find%20BTT-MoE%20provides%20a%20substantial%20compute-efficiency%0Again%20over%20dense%20layers%20and%20standard%20MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02117v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearching%2520for%2520Efficient%2520Linear%2520Layers%2520over%2520a%2520Continuous%2520Space%2520of%250A%2520%2520Structured%2520Matrices%26entry.906535625%3DAndres%2520Potapczynski%2520and%2520Shikai%2520Qiu%2520and%2520Marc%2520Finzi%2520and%2520Christopher%2520Ferri%2520and%2520Zixi%2520Chen%2520and%2520Micah%2520Goldblum%2520and%2520Bayan%2520Bruss%2520and%2520Christopher%2520De%2520Sa%2520and%2520Andrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520Dense%2520linear%2520layers%2520are%2520the%2520dominant%2520computational%2520bottleneck%2520in%2520large%2520neural%250Anetworks%252C%2520presenting%2520a%2520critical%2520need%2520for%2520more%2520efficient%2520alternatives.%2520Previous%250Aefforts%2520focused%2520on%2520a%2520small%2520number%2520of%2520hand-crafted%2520structured%2520matrices%2520and%250Aneglected%2520to%2520investigate%2520whether%2520these%2520structures%2520can%2520surpass%2520dense%2520layers%2520in%250Aterms%2520of%2520compute-optimal%2520scaling%2520laws%2520when%2520both%2520the%2520model%2520size%2520and%2520training%250Aexamples%2520are%2520optimally%2520allocated.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520unifying%2520framework%250Athat%2520enables%2520searching%2520among%2520all%2520linear%2520operators%2520expressible%2520via%2520an%2520Einstein%250Asummation.%2520This%2520framework%2520encompasses%2520many%2520previously%2520proposed%2520structures%252C%2520such%250Aas%2520low-rank%252C%2520Kronecker%252C%2520Tensor-Train%252C%2520Block%2520Tensor-Train%2520%2528BTT%2529%252C%2520and%2520Monarch%252C%250Aalong%2520with%2520many%2520novel%2520structures.%2520To%2520analyze%2520the%2520framework%252C%2520we%2520develop%2520a%250Ataxonomy%2520of%2520all%2520such%2520operators%2520based%2520on%2520their%2520computational%2520and%2520algebraic%250Aproperties%2520and%2520show%2520that%2520differences%2520in%2520the%2520compute-optimal%2520scaling%2520laws%2520are%250Amostly%2520governed%2520by%2520a%2520small%2520number%2520of%2520variables%2520that%2520we%2520introduce.%2520Namely%252C%2520a%250Asmall%2520%2524%255Comega%2524%2520%2528which%2520measures%2520parameter%2520sharing%2529%2520and%2520large%2520%2524%255Cpsi%2524%2520%2528which%250Ameasures%2520the%2520rank%2529%2520reliably%2520led%2520to%2520better%2520scaling%2520laws.%2520Guided%2520by%2520the%2520insight%250Athat%2520full-rank%2520structures%2520that%2520maximize%2520parameters%2520per%2520unit%2520of%2520compute%2520perform%250Athe%2520best%252C%2520we%2520propose%2520BTT-MoE%252C%2520a%2520novel%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture%250Aobtained%2520by%2520sparsifying%2520computation%2520in%2520the%2520BTT%2520structure.%2520In%2520contrast%2520to%2520the%250Astandard%2520sparse%2520MoE%2520for%2520each%2520entire%2520feed-forward%2520network%252C%2520BTT-MoE%2520learns%2520an%2520MoE%250Ain%2520every%2520single%2520linear%2520layer%2520of%2520the%2520model%252C%2520including%2520the%2520projection%2520matrices%2520in%250Athe%2520attention%2520blocks.%2520We%2520find%2520BTT-MoE%2520provides%2520a%2520substantial%2520compute-efficiency%250Again%2520over%2520dense%2520layers%2520and%2520standard%2520MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02117v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Searching%20for%20Efficient%20Linear%20Layers%20over%20a%20Continuous%20Space%20of%0A%20%20Structured%20Matrices&entry.906535625=Andres%20Potapczynski%20and%20Shikai%20Qiu%20and%20Marc%20Finzi%20and%20Christopher%20Ferri%20and%20Zixi%20Chen%20and%20Micah%20Goldblum%20and%20Bayan%20Bruss%20and%20Christopher%20De%20Sa%20and%20Andrew%20Gordon%20Wilson&entry.1292438233=%20%20Dense%20linear%20layers%20are%20the%20dominant%20computational%20bottleneck%20in%20large%20neural%0Anetworks%2C%20presenting%20a%20critical%20need%20for%20more%20efficient%20alternatives.%20Previous%0Aefforts%20focused%20on%20a%20small%20number%20of%20hand-crafted%20structured%20matrices%20and%0Aneglected%20to%20investigate%20whether%20these%20structures%20can%20surpass%20dense%20layers%20in%0Aterms%20of%20compute-optimal%20scaling%20laws%20when%20both%20the%20model%20size%20and%20training%0Aexamples%20are%20optimally%20allocated.%20In%20this%20work%2C%20we%20present%20a%20unifying%20framework%0Athat%20enables%20searching%20among%20all%20linear%20operators%20expressible%20via%20an%20Einstein%0Asummation.%20This%20framework%20encompasses%20many%20previously%20proposed%20structures%2C%20such%0Aas%20low-rank%2C%20Kronecker%2C%20Tensor-Train%2C%20Block%20Tensor-Train%20%28BTT%29%2C%20and%20Monarch%2C%0Aalong%20with%20many%20novel%20structures.%20To%20analyze%20the%20framework%2C%20we%20develop%20a%0Ataxonomy%20of%20all%20such%20operators%20based%20on%20their%20computational%20and%20algebraic%0Aproperties%20and%20show%20that%20differences%20in%20the%20compute-optimal%20scaling%20laws%20are%0Amostly%20governed%20by%20a%20small%20number%20of%20variables%20that%20we%20introduce.%20Namely%2C%20a%0Asmall%20%24%5Comega%24%20%28which%20measures%20parameter%20sharing%29%20and%20large%20%24%5Cpsi%24%20%28which%0Ameasures%20the%20rank%29%20reliably%20led%20to%20better%20scaling%20laws.%20Guided%20by%20the%20insight%0Athat%20full-rank%20structures%20that%20maximize%20parameters%20per%20unit%20of%20compute%20perform%0Athe%20best%2C%20we%20propose%20BTT-MoE%2C%20a%20novel%20Mixture-of-Experts%20%28MoE%29%20architecture%0Aobtained%20by%20sparsifying%20computation%20in%20the%20BTT%20structure.%20In%20contrast%20to%20the%0Astandard%20sparse%20MoE%20for%20each%20entire%20feed-forward%20network%2C%20BTT-MoE%20learns%20an%20MoE%0Ain%20every%20single%20linear%20layer%20of%20the%20model%2C%20including%20the%20projection%20matrices%20in%0Athe%20attention%20blocks.%20We%20find%20BTT-MoE%20provides%20a%20substantial%20compute-efficiency%0Again%20over%20dense%20layers%20and%20standard%20MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02117v2&entry.124074799=Read"},
{"title": "Identifying Factual Inconsistencies in Summaries: Grounding LLM\n  Inference via Task Taxonomy", "author": "Liyan Xu and Zhenlin Su and Mo Yu and Jin Xu and Jinho D. Choi and Jie Zhou and Fei Liu", "abstract": "  Factual inconsistencies pose a significant hurdle for the faithful\nsummarization by generative models. While a major direction to enhance\ninconsistency detection is to derive stronger Natural Language Inference (NLI)\nmodels, we propose an orthogonal aspect that underscores the importance of\nincorporating task-specific taxonomy into the inference. To this end, we\nconsolidate key error types of inconsistent facts in summaries, and incorporate\nthem to facilitate both the zero-shot and supervised paradigms of LLMs.\nExtensive experiments on ten datasets of five distinct domains suggest that,\nzero-shot LLM inference could benefit from the explicit solution space depicted\nby the error type taxonomy, and achieves state-of-the-art performance overall,\nsurpassing specialized non-LLM baselines, as well as recent LLM baselines. We\nfurther distill models that fuse the taxonomy into parameters through our\ndesigned prompt completions and supervised training strategies, efficiently\nsubstituting state-of-the-art zero-shot inference with much larger LLMs.\n", "link": "http://arxiv.org/abs/2402.12821v3", "date": "2024-10-04", "relevancy": 2.0008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Factual%20Inconsistencies%20in%20Summaries%3A%20Grounding%20LLM%0A%20%20Inference%20via%20Task%20Taxonomy&body=Title%3A%20Identifying%20Factual%20Inconsistencies%20in%20Summaries%3A%20Grounding%20LLM%0A%20%20Inference%20via%20Task%20Taxonomy%0AAuthor%3A%20Liyan%20Xu%20and%20Zhenlin%20Su%20and%20Mo%20Yu%20and%20Jin%20Xu%20and%20Jinho%20D.%20Choi%20and%20Jie%20Zhou%20and%20Fei%20Liu%0AAbstract%3A%20%20%20Factual%20inconsistencies%20pose%20a%20significant%20hurdle%20for%20the%20faithful%0Asummarization%20by%20generative%20models.%20While%20a%20major%20direction%20to%20enhance%0Ainconsistency%20detection%20is%20to%20derive%20stronger%20Natural%20Language%20Inference%20%28NLI%29%0Amodels%2C%20we%20propose%20an%20orthogonal%20aspect%20that%20underscores%20the%20importance%20of%0Aincorporating%20task-specific%20taxonomy%20into%20the%20inference.%20To%20this%20end%2C%20we%0Aconsolidate%20key%20error%20types%20of%20inconsistent%20facts%20in%20summaries%2C%20and%20incorporate%0Athem%20to%20facilitate%20both%20the%20zero-shot%20and%20supervised%20paradigms%20of%20LLMs.%0AExtensive%20experiments%20on%20ten%20datasets%20of%20five%20distinct%20domains%20suggest%20that%2C%0Azero-shot%20LLM%20inference%20could%20benefit%20from%20the%20explicit%20solution%20space%20depicted%0Aby%20the%20error%20type%20taxonomy%2C%20and%20achieves%20state-of-the-art%20performance%20overall%2C%0Asurpassing%20specialized%20non-LLM%20baselines%2C%20as%20well%20as%20recent%20LLM%20baselines.%20We%0Afurther%20distill%20models%20that%20fuse%20the%20taxonomy%20into%20parameters%20through%20our%0Adesigned%20prompt%20completions%20and%20supervised%20training%20strategies%2C%20efficiently%0Asubstituting%20state-of-the-art%20zero-shot%20inference%20with%20much%20larger%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12821v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Factual%2520Inconsistencies%2520in%2520Summaries%253A%2520Grounding%2520LLM%250A%2520%2520Inference%2520via%2520Task%2520Taxonomy%26entry.906535625%3DLiyan%2520Xu%2520and%2520Zhenlin%2520Su%2520and%2520Mo%2520Yu%2520and%2520Jin%2520Xu%2520and%2520Jinho%2520D.%2520Choi%2520and%2520Jie%2520Zhou%2520and%2520Fei%2520Liu%26entry.1292438233%3D%2520%2520Factual%2520inconsistencies%2520pose%2520a%2520significant%2520hurdle%2520for%2520the%2520faithful%250Asummarization%2520by%2520generative%2520models.%2520While%2520a%2520major%2520direction%2520to%2520enhance%250Ainconsistency%2520detection%2520is%2520to%2520derive%2520stronger%2520Natural%2520Language%2520Inference%2520%2528NLI%2529%250Amodels%252C%2520we%2520propose%2520an%2520orthogonal%2520aspect%2520that%2520underscores%2520the%2520importance%2520of%250Aincorporating%2520task-specific%2520taxonomy%2520into%2520the%2520inference.%2520To%2520this%2520end%252C%2520we%250Aconsolidate%2520key%2520error%2520types%2520of%2520inconsistent%2520facts%2520in%2520summaries%252C%2520and%2520incorporate%250Athem%2520to%2520facilitate%2520both%2520the%2520zero-shot%2520and%2520supervised%2520paradigms%2520of%2520LLMs.%250AExtensive%2520experiments%2520on%2520ten%2520datasets%2520of%2520five%2520distinct%2520domains%2520suggest%2520that%252C%250Azero-shot%2520LLM%2520inference%2520could%2520benefit%2520from%2520the%2520explicit%2520solution%2520space%2520depicted%250Aby%2520the%2520error%2520type%2520taxonomy%252C%2520and%2520achieves%2520state-of-the-art%2520performance%2520overall%252C%250Asurpassing%2520specialized%2520non-LLM%2520baselines%252C%2520as%2520well%2520as%2520recent%2520LLM%2520baselines.%2520We%250Afurther%2520distill%2520models%2520that%2520fuse%2520the%2520taxonomy%2520into%2520parameters%2520through%2520our%250Adesigned%2520prompt%2520completions%2520and%2520supervised%2520training%2520strategies%252C%2520efficiently%250Asubstituting%2520state-of-the-art%2520zero-shot%2520inference%2520with%2520much%2520larger%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12821v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Factual%20Inconsistencies%20in%20Summaries%3A%20Grounding%20LLM%0A%20%20Inference%20via%20Task%20Taxonomy&entry.906535625=Liyan%20Xu%20and%20Zhenlin%20Su%20and%20Mo%20Yu%20and%20Jin%20Xu%20and%20Jinho%20D.%20Choi%20and%20Jie%20Zhou%20and%20Fei%20Liu&entry.1292438233=%20%20Factual%20inconsistencies%20pose%20a%20significant%20hurdle%20for%20the%20faithful%0Asummarization%20by%20generative%20models.%20While%20a%20major%20direction%20to%20enhance%0Ainconsistency%20detection%20is%20to%20derive%20stronger%20Natural%20Language%20Inference%20%28NLI%29%0Amodels%2C%20we%20propose%20an%20orthogonal%20aspect%20that%20underscores%20the%20importance%20of%0Aincorporating%20task-specific%20taxonomy%20into%20the%20inference.%20To%20this%20end%2C%20we%0Aconsolidate%20key%20error%20types%20of%20inconsistent%20facts%20in%20summaries%2C%20and%20incorporate%0Athem%20to%20facilitate%20both%20the%20zero-shot%20and%20supervised%20paradigms%20of%20LLMs.%0AExtensive%20experiments%20on%20ten%20datasets%20of%20five%20distinct%20domains%20suggest%20that%2C%0Azero-shot%20LLM%20inference%20could%20benefit%20from%20the%20explicit%20solution%20space%20depicted%0Aby%20the%20error%20type%20taxonomy%2C%20and%20achieves%20state-of-the-art%20performance%20overall%2C%0Asurpassing%20specialized%20non-LLM%20baselines%2C%20as%20well%20as%20recent%20LLM%20baselines.%20We%0Afurther%20distill%20models%20that%20fuse%20the%20taxonomy%20into%20parameters%20through%20our%0Adesigned%20prompt%20completions%20and%20supervised%20training%20strategies%2C%20efficiently%0Asubstituting%20state-of-the-art%20zero-shot%20inference%20with%20much%20larger%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12821v3&entry.124074799=Read"},
{"title": "Densely Multiplied Physics Informed Neural Networks", "author": "Feilong Jiang and Xiaonan Hou and Min Xia", "abstract": "  Although physics-informed neural networks (PINNs) have shown great potential\nin dealing with nonlinear partial differential equations (PDEs), it is common\nthat PINNs will suffer from the problem of insufficient precision or obtaining\nincorrect outcomes. Unlike most of the existing solutions trying to enhance the\nability of PINN by optimizing the training process, this paper improved the\nneural network architecture to improve the performance of PINN. We propose a\ndensely multiply PINN (DM-PINN) architecture, which multiplies the output of a\nhidden layer with the outputs of all the behind hidden layers. Without\nintroducing more trainable parameters, this effective mechanism can\nsignificantly improve the accuracy of PINNs. The proposed architecture is\nevaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation,\nBurgers equation and 1D convection equation). Comparisons between the proposed\narchitecture and different PINN structures demonstrate the superior performance\nof the DM-PINN in both accuracy and efficiency.\n", "link": "http://arxiv.org/abs/2402.04390v3", "date": "2024-10-04", "relevancy": 2.0005, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5351}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4856}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Densely%20Multiplied%20Physics%20Informed%20Neural%20Networks&body=Title%3A%20Densely%20Multiplied%20Physics%20Informed%20Neural%20Networks%0AAuthor%3A%20Feilong%20Jiang%20and%20Xiaonan%20Hou%20and%20Min%20Xia%0AAbstract%3A%20%20%20Although%20physics-informed%20neural%20networks%20%28PINNs%29%20have%20shown%20great%20potential%0Ain%20dealing%20with%20nonlinear%20partial%20differential%20equations%20%28PDEs%29%2C%20it%20is%20common%0Athat%20PINNs%20will%20suffer%20from%20the%20problem%20of%20insufficient%20precision%20or%20obtaining%0Aincorrect%20outcomes.%20Unlike%20most%20of%20the%20existing%20solutions%20trying%20to%20enhance%20the%0Aability%20of%20PINN%20by%20optimizing%20the%20training%20process%2C%20this%20paper%20improved%20the%0Aneural%20network%20architecture%20to%20improve%20the%20performance%20of%20PINN.%20We%20propose%20a%0Adensely%20multiply%20PINN%20%28DM-PINN%29%20architecture%2C%20which%20multiplies%20the%20output%20of%20a%0Ahidden%20layer%20with%20the%20outputs%20of%20all%20the%20behind%20hidden%20layers.%20Without%0Aintroducing%20more%20trainable%20parameters%2C%20this%20effective%20mechanism%20can%0Asignificantly%20improve%20the%20accuracy%20of%20PINNs.%20The%20proposed%20architecture%20is%0Aevaluated%20on%20four%20benchmark%20examples%20%28Allan-Cahn%20equation%2C%20Helmholtz%20equation%2C%0ABurgers%20equation%20and%201D%20convection%20equation%29.%20Comparisons%20between%20the%20proposed%0Aarchitecture%20and%20different%20PINN%20structures%20demonstrate%20the%20superior%20performance%0Aof%20the%20DM-PINN%20in%20both%20accuracy%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04390v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDensely%2520Multiplied%2520Physics%2520Informed%2520Neural%2520Networks%26entry.906535625%3DFeilong%2520Jiang%2520and%2520Xiaonan%2520Hou%2520and%2520Min%2520Xia%26entry.1292438233%3D%2520%2520Although%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520have%2520shown%2520great%2520potential%250Ain%2520dealing%2520with%2520nonlinear%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520it%2520is%2520common%250Athat%2520PINNs%2520will%2520suffer%2520from%2520the%2520problem%2520of%2520insufficient%2520precision%2520or%2520obtaining%250Aincorrect%2520outcomes.%2520Unlike%2520most%2520of%2520the%2520existing%2520solutions%2520trying%2520to%2520enhance%2520the%250Aability%2520of%2520PINN%2520by%2520optimizing%2520the%2520training%2520process%252C%2520this%2520paper%2520improved%2520the%250Aneural%2520network%2520architecture%2520to%2520improve%2520the%2520performance%2520of%2520PINN.%2520We%2520propose%2520a%250Adensely%2520multiply%2520PINN%2520%2528DM-PINN%2529%2520architecture%252C%2520which%2520multiplies%2520the%2520output%2520of%2520a%250Ahidden%2520layer%2520with%2520the%2520outputs%2520of%2520all%2520the%2520behind%2520hidden%2520layers.%2520Without%250Aintroducing%2520more%2520trainable%2520parameters%252C%2520this%2520effective%2520mechanism%2520can%250Asignificantly%2520improve%2520the%2520accuracy%2520of%2520PINNs.%2520The%2520proposed%2520architecture%2520is%250Aevaluated%2520on%2520four%2520benchmark%2520examples%2520%2528Allan-Cahn%2520equation%252C%2520Helmholtz%2520equation%252C%250ABurgers%2520equation%2520and%25201D%2520convection%2520equation%2529.%2520Comparisons%2520between%2520the%2520proposed%250Aarchitecture%2520and%2520different%2520PINN%2520structures%2520demonstrate%2520the%2520superior%2520performance%250Aof%2520the%2520DM-PINN%2520in%2520both%2520accuracy%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04390v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Densely%20Multiplied%20Physics%20Informed%20Neural%20Networks&entry.906535625=Feilong%20Jiang%20and%20Xiaonan%20Hou%20and%20Min%20Xia&entry.1292438233=%20%20Although%20physics-informed%20neural%20networks%20%28PINNs%29%20have%20shown%20great%20potential%0Ain%20dealing%20with%20nonlinear%20partial%20differential%20equations%20%28PDEs%29%2C%20it%20is%20common%0Athat%20PINNs%20will%20suffer%20from%20the%20problem%20of%20insufficient%20precision%20or%20obtaining%0Aincorrect%20outcomes.%20Unlike%20most%20of%20the%20existing%20solutions%20trying%20to%20enhance%20the%0Aability%20of%20PINN%20by%20optimizing%20the%20training%20process%2C%20this%20paper%20improved%20the%0Aneural%20network%20architecture%20to%20improve%20the%20performance%20of%20PINN.%20We%20propose%20a%0Adensely%20multiply%20PINN%20%28DM-PINN%29%20architecture%2C%20which%20multiplies%20the%20output%20of%20a%0Ahidden%20layer%20with%20the%20outputs%20of%20all%20the%20behind%20hidden%20layers.%20Without%0Aintroducing%20more%20trainable%20parameters%2C%20this%20effective%20mechanism%20can%0Asignificantly%20improve%20the%20accuracy%20of%20PINNs.%20The%20proposed%20architecture%20is%0Aevaluated%20on%20four%20benchmark%20examples%20%28Allan-Cahn%20equation%2C%20Helmholtz%20equation%2C%0ABurgers%20equation%20and%201D%20convection%20equation%29.%20Comparisons%20between%20the%20proposed%0Aarchitecture%20and%20different%20PINN%20structures%20demonstrate%20the%20superior%20performance%0Aof%20the%20DM-PINN%20in%20both%20accuracy%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04390v3&entry.124074799=Read"},
{"title": "Infinite Limits of Multi-head Transformer Dynamics", "author": "Blake Bordelon and Hamza Tahir Chaudhry and Cengiz Pehlevan", "abstract": "  In this work, we analyze various scaling limits of the training dynamics of\ntransformer models in the feature learning regime. We identify the set of\nparameterizations that admit well-defined infinite width and depth limits,\nallowing the attention layers to update throughout training--a relevant notion\nof feature learning in these models. We then use tools from dynamical mean\nfield theory (DMFT) to analyze various infinite limits (infinite key/query\ndimension, infinite heads, and infinite depth) which have different statistical\ndescriptions depending on which infinite limit is taken and how attention\nlayers are scaled. We provide numerical evidence of convergence to the limits\nand discuss how the parameterization qualitatively influences learned features.\n", "link": "http://arxiv.org/abs/2405.15712v2", "date": "2024-10-04", "relevancy": 1.9945, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5724}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4868}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinite%20Limits%20of%20Multi-head%20Transformer%20Dynamics&body=Title%3A%20Infinite%20Limits%20of%20Multi-head%20Transformer%20Dynamics%0AAuthor%3A%20Blake%20Bordelon%20and%20Hamza%20Tahir%20Chaudhry%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20analyze%20various%20scaling%20limits%20of%20the%20training%20dynamics%20of%0Atransformer%20models%20in%20the%20feature%20learning%20regime.%20We%20identify%20the%20set%20of%0Aparameterizations%20that%20admit%20well-defined%20infinite%20width%20and%20depth%20limits%2C%0Aallowing%20the%20attention%20layers%20to%20update%20throughout%20training--a%20relevant%20notion%0Aof%20feature%20learning%20in%20these%20models.%20We%20then%20use%20tools%20from%20dynamical%20mean%0Afield%20theory%20%28DMFT%29%20to%20analyze%20various%20infinite%20limits%20%28infinite%20key/query%0Adimension%2C%20infinite%20heads%2C%20and%20infinite%20depth%29%20which%20have%20different%20statistical%0Adescriptions%20depending%20on%20which%20infinite%20limit%20is%20taken%20and%20how%20attention%0Alayers%20are%20scaled.%20We%20provide%20numerical%20evidence%20of%20convergence%20to%20the%20limits%0Aand%20discuss%20how%20the%20parameterization%20qualitatively%20influences%20learned%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinite%2520Limits%2520of%2520Multi-head%2520Transformer%2520Dynamics%26entry.906535625%3DBlake%2520Bordelon%2520and%2520Hamza%2520Tahir%2520Chaudhry%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520analyze%2520various%2520scaling%2520limits%2520of%2520the%2520training%2520dynamics%2520of%250Atransformer%2520models%2520in%2520the%2520feature%2520learning%2520regime.%2520We%2520identify%2520the%2520set%2520of%250Aparameterizations%2520that%2520admit%2520well-defined%2520infinite%2520width%2520and%2520depth%2520limits%252C%250Aallowing%2520the%2520attention%2520layers%2520to%2520update%2520throughout%2520training--a%2520relevant%2520notion%250Aof%2520feature%2520learning%2520in%2520these%2520models.%2520We%2520then%2520use%2520tools%2520from%2520dynamical%2520mean%250Afield%2520theory%2520%2528DMFT%2529%2520to%2520analyze%2520various%2520infinite%2520limits%2520%2528infinite%2520key/query%250Adimension%252C%2520infinite%2520heads%252C%2520and%2520infinite%2520depth%2529%2520which%2520have%2520different%2520statistical%250Adescriptions%2520depending%2520on%2520which%2520infinite%2520limit%2520is%2520taken%2520and%2520how%2520attention%250Alayers%2520are%2520scaled.%2520We%2520provide%2520numerical%2520evidence%2520of%2520convergence%2520to%2520the%2520limits%250Aand%2520discuss%2520how%2520the%2520parameterization%2520qualitatively%2520influences%2520learned%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinite%20Limits%20of%20Multi-head%20Transformer%20Dynamics&entry.906535625=Blake%20Bordelon%20and%20Hamza%20Tahir%20Chaudhry%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20In%20this%20work%2C%20we%20analyze%20various%20scaling%20limits%20of%20the%20training%20dynamics%20of%0Atransformer%20models%20in%20the%20feature%20learning%20regime.%20We%20identify%20the%20set%20of%0Aparameterizations%20that%20admit%20well-defined%20infinite%20width%20and%20depth%20limits%2C%0Aallowing%20the%20attention%20layers%20to%20update%20throughout%20training--a%20relevant%20notion%0Aof%20feature%20learning%20in%20these%20models.%20We%20then%20use%20tools%20from%20dynamical%20mean%0Afield%20theory%20%28DMFT%29%20to%20analyze%20various%20infinite%20limits%20%28infinite%20key/query%0Adimension%2C%20infinite%20heads%2C%20and%20infinite%20depth%29%20which%20have%20different%20statistical%0Adescriptions%20depending%20on%20which%20infinite%20limit%20is%20taken%20and%20how%20attention%0Alayers%20are%20scaled.%20We%20provide%20numerical%20evidence%20of%20convergence%20to%20the%20limits%0Aand%20discuss%20how%20the%20parameterization%20qualitatively%20influences%20learned%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15712v2&entry.124074799=Read"},
{"title": "Resfusion: Denoising Diffusion Probabilistic Models for Image\n  Restoration Based on Prior Residual Noise", "author": "Zhenning Shi and Haoshuai Zheng and Chen Xu and Changsheng Dong and Bin Pan and Xueshuo Xie and Along He and Tao Li and Huazhu Fu", "abstract": "  Recently, research on denoising diffusion models has expanded its application\nto the field of image restoration. Traditional diffusion-based image\nrestoration methods utilize degraded images as conditional input to effectively\nguide the reverse generation process, without modifying the original denoising\ndiffusion process. However, since the degraded images already include\nlow-frequency information, starting from Gaussian white noise will result in\nincreased sampling steps. We propose Resfusion, a general framework that\nincorporates the residual term into the diffusion forward process, starting the\nreverse process directly from the noisy degraded images. The form of our\ninference process is consistent with the DDPM. We introduced a weighted\nresidual noise, named resnoise, as the prediction target and explicitly provide\nthe quantitative relationship between the residual term and the noise term in\nresnoise. By leveraging a smooth equivalence transformation, Resfusion\ndetermine the optimal acceleration step and maintains the integrity of existing\nnoise schedules, unifying the training and inference processes. The\nexperimental results demonstrate that Resfusion exhibits competitive\nperformance on ISTD dataset, LOL dataset and Raindrop dataset with only five\nsampling steps. Furthermore, Resfusion can be easily applied to image\ngeneration and emerges with strong versatility. Our code and model are\navailable at https://github.com/nkicsl/Resfusion.\n", "link": "http://arxiv.org/abs/2311.14900v3", "date": "2024-10-04", "relevancy": 1.928, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6589}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6321}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resfusion%3A%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Image%0A%20%20Restoration%20Based%20on%20Prior%20Residual%20Noise&body=Title%3A%20Resfusion%3A%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Image%0A%20%20Restoration%20Based%20on%20Prior%20Residual%20Noise%0AAuthor%3A%20Zhenning%20Shi%20and%20Haoshuai%20Zheng%20and%20Chen%20Xu%20and%20Changsheng%20Dong%20and%20Bin%20Pan%20and%20Xueshuo%20Xie%20and%20Along%20He%20and%20Tao%20Li%20and%20Huazhu%20Fu%0AAbstract%3A%20%20%20Recently%2C%20research%20on%20denoising%20diffusion%20models%20has%20expanded%20its%20application%0Ato%20the%20field%20of%20image%20restoration.%20Traditional%20diffusion-based%20image%0Arestoration%20methods%20utilize%20degraded%20images%20as%20conditional%20input%20to%20effectively%0Aguide%20the%20reverse%20generation%20process%2C%20without%20modifying%20the%20original%20denoising%0Adiffusion%20process.%20However%2C%20since%20the%20degraded%20images%20already%20include%0Alow-frequency%20information%2C%20starting%20from%20Gaussian%20white%20noise%20will%20result%20in%0Aincreased%20sampling%20steps.%20We%20propose%20Resfusion%2C%20a%20general%20framework%20that%0Aincorporates%20the%20residual%20term%20into%20the%20diffusion%20forward%20process%2C%20starting%20the%0Areverse%20process%20directly%20from%20the%20noisy%20degraded%20images.%20The%20form%20of%20our%0Ainference%20process%20is%20consistent%20with%20the%20DDPM.%20We%20introduced%20a%20weighted%0Aresidual%20noise%2C%20named%20resnoise%2C%20as%20the%20prediction%20target%20and%20explicitly%20provide%0Athe%20quantitative%20relationship%20between%20the%20residual%20term%20and%20the%20noise%20term%20in%0Aresnoise.%20By%20leveraging%20a%20smooth%20equivalence%20transformation%2C%20Resfusion%0Adetermine%20the%20optimal%20acceleration%20step%20and%20maintains%20the%20integrity%20of%20existing%0Anoise%20schedules%2C%20unifying%20the%20training%20and%20inference%20processes.%20The%0Aexperimental%20results%20demonstrate%20that%20Resfusion%20exhibits%20competitive%0Aperformance%20on%20ISTD%20dataset%2C%20LOL%20dataset%20and%20Raindrop%20dataset%20with%20only%20five%0Asampling%20steps.%20Furthermore%2C%20Resfusion%20can%20be%20easily%20applied%20to%20image%0Ageneration%20and%20emerges%20with%20strong%20versatility.%20Our%20code%20and%20model%20are%0Aavailable%20at%20https%3A//github.com/nkicsl/Resfusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14900v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResfusion%253A%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520for%2520Image%250A%2520%2520Restoration%2520Based%2520on%2520Prior%2520Residual%2520Noise%26entry.906535625%3DZhenning%2520Shi%2520and%2520Haoshuai%2520Zheng%2520and%2520Chen%2520Xu%2520and%2520Changsheng%2520Dong%2520and%2520Bin%2520Pan%2520and%2520Xueshuo%2520Xie%2520and%2520Along%2520He%2520and%2520Tao%2520Li%2520and%2520Huazhu%2520Fu%26entry.1292438233%3D%2520%2520Recently%252C%2520research%2520on%2520denoising%2520diffusion%2520models%2520has%2520expanded%2520its%2520application%250Ato%2520the%2520field%2520of%2520image%2520restoration.%2520Traditional%2520diffusion-based%2520image%250Arestoration%2520methods%2520utilize%2520degraded%2520images%2520as%2520conditional%2520input%2520to%2520effectively%250Aguide%2520the%2520reverse%2520generation%2520process%252C%2520without%2520modifying%2520the%2520original%2520denoising%250Adiffusion%2520process.%2520However%252C%2520since%2520the%2520degraded%2520images%2520already%2520include%250Alow-frequency%2520information%252C%2520starting%2520from%2520Gaussian%2520white%2520noise%2520will%2520result%2520in%250Aincreased%2520sampling%2520steps.%2520We%2520propose%2520Resfusion%252C%2520a%2520general%2520framework%2520that%250Aincorporates%2520the%2520residual%2520term%2520into%2520the%2520diffusion%2520forward%2520process%252C%2520starting%2520the%250Areverse%2520process%2520directly%2520from%2520the%2520noisy%2520degraded%2520images.%2520The%2520form%2520of%2520our%250Ainference%2520process%2520is%2520consistent%2520with%2520the%2520DDPM.%2520We%2520introduced%2520a%2520weighted%250Aresidual%2520noise%252C%2520named%2520resnoise%252C%2520as%2520the%2520prediction%2520target%2520and%2520explicitly%2520provide%250Athe%2520quantitative%2520relationship%2520between%2520the%2520residual%2520term%2520and%2520the%2520noise%2520term%2520in%250Aresnoise.%2520By%2520leveraging%2520a%2520smooth%2520equivalence%2520transformation%252C%2520Resfusion%250Adetermine%2520the%2520optimal%2520acceleration%2520step%2520and%2520maintains%2520the%2520integrity%2520of%2520existing%250Anoise%2520schedules%252C%2520unifying%2520the%2520training%2520and%2520inference%2520processes.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520Resfusion%2520exhibits%2520competitive%250Aperformance%2520on%2520ISTD%2520dataset%252C%2520LOL%2520dataset%2520and%2520Raindrop%2520dataset%2520with%2520only%2520five%250Asampling%2520steps.%2520Furthermore%252C%2520Resfusion%2520can%2520be%2520easily%2520applied%2520to%2520image%250Ageneration%2520and%2520emerges%2520with%2520strong%2520versatility.%2520Our%2520code%2520and%2520model%2520are%250Aavailable%2520at%2520https%253A//github.com/nkicsl/Resfusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14900v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resfusion%3A%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Image%0A%20%20Restoration%20Based%20on%20Prior%20Residual%20Noise&entry.906535625=Zhenning%20Shi%20and%20Haoshuai%20Zheng%20and%20Chen%20Xu%20and%20Changsheng%20Dong%20and%20Bin%20Pan%20and%20Xueshuo%20Xie%20and%20Along%20He%20and%20Tao%20Li%20and%20Huazhu%20Fu&entry.1292438233=%20%20Recently%2C%20research%20on%20denoising%20diffusion%20models%20has%20expanded%20its%20application%0Ato%20the%20field%20of%20image%20restoration.%20Traditional%20diffusion-based%20image%0Arestoration%20methods%20utilize%20degraded%20images%20as%20conditional%20input%20to%20effectively%0Aguide%20the%20reverse%20generation%20process%2C%20without%20modifying%20the%20original%20denoising%0Adiffusion%20process.%20However%2C%20since%20the%20degraded%20images%20already%20include%0Alow-frequency%20information%2C%20starting%20from%20Gaussian%20white%20noise%20will%20result%20in%0Aincreased%20sampling%20steps.%20We%20propose%20Resfusion%2C%20a%20general%20framework%20that%0Aincorporates%20the%20residual%20term%20into%20the%20diffusion%20forward%20process%2C%20starting%20the%0Areverse%20process%20directly%20from%20the%20noisy%20degraded%20images.%20The%20form%20of%20our%0Ainference%20process%20is%20consistent%20with%20the%20DDPM.%20We%20introduced%20a%20weighted%0Aresidual%20noise%2C%20named%20resnoise%2C%20as%20the%20prediction%20target%20and%20explicitly%20provide%0Athe%20quantitative%20relationship%20between%20the%20residual%20term%20and%20the%20noise%20term%20in%0Aresnoise.%20By%20leveraging%20a%20smooth%20equivalence%20transformation%2C%20Resfusion%0Adetermine%20the%20optimal%20acceleration%20step%20and%20maintains%20the%20integrity%20of%20existing%0Anoise%20schedules%2C%20unifying%20the%20training%20and%20inference%20processes.%20The%0Aexperimental%20results%20demonstrate%20that%20Resfusion%20exhibits%20competitive%0Aperformance%20on%20ISTD%20dataset%2C%20LOL%20dataset%20and%20Raindrop%20dataset%20with%20only%20five%0Asampling%20steps.%20Furthermore%2C%20Resfusion%20can%20be%20easily%20applied%20to%20image%0Ageneration%20and%20emerges%20with%20strong%20versatility.%20Our%20code%20and%20model%20are%0Aavailable%20at%20https%3A//github.com/nkicsl/Resfusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14900v3&entry.124074799=Read"},
{"title": "On the Hardness of Learning One Hidden Layer Neural Networks", "author": "Shuchen Li and Ilias Zadik and Manolis Zampetakis", "abstract": "  In this work, we consider the problem of learning one hidden layer ReLU\nneural networks with inputs from $\\mathbb{R}^d$. We show that this learning\nproblem is hard under standard cryptographic assumptions even when: (1) the\nsize of the neural network is polynomial in $d$, (2) its input distribution is\na standard Gaussian, and (3) the noise is Gaussian and polynomially small in\n$d$. Our hardness result is based on the hardness of the Continuous Learning\nwith Errors (CLWE) problem, and in particular, is based on the largely believed\nworst-case hardness of approximately solving the shortest vector problem up to\na multiplicative polynomial factor.\n", "link": "http://arxiv.org/abs/2410.03477v1", "date": "2024-10-04", "relevancy": 1.6256, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4435}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4018}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Hardness%20of%20Learning%20One%20Hidden%20Layer%20Neural%20Networks&body=Title%3A%20On%20the%20Hardness%20of%20Learning%20One%20Hidden%20Layer%20Neural%20Networks%0AAuthor%3A%20Shuchen%20Li%20and%20Ilias%20Zadik%20and%20Manolis%20Zampetakis%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20consider%20the%20problem%20of%20learning%20one%20hidden%20layer%20ReLU%0Aneural%20networks%20with%20inputs%20from%20%24%5Cmathbb%7BR%7D%5Ed%24.%20We%20show%20that%20this%20learning%0Aproblem%20is%20hard%20under%20standard%20cryptographic%20assumptions%20even%20when%3A%20%281%29%20the%0Asize%20of%20the%20neural%20network%20is%20polynomial%20in%20%24d%24%2C%20%282%29%20its%20input%20distribution%20is%0Aa%20standard%20Gaussian%2C%20and%20%283%29%20the%20noise%20is%20Gaussian%20and%20polynomially%20small%20in%0A%24d%24.%20Our%20hardness%20result%20is%20based%20on%20the%20hardness%20of%20the%20Continuous%20Learning%0Awith%20Errors%20%28CLWE%29%20problem%2C%20and%20in%20particular%2C%20is%20based%20on%20the%20largely%20believed%0Aworst-case%20hardness%20of%20approximately%20solving%20the%20shortest%20vector%20problem%20up%20to%0Aa%20multiplicative%20polynomial%20factor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Hardness%2520of%2520Learning%2520One%2520Hidden%2520Layer%2520Neural%2520Networks%26entry.906535625%3DShuchen%2520Li%2520and%2520Ilias%2520Zadik%2520and%2520Manolis%2520Zampetakis%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520problem%2520of%2520learning%2520one%2520hidden%2520layer%2520ReLU%250Aneural%2520networks%2520with%2520inputs%2520from%2520%2524%255Cmathbb%257BR%257D%255Ed%2524.%2520We%2520show%2520that%2520this%2520learning%250Aproblem%2520is%2520hard%2520under%2520standard%2520cryptographic%2520assumptions%2520even%2520when%253A%2520%25281%2529%2520the%250Asize%2520of%2520the%2520neural%2520network%2520is%2520polynomial%2520in%2520%2524d%2524%252C%2520%25282%2529%2520its%2520input%2520distribution%2520is%250Aa%2520standard%2520Gaussian%252C%2520and%2520%25283%2529%2520the%2520noise%2520is%2520Gaussian%2520and%2520polynomially%2520small%2520in%250A%2524d%2524.%2520Our%2520hardness%2520result%2520is%2520based%2520on%2520the%2520hardness%2520of%2520the%2520Continuous%2520Learning%250Awith%2520Errors%2520%2528CLWE%2529%2520problem%252C%2520and%2520in%2520particular%252C%2520is%2520based%2520on%2520the%2520largely%2520believed%250Aworst-case%2520hardness%2520of%2520approximately%2520solving%2520the%2520shortest%2520vector%2520problem%2520up%2520to%250Aa%2520multiplicative%2520polynomial%2520factor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Hardness%20of%20Learning%20One%20Hidden%20Layer%20Neural%20Networks&entry.906535625=Shuchen%20Li%20and%20Ilias%20Zadik%20and%20Manolis%20Zampetakis&entry.1292438233=%20%20In%20this%20work%2C%20we%20consider%20the%20problem%20of%20learning%20one%20hidden%20layer%20ReLU%0Aneural%20networks%20with%20inputs%20from%20%24%5Cmathbb%7BR%7D%5Ed%24.%20We%20show%20that%20this%20learning%0Aproblem%20is%20hard%20under%20standard%20cryptographic%20assumptions%20even%20when%3A%20%281%29%20the%0Asize%20of%20the%20neural%20network%20is%20polynomial%20in%20%24d%24%2C%20%282%29%20its%20input%20distribution%20is%0Aa%20standard%20Gaussian%2C%20and%20%283%29%20the%20noise%20is%20Gaussian%20and%20polynomially%20small%20in%0A%24d%24.%20Our%20hardness%20result%20is%20based%20on%20the%20hardness%20of%20the%20Continuous%20Learning%0Awith%20Errors%20%28CLWE%29%20problem%2C%20and%20in%20particular%2C%20is%20based%20on%20the%20largely%20believed%0Aworst-case%20hardness%20of%20approximately%20solving%20the%20shortest%20vector%20problem%20up%20to%0Aa%20multiplicative%20polynomial%20factor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03477v1&entry.124074799=Read"},
{"title": "Any-Quantile Probabilistic Forecasting of Short-Term Electricity Demand", "author": "Slawek Smyl and Boris N. Oreshkin and Pawe\u0142 Pe\u0142ka and Grzegorz Dudek", "abstract": "  Power systems operate under uncertainty originating from multiple factors\nthat are impossible to account for deterministically. Distributional\nforecasting is used to control and mitigate risks associated with this\nuncertainty. Recent progress in deep learning has helped to significantly\nimprove the accuracy of point forecasts, while accurate distributional\nforecasting still presents a significant challenge. In this paper, we propose a\nnovel general approach for distributional forecasting capable of predicting\narbitrary quantiles. We show that our general approach can be seamlessly\napplied to two distinct neural architectures leading to the state-of-the-art\ndistributional forecasting results in the context of short-term electricity\ndemand forecasting task. We empirically validate our method on 35 hourly\nelectricity demand time-series for European countries. Our code is available\nhere: https://github.com/boreshkinai/any-quantile.\n", "link": "http://arxiv.org/abs/2404.17451v2", "date": "2024-10-04", "relevancy": 1.2831, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4296}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4266}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any-Quantile%20Probabilistic%20Forecasting%20of%20Short-Term%20Electricity%20Demand&body=Title%3A%20Any-Quantile%20Probabilistic%20Forecasting%20of%20Short-Term%20Electricity%20Demand%0AAuthor%3A%20Slawek%20Smyl%20and%20Boris%20N.%20Oreshkin%20and%20Pawe%C5%82%20Pe%C5%82ka%20and%20Grzegorz%20Dudek%0AAbstract%3A%20%20%20Power%20systems%20operate%20under%20uncertainty%20originating%20from%20multiple%20factors%0Athat%20are%20impossible%20to%20account%20for%20deterministically.%20Distributional%0Aforecasting%20is%20used%20to%20control%20and%20mitigate%20risks%20associated%20with%20this%0Auncertainty.%20Recent%20progress%20in%20deep%20learning%20has%20helped%20to%20significantly%0Aimprove%20the%20accuracy%20of%20point%20forecasts%2C%20while%20accurate%20distributional%0Aforecasting%20still%20presents%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20general%20approach%20for%20distributional%20forecasting%20capable%20of%20predicting%0Aarbitrary%20quantiles.%20We%20show%20that%20our%20general%20approach%20can%20be%20seamlessly%0Aapplied%20to%20two%20distinct%20neural%20architectures%20leading%20to%20the%20state-of-the-art%0Adistributional%20forecasting%20results%20in%20the%20context%20of%20short-term%20electricity%0Ademand%20forecasting%20task.%20We%20empirically%20validate%20our%20method%20on%2035%20hourly%0Aelectricity%20demand%20time-series%20for%20European%20countries.%20Our%20code%20is%20available%0Ahere%3A%20https%3A//github.com/boreshkinai/any-quantile.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny-Quantile%2520Probabilistic%2520Forecasting%2520of%2520Short-Term%2520Electricity%2520Demand%26entry.906535625%3DSlawek%2520Smyl%2520and%2520Boris%2520N.%2520Oreshkin%2520and%2520Pawe%25C5%2582%2520Pe%25C5%2582ka%2520and%2520Grzegorz%2520Dudek%26entry.1292438233%3D%2520%2520Power%2520systems%2520operate%2520under%2520uncertainty%2520originating%2520from%2520multiple%2520factors%250Athat%2520are%2520impossible%2520to%2520account%2520for%2520deterministically.%2520Distributional%250Aforecasting%2520is%2520used%2520to%2520control%2520and%2520mitigate%2520risks%2520associated%2520with%2520this%250Auncertainty.%2520Recent%2520progress%2520in%2520deep%2520learning%2520has%2520helped%2520to%2520significantly%250Aimprove%2520the%2520accuracy%2520of%2520point%2520forecasts%252C%2520while%2520accurate%2520distributional%250Aforecasting%2520still%2520presents%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520general%2520approach%2520for%2520distributional%2520forecasting%2520capable%2520of%2520predicting%250Aarbitrary%2520quantiles.%2520We%2520show%2520that%2520our%2520general%2520approach%2520can%2520be%2520seamlessly%250Aapplied%2520to%2520two%2520distinct%2520neural%2520architectures%2520leading%2520to%2520the%2520state-of-the-art%250Adistributional%2520forecasting%2520results%2520in%2520the%2520context%2520of%2520short-term%2520electricity%250Ademand%2520forecasting%2520task.%2520We%2520empirically%2520validate%2520our%2520method%2520on%252035%2520hourly%250Aelectricity%2520demand%2520time-series%2520for%2520European%2520countries.%2520Our%2520code%2520is%2520available%250Ahere%253A%2520https%253A//github.com/boreshkinai/any-quantile.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any-Quantile%20Probabilistic%20Forecasting%20of%20Short-Term%20Electricity%20Demand&entry.906535625=Slawek%20Smyl%20and%20Boris%20N.%20Oreshkin%20and%20Pawe%C5%82%20Pe%C5%82ka%20and%20Grzegorz%20Dudek&entry.1292438233=%20%20Power%20systems%20operate%20under%20uncertainty%20originating%20from%20multiple%20factors%0Athat%20are%20impossible%20to%20account%20for%20deterministically.%20Distributional%0Aforecasting%20is%20used%20to%20control%20and%20mitigate%20risks%20associated%20with%20this%0Auncertainty.%20Recent%20progress%20in%20deep%20learning%20has%20helped%20to%20significantly%0Aimprove%20the%20accuracy%20of%20point%20forecasts%2C%20while%20accurate%20distributional%0Aforecasting%20still%20presents%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20general%20approach%20for%20distributional%20forecasting%20capable%20of%20predicting%0Aarbitrary%20quantiles.%20We%20show%20that%20our%20general%20approach%20can%20be%20seamlessly%0Aapplied%20to%20two%20distinct%20neural%20architectures%20leading%20to%20the%20state-of-the-art%0Adistributional%20forecasting%20results%20in%20the%20context%20of%20short-term%20electricity%0Ademand%20forecasting%20task.%20We%20empirically%20validate%20our%20method%20on%2035%20hourly%0Aelectricity%20demand%20time-series%20for%20European%20countries.%20Our%20code%20is%20available%0Ahere%3A%20https%3A//github.com/boreshkinai/any-quantile.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17451v2&entry.124074799=Read"},
{"title": "Predicting perturbation targets with causal differential networks", "author": "Menghua Wu and Umesh Padia and Sean H. Murphy and Regina Barzilay and Tommi Jaakkola", "abstract": "  Rationally identifying variables responsible for changes to a biological\nsystem can enable myriad applications in disease understanding and cell\nengineering. From a causality perspective, we are given two datasets generated\nby the same causal model, one observational (control) and one interventional\n(perturbed). The goal is to isolate the subset of measured variables (e.g.\ngenes) that were the targets of the intervention, i.e. those whose conditional\nindependencies have changed. Knowing the causal graph would limit the search\nspace, allowing us to efficiently pinpoint these variables. However, current\nalgorithms that infer causal graphs in the presence of unknown intervention\ntargets scale poorly to the hundreds or thousands of variables in biological\ndata, as they must jointly search the combinatorial spaces of graphs and\nconsistent intervention targets. In this work, we propose a causality-inspired\napproach for predicting perturbation targets that decouples the two search\nsteps. First, we use an amortized causal discovery model to separately infer\ncausal graphs from the observational and interventional datasets. Then, we\nlearn to map these paired graphs to the sets of variables that were intervened\nupon, in a supervised learning framework. This approach consistently\noutperforms baselines for perturbation modeling on seven single-cell\ntranscriptomics datasets, each with thousands of measured variables. We also\ndemonstrate significant improvements over six causal discovery algorithms in\npredicting intervention targets across a variety of tractable, synthetic\ndatasets.\n", "link": "http://arxiv.org/abs/2410.03380v1", "date": "2024-10-04", "relevancy": 1.3827, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4675}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4665}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20perturbation%20targets%20with%20causal%20differential%20networks&body=Title%3A%20Predicting%20perturbation%20targets%20with%20causal%20differential%20networks%0AAuthor%3A%20Menghua%20Wu%20and%20Umesh%20Padia%20and%20Sean%20H.%20Murphy%20and%20Regina%20Barzilay%20and%20Tommi%20Jaakkola%0AAbstract%3A%20%20%20Rationally%20identifying%20variables%20responsible%20for%20changes%20to%20a%20biological%0Asystem%20can%20enable%20myriad%20applications%20in%20disease%20understanding%20and%20cell%0Aengineering.%20From%20a%20causality%20perspective%2C%20we%20are%20given%20two%20datasets%20generated%0Aby%20the%20same%20causal%20model%2C%20one%20observational%20%28control%29%20and%20one%20interventional%0A%28perturbed%29.%20The%20goal%20is%20to%20isolate%20the%20subset%20of%20measured%20variables%20%28e.g.%0Agenes%29%20that%20were%20the%20targets%20of%20the%20intervention%2C%20i.e.%20those%20whose%20conditional%0Aindependencies%20have%20changed.%20Knowing%20the%20causal%20graph%20would%20limit%20the%20search%0Aspace%2C%20allowing%20us%20to%20efficiently%20pinpoint%20these%20variables.%20However%2C%20current%0Aalgorithms%20that%20infer%20causal%20graphs%20in%20the%20presence%20of%20unknown%20intervention%0Atargets%20scale%20poorly%20to%20the%20hundreds%20or%20thousands%20of%20variables%20in%20biological%0Adata%2C%20as%20they%20must%20jointly%20search%20the%20combinatorial%20spaces%20of%20graphs%20and%0Aconsistent%20intervention%20targets.%20In%20this%20work%2C%20we%20propose%20a%20causality-inspired%0Aapproach%20for%20predicting%20perturbation%20targets%20that%20decouples%20the%20two%20search%0Asteps.%20First%2C%20we%20use%20an%20amortized%20causal%20discovery%20model%20to%20separately%20infer%0Acausal%20graphs%20from%20the%20observational%20and%20interventional%20datasets.%20Then%2C%20we%0Alearn%20to%20map%20these%20paired%20graphs%20to%20the%20sets%20of%20variables%20that%20were%20intervened%0Aupon%2C%20in%20a%20supervised%20learning%20framework.%20This%20approach%20consistently%0Aoutperforms%20baselines%20for%20perturbation%20modeling%20on%20seven%20single-cell%0Atranscriptomics%20datasets%2C%20each%20with%20thousands%20of%20measured%20variables.%20We%20also%0Ademonstrate%20significant%20improvements%20over%20six%20causal%20discovery%20algorithms%20in%0Apredicting%20intervention%20targets%20across%20a%20variety%20of%20tractable%2C%20synthetic%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520perturbation%2520targets%2520with%2520causal%2520differential%2520networks%26entry.906535625%3DMenghua%2520Wu%2520and%2520Umesh%2520Padia%2520and%2520Sean%2520H.%2520Murphy%2520and%2520Regina%2520Barzilay%2520and%2520Tommi%2520Jaakkola%26entry.1292438233%3D%2520%2520Rationally%2520identifying%2520variables%2520responsible%2520for%2520changes%2520to%2520a%2520biological%250Asystem%2520can%2520enable%2520myriad%2520applications%2520in%2520disease%2520understanding%2520and%2520cell%250Aengineering.%2520From%2520a%2520causality%2520perspective%252C%2520we%2520are%2520given%2520two%2520datasets%2520generated%250Aby%2520the%2520same%2520causal%2520model%252C%2520one%2520observational%2520%2528control%2529%2520and%2520one%2520interventional%250A%2528perturbed%2529.%2520The%2520goal%2520is%2520to%2520isolate%2520the%2520subset%2520of%2520measured%2520variables%2520%2528e.g.%250Agenes%2529%2520that%2520were%2520the%2520targets%2520of%2520the%2520intervention%252C%2520i.e.%2520those%2520whose%2520conditional%250Aindependencies%2520have%2520changed.%2520Knowing%2520the%2520causal%2520graph%2520would%2520limit%2520the%2520search%250Aspace%252C%2520allowing%2520us%2520to%2520efficiently%2520pinpoint%2520these%2520variables.%2520However%252C%2520current%250Aalgorithms%2520that%2520infer%2520causal%2520graphs%2520in%2520the%2520presence%2520of%2520unknown%2520intervention%250Atargets%2520scale%2520poorly%2520to%2520the%2520hundreds%2520or%2520thousands%2520of%2520variables%2520in%2520biological%250Adata%252C%2520as%2520they%2520must%2520jointly%2520search%2520the%2520combinatorial%2520spaces%2520of%2520graphs%2520and%250Aconsistent%2520intervention%2520targets.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520causality-inspired%250Aapproach%2520for%2520predicting%2520perturbation%2520targets%2520that%2520decouples%2520the%2520two%2520search%250Asteps.%2520First%252C%2520we%2520use%2520an%2520amortized%2520causal%2520discovery%2520model%2520to%2520separately%2520infer%250Acausal%2520graphs%2520from%2520the%2520observational%2520and%2520interventional%2520datasets.%2520Then%252C%2520we%250Alearn%2520to%2520map%2520these%2520paired%2520graphs%2520to%2520the%2520sets%2520of%2520variables%2520that%2520were%2520intervened%250Aupon%252C%2520in%2520a%2520supervised%2520learning%2520framework.%2520This%2520approach%2520consistently%250Aoutperforms%2520baselines%2520for%2520perturbation%2520modeling%2520on%2520seven%2520single-cell%250Atranscriptomics%2520datasets%252C%2520each%2520with%2520thousands%2520of%2520measured%2520variables.%2520We%2520also%250Ademonstrate%2520significant%2520improvements%2520over%2520six%2520causal%2520discovery%2520algorithms%2520in%250Apredicting%2520intervention%2520targets%2520across%2520a%2520variety%2520of%2520tractable%252C%2520synthetic%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20perturbation%20targets%20with%20causal%20differential%20networks&entry.906535625=Menghua%20Wu%20and%20Umesh%20Padia%20and%20Sean%20H.%20Murphy%20and%20Regina%20Barzilay%20and%20Tommi%20Jaakkola&entry.1292438233=%20%20Rationally%20identifying%20variables%20responsible%20for%20changes%20to%20a%20biological%0Asystem%20can%20enable%20myriad%20applications%20in%20disease%20understanding%20and%20cell%0Aengineering.%20From%20a%20causality%20perspective%2C%20we%20are%20given%20two%20datasets%20generated%0Aby%20the%20same%20causal%20model%2C%20one%20observational%20%28control%29%20and%20one%20interventional%0A%28perturbed%29.%20The%20goal%20is%20to%20isolate%20the%20subset%20of%20measured%20variables%20%28e.g.%0Agenes%29%20that%20were%20the%20targets%20of%20the%20intervention%2C%20i.e.%20those%20whose%20conditional%0Aindependencies%20have%20changed.%20Knowing%20the%20causal%20graph%20would%20limit%20the%20search%0Aspace%2C%20allowing%20us%20to%20efficiently%20pinpoint%20these%20variables.%20However%2C%20current%0Aalgorithms%20that%20infer%20causal%20graphs%20in%20the%20presence%20of%20unknown%20intervention%0Atargets%20scale%20poorly%20to%20the%20hundreds%20or%20thousands%20of%20variables%20in%20biological%0Adata%2C%20as%20they%20must%20jointly%20search%20the%20combinatorial%20spaces%20of%20graphs%20and%0Aconsistent%20intervention%20targets.%20In%20this%20work%2C%20we%20propose%20a%20causality-inspired%0Aapproach%20for%20predicting%20perturbation%20targets%20that%20decouples%20the%20two%20search%0Asteps.%20First%2C%20we%20use%20an%20amortized%20causal%20discovery%20model%20to%20separately%20infer%0Acausal%20graphs%20from%20the%20observational%20and%20interventional%20datasets.%20Then%2C%20we%0Alearn%20to%20map%20these%20paired%20graphs%20to%20the%20sets%20of%20variables%20that%20were%20intervened%0Aupon%2C%20in%20a%20supervised%20learning%20framework.%20This%20approach%20consistently%0Aoutperforms%20baselines%20for%20perturbation%20modeling%20on%20seven%20single-cell%0Atranscriptomics%20datasets%2C%20each%20with%20thousands%20of%20measured%20variables.%20We%20also%0Ademonstrate%20significant%20improvements%20over%20six%20causal%20discovery%20algorithms%20in%0Apredicting%20intervention%20targets%20across%20a%20variety%20of%20tractable%2C%20synthetic%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03380v1&entry.124074799=Read"},
{"title": "Demystifying the Token Dynamics of Deep Selective State Space Models", "author": "Thieu N Vo and Tung D. Pham and Xin T. Tong and Tan Minh Nguyen", "abstract": "  Selective state space models (SSM), such as Mamba, have gained prominence for\ntheir effectiveness in modeling sequential data. Despite their outstanding\nempirical performance, a comprehensive theoretical understanding of deep\nselective SSM remains elusive, hindering their further development and adoption\nfor applications that need high fidelity. In this paper, we investigate the\ndynamical properties of tokens in a pre-trained Mamba model. In particular, we\nderive the dynamical system governing the continuous-time limit of the Mamba\nmodel and characterize the asymptotic behavior of its solutions. In the\none-dimensional case, we prove that only one of the following two scenarios\nhappens: either all tokens converge to zero, or all tokens diverge to infinity.\nWe provide criteria based on model parameters to determine when each scenario\noccurs. For the convergent scenario, we empirically verify that this scenario\nnegatively impacts the model's performance. For the divergent scenario, we\nprove that different tokens will diverge to infinity at different rates,\nthereby contributing unequally to the updates during model training. Based on\nthese investigations, we propose two refinements for the model: excluding the\nconvergent scenario and reordering tokens based on their importance scores,\nboth aimed at improving practical performance. Our experimental results\nvalidate these refinements, offering insights into enhancing Mamba's\neffectiveness in real-world applications.\n", "link": "http://arxiv.org/abs/2410.03292v1", "date": "2024-10-04", "relevancy": 1.5533, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5486}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4991}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20the%20Token%20Dynamics%20of%20Deep%20Selective%20State%20Space%20Models&body=Title%3A%20Demystifying%20the%20Token%20Dynamics%20of%20Deep%20Selective%20State%20Space%20Models%0AAuthor%3A%20Thieu%20N%20Vo%20and%20Tung%20D.%20Pham%20and%20Xin%20T.%20Tong%20and%20Tan%20Minh%20Nguyen%0AAbstract%3A%20%20%20Selective%20state%20space%20models%20%28SSM%29%2C%20such%20as%20Mamba%2C%20have%20gained%20prominence%20for%0Atheir%20effectiveness%20in%20modeling%20sequential%20data.%20Despite%20their%20outstanding%0Aempirical%20performance%2C%20a%20comprehensive%20theoretical%20understanding%20of%20deep%0Aselective%20SSM%20remains%20elusive%2C%20hindering%20their%20further%20development%20and%20adoption%0Afor%20applications%20that%20need%20high%20fidelity.%20In%20this%20paper%2C%20we%20investigate%20the%0Adynamical%20properties%20of%20tokens%20in%20a%20pre-trained%20Mamba%20model.%20In%20particular%2C%20we%0Aderive%20the%20dynamical%20system%20governing%20the%20continuous-time%20limit%20of%20the%20Mamba%0Amodel%20and%20characterize%20the%20asymptotic%20behavior%20of%20its%20solutions.%20In%20the%0Aone-dimensional%20case%2C%20we%20prove%20that%20only%20one%20of%20the%20following%20two%20scenarios%0Ahappens%3A%20either%20all%20tokens%20converge%20to%20zero%2C%20or%20all%20tokens%20diverge%20to%20infinity.%0AWe%20provide%20criteria%20based%20on%20model%20parameters%20to%20determine%20when%20each%20scenario%0Aoccurs.%20For%20the%20convergent%20scenario%2C%20we%20empirically%20verify%20that%20this%20scenario%0Anegatively%20impacts%20the%20model%27s%20performance.%20For%20the%20divergent%20scenario%2C%20we%0Aprove%20that%20different%20tokens%20will%20diverge%20to%20infinity%20at%20different%20rates%2C%0Athereby%20contributing%20unequally%20to%20the%20updates%20during%20model%20training.%20Based%20on%0Athese%20investigations%2C%20we%20propose%20two%20refinements%20for%20the%20model%3A%20excluding%20the%0Aconvergent%20scenario%20and%20reordering%20tokens%20based%20on%20their%20importance%20scores%2C%0Aboth%20aimed%20at%20improving%20practical%20performance.%20Our%20experimental%20results%0Avalidate%20these%20refinements%2C%20offering%20insights%20into%20enhancing%20Mamba%27s%0Aeffectiveness%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520the%2520Token%2520Dynamics%2520of%2520Deep%2520Selective%2520State%2520Space%2520Models%26entry.906535625%3DThieu%2520N%2520Vo%2520and%2520Tung%2520D.%2520Pham%2520and%2520Xin%2520T.%2520Tong%2520and%2520Tan%2520Minh%2520Nguyen%26entry.1292438233%3D%2520%2520Selective%2520state%2520space%2520models%2520%2528SSM%2529%252C%2520such%2520as%2520Mamba%252C%2520have%2520gained%2520prominence%2520for%250Atheir%2520effectiveness%2520in%2520modeling%2520sequential%2520data.%2520Despite%2520their%2520outstanding%250Aempirical%2520performance%252C%2520a%2520comprehensive%2520theoretical%2520understanding%2520of%2520deep%250Aselective%2520SSM%2520remains%2520elusive%252C%2520hindering%2520their%2520further%2520development%2520and%2520adoption%250Afor%2520applications%2520that%2520need%2520high%2520fidelity.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Adynamical%2520properties%2520of%2520tokens%2520in%2520a%2520pre-trained%2520Mamba%2520model.%2520In%2520particular%252C%2520we%250Aderive%2520the%2520dynamical%2520system%2520governing%2520the%2520continuous-time%2520limit%2520of%2520the%2520Mamba%250Amodel%2520and%2520characterize%2520the%2520asymptotic%2520behavior%2520of%2520its%2520solutions.%2520In%2520the%250Aone-dimensional%2520case%252C%2520we%2520prove%2520that%2520only%2520one%2520of%2520the%2520following%2520two%2520scenarios%250Ahappens%253A%2520either%2520all%2520tokens%2520converge%2520to%2520zero%252C%2520or%2520all%2520tokens%2520diverge%2520to%2520infinity.%250AWe%2520provide%2520criteria%2520based%2520on%2520model%2520parameters%2520to%2520determine%2520when%2520each%2520scenario%250Aoccurs.%2520For%2520the%2520convergent%2520scenario%252C%2520we%2520empirically%2520verify%2520that%2520this%2520scenario%250Anegatively%2520impacts%2520the%2520model%2527s%2520performance.%2520For%2520the%2520divergent%2520scenario%252C%2520we%250Aprove%2520that%2520different%2520tokens%2520will%2520diverge%2520to%2520infinity%2520at%2520different%2520rates%252C%250Athereby%2520contributing%2520unequally%2520to%2520the%2520updates%2520during%2520model%2520training.%2520Based%2520on%250Athese%2520investigations%252C%2520we%2520propose%2520two%2520refinements%2520for%2520the%2520model%253A%2520excluding%2520the%250Aconvergent%2520scenario%2520and%2520reordering%2520tokens%2520based%2520on%2520their%2520importance%2520scores%252C%250Aboth%2520aimed%2520at%2520improving%2520practical%2520performance.%2520Our%2520experimental%2520results%250Avalidate%2520these%2520refinements%252C%2520offering%2520insights%2520into%2520enhancing%2520Mamba%2527s%250Aeffectiveness%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20the%20Token%20Dynamics%20of%20Deep%20Selective%20State%20Space%20Models&entry.906535625=Thieu%20N%20Vo%20and%20Tung%20D.%20Pham%20and%20Xin%20T.%20Tong%20and%20Tan%20Minh%20Nguyen&entry.1292438233=%20%20Selective%20state%20space%20models%20%28SSM%29%2C%20such%20as%20Mamba%2C%20have%20gained%20prominence%20for%0Atheir%20effectiveness%20in%20modeling%20sequential%20data.%20Despite%20their%20outstanding%0Aempirical%20performance%2C%20a%20comprehensive%20theoretical%20understanding%20of%20deep%0Aselective%20SSM%20remains%20elusive%2C%20hindering%20their%20further%20development%20and%20adoption%0Afor%20applications%20that%20need%20high%20fidelity.%20In%20this%20paper%2C%20we%20investigate%20the%0Adynamical%20properties%20of%20tokens%20in%20a%20pre-trained%20Mamba%20model.%20In%20particular%2C%20we%0Aderive%20the%20dynamical%20system%20governing%20the%20continuous-time%20limit%20of%20the%20Mamba%0Amodel%20and%20characterize%20the%20asymptotic%20behavior%20of%20its%20solutions.%20In%20the%0Aone-dimensional%20case%2C%20we%20prove%20that%20only%20one%20of%20the%20following%20two%20scenarios%0Ahappens%3A%20either%20all%20tokens%20converge%20to%20zero%2C%20or%20all%20tokens%20diverge%20to%20infinity.%0AWe%20provide%20criteria%20based%20on%20model%20parameters%20to%20determine%20when%20each%20scenario%0Aoccurs.%20For%20the%20convergent%20scenario%2C%20we%20empirically%20verify%20that%20this%20scenario%0Anegatively%20impacts%20the%20model%27s%20performance.%20For%20the%20divergent%20scenario%2C%20we%0Aprove%20that%20different%20tokens%20will%20diverge%20to%20infinity%20at%20different%20rates%2C%0Athereby%20contributing%20unequally%20to%20the%20updates%20during%20model%20training.%20Based%20on%0Athese%20investigations%2C%20we%20propose%20two%20refinements%20for%20the%20model%3A%20excluding%20the%0Aconvergent%20scenario%20and%20reordering%20tokens%20based%20on%20their%20importance%20scores%2C%0Aboth%20aimed%20at%20improving%20practical%20performance.%20Our%20experimental%20results%0Avalidate%20these%20refinements%2C%20offering%20insights%20into%20enhancing%20Mamba%27s%0Aeffectiveness%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03292v1&entry.124074799=Read"},
{"title": "Universal Functional Regression with Neural Operator Flows", "author": "Yaozhong Shi and Angela F. Gao and Zachary E. Ross and Kamyar Azizzadenesheli", "abstract": "  Regression on function spaces is typically limited to models with Gaussian\nprocess priors. We introduce the notion of universal functional regression, in\nwhich we aim to learn a prior distribution over non-Gaussian function spaces\nthat remains mathematically tractable for functional regression. To do this, we\ndevelop Neural Operator Flows (OpFlow), an infinite-dimensional extension of\nnormalizing flows. OpFlow is an invertible operator that maps the (potentially\nunknown) data function space into a Gaussian process, allowing for exact\nlikelihood estimation of functional point evaluations. OpFlow enables robust\nand accurate uncertainty quantification via drawing posterior samples of the\nGaussian process and subsequently mapping them into the data function space. We\nempirically study the performance of OpFlow on regression and generation tasks\nwith data generated from Gaussian processes with known posterior forms and\nnon-Gaussian processes, as well as real-world earthquake seismograms with an\nunknown closed-form distribution.\n", "link": "http://arxiv.org/abs/2404.02986v2", "date": "2024-10-04", "relevancy": 1.4499, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5255}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5015}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Functional%20Regression%20with%20Neural%20Operator%20Flows&body=Title%3A%20Universal%20Functional%20Regression%20with%20Neural%20Operator%20Flows%0AAuthor%3A%20Yaozhong%20Shi%20and%20Angela%20F.%20Gao%20and%20Zachary%20E.%20Ross%20and%20Kamyar%20Azizzadenesheli%0AAbstract%3A%20%20%20Regression%20on%20function%20spaces%20is%20typically%20limited%20to%20models%20with%20Gaussian%0Aprocess%20priors.%20We%20introduce%20the%20notion%20of%20universal%20functional%20regression%2C%20in%0Awhich%20we%20aim%20to%20learn%20a%20prior%20distribution%20over%20non-Gaussian%20function%20spaces%0Athat%20remains%20mathematically%20tractable%20for%20functional%20regression.%20To%20do%20this%2C%20we%0Adevelop%20Neural%20Operator%20Flows%20%28OpFlow%29%2C%20an%20infinite-dimensional%20extension%20of%0Anormalizing%20flows.%20OpFlow%20is%20an%20invertible%20operator%20that%20maps%20the%20%28potentially%0Aunknown%29%20data%20function%20space%20into%20a%20Gaussian%20process%2C%20allowing%20for%20exact%0Alikelihood%20estimation%20of%20functional%20point%20evaluations.%20OpFlow%20enables%20robust%0Aand%20accurate%20uncertainty%20quantification%20via%20drawing%20posterior%20samples%20of%20the%0AGaussian%20process%20and%20subsequently%20mapping%20them%20into%20the%20data%20function%20space.%20We%0Aempirically%20study%20the%20performance%20of%20OpFlow%20on%20regression%20and%20generation%20tasks%0Awith%20data%20generated%20from%20Gaussian%20processes%20with%20known%20posterior%20forms%20and%0Anon-Gaussian%20processes%2C%20as%20well%20as%20real-world%20earthquake%20seismograms%20with%20an%0Aunknown%20closed-form%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02986v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Functional%2520Regression%2520with%2520Neural%2520Operator%2520Flows%26entry.906535625%3DYaozhong%2520Shi%2520and%2520Angela%2520F.%2520Gao%2520and%2520Zachary%2520E.%2520Ross%2520and%2520Kamyar%2520Azizzadenesheli%26entry.1292438233%3D%2520%2520Regression%2520on%2520function%2520spaces%2520is%2520typically%2520limited%2520to%2520models%2520with%2520Gaussian%250Aprocess%2520priors.%2520We%2520introduce%2520the%2520notion%2520of%2520universal%2520functional%2520regression%252C%2520in%250Awhich%2520we%2520aim%2520to%2520learn%2520a%2520prior%2520distribution%2520over%2520non-Gaussian%2520function%2520spaces%250Athat%2520remains%2520mathematically%2520tractable%2520for%2520functional%2520regression.%2520To%2520do%2520this%252C%2520we%250Adevelop%2520Neural%2520Operator%2520Flows%2520%2528OpFlow%2529%252C%2520an%2520infinite-dimensional%2520extension%2520of%250Anormalizing%2520flows.%2520OpFlow%2520is%2520an%2520invertible%2520operator%2520that%2520maps%2520the%2520%2528potentially%250Aunknown%2529%2520data%2520function%2520space%2520into%2520a%2520Gaussian%2520process%252C%2520allowing%2520for%2520exact%250Alikelihood%2520estimation%2520of%2520functional%2520point%2520evaluations.%2520OpFlow%2520enables%2520robust%250Aand%2520accurate%2520uncertainty%2520quantification%2520via%2520drawing%2520posterior%2520samples%2520of%2520the%250AGaussian%2520process%2520and%2520subsequently%2520mapping%2520them%2520into%2520the%2520data%2520function%2520space.%2520We%250Aempirically%2520study%2520the%2520performance%2520of%2520OpFlow%2520on%2520regression%2520and%2520generation%2520tasks%250Awith%2520data%2520generated%2520from%2520Gaussian%2520processes%2520with%2520known%2520posterior%2520forms%2520and%250Anon-Gaussian%2520processes%252C%2520as%2520well%2520as%2520real-world%2520earthquake%2520seismograms%2520with%2520an%250Aunknown%2520closed-form%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02986v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Functional%20Regression%20with%20Neural%20Operator%20Flows&entry.906535625=Yaozhong%20Shi%20and%20Angela%20F.%20Gao%20and%20Zachary%20E.%20Ross%20and%20Kamyar%20Azizzadenesheli&entry.1292438233=%20%20Regression%20on%20function%20spaces%20is%20typically%20limited%20to%20models%20with%20Gaussian%0Aprocess%20priors.%20We%20introduce%20the%20notion%20of%20universal%20functional%20regression%2C%20in%0Awhich%20we%20aim%20to%20learn%20a%20prior%20distribution%20over%20non-Gaussian%20function%20spaces%0Athat%20remains%20mathematically%20tractable%20for%20functional%20regression.%20To%20do%20this%2C%20we%0Adevelop%20Neural%20Operator%20Flows%20%28OpFlow%29%2C%20an%20infinite-dimensional%20extension%20of%0Anormalizing%20flows.%20OpFlow%20is%20an%20invertible%20operator%20that%20maps%20the%20%28potentially%0Aunknown%29%20data%20function%20space%20into%20a%20Gaussian%20process%2C%20allowing%20for%20exact%0Alikelihood%20estimation%20of%20functional%20point%20evaluations.%20OpFlow%20enables%20robust%0Aand%20accurate%20uncertainty%20quantification%20via%20drawing%20posterior%20samples%20of%20the%0AGaussian%20process%20and%20subsequently%20mapping%20them%20into%20the%20data%20function%20space.%20We%0Aempirically%20study%20the%20performance%20of%20OpFlow%20on%20regression%20and%20generation%20tasks%0Awith%20data%20generated%20from%20Gaussian%20processes%20with%20known%20posterior%20forms%20and%0Anon-Gaussian%20processes%2C%20as%20well%20as%20real-world%20earthquake%20seismograms%20with%20an%0Aunknown%20closed-form%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02986v2&entry.124074799=Read"},
{"title": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples", "author": "Fangxu Yu and Lai Jiang and Haoqiang Kang and Shibo Hao and Lianhui Qin", "abstract": "  The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reinforcement learning aims to find limited highest-reward\nsolutions while neglecting the solution diversity. To fill this gap, we propose\nFlow of Reasoning (FoR), an efficient diversity-seeking LLM finetuning method\naimed at improving reasoning quality and diversity with minimal data. FoR\nformulates multi-step LLM reasoning as a Markovian flow on a DAG-structured\nreasoning graph. This formulation allows us to incorporate and adapt principled\nGFlowNet approaches, for finetuning LLMs to sample diverse reasoning paths with\nprobabilities proportional to the (unnormalized) reward of target problems.\nExtensive experiments show that, with limited training examples (e.g., 15\nexamples), FoR enables the discovery of diverse, creative, high-quality\nsolutions, greatly outperforming a wide range of existing inference and\ntraining methods across five challenging puzzle-solving tasks, including\nBlocksWorld (embodied reasoning), Game24 (math puzzle solving), Rubik's Cube\n(spatial reasoning), 1D-ARC (abstraction reasoning), and PrOntoQA (logical\nreasoning). Code is available at https://github.com/Yu-Fangxu/FoR.\n", "link": "http://arxiv.org/abs/2406.05673v3", "date": "2024-10-04", "relevancy": 1.5436, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.523}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20of%20Reasoning%3ATraining%20LLMs%20for%20Divergent%20Problem%20Solving%20with%0A%20%20Minimal%20Examples&body=Title%3A%20Flow%20of%20Reasoning%3ATraining%20LLMs%20for%20Divergent%20Problem%20Solving%20with%0A%20%20Minimal%20Examples%0AAuthor%3A%20Fangxu%20Yu%20and%20Lai%20Jiang%20and%20Haoqiang%20Kang%20and%20Shibo%20Hao%20and%20Lianhui%20Qin%0AAbstract%3A%20%20%20The%20ability%20to%20generate%20diverse%20solutions%20to%20a%20given%20problem%20is%20a%20hallmark%20of%0Ahuman%20creativity.%20This%20divergent%20reasoning%20is%20also%20crucial%20for%20machines%2C%0Aenhancing%20their%20robustness%20and%20enabling%20them%20to%20assist%20humans%20in%20many%0Aapplications%20such%20as%20scientific%20discovery.%20However%2C%20existing%20approaches%20to%0Amulti-step%20reasoning%20with%20large%20language%20models%20%28LLMs%29%20have%20mostly%20focused%20only%0Aon%20reasoning%20accuracy%2C%20without%20further%20discovering%20more%20diverse%20valid%0Asolutions.%20For%20example%2C%20supervised%20fine-tuning%20can%20improve%20LLM%20reasoning%0Aquality%2C%20but%20requires%20extensive%20supervised%20data%20to%20capture%20the%20full%20range%20of%0Apossible%20solutions.%20Reinforcement%20learning%20aims%20to%20find%20limited%20highest-reward%0Asolutions%20while%20neglecting%20the%20solution%20diversity.%20To%20fill%20this%20gap%2C%20we%20propose%0AFlow%20of%20Reasoning%20%28FoR%29%2C%20an%20efficient%20diversity-seeking%20LLM%20finetuning%20method%0Aaimed%20at%20improving%20reasoning%20quality%20and%20diversity%20with%20minimal%20data.%20FoR%0Aformulates%20multi-step%20LLM%20reasoning%20as%20a%20Markovian%20flow%20on%20a%20DAG-structured%0Areasoning%20graph.%20This%20formulation%20allows%20us%20to%20incorporate%20and%20adapt%20principled%0AGFlowNet%20approaches%2C%20for%20finetuning%20LLMs%20to%20sample%20diverse%20reasoning%20paths%20with%0Aprobabilities%20proportional%20to%20the%20%28unnormalized%29%20reward%20of%20target%20problems.%0AExtensive%20experiments%20show%20that%2C%20with%20limited%20training%20examples%20%28e.g.%2C%2015%0Aexamples%29%2C%20FoR%20enables%20the%20discovery%20of%20diverse%2C%20creative%2C%20high-quality%0Asolutions%2C%20greatly%20outperforming%20a%20wide%20range%20of%20existing%20inference%20and%0Atraining%20methods%20across%20five%20challenging%20puzzle-solving%20tasks%2C%20including%0ABlocksWorld%20%28embodied%20reasoning%29%2C%20Game24%20%28math%20puzzle%20solving%29%2C%20Rubik%27s%20Cube%0A%28spatial%20reasoning%29%2C%201D-ARC%20%28abstraction%20reasoning%29%2C%20and%20PrOntoQA%20%28logical%0Areasoning%29.%20Code%20is%20available%20at%20https%3A//github.com/Yu-Fangxu/FoR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05673v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520of%2520Reasoning%253ATraining%2520LLMs%2520for%2520Divergent%2520Problem%2520Solving%2520with%250A%2520%2520Minimal%2520Examples%26entry.906535625%3DFangxu%2520Yu%2520and%2520Lai%2520Jiang%2520and%2520Haoqiang%2520Kang%2520and%2520Shibo%2520Hao%2520and%2520Lianhui%2520Qin%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520generate%2520diverse%2520solutions%2520to%2520a%2520given%2520problem%2520is%2520a%2520hallmark%2520of%250Ahuman%2520creativity.%2520This%2520divergent%2520reasoning%2520is%2520also%2520crucial%2520for%2520machines%252C%250Aenhancing%2520their%2520robustness%2520and%2520enabling%2520them%2520to%2520assist%2520humans%2520in%2520many%250Aapplications%2520such%2520as%2520scientific%2520discovery.%2520However%252C%2520existing%2520approaches%2520to%250Amulti-step%2520reasoning%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520mostly%2520focused%2520only%250Aon%2520reasoning%2520accuracy%252C%2520without%2520further%2520discovering%2520more%2520diverse%2520valid%250Asolutions.%2520For%2520example%252C%2520supervised%2520fine-tuning%2520can%2520improve%2520LLM%2520reasoning%250Aquality%252C%2520but%2520requires%2520extensive%2520supervised%2520data%2520to%2520capture%2520the%2520full%2520range%2520of%250Apossible%2520solutions.%2520Reinforcement%2520learning%2520aims%2520to%2520find%2520limited%2520highest-reward%250Asolutions%2520while%2520neglecting%2520the%2520solution%2520diversity.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%250AFlow%2520of%2520Reasoning%2520%2528FoR%2529%252C%2520an%2520efficient%2520diversity-seeking%2520LLM%2520finetuning%2520method%250Aaimed%2520at%2520improving%2520reasoning%2520quality%2520and%2520diversity%2520with%2520minimal%2520data.%2520FoR%250Aformulates%2520multi-step%2520LLM%2520reasoning%2520as%2520a%2520Markovian%2520flow%2520on%2520a%2520DAG-structured%250Areasoning%2520graph.%2520This%2520formulation%2520allows%2520us%2520to%2520incorporate%2520and%2520adapt%2520principled%250AGFlowNet%2520approaches%252C%2520for%2520finetuning%2520LLMs%2520to%2520sample%2520diverse%2520reasoning%2520paths%2520with%250Aprobabilities%2520proportional%2520to%2520the%2520%2528unnormalized%2529%2520reward%2520of%2520target%2520problems.%250AExtensive%2520experiments%2520show%2520that%252C%2520with%2520limited%2520training%2520examples%2520%2528e.g.%252C%252015%250Aexamples%2529%252C%2520FoR%2520enables%2520the%2520discovery%2520of%2520diverse%252C%2520creative%252C%2520high-quality%250Asolutions%252C%2520greatly%2520outperforming%2520a%2520wide%2520range%2520of%2520existing%2520inference%2520and%250Atraining%2520methods%2520across%2520five%2520challenging%2520puzzle-solving%2520tasks%252C%2520including%250ABlocksWorld%2520%2528embodied%2520reasoning%2529%252C%2520Game24%2520%2528math%2520puzzle%2520solving%2529%252C%2520Rubik%2527s%2520Cube%250A%2528spatial%2520reasoning%2529%252C%25201D-ARC%2520%2528abstraction%2520reasoning%2529%252C%2520and%2520PrOntoQA%2520%2528logical%250Areasoning%2529.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Yu-Fangxu/FoR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05673v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20of%20Reasoning%3ATraining%20LLMs%20for%20Divergent%20Problem%20Solving%20with%0A%20%20Minimal%20Examples&entry.906535625=Fangxu%20Yu%20and%20Lai%20Jiang%20and%20Haoqiang%20Kang%20and%20Shibo%20Hao%20and%20Lianhui%20Qin&entry.1292438233=%20%20The%20ability%20to%20generate%20diverse%20solutions%20to%20a%20given%20problem%20is%20a%20hallmark%20of%0Ahuman%20creativity.%20This%20divergent%20reasoning%20is%20also%20crucial%20for%20machines%2C%0Aenhancing%20their%20robustness%20and%20enabling%20them%20to%20assist%20humans%20in%20many%0Aapplications%20such%20as%20scientific%20discovery.%20However%2C%20existing%20approaches%20to%0Amulti-step%20reasoning%20with%20large%20language%20models%20%28LLMs%29%20have%20mostly%20focused%20only%0Aon%20reasoning%20accuracy%2C%20without%20further%20discovering%20more%20diverse%20valid%0Asolutions.%20For%20example%2C%20supervised%20fine-tuning%20can%20improve%20LLM%20reasoning%0Aquality%2C%20but%20requires%20extensive%20supervised%20data%20to%20capture%20the%20full%20range%20of%0Apossible%20solutions.%20Reinforcement%20learning%20aims%20to%20find%20limited%20highest-reward%0Asolutions%20while%20neglecting%20the%20solution%20diversity.%20To%20fill%20this%20gap%2C%20we%20propose%0AFlow%20of%20Reasoning%20%28FoR%29%2C%20an%20efficient%20diversity-seeking%20LLM%20finetuning%20method%0Aaimed%20at%20improving%20reasoning%20quality%20and%20diversity%20with%20minimal%20data.%20FoR%0Aformulates%20multi-step%20LLM%20reasoning%20as%20a%20Markovian%20flow%20on%20a%20DAG-structured%0Areasoning%20graph.%20This%20formulation%20allows%20us%20to%20incorporate%20and%20adapt%20principled%0AGFlowNet%20approaches%2C%20for%20finetuning%20LLMs%20to%20sample%20diverse%20reasoning%20paths%20with%0Aprobabilities%20proportional%20to%20the%20%28unnormalized%29%20reward%20of%20target%20problems.%0AExtensive%20experiments%20show%20that%2C%20with%20limited%20training%20examples%20%28e.g.%2C%2015%0Aexamples%29%2C%20FoR%20enables%20the%20discovery%20of%20diverse%2C%20creative%2C%20high-quality%0Asolutions%2C%20greatly%20outperforming%20a%20wide%20range%20of%20existing%20inference%20and%0Atraining%20methods%20across%20five%20challenging%20puzzle-solving%20tasks%2C%20including%0ABlocksWorld%20%28embodied%20reasoning%29%2C%20Game24%20%28math%20puzzle%20solving%29%2C%20Rubik%27s%20Cube%0A%28spatial%20reasoning%29%2C%201D-ARC%20%28abstraction%20reasoning%29%2C%20and%20PrOntoQA%20%28logical%0Areasoning%29.%20Code%20is%20available%20at%20https%3A//github.com/Yu-Fangxu/FoR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05673v3&entry.124074799=Read"},
{"title": "A Service Robot in the Wild: Analysis of Users Intentions, Robot\n  Behaviors, and Their Impact on the Interaction", "author": "Simone Arreghini and Gabriele Abbate and Alessandro Giusti and Antonio Paolillo", "abstract": "  We consider a service robot that offers chocolate treats to people passing in\nits proximity: it has the capability of predicting in advance a person's\nintention to interact, and to actuate an \"offering\" gesture, subtly extending\nthe tray of chocolates towards a given target. We run the system for more than\n5 hours across 3 days and two different crowded public locations; the system\nimplements three possible behaviors that are randomly toggled every few\nminutes: passive (e.g. never performing the offering gesture); or active,\ntriggered by either a naive distance-based rule, or a smart approach that\nrelies on various behavioral cues of the user. We collect a real-world dataset\nthat includes information on 1777 users with several spontaneous human-robot\ninteractions and study the influence of robot actions on people's behavior. Our\ncomprehensive analysis suggests that users are more prone to engage with the\nrobot when it proactively starts the interaction. We release the dataset and\nprovide insights to make our work reproducible for the community. Also, we\nreport qualitative observations collected during the acquisition campaign and\nidentify future challenges and research directions in the domain of social\nhuman-robot interaction.\n", "link": "http://arxiv.org/abs/2410.03287v1", "date": "2024-10-04", "relevancy": 1.595, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.602}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5378}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Service%20Robot%20in%20the%20Wild%3A%20Analysis%20of%20Users%20Intentions%2C%20Robot%0A%20%20Behaviors%2C%20and%20Their%20Impact%20on%20the%20Interaction&body=Title%3A%20A%20Service%20Robot%20in%20the%20Wild%3A%20Analysis%20of%20Users%20Intentions%2C%20Robot%0A%20%20Behaviors%2C%20and%20Their%20Impact%20on%20the%20Interaction%0AAuthor%3A%20Simone%20Arreghini%20and%20Gabriele%20Abbate%20and%20Alessandro%20Giusti%20and%20Antonio%20Paolillo%0AAbstract%3A%20%20%20We%20consider%20a%20service%20robot%20that%20offers%20chocolate%20treats%20to%20people%20passing%20in%0Aits%20proximity%3A%20it%20has%20the%20capability%20of%20predicting%20in%20advance%20a%20person%27s%0Aintention%20to%20interact%2C%20and%20to%20actuate%20an%20%22offering%22%20gesture%2C%20subtly%20extending%0Athe%20tray%20of%20chocolates%20towards%20a%20given%20target.%20We%20run%20the%20system%20for%20more%20than%0A5%20hours%20across%203%20days%20and%20two%20different%20crowded%20public%20locations%3B%20the%20system%0Aimplements%20three%20possible%20behaviors%20that%20are%20randomly%20toggled%20every%20few%0Aminutes%3A%20passive%20%28e.g.%20never%20performing%20the%20offering%20gesture%29%3B%20or%20active%2C%0Atriggered%20by%20either%20a%20naive%20distance-based%20rule%2C%20or%20a%20smart%20approach%20that%0Arelies%20on%20various%20behavioral%20cues%20of%20the%20user.%20We%20collect%20a%20real-world%20dataset%0Athat%20includes%20information%20on%201777%20users%20with%20several%20spontaneous%20human-robot%0Ainteractions%20and%20study%20the%20influence%20of%20robot%20actions%20on%20people%27s%20behavior.%20Our%0Acomprehensive%20analysis%20suggests%20that%20users%20are%20more%20prone%20to%20engage%20with%20the%0Arobot%20when%20it%20proactively%20starts%20the%20interaction.%20We%20release%20the%20dataset%20and%0Aprovide%20insights%20to%20make%20our%20work%20reproducible%20for%20the%20community.%20Also%2C%20we%0Areport%20qualitative%20observations%20collected%20during%20the%20acquisition%20campaign%20and%0Aidentify%20future%20challenges%20and%20research%20directions%20in%20the%20domain%20of%20social%0Ahuman-robot%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Service%2520Robot%2520in%2520the%2520Wild%253A%2520Analysis%2520of%2520Users%2520Intentions%252C%2520Robot%250A%2520%2520Behaviors%252C%2520and%2520Their%2520Impact%2520on%2520the%2520Interaction%26entry.906535625%3DSimone%2520Arreghini%2520and%2520Gabriele%2520Abbate%2520and%2520Alessandro%2520Giusti%2520and%2520Antonio%2520Paolillo%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520service%2520robot%2520that%2520offers%2520chocolate%2520treats%2520to%2520people%2520passing%2520in%250Aits%2520proximity%253A%2520it%2520has%2520the%2520capability%2520of%2520predicting%2520in%2520advance%2520a%2520person%2527s%250Aintention%2520to%2520interact%252C%2520and%2520to%2520actuate%2520an%2520%2522offering%2522%2520gesture%252C%2520subtly%2520extending%250Athe%2520tray%2520of%2520chocolates%2520towards%2520a%2520given%2520target.%2520We%2520run%2520the%2520system%2520for%2520more%2520than%250A5%2520hours%2520across%25203%2520days%2520and%2520two%2520different%2520crowded%2520public%2520locations%253B%2520the%2520system%250Aimplements%2520three%2520possible%2520behaviors%2520that%2520are%2520randomly%2520toggled%2520every%2520few%250Aminutes%253A%2520passive%2520%2528e.g.%2520never%2520performing%2520the%2520offering%2520gesture%2529%253B%2520or%2520active%252C%250Atriggered%2520by%2520either%2520a%2520naive%2520distance-based%2520rule%252C%2520or%2520a%2520smart%2520approach%2520that%250Arelies%2520on%2520various%2520behavioral%2520cues%2520of%2520the%2520user.%2520We%2520collect%2520a%2520real-world%2520dataset%250Athat%2520includes%2520information%2520on%25201777%2520users%2520with%2520several%2520spontaneous%2520human-robot%250Ainteractions%2520and%2520study%2520the%2520influence%2520of%2520robot%2520actions%2520on%2520people%2527s%2520behavior.%2520Our%250Acomprehensive%2520analysis%2520suggests%2520that%2520users%2520are%2520more%2520prone%2520to%2520engage%2520with%2520the%250Arobot%2520when%2520it%2520proactively%2520starts%2520the%2520interaction.%2520We%2520release%2520the%2520dataset%2520and%250Aprovide%2520insights%2520to%2520make%2520our%2520work%2520reproducible%2520for%2520the%2520community.%2520Also%252C%2520we%250Areport%2520qualitative%2520observations%2520collected%2520during%2520the%2520acquisition%2520campaign%2520and%250Aidentify%2520future%2520challenges%2520and%2520research%2520directions%2520in%2520the%2520domain%2520of%2520social%250Ahuman-robot%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Service%20Robot%20in%20the%20Wild%3A%20Analysis%20of%20Users%20Intentions%2C%20Robot%0A%20%20Behaviors%2C%20and%20Their%20Impact%20on%20the%20Interaction&entry.906535625=Simone%20Arreghini%20and%20Gabriele%20Abbate%20and%20Alessandro%20Giusti%20and%20Antonio%20Paolillo&entry.1292438233=%20%20We%20consider%20a%20service%20robot%20that%20offers%20chocolate%20treats%20to%20people%20passing%20in%0Aits%20proximity%3A%20it%20has%20the%20capability%20of%20predicting%20in%20advance%20a%20person%27s%0Aintention%20to%20interact%2C%20and%20to%20actuate%20an%20%22offering%22%20gesture%2C%20subtly%20extending%0Athe%20tray%20of%20chocolates%20towards%20a%20given%20target.%20We%20run%20the%20system%20for%20more%20than%0A5%20hours%20across%203%20days%20and%20two%20different%20crowded%20public%20locations%3B%20the%20system%0Aimplements%20three%20possible%20behaviors%20that%20are%20randomly%20toggled%20every%20few%0Aminutes%3A%20passive%20%28e.g.%20never%20performing%20the%20offering%20gesture%29%3B%20or%20active%2C%0Atriggered%20by%20either%20a%20naive%20distance-based%20rule%2C%20or%20a%20smart%20approach%20that%0Arelies%20on%20various%20behavioral%20cues%20of%20the%20user.%20We%20collect%20a%20real-world%20dataset%0Athat%20includes%20information%20on%201777%20users%20with%20several%20spontaneous%20human-robot%0Ainteractions%20and%20study%20the%20influence%20of%20robot%20actions%20on%20people%27s%20behavior.%20Our%0Acomprehensive%20analysis%20suggests%20that%20users%20are%20more%20prone%20to%20engage%20with%20the%0Arobot%20when%20it%20proactively%20starts%20the%20interaction.%20We%20release%20the%20dataset%20and%0Aprovide%20insights%20to%20make%20our%20work%20reproducible%20for%20the%20community.%20Also%2C%20we%0Areport%20qualitative%20observations%20collected%20during%20the%20acquisition%20campaign%20and%0Aidentify%20future%20challenges%20and%20research%20directions%20in%20the%20domain%20of%20social%0Ahuman-robot%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03287v1&entry.124074799=Read"},
{"title": "Predictive Coding for Decision Transformer", "author": "Tung M. Luu and Donghoon Lee and Chang D. Yoo", "abstract": "  Recent work in offline reinforcement learning (RL) has demonstrated the\neffectiveness of formulating decision-making as return-conditioned supervised\nlearning. Notably, the decision transformer (DT) architecture has shown promise\nacross various domains. However, despite its initial success, DTs have\nunderperformed on several challenging datasets in goal-conditioned RL. This\nlimitation stems from the inefficiency of return conditioning for guiding\npolicy learning, particularly in unstructured and suboptimal datasets,\nresulting in DTs failing to effectively learn temporal compositionality.\nMoreover, this problem might be further exacerbated in long-horizon\nsparse-reward tasks. To address this challenge, we propose the Predictive\nCoding for Decision Transformer (PCDT) framework, which leverages generalized\nfuture conditioning to enhance DT methods. PCDT utilizes an architecture that\nextends the DT framework, conditioned on predictive codings, enabling\ndecision-making based on both past and future factors, thereby improving\ngeneralization. Through extensive experiments on eight datasets from the\nAntMaze and FrankaKitchen environments, our proposed method achieves\nperformance on par with or surpassing existing popular value-based and\ntransformer-based methods in offline goal-conditioned RL. Furthermore, we also\nevaluate our method on a goal-reaching task with a physical robot.\n", "link": "http://arxiv.org/abs/2410.03408v1", "date": "2024-10-04", "relevancy": 1.0155, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5166}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5065}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictive%20Coding%20for%20Decision%20Transformer&body=Title%3A%20Predictive%20Coding%20for%20Decision%20Transformer%0AAuthor%3A%20Tung%20M.%20Luu%20and%20Donghoon%20Lee%20and%20Chang%20D.%20Yoo%0AAbstract%3A%20%20%20Recent%20work%20in%20offline%20reinforcement%20learning%20%28RL%29%20has%20demonstrated%20the%0Aeffectiveness%20of%20formulating%20decision-making%20as%20return-conditioned%20supervised%0Alearning.%20Notably%2C%20the%20decision%20transformer%20%28DT%29%20architecture%20has%20shown%20promise%0Aacross%20various%20domains.%20However%2C%20despite%20its%20initial%20success%2C%20DTs%20have%0Aunderperformed%20on%20several%20challenging%20datasets%20in%20goal-conditioned%20RL.%20This%0Alimitation%20stems%20from%20the%20inefficiency%20of%20return%20conditioning%20for%20guiding%0Apolicy%20learning%2C%20particularly%20in%20unstructured%20and%20suboptimal%20datasets%2C%0Aresulting%20in%20DTs%20failing%20to%20effectively%20learn%20temporal%20compositionality.%0AMoreover%2C%20this%20problem%20might%20be%20further%20exacerbated%20in%20long-horizon%0Asparse-reward%20tasks.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Predictive%0ACoding%20for%20Decision%20Transformer%20%28PCDT%29%20framework%2C%20which%20leverages%20generalized%0Afuture%20conditioning%20to%20enhance%20DT%20methods.%20PCDT%20utilizes%20an%20architecture%20that%0Aextends%20the%20DT%20framework%2C%20conditioned%20on%20predictive%20codings%2C%20enabling%0Adecision-making%20based%20on%20both%20past%20and%20future%20factors%2C%20thereby%20improving%0Ageneralization.%20Through%20extensive%20experiments%20on%20eight%20datasets%20from%20the%0AAntMaze%20and%20FrankaKitchen%20environments%2C%20our%20proposed%20method%20achieves%0Aperformance%20on%20par%20with%20or%20surpassing%20existing%20popular%20value-based%20and%0Atransformer-based%20methods%20in%20offline%20goal-conditioned%20RL.%20Furthermore%2C%20we%20also%0Aevaluate%20our%20method%20on%20a%20goal-reaching%20task%20with%20a%20physical%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictive%2520Coding%2520for%2520Decision%2520Transformer%26entry.906535625%3DTung%2520M.%2520Luu%2520and%2520Donghoon%2520Lee%2520and%2520Chang%2520D.%2520Yoo%26entry.1292438233%3D%2520%2520Recent%2520work%2520in%2520offline%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520demonstrated%2520the%250Aeffectiveness%2520of%2520formulating%2520decision-making%2520as%2520return-conditioned%2520supervised%250Alearning.%2520Notably%252C%2520the%2520decision%2520transformer%2520%2528DT%2529%2520architecture%2520has%2520shown%2520promise%250Aacross%2520various%2520domains.%2520However%252C%2520despite%2520its%2520initial%2520success%252C%2520DTs%2520have%250Aunderperformed%2520on%2520several%2520challenging%2520datasets%2520in%2520goal-conditioned%2520RL.%2520This%250Alimitation%2520stems%2520from%2520the%2520inefficiency%2520of%2520return%2520conditioning%2520for%2520guiding%250Apolicy%2520learning%252C%2520particularly%2520in%2520unstructured%2520and%2520suboptimal%2520datasets%252C%250Aresulting%2520in%2520DTs%2520failing%2520to%2520effectively%2520learn%2520temporal%2520compositionality.%250AMoreover%252C%2520this%2520problem%2520might%2520be%2520further%2520exacerbated%2520in%2520long-horizon%250Asparse-reward%2520tasks.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520the%2520Predictive%250ACoding%2520for%2520Decision%2520Transformer%2520%2528PCDT%2529%2520framework%252C%2520which%2520leverages%2520generalized%250Afuture%2520conditioning%2520to%2520enhance%2520DT%2520methods.%2520PCDT%2520utilizes%2520an%2520architecture%2520that%250Aextends%2520the%2520DT%2520framework%252C%2520conditioned%2520on%2520predictive%2520codings%252C%2520enabling%250Adecision-making%2520based%2520on%2520both%2520past%2520and%2520future%2520factors%252C%2520thereby%2520improving%250Ageneralization.%2520Through%2520extensive%2520experiments%2520on%2520eight%2520datasets%2520from%2520the%250AAntMaze%2520and%2520FrankaKitchen%2520environments%252C%2520our%2520proposed%2520method%2520achieves%250Aperformance%2520on%2520par%2520with%2520or%2520surpassing%2520existing%2520popular%2520value-based%2520and%250Atransformer-based%2520methods%2520in%2520offline%2520goal-conditioned%2520RL.%2520Furthermore%252C%2520we%2520also%250Aevaluate%2520our%2520method%2520on%2520a%2520goal-reaching%2520task%2520with%2520a%2520physical%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Coding%20for%20Decision%20Transformer&entry.906535625=Tung%20M.%20Luu%20and%20Donghoon%20Lee%20and%20Chang%20D.%20Yoo&entry.1292438233=%20%20Recent%20work%20in%20offline%20reinforcement%20learning%20%28RL%29%20has%20demonstrated%20the%0Aeffectiveness%20of%20formulating%20decision-making%20as%20return-conditioned%20supervised%0Alearning.%20Notably%2C%20the%20decision%20transformer%20%28DT%29%20architecture%20has%20shown%20promise%0Aacross%20various%20domains.%20However%2C%20despite%20its%20initial%20success%2C%20DTs%20have%0Aunderperformed%20on%20several%20challenging%20datasets%20in%20goal-conditioned%20RL.%20This%0Alimitation%20stems%20from%20the%20inefficiency%20of%20return%20conditioning%20for%20guiding%0Apolicy%20learning%2C%20particularly%20in%20unstructured%20and%20suboptimal%20datasets%2C%0Aresulting%20in%20DTs%20failing%20to%20effectively%20learn%20temporal%20compositionality.%0AMoreover%2C%20this%20problem%20might%20be%20further%20exacerbated%20in%20long-horizon%0Asparse-reward%20tasks.%20To%20address%20this%20challenge%2C%20we%20propose%20the%20Predictive%0ACoding%20for%20Decision%20Transformer%20%28PCDT%29%20framework%2C%20which%20leverages%20generalized%0Afuture%20conditioning%20to%20enhance%20DT%20methods.%20PCDT%20utilizes%20an%20architecture%20that%0Aextends%20the%20DT%20framework%2C%20conditioned%20on%20predictive%20codings%2C%20enabling%0Adecision-making%20based%20on%20both%20past%20and%20future%20factors%2C%20thereby%20improving%0Ageneralization.%20Through%20extensive%20experiments%20on%20eight%20datasets%20from%20the%0AAntMaze%20and%20FrankaKitchen%20environments%2C%20our%20proposed%20method%20achieves%0Aperformance%20on%20par%20with%20or%20surpassing%20existing%20popular%20value-based%20and%0Atransformer-based%20methods%20in%20offline%20goal-conditioned%20RL.%20Furthermore%2C%20we%20also%0Aevaluate%20our%20method%20on%20a%20goal-reaching%20task%20with%20a%20physical%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03408v1&entry.124074799=Read"},
{"title": "Comparative study of regression vs pairwise models for surrogate-based\n  heuristic optimisation", "author": "Pablo S. Naharro and Pablo Toharia and Antonio LaTorre and Jos\u00e9-Mar\u00eda Pe\u00f1a", "abstract": "  Heuristic optimisation algorithms explore the search space by sampling\nsolutions, evaluating their fitness, and biasing the search in the direction of\npromising solutions. However, in many cases, this fitness function involves\nexecuting expensive computational calculations, drastically reducing the\nreasonable number of evaluations. In this context, surrogate models have\nemerged as an excellent alternative to alleviate these computational problems.\nThis paper addresses the formulation of surrogate problems as both regression\nmodels that approximate fitness (surface surrogate models) and a novel way to\nconnect classification models (pairwise surrogate models). The pairwise\napproach can be directly exploited by some algorithms, such as Differential\nEvolution, in which the fitness value is not actually needed to drive the\nsearch, and it is sufficient to know whether a solution is better than another\none or not. Based on these modelling approaches, we have conducted a\nmultidimensional analysis of surrogate models under different configurations:\ndifferent machine learning algorithms (regularised regression, neural networks,\ndecision trees, boosting methods, and random forests), different surrogate\nstrategies (encouraging diversity or relaxing prediction thresholds), and\ncompare them for both surface and pairwise surrogate models. The experimental\npart of the article includes the benchmark problems already proposed for the\nSOCO2011 competition in continuous optimisation and a simulation problem\nincluded in the recent GECCO2021 Industrial Challenge. This paper shows that\nthe performance of the overall search, when using online machine learning-based\nsurrogate models, depends not only on the accuracy of the predictive model but\nalso on both the kind of bias towards positive or negative cases and how the\noptimisation uses those predictions to decide whether to execute the actual\nfitness function.\n", "link": "http://arxiv.org/abs/2410.03409v1", "date": "2024-10-04", "relevancy": 1.3797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.517}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4699}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20study%20of%20regression%20vs%20pairwise%20models%20for%20surrogate-based%0A%20%20heuristic%20optimisation&body=Title%3A%20Comparative%20study%20of%20regression%20vs%20pairwise%20models%20for%20surrogate-based%0A%20%20heuristic%20optimisation%0AAuthor%3A%20Pablo%20S.%20Naharro%20and%20Pablo%20Toharia%20and%20Antonio%20LaTorre%20and%20Jos%C3%A9-Mar%C3%ADa%20Pe%C3%B1a%0AAbstract%3A%20%20%20Heuristic%20optimisation%20algorithms%20explore%20the%20search%20space%20by%20sampling%0Asolutions%2C%20evaluating%20their%20fitness%2C%20and%20biasing%20the%20search%20in%20the%20direction%20of%0Apromising%20solutions.%20However%2C%20in%20many%20cases%2C%20this%20fitness%20function%20involves%0Aexecuting%20expensive%20computational%20calculations%2C%20drastically%20reducing%20the%0Areasonable%20number%20of%20evaluations.%20In%20this%20context%2C%20surrogate%20models%20have%0Aemerged%20as%20an%20excellent%20alternative%20to%20alleviate%20these%20computational%20problems.%0AThis%20paper%20addresses%20the%20formulation%20of%20surrogate%20problems%20as%20both%20regression%0Amodels%20that%20approximate%20fitness%20%28surface%20surrogate%20models%29%20and%20a%20novel%20way%20to%0Aconnect%20classification%20models%20%28pairwise%20surrogate%20models%29.%20The%20pairwise%0Aapproach%20can%20be%20directly%20exploited%20by%20some%20algorithms%2C%20such%20as%20Differential%0AEvolution%2C%20in%20which%20the%20fitness%20value%20is%20not%20actually%20needed%20to%20drive%20the%0Asearch%2C%20and%20it%20is%20sufficient%20to%20know%20whether%20a%20solution%20is%20better%20than%20another%0Aone%20or%20not.%20Based%20on%20these%20modelling%20approaches%2C%20we%20have%20conducted%20a%0Amultidimensional%20analysis%20of%20surrogate%20models%20under%20different%20configurations%3A%0Adifferent%20machine%20learning%20algorithms%20%28regularised%20regression%2C%20neural%20networks%2C%0Adecision%20trees%2C%20boosting%20methods%2C%20and%20random%20forests%29%2C%20different%20surrogate%0Astrategies%20%28encouraging%20diversity%20or%20relaxing%20prediction%20thresholds%29%2C%20and%0Acompare%20them%20for%20both%20surface%20and%20pairwise%20surrogate%20models.%20The%20experimental%0Apart%20of%20the%20article%20includes%20the%20benchmark%20problems%20already%20proposed%20for%20the%0ASOCO2011%20competition%20in%20continuous%20optimisation%20and%20a%20simulation%20problem%0Aincluded%20in%20the%20recent%20GECCO2021%20Industrial%20Challenge.%20This%20paper%20shows%20that%0Athe%20performance%20of%20the%20overall%20search%2C%20when%20using%20online%20machine%20learning-based%0Asurrogate%20models%2C%20depends%20not%20only%20on%20the%20accuracy%20of%20the%20predictive%20model%20but%0Aalso%20on%20both%20the%20kind%20of%20bias%20towards%20positive%20or%20negative%20cases%20and%20how%20the%0Aoptimisation%20uses%20those%20predictions%20to%20decide%20whether%20to%20execute%20the%20actual%0Afitness%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520study%2520of%2520regression%2520vs%2520pairwise%2520models%2520for%2520surrogate-based%250A%2520%2520heuristic%2520optimisation%26entry.906535625%3DPablo%2520S.%2520Naharro%2520and%2520Pablo%2520Toharia%2520and%2520Antonio%2520LaTorre%2520and%2520Jos%25C3%25A9-Mar%25C3%25ADa%2520Pe%25C3%25B1a%26entry.1292438233%3D%2520%2520Heuristic%2520optimisation%2520algorithms%2520explore%2520the%2520search%2520space%2520by%2520sampling%250Asolutions%252C%2520evaluating%2520their%2520fitness%252C%2520and%2520biasing%2520the%2520search%2520in%2520the%2520direction%2520of%250Apromising%2520solutions.%2520However%252C%2520in%2520many%2520cases%252C%2520this%2520fitness%2520function%2520involves%250Aexecuting%2520expensive%2520computational%2520calculations%252C%2520drastically%2520reducing%2520the%250Areasonable%2520number%2520of%2520evaluations.%2520In%2520this%2520context%252C%2520surrogate%2520models%2520have%250Aemerged%2520as%2520an%2520excellent%2520alternative%2520to%2520alleviate%2520these%2520computational%2520problems.%250AThis%2520paper%2520addresses%2520the%2520formulation%2520of%2520surrogate%2520problems%2520as%2520both%2520regression%250Amodels%2520that%2520approximate%2520fitness%2520%2528surface%2520surrogate%2520models%2529%2520and%2520a%2520novel%2520way%2520to%250Aconnect%2520classification%2520models%2520%2528pairwise%2520surrogate%2520models%2529.%2520The%2520pairwise%250Aapproach%2520can%2520be%2520directly%2520exploited%2520by%2520some%2520algorithms%252C%2520such%2520as%2520Differential%250AEvolution%252C%2520in%2520which%2520the%2520fitness%2520value%2520is%2520not%2520actually%2520needed%2520to%2520drive%2520the%250Asearch%252C%2520and%2520it%2520is%2520sufficient%2520to%2520know%2520whether%2520a%2520solution%2520is%2520better%2520than%2520another%250Aone%2520or%2520not.%2520Based%2520on%2520these%2520modelling%2520approaches%252C%2520we%2520have%2520conducted%2520a%250Amultidimensional%2520analysis%2520of%2520surrogate%2520models%2520under%2520different%2520configurations%253A%250Adifferent%2520machine%2520learning%2520algorithms%2520%2528regularised%2520regression%252C%2520neural%2520networks%252C%250Adecision%2520trees%252C%2520boosting%2520methods%252C%2520and%2520random%2520forests%2529%252C%2520different%2520surrogate%250Astrategies%2520%2528encouraging%2520diversity%2520or%2520relaxing%2520prediction%2520thresholds%2529%252C%2520and%250Acompare%2520them%2520for%2520both%2520surface%2520and%2520pairwise%2520surrogate%2520models.%2520The%2520experimental%250Apart%2520of%2520the%2520article%2520includes%2520the%2520benchmark%2520problems%2520already%2520proposed%2520for%2520the%250ASOCO2011%2520competition%2520in%2520continuous%2520optimisation%2520and%2520a%2520simulation%2520problem%250Aincluded%2520in%2520the%2520recent%2520GECCO2021%2520Industrial%2520Challenge.%2520This%2520paper%2520shows%2520that%250Athe%2520performance%2520of%2520the%2520overall%2520search%252C%2520when%2520using%2520online%2520machine%2520learning-based%250Asurrogate%2520models%252C%2520depends%2520not%2520only%2520on%2520the%2520accuracy%2520of%2520the%2520predictive%2520model%2520but%250Aalso%2520on%2520both%2520the%2520kind%2520of%2520bias%2520towards%2520positive%2520or%2520negative%2520cases%2520and%2520how%2520the%250Aoptimisation%2520uses%2520those%2520predictions%2520to%2520decide%2520whether%2520to%2520execute%2520the%2520actual%250Afitness%2520function.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20study%20of%20regression%20vs%20pairwise%20models%20for%20surrogate-based%0A%20%20heuristic%20optimisation&entry.906535625=Pablo%20S.%20Naharro%20and%20Pablo%20Toharia%20and%20Antonio%20LaTorre%20and%20Jos%C3%A9-Mar%C3%ADa%20Pe%C3%B1a&entry.1292438233=%20%20Heuristic%20optimisation%20algorithms%20explore%20the%20search%20space%20by%20sampling%0Asolutions%2C%20evaluating%20their%20fitness%2C%20and%20biasing%20the%20search%20in%20the%20direction%20of%0Apromising%20solutions.%20However%2C%20in%20many%20cases%2C%20this%20fitness%20function%20involves%0Aexecuting%20expensive%20computational%20calculations%2C%20drastically%20reducing%20the%0Areasonable%20number%20of%20evaluations.%20In%20this%20context%2C%20surrogate%20models%20have%0Aemerged%20as%20an%20excellent%20alternative%20to%20alleviate%20these%20computational%20problems.%0AThis%20paper%20addresses%20the%20formulation%20of%20surrogate%20problems%20as%20both%20regression%0Amodels%20that%20approximate%20fitness%20%28surface%20surrogate%20models%29%20and%20a%20novel%20way%20to%0Aconnect%20classification%20models%20%28pairwise%20surrogate%20models%29.%20The%20pairwise%0Aapproach%20can%20be%20directly%20exploited%20by%20some%20algorithms%2C%20such%20as%20Differential%0AEvolution%2C%20in%20which%20the%20fitness%20value%20is%20not%20actually%20needed%20to%20drive%20the%0Asearch%2C%20and%20it%20is%20sufficient%20to%20know%20whether%20a%20solution%20is%20better%20than%20another%0Aone%20or%20not.%20Based%20on%20these%20modelling%20approaches%2C%20we%20have%20conducted%20a%0Amultidimensional%20analysis%20of%20surrogate%20models%20under%20different%20configurations%3A%0Adifferent%20machine%20learning%20algorithms%20%28regularised%20regression%2C%20neural%20networks%2C%0Adecision%20trees%2C%20boosting%20methods%2C%20and%20random%20forests%29%2C%20different%20surrogate%0Astrategies%20%28encouraging%20diversity%20or%20relaxing%20prediction%20thresholds%29%2C%20and%0Acompare%20them%20for%20both%20surface%20and%20pairwise%20surrogate%20models.%20The%20experimental%0Apart%20of%20the%20article%20includes%20the%20benchmark%20problems%20already%20proposed%20for%20the%0ASOCO2011%20competition%20in%20continuous%20optimisation%20and%20a%20simulation%20problem%0Aincluded%20in%20the%20recent%20GECCO2021%20Industrial%20Challenge.%20This%20paper%20shows%20that%0Athe%20performance%20of%20the%20overall%20search%2C%20when%20using%20online%20machine%20learning-based%0Asurrogate%20models%2C%20depends%20not%20only%20on%20the%20accuracy%20of%20the%20predictive%20model%20but%0Aalso%20on%20both%20the%20kind%20of%20bias%20towards%20positive%20or%20negative%20cases%20and%20how%20the%0Aoptimisation%20uses%20those%20predictions%20to%20decide%20whether%20to%20execute%20the%20actual%0Afitness%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03409v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


