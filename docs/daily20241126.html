<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241125.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Quadratic Gaussian Splatting for Efficient and Detailed Surface\n  Reconstruction", "author": "Ziyu Zhang and Binbin Huang and Hanqing Jiang and Liyang Zhou and Xiaojun Xiang and Shunhan Shen", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has attracted attention for its\nsuperior rendering quality and speed over Neural Radiance Fields (NeRF). To\naddress 3DGS's limitations in surface representation, 2D Gaussian Splatting\n(2DGS) introduced disks as scene primitives to model and reconstruct geometries\nfrom multi-view images, offering view-consistent geometry. However, the disk's\nfirst-order linear approximation often leads to over-smoothed results. We\npropose Quadratic Gaussian Splatting (QGS), a novel method that replaces disks\nwith quadric surfaces, enhancing geometric fitting, whose code will be\nopen-sourced. QGS defines Gaussian distributions in non-Euclidean space,\nallowing primitives to capture more complex textures. As a second-order surface\napproximation, QGS also renders spatial curvature to guide the normal\nconsistency term, to effectively reduce over-smoothing. Moreover, QGS is a\ngeneralized version of 2DGS that achieves more accurate and detailed\nreconstructions, as verified by experiments on DTU and TNT, demonstrating its\neffectiveness in surpassing current state-of-the-art methods in geometry\nreconstruction. Our code willbe released as open source.\n", "link": "http://arxiv.org/abs/2411.16392v1", "date": "2024-11-25", "relevancy": 3.3956, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7249}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6582}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quadratic%20Gaussian%20Splatting%20for%20Efficient%20and%20Detailed%20Surface%0A%20%20Reconstruction&body=Title%3A%20Quadratic%20Gaussian%20Splatting%20for%20Efficient%20and%20Detailed%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Ziyu%20Zhang%20and%20Binbin%20Huang%20and%20Hanqing%20Jiang%20and%20Liyang%20Zhou%20and%20Xiaojun%20Xiang%20and%20Shunhan%20Shen%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20attention%20for%20its%0Asuperior%20rendering%20quality%20and%20speed%20over%20Neural%20Radiance%20Fields%20%28NeRF%29.%20To%0Aaddress%203DGS%27s%20limitations%20in%20surface%20representation%2C%202D%20Gaussian%20Splatting%0A%282DGS%29%20introduced%20disks%20as%20scene%20primitives%20to%20model%20and%20reconstruct%20geometries%0Afrom%20multi-view%20images%2C%20offering%20view-consistent%20geometry.%20However%2C%20the%20disk%27s%0Afirst-order%20linear%20approximation%20often%20leads%20to%20over-smoothed%20results.%20We%0Apropose%20Quadratic%20Gaussian%20Splatting%20%28QGS%29%2C%20a%20novel%20method%20that%20replaces%20disks%0Awith%20quadric%20surfaces%2C%20enhancing%20geometric%20fitting%2C%20whose%20code%20will%20be%0Aopen-sourced.%20QGS%20defines%20Gaussian%20distributions%20in%20non-Euclidean%20space%2C%0Aallowing%20primitives%20to%20capture%20more%20complex%20textures.%20As%20a%20second-order%20surface%0Aapproximation%2C%20QGS%20also%20renders%20spatial%20curvature%20to%20guide%20the%20normal%0Aconsistency%20term%2C%20to%20effectively%20reduce%20over-smoothing.%20Moreover%2C%20QGS%20is%20a%0Ageneralized%20version%20of%202DGS%20that%20achieves%20more%20accurate%20and%20detailed%0Areconstructions%2C%20as%20verified%20by%20experiments%20on%20DTU%20and%20TNT%2C%20demonstrating%20its%0Aeffectiveness%20in%20surpassing%20current%20state-of-the-art%20methods%20in%20geometry%0Areconstruction.%20Our%20code%20willbe%20released%20as%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuadratic%2520Gaussian%2520Splatting%2520for%2520Efficient%2520and%2520Detailed%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DZiyu%2520Zhang%2520and%2520Binbin%2520Huang%2520and%2520Hanqing%2520Jiang%2520and%2520Liyang%2520Zhou%2520and%2520Xiaojun%2520Xiang%2520and%2520Shunhan%2520Shen%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520attracted%2520attention%2520for%2520its%250Asuperior%2520rendering%2520quality%2520and%2520speed%2520over%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529.%2520To%250Aaddress%25203DGS%2527s%2520limitations%2520in%2520surface%2520representation%252C%25202D%2520Gaussian%2520Splatting%250A%25282DGS%2529%2520introduced%2520disks%2520as%2520scene%2520primitives%2520to%2520model%2520and%2520reconstruct%2520geometries%250Afrom%2520multi-view%2520images%252C%2520offering%2520view-consistent%2520geometry.%2520However%252C%2520the%2520disk%2527s%250Afirst-order%2520linear%2520approximation%2520often%2520leads%2520to%2520over-smoothed%2520results.%2520We%250Apropose%2520Quadratic%2520Gaussian%2520Splatting%2520%2528QGS%2529%252C%2520a%2520novel%2520method%2520that%2520replaces%2520disks%250Awith%2520quadric%2520surfaces%252C%2520enhancing%2520geometric%2520fitting%252C%2520whose%2520code%2520will%2520be%250Aopen-sourced.%2520QGS%2520defines%2520Gaussian%2520distributions%2520in%2520non-Euclidean%2520space%252C%250Aallowing%2520primitives%2520to%2520capture%2520more%2520complex%2520textures.%2520As%2520a%2520second-order%2520surface%250Aapproximation%252C%2520QGS%2520also%2520renders%2520spatial%2520curvature%2520to%2520guide%2520the%2520normal%250Aconsistency%2520term%252C%2520to%2520effectively%2520reduce%2520over-smoothing.%2520Moreover%252C%2520QGS%2520is%2520a%250Ageneralized%2520version%2520of%25202DGS%2520that%2520achieves%2520more%2520accurate%2520and%2520detailed%250Areconstructions%252C%2520as%2520verified%2520by%2520experiments%2520on%2520DTU%2520and%2520TNT%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520surpassing%2520current%2520state-of-the-art%2520methods%2520in%2520geometry%250Areconstruction.%2520Our%2520code%2520willbe%2520released%2520as%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quadratic%20Gaussian%20Splatting%20for%20Efficient%20and%20Detailed%20Surface%0A%20%20Reconstruction&entry.906535625=Ziyu%20Zhang%20and%20Binbin%20Huang%20and%20Hanqing%20Jiang%20and%20Liyang%20Zhou%20and%20Xiaojun%20Xiang%20and%20Shunhan%20Shen&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20attracted%20attention%20for%20its%0Asuperior%20rendering%20quality%20and%20speed%20over%20Neural%20Radiance%20Fields%20%28NeRF%29.%20To%0Aaddress%203DGS%27s%20limitations%20in%20surface%20representation%2C%202D%20Gaussian%20Splatting%0A%282DGS%29%20introduced%20disks%20as%20scene%20primitives%20to%20model%20and%20reconstruct%20geometries%0Afrom%20multi-view%20images%2C%20offering%20view-consistent%20geometry.%20However%2C%20the%20disk%27s%0Afirst-order%20linear%20approximation%20often%20leads%20to%20over-smoothed%20results.%20We%0Apropose%20Quadratic%20Gaussian%20Splatting%20%28QGS%29%2C%20a%20novel%20method%20that%20replaces%20disks%0Awith%20quadric%20surfaces%2C%20enhancing%20geometric%20fitting%2C%20whose%20code%20will%20be%0Aopen-sourced.%20QGS%20defines%20Gaussian%20distributions%20in%20non-Euclidean%20space%2C%0Aallowing%20primitives%20to%20capture%20more%20complex%20textures.%20As%20a%20second-order%20surface%0Aapproximation%2C%20QGS%20also%20renders%20spatial%20curvature%20to%20guide%20the%20normal%0Aconsistency%20term%2C%20to%20effectively%20reduce%20over-smoothing.%20Moreover%2C%20QGS%20is%20a%0Ageneralized%20version%20of%202DGS%20that%20achieves%20more%20accurate%20and%20detailed%0Areconstructions%2C%20as%20verified%20by%20experiments%20on%20DTU%20and%20TNT%2C%20demonstrating%20its%0Aeffectiveness%20in%20surpassing%20current%20state-of-the-art%20methods%20in%20geometry%0Areconstruction.%20Our%20code%20willbe%20released%20as%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16392v1&entry.124074799=Read"},
{"title": "NexusSplats: Efficient 3D Gaussian Splatting in the Wild", "author": "Yuzhou Tang and Dejun Xu and Yongjie Hou and Zhenzhong Wang and Min Jiang", "abstract": "  While 3D Gaussian Splatting (3DGS) has recently demonstrated remarkable\nrendering quality and efficiency in 3D scene reconstruction, it struggles with\nvarying lighting conditions and incidental occlusions in real-world scenarios.\nTo accommodate varying lighting conditions, existing 3DGS extensions apply\ncolor mapping to the massive Gaussian primitives with individually optimized\nappearance embeddings. To handle occlusions, they predict pixel-wise\nuncertainties via 2D image features for occlusion capture. Nevertheless, such\nmassive color mapping and pixel-wise uncertainty prediction strategies suffer\nfrom not only additional computational costs but also coarse-grained lighting\nand occlusion handling. In this work, we propose a nexus kernel-driven\napproach, termed NexusSplats, for efficient and finer 3D scene reconstruction\nunder complex lighting and occlusion conditions. In particular, NexusSplats\nleverages a novel light decoupling strategy where appearance embeddings are\noptimized based on nexus kernels instead of massive Gaussian primitives, thus\naccelerating reconstruction speeds while ensuring local color consistency for\nfiner textures. Additionally, a Gaussian-wise uncertainty mechanism is\ndeveloped, aligning 3D structures with 2D image features for fine-grained\nocclusion handling. Experimental results demonstrate that NexusSplats achieves\nstate-of-the-art rendering quality while reducing reconstruction time by up to\n70.4% compared to the current best in quality.\n", "link": "http://arxiv.org/abs/2411.14514v2", "date": "2024-11-25", "relevancy": 3.3547, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7149}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.665}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NexusSplats%3A%20Efficient%203D%20Gaussian%20Splatting%20in%20the%20Wild&body=Title%3A%20NexusSplats%3A%20Efficient%203D%20Gaussian%20Splatting%20in%20the%20Wild%0AAuthor%3A%20Yuzhou%20Tang%20and%20Dejun%20Xu%20and%20Yongjie%20Hou%20and%20Zhenzhong%20Wang%20and%20Min%20Jiang%0AAbstract%3A%20%20%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20demonstrated%20remarkable%0Arendering%20quality%20and%20efficiency%20in%203D%20scene%20reconstruction%2C%20it%20struggles%20with%0Avarying%20lighting%20conditions%20and%20incidental%20occlusions%20in%20real-world%20scenarios.%0ATo%20accommodate%20varying%20lighting%20conditions%2C%20existing%203DGS%20extensions%20apply%0Acolor%20mapping%20to%20the%20massive%20Gaussian%20primitives%20with%20individually%20optimized%0Aappearance%20embeddings.%20To%20handle%20occlusions%2C%20they%20predict%20pixel-wise%0Auncertainties%20via%202D%20image%20features%20for%20occlusion%20capture.%20Nevertheless%2C%20such%0Amassive%20color%20mapping%20and%20pixel-wise%20uncertainty%20prediction%20strategies%20suffer%0Afrom%20not%20only%20additional%20computational%20costs%20but%20also%20coarse-grained%20lighting%0Aand%20occlusion%20handling.%20In%20this%20work%2C%20we%20propose%20a%20nexus%20kernel-driven%0Aapproach%2C%20termed%20NexusSplats%2C%20for%20efficient%20and%20finer%203D%20scene%20reconstruction%0Aunder%20complex%20lighting%20and%20occlusion%20conditions.%20In%20particular%2C%20NexusSplats%0Aleverages%20a%20novel%20light%20decoupling%20strategy%20where%20appearance%20embeddings%20are%0Aoptimized%20based%20on%20nexus%20kernels%20instead%20of%20massive%20Gaussian%20primitives%2C%20thus%0Aaccelerating%20reconstruction%20speeds%20while%20ensuring%20local%20color%20consistency%20for%0Afiner%20textures.%20Additionally%2C%20a%20Gaussian-wise%20uncertainty%20mechanism%20is%0Adeveloped%2C%20aligning%203D%20structures%20with%202D%20image%20features%20for%20fine-grained%0Aocclusion%20handling.%20Experimental%20results%20demonstrate%20that%20NexusSplats%20achieves%0Astate-of-the-art%20rendering%20quality%20while%20reducing%20reconstruction%20time%20by%20up%20to%0A70.4%25%20compared%20to%20the%20current%20best%20in%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNexusSplats%253A%2520Efficient%25203D%2520Gaussian%2520Splatting%2520in%2520the%2520Wild%26entry.906535625%3DYuzhou%2520Tang%2520and%2520Dejun%2520Xu%2520and%2520Yongjie%2520Hou%2520and%2520Zhenzhong%2520Wang%2520and%2520Min%2520Jiang%26entry.1292438233%3D%2520%2520While%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520demonstrated%2520remarkable%250Arendering%2520quality%2520and%2520efficiency%2520in%25203D%2520scene%2520reconstruction%252C%2520it%2520struggles%2520with%250Avarying%2520lighting%2520conditions%2520and%2520incidental%2520occlusions%2520in%2520real-world%2520scenarios.%250ATo%2520accommodate%2520varying%2520lighting%2520conditions%252C%2520existing%25203DGS%2520extensions%2520apply%250Acolor%2520mapping%2520to%2520the%2520massive%2520Gaussian%2520primitives%2520with%2520individually%2520optimized%250Aappearance%2520embeddings.%2520To%2520handle%2520occlusions%252C%2520they%2520predict%2520pixel-wise%250Auncertainties%2520via%25202D%2520image%2520features%2520for%2520occlusion%2520capture.%2520Nevertheless%252C%2520such%250Amassive%2520color%2520mapping%2520and%2520pixel-wise%2520uncertainty%2520prediction%2520strategies%2520suffer%250Afrom%2520not%2520only%2520additional%2520computational%2520costs%2520but%2520also%2520coarse-grained%2520lighting%250Aand%2520occlusion%2520handling.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520nexus%2520kernel-driven%250Aapproach%252C%2520termed%2520NexusSplats%252C%2520for%2520efficient%2520and%2520finer%25203D%2520scene%2520reconstruction%250Aunder%2520complex%2520lighting%2520and%2520occlusion%2520conditions.%2520In%2520particular%252C%2520NexusSplats%250Aleverages%2520a%2520novel%2520light%2520decoupling%2520strategy%2520where%2520appearance%2520embeddings%2520are%250Aoptimized%2520based%2520on%2520nexus%2520kernels%2520instead%2520of%2520massive%2520Gaussian%2520primitives%252C%2520thus%250Aaccelerating%2520reconstruction%2520speeds%2520while%2520ensuring%2520local%2520color%2520consistency%2520for%250Afiner%2520textures.%2520Additionally%252C%2520a%2520Gaussian-wise%2520uncertainty%2520mechanism%2520is%250Adeveloped%252C%2520aligning%25203D%2520structures%2520with%25202D%2520image%2520features%2520for%2520fine-grained%250Aocclusion%2520handling.%2520Experimental%2520results%2520demonstrate%2520that%2520NexusSplats%2520achieves%250Astate-of-the-art%2520rendering%2520quality%2520while%2520reducing%2520reconstruction%2520time%2520by%2520up%2520to%250A70.4%2525%2520compared%2520to%2520the%2520current%2520best%2520in%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NexusSplats%3A%20Efficient%203D%20Gaussian%20Splatting%20in%20the%20Wild&entry.906535625=Yuzhou%20Tang%20and%20Dejun%20Xu%20and%20Yongjie%20Hou%20and%20Zhenzhong%20Wang%20and%20Min%20Jiang&entry.1292438233=%20%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20demonstrated%20remarkable%0Arendering%20quality%20and%20efficiency%20in%203D%20scene%20reconstruction%2C%20it%20struggles%20with%0Avarying%20lighting%20conditions%20and%20incidental%20occlusions%20in%20real-world%20scenarios.%0ATo%20accommodate%20varying%20lighting%20conditions%2C%20existing%203DGS%20extensions%20apply%0Acolor%20mapping%20to%20the%20massive%20Gaussian%20primitives%20with%20individually%20optimized%0Aappearance%20embeddings.%20To%20handle%20occlusions%2C%20they%20predict%20pixel-wise%0Auncertainties%20via%202D%20image%20features%20for%20occlusion%20capture.%20Nevertheless%2C%20such%0Amassive%20color%20mapping%20and%20pixel-wise%20uncertainty%20prediction%20strategies%20suffer%0Afrom%20not%20only%20additional%20computational%20costs%20but%20also%20coarse-grained%20lighting%0Aand%20occlusion%20handling.%20In%20this%20work%2C%20we%20propose%20a%20nexus%20kernel-driven%0Aapproach%2C%20termed%20NexusSplats%2C%20for%20efficient%20and%20finer%203D%20scene%20reconstruction%0Aunder%20complex%20lighting%20and%20occlusion%20conditions.%20In%20particular%2C%20NexusSplats%0Aleverages%20a%20novel%20light%20decoupling%20strategy%20where%20appearance%20embeddings%20are%0Aoptimized%20based%20on%20nexus%20kernels%20instead%20of%20massive%20Gaussian%20primitives%2C%20thus%0Aaccelerating%20reconstruction%20speeds%20while%20ensuring%20local%20color%20consistency%20for%0Afiner%20textures.%20Additionally%2C%20a%20Gaussian-wise%20uncertainty%20mechanism%20is%0Adeveloped%2C%20aligning%203D%20structures%20with%202D%20image%20features%20for%20fine-grained%0Aocclusion%20handling.%20Experimental%20results%20demonstrate%20that%20NexusSplats%20achieves%0Astate-of-the-art%20rendering%20quality%20while%20reducing%20reconstruction%20time%20by%20up%20to%0A70.4%25%20compared%20to%20the%20current%20best%20in%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14514v2&entry.124074799=Read"},
{"title": "Quark: Real-time, High-resolution, and General Neural View Synthesis", "author": "John Flynn and Michael Broxton and Lukas Murmann and Lucy Chai and Matthew DuVall and Cl\u00e9ment Godard and Kathryn Heal and Srinivas Kaza and Stephen Lombardi and Xuan Luo and Supreeth Achar and Kira Prabhu and Tiancheng Sun and Lynn Tsai and Ryan Overbeck", "abstract": "  We present a novel neural algorithm for performing high-quality,\nhigh-resolution, real-time novel view synthesis. From a sparse set of input RGB\nimages or videos streams, our network both reconstructs the 3D scene and\nrenders novel views at 1080p resolution at 30fps on an NVIDIA A100. Our\nfeed-forward network generalizes across a wide variety of datasets and scenes\nand produces state-of-the-art quality for a real-time method. Our quality\napproaches, and in some cases surpasses, the quality of some of the top offline\nmethods. In order to achieve these results we use a novel combination of\nseveral key concepts, and tie them together into a cohesive and effective\nalgorithm. We build on previous works that represent the scene using\nsemi-transparent layers and use an iterative learned render-and-refine approach\nto improve those layers. Instead of flat layers, our method reconstructs\nlayered depth maps (LDMs) that efficiently represent scenes with complex depth\nand occlusions. The iterative update steps are embedded in a multi-scale,\nUNet-style architecture to perform as much compute as possible at reduced\nresolution. Within each update step, to better aggregate the information from\nmultiple input views, we use a specialized Transformer-based network component.\nThis allows the majority of the per-input image processing to be performed in\nthe input image space, as opposed to layer space, further increasing\nefficiency. Finally, due to the real-time nature of our reconstruction and\nrendering, we dynamically create and discard the internal 3D geometry for each\nframe, generating the LDM for each view. Taken together, this produces a novel\nand effective algorithm for view synthesis. Through extensive evaluation, we\ndemonstrate that we achieve state-of-the-art quality at real-time rates.\nProject page: https://quark-3d.github.io/\n", "link": "http://arxiv.org/abs/2411.16680v1", "date": "2024-11-25", "relevancy": 3.1021, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quark%3A%20Real-time%2C%20High-resolution%2C%20and%20General%20Neural%20View%20Synthesis&body=Title%3A%20Quark%3A%20Real-time%2C%20High-resolution%2C%20and%20General%20Neural%20View%20Synthesis%0AAuthor%3A%20John%20Flynn%20and%20Michael%20Broxton%20and%20Lukas%20Murmann%20and%20Lucy%20Chai%20and%20Matthew%20DuVall%20and%20Cl%C3%A9ment%20Godard%20and%20Kathryn%20Heal%20and%20Srinivas%20Kaza%20and%20Stephen%20Lombardi%20and%20Xuan%20Luo%20and%20Supreeth%20Achar%20and%20Kira%20Prabhu%20and%20Tiancheng%20Sun%20and%20Lynn%20Tsai%20and%20Ryan%20Overbeck%0AAbstract%3A%20%20%20We%20present%20a%20novel%20neural%20algorithm%20for%20performing%20high-quality%2C%0Ahigh-resolution%2C%20real-time%20novel%20view%20synthesis.%20From%20a%20sparse%20set%20of%20input%20RGB%0Aimages%20or%20videos%20streams%2C%20our%20network%20both%20reconstructs%20the%203D%20scene%20and%0Arenders%20novel%20views%20at%201080p%20resolution%20at%2030fps%20on%20an%20NVIDIA%20A100.%20Our%0Afeed-forward%20network%20generalizes%20across%20a%20wide%20variety%20of%20datasets%20and%20scenes%0Aand%20produces%20state-of-the-art%20quality%20for%20a%20real-time%20method.%20Our%20quality%0Aapproaches%2C%20and%20in%20some%20cases%20surpasses%2C%20the%20quality%20of%20some%20of%20the%20top%20offline%0Amethods.%20In%20order%20to%20achieve%20these%20results%20we%20use%20a%20novel%20combination%20of%0Aseveral%20key%20concepts%2C%20and%20tie%20them%20together%20into%20a%20cohesive%20and%20effective%0Aalgorithm.%20We%20build%20on%20previous%20works%20that%20represent%20the%20scene%20using%0Asemi-transparent%20layers%20and%20use%20an%20iterative%20learned%20render-and-refine%20approach%0Ato%20improve%20those%20layers.%20Instead%20of%20flat%20layers%2C%20our%20method%20reconstructs%0Alayered%20depth%20maps%20%28LDMs%29%20that%20efficiently%20represent%20scenes%20with%20complex%20depth%0Aand%20occlusions.%20The%20iterative%20update%20steps%20are%20embedded%20in%20a%20multi-scale%2C%0AUNet-style%20architecture%20to%20perform%20as%20much%20compute%20as%20possible%20at%20reduced%0Aresolution.%20Within%20each%20update%20step%2C%20to%20better%20aggregate%20the%20information%20from%0Amultiple%20input%20views%2C%20we%20use%20a%20specialized%20Transformer-based%20network%20component.%0AThis%20allows%20the%20majority%20of%20the%20per-input%20image%20processing%20to%20be%20performed%20in%0Athe%20input%20image%20space%2C%20as%20opposed%20to%20layer%20space%2C%20further%20increasing%0Aefficiency.%20Finally%2C%20due%20to%20the%20real-time%20nature%20of%20our%20reconstruction%20and%0Arendering%2C%20we%20dynamically%20create%20and%20discard%20the%20internal%203D%20geometry%20for%20each%0Aframe%2C%20generating%20the%20LDM%20for%20each%20view.%20Taken%20together%2C%20this%20produces%20a%20novel%0Aand%20effective%20algorithm%20for%20view%20synthesis.%20Through%20extensive%20evaluation%2C%20we%0Ademonstrate%20that%20we%20achieve%20state-of-the-art%20quality%20at%20real-time%20rates.%0AProject%20page%3A%20https%3A//quark-3d.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuark%253A%2520Real-time%252C%2520High-resolution%252C%2520and%2520General%2520Neural%2520View%2520Synthesis%26entry.906535625%3DJohn%2520Flynn%2520and%2520Michael%2520Broxton%2520and%2520Lukas%2520Murmann%2520and%2520Lucy%2520Chai%2520and%2520Matthew%2520DuVall%2520and%2520Cl%25C3%25A9ment%2520Godard%2520and%2520Kathryn%2520Heal%2520and%2520Srinivas%2520Kaza%2520and%2520Stephen%2520Lombardi%2520and%2520Xuan%2520Luo%2520and%2520Supreeth%2520Achar%2520and%2520Kira%2520Prabhu%2520and%2520Tiancheng%2520Sun%2520and%2520Lynn%2520Tsai%2520and%2520Ryan%2520Overbeck%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520neural%2520algorithm%2520for%2520performing%2520high-quality%252C%250Ahigh-resolution%252C%2520real-time%2520novel%2520view%2520synthesis.%2520From%2520a%2520sparse%2520set%2520of%2520input%2520RGB%250Aimages%2520or%2520videos%2520streams%252C%2520our%2520network%2520both%2520reconstructs%2520the%25203D%2520scene%2520and%250Arenders%2520novel%2520views%2520at%25201080p%2520resolution%2520at%252030fps%2520on%2520an%2520NVIDIA%2520A100.%2520Our%250Afeed-forward%2520network%2520generalizes%2520across%2520a%2520wide%2520variety%2520of%2520datasets%2520and%2520scenes%250Aand%2520produces%2520state-of-the-art%2520quality%2520for%2520a%2520real-time%2520method.%2520Our%2520quality%250Aapproaches%252C%2520and%2520in%2520some%2520cases%2520surpasses%252C%2520the%2520quality%2520of%2520some%2520of%2520the%2520top%2520offline%250Amethods.%2520In%2520order%2520to%2520achieve%2520these%2520results%2520we%2520use%2520a%2520novel%2520combination%2520of%250Aseveral%2520key%2520concepts%252C%2520and%2520tie%2520them%2520together%2520into%2520a%2520cohesive%2520and%2520effective%250Aalgorithm.%2520We%2520build%2520on%2520previous%2520works%2520that%2520represent%2520the%2520scene%2520using%250Asemi-transparent%2520layers%2520and%2520use%2520an%2520iterative%2520learned%2520render-and-refine%2520approach%250Ato%2520improve%2520those%2520layers.%2520Instead%2520of%2520flat%2520layers%252C%2520our%2520method%2520reconstructs%250Alayered%2520depth%2520maps%2520%2528LDMs%2529%2520that%2520efficiently%2520represent%2520scenes%2520with%2520complex%2520depth%250Aand%2520occlusions.%2520The%2520iterative%2520update%2520steps%2520are%2520embedded%2520in%2520a%2520multi-scale%252C%250AUNet-style%2520architecture%2520to%2520perform%2520as%2520much%2520compute%2520as%2520possible%2520at%2520reduced%250Aresolution.%2520Within%2520each%2520update%2520step%252C%2520to%2520better%2520aggregate%2520the%2520information%2520from%250Amultiple%2520input%2520views%252C%2520we%2520use%2520a%2520specialized%2520Transformer-based%2520network%2520component.%250AThis%2520allows%2520the%2520majority%2520of%2520the%2520per-input%2520image%2520processing%2520to%2520be%2520performed%2520in%250Athe%2520input%2520image%2520space%252C%2520as%2520opposed%2520to%2520layer%2520space%252C%2520further%2520increasing%250Aefficiency.%2520Finally%252C%2520due%2520to%2520the%2520real-time%2520nature%2520of%2520our%2520reconstruction%2520and%250Arendering%252C%2520we%2520dynamically%2520create%2520and%2520discard%2520the%2520internal%25203D%2520geometry%2520for%2520each%250Aframe%252C%2520generating%2520the%2520LDM%2520for%2520each%2520view.%2520Taken%2520together%252C%2520this%2520produces%2520a%2520novel%250Aand%2520effective%2520algorithm%2520for%2520view%2520synthesis.%2520Through%2520extensive%2520evaluation%252C%2520we%250Ademonstrate%2520that%2520we%2520achieve%2520state-of-the-art%2520quality%2520at%2520real-time%2520rates.%250AProject%2520page%253A%2520https%253A//quark-3d.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quark%3A%20Real-time%2C%20High-resolution%2C%20and%20General%20Neural%20View%20Synthesis&entry.906535625=John%20Flynn%20and%20Michael%20Broxton%20and%20Lukas%20Murmann%20and%20Lucy%20Chai%20and%20Matthew%20DuVall%20and%20Cl%C3%A9ment%20Godard%20and%20Kathryn%20Heal%20and%20Srinivas%20Kaza%20and%20Stephen%20Lombardi%20and%20Xuan%20Luo%20and%20Supreeth%20Achar%20and%20Kira%20Prabhu%20and%20Tiancheng%20Sun%20and%20Lynn%20Tsai%20and%20Ryan%20Overbeck&entry.1292438233=%20%20We%20present%20a%20novel%20neural%20algorithm%20for%20performing%20high-quality%2C%0Ahigh-resolution%2C%20real-time%20novel%20view%20synthesis.%20From%20a%20sparse%20set%20of%20input%20RGB%0Aimages%20or%20videos%20streams%2C%20our%20network%20both%20reconstructs%20the%203D%20scene%20and%0Arenders%20novel%20views%20at%201080p%20resolution%20at%2030fps%20on%20an%20NVIDIA%20A100.%20Our%0Afeed-forward%20network%20generalizes%20across%20a%20wide%20variety%20of%20datasets%20and%20scenes%0Aand%20produces%20state-of-the-art%20quality%20for%20a%20real-time%20method.%20Our%20quality%0Aapproaches%2C%20and%20in%20some%20cases%20surpasses%2C%20the%20quality%20of%20some%20of%20the%20top%20offline%0Amethods.%20In%20order%20to%20achieve%20these%20results%20we%20use%20a%20novel%20combination%20of%0Aseveral%20key%20concepts%2C%20and%20tie%20them%20together%20into%20a%20cohesive%20and%20effective%0Aalgorithm.%20We%20build%20on%20previous%20works%20that%20represent%20the%20scene%20using%0Asemi-transparent%20layers%20and%20use%20an%20iterative%20learned%20render-and-refine%20approach%0Ato%20improve%20those%20layers.%20Instead%20of%20flat%20layers%2C%20our%20method%20reconstructs%0Alayered%20depth%20maps%20%28LDMs%29%20that%20efficiently%20represent%20scenes%20with%20complex%20depth%0Aand%20occlusions.%20The%20iterative%20update%20steps%20are%20embedded%20in%20a%20multi-scale%2C%0AUNet-style%20architecture%20to%20perform%20as%20much%20compute%20as%20possible%20at%20reduced%0Aresolution.%20Within%20each%20update%20step%2C%20to%20better%20aggregate%20the%20information%20from%0Amultiple%20input%20views%2C%20we%20use%20a%20specialized%20Transformer-based%20network%20component.%0AThis%20allows%20the%20majority%20of%20the%20per-input%20image%20processing%20to%20be%20performed%20in%0Athe%20input%20image%20space%2C%20as%20opposed%20to%20layer%20space%2C%20further%20increasing%0Aefficiency.%20Finally%2C%20due%20to%20the%20real-time%20nature%20of%20our%20reconstruction%20and%0Arendering%2C%20we%20dynamically%20create%20and%20discard%20the%20internal%203D%20geometry%20for%20each%0Aframe%2C%20generating%20the%20LDM%20for%20each%20view.%20Taken%20together%2C%20this%20produces%20a%20novel%0Aand%20effective%20algorithm%20for%20view%20synthesis.%20Through%20extensive%20evaluation%2C%20we%0Ademonstrate%20that%20we%20achieve%20state-of-the-art%20quality%20at%20real-time%20rates.%0AProject%20page%3A%20https%3A//quark-3d.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16680v1&entry.124074799=Read"},
{"title": "Visual Riddles: a Commonsense and World Knowledge Challenge for Large\n  Vision and Language Models", "author": "Nitzan Bitton-Guetta and Aviv Slobodkin and Aviya Maimon and Eliya Habba and Royi Rassin and Yonatan Bitton and Idan Szpektor and Amir Globerson and Yuval Elovici", "abstract": "  Imagine observing someone scratching their arm; to understand why, additional\ncontext would be necessary. However, spotting a mosquito nearby would\nimmediately offer a likely explanation for the person's discomfort, thereby\nalleviating the need for further information. This example illustrates how\nsubtle visual cues can challenge our cognitive skills and demonstrates the\ncomplexity of interpreting visual scenarios. To study these skills, we present\nVisual Riddles, a benchmark aimed to test vision and language models on visual\nriddles requiring commonsense and world knowledge. The benchmark comprises 400\nvisual riddles, each featuring a unique image created by a variety of\ntext-to-image models, question, ground-truth answer, textual hint, and\nattribution. Human evaluation reveals that existing models lag significantly\nbehind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading\nwith 40% accuracy. Our benchmark comes with automatic evaluation tasks to make\nassessment scalable. These findings underscore the potential of Visual Riddles\nas a valuable resource for enhancing vision and language models' capabilities\nin interpreting complex visual scenarios.\n", "link": "http://arxiv.org/abs/2407.19474v2", "date": "2024-11-25", "relevancy": 3.0467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6428}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Riddles%3A%20a%20Commonsense%20and%20World%20Knowledge%20Challenge%20for%20Large%0A%20%20Vision%20and%20Language%20Models&body=Title%3A%20Visual%20Riddles%3A%20a%20Commonsense%20and%20World%20Knowledge%20Challenge%20for%20Large%0A%20%20Vision%20and%20Language%20Models%0AAuthor%3A%20Nitzan%20Bitton-Guetta%20and%20Aviv%20Slobodkin%20and%20Aviya%20Maimon%20and%20Eliya%20Habba%20and%20Royi%20Rassin%20and%20Yonatan%20Bitton%20and%20Idan%20Szpektor%20and%20Amir%20Globerson%20and%20Yuval%20Elovici%0AAbstract%3A%20%20%20Imagine%20observing%20someone%20scratching%20their%20arm%3B%20to%20understand%20why%2C%20additional%0Acontext%20would%20be%20necessary.%20However%2C%20spotting%20a%20mosquito%20nearby%20would%0Aimmediately%20offer%20a%20likely%20explanation%20for%20the%20person%27s%20discomfort%2C%20thereby%0Aalleviating%20the%20need%20for%20further%20information.%20This%20example%20illustrates%20how%0Asubtle%20visual%20cues%20can%20challenge%20our%20cognitive%20skills%20and%20demonstrates%20the%0Acomplexity%20of%20interpreting%20visual%20scenarios.%20To%20study%20these%20skills%2C%20we%20present%0AVisual%20Riddles%2C%20a%20benchmark%20aimed%20to%20test%20vision%20and%20language%20models%20on%20visual%0Ariddles%20requiring%20commonsense%20and%20world%20knowledge.%20The%20benchmark%20comprises%20400%0Avisual%20riddles%2C%20each%20featuring%20a%20unique%20image%20created%20by%20a%20variety%20of%0Atext-to-image%20models%2C%20question%2C%20ground-truth%20answer%2C%20textual%20hint%2C%20and%0Aattribution.%20Human%20evaluation%20reveals%20that%20existing%20models%20lag%20significantly%0Abehind%20human%20performance%2C%20which%20is%20at%2082%25%20accuracy%2C%20with%20Gemini-Pro-1.5%20leading%0Awith%2040%25%20accuracy.%20Our%20benchmark%20comes%20with%20automatic%20evaluation%20tasks%20to%20make%0Aassessment%20scalable.%20These%20findings%20underscore%20the%20potential%20of%20Visual%20Riddles%0Aas%20a%20valuable%20resource%20for%20enhancing%20vision%20and%20language%20models%27%20capabilities%0Ain%20interpreting%20complex%20visual%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19474v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Riddles%253A%2520a%2520Commonsense%2520and%2520World%2520Knowledge%2520Challenge%2520for%2520Large%250A%2520%2520Vision%2520and%2520Language%2520Models%26entry.906535625%3DNitzan%2520Bitton-Guetta%2520and%2520Aviv%2520Slobodkin%2520and%2520Aviya%2520Maimon%2520and%2520Eliya%2520Habba%2520and%2520Royi%2520Rassin%2520and%2520Yonatan%2520Bitton%2520and%2520Idan%2520Szpektor%2520and%2520Amir%2520Globerson%2520and%2520Yuval%2520Elovici%26entry.1292438233%3D%2520%2520Imagine%2520observing%2520someone%2520scratching%2520their%2520arm%253B%2520to%2520understand%2520why%252C%2520additional%250Acontext%2520would%2520be%2520necessary.%2520However%252C%2520spotting%2520a%2520mosquito%2520nearby%2520would%250Aimmediately%2520offer%2520a%2520likely%2520explanation%2520for%2520the%2520person%2527s%2520discomfort%252C%2520thereby%250Aalleviating%2520the%2520need%2520for%2520further%2520information.%2520This%2520example%2520illustrates%2520how%250Asubtle%2520visual%2520cues%2520can%2520challenge%2520our%2520cognitive%2520skills%2520and%2520demonstrates%2520the%250Acomplexity%2520of%2520interpreting%2520visual%2520scenarios.%2520To%2520study%2520these%2520skills%252C%2520we%2520present%250AVisual%2520Riddles%252C%2520a%2520benchmark%2520aimed%2520to%2520test%2520vision%2520and%2520language%2520models%2520on%2520visual%250Ariddles%2520requiring%2520commonsense%2520and%2520world%2520knowledge.%2520The%2520benchmark%2520comprises%2520400%250Avisual%2520riddles%252C%2520each%2520featuring%2520a%2520unique%2520image%2520created%2520by%2520a%2520variety%2520of%250Atext-to-image%2520models%252C%2520question%252C%2520ground-truth%2520answer%252C%2520textual%2520hint%252C%2520and%250Aattribution.%2520Human%2520evaluation%2520reveals%2520that%2520existing%2520models%2520lag%2520significantly%250Abehind%2520human%2520performance%252C%2520which%2520is%2520at%252082%2525%2520accuracy%252C%2520with%2520Gemini-Pro-1.5%2520leading%250Awith%252040%2525%2520accuracy.%2520Our%2520benchmark%2520comes%2520with%2520automatic%2520evaluation%2520tasks%2520to%2520make%250Aassessment%2520scalable.%2520These%2520findings%2520underscore%2520the%2520potential%2520of%2520Visual%2520Riddles%250Aas%2520a%2520valuable%2520resource%2520for%2520enhancing%2520vision%2520and%2520language%2520models%2527%2520capabilities%250Ain%2520interpreting%2520complex%2520visual%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19474v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Riddles%3A%20a%20Commonsense%20and%20World%20Knowledge%20Challenge%20for%20Large%0A%20%20Vision%20and%20Language%20Models&entry.906535625=Nitzan%20Bitton-Guetta%20and%20Aviv%20Slobodkin%20and%20Aviya%20Maimon%20and%20Eliya%20Habba%20and%20Royi%20Rassin%20and%20Yonatan%20Bitton%20and%20Idan%20Szpektor%20and%20Amir%20Globerson%20and%20Yuval%20Elovici&entry.1292438233=%20%20Imagine%20observing%20someone%20scratching%20their%20arm%3B%20to%20understand%20why%2C%20additional%0Acontext%20would%20be%20necessary.%20However%2C%20spotting%20a%20mosquito%20nearby%20would%0Aimmediately%20offer%20a%20likely%20explanation%20for%20the%20person%27s%20discomfort%2C%20thereby%0Aalleviating%20the%20need%20for%20further%20information.%20This%20example%20illustrates%20how%0Asubtle%20visual%20cues%20can%20challenge%20our%20cognitive%20skills%20and%20demonstrates%20the%0Acomplexity%20of%20interpreting%20visual%20scenarios.%20To%20study%20these%20skills%2C%20we%20present%0AVisual%20Riddles%2C%20a%20benchmark%20aimed%20to%20test%20vision%20and%20language%20models%20on%20visual%0Ariddles%20requiring%20commonsense%20and%20world%20knowledge.%20The%20benchmark%20comprises%20400%0Avisual%20riddles%2C%20each%20featuring%20a%20unique%20image%20created%20by%20a%20variety%20of%0Atext-to-image%20models%2C%20question%2C%20ground-truth%20answer%2C%20textual%20hint%2C%20and%0Aattribution.%20Human%20evaluation%20reveals%20that%20existing%20models%20lag%20significantly%0Abehind%20human%20performance%2C%20which%20is%20at%2082%25%20accuracy%2C%20with%20Gemini-Pro-1.5%20leading%0Awith%2040%25%20accuracy.%20Our%20benchmark%20comes%20with%20automatic%20evaluation%20tasks%20to%20make%0Aassessment%20scalable.%20These%20findings%20underscore%20the%20potential%20of%20Visual%20Riddles%0Aas%20a%20valuable%20resource%20for%20enhancing%20vision%20and%20language%20models%27%20capabilities%0Ain%20interpreting%20complex%20visual%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19474v2&entry.124074799=Read"},
{"title": "Cutting Voxel Projector a New Approach to Construct 3D Cone Beam CT\n  Operator", "author": "Vojt\u011bch Kulvait and Julian Moosmann and Georg Rose", "abstract": "  In this paper, we introduce a novel class of projectors for 3D cone beam\ntomographic reconstruction. Analytical formulas are derived to compute the\nrelationship between the volume of a voxel projected onto a detector pixel and\nits contribution to the line integral of attenuation recorded by that pixel.\nBased on these formulas, we construct a near-exact projector and backprojector,\nparticularly suited for algebraic reconstruction techniques and hierarchical\nreconstruction approaches with nonuniform voxel grids. Unlike traditional\nprojectors, which assume a uniform grid with fixed voxel sizes, our method\nenables local refinement of voxels, allowing for adaptive grid resolution and\nimproved reconstruction quality in regions of interest. We have implemented\nthis cutting voxel projector along with a relaxed, speed-optimized version and\ncompared them to two established projectors: a ray-tracing projector based on\nSiddon's algorithm and a TT footprint projector. Our results demonstrate that\nthe cutting voxel projector achieves higher accuracy than the TT projector,\nespecially for large cone beam angles. Furthermore, the relaxed version of the\ncutting voxel projector offers a significant speed advantage over current\nfootprint projector implementations, while maintaining comparable accuracy. In\ncontrast, Siddon's algorithm, when achieving similar accuracy, is considerably\nslower than the cutting voxel projector. All algorithms are implemented in an\nopen-source framework for algebraic reconstruction using OpenCL and C++,\noptimized for efficient GPU computation. GitHub repository of the project\nhttps://github.com/kulvait/KCT_cbct.\n", "link": "http://arxiv.org/abs/2110.09841v2", "date": "2024-11-25", "relevancy": 2.9596, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6094}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6094}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cutting%20Voxel%20Projector%20a%20New%20Approach%20to%20Construct%203D%20Cone%20Beam%20CT%0A%20%20Operator&body=Title%3A%20Cutting%20Voxel%20Projector%20a%20New%20Approach%20to%20Construct%203D%20Cone%20Beam%20CT%0A%20%20Operator%0AAuthor%3A%20Vojt%C4%9Bch%20Kulvait%20and%20Julian%20Moosmann%20and%20Georg%20Rose%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20class%20of%20projectors%20for%203D%20cone%20beam%0Atomographic%20reconstruction.%20Analytical%20formulas%20are%20derived%20to%20compute%20the%0Arelationship%20between%20the%20volume%20of%20a%20voxel%20projected%20onto%20a%20detector%20pixel%20and%0Aits%20contribution%20to%20the%20line%20integral%20of%20attenuation%20recorded%20by%20that%20pixel.%0ABased%20on%20these%20formulas%2C%20we%20construct%20a%20near-exact%20projector%20and%20backprojector%2C%0Aparticularly%20suited%20for%20algebraic%20reconstruction%20techniques%20and%20hierarchical%0Areconstruction%20approaches%20with%20nonuniform%20voxel%20grids.%20Unlike%20traditional%0Aprojectors%2C%20which%20assume%20a%20uniform%20grid%20with%20fixed%20voxel%20sizes%2C%20our%20method%0Aenables%20local%20refinement%20of%20voxels%2C%20allowing%20for%20adaptive%20grid%20resolution%20and%0Aimproved%20reconstruction%20quality%20in%20regions%20of%20interest.%20We%20have%20implemented%0Athis%20cutting%20voxel%20projector%20along%20with%20a%20relaxed%2C%20speed-optimized%20version%20and%0Acompared%20them%20to%20two%20established%20projectors%3A%20a%20ray-tracing%20projector%20based%20on%0ASiddon%27s%20algorithm%20and%20a%20TT%20footprint%20projector.%20Our%20results%20demonstrate%20that%0Athe%20cutting%20voxel%20projector%20achieves%20higher%20accuracy%20than%20the%20TT%20projector%2C%0Aespecially%20for%20large%20cone%20beam%20angles.%20Furthermore%2C%20the%20relaxed%20version%20of%20the%0Acutting%20voxel%20projector%20offers%20a%20significant%20speed%20advantage%20over%20current%0Afootprint%20projector%20implementations%2C%20while%20maintaining%20comparable%20accuracy.%20In%0Acontrast%2C%20Siddon%27s%20algorithm%2C%20when%20achieving%20similar%20accuracy%2C%20is%20considerably%0Aslower%20than%20the%20cutting%20voxel%20projector.%20All%20algorithms%20are%20implemented%20in%20an%0Aopen-source%20framework%20for%20algebraic%20reconstruction%20using%20OpenCL%20and%20C%2B%2B%2C%0Aoptimized%20for%20efficient%20GPU%20computation.%20GitHub%20repository%20of%20the%20project%0Ahttps%3A//github.com/kulvait/KCT_cbct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2110.09841v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCutting%2520Voxel%2520Projector%2520a%2520New%2520Approach%2520to%2520Construct%25203D%2520Cone%2520Beam%2520CT%250A%2520%2520Operator%26entry.906535625%3DVojt%25C4%259Bch%2520Kulvait%2520and%2520Julian%2520Moosmann%2520and%2520Georg%2520Rose%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520class%2520of%2520projectors%2520for%25203D%2520cone%2520beam%250Atomographic%2520reconstruction.%2520Analytical%2520formulas%2520are%2520derived%2520to%2520compute%2520the%250Arelationship%2520between%2520the%2520volume%2520of%2520a%2520voxel%2520projected%2520onto%2520a%2520detector%2520pixel%2520and%250Aits%2520contribution%2520to%2520the%2520line%2520integral%2520of%2520attenuation%2520recorded%2520by%2520that%2520pixel.%250ABased%2520on%2520these%2520formulas%252C%2520we%2520construct%2520a%2520near-exact%2520projector%2520and%2520backprojector%252C%250Aparticularly%2520suited%2520for%2520algebraic%2520reconstruction%2520techniques%2520and%2520hierarchical%250Areconstruction%2520approaches%2520with%2520nonuniform%2520voxel%2520grids.%2520Unlike%2520traditional%250Aprojectors%252C%2520which%2520assume%2520a%2520uniform%2520grid%2520with%2520fixed%2520voxel%2520sizes%252C%2520our%2520method%250Aenables%2520local%2520refinement%2520of%2520voxels%252C%2520allowing%2520for%2520adaptive%2520grid%2520resolution%2520and%250Aimproved%2520reconstruction%2520quality%2520in%2520regions%2520of%2520interest.%2520We%2520have%2520implemented%250Athis%2520cutting%2520voxel%2520projector%2520along%2520with%2520a%2520relaxed%252C%2520speed-optimized%2520version%2520and%250Acompared%2520them%2520to%2520two%2520established%2520projectors%253A%2520a%2520ray-tracing%2520projector%2520based%2520on%250ASiddon%2527s%2520algorithm%2520and%2520a%2520TT%2520footprint%2520projector.%2520Our%2520results%2520demonstrate%2520that%250Athe%2520cutting%2520voxel%2520projector%2520achieves%2520higher%2520accuracy%2520than%2520the%2520TT%2520projector%252C%250Aespecially%2520for%2520large%2520cone%2520beam%2520angles.%2520Furthermore%252C%2520the%2520relaxed%2520version%2520of%2520the%250Acutting%2520voxel%2520projector%2520offers%2520a%2520significant%2520speed%2520advantage%2520over%2520current%250Afootprint%2520projector%2520implementations%252C%2520while%2520maintaining%2520comparable%2520accuracy.%2520In%250Acontrast%252C%2520Siddon%2527s%2520algorithm%252C%2520when%2520achieving%2520similar%2520accuracy%252C%2520is%2520considerably%250Aslower%2520than%2520the%2520cutting%2520voxel%2520projector.%2520All%2520algorithms%2520are%2520implemented%2520in%2520an%250Aopen-source%2520framework%2520for%2520algebraic%2520reconstruction%2520using%2520OpenCL%2520and%2520C%252B%252B%252C%250Aoptimized%2520for%2520efficient%2520GPU%2520computation.%2520GitHub%2520repository%2520of%2520the%2520project%250Ahttps%253A//github.com/kulvait/KCT_cbct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2110.09841v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cutting%20Voxel%20Projector%20a%20New%20Approach%20to%20Construct%203D%20Cone%20Beam%20CT%0A%20%20Operator&entry.906535625=Vojt%C4%9Bch%20Kulvait%20and%20Julian%20Moosmann%20and%20Georg%20Rose&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20class%20of%20projectors%20for%203D%20cone%20beam%0Atomographic%20reconstruction.%20Analytical%20formulas%20are%20derived%20to%20compute%20the%0Arelationship%20between%20the%20volume%20of%20a%20voxel%20projected%20onto%20a%20detector%20pixel%20and%0Aits%20contribution%20to%20the%20line%20integral%20of%20attenuation%20recorded%20by%20that%20pixel.%0ABased%20on%20these%20formulas%2C%20we%20construct%20a%20near-exact%20projector%20and%20backprojector%2C%0Aparticularly%20suited%20for%20algebraic%20reconstruction%20techniques%20and%20hierarchical%0Areconstruction%20approaches%20with%20nonuniform%20voxel%20grids.%20Unlike%20traditional%0Aprojectors%2C%20which%20assume%20a%20uniform%20grid%20with%20fixed%20voxel%20sizes%2C%20our%20method%0Aenables%20local%20refinement%20of%20voxels%2C%20allowing%20for%20adaptive%20grid%20resolution%20and%0Aimproved%20reconstruction%20quality%20in%20regions%20of%20interest.%20We%20have%20implemented%0Athis%20cutting%20voxel%20projector%20along%20with%20a%20relaxed%2C%20speed-optimized%20version%20and%0Acompared%20them%20to%20two%20established%20projectors%3A%20a%20ray-tracing%20projector%20based%20on%0ASiddon%27s%20algorithm%20and%20a%20TT%20footprint%20projector.%20Our%20results%20demonstrate%20that%0Athe%20cutting%20voxel%20projector%20achieves%20higher%20accuracy%20than%20the%20TT%20projector%2C%0Aespecially%20for%20large%20cone%20beam%20angles.%20Furthermore%2C%20the%20relaxed%20version%20of%20the%0Acutting%20voxel%20projector%20offers%20a%20significant%20speed%20advantage%20over%20current%0Afootprint%20projector%20implementations%2C%20while%20maintaining%20comparable%20accuracy.%20In%0Acontrast%2C%20Siddon%27s%20algorithm%2C%20when%20achieving%20similar%20accuracy%2C%20is%20considerably%0Aslower%20than%20the%20cutting%20voxel%20projector.%20All%20algorithms%20are%20implemented%20in%20an%0Aopen-source%20framework%20for%20algebraic%20reconstruction%20using%20OpenCL%20and%20C%2B%2B%2C%0Aoptimized%20for%20efficient%20GPU%20computation.%20GitHub%20repository%20of%20the%20project%0Ahttps%3A//github.com/kulvait/KCT_cbct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2110.09841v2&entry.124074799=Read"},
{"title": "DocPedia: Unleashing the Power of Large Multimodal Model in the\n  Frequency Domain for Versatile Document Understanding", "author": "Hao Feng and Qi Liu and Hao Liu and Jingqun Tang and Wengang Zhou and Houqiang Li and Can Huang", "abstract": "  This work presents DocPedia, a novel large multimodal model (LMM) for\nversatile OCR-free document understanding, capable of parsing images up to\n2,560$\\times$2,560 resolution. Unlike existing work either struggle with\nhigh-resolution documents or give up the large language model thus vision or\nlanguage ability constrained, our DocPedia directly processes visual input in\nthe frequency domain rather than the pixel space. The unique characteristic\nenables DocPedia to capture a greater amount of visual and textual information\nusing a limited number of visual tokens. To consistently enhance both\nperception and comprehension abilities of our model, we develop a dual-stage\ntraining strategy and enrich instructions/annotations of all training tasks\ncovering multiple document types. Extensive quantitative and qualitative\nexperiments conducted on various publicly available benchmarks confirm the\nmutual benefits of jointly learning perception and comprehension tasks. The\nresults provide further evidence of the effectiveness and superior performance\nof our DocPedia over other methods.\n", "link": "http://arxiv.org/abs/2311.11810v4", "date": "2024-11-25", "relevancy": 2.9515, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocPedia%3A%20Unleashing%20the%20Power%20of%20Large%20Multimodal%20Model%20in%20the%0A%20%20Frequency%20Domain%20for%20Versatile%20Document%20Understanding&body=Title%3A%20DocPedia%3A%20Unleashing%20the%20Power%20of%20Large%20Multimodal%20Model%20in%20the%0A%20%20Frequency%20Domain%20for%20Versatile%20Document%20Understanding%0AAuthor%3A%20Hao%20Feng%20and%20Qi%20Liu%20and%20Hao%20Liu%20and%20Jingqun%20Tang%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%20and%20Can%20Huang%0AAbstract%3A%20%20%20This%20work%20presents%20DocPedia%2C%20a%20novel%20large%20multimodal%20model%20%28LMM%29%20for%0Aversatile%20OCR-free%20document%20understanding%2C%20capable%20of%20parsing%20images%20up%20to%0A2%2C560%24%5Ctimes%242%2C560%20resolution.%20Unlike%20existing%20work%20either%20struggle%20with%0Ahigh-resolution%20documents%20or%20give%20up%20the%20large%20language%20model%20thus%20vision%20or%0Alanguage%20ability%20constrained%2C%20our%20DocPedia%20directly%20processes%20visual%20input%20in%0Athe%20frequency%20domain%20rather%20than%20the%20pixel%20space.%20The%20unique%20characteristic%0Aenables%20DocPedia%20to%20capture%20a%20greater%20amount%20of%20visual%20and%20textual%20information%0Ausing%20a%20limited%20number%20of%20visual%20tokens.%20To%20consistently%20enhance%20both%0Aperception%20and%20comprehension%20abilities%20of%20our%20model%2C%20we%20develop%20a%20dual-stage%0Atraining%20strategy%20and%20enrich%20instructions/annotations%20of%20all%20training%20tasks%0Acovering%20multiple%20document%20types.%20Extensive%20quantitative%20and%20qualitative%0Aexperiments%20conducted%20on%20various%20publicly%20available%20benchmarks%20confirm%20the%0Amutual%20benefits%20of%20jointly%20learning%20perception%20and%20comprehension%20tasks.%20The%0Aresults%20provide%20further%20evidence%20of%20the%20effectiveness%20and%20superior%20performance%0Aof%20our%20DocPedia%20over%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11810v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocPedia%253A%2520Unleashing%2520the%2520Power%2520of%2520Large%2520Multimodal%2520Model%2520in%2520the%250A%2520%2520Frequency%2520Domain%2520for%2520Versatile%2520Document%2520Understanding%26entry.906535625%3DHao%2520Feng%2520and%2520Qi%2520Liu%2520and%2520Hao%2520Liu%2520and%2520Jingqun%2520Tang%2520and%2520Wengang%2520Zhou%2520and%2520Houqiang%2520Li%2520and%2520Can%2520Huang%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520DocPedia%252C%2520a%2520novel%2520large%2520multimodal%2520model%2520%2528LMM%2529%2520for%250Aversatile%2520OCR-free%2520document%2520understanding%252C%2520capable%2520of%2520parsing%2520images%2520up%2520to%250A2%252C560%2524%255Ctimes%25242%252C560%2520resolution.%2520Unlike%2520existing%2520work%2520either%2520struggle%2520with%250Ahigh-resolution%2520documents%2520or%2520give%2520up%2520the%2520large%2520language%2520model%2520thus%2520vision%2520or%250Alanguage%2520ability%2520constrained%252C%2520our%2520DocPedia%2520directly%2520processes%2520visual%2520input%2520in%250Athe%2520frequency%2520domain%2520rather%2520than%2520the%2520pixel%2520space.%2520The%2520unique%2520characteristic%250Aenables%2520DocPedia%2520to%2520capture%2520a%2520greater%2520amount%2520of%2520visual%2520and%2520textual%2520information%250Ausing%2520a%2520limited%2520number%2520of%2520visual%2520tokens.%2520To%2520consistently%2520enhance%2520both%250Aperception%2520and%2520comprehension%2520abilities%2520of%2520our%2520model%252C%2520we%2520develop%2520a%2520dual-stage%250Atraining%2520strategy%2520and%2520enrich%2520instructions/annotations%2520of%2520all%2520training%2520tasks%250Acovering%2520multiple%2520document%2520types.%2520Extensive%2520quantitative%2520and%2520qualitative%250Aexperiments%2520conducted%2520on%2520various%2520publicly%2520available%2520benchmarks%2520confirm%2520the%250Amutual%2520benefits%2520of%2520jointly%2520learning%2520perception%2520and%2520comprehension%2520tasks.%2520The%250Aresults%2520provide%2520further%2520evidence%2520of%2520the%2520effectiveness%2520and%2520superior%2520performance%250Aof%2520our%2520DocPedia%2520over%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11810v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocPedia%3A%20Unleashing%20the%20Power%20of%20Large%20Multimodal%20Model%20in%20the%0A%20%20Frequency%20Domain%20for%20Versatile%20Document%20Understanding&entry.906535625=Hao%20Feng%20and%20Qi%20Liu%20and%20Hao%20Liu%20and%20Jingqun%20Tang%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%20and%20Can%20Huang&entry.1292438233=%20%20This%20work%20presents%20DocPedia%2C%20a%20novel%20large%20multimodal%20model%20%28LMM%29%20for%0Aversatile%20OCR-free%20document%20understanding%2C%20capable%20of%20parsing%20images%20up%20to%0A2%2C560%24%5Ctimes%242%2C560%20resolution.%20Unlike%20existing%20work%20either%20struggle%20with%0Ahigh-resolution%20documents%20or%20give%20up%20the%20large%20language%20model%20thus%20vision%20or%0Alanguage%20ability%20constrained%2C%20our%20DocPedia%20directly%20processes%20visual%20input%20in%0Athe%20frequency%20domain%20rather%20than%20the%20pixel%20space.%20The%20unique%20characteristic%0Aenables%20DocPedia%20to%20capture%20a%20greater%20amount%20of%20visual%20and%20textual%20information%0Ausing%20a%20limited%20number%20of%20visual%20tokens.%20To%20consistently%20enhance%20both%0Aperception%20and%20comprehension%20abilities%20of%20our%20model%2C%20we%20develop%20a%20dual-stage%0Atraining%20strategy%20and%20enrich%20instructions/annotations%20of%20all%20training%20tasks%0Acovering%20multiple%20document%20types.%20Extensive%20quantitative%20and%20qualitative%0Aexperiments%20conducted%20on%20various%20publicly%20available%20benchmarks%20confirm%20the%0Amutual%20benefits%20of%20jointly%20learning%20perception%20and%20comprehension%20tasks.%20The%0Aresults%20provide%20further%20evidence%20of%20the%20effectiveness%20and%20superior%20performance%0Aof%20our%20DocPedia%20over%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11810v4&entry.124074799=Read"},
{"title": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation", "author": "Linqing Zhong and Chen Gao and Zihan Ding and Yue Liao and Si Liu", "abstract": "  The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, a MLLM-based method that\ndirectly reasons on the top-view map with complete spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Target-Guided Navigation (TGN) mechanism\nto predict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D benchmarks demonstrate the\nsuperiority of our TopV-Nav, e.g., $+3.9\\%$ SR and $+2.0\\%$ SPL absolute\nimprovements on HM3D.\n", "link": "http://arxiv.org/abs/2411.16425v1", "date": "2024-11-25", "relevancy": 2.9347, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5879}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopV-Nav%3A%20Unlocking%20the%20Top-View%20Spatial%20Reasoning%20Potential%20of%20MLLM%20for%0A%20%20Zero-shot%20Object%20Navigation&body=Title%3A%20TopV-Nav%3A%20Unlocking%20the%20Top-View%20Spatial%20Reasoning%20Potential%20of%20MLLM%20for%0A%20%20Zero-shot%20Object%20Navigation%0AAuthor%3A%20Linqing%20Zhong%20and%20Chen%20Gao%20and%20Zihan%20Ding%20and%20Yue%20Liao%20and%20Si%20Liu%0AAbstract%3A%20%20%20The%20Zero-Shot%20Object%20Navigation%20%28ZSON%29%20task%20requires%20embodied%20agents%20to%20find%0Aa%20previously%20unseen%20object%20by%20navigating%20in%20unfamiliar%20environments.%20Such%20a%0Agoal-oriented%20exploration%20heavily%20relies%20on%20the%20ability%20to%20perceive%2C%0Aunderstand%2C%20and%20reason%20based%20on%20the%20spatial%20information%20of%20the%20environment.%0AHowever%2C%20current%20LLM-based%20approaches%20convert%20visual%20observations%20to%20language%0Adescriptions%20and%20reason%20in%20the%20linguistic%20space%2C%20leading%20to%20the%20loss%20of%20spatial%0Ainformation.%20In%20this%20paper%2C%20we%20introduce%20TopV-Nav%2C%20a%20MLLM-based%20method%20that%0Adirectly%20reasons%20on%20the%20top-view%20map%20with%20complete%20spatial%20information.%20To%0Afully%20unlock%20the%20MLLM%27s%20spatial%20reasoning%20potential%20in%20top-view%20perspective%2C%20we%0Apropose%20the%20Adaptive%20Visual%20Prompt%20Generation%20%28AVPG%29%20method%20to%20adaptively%0Aconstruct%20semantically-rich%20top-view%20map.%20It%20enables%20the%20agent%20to%20directly%0Autilize%20spatial%20information%20contained%20in%20the%20top-view%20map%20to%20conduct%20thorough%0Areasoning.%20Besides%2C%20we%20design%20a%20Dynamic%20Map%20Scaling%20%28DMS%29%20mechanism%20to%0Adynamically%20zoom%20top-view%20map%20at%20preferred%20scales%2C%20enhancing%20local%20fine-grained%0Areasoning.%20Additionally%2C%20we%20devise%20a%20Target-Guided%20Navigation%20%28TGN%29%20mechanism%0Ato%20predict%20and%20to%20utilize%20target%20locations%2C%20facilitating%20global%20and%20human-like%0Aexploration.%20Experiments%20on%20MP3D%20and%20HM3D%20benchmarks%20demonstrate%20the%0Asuperiority%20of%20our%20TopV-Nav%2C%20e.g.%2C%20%24%2B3.9%5C%25%24%20SR%20and%20%24%2B2.0%5C%25%24%20SPL%20absolute%0Aimprovements%20on%20HM3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopV-Nav%253A%2520Unlocking%2520the%2520Top-View%2520Spatial%2520Reasoning%2520Potential%2520of%2520MLLM%2520for%250A%2520%2520Zero-shot%2520Object%2520Navigation%26entry.906535625%3DLinqing%2520Zhong%2520and%2520Chen%2520Gao%2520and%2520Zihan%2520Ding%2520and%2520Yue%2520Liao%2520and%2520Si%2520Liu%26entry.1292438233%3D%2520%2520The%2520Zero-Shot%2520Object%2520Navigation%2520%2528ZSON%2529%2520task%2520requires%2520embodied%2520agents%2520to%2520find%250Aa%2520previously%2520unseen%2520object%2520by%2520navigating%2520in%2520unfamiliar%2520environments.%2520Such%2520a%250Agoal-oriented%2520exploration%2520heavily%2520relies%2520on%2520the%2520ability%2520to%2520perceive%252C%250Aunderstand%252C%2520and%2520reason%2520based%2520on%2520the%2520spatial%2520information%2520of%2520the%2520environment.%250AHowever%252C%2520current%2520LLM-based%2520approaches%2520convert%2520visual%2520observations%2520to%2520language%250Adescriptions%2520and%2520reason%2520in%2520the%2520linguistic%2520space%252C%2520leading%2520to%2520the%2520loss%2520of%2520spatial%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520TopV-Nav%252C%2520a%2520MLLM-based%2520method%2520that%250Adirectly%2520reasons%2520on%2520the%2520top-view%2520map%2520with%2520complete%2520spatial%2520information.%2520To%250Afully%2520unlock%2520the%2520MLLM%2527s%2520spatial%2520reasoning%2520potential%2520in%2520top-view%2520perspective%252C%2520we%250Apropose%2520the%2520Adaptive%2520Visual%2520Prompt%2520Generation%2520%2528AVPG%2529%2520method%2520to%2520adaptively%250Aconstruct%2520semantically-rich%2520top-view%2520map.%2520It%2520enables%2520the%2520agent%2520to%2520directly%250Autilize%2520spatial%2520information%2520contained%2520in%2520the%2520top-view%2520map%2520to%2520conduct%2520thorough%250Areasoning.%2520Besides%252C%2520we%2520design%2520a%2520Dynamic%2520Map%2520Scaling%2520%2528DMS%2529%2520mechanism%2520to%250Adynamically%2520zoom%2520top-view%2520map%2520at%2520preferred%2520scales%252C%2520enhancing%2520local%2520fine-grained%250Areasoning.%2520Additionally%252C%2520we%2520devise%2520a%2520Target-Guided%2520Navigation%2520%2528TGN%2529%2520mechanism%250Ato%2520predict%2520and%2520to%2520utilize%2520target%2520locations%252C%2520facilitating%2520global%2520and%2520human-like%250Aexploration.%2520Experiments%2520on%2520MP3D%2520and%2520HM3D%2520benchmarks%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520TopV-Nav%252C%2520e.g.%252C%2520%2524%252B3.9%255C%2525%2524%2520SR%2520and%2520%2524%252B2.0%255C%2525%2524%2520SPL%2520absolute%250Aimprovements%2520on%2520HM3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopV-Nav%3A%20Unlocking%20the%20Top-View%20Spatial%20Reasoning%20Potential%20of%20MLLM%20for%0A%20%20Zero-shot%20Object%20Navigation&entry.906535625=Linqing%20Zhong%20and%20Chen%20Gao%20and%20Zihan%20Ding%20and%20Yue%20Liao%20and%20Si%20Liu&entry.1292438233=%20%20The%20Zero-Shot%20Object%20Navigation%20%28ZSON%29%20task%20requires%20embodied%20agents%20to%20find%0Aa%20previously%20unseen%20object%20by%20navigating%20in%20unfamiliar%20environments.%20Such%20a%0Agoal-oriented%20exploration%20heavily%20relies%20on%20the%20ability%20to%20perceive%2C%0Aunderstand%2C%20and%20reason%20based%20on%20the%20spatial%20information%20of%20the%20environment.%0AHowever%2C%20current%20LLM-based%20approaches%20convert%20visual%20observations%20to%20language%0Adescriptions%20and%20reason%20in%20the%20linguistic%20space%2C%20leading%20to%20the%20loss%20of%20spatial%0Ainformation.%20In%20this%20paper%2C%20we%20introduce%20TopV-Nav%2C%20a%20MLLM-based%20method%20that%0Adirectly%20reasons%20on%20the%20top-view%20map%20with%20complete%20spatial%20information.%20To%0Afully%20unlock%20the%20MLLM%27s%20spatial%20reasoning%20potential%20in%20top-view%20perspective%2C%20we%0Apropose%20the%20Adaptive%20Visual%20Prompt%20Generation%20%28AVPG%29%20method%20to%20adaptively%0Aconstruct%20semantically-rich%20top-view%20map.%20It%20enables%20the%20agent%20to%20directly%0Autilize%20spatial%20information%20contained%20in%20the%20top-view%20map%20to%20conduct%20thorough%0Areasoning.%20Besides%2C%20we%20design%20a%20Dynamic%20Map%20Scaling%20%28DMS%29%20mechanism%20to%0Adynamically%20zoom%20top-view%20map%20at%20preferred%20scales%2C%20enhancing%20local%20fine-grained%0Areasoning.%20Additionally%2C%20we%20devise%20a%20Target-Guided%20Navigation%20%28TGN%29%20mechanism%0Ato%20predict%20and%20to%20utilize%20target%20locations%2C%20facilitating%20global%20and%20human-like%0Aexploration.%20Experiments%20on%20MP3D%20and%20HM3D%20benchmarks%20demonstrate%20the%0Asuperiority%20of%20our%20TopV-Nav%2C%20e.g.%2C%20%24%2B3.9%5C%25%24%20SR%20and%20%24%2B2.0%5C%25%24%20SPL%20absolute%0Aimprovements%20on%20HM3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16425v1&entry.124074799=Read"},
{"title": "Reconstructing Hand-Held Objects in 3D from Images and Videos", "author": "Jane Wu and Georgios Pavlakos and Georgia Gkioxari and Jitendra Malik", "abstract": "  Objects manipulated by the hand (i.e., manipulanda) are particularly\nchallenging to reconstruct from Internet videos. Not only does the hand occlude\nmuch of the object, but also the object is often only visible in a small number\nof image pixels. At the same time, two strong anchors emerge in this setting:\n(1) estimated 3D hands help disambiguate the location and scale of the object,\nand (2) the set of manipulanda is small relative to all possible objects. With\nthese insights in mind, we present a scalable paradigm for hand-held object\nreconstruction that builds on recent breakthroughs in large language/vision\nmodels and 3D object datasets. Given a monocular RGB video, we aim to\nreconstruct hand-held object geometry in 3D, over time. In order to obtain the\nbest performing single frame model, we first present MCC-Hand-Object (MCC-HO),\nwhich jointly reconstructs hand and object geometry given a single RGB image\nand inferred 3D hand as inputs. Subsequently, we prompt a text-to-3D generative\nmodel using GPT-4(V) to retrieve a 3D object model that matches the object in\nthe image(s); we call this alignment Retrieval-Augmented Reconstruction (RAR).\nRAR provides unified object geometry across all frames, and the result is\nrigidly aligned with both the input images and 3D MCC-HO observations in a\ntemporally consistent manner. Experiments demonstrate that our approach\nachieves state-of-the-art performance on lab and Internet image/video datasets.\nWe make our code and models available on the project website:\nhttps://janehwu.github.io/mcc-ho\n", "link": "http://arxiv.org/abs/2404.06507v3", "date": "2024-11-25", "relevancy": 2.9202, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5806}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20Hand-Held%20Objects%20in%203D%20from%20Images%20and%20Videos&body=Title%3A%20Reconstructing%20Hand-Held%20Objects%20in%203D%20from%20Images%20and%20Videos%0AAuthor%3A%20Jane%20Wu%20and%20Georgios%20Pavlakos%20and%20Georgia%20Gkioxari%20and%20Jitendra%20Malik%0AAbstract%3A%20%20%20Objects%20manipulated%20by%20the%20hand%20%28i.e.%2C%20manipulanda%29%20are%20particularly%0Achallenging%20to%20reconstruct%20from%20Internet%20videos.%20Not%20only%20does%20the%20hand%20occlude%0Amuch%20of%20the%20object%2C%20but%20also%20the%20object%20is%20often%20only%20visible%20in%20a%20small%20number%0Aof%20image%20pixels.%20At%20the%20same%20time%2C%20two%20strong%20anchors%20emerge%20in%20this%20setting%3A%0A%281%29%20estimated%203D%20hands%20help%20disambiguate%20the%20location%20and%20scale%20of%20the%20object%2C%0Aand%20%282%29%20the%20set%20of%20manipulanda%20is%20small%20relative%20to%20all%20possible%20objects.%20With%0Athese%20insights%20in%20mind%2C%20we%20present%20a%20scalable%20paradigm%20for%20hand-held%20object%0Areconstruction%20that%20builds%20on%20recent%20breakthroughs%20in%20large%20language/vision%0Amodels%20and%203D%20object%20datasets.%20Given%20a%20monocular%20RGB%20video%2C%20we%20aim%20to%0Areconstruct%20hand-held%20object%20geometry%20in%203D%2C%20over%20time.%20In%20order%20to%20obtain%20the%0Abest%20performing%20single%20frame%20model%2C%20we%20first%20present%20MCC-Hand-Object%20%28MCC-HO%29%2C%0Awhich%20jointly%20reconstructs%20hand%20and%20object%20geometry%20given%20a%20single%20RGB%20image%0Aand%20inferred%203D%20hand%20as%20inputs.%20Subsequently%2C%20we%20prompt%20a%20text-to-3D%20generative%0Amodel%20using%20GPT-4%28V%29%20to%20retrieve%20a%203D%20object%20model%20that%20matches%20the%20object%20in%0Athe%20image%28s%29%3B%20we%20call%20this%20alignment%20Retrieval-Augmented%20Reconstruction%20%28RAR%29.%0ARAR%20provides%20unified%20object%20geometry%20across%20all%20frames%2C%20and%20the%20result%20is%0Arigidly%20aligned%20with%20both%20the%20input%20images%20and%203D%20MCC-HO%20observations%20in%20a%0Atemporally%20consistent%20manner.%20Experiments%20demonstrate%20that%20our%20approach%0Aachieves%20state-of-the-art%20performance%20on%20lab%20and%20Internet%20image/video%20datasets.%0AWe%20make%20our%20code%20and%20models%20available%20on%20the%20project%20website%3A%0Ahttps%3A//janehwu.github.io/mcc-ho%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06507v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520Hand-Held%2520Objects%2520in%25203D%2520from%2520Images%2520and%2520Videos%26entry.906535625%3DJane%2520Wu%2520and%2520Georgios%2520Pavlakos%2520and%2520Georgia%2520Gkioxari%2520and%2520Jitendra%2520Malik%26entry.1292438233%3D%2520%2520Objects%2520manipulated%2520by%2520the%2520hand%2520%2528i.e.%252C%2520manipulanda%2529%2520are%2520particularly%250Achallenging%2520to%2520reconstruct%2520from%2520Internet%2520videos.%2520Not%2520only%2520does%2520the%2520hand%2520occlude%250Amuch%2520of%2520the%2520object%252C%2520but%2520also%2520the%2520object%2520is%2520often%2520only%2520visible%2520in%2520a%2520small%2520number%250Aof%2520image%2520pixels.%2520At%2520the%2520same%2520time%252C%2520two%2520strong%2520anchors%2520emerge%2520in%2520this%2520setting%253A%250A%25281%2529%2520estimated%25203D%2520hands%2520help%2520disambiguate%2520the%2520location%2520and%2520scale%2520of%2520the%2520object%252C%250Aand%2520%25282%2529%2520the%2520set%2520of%2520manipulanda%2520is%2520small%2520relative%2520to%2520all%2520possible%2520objects.%2520With%250Athese%2520insights%2520in%2520mind%252C%2520we%2520present%2520a%2520scalable%2520paradigm%2520for%2520hand-held%2520object%250Areconstruction%2520that%2520builds%2520on%2520recent%2520breakthroughs%2520in%2520large%2520language/vision%250Amodels%2520and%25203D%2520object%2520datasets.%2520Given%2520a%2520monocular%2520RGB%2520video%252C%2520we%2520aim%2520to%250Areconstruct%2520hand-held%2520object%2520geometry%2520in%25203D%252C%2520over%2520time.%2520In%2520order%2520to%2520obtain%2520the%250Abest%2520performing%2520single%2520frame%2520model%252C%2520we%2520first%2520present%2520MCC-Hand-Object%2520%2528MCC-HO%2529%252C%250Awhich%2520jointly%2520reconstructs%2520hand%2520and%2520object%2520geometry%2520given%2520a%2520single%2520RGB%2520image%250Aand%2520inferred%25203D%2520hand%2520as%2520inputs.%2520Subsequently%252C%2520we%2520prompt%2520a%2520text-to-3D%2520generative%250Amodel%2520using%2520GPT-4%2528V%2529%2520to%2520retrieve%2520a%25203D%2520object%2520model%2520that%2520matches%2520the%2520object%2520in%250Athe%2520image%2528s%2529%253B%2520we%2520call%2520this%2520alignment%2520Retrieval-Augmented%2520Reconstruction%2520%2528RAR%2529.%250ARAR%2520provides%2520unified%2520object%2520geometry%2520across%2520all%2520frames%252C%2520and%2520the%2520result%2520is%250Arigidly%2520aligned%2520with%2520both%2520the%2520input%2520images%2520and%25203D%2520MCC-HO%2520observations%2520in%2520a%250Atemporally%2520consistent%2520manner.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%250Aachieves%2520state-of-the-art%2520performance%2520on%2520lab%2520and%2520Internet%2520image/video%2520datasets.%250AWe%2520make%2520our%2520code%2520and%2520models%2520available%2520on%2520the%2520project%2520website%253A%250Ahttps%253A//janehwu.github.io/mcc-ho%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06507v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20Hand-Held%20Objects%20in%203D%20from%20Images%20and%20Videos&entry.906535625=Jane%20Wu%20and%20Georgios%20Pavlakos%20and%20Georgia%20Gkioxari%20and%20Jitendra%20Malik&entry.1292438233=%20%20Objects%20manipulated%20by%20the%20hand%20%28i.e.%2C%20manipulanda%29%20are%20particularly%0Achallenging%20to%20reconstruct%20from%20Internet%20videos.%20Not%20only%20does%20the%20hand%20occlude%0Amuch%20of%20the%20object%2C%20but%20also%20the%20object%20is%20often%20only%20visible%20in%20a%20small%20number%0Aof%20image%20pixels.%20At%20the%20same%20time%2C%20two%20strong%20anchors%20emerge%20in%20this%20setting%3A%0A%281%29%20estimated%203D%20hands%20help%20disambiguate%20the%20location%20and%20scale%20of%20the%20object%2C%0Aand%20%282%29%20the%20set%20of%20manipulanda%20is%20small%20relative%20to%20all%20possible%20objects.%20With%0Athese%20insights%20in%20mind%2C%20we%20present%20a%20scalable%20paradigm%20for%20hand-held%20object%0Areconstruction%20that%20builds%20on%20recent%20breakthroughs%20in%20large%20language/vision%0Amodels%20and%203D%20object%20datasets.%20Given%20a%20monocular%20RGB%20video%2C%20we%20aim%20to%0Areconstruct%20hand-held%20object%20geometry%20in%203D%2C%20over%20time.%20In%20order%20to%20obtain%20the%0Abest%20performing%20single%20frame%20model%2C%20we%20first%20present%20MCC-Hand-Object%20%28MCC-HO%29%2C%0Awhich%20jointly%20reconstructs%20hand%20and%20object%20geometry%20given%20a%20single%20RGB%20image%0Aand%20inferred%203D%20hand%20as%20inputs.%20Subsequently%2C%20we%20prompt%20a%20text-to-3D%20generative%0Amodel%20using%20GPT-4%28V%29%20to%20retrieve%20a%203D%20object%20model%20that%20matches%20the%20object%20in%0Athe%20image%28s%29%3B%20we%20call%20this%20alignment%20Retrieval-Augmented%20Reconstruction%20%28RAR%29.%0ARAR%20provides%20unified%20object%20geometry%20across%20all%20frames%2C%20and%20the%20result%20is%0Arigidly%20aligned%20with%20both%20the%20input%20images%20and%203D%20MCC-HO%20observations%20in%20a%0Atemporally%20consistent%20manner.%20Experiments%20demonstrate%20that%20our%20approach%0Aachieves%20state-of-the-art%20performance%20on%20lab%20and%20Internet%20image/video%20datasets.%0AWe%20make%20our%20code%20and%20models%20available%20on%20the%20project%20website%3A%0Ahttps%3A//janehwu.github.io/mcc-ho%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06507v3&entry.124074799=Read"},
{"title": "A Study on Unsupervised Domain Adaptation for Semantic Segmentation in\n  the Era of Vision-Language Models", "author": "Manuel Schwonberg and Claus Werner and Hanno Gottschalk and Carsten Meyer", "abstract": "  Despite the recent progress in deep learning based computer vision, domain\nshifts are still one of the major challenges. Semantic segmentation for\nautonomous driving faces a wide range of domain shifts, e.g. caused by changing\nweather conditions, new geolocations and the frequent use of synthetic data in\nmodel training. Unsupervised domain adaptation (UDA) methods have emerged which\nadapt a model to a new target domain by only using unlabeled data of that\ndomain. The variety of UDA methods is large but all of them use ImageNet\npre-trained models. Recently, vision-language models have demonstrated strong\ngeneralization capabilities which may facilitate domain adaptation. We show\nthat simply replacing the encoder of existing UDA methods like DACS by a\nvision-language pre-trained encoder can result in significant performance\nimprovements of up to 10.0% mIoU on the GTA5-to-Cityscapes domain shift. For\nthe generalization performance to unseen domains, the newly employed\nvision-language pre-trained encoder provides a gain of up to 13.7% mIoU across\nthree unseen datasets. However, we find that not all UDA methods can be easily\npaired with the new encoder and that the UDA performance does not always\nlikewise transfer into generalization performance. Finally, we perform our\nexperiments on an adverse weather condition domain shift to further verify our\nfindings on a pure real-to-real domain shift.\n", "link": "http://arxiv.org/abs/2411.16407v1", "date": "2024-11-25", "relevancy": 2.8613, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20on%20Unsupervised%20Domain%20Adaptation%20for%20Semantic%20Segmentation%20in%0A%20%20the%20Era%20of%20Vision-Language%20Models&body=Title%3A%20A%20Study%20on%20Unsupervised%20Domain%20Adaptation%20for%20Semantic%20Segmentation%20in%0A%20%20the%20Era%20of%20Vision-Language%20Models%0AAuthor%3A%20Manuel%20Schwonberg%20and%20Claus%20Werner%20and%20Hanno%20Gottschalk%20and%20Carsten%20Meyer%0AAbstract%3A%20%20%20Despite%20the%20recent%20progress%20in%20deep%20learning%20based%20computer%20vision%2C%20domain%0Ashifts%20are%20still%20one%20of%20the%20major%20challenges.%20Semantic%20segmentation%20for%0Aautonomous%20driving%20faces%20a%20wide%20range%20of%20domain%20shifts%2C%20e.g.%20caused%20by%20changing%0Aweather%20conditions%2C%20new%20geolocations%20and%20the%20frequent%20use%20of%20synthetic%20data%20in%0Amodel%20training.%20Unsupervised%20domain%20adaptation%20%28UDA%29%20methods%20have%20emerged%20which%0Aadapt%20a%20model%20to%20a%20new%20target%20domain%20by%20only%20using%20unlabeled%20data%20of%20that%0Adomain.%20The%20variety%20of%20UDA%20methods%20is%20large%20but%20all%20of%20them%20use%20ImageNet%0Apre-trained%20models.%20Recently%2C%20vision-language%20models%20have%20demonstrated%20strong%0Ageneralization%20capabilities%20which%20may%20facilitate%20domain%20adaptation.%20We%20show%0Athat%20simply%20replacing%20the%20encoder%20of%20existing%20UDA%20methods%20like%20DACS%20by%20a%0Avision-language%20pre-trained%20encoder%20can%20result%20in%20significant%20performance%0Aimprovements%20of%20up%20to%2010.0%25%20mIoU%20on%20the%20GTA5-to-Cityscapes%20domain%20shift.%20For%0Athe%20generalization%20performance%20to%20unseen%20domains%2C%20the%20newly%20employed%0Avision-language%20pre-trained%20encoder%20provides%20a%20gain%20of%20up%20to%2013.7%25%20mIoU%20across%0Athree%20unseen%20datasets.%20However%2C%20we%20find%20that%20not%20all%20UDA%20methods%20can%20be%20easily%0Apaired%20with%20the%20new%20encoder%20and%20that%20the%20UDA%20performance%20does%20not%20always%0Alikewise%20transfer%20into%20generalization%20performance.%20Finally%2C%20we%20perform%20our%0Aexperiments%20on%20an%20adverse%20weather%20condition%20domain%20shift%20to%20further%20verify%20our%0Afindings%20on%20a%20pure%20real-to-real%20domain%20shift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520on%2520Unsupervised%2520Domain%2520Adaptation%2520for%2520Semantic%2520Segmentation%2520in%250A%2520%2520the%2520Era%2520of%2520Vision-Language%2520Models%26entry.906535625%3DManuel%2520Schwonberg%2520and%2520Claus%2520Werner%2520and%2520Hanno%2520Gottschalk%2520and%2520Carsten%2520Meyer%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520progress%2520in%2520deep%2520learning%2520based%2520computer%2520vision%252C%2520domain%250Ashifts%2520are%2520still%2520one%2520of%2520the%2520major%2520challenges.%2520Semantic%2520segmentation%2520for%250Aautonomous%2520driving%2520faces%2520a%2520wide%2520range%2520of%2520domain%2520shifts%252C%2520e.g.%2520caused%2520by%2520changing%250Aweather%2520conditions%252C%2520new%2520geolocations%2520and%2520the%2520frequent%2520use%2520of%2520synthetic%2520data%2520in%250Amodel%2520training.%2520Unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520methods%2520have%2520emerged%2520which%250Aadapt%2520a%2520model%2520to%2520a%2520new%2520target%2520domain%2520by%2520only%2520using%2520unlabeled%2520data%2520of%2520that%250Adomain.%2520The%2520variety%2520of%2520UDA%2520methods%2520is%2520large%2520but%2520all%2520of%2520them%2520use%2520ImageNet%250Apre-trained%2520models.%2520Recently%252C%2520vision-language%2520models%2520have%2520demonstrated%2520strong%250Ageneralization%2520capabilities%2520which%2520may%2520facilitate%2520domain%2520adaptation.%2520We%2520show%250Athat%2520simply%2520replacing%2520the%2520encoder%2520of%2520existing%2520UDA%2520methods%2520like%2520DACS%2520by%2520a%250Avision-language%2520pre-trained%2520encoder%2520can%2520result%2520in%2520significant%2520performance%250Aimprovements%2520of%2520up%2520to%252010.0%2525%2520mIoU%2520on%2520the%2520GTA5-to-Cityscapes%2520domain%2520shift.%2520For%250Athe%2520generalization%2520performance%2520to%2520unseen%2520domains%252C%2520the%2520newly%2520employed%250Avision-language%2520pre-trained%2520encoder%2520provides%2520a%2520gain%2520of%2520up%2520to%252013.7%2525%2520mIoU%2520across%250Athree%2520unseen%2520datasets.%2520However%252C%2520we%2520find%2520that%2520not%2520all%2520UDA%2520methods%2520can%2520be%2520easily%250Apaired%2520with%2520the%2520new%2520encoder%2520and%2520that%2520the%2520UDA%2520performance%2520does%2520not%2520always%250Alikewise%2520transfer%2520into%2520generalization%2520performance.%2520Finally%252C%2520we%2520perform%2520our%250Aexperiments%2520on%2520an%2520adverse%2520weather%2520condition%2520domain%2520shift%2520to%2520further%2520verify%2520our%250Afindings%2520on%2520a%2520pure%2520real-to-real%2520domain%2520shift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20on%20Unsupervised%20Domain%20Adaptation%20for%20Semantic%20Segmentation%20in%0A%20%20the%20Era%20of%20Vision-Language%20Models&entry.906535625=Manuel%20Schwonberg%20and%20Claus%20Werner%20and%20Hanno%20Gottschalk%20and%20Carsten%20Meyer&entry.1292438233=%20%20Despite%20the%20recent%20progress%20in%20deep%20learning%20based%20computer%20vision%2C%20domain%0Ashifts%20are%20still%20one%20of%20the%20major%20challenges.%20Semantic%20segmentation%20for%0Aautonomous%20driving%20faces%20a%20wide%20range%20of%20domain%20shifts%2C%20e.g.%20caused%20by%20changing%0Aweather%20conditions%2C%20new%20geolocations%20and%20the%20frequent%20use%20of%20synthetic%20data%20in%0Amodel%20training.%20Unsupervised%20domain%20adaptation%20%28UDA%29%20methods%20have%20emerged%20which%0Aadapt%20a%20model%20to%20a%20new%20target%20domain%20by%20only%20using%20unlabeled%20data%20of%20that%0Adomain.%20The%20variety%20of%20UDA%20methods%20is%20large%20but%20all%20of%20them%20use%20ImageNet%0Apre-trained%20models.%20Recently%2C%20vision-language%20models%20have%20demonstrated%20strong%0Ageneralization%20capabilities%20which%20may%20facilitate%20domain%20adaptation.%20We%20show%0Athat%20simply%20replacing%20the%20encoder%20of%20existing%20UDA%20methods%20like%20DACS%20by%20a%0Avision-language%20pre-trained%20encoder%20can%20result%20in%20significant%20performance%0Aimprovements%20of%20up%20to%2010.0%25%20mIoU%20on%20the%20GTA5-to-Cityscapes%20domain%20shift.%20For%0Athe%20generalization%20performance%20to%20unseen%20domains%2C%20the%20newly%20employed%0Avision-language%20pre-trained%20encoder%20provides%20a%20gain%20of%20up%20to%2013.7%25%20mIoU%20across%0Athree%20unseen%20datasets.%20However%2C%20we%20find%20that%20not%20all%20UDA%20methods%20can%20be%20easily%0Apaired%20with%20the%20new%20encoder%20and%20that%20the%20UDA%20performance%20does%20not%20always%0Alikewise%20transfer%20into%20generalization%20performance.%20Finally%2C%20we%20perform%20our%0Aexperiments%20on%20an%20adverse%20weather%20condition%20domain%20shift%20to%20further%20verify%20our%0Afindings%20on%20a%20pure%20real-to-real%20domain%20shift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16407v1&entry.124074799=Read"},
{"title": "Scalable and Efficient Temporal Graph Representation Learning via\n  Forward Recent Sampling", "author": "Yuhong Luo and Pan Li", "abstract": "  Temporal graph representation learning (TGRL) is essential for modeling\ndynamic systems in real-world networks. However, traditional TGRL methods,\ndespite their effectiveness, often face significant computational challenges\nand inference delays due to the inefficient sampling of temporal neighbors.\nConventional sampling methods typically involve backtracking through the\ninteraction history of each node. In this paper, we propose a novel TGRL\nframework, No-Looking-Back (NLB), which overcomes these challenges by\nintroducing a forward recent sampling strategy. This strategy eliminates the\nneed to backtrack through historical interactions by utilizing a\nGPU-executable, size-constrained hash table for each node. The hash table\nrecords a down-sampled set of recent interactions, enabling rapid query\nresponses with minimal inference latency. The maintenance of this hash table is\nhighly efficient, operating with $O(1)$ complexity. Fully compatible with GPU\nprocessing, NLB maximizes programmability, parallelism, and power efficiency.\nEmpirical evaluations demonstrate that NLB not only matches or surpasses\nstate-of-the-art methods in accuracy for tasks like link prediction and node\nclassification across six real-world datasets but also achieves 1.32-4.40x\nfaster training, 1.2-7.94x greater energy efficiency, and 1.63-12.95x lower\ninference latency compared to competitive baselines. The link to the code:\nhttps://github.com/Graph-COM/NLB.\n", "link": "http://arxiv.org/abs/2402.01964v2", "date": "2024-11-25", "relevancy": 2.7801, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5989}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5419}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20and%20Efficient%20Temporal%20Graph%20Representation%20Learning%20via%0A%20%20Forward%20Recent%20Sampling&body=Title%3A%20Scalable%20and%20Efficient%20Temporal%20Graph%20Representation%20Learning%20via%0A%20%20Forward%20Recent%20Sampling%0AAuthor%3A%20Yuhong%20Luo%20and%20Pan%20Li%0AAbstract%3A%20%20%20Temporal%20graph%20representation%20learning%20%28TGRL%29%20is%20essential%20for%20modeling%0Adynamic%20systems%20in%20real-world%20networks.%20However%2C%20traditional%20TGRL%20methods%2C%0Adespite%20their%20effectiveness%2C%20often%20face%20significant%20computational%20challenges%0Aand%20inference%20delays%20due%20to%20the%20inefficient%20sampling%20of%20temporal%20neighbors.%0AConventional%20sampling%20methods%20typically%20involve%20backtracking%20through%20the%0Ainteraction%20history%20of%20each%20node.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20TGRL%0Aframework%2C%20No-Looking-Back%20%28NLB%29%2C%20which%20overcomes%20these%20challenges%20by%0Aintroducing%20a%20forward%20recent%20sampling%20strategy.%20This%20strategy%20eliminates%20the%0Aneed%20to%20backtrack%20through%20historical%20interactions%20by%20utilizing%20a%0AGPU-executable%2C%20size-constrained%20hash%20table%20for%20each%20node.%20The%20hash%20table%0Arecords%20a%20down-sampled%20set%20of%20recent%20interactions%2C%20enabling%20rapid%20query%0Aresponses%20with%20minimal%20inference%20latency.%20The%20maintenance%20of%20this%20hash%20table%20is%0Ahighly%20efficient%2C%20operating%20with%20%24O%281%29%24%20complexity.%20Fully%20compatible%20with%20GPU%0Aprocessing%2C%20NLB%20maximizes%20programmability%2C%20parallelism%2C%20and%20power%20efficiency.%0AEmpirical%20evaluations%20demonstrate%20that%20NLB%20not%20only%20matches%20or%20surpasses%0Astate-of-the-art%20methods%20in%20accuracy%20for%20tasks%20like%20link%20prediction%20and%20node%0Aclassification%20across%20six%20real-world%20datasets%20but%20also%20achieves%201.32-4.40x%0Afaster%20training%2C%201.2-7.94x%20greater%20energy%20efficiency%2C%20and%201.63-12.95x%20lower%0Ainference%20latency%20compared%20to%20competitive%20baselines.%20The%20link%20to%20the%20code%3A%0Ahttps%3A//github.com/Graph-COM/NLB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01964v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520and%2520Efficient%2520Temporal%2520Graph%2520Representation%2520Learning%2520via%250A%2520%2520Forward%2520Recent%2520Sampling%26entry.906535625%3DYuhong%2520Luo%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520Temporal%2520graph%2520representation%2520learning%2520%2528TGRL%2529%2520is%2520essential%2520for%2520modeling%250Adynamic%2520systems%2520in%2520real-world%2520networks.%2520However%252C%2520traditional%2520TGRL%2520methods%252C%250Adespite%2520their%2520effectiveness%252C%2520often%2520face%2520significant%2520computational%2520challenges%250Aand%2520inference%2520delays%2520due%2520to%2520the%2520inefficient%2520sampling%2520of%2520temporal%2520neighbors.%250AConventional%2520sampling%2520methods%2520typically%2520involve%2520backtracking%2520through%2520the%250Ainteraction%2520history%2520of%2520each%2520node.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520TGRL%250Aframework%252C%2520No-Looking-Back%2520%2528NLB%2529%252C%2520which%2520overcomes%2520these%2520challenges%2520by%250Aintroducing%2520a%2520forward%2520recent%2520sampling%2520strategy.%2520This%2520strategy%2520eliminates%2520the%250Aneed%2520to%2520backtrack%2520through%2520historical%2520interactions%2520by%2520utilizing%2520a%250AGPU-executable%252C%2520size-constrained%2520hash%2520table%2520for%2520each%2520node.%2520The%2520hash%2520table%250Arecords%2520a%2520down-sampled%2520set%2520of%2520recent%2520interactions%252C%2520enabling%2520rapid%2520query%250Aresponses%2520with%2520minimal%2520inference%2520latency.%2520The%2520maintenance%2520of%2520this%2520hash%2520table%2520is%250Ahighly%2520efficient%252C%2520operating%2520with%2520%2524O%25281%2529%2524%2520complexity.%2520Fully%2520compatible%2520with%2520GPU%250Aprocessing%252C%2520NLB%2520maximizes%2520programmability%252C%2520parallelism%252C%2520and%2520power%2520efficiency.%250AEmpirical%2520evaluations%2520demonstrate%2520that%2520NLB%2520not%2520only%2520matches%2520or%2520surpasses%250Astate-of-the-art%2520methods%2520in%2520accuracy%2520for%2520tasks%2520like%2520link%2520prediction%2520and%2520node%250Aclassification%2520across%2520six%2520real-world%2520datasets%2520but%2520also%2520achieves%25201.32-4.40x%250Afaster%2520training%252C%25201.2-7.94x%2520greater%2520energy%2520efficiency%252C%2520and%25201.63-12.95x%2520lower%250Ainference%2520latency%2520compared%2520to%2520competitive%2520baselines.%2520The%2520link%2520to%2520the%2520code%253A%250Ahttps%253A//github.com/Graph-COM/NLB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01964v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20and%20Efficient%20Temporal%20Graph%20Representation%20Learning%20via%0A%20%20Forward%20Recent%20Sampling&entry.906535625=Yuhong%20Luo%20and%20Pan%20Li&entry.1292438233=%20%20Temporal%20graph%20representation%20learning%20%28TGRL%29%20is%20essential%20for%20modeling%0Adynamic%20systems%20in%20real-world%20networks.%20However%2C%20traditional%20TGRL%20methods%2C%0Adespite%20their%20effectiveness%2C%20often%20face%20significant%20computational%20challenges%0Aand%20inference%20delays%20due%20to%20the%20inefficient%20sampling%20of%20temporal%20neighbors.%0AConventional%20sampling%20methods%20typically%20involve%20backtracking%20through%20the%0Ainteraction%20history%20of%20each%20node.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20TGRL%0Aframework%2C%20No-Looking-Back%20%28NLB%29%2C%20which%20overcomes%20these%20challenges%20by%0Aintroducing%20a%20forward%20recent%20sampling%20strategy.%20This%20strategy%20eliminates%20the%0Aneed%20to%20backtrack%20through%20historical%20interactions%20by%20utilizing%20a%0AGPU-executable%2C%20size-constrained%20hash%20table%20for%20each%20node.%20The%20hash%20table%0Arecords%20a%20down-sampled%20set%20of%20recent%20interactions%2C%20enabling%20rapid%20query%0Aresponses%20with%20minimal%20inference%20latency.%20The%20maintenance%20of%20this%20hash%20table%20is%0Ahighly%20efficient%2C%20operating%20with%20%24O%281%29%24%20complexity.%20Fully%20compatible%20with%20GPU%0Aprocessing%2C%20NLB%20maximizes%20programmability%2C%20parallelism%2C%20and%20power%20efficiency.%0AEmpirical%20evaluations%20demonstrate%20that%20NLB%20not%20only%20matches%20or%20surpasses%0Astate-of-the-art%20methods%20in%20accuracy%20for%20tasks%20like%20link%20prediction%20and%20node%0Aclassification%20across%20six%20real-world%20datasets%20but%20also%20achieves%201.32-4.40x%0Afaster%20training%2C%201.2-7.94x%20greater%20energy%20efficiency%2C%20and%201.63-12.95x%20lower%0Ainference%20latency%20compared%20to%20competitive%20baselines.%20The%20link%20to%20the%20code%3A%0Ahttps%3A//github.com/Graph-COM/NLB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01964v2&entry.124074799=Read"},
{"title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?", "author": "Yunlong Tang and Junjia Guo and Hang Hua and Susan Liang and Mingqian Feng and Xinyang Li and Rui Mao and Chao Huang and Jing Bi and Zeliang Zhang and Pooyan Fazli and Chenliang Xu", "abstract": "  The advancement of Multimodal Large Language Models (MLLMs) has enabled\nsignificant progress in multimodal understanding, expanding their capacity to\nanalyze video content. However, existing evaluation benchmarks for MLLMs\nprimarily focus on abstract video comprehension, lacking a detailed assessment\nof their ability to understand video compositions, the nuanced interpretation\nof how visual elements combine and interact within highly compiled video\ncontexts. We introduce VidComposition, a new benchmark specifically designed to\nevaluate the video composition understanding capabilities of MLLMs using\ncarefully curated compiled videos and cinematic-level annotations.\nVidComposition includes 982 videos with 1706 multiple-choice questions,\ncovering various compositional aspects such as camera movement, angle, shot\nsize, narrative structure, character actions and emotions, etc. Our\ncomprehensive evaluation of 33 open-source and proprietary MLLMs reveals a\nsignificant performance gap between human and model capabilities. This\nhighlights the limitations of current MLLMs in understanding complex, compiled\nvideo compositions and offers insights into areas for further improvement. The\nleaderboard and evaluation code are available at\nhttps://yunlong10.github.io/VidComposition/.\n", "link": "http://arxiv.org/abs/2411.10979v3", "date": "2024-11-25", "relevancy": 2.7741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidComposition%3A%20Can%20MLLMs%20Analyze%20Compositions%20in%20Compiled%20Videos%3F&body=Title%3A%20VidComposition%3A%20Can%20MLLMs%20Analyze%20Compositions%20in%20Compiled%20Videos%3F%0AAuthor%3A%20Yunlong%20Tang%20and%20Junjia%20Guo%20and%20Hang%20Hua%20and%20Susan%20Liang%20and%20Mingqian%20Feng%20and%20Xinyang%20Li%20and%20Rui%20Mao%20and%20Chao%20Huang%20and%20Jing%20Bi%20and%20Zeliang%20Zhang%20and%20Pooyan%20Fazli%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20The%20advancement%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20enabled%0Asignificant%20progress%20in%20multimodal%20understanding%2C%20expanding%20their%20capacity%20to%0Aanalyze%20video%20content.%20However%2C%20existing%20evaluation%20benchmarks%20for%20MLLMs%0Aprimarily%20focus%20on%20abstract%20video%20comprehension%2C%20lacking%20a%20detailed%20assessment%0Aof%20their%20ability%20to%20understand%20video%20compositions%2C%20the%20nuanced%20interpretation%0Aof%20how%20visual%20elements%20combine%20and%20interact%20within%20highly%20compiled%20video%0Acontexts.%20We%20introduce%20VidComposition%2C%20a%20new%20benchmark%20specifically%20designed%20to%0Aevaluate%20the%20video%20composition%20understanding%20capabilities%20of%20MLLMs%20using%0Acarefully%20curated%20compiled%20videos%20and%20cinematic-level%20annotations.%0AVidComposition%20includes%20982%20videos%20with%201706%20multiple-choice%20questions%2C%0Acovering%20various%20compositional%20aspects%20such%20as%20camera%20movement%2C%20angle%2C%20shot%0Asize%2C%20narrative%20structure%2C%20character%20actions%20and%20emotions%2C%20etc.%20Our%0Acomprehensive%20evaluation%20of%2033%20open-source%20and%20proprietary%20MLLMs%20reveals%20a%0Asignificant%20performance%20gap%20between%20human%20and%20model%20capabilities.%20This%0Ahighlights%20the%20limitations%20of%20current%20MLLMs%20in%20understanding%20complex%2C%20compiled%0Avideo%20compositions%20and%20offers%20insights%20into%20areas%20for%20further%20improvement.%20The%0Aleaderboard%20and%20evaluation%20code%20are%20available%20at%0Ahttps%3A//yunlong10.github.io/VidComposition/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidComposition%253A%2520Can%2520MLLMs%2520Analyze%2520Compositions%2520in%2520Compiled%2520Videos%253F%26entry.906535625%3DYunlong%2520Tang%2520and%2520Junjia%2520Guo%2520and%2520Hang%2520Hua%2520and%2520Susan%2520Liang%2520and%2520Mingqian%2520Feng%2520and%2520Xinyang%2520Li%2520and%2520Rui%2520Mao%2520and%2520Chao%2520Huang%2520and%2520Jing%2520Bi%2520and%2520Zeliang%2520Zhang%2520and%2520Pooyan%2520Fazli%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520enabled%250Asignificant%2520progress%2520in%2520multimodal%2520understanding%252C%2520expanding%2520their%2520capacity%2520to%250Aanalyze%2520video%2520content.%2520However%252C%2520existing%2520evaluation%2520benchmarks%2520for%2520MLLMs%250Aprimarily%2520focus%2520on%2520abstract%2520video%2520comprehension%252C%2520lacking%2520a%2520detailed%2520assessment%250Aof%2520their%2520ability%2520to%2520understand%2520video%2520compositions%252C%2520the%2520nuanced%2520interpretation%250Aof%2520how%2520visual%2520elements%2520combine%2520and%2520interact%2520within%2520highly%2520compiled%2520video%250Acontexts.%2520We%2520introduce%2520VidComposition%252C%2520a%2520new%2520benchmark%2520specifically%2520designed%2520to%250Aevaluate%2520the%2520video%2520composition%2520understanding%2520capabilities%2520of%2520MLLMs%2520using%250Acarefully%2520curated%2520compiled%2520videos%2520and%2520cinematic-level%2520annotations.%250AVidComposition%2520includes%2520982%2520videos%2520with%25201706%2520multiple-choice%2520questions%252C%250Acovering%2520various%2520compositional%2520aspects%2520such%2520as%2520camera%2520movement%252C%2520angle%252C%2520shot%250Asize%252C%2520narrative%2520structure%252C%2520character%2520actions%2520and%2520emotions%252C%2520etc.%2520Our%250Acomprehensive%2520evaluation%2520of%252033%2520open-source%2520and%2520proprietary%2520MLLMs%2520reveals%2520a%250Asignificant%2520performance%2520gap%2520between%2520human%2520and%2520model%2520capabilities.%2520This%250Ahighlights%2520the%2520limitations%2520of%2520current%2520MLLMs%2520in%2520understanding%2520complex%252C%2520compiled%250Avideo%2520compositions%2520and%2520offers%2520insights%2520into%2520areas%2520for%2520further%2520improvement.%2520The%250Aleaderboard%2520and%2520evaluation%2520code%2520are%2520available%2520at%250Ahttps%253A//yunlong10.github.io/VidComposition/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidComposition%3A%20Can%20MLLMs%20Analyze%20Compositions%20in%20Compiled%20Videos%3F&entry.906535625=Yunlong%20Tang%20and%20Junjia%20Guo%20and%20Hang%20Hua%20and%20Susan%20Liang%20and%20Mingqian%20Feng%20and%20Xinyang%20Li%20and%20Rui%20Mao%20and%20Chao%20Huang%20and%20Jing%20Bi%20and%20Zeliang%20Zhang%20and%20Pooyan%20Fazli%20and%20Chenliang%20Xu&entry.1292438233=%20%20The%20advancement%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20enabled%0Asignificant%20progress%20in%20multimodal%20understanding%2C%20expanding%20their%20capacity%20to%0Aanalyze%20video%20content.%20However%2C%20existing%20evaluation%20benchmarks%20for%20MLLMs%0Aprimarily%20focus%20on%20abstract%20video%20comprehension%2C%20lacking%20a%20detailed%20assessment%0Aof%20their%20ability%20to%20understand%20video%20compositions%2C%20the%20nuanced%20interpretation%0Aof%20how%20visual%20elements%20combine%20and%20interact%20within%20highly%20compiled%20video%0Acontexts.%20We%20introduce%20VidComposition%2C%20a%20new%20benchmark%20specifically%20designed%20to%0Aevaluate%20the%20video%20composition%20understanding%20capabilities%20of%20MLLMs%20using%0Acarefully%20curated%20compiled%20videos%20and%20cinematic-level%20annotations.%0AVidComposition%20includes%20982%20videos%20with%201706%20multiple-choice%20questions%2C%0Acovering%20various%20compositional%20aspects%20such%20as%20camera%20movement%2C%20angle%2C%20shot%0Asize%2C%20narrative%20structure%2C%20character%20actions%20and%20emotions%2C%20etc.%20Our%0Acomprehensive%20evaluation%20of%2033%20open-source%20and%20proprietary%20MLLMs%20reveals%20a%0Asignificant%20performance%20gap%20between%20human%20and%20model%20capabilities.%20This%0Ahighlights%20the%20limitations%20of%20current%20MLLMs%20in%20understanding%20complex%2C%20compiled%0Avideo%20compositions%20and%20offers%20insights%20into%20areas%20for%20further%20improvement.%20The%0Aleaderboard%20and%20evaluation%20code%20are%20available%20at%0Ahttps%3A//yunlong10.github.io/VidComposition/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10979v3&entry.124074799=Read"},
{"title": "@Bench: Benchmarking Vision-Language Models for Human-centered Assistive\n  Technology", "author": "Xin Jiang and Junwei Zheng and Ruiping Liu and Jiahang Li and Jiaming Zhang and Sven Matthiesen and Rainer Stiefelhagen", "abstract": "  As Vision-Language Models (VLMs) advance, human-centered Assistive\nTechnologies (ATs) for helping People with Visual Impairments (PVIs) are\nevolving into generalists, capable of performing multiple tasks simultaneously.\nHowever, benchmarking VLMs for ATs remains under-explored. To bridge this gap,\nwe first create a novel AT benchmark (@Bench). Guided by a pre-design user\nstudy with PVIs, our benchmark includes the five most crucial vision-language\ntasks: Panoptic Segmentation, Depth Estimation, Optical Character Recognition\n(OCR), Image Captioning, and Visual Question Answering (VQA). Besides, we\npropose a novel AT model (@Model) that addresses all tasks simultaneously and\ncan be expanded to more assistive functions for helping PVIs. Our framework\nexhibits outstanding performance across tasks by integrating multi-modal\ninformation, and it offers PVIs a more comprehensive assistance. Extensive\nexperiments prove the effectiveness and generalizability of our framework.\n", "link": "http://arxiv.org/abs/2409.14215v2", "date": "2024-11-25", "relevancy": 2.7731, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%40Bench%3A%20Benchmarking%20Vision-Language%20Models%20for%20Human-centered%20Assistive%0A%20%20Technology&body=Title%3A%20%40Bench%3A%20Benchmarking%20Vision-Language%20Models%20for%20Human-centered%20Assistive%0A%20%20Technology%0AAuthor%3A%20Xin%20Jiang%20and%20Junwei%20Zheng%20and%20Ruiping%20Liu%20and%20Jiahang%20Li%20and%20Jiaming%20Zhang%20and%20Sven%20Matthiesen%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20As%20Vision-Language%20Models%20%28VLMs%29%20advance%2C%20human-centered%20Assistive%0ATechnologies%20%28ATs%29%20for%20helping%20People%20with%20Visual%20Impairments%20%28PVIs%29%20are%0Aevolving%20into%20generalists%2C%20capable%20of%20performing%20multiple%20tasks%20simultaneously.%0AHowever%2C%20benchmarking%20VLMs%20for%20ATs%20remains%20under-explored.%20To%20bridge%20this%20gap%2C%0Awe%20first%20create%20a%20novel%20AT%20benchmark%20%28%40Bench%29.%20Guided%20by%20a%20pre-design%20user%0Astudy%20with%20PVIs%2C%20our%20benchmark%20includes%20the%20five%20most%20crucial%20vision-language%0Atasks%3A%20Panoptic%20Segmentation%2C%20Depth%20Estimation%2C%20Optical%20Character%20Recognition%0A%28OCR%29%2C%20Image%20Captioning%2C%20and%20Visual%20Question%20Answering%20%28VQA%29.%20Besides%2C%20we%0Apropose%20a%20novel%20AT%20model%20%28%40Model%29%20that%20addresses%20all%20tasks%20simultaneously%20and%0Acan%20be%20expanded%20to%20more%20assistive%20functions%20for%20helping%20PVIs.%20Our%20framework%0Aexhibits%20outstanding%20performance%20across%20tasks%20by%20integrating%20multi-modal%0Ainformation%2C%20and%20it%20offers%20PVIs%20a%20more%20comprehensive%20assistance.%20Extensive%0Aexperiments%20prove%20the%20effectiveness%20and%20generalizability%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14215v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2540Bench%253A%2520Benchmarking%2520Vision-Language%2520Models%2520for%2520Human-centered%2520Assistive%250A%2520%2520Technology%26entry.906535625%3DXin%2520Jiang%2520and%2520Junwei%2520Zheng%2520and%2520Ruiping%2520Liu%2520and%2520Jiahang%2520Li%2520and%2520Jiaming%2520Zhang%2520and%2520Sven%2520Matthiesen%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520As%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520advance%252C%2520human-centered%2520Assistive%250ATechnologies%2520%2528ATs%2529%2520for%2520helping%2520People%2520with%2520Visual%2520Impairments%2520%2528PVIs%2529%2520are%250Aevolving%2520into%2520generalists%252C%2520capable%2520of%2520performing%2520multiple%2520tasks%2520simultaneously.%250AHowever%252C%2520benchmarking%2520VLMs%2520for%2520ATs%2520remains%2520under-explored.%2520To%2520bridge%2520this%2520gap%252C%250Awe%2520first%2520create%2520a%2520novel%2520AT%2520benchmark%2520%2528%2540Bench%2529.%2520Guided%2520by%2520a%2520pre-design%2520user%250Astudy%2520with%2520PVIs%252C%2520our%2520benchmark%2520includes%2520the%2520five%2520most%2520crucial%2520vision-language%250Atasks%253A%2520Panoptic%2520Segmentation%252C%2520Depth%2520Estimation%252C%2520Optical%2520Character%2520Recognition%250A%2528OCR%2529%252C%2520Image%2520Captioning%252C%2520and%2520Visual%2520Question%2520Answering%2520%2528VQA%2529.%2520Besides%252C%2520we%250Apropose%2520a%2520novel%2520AT%2520model%2520%2528%2540Model%2529%2520that%2520addresses%2520all%2520tasks%2520simultaneously%2520and%250Acan%2520be%2520expanded%2520to%2520more%2520assistive%2520functions%2520for%2520helping%2520PVIs.%2520Our%2520framework%250Aexhibits%2520outstanding%2520performance%2520across%2520tasks%2520by%2520integrating%2520multi-modal%250Ainformation%252C%2520and%2520it%2520offers%2520PVIs%2520a%2520more%2520comprehensive%2520assistance.%2520Extensive%250Aexperiments%2520prove%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520our%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14215v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%40Bench%3A%20Benchmarking%20Vision-Language%20Models%20for%20Human-centered%20Assistive%0A%20%20Technology&entry.906535625=Xin%20Jiang%20and%20Junwei%20Zheng%20and%20Ruiping%20Liu%20and%20Jiahang%20Li%20and%20Jiaming%20Zhang%20and%20Sven%20Matthiesen%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20As%20Vision-Language%20Models%20%28VLMs%29%20advance%2C%20human-centered%20Assistive%0ATechnologies%20%28ATs%29%20for%20helping%20People%20with%20Visual%20Impairments%20%28PVIs%29%20are%0Aevolving%20into%20generalists%2C%20capable%20of%20performing%20multiple%20tasks%20simultaneously.%0AHowever%2C%20benchmarking%20VLMs%20for%20ATs%20remains%20under-explored.%20To%20bridge%20this%20gap%2C%0Awe%20first%20create%20a%20novel%20AT%20benchmark%20%28%40Bench%29.%20Guided%20by%20a%20pre-design%20user%0Astudy%20with%20PVIs%2C%20our%20benchmark%20includes%20the%20five%20most%20crucial%20vision-language%0Atasks%3A%20Panoptic%20Segmentation%2C%20Depth%20Estimation%2C%20Optical%20Character%20Recognition%0A%28OCR%29%2C%20Image%20Captioning%2C%20and%20Visual%20Question%20Answering%20%28VQA%29.%20Besides%2C%20we%0Apropose%20a%20novel%20AT%20model%20%28%40Model%29%20that%20addresses%20all%20tasks%20simultaneously%20and%0Acan%20be%20expanded%20to%20more%20assistive%20functions%20for%20helping%20PVIs.%20Our%20framework%0Aexhibits%20outstanding%20performance%20across%20tasks%20by%20integrating%20multi-modal%0Ainformation%2C%20and%20it%20offers%20PVIs%20a%20more%20comprehensive%20assistance.%20Extensive%0Aexperiments%20prove%20the%20effectiveness%20and%20generalizability%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14215v2&entry.124074799=Read"},
{"title": "Autoassociative Learning of Structural Representations for Modeling and\n  Classification in Medical Imaging", "author": "Zuzanna Buchnajzer and Kacper Dobek and Stanis\u0142aw Hapke and Daniel Jankowski and Krzysztof Krawiec", "abstract": "  Deep learning architectures based on convolutional neural networks tend to\nrely on continuous, smooth features. While this characteristics provides\nsignificant robustness and proves useful in many real-world tasks, it is\nstrikingly incompatible with the physical characteristic of the world, which,\nat the scale in which humans operate, comprises crisp objects, typically\nrepresenting well-defined categories. This study proposes a class of\nneurosymbolic systems that learn by reconstructing the observed images in terms\nof visual primitives and are thus forced to form high-level, structural\nexplanations of them. When applied to the task of diagnosing abnormalities in\nhistological imaging, the method proved superior to a conventional deep\nlearning architecture in terms of classification accuracy, while being more\ntransparent.\n", "link": "http://arxiv.org/abs/2411.12070v2", "date": "2024-11-25", "relevancy": 2.7642, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5967}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5373}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoassociative%20Learning%20of%20Structural%20Representations%20for%20Modeling%20and%0A%20%20Classification%20in%20Medical%20Imaging&body=Title%3A%20Autoassociative%20Learning%20of%20Structural%20Representations%20for%20Modeling%20and%0A%20%20Classification%20in%20Medical%20Imaging%0AAuthor%3A%20Zuzanna%20Buchnajzer%20and%20Kacper%20Dobek%20and%20Stanis%C5%82aw%20Hapke%20and%20Daniel%20Jankowski%20and%20Krzysztof%20Krawiec%0AAbstract%3A%20%20%20Deep%20learning%20architectures%20based%20on%20convolutional%20neural%20networks%20tend%20to%0Arely%20on%20continuous%2C%20smooth%20features.%20While%20this%20characteristics%20provides%0Asignificant%20robustness%20and%20proves%20useful%20in%20many%20real-world%20tasks%2C%20it%20is%0Astrikingly%20incompatible%20with%20the%20physical%20characteristic%20of%20the%20world%2C%20which%2C%0Aat%20the%20scale%20in%20which%20humans%20operate%2C%20comprises%20crisp%20objects%2C%20typically%0Arepresenting%20well-defined%20categories.%20This%20study%20proposes%20a%20class%20of%0Aneurosymbolic%20systems%20that%20learn%20by%20reconstructing%20the%20observed%20images%20in%20terms%0Aof%20visual%20primitives%20and%20are%20thus%20forced%20to%20form%20high-level%2C%20structural%0Aexplanations%20of%20them.%20When%20applied%20to%20the%20task%20of%20diagnosing%20abnormalities%20in%0Ahistological%20imaging%2C%20the%20method%20proved%20superior%20to%20a%20conventional%20deep%0Alearning%20architecture%20in%20terms%20of%20classification%20accuracy%2C%20while%20being%20more%0Atransparent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12070v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoassociative%2520Learning%2520of%2520Structural%2520Representations%2520for%2520Modeling%2520and%250A%2520%2520Classification%2520in%2520Medical%2520Imaging%26entry.906535625%3DZuzanna%2520Buchnajzer%2520and%2520Kacper%2520Dobek%2520and%2520Stanis%25C5%2582aw%2520Hapke%2520and%2520Daniel%2520Jankowski%2520and%2520Krzysztof%2520Krawiec%26entry.1292438233%3D%2520%2520Deep%2520learning%2520architectures%2520based%2520on%2520convolutional%2520neural%2520networks%2520tend%2520to%250Arely%2520on%2520continuous%252C%2520smooth%2520features.%2520While%2520this%2520characteristics%2520provides%250Asignificant%2520robustness%2520and%2520proves%2520useful%2520in%2520many%2520real-world%2520tasks%252C%2520it%2520is%250Astrikingly%2520incompatible%2520with%2520the%2520physical%2520characteristic%2520of%2520the%2520world%252C%2520which%252C%250Aat%2520the%2520scale%2520in%2520which%2520humans%2520operate%252C%2520comprises%2520crisp%2520objects%252C%2520typically%250Arepresenting%2520well-defined%2520categories.%2520This%2520study%2520proposes%2520a%2520class%2520of%250Aneurosymbolic%2520systems%2520that%2520learn%2520by%2520reconstructing%2520the%2520observed%2520images%2520in%2520terms%250Aof%2520visual%2520primitives%2520and%2520are%2520thus%2520forced%2520to%2520form%2520high-level%252C%2520structural%250Aexplanations%2520of%2520them.%2520When%2520applied%2520to%2520the%2520task%2520of%2520diagnosing%2520abnormalities%2520in%250Ahistological%2520imaging%252C%2520the%2520method%2520proved%2520superior%2520to%2520a%2520conventional%2520deep%250Alearning%2520architecture%2520in%2520terms%2520of%2520classification%2520accuracy%252C%2520while%2520being%2520more%250Atransparent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12070v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoassociative%20Learning%20of%20Structural%20Representations%20for%20Modeling%20and%0A%20%20Classification%20in%20Medical%20Imaging&entry.906535625=Zuzanna%20Buchnajzer%20and%20Kacper%20Dobek%20and%20Stanis%C5%82aw%20Hapke%20and%20Daniel%20Jankowski%20and%20Krzysztof%20Krawiec&entry.1292438233=%20%20Deep%20learning%20architectures%20based%20on%20convolutional%20neural%20networks%20tend%20to%0Arely%20on%20continuous%2C%20smooth%20features.%20While%20this%20characteristics%20provides%0Asignificant%20robustness%20and%20proves%20useful%20in%20many%20real-world%20tasks%2C%20it%20is%0Astrikingly%20incompatible%20with%20the%20physical%20characteristic%20of%20the%20world%2C%20which%2C%0Aat%20the%20scale%20in%20which%20humans%20operate%2C%20comprises%20crisp%20objects%2C%20typically%0Arepresenting%20well-defined%20categories.%20This%20study%20proposes%20a%20class%20of%0Aneurosymbolic%20systems%20that%20learn%20by%20reconstructing%20the%20observed%20images%20in%20terms%0Aof%20visual%20primitives%20and%20are%20thus%20forced%20to%20form%20high-level%2C%20structural%0Aexplanations%20of%20them.%20When%20applied%20to%20the%20task%20of%20diagnosing%20abnormalities%20in%0Ahistological%20imaging%2C%20the%20method%20proved%20superior%20to%20a%20conventional%20deep%0Alearning%20architecture%20in%20terms%20of%20classification%20accuracy%2C%20while%20being%20more%0Atransparent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12070v2&entry.124074799=Read"},
{"title": "Human-Activity AGV Quality Assessment: A Benchmark Dataset and an\n  Objective Evaluation Metric", "author": "Zhichao Zhang and Wei Sun and Xinyue Li and Yunhao Li and Qihang Ge and Jun Jia and Zicheng Zhang and Zhongpeng Ji and Fengyu Sun and Shangling Jui and Xiongkuo Min and Guangtao Zhai", "abstract": "  AI-driven video generation techniques have made significant progress in\nrecent years. However, AI-generated videos (AGVs) involving human activities\noften exhibit substantial visual and semantic distortions, hindering the\npractical application of video generation technologies in real-world scenarios.\nTo address this challenge, we conduct a pioneering study on human activity AGV\nquality assessment, focusing on visual quality evaluation and the\nidentification of semantic distortions. First, we construct the AI-Generated\nHuman activity Video Quality Assessment (Human-AGVQA) dataset, consisting of\n3,200 AGVs derived from 8 popular text-to-video (T2V) models using 400 text\nprompts that describe diverse human activities. We conduct a subjective study\nto evaluate the human appearance quality, action continuity quality, and\noverall video quality of AGVs, and identify semantic issues of human body\nparts. Based on Human-AGVQA, we benchmark the performance of T2V models and\nanalyze their strengths and weaknesses in generating different categories of\nhuman activities. Second, we develop an objective evaluation metric, named\nAI-Generated Human activity Video Quality metric (GHVQ), to automatically\nanalyze the quality of human activity AGVs. GHVQ systematically extracts\nhuman-focused quality features, AI-generated content-aware quality features,\nand temporal continuity features, making it a comprehensive and explainable\nquality metric for human activity AGVs. The extensive experimental results show\nthat GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a\nlarge margin, demonstrating its efficacy in assessing the quality of human\nactivity AGVs. The Human-AGVQA dataset and GHVQ metric will be released in\npublic at https://github.com/zczhang-sjtu/GHVQ.git\n", "link": "http://arxiv.org/abs/2411.16619v1", "date": "2024-11-25", "relevancy": 2.7556, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5604}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.55}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Activity%20AGV%20Quality%20Assessment%3A%20A%20Benchmark%20Dataset%20and%20an%0A%20%20Objective%20Evaluation%20Metric&body=Title%3A%20Human-Activity%20AGV%20Quality%20Assessment%3A%20A%20Benchmark%20Dataset%20and%20an%0A%20%20Objective%20Evaluation%20Metric%0AAuthor%3A%20Zhichao%20Zhang%20and%20Wei%20Sun%20and%20Xinyue%20Li%20and%20Yunhao%20Li%20and%20Qihang%20Ge%20and%20Jun%20Jia%20and%20Zicheng%20Zhang%20and%20Zhongpeng%20Ji%20and%20Fengyu%20Sun%20and%20Shangling%20Jui%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20AI-driven%20video%20generation%20techniques%20have%20made%20significant%20progress%20in%0Arecent%20years.%20However%2C%20AI-generated%20videos%20%28AGVs%29%20involving%20human%20activities%0Aoften%20exhibit%20substantial%20visual%20and%20semantic%20distortions%2C%20hindering%20the%0Apractical%20application%20of%20video%20generation%20technologies%20in%20real-world%20scenarios.%0ATo%20address%20this%20challenge%2C%20we%20conduct%20a%20pioneering%20study%20on%20human%20activity%20AGV%0Aquality%20assessment%2C%20focusing%20on%20visual%20quality%20evaluation%20and%20the%0Aidentification%20of%20semantic%20distortions.%20First%2C%20we%20construct%20the%20AI-Generated%0AHuman%20activity%20Video%20Quality%20Assessment%20%28Human-AGVQA%29%20dataset%2C%20consisting%20of%0A3%2C200%20AGVs%20derived%20from%208%20popular%20text-to-video%20%28T2V%29%20models%20using%20400%20text%0Aprompts%20that%20describe%20diverse%20human%20activities.%20We%20conduct%20a%20subjective%20study%0Ato%20evaluate%20the%20human%20appearance%20quality%2C%20action%20continuity%20quality%2C%20and%0Aoverall%20video%20quality%20of%20AGVs%2C%20and%20identify%20semantic%20issues%20of%20human%20body%0Aparts.%20Based%20on%20Human-AGVQA%2C%20we%20benchmark%20the%20performance%20of%20T2V%20models%20and%0Aanalyze%20their%20strengths%20and%20weaknesses%20in%20generating%20different%20categories%20of%0Ahuman%20activities.%20Second%2C%20we%20develop%20an%20objective%20evaluation%20metric%2C%20named%0AAI-Generated%20Human%20activity%20Video%20Quality%20metric%20%28GHVQ%29%2C%20to%20automatically%0Aanalyze%20the%20quality%20of%20human%20activity%20AGVs.%20GHVQ%20systematically%20extracts%0Ahuman-focused%20quality%20features%2C%20AI-generated%20content-aware%20quality%20features%2C%0Aand%20temporal%20continuity%20features%2C%20making%20it%20a%20comprehensive%20and%20explainable%0Aquality%20metric%20for%20human%20activity%20AGVs.%20The%20extensive%20experimental%20results%20show%0Athat%20GHVQ%20outperforms%20existing%20quality%20metrics%20on%20the%20Human-AGVQA%20dataset%20by%20a%0Alarge%20margin%2C%20demonstrating%20its%20efficacy%20in%20assessing%20the%20quality%20of%20human%0Aactivity%20AGVs.%20The%20Human-AGVQA%20dataset%20and%20GHVQ%20metric%20will%20be%20released%20in%0Apublic%20at%20https%3A//github.com/zczhang-sjtu/GHVQ.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Activity%2520AGV%2520Quality%2520Assessment%253A%2520A%2520Benchmark%2520Dataset%2520and%2520an%250A%2520%2520Objective%2520Evaluation%2520Metric%26entry.906535625%3DZhichao%2520Zhang%2520and%2520Wei%2520Sun%2520and%2520Xinyue%2520Li%2520and%2520Yunhao%2520Li%2520and%2520Qihang%2520Ge%2520and%2520Jun%2520Jia%2520and%2520Zicheng%2520Zhang%2520and%2520Zhongpeng%2520Ji%2520and%2520Fengyu%2520Sun%2520and%2520Shangling%2520Jui%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520AI-driven%2520video%2520generation%2520techniques%2520have%2520made%2520significant%2520progress%2520in%250Arecent%2520years.%2520However%252C%2520AI-generated%2520videos%2520%2528AGVs%2529%2520involving%2520human%2520activities%250Aoften%2520exhibit%2520substantial%2520visual%2520and%2520semantic%2520distortions%252C%2520hindering%2520the%250Apractical%2520application%2520of%2520video%2520generation%2520technologies%2520in%2520real-world%2520scenarios.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520conduct%2520a%2520pioneering%2520study%2520on%2520human%2520activity%2520AGV%250Aquality%2520assessment%252C%2520focusing%2520on%2520visual%2520quality%2520evaluation%2520and%2520the%250Aidentification%2520of%2520semantic%2520distortions.%2520First%252C%2520we%2520construct%2520the%2520AI-Generated%250AHuman%2520activity%2520Video%2520Quality%2520Assessment%2520%2528Human-AGVQA%2529%2520dataset%252C%2520consisting%2520of%250A3%252C200%2520AGVs%2520derived%2520from%25208%2520popular%2520text-to-video%2520%2528T2V%2529%2520models%2520using%2520400%2520text%250Aprompts%2520that%2520describe%2520diverse%2520human%2520activities.%2520We%2520conduct%2520a%2520subjective%2520study%250Ato%2520evaluate%2520the%2520human%2520appearance%2520quality%252C%2520action%2520continuity%2520quality%252C%2520and%250Aoverall%2520video%2520quality%2520of%2520AGVs%252C%2520and%2520identify%2520semantic%2520issues%2520of%2520human%2520body%250Aparts.%2520Based%2520on%2520Human-AGVQA%252C%2520we%2520benchmark%2520the%2520performance%2520of%2520T2V%2520models%2520and%250Aanalyze%2520their%2520strengths%2520and%2520weaknesses%2520in%2520generating%2520different%2520categories%2520of%250Ahuman%2520activities.%2520Second%252C%2520we%2520develop%2520an%2520objective%2520evaluation%2520metric%252C%2520named%250AAI-Generated%2520Human%2520activity%2520Video%2520Quality%2520metric%2520%2528GHVQ%2529%252C%2520to%2520automatically%250Aanalyze%2520the%2520quality%2520of%2520human%2520activity%2520AGVs.%2520GHVQ%2520systematically%2520extracts%250Ahuman-focused%2520quality%2520features%252C%2520AI-generated%2520content-aware%2520quality%2520features%252C%250Aand%2520temporal%2520continuity%2520features%252C%2520making%2520it%2520a%2520comprehensive%2520and%2520explainable%250Aquality%2520metric%2520for%2520human%2520activity%2520AGVs.%2520The%2520extensive%2520experimental%2520results%2520show%250Athat%2520GHVQ%2520outperforms%2520existing%2520quality%2520metrics%2520on%2520the%2520Human-AGVQA%2520dataset%2520by%2520a%250Alarge%2520margin%252C%2520demonstrating%2520its%2520efficacy%2520in%2520assessing%2520the%2520quality%2520of%2520human%250Aactivity%2520AGVs.%2520The%2520Human-AGVQA%2520dataset%2520and%2520GHVQ%2520metric%2520will%2520be%2520released%2520in%250Apublic%2520at%2520https%253A//github.com/zczhang-sjtu/GHVQ.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Activity%20AGV%20Quality%20Assessment%3A%20A%20Benchmark%20Dataset%20and%20an%0A%20%20Objective%20Evaluation%20Metric&entry.906535625=Zhichao%20Zhang%20and%20Wei%20Sun%20and%20Xinyue%20Li%20and%20Yunhao%20Li%20and%20Qihang%20Ge%20and%20Jun%20Jia%20and%20Zicheng%20Zhang%20and%20Zhongpeng%20Ji%20and%20Fengyu%20Sun%20and%20Shangling%20Jui%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20AI-driven%20video%20generation%20techniques%20have%20made%20significant%20progress%20in%0Arecent%20years.%20However%2C%20AI-generated%20videos%20%28AGVs%29%20involving%20human%20activities%0Aoften%20exhibit%20substantial%20visual%20and%20semantic%20distortions%2C%20hindering%20the%0Apractical%20application%20of%20video%20generation%20technologies%20in%20real-world%20scenarios.%0ATo%20address%20this%20challenge%2C%20we%20conduct%20a%20pioneering%20study%20on%20human%20activity%20AGV%0Aquality%20assessment%2C%20focusing%20on%20visual%20quality%20evaluation%20and%20the%0Aidentification%20of%20semantic%20distortions.%20First%2C%20we%20construct%20the%20AI-Generated%0AHuman%20activity%20Video%20Quality%20Assessment%20%28Human-AGVQA%29%20dataset%2C%20consisting%20of%0A3%2C200%20AGVs%20derived%20from%208%20popular%20text-to-video%20%28T2V%29%20models%20using%20400%20text%0Aprompts%20that%20describe%20diverse%20human%20activities.%20We%20conduct%20a%20subjective%20study%0Ato%20evaluate%20the%20human%20appearance%20quality%2C%20action%20continuity%20quality%2C%20and%0Aoverall%20video%20quality%20of%20AGVs%2C%20and%20identify%20semantic%20issues%20of%20human%20body%0Aparts.%20Based%20on%20Human-AGVQA%2C%20we%20benchmark%20the%20performance%20of%20T2V%20models%20and%0Aanalyze%20their%20strengths%20and%20weaknesses%20in%20generating%20different%20categories%20of%0Ahuman%20activities.%20Second%2C%20we%20develop%20an%20objective%20evaluation%20metric%2C%20named%0AAI-Generated%20Human%20activity%20Video%20Quality%20metric%20%28GHVQ%29%2C%20to%20automatically%0Aanalyze%20the%20quality%20of%20human%20activity%20AGVs.%20GHVQ%20systematically%20extracts%0Ahuman-focused%20quality%20features%2C%20AI-generated%20content-aware%20quality%20features%2C%0Aand%20temporal%20continuity%20features%2C%20making%20it%20a%20comprehensive%20and%20explainable%0Aquality%20metric%20for%20human%20activity%20AGVs.%20The%20extensive%20experimental%20results%20show%0Athat%20GHVQ%20outperforms%20existing%20quality%20metrics%20on%20the%20Human-AGVQA%20dataset%20by%20a%0Alarge%20margin%2C%20demonstrating%20its%20efficacy%20in%20assessing%20the%20quality%20of%20human%0Aactivity%20AGVs.%20The%20Human-AGVQA%20dataset%20and%20GHVQ%20metric%20will%20be%20released%20in%0Apublic%20at%20https%3A//github.com/zczhang-sjtu/GHVQ.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16619v1&entry.124074799=Read"},
{"title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting\n  Synthesis", "author": "Hyojun Go and Byeongjun Park and Jiho Jang and Jin-Young Kim and Soonwoo Kwon and Changick Kim", "abstract": "  Text-based generation and editing of 3D scenes hold significant potential for\nstreamlining content creation through intuitive user interactions. While recent\nadvances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time\nrendering, existing methods are often specialized and task-focused, lacking a\nunified framework for both generation and editing. In this paper, we introduce\nSplatFlow, a comprehensive framework that addresses this gap by enabling direct\n3DGS generation and editing. SplatFlow comprises two main components: a\nmulti-view rectified flow (RF) model and a Gaussian Splatting Decoder\n(GSDecoder). The multi-view RF model operates in latent space, generating\nmulti-view images, depths, and camera poses simultaneously, conditioned on text\nprompts, thus addressing challenges like diverse scene scales and complex\ncamera trajectories in real-world settings. Then, the GSDecoder efficiently\ntranslates these latent outputs into 3DGS representations through a\nfeed-forward 3DGS method. Leveraging training-free inversion and inpainting\ntechniques, SplatFlow enables seamless 3DGS editing and supports a broad range\nof 3D tasks-including object editing, novel view synthesis, and camera pose\nestimation-within a unified framework without requiring additional complex\npipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K\ndatasets, demonstrating its versatility and effectiveness in various 3D\ngeneration, editing, and inpainting-based tasks.\n", "link": "http://arxiv.org/abs/2411.16443v1", "date": "2024-11-25", "relevancy": 2.7333, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.693}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6826}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SplatFlow%3A%20Multi-View%20Rectified%20Flow%20Model%20for%203D%20Gaussian%20Splatting%0A%20%20Synthesis&body=Title%3A%20SplatFlow%3A%20Multi-View%20Rectified%20Flow%20Model%20for%203D%20Gaussian%20Splatting%0A%20%20Synthesis%0AAuthor%3A%20Hyojun%20Go%20and%20Byeongjun%20Park%20and%20Jiho%20Jang%20and%20Jin-Young%20Kim%20and%20Soonwoo%20Kwon%20and%20Changick%20Kim%0AAbstract%3A%20%20%20Text-based%20generation%20and%20editing%20of%203D%20scenes%20hold%20significant%20potential%20for%0Astreamlining%20content%20creation%20through%20intuitive%20user%20interactions.%20While%20recent%0Aadvances%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20for%20high-fidelity%20and%20real-time%0Arendering%2C%20existing%20methods%20are%20often%20specialized%20and%20task-focused%2C%20lacking%20a%0Aunified%20framework%20for%20both%20generation%20and%20editing.%20In%20this%20paper%2C%20we%20introduce%0ASplatFlow%2C%20a%20comprehensive%20framework%20that%20addresses%20this%20gap%20by%20enabling%20direct%0A3DGS%20generation%20and%20editing.%20SplatFlow%20comprises%20two%20main%20components%3A%20a%0Amulti-view%20rectified%20flow%20%28RF%29%20model%20and%20a%20Gaussian%20Splatting%20Decoder%0A%28GSDecoder%29.%20The%20multi-view%20RF%20model%20operates%20in%20latent%20space%2C%20generating%0Amulti-view%20images%2C%20depths%2C%20and%20camera%20poses%20simultaneously%2C%20conditioned%20on%20text%0Aprompts%2C%20thus%20addressing%20challenges%20like%20diverse%20scene%20scales%20and%20complex%0Acamera%20trajectories%20in%20real-world%20settings.%20Then%2C%20the%20GSDecoder%20efficiently%0Atranslates%20these%20latent%20outputs%20into%203DGS%20representations%20through%20a%0Afeed-forward%203DGS%20method.%20Leveraging%20training-free%20inversion%20and%20inpainting%0Atechniques%2C%20SplatFlow%20enables%20seamless%203DGS%20editing%20and%20supports%20a%20broad%20range%0Aof%203D%20tasks-including%20object%20editing%2C%20novel%20view%20synthesis%2C%20and%20camera%20pose%0Aestimation-within%20a%20unified%20framework%20without%20requiring%20additional%20complex%0Apipelines.%20We%20validate%20SplatFlow%27s%20capabilities%20on%20the%20MVImgNet%20and%20DL3DV-7K%0Adatasets%2C%20demonstrating%20its%20versatility%20and%20effectiveness%20in%20various%203D%0Ageneration%2C%20editing%2C%20and%20inpainting-based%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatFlow%253A%2520Multi-View%2520Rectified%2520Flow%2520Model%2520for%25203D%2520Gaussian%2520Splatting%250A%2520%2520Synthesis%26entry.906535625%3DHyojun%2520Go%2520and%2520Byeongjun%2520Park%2520and%2520Jiho%2520Jang%2520and%2520Jin-Young%2520Kim%2520and%2520Soonwoo%2520Kwon%2520and%2520Changick%2520Kim%26entry.1292438233%3D%2520%2520Text-based%2520generation%2520and%2520editing%2520of%25203D%2520scenes%2520hold%2520significant%2520potential%2520for%250Astreamlining%2520content%2520creation%2520through%2520intuitive%2520user%2520interactions.%2520While%2520recent%250Aadvances%2520leverage%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520for%2520high-fidelity%2520and%2520real-time%250Arendering%252C%2520existing%2520methods%2520are%2520often%2520specialized%2520and%2520task-focused%252C%2520lacking%2520a%250Aunified%2520framework%2520for%2520both%2520generation%2520and%2520editing.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ASplatFlow%252C%2520a%2520comprehensive%2520framework%2520that%2520addresses%2520this%2520gap%2520by%2520enabling%2520direct%250A3DGS%2520generation%2520and%2520editing.%2520SplatFlow%2520comprises%2520two%2520main%2520components%253A%2520a%250Amulti-view%2520rectified%2520flow%2520%2528RF%2529%2520model%2520and%2520a%2520Gaussian%2520Splatting%2520Decoder%250A%2528GSDecoder%2529.%2520The%2520multi-view%2520RF%2520model%2520operates%2520in%2520latent%2520space%252C%2520generating%250Amulti-view%2520images%252C%2520depths%252C%2520and%2520camera%2520poses%2520simultaneously%252C%2520conditioned%2520on%2520text%250Aprompts%252C%2520thus%2520addressing%2520challenges%2520like%2520diverse%2520scene%2520scales%2520and%2520complex%250Acamera%2520trajectories%2520in%2520real-world%2520settings.%2520Then%252C%2520the%2520GSDecoder%2520efficiently%250Atranslates%2520these%2520latent%2520outputs%2520into%25203DGS%2520representations%2520through%2520a%250Afeed-forward%25203DGS%2520method.%2520Leveraging%2520training-free%2520inversion%2520and%2520inpainting%250Atechniques%252C%2520SplatFlow%2520enables%2520seamless%25203DGS%2520editing%2520and%2520supports%2520a%2520broad%2520range%250Aof%25203D%2520tasks-including%2520object%2520editing%252C%2520novel%2520view%2520synthesis%252C%2520and%2520camera%2520pose%250Aestimation-within%2520a%2520unified%2520framework%2520without%2520requiring%2520additional%2520complex%250Apipelines.%2520We%2520validate%2520SplatFlow%2527s%2520capabilities%2520on%2520the%2520MVImgNet%2520and%2520DL3DV-7K%250Adatasets%252C%2520demonstrating%2520its%2520versatility%2520and%2520effectiveness%2520in%2520various%25203D%250Ageneration%252C%2520editing%252C%2520and%2520inpainting-based%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplatFlow%3A%20Multi-View%20Rectified%20Flow%20Model%20for%203D%20Gaussian%20Splatting%0A%20%20Synthesis&entry.906535625=Hyojun%20Go%20and%20Byeongjun%20Park%20and%20Jiho%20Jang%20and%20Jin-Young%20Kim%20and%20Soonwoo%20Kwon%20and%20Changick%20Kim&entry.1292438233=%20%20Text-based%20generation%20and%20editing%20of%203D%20scenes%20hold%20significant%20potential%20for%0Astreamlining%20content%20creation%20through%20intuitive%20user%20interactions.%20While%20recent%0Aadvances%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20for%20high-fidelity%20and%20real-time%0Arendering%2C%20existing%20methods%20are%20often%20specialized%20and%20task-focused%2C%20lacking%20a%0Aunified%20framework%20for%20both%20generation%20and%20editing.%20In%20this%20paper%2C%20we%20introduce%0ASplatFlow%2C%20a%20comprehensive%20framework%20that%20addresses%20this%20gap%20by%20enabling%20direct%0A3DGS%20generation%20and%20editing.%20SplatFlow%20comprises%20two%20main%20components%3A%20a%0Amulti-view%20rectified%20flow%20%28RF%29%20model%20and%20a%20Gaussian%20Splatting%20Decoder%0A%28GSDecoder%29.%20The%20multi-view%20RF%20model%20operates%20in%20latent%20space%2C%20generating%0Amulti-view%20images%2C%20depths%2C%20and%20camera%20poses%20simultaneously%2C%20conditioned%20on%20text%0Aprompts%2C%20thus%20addressing%20challenges%20like%20diverse%20scene%20scales%20and%20complex%0Acamera%20trajectories%20in%20real-world%20settings.%20Then%2C%20the%20GSDecoder%20efficiently%0Atranslates%20these%20latent%20outputs%20into%203DGS%20representations%20through%20a%0Afeed-forward%203DGS%20method.%20Leveraging%20training-free%20inversion%20and%20inpainting%0Atechniques%2C%20SplatFlow%20enables%20seamless%203DGS%20editing%20and%20supports%20a%20broad%20range%0Aof%203D%20tasks-including%20object%20editing%2C%20novel%20view%20synthesis%2C%20and%20camera%20pose%0Aestimation-within%20a%20unified%20framework%20without%20requiring%20additional%20complex%0Apipelines.%20We%20validate%20SplatFlow%27s%20capabilities%20on%20the%20MVImgNet%20and%20DL3DV-7K%0Adatasets%2C%20demonstrating%20its%20versatility%20and%20effectiveness%20in%20various%203D%0Ageneration%2C%20editing%2C%20and%20inpainting-based%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16443v1&entry.124074799=Read"},
{"title": "Brain-like emergent properties in deep networks: impact of network\n  architecture, datasets and training", "author": "Niranjan Rajesh and Georgin Jacob and SP Arun", "abstract": "  Despite the rapid pace at which deep networks are improving on standardized\nvision benchmarks, they are still outperformed by humans on real-world vision\ntasks. This paradoxical lack of generalization could be addressed by making\ndeep networks more brain-like. Although several benchmarks have compared the\nability of deep networks to predict brain responses to natural images, they do\nnot capture subtle but important brain-like emergent properties. To resolve\nthis issue, we report several well-known perceptual and neural emergent\nproperties that can be tested on deep networks. To evaluate how various design\nfactors impact brain-like properties, we systematically evaluated over 30\nstate-of-the-art networks with varying network architectures, training datasets\nand training regimes. Our main findings are as follows. First, network\narchitecture had the strongest impact on brain-like properties compared to\ndataset and training regime variations. Second, networks varied widely in their\nalignment to the brain with no single network outperforming all others. Taken\ntogether, our results complement existing benchmarks by revealing brain-like\nproperties that are either emergent or lacking in state-of-the-art deep\nnetworks.\n", "link": "http://arxiv.org/abs/2411.16326v1", "date": "2024-11-25", "relevancy": 2.7316, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-like%20emergent%20properties%20in%20deep%20networks%3A%20impact%20of%20network%0A%20%20architecture%2C%20datasets%20and%20training&body=Title%3A%20Brain-like%20emergent%20properties%20in%20deep%20networks%3A%20impact%20of%20network%0A%20%20architecture%2C%20datasets%20and%20training%0AAuthor%3A%20Niranjan%20Rajesh%20and%20Georgin%20Jacob%20and%20SP%20Arun%0AAbstract%3A%20%20%20Despite%20the%20rapid%20pace%20at%20which%20deep%20networks%20are%20improving%20on%20standardized%0Avision%20benchmarks%2C%20they%20are%20still%20outperformed%20by%20humans%20on%20real-world%20vision%0Atasks.%20This%20paradoxical%20lack%20of%20generalization%20could%20be%20addressed%20by%20making%0Adeep%20networks%20more%20brain-like.%20Although%20several%20benchmarks%20have%20compared%20the%0Aability%20of%20deep%20networks%20to%20predict%20brain%20responses%20to%20natural%20images%2C%20they%20do%0Anot%20capture%20subtle%20but%20important%20brain-like%20emergent%20properties.%20To%20resolve%0Athis%20issue%2C%20we%20report%20several%20well-known%20perceptual%20and%20neural%20emergent%0Aproperties%20that%20can%20be%20tested%20on%20deep%20networks.%20To%20evaluate%20how%20various%20design%0Afactors%20impact%20brain-like%20properties%2C%20we%20systematically%20evaluated%20over%2030%0Astate-of-the-art%20networks%20with%20varying%20network%20architectures%2C%20training%20datasets%0Aand%20training%20regimes.%20Our%20main%20findings%20are%20as%20follows.%20First%2C%20network%0Aarchitecture%20had%20the%20strongest%20impact%20on%20brain-like%20properties%20compared%20to%0Adataset%20and%20training%20regime%20variations.%20Second%2C%20networks%20varied%20widely%20in%20their%0Aalignment%20to%20the%20brain%20with%20no%20single%20network%20outperforming%20all%20others.%20Taken%0Atogether%2C%20our%20results%20complement%20existing%20benchmarks%20by%20revealing%20brain-like%0Aproperties%20that%20are%20either%20emergent%20or%20lacking%20in%20state-of-the-art%20deep%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-like%2520emergent%2520properties%2520in%2520deep%2520networks%253A%2520impact%2520of%2520network%250A%2520%2520architecture%252C%2520datasets%2520and%2520training%26entry.906535625%3DNiranjan%2520Rajesh%2520and%2520Georgin%2520Jacob%2520and%2520SP%2520Arun%26entry.1292438233%3D%2520%2520Despite%2520the%2520rapid%2520pace%2520at%2520which%2520deep%2520networks%2520are%2520improving%2520on%2520standardized%250Avision%2520benchmarks%252C%2520they%2520are%2520still%2520outperformed%2520by%2520humans%2520on%2520real-world%2520vision%250Atasks.%2520This%2520paradoxical%2520lack%2520of%2520generalization%2520could%2520be%2520addressed%2520by%2520making%250Adeep%2520networks%2520more%2520brain-like.%2520Although%2520several%2520benchmarks%2520have%2520compared%2520the%250Aability%2520of%2520deep%2520networks%2520to%2520predict%2520brain%2520responses%2520to%2520natural%2520images%252C%2520they%2520do%250Anot%2520capture%2520subtle%2520but%2520important%2520brain-like%2520emergent%2520properties.%2520To%2520resolve%250Athis%2520issue%252C%2520we%2520report%2520several%2520well-known%2520perceptual%2520and%2520neural%2520emergent%250Aproperties%2520that%2520can%2520be%2520tested%2520on%2520deep%2520networks.%2520To%2520evaluate%2520how%2520various%2520design%250Afactors%2520impact%2520brain-like%2520properties%252C%2520we%2520systematically%2520evaluated%2520over%252030%250Astate-of-the-art%2520networks%2520with%2520varying%2520network%2520architectures%252C%2520training%2520datasets%250Aand%2520training%2520regimes.%2520Our%2520main%2520findings%2520are%2520as%2520follows.%2520First%252C%2520network%250Aarchitecture%2520had%2520the%2520strongest%2520impact%2520on%2520brain-like%2520properties%2520compared%2520to%250Adataset%2520and%2520training%2520regime%2520variations.%2520Second%252C%2520networks%2520varied%2520widely%2520in%2520their%250Aalignment%2520to%2520the%2520brain%2520with%2520no%2520single%2520network%2520outperforming%2520all%2520others.%2520Taken%250Atogether%252C%2520our%2520results%2520complement%2520existing%2520benchmarks%2520by%2520revealing%2520brain-like%250Aproperties%2520that%2520are%2520either%2520emergent%2520or%2520lacking%2520in%2520state-of-the-art%2520deep%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-like%20emergent%20properties%20in%20deep%20networks%3A%20impact%20of%20network%0A%20%20architecture%2C%20datasets%20and%20training&entry.906535625=Niranjan%20Rajesh%20and%20Georgin%20Jacob%20and%20SP%20Arun&entry.1292438233=%20%20Despite%20the%20rapid%20pace%20at%20which%20deep%20networks%20are%20improving%20on%20standardized%0Avision%20benchmarks%2C%20they%20are%20still%20outperformed%20by%20humans%20on%20real-world%20vision%0Atasks.%20This%20paradoxical%20lack%20of%20generalization%20could%20be%20addressed%20by%20making%0Adeep%20networks%20more%20brain-like.%20Although%20several%20benchmarks%20have%20compared%20the%0Aability%20of%20deep%20networks%20to%20predict%20brain%20responses%20to%20natural%20images%2C%20they%20do%0Anot%20capture%20subtle%20but%20important%20brain-like%20emergent%20properties.%20To%20resolve%0Athis%20issue%2C%20we%20report%20several%20well-known%20perceptual%20and%20neural%20emergent%0Aproperties%20that%20can%20be%20tested%20on%20deep%20networks.%20To%20evaluate%20how%20various%20design%0Afactors%20impact%20brain-like%20properties%2C%20we%20systematically%20evaluated%20over%2030%0Astate-of-the-art%20networks%20with%20varying%20network%20architectures%2C%20training%20datasets%0Aand%20training%20regimes.%20Our%20main%20findings%20are%20as%20follows.%20First%2C%20network%0Aarchitecture%20had%20the%20strongest%20impact%20on%20brain-like%20properties%20compared%20to%0Adataset%20and%20training%20regime%20variations.%20Second%2C%20networks%20varied%20widely%20in%20their%0Aalignment%20to%20the%20brain%20with%20no%20single%20network%20outperforming%20all%20others.%20Taken%0Atogether%2C%20our%20results%20complement%20existing%20benchmarks%20by%20revealing%20brain-like%0Aproperties%20that%20are%20either%20emergent%20or%20lacking%20in%20state-of-the-art%20deep%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16326v1&entry.124074799=Read"},
{"title": "CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features", "author": "Po-han Li and Sandeep P. Chinchali and Ufuk Topcu", "abstract": "  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs\nand $6\\times$ fewer unimodal data for ImageNet classification and\nmisinformative news captions detection. CSA surpasses the state-of-the-art\nmethod to map unimodal features to multimodal features. We also demonstrate the\nability of CSA with modalities beyond image and text, paving the way for future\nmodality pairs with limited paired multimodal data but abundant unpaired\nunimodal data, such as lidar and text.\n", "link": "http://arxiv.org/abs/2410.07610v2", "date": "2024-11-25", "relevancy": 2.7292, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5964}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSA%3A%20Data-efficient%20Mapping%20of%20Unimodal%20Features%20to%20Multimodal%20Features&body=Title%3A%20CSA%3A%20Data-efficient%20Mapping%20of%20Unimodal%20Features%20to%20Multimodal%20Features%0AAuthor%3A%20Po-han%20Li%20and%20Sandeep%20P.%20Chinchali%20and%20Ufuk%20Topcu%0AAbstract%3A%20%20%20Multimodal%20encoders%20like%20CLIP%20excel%20in%20tasks%20such%20as%20zero-shot%20image%0Aclassification%20and%20cross-modal%20retrieval.%20However%2C%20they%20require%20excessive%0Atraining%20data.%20We%20propose%20canonical%20similarity%20analysis%20%28CSA%29%2C%20which%20uses%20two%0Aunimodal%20encoders%20to%20replicate%20multimodal%20encoders%20using%20limited%20data.%20CSA%20maps%0Aunimodal%20features%20into%20a%20multimodal%20space%2C%20using%20a%20new%20similarity%20score%20to%0Aretain%20only%20the%20multimodal%20information.%20CSA%20only%20involves%20the%20inference%20of%0Aunimodal%20encoders%20and%20a%20cubic-complexity%20matrix%20decomposition%2C%20eliminating%20the%0Aneed%20for%20extensive%20GPU-based%20model%20training.%20Experiments%20show%20that%20CSA%0Aoutperforms%20CLIP%20while%20requiring%20%24300%2C000%5Ctimes%24%20fewer%20multimodal%20data%20pairs%0Aand%20%246%5Ctimes%24%20fewer%20unimodal%20data%20for%20ImageNet%20classification%20and%0Amisinformative%20news%20captions%20detection.%20CSA%20surpasses%20the%20state-of-the-art%0Amethod%20to%20map%20unimodal%20features%20to%20multimodal%20features.%20We%20also%20demonstrate%20the%0Aability%20of%20CSA%20with%20modalities%20beyond%20image%20and%20text%2C%20paving%20the%20way%20for%20future%0Amodality%20pairs%20with%20limited%20paired%20multimodal%20data%20but%20abundant%20unpaired%0Aunimodal%20data%2C%20such%20as%20lidar%20and%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07610v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSA%253A%2520Data-efficient%2520Mapping%2520of%2520Unimodal%2520Features%2520to%2520Multimodal%2520Features%26entry.906535625%3DPo-han%2520Li%2520and%2520Sandeep%2520P.%2520Chinchali%2520and%2520Ufuk%2520Topcu%26entry.1292438233%3D%2520%2520Multimodal%2520encoders%2520like%2520CLIP%2520excel%2520in%2520tasks%2520such%2520as%2520zero-shot%2520image%250Aclassification%2520and%2520cross-modal%2520retrieval.%2520However%252C%2520they%2520require%2520excessive%250Atraining%2520data.%2520We%2520propose%2520canonical%2520similarity%2520analysis%2520%2528CSA%2529%252C%2520which%2520uses%2520two%250Aunimodal%2520encoders%2520to%2520replicate%2520multimodal%2520encoders%2520using%2520limited%2520data.%2520CSA%2520maps%250Aunimodal%2520features%2520into%2520a%2520multimodal%2520space%252C%2520using%2520a%2520new%2520similarity%2520score%2520to%250Aretain%2520only%2520the%2520multimodal%2520information.%2520CSA%2520only%2520involves%2520the%2520inference%2520of%250Aunimodal%2520encoders%2520and%2520a%2520cubic-complexity%2520matrix%2520decomposition%252C%2520eliminating%2520the%250Aneed%2520for%2520extensive%2520GPU-based%2520model%2520training.%2520Experiments%2520show%2520that%2520CSA%250Aoutperforms%2520CLIP%2520while%2520requiring%2520%2524300%252C000%255Ctimes%2524%2520fewer%2520multimodal%2520data%2520pairs%250Aand%2520%25246%255Ctimes%2524%2520fewer%2520unimodal%2520data%2520for%2520ImageNet%2520classification%2520and%250Amisinformative%2520news%2520captions%2520detection.%2520CSA%2520surpasses%2520the%2520state-of-the-art%250Amethod%2520to%2520map%2520unimodal%2520features%2520to%2520multimodal%2520features.%2520We%2520also%2520demonstrate%2520the%250Aability%2520of%2520CSA%2520with%2520modalities%2520beyond%2520image%2520and%2520text%252C%2520paving%2520the%2520way%2520for%2520future%250Amodality%2520pairs%2520with%2520limited%2520paired%2520multimodal%2520data%2520but%2520abundant%2520unpaired%250Aunimodal%2520data%252C%2520such%2520as%2520lidar%2520and%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07610v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSA%3A%20Data-efficient%20Mapping%20of%20Unimodal%20Features%20to%20Multimodal%20Features&entry.906535625=Po-han%20Li%20and%20Sandeep%20P.%20Chinchali%20and%20Ufuk%20Topcu&entry.1292438233=%20%20Multimodal%20encoders%20like%20CLIP%20excel%20in%20tasks%20such%20as%20zero-shot%20image%0Aclassification%20and%20cross-modal%20retrieval.%20However%2C%20they%20require%20excessive%0Atraining%20data.%20We%20propose%20canonical%20similarity%20analysis%20%28CSA%29%2C%20which%20uses%20two%0Aunimodal%20encoders%20to%20replicate%20multimodal%20encoders%20using%20limited%20data.%20CSA%20maps%0Aunimodal%20features%20into%20a%20multimodal%20space%2C%20using%20a%20new%20similarity%20score%20to%0Aretain%20only%20the%20multimodal%20information.%20CSA%20only%20involves%20the%20inference%20of%0Aunimodal%20encoders%20and%20a%20cubic-complexity%20matrix%20decomposition%2C%20eliminating%20the%0Aneed%20for%20extensive%20GPU-based%20model%20training.%20Experiments%20show%20that%20CSA%0Aoutperforms%20CLIP%20while%20requiring%20%24300%2C000%5Ctimes%24%20fewer%20multimodal%20data%20pairs%0Aand%20%246%5Ctimes%24%20fewer%20unimodal%20data%20for%20ImageNet%20classification%20and%0Amisinformative%20news%20captions%20detection.%20CSA%20surpasses%20the%20state-of-the-art%0Amethod%20to%20map%20unimodal%20features%20to%20multimodal%20features.%20We%20also%20demonstrate%20the%0Aability%20of%20CSA%20with%20modalities%20beyond%20image%20and%20text%2C%20paving%20the%20way%20for%20future%0Amodality%20pairs%20with%20limited%20paired%20multimodal%20data%20but%20abundant%20unpaired%0Aunimodal%20data%2C%20such%20as%20lidar%20and%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07610v2&entry.124074799=Read"},
{"title": "Text-guided Image Restoration and Semantic Enhancement for Text-to-Image\n  Person Retrieval", "author": "Delong Liu and Haiwen Li and Zhicheng Zhao and Yuan Dong and Nikolaos V. Boulgouris", "abstract": "  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific\nperson images according to the given textual descriptions. A primary challenge\nin this task is bridging the substantial representational gap between visual\nand textual modalities. The prevailing methods map texts and images into\nunified embedding space for matching, while the intricate semantic\ncorrespondences between texts and images are still not effectively constructed.\nTo address this issue, we propose a novel TIPR framework to build fine-grained\ninteractions and alignment between person images and the corresponding texts.\nSpecifically, via fine-tuning the Contrastive Language-Image Pre-training\n(CLIP) model, a visual-textual dual encoder is firstly constructed, to\npreliminarily align the image and text features. Secondly, a Text-guided Image\nRestoration (TIR) auxiliary task is proposed to map abstract textual entities\nto specific image regions, improving the alignment between local textual and\nvisual embeddings. Additionally, a cross-modal triplet loss is presented to\nhandle hard samples, and further enhance the model's discriminability for minor\ndifferences. Moreover, a pruning-based text data augmentation approach is\nproposed to enhance focus on essential elements in descriptions, thereby\navoiding excessive model attention to less significant information. The\nexperimental results show our proposed method outperforms state-of-the-art\nmethods on three popular benchmark datasets, and the code will be made publicly\navailable at https://github.com/Delong-liu-bupt/SEN.\n", "link": "http://arxiv.org/abs/2307.09059v2", "date": "2024-11-25", "relevancy": 2.7208, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5848}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5285}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-guided%20Image%20Restoration%20and%20Semantic%20Enhancement%20for%20Text-to-Image%0A%20%20Person%20Retrieval&body=Title%3A%20Text-guided%20Image%20Restoration%20and%20Semantic%20Enhancement%20for%20Text-to-Image%0A%20%20Person%20Retrieval%0AAuthor%3A%20Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhicheng%20Zhao%20and%20Yuan%20Dong%20and%20Nikolaos%20V.%20Boulgouris%0AAbstract%3A%20%20%20The%20goal%20of%20Text-to-Image%20Person%20Retrieval%20%28TIPR%29%20is%20to%20retrieve%20specific%0Aperson%20images%20according%20to%20the%20given%20textual%20descriptions.%20A%20primary%20challenge%0Ain%20this%20task%20is%20bridging%20the%20substantial%20representational%20gap%20between%20visual%0Aand%20textual%20modalities.%20The%20prevailing%20methods%20map%20texts%20and%20images%20into%0Aunified%20embedding%20space%20for%20matching%2C%20while%20the%20intricate%20semantic%0Acorrespondences%20between%20texts%20and%20images%20are%20still%20not%20effectively%20constructed.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20TIPR%20framework%20to%20build%20fine-grained%0Ainteractions%20and%20alignment%20between%20person%20images%20and%20the%20corresponding%20texts.%0ASpecifically%2C%20via%20fine-tuning%20the%20Contrastive%20Language-Image%20Pre-training%0A%28CLIP%29%20model%2C%20a%20visual-textual%20dual%20encoder%20is%20firstly%20constructed%2C%20to%0Apreliminarily%20align%20the%20image%20and%20text%20features.%20Secondly%2C%20a%20Text-guided%20Image%0ARestoration%20%28TIR%29%20auxiliary%20task%20is%20proposed%20to%20map%20abstract%20textual%20entities%0Ato%20specific%20image%20regions%2C%20improving%20the%20alignment%20between%20local%20textual%20and%0Avisual%20embeddings.%20Additionally%2C%20a%20cross-modal%20triplet%20loss%20is%20presented%20to%0Ahandle%20hard%20samples%2C%20and%20further%20enhance%20the%20model%27s%20discriminability%20for%20minor%0Adifferences.%20Moreover%2C%20a%20pruning-based%20text%20data%20augmentation%20approach%20is%0Aproposed%20to%20enhance%20focus%20on%20essential%20elements%20in%20descriptions%2C%20thereby%0Aavoiding%20excessive%20model%20attention%20to%20less%20significant%20information.%20The%0Aexperimental%20results%20show%20our%20proposed%20method%20outperforms%20state-of-the-art%0Amethods%20on%20three%20popular%20benchmark%20datasets%2C%20and%20the%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/Delong-liu-bupt/SEN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-guided%2520Image%2520Restoration%2520and%2520Semantic%2520Enhancement%2520for%2520Text-to-Image%250A%2520%2520Person%2520Retrieval%26entry.906535625%3DDelong%2520Liu%2520and%2520Haiwen%2520Li%2520and%2520Zhicheng%2520Zhao%2520and%2520Yuan%2520Dong%2520and%2520Nikolaos%2520V.%2520Boulgouris%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520Text-to-Image%2520Person%2520Retrieval%2520%2528TIPR%2529%2520is%2520to%2520retrieve%2520specific%250Aperson%2520images%2520according%2520to%2520the%2520given%2520textual%2520descriptions.%2520A%2520primary%2520challenge%250Ain%2520this%2520task%2520is%2520bridging%2520the%2520substantial%2520representational%2520gap%2520between%2520visual%250Aand%2520textual%2520modalities.%2520The%2520prevailing%2520methods%2520map%2520texts%2520and%2520images%2520into%250Aunified%2520embedding%2520space%2520for%2520matching%252C%2520while%2520the%2520intricate%2520semantic%250Acorrespondences%2520between%2520texts%2520and%2520images%2520are%2520still%2520not%2520effectively%2520constructed.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520TIPR%2520framework%2520to%2520build%2520fine-grained%250Ainteractions%2520and%2520alignment%2520between%2520person%2520images%2520and%2520the%2520corresponding%2520texts.%250ASpecifically%252C%2520via%2520fine-tuning%2520the%2520Contrastive%2520Language-Image%2520Pre-training%250A%2528CLIP%2529%2520model%252C%2520a%2520visual-textual%2520dual%2520encoder%2520is%2520firstly%2520constructed%252C%2520to%250Apreliminarily%2520align%2520the%2520image%2520and%2520text%2520features.%2520Secondly%252C%2520a%2520Text-guided%2520Image%250ARestoration%2520%2528TIR%2529%2520auxiliary%2520task%2520is%2520proposed%2520to%2520map%2520abstract%2520textual%2520entities%250Ato%2520specific%2520image%2520regions%252C%2520improving%2520the%2520alignment%2520between%2520local%2520textual%2520and%250Avisual%2520embeddings.%2520Additionally%252C%2520a%2520cross-modal%2520triplet%2520loss%2520is%2520presented%2520to%250Ahandle%2520hard%2520samples%252C%2520and%2520further%2520enhance%2520the%2520model%2527s%2520discriminability%2520for%2520minor%250Adifferences.%2520Moreover%252C%2520a%2520pruning-based%2520text%2520data%2520augmentation%2520approach%2520is%250Aproposed%2520to%2520enhance%2520focus%2520on%2520essential%2520elements%2520in%2520descriptions%252C%2520thereby%250Aavoiding%2520excessive%2520model%2520attention%2520to%2520less%2520significant%2520information.%2520The%250Aexperimental%2520results%2520show%2520our%2520proposed%2520method%2520outperforms%2520state-of-the-art%250Amethods%2520on%2520three%2520popular%2520benchmark%2520datasets%252C%2520and%2520the%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%2520https%253A//github.com/Delong-liu-bupt/SEN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-guided%20Image%20Restoration%20and%20Semantic%20Enhancement%20for%20Text-to-Image%0A%20%20Person%20Retrieval&entry.906535625=Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhicheng%20Zhao%20and%20Yuan%20Dong%20and%20Nikolaos%20V.%20Boulgouris&entry.1292438233=%20%20The%20goal%20of%20Text-to-Image%20Person%20Retrieval%20%28TIPR%29%20is%20to%20retrieve%20specific%0Aperson%20images%20according%20to%20the%20given%20textual%20descriptions.%20A%20primary%20challenge%0Ain%20this%20task%20is%20bridging%20the%20substantial%20representational%20gap%20between%20visual%0Aand%20textual%20modalities.%20The%20prevailing%20methods%20map%20texts%20and%20images%20into%0Aunified%20embedding%20space%20for%20matching%2C%20while%20the%20intricate%20semantic%0Acorrespondences%20between%20texts%20and%20images%20are%20still%20not%20effectively%20constructed.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20TIPR%20framework%20to%20build%20fine-grained%0Ainteractions%20and%20alignment%20between%20person%20images%20and%20the%20corresponding%20texts.%0ASpecifically%2C%20via%20fine-tuning%20the%20Contrastive%20Language-Image%20Pre-training%0A%28CLIP%29%20model%2C%20a%20visual-textual%20dual%20encoder%20is%20firstly%20constructed%2C%20to%0Apreliminarily%20align%20the%20image%20and%20text%20features.%20Secondly%2C%20a%20Text-guided%20Image%0ARestoration%20%28TIR%29%20auxiliary%20task%20is%20proposed%20to%20map%20abstract%20textual%20entities%0Ato%20specific%20image%20regions%2C%20improving%20the%20alignment%20between%20local%20textual%20and%0Avisual%20embeddings.%20Additionally%2C%20a%20cross-modal%20triplet%20loss%20is%20presented%20to%0Ahandle%20hard%20samples%2C%20and%20further%20enhance%20the%20model%27s%20discriminability%20for%20minor%0Adifferences.%20Moreover%2C%20a%20pruning-based%20text%20data%20augmentation%20approach%20is%0Aproposed%20to%20enhance%20focus%20on%20essential%20elements%20in%20descriptions%2C%20thereby%0Aavoiding%20excessive%20model%20attention%20to%20less%20significant%20information.%20The%0Aexperimental%20results%20show%20our%20proposed%20method%20outperforms%20state-of-the-art%0Amethods%20on%20three%20popular%20benchmark%20datasets%2C%20and%20the%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/Delong-liu-bupt/SEN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09059v2&entry.124074799=Read"},
{"title": "Open-Vocabulary Octree-Graph for 3D Scene Understanding", "author": "Zhigang Wang and Yifei Su and Chenhui Li and Dong Wang and Yan Huang and Bin Zhao and Xuelong Li", "abstract": "  Open-vocabulary 3D scene understanding is indispensable for embodied agents.\nRecent works leverage pretrained vision-language models (VLMs) for object\nsegmentation and project them to point clouds to build 3D maps. Despite\nprogress, a point cloud is a set of unordered coordinates that requires\nsubstantial storage space and does not directly convey occupancy information or\nspatial relation, making existing methods inefficient for downstream tasks,\ne.g., path planning and complex text-based object retrieval. To address these\nissues, we propose Octree-Graph, a novel scene representation for\nopen-vocabulary 3D scene understanding. Specifically, a Chronological\nGroup-wise Segment Merging (CGSM) strategy and an Instance Feature Aggregation\n(IFA) algorithm are first designed to get 3D instances and corresponding\nsemantic features. Subsequently, an adaptive-octree structure is developed that\nstores semantics and depicts the occupancy of an object adjustably according to\nits shape. Finally, the Octree-Graph is constructed where each adaptive-octree\nacts as a graph node, and edges describe the spatial relations among nodes.\nExtensive experiments on various tasks are conducted on several widely-used\ndatasets, demonstrating the versatility and effectiveness of our method.\n", "link": "http://arxiv.org/abs/2411.16253v1", "date": "2024-11-25", "relevancy": 2.6989, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6866}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%20Octree-Graph%20for%203D%20Scene%20Understanding&body=Title%3A%20Open-Vocabulary%20Octree-Graph%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Zhigang%20Wang%20and%20Yifei%20Su%20and%20Chenhui%20Li%20and%20Dong%20Wang%20and%20Yan%20Huang%20and%20Bin%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Open-vocabulary%203D%20scene%20understanding%20is%20indispensable%20for%20embodied%20agents.%0ARecent%20works%20leverage%20pretrained%20vision-language%20models%20%28VLMs%29%20for%20object%0Asegmentation%20and%20project%20them%20to%20point%20clouds%20to%20build%203D%20maps.%20Despite%0Aprogress%2C%20a%20point%20cloud%20is%20a%20set%20of%20unordered%20coordinates%20that%20requires%0Asubstantial%20storage%20space%20and%20does%20not%20directly%20convey%20occupancy%20information%20or%0Aspatial%20relation%2C%20making%20existing%20methods%20inefficient%20for%20downstream%20tasks%2C%0Ae.g.%2C%20path%20planning%20and%20complex%20text-based%20object%20retrieval.%20To%20address%20these%0Aissues%2C%20we%20propose%20Octree-Graph%2C%20a%20novel%20scene%20representation%20for%0Aopen-vocabulary%203D%20scene%20understanding.%20Specifically%2C%20a%20Chronological%0AGroup-wise%20Segment%20Merging%20%28CGSM%29%20strategy%20and%20an%20Instance%20Feature%20Aggregation%0A%28IFA%29%20algorithm%20are%20first%20designed%20to%20get%203D%20instances%20and%20corresponding%0Asemantic%20features.%20Subsequently%2C%20an%20adaptive-octree%20structure%20is%20developed%20that%0Astores%20semantics%20and%20depicts%20the%20occupancy%20of%20an%20object%20adjustably%20according%20to%0Aits%20shape.%20Finally%2C%20the%20Octree-Graph%20is%20constructed%20where%20each%20adaptive-octree%0Aacts%20as%20a%20graph%20node%2C%20and%20edges%20describe%20the%20spatial%20relations%20among%20nodes.%0AExtensive%20experiments%20on%20various%20tasks%20are%20conducted%20on%20several%20widely-used%0Adatasets%2C%20demonstrating%20the%20versatility%20and%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%2520Octree-Graph%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DZhigang%2520Wang%2520and%2520Yifei%2520Su%2520and%2520Chenhui%2520Li%2520and%2520Dong%2520Wang%2520and%2520Yan%2520Huang%2520and%2520Bin%2520Zhao%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520scene%2520understanding%2520is%2520indispensable%2520for%2520embodied%2520agents.%250ARecent%2520works%2520leverage%2520pretrained%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520object%250Asegmentation%2520and%2520project%2520them%2520to%2520point%2520clouds%2520to%2520build%25203D%2520maps.%2520Despite%250Aprogress%252C%2520a%2520point%2520cloud%2520is%2520a%2520set%2520of%2520unordered%2520coordinates%2520that%2520requires%250Asubstantial%2520storage%2520space%2520and%2520does%2520not%2520directly%2520convey%2520occupancy%2520information%2520or%250Aspatial%2520relation%252C%2520making%2520existing%2520methods%2520inefficient%2520for%2520downstream%2520tasks%252C%250Ae.g.%252C%2520path%2520planning%2520and%2520complex%2520text-based%2520object%2520retrieval.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520Octree-Graph%252C%2520a%2520novel%2520scene%2520representation%2520for%250Aopen-vocabulary%25203D%2520scene%2520understanding.%2520Specifically%252C%2520a%2520Chronological%250AGroup-wise%2520Segment%2520Merging%2520%2528CGSM%2529%2520strategy%2520and%2520an%2520Instance%2520Feature%2520Aggregation%250A%2528IFA%2529%2520algorithm%2520are%2520first%2520designed%2520to%2520get%25203D%2520instances%2520and%2520corresponding%250Asemantic%2520features.%2520Subsequently%252C%2520an%2520adaptive-octree%2520structure%2520is%2520developed%2520that%250Astores%2520semantics%2520and%2520depicts%2520the%2520occupancy%2520of%2520an%2520object%2520adjustably%2520according%2520to%250Aits%2520shape.%2520Finally%252C%2520the%2520Octree-Graph%2520is%2520constructed%2520where%2520each%2520adaptive-octree%250Aacts%2520as%2520a%2520graph%2520node%252C%2520and%2520edges%2520describe%2520the%2520spatial%2520relations%2520among%2520nodes.%250AExtensive%2520experiments%2520on%2520various%2520tasks%2520are%2520conducted%2520on%2520several%2520widely-used%250Adatasets%252C%2520demonstrating%2520the%2520versatility%2520and%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%20Octree-Graph%20for%203D%20Scene%20Understanding&entry.906535625=Zhigang%20Wang%20and%20Yifei%20Su%20and%20Chenhui%20Li%20and%20Dong%20Wang%20and%20Yan%20Huang%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=%20%20Open-vocabulary%203D%20scene%20understanding%20is%20indispensable%20for%20embodied%20agents.%0ARecent%20works%20leverage%20pretrained%20vision-language%20models%20%28VLMs%29%20for%20object%0Asegmentation%20and%20project%20them%20to%20point%20clouds%20to%20build%203D%20maps.%20Despite%0Aprogress%2C%20a%20point%20cloud%20is%20a%20set%20of%20unordered%20coordinates%20that%20requires%0Asubstantial%20storage%20space%20and%20does%20not%20directly%20convey%20occupancy%20information%20or%0Aspatial%20relation%2C%20making%20existing%20methods%20inefficient%20for%20downstream%20tasks%2C%0Ae.g.%2C%20path%20planning%20and%20complex%20text-based%20object%20retrieval.%20To%20address%20these%0Aissues%2C%20we%20propose%20Octree-Graph%2C%20a%20novel%20scene%20representation%20for%0Aopen-vocabulary%203D%20scene%20understanding.%20Specifically%2C%20a%20Chronological%0AGroup-wise%20Segment%20Merging%20%28CGSM%29%20strategy%20and%20an%20Instance%20Feature%20Aggregation%0A%28IFA%29%20algorithm%20are%20first%20designed%20to%20get%203D%20instances%20and%20corresponding%0Asemantic%20features.%20Subsequently%2C%20an%20adaptive-octree%20structure%20is%20developed%20that%0Astores%20semantics%20and%20depicts%20the%20occupancy%20of%20an%20object%20adjustably%20according%20to%0Aits%20shape.%20Finally%2C%20the%20Octree-Graph%20is%20constructed%20where%20each%20adaptive-octree%0Aacts%20as%20a%20graph%20node%2C%20and%20edges%20describe%20the%20spatial%20relations%20among%20nodes.%0AExtensive%20experiments%20on%20various%20tasks%20are%20conducted%20on%20several%20widely-used%0Adatasets%2C%20demonstrating%20the%20versatility%20and%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16253v1&entry.124074799=Read"},
{"title": "Functionality understanding and segmentation in 3D scenes", "author": "Jaime Corsetti and Francesco Giuliari and Alice Fasoli and Davide Boscaini and Fabio Poiesi", "abstract": "  Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Code will be\nreleased publicly.\n", "link": "http://arxiv.org/abs/2411.16310v1", "date": "2024-11-25", "relevancy": 2.6682, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6847}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Functionality%20understanding%20and%20segmentation%20in%203D%20scenes&body=Title%3A%20Functionality%20understanding%20and%20segmentation%20in%203D%20scenes%0AAuthor%3A%20Jaime%20Corsetti%20and%20Francesco%20Giuliari%20and%20Alice%20Fasoli%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20Understanding%20functionalities%20in%203D%20scenes%20involves%20interpreting%20natural%0Alanguage%20descriptions%20to%20locate%20functional%20interactive%20objects%2C%20such%20as%20handles%0Aand%20buttons%2C%20in%20a%203D%20environment.%20Functionality%20understanding%20is%20highly%0Achallenging%2C%20as%20it%20requires%20both%20world%20knowledge%20to%20interpret%20language%20and%0Aspatial%20perception%20to%20identify%20fine-grained%20objects.%20For%20example%2C%20given%20a%20task%0Alike%20%27turn%20on%20the%20ceiling%20light%27%2C%20an%20embodied%20AI%20agent%20must%20infer%20that%20it%20needs%0Ato%20locate%20the%20light%20switch%2C%20even%20though%20the%20switch%20is%20not%20explicitly%20mentioned%0Ain%20the%20task%20description.%20To%20date%2C%20no%20dedicated%20methods%20have%20been%20developed%20for%0Athis%20problem.%20In%20this%20paper%2C%20we%20introduce%20Fun3DU%2C%20the%20first%20approach%20designed%0Afor%20functionality%20understanding%20in%203D%20scenes.%20Fun3DU%20uses%20a%20language%20model%20to%0Aparse%20the%20task%20description%20through%20Chain-of-Thought%20reasoning%20in%20order%20to%0Aidentify%20the%20object%20of%20interest.%20The%20identified%20object%20is%20segmented%20across%0Amultiple%20views%20of%20the%20captured%20scene%20by%20using%20a%20vision%20and%20language%20model.%20The%0Asegmentation%20results%20from%20each%20view%20are%20lifted%20in%203D%20and%20aggregated%20into%20the%0Apoint%20cloud%20using%20geometric%20information.%20Fun3DU%20is%20training-free%2C%20relying%0Aentirely%20on%20pre-trained%20models.%20We%20evaluate%20Fun3DU%20on%20SceneFun3D%2C%20the%20most%0Arecent%20and%20only%20dataset%20to%20benchmark%20this%20task%2C%20which%20comprises%20over%203000%20task%0Adescriptions%20on%20230%20scenes.%20Our%20method%20significantly%20outperforms%0Astate-of-the-art%20open-vocabulary%203D%20segmentation%20approaches.%20Code%20will%20be%0Areleased%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctionality%2520understanding%2520and%2520segmentation%2520in%25203D%2520scenes%26entry.906535625%3DJaime%2520Corsetti%2520and%2520Francesco%2520Giuliari%2520and%2520Alice%2520Fasoli%2520and%2520Davide%2520Boscaini%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3D%2520%2520Understanding%2520functionalities%2520in%25203D%2520scenes%2520involves%2520interpreting%2520natural%250Alanguage%2520descriptions%2520to%2520locate%2520functional%2520interactive%2520objects%252C%2520such%2520as%2520handles%250Aand%2520buttons%252C%2520in%2520a%25203D%2520environment.%2520Functionality%2520understanding%2520is%2520highly%250Achallenging%252C%2520as%2520it%2520requires%2520both%2520world%2520knowledge%2520to%2520interpret%2520language%2520and%250Aspatial%2520perception%2520to%2520identify%2520fine-grained%2520objects.%2520For%2520example%252C%2520given%2520a%2520task%250Alike%2520%2527turn%2520on%2520the%2520ceiling%2520light%2527%252C%2520an%2520embodied%2520AI%2520agent%2520must%2520infer%2520that%2520it%2520needs%250Ato%2520locate%2520the%2520light%2520switch%252C%2520even%2520though%2520the%2520switch%2520is%2520not%2520explicitly%2520mentioned%250Ain%2520the%2520task%2520description.%2520To%2520date%252C%2520no%2520dedicated%2520methods%2520have%2520been%2520developed%2520for%250Athis%2520problem.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Fun3DU%252C%2520the%2520first%2520approach%2520designed%250Afor%2520functionality%2520understanding%2520in%25203D%2520scenes.%2520Fun3DU%2520uses%2520a%2520language%2520model%2520to%250Aparse%2520the%2520task%2520description%2520through%2520Chain-of-Thought%2520reasoning%2520in%2520order%2520to%250Aidentify%2520the%2520object%2520of%2520interest.%2520The%2520identified%2520object%2520is%2520segmented%2520across%250Amultiple%2520views%2520of%2520the%2520captured%2520scene%2520by%2520using%2520a%2520vision%2520and%2520language%2520model.%2520The%250Asegmentation%2520results%2520from%2520each%2520view%2520are%2520lifted%2520in%25203D%2520and%2520aggregated%2520into%2520the%250Apoint%2520cloud%2520using%2520geometric%2520information.%2520Fun3DU%2520is%2520training-free%252C%2520relying%250Aentirely%2520on%2520pre-trained%2520models.%2520We%2520evaluate%2520Fun3DU%2520on%2520SceneFun3D%252C%2520the%2520most%250Arecent%2520and%2520only%2520dataset%2520to%2520benchmark%2520this%2520task%252C%2520which%2520comprises%2520over%25203000%2520task%250Adescriptions%2520on%2520230%2520scenes.%2520Our%2520method%2520significantly%2520outperforms%250Astate-of-the-art%2520open-vocabulary%25203D%2520segmentation%2520approaches.%2520Code%2520will%2520be%250Areleased%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functionality%20understanding%20and%20segmentation%20in%203D%20scenes&entry.906535625=Jaime%20Corsetti%20and%20Francesco%20Giuliari%20and%20Alice%20Fasoli%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi&entry.1292438233=%20%20Understanding%20functionalities%20in%203D%20scenes%20involves%20interpreting%20natural%0Alanguage%20descriptions%20to%20locate%20functional%20interactive%20objects%2C%20such%20as%20handles%0Aand%20buttons%2C%20in%20a%203D%20environment.%20Functionality%20understanding%20is%20highly%0Achallenging%2C%20as%20it%20requires%20both%20world%20knowledge%20to%20interpret%20language%20and%0Aspatial%20perception%20to%20identify%20fine-grained%20objects.%20For%20example%2C%20given%20a%20task%0Alike%20%27turn%20on%20the%20ceiling%20light%27%2C%20an%20embodied%20AI%20agent%20must%20infer%20that%20it%20needs%0Ato%20locate%20the%20light%20switch%2C%20even%20though%20the%20switch%20is%20not%20explicitly%20mentioned%0Ain%20the%20task%20description.%20To%20date%2C%20no%20dedicated%20methods%20have%20been%20developed%20for%0Athis%20problem.%20In%20this%20paper%2C%20we%20introduce%20Fun3DU%2C%20the%20first%20approach%20designed%0Afor%20functionality%20understanding%20in%203D%20scenes.%20Fun3DU%20uses%20a%20language%20model%20to%0Aparse%20the%20task%20description%20through%20Chain-of-Thought%20reasoning%20in%20order%20to%0Aidentify%20the%20object%20of%20interest.%20The%20identified%20object%20is%20segmented%20across%0Amultiple%20views%20of%20the%20captured%20scene%20by%20using%20a%20vision%20and%20language%20model.%20The%0Asegmentation%20results%20from%20each%20view%20are%20lifted%20in%203D%20and%20aggregated%20into%20the%0Apoint%20cloud%20using%20geometric%20information.%20Fun3DU%20is%20training-free%2C%20relying%0Aentirely%20on%20pre-trained%20models.%20We%20evaluate%20Fun3DU%20on%20SceneFun3D%2C%20the%20most%0Arecent%20and%20only%20dataset%20to%20benchmark%20this%20task%2C%20which%20comprises%20over%203000%20task%0Adescriptions%20on%20230%20scenes.%20Our%20method%20significantly%20outperforms%0Astate-of-the-art%20open-vocabulary%203D%20segmentation%20approaches.%20Code%20will%20be%0Areleased%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16310v1&entry.124074799=Read"},
{"title": "Monocular Lane Detection Based on Deep Learning: A Survey", "author": "Xin He and Haiyun Guo and Kuan Zhu and Bingke Zhu and Xu Zhao and Jianwu Fang and Jinqiao Wang", "abstract": "  Lane detection plays an important role in autonomous driving perception\nsystem. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on deep learning have demonstrated superior performance and\nemerged as a key research direction in autonomous driving perception. The core\ndesign of these algorithmic frameworks can be summarized as follows: (1) Task\nparadigm, focusing on lane instance-level discrimination; (2) Lane modeling,\nrepresenting lanes as a set of learnable parameters in the neural network; (3)\nGlobal context supplementation, enhancing the detection of obscured lanes; (4)\nPerspective effect elimination, providing 3D lanes usable for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. For a\nrelatively fair comparison, in addition to comparing the performance of\nmainstream methods on different benchmarks, their inference speed is also\ninvestigated under a unified setting. Moreover, we present some extended works\non lane detection, including multi-task perception, video lane detection,\nonline high-definition (HD) map construction, and lane topology reasoning, to\noffer readers a comprehensive roadmap for the evolution of lane detection.\nFinally, we point out some potential future research directions in this field.\nWe exhaustively collect the papers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.\n", "link": "http://arxiv.org/abs/2411.16316v1", "date": "2024-11-25", "relevancy": 2.6652, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5386}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5376}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey&body=Title%3A%20Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey%0AAuthor%3A%20Xin%20He%20and%20Haiyun%20Guo%20and%20Kuan%20Zhu%20and%20Bingke%20Zhu%20and%20Xu%20Zhao%20and%20Jianwu%20Fang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Lane%20detection%20plays%20an%20important%20role%20in%20autonomous%20driving%20perception%0Asystem.%20As%20deep%20learning%20algorithms%20gain%20popularity%2C%20monocular%20lane%20detection%0Amethods%20based%20on%20deep%20learning%20have%20demonstrated%20superior%20performance%20and%0Aemerged%20as%20a%20key%20research%20direction%20in%20autonomous%20driving%20perception.%20The%20core%0Adesign%20of%20these%20algorithmic%20frameworks%20can%20be%20summarized%20as%20follows%3A%20%281%29%20Task%0Aparadigm%2C%20focusing%20on%20lane%20instance-level%20discrimination%3B%20%282%29%20Lane%20modeling%2C%0Arepresenting%20lanes%20as%20a%20set%20of%20learnable%20parameters%20in%20the%20neural%20network%3B%20%283%29%0AGlobal%20context%20supplementation%2C%20enhancing%20the%20detection%20of%20obscured%20lanes%3B%20%284%29%0APerspective%20effect%20elimination%2C%20providing%203D%20lanes%20usable%20for%20downstream%0Aapplications.%20From%20these%20perspectives%2C%20this%20paper%20presents%20a%20comprehensive%0Aoverview%20of%20existing%20methods%2C%20encompassing%20both%20the%20increasingly%20mature%202D%20lane%0Adetection%20approaches%20and%20the%20developing%203D%20lane%20detection%20works.%20For%20a%0Arelatively%20fair%20comparison%2C%20in%20addition%20to%20comparing%20the%20performance%20of%0Amainstream%20methods%20on%20different%20benchmarks%2C%20their%20inference%20speed%20is%20also%0Ainvestigated%20under%20a%20unified%20setting.%20Moreover%2C%20we%20present%20some%20extended%20works%0Aon%20lane%20detection%2C%20including%20multi-task%20perception%2C%20video%20lane%20detection%2C%0Aonline%20high-definition%20%28HD%29%20map%20construction%2C%20and%20lane%20topology%20reasoning%2C%20to%0Aoffer%20readers%20a%20comprehensive%20roadmap%20for%20the%20evolution%20of%20lane%20detection.%0AFinally%2C%20we%20point%20out%20some%20potential%20future%20research%20directions%20in%20this%20field.%0AWe%20exhaustively%20collect%20the%20papers%20and%20codes%20of%20existing%20works%20at%0Ahttps%3A//github.com/Core9724/Awesome-Lane-Detection%20and%20will%20keep%20tracing%20the%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Lane%2520Detection%2520Based%2520on%2520Deep%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DXin%2520He%2520and%2520Haiyun%2520Guo%2520and%2520Kuan%2520Zhu%2520and%2520Bingke%2520Zhu%2520and%2520Xu%2520Zhao%2520and%2520Jianwu%2520Fang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Lane%2520detection%2520plays%2520an%2520important%2520role%2520in%2520autonomous%2520driving%2520perception%250Asystem.%2520As%2520deep%2520learning%2520algorithms%2520gain%2520popularity%252C%2520monocular%2520lane%2520detection%250Amethods%2520based%2520on%2520deep%2520learning%2520have%2520demonstrated%2520superior%2520performance%2520and%250Aemerged%2520as%2520a%2520key%2520research%2520direction%2520in%2520autonomous%2520driving%2520perception.%2520The%2520core%250Adesign%2520of%2520these%2520algorithmic%2520frameworks%2520can%2520be%2520summarized%2520as%2520follows%253A%2520%25281%2529%2520Task%250Aparadigm%252C%2520focusing%2520on%2520lane%2520instance-level%2520discrimination%253B%2520%25282%2529%2520Lane%2520modeling%252C%250Arepresenting%2520lanes%2520as%2520a%2520set%2520of%2520learnable%2520parameters%2520in%2520the%2520neural%2520network%253B%2520%25283%2529%250AGlobal%2520context%2520supplementation%252C%2520enhancing%2520the%2520detection%2520of%2520obscured%2520lanes%253B%2520%25284%2529%250APerspective%2520effect%2520elimination%252C%2520providing%25203D%2520lanes%2520usable%2520for%2520downstream%250Aapplications.%2520From%2520these%2520perspectives%252C%2520this%2520paper%2520presents%2520a%2520comprehensive%250Aoverview%2520of%2520existing%2520methods%252C%2520encompassing%2520both%2520the%2520increasingly%2520mature%25202D%2520lane%250Adetection%2520approaches%2520and%2520the%2520developing%25203D%2520lane%2520detection%2520works.%2520For%2520a%250Arelatively%2520fair%2520comparison%252C%2520in%2520addition%2520to%2520comparing%2520the%2520performance%2520of%250Amainstream%2520methods%2520on%2520different%2520benchmarks%252C%2520their%2520inference%2520speed%2520is%2520also%250Ainvestigated%2520under%2520a%2520unified%2520setting.%2520Moreover%252C%2520we%2520present%2520some%2520extended%2520works%250Aon%2520lane%2520detection%252C%2520including%2520multi-task%2520perception%252C%2520video%2520lane%2520detection%252C%250Aonline%2520high-definition%2520%2528HD%2529%2520map%2520construction%252C%2520and%2520lane%2520topology%2520reasoning%252C%2520to%250Aoffer%2520readers%2520a%2520comprehensive%2520roadmap%2520for%2520the%2520evolution%2520of%2520lane%2520detection.%250AFinally%252C%2520we%2520point%2520out%2520some%2520potential%2520future%2520research%2520directions%2520in%2520this%2520field.%250AWe%2520exhaustively%2520collect%2520the%2520papers%2520and%2520codes%2520of%2520existing%2520works%2520at%250Ahttps%253A//github.com/Core9724/Awesome-Lane-Detection%2520and%2520will%2520keep%2520tracing%2520the%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey&entry.906535625=Xin%20He%20and%20Haiyun%20Guo%20and%20Kuan%20Zhu%20and%20Bingke%20Zhu%20and%20Xu%20Zhao%20and%20Jianwu%20Fang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Lane%20detection%20plays%20an%20important%20role%20in%20autonomous%20driving%20perception%0Asystem.%20As%20deep%20learning%20algorithms%20gain%20popularity%2C%20monocular%20lane%20detection%0Amethods%20based%20on%20deep%20learning%20have%20demonstrated%20superior%20performance%20and%0Aemerged%20as%20a%20key%20research%20direction%20in%20autonomous%20driving%20perception.%20The%20core%0Adesign%20of%20these%20algorithmic%20frameworks%20can%20be%20summarized%20as%20follows%3A%20%281%29%20Task%0Aparadigm%2C%20focusing%20on%20lane%20instance-level%20discrimination%3B%20%282%29%20Lane%20modeling%2C%0Arepresenting%20lanes%20as%20a%20set%20of%20learnable%20parameters%20in%20the%20neural%20network%3B%20%283%29%0AGlobal%20context%20supplementation%2C%20enhancing%20the%20detection%20of%20obscured%20lanes%3B%20%284%29%0APerspective%20effect%20elimination%2C%20providing%203D%20lanes%20usable%20for%20downstream%0Aapplications.%20From%20these%20perspectives%2C%20this%20paper%20presents%20a%20comprehensive%0Aoverview%20of%20existing%20methods%2C%20encompassing%20both%20the%20increasingly%20mature%202D%20lane%0Adetection%20approaches%20and%20the%20developing%203D%20lane%20detection%20works.%20For%20a%0Arelatively%20fair%20comparison%2C%20in%20addition%20to%20comparing%20the%20performance%20of%0Amainstream%20methods%20on%20different%20benchmarks%2C%20their%20inference%20speed%20is%20also%0Ainvestigated%20under%20a%20unified%20setting.%20Moreover%2C%20we%20present%20some%20extended%20works%0Aon%20lane%20detection%2C%20including%20multi-task%20perception%2C%20video%20lane%20detection%2C%0Aonline%20high-definition%20%28HD%29%20map%20construction%2C%20and%20lane%20topology%20reasoning%2C%20to%0Aoffer%20readers%20a%20comprehensive%20roadmap%20for%20the%20evolution%20of%20lane%20detection.%0AFinally%2C%20we%20point%20out%20some%20potential%20future%20research%20directions%20in%20this%20field.%0AWe%20exhaustively%20collect%20the%20papers%20and%20codes%20of%20existing%20works%20at%0Ahttps%3A//github.com/Core9724/Awesome-Lane-Detection%20and%20will%20keep%20tracing%20the%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16316v1&entry.124074799=Read"},
{"title": "Efficient Video Face Enhancement with Enhanced Spatial-Temporal\n  Consistency", "author": "Yutong Wang and Jiajie Teng and Jiajiong Cao and Yuming Li and Chenguang Ma and Hongteng Xu and Dixin Luo", "abstract": "  As a very common type of video, face videos often appear in movies, talk\nshows, live broadcasts, and other scenes. Real-world online videos are often\nplagued by degradations such as blurring and quantization noise, due to the\nhigh compression ratio caused by high communication costs and limited\ntransmission bandwidth. These degradations have a particularly serious impact\non face videos because the human visual system is highly sensitive to facial\ndetails. Despite the significant advancement in video face enhancement, current\nmethods still suffer from $i)$ long processing time and $ii)$ inconsistent\nspatial-temporal visual effects (e.g., flickering). This study proposes a novel\nand efficient blind video face enhancement method to overcome the above two\nchallenges, restoring high-quality videos from their compressed low-quality\nversions with an effective de-flickering mechanism. In particular, the proposed\nmethod develops upon a 3D-VQGAN backbone associated with spatial-temporal\ncodebooks recording high-quality portrait features and residual-based temporal\ninformation. We develop a two-stage learning framework for the model. In Stage\n\\Rmnum{1}, we learn the model with a regularizer mitigating the codebook\ncollapse problem. In Stage \\Rmnum{2}, we learn two transformers to lookup code\nfrom the codebooks and further update the encoder of low-quality videos.\nExperiments conducted on the VFHQ-Test dataset demonstrate that our method\nsurpasses the current state-of-the-art blind face video restoration and\nde-flickering methods on both efficiency and effectiveness. Code is available\nat \\url{https://github.com/Dixin-Lab/BFVR-STC}.\n", "link": "http://arxiv.org/abs/2411.16468v1", "date": "2024-11-25", "relevancy": 2.6375, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6718}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.657}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Video%20Face%20Enhancement%20with%20Enhanced%20Spatial-Temporal%0A%20%20Consistency&body=Title%3A%20Efficient%20Video%20Face%20Enhancement%20with%20Enhanced%20Spatial-Temporal%0A%20%20Consistency%0AAuthor%3A%20Yutong%20Wang%20and%20Jiajie%20Teng%20and%20Jiajiong%20Cao%20and%20Yuming%20Li%20and%20Chenguang%20Ma%20and%20Hongteng%20Xu%20and%20Dixin%20Luo%0AAbstract%3A%20%20%20As%20a%20very%20common%20type%20of%20video%2C%20face%20videos%20often%20appear%20in%20movies%2C%20talk%0Ashows%2C%20live%20broadcasts%2C%20and%20other%20scenes.%20Real-world%20online%20videos%20are%20often%0Aplagued%20by%20degradations%20such%20as%20blurring%20and%20quantization%20noise%2C%20due%20to%20the%0Ahigh%20compression%20ratio%20caused%20by%20high%20communication%20costs%20and%20limited%0Atransmission%20bandwidth.%20These%20degradations%20have%20a%20particularly%20serious%20impact%0Aon%20face%20videos%20because%20the%20human%20visual%20system%20is%20highly%20sensitive%20to%20facial%0Adetails.%20Despite%20the%20significant%20advancement%20in%20video%20face%20enhancement%2C%20current%0Amethods%20still%20suffer%20from%20%24i%29%24%20long%20processing%20time%20and%20%24ii%29%24%20inconsistent%0Aspatial-temporal%20visual%20effects%20%28e.g.%2C%20flickering%29.%20This%20study%20proposes%20a%20novel%0Aand%20efficient%20blind%20video%20face%20enhancement%20method%20to%20overcome%20the%20above%20two%0Achallenges%2C%20restoring%20high-quality%20videos%20from%20their%20compressed%20low-quality%0Aversions%20with%20an%20effective%20de-flickering%20mechanism.%20In%20particular%2C%20the%20proposed%0Amethod%20develops%20upon%20a%203D-VQGAN%20backbone%20associated%20with%20spatial-temporal%0Acodebooks%20recording%20high-quality%20portrait%20features%20and%20residual-based%20temporal%0Ainformation.%20We%20develop%20a%20two-stage%20learning%20framework%20for%20the%20model.%20In%20Stage%0A%5CRmnum%7B1%7D%2C%20we%20learn%20the%20model%20with%20a%20regularizer%20mitigating%20the%20codebook%0Acollapse%20problem.%20In%20Stage%20%5CRmnum%7B2%7D%2C%20we%20learn%20two%20transformers%20to%20lookup%20code%0Afrom%20the%20codebooks%20and%20further%20update%20the%20encoder%20of%20low-quality%20videos.%0AExperiments%20conducted%20on%20the%20VFHQ-Test%20dataset%20demonstrate%20that%20our%20method%0Asurpasses%20the%20current%20state-of-the-art%20blind%20face%20video%20restoration%20and%0Ade-flickering%20methods%20on%20both%20efficiency%20and%20effectiveness.%20Code%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/Dixin-Lab/BFVR-STC%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Video%2520Face%2520Enhancement%2520with%2520Enhanced%2520Spatial-Temporal%250A%2520%2520Consistency%26entry.906535625%3DYutong%2520Wang%2520and%2520Jiajie%2520Teng%2520and%2520Jiajiong%2520Cao%2520and%2520Yuming%2520Li%2520and%2520Chenguang%2520Ma%2520and%2520Hongteng%2520Xu%2520and%2520Dixin%2520Luo%26entry.1292438233%3D%2520%2520As%2520a%2520very%2520common%2520type%2520of%2520video%252C%2520face%2520videos%2520often%2520appear%2520in%2520movies%252C%2520talk%250Ashows%252C%2520live%2520broadcasts%252C%2520and%2520other%2520scenes.%2520Real-world%2520online%2520videos%2520are%2520often%250Aplagued%2520by%2520degradations%2520such%2520as%2520blurring%2520and%2520quantization%2520noise%252C%2520due%2520to%2520the%250Ahigh%2520compression%2520ratio%2520caused%2520by%2520high%2520communication%2520costs%2520and%2520limited%250Atransmission%2520bandwidth.%2520These%2520degradations%2520have%2520a%2520particularly%2520serious%2520impact%250Aon%2520face%2520videos%2520because%2520the%2520human%2520visual%2520system%2520is%2520highly%2520sensitive%2520to%2520facial%250Adetails.%2520Despite%2520the%2520significant%2520advancement%2520in%2520video%2520face%2520enhancement%252C%2520current%250Amethods%2520still%2520suffer%2520from%2520%2524i%2529%2524%2520long%2520processing%2520time%2520and%2520%2524ii%2529%2524%2520inconsistent%250Aspatial-temporal%2520visual%2520effects%2520%2528e.g.%252C%2520flickering%2529.%2520This%2520study%2520proposes%2520a%2520novel%250Aand%2520efficient%2520blind%2520video%2520face%2520enhancement%2520method%2520to%2520overcome%2520the%2520above%2520two%250Achallenges%252C%2520restoring%2520high-quality%2520videos%2520from%2520their%2520compressed%2520low-quality%250Aversions%2520with%2520an%2520effective%2520de-flickering%2520mechanism.%2520In%2520particular%252C%2520the%2520proposed%250Amethod%2520develops%2520upon%2520a%25203D-VQGAN%2520backbone%2520associated%2520with%2520spatial-temporal%250Acodebooks%2520recording%2520high-quality%2520portrait%2520features%2520and%2520residual-based%2520temporal%250Ainformation.%2520We%2520develop%2520a%2520two-stage%2520learning%2520framework%2520for%2520the%2520model.%2520In%2520Stage%250A%255CRmnum%257B1%257D%252C%2520we%2520learn%2520the%2520model%2520with%2520a%2520regularizer%2520mitigating%2520the%2520codebook%250Acollapse%2520problem.%2520In%2520Stage%2520%255CRmnum%257B2%257D%252C%2520we%2520learn%2520two%2520transformers%2520to%2520lookup%2520code%250Afrom%2520the%2520codebooks%2520and%2520further%2520update%2520the%2520encoder%2520of%2520low-quality%2520videos.%250AExperiments%2520conducted%2520on%2520the%2520VFHQ-Test%2520dataset%2520demonstrate%2520that%2520our%2520method%250Asurpasses%2520the%2520current%2520state-of-the-art%2520blind%2520face%2520video%2520restoration%2520and%250Ade-flickering%2520methods%2520on%2520both%2520efficiency%2520and%2520effectiveness.%2520Code%2520is%2520available%250Aat%2520%255Curl%257Bhttps%253A//github.com/Dixin-Lab/BFVR-STC%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Video%20Face%20Enhancement%20with%20Enhanced%20Spatial-Temporal%0A%20%20Consistency&entry.906535625=Yutong%20Wang%20and%20Jiajie%20Teng%20and%20Jiajiong%20Cao%20and%20Yuming%20Li%20and%20Chenguang%20Ma%20and%20Hongteng%20Xu%20and%20Dixin%20Luo&entry.1292438233=%20%20As%20a%20very%20common%20type%20of%20video%2C%20face%20videos%20often%20appear%20in%20movies%2C%20talk%0Ashows%2C%20live%20broadcasts%2C%20and%20other%20scenes.%20Real-world%20online%20videos%20are%20often%0Aplagued%20by%20degradations%20such%20as%20blurring%20and%20quantization%20noise%2C%20due%20to%20the%0Ahigh%20compression%20ratio%20caused%20by%20high%20communication%20costs%20and%20limited%0Atransmission%20bandwidth.%20These%20degradations%20have%20a%20particularly%20serious%20impact%0Aon%20face%20videos%20because%20the%20human%20visual%20system%20is%20highly%20sensitive%20to%20facial%0Adetails.%20Despite%20the%20significant%20advancement%20in%20video%20face%20enhancement%2C%20current%0Amethods%20still%20suffer%20from%20%24i%29%24%20long%20processing%20time%20and%20%24ii%29%24%20inconsistent%0Aspatial-temporal%20visual%20effects%20%28e.g.%2C%20flickering%29.%20This%20study%20proposes%20a%20novel%0Aand%20efficient%20blind%20video%20face%20enhancement%20method%20to%20overcome%20the%20above%20two%0Achallenges%2C%20restoring%20high-quality%20videos%20from%20their%20compressed%20low-quality%0Aversions%20with%20an%20effective%20de-flickering%20mechanism.%20In%20particular%2C%20the%20proposed%0Amethod%20develops%20upon%20a%203D-VQGAN%20backbone%20associated%20with%20spatial-temporal%0Acodebooks%20recording%20high-quality%20portrait%20features%20and%20residual-based%20temporal%0Ainformation.%20We%20develop%20a%20two-stage%20learning%20framework%20for%20the%20model.%20In%20Stage%0A%5CRmnum%7B1%7D%2C%20we%20learn%20the%20model%20with%20a%20regularizer%20mitigating%20the%20codebook%0Acollapse%20problem.%20In%20Stage%20%5CRmnum%7B2%7D%2C%20we%20learn%20two%20transformers%20to%20lookup%20code%0Afrom%20the%20codebooks%20and%20further%20update%20the%20encoder%20of%20low-quality%20videos.%0AExperiments%20conducted%20on%20the%20VFHQ-Test%20dataset%20demonstrate%20that%20our%20method%0Asurpasses%20the%20current%20state-of-the-art%20blind%20face%20video%20restoration%20and%0Ade-flickering%20methods%20on%20both%20efficiency%20and%20effectiveness.%20Code%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/Dixin-Lab/BFVR-STC%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16468v1&entry.124074799=Read"},
{"title": "J-CaPA : Joint Channel and Pyramid Attention Improves Medical Image\n  Segmentation", "author": "Marzia Binta Nizam and Marian Zlateva and James Davis", "abstract": "  Medical image segmentation is crucial for diagnosis and treatment planning.\nTraditional CNN-based models, like U-Net, have shown promising results but\nstruggle to capture long-range dependencies and global context. To address\nthese limitations, we propose a transformer-based architecture that jointly\napplies Channel Attention and Pyramid Attention mechanisms to improve\nmulti-scale feature extraction and enhance segmentation performance for medical\nimages. Increasing model complexity requires more training data, and we further\nimprove model generalization with CutMix data augmentation. Our approach is\nevaluated on the Synapse multi-organ segmentation dataset, achieving a 6.9%\nimprovement in Mean Dice score and a 39.9% improvement in Hausdorff Distance\n(HD95) over an implementation without our enhancements. Our proposed model\ndemonstrates improved segmentation accuracy for complex anatomical structures,\noutperforming existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2411.16568v1", "date": "2024-11-25", "relevancy": 2.6227, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.521}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20J-CaPA%20%3A%20Joint%20Channel%20and%20Pyramid%20Attention%20Improves%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20J-CaPA%20%3A%20Joint%20Channel%20and%20Pyramid%20Attention%20Improves%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Marzia%20Binta%20Nizam%20and%20Marian%20Zlateva%20and%20James%20Davis%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20crucial%20for%20diagnosis%20and%20treatment%20planning.%0ATraditional%20CNN-based%20models%2C%20like%20U-Net%2C%20have%20shown%20promising%20results%20but%0Astruggle%20to%20capture%20long-range%20dependencies%20and%20global%20context.%20To%20address%0Athese%20limitations%2C%20we%20propose%20a%20transformer-based%20architecture%20that%20jointly%0Aapplies%20Channel%20Attention%20and%20Pyramid%20Attention%20mechanisms%20to%20improve%0Amulti-scale%20feature%20extraction%20and%20enhance%20segmentation%20performance%20for%20medical%0Aimages.%20Increasing%20model%20complexity%20requires%20more%20training%20data%2C%20and%20we%20further%0Aimprove%20model%20generalization%20with%20CutMix%20data%20augmentation.%20Our%20approach%20is%0Aevaluated%20on%20the%20Synapse%20multi-organ%20segmentation%20dataset%2C%20achieving%20a%206.9%25%0Aimprovement%20in%20Mean%20Dice%20score%20and%20a%2039.9%25%20improvement%20in%20Hausdorff%20Distance%0A%28HD95%29%20over%20an%20implementation%20without%20our%20enhancements.%20Our%20proposed%20model%0Ademonstrates%20improved%20segmentation%20accuracy%20for%20complex%20anatomical%20structures%2C%0Aoutperforming%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJ-CaPA%2520%253A%2520Joint%2520Channel%2520and%2520Pyramid%2520Attention%2520Improves%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DMarzia%2520Binta%2520Nizam%2520and%2520Marian%2520Zlateva%2520and%2520James%2520Davis%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520crucial%2520for%2520diagnosis%2520and%2520treatment%2520planning.%250ATraditional%2520CNN-based%2520models%252C%2520like%2520U-Net%252C%2520have%2520shown%2520promising%2520results%2520but%250Astruggle%2520to%2520capture%2520long-range%2520dependencies%2520and%2520global%2520context.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520a%2520transformer-based%2520architecture%2520that%2520jointly%250Aapplies%2520Channel%2520Attention%2520and%2520Pyramid%2520Attention%2520mechanisms%2520to%2520improve%250Amulti-scale%2520feature%2520extraction%2520and%2520enhance%2520segmentation%2520performance%2520for%2520medical%250Aimages.%2520Increasing%2520model%2520complexity%2520requires%2520more%2520training%2520data%252C%2520and%2520we%2520further%250Aimprove%2520model%2520generalization%2520with%2520CutMix%2520data%2520augmentation.%2520Our%2520approach%2520is%250Aevaluated%2520on%2520the%2520Synapse%2520multi-organ%2520segmentation%2520dataset%252C%2520achieving%2520a%25206.9%2525%250Aimprovement%2520in%2520Mean%2520Dice%2520score%2520and%2520a%252039.9%2525%2520improvement%2520in%2520Hausdorff%2520Distance%250A%2528HD95%2529%2520over%2520an%2520implementation%2520without%2520our%2520enhancements.%2520Our%2520proposed%2520model%250Ademonstrates%2520improved%2520segmentation%2520accuracy%2520for%2520complex%2520anatomical%2520structures%252C%250Aoutperforming%2520existing%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=J-CaPA%20%3A%20Joint%20Channel%20and%20Pyramid%20Attention%20Improves%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Marzia%20Binta%20Nizam%20and%20Marian%20Zlateva%20and%20James%20Davis&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20crucial%20for%20diagnosis%20and%20treatment%20planning.%0ATraditional%20CNN-based%20models%2C%20like%20U-Net%2C%20have%20shown%20promising%20results%20but%0Astruggle%20to%20capture%20long-range%20dependencies%20and%20global%20context.%20To%20address%0Athese%20limitations%2C%20we%20propose%20a%20transformer-based%20architecture%20that%20jointly%0Aapplies%20Channel%20Attention%20and%20Pyramid%20Attention%20mechanisms%20to%20improve%0Amulti-scale%20feature%20extraction%20and%20enhance%20segmentation%20performance%20for%20medical%0Aimages.%20Increasing%20model%20complexity%20requires%20more%20training%20data%2C%20and%20we%20further%0Aimprove%20model%20generalization%20with%20CutMix%20data%20augmentation.%20Our%20approach%20is%0Aevaluated%20on%20the%20Synapse%20multi-organ%20segmentation%20dataset%2C%20achieving%20a%206.9%25%0Aimprovement%20in%20Mean%20Dice%20score%20and%20a%2039.9%25%20improvement%20in%20Hausdorff%20Distance%0A%28HD95%29%20over%20an%20implementation%20without%20our%20enhancements.%20Our%20proposed%20model%0Ademonstrates%20improved%20segmentation%20accuracy%20for%20complex%20anatomical%20structures%2C%0Aoutperforming%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16568v1&entry.124074799=Read"},
{"title": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language\n  Models for Robotics", "author": "Chan Hee Song and Valts Blukis and Jonathan Tremblay and Stephen Tyree and Yu Su and Stan Birchfield", "abstract": "  Spatial understanding is a crucial capability for robots to make grounded\ndecisions based on their environment. This foundational skill enables robots\nnot only to perceive their surroundings but also to reason about and interact\nmeaningfully within the world. In modern robotics, these capabilities are taken\non by visual language models, and they face significant challenges when applied\nto spatial reasoning context due to their training data sources. These sources\nutilize general-purpose image datasets, and they often lack sophisticated\nspatial scene understanding capabilities. For example, the datasets do not\naddress reference frame comprehension - spatial relationships require clear\ncontextual understanding, whether from an ego-centric, object-centric, or\nworld-centric perspective, which allow for effective real-world interaction. To\naddress this issue, we introduce RoboSpatial, a large-scale spatial\nunderstanding dataset consisting of real indoor and tabletop scenes captured as\n3D scans and egocentric images, annotated with rich spatial information\nrelevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M\nannotated spatial relationships, with paired 2D egocentric images and 3D scans\nto make it both 2D and 3D ready. Our experiments show that models trained with\nRoboSpatial outperform baselines on downstream tasks such as spatial affordance\nprediction, spatial relationship prediction, and robotics manipulation.\n", "link": "http://arxiv.org/abs/2411.16537v1", "date": "2024-11-25", "relevancy": 2.6225, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6571}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboSpatial%3A%20Teaching%20Spatial%20Understanding%20to%202D%20and%203D%20Vision-Language%0A%20%20Models%20for%20Robotics&body=Title%3A%20RoboSpatial%3A%20Teaching%20Spatial%20Understanding%20to%202D%20and%203D%20Vision-Language%0A%20%20Models%20for%20Robotics%0AAuthor%3A%20Chan%20Hee%20Song%20and%20Valts%20Blukis%20and%20Jonathan%20Tremblay%20and%20Stephen%20Tyree%20and%20Yu%20Su%20and%20Stan%20Birchfield%0AAbstract%3A%20%20%20Spatial%20understanding%20is%20a%20crucial%20capability%20for%20robots%20to%20make%20grounded%0Adecisions%20based%20on%20their%20environment.%20This%20foundational%20skill%20enables%20robots%0Anot%20only%20to%20perceive%20their%20surroundings%20but%20also%20to%20reason%20about%20and%20interact%0Ameaningfully%20within%20the%20world.%20In%20modern%20robotics%2C%20these%20capabilities%20are%20taken%0Aon%20by%20visual%20language%20models%2C%20and%20they%20face%20significant%20challenges%20when%20applied%0Ato%20spatial%20reasoning%20context%20due%20to%20their%20training%20data%20sources.%20These%20sources%0Autilize%20general-purpose%20image%20datasets%2C%20and%20they%20often%20lack%20sophisticated%0Aspatial%20scene%20understanding%20capabilities.%20For%20example%2C%20the%20datasets%20do%20not%0Aaddress%20reference%20frame%20comprehension%20-%20spatial%20relationships%20require%20clear%0Acontextual%20understanding%2C%20whether%20from%20an%20ego-centric%2C%20object-centric%2C%20or%0Aworld-centric%20perspective%2C%20which%20allow%20for%20effective%20real-world%20interaction.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20RoboSpatial%2C%20a%20large-scale%20spatial%0Aunderstanding%20dataset%20consisting%20of%20real%20indoor%20and%20tabletop%20scenes%20captured%20as%0A3D%20scans%20and%20egocentric%20images%2C%20annotated%20with%20rich%20spatial%20information%0Arelevant%20to%20robotics.%20The%20dataset%20includes%201M%20images%2C%205K%203D%20scans%2C%20and%203M%0Aannotated%20spatial%20relationships%2C%20with%20paired%202D%20egocentric%20images%20and%203D%20scans%0Ato%20make%20it%20both%202D%20and%203D%20ready.%20Our%20experiments%20show%20that%20models%20trained%20with%0ARoboSpatial%20outperform%20baselines%20on%20downstream%20tasks%20such%20as%20spatial%20affordance%0Aprediction%2C%20spatial%20relationship%20prediction%2C%20and%20robotics%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboSpatial%253A%2520Teaching%2520Spatial%2520Understanding%2520to%25202D%2520and%25203D%2520Vision-Language%250A%2520%2520Models%2520for%2520Robotics%26entry.906535625%3DChan%2520Hee%2520Song%2520and%2520Valts%2520Blukis%2520and%2520Jonathan%2520Tremblay%2520and%2520Stephen%2520Tyree%2520and%2520Yu%2520Su%2520and%2520Stan%2520Birchfield%26entry.1292438233%3D%2520%2520Spatial%2520understanding%2520is%2520a%2520crucial%2520capability%2520for%2520robots%2520to%2520make%2520grounded%250Adecisions%2520based%2520on%2520their%2520environment.%2520This%2520foundational%2520skill%2520enables%2520robots%250Anot%2520only%2520to%2520perceive%2520their%2520surroundings%2520but%2520also%2520to%2520reason%2520about%2520and%2520interact%250Ameaningfully%2520within%2520the%2520world.%2520In%2520modern%2520robotics%252C%2520these%2520capabilities%2520are%2520taken%250Aon%2520by%2520visual%2520language%2520models%252C%2520and%2520they%2520face%2520significant%2520challenges%2520when%2520applied%250Ato%2520spatial%2520reasoning%2520context%2520due%2520to%2520their%2520training%2520data%2520sources.%2520These%2520sources%250Autilize%2520general-purpose%2520image%2520datasets%252C%2520and%2520they%2520often%2520lack%2520sophisticated%250Aspatial%2520scene%2520understanding%2520capabilities.%2520For%2520example%252C%2520the%2520datasets%2520do%2520not%250Aaddress%2520reference%2520frame%2520comprehension%2520-%2520spatial%2520relationships%2520require%2520clear%250Acontextual%2520understanding%252C%2520whether%2520from%2520an%2520ego-centric%252C%2520object-centric%252C%2520or%250Aworld-centric%2520perspective%252C%2520which%2520allow%2520for%2520effective%2520real-world%2520interaction.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520introduce%2520RoboSpatial%252C%2520a%2520large-scale%2520spatial%250Aunderstanding%2520dataset%2520consisting%2520of%2520real%2520indoor%2520and%2520tabletop%2520scenes%2520captured%2520as%250A3D%2520scans%2520and%2520egocentric%2520images%252C%2520annotated%2520with%2520rich%2520spatial%2520information%250Arelevant%2520to%2520robotics.%2520The%2520dataset%2520includes%25201M%2520images%252C%25205K%25203D%2520scans%252C%2520and%25203M%250Aannotated%2520spatial%2520relationships%252C%2520with%2520paired%25202D%2520egocentric%2520images%2520and%25203D%2520scans%250Ato%2520make%2520it%2520both%25202D%2520and%25203D%2520ready.%2520Our%2520experiments%2520show%2520that%2520models%2520trained%2520with%250ARoboSpatial%2520outperform%2520baselines%2520on%2520downstream%2520tasks%2520such%2520as%2520spatial%2520affordance%250Aprediction%252C%2520spatial%2520relationship%2520prediction%252C%2520and%2520robotics%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboSpatial%3A%20Teaching%20Spatial%20Understanding%20to%202D%20and%203D%20Vision-Language%0A%20%20Models%20for%20Robotics&entry.906535625=Chan%20Hee%20Song%20and%20Valts%20Blukis%20and%20Jonathan%20Tremblay%20and%20Stephen%20Tyree%20and%20Yu%20Su%20and%20Stan%20Birchfield&entry.1292438233=%20%20Spatial%20understanding%20is%20a%20crucial%20capability%20for%20robots%20to%20make%20grounded%0Adecisions%20based%20on%20their%20environment.%20This%20foundational%20skill%20enables%20robots%0Anot%20only%20to%20perceive%20their%20surroundings%20but%20also%20to%20reason%20about%20and%20interact%0Ameaningfully%20within%20the%20world.%20In%20modern%20robotics%2C%20these%20capabilities%20are%20taken%0Aon%20by%20visual%20language%20models%2C%20and%20they%20face%20significant%20challenges%20when%20applied%0Ato%20spatial%20reasoning%20context%20due%20to%20their%20training%20data%20sources.%20These%20sources%0Autilize%20general-purpose%20image%20datasets%2C%20and%20they%20often%20lack%20sophisticated%0Aspatial%20scene%20understanding%20capabilities.%20For%20example%2C%20the%20datasets%20do%20not%0Aaddress%20reference%20frame%20comprehension%20-%20spatial%20relationships%20require%20clear%0Acontextual%20understanding%2C%20whether%20from%20an%20ego-centric%2C%20object-centric%2C%20or%0Aworld-centric%20perspective%2C%20which%20allow%20for%20effective%20real-world%20interaction.%20To%0Aaddress%20this%20issue%2C%20we%20introduce%20RoboSpatial%2C%20a%20large-scale%20spatial%0Aunderstanding%20dataset%20consisting%20of%20real%20indoor%20and%20tabletop%20scenes%20captured%20as%0A3D%20scans%20and%20egocentric%20images%2C%20annotated%20with%20rich%20spatial%20information%0Arelevant%20to%20robotics.%20The%20dataset%20includes%201M%20images%2C%205K%203D%20scans%2C%20and%203M%0Aannotated%20spatial%20relationships%2C%20with%20paired%202D%20egocentric%20images%20and%203D%20scans%0Ato%20make%20it%20both%202D%20and%203D%20ready.%20Our%20experiments%20show%20that%20models%20trained%20with%0ARoboSpatial%20outperform%20baselines%20on%20downstream%20tasks%20such%20as%20spatial%20affordance%0Aprediction%2C%20spatial%20relationship%20prediction%2C%20and%20robotics%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16537v1&entry.124074799=Read"},
{"title": "Deformable Mamba for Wide Field of View Segmentation", "author": "Jie Hu and Junwei Zheng and Jiale Wei and Jiaming Zhang and Rainer Stiefelhagen", "abstract": "  Wide-FoV cameras, like fisheye and panoramic setups, are essential for\nbroader perception but introduce significant distortions in 180{\\deg} and\n360{\\deg} images, complicating dense prediction tasks. For instance, existing\nMAMBA models lacking distortion-aware capacity cannot perform well in panoramic\nsemantic segmentation. To address this problem, this work presents Deformable\nMamba, a unified framework specifically designed to address imaging distortions\nwithin the context of panoramic and fisheye semantic segmentation. At the core\nis a decoder constructed with a series of Deformable Mamba Fusion (DMF) blocks,\nmaking the whole framework more deformable, efficient, and accurate, when\nhandling extreme distortions. Extensive evaluations across five datasets\ndemonstrate that our method consistently improves segmentation accuracy\ncompared to the previous state-of-the-art methods tailored for specific FoVs.\nNotably, Deformable Mamba achieves a +2.5% performance improvement on the\n360{\\deg} Stanford2D3D dataset, and shows better results across FoVs from\n60{\\deg} to 360{\\deg}.\n", "link": "http://arxiv.org/abs/2411.16481v1", "date": "2024-11-25", "relevancy": 2.6225, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.53}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5234}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deformable%20Mamba%20for%20Wide%20Field%20of%20View%20Segmentation&body=Title%3A%20Deformable%20Mamba%20for%20Wide%20Field%20of%20View%20Segmentation%0AAuthor%3A%20Jie%20Hu%20and%20Junwei%20Zheng%20and%20Jiale%20Wei%20and%20Jiaming%20Zhang%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20Wide-FoV%20cameras%2C%20like%20fisheye%20and%20panoramic%20setups%2C%20are%20essential%20for%0Abroader%20perception%20but%20introduce%20significant%20distortions%20in%20180%7B%5Cdeg%7D%20and%0A360%7B%5Cdeg%7D%20images%2C%20complicating%20dense%20prediction%20tasks.%20For%20instance%2C%20existing%0AMAMBA%20models%20lacking%20distortion-aware%20capacity%20cannot%20perform%20well%20in%20panoramic%0Asemantic%20segmentation.%20To%20address%20this%20problem%2C%20this%20work%20presents%20Deformable%0AMamba%2C%20a%20unified%20framework%20specifically%20designed%20to%20address%20imaging%20distortions%0Awithin%20the%20context%20of%20panoramic%20and%20fisheye%20semantic%20segmentation.%20At%20the%20core%0Ais%20a%20decoder%20constructed%20with%20a%20series%20of%20Deformable%20Mamba%20Fusion%20%28DMF%29%20blocks%2C%0Amaking%20the%20whole%20framework%20more%20deformable%2C%20efficient%2C%20and%20accurate%2C%20when%0Ahandling%20extreme%20distortions.%20Extensive%20evaluations%20across%20five%20datasets%0Ademonstrate%20that%20our%20method%20consistently%20improves%20segmentation%20accuracy%0Acompared%20to%20the%20previous%20state-of-the-art%20methods%20tailored%20for%20specific%20FoVs.%0ANotably%2C%20Deformable%20Mamba%20achieves%20a%20%2B2.5%25%20performance%20improvement%20on%20the%0A360%7B%5Cdeg%7D%20Stanford2D3D%20dataset%2C%20and%20shows%20better%20results%20across%20FoVs%20from%0A60%7B%5Cdeg%7D%20to%20360%7B%5Cdeg%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeformable%2520Mamba%2520for%2520Wide%2520Field%2520of%2520View%2520Segmentation%26entry.906535625%3DJie%2520Hu%2520and%2520Junwei%2520Zheng%2520and%2520Jiale%2520Wei%2520and%2520Jiaming%2520Zhang%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520Wide-FoV%2520cameras%252C%2520like%2520fisheye%2520and%2520panoramic%2520setups%252C%2520are%2520essential%2520for%250Abroader%2520perception%2520but%2520introduce%2520significant%2520distortions%2520in%2520180%257B%255Cdeg%257D%2520and%250A360%257B%255Cdeg%257D%2520images%252C%2520complicating%2520dense%2520prediction%2520tasks.%2520For%2520instance%252C%2520existing%250AMAMBA%2520models%2520lacking%2520distortion-aware%2520capacity%2520cannot%2520perform%2520well%2520in%2520panoramic%250Asemantic%2520segmentation.%2520To%2520address%2520this%2520problem%252C%2520this%2520work%2520presents%2520Deformable%250AMamba%252C%2520a%2520unified%2520framework%2520specifically%2520designed%2520to%2520address%2520imaging%2520distortions%250Awithin%2520the%2520context%2520of%2520panoramic%2520and%2520fisheye%2520semantic%2520segmentation.%2520At%2520the%2520core%250Ais%2520a%2520decoder%2520constructed%2520with%2520a%2520series%2520of%2520Deformable%2520Mamba%2520Fusion%2520%2528DMF%2529%2520blocks%252C%250Amaking%2520the%2520whole%2520framework%2520more%2520deformable%252C%2520efficient%252C%2520and%2520accurate%252C%2520when%250Ahandling%2520extreme%2520distortions.%2520Extensive%2520evaluations%2520across%2520five%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520consistently%2520improves%2520segmentation%2520accuracy%250Acompared%2520to%2520the%2520previous%2520state-of-the-art%2520methods%2520tailored%2520for%2520specific%2520FoVs.%250ANotably%252C%2520Deformable%2520Mamba%2520achieves%2520a%2520%252B2.5%2525%2520performance%2520improvement%2520on%2520the%250A360%257B%255Cdeg%257D%2520Stanford2D3D%2520dataset%252C%2520and%2520shows%2520better%2520results%2520across%2520FoVs%2520from%250A60%257B%255Cdeg%257D%2520to%2520360%257B%255Cdeg%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deformable%20Mamba%20for%20Wide%20Field%20of%20View%20Segmentation&entry.906535625=Jie%20Hu%20and%20Junwei%20Zheng%20and%20Jiale%20Wei%20and%20Jiaming%20Zhang%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20Wide-FoV%20cameras%2C%20like%20fisheye%20and%20panoramic%20setups%2C%20are%20essential%20for%0Abroader%20perception%20but%20introduce%20significant%20distortions%20in%20180%7B%5Cdeg%7D%20and%0A360%7B%5Cdeg%7D%20images%2C%20complicating%20dense%20prediction%20tasks.%20For%20instance%2C%20existing%0AMAMBA%20models%20lacking%20distortion-aware%20capacity%20cannot%20perform%20well%20in%20panoramic%0Asemantic%20segmentation.%20To%20address%20this%20problem%2C%20this%20work%20presents%20Deformable%0AMamba%2C%20a%20unified%20framework%20specifically%20designed%20to%20address%20imaging%20distortions%0Awithin%20the%20context%20of%20panoramic%20and%20fisheye%20semantic%20segmentation.%20At%20the%20core%0Ais%20a%20decoder%20constructed%20with%20a%20series%20of%20Deformable%20Mamba%20Fusion%20%28DMF%29%20blocks%2C%0Amaking%20the%20whole%20framework%20more%20deformable%2C%20efficient%2C%20and%20accurate%2C%20when%0Ahandling%20extreme%20distortions.%20Extensive%20evaluations%20across%20five%20datasets%0Ademonstrate%20that%20our%20method%20consistently%20improves%20segmentation%20accuracy%0Acompared%20to%20the%20previous%20state-of-the-art%20methods%20tailored%20for%20specific%20FoVs.%0ANotably%2C%20Deformable%20Mamba%20achieves%20a%20%2B2.5%25%20performance%20improvement%20on%20the%0A360%7B%5Cdeg%7D%20Stanford2D3D%20dataset%2C%20and%20shows%20better%20results%20across%20FoVs%20from%0A60%7B%5Cdeg%7D%20to%20360%7B%5Cdeg%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16481v1&entry.124074799=Read"},
{"title": "Adapter-based Approaches to Knowledge-enhanced Language Models -- A\n  Survey", "author": "Alexander Fichtl and Juraj Vladika and Georg Groh", "abstract": "  Knowledge-enhanced language models (KELMs) have emerged as promising tools to\nbridge the gap between large-scale language models and domain-specific\nknowledge. KELMs can achieve higher factual accuracy and mitigate\nhallucinations by leveraging knowledge graphs (KGs). They are frequently\ncombined with adapter modules to reduce the computational load and risk of\ncatastrophic forgetting. In this paper, we conduct a systematic literature\nreview (SLR) on adapter-based approaches to KELMs. We provide a structured\noverview of existing methodologies in the field through quantitative and\nqualitative analysis and explore the strengths and potential shortcomings of\nindividual approaches. We show that general knowledge and domain-specific\napproaches have been frequently explored along with various adapter\narchitectures and downstream tasks. We particularly focused on the popular\nbiomedical domain, where we provided an insightful performance comparison of\nexisting KELMs. We outline the main trends and propose promising future\ndirections.\n", "link": "http://arxiv.org/abs/2411.16403v1", "date": "2024-11-25", "relevancy": 2.5936, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapter-based%20Approaches%20to%20Knowledge-enhanced%20Language%20Models%20--%20A%0A%20%20Survey&body=Title%3A%20Adapter-based%20Approaches%20to%20Knowledge-enhanced%20Language%20Models%20--%20A%0A%20%20Survey%0AAuthor%3A%20Alexander%20Fichtl%20and%20Juraj%20Vladika%20and%20Georg%20Groh%0AAbstract%3A%20%20%20Knowledge-enhanced%20language%20models%20%28KELMs%29%20have%20emerged%20as%20promising%20tools%20to%0Abridge%20the%20gap%20between%20large-scale%20language%20models%20and%20domain-specific%0Aknowledge.%20KELMs%20can%20achieve%20higher%20factual%20accuracy%20and%20mitigate%0Ahallucinations%20by%20leveraging%20knowledge%20graphs%20%28KGs%29.%20They%20are%20frequently%0Acombined%20with%20adapter%20modules%20to%20reduce%20the%20computational%20load%20and%20risk%20of%0Acatastrophic%20forgetting.%20In%20this%20paper%2C%20we%20conduct%20a%20systematic%20literature%0Areview%20%28SLR%29%20on%20adapter-based%20approaches%20to%20KELMs.%20We%20provide%20a%20structured%0Aoverview%20of%20existing%20methodologies%20in%20the%20field%20through%20quantitative%20and%0Aqualitative%20analysis%20and%20explore%20the%20strengths%20and%20potential%20shortcomings%20of%0Aindividual%20approaches.%20We%20show%20that%20general%20knowledge%20and%20domain-specific%0Aapproaches%20have%20been%20frequently%20explored%20along%20with%20various%20adapter%0Aarchitectures%20and%20downstream%20tasks.%20We%20particularly%20focused%20on%20the%20popular%0Abiomedical%20domain%2C%20where%20we%20provided%20an%20insightful%20performance%20comparison%20of%0Aexisting%20KELMs.%20We%20outline%20the%20main%20trends%20and%20propose%20promising%20future%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapter-based%2520Approaches%2520to%2520Knowledge-enhanced%2520Language%2520Models%2520--%2520A%250A%2520%2520Survey%26entry.906535625%3DAlexander%2520Fichtl%2520and%2520Juraj%2520Vladika%2520and%2520Georg%2520Groh%26entry.1292438233%3D%2520%2520Knowledge-enhanced%2520language%2520models%2520%2528KELMs%2529%2520have%2520emerged%2520as%2520promising%2520tools%2520to%250Abridge%2520the%2520gap%2520between%2520large-scale%2520language%2520models%2520and%2520domain-specific%250Aknowledge.%2520KELMs%2520can%2520achieve%2520higher%2520factual%2520accuracy%2520and%2520mitigate%250Ahallucinations%2520by%2520leveraging%2520knowledge%2520graphs%2520%2528KGs%2529.%2520They%2520are%2520frequently%250Acombined%2520with%2520adapter%2520modules%2520to%2520reduce%2520the%2520computational%2520load%2520and%2520risk%2520of%250Acatastrophic%2520forgetting.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520systematic%2520literature%250Areview%2520%2528SLR%2529%2520on%2520adapter-based%2520approaches%2520to%2520KELMs.%2520We%2520provide%2520a%2520structured%250Aoverview%2520of%2520existing%2520methodologies%2520in%2520the%2520field%2520through%2520quantitative%2520and%250Aqualitative%2520analysis%2520and%2520explore%2520the%2520strengths%2520and%2520potential%2520shortcomings%2520of%250Aindividual%2520approaches.%2520We%2520show%2520that%2520general%2520knowledge%2520and%2520domain-specific%250Aapproaches%2520have%2520been%2520frequently%2520explored%2520along%2520with%2520various%2520adapter%250Aarchitectures%2520and%2520downstream%2520tasks.%2520We%2520particularly%2520focused%2520on%2520the%2520popular%250Abiomedical%2520domain%252C%2520where%2520we%2520provided%2520an%2520insightful%2520performance%2520comparison%2520of%250Aexisting%2520KELMs.%2520We%2520outline%2520the%2520main%2520trends%2520and%2520propose%2520promising%2520future%250Adirections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapter-based%20Approaches%20to%20Knowledge-enhanced%20Language%20Models%20--%20A%0A%20%20Survey&entry.906535625=Alexander%20Fichtl%20and%20Juraj%20Vladika%20and%20Georg%20Groh&entry.1292438233=%20%20Knowledge-enhanced%20language%20models%20%28KELMs%29%20have%20emerged%20as%20promising%20tools%20to%0Abridge%20the%20gap%20between%20large-scale%20language%20models%20and%20domain-specific%0Aknowledge.%20KELMs%20can%20achieve%20higher%20factual%20accuracy%20and%20mitigate%0Ahallucinations%20by%20leveraging%20knowledge%20graphs%20%28KGs%29.%20They%20are%20frequently%0Acombined%20with%20adapter%20modules%20to%20reduce%20the%20computational%20load%20and%20risk%20of%0Acatastrophic%20forgetting.%20In%20this%20paper%2C%20we%20conduct%20a%20systematic%20literature%0Areview%20%28SLR%29%20on%20adapter-based%20approaches%20to%20KELMs.%20We%20provide%20a%20structured%0Aoverview%20of%20existing%20methodologies%20in%20the%20field%20through%20quantitative%20and%0Aqualitative%20analysis%20and%20explore%20the%20strengths%20and%20potential%20shortcomings%20of%0Aindividual%20approaches.%20We%20show%20that%20general%20knowledge%20and%20domain-specific%0Aapproaches%20have%20been%20frequently%20explored%20along%20with%20various%20adapter%0Aarchitectures%20and%20downstream%20tasks.%20We%20particularly%20focused%20on%20the%20popular%0Abiomedical%20domain%2C%20where%20we%20provided%20an%20insightful%20performance%20comparison%20of%0Aexisting%20KELMs.%20We%20outline%20the%20main%20trends%20and%20propose%20promising%20future%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16403v1&entry.124074799=Read"},
{"title": "BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment", "author": "Shaolei Zhang and Kehao Zhang and Qingkai Fang and Shoutao Guo and Yan Zhou and Xiaodong Liu and Yang Feng", "abstract": "  Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-3-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.\n", "link": "http://arxiv.org/abs/2411.16300v1", "date": "2024-11-25", "relevancy": 2.5894, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5147}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment&body=Title%3A%20BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment%0AAuthor%3A%20Shaolei%20Zhang%20and%20Kehao%20Zhang%20and%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Xiaodong%20Liu%20and%20Yang%20Feng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20their%20powerful%20generative%20capabilities%20and%0Avast%20knowledge%2C%20empower%20various%20tasks%20in%20everyday%20life.%20However%2C%20these%0Aabilities%20are%20primarily%20concentrated%20in%20high-resource%20languages%2C%20leaving%0Alow-resource%20languages%20with%20weaker%20generative%20capabilities%20and%20relatively%0Alimited%20knowledge.%20Enhancing%20the%20multilingual%20capabilities%20of%20LLMs%20is%20therefore%0Acrucial%20for%20serving%20over%20100%20linguistic%20communities%20worldwide.%20An%20intuitive%0Aapproach%20to%20enhance%20the%20multilingual%20capabilities%20would%20be%20to%20construct%0Ainstruction%20data%20for%20various%20languages%2C%20but%20constructing%20instruction%20data%20for%0Aover%20100%20languages%20is%20prohibitively%20costly.%20In%20this%20paper%2C%20we%20introduce%20BayLing%0A2%2C%20which%20efficiently%20transfers%20generative%20capabilities%20and%20knowledge%20from%0Ahigh-resource%20languages%20to%20low-resource%20languages%20through%20language%20alignment.%0ATo%20achieve%20this%2C%20we%20constructed%20a%20dataset%20of%203.2%20million%20instructions%2C%0Acomprising%20high-resource%20language%20instructions%20%28Chinese%20and%20English%29%20and%0Across-lingual%20instructions%20for%20100%2B%20languages%20and%20performed%20instruction%20tuning%0Abased%20on%20the%20dataset%20to%20facilitate%20the%20capability%20transfer%20between%20languages.%0AUsing%20Llama%20as%20the%20foundation%20model%2C%20we%20developed%20BayLing-2-7B%2C%20BayLing-2-13B%2C%0Aand%20BayLing-3-8B%2C%20and%20conducted%20a%20comprehensive%20evaluation%20of%20BayLing.%20For%0Amultilingual%20translation%20across%20100%2B%20languages%2C%20BayLing%20shows%20superior%0Aperformance%20compared%20to%20open-source%20models%20of%20similar%20scale.%20For%20multilingual%0Aknowledge%20and%20understanding%20benchmarks%2C%20BayLing%20achieves%20significant%0Aimprovements%20across%20over%2020%20low-resource%20languages%2C%20demonstrating%20its%0Acapability%20of%20effective%20knowledge%20transfer%20from%20high-resource%20to%20low-resource%0Alanguages.%20Furthermore%2C%20results%20on%20English%20benchmarks%20indicate%20that%20BayLing%0Amaintains%20high%20performance%20in%20highresource%20languages%20while%20enhancing%20the%0Aperformance%20in%20low-resource%20languages.%20Demo%2C%20homepage%2C%20code%20and%20models%20of%0ABayLing%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayLing%25202%253A%2520A%2520Multilingual%2520Large%2520Language%2520Model%2520with%2520Efficient%2520Language%250A%2520%2520Alignment%26entry.906535625%3DShaolei%2520Zhang%2520and%2520Kehao%2520Zhang%2520and%2520Qingkai%2520Fang%2520and%2520Shoutao%2520Guo%2520and%2520Yan%2520Zhou%2520and%2520Xiaodong%2520Liu%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520with%2520their%2520powerful%2520generative%2520capabilities%2520and%250Avast%2520knowledge%252C%2520empower%2520various%2520tasks%2520in%2520everyday%2520life.%2520However%252C%2520these%250Aabilities%2520are%2520primarily%2520concentrated%2520in%2520high-resource%2520languages%252C%2520leaving%250Alow-resource%2520languages%2520with%2520weaker%2520generative%2520capabilities%2520and%2520relatively%250Alimited%2520knowledge.%2520Enhancing%2520the%2520multilingual%2520capabilities%2520of%2520LLMs%2520is%2520therefore%250Acrucial%2520for%2520serving%2520over%2520100%2520linguistic%2520communities%2520worldwide.%2520An%2520intuitive%250Aapproach%2520to%2520enhance%2520the%2520multilingual%2520capabilities%2520would%2520be%2520to%2520construct%250Ainstruction%2520data%2520for%2520various%2520languages%252C%2520but%2520constructing%2520instruction%2520data%2520for%250Aover%2520100%2520languages%2520is%2520prohibitively%2520costly.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520BayLing%250A2%252C%2520which%2520efficiently%2520transfers%2520generative%2520capabilities%2520and%2520knowledge%2520from%250Ahigh-resource%2520languages%2520to%2520low-resource%2520languages%2520through%2520language%2520alignment.%250ATo%2520achieve%2520this%252C%2520we%2520constructed%2520a%2520dataset%2520of%25203.2%2520million%2520instructions%252C%250Acomprising%2520high-resource%2520language%2520instructions%2520%2528Chinese%2520and%2520English%2529%2520and%250Across-lingual%2520instructions%2520for%2520100%252B%2520languages%2520and%2520performed%2520instruction%2520tuning%250Abased%2520on%2520the%2520dataset%2520to%2520facilitate%2520the%2520capability%2520transfer%2520between%2520languages.%250AUsing%2520Llama%2520as%2520the%2520foundation%2520model%252C%2520we%2520developed%2520BayLing-2-7B%252C%2520BayLing-2-13B%252C%250Aand%2520BayLing-3-8B%252C%2520and%2520conducted%2520a%2520comprehensive%2520evaluation%2520of%2520BayLing.%2520For%250Amultilingual%2520translation%2520across%2520100%252B%2520languages%252C%2520BayLing%2520shows%2520superior%250Aperformance%2520compared%2520to%2520open-source%2520models%2520of%2520similar%2520scale.%2520For%2520multilingual%250Aknowledge%2520and%2520understanding%2520benchmarks%252C%2520BayLing%2520achieves%2520significant%250Aimprovements%2520across%2520over%252020%2520low-resource%2520languages%252C%2520demonstrating%2520its%250Acapability%2520of%2520effective%2520knowledge%2520transfer%2520from%2520high-resource%2520to%2520low-resource%250Alanguages.%2520Furthermore%252C%2520results%2520on%2520English%2520benchmarks%2520indicate%2520that%2520BayLing%250Amaintains%2520high%2520performance%2520in%2520highresource%2520languages%2520while%2520enhancing%2520the%250Aperformance%2520in%2520low-resource%2520languages.%2520Demo%252C%2520homepage%252C%2520code%2520and%2520models%2520of%250ABayLing%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayLing%202%3A%20A%20Multilingual%20Large%20Language%20Model%20with%20Efficient%20Language%0A%20%20Alignment&entry.906535625=Shaolei%20Zhang%20and%20Kehao%20Zhang%20and%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Xiaodong%20Liu%20and%20Yang%20Feng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20with%20their%20powerful%20generative%20capabilities%20and%0Avast%20knowledge%2C%20empower%20various%20tasks%20in%20everyday%20life.%20However%2C%20these%0Aabilities%20are%20primarily%20concentrated%20in%20high-resource%20languages%2C%20leaving%0Alow-resource%20languages%20with%20weaker%20generative%20capabilities%20and%20relatively%0Alimited%20knowledge.%20Enhancing%20the%20multilingual%20capabilities%20of%20LLMs%20is%20therefore%0Acrucial%20for%20serving%20over%20100%20linguistic%20communities%20worldwide.%20An%20intuitive%0Aapproach%20to%20enhance%20the%20multilingual%20capabilities%20would%20be%20to%20construct%0Ainstruction%20data%20for%20various%20languages%2C%20but%20constructing%20instruction%20data%20for%0Aover%20100%20languages%20is%20prohibitively%20costly.%20In%20this%20paper%2C%20we%20introduce%20BayLing%0A2%2C%20which%20efficiently%20transfers%20generative%20capabilities%20and%20knowledge%20from%0Ahigh-resource%20languages%20to%20low-resource%20languages%20through%20language%20alignment.%0ATo%20achieve%20this%2C%20we%20constructed%20a%20dataset%20of%203.2%20million%20instructions%2C%0Acomprising%20high-resource%20language%20instructions%20%28Chinese%20and%20English%29%20and%0Across-lingual%20instructions%20for%20100%2B%20languages%20and%20performed%20instruction%20tuning%0Abased%20on%20the%20dataset%20to%20facilitate%20the%20capability%20transfer%20between%20languages.%0AUsing%20Llama%20as%20the%20foundation%20model%2C%20we%20developed%20BayLing-2-7B%2C%20BayLing-2-13B%2C%0Aand%20BayLing-3-8B%2C%20and%20conducted%20a%20comprehensive%20evaluation%20of%20BayLing.%20For%0Amultilingual%20translation%20across%20100%2B%20languages%2C%20BayLing%20shows%20superior%0Aperformance%20compared%20to%20open-source%20models%20of%20similar%20scale.%20For%20multilingual%0Aknowledge%20and%20understanding%20benchmarks%2C%20BayLing%20achieves%20significant%0Aimprovements%20across%20over%2020%20low-resource%20languages%2C%20demonstrating%20its%0Acapability%20of%20effective%20knowledge%20transfer%20from%20high-resource%20to%20low-resource%0Alanguages.%20Furthermore%2C%20results%20on%20English%20benchmarks%20indicate%20that%20BayLing%0Amaintains%20high%20performance%20in%20highresource%20languages%20while%20enhancing%20the%0Aperformance%20in%20low-resource%20languages.%20Demo%2C%20homepage%2C%20code%20and%20models%20of%0ABayLing%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16300v1&entry.124074799=Read"},
{"title": "AnonyNoise: Anonymizing Event Data with Smart Noise to Outsmart\n  Re-Identification and Preserve Privacy", "author": "Katharina Bendig and Ren\u00e9 Schuster and Nicole Thiemer and Karen Joisten and Didier Stricker", "abstract": "  The increasing capabilities of deep neural networks for re-identification,\ncombined with the rise in public surveillance in recent years, pose a\nsubstantial threat to individual privacy. Event cameras were initially\nconsidered as a promising solution since their output is sparse and therefore\ndifficult for humans to interpret. However, recent advances in deep learning\nproof that neural networks are able to reconstruct high-quality grayscale\nimages and re-identify individuals using data from event cameras. In our paper,\nwe contribute a crucial ethical discussion on data privacy and present the\nfirst event anonymization pipeline to prevent re-identification not only by\nhumans but also by neural networks. Our method effectively introduces learnable\ndata-dependent noise to cover personally identifiable information in raw event\ndata, reducing attackers' re-identification capabilities by up to 60%, while\nmaintaining substantial information for the performing of downstream tasks.\nMoreover, our anonymization generalizes well on unseen data and is robust\nagainst image reconstruction and inversion attacks. Code:\nhttps://github.com/dfki-av/AnonyNoise\n", "link": "http://arxiv.org/abs/2411.16440v1", "date": "2024-11-25", "relevancy": 2.5351, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5516}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4876}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnonyNoise%3A%20Anonymizing%20Event%20Data%20with%20Smart%20Noise%20to%20Outsmart%0A%20%20Re-Identification%20and%20Preserve%20Privacy&body=Title%3A%20AnonyNoise%3A%20Anonymizing%20Event%20Data%20with%20Smart%20Noise%20to%20Outsmart%0A%20%20Re-Identification%20and%20Preserve%20Privacy%0AAuthor%3A%20Katharina%20Bendig%20and%20Ren%C3%A9%20Schuster%20and%20Nicole%20Thiemer%20and%20Karen%20Joisten%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20The%20increasing%20capabilities%20of%20deep%20neural%20networks%20for%20re-identification%2C%0Acombined%20with%20the%20rise%20in%20public%20surveillance%20in%20recent%20years%2C%20pose%20a%0Asubstantial%20threat%20to%20individual%20privacy.%20Event%20cameras%20were%20initially%0Aconsidered%20as%20a%20promising%20solution%20since%20their%20output%20is%20sparse%20and%20therefore%0Adifficult%20for%20humans%20to%20interpret.%20However%2C%20recent%20advances%20in%20deep%20learning%0Aproof%20that%20neural%20networks%20are%20able%20to%20reconstruct%20high-quality%20grayscale%0Aimages%20and%20re-identify%20individuals%20using%20data%20from%20event%20cameras.%20In%20our%20paper%2C%0Awe%20contribute%20a%20crucial%20ethical%20discussion%20on%20data%20privacy%20and%20present%20the%0Afirst%20event%20anonymization%20pipeline%20to%20prevent%20re-identification%20not%20only%20by%0Ahumans%20but%20also%20by%20neural%20networks.%20Our%20method%20effectively%20introduces%20learnable%0Adata-dependent%20noise%20to%20cover%20personally%20identifiable%20information%20in%20raw%20event%0Adata%2C%20reducing%20attackers%27%20re-identification%20capabilities%20by%20up%20to%2060%25%2C%20while%0Amaintaining%20substantial%20information%20for%20the%20performing%20of%20downstream%20tasks.%0AMoreover%2C%20our%20anonymization%20generalizes%20well%20on%20unseen%20data%20and%20is%20robust%0Aagainst%20image%20reconstruction%20and%20inversion%20attacks.%20Code%3A%0Ahttps%3A//github.com/dfki-av/AnonyNoise%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnonyNoise%253A%2520Anonymizing%2520Event%2520Data%2520with%2520Smart%2520Noise%2520to%2520Outsmart%250A%2520%2520Re-Identification%2520and%2520Preserve%2520Privacy%26entry.906535625%3DKatharina%2520Bendig%2520and%2520Ren%25C3%25A9%2520Schuster%2520and%2520Nicole%2520Thiemer%2520and%2520Karen%2520Joisten%2520and%2520Didier%2520Stricker%26entry.1292438233%3D%2520%2520The%2520increasing%2520capabilities%2520of%2520deep%2520neural%2520networks%2520for%2520re-identification%252C%250Acombined%2520with%2520the%2520rise%2520in%2520public%2520surveillance%2520in%2520recent%2520years%252C%2520pose%2520a%250Asubstantial%2520threat%2520to%2520individual%2520privacy.%2520Event%2520cameras%2520were%2520initially%250Aconsidered%2520as%2520a%2520promising%2520solution%2520since%2520their%2520output%2520is%2520sparse%2520and%2520therefore%250Adifficult%2520for%2520humans%2520to%2520interpret.%2520However%252C%2520recent%2520advances%2520in%2520deep%2520learning%250Aproof%2520that%2520neural%2520networks%2520are%2520able%2520to%2520reconstruct%2520high-quality%2520grayscale%250Aimages%2520and%2520re-identify%2520individuals%2520using%2520data%2520from%2520event%2520cameras.%2520In%2520our%2520paper%252C%250Awe%2520contribute%2520a%2520crucial%2520ethical%2520discussion%2520on%2520data%2520privacy%2520and%2520present%2520the%250Afirst%2520event%2520anonymization%2520pipeline%2520to%2520prevent%2520re-identification%2520not%2520only%2520by%250Ahumans%2520but%2520also%2520by%2520neural%2520networks.%2520Our%2520method%2520effectively%2520introduces%2520learnable%250Adata-dependent%2520noise%2520to%2520cover%2520personally%2520identifiable%2520information%2520in%2520raw%2520event%250Adata%252C%2520reducing%2520attackers%2527%2520re-identification%2520capabilities%2520by%2520up%2520to%252060%2525%252C%2520while%250Amaintaining%2520substantial%2520information%2520for%2520the%2520performing%2520of%2520downstream%2520tasks.%250AMoreover%252C%2520our%2520anonymization%2520generalizes%2520well%2520on%2520unseen%2520data%2520and%2520is%2520robust%250Aagainst%2520image%2520reconstruction%2520and%2520inversion%2520attacks.%2520Code%253A%250Ahttps%253A//github.com/dfki-av/AnonyNoise%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnonyNoise%3A%20Anonymizing%20Event%20Data%20with%20Smart%20Noise%20to%20Outsmart%0A%20%20Re-Identification%20and%20Preserve%20Privacy&entry.906535625=Katharina%20Bendig%20and%20Ren%C3%A9%20Schuster%20and%20Nicole%20Thiemer%20and%20Karen%20Joisten%20and%20Didier%20Stricker&entry.1292438233=%20%20The%20increasing%20capabilities%20of%20deep%20neural%20networks%20for%20re-identification%2C%0Acombined%20with%20the%20rise%20in%20public%20surveillance%20in%20recent%20years%2C%20pose%20a%0Asubstantial%20threat%20to%20individual%20privacy.%20Event%20cameras%20were%20initially%0Aconsidered%20as%20a%20promising%20solution%20since%20their%20output%20is%20sparse%20and%20therefore%0Adifficult%20for%20humans%20to%20interpret.%20However%2C%20recent%20advances%20in%20deep%20learning%0Aproof%20that%20neural%20networks%20are%20able%20to%20reconstruct%20high-quality%20grayscale%0Aimages%20and%20re-identify%20individuals%20using%20data%20from%20event%20cameras.%20In%20our%20paper%2C%0Awe%20contribute%20a%20crucial%20ethical%20discussion%20on%20data%20privacy%20and%20present%20the%0Afirst%20event%20anonymization%20pipeline%20to%20prevent%20re-identification%20not%20only%20by%0Ahumans%20but%20also%20by%20neural%20networks.%20Our%20method%20effectively%20introduces%20learnable%0Adata-dependent%20noise%20to%20cover%20personally%20identifiable%20information%20in%20raw%20event%0Adata%2C%20reducing%20attackers%27%20re-identification%20capabilities%20by%20up%20to%2060%25%2C%20while%0Amaintaining%20substantial%20information%20for%20the%20performing%20of%20downstream%20tasks.%0AMoreover%2C%20our%20anonymization%20generalizes%20well%20on%20unseen%20data%20and%20is%20robust%0Aagainst%20image%20reconstruction%20and%20inversion%20attacks.%20Code%3A%0Ahttps%3A//github.com/dfki-av/AnonyNoise%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16440v1&entry.124074799=Read"},
{"title": "Word4Per: Zero-shot Composed Person Retrieval", "author": "Delong Liu and Haiwen Li and Zhicheng Zhao and Fei Su and Yuan Dong", "abstract": "  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n", "link": "http://arxiv.org/abs/2311.16515v3", "date": "2024-11-25", "relevancy": 2.5312, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Word4Per%3A%20Zero-shot%20Composed%20Person%20Retrieval&body=Title%3A%20Word4Per%3A%20Zero-shot%20Composed%20Person%20Retrieval%0AAuthor%3A%20Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%20and%20Yuan%20Dong%0AAbstract%3A%20%20%20Searching%20for%20specific%20person%20has%20great%20social%20benefits%20and%20security%20value%2C%0Aand%20it%20often%20involves%20a%20combination%20of%20visual%20and%20textual%20information.%0AConventional%20person%20retrieval%20methods%2C%20whether%20image-based%20or%20text-based%2C%0Ausually%20fall%20short%20in%20effectively%20harnessing%20both%20types%20of%20information%2C%20leading%0Ato%20the%20loss%20of%20accuracy.%20In%20this%20paper%2C%20a%20whole%20new%20task%20called%20Composed%20Person%0ARetrieval%20%28CPR%29%20is%20proposed%20to%20jointly%20utilize%20both%20image%20and%20text%20information%0Afor%20target%20person%20retrieval.%20However%2C%20the%20supervised%20CPR%20requires%20very%20costly%0Amanual%20annotation%20dataset%2C%20while%20there%20are%20currently%20no%20available%20resources.%20To%0Amitigate%20this%20issue%2C%20we%20firstly%20introduce%20the%20Zero-shot%20Composed%20Person%0ARetrieval%20%28ZS-CPR%29%2C%20which%20leverages%20existing%20domain-related%20data%20to%20resolve%20the%0ACPR%20problem%20without%20expensive%20annotations.%20Secondly%2C%20to%20learn%20ZS-CPR%20model%2C%20we%0Apropose%20a%20two-stage%20learning%20framework%2C%20Word4Per%2C%20where%20a%20lightweight%20Textual%0AInversion%20Network%20%28TINet%29%20and%20a%20text-based%20person%20retrieval%20model%20based%20on%0Afine-tuned%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20network%20are%20learned%0Awithout%20utilizing%20any%20CPR%20data.%20Thirdly%2C%20a%20finely%20annotated%20Image-Text%20Composed%0APerson%20Retrieval%20%28ITCPR%29%20dataset%20is%20built%20as%20the%20benchmark%20to%20assess%20the%0Aperformance%20of%20the%20proposed%20Word4Per%20framework.%20Extensive%20experiments%20under%0Aboth%20Rank-1%20and%20mAP%20demonstrate%20the%20effectiveness%20of%20Word4Per%20for%20the%20ZS-CPR%0Atask%2C%20surpassing%20the%20comparative%20methods%20by%20over%2010%5C%25.%20The%20code%20and%20ITCPR%0Adataset%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/Delong-liu-bupt/Word4Per.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16515v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWord4Per%253A%2520Zero-shot%2520Composed%2520Person%2520Retrieval%26entry.906535625%3DDelong%2520Liu%2520and%2520Haiwen%2520Li%2520and%2520Zhicheng%2520Zhao%2520and%2520Fei%2520Su%2520and%2520Yuan%2520Dong%26entry.1292438233%3D%2520%2520Searching%2520for%2520specific%2520person%2520has%2520great%2520social%2520benefits%2520and%2520security%2520value%252C%250Aand%2520it%2520often%2520involves%2520a%2520combination%2520of%2520visual%2520and%2520textual%2520information.%250AConventional%2520person%2520retrieval%2520methods%252C%2520whether%2520image-based%2520or%2520text-based%252C%250Ausually%2520fall%2520short%2520in%2520effectively%2520harnessing%2520both%2520types%2520of%2520information%252C%2520leading%250Ato%2520the%2520loss%2520of%2520accuracy.%2520In%2520this%2520paper%252C%2520a%2520whole%2520new%2520task%2520called%2520Composed%2520Person%250ARetrieval%2520%2528CPR%2529%2520is%2520proposed%2520to%2520jointly%2520utilize%2520both%2520image%2520and%2520text%2520information%250Afor%2520target%2520person%2520retrieval.%2520However%252C%2520the%2520supervised%2520CPR%2520requires%2520very%2520costly%250Amanual%2520annotation%2520dataset%252C%2520while%2520there%2520are%2520currently%2520no%2520available%2520resources.%2520To%250Amitigate%2520this%2520issue%252C%2520we%2520firstly%2520introduce%2520the%2520Zero-shot%2520Composed%2520Person%250ARetrieval%2520%2528ZS-CPR%2529%252C%2520which%2520leverages%2520existing%2520domain-related%2520data%2520to%2520resolve%2520the%250ACPR%2520problem%2520without%2520expensive%2520annotations.%2520Secondly%252C%2520to%2520learn%2520ZS-CPR%2520model%252C%2520we%250Apropose%2520a%2520two-stage%2520learning%2520framework%252C%2520Word4Per%252C%2520where%2520a%2520lightweight%2520Textual%250AInversion%2520Network%2520%2528TINet%2529%2520and%2520a%2520text-based%2520person%2520retrieval%2520model%2520based%2520on%250Afine-tuned%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520network%2520are%2520learned%250Awithout%2520utilizing%2520any%2520CPR%2520data.%2520Thirdly%252C%2520a%2520finely%2520annotated%2520Image-Text%2520Composed%250APerson%2520Retrieval%2520%2528ITCPR%2529%2520dataset%2520is%2520built%2520as%2520the%2520benchmark%2520to%2520assess%2520the%250Aperformance%2520of%2520the%2520proposed%2520Word4Per%2520framework.%2520Extensive%2520experiments%2520under%250Aboth%2520Rank-1%2520and%2520mAP%2520demonstrate%2520the%2520effectiveness%2520of%2520Word4Per%2520for%2520the%2520ZS-CPR%250Atask%252C%2520surpassing%2520the%2520comparative%2520methods%2520by%2520over%252010%255C%2525.%2520The%2520code%2520and%2520ITCPR%250Adataset%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Delong-liu-bupt/Word4Per.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16515v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Word4Per%3A%20Zero-shot%20Composed%20Person%20Retrieval&entry.906535625=Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%20and%20Yuan%20Dong&entry.1292438233=%20%20Searching%20for%20specific%20person%20has%20great%20social%20benefits%20and%20security%20value%2C%0Aand%20it%20often%20involves%20a%20combination%20of%20visual%20and%20textual%20information.%0AConventional%20person%20retrieval%20methods%2C%20whether%20image-based%20or%20text-based%2C%0Ausually%20fall%20short%20in%20effectively%20harnessing%20both%20types%20of%20information%2C%20leading%0Ato%20the%20loss%20of%20accuracy.%20In%20this%20paper%2C%20a%20whole%20new%20task%20called%20Composed%20Person%0ARetrieval%20%28CPR%29%20is%20proposed%20to%20jointly%20utilize%20both%20image%20and%20text%20information%0Afor%20target%20person%20retrieval.%20However%2C%20the%20supervised%20CPR%20requires%20very%20costly%0Amanual%20annotation%20dataset%2C%20while%20there%20are%20currently%20no%20available%20resources.%20To%0Amitigate%20this%20issue%2C%20we%20firstly%20introduce%20the%20Zero-shot%20Composed%20Person%0ARetrieval%20%28ZS-CPR%29%2C%20which%20leverages%20existing%20domain-related%20data%20to%20resolve%20the%0ACPR%20problem%20without%20expensive%20annotations.%20Secondly%2C%20to%20learn%20ZS-CPR%20model%2C%20we%0Apropose%20a%20two-stage%20learning%20framework%2C%20Word4Per%2C%20where%20a%20lightweight%20Textual%0AInversion%20Network%20%28TINet%29%20and%20a%20text-based%20person%20retrieval%20model%20based%20on%0Afine-tuned%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20network%20are%20learned%0Awithout%20utilizing%20any%20CPR%20data.%20Thirdly%2C%20a%20finely%20annotated%20Image-Text%20Composed%0APerson%20Retrieval%20%28ITCPR%29%20dataset%20is%20built%20as%20the%20benchmark%20to%20assess%20the%0Aperformance%20of%20the%20proposed%20Word4Per%20framework.%20Extensive%20experiments%20under%0Aboth%20Rank-1%20and%20mAP%20demonstrate%20the%20effectiveness%20of%20Word4Per%20for%20the%20ZS-CPR%0Atask%2C%20surpassing%20the%20comparative%20methods%20by%20over%2010%5C%25.%20The%20code%20and%20ITCPR%0Adataset%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/Delong-liu-bupt/Word4Per.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16515v3&entry.124074799=Read"},
{"title": "Utilizing Uncertainty in 2D Pose Detectors for Probabilistic 3D Human\n  Mesh Recovery", "author": "Tom Wehrbein and Marco Rudolph and Bodo Rosenhahn and Bastian Wandt", "abstract": "  Monocular 3D human pose and shape estimation is an inherently ill-posed\nproblem due to depth ambiguities, occlusions, and truncations. Recent\nprobabilistic approaches learn a distribution over plausible 3D human meshes by\nmaximizing the likelihood of the ground-truth pose given an image. We show that\nthis objective function alone is not sufficient to best capture the full\ndistributions. Instead, we propose to additionally supervise the learned\ndistributions by minimizing the distance to distributions encoded in heatmaps\nof a 2D pose detector. Moreover, we reveal that current methods often generate\nincorrect hypotheses for invisible joints which is not detected by the\nevaluation protocols. We demonstrate that person segmentation masks can be\nutilized during training to significantly decrease the number of invalid\nsamples and introduce two metrics to evaluate it. Our normalizing flow-based\napproach predicts plausible 3D human mesh hypotheses that are consistent with\nthe image evidence while maintaining high diversity for ambiguous body parts.\nExperiments on 3DPW and EMDB show that we outperform other state-of-the-art\nprobabilistic methods. Code is available for research purposes at\nhttps://github.com/twehrbein/humr.\n", "link": "http://arxiv.org/abs/2411.16289v1", "date": "2024-11-25", "relevancy": 2.5138, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.658}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6418}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Uncertainty%20in%202D%20Pose%20Detectors%20for%20Probabilistic%203D%20Human%0A%20%20Mesh%20Recovery&body=Title%3A%20Utilizing%20Uncertainty%20in%202D%20Pose%20Detectors%20for%20Probabilistic%203D%20Human%0A%20%20Mesh%20Recovery%0AAuthor%3A%20Tom%20Wehrbein%20and%20Marco%20Rudolph%20and%20Bodo%20Rosenhahn%20and%20Bastian%20Wandt%0AAbstract%3A%20%20%20Monocular%203D%20human%20pose%20and%20shape%20estimation%20is%20an%20inherently%20ill-posed%0Aproblem%20due%20to%20depth%20ambiguities%2C%20occlusions%2C%20and%20truncations.%20Recent%0Aprobabilistic%20approaches%20learn%20a%20distribution%20over%20plausible%203D%20human%20meshes%20by%0Amaximizing%20the%20likelihood%20of%20the%20ground-truth%20pose%20given%20an%20image.%20We%20show%20that%0Athis%20objective%20function%20alone%20is%20not%20sufficient%20to%20best%20capture%20the%20full%0Adistributions.%20Instead%2C%20we%20propose%20to%20additionally%20supervise%20the%20learned%0Adistributions%20by%20minimizing%20the%20distance%20to%20distributions%20encoded%20in%20heatmaps%0Aof%20a%202D%20pose%20detector.%20Moreover%2C%20we%20reveal%20that%20current%20methods%20often%20generate%0Aincorrect%20hypotheses%20for%20invisible%20joints%20which%20is%20not%20detected%20by%20the%0Aevaluation%20protocols.%20We%20demonstrate%20that%20person%20segmentation%20masks%20can%20be%0Autilized%20during%20training%20to%20significantly%20decrease%20the%20number%20of%20invalid%0Asamples%20and%20introduce%20two%20metrics%20to%20evaluate%20it.%20Our%20normalizing%20flow-based%0Aapproach%20predicts%20plausible%203D%20human%20mesh%20hypotheses%20that%20are%20consistent%20with%0Athe%20image%20evidence%20while%20maintaining%20high%20diversity%20for%20ambiguous%20body%20parts.%0AExperiments%20on%203DPW%20and%20EMDB%20show%20that%20we%20outperform%20other%20state-of-the-art%0Aprobabilistic%20methods.%20Code%20is%20available%20for%20research%20purposes%20at%0Ahttps%3A//github.com/twehrbein/humr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520Uncertainty%2520in%25202D%2520Pose%2520Detectors%2520for%2520Probabilistic%25203D%2520Human%250A%2520%2520Mesh%2520Recovery%26entry.906535625%3DTom%2520Wehrbein%2520and%2520Marco%2520Rudolph%2520and%2520Bodo%2520Rosenhahn%2520and%2520Bastian%2520Wandt%26entry.1292438233%3D%2520%2520Monocular%25203D%2520human%2520pose%2520and%2520shape%2520estimation%2520is%2520an%2520inherently%2520ill-posed%250Aproblem%2520due%2520to%2520depth%2520ambiguities%252C%2520occlusions%252C%2520and%2520truncations.%2520Recent%250Aprobabilistic%2520approaches%2520learn%2520a%2520distribution%2520over%2520plausible%25203D%2520human%2520meshes%2520by%250Amaximizing%2520the%2520likelihood%2520of%2520the%2520ground-truth%2520pose%2520given%2520an%2520image.%2520We%2520show%2520that%250Athis%2520objective%2520function%2520alone%2520is%2520not%2520sufficient%2520to%2520best%2520capture%2520the%2520full%250Adistributions.%2520Instead%252C%2520we%2520propose%2520to%2520additionally%2520supervise%2520the%2520learned%250Adistributions%2520by%2520minimizing%2520the%2520distance%2520to%2520distributions%2520encoded%2520in%2520heatmaps%250Aof%2520a%25202D%2520pose%2520detector.%2520Moreover%252C%2520we%2520reveal%2520that%2520current%2520methods%2520often%2520generate%250Aincorrect%2520hypotheses%2520for%2520invisible%2520joints%2520which%2520is%2520not%2520detected%2520by%2520the%250Aevaluation%2520protocols.%2520We%2520demonstrate%2520that%2520person%2520segmentation%2520masks%2520can%2520be%250Autilized%2520during%2520training%2520to%2520significantly%2520decrease%2520the%2520number%2520of%2520invalid%250Asamples%2520and%2520introduce%2520two%2520metrics%2520to%2520evaluate%2520it.%2520Our%2520normalizing%2520flow-based%250Aapproach%2520predicts%2520plausible%25203D%2520human%2520mesh%2520hypotheses%2520that%2520are%2520consistent%2520with%250Athe%2520image%2520evidence%2520while%2520maintaining%2520high%2520diversity%2520for%2520ambiguous%2520body%2520parts.%250AExperiments%2520on%25203DPW%2520and%2520EMDB%2520show%2520that%2520we%2520outperform%2520other%2520state-of-the-art%250Aprobabilistic%2520methods.%2520Code%2520is%2520available%2520for%2520research%2520purposes%2520at%250Ahttps%253A//github.com/twehrbein/humr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Uncertainty%20in%202D%20Pose%20Detectors%20for%20Probabilistic%203D%20Human%0A%20%20Mesh%20Recovery&entry.906535625=Tom%20Wehrbein%20and%20Marco%20Rudolph%20and%20Bodo%20Rosenhahn%20and%20Bastian%20Wandt&entry.1292438233=%20%20Monocular%203D%20human%20pose%20and%20shape%20estimation%20is%20an%20inherently%20ill-posed%0Aproblem%20due%20to%20depth%20ambiguities%2C%20occlusions%2C%20and%20truncations.%20Recent%0Aprobabilistic%20approaches%20learn%20a%20distribution%20over%20plausible%203D%20human%20meshes%20by%0Amaximizing%20the%20likelihood%20of%20the%20ground-truth%20pose%20given%20an%20image.%20We%20show%20that%0Athis%20objective%20function%20alone%20is%20not%20sufficient%20to%20best%20capture%20the%20full%0Adistributions.%20Instead%2C%20we%20propose%20to%20additionally%20supervise%20the%20learned%0Adistributions%20by%20minimizing%20the%20distance%20to%20distributions%20encoded%20in%20heatmaps%0Aof%20a%202D%20pose%20detector.%20Moreover%2C%20we%20reveal%20that%20current%20methods%20often%20generate%0Aincorrect%20hypotheses%20for%20invisible%20joints%20which%20is%20not%20detected%20by%20the%0Aevaluation%20protocols.%20We%20demonstrate%20that%20person%20segmentation%20masks%20can%20be%0Autilized%20during%20training%20to%20significantly%20decrease%20the%20number%20of%20invalid%0Asamples%20and%20introduce%20two%20metrics%20to%20evaluate%20it.%20Our%20normalizing%20flow-based%0Aapproach%20predicts%20plausible%203D%20human%20mesh%20hypotheses%20that%20are%20consistent%20with%0Athe%20image%20evidence%20while%20maintaining%20high%20diversity%20for%20ambiguous%20body%20parts.%0AExperiments%20on%203DPW%20and%20EMDB%20show%20that%20we%20outperform%20other%20state-of-the-art%0Aprobabilistic%20methods.%20Code%20is%20available%20for%20research%20purposes%20at%0Ahttps%3A//github.com/twehrbein/humr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16289v1&entry.124074799=Read"},
{"title": "Interpreting Language Reward Models via Contrastive Explanations", "author": "Junqi Jiang and Tom Bewley and Saumitra Mishra and Freddy Lecue and Manuela Veloso", "abstract": "  Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment.\n", "link": "http://arxiv.org/abs/2411.16502v1", "date": "2024-11-25", "relevancy": 2.506, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Language%20Reward%20Models%20via%20Contrastive%20Explanations&body=Title%3A%20Interpreting%20Language%20Reward%20Models%20via%20Contrastive%20Explanations%0AAuthor%3A%20Junqi%20Jiang%20and%20Tom%20Bewley%20and%20Saumitra%20Mishra%20and%20Freddy%20Lecue%20and%20Manuela%20Veloso%0AAbstract%3A%20%20%20Reward%20models%20%28RMs%29%20are%20a%20crucial%20component%20in%20the%20alignment%20of%20large%0Alanguage%20models%27%20%28LLMs%29%20outputs%20with%20human%20values.%20RMs%20approximate%20human%0Apreferences%20over%20possible%20LLM%20responses%20to%20the%20same%20prompt%20by%20predicting%20and%0Acomparing%20reward%20scores.%20However%2C%20as%20they%20are%20typically%20modified%20versions%20of%0ALLMs%20with%20scalar%20output%20heads%2C%20RMs%20are%20large%20black%20boxes%20whose%20predictions%20are%0Anot%20explainable.%20More%20transparent%20RMs%20would%20enable%20improved%20trust%20in%20the%0Aalignment%20of%20LLMs.%20In%20this%20work%2C%20we%20propose%20to%20use%20contrastive%20explanations%20to%0Aexplain%20any%20binary%20response%20comparison%20made%20by%20an%20RM.%20Specifically%2C%20we%20generate%0Aa%20diverse%20set%20of%20new%20comparisons%20similar%20to%20the%20original%20one%20to%20characterise%0Athe%20RM%27s%20local%20behaviour.%20The%20perturbed%20responses%20forming%20the%20new%20comparisons%0Aare%20generated%20to%20explicitly%20modify%20manually%20specified%20high-level%20evaluation%0Aattributes%2C%20on%20which%20analyses%20of%20RM%20behaviour%20are%20grounded.%20In%20quantitative%0Aexperiments%2C%20we%20validate%20the%20effectiveness%20of%20our%20method%20for%20finding%0Ahigh-quality%20contrastive%20explanations.%20We%20then%20showcase%20the%20qualitative%0Ausefulness%20of%20our%20method%20for%20investigating%20global%20sensitivity%20of%20RMs%20to%20each%0Aevaluation%20attribute%2C%20and%20demonstrate%20how%20representative%20examples%20can%20be%0Aautomatically%20extracted%20to%20explain%20and%20compare%20behaviours%20of%20different%20RMs.%20We%0Asee%20our%20method%20as%20a%20flexible%20framework%20for%20RM%20explanation%2C%20providing%20a%20basis%0Afor%20more%20interpretable%20and%20trustworthy%20LLM%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Language%2520Reward%2520Models%2520via%2520Contrastive%2520Explanations%26entry.906535625%3DJunqi%2520Jiang%2520and%2520Tom%2520Bewley%2520and%2520Saumitra%2520Mishra%2520and%2520Freddy%2520Lecue%2520and%2520Manuela%2520Veloso%26entry.1292438233%3D%2520%2520Reward%2520models%2520%2528RMs%2529%2520are%2520a%2520crucial%2520component%2520in%2520the%2520alignment%2520of%2520large%250Alanguage%2520models%2527%2520%2528LLMs%2529%2520outputs%2520with%2520human%2520values.%2520RMs%2520approximate%2520human%250Apreferences%2520over%2520possible%2520LLM%2520responses%2520to%2520the%2520same%2520prompt%2520by%2520predicting%2520and%250Acomparing%2520reward%2520scores.%2520However%252C%2520as%2520they%2520are%2520typically%2520modified%2520versions%2520of%250ALLMs%2520with%2520scalar%2520output%2520heads%252C%2520RMs%2520are%2520large%2520black%2520boxes%2520whose%2520predictions%2520are%250Anot%2520explainable.%2520More%2520transparent%2520RMs%2520would%2520enable%2520improved%2520trust%2520in%2520the%250Aalignment%2520of%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520use%2520contrastive%2520explanations%2520to%250Aexplain%2520any%2520binary%2520response%2520comparison%2520made%2520by%2520an%2520RM.%2520Specifically%252C%2520we%2520generate%250Aa%2520diverse%2520set%2520of%2520new%2520comparisons%2520similar%2520to%2520the%2520original%2520one%2520to%2520characterise%250Athe%2520RM%2527s%2520local%2520behaviour.%2520The%2520perturbed%2520responses%2520forming%2520the%2520new%2520comparisons%250Aare%2520generated%2520to%2520explicitly%2520modify%2520manually%2520specified%2520high-level%2520evaluation%250Aattributes%252C%2520on%2520which%2520analyses%2520of%2520RM%2520behaviour%2520are%2520grounded.%2520In%2520quantitative%250Aexperiments%252C%2520we%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520for%2520finding%250Ahigh-quality%2520contrastive%2520explanations.%2520We%2520then%2520showcase%2520the%2520qualitative%250Ausefulness%2520of%2520our%2520method%2520for%2520investigating%2520global%2520sensitivity%2520of%2520RMs%2520to%2520each%250Aevaluation%2520attribute%252C%2520and%2520demonstrate%2520how%2520representative%2520examples%2520can%2520be%250Aautomatically%2520extracted%2520to%2520explain%2520and%2520compare%2520behaviours%2520of%2520different%2520RMs.%2520We%250Asee%2520our%2520method%2520as%2520a%2520flexible%2520framework%2520for%2520RM%2520explanation%252C%2520providing%2520a%2520basis%250Afor%2520more%2520interpretable%2520and%2520trustworthy%2520LLM%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Language%20Reward%20Models%20via%20Contrastive%20Explanations&entry.906535625=Junqi%20Jiang%20and%20Tom%20Bewley%20and%20Saumitra%20Mishra%20and%20Freddy%20Lecue%20and%20Manuela%20Veloso&entry.1292438233=%20%20Reward%20models%20%28RMs%29%20are%20a%20crucial%20component%20in%20the%20alignment%20of%20large%0Alanguage%20models%27%20%28LLMs%29%20outputs%20with%20human%20values.%20RMs%20approximate%20human%0Apreferences%20over%20possible%20LLM%20responses%20to%20the%20same%20prompt%20by%20predicting%20and%0Acomparing%20reward%20scores.%20However%2C%20as%20they%20are%20typically%20modified%20versions%20of%0ALLMs%20with%20scalar%20output%20heads%2C%20RMs%20are%20large%20black%20boxes%20whose%20predictions%20are%0Anot%20explainable.%20More%20transparent%20RMs%20would%20enable%20improved%20trust%20in%20the%0Aalignment%20of%20LLMs.%20In%20this%20work%2C%20we%20propose%20to%20use%20contrastive%20explanations%20to%0Aexplain%20any%20binary%20response%20comparison%20made%20by%20an%20RM.%20Specifically%2C%20we%20generate%0Aa%20diverse%20set%20of%20new%20comparisons%20similar%20to%20the%20original%20one%20to%20characterise%0Athe%20RM%27s%20local%20behaviour.%20The%20perturbed%20responses%20forming%20the%20new%20comparisons%0Aare%20generated%20to%20explicitly%20modify%20manually%20specified%20high-level%20evaluation%0Aattributes%2C%20on%20which%20analyses%20of%20RM%20behaviour%20are%20grounded.%20In%20quantitative%0Aexperiments%2C%20we%20validate%20the%20effectiveness%20of%20our%20method%20for%20finding%0Ahigh-quality%20contrastive%20explanations.%20We%20then%20showcase%20the%20qualitative%0Ausefulness%20of%20our%20method%20for%20investigating%20global%20sensitivity%20of%20RMs%20to%20each%0Aevaluation%20attribute%2C%20and%20demonstrate%20how%20representative%20examples%20can%20be%0Aautomatically%20extracted%20to%20explain%20and%20compare%20behaviours%20of%20different%20RMs.%20We%0Asee%20our%20method%20as%20a%20flexible%20framework%20for%20RM%20explanation%2C%20providing%20a%20basis%0Afor%20more%20interpretable%20and%20trustworthy%20LLM%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16502v1&entry.124074799=Read"},
{"title": "Continual Deep Reinforcement Learning with Task-Agnostic Policy\n  Distillation", "author": "Muhammad Burhan Hafez and Kerim Erekmen", "abstract": "  Central to the development of universal learning systems is the ability to\nsolve multiple tasks without retraining from scratch when new data arrives.\nThis is crucial because each task requires significant training time.\nAddressing the problem of continual learning necessitates various methods due\nto the complexity of the problem space. This problem space includes: (1)\naddressing catastrophic forgetting to retain previously learned tasks, (2)\ndemonstrating positive forward transfer for faster learning, (3) ensuring\nscalability across numerous tasks, and (4) facilitating learning without\nrequiring task labels, even in the absence of clear task boundaries. In this\npaper, the Task-Agnostic Policy Distillation (TAPD) framework is introduced.\nThis framework alleviates problems (1)-(4) by incorporating a task-agnostic\nphase, where an agent explores its environment without any external goal and\nmaximizes only its intrinsic motivation. The knowledge gained during this phase\nis later distilled for further exploration. Therefore, the agent acts in a\nself-supervised manner by systematically seeking novel states. By utilizing\ntask-agnostic distilled knowledge, the agent can solve downstream tasks more\nefficiently, leading to improved sample efficiency. Our code is available at\nthe repository: https://github.com/wabbajack1/TAPD.\n", "link": "http://arxiv.org/abs/2411.16532v1", "date": "2024-11-25", "relevancy": 2.499, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5225}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Deep%20Reinforcement%20Learning%20with%20Task-Agnostic%20Policy%0A%20%20Distillation&body=Title%3A%20Continual%20Deep%20Reinforcement%20Learning%20with%20Task-Agnostic%20Policy%0A%20%20Distillation%0AAuthor%3A%20Muhammad%20Burhan%20Hafez%20and%20Kerim%20Erekmen%0AAbstract%3A%20%20%20Central%20to%20the%20development%20of%20universal%20learning%20systems%20is%20the%20ability%20to%0Asolve%20multiple%20tasks%20without%20retraining%20from%20scratch%20when%20new%20data%20arrives.%0AThis%20is%20crucial%20because%20each%20task%20requires%20significant%20training%20time.%0AAddressing%20the%20problem%20of%20continual%20learning%20necessitates%20various%20methods%20due%0Ato%20the%20complexity%20of%20the%20problem%20space.%20This%20problem%20space%20includes%3A%20%281%29%0Aaddressing%20catastrophic%20forgetting%20to%20retain%20previously%20learned%20tasks%2C%20%282%29%0Ademonstrating%20positive%20forward%20transfer%20for%20faster%20learning%2C%20%283%29%20ensuring%0Ascalability%20across%20numerous%20tasks%2C%20and%20%284%29%20facilitating%20learning%20without%0Arequiring%20task%20labels%2C%20even%20in%20the%20absence%20of%20clear%20task%20boundaries.%20In%20this%0Apaper%2C%20the%20Task-Agnostic%20Policy%20Distillation%20%28TAPD%29%20framework%20is%20introduced.%0AThis%20framework%20alleviates%20problems%20%281%29-%284%29%20by%20incorporating%20a%20task-agnostic%0Aphase%2C%20where%20an%20agent%20explores%20its%20environment%20without%20any%20external%20goal%20and%0Amaximizes%20only%20its%20intrinsic%20motivation.%20The%20knowledge%20gained%20during%20this%20phase%0Ais%20later%20distilled%20for%20further%20exploration.%20Therefore%2C%20the%20agent%20acts%20in%20a%0Aself-supervised%20manner%20by%20systematically%20seeking%20novel%20states.%20By%20utilizing%0Atask-agnostic%20distilled%20knowledge%2C%20the%20agent%20can%20solve%20downstream%20tasks%20more%0Aefficiently%2C%20leading%20to%20improved%20sample%20efficiency.%20Our%20code%20is%20available%20at%0Athe%20repository%3A%20https%3A//github.com/wabbajack1/TAPD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Deep%2520Reinforcement%2520Learning%2520with%2520Task-Agnostic%2520Policy%250A%2520%2520Distillation%26entry.906535625%3DMuhammad%2520Burhan%2520Hafez%2520and%2520Kerim%2520Erekmen%26entry.1292438233%3D%2520%2520Central%2520to%2520the%2520development%2520of%2520universal%2520learning%2520systems%2520is%2520the%2520ability%2520to%250Asolve%2520multiple%2520tasks%2520without%2520retraining%2520from%2520scratch%2520when%2520new%2520data%2520arrives.%250AThis%2520is%2520crucial%2520because%2520each%2520task%2520requires%2520significant%2520training%2520time.%250AAddressing%2520the%2520problem%2520of%2520continual%2520learning%2520necessitates%2520various%2520methods%2520due%250Ato%2520the%2520complexity%2520of%2520the%2520problem%2520space.%2520This%2520problem%2520space%2520includes%253A%2520%25281%2529%250Aaddressing%2520catastrophic%2520forgetting%2520to%2520retain%2520previously%2520learned%2520tasks%252C%2520%25282%2529%250Ademonstrating%2520positive%2520forward%2520transfer%2520for%2520faster%2520learning%252C%2520%25283%2529%2520ensuring%250Ascalability%2520across%2520numerous%2520tasks%252C%2520and%2520%25284%2529%2520facilitating%2520learning%2520without%250Arequiring%2520task%2520labels%252C%2520even%2520in%2520the%2520absence%2520of%2520clear%2520task%2520boundaries.%2520In%2520this%250Apaper%252C%2520the%2520Task-Agnostic%2520Policy%2520Distillation%2520%2528TAPD%2529%2520framework%2520is%2520introduced.%250AThis%2520framework%2520alleviates%2520problems%2520%25281%2529-%25284%2529%2520by%2520incorporating%2520a%2520task-agnostic%250Aphase%252C%2520where%2520an%2520agent%2520explores%2520its%2520environment%2520without%2520any%2520external%2520goal%2520and%250Amaximizes%2520only%2520its%2520intrinsic%2520motivation.%2520The%2520knowledge%2520gained%2520during%2520this%2520phase%250Ais%2520later%2520distilled%2520for%2520further%2520exploration.%2520Therefore%252C%2520the%2520agent%2520acts%2520in%2520a%250Aself-supervised%2520manner%2520by%2520systematically%2520seeking%2520novel%2520states.%2520By%2520utilizing%250Atask-agnostic%2520distilled%2520knowledge%252C%2520the%2520agent%2520can%2520solve%2520downstream%2520tasks%2520more%250Aefficiently%252C%2520leading%2520to%2520improved%2520sample%2520efficiency.%2520Our%2520code%2520is%2520available%2520at%250Athe%2520repository%253A%2520https%253A//github.com/wabbajack1/TAPD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Deep%20Reinforcement%20Learning%20with%20Task-Agnostic%20Policy%0A%20%20Distillation&entry.906535625=Muhammad%20Burhan%20Hafez%20and%20Kerim%20Erekmen&entry.1292438233=%20%20Central%20to%20the%20development%20of%20universal%20learning%20systems%20is%20the%20ability%20to%0Asolve%20multiple%20tasks%20without%20retraining%20from%20scratch%20when%20new%20data%20arrives.%0AThis%20is%20crucial%20because%20each%20task%20requires%20significant%20training%20time.%0AAddressing%20the%20problem%20of%20continual%20learning%20necessitates%20various%20methods%20due%0Ato%20the%20complexity%20of%20the%20problem%20space.%20This%20problem%20space%20includes%3A%20%281%29%0Aaddressing%20catastrophic%20forgetting%20to%20retain%20previously%20learned%20tasks%2C%20%282%29%0Ademonstrating%20positive%20forward%20transfer%20for%20faster%20learning%2C%20%283%29%20ensuring%0Ascalability%20across%20numerous%20tasks%2C%20and%20%284%29%20facilitating%20learning%20without%0Arequiring%20task%20labels%2C%20even%20in%20the%20absence%20of%20clear%20task%20boundaries.%20In%20this%0Apaper%2C%20the%20Task-Agnostic%20Policy%20Distillation%20%28TAPD%29%20framework%20is%20introduced.%0AThis%20framework%20alleviates%20problems%20%281%29-%284%29%20by%20incorporating%20a%20task-agnostic%0Aphase%2C%20where%20an%20agent%20explores%20its%20environment%20without%20any%20external%20goal%20and%0Amaximizes%20only%20its%20intrinsic%20motivation.%20The%20knowledge%20gained%20during%20this%20phase%0Ais%20later%20distilled%20for%20further%20exploration.%20Therefore%2C%20the%20agent%20acts%20in%20a%0Aself-supervised%20manner%20by%20systematically%20seeking%20novel%20states.%20By%20utilizing%0Atask-agnostic%20distilled%20knowledge%2C%20the%20agent%20can%20solve%20downstream%20tasks%20more%0Aefficiently%2C%20leading%20to%20improved%20sample%20efficiency.%20Our%20code%20is%20available%20at%0Athe%20repository%3A%20https%3A//github.com/wabbajack1/TAPD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16532v1&entry.124074799=Read"},
{"title": "Harnessing Superclasses for Learning from Hierarchical Databases", "author": "Nicolas Urbani and Sylvain Rousseau and Yves Grandvalet and Leonardo Tanzi", "abstract": "  In many large-scale classification problems, classes are organized in a known\nhierarchy, typically represented as a tree expressing the inclusion of classes\nin superclasses. We introduce a loss for this type of supervised hierarchical\nclassification. It utilizes the knowledge of the hierarchy to assign each\nexample not only to a class but also to all encompassing superclasses.\nApplicable to any feedforward architecture with a softmax output layer, this\nloss is a proper scoring rule, in that its expectation is minimized by the true\nposterior class probabilities. This property allows us to simultaneously pursue\nconsistent classification objectives between superclasses and fine-grained\nclasses, and eliminates the need for a performance trade-off between different\ngranularities. We conduct an experimental study on three reference benchmarks,\nin which we vary the size of the training sets to cover a diverse set of\nlearning scenarios. Our approach does not entail any significant additional\ncomputational cost compared with the loss of cross-entropy. It improves\naccuracy and reduces the number of coarse errors, with predicted labels that\nare distant from ground-truth labels in the tree.\n", "link": "http://arxiv.org/abs/2411.16438v1", "date": "2024-11-25", "relevancy": 2.4933, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5182}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5067}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Superclasses%20for%20Learning%20from%20Hierarchical%20Databases&body=Title%3A%20Harnessing%20Superclasses%20for%20Learning%20from%20Hierarchical%20Databases%0AAuthor%3A%20Nicolas%20Urbani%20and%20Sylvain%20Rousseau%20and%20Yves%20Grandvalet%20and%20Leonardo%20Tanzi%0AAbstract%3A%20%20%20In%20many%20large-scale%20classification%20problems%2C%20classes%20are%20organized%20in%20a%20known%0Ahierarchy%2C%20typically%20represented%20as%20a%20tree%20expressing%20the%20inclusion%20of%20classes%0Ain%20superclasses.%20We%20introduce%20a%20loss%20for%20this%20type%20of%20supervised%20hierarchical%0Aclassification.%20It%20utilizes%20the%20knowledge%20of%20the%20hierarchy%20to%20assign%20each%0Aexample%20not%20only%20to%20a%20class%20but%20also%20to%20all%20encompassing%20superclasses.%0AApplicable%20to%20any%20feedforward%20architecture%20with%20a%20softmax%20output%20layer%2C%20this%0Aloss%20is%20a%20proper%20scoring%20rule%2C%20in%20that%20its%20expectation%20is%20minimized%20by%20the%20true%0Aposterior%20class%20probabilities.%20This%20property%20allows%20us%20to%20simultaneously%20pursue%0Aconsistent%20classification%20objectives%20between%20superclasses%20and%20fine-grained%0Aclasses%2C%20and%20eliminates%20the%20need%20for%20a%20performance%20trade-off%20between%20different%0Agranularities.%20We%20conduct%20an%20experimental%20study%20on%20three%20reference%20benchmarks%2C%0Ain%20which%20we%20vary%20the%20size%20of%20the%20training%20sets%20to%20cover%20a%20diverse%20set%20of%0Alearning%20scenarios.%20Our%20approach%20does%20not%20entail%20any%20significant%20additional%0Acomputational%20cost%20compared%20with%20the%20loss%20of%20cross-entropy.%20It%20improves%0Aaccuracy%20and%20reduces%20the%20number%20of%20coarse%20errors%2C%20with%20predicted%20labels%20that%0Aare%20distant%20from%20ground-truth%20labels%20in%20the%20tree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Superclasses%2520for%2520Learning%2520from%2520Hierarchical%2520Databases%26entry.906535625%3DNicolas%2520Urbani%2520and%2520Sylvain%2520Rousseau%2520and%2520Yves%2520Grandvalet%2520and%2520Leonardo%2520Tanzi%26entry.1292438233%3D%2520%2520In%2520many%2520large-scale%2520classification%2520problems%252C%2520classes%2520are%2520organized%2520in%2520a%2520known%250Ahierarchy%252C%2520typically%2520represented%2520as%2520a%2520tree%2520expressing%2520the%2520inclusion%2520of%2520classes%250Ain%2520superclasses.%2520We%2520introduce%2520a%2520loss%2520for%2520this%2520type%2520of%2520supervised%2520hierarchical%250Aclassification.%2520It%2520utilizes%2520the%2520knowledge%2520of%2520the%2520hierarchy%2520to%2520assign%2520each%250Aexample%2520not%2520only%2520to%2520a%2520class%2520but%2520also%2520to%2520all%2520encompassing%2520superclasses.%250AApplicable%2520to%2520any%2520feedforward%2520architecture%2520with%2520a%2520softmax%2520output%2520layer%252C%2520this%250Aloss%2520is%2520a%2520proper%2520scoring%2520rule%252C%2520in%2520that%2520its%2520expectation%2520is%2520minimized%2520by%2520the%2520true%250Aposterior%2520class%2520probabilities.%2520This%2520property%2520allows%2520us%2520to%2520simultaneously%2520pursue%250Aconsistent%2520classification%2520objectives%2520between%2520superclasses%2520and%2520fine-grained%250Aclasses%252C%2520and%2520eliminates%2520the%2520need%2520for%2520a%2520performance%2520trade-off%2520between%2520different%250Agranularities.%2520We%2520conduct%2520an%2520experimental%2520study%2520on%2520three%2520reference%2520benchmarks%252C%250Ain%2520which%2520we%2520vary%2520the%2520size%2520of%2520the%2520training%2520sets%2520to%2520cover%2520a%2520diverse%2520set%2520of%250Alearning%2520scenarios.%2520Our%2520approach%2520does%2520not%2520entail%2520any%2520significant%2520additional%250Acomputational%2520cost%2520compared%2520with%2520the%2520loss%2520of%2520cross-entropy.%2520It%2520improves%250Aaccuracy%2520and%2520reduces%2520the%2520number%2520of%2520coarse%2520errors%252C%2520with%2520predicted%2520labels%2520that%250Aare%2520distant%2520from%2520ground-truth%2520labels%2520in%2520the%2520tree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Superclasses%20for%20Learning%20from%20Hierarchical%20Databases&entry.906535625=Nicolas%20Urbani%20and%20Sylvain%20Rousseau%20and%20Yves%20Grandvalet%20and%20Leonardo%20Tanzi&entry.1292438233=%20%20In%20many%20large-scale%20classification%20problems%2C%20classes%20are%20organized%20in%20a%20known%0Ahierarchy%2C%20typically%20represented%20as%20a%20tree%20expressing%20the%20inclusion%20of%20classes%0Ain%20superclasses.%20We%20introduce%20a%20loss%20for%20this%20type%20of%20supervised%20hierarchical%0Aclassification.%20It%20utilizes%20the%20knowledge%20of%20the%20hierarchy%20to%20assign%20each%0Aexample%20not%20only%20to%20a%20class%20but%20also%20to%20all%20encompassing%20superclasses.%0AApplicable%20to%20any%20feedforward%20architecture%20with%20a%20softmax%20output%20layer%2C%20this%0Aloss%20is%20a%20proper%20scoring%20rule%2C%20in%20that%20its%20expectation%20is%20minimized%20by%20the%20true%0Aposterior%20class%20probabilities.%20This%20property%20allows%20us%20to%20simultaneously%20pursue%0Aconsistent%20classification%20objectives%20between%20superclasses%20and%20fine-grained%0Aclasses%2C%20and%20eliminates%20the%20need%20for%20a%20performance%20trade-off%20between%20different%0Agranularities.%20We%20conduct%20an%20experimental%20study%20on%20three%20reference%20benchmarks%2C%0Ain%20which%20we%20vary%20the%20size%20of%20the%20training%20sets%20to%20cover%20a%20diverse%20set%20of%0Alearning%20scenarios.%20Our%20approach%20does%20not%20entail%20any%20significant%20additional%0Acomputational%20cost%20compared%20with%20the%20loss%20of%20cross-entropy.%20It%20improves%0Aaccuracy%20and%20reduces%20the%20number%20of%20coarse%20errors%2C%20with%20predicted%20labels%20that%0Aare%20distant%20from%20ground-truth%20labels%20in%20the%20tree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16438v1&entry.124074799=Read"},
{"title": "A Unified Framework for Center-based Clustering of Distributed Data", "author": "Aleksandar Armacki and Dragana Bajovi\u0107 and Du\u0161an Jakoveti\u0107 and Soummya Kar", "abstract": "  We develop a family of distributed center-based clustering algorithms that\nwork over networks of users. In the proposed scenario, users contain a local\ndataset and communicate only with their immediate neighbours, with the aim of\nfinding a clustering of the full, joint data. The proposed family, termed\nDistributed Gradient Clustering (DGC-$\\mathcal{F}_\\rho$), is parametrized by\n$\\rho \\geq 1$, controling the proximity of users' center estimates, with\n$\\mathcal{F}$ determining the clustering loss. Our framework allows for a broad\nclass of smooth convex loss functions, including popular clustering losses like\n$K$-means and Huber loss. Specialized to popular clustering losses like\n$K$-means and Huber loss, DGC-$\\mathcal{F}_\\rho$ gives rise to novel\ndistributed clustering algorithms DGC-KM$_\\rho$ and DGC-HL$_\\rho$, while novel\nclustering losses based on Logistic and Fair functions lead to DGC-LL$_\\rho$\nand DGC-FL$_\\rho$. We provide a unified analysis and establish several strong\nresults, under mild assumptions. First, we show that the sequence of centers\ngenerated by the methods converges to a well-defined notion of fixed point,\nunder any center initialization and value of $\\rho$. Second, we prove that, as\n$\\rho$ increases, the family of fixed points produced by DGC-$\\mathcal{F}_\\rho$\nconverges to a notion of consensus fixed points. We show that consensus fixed\npoints of DGC-$\\mathcal{F}_{\\rho}$ are equivalent to fixed points of gradient\nclustering over the full data, guaranteeing a clustering of the full data is\nproduced. For the special case of Bregman losses, we show that our fixed points\nconverge to the set of Lloyd points. Extensive numerical experiments on\nsynthetic and real data confirm our theoretical findings, show strong\nperformance of our methods and demonstrate the usefulness and wide range of\npotential applications of our general framework, such as outlier detection.\n", "link": "http://arxiv.org/abs/2402.01302v2", "date": "2024-11-25", "relevancy": 2.4797, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5116}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5006}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Center-based%20Clustering%20of%20Distributed%20Data&body=Title%3A%20A%20Unified%20Framework%20for%20Center-based%20Clustering%20of%20Distributed%20Data%0AAuthor%3A%20Aleksandar%20Armacki%20and%20Dragana%20Bajovi%C4%87%20and%20Du%C5%A1an%20Jakoveti%C4%87%20and%20Soummya%20Kar%0AAbstract%3A%20%20%20We%20develop%20a%20family%20of%20distributed%20center-based%20clustering%20algorithms%20that%0Awork%20over%20networks%20of%20users.%20In%20the%20proposed%20scenario%2C%20users%20contain%20a%20local%0Adataset%20and%20communicate%20only%20with%20their%20immediate%20neighbours%2C%20with%20the%20aim%20of%0Afinding%20a%20clustering%20of%20the%20full%2C%20joint%20data.%20The%20proposed%20family%2C%20termed%0ADistributed%20Gradient%20Clustering%20%28DGC-%24%5Cmathcal%7BF%7D_%5Crho%24%29%2C%20is%20parametrized%20by%0A%24%5Crho%20%5Cgeq%201%24%2C%20controling%20the%20proximity%20of%20users%27%20center%20estimates%2C%20with%0A%24%5Cmathcal%7BF%7D%24%20determining%20the%20clustering%20loss.%20Our%20framework%20allows%20for%20a%20broad%0Aclass%20of%20smooth%20convex%20loss%20functions%2C%20including%20popular%20clustering%20losses%20like%0A%24K%24-means%20and%20Huber%20loss.%20Specialized%20to%20popular%20clustering%20losses%20like%0A%24K%24-means%20and%20Huber%20loss%2C%20DGC-%24%5Cmathcal%7BF%7D_%5Crho%24%20gives%20rise%20to%20novel%0Adistributed%20clustering%20algorithms%20DGC-KM%24_%5Crho%24%20and%20DGC-HL%24_%5Crho%24%2C%20while%20novel%0Aclustering%20losses%20based%20on%20Logistic%20and%20Fair%20functions%20lead%20to%20DGC-LL%24_%5Crho%24%0Aand%20DGC-FL%24_%5Crho%24.%20We%20provide%20a%20unified%20analysis%20and%20establish%20several%20strong%0Aresults%2C%20under%20mild%20assumptions.%20First%2C%20we%20show%20that%20the%20sequence%20of%20centers%0Agenerated%20by%20the%20methods%20converges%20to%20a%20well-defined%20notion%20of%20fixed%20point%2C%0Aunder%20any%20center%20initialization%20and%20value%20of%20%24%5Crho%24.%20Second%2C%20we%20prove%20that%2C%20as%0A%24%5Crho%24%20increases%2C%20the%20family%20of%20fixed%20points%20produced%20by%20DGC-%24%5Cmathcal%7BF%7D_%5Crho%24%0Aconverges%20to%20a%20notion%20of%20consensus%20fixed%20points.%20We%20show%20that%20consensus%20fixed%0Apoints%20of%20DGC-%24%5Cmathcal%7BF%7D_%7B%5Crho%7D%24%20are%20equivalent%20to%20fixed%20points%20of%20gradient%0Aclustering%20over%20the%20full%20data%2C%20guaranteeing%20a%20clustering%20of%20the%20full%20data%20is%0Aproduced.%20For%20the%20special%20case%20of%20Bregman%20losses%2C%20we%20show%20that%20our%20fixed%20points%0Aconverge%20to%20the%20set%20of%20Lloyd%20points.%20Extensive%20numerical%20experiments%20on%0Asynthetic%20and%20real%20data%20confirm%20our%20theoretical%20findings%2C%20show%20strong%0Aperformance%20of%20our%20methods%20and%20demonstrate%20the%20usefulness%20and%20wide%20range%20of%0Apotential%20applications%20of%20our%20general%20framework%2C%20such%20as%20outlier%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%2520Center-based%2520Clustering%2520of%2520Distributed%2520Data%26entry.906535625%3DAleksandar%2520Armacki%2520and%2520Dragana%2520Bajovi%25C4%2587%2520and%2520Du%25C5%25A1an%2520Jakoveti%25C4%2587%2520and%2520Soummya%2520Kar%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520family%2520of%2520distributed%2520center-based%2520clustering%2520algorithms%2520that%250Awork%2520over%2520networks%2520of%2520users.%2520In%2520the%2520proposed%2520scenario%252C%2520users%2520contain%2520a%2520local%250Adataset%2520and%2520communicate%2520only%2520with%2520their%2520immediate%2520neighbours%252C%2520with%2520the%2520aim%2520of%250Afinding%2520a%2520clustering%2520of%2520the%2520full%252C%2520joint%2520data.%2520The%2520proposed%2520family%252C%2520termed%250ADistributed%2520Gradient%2520Clustering%2520%2528DGC-%2524%255Cmathcal%257BF%257D_%255Crho%2524%2529%252C%2520is%2520parametrized%2520by%250A%2524%255Crho%2520%255Cgeq%25201%2524%252C%2520controling%2520the%2520proximity%2520of%2520users%2527%2520center%2520estimates%252C%2520with%250A%2524%255Cmathcal%257BF%257D%2524%2520determining%2520the%2520clustering%2520loss.%2520Our%2520framework%2520allows%2520for%2520a%2520broad%250Aclass%2520of%2520smooth%2520convex%2520loss%2520functions%252C%2520including%2520popular%2520clustering%2520losses%2520like%250A%2524K%2524-means%2520and%2520Huber%2520loss.%2520Specialized%2520to%2520popular%2520clustering%2520losses%2520like%250A%2524K%2524-means%2520and%2520Huber%2520loss%252C%2520DGC-%2524%255Cmathcal%257BF%257D_%255Crho%2524%2520gives%2520rise%2520to%2520novel%250Adistributed%2520clustering%2520algorithms%2520DGC-KM%2524_%255Crho%2524%2520and%2520DGC-HL%2524_%255Crho%2524%252C%2520while%2520novel%250Aclustering%2520losses%2520based%2520on%2520Logistic%2520and%2520Fair%2520functions%2520lead%2520to%2520DGC-LL%2524_%255Crho%2524%250Aand%2520DGC-FL%2524_%255Crho%2524.%2520We%2520provide%2520a%2520unified%2520analysis%2520and%2520establish%2520several%2520strong%250Aresults%252C%2520under%2520mild%2520assumptions.%2520First%252C%2520we%2520show%2520that%2520the%2520sequence%2520of%2520centers%250Agenerated%2520by%2520the%2520methods%2520converges%2520to%2520a%2520well-defined%2520notion%2520of%2520fixed%2520point%252C%250Aunder%2520any%2520center%2520initialization%2520and%2520value%2520of%2520%2524%255Crho%2524.%2520Second%252C%2520we%2520prove%2520that%252C%2520as%250A%2524%255Crho%2524%2520increases%252C%2520the%2520family%2520of%2520fixed%2520points%2520produced%2520by%2520DGC-%2524%255Cmathcal%257BF%257D_%255Crho%2524%250Aconverges%2520to%2520a%2520notion%2520of%2520consensus%2520fixed%2520points.%2520We%2520show%2520that%2520consensus%2520fixed%250Apoints%2520of%2520DGC-%2524%255Cmathcal%257BF%257D_%257B%255Crho%257D%2524%2520are%2520equivalent%2520to%2520fixed%2520points%2520of%2520gradient%250Aclustering%2520over%2520the%2520full%2520data%252C%2520guaranteeing%2520a%2520clustering%2520of%2520the%2520full%2520data%2520is%250Aproduced.%2520For%2520the%2520special%2520case%2520of%2520Bregman%2520losses%252C%2520we%2520show%2520that%2520our%2520fixed%2520points%250Aconverge%2520to%2520the%2520set%2520of%2520Lloyd%2520points.%2520Extensive%2520numerical%2520experiments%2520on%250Asynthetic%2520and%2520real%2520data%2520confirm%2520our%2520theoretical%2520findings%252C%2520show%2520strong%250Aperformance%2520of%2520our%2520methods%2520and%2520demonstrate%2520the%2520usefulness%2520and%2520wide%2520range%2520of%250Apotential%2520applications%2520of%2520our%2520general%2520framework%252C%2520such%2520as%2520outlier%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Center-based%20Clustering%20of%20Distributed%20Data&entry.906535625=Aleksandar%20Armacki%20and%20Dragana%20Bajovi%C4%87%20and%20Du%C5%A1an%20Jakoveti%C4%87%20and%20Soummya%20Kar&entry.1292438233=%20%20We%20develop%20a%20family%20of%20distributed%20center-based%20clustering%20algorithms%20that%0Awork%20over%20networks%20of%20users.%20In%20the%20proposed%20scenario%2C%20users%20contain%20a%20local%0Adataset%20and%20communicate%20only%20with%20their%20immediate%20neighbours%2C%20with%20the%20aim%20of%0Afinding%20a%20clustering%20of%20the%20full%2C%20joint%20data.%20The%20proposed%20family%2C%20termed%0ADistributed%20Gradient%20Clustering%20%28DGC-%24%5Cmathcal%7BF%7D_%5Crho%24%29%2C%20is%20parametrized%20by%0A%24%5Crho%20%5Cgeq%201%24%2C%20controling%20the%20proximity%20of%20users%27%20center%20estimates%2C%20with%0A%24%5Cmathcal%7BF%7D%24%20determining%20the%20clustering%20loss.%20Our%20framework%20allows%20for%20a%20broad%0Aclass%20of%20smooth%20convex%20loss%20functions%2C%20including%20popular%20clustering%20losses%20like%0A%24K%24-means%20and%20Huber%20loss.%20Specialized%20to%20popular%20clustering%20losses%20like%0A%24K%24-means%20and%20Huber%20loss%2C%20DGC-%24%5Cmathcal%7BF%7D_%5Crho%24%20gives%20rise%20to%20novel%0Adistributed%20clustering%20algorithms%20DGC-KM%24_%5Crho%24%20and%20DGC-HL%24_%5Crho%24%2C%20while%20novel%0Aclustering%20losses%20based%20on%20Logistic%20and%20Fair%20functions%20lead%20to%20DGC-LL%24_%5Crho%24%0Aand%20DGC-FL%24_%5Crho%24.%20We%20provide%20a%20unified%20analysis%20and%20establish%20several%20strong%0Aresults%2C%20under%20mild%20assumptions.%20First%2C%20we%20show%20that%20the%20sequence%20of%20centers%0Agenerated%20by%20the%20methods%20converges%20to%20a%20well-defined%20notion%20of%20fixed%20point%2C%0Aunder%20any%20center%20initialization%20and%20value%20of%20%24%5Crho%24.%20Second%2C%20we%20prove%20that%2C%20as%0A%24%5Crho%24%20increases%2C%20the%20family%20of%20fixed%20points%20produced%20by%20DGC-%24%5Cmathcal%7BF%7D_%5Crho%24%0Aconverges%20to%20a%20notion%20of%20consensus%20fixed%20points.%20We%20show%20that%20consensus%20fixed%0Apoints%20of%20DGC-%24%5Cmathcal%7BF%7D_%7B%5Crho%7D%24%20are%20equivalent%20to%20fixed%20points%20of%20gradient%0Aclustering%20over%20the%20full%20data%2C%20guaranteeing%20a%20clustering%20of%20the%20full%20data%20is%0Aproduced.%20For%20the%20special%20case%20of%20Bregman%20losses%2C%20we%20show%20that%20our%20fixed%20points%0Aconverge%20to%20the%20set%20of%20Lloyd%20points.%20Extensive%20numerical%20experiments%20on%0Asynthetic%20and%20real%20data%20confirm%20our%20theoretical%20findings%2C%20show%20strong%0Aperformance%20of%20our%20methods%20and%20demonstrate%20the%20usefulness%20and%20wide%20range%20of%0Apotential%20applications%20of%20our%20general%20framework%2C%20such%20as%20outlier%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01302v2&entry.124074799=Read"},
{"title": "DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for\n  Task-Oriented Manipulation", "author": "Qian Feng and David S. Martinez Lema and Mohammadhossein Malmir and Hang Li and Jianxiang Feng and Zhaopeng Chen and Alois Knoll", "abstract": "  We introduce DexGanGrasp, a dexterous grasping synthesis method that\ngenerates and evaluates grasps with single view in real time. DexGanGrasp\ncomprises a Conditional Generative Adversarial Networks (cGANs)-based\nDexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor\nto assess the stability of these grasps. Extensive simulation and real-world\nexpriments showcases the effectiveness of our proposed method, outperforming\nthe baseline FFHNet with an 18.57% higher success rate in real-world\nevaluation. We further extend DexGanGrasp to DexAfford-Prompt, an\nopen-vocabulary affordance grounding pipeline for dexterous grasping leveraging\nMultimodal Large Language Models (MLLMs) and Vision Language Models (VLMs), to\nachieve task-oriented grasping with successful real-world deployments.\n", "link": "http://arxiv.org/abs/2407.17348v2", "date": "2024-11-25", "relevancy": 2.4773, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7378}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5723}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexGANGrasp%3A%20Dexterous%20Generative%20Adversarial%20Grasping%20Synthesis%20for%0A%20%20Task-Oriented%20Manipulation&body=Title%3A%20DexGANGrasp%3A%20Dexterous%20Generative%20Adversarial%20Grasping%20Synthesis%20for%0A%20%20Task-Oriented%20Manipulation%0AAuthor%3A%20Qian%20Feng%20and%20David%20S.%20Martinez%20Lema%20and%20Mohammadhossein%20Malmir%20and%20Hang%20Li%20and%20Jianxiang%20Feng%20and%20Zhaopeng%20Chen%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20We%20introduce%20DexGanGrasp%2C%20a%20dexterous%20grasping%20synthesis%20method%20that%0Agenerates%20and%20evaluates%20grasps%20with%20single%20view%20in%20real%20time.%20DexGanGrasp%0Acomprises%20a%20Conditional%20Generative%20Adversarial%20Networks%20%28cGANs%29-based%0ADexGenerator%20to%20generate%20dexterous%20grasps%20and%20a%20discriminator-like%20DexEvalautor%0Ato%20assess%20the%20stability%20of%20these%20grasps.%20Extensive%20simulation%20and%20real-world%0Aexpriments%20showcases%20the%20effectiveness%20of%20our%20proposed%20method%2C%20outperforming%0Athe%20baseline%20FFHNet%20with%20an%2018.57%25%20higher%20success%20rate%20in%20real-world%0Aevaluation.%20We%20further%20extend%20DexGanGrasp%20to%20DexAfford-Prompt%2C%20an%0Aopen-vocabulary%20affordance%20grounding%20pipeline%20for%20dexterous%20grasping%20leveraging%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20Vision%20Language%20Models%20%28VLMs%29%2C%20to%0Aachieve%20task-oriented%20grasping%20with%20successful%20real-world%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17348v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexGANGrasp%253A%2520Dexterous%2520Generative%2520Adversarial%2520Grasping%2520Synthesis%2520for%250A%2520%2520Task-Oriented%2520Manipulation%26entry.906535625%3DQian%2520Feng%2520and%2520David%2520S.%2520Martinez%2520Lema%2520and%2520Mohammadhossein%2520Malmir%2520and%2520Hang%2520Li%2520and%2520Jianxiang%2520Feng%2520and%2520Zhaopeng%2520Chen%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520We%2520introduce%2520DexGanGrasp%252C%2520a%2520dexterous%2520grasping%2520synthesis%2520method%2520that%250Agenerates%2520and%2520evaluates%2520grasps%2520with%2520single%2520view%2520in%2520real%2520time.%2520DexGanGrasp%250Acomprises%2520a%2520Conditional%2520Generative%2520Adversarial%2520Networks%2520%2528cGANs%2529-based%250ADexGenerator%2520to%2520generate%2520dexterous%2520grasps%2520and%2520a%2520discriminator-like%2520DexEvalautor%250Ato%2520assess%2520the%2520stability%2520of%2520these%2520grasps.%2520Extensive%2520simulation%2520and%2520real-world%250Aexpriments%2520showcases%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%252C%2520outperforming%250Athe%2520baseline%2520FFHNet%2520with%2520an%252018.57%2525%2520higher%2520success%2520rate%2520in%2520real-world%250Aevaluation.%2520We%2520further%2520extend%2520DexGanGrasp%2520to%2520DexAfford-Prompt%252C%2520an%250Aopen-vocabulary%2520affordance%2520grounding%2520pipeline%2520for%2520dexterous%2520grasping%2520leveraging%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520and%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520to%250Aachieve%2520task-oriented%2520grasping%2520with%2520successful%2520real-world%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17348v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexGANGrasp%3A%20Dexterous%20Generative%20Adversarial%20Grasping%20Synthesis%20for%0A%20%20Task-Oriented%20Manipulation&entry.906535625=Qian%20Feng%20and%20David%20S.%20Martinez%20Lema%20and%20Mohammadhossein%20Malmir%20and%20Hang%20Li%20and%20Jianxiang%20Feng%20and%20Zhaopeng%20Chen%20and%20Alois%20Knoll&entry.1292438233=%20%20We%20introduce%20DexGanGrasp%2C%20a%20dexterous%20grasping%20synthesis%20method%20that%0Agenerates%20and%20evaluates%20grasps%20with%20single%20view%20in%20real%20time.%20DexGanGrasp%0Acomprises%20a%20Conditional%20Generative%20Adversarial%20Networks%20%28cGANs%29-based%0ADexGenerator%20to%20generate%20dexterous%20grasps%20and%20a%20discriminator-like%20DexEvalautor%0Ato%20assess%20the%20stability%20of%20these%20grasps.%20Extensive%20simulation%20and%20real-world%0Aexpriments%20showcases%20the%20effectiveness%20of%20our%20proposed%20method%2C%20outperforming%0Athe%20baseline%20FFHNet%20with%20an%2018.57%25%20higher%20success%20rate%20in%20real-world%0Aevaluation.%20We%20further%20extend%20DexGanGrasp%20to%20DexAfford-Prompt%2C%20an%0Aopen-vocabulary%20affordance%20grounding%20pipeline%20for%20dexterous%20grasping%20leveraging%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20and%20Vision%20Language%20Models%20%28VLMs%29%2C%20to%0Aachieve%20task-oriented%20grasping%20with%20successful%20real-world%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17348v2&entry.124074799=Read"},
{"title": "Low-Data Classification of Historical Music Manuscripts: A Few-Shot\n  Learning Approach", "author": "Elona Shatri and Daniel Raymond and George Fazekas", "abstract": "  In this paper, we explore the intersection of technology and cultural\npreservation by developing a self-supervised learning framework for the\nclassification of musical symbols in historical manuscripts. Optical Music\nRecognition (OMR) plays a vital role in digitising and preserving musical\nheritage, but historical documents often lack the labelled data required by\ntraditional methods. We overcome this challenge by training a neural-based\nfeature extractor on unlabelled data, enabling effective classification with\nminimal samples. Key contributions include optimising crop preprocessing for a\nself-supervised Convolutional Neural Network and evaluating classification\nmethods, including SVM, multilayer perceptrons, and prototypical networks. Our\nexperiments yield an accuracy of 87.66\\%, showcasing the potential of AI-driven\nmethods to ensure the survival of historical music for future generations\nthrough advanced digital archiving techniques.\n", "link": "http://arxiv.org/abs/2411.16408v1", "date": "2024-11-25", "relevancy": 2.4725, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5195}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5035}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Data%20Classification%20of%20Historical%20Music%20Manuscripts%3A%20A%20Few-Shot%0A%20%20Learning%20Approach&body=Title%3A%20Low-Data%20Classification%20of%20Historical%20Music%20Manuscripts%3A%20A%20Few-Shot%0A%20%20Learning%20Approach%0AAuthor%3A%20Elona%20Shatri%20and%20Daniel%20Raymond%20and%20George%20Fazekas%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explore%20the%20intersection%20of%20technology%20and%20cultural%0Apreservation%20by%20developing%20a%20self-supervised%20learning%20framework%20for%20the%0Aclassification%20of%20musical%20symbols%20in%20historical%20manuscripts.%20Optical%20Music%0ARecognition%20%28OMR%29%20plays%20a%20vital%20role%20in%20digitising%20and%20preserving%20musical%0Aheritage%2C%20but%20historical%20documents%20often%20lack%20the%20labelled%20data%20required%20by%0Atraditional%20methods.%20We%20overcome%20this%20challenge%20by%20training%20a%20neural-based%0Afeature%20extractor%20on%20unlabelled%20data%2C%20enabling%20effective%20classification%20with%0Aminimal%20samples.%20Key%20contributions%20include%20optimising%20crop%20preprocessing%20for%20a%0Aself-supervised%20Convolutional%20Neural%20Network%20and%20evaluating%20classification%0Amethods%2C%20including%20SVM%2C%20multilayer%20perceptrons%2C%20and%20prototypical%20networks.%20Our%0Aexperiments%20yield%20an%20accuracy%20of%2087.66%5C%25%2C%20showcasing%20the%20potential%20of%20AI-driven%0Amethods%20to%20ensure%20the%20survival%20of%20historical%20music%20for%20future%20generations%0Athrough%20advanced%20digital%20archiving%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Data%2520Classification%2520of%2520Historical%2520Music%2520Manuscripts%253A%2520A%2520Few-Shot%250A%2520%2520Learning%2520Approach%26entry.906535625%3DElona%2520Shatri%2520and%2520Daniel%2520Raymond%2520and%2520George%2520Fazekas%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520intersection%2520of%2520technology%2520and%2520cultural%250Apreservation%2520by%2520developing%2520a%2520self-supervised%2520learning%2520framework%2520for%2520the%250Aclassification%2520of%2520musical%2520symbols%2520in%2520historical%2520manuscripts.%2520Optical%2520Music%250ARecognition%2520%2528OMR%2529%2520plays%2520a%2520vital%2520role%2520in%2520digitising%2520and%2520preserving%2520musical%250Aheritage%252C%2520but%2520historical%2520documents%2520often%2520lack%2520the%2520labelled%2520data%2520required%2520by%250Atraditional%2520methods.%2520We%2520overcome%2520this%2520challenge%2520by%2520training%2520a%2520neural-based%250Afeature%2520extractor%2520on%2520unlabelled%2520data%252C%2520enabling%2520effective%2520classification%2520with%250Aminimal%2520samples.%2520Key%2520contributions%2520include%2520optimising%2520crop%2520preprocessing%2520for%2520a%250Aself-supervised%2520Convolutional%2520Neural%2520Network%2520and%2520evaluating%2520classification%250Amethods%252C%2520including%2520SVM%252C%2520multilayer%2520perceptrons%252C%2520and%2520prototypical%2520networks.%2520Our%250Aexperiments%2520yield%2520an%2520accuracy%2520of%252087.66%255C%2525%252C%2520showcasing%2520the%2520potential%2520of%2520AI-driven%250Amethods%2520to%2520ensure%2520the%2520survival%2520of%2520historical%2520music%2520for%2520future%2520generations%250Athrough%2520advanced%2520digital%2520archiving%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Data%20Classification%20of%20Historical%20Music%20Manuscripts%3A%20A%20Few-Shot%0A%20%20Learning%20Approach&entry.906535625=Elona%20Shatri%20and%20Daniel%20Raymond%20and%20George%20Fazekas&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explore%20the%20intersection%20of%20technology%20and%20cultural%0Apreservation%20by%20developing%20a%20self-supervised%20learning%20framework%20for%20the%0Aclassification%20of%20musical%20symbols%20in%20historical%20manuscripts.%20Optical%20Music%0ARecognition%20%28OMR%29%20plays%20a%20vital%20role%20in%20digitising%20and%20preserving%20musical%0Aheritage%2C%20but%20historical%20documents%20often%20lack%20the%20labelled%20data%20required%20by%0Atraditional%20methods.%20We%20overcome%20this%20challenge%20by%20training%20a%20neural-based%0Afeature%20extractor%20on%20unlabelled%20data%2C%20enabling%20effective%20classification%20with%0Aminimal%20samples.%20Key%20contributions%20include%20optimising%20crop%20preprocessing%20for%20a%0Aself-supervised%20Convolutional%20Neural%20Network%20and%20evaluating%20classification%0Amethods%2C%20including%20SVM%2C%20multilayer%20perceptrons%2C%20and%20prototypical%20networks.%20Our%0Aexperiments%20yield%20an%20accuracy%20of%2087.66%5C%25%2C%20showcasing%20the%20potential%20of%20AI-driven%0Amethods%20to%20ensure%20the%20survival%20of%20historical%20music%20for%20future%20generations%0Athrough%20advanced%20digital%20archiving%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16408v1&entry.124074799=Read"},
{"title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures", "author": "Fu-Chieh Chang and Pei-Yuan Wu", "abstract": "  Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as \\emph{Commutativity} and\n\\emph{Identity} properties. Since these structures are observable through\ninput-output relationships, they can generalize to unseen data. We empirically\ndemonstrate that LLMs can learn algebraic structures using a custom dataset of\narithmetic problems. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance.\n", "link": "http://arxiv.org/abs/2411.16260v1", "date": "2024-11-25", "relevancy": 2.4604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20Arithmetic%20in%20Large%20Language%20Models%3A%20The%20Role%20of%20Algebraic%0A%20%20Structures&body=Title%3A%20Unraveling%20Arithmetic%20in%20Large%20Language%20Models%3A%20The%20Role%20of%20Algebraic%0A%20%20Structures%0AAuthor%3A%20Fu-Chieh%20Chang%20and%20Pei-Yuan%20Wu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20mathematical%0Acapabilities%2C%20largely%20driven%20by%20chain-of-thought%20%28CoT%29%20prompting%2C%20which%0Adecomposes%20complex%20reasoning%20into%20step-by-step%20solutions.%20This%20approach%20has%0Aenabled%20significant%20advancements%2C%20as%20evidenced%20by%20performance%20on%20benchmarks%0Alike%20GSM8K%20and%20MATH.%20However%2C%20the%20mechanisms%20underlying%20LLMs%27%20ability%20to%0Aperform%20arithmetic%20in%20a%20single%20step%20of%20CoT%20remain%20poorly%20understood.%20Existing%0Astudies%20debate%20whether%20LLMs%20encode%20numerical%20values%20or%20rely%20on%20symbolic%0Areasoning%2C%20while%20others%20explore%20attention%20and%20multi-layered%20processing%20in%0Aarithmetic%20tasks.%20In%20this%20work%2C%20we%20propose%20that%20LLMs%20learn%20arithmetic%20by%0Acapturing%20algebraic%20structures%2C%20such%20as%20%5Cemph%7BCommutativity%7D%20and%0A%5Cemph%7BIdentity%7D%20properties.%20Since%20these%20structures%20are%20observable%20through%0Ainput-output%20relationships%2C%20they%20can%20generalize%20to%20unseen%20data.%20We%20empirically%0Ademonstrate%20that%20LLMs%20can%20learn%20algebraic%20structures%20using%20a%20custom%20dataset%20of%0Aarithmetic%20problems.%20Our%20findings%20indicate%20that%20leveraging%20algebraic%20structures%0Acan%20enhance%20the%20LLMs%27%20arithmetic%20capabilities%2C%20offering%20insights%20into%20improving%0Atheir%20arithmetic%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520Arithmetic%2520in%2520Large%2520Language%2520Models%253A%2520The%2520Role%2520of%2520Algebraic%250A%2520%2520Structures%26entry.906535625%3DFu-Chieh%2520Chang%2520and%2520Pei-Yuan%2520Wu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520mathematical%250Acapabilities%252C%2520largely%2520driven%2520by%2520chain-of-thought%2520%2528CoT%2529%2520prompting%252C%2520which%250Adecomposes%2520complex%2520reasoning%2520into%2520step-by-step%2520solutions.%2520This%2520approach%2520has%250Aenabled%2520significant%2520advancements%252C%2520as%2520evidenced%2520by%2520performance%2520on%2520benchmarks%250Alike%2520GSM8K%2520and%2520MATH.%2520However%252C%2520the%2520mechanisms%2520underlying%2520LLMs%2527%2520ability%2520to%250Aperform%2520arithmetic%2520in%2520a%2520single%2520step%2520of%2520CoT%2520remain%2520poorly%2520understood.%2520Existing%250Astudies%2520debate%2520whether%2520LLMs%2520encode%2520numerical%2520values%2520or%2520rely%2520on%2520symbolic%250Areasoning%252C%2520while%2520others%2520explore%2520attention%2520and%2520multi-layered%2520processing%2520in%250Aarithmetic%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520that%2520LLMs%2520learn%2520arithmetic%2520by%250Acapturing%2520algebraic%2520structures%252C%2520such%2520as%2520%255Cemph%257BCommutativity%257D%2520and%250A%255Cemph%257BIdentity%257D%2520properties.%2520Since%2520these%2520structures%2520are%2520observable%2520through%250Ainput-output%2520relationships%252C%2520they%2520can%2520generalize%2520to%2520unseen%2520data.%2520We%2520empirically%250Ademonstrate%2520that%2520LLMs%2520can%2520learn%2520algebraic%2520structures%2520using%2520a%2520custom%2520dataset%2520of%250Aarithmetic%2520problems.%2520Our%2520findings%2520indicate%2520that%2520leveraging%2520algebraic%2520structures%250Acan%2520enhance%2520the%2520LLMs%2527%2520arithmetic%2520capabilities%252C%2520offering%2520insights%2520into%2520improving%250Atheir%2520arithmetic%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20Arithmetic%20in%20Large%20Language%20Models%3A%20The%20Role%20of%20Algebraic%0A%20%20Structures&entry.906535625=Fu-Chieh%20Chang%20and%20Pei-Yuan%20Wu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20mathematical%0Acapabilities%2C%20largely%20driven%20by%20chain-of-thought%20%28CoT%29%20prompting%2C%20which%0Adecomposes%20complex%20reasoning%20into%20step-by-step%20solutions.%20This%20approach%20has%0Aenabled%20significant%20advancements%2C%20as%20evidenced%20by%20performance%20on%20benchmarks%0Alike%20GSM8K%20and%20MATH.%20However%2C%20the%20mechanisms%20underlying%20LLMs%27%20ability%20to%0Aperform%20arithmetic%20in%20a%20single%20step%20of%20CoT%20remain%20poorly%20understood.%20Existing%0Astudies%20debate%20whether%20LLMs%20encode%20numerical%20values%20or%20rely%20on%20symbolic%0Areasoning%2C%20while%20others%20explore%20attention%20and%20multi-layered%20processing%20in%0Aarithmetic%20tasks.%20In%20this%20work%2C%20we%20propose%20that%20LLMs%20learn%20arithmetic%20by%0Acapturing%20algebraic%20structures%2C%20such%20as%20%5Cemph%7BCommutativity%7D%20and%0A%5Cemph%7BIdentity%7D%20properties.%20Since%20these%20structures%20are%20observable%20through%0Ainput-output%20relationships%2C%20they%20can%20generalize%20to%20unseen%20data.%20We%20empirically%0Ademonstrate%20that%20LLMs%20can%20learn%20algebraic%20structures%20using%20a%20custom%20dataset%20of%0Aarithmetic%20problems.%20Our%20findings%20indicate%20that%20leveraging%20algebraic%20structures%0Acan%20enhance%20the%20LLMs%27%20arithmetic%20capabilities%2C%20offering%20insights%20into%20improving%0Atheir%20arithmetic%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16260v1&entry.124074799=Read"},
{"title": "Enhancing LLM Reasoning via Critique Models with Test-Time and\n  Training-Time Supervision", "author": "Zhiheng Xi and Dingwen Yang and Jixuan Huang and Jiafu Tang and Guanyu Li and Yiwen Ding and Wei He and Boyang Hong and Shihan Do and Wenyu Zhan and Xiao Wang and Rui Zheng and Tao Ji and Xiaowei Shi and Yitao Zhai and Rongxiang Weng and Jingang Wang and Xunliang Cai and Tao Gui and Zuxuan Wu and Qi Zhang and Xipeng Qiu and Xuanjing Huang and Yu-Gang Jiang", "abstract": "  Training large language models (LLMs) to spend more time thinking and\nreflection before responding is crucial for effectively solving complex\nreasoning tasks in fields such as science, coding, and mathematics. However,\nthe effectiveness of mechanisms like self-reflection and self-correction\ndepends on the model's capacity to accurately assess its own performance, which\ncan be limited by factors such as initial accuracy, question difficulty, and\nthe lack of external feedback. In this paper, we delve into a two-player\nparadigm that separates the roles of reasoning and critique models, where the\ncritique model provides step-level feedback to supervise the reasoning (actor)\nmodel during both test-time and train-time. We first propose AutoMathCritique,\nan automated and scalable framework for collecting critique data, resulting in\na dataset of $76,321$ responses paired with step-level feedback. Fine-tuning\nlanguage models with this dataset enables them to generate natural language\nfeedback for mathematical reasoning. We demonstrate that the critique models\nconsistently improve the actor's performance on difficult queries at test-time,\nespecially when scaling up inference-time computation. Motivated by these\nfindings, we introduce the critique-based supervision to the actor's\nself-training process, and propose a critique-in-the-loop self-improvement\nmethod. Experiments show that the method improves the actor's exploration\nefficiency and solution diversity, especially on challenging queries, leading\nto a stronger reasoning model. Lastly, we take the preliminary step to explore\ntraining self-talk reasoning models via critique supervision and showcase its\npotential. Our code and datasets are at\n\\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.\n", "link": "http://arxiv.org/abs/2411.16579v1", "date": "2024-11-25", "relevancy": 2.4506, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20LLM%20Reasoning%20via%20Critique%20Models%20with%20Test-Time%20and%0A%20%20Training-Time%20Supervision&body=Title%3A%20Enhancing%20LLM%20Reasoning%20via%20Critique%20Models%20with%20Test-Time%20and%0A%20%20Training-Time%20Supervision%0AAuthor%3A%20Zhiheng%20Xi%20and%20Dingwen%20Yang%20and%20Jixuan%20Huang%20and%20Jiafu%20Tang%20and%20Guanyu%20Li%20and%20Yiwen%20Ding%20and%20Wei%20He%20and%20Boyang%20Hong%20and%20Shihan%20Do%20and%20Wenyu%20Zhan%20and%20Xiao%20Wang%20and%20Rui%20Zheng%20and%20Tao%20Ji%20and%20Xiaowei%20Shi%20and%20Yitao%20Zhai%20and%20Rongxiang%20Weng%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%20and%20Tao%20Gui%20and%20Zuxuan%20Wu%20and%20Qi%20Zhang%20and%20Xipeng%20Qiu%20and%20Xuanjing%20Huang%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Training%20large%20language%20models%20%28LLMs%29%20to%20spend%20more%20time%20thinking%20and%0Areflection%20before%20responding%20is%20crucial%20for%20effectively%20solving%20complex%0Areasoning%20tasks%20in%20fields%20such%20as%20science%2C%20coding%2C%20and%20mathematics.%20However%2C%0Athe%20effectiveness%20of%20mechanisms%20like%20self-reflection%20and%20self-correction%0Adepends%20on%20the%20model%27s%20capacity%20to%20accurately%20assess%20its%20own%20performance%2C%20which%0Acan%20be%20limited%20by%20factors%20such%20as%20initial%20accuracy%2C%20question%20difficulty%2C%20and%0Athe%20lack%20of%20external%20feedback.%20In%20this%20paper%2C%20we%20delve%20into%20a%20two-player%0Aparadigm%20that%20separates%20the%20roles%20of%20reasoning%20and%20critique%20models%2C%20where%20the%0Acritique%20model%20provides%20step-level%20feedback%20to%20supervise%20the%20reasoning%20%28actor%29%0Amodel%20during%20both%20test-time%20and%20train-time.%20We%20first%20propose%20AutoMathCritique%2C%0Aan%20automated%20and%20scalable%20framework%20for%20collecting%20critique%20data%2C%20resulting%20in%0Aa%20dataset%20of%20%2476%2C321%24%20responses%20paired%20with%20step-level%20feedback.%20Fine-tuning%0Alanguage%20models%20with%20this%20dataset%20enables%20them%20to%20generate%20natural%20language%0Afeedback%20for%20mathematical%20reasoning.%20We%20demonstrate%20that%20the%20critique%20models%0Aconsistently%20improve%20the%20actor%27s%20performance%20on%20difficult%20queries%20at%20test-time%2C%0Aespecially%20when%20scaling%20up%20inference-time%20computation.%20Motivated%20by%20these%0Afindings%2C%20we%20introduce%20the%20critique-based%20supervision%20to%20the%20actor%27s%0Aself-training%20process%2C%20and%20propose%20a%20critique-in-the-loop%20self-improvement%0Amethod.%20Experiments%20show%20that%20the%20method%20improves%20the%20actor%27s%20exploration%0Aefficiency%20and%20solution%20diversity%2C%20especially%20on%20challenging%20queries%2C%20leading%0Ato%20a%20stronger%20reasoning%20model.%20Lastly%2C%20we%20take%20the%20preliminary%20step%20to%20explore%0Atraining%20self-talk%20reasoning%20models%20via%20critique%20supervision%20and%20showcase%20its%0Apotential.%20Our%20code%20and%20datasets%20are%20at%0A%5Chref%7Bhttps%3A//mathcritique.github.io/%7D%7Bhttps%3A//mathcritique.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520LLM%2520Reasoning%2520via%2520Critique%2520Models%2520with%2520Test-Time%2520and%250A%2520%2520Training-Time%2520Supervision%26entry.906535625%3DZhiheng%2520Xi%2520and%2520Dingwen%2520Yang%2520and%2520Jixuan%2520Huang%2520and%2520Jiafu%2520Tang%2520and%2520Guanyu%2520Li%2520and%2520Yiwen%2520Ding%2520and%2520Wei%2520He%2520and%2520Boyang%2520Hong%2520and%2520Shihan%2520Do%2520and%2520Wenyu%2520Zhan%2520and%2520Xiao%2520Wang%2520and%2520Rui%2520Zheng%2520and%2520Tao%2520Ji%2520and%2520Xiaowei%2520Shi%2520and%2520Yitao%2520Zhai%2520and%2520Rongxiang%2520Weng%2520and%2520Jingang%2520Wang%2520and%2520Xunliang%2520Cai%2520and%2520Tao%2520Gui%2520and%2520Zuxuan%2520Wu%2520and%2520Qi%2520Zhang%2520and%2520Xipeng%2520Qiu%2520and%2520Xuanjing%2520Huang%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520spend%2520more%2520time%2520thinking%2520and%250Areflection%2520before%2520responding%2520is%2520crucial%2520for%2520effectively%2520solving%2520complex%250Areasoning%2520tasks%2520in%2520fields%2520such%2520as%2520science%252C%2520coding%252C%2520and%2520mathematics.%2520However%252C%250Athe%2520effectiveness%2520of%2520mechanisms%2520like%2520self-reflection%2520and%2520self-correction%250Adepends%2520on%2520the%2520model%2527s%2520capacity%2520to%2520accurately%2520assess%2520its%2520own%2520performance%252C%2520which%250Acan%2520be%2520limited%2520by%2520factors%2520such%2520as%2520initial%2520accuracy%252C%2520question%2520difficulty%252C%2520and%250Athe%2520lack%2520of%2520external%2520feedback.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520a%2520two-player%250Aparadigm%2520that%2520separates%2520the%2520roles%2520of%2520reasoning%2520and%2520critique%2520models%252C%2520where%2520the%250Acritique%2520model%2520provides%2520step-level%2520feedback%2520to%2520supervise%2520the%2520reasoning%2520%2528actor%2529%250Amodel%2520during%2520both%2520test-time%2520and%2520train-time.%2520We%2520first%2520propose%2520AutoMathCritique%252C%250Aan%2520automated%2520and%2520scalable%2520framework%2520for%2520collecting%2520critique%2520data%252C%2520resulting%2520in%250Aa%2520dataset%2520of%2520%252476%252C321%2524%2520responses%2520paired%2520with%2520step-level%2520feedback.%2520Fine-tuning%250Alanguage%2520models%2520with%2520this%2520dataset%2520enables%2520them%2520to%2520generate%2520natural%2520language%250Afeedback%2520for%2520mathematical%2520reasoning.%2520We%2520demonstrate%2520that%2520the%2520critique%2520models%250Aconsistently%2520improve%2520the%2520actor%2527s%2520performance%2520on%2520difficult%2520queries%2520at%2520test-time%252C%250Aespecially%2520when%2520scaling%2520up%2520inference-time%2520computation.%2520Motivated%2520by%2520these%250Afindings%252C%2520we%2520introduce%2520the%2520critique-based%2520supervision%2520to%2520the%2520actor%2527s%250Aself-training%2520process%252C%2520and%2520propose%2520a%2520critique-in-the-loop%2520self-improvement%250Amethod.%2520Experiments%2520show%2520that%2520the%2520method%2520improves%2520the%2520actor%2527s%2520exploration%250Aefficiency%2520and%2520solution%2520diversity%252C%2520especially%2520on%2520challenging%2520queries%252C%2520leading%250Ato%2520a%2520stronger%2520reasoning%2520model.%2520Lastly%252C%2520we%2520take%2520the%2520preliminary%2520step%2520to%2520explore%250Atraining%2520self-talk%2520reasoning%2520models%2520via%2520critique%2520supervision%2520and%2520showcase%2520its%250Apotential.%2520Our%2520code%2520and%2520datasets%2520are%2520at%250A%255Chref%257Bhttps%253A//mathcritique.github.io/%257D%257Bhttps%253A//mathcritique.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20LLM%20Reasoning%20via%20Critique%20Models%20with%20Test-Time%20and%0A%20%20Training-Time%20Supervision&entry.906535625=Zhiheng%20Xi%20and%20Dingwen%20Yang%20and%20Jixuan%20Huang%20and%20Jiafu%20Tang%20and%20Guanyu%20Li%20and%20Yiwen%20Ding%20and%20Wei%20He%20and%20Boyang%20Hong%20and%20Shihan%20Do%20and%20Wenyu%20Zhan%20and%20Xiao%20Wang%20and%20Rui%20Zheng%20and%20Tao%20Ji%20and%20Xiaowei%20Shi%20and%20Yitao%20Zhai%20and%20Rongxiang%20Weng%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%20and%20Tao%20Gui%20and%20Zuxuan%20Wu%20and%20Qi%20Zhang%20and%20Xipeng%20Qiu%20and%20Xuanjing%20Huang%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Training%20large%20language%20models%20%28LLMs%29%20to%20spend%20more%20time%20thinking%20and%0Areflection%20before%20responding%20is%20crucial%20for%20effectively%20solving%20complex%0Areasoning%20tasks%20in%20fields%20such%20as%20science%2C%20coding%2C%20and%20mathematics.%20However%2C%0Athe%20effectiveness%20of%20mechanisms%20like%20self-reflection%20and%20self-correction%0Adepends%20on%20the%20model%27s%20capacity%20to%20accurately%20assess%20its%20own%20performance%2C%20which%0Acan%20be%20limited%20by%20factors%20such%20as%20initial%20accuracy%2C%20question%20difficulty%2C%20and%0Athe%20lack%20of%20external%20feedback.%20In%20this%20paper%2C%20we%20delve%20into%20a%20two-player%0Aparadigm%20that%20separates%20the%20roles%20of%20reasoning%20and%20critique%20models%2C%20where%20the%0Acritique%20model%20provides%20step-level%20feedback%20to%20supervise%20the%20reasoning%20%28actor%29%0Amodel%20during%20both%20test-time%20and%20train-time.%20We%20first%20propose%20AutoMathCritique%2C%0Aan%20automated%20and%20scalable%20framework%20for%20collecting%20critique%20data%2C%20resulting%20in%0Aa%20dataset%20of%20%2476%2C321%24%20responses%20paired%20with%20step-level%20feedback.%20Fine-tuning%0Alanguage%20models%20with%20this%20dataset%20enables%20them%20to%20generate%20natural%20language%0Afeedback%20for%20mathematical%20reasoning.%20We%20demonstrate%20that%20the%20critique%20models%0Aconsistently%20improve%20the%20actor%27s%20performance%20on%20difficult%20queries%20at%20test-time%2C%0Aespecially%20when%20scaling%20up%20inference-time%20computation.%20Motivated%20by%20these%0Afindings%2C%20we%20introduce%20the%20critique-based%20supervision%20to%20the%20actor%27s%0Aself-training%20process%2C%20and%20propose%20a%20critique-in-the-loop%20self-improvement%0Amethod.%20Experiments%20show%20that%20the%20method%20improves%20the%20actor%27s%20exploration%0Aefficiency%20and%20solution%20diversity%2C%20especially%20on%20challenging%20queries%2C%20leading%0Ato%20a%20stronger%20reasoning%20model.%20Lastly%2C%20we%20take%20the%20preliminary%20step%20to%20explore%0Atraining%20self-talk%20reasoning%20models%20via%20critique%20supervision%20and%20showcase%20its%0Apotential.%20Our%20code%20and%20datasets%20are%20at%0A%5Chref%7Bhttps%3A//mathcritique.github.io/%7D%7Bhttps%3A//mathcritique.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16579v1&entry.124074799=Read"},
{"title": "Using Large Language Models for a standard assessment mapping for\n  sustainable communities", "author": "Luc Jonveaux", "abstract": "  This paper presents a new approach to urban sustainability assessment through\nthe use of Large Language Models (LLMs) to streamline the use of the ISO 37101\nframework to automate and standardise the assessment of urban initiatives\nagainst the six \"sustainability purposes\" and twelve \"issues\" outlined in the\nstandard. The methodology includes the development of a custom prompt based on\nthe standard definitions and its application to two different datasets: 527\nprojects from the Paris Participatory Budget and 398 activities from the\nPROBONO Horizon 2020 project. The results show the effectiveness of LLMs in\nquickly and consistently categorising different urban initiatives according to\nsustainability criteria. The approach is particularly promising when it comes\nto breaking down silos in urban planning by providing a holistic view of the\nimpact of projects. The paper discusses the advantages of this method over\ntraditional human-led assessments, including significant time savings and\nimproved consistency. However, it also points out the importance of human\nexpertise in interpreting results and ethical considerations. This study\nhopefully can contribute to the growing body of work on AI applications in\nurban planning and provides a novel method for operationalising standardised\nsustainability frameworks in different urban contexts.\n", "link": "http://arxiv.org/abs/2411.00208v2", "date": "2024-11-25", "relevancy": 2.4441, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4839}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Large%20Language%20Models%20for%20a%20standard%20assessment%20mapping%20for%0A%20%20sustainable%20communities&body=Title%3A%20Using%20Large%20Language%20Models%20for%20a%20standard%20assessment%20mapping%20for%0A%20%20sustainable%20communities%0AAuthor%3A%20Luc%20Jonveaux%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20approach%20to%20urban%20sustainability%20assessment%20through%0Athe%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20streamline%20the%20use%20of%20the%20ISO%2037101%0Aframework%20to%20automate%20and%20standardise%20the%20assessment%20of%20urban%20initiatives%0Aagainst%20the%20six%20%22sustainability%20purposes%22%20and%20twelve%20%22issues%22%20outlined%20in%20the%0Astandard.%20The%20methodology%20includes%20the%20development%20of%20a%20custom%20prompt%20based%20on%0Athe%20standard%20definitions%20and%20its%20application%20to%20two%20different%20datasets%3A%20527%0Aprojects%20from%20the%20Paris%20Participatory%20Budget%20and%20398%20activities%20from%20the%0APROBONO%20Horizon%202020%20project.%20The%20results%20show%20the%20effectiveness%20of%20LLMs%20in%0Aquickly%20and%20consistently%20categorising%20different%20urban%20initiatives%20according%20to%0Asustainability%20criteria.%20The%20approach%20is%20particularly%20promising%20when%20it%20comes%0Ato%20breaking%20down%20silos%20in%20urban%20planning%20by%20providing%20a%20holistic%20view%20of%20the%0Aimpact%20of%20projects.%20The%20paper%20discusses%20the%20advantages%20of%20this%20method%20over%0Atraditional%20human-led%20assessments%2C%20including%20significant%20time%20savings%20and%0Aimproved%20consistency.%20However%2C%20it%20also%20points%20out%20the%20importance%20of%20human%0Aexpertise%20in%20interpreting%20results%20and%20ethical%20considerations.%20This%20study%0Ahopefully%20can%20contribute%20to%20the%20growing%20body%20of%20work%20on%20AI%20applications%20in%0Aurban%20planning%20and%20provides%20a%20novel%20method%20for%20operationalising%20standardised%0Asustainability%20frameworks%20in%20different%20urban%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Large%2520Language%2520Models%2520for%2520a%2520standard%2520assessment%2520mapping%2520for%250A%2520%2520sustainable%2520communities%26entry.906535625%3DLuc%2520Jonveaux%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520approach%2520to%2520urban%2520sustainability%2520assessment%2520through%250Athe%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520streamline%2520the%2520use%2520of%2520the%2520ISO%252037101%250Aframework%2520to%2520automate%2520and%2520standardise%2520the%2520assessment%2520of%2520urban%2520initiatives%250Aagainst%2520the%2520six%2520%2522sustainability%2520purposes%2522%2520and%2520twelve%2520%2522issues%2522%2520outlined%2520in%2520the%250Astandard.%2520The%2520methodology%2520includes%2520the%2520development%2520of%2520a%2520custom%2520prompt%2520based%2520on%250Athe%2520standard%2520definitions%2520and%2520its%2520application%2520to%2520two%2520different%2520datasets%253A%2520527%250Aprojects%2520from%2520the%2520Paris%2520Participatory%2520Budget%2520and%2520398%2520activities%2520from%2520the%250APROBONO%2520Horizon%25202020%2520project.%2520The%2520results%2520show%2520the%2520effectiveness%2520of%2520LLMs%2520in%250Aquickly%2520and%2520consistently%2520categorising%2520different%2520urban%2520initiatives%2520according%2520to%250Asustainability%2520criteria.%2520The%2520approach%2520is%2520particularly%2520promising%2520when%2520it%2520comes%250Ato%2520breaking%2520down%2520silos%2520in%2520urban%2520planning%2520by%2520providing%2520a%2520holistic%2520view%2520of%2520the%250Aimpact%2520of%2520projects.%2520The%2520paper%2520discusses%2520the%2520advantages%2520of%2520this%2520method%2520over%250Atraditional%2520human-led%2520assessments%252C%2520including%2520significant%2520time%2520savings%2520and%250Aimproved%2520consistency.%2520However%252C%2520it%2520also%2520points%2520out%2520the%2520importance%2520of%2520human%250Aexpertise%2520in%2520interpreting%2520results%2520and%2520ethical%2520considerations.%2520This%2520study%250Ahopefully%2520can%2520contribute%2520to%2520the%2520growing%2520body%2520of%2520work%2520on%2520AI%2520applications%2520in%250Aurban%2520planning%2520and%2520provides%2520a%2520novel%2520method%2520for%2520operationalising%2520standardised%250Asustainability%2520frameworks%2520in%2520different%2520urban%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Large%20Language%20Models%20for%20a%20standard%20assessment%20mapping%20for%0A%20%20sustainable%20communities&entry.906535625=Luc%20Jonveaux&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20approach%20to%20urban%20sustainability%20assessment%20through%0Athe%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20streamline%20the%20use%20of%20the%20ISO%2037101%0Aframework%20to%20automate%20and%20standardise%20the%20assessment%20of%20urban%20initiatives%0Aagainst%20the%20six%20%22sustainability%20purposes%22%20and%20twelve%20%22issues%22%20outlined%20in%20the%0Astandard.%20The%20methodology%20includes%20the%20development%20of%20a%20custom%20prompt%20based%20on%0Athe%20standard%20definitions%20and%20its%20application%20to%20two%20different%20datasets%3A%20527%0Aprojects%20from%20the%20Paris%20Participatory%20Budget%20and%20398%20activities%20from%20the%0APROBONO%20Horizon%202020%20project.%20The%20results%20show%20the%20effectiveness%20of%20LLMs%20in%0Aquickly%20and%20consistently%20categorising%20different%20urban%20initiatives%20according%20to%0Asustainability%20criteria.%20The%20approach%20is%20particularly%20promising%20when%20it%20comes%0Ato%20breaking%20down%20silos%20in%20urban%20planning%20by%20providing%20a%20holistic%20view%20of%20the%0Aimpact%20of%20projects.%20The%20paper%20discusses%20the%20advantages%20of%20this%20method%20over%0Atraditional%20human-led%20assessments%2C%20including%20significant%20time%20savings%20and%0Aimproved%20consistency.%20However%2C%20it%20also%20points%20out%20the%20importance%20of%20human%0Aexpertise%20in%20interpreting%20results%20and%20ethical%20considerations.%20This%20study%0Ahopefully%20can%20contribute%20to%20the%20growing%20body%20of%20work%20on%20AI%20applications%20in%0Aurban%20planning%20and%20provides%20a%20novel%20method%20for%20operationalising%20standardised%0Asustainability%20frameworks%20in%20different%20urban%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00208v2&entry.124074799=Read"},
{"title": "When Babies Teach Babies: Can student knowledge sharing outperform\n  Teacher-Guided Distillation on small datasets?", "author": "Srikrishna Iyer", "abstract": "  We present our submission to the BabyLM challenge, aiming to push the\nboundaries of data-efficient language model pretraining. Our method builds upon\ndeep mutual learning, introducing a student model search for diverse\ninitialization. We address the limitation of treating students equally by\nformulating weighted mutual learning as a bi-level optimization problem. The\ninner loop learns compact students through online distillation, while the outer\nloop optimizes weights for better knowledge distillation from diverse students.\nThis dynamic weighting strategy eliminates the need for a teacher model,\nreducing computational requirements. Our evaluations show that teacher-less\nmethods can match or surpass teacher-supervised approaches.\n", "link": "http://arxiv.org/abs/2411.16487v1", "date": "2024-11-25", "relevancy": 2.4314, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5026}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4822}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Babies%20Teach%20Babies%3A%20Can%20student%20knowledge%20sharing%20outperform%0A%20%20Teacher-Guided%20Distillation%20on%20small%20datasets%3F&body=Title%3A%20When%20Babies%20Teach%20Babies%3A%20Can%20student%20knowledge%20sharing%20outperform%0A%20%20Teacher-Guided%20Distillation%20on%20small%20datasets%3F%0AAuthor%3A%20Srikrishna%20Iyer%0AAbstract%3A%20%20%20We%20present%20our%20submission%20to%20the%20BabyLM%20challenge%2C%20aiming%20to%20push%20the%0Aboundaries%20of%20data-efficient%20language%20model%20pretraining.%20Our%20method%20builds%20upon%0Adeep%20mutual%20learning%2C%20introducing%20a%20student%20model%20search%20for%20diverse%0Ainitialization.%20We%20address%20the%20limitation%20of%20treating%20students%20equally%20by%0Aformulating%20weighted%20mutual%20learning%20as%20a%20bi-level%20optimization%20problem.%20The%0Ainner%20loop%20learns%20compact%20students%20through%20online%20distillation%2C%20while%20the%20outer%0Aloop%20optimizes%20weights%20for%20better%20knowledge%20distillation%20from%20diverse%20students.%0AThis%20dynamic%20weighting%20strategy%20eliminates%20the%20need%20for%20a%20teacher%20model%2C%0Areducing%20computational%20requirements.%20Our%20evaluations%20show%20that%20teacher-less%0Amethods%20can%20match%20or%20surpass%20teacher-supervised%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Babies%2520Teach%2520Babies%253A%2520Can%2520student%2520knowledge%2520sharing%2520outperform%250A%2520%2520Teacher-Guided%2520Distillation%2520on%2520small%2520datasets%253F%26entry.906535625%3DSrikrishna%2520Iyer%26entry.1292438233%3D%2520%2520We%2520present%2520our%2520submission%2520to%2520the%2520BabyLM%2520challenge%252C%2520aiming%2520to%2520push%2520the%250Aboundaries%2520of%2520data-efficient%2520language%2520model%2520pretraining.%2520Our%2520method%2520builds%2520upon%250Adeep%2520mutual%2520learning%252C%2520introducing%2520a%2520student%2520model%2520search%2520for%2520diverse%250Ainitialization.%2520We%2520address%2520the%2520limitation%2520of%2520treating%2520students%2520equally%2520by%250Aformulating%2520weighted%2520mutual%2520learning%2520as%2520a%2520bi-level%2520optimization%2520problem.%2520The%250Ainner%2520loop%2520learns%2520compact%2520students%2520through%2520online%2520distillation%252C%2520while%2520the%2520outer%250Aloop%2520optimizes%2520weights%2520for%2520better%2520knowledge%2520distillation%2520from%2520diverse%2520students.%250AThis%2520dynamic%2520weighting%2520strategy%2520eliminates%2520the%2520need%2520for%2520a%2520teacher%2520model%252C%250Areducing%2520computational%2520requirements.%2520Our%2520evaluations%2520show%2520that%2520teacher-less%250Amethods%2520can%2520match%2520or%2520surpass%2520teacher-supervised%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Babies%20Teach%20Babies%3A%20Can%20student%20knowledge%20sharing%20outperform%0A%20%20Teacher-Guided%20Distillation%20on%20small%20datasets%3F&entry.906535625=Srikrishna%20Iyer&entry.1292438233=%20%20We%20present%20our%20submission%20to%20the%20BabyLM%20challenge%2C%20aiming%20to%20push%20the%0Aboundaries%20of%20data-efficient%20language%20model%20pretraining.%20Our%20method%20builds%20upon%0Adeep%20mutual%20learning%2C%20introducing%20a%20student%20model%20search%20for%20diverse%0Ainitialization.%20We%20address%20the%20limitation%20of%20treating%20students%20equally%20by%0Aformulating%20weighted%20mutual%20learning%20as%20a%20bi-level%20optimization%20problem.%20The%0Ainner%20loop%20learns%20compact%20students%20through%20online%20distillation%2C%20while%20the%20outer%0Aloop%20optimizes%20weights%20for%20better%20knowledge%20distillation%20from%20diverse%20students.%0AThis%20dynamic%20weighting%20strategy%20eliminates%20the%20need%20for%20a%20teacher%20model%2C%0Areducing%20computational%20requirements.%20Our%20evaluations%20show%20that%20teacher-less%0Amethods%20can%20match%20or%20surpass%20teacher-supervised%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16487v1&entry.124074799=Read"},
{"title": "Melody Is All You Need For Music Generation", "author": "Shaopeng Wei and Manzhen Wei and Haoyu Wang and Yu Zhao and Gang Kou", "abstract": "  We present the Melody Guided Music Generation (MG2) model, a novel approach\nusing melody to guide the text-to-music generation that, despite a pretty\nsimple method and extremely limited resources, achieves excellent performance.\nSpecifically, we first align the text with audio waveforms and their associated\nmelodies using the newly proposed Contrastive Language-Music Pretraining,\nenabling the learned text representation fused with implicit melody\ninformation. Subsequently, we condition the retrieval-augmented diffusion\nmodule on both text prompt and retrieved melody. This allows MG2to generate\nmusic that reflects the content of the given text description, meantime keeping\nthe intrinsic harmony under the guidance of explicit melody information. We\nconducted extensive experiments on two public datasets: MusicCaps and\nMusicBench. The experimental results demonstrate that the proposed MG2 model\nsurpasses current open-source text-to-music generation models, utilizing fewer\nthan 1/3 of the parameters and less than 1/200 of the training data compared to\nstate-of-the-art counterparts. Furthermore, we carried out comprehensive human\nevaluations to explore the potential applications of MG2 in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2409.20196v3", "date": "2024-11-25", "relevancy": 2.4305, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4989}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4849}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Melody%20Is%20All%20You%20Need%20For%20Music%20Generation&body=Title%3A%20Melody%20Is%20All%20You%20Need%20For%20Music%20Generation%0AAuthor%3A%20Shaopeng%20Wei%20and%20Manzhen%20Wei%20and%20Haoyu%20Wang%20and%20Yu%20Zhao%20and%20Gang%20Kou%0AAbstract%3A%20%20%20We%20present%20the%20Melody%20Guided%20Music%20Generation%20%28MG2%29%20model%2C%20a%20novel%20approach%0Ausing%20melody%20to%20guide%20the%20text-to-music%20generation%20that%2C%20despite%20a%20pretty%0Asimple%20method%20and%20extremely%20limited%20resources%2C%20achieves%20excellent%20performance.%0ASpecifically%2C%20we%20first%20align%20the%20text%20with%20audio%20waveforms%20and%20their%20associated%0Amelodies%20using%20the%20newly%20proposed%20Contrastive%20Language-Music%20Pretraining%2C%0Aenabling%20the%20learned%20text%20representation%20fused%20with%20implicit%20melody%0Ainformation.%20Subsequently%2C%20we%20condition%20the%20retrieval-augmented%20diffusion%0Amodule%20on%20both%20text%20prompt%20and%20retrieved%20melody.%20This%20allows%20MG2to%20generate%0Amusic%20that%20reflects%20the%20content%20of%20the%20given%20text%20description%2C%20meantime%20keeping%0Athe%20intrinsic%20harmony%20under%20the%20guidance%20of%20explicit%20melody%20information.%20We%0Aconducted%20extensive%20experiments%20on%20two%20public%20datasets%3A%20MusicCaps%20and%0AMusicBench.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%20MG2%20model%0Asurpasses%20current%20open-source%20text-to-music%20generation%20models%2C%20utilizing%20fewer%0Athan%201/3%20of%20the%20parameters%20and%20less%20than%201/200%20of%20the%20training%20data%20compared%20to%0Astate-of-the-art%20counterparts.%20Furthermore%2C%20we%20carried%20out%20comprehensive%20human%0Aevaluations%20to%20explore%20the%20potential%20applications%20of%20MG2%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20196v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMelody%2520Is%2520All%2520You%2520Need%2520For%2520Music%2520Generation%26entry.906535625%3DShaopeng%2520Wei%2520and%2520Manzhen%2520Wei%2520and%2520Haoyu%2520Wang%2520and%2520Yu%2520Zhao%2520and%2520Gang%2520Kou%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Melody%2520Guided%2520Music%2520Generation%2520%2528MG2%2529%2520model%252C%2520a%2520novel%2520approach%250Ausing%2520melody%2520to%2520guide%2520the%2520text-to-music%2520generation%2520that%252C%2520despite%2520a%2520pretty%250Asimple%2520method%2520and%2520extremely%2520limited%2520resources%252C%2520achieves%2520excellent%2520performance.%250ASpecifically%252C%2520we%2520first%2520align%2520the%2520text%2520with%2520audio%2520waveforms%2520and%2520their%2520associated%250Amelodies%2520using%2520the%2520newly%2520proposed%2520Contrastive%2520Language-Music%2520Pretraining%252C%250Aenabling%2520the%2520learned%2520text%2520representation%2520fused%2520with%2520implicit%2520melody%250Ainformation.%2520Subsequently%252C%2520we%2520condition%2520the%2520retrieval-augmented%2520diffusion%250Amodule%2520on%2520both%2520text%2520prompt%2520and%2520retrieved%2520melody.%2520This%2520allows%2520MG2to%2520generate%250Amusic%2520that%2520reflects%2520the%2520content%2520of%2520the%2520given%2520text%2520description%252C%2520meantime%2520keeping%250Athe%2520intrinsic%2520harmony%2520under%2520the%2520guidance%2520of%2520explicit%2520melody%2520information.%2520We%250Aconducted%2520extensive%2520experiments%2520on%2520two%2520public%2520datasets%253A%2520MusicCaps%2520and%250AMusicBench.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520MG2%2520model%250Asurpasses%2520current%2520open-source%2520text-to-music%2520generation%2520models%252C%2520utilizing%2520fewer%250Athan%25201/3%2520of%2520the%2520parameters%2520and%2520less%2520than%25201/200%2520of%2520the%2520training%2520data%2520compared%2520to%250Astate-of-the-art%2520counterparts.%2520Furthermore%252C%2520we%2520carried%2520out%2520comprehensive%2520human%250Aevaluations%2520to%2520explore%2520the%2520potential%2520applications%2520of%2520MG2%2520in%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20196v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Melody%20Is%20All%20You%20Need%20For%20Music%20Generation&entry.906535625=Shaopeng%20Wei%20and%20Manzhen%20Wei%20and%20Haoyu%20Wang%20and%20Yu%20Zhao%20and%20Gang%20Kou&entry.1292438233=%20%20We%20present%20the%20Melody%20Guided%20Music%20Generation%20%28MG2%29%20model%2C%20a%20novel%20approach%0Ausing%20melody%20to%20guide%20the%20text-to-music%20generation%20that%2C%20despite%20a%20pretty%0Asimple%20method%20and%20extremely%20limited%20resources%2C%20achieves%20excellent%20performance.%0ASpecifically%2C%20we%20first%20align%20the%20text%20with%20audio%20waveforms%20and%20their%20associated%0Amelodies%20using%20the%20newly%20proposed%20Contrastive%20Language-Music%20Pretraining%2C%0Aenabling%20the%20learned%20text%20representation%20fused%20with%20implicit%20melody%0Ainformation.%20Subsequently%2C%20we%20condition%20the%20retrieval-augmented%20diffusion%0Amodule%20on%20both%20text%20prompt%20and%20retrieved%20melody.%20This%20allows%20MG2to%20generate%0Amusic%20that%20reflects%20the%20content%20of%20the%20given%20text%20description%2C%20meantime%20keeping%0Athe%20intrinsic%20harmony%20under%20the%20guidance%20of%20explicit%20melody%20information.%20We%0Aconducted%20extensive%20experiments%20on%20two%20public%20datasets%3A%20MusicCaps%20and%0AMusicBench.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%20MG2%20model%0Asurpasses%20current%20open-source%20text-to-music%20generation%20models%2C%20utilizing%20fewer%0Athan%201/3%20of%20the%20parameters%20and%20less%20than%201/200%20of%20the%20training%20data%20compared%20to%0Astate-of-the-art%20counterparts.%20Furthermore%2C%20we%20carried%20out%20comprehensive%20human%0Aevaluations%20to%20explore%20the%20potential%20applications%20of%20MG2%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20196v3&entry.124074799=Read"},
{"title": "Oriented histogram-based vector field embedding for characterizing 4D CT\n  data sets in radiotherapy", "author": "Frederic Madesta and Lukas Wimmert and Tobias Gauer and Ren\u00e9 Werner and Thilo Sentker", "abstract": "  In lung radiotherapy, the primary objective is to optimize treatment outcomes\nby minimizing exposure to healthy tissues while delivering the prescribed dose\nto the target volume. The challenge lies in accounting for lung tissue motion\ndue to breathing, which impacts precise treatment alignment. To address this,\nthe paper proposes a prospective approach that relies solely on pre-treatment\ninformation, such as planning CT scans and derived data like vector fields from\ndeformable image registration. This data is compared to analogous patient data\nto tailor treatment strategies, i.e., to be able to review treatment parameters\nand success for similar patients. To allow for such a comparison, an embedding\nand clustering strategy of prospective patient data is needed. Therefore, the\nmain focus of this study lies on reducing the dimensionality of deformable\nregistration-based vector fields by employing a voxel-wise spherical coordinate\ntransformation and a low-dimensional 2D oriented histogram representation.\nAfterwards, a fully unsupervised UMAP embedding of the encoded vector fields\n(i.e., patient-specific motion information) becomes applicable. The\nfunctionality of the proposed method is demonstrated with 71 in-house acquired\n4D CT data sets and 33 external 4D CT data sets. A comprehensive analysis of\nthe patient clusters is conducted, focusing on the similarity of breathing\npatterns of clustered patients. The proposed general approach of reducing the\ndimensionality of registration vector fields by encoding the inherent\ninformation into oriented histograms is, however, applicable to other tasks.\n", "link": "http://arxiv.org/abs/2411.16314v1", "date": "2024-11-25", "relevancy": 2.4032, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4831}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Oriented%20histogram-based%20vector%20field%20embedding%20for%20characterizing%204D%20CT%0A%20%20data%20sets%20in%20radiotherapy&body=Title%3A%20Oriented%20histogram-based%20vector%20field%20embedding%20for%20characterizing%204D%20CT%0A%20%20data%20sets%20in%20radiotherapy%0AAuthor%3A%20Frederic%20Madesta%20and%20Lukas%20Wimmert%20and%20Tobias%20Gauer%20and%20Ren%C3%A9%20Werner%20and%20Thilo%20Sentker%0AAbstract%3A%20%20%20In%20lung%20radiotherapy%2C%20the%20primary%20objective%20is%20to%20optimize%20treatment%20outcomes%0Aby%20minimizing%20exposure%20to%20healthy%20tissues%20while%20delivering%20the%20prescribed%20dose%0Ato%20the%20target%20volume.%20The%20challenge%20lies%20in%20accounting%20for%20lung%20tissue%20motion%0Adue%20to%20breathing%2C%20which%20impacts%20precise%20treatment%20alignment.%20To%20address%20this%2C%0Athe%20paper%20proposes%20a%20prospective%20approach%20that%20relies%20solely%20on%20pre-treatment%0Ainformation%2C%20such%20as%20planning%20CT%20scans%20and%20derived%20data%20like%20vector%20fields%20from%0Adeformable%20image%20registration.%20This%20data%20is%20compared%20to%20analogous%20patient%20data%0Ato%20tailor%20treatment%20strategies%2C%20i.e.%2C%20to%20be%20able%20to%20review%20treatment%20parameters%0Aand%20success%20for%20similar%20patients.%20To%20allow%20for%20such%20a%20comparison%2C%20an%20embedding%0Aand%20clustering%20strategy%20of%20prospective%20patient%20data%20is%20needed.%20Therefore%2C%20the%0Amain%20focus%20of%20this%20study%20lies%20on%20reducing%20the%20dimensionality%20of%20deformable%0Aregistration-based%20vector%20fields%20by%20employing%20a%20voxel-wise%20spherical%20coordinate%0Atransformation%20and%20a%20low-dimensional%202D%20oriented%20histogram%20representation.%0AAfterwards%2C%20a%20fully%20unsupervised%20UMAP%20embedding%20of%20the%20encoded%20vector%20fields%0A%28i.e.%2C%20patient-specific%20motion%20information%29%20becomes%20applicable.%20The%0Afunctionality%20of%20the%20proposed%20method%20is%20demonstrated%20with%2071%20in-house%20acquired%0A4D%20CT%20data%20sets%20and%2033%20external%204D%20CT%20data%20sets.%20A%20comprehensive%20analysis%20of%0Athe%20patient%20clusters%20is%20conducted%2C%20focusing%20on%20the%20similarity%20of%20breathing%0Apatterns%20of%20clustered%20patients.%20The%20proposed%20general%20approach%20of%20reducing%20the%0Adimensionality%20of%20registration%20vector%20fields%20by%20encoding%20the%20inherent%0Ainformation%20into%20oriented%20histograms%20is%2C%20however%2C%20applicable%20to%20other%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOriented%2520histogram-based%2520vector%2520field%2520embedding%2520for%2520characterizing%25204D%2520CT%250A%2520%2520data%2520sets%2520in%2520radiotherapy%26entry.906535625%3DFrederic%2520Madesta%2520and%2520Lukas%2520Wimmert%2520and%2520Tobias%2520Gauer%2520and%2520Ren%25C3%25A9%2520Werner%2520and%2520Thilo%2520Sentker%26entry.1292438233%3D%2520%2520In%2520lung%2520radiotherapy%252C%2520the%2520primary%2520objective%2520is%2520to%2520optimize%2520treatment%2520outcomes%250Aby%2520minimizing%2520exposure%2520to%2520healthy%2520tissues%2520while%2520delivering%2520the%2520prescribed%2520dose%250Ato%2520the%2520target%2520volume.%2520The%2520challenge%2520lies%2520in%2520accounting%2520for%2520lung%2520tissue%2520motion%250Adue%2520to%2520breathing%252C%2520which%2520impacts%2520precise%2520treatment%2520alignment.%2520To%2520address%2520this%252C%250Athe%2520paper%2520proposes%2520a%2520prospective%2520approach%2520that%2520relies%2520solely%2520on%2520pre-treatment%250Ainformation%252C%2520such%2520as%2520planning%2520CT%2520scans%2520and%2520derived%2520data%2520like%2520vector%2520fields%2520from%250Adeformable%2520image%2520registration.%2520This%2520data%2520is%2520compared%2520to%2520analogous%2520patient%2520data%250Ato%2520tailor%2520treatment%2520strategies%252C%2520i.e.%252C%2520to%2520be%2520able%2520to%2520review%2520treatment%2520parameters%250Aand%2520success%2520for%2520similar%2520patients.%2520To%2520allow%2520for%2520such%2520a%2520comparison%252C%2520an%2520embedding%250Aand%2520clustering%2520strategy%2520of%2520prospective%2520patient%2520data%2520is%2520needed.%2520Therefore%252C%2520the%250Amain%2520focus%2520of%2520this%2520study%2520lies%2520on%2520reducing%2520the%2520dimensionality%2520of%2520deformable%250Aregistration-based%2520vector%2520fields%2520by%2520employing%2520a%2520voxel-wise%2520spherical%2520coordinate%250Atransformation%2520and%2520a%2520low-dimensional%25202D%2520oriented%2520histogram%2520representation.%250AAfterwards%252C%2520a%2520fully%2520unsupervised%2520UMAP%2520embedding%2520of%2520the%2520encoded%2520vector%2520fields%250A%2528i.e.%252C%2520patient-specific%2520motion%2520information%2529%2520becomes%2520applicable.%2520The%250Afunctionality%2520of%2520the%2520proposed%2520method%2520is%2520demonstrated%2520with%252071%2520in-house%2520acquired%250A4D%2520CT%2520data%2520sets%2520and%252033%2520external%25204D%2520CT%2520data%2520sets.%2520A%2520comprehensive%2520analysis%2520of%250Athe%2520patient%2520clusters%2520is%2520conducted%252C%2520focusing%2520on%2520the%2520similarity%2520of%2520breathing%250Apatterns%2520of%2520clustered%2520patients.%2520The%2520proposed%2520general%2520approach%2520of%2520reducing%2520the%250Adimensionality%2520of%2520registration%2520vector%2520fields%2520by%2520encoding%2520the%2520inherent%250Ainformation%2520into%2520oriented%2520histograms%2520is%252C%2520however%252C%2520applicable%2520to%2520other%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Oriented%20histogram-based%20vector%20field%20embedding%20for%20characterizing%204D%20CT%0A%20%20data%20sets%20in%20radiotherapy&entry.906535625=Frederic%20Madesta%20and%20Lukas%20Wimmert%20and%20Tobias%20Gauer%20and%20Ren%C3%A9%20Werner%20and%20Thilo%20Sentker&entry.1292438233=%20%20In%20lung%20radiotherapy%2C%20the%20primary%20objective%20is%20to%20optimize%20treatment%20outcomes%0Aby%20minimizing%20exposure%20to%20healthy%20tissues%20while%20delivering%20the%20prescribed%20dose%0Ato%20the%20target%20volume.%20The%20challenge%20lies%20in%20accounting%20for%20lung%20tissue%20motion%0Adue%20to%20breathing%2C%20which%20impacts%20precise%20treatment%20alignment.%20To%20address%20this%2C%0Athe%20paper%20proposes%20a%20prospective%20approach%20that%20relies%20solely%20on%20pre-treatment%0Ainformation%2C%20such%20as%20planning%20CT%20scans%20and%20derived%20data%20like%20vector%20fields%20from%0Adeformable%20image%20registration.%20This%20data%20is%20compared%20to%20analogous%20patient%20data%0Ato%20tailor%20treatment%20strategies%2C%20i.e.%2C%20to%20be%20able%20to%20review%20treatment%20parameters%0Aand%20success%20for%20similar%20patients.%20To%20allow%20for%20such%20a%20comparison%2C%20an%20embedding%0Aand%20clustering%20strategy%20of%20prospective%20patient%20data%20is%20needed.%20Therefore%2C%20the%0Amain%20focus%20of%20this%20study%20lies%20on%20reducing%20the%20dimensionality%20of%20deformable%0Aregistration-based%20vector%20fields%20by%20employing%20a%20voxel-wise%20spherical%20coordinate%0Atransformation%20and%20a%20low-dimensional%202D%20oriented%20histogram%20representation.%0AAfterwards%2C%20a%20fully%20unsupervised%20UMAP%20embedding%20of%20the%20encoded%20vector%20fields%0A%28i.e.%2C%20patient-specific%20motion%20information%29%20becomes%20applicable.%20The%0Afunctionality%20of%20the%20proposed%20method%20is%20demonstrated%20with%2071%20in-house%20acquired%0A4D%20CT%20data%20sets%20and%2033%20external%204D%20CT%20data%20sets.%20A%20comprehensive%20analysis%20of%0Athe%20patient%20clusters%20is%20conducted%2C%20focusing%20on%20the%20similarity%20of%20breathing%0Apatterns%20of%20clustered%20patients.%20The%20proposed%20general%20approach%20of%20reducing%20the%0Adimensionality%20of%20registration%20vector%20fields%20by%20encoding%20the%20inherent%0Ainformation%20into%20oriented%20histograms%20is%2C%20however%2C%20applicable%20to%20other%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16314v1&entry.124074799=Read"},
{"title": "Learning from Relevant Subgoals in Successful Dialogs using Iterative\n  Training for Task-oriented Dialog Systems", "author": "Magdalena Kaiser and Patrick Ernst and Gy\u00f6rgy Szarvas", "abstract": "  Task-oriented Dialog (ToD) systems have to solve multiple subgoals to\naccomplish user goals, whereas feedback is often obtained only at the end of\nthe dialog. In this work, we propose SUIT (SUbgoal-aware ITerative Training),\nan iterative training approach for improving ToD systems. We sample dialogs\nfrom the model we aim to improve and determine subgoals that contribute to\ndialog success using distant supervision to obtain high quality training\nsamples. We show how this data improves supervised fine-tuning or,\nalternatively, preference learning results. SUIT is able to iteratively\ngenerate more data instead of relying on fixed static sets. SUIT reaches new\nstate-of-the-art performance on a popular ToD benchmark.\n", "link": "http://arxiv.org/abs/2411.16305v1", "date": "2024-11-25", "relevancy": 2.383, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4884}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.475}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Relevant%20Subgoals%20in%20Successful%20Dialogs%20using%20Iterative%0A%20%20Training%20for%20Task-oriented%20Dialog%20Systems&body=Title%3A%20Learning%20from%20Relevant%20Subgoals%20in%20Successful%20Dialogs%20using%20Iterative%0A%20%20Training%20for%20Task-oriented%20Dialog%20Systems%0AAuthor%3A%20Magdalena%20Kaiser%20and%20Patrick%20Ernst%20and%20Gy%C3%B6rgy%20Szarvas%0AAbstract%3A%20%20%20Task-oriented%20Dialog%20%28ToD%29%20systems%20have%20to%20solve%20multiple%20subgoals%20to%0Aaccomplish%20user%20goals%2C%20whereas%20feedback%20is%20often%20obtained%20only%20at%20the%20end%20of%0Athe%20dialog.%20In%20this%20work%2C%20we%20propose%20SUIT%20%28SUbgoal-aware%20ITerative%20Training%29%2C%0Aan%20iterative%20training%20approach%20for%20improving%20ToD%20systems.%20We%20sample%20dialogs%0Afrom%20the%20model%20we%20aim%20to%20improve%20and%20determine%20subgoals%20that%20contribute%20to%0Adialog%20success%20using%20distant%20supervision%20to%20obtain%20high%20quality%20training%0Asamples.%20We%20show%20how%20this%20data%20improves%20supervised%20fine-tuning%20or%2C%0Aalternatively%2C%20preference%20learning%20results.%20SUIT%20is%20able%20to%20iteratively%0Agenerate%20more%20data%20instead%20of%20relying%20on%20fixed%20static%20sets.%20SUIT%20reaches%20new%0Astate-of-the-art%20performance%20on%20a%20popular%20ToD%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Relevant%2520Subgoals%2520in%2520Successful%2520Dialogs%2520using%2520Iterative%250A%2520%2520Training%2520for%2520Task-oriented%2520Dialog%2520Systems%26entry.906535625%3DMagdalena%2520Kaiser%2520and%2520Patrick%2520Ernst%2520and%2520Gy%25C3%25B6rgy%2520Szarvas%26entry.1292438233%3D%2520%2520Task-oriented%2520Dialog%2520%2528ToD%2529%2520systems%2520have%2520to%2520solve%2520multiple%2520subgoals%2520to%250Aaccomplish%2520user%2520goals%252C%2520whereas%2520feedback%2520is%2520often%2520obtained%2520only%2520at%2520the%2520end%2520of%250Athe%2520dialog.%2520In%2520this%2520work%252C%2520we%2520propose%2520SUIT%2520%2528SUbgoal-aware%2520ITerative%2520Training%2529%252C%250Aan%2520iterative%2520training%2520approach%2520for%2520improving%2520ToD%2520systems.%2520We%2520sample%2520dialogs%250Afrom%2520the%2520model%2520we%2520aim%2520to%2520improve%2520and%2520determine%2520subgoals%2520that%2520contribute%2520to%250Adialog%2520success%2520using%2520distant%2520supervision%2520to%2520obtain%2520high%2520quality%2520training%250Asamples.%2520We%2520show%2520how%2520this%2520data%2520improves%2520supervised%2520fine-tuning%2520or%252C%250Aalternatively%252C%2520preference%2520learning%2520results.%2520SUIT%2520is%2520able%2520to%2520iteratively%250Agenerate%2520more%2520data%2520instead%2520of%2520relying%2520on%2520fixed%2520static%2520sets.%2520SUIT%2520reaches%2520new%250Astate-of-the-art%2520performance%2520on%2520a%2520popular%2520ToD%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Relevant%20Subgoals%20in%20Successful%20Dialogs%20using%20Iterative%0A%20%20Training%20for%20Task-oriented%20Dialog%20Systems&entry.906535625=Magdalena%20Kaiser%20and%20Patrick%20Ernst%20and%20Gy%C3%B6rgy%20Szarvas&entry.1292438233=%20%20Task-oriented%20Dialog%20%28ToD%29%20systems%20have%20to%20solve%20multiple%20subgoals%20to%0Aaccomplish%20user%20goals%2C%20whereas%20feedback%20is%20often%20obtained%20only%20at%20the%20end%20of%0Athe%20dialog.%20In%20this%20work%2C%20we%20propose%20SUIT%20%28SUbgoal-aware%20ITerative%20Training%29%2C%0Aan%20iterative%20training%20approach%20for%20improving%20ToD%20systems.%20We%20sample%20dialogs%0Afrom%20the%20model%20we%20aim%20to%20improve%20and%20determine%20subgoals%20that%20contribute%20to%0Adialog%20success%20using%20distant%20supervision%20to%20obtain%20high%20quality%20training%0Asamples.%20We%20show%20how%20this%20data%20improves%20supervised%20fine-tuning%20or%2C%0Aalternatively%2C%20preference%20learning%20results.%20SUIT%20is%20able%20to%20iteratively%0Agenerate%20more%20data%20instead%20of%20relying%20on%20fixed%20static%20sets.%20SUIT%20reaches%20new%0Astate-of-the-art%20performance%20on%20a%20popular%20ToD%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16305v1&entry.124074799=Read"},
{"title": "Minority-Focused Text-to-Image Generation via Prompt Optimization", "author": "Soobin Um and Jong Chul Ye", "abstract": "  We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for producing high-quality generations. To address\nthis, we present a novel framework to counter the high-density-focus of T2I\ndiffusion models. Specifically, we first develop an online prompt optimization\nframework that can encourage the emergence of desired properties during\ninference while preserving semantic contents of user-provided prompts. We\nsubsequently tailor this generic prompt optimizer into a specialized solver\nthat promotes the generation of minority features by incorporating a\ncarefully-crafted likelihood objective. Our comprehensive experiments,\nconducted across various types of T2I models, demonstrate that our approach\nsignificantly enhances the capability to produce high-quality minority\ninstances compared to existing samplers.\n", "link": "http://arxiv.org/abs/2410.07838v2", "date": "2024-11-25", "relevancy": 2.3765, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6155}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5813}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minority-Focused%20Text-to-Image%20Generation%20via%20Prompt%20Optimization&body=Title%3A%20Minority-Focused%20Text-to-Image%20Generation%20via%20Prompt%20Optimization%0AAuthor%3A%20Soobin%20Um%20and%20Jong%20Chul%20Ye%0AAbstract%3A%20%20%20We%20investigate%20the%20generation%20of%20minority%20samples%20using%20pretrained%0Atext-to-image%20%28T2I%29%20latent%20diffusion%20models.%20Minority%20instances%2C%20in%20the%20context%0Aof%20T2I%20generation%2C%20can%20be%20defined%20as%20ones%20living%20on%20low-density%20regions%20of%0Atext-conditional%20data%20distributions.%20They%20are%20valuable%20for%20various%20applications%0Aof%20modern%20T2I%20generators%2C%20such%20as%20data%20augmentation%20and%20creative%20AI.%0AUnfortunately%2C%20existing%20pretrained%20T2I%20diffusion%20models%20primarily%20focus%20on%0Ahigh-density%20regions%2C%20largely%20due%20to%20the%20influence%20of%20guided%20samplers%20%28like%0ACFG%29%20that%20are%20essential%20for%20producing%20high-quality%20generations.%20To%20address%0Athis%2C%20we%20present%20a%20novel%20framework%20to%20counter%20the%20high-density-focus%20of%20T2I%0Adiffusion%20models.%20Specifically%2C%20we%20first%20develop%20an%20online%20prompt%20optimization%0Aframework%20that%20can%20encourage%20the%20emergence%20of%20desired%20properties%20during%0Ainference%20while%20preserving%20semantic%20contents%20of%20user-provided%20prompts.%20We%0Asubsequently%20tailor%20this%20generic%20prompt%20optimizer%20into%20a%20specialized%20solver%0Athat%20promotes%20the%20generation%20of%20minority%20features%20by%20incorporating%20a%0Acarefully-crafted%20likelihood%20objective.%20Our%20comprehensive%20experiments%2C%0Aconducted%20across%20various%20types%20of%20T2I%20models%2C%20demonstrate%20that%20our%20approach%0Asignificantly%20enhances%20the%20capability%20to%20produce%20high-quality%20minority%0Ainstances%20compared%20to%20existing%20samplers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinority-Focused%2520Text-to-Image%2520Generation%2520via%2520Prompt%2520Optimization%26entry.906535625%3DSoobin%2520Um%2520and%2520Jong%2520Chul%2520Ye%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520generation%2520of%2520minority%2520samples%2520using%2520pretrained%250Atext-to-image%2520%2528T2I%2529%2520latent%2520diffusion%2520models.%2520Minority%2520instances%252C%2520in%2520the%2520context%250Aof%2520T2I%2520generation%252C%2520can%2520be%2520defined%2520as%2520ones%2520living%2520on%2520low-density%2520regions%2520of%250Atext-conditional%2520data%2520distributions.%2520They%2520are%2520valuable%2520for%2520various%2520applications%250Aof%2520modern%2520T2I%2520generators%252C%2520such%2520as%2520data%2520augmentation%2520and%2520creative%2520AI.%250AUnfortunately%252C%2520existing%2520pretrained%2520T2I%2520diffusion%2520models%2520primarily%2520focus%2520on%250Ahigh-density%2520regions%252C%2520largely%2520due%2520to%2520the%2520influence%2520of%2520guided%2520samplers%2520%2528like%250ACFG%2529%2520that%2520are%2520essential%2520for%2520producing%2520high-quality%2520generations.%2520To%2520address%250Athis%252C%2520we%2520present%2520a%2520novel%2520framework%2520to%2520counter%2520the%2520high-density-focus%2520of%2520T2I%250Adiffusion%2520models.%2520Specifically%252C%2520we%2520first%2520develop%2520an%2520online%2520prompt%2520optimization%250Aframework%2520that%2520can%2520encourage%2520the%2520emergence%2520of%2520desired%2520properties%2520during%250Ainference%2520while%2520preserving%2520semantic%2520contents%2520of%2520user-provided%2520prompts.%2520We%250Asubsequently%2520tailor%2520this%2520generic%2520prompt%2520optimizer%2520into%2520a%2520specialized%2520solver%250Athat%2520promotes%2520the%2520generation%2520of%2520minority%2520features%2520by%2520incorporating%2520a%250Acarefully-crafted%2520likelihood%2520objective.%2520Our%2520comprehensive%2520experiments%252C%250Aconducted%2520across%2520various%2520types%2520of%2520T2I%2520models%252C%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520enhances%2520the%2520capability%2520to%2520produce%2520high-quality%2520minority%250Ainstances%2520compared%2520to%2520existing%2520samplers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minority-Focused%20Text-to-Image%20Generation%20via%20Prompt%20Optimization&entry.906535625=Soobin%20Um%20and%20Jong%20Chul%20Ye&entry.1292438233=%20%20We%20investigate%20the%20generation%20of%20minority%20samples%20using%20pretrained%0Atext-to-image%20%28T2I%29%20latent%20diffusion%20models.%20Minority%20instances%2C%20in%20the%20context%0Aof%20T2I%20generation%2C%20can%20be%20defined%20as%20ones%20living%20on%20low-density%20regions%20of%0Atext-conditional%20data%20distributions.%20They%20are%20valuable%20for%20various%20applications%0Aof%20modern%20T2I%20generators%2C%20such%20as%20data%20augmentation%20and%20creative%20AI.%0AUnfortunately%2C%20existing%20pretrained%20T2I%20diffusion%20models%20primarily%20focus%20on%0Ahigh-density%20regions%2C%20largely%20due%20to%20the%20influence%20of%20guided%20samplers%20%28like%0ACFG%29%20that%20are%20essential%20for%20producing%20high-quality%20generations.%20To%20address%0Athis%2C%20we%20present%20a%20novel%20framework%20to%20counter%20the%20high-density-focus%20of%20T2I%0Adiffusion%20models.%20Specifically%2C%20we%20first%20develop%20an%20online%20prompt%20optimization%0Aframework%20that%20can%20encourage%20the%20emergence%20of%20desired%20properties%20during%0Ainference%20while%20preserving%20semantic%20contents%20of%20user-provided%20prompts.%20We%0Asubsequently%20tailor%20this%20generic%20prompt%20optimizer%20into%20a%20specialized%20solver%0Athat%20promotes%20the%20generation%20of%20minority%20features%20by%20incorporating%20a%0Acarefully-crafted%20likelihood%20objective.%20Our%20comprehensive%20experiments%2C%0Aconducted%20across%20various%20types%20of%20T2I%20models%2C%20demonstrate%20that%20our%20approach%0Asignificantly%20enhances%20the%20capability%20to%20produce%20high-quality%20minority%0Ainstances%20compared%20to%20existing%20samplers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07838v2&entry.124074799=Read"},
{"title": "Diffusion Features for Zero-Shot 6DoF Object Pose Estimation", "author": "Bernd Von Gimborn and Philipp Ausserlechner and Markus Vincze and Stefan Thalhammer", "abstract": "  Zero-shot object pose estimation enables the retrieval of object poses from\nimages without necessitating object-specific training. In recent approaches\nthis is facilitated by vision foundation models (VFM), which are pre-trained\nmodels that are effectively general-purpose feature extractors. The\ncharacteristics exhibited by these VFMs vary depending on the training data,\nnetwork architecture, and training paradigm. The prevailing choice in this\nfield are self-supervised Vision Transformers (ViT). This study assesses the\ninfluence of Latent Diffusion Model (LDM) backbones on zero-shot pose\nestimation. In order to facilitate a comparison between the two families of\nmodels on a common ground we adopt and modify a recent approach. Therefore, a\ntemplate-based multi-staged method for estimating poses in a zero-shot fashion\nusing LDMs is presented. The efficacy of the proposed approach is empirically\nevaluated on three standard datasets for object-specific 6DoF pose estimation.\nThe experiments demonstrate an Average Recall improvement of up to 27% over the\nViT baseline. The source code is available at: https://github.com/BvG1993/DZOP.\n", "link": "http://arxiv.org/abs/2411.16668v1", "date": "2024-11-25", "relevancy": 2.3671, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6199}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Features%20for%20Zero-Shot%206DoF%20Object%20Pose%20Estimation&body=Title%3A%20Diffusion%20Features%20for%20Zero-Shot%206DoF%20Object%20Pose%20Estimation%0AAuthor%3A%20Bernd%20Von%20Gimborn%20and%20Philipp%20Ausserlechner%20and%20Markus%20Vincze%20and%20Stefan%20Thalhammer%0AAbstract%3A%20%20%20Zero-shot%20object%20pose%20estimation%20enables%20the%20retrieval%20of%20object%20poses%20from%0Aimages%20without%20necessitating%20object-specific%20training.%20In%20recent%20approaches%0Athis%20is%20facilitated%20by%20vision%20foundation%20models%20%28VFM%29%2C%20which%20are%20pre-trained%0Amodels%20that%20are%20effectively%20general-purpose%20feature%20extractors.%20The%0Acharacteristics%20exhibited%20by%20these%20VFMs%20vary%20depending%20on%20the%20training%20data%2C%0Anetwork%20architecture%2C%20and%20training%20paradigm.%20The%20prevailing%20choice%20in%20this%0Afield%20are%20self-supervised%20Vision%20Transformers%20%28ViT%29.%20This%20study%20assesses%20the%0Ainfluence%20of%20Latent%20Diffusion%20Model%20%28LDM%29%20backbones%20on%20zero-shot%20pose%0Aestimation.%20In%20order%20to%20facilitate%20a%20comparison%20between%20the%20two%20families%20of%0Amodels%20on%20a%20common%20ground%20we%20adopt%20and%20modify%20a%20recent%20approach.%20Therefore%2C%20a%0Atemplate-based%20multi-staged%20method%20for%20estimating%20poses%20in%20a%20zero-shot%20fashion%0Ausing%20LDMs%20is%20presented.%20The%20efficacy%20of%20the%20proposed%20approach%20is%20empirically%0Aevaluated%20on%20three%20standard%20datasets%20for%20object-specific%206DoF%20pose%20estimation.%0AThe%20experiments%20demonstrate%20an%20Average%20Recall%20improvement%20of%20up%20to%2027%25%20over%20the%0AViT%20baseline.%20The%20source%20code%20is%20available%20at%3A%20https%3A//github.com/BvG1993/DZOP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Features%2520for%2520Zero-Shot%25206DoF%2520Object%2520Pose%2520Estimation%26entry.906535625%3DBernd%2520Von%2520Gimborn%2520and%2520Philipp%2520Ausserlechner%2520and%2520Markus%2520Vincze%2520and%2520Stefan%2520Thalhammer%26entry.1292438233%3D%2520%2520Zero-shot%2520object%2520pose%2520estimation%2520enables%2520the%2520retrieval%2520of%2520object%2520poses%2520from%250Aimages%2520without%2520necessitating%2520object-specific%2520training.%2520In%2520recent%2520approaches%250Athis%2520is%2520facilitated%2520by%2520vision%2520foundation%2520models%2520%2528VFM%2529%252C%2520which%2520are%2520pre-trained%250Amodels%2520that%2520are%2520effectively%2520general-purpose%2520feature%2520extractors.%2520The%250Acharacteristics%2520exhibited%2520by%2520these%2520VFMs%2520vary%2520depending%2520on%2520the%2520training%2520data%252C%250Anetwork%2520architecture%252C%2520and%2520training%2520paradigm.%2520The%2520prevailing%2520choice%2520in%2520this%250Afield%2520are%2520self-supervised%2520Vision%2520Transformers%2520%2528ViT%2529.%2520This%2520study%2520assesses%2520the%250Ainfluence%2520of%2520Latent%2520Diffusion%2520Model%2520%2528LDM%2529%2520backbones%2520on%2520zero-shot%2520pose%250Aestimation.%2520In%2520order%2520to%2520facilitate%2520a%2520comparison%2520between%2520the%2520two%2520families%2520of%250Amodels%2520on%2520a%2520common%2520ground%2520we%2520adopt%2520and%2520modify%2520a%2520recent%2520approach.%2520Therefore%252C%2520a%250Atemplate-based%2520multi-staged%2520method%2520for%2520estimating%2520poses%2520in%2520a%2520zero-shot%2520fashion%250Ausing%2520LDMs%2520is%2520presented.%2520The%2520efficacy%2520of%2520the%2520proposed%2520approach%2520is%2520empirically%250Aevaluated%2520on%2520three%2520standard%2520datasets%2520for%2520object-specific%25206DoF%2520pose%2520estimation.%250AThe%2520experiments%2520demonstrate%2520an%2520Average%2520Recall%2520improvement%2520of%2520up%2520to%252027%2525%2520over%2520the%250AViT%2520baseline.%2520The%2520source%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/BvG1993/DZOP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Features%20for%20Zero-Shot%206DoF%20Object%20Pose%20Estimation&entry.906535625=Bernd%20Von%20Gimborn%20and%20Philipp%20Ausserlechner%20and%20Markus%20Vincze%20and%20Stefan%20Thalhammer&entry.1292438233=%20%20Zero-shot%20object%20pose%20estimation%20enables%20the%20retrieval%20of%20object%20poses%20from%0Aimages%20without%20necessitating%20object-specific%20training.%20In%20recent%20approaches%0Athis%20is%20facilitated%20by%20vision%20foundation%20models%20%28VFM%29%2C%20which%20are%20pre-trained%0Amodels%20that%20are%20effectively%20general-purpose%20feature%20extractors.%20The%0Acharacteristics%20exhibited%20by%20these%20VFMs%20vary%20depending%20on%20the%20training%20data%2C%0Anetwork%20architecture%2C%20and%20training%20paradigm.%20The%20prevailing%20choice%20in%20this%0Afield%20are%20self-supervised%20Vision%20Transformers%20%28ViT%29.%20This%20study%20assesses%20the%0Ainfluence%20of%20Latent%20Diffusion%20Model%20%28LDM%29%20backbones%20on%20zero-shot%20pose%0Aestimation.%20In%20order%20to%20facilitate%20a%20comparison%20between%20the%20two%20families%20of%0Amodels%20on%20a%20common%20ground%20we%20adopt%20and%20modify%20a%20recent%20approach.%20Therefore%2C%20a%0Atemplate-based%20multi-staged%20method%20for%20estimating%20poses%20in%20a%20zero-shot%20fashion%0Ausing%20LDMs%20is%20presented.%20The%20efficacy%20of%20the%20proposed%20approach%20is%20empirically%0Aevaluated%20on%20three%20standard%20datasets%20for%20object-specific%206DoF%20pose%20estimation.%0AThe%20experiments%20demonstrate%20an%20Average%20Recall%20improvement%20of%20up%20to%2027%25%20over%20the%0AViT%20baseline.%20The%20source%20code%20is%20available%20at%3A%20https%3A//github.com/BvG1993/DZOP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16668v1&entry.124074799=Read"},
{"title": "CatNet: Effective FDR Control in LSTM with Gaussian Mirrors and SHAP\n  Feature Importance", "author": "Jiaan Han and Junxiao Chen and Yanzhe Fu", "abstract": "  We introduce CatNet, an algorithm that effectively controls False Discovery\nRate (FDR) and selects significant features in LSTM with the Gaussian Mirror\n(GM) method. To evaluate the feature importance of LSTM in time series, we\nintroduce a vector of the derivative of the SHapley Additive exPlanations\n(SHAP) to measure feature importance. We also propose a new kernel-based\ndependence measure to avoid multicollinearity in the GM algorithm, to make a\nrobust feature selection with controlled FDR. We use simulated data to evaluate\nCatNet's performance in both linear models and LSTM models with different link\nfunctions. The algorithm effectively controls the FDR while maintaining a high\nstatistical power in all cases. We also evaluate the algorithm's performance in\ndifferent low-dimensional and high-dimensional cases, demonstrating its\nrobustness in various input dimensions. To evaluate CatNet's performance in\nreal world applications, we construct a multi-factor investment portfolio to\nforecast the prices of S\\&P 500 index components. The results demonstrate that\nour model achieves superior predictive accuracy compared to traditional LSTM\nmodels without feature selection and FDR control. Additionally, CatNet\neffectively captures common market-driving features, which helps informed\ndecision-making in financial markets by enhancing the interpretability of\npredictions. Our study integrates of the Gaussian Mirror algorithm with LSTM\nmodels for the first time, and introduces SHAP values as a new feature\nimportance metric for FDR control methods, marking a significant advancement in\nfeature selection and error control for neural networks.\n", "link": "http://arxiv.org/abs/2411.16666v1", "date": "2024-11-25", "relevancy": 2.3653, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.48}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4734}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CatNet%3A%20Effective%20FDR%20Control%20in%20LSTM%20with%20Gaussian%20Mirrors%20and%20SHAP%0A%20%20Feature%20Importance&body=Title%3A%20CatNet%3A%20Effective%20FDR%20Control%20in%20LSTM%20with%20Gaussian%20Mirrors%20and%20SHAP%0A%20%20Feature%20Importance%0AAuthor%3A%20Jiaan%20Han%20and%20Junxiao%20Chen%20and%20Yanzhe%20Fu%0AAbstract%3A%20%20%20We%20introduce%20CatNet%2C%20an%20algorithm%20that%20effectively%20controls%20False%20Discovery%0ARate%20%28FDR%29%20and%20selects%20significant%20features%20in%20LSTM%20with%20the%20Gaussian%20Mirror%0A%28GM%29%20method.%20To%20evaluate%20the%20feature%20importance%20of%20LSTM%20in%20time%20series%2C%20we%0Aintroduce%20a%20vector%20of%20the%20derivative%20of%20the%20SHapley%20Additive%20exPlanations%0A%28SHAP%29%20to%20measure%20feature%20importance.%20We%20also%20propose%20a%20new%20kernel-based%0Adependence%20measure%20to%20avoid%20multicollinearity%20in%20the%20GM%20algorithm%2C%20to%20make%20a%0Arobust%20feature%20selection%20with%20controlled%20FDR.%20We%20use%20simulated%20data%20to%20evaluate%0ACatNet%27s%20performance%20in%20both%20linear%20models%20and%20LSTM%20models%20with%20different%20link%0Afunctions.%20The%20algorithm%20effectively%20controls%20the%20FDR%20while%20maintaining%20a%20high%0Astatistical%20power%20in%20all%20cases.%20We%20also%20evaluate%20the%20algorithm%27s%20performance%20in%0Adifferent%20low-dimensional%20and%20high-dimensional%20cases%2C%20demonstrating%20its%0Arobustness%20in%20various%20input%20dimensions.%20To%20evaluate%20CatNet%27s%20performance%20in%0Areal%20world%20applications%2C%20we%20construct%20a%20multi-factor%20investment%20portfolio%20to%0Aforecast%20the%20prices%20of%20S%5C%26P%20500%20index%20components.%20The%20results%20demonstrate%20that%0Aour%20model%20achieves%20superior%20predictive%20accuracy%20compared%20to%20traditional%20LSTM%0Amodels%20without%20feature%20selection%20and%20FDR%20control.%20Additionally%2C%20CatNet%0Aeffectively%20captures%20common%20market-driving%20features%2C%20which%20helps%20informed%0Adecision-making%20in%20financial%20markets%20by%20enhancing%20the%20interpretability%20of%0Apredictions.%20Our%20study%20integrates%20of%20the%20Gaussian%20Mirror%20algorithm%20with%20LSTM%0Amodels%20for%20the%20first%20time%2C%20and%20introduces%20SHAP%20values%20as%20a%20new%20feature%0Aimportance%20metric%20for%20FDR%20control%20methods%2C%20marking%20a%20significant%20advancement%20in%0Afeature%20selection%20and%20error%20control%20for%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCatNet%253A%2520Effective%2520FDR%2520Control%2520in%2520LSTM%2520with%2520Gaussian%2520Mirrors%2520and%2520SHAP%250A%2520%2520Feature%2520Importance%26entry.906535625%3DJiaan%2520Han%2520and%2520Junxiao%2520Chen%2520and%2520Yanzhe%2520Fu%26entry.1292438233%3D%2520%2520We%2520introduce%2520CatNet%252C%2520an%2520algorithm%2520that%2520effectively%2520controls%2520False%2520Discovery%250ARate%2520%2528FDR%2529%2520and%2520selects%2520significant%2520features%2520in%2520LSTM%2520with%2520the%2520Gaussian%2520Mirror%250A%2528GM%2529%2520method.%2520To%2520evaluate%2520the%2520feature%2520importance%2520of%2520LSTM%2520in%2520time%2520series%252C%2520we%250Aintroduce%2520a%2520vector%2520of%2520the%2520derivative%2520of%2520the%2520SHapley%2520Additive%2520exPlanations%250A%2528SHAP%2529%2520to%2520measure%2520feature%2520importance.%2520We%2520also%2520propose%2520a%2520new%2520kernel-based%250Adependence%2520measure%2520to%2520avoid%2520multicollinearity%2520in%2520the%2520GM%2520algorithm%252C%2520to%2520make%2520a%250Arobust%2520feature%2520selection%2520with%2520controlled%2520FDR.%2520We%2520use%2520simulated%2520data%2520to%2520evaluate%250ACatNet%2527s%2520performance%2520in%2520both%2520linear%2520models%2520and%2520LSTM%2520models%2520with%2520different%2520link%250Afunctions.%2520The%2520algorithm%2520effectively%2520controls%2520the%2520FDR%2520while%2520maintaining%2520a%2520high%250Astatistical%2520power%2520in%2520all%2520cases.%2520We%2520also%2520evaluate%2520the%2520algorithm%2527s%2520performance%2520in%250Adifferent%2520low-dimensional%2520and%2520high-dimensional%2520cases%252C%2520demonstrating%2520its%250Arobustness%2520in%2520various%2520input%2520dimensions.%2520To%2520evaluate%2520CatNet%2527s%2520performance%2520in%250Areal%2520world%2520applications%252C%2520we%2520construct%2520a%2520multi-factor%2520investment%2520portfolio%2520to%250Aforecast%2520the%2520prices%2520of%2520S%255C%2526P%2520500%2520index%2520components.%2520The%2520results%2520demonstrate%2520that%250Aour%2520model%2520achieves%2520superior%2520predictive%2520accuracy%2520compared%2520to%2520traditional%2520LSTM%250Amodels%2520without%2520feature%2520selection%2520and%2520FDR%2520control.%2520Additionally%252C%2520CatNet%250Aeffectively%2520captures%2520common%2520market-driving%2520features%252C%2520which%2520helps%2520informed%250Adecision-making%2520in%2520financial%2520markets%2520by%2520enhancing%2520the%2520interpretability%2520of%250Apredictions.%2520Our%2520study%2520integrates%2520of%2520the%2520Gaussian%2520Mirror%2520algorithm%2520with%2520LSTM%250Amodels%2520for%2520the%2520first%2520time%252C%2520and%2520introduces%2520SHAP%2520values%2520as%2520a%2520new%2520feature%250Aimportance%2520metric%2520for%2520FDR%2520control%2520methods%252C%2520marking%2520a%2520significant%2520advancement%2520in%250Afeature%2520selection%2520and%2520error%2520control%2520for%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CatNet%3A%20Effective%20FDR%20Control%20in%20LSTM%20with%20Gaussian%20Mirrors%20and%20SHAP%0A%20%20Feature%20Importance&entry.906535625=Jiaan%20Han%20and%20Junxiao%20Chen%20and%20Yanzhe%20Fu&entry.1292438233=%20%20We%20introduce%20CatNet%2C%20an%20algorithm%20that%20effectively%20controls%20False%20Discovery%0ARate%20%28FDR%29%20and%20selects%20significant%20features%20in%20LSTM%20with%20the%20Gaussian%20Mirror%0A%28GM%29%20method.%20To%20evaluate%20the%20feature%20importance%20of%20LSTM%20in%20time%20series%2C%20we%0Aintroduce%20a%20vector%20of%20the%20derivative%20of%20the%20SHapley%20Additive%20exPlanations%0A%28SHAP%29%20to%20measure%20feature%20importance.%20We%20also%20propose%20a%20new%20kernel-based%0Adependence%20measure%20to%20avoid%20multicollinearity%20in%20the%20GM%20algorithm%2C%20to%20make%20a%0Arobust%20feature%20selection%20with%20controlled%20FDR.%20We%20use%20simulated%20data%20to%20evaluate%0ACatNet%27s%20performance%20in%20both%20linear%20models%20and%20LSTM%20models%20with%20different%20link%0Afunctions.%20The%20algorithm%20effectively%20controls%20the%20FDR%20while%20maintaining%20a%20high%0Astatistical%20power%20in%20all%20cases.%20We%20also%20evaluate%20the%20algorithm%27s%20performance%20in%0Adifferent%20low-dimensional%20and%20high-dimensional%20cases%2C%20demonstrating%20its%0Arobustness%20in%20various%20input%20dimensions.%20To%20evaluate%20CatNet%27s%20performance%20in%0Areal%20world%20applications%2C%20we%20construct%20a%20multi-factor%20investment%20portfolio%20to%0Aforecast%20the%20prices%20of%20S%5C%26P%20500%20index%20components.%20The%20results%20demonstrate%20that%0Aour%20model%20achieves%20superior%20predictive%20accuracy%20compared%20to%20traditional%20LSTM%0Amodels%20without%20feature%20selection%20and%20FDR%20control.%20Additionally%2C%20CatNet%0Aeffectively%20captures%20common%20market-driving%20features%2C%20which%20helps%20informed%0Adecision-making%20in%20financial%20markets%20by%20enhancing%20the%20interpretability%20of%0Apredictions.%20Our%20study%20integrates%20of%20the%20Gaussian%20Mirror%20algorithm%20with%20LSTM%0Amodels%20for%20the%20first%20time%2C%20and%20introduces%20SHAP%20values%20as%20a%20new%20feature%0Aimportance%20metric%20for%20FDR%20control%20methods%2C%20marking%20a%20significant%20advancement%20in%0Afeature%20selection%20and%20error%20control%20for%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16666v1&entry.124074799=Read"},
{"title": "SADA: Semantic adversarial unsupervised domain adaptation for Temporal\n  Action Localization", "author": "David Pujol-Perich and Albert Clap\u00e9s and Sergio Escalera", "abstract": "  Temporal Action Localization (TAL) is a complex task that poses relevant\nchallenges, particularly when attempting to generalize on new -- unseen --\ndomains in real-world applications. These scenarios, despite realistic, are\noften neglected in the literature, exposing these solutions to important\nperformance degradation. In this work, we tackle this issue by introducing, for\nthe first time, an approach for Unsupervised Domain Adaptation (UDA) in sparse\nTAL, which we refer to as Semantic Adversarial unsupervised Domain Adaptation\n(SADA). Our contributions are threefold: (1) we pioneer the development of a\ndomain adaptation model that operates on realistic sparse action detection\nbenchmarks; (2) we tackle the limitations of global-distribution alignment\ntechniques by introducing a novel adversarial loss that is sensitive to local\nclass distributions, ensuring finer-grained adaptation; and (3) we present a\nnovel set of benchmarks based on EpicKitchens100 and CharadesEgo, that evaluate\nmultiple domain shifts in a comprehensive manner. Our experiments indicate that\nSADA improves the adaptation across domains when compared to fully supervised\nstate-of-the-art and alternative UDA methods, attaining a performance boost of\nup to 6.14% mAP.\n", "link": "http://arxiv.org/abs/2312.13377v3", "date": "2024-11-25", "relevancy": 2.3582, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6032}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SADA%3A%20Semantic%20adversarial%20unsupervised%20domain%20adaptation%20for%20Temporal%0A%20%20Action%20Localization&body=Title%3A%20SADA%3A%20Semantic%20adversarial%20unsupervised%20domain%20adaptation%20for%20Temporal%0A%20%20Action%20Localization%0AAuthor%3A%20David%20Pujol-Perich%20and%20Albert%20Clap%C3%A9s%20and%20Sergio%20Escalera%0AAbstract%3A%20%20%20Temporal%20Action%20Localization%20%28TAL%29%20is%20a%20complex%20task%20that%20poses%20relevant%0Achallenges%2C%20particularly%20when%20attempting%20to%20generalize%20on%20new%20--%20unseen%20--%0Adomains%20in%20real-world%20applications.%20These%20scenarios%2C%20despite%20realistic%2C%20are%0Aoften%20neglected%20in%20the%20literature%2C%20exposing%20these%20solutions%20to%20important%0Aperformance%20degradation.%20In%20this%20work%2C%20we%20tackle%20this%20issue%20by%20introducing%2C%20for%0Athe%20first%20time%2C%20an%20approach%20for%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20in%20sparse%0ATAL%2C%20which%20we%20refer%20to%20as%20Semantic%20Adversarial%20unsupervised%20Domain%20Adaptation%0A%28SADA%29.%20Our%20contributions%20are%20threefold%3A%20%281%29%20we%20pioneer%20the%20development%20of%20a%0Adomain%20adaptation%20model%20that%20operates%20on%20realistic%20sparse%20action%20detection%0Abenchmarks%3B%20%282%29%20we%20tackle%20the%20limitations%20of%20global-distribution%20alignment%0Atechniques%20by%20introducing%20a%20novel%20adversarial%20loss%20that%20is%20sensitive%20to%20local%0Aclass%20distributions%2C%20ensuring%20finer-grained%20adaptation%3B%20and%20%283%29%20we%20present%20a%0Anovel%20set%20of%20benchmarks%20based%20on%20EpicKitchens100%20and%20CharadesEgo%2C%20that%20evaluate%0Amultiple%20domain%20shifts%20in%20a%20comprehensive%20manner.%20Our%20experiments%20indicate%20that%0ASADA%20improves%20the%20adaptation%20across%20domains%20when%20compared%20to%20fully%20supervised%0Astate-of-the-art%20and%20alternative%20UDA%20methods%2C%20attaining%20a%20performance%20boost%20of%0Aup%20to%206.14%25%20mAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13377v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSADA%253A%2520Semantic%2520adversarial%2520unsupervised%2520domain%2520adaptation%2520for%2520Temporal%250A%2520%2520Action%2520Localization%26entry.906535625%3DDavid%2520Pujol-Perich%2520and%2520Albert%2520Clap%25C3%25A9s%2520and%2520Sergio%2520Escalera%26entry.1292438233%3D%2520%2520Temporal%2520Action%2520Localization%2520%2528TAL%2529%2520is%2520a%2520complex%2520task%2520that%2520poses%2520relevant%250Achallenges%252C%2520particularly%2520when%2520attempting%2520to%2520generalize%2520on%2520new%2520--%2520unseen%2520--%250Adomains%2520in%2520real-world%2520applications.%2520These%2520scenarios%252C%2520despite%2520realistic%252C%2520are%250Aoften%2520neglected%2520in%2520the%2520literature%252C%2520exposing%2520these%2520solutions%2520to%2520important%250Aperformance%2520degradation.%2520In%2520this%2520work%252C%2520we%2520tackle%2520this%2520issue%2520by%2520introducing%252C%2520for%250Athe%2520first%2520time%252C%2520an%2520approach%2520for%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520in%2520sparse%250ATAL%252C%2520which%2520we%2520refer%2520to%2520as%2520Semantic%2520Adversarial%2520unsupervised%2520Domain%2520Adaptation%250A%2528SADA%2529.%2520Our%2520contributions%2520are%2520threefold%253A%2520%25281%2529%2520we%2520pioneer%2520the%2520development%2520of%2520a%250Adomain%2520adaptation%2520model%2520that%2520operates%2520on%2520realistic%2520sparse%2520action%2520detection%250Abenchmarks%253B%2520%25282%2529%2520we%2520tackle%2520the%2520limitations%2520of%2520global-distribution%2520alignment%250Atechniques%2520by%2520introducing%2520a%2520novel%2520adversarial%2520loss%2520that%2520is%2520sensitive%2520to%2520local%250Aclass%2520distributions%252C%2520ensuring%2520finer-grained%2520adaptation%253B%2520and%2520%25283%2529%2520we%2520present%2520a%250Anovel%2520set%2520of%2520benchmarks%2520based%2520on%2520EpicKitchens100%2520and%2520CharadesEgo%252C%2520that%2520evaluate%250Amultiple%2520domain%2520shifts%2520in%2520a%2520comprehensive%2520manner.%2520Our%2520experiments%2520indicate%2520that%250ASADA%2520improves%2520the%2520adaptation%2520across%2520domains%2520when%2520compared%2520to%2520fully%2520supervised%250Astate-of-the-art%2520and%2520alternative%2520UDA%2520methods%252C%2520attaining%2520a%2520performance%2520boost%2520of%250Aup%2520to%25206.14%2525%2520mAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13377v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SADA%3A%20Semantic%20adversarial%20unsupervised%20domain%20adaptation%20for%20Temporal%0A%20%20Action%20Localization&entry.906535625=David%20Pujol-Perich%20and%20Albert%20Clap%C3%A9s%20and%20Sergio%20Escalera&entry.1292438233=%20%20Temporal%20Action%20Localization%20%28TAL%29%20is%20a%20complex%20task%20that%20poses%20relevant%0Achallenges%2C%20particularly%20when%20attempting%20to%20generalize%20on%20new%20--%20unseen%20--%0Adomains%20in%20real-world%20applications.%20These%20scenarios%2C%20despite%20realistic%2C%20are%0Aoften%20neglected%20in%20the%20literature%2C%20exposing%20these%20solutions%20to%20important%0Aperformance%20degradation.%20In%20this%20work%2C%20we%20tackle%20this%20issue%20by%20introducing%2C%20for%0Athe%20first%20time%2C%20an%20approach%20for%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20in%20sparse%0ATAL%2C%20which%20we%20refer%20to%20as%20Semantic%20Adversarial%20unsupervised%20Domain%20Adaptation%0A%28SADA%29.%20Our%20contributions%20are%20threefold%3A%20%281%29%20we%20pioneer%20the%20development%20of%20a%0Adomain%20adaptation%20model%20that%20operates%20on%20realistic%20sparse%20action%20detection%0Abenchmarks%3B%20%282%29%20we%20tackle%20the%20limitations%20of%20global-distribution%20alignment%0Atechniques%20by%20introducing%20a%20novel%20adversarial%20loss%20that%20is%20sensitive%20to%20local%0Aclass%20distributions%2C%20ensuring%20finer-grained%20adaptation%3B%20and%20%283%29%20we%20present%20a%0Anovel%20set%20of%20benchmarks%20based%20on%20EpicKitchens100%20and%20CharadesEgo%2C%20that%20evaluate%0Amultiple%20domain%20shifts%20in%20a%20comprehensive%20manner.%20Our%20experiments%20indicate%20that%0ASADA%20improves%20the%20adaptation%20across%20domains%20when%20compared%20to%20fully%20supervised%0Astate-of-the-art%20and%20alternative%20UDA%20methods%2C%20attaining%20a%20performance%20boost%20of%0Aup%20to%206.14%25%20mAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13377v3&entry.124074799=Read"},
{"title": "TLCFuse: Temporal Multi-Modality Fusion Towards Occlusion-Aware Semantic\n  Segmentation-Aided Motion Planning", "author": "Gustavo Salazar-Gomez and Wenqian Liu and Manuel Diaz-Zapata and David Sierra-Gonzalez and Christian Laugier", "abstract": "  In autonomous driving, addressing occlusion scenarios is crucial yet\nchallenging. Robust surrounding perception is essential for handling occlusions\nand aiding motion planning. State-of-the-art models fuse Lidar and Camera data\nto produce impressive perception results, but detecting occluded objects\nremains challenging. In this paper, we emphasize the crucial role of temporal\ncues by integrating them alongside these modalities to address this challenge.\nWe propose a novel approach for bird's eye view semantic grid segmentation,\nthat leverages sequential sensor data to achieve robustness against occlusions.\nOur model extracts information from the sensor readings using attention\noperations and aggregates this information into a lower-dimensional latent\nrepresentation, enabling thus the processing of multi-step inputs at each\nprediction step. Moreover, we show how it can also be directly applied to\nforecast the development of traffic scenes and be seamlessly integrated into a\nmotion planner for trajectory planning. On the semantic segmentation tasks, we\nevaluate our model on the nuScenes dataset and show that it outperforms other\nbaselines, with particularly large differences when evaluating on occluded and\npartially-occluded vehicles. Additionally, on motion planning task we are among\nthe early teams to train and evaluate on nuPlan, a cutting-edge large-scale\ndataset for motion planning.\n", "link": "http://arxiv.org/abs/2311.05319v2", "date": "2024-11-25", "relevancy": 2.344, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5978}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TLCFuse%3A%20Temporal%20Multi-Modality%20Fusion%20Towards%20Occlusion-Aware%20Semantic%0A%20%20Segmentation-Aided%20Motion%20Planning&body=Title%3A%20TLCFuse%3A%20Temporal%20Multi-Modality%20Fusion%20Towards%20Occlusion-Aware%20Semantic%0A%20%20Segmentation-Aided%20Motion%20Planning%0AAuthor%3A%20Gustavo%20Salazar-Gomez%20and%20Wenqian%20Liu%20and%20Manuel%20Diaz-Zapata%20and%20David%20Sierra-Gonzalez%20and%20Christian%20Laugier%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20addressing%20occlusion%20scenarios%20is%20crucial%20yet%0Achallenging.%20Robust%20surrounding%20perception%20is%20essential%20for%20handling%20occlusions%0Aand%20aiding%20motion%20planning.%20State-of-the-art%20models%20fuse%20Lidar%20and%20Camera%20data%0Ato%20produce%20impressive%20perception%20results%2C%20but%20detecting%20occluded%20objects%0Aremains%20challenging.%20In%20this%20paper%2C%20we%20emphasize%20the%20crucial%20role%20of%20temporal%0Acues%20by%20integrating%20them%20alongside%20these%20modalities%20to%20address%20this%20challenge.%0AWe%20propose%20a%20novel%20approach%20for%20bird%27s%20eye%20view%20semantic%20grid%20segmentation%2C%0Athat%20leverages%20sequential%20sensor%20data%20to%20achieve%20robustness%20against%20occlusions.%0AOur%20model%20extracts%20information%20from%20the%20sensor%20readings%20using%20attention%0Aoperations%20and%20aggregates%20this%20information%20into%20a%20lower-dimensional%20latent%0Arepresentation%2C%20enabling%20thus%20the%20processing%20of%20multi-step%20inputs%20at%20each%0Aprediction%20step.%20Moreover%2C%20we%20show%20how%20it%20can%20also%20be%20directly%20applied%20to%0Aforecast%20the%20development%20of%20traffic%20scenes%20and%20be%20seamlessly%20integrated%20into%20a%0Amotion%20planner%20for%20trajectory%20planning.%20On%20the%20semantic%20segmentation%20tasks%2C%20we%0Aevaluate%20our%20model%20on%20the%20nuScenes%20dataset%20and%20show%20that%20it%20outperforms%20other%0Abaselines%2C%20with%20particularly%20large%20differences%20when%20evaluating%20on%20occluded%20and%0Apartially-occluded%20vehicles.%20Additionally%2C%20on%20motion%20planning%20task%20we%20are%20among%0Athe%20early%20teams%20to%20train%20and%20evaluate%20on%20nuPlan%2C%20a%20cutting-edge%20large-scale%0Adataset%20for%20motion%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTLCFuse%253A%2520Temporal%2520Multi-Modality%2520Fusion%2520Towards%2520Occlusion-Aware%2520Semantic%250A%2520%2520Segmentation-Aided%2520Motion%2520Planning%26entry.906535625%3DGustavo%2520Salazar-Gomez%2520and%2520Wenqian%2520Liu%2520and%2520Manuel%2520Diaz-Zapata%2520and%2520David%2520Sierra-Gonzalez%2520and%2520Christian%2520Laugier%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520addressing%2520occlusion%2520scenarios%2520is%2520crucial%2520yet%250Achallenging.%2520Robust%2520surrounding%2520perception%2520is%2520essential%2520for%2520handling%2520occlusions%250Aand%2520aiding%2520motion%2520planning.%2520State-of-the-art%2520models%2520fuse%2520Lidar%2520and%2520Camera%2520data%250Ato%2520produce%2520impressive%2520perception%2520results%252C%2520but%2520detecting%2520occluded%2520objects%250Aremains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520emphasize%2520the%2520crucial%2520role%2520of%2520temporal%250Acues%2520by%2520integrating%2520them%2520alongside%2520these%2520modalities%2520to%2520address%2520this%2520challenge.%250AWe%2520propose%2520a%2520novel%2520approach%2520for%2520bird%2527s%2520eye%2520view%2520semantic%2520grid%2520segmentation%252C%250Athat%2520leverages%2520sequential%2520sensor%2520data%2520to%2520achieve%2520robustness%2520against%2520occlusions.%250AOur%2520model%2520extracts%2520information%2520from%2520the%2520sensor%2520readings%2520using%2520attention%250Aoperations%2520and%2520aggregates%2520this%2520information%2520into%2520a%2520lower-dimensional%2520latent%250Arepresentation%252C%2520enabling%2520thus%2520the%2520processing%2520of%2520multi-step%2520inputs%2520at%2520each%250Aprediction%2520step.%2520Moreover%252C%2520we%2520show%2520how%2520it%2520can%2520also%2520be%2520directly%2520applied%2520to%250Aforecast%2520the%2520development%2520of%2520traffic%2520scenes%2520and%2520be%2520seamlessly%2520integrated%2520into%2520a%250Amotion%2520planner%2520for%2520trajectory%2520planning.%2520On%2520the%2520semantic%2520segmentation%2520tasks%252C%2520we%250Aevaluate%2520our%2520model%2520on%2520the%2520nuScenes%2520dataset%2520and%2520show%2520that%2520it%2520outperforms%2520other%250Abaselines%252C%2520with%2520particularly%2520large%2520differences%2520when%2520evaluating%2520on%2520occluded%2520and%250Apartially-occluded%2520vehicles.%2520Additionally%252C%2520on%2520motion%2520planning%2520task%2520we%2520are%2520among%250Athe%2520early%2520teams%2520to%2520train%2520and%2520evaluate%2520on%2520nuPlan%252C%2520a%2520cutting-edge%2520large-scale%250Adataset%2520for%2520motion%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TLCFuse%3A%20Temporal%20Multi-Modality%20Fusion%20Towards%20Occlusion-Aware%20Semantic%0A%20%20Segmentation-Aided%20Motion%20Planning&entry.906535625=Gustavo%20Salazar-Gomez%20and%20Wenqian%20Liu%20and%20Manuel%20Diaz-Zapata%20and%20David%20Sierra-Gonzalez%20and%20Christian%20Laugier&entry.1292438233=%20%20In%20autonomous%20driving%2C%20addressing%20occlusion%20scenarios%20is%20crucial%20yet%0Achallenging.%20Robust%20surrounding%20perception%20is%20essential%20for%20handling%20occlusions%0Aand%20aiding%20motion%20planning.%20State-of-the-art%20models%20fuse%20Lidar%20and%20Camera%20data%0Ato%20produce%20impressive%20perception%20results%2C%20but%20detecting%20occluded%20objects%0Aremains%20challenging.%20In%20this%20paper%2C%20we%20emphasize%20the%20crucial%20role%20of%20temporal%0Acues%20by%20integrating%20them%20alongside%20these%20modalities%20to%20address%20this%20challenge.%0AWe%20propose%20a%20novel%20approach%20for%20bird%27s%20eye%20view%20semantic%20grid%20segmentation%2C%0Athat%20leverages%20sequential%20sensor%20data%20to%20achieve%20robustness%20against%20occlusions.%0AOur%20model%20extracts%20information%20from%20the%20sensor%20readings%20using%20attention%0Aoperations%20and%20aggregates%20this%20information%20into%20a%20lower-dimensional%20latent%0Arepresentation%2C%20enabling%20thus%20the%20processing%20of%20multi-step%20inputs%20at%20each%0Aprediction%20step.%20Moreover%2C%20we%20show%20how%20it%20can%20also%20be%20directly%20applied%20to%0Aforecast%20the%20development%20of%20traffic%20scenes%20and%20be%20seamlessly%20integrated%20into%20a%0Amotion%20planner%20for%20trajectory%20planning.%20On%20the%20semantic%20segmentation%20tasks%2C%20we%0Aevaluate%20our%20model%20on%20the%20nuScenes%20dataset%20and%20show%20that%20it%20outperforms%20other%0Abaselines%2C%20with%20particularly%20large%20differences%20when%20evaluating%20on%20occluded%20and%0Apartially-occluded%20vehicles.%20Additionally%2C%20on%20motion%20planning%20task%20we%20are%20among%0Athe%20early%20teams%20to%20train%20and%20evaluate%20on%20nuPlan%2C%20a%20cutting-edge%20large-scale%0Adataset%20for%20motion%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05319v2&entry.124074799=Read"},
{"title": "Privacy Protection in Personalized Diffusion Models via Targeted\n  Cross-Attention Adversarial Attack", "author": "Xide Xu and Muhammad Atif Butt and Sandesh Kamath and Bogdan Raducanu", "abstract": "  The growing demand for customized visual content has led to the rise of\npersonalized text-to-image (T2I) diffusion models. Despite their remarkable\npotential, they pose significant privacy risk when misused for malicious\npurposes. In this paper, we propose a novel and efficient adversarial attack\nmethod, Concept Protection by Selective Attention Manipulation (CoPSAM) which\ntargets only the cross-attention layers of a T2I diffusion model. For this\npurpose, we carefully construct an imperceptible noise to be added to clean\nsamples to get their adversarial counterparts. This is obtained during the\nfine-tuning process by maximizing the discrepancy between the corresponding\ncross-attention maps of the user-specific token and the class-specific token,\nrespectively. Experimental validation on a subset of CelebA-HQ face images\ndataset demonstrates that our approach outperforms existing methods. Besides\nthis, our method presents two important advantages derived from the qualitative\nevaluation: (i) we obtain better protection results for lower noise levels than\nour competitors; and (ii) we protect the content from unauthorized use thereby\nprotecting the individual's identity from potential misuse.\n", "link": "http://arxiv.org/abs/2411.16437v1", "date": "2024-11-25", "relevancy": 2.3399, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6251}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5976}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy%20Protection%20in%20Personalized%20Diffusion%20Models%20via%20Targeted%0A%20%20Cross-Attention%20Adversarial%20Attack&body=Title%3A%20Privacy%20Protection%20in%20Personalized%20Diffusion%20Models%20via%20Targeted%0A%20%20Cross-Attention%20Adversarial%20Attack%0AAuthor%3A%20Xide%20Xu%20and%20Muhammad%20Atif%20Butt%20and%20Sandesh%20Kamath%20and%20Bogdan%20Raducanu%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20customized%20visual%20content%20has%20led%20to%20the%20rise%20of%0Apersonalized%20text-to-image%20%28T2I%29%20diffusion%20models.%20Despite%20their%20remarkable%0Apotential%2C%20they%20pose%20significant%20privacy%20risk%20when%20misused%20for%20malicious%0Apurposes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20efficient%20adversarial%20attack%0Amethod%2C%20Concept%20Protection%20by%20Selective%20Attention%20Manipulation%20%28CoPSAM%29%20which%0Atargets%20only%20the%20cross-attention%20layers%20of%20a%20T2I%20diffusion%20model.%20For%20this%0Apurpose%2C%20we%20carefully%20construct%20an%20imperceptible%20noise%20to%20be%20added%20to%20clean%0Asamples%20to%20get%20their%20adversarial%20counterparts.%20This%20is%20obtained%20during%20the%0Afine-tuning%20process%20by%20maximizing%20the%20discrepancy%20between%20the%20corresponding%0Across-attention%20maps%20of%20the%20user-specific%20token%20and%20the%20class-specific%20token%2C%0Arespectively.%20Experimental%20validation%20on%20a%20subset%20of%20CelebA-HQ%20face%20images%0Adataset%20demonstrates%20that%20our%20approach%20outperforms%20existing%20methods.%20Besides%0Athis%2C%20our%20method%20presents%20two%20important%20advantages%20derived%20from%20the%20qualitative%0Aevaluation%3A%20%28i%29%20we%20obtain%20better%20protection%20results%20for%20lower%20noise%20levels%20than%0Aour%20competitors%3B%20and%20%28ii%29%20we%20protect%20the%20content%20from%20unauthorized%20use%20thereby%0Aprotecting%20the%20individual%27s%20identity%20from%20potential%20misuse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy%2520Protection%2520in%2520Personalized%2520Diffusion%2520Models%2520via%2520Targeted%250A%2520%2520Cross-Attention%2520Adversarial%2520Attack%26entry.906535625%3DXide%2520Xu%2520and%2520Muhammad%2520Atif%2520Butt%2520and%2520Sandesh%2520Kamath%2520and%2520Bogdan%2520Raducanu%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520customized%2520visual%2520content%2520has%2520led%2520to%2520the%2520rise%2520of%250Apersonalized%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models.%2520Despite%2520their%2520remarkable%250Apotential%252C%2520they%2520pose%2520significant%2520privacy%2520risk%2520when%2520misused%2520for%2520malicious%250Apurposes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520and%2520efficient%2520adversarial%2520attack%250Amethod%252C%2520Concept%2520Protection%2520by%2520Selective%2520Attention%2520Manipulation%2520%2528CoPSAM%2529%2520which%250Atargets%2520only%2520the%2520cross-attention%2520layers%2520of%2520a%2520T2I%2520diffusion%2520model.%2520For%2520this%250Apurpose%252C%2520we%2520carefully%2520construct%2520an%2520imperceptible%2520noise%2520to%2520be%2520added%2520to%2520clean%250Asamples%2520to%2520get%2520their%2520adversarial%2520counterparts.%2520This%2520is%2520obtained%2520during%2520the%250Afine-tuning%2520process%2520by%2520maximizing%2520the%2520discrepancy%2520between%2520the%2520corresponding%250Across-attention%2520maps%2520of%2520the%2520user-specific%2520token%2520and%2520the%2520class-specific%2520token%252C%250Arespectively.%2520Experimental%2520validation%2520on%2520a%2520subset%2520of%2520CelebA-HQ%2520face%2520images%250Adataset%2520demonstrates%2520that%2520our%2520approach%2520outperforms%2520existing%2520methods.%2520Besides%250Athis%252C%2520our%2520method%2520presents%2520two%2520important%2520advantages%2520derived%2520from%2520the%2520qualitative%250Aevaluation%253A%2520%2528i%2529%2520we%2520obtain%2520better%2520protection%2520results%2520for%2520lower%2520noise%2520levels%2520than%250Aour%2520competitors%253B%2520and%2520%2528ii%2529%2520we%2520protect%2520the%2520content%2520from%2520unauthorized%2520use%2520thereby%250Aprotecting%2520the%2520individual%2527s%2520identity%2520from%2520potential%2520misuse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy%20Protection%20in%20Personalized%20Diffusion%20Models%20via%20Targeted%0A%20%20Cross-Attention%20Adversarial%20Attack&entry.906535625=Xide%20Xu%20and%20Muhammad%20Atif%20Butt%20and%20Sandesh%20Kamath%20and%20Bogdan%20Raducanu&entry.1292438233=%20%20The%20growing%20demand%20for%20customized%20visual%20content%20has%20led%20to%20the%20rise%20of%0Apersonalized%20text-to-image%20%28T2I%29%20diffusion%20models.%20Despite%20their%20remarkable%0Apotential%2C%20they%20pose%20significant%20privacy%20risk%20when%20misused%20for%20malicious%0Apurposes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20efficient%20adversarial%20attack%0Amethod%2C%20Concept%20Protection%20by%20Selective%20Attention%20Manipulation%20%28CoPSAM%29%20which%0Atargets%20only%20the%20cross-attention%20layers%20of%20a%20T2I%20diffusion%20model.%20For%20this%0Apurpose%2C%20we%20carefully%20construct%20an%20imperceptible%20noise%20to%20be%20added%20to%20clean%0Asamples%20to%20get%20their%20adversarial%20counterparts.%20This%20is%20obtained%20during%20the%0Afine-tuning%20process%20by%20maximizing%20the%20discrepancy%20between%20the%20corresponding%0Across-attention%20maps%20of%20the%20user-specific%20token%20and%20the%20class-specific%20token%2C%0Arespectively.%20Experimental%20validation%20on%20a%20subset%20of%20CelebA-HQ%20face%20images%0Adataset%20demonstrates%20that%20our%20approach%20outperforms%20existing%20methods.%20Besides%0Athis%2C%20our%20method%20presents%20two%20important%20advantages%20derived%20from%20the%20qualitative%0Aevaluation%3A%20%28i%29%20we%20obtain%20better%20protection%20results%20for%20lower%20noise%20levels%20than%0Aour%20competitors%3B%20and%20%28ii%29%20we%20protect%20the%20content%20from%20unauthorized%20use%20thereby%0Aprotecting%20the%20individual%27s%20identity%20from%20potential%20misuse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16437v1&entry.124074799=Read"},
{"title": "Multi-Resolution Generative Modeling of Human Motion from Limited Data", "author": "David Eduardo Moreno-Villamar\u00edn and Anna Hilsmann and Peter Eisert", "abstract": "  We present a generative model that learns to synthesize human motion from\nlimited training sequences. Our framework provides conditional generation and\nblending across multiple temporal resolutions. The model adeptly captures human\nmotion patterns by integrating skeletal convolution layers and a multi-scale\narchitecture. Our model contains a set of generative and adversarial networks,\nalong with embedding modules, each tailored for generating motions at specific\nframe rates while exerting control over their content and details. Notably, our\napproach also extends to the synthesis of co-speech gestures, demonstrating its\nability to generate synchronized gestures from speech inputs, even with limited\npaired data. Through direct synthesis of SMPL pose parameters, our approach\navoids test-time adjustments to fit human body meshes. Experimental results\nshowcase our model's ability to achieve extensive coverage of training\nexamples, while generating diverse motions, as indicated by local and global\ndiversity metrics.\n", "link": "http://arxiv.org/abs/2411.16498v1", "date": "2024-11-25", "relevancy": 2.3352, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5862}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5849}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Resolution%20Generative%20Modeling%20of%20Human%20Motion%20from%20Limited%20Data&body=Title%3A%20Multi-Resolution%20Generative%20Modeling%20of%20Human%20Motion%20from%20Limited%20Data%0AAuthor%3A%20David%20Eduardo%20Moreno-Villamar%C3%ADn%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20%20%20We%20present%20a%20generative%20model%20that%20learns%20to%20synthesize%20human%20motion%20from%0Alimited%20training%20sequences.%20Our%20framework%20provides%20conditional%20generation%20and%0Ablending%20across%20multiple%20temporal%20resolutions.%20The%20model%20adeptly%20captures%20human%0Amotion%20patterns%20by%20integrating%20skeletal%20convolution%20layers%20and%20a%20multi-scale%0Aarchitecture.%20Our%20model%20contains%20a%20set%20of%20generative%20and%20adversarial%20networks%2C%0Aalong%20with%20embedding%20modules%2C%20each%20tailored%20for%20generating%20motions%20at%20specific%0Aframe%20rates%20while%20exerting%20control%20over%20their%20content%20and%20details.%20Notably%2C%20our%0Aapproach%20also%20extends%20to%20the%20synthesis%20of%20co-speech%20gestures%2C%20demonstrating%20its%0Aability%20to%20generate%20synchronized%20gestures%20from%20speech%20inputs%2C%20even%20with%20limited%0Apaired%20data.%20Through%20direct%20synthesis%20of%20SMPL%20pose%20parameters%2C%20our%20approach%0Aavoids%20test-time%20adjustments%20to%20fit%20human%20body%20meshes.%20Experimental%20results%0Ashowcase%20our%20model%27s%20ability%20to%20achieve%20extensive%20coverage%20of%20training%0Aexamples%2C%20while%20generating%20diverse%20motions%2C%20as%20indicated%20by%20local%20and%20global%0Adiversity%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Resolution%2520Generative%2520Modeling%2520of%2520Human%2520Motion%2520from%2520Limited%2520Data%26entry.906535625%3DDavid%2520Eduardo%2520Moreno-Villamar%25C3%25ADn%2520and%2520Anna%2520Hilsmann%2520and%2520Peter%2520Eisert%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520generative%2520model%2520that%2520learns%2520to%2520synthesize%2520human%2520motion%2520from%250Alimited%2520training%2520sequences.%2520Our%2520framework%2520provides%2520conditional%2520generation%2520and%250Ablending%2520across%2520multiple%2520temporal%2520resolutions.%2520The%2520model%2520adeptly%2520captures%2520human%250Amotion%2520patterns%2520by%2520integrating%2520skeletal%2520convolution%2520layers%2520and%2520a%2520multi-scale%250Aarchitecture.%2520Our%2520model%2520contains%2520a%2520set%2520of%2520generative%2520and%2520adversarial%2520networks%252C%250Aalong%2520with%2520embedding%2520modules%252C%2520each%2520tailored%2520for%2520generating%2520motions%2520at%2520specific%250Aframe%2520rates%2520while%2520exerting%2520control%2520over%2520their%2520content%2520and%2520details.%2520Notably%252C%2520our%250Aapproach%2520also%2520extends%2520to%2520the%2520synthesis%2520of%2520co-speech%2520gestures%252C%2520demonstrating%2520its%250Aability%2520to%2520generate%2520synchronized%2520gestures%2520from%2520speech%2520inputs%252C%2520even%2520with%2520limited%250Apaired%2520data.%2520Through%2520direct%2520synthesis%2520of%2520SMPL%2520pose%2520parameters%252C%2520our%2520approach%250Aavoids%2520test-time%2520adjustments%2520to%2520fit%2520human%2520body%2520meshes.%2520Experimental%2520results%250Ashowcase%2520our%2520model%2527s%2520ability%2520to%2520achieve%2520extensive%2520coverage%2520of%2520training%250Aexamples%252C%2520while%2520generating%2520diverse%2520motions%252C%2520as%2520indicated%2520by%2520local%2520and%2520global%250Adiversity%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Resolution%20Generative%20Modeling%20of%20Human%20Motion%20from%20Limited%20Data&entry.906535625=David%20Eduardo%20Moreno-Villamar%C3%ADn%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=%20%20We%20present%20a%20generative%20model%20that%20learns%20to%20synthesize%20human%20motion%20from%0Alimited%20training%20sequences.%20Our%20framework%20provides%20conditional%20generation%20and%0Ablending%20across%20multiple%20temporal%20resolutions.%20The%20model%20adeptly%20captures%20human%0Amotion%20patterns%20by%20integrating%20skeletal%20convolution%20layers%20and%20a%20multi-scale%0Aarchitecture.%20Our%20model%20contains%20a%20set%20of%20generative%20and%20adversarial%20networks%2C%0Aalong%20with%20embedding%20modules%2C%20each%20tailored%20for%20generating%20motions%20at%20specific%0Aframe%20rates%20while%20exerting%20control%20over%20their%20content%20and%20details.%20Notably%2C%20our%0Aapproach%20also%20extends%20to%20the%20synthesis%20of%20co-speech%20gestures%2C%20demonstrating%20its%0Aability%20to%20generate%20synchronized%20gestures%20from%20speech%20inputs%2C%20even%20with%20limited%0Apaired%20data.%20Through%20direct%20synthesis%20of%20SMPL%20pose%20parameters%2C%20our%20approach%0Aavoids%20test-time%20adjustments%20to%20fit%20human%20body%20meshes.%20Experimental%20results%0Ashowcase%20our%20model%27s%20ability%20to%20achieve%20extensive%20coverage%20of%20training%0Aexamples%2C%20while%20generating%20diverse%20motions%2C%20as%20indicated%20by%20local%20and%20global%0Adiversity%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16498v1&entry.124074799=Read"},
{"title": "VQA$^2$: Visual Question Answering for Video Quality Assessment", "author": "Ziheng Jia and Zicheng Zhang and Jiaying Qian and Haoning Wu and Wei Sun and Chunyi Li and Xiaohong Liu and Weisi Lin and Guangtao Zhai and Xiongkuo Min", "abstract": "  The advent and proliferation of large multi-modal models (LMMs) have\nintroduced new paradigms to computer vision, transforming various tasks into a\nunified visual question answering framework. Video Quality Assessment (VQA), a\nclassic field in low-level visual perception, focused initially on quantitative\nvideo quality scoring. However, driven by advances in LMMs, it is now\nprogressing toward more holistic visual quality understanding tasks. Recent\nstudies in the image domain have demonstrated that Visual Question Answering\n(VQA) can markedly enhance low-level visual quality evaluation. Nevertheless,\nrelated work has not been explored in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset - the first visual question answering instruction dataset that focuses\non video quality assessment. This dataset consists of 3 subsets and covers\nvarious video types, containing 157,755 instruction question-answer pairs.\nThen, leveraging this foundation, we present the VQA2 series models. The VQA2\nseries models interleave visual and motion tokens to enhance the perception of\nspatial-temporal quality details in videos. We conduct extensive experiments on\nvideo quality scoring and understanding tasks, and results demonstrate that the\nVQA2series models achieve excellent performance in both tasks. Notably, our\nfinal model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual quality\nunderstanding tasks while maintaining strong competitiveness in quality scoring\ntasks. Our work provides a foundation and feasible approach for integrating\nlow-level video quality assessment and understanding with LMMs.\n", "link": "http://arxiv.org/abs/2411.03795v3", "date": "2024-11-25", "relevancy": 2.3234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5862}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQA%24%5E2%24%3A%20Visual%20Question%20Answering%20for%20Video%20Quality%20Assessment&body=Title%3A%20VQA%24%5E2%24%3A%20Visual%20Question%20Answering%20for%20Video%20Quality%20Assessment%0AAuthor%3A%20Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Jiaying%20Qian%20and%20Haoning%20Wu%20and%20Wei%20Sun%20and%20Chunyi%20Li%20and%20Xiaohong%20Liu%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min%0AAbstract%3A%20%20%20The%20advent%20and%20proliferation%20of%20large%20multi-modal%20models%20%28LMMs%29%20have%0Aintroduced%20new%20paradigms%20to%20computer%20vision%2C%20transforming%20various%20tasks%20into%20a%0Aunified%20visual%20question%20answering%20framework.%20Video%20Quality%20Assessment%20%28VQA%29%2C%20a%0Aclassic%20field%20in%20low-level%20visual%20perception%2C%20focused%20initially%20on%20quantitative%0Avideo%20quality%20scoring.%20However%2C%20driven%20by%20advances%20in%20LMMs%2C%20it%20is%20now%0Aprogressing%20toward%20more%20holistic%20visual%20quality%20understanding%20tasks.%20Recent%0Astudies%20in%20the%20image%20domain%20have%20demonstrated%20that%20Visual%20Question%20Answering%0A%28VQA%29%20can%20markedly%20enhance%20low-level%20visual%20quality%20evaluation.%20Nevertheless%2C%0Arelated%20work%20has%20not%20been%20explored%20in%20the%20video%20domain%2C%20leaving%20substantial%0Aroom%20for%20improvement.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20VQA2%20Instruction%0ADataset%20-%20the%20first%20visual%20question%20answering%20instruction%20dataset%20that%20focuses%0Aon%20video%20quality%20assessment.%20This%20dataset%20consists%20of%203%20subsets%20and%20covers%0Avarious%20video%20types%2C%20containing%20157%2C755%20instruction%20question-answer%20pairs.%0AThen%2C%20leveraging%20this%20foundation%2C%20we%20present%20the%20VQA2%20series%20models.%20The%20VQA2%0Aseries%20models%20interleave%20visual%20and%20motion%20tokens%20to%20enhance%20the%20perception%20of%0Aspatial-temporal%20quality%20details%20in%20videos.%20We%20conduct%20extensive%20experiments%20on%0Avideo%20quality%20scoring%20and%20understanding%20tasks%2C%20and%20results%20demonstrate%20that%20the%0AVQA2series%20models%20achieve%20excellent%20performance%20in%20both%20tasks.%20Notably%2C%20our%0Afinal%20model%2C%20the%20VQA2-Assistant%2C%20exceeds%20the%20renowned%20GPT-4o%20in%20visual%20quality%0Aunderstanding%20tasks%20while%20maintaining%20strong%20competitiveness%20in%20quality%20scoring%0Atasks.%20Our%20work%20provides%20a%20foundation%20and%20feasible%20approach%20for%20integrating%0Alow-level%20video%20quality%20assessment%20and%20understanding%20with%20LMMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03795v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQA%2524%255E2%2524%253A%2520Visual%2520Question%2520Answering%2520for%2520Video%2520Quality%2520Assessment%26entry.906535625%3DZiheng%2520Jia%2520and%2520Zicheng%2520Zhang%2520and%2520Jiaying%2520Qian%2520and%2520Haoning%2520Wu%2520and%2520Wei%2520Sun%2520and%2520Chunyi%2520Li%2520and%2520Xiaohong%2520Liu%2520and%2520Weisi%2520Lin%2520and%2520Guangtao%2520Zhai%2520and%2520Xiongkuo%2520Min%26entry.1292438233%3D%2520%2520The%2520advent%2520and%2520proliferation%2520of%2520large%2520multi-modal%2520models%2520%2528LMMs%2529%2520have%250Aintroduced%2520new%2520paradigms%2520to%2520computer%2520vision%252C%2520transforming%2520various%2520tasks%2520into%2520a%250Aunified%2520visual%2520question%2520answering%2520framework.%2520Video%2520Quality%2520Assessment%2520%2528VQA%2529%252C%2520a%250Aclassic%2520field%2520in%2520low-level%2520visual%2520perception%252C%2520focused%2520initially%2520on%2520quantitative%250Avideo%2520quality%2520scoring.%2520However%252C%2520driven%2520by%2520advances%2520in%2520LMMs%252C%2520it%2520is%2520now%250Aprogressing%2520toward%2520more%2520holistic%2520visual%2520quality%2520understanding%2520tasks.%2520Recent%250Astudies%2520in%2520the%2520image%2520domain%2520have%2520demonstrated%2520that%2520Visual%2520Question%2520Answering%250A%2528VQA%2529%2520can%2520markedly%2520enhance%2520low-level%2520visual%2520quality%2520evaluation.%2520Nevertheless%252C%250Arelated%2520work%2520has%2520not%2520been%2520explored%2520in%2520the%2520video%2520domain%252C%2520leaving%2520substantial%250Aroom%2520for%2520improvement.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520VQA2%2520Instruction%250ADataset%2520-%2520the%2520first%2520visual%2520question%2520answering%2520instruction%2520dataset%2520that%2520focuses%250Aon%2520video%2520quality%2520assessment.%2520This%2520dataset%2520consists%2520of%25203%2520subsets%2520and%2520covers%250Avarious%2520video%2520types%252C%2520containing%2520157%252C755%2520instruction%2520question-answer%2520pairs.%250AThen%252C%2520leveraging%2520this%2520foundation%252C%2520we%2520present%2520the%2520VQA2%2520series%2520models.%2520The%2520VQA2%250Aseries%2520models%2520interleave%2520visual%2520and%2520motion%2520tokens%2520to%2520enhance%2520the%2520perception%2520of%250Aspatial-temporal%2520quality%2520details%2520in%2520videos.%2520We%2520conduct%2520extensive%2520experiments%2520on%250Avideo%2520quality%2520scoring%2520and%2520understanding%2520tasks%252C%2520and%2520results%2520demonstrate%2520that%2520the%250AVQA2series%2520models%2520achieve%2520excellent%2520performance%2520in%2520both%2520tasks.%2520Notably%252C%2520our%250Afinal%2520model%252C%2520the%2520VQA2-Assistant%252C%2520exceeds%2520the%2520renowned%2520GPT-4o%2520in%2520visual%2520quality%250Aunderstanding%2520tasks%2520while%2520maintaining%2520strong%2520competitiveness%2520in%2520quality%2520scoring%250Atasks.%2520Our%2520work%2520provides%2520a%2520foundation%2520and%2520feasible%2520approach%2520for%2520integrating%250Alow-level%2520video%2520quality%2520assessment%2520and%2520understanding%2520with%2520LMMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03795v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQA%24%5E2%24%3A%20Visual%20Question%20Answering%20for%20Video%20Quality%20Assessment&entry.906535625=Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Jiaying%20Qian%20and%20Haoning%20Wu%20and%20Wei%20Sun%20and%20Chunyi%20Li%20and%20Xiaohong%20Liu%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai%20and%20Xiongkuo%20Min&entry.1292438233=%20%20The%20advent%20and%20proliferation%20of%20large%20multi-modal%20models%20%28LMMs%29%20have%0Aintroduced%20new%20paradigms%20to%20computer%20vision%2C%20transforming%20various%20tasks%20into%20a%0Aunified%20visual%20question%20answering%20framework.%20Video%20Quality%20Assessment%20%28VQA%29%2C%20a%0Aclassic%20field%20in%20low-level%20visual%20perception%2C%20focused%20initially%20on%20quantitative%0Avideo%20quality%20scoring.%20However%2C%20driven%20by%20advances%20in%20LMMs%2C%20it%20is%20now%0Aprogressing%20toward%20more%20holistic%20visual%20quality%20understanding%20tasks.%20Recent%0Astudies%20in%20the%20image%20domain%20have%20demonstrated%20that%20Visual%20Question%20Answering%0A%28VQA%29%20can%20markedly%20enhance%20low-level%20visual%20quality%20evaluation.%20Nevertheless%2C%0Arelated%20work%20has%20not%20been%20explored%20in%20the%20video%20domain%2C%20leaving%20substantial%0Aroom%20for%20improvement.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20VQA2%20Instruction%0ADataset%20-%20the%20first%20visual%20question%20answering%20instruction%20dataset%20that%20focuses%0Aon%20video%20quality%20assessment.%20This%20dataset%20consists%20of%203%20subsets%20and%20covers%0Avarious%20video%20types%2C%20containing%20157%2C755%20instruction%20question-answer%20pairs.%0AThen%2C%20leveraging%20this%20foundation%2C%20we%20present%20the%20VQA2%20series%20models.%20The%20VQA2%0Aseries%20models%20interleave%20visual%20and%20motion%20tokens%20to%20enhance%20the%20perception%20of%0Aspatial-temporal%20quality%20details%20in%20videos.%20We%20conduct%20extensive%20experiments%20on%0Avideo%20quality%20scoring%20and%20understanding%20tasks%2C%20and%20results%20demonstrate%20that%20the%0AVQA2series%20models%20achieve%20excellent%20performance%20in%20both%20tasks.%20Notably%2C%20our%0Afinal%20model%2C%20the%20VQA2-Assistant%2C%20exceeds%20the%20renowned%20GPT-4o%20in%20visual%20quality%0Aunderstanding%20tasks%20while%20maintaining%20strong%20competitiveness%20in%20quality%20scoring%0Atasks.%20Our%20work%20provides%20a%20foundation%20and%20feasible%20approach%20for%20integrating%0Alow-level%20video%20quality%20assessment%20and%20understanding%20with%20LMMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03795v3&entry.124074799=Read"},
{"title": "DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation", "author": "Zun Wang and Jialu Li and Han Lin and Jaehong Yoon and Mohit Bansal", "abstract": "  Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.\n", "link": "http://arxiv.org/abs/2411.16657v1", "date": "2024-11-25", "relevancy": 2.3182, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6172}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5948}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamRunner%3A%20Fine-Grained%20Storytelling%20Video%20Generation%20with%0A%20%20Retrieval-Augmented%20Motion%20Adaptation&body=Title%3A%20DreamRunner%3A%20Fine-Grained%20Storytelling%20Video%20Generation%20with%0A%20%20Retrieval-Augmented%20Motion%20Adaptation%0AAuthor%3A%20Zun%20Wang%20and%20Jialu%20Li%20and%20Han%20Lin%20and%20Jaehong%20Yoon%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Storytelling%20video%20generation%20%28SVG%29%20has%20recently%20emerged%20as%20a%20task%20to%20create%0Along%2C%20multi-motion%2C%20multi-scene%20videos%20that%20consistently%20represent%20the%20story%0Adescribed%20in%20the%20input%20text%20script.%20SVG%20holds%20great%20potential%20for%20diverse%0Acontent%20creation%20in%20media%20and%20entertainment%3B%20however%2C%20it%20also%20presents%0Asignificant%20challenges%3A%20%281%29%20objects%20must%20exhibit%20a%20range%20of%20fine-grained%2C%0Acomplex%20motions%2C%20%282%29%20multiple%20objects%20need%20to%20appear%20consistently%20across%0Ascenes%2C%20and%20%283%29%20subjects%20may%20require%20multiple%20motions%20with%20seamless%20transitions%0Awithin%20a%20single%20scene.%20To%20address%20these%20challenges%2C%20we%20propose%20DreamRunner%2C%20a%0Anovel%20story-to-video%20generation%20method%3A%20First%2C%20we%20structure%20the%20input%20script%0Ausing%20a%20large%20language%20model%20%28LLM%29%20to%20facilitate%20both%20coarse-grained%20scene%0Aplanning%20as%20well%20as%20fine-grained%20object-level%20layout%20and%20motion%20planning.%20Next%2C%0ADreamRunner%20presents%20retrieval-augmented%20test-time%20adaptation%20to%20capture%20target%0Amotion%20priors%20for%20objects%20in%20each%20scene%2C%20supporting%20diverse%20motion%0Acustomization%20based%20on%20retrieved%20videos%2C%20thus%20facilitating%20the%20generation%20of%0Anew%20videos%20with%20complex%2C%20scripted%20motions.%20Lastly%2C%20we%20propose%20a%20novel%0Aspatial-temporal%20region-based%203D%20attention%20and%20prior%20injection%20module%20SR3AI%20for%0Afine-grained%20object-motion%20binding%20and%20frame-by-frame%20semantic%20control.%20We%0Acompare%20DreamRunner%20with%20various%20SVG%20baselines%2C%20demonstrating%20state-of-the-art%0Aperformance%20in%20character%20consistency%2C%20text%20alignment%2C%20and%20smooth%20transitions.%0AAdditionally%2C%20DreamRunner%20exhibits%20strong%20fine-grained%20condition-following%0Aability%20in%20compositional%20text-to-video%20generation%2C%20significantly%20outperforming%0Abaselines%20on%20T2V-ComBench.%20Finally%2C%20we%20validate%20DreamRunner%27s%20robust%20ability%20to%0Agenerate%20multi-object%20interactions%20with%20qualitative%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamRunner%253A%2520Fine-Grained%2520Storytelling%2520Video%2520Generation%2520with%250A%2520%2520Retrieval-Augmented%2520Motion%2520Adaptation%26entry.906535625%3DZun%2520Wang%2520and%2520Jialu%2520Li%2520and%2520Han%2520Lin%2520and%2520Jaehong%2520Yoon%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Storytelling%2520video%2520generation%2520%2528SVG%2529%2520has%2520recently%2520emerged%2520as%2520a%2520task%2520to%2520create%250Along%252C%2520multi-motion%252C%2520multi-scene%2520videos%2520that%2520consistently%2520represent%2520the%2520story%250Adescribed%2520in%2520the%2520input%2520text%2520script.%2520SVG%2520holds%2520great%2520potential%2520for%2520diverse%250Acontent%2520creation%2520in%2520media%2520and%2520entertainment%253B%2520however%252C%2520it%2520also%2520presents%250Asignificant%2520challenges%253A%2520%25281%2529%2520objects%2520must%2520exhibit%2520a%2520range%2520of%2520fine-grained%252C%250Acomplex%2520motions%252C%2520%25282%2529%2520multiple%2520objects%2520need%2520to%2520appear%2520consistently%2520across%250Ascenes%252C%2520and%2520%25283%2529%2520subjects%2520may%2520require%2520multiple%2520motions%2520with%2520seamless%2520transitions%250Awithin%2520a%2520single%2520scene.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DreamRunner%252C%2520a%250Anovel%2520story-to-video%2520generation%2520method%253A%2520First%252C%2520we%2520structure%2520the%2520input%2520script%250Ausing%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520facilitate%2520both%2520coarse-grained%2520scene%250Aplanning%2520as%2520well%2520as%2520fine-grained%2520object-level%2520layout%2520and%2520motion%2520planning.%2520Next%252C%250ADreamRunner%2520presents%2520retrieval-augmented%2520test-time%2520adaptation%2520to%2520capture%2520target%250Amotion%2520priors%2520for%2520objects%2520in%2520each%2520scene%252C%2520supporting%2520diverse%2520motion%250Acustomization%2520based%2520on%2520retrieved%2520videos%252C%2520thus%2520facilitating%2520the%2520generation%2520of%250Anew%2520videos%2520with%2520complex%252C%2520scripted%2520motions.%2520Lastly%252C%2520we%2520propose%2520a%2520novel%250Aspatial-temporal%2520region-based%25203D%2520attention%2520and%2520prior%2520injection%2520module%2520SR3AI%2520for%250Afine-grained%2520object-motion%2520binding%2520and%2520frame-by-frame%2520semantic%2520control.%2520We%250Acompare%2520DreamRunner%2520with%2520various%2520SVG%2520baselines%252C%2520demonstrating%2520state-of-the-art%250Aperformance%2520in%2520character%2520consistency%252C%2520text%2520alignment%252C%2520and%2520smooth%2520transitions.%250AAdditionally%252C%2520DreamRunner%2520exhibits%2520strong%2520fine-grained%2520condition-following%250Aability%2520in%2520compositional%2520text-to-video%2520generation%252C%2520significantly%2520outperforming%250Abaselines%2520on%2520T2V-ComBench.%2520Finally%252C%2520we%2520validate%2520DreamRunner%2527s%2520robust%2520ability%2520to%250Agenerate%2520multi-object%2520interactions%2520with%2520qualitative%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamRunner%3A%20Fine-Grained%20Storytelling%20Video%20Generation%20with%0A%20%20Retrieval-Augmented%20Motion%20Adaptation&entry.906535625=Zun%20Wang%20and%20Jialu%20Li%20and%20Han%20Lin%20and%20Jaehong%20Yoon%20and%20Mohit%20Bansal&entry.1292438233=%20%20Storytelling%20video%20generation%20%28SVG%29%20has%20recently%20emerged%20as%20a%20task%20to%20create%0Along%2C%20multi-motion%2C%20multi-scene%20videos%20that%20consistently%20represent%20the%20story%0Adescribed%20in%20the%20input%20text%20script.%20SVG%20holds%20great%20potential%20for%20diverse%0Acontent%20creation%20in%20media%20and%20entertainment%3B%20however%2C%20it%20also%20presents%0Asignificant%20challenges%3A%20%281%29%20objects%20must%20exhibit%20a%20range%20of%20fine-grained%2C%0Acomplex%20motions%2C%20%282%29%20multiple%20objects%20need%20to%20appear%20consistently%20across%0Ascenes%2C%20and%20%283%29%20subjects%20may%20require%20multiple%20motions%20with%20seamless%20transitions%0Awithin%20a%20single%20scene.%20To%20address%20these%20challenges%2C%20we%20propose%20DreamRunner%2C%20a%0Anovel%20story-to-video%20generation%20method%3A%20First%2C%20we%20structure%20the%20input%20script%0Ausing%20a%20large%20language%20model%20%28LLM%29%20to%20facilitate%20both%20coarse-grained%20scene%0Aplanning%20as%20well%20as%20fine-grained%20object-level%20layout%20and%20motion%20planning.%20Next%2C%0ADreamRunner%20presents%20retrieval-augmented%20test-time%20adaptation%20to%20capture%20target%0Amotion%20priors%20for%20objects%20in%20each%20scene%2C%20supporting%20diverse%20motion%0Acustomization%20based%20on%20retrieved%20videos%2C%20thus%20facilitating%20the%20generation%20of%0Anew%20videos%20with%20complex%2C%20scripted%20motions.%20Lastly%2C%20we%20propose%20a%20novel%0Aspatial-temporal%20region-based%203D%20attention%20and%20prior%20injection%20module%20SR3AI%20for%0Afine-grained%20object-motion%20binding%20and%20frame-by-frame%20semantic%20control.%20We%0Acompare%20DreamRunner%20with%20various%20SVG%20baselines%2C%20demonstrating%20state-of-the-art%0Aperformance%20in%20character%20consistency%2C%20text%20alignment%2C%20and%20smooth%20transitions.%0AAdditionally%2C%20DreamRunner%20exhibits%20strong%20fine-grained%20condition-following%0Aability%20in%20compositional%20text-to-video%20generation%2C%20significantly%20outperforming%0Abaselines%20on%20T2V-ComBench.%20Finally%2C%20we%20validate%20DreamRunner%27s%20robust%20ability%20to%0Agenerate%20multi-object%20interactions%20with%20qualitative%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16657v1&entry.124074799=Read"},
{"title": "Multimodal Foundation Models Exploit Text to Make Medical Image\n  Predictions", "author": "Thomas Buckley and James A. Diao and Pranav Rajpurkar and Adam Rodman and Arjun K. Manrai", "abstract": "  Multimodal foundation models have shown compelling but conflicting\nperformance in medical image interpretation. However, the mechanisms by which\nthese models integrate and prioritize different data modalities, including\nimages and text, remain poorly understood. Here, using a diverse collection of\n1014 multimodal medical cases, we evaluate the unimodal and multimodal image\ninterpretation abilities of proprietary (GPT-4, Gemini Pro 1.0) and open-source\n(Llama-3.2-90B, LLaVA-Med-v1.5) multimodal foundational models with and without\nthe use of text descriptions. Across all models, image predictions were largely\ndriven by exploiting text, with accuracy increasing monotonically with the\namount of informative text. By contrast, human performance on medical image\ninterpretation did not improve with informative text. Exploitation of text is a\ndouble-edged sword; we show that even mild suggestions of an incorrect\ndiagnosis in text diminishes image-based classification, reducing performance\ndramatically in cases the model could previously answer with images alone.\nFinally, we conducted a physician evaluation of model performance on long-form\nmedical cases, finding that the provision of images either reduced or had no\neffect on model performance when text is already highly informative. Our\nresults suggest that multimodal AI models may be useful in medical diagnostic\nreasoning but that their accuracy is largely driven, for better and worse, by\ntheir exploitation of text.\n", "link": "http://arxiv.org/abs/2311.05591v2", "date": "2024-11-25", "relevancy": 2.3042, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Foundation%20Models%20Exploit%20Text%20to%20Make%20Medical%20Image%0A%20%20Predictions&body=Title%3A%20Multimodal%20Foundation%20Models%20Exploit%20Text%20to%20Make%20Medical%20Image%0A%20%20Predictions%0AAuthor%3A%20Thomas%20Buckley%20and%20James%20A.%20Diao%20and%20Pranav%20Rajpurkar%20and%20Adam%20Rodman%20and%20Arjun%20K.%20Manrai%0AAbstract%3A%20%20%20Multimodal%20foundation%20models%20have%20shown%20compelling%20but%20conflicting%0Aperformance%20in%20medical%20image%20interpretation.%20However%2C%20the%20mechanisms%20by%20which%0Athese%20models%20integrate%20and%20prioritize%20different%20data%20modalities%2C%20including%0Aimages%20and%20text%2C%20remain%20poorly%20understood.%20Here%2C%20using%20a%20diverse%20collection%20of%0A1014%20multimodal%20medical%20cases%2C%20we%20evaluate%20the%20unimodal%20and%20multimodal%20image%0Ainterpretation%20abilities%20of%20proprietary%20%28GPT-4%2C%20Gemini%20Pro%201.0%29%20and%20open-source%0A%28Llama-3.2-90B%2C%20LLaVA-Med-v1.5%29%20multimodal%20foundational%20models%20with%20and%20without%0Athe%20use%20of%20text%20descriptions.%20Across%20all%20models%2C%20image%20predictions%20were%20largely%0Adriven%20by%20exploiting%20text%2C%20with%20accuracy%20increasing%20monotonically%20with%20the%0Aamount%20of%20informative%20text.%20By%20contrast%2C%20human%20performance%20on%20medical%20image%0Ainterpretation%20did%20not%20improve%20with%20informative%20text.%20Exploitation%20of%20text%20is%20a%0Adouble-edged%20sword%3B%20we%20show%20that%20even%20mild%20suggestions%20of%20an%20incorrect%0Adiagnosis%20in%20text%20diminishes%20image-based%20classification%2C%20reducing%20performance%0Adramatically%20in%20cases%20the%20model%20could%20previously%20answer%20with%20images%20alone.%0AFinally%2C%20we%20conducted%20a%20physician%20evaluation%20of%20model%20performance%20on%20long-form%0Amedical%20cases%2C%20finding%20that%20the%20provision%20of%20images%20either%20reduced%20or%20had%20no%0Aeffect%20on%20model%20performance%20when%20text%20is%20already%20highly%20informative.%20Our%0Aresults%20suggest%20that%20multimodal%20AI%20models%20may%20be%20useful%20in%20medical%20diagnostic%0Areasoning%20but%20that%20their%20accuracy%20is%20largely%20driven%2C%20for%20better%20and%20worse%2C%20by%0Atheir%20exploitation%20of%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05591v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Foundation%2520Models%2520Exploit%2520Text%2520to%2520Make%2520Medical%2520Image%250A%2520%2520Predictions%26entry.906535625%3DThomas%2520Buckley%2520and%2520James%2520A.%2520Diao%2520and%2520Pranav%2520Rajpurkar%2520and%2520Adam%2520Rodman%2520and%2520Arjun%2520K.%2520Manrai%26entry.1292438233%3D%2520%2520Multimodal%2520foundation%2520models%2520have%2520shown%2520compelling%2520but%2520conflicting%250Aperformance%2520in%2520medical%2520image%2520interpretation.%2520However%252C%2520the%2520mechanisms%2520by%2520which%250Athese%2520models%2520integrate%2520and%2520prioritize%2520different%2520data%2520modalities%252C%2520including%250Aimages%2520and%2520text%252C%2520remain%2520poorly%2520understood.%2520Here%252C%2520using%2520a%2520diverse%2520collection%2520of%250A1014%2520multimodal%2520medical%2520cases%252C%2520we%2520evaluate%2520the%2520unimodal%2520and%2520multimodal%2520image%250Ainterpretation%2520abilities%2520of%2520proprietary%2520%2528GPT-4%252C%2520Gemini%2520Pro%25201.0%2529%2520and%2520open-source%250A%2528Llama-3.2-90B%252C%2520LLaVA-Med-v1.5%2529%2520multimodal%2520foundational%2520models%2520with%2520and%2520without%250Athe%2520use%2520of%2520text%2520descriptions.%2520Across%2520all%2520models%252C%2520image%2520predictions%2520were%2520largely%250Adriven%2520by%2520exploiting%2520text%252C%2520with%2520accuracy%2520increasing%2520monotonically%2520with%2520the%250Aamount%2520of%2520informative%2520text.%2520By%2520contrast%252C%2520human%2520performance%2520on%2520medical%2520image%250Ainterpretation%2520did%2520not%2520improve%2520with%2520informative%2520text.%2520Exploitation%2520of%2520text%2520is%2520a%250Adouble-edged%2520sword%253B%2520we%2520show%2520that%2520even%2520mild%2520suggestions%2520of%2520an%2520incorrect%250Adiagnosis%2520in%2520text%2520diminishes%2520image-based%2520classification%252C%2520reducing%2520performance%250Adramatically%2520in%2520cases%2520the%2520model%2520could%2520previously%2520answer%2520with%2520images%2520alone.%250AFinally%252C%2520we%2520conducted%2520a%2520physician%2520evaluation%2520of%2520model%2520performance%2520on%2520long-form%250Amedical%2520cases%252C%2520finding%2520that%2520the%2520provision%2520of%2520images%2520either%2520reduced%2520or%2520had%2520no%250Aeffect%2520on%2520model%2520performance%2520when%2520text%2520is%2520already%2520highly%2520informative.%2520Our%250Aresults%2520suggest%2520that%2520multimodal%2520AI%2520models%2520may%2520be%2520useful%2520in%2520medical%2520diagnostic%250Areasoning%2520but%2520that%2520their%2520accuracy%2520is%2520largely%2520driven%252C%2520for%2520better%2520and%2520worse%252C%2520by%250Atheir%2520exploitation%2520of%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05591v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Foundation%20Models%20Exploit%20Text%20to%20Make%20Medical%20Image%0A%20%20Predictions&entry.906535625=Thomas%20Buckley%20and%20James%20A.%20Diao%20and%20Pranav%20Rajpurkar%20and%20Adam%20Rodman%20and%20Arjun%20K.%20Manrai&entry.1292438233=%20%20Multimodal%20foundation%20models%20have%20shown%20compelling%20but%20conflicting%0Aperformance%20in%20medical%20image%20interpretation.%20However%2C%20the%20mechanisms%20by%20which%0Athese%20models%20integrate%20and%20prioritize%20different%20data%20modalities%2C%20including%0Aimages%20and%20text%2C%20remain%20poorly%20understood.%20Here%2C%20using%20a%20diverse%20collection%20of%0A1014%20multimodal%20medical%20cases%2C%20we%20evaluate%20the%20unimodal%20and%20multimodal%20image%0Ainterpretation%20abilities%20of%20proprietary%20%28GPT-4%2C%20Gemini%20Pro%201.0%29%20and%20open-source%0A%28Llama-3.2-90B%2C%20LLaVA-Med-v1.5%29%20multimodal%20foundational%20models%20with%20and%20without%0Athe%20use%20of%20text%20descriptions.%20Across%20all%20models%2C%20image%20predictions%20were%20largely%0Adriven%20by%20exploiting%20text%2C%20with%20accuracy%20increasing%20monotonically%20with%20the%0Aamount%20of%20informative%20text.%20By%20contrast%2C%20human%20performance%20on%20medical%20image%0Ainterpretation%20did%20not%20improve%20with%20informative%20text.%20Exploitation%20of%20text%20is%20a%0Adouble-edged%20sword%3B%20we%20show%20that%20even%20mild%20suggestions%20of%20an%20incorrect%0Adiagnosis%20in%20text%20diminishes%20image-based%20classification%2C%20reducing%20performance%0Adramatically%20in%20cases%20the%20model%20could%20previously%20answer%20with%20images%20alone.%0AFinally%2C%20we%20conducted%20a%20physician%20evaluation%20of%20model%20performance%20on%20long-form%0Amedical%20cases%2C%20finding%20that%20the%20provision%20of%20images%20either%20reduced%20or%20had%20no%0Aeffect%20on%20model%20performance%20when%20text%20is%20already%20highly%20informative.%20Our%0Aresults%20suggest%20that%20multimodal%20AI%20models%20may%20be%20useful%20in%20medical%20diagnostic%0Areasoning%20but%20that%20their%20accuracy%20is%20largely%20driven%2C%20for%20better%20and%20worse%2C%20by%0Atheir%20exploitation%20of%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05591v2&entry.124074799=Read"},
{"title": "LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions\n  for Brain Tumor Segmentation", "author": "Ebtihal J. Alwadee and Xianfang Sun and Yipeng Qin and Frank C. Langbein", "abstract": "  Early-stage 3D brain tumor segmentation from magnetic resonance imaging (MRI)\nscans is crucial for prompt and effective treatment. However, this process\nfaces the challenge of precise delineation due to the tumors' complex\nheterogeneity. Moreover, energy sustainability targets and resource\nlimitations, especially in developing countries, require efficient and\naccessible medical imaging solutions. The proposed architecture, a Lightweight\n3D ATtention U-Net with Parallel convolutions, LATUP-Net, addresses these\nissues. It is specifically designed to reduce computational requirements\nsignificantly while maintaining high segmentation performance. By incorporating\nparallel convolutions, it enhances feature representation by capturing\nmulti-scale information. It further integrates an attention mechanism to refine\nsegmentation through selective feature recalibration. LATUP-Net achieves\npromising segmentation performance: the average Dice scores for the whole\ntumor, tumor core, and enhancing tumor on the BraTS 2020 dataset are 88.41%,\n83.82%, and 73.67%, and on the BraTS 2021 dataset, they are 90.29%, 89.54%, and\n83.92%, respectively. Hausdorff distance metrics further indicate its improved\nability to delineate tumor boundaries. With its significantly reduced\ncomputational demand using only 3.07M parameters, about 59 times fewer than\nother state-of-the-art models, and running on a single NVIDIA GeForce RTX3060\n12GB GPU, LATUP-Net requires just 15.79 GFLOPs. This makes it a promising\nsolution for real-world clinical applications, particularly in settings with\nlimited resources. Investigations into the model's interpretability, utilizing\ngradient-weighted class activation mapping and confusion matrices, reveal that\nwhile attention mechanisms enhance the segmentation of small regions, their\nimpact is nuanced. Achieving the most [...]. The code is available at\nhttps://qyber.black/ca/code-bca.\n", "link": "http://arxiv.org/abs/2404.05911v2", "date": "2024-11-25", "relevancy": 2.3019, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6079}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LATUP-Net%3A%20A%20Lightweight%203D%20Attention%20U-Net%20with%20Parallel%20Convolutions%0A%20%20for%20Brain%20Tumor%20Segmentation&body=Title%3A%20LATUP-Net%3A%20A%20Lightweight%203D%20Attention%20U-Net%20with%20Parallel%20Convolutions%0A%20%20for%20Brain%20Tumor%20Segmentation%0AAuthor%3A%20Ebtihal%20J.%20Alwadee%20and%20Xianfang%20Sun%20and%20Yipeng%20Qin%20and%20Frank%20C.%20Langbein%0AAbstract%3A%20%20%20Early-stage%203D%20brain%20tumor%20segmentation%20from%20magnetic%20resonance%20imaging%20%28MRI%29%0Ascans%20is%20crucial%20for%20prompt%20and%20effective%20treatment.%20However%2C%20this%20process%0Afaces%20the%20challenge%20of%20precise%20delineation%20due%20to%20the%20tumors%27%20complex%0Aheterogeneity.%20Moreover%2C%20energy%20sustainability%20targets%20and%20resource%0Alimitations%2C%20especially%20in%20developing%20countries%2C%20require%20efficient%20and%0Aaccessible%20medical%20imaging%20solutions.%20The%20proposed%20architecture%2C%20a%20Lightweight%0A3D%20ATtention%20U-Net%20with%20Parallel%20convolutions%2C%20LATUP-Net%2C%20addresses%20these%0Aissues.%20It%20is%20specifically%20designed%20to%20reduce%20computational%20requirements%0Asignificantly%20while%20maintaining%20high%20segmentation%20performance.%20By%20incorporating%0Aparallel%20convolutions%2C%20it%20enhances%20feature%20representation%20by%20capturing%0Amulti-scale%20information.%20It%20further%20integrates%20an%20attention%20mechanism%20to%20refine%0Asegmentation%20through%20selective%20feature%20recalibration.%20LATUP-Net%20achieves%0Apromising%20segmentation%20performance%3A%20the%20average%20Dice%20scores%20for%20the%20whole%0Atumor%2C%20tumor%20core%2C%20and%20enhancing%20tumor%20on%20the%20BraTS%202020%20dataset%20are%2088.41%25%2C%0A83.82%25%2C%20and%2073.67%25%2C%20and%20on%20the%20BraTS%202021%20dataset%2C%20they%20are%2090.29%25%2C%2089.54%25%2C%20and%0A83.92%25%2C%20respectively.%20Hausdorff%20distance%20metrics%20further%20indicate%20its%20improved%0Aability%20to%20delineate%20tumor%20boundaries.%20With%20its%20significantly%20reduced%0Acomputational%20demand%20using%20only%203.07M%20parameters%2C%20about%2059%20times%20fewer%20than%0Aother%20state-of-the-art%20models%2C%20and%20running%20on%20a%20single%20NVIDIA%20GeForce%20RTX3060%0A12GB%20GPU%2C%20LATUP-Net%20requires%20just%2015.79%20GFLOPs.%20This%20makes%20it%20a%20promising%0Asolution%20for%20real-world%20clinical%20applications%2C%20particularly%20in%20settings%20with%0Alimited%20resources.%20Investigations%20into%20the%20model%27s%20interpretability%2C%20utilizing%0Agradient-weighted%20class%20activation%20mapping%20and%20confusion%20matrices%2C%20reveal%20that%0Awhile%20attention%20mechanisms%20enhance%20the%20segmentation%20of%20small%20regions%2C%20their%0Aimpact%20is%20nuanced.%20Achieving%20the%20most%20%5B...%5D.%20The%20code%20is%20available%20at%0Ahttps%3A//qyber.black/ca/code-bca.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05911v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLATUP-Net%253A%2520A%2520Lightweight%25203D%2520Attention%2520U-Net%2520with%2520Parallel%2520Convolutions%250A%2520%2520for%2520Brain%2520Tumor%2520Segmentation%26entry.906535625%3DEbtihal%2520J.%2520Alwadee%2520and%2520Xianfang%2520Sun%2520and%2520Yipeng%2520Qin%2520and%2520Frank%2520C.%2520Langbein%26entry.1292438233%3D%2520%2520Early-stage%25203D%2520brain%2520tumor%2520segmentation%2520from%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%250Ascans%2520is%2520crucial%2520for%2520prompt%2520and%2520effective%2520treatment.%2520However%252C%2520this%2520process%250Afaces%2520the%2520challenge%2520of%2520precise%2520delineation%2520due%2520to%2520the%2520tumors%2527%2520complex%250Aheterogeneity.%2520Moreover%252C%2520energy%2520sustainability%2520targets%2520and%2520resource%250Alimitations%252C%2520especially%2520in%2520developing%2520countries%252C%2520require%2520efficient%2520and%250Aaccessible%2520medical%2520imaging%2520solutions.%2520The%2520proposed%2520architecture%252C%2520a%2520Lightweight%250A3D%2520ATtention%2520U-Net%2520with%2520Parallel%2520convolutions%252C%2520LATUP-Net%252C%2520addresses%2520these%250Aissues.%2520It%2520is%2520specifically%2520designed%2520to%2520reduce%2520computational%2520requirements%250Asignificantly%2520while%2520maintaining%2520high%2520segmentation%2520performance.%2520By%2520incorporating%250Aparallel%2520convolutions%252C%2520it%2520enhances%2520feature%2520representation%2520by%2520capturing%250Amulti-scale%2520information.%2520It%2520further%2520integrates%2520an%2520attention%2520mechanism%2520to%2520refine%250Asegmentation%2520through%2520selective%2520feature%2520recalibration.%2520LATUP-Net%2520achieves%250Apromising%2520segmentation%2520performance%253A%2520the%2520average%2520Dice%2520scores%2520for%2520the%2520whole%250Atumor%252C%2520tumor%2520core%252C%2520and%2520enhancing%2520tumor%2520on%2520the%2520BraTS%25202020%2520dataset%2520are%252088.41%2525%252C%250A83.82%2525%252C%2520and%252073.67%2525%252C%2520and%2520on%2520the%2520BraTS%25202021%2520dataset%252C%2520they%2520are%252090.29%2525%252C%252089.54%2525%252C%2520and%250A83.92%2525%252C%2520respectively.%2520Hausdorff%2520distance%2520metrics%2520further%2520indicate%2520its%2520improved%250Aability%2520to%2520delineate%2520tumor%2520boundaries.%2520With%2520its%2520significantly%2520reduced%250Acomputational%2520demand%2520using%2520only%25203.07M%2520parameters%252C%2520about%252059%2520times%2520fewer%2520than%250Aother%2520state-of-the-art%2520models%252C%2520and%2520running%2520on%2520a%2520single%2520NVIDIA%2520GeForce%2520RTX3060%250A12GB%2520GPU%252C%2520LATUP-Net%2520requires%2520just%252015.79%2520GFLOPs.%2520This%2520makes%2520it%2520a%2520promising%250Asolution%2520for%2520real-world%2520clinical%2520applications%252C%2520particularly%2520in%2520settings%2520with%250Alimited%2520resources.%2520Investigations%2520into%2520the%2520model%2527s%2520interpretability%252C%2520utilizing%250Agradient-weighted%2520class%2520activation%2520mapping%2520and%2520confusion%2520matrices%252C%2520reveal%2520that%250Awhile%2520attention%2520mechanisms%2520enhance%2520the%2520segmentation%2520of%2520small%2520regions%252C%2520their%250Aimpact%2520is%2520nuanced.%2520Achieving%2520the%2520most%2520%255B...%255D.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//qyber.black/ca/code-bca.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05911v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LATUP-Net%3A%20A%20Lightweight%203D%20Attention%20U-Net%20with%20Parallel%20Convolutions%0A%20%20for%20Brain%20Tumor%20Segmentation&entry.906535625=Ebtihal%20J.%20Alwadee%20and%20Xianfang%20Sun%20and%20Yipeng%20Qin%20and%20Frank%20C.%20Langbein&entry.1292438233=%20%20Early-stage%203D%20brain%20tumor%20segmentation%20from%20magnetic%20resonance%20imaging%20%28MRI%29%0Ascans%20is%20crucial%20for%20prompt%20and%20effective%20treatment.%20However%2C%20this%20process%0Afaces%20the%20challenge%20of%20precise%20delineation%20due%20to%20the%20tumors%27%20complex%0Aheterogeneity.%20Moreover%2C%20energy%20sustainability%20targets%20and%20resource%0Alimitations%2C%20especially%20in%20developing%20countries%2C%20require%20efficient%20and%0Aaccessible%20medical%20imaging%20solutions.%20The%20proposed%20architecture%2C%20a%20Lightweight%0A3D%20ATtention%20U-Net%20with%20Parallel%20convolutions%2C%20LATUP-Net%2C%20addresses%20these%0Aissues.%20It%20is%20specifically%20designed%20to%20reduce%20computational%20requirements%0Asignificantly%20while%20maintaining%20high%20segmentation%20performance.%20By%20incorporating%0Aparallel%20convolutions%2C%20it%20enhances%20feature%20representation%20by%20capturing%0Amulti-scale%20information.%20It%20further%20integrates%20an%20attention%20mechanism%20to%20refine%0Asegmentation%20through%20selective%20feature%20recalibration.%20LATUP-Net%20achieves%0Apromising%20segmentation%20performance%3A%20the%20average%20Dice%20scores%20for%20the%20whole%0Atumor%2C%20tumor%20core%2C%20and%20enhancing%20tumor%20on%20the%20BraTS%202020%20dataset%20are%2088.41%25%2C%0A83.82%25%2C%20and%2073.67%25%2C%20and%20on%20the%20BraTS%202021%20dataset%2C%20they%20are%2090.29%25%2C%2089.54%25%2C%20and%0A83.92%25%2C%20respectively.%20Hausdorff%20distance%20metrics%20further%20indicate%20its%20improved%0Aability%20to%20delineate%20tumor%20boundaries.%20With%20its%20significantly%20reduced%0Acomputational%20demand%20using%20only%203.07M%20parameters%2C%20about%2059%20times%20fewer%20than%0Aother%20state-of-the-art%20models%2C%20and%20running%20on%20a%20single%20NVIDIA%20GeForce%20RTX3060%0A12GB%20GPU%2C%20LATUP-Net%20requires%20just%2015.79%20GFLOPs.%20This%20makes%20it%20a%20promising%0Asolution%20for%20real-world%20clinical%20applications%2C%20particularly%20in%20settings%20with%0Alimited%20resources.%20Investigations%20into%20the%20model%27s%20interpretability%2C%20utilizing%0Agradient-weighted%20class%20activation%20mapping%20and%20confusion%20matrices%2C%20reveal%20that%0Awhile%20attention%20mechanisms%20enhance%20the%20segmentation%20of%20small%20regions%2C%20their%0Aimpact%20is%20nuanced.%20Achieving%20the%20most%20%5B...%5D.%20The%20code%20is%20available%20at%0Ahttps%3A//qyber.black/ca/code-bca.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05911v2&entry.124074799=Read"},
{"title": "F -- A Model of Events based on the Foundational Ontology DOLCE+DnS\n  Ultralite", "author": "Ansgar Scherp and Thomas Franz and Carsten Saathoff and Steffen Staab", "abstract": "  The lack of a formal model of events hinders interoperability in distributed\nevent-based systems. In this paper, we present a formal model of events, called\nEvent-Model-F. The model is based on the foundational ontology DOLCE+DnS\nUltralite (DUL) and provides comprehensive support to represent time and space,\nobjects and persons, as well as mereological, causal, and correlative\nrelationships between events. In addition, the Event-Model-F provides a\nflexible means for event composition, modeling event causality and event\ncorrelation, and representing different interpretations of the same event. The\nEvent-Model-F is developed following the pattern-oriented approach of DUL, is\nmodularized in different ontologies, and can be easily extended by domain\nspecific ontologies.\n", "link": "http://arxiv.org/abs/2411.16609v1", "date": "2024-11-25", "relevancy": 2.3016, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F%20--%20A%20Model%20of%20Events%20based%20on%20the%20Foundational%20Ontology%20DOLCE%2BDnS%0A%20%20Ultralite&body=Title%3A%20F%20--%20A%20Model%20of%20Events%20based%20on%20the%20Foundational%20Ontology%20DOLCE%2BDnS%0A%20%20Ultralite%0AAuthor%3A%20Ansgar%20Scherp%20and%20Thomas%20Franz%20and%20Carsten%20Saathoff%20and%20Steffen%20Staab%0AAbstract%3A%20%20%20The%20lack%20of%20a%20formal%20model%20of%20events%20hinders%20interoperability%20in%20distributed%0Aevent-based%20systems.%20In%20this%20paper%2C%20we%20present%20a%20formal%20model%20of%20events%2C%20called%0AEvent-Model-F.%20The%20model%20is%20based%20on%20the%20foundational%20ontology%20DOLCE%2BDnS%0AUltralite%20%28DUL%29%20and%20provides%20comprehensive%20support%20to%20represent%20time%20and%20space%2C%0Aobjects%20and%20persons%2C%20as%20well%20as%20mereological%2C%20causal%2C%20and%20correlative%0Arelationships%20between%20events.%20In%20addition%2C%20the%20Event-Model-F%20provides%20a%0Aflexible%20means%20for%20event%20composition%2C%20modeling%20event%20causality%20and%20event%0Acorrelation%2C%20and%20representing%20different%20interpretations%20of%20the%20same%20event.%20The%0AEvent-Model-F%20is%20developed%20following%20the%20pattern-oriented%20approach%20of%20DUL%2C%20is%0Amodularized%20in%20different%20ontologies%2C%20and%20can%20be%20easily%20extended%20by%20domain%0Aspecific%20ontologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF%2520--%2520A%2520Model%2520of%2520Events%2520based%2520on%2520the%2520Foundational%2520Ontology%2520DOLCE%252BDnS%250A%2520%2520Ultralite%26entry.906535625%3DAnsgar%2520Scherp%2520and%2520Thomas%2520Franz%2520and%2520Carsten%2520Saathoff%2520and%2520Steffen%2520Staab%26entry.1292438233%3D%2520%2520The%2520lack%2520of%2520a%2520formal%2520model%2520of%2520events%2520hinders%2520interoperability%2520in%2520distributed%250Aevent-based%2520systems.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520formal%2520model%2520of%2520events%252C%2520called%250AEvent-Model-F.%2520The%2520model%2520is%2520based%2520on%2520the%2520foundational%2520ontology%2520DOLCE%252BDnS%250AUltralite%2520%2528DUL%2529%2520and%2520provides%2520comprehensive%2520support%2520to%2520represent%2520time%2520and%2520space%252C%250Aobjects%2520and%2520persons%252C%2520as%2520well%2520as%2520mereological%252C%2520causal%252C%2520and%2520correlative%250Arelationships%2520between%2520events.%2520In%2520addition%252C%2520the%2520Event-Model-F%2520provides%2520a%250Aflexible%2520means%2520for%2520event%2520composition%252C%2520modeling%2520event%2520causality%2520and%2520event%250Acorrelation%252C%2520and%2520representing%2520different%2520interpretations%2520of%2520the%2520same%2520event.%2520The%250AEvent-Model-F%2520is%2520developed%2520following%2520the%2520pattern-oriented%2520approach%2520of%2520DUL%252C%2520is%250Amodularized%2520in%2520different%2520ontologies%252C%2520and%2520can%2520be%2520easily%2520extended%2520by%2520domain%250Aspecific%2520ontologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F%20--%20A%20Model%20of%20Events%20based%20on%20the%20Foundational%20Ontology%20DOLCE%2BDnS%0A%20%20Ultralite&entry.906535625=Ansgar%20Scherp%20and%20Thomas%20Franz%20and%20Carsten%20Saathoff%20and%20Steffen%20Staab&entry.1292438233=%20%20The%20lack%20of%20a%20formal%20model%20of%20events%20hinders%20interoperability%20in%20distributed%0Aevent-based%20systems.%20In%20this%20paper%2C%20we%20present%20a%20formal%20model%20of%20events%2C%20called%0AEvent-Model-F.%20The%20model%20is%20based%20on%20the%20foundational%20ontology%20DOLCE%2BDnS%0AUltralite%20%28DUL%29%20and%20provides%20comprehensive%20support%20to%20represent%20time%20and%20space%2C%0Aobjects%20and%20persons%2C%20as%20well%20as%20mereological%2C%20causal%2C%20and%20correlative%0Arelationships%20between%20events.%20In%20addition%2C%20the%20Event-Model-F%20provides%20a%0Aflexible%20means%20for%20event%20composition%2C%20modeling%20event%20causality%20and%20event%0Acorrelation%2C%20and%20representing%20different%20interpretations%20of%20the%20same%20event.%20The%0AEvent-Model-F%20is%20developed%20following%20the%20pattern-oriented%20approach%20of%20DUL%2C%20is%0Amodularized%20in%20different%20ontologies%2C%20and%20can%20be%20easily%20extended%20by%20domain%0Aspecific%20ontologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16609v1&entry.124074799=Read"},
{"title": "TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs", "author": "Zhuofeng Li and Zixing Gou and Xiangnan Zhang and Zhongyuan Liu and Sirui Li and Yuntong Hu and Chen Ling and Zheng Zhang and Liang Zhao", "abstract": "  Text-Attributed Graphs (TAGs) augment graph structures with natural language\ndescriptions, facilitating detailed depictions of data and their\ninterconnections across various real-world settings. However, existing TAG\ndatasets predominantly feature textual information only at the nodes, with\nedges typically represented by mere binary or categorical attributes. This lack\nof rich textual edge annotations significantly limits the exploration of\ncontextual relationships between entities, hindering deeper insights into\ngraph-structured data. To address this gap, we introduce Textual-Edge Graphs\nDatasets and Benchmark (TEG-DB), a comprehensive and diverse collection of\nbenchmark textual-edge datasets featuring rich textual descriptions on nodes\nand edges. The TEG-DB datasets are large-scale and encompass a wide range of\ndomains, from citation networks to social networks. In addition, we conduct\nextensive benchmark experiments on TEG-DB to assess the extent to which current\ntechniques, including pre-trained language models, graph neural networks, and\ntheir combinations, can utilize textual node and edge information. Our goal is\nto elicit advancements in textual-edge graph research, specifically in\ndeveloping methodologies that exploit rich textual node and edge descriptions\nto enhance graph analysis and provide deeper insights into complex real-world\nnetworks. The entire TEG-DB project is publicly accessible as an open-source\nrepository on Github, accessible at\nhttps://github.com/Zhuofeng-Li/TEG-Benchmark.\n", "link": "http://arxiv.org/abs/2406.10310v3", "date": "2024-11-25", "relevancy": 2.2951, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4701}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TEG-DB%3A%20A%20Comprehensive%20Dataset%20and%20Benchmark%20of%20Textual-Edge%20Graphs&body=Title%3A%20TEG-DB%3A%20A%20Comprehensive%20Dataset%20and%20Benchmark%20of%20Textual-Edge%20Graphs%0AAuthor%3A%20Zhuofeng%20Li%20and%20Zixing%20Gou%20and%20Xiangnan%20Zhang%20and%20Zhongyuan%20Liu%20and%20Sirui%20Li%20and%20Yuntong%20Hu%20and%20Chen%20Ling%20and%20Zheng%20Zhang%20and%20Liang%20Zhao%0AAbstract%3A%20%20%20Text-Attributed%20Graphs%20%28TAGs%29%20augment%20graph%20structures%20with%20natural%20language%0Adescriptions%2C%20facilitating%20detailed%20depictions%20of%20data%20and%20their%0Ainterconnections%20across%20various%20real-world%20settings.%20However%2C%20existing%20TAG%0Adatasets%20predominantly%20feature%20textual%20information%20only%20at%20the%20nodes%2C%20with%0Aedges%20typically%20represented%20by%20mere%20binary%20or%20categorical%20attributes.%20This%20lack%0Aof%20rich%20textual%20edge%20annotations%20significantly%20limits%20the%20exploration%20of%0Acontextual%20relationships%20between%20entities%2C%20hindering%20deeper%20insights%20into%0Agraph-structured%20data.%20To%20address%20this%20gap%2C%20we%20introduce%20Textual-Edge%20Graphs%0ADatasets%20and%20Benchmark%20%28TEG-DB%29%2C%20a%20comprehensive%20and%20diverse%20collection%20of%0Abenchmark%20textual-edge%20datasets%20featuring%20rich%20textual%20descriptions%20on%20nodes%0Aand%20edges.%20The%20TEG-DB%20datasets%20are%20large-scale%20and%20encompass%20a%20wide%20range%20of%0Adomains%2C%20from%20citation%20networks%20to%20social%20networks.%20In%20addition%2C%20we%20conduct%0Aextensive%20benchmark%20experiments%20on%20TEG-DB%20to%20assess%20the%20extent%20to%20which%20current%0Atechniques%2C%20including%20pre-trained%20language%20models%2C%20graph%20neural%20networks%2C%20and%0Atheir%20combinations%2C%20can%20utilize%20textual%20node%20and%20edge%20information.%20Our%20goal%20is%0Ato%20elicit%20advancements%20in%20textual-edge%20graph%20research%2C%20specifically%20in%0Adeveloping%20methodologies%20that%20exploit%20rich%20textual%20node%20and%20edge%20descriptions%0Ato%20enhance%20graph%20analysis%20and%20provide%20deeper%20insights%20into%20complex%20real-world%0Anetworks.%20The%20entire%20TEG-DB%20project%20is%20publicly%20accessible%20as%20an%20open-source%0Arepository%20on%20Github%2C%20accessible%20at%0Ahttps%3A//github.com/Zhuofeng-Li/TEG-Benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10310v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTEG-DB%253A%2520A%2520Comprehensive%2520Dataset%2520and%2520Benchmark%2520of%2520Textual-Edge%2520Graphs%26entry.906535625%3DZhuofeng%2520Li%2520and%2520Zixing%2520Gou%2520and%2520Xiangnan%2520Zhang%2520and%2520Zhongyuan%2520Liu%2520and%2520Sirui%2520Li%2520and%2520Yuntong%2520Hu%2520and%2520Chen%2520Ling%2520and%2520Zheng%2520Zhang%2520and%2520Liang%2520Zhao%26entry.1292438233%3D%2520%2520Text-Attributed%2520Graphs%2520%2528TAGs%2529%2520augment%2520graph%2520structures%2520with%2520natural%2520language%250Adescriptions%252C%2520facilitating%2520detailed%2520depictions%2520of%2520data%2520and%2520their%250Ainterconnections%2520across%2520various%2520real-world%2520settings.%2520However%252C%2520existing%2520TAG%250Adatasets%2520predominantly%2520feature%2520textual%2520information%2520only%2520at%2520the%2520nodes%252C%2520with%250Aedges%2520typically%2520represented%2520by%2520mere%2520binary%2520or%2520categorical%2520attributes.%2520This%2520lack%250Aof%2520rich%2520textual%2520edge%2520annotations%2520significantly%2520limits%2520the%2520exploration%2520of%250Acontextual%2520relationships%2520between%2520entities%252C%2520hindering%2520deeper%2520insights%2520into%250Agraph-structured%2520data.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Textual-Edge%2520Graphs%250ADatasets%2520and%2520Benchmark%2520%2528TEG-DB%2529%252C%2520a%2520comprehensive%2520and%2520diverse%2520collection%2520of%250Abenchmark%2520textual-edge%2520datasets%2520featuring%2520rich%2520textual%2520descriptions%2520on%2520nodes%250Aand%2520edges.%2520The%2520TEG-DB%2520datasets%2520are%2520large-scale%2520and%2520encompass%2520a%2520wide%2520range%2520of%250Adomains%252C%2520from%2520citation%2520networks%2520to%2520social%2520networks.%2520In%2520addition%252C%2520we%2520conduct%250Aextensive%2520benchmark%2520experiments%2520on%2520TEG-DB%2520to%2520assess%2520the%2520extent%2520to%2520which%2520current%250Atechniques%252C%2520including%2520pre-trained%2520language%2520models%252C%2520graph%2520neural%2520networks%252C%2520and%250Atheir%2520combinations%252C%2520can%2520utilize%2520textual%2520node%2520and%2520edge%2520information.%2520Our%2520goal%2520is%250Ato%2520elicit%2520advancements%2520in%2520textual-edge%2520graph%2520research%252C%2520specifically%2520in%250Adeveloping%2520methodologies%2520that%2520exploit%2520rich%2520textual%2520node%2520and%2520edge%2520descriptions%250Ato%2520enhance%2520graph%2520analysis%2520and%2520provide%2520deeper%2520insights%2520into%2520complex%2520real-world%250Anetworks.%2520The%2520entire%2520TEG-DB%2520project%2520is%2520publicly%2520accessible%2520as%2520an%2520open-source%250Arepository%2520on%2520Github%252C%2520accessible%2520at%250Ahttps%253A//github.com/Zhuofeng-Li/TEG-Benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10310v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TEG-DB%3A%20A%20Comprehensive%20Dataset%20and%20Benchmark%20of%20Textual-Edge%20Graphs&entry.906535625=Zhuofeng%20Li%20and%20Zixing%20Gou%20and%20Xiangnan%20Zhang%20and%20Zhongyuan%20Liu%20and%20Sirui%20Li%20and%20Yuntong%20Hu%20and%20Chen%20Ling%20and%20Zheng%20Zhang%20and%20Liang%20Zhao&entry.1292438233=%20%20Text-Attributed%20Graphs%20%28TAGs%29%20augment%20graph%20structures%20with%20natural%20language%0Adescriptions%2C%20facilitating%20detailed%20depictions%20of%20data%20and%20their%0Ainterconnections%20across%20various%20real-world%20settings.%20However%2C%20existing%20TAG%0Adatasets%20predominantly%20feature%20textual%20information%20only%20at%20the%20nodes%2C%20with%0Aedges%20typically%20represented%20by%20mere%20binary%20or%20categorical%20attributes.%20This%20lack%0Aof%20rich%20textual%20edge%20annotations%20significantly%20limits%20the%20exploration%20of%0Acontextual%20relationships%20between%20entities%2C%20hindering%20deeper%20insights%20into%0Agraph-structured%20data.%20To%20address%20this%20gap%2C%20we%20introduce%20Textual-Edge%20Graphs%0ADatasets%20and%20Benchmark%20%28TEG-DB%29%2C%20a%20comprehensive%20and%20diverse%20collection%20of%0Abenchmark%20textual-edge%20datasets%20featuring%20rich%20textual%20descriptions%20on%20nodes%0Aand%20edges.%20The%20TEG-DB%20datasets%20are%20large-scale%20and%20encompass%20a%20wide%20range%20of%0Adomains%2C%20from%20citation%20networks%20to%20social%20networks.%20In%20addition%2C%20we%20conduct%0Aextensive%20benchmark%20experiments%20on%20TEG-DB%20to%20assess%20the%20extent%20to%20which%20current%0Atechniques%2C%20including%20pre-trained%20language%20models%2C%20graph%20neural%20networks%2C%20and%0Atheir%20combinations%2C%20can%20utilize%20textual%20node%20and%20edge%20information.%20Our%20goal%20is%0Ato%20elicit%20advancements%20in%20textual-edge%20graph%20research%2C%20specifically%20in%0Adeveloping%20methodologies%20that%20exploit%20rich%20textual%20node%20and%20edge%20descriptions%0Ato%20enhance%20graph%20analysis%20and%20provide%20deeper%20insights%20into%20complex%20real-world%0Anetworks.%20The%20entire%20TEG-DB%20project%20is%20publicly%20accessible%20as%20an%20open-source%0Arepository%20on%20Github%2C%20accessible%20at%0Ahttps%3A//github.com/Zhuofeng-Li/TEG-Benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10310v3&entry.124074799=Read"},
{"title": "Dark Miner: Defend against undesired generation for text-to-image\n  diffusion models", "author": "Zheling Meng and Bo Peng and Xiaochuan Jin and Yue Jiang and Jing Dong and Wei Wang", "abstract": "  Text-to-image diffusion models have been demonstrated with undesired\ngeneration due to unfiltered large-scale training data, such as sexual images\nand copyrights, necessitating the erasure of undesired concepts. Most existing\nmethods focus on modifying the generation probabilities conditioned on the\ntexts containing target concepts. However, they fail to guarantee the desired\ngeneration of texts unseen in the training phase, especially for the\nadversarial texts from malicious attacks. In this paper, we analyze the erasure\ntask and point out that existing methods cannot guarantee the minimization of\nthe total probabilities of undesired generation. To tackle this problem, we\npropose Dark Miner. It entails a recurring three-stage process that comprises\nmining, verifying, and circumventing. This method greedily mines embeddings\nwith maximum generation probabilities of target concepts and more effectively\nreduces their generation. In the experiments, we evaluate its performance on\nthe inappropriateness, object, and style concepts. Compared with the previous\nmethods, our method achieves better erasure and defense results, especially\nunder multiple adversarial attacks, while preserving the native generation\ncapability of the models. Our code will be available at\nhttps://github.com/RichardSunnyMeng/DarkMiner-offical-codes.\n", "link": "http://arxiv.org/abs/2409.17682v2", "date": "2024-11-25", "relevancy": 2.2762, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5883}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5575}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dark%20Miner%3A%20Defend%20against%20undesired%20generation%20for%20text-to-image%0A%20%20diffusion%20models&body=Title%3A%20Dark%20Miner%3A%20Defend%20against%20undesired%20generation%20for%20text-to-image%0A%20%20diffusion%20models%0AAuthor%3A%20Zheling%20Meng%20and%20Bo%20Peng%20and%20Xiaochuan%20Jin%20and%20Yue%20Jiang%20and%20Jing%20Dong%20and%20Wei%20Wang%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20been%20demonstrated%20with%20undesired%0Ageneration%20due%20to%20unfiltered%20large-scale%20training%20data%2C%20such%20as%20sexual%20images%0Aand%20copyrights%2C%20necessitating%20the%20erasure%20of%20undesired%20concepts.%20Most%20existing%0Amethods%20focus%20on%20modifying%20the%20generation%20probabilities%20conditioned%20on%20the%0Atexts%20containing%20target%20concepts.%20However%2C%20they%20fail%20to%20guarantee%20the%20desired%0Ageneration%20of%20texts%20unseen%20in%20the%20training%20phase%2C%20especially%20for%20the%0Aadversarial%20texts%20from%20malicious%20attacks.%20In%20this%20paper%2C%20we%20analyze%20the%20erasure%0Atask%20and%20point%20out%20that%20existing%20methods%20cannot%20guarantee%20the%20minimization%20of%0Athe%20total%20probabilities%20of%20undesired%20generation.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20Dark%20Miner.%20It%20entails%20a%20recurring%20three-stage%20process%20that%20comprises%0Amining%2C%20verifying%2C%20and%20circumventing.%20This%20method%20greedily%20mines%20embeddings%0Awith%20maximum%20generation%20probabilities%20of%20target%20concepts%20and%20more%20effectively%0Areduces%20their%20generation.%20In%20the%20experiments%2C%20we%20evaluate%20its%20performance%20on%0Athe%20inappropriateness%2C%20object%2C%20and%20style%20concepts.%20Compared%20with%20the%20previous%0Amethods%2C%20our%20method%20achieves%20better%20erasure%20and%20defense%20results%2C%20especially%0Aunder%20multiple%20adversarial%20attacks%2C%20while%20preserving%20the%20native%20generation%0Acapability%20of%20the%20models.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/RichardSunnyMeng/DarkMiner-offical-codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDark%2520Miner%253A%2520Defend%2520against%2520undesired%2520generation%2520for%2520text-to-image%250A%2520%2520diffusion%2520models%26entry.906535625%3DZheling%2520Meng%2520and%2520Bo%2520Peng%2520and%2520Xiaochuan%2520Jin%2520and%2520Yue%2520Jiang%2520and%2520Jing%2520Dong%2520and%2520Wei%2520Wang%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520been%2520demonstrated%2520with%2520undesired%250Ageneration%2520due%2520to%2520unfiltered%2520large-scale%2520training%2520data%252C%2520such%2520as%2520sexual%2520images%250Aand%2520copyrights%252C%2520necessitating%2520the%2520erasure%2520of%2520undesired%2520concepts.%2520Most%2520existing%250Amethods%2520focus%2520on%2520modifying%2520the%2520generation%2520probabilities%2520conditioned%2520on%2520the%250Atexts%2520containing%2520target%2520concepts.%2520However%252C%2520they%2520fail%2520to%2520guarantee%2520the%2520desired%250Ageneration%2520of%2520texts%2520unseen%2520in%2520the%2520training%2520phase%252C%2520especially%2520for%2520the%250Aadversarial%2520texts%2520from%2520malicious%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520erasure%250Atask%2520and%2520point%2520out%2520that%2520existing%2520methods%2520cannot%2520guarantee%2520the%2520minimization%2520of%250Athe%2520total%2520probabilities%2520of%2520undesired%2520generation.%2520To%2520tackle%2520this%2520problem%252C%2520we%250Apropose%2520Dark%2520Miner.%2520It%2520entails%2520a%2520recurring%2520three-stage%2520process%2520that%2520comprises%250Amining%252C%2520verifying%252C%2520and%2520circumventing.%2520This%2520method%2520greedily%2520mines%2520embeddings%250Awith%2520maximum%2520generation%2520probabilities%2520of%2520target%2520concepts%2520and%2520more%2520effectively%250Areduces%2520their%2520generation.%2520In%2520the%2520experiments%252C%2520we%2520evaluate%2520its%2520performance%2520on%250Athe%2520inappropriateness%252C%2520object%252C%2520and%2520style%2520concepts.%2520Compared%2520with%2520the%2520previous%250Amethods%252C%2520our%2520method%2520achieves%2520better%2520erasure%2520and%2520defense%2520results%252C%2520especially%250Aunder%2520multiple%2520adversarial%2520attacks%252C%2520while%2520preserving%2520the%2520native%2520generation%250Acapability%2520of%2520the%2520models.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/RichardSunnyMeng/DarkMiner-offical-codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dark%20Miner%3A%20Defend%20against%20undesired%20generation%20for%20text-to-image%0A%20%20diffusion%20models&entry.906535625=Zheling%20Meng%20and%20Bo%20Peng%20and%20Xiaochuan%20Jin%20and%20Yue%20Jiang%20and%20Jing%20Dong%20and%20Wei%20Wang&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20been%20demonstrated%20with%20undesired%0Ageneration%20due%20to%20unfiltered%20large-scale%20training%20data%2C%20such%20as%20sexual%20images%0Aand%20copyrights%2C%20necessitating%20the%20erasure%20of%20undesired%20concepts.%20Most%20existing%0Amethods%20focus%20on%20modifying%20the%20generation%20probabilities%20conditioned%20on%20the%0Atexts%20containing%20target%20concepts.%20However%2C%20they%20fail%20to%20guarantee%20the%20desired%0Ageneration%20of%20texts%20unseen%20in%20the%20training%20phase%2C%20especially%20for%20the%0Aadversarial%20texts%20from%20malicious%20attacks.%20In%20this%20paper%2C%20we%20analyze%20the%20erasure%0Atask%20and%20point%20out%20that%20existing%20methods%20cannot%20guarantee%20the%20minimization%20of%0Athe%20total%20probabilities%20of%20undesired%20generation.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20Dark%20Miner.%20It%20entails%20a%20recurring%20three-stage%20process%20that%20comprises%0Amining%2C%20verifying%2C%20and%20circumventing.%20This%20method%20greedily%20mines%20embeddings%0Awith%20maximum%20generation%20probabilities%20of%20target%20concepts%20and%20more%20effectively%0Areduces%20their%20generation.%20In%20the%20experiments%2C%20we%20evaluate%20its%20performance%20on%0Athe%20inappropriateness%2C%20object%2C%20and%20style%20concepts.%20Compared%20with%20the%20previous%0Amethods%2C%20our%20method%20achieves%20better%20erasure%20and%20defense%20results%2C%20especially%0Aunder%20multiple%20adversarial%20attacks%2C%20while%20preserving%20the%20native%20generation%0Acapability%20of%20the%20models.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/RichardSunnyMeng/DarkMiner-offical-codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17682v2&entry.124074799=Read"},
{"title": "Anomaly Detection and RFI Classification with Unsupervised Learning in\n  Narrowband Radio Technosignature Searches", "author": "Ben Jacobson-Bell and Steve Croft and Carmen Choza and Alex Andersson and Daniel Bautista and Vishal Gajjar and Matthew Lebofsky and David H. E. MacMahon and Caleb Painter and Andrew P. V. Siemion", "abstract": "  The search for radio technosignatures is an anomaly detection problem:\ncandidate signals represent needles of interest in the proverbial haystack of\nradio-frequency interference (RFI). Current search frameworks find an enormity\nof false-positive signals, especially in large surveys, requiring manual\nfollow-up to a sometimes prohibitive degree. Unsupervised learning provides an\nalgorithmic way to winnow the most anomalous signals from the chaff, as well as\ngroup together RFI signals that bear morphological similarities. We present\nGLOBULAR (Grouping Low-frequency Observations By Unsupervised Learning After\nReduction) clustering, a signal processing method that uses HDBSCAN to reduce\nthe false-positive rate and isolate outlier signals for further analysis. When\ncombined with a standard narrowband signal detection and spatial filtering\npipeline, such as turboSETI, GLOBULAR clustering offers significant\nimprovements in the false-positive rate over the standard pipeline alone,\nsuggesting dramatic potential for the amelioration of manual follow-up\nrequirements for future large surveys. By removing RFI signals in regions of\nhigh spectral occupancy, GLOBULAR clustering may also enable the detection of\nsignals missed by the standard pipeline. We benchmark our method against the\nChoza et al. (2024) turboSETI-only search of 97 nearby galaxies at L-band,\ndemonstrating a false-positive hit reduction rate of 93.1% and a false-positive\nevent reduction rate of 99.3%.\n", "link": "http://arxiv.org/abs/2411.16556v1", "date": "2024-11-25", "relevancy": 2.276, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4726}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4484}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomaly%20Detection%20and%20RFI%20Classification%20with%20Unsupervised%20Learning%20in%0A%20%20Narrowband%20Radio%20Technosignature%20Searches&body=Title%3A%20Anomaly%20Detection%20and%20RFI%20Classification%20with%20Unsupervised%20Learning%20in%0A%20%20Narrowband%20Radio%20Technosignature%20Searches%0AAuthor%3A%20Ben%20Jacobson-Bell%20and%20Steve%20Croft%20and%20Carmen%20Choza%20and%20Alex%20Andersson%20and%20Daniel%20Bautista%20and%20Vishal%20Gajjar%20and%20Matthew%20Lebofsky%20and%20David%20H.%20E.%20MacMahon%20and%20Caleb%20Painter%20and%20Andrew%20P.%20V.%20Siemion%0AAbstract%3A%20%20%20The%20search%20for%20radio%20technosignatures%20is%20an%20anomaly%20detection%20problem%3A%0Acandidate%20signals%20represent%20needles%20of%20interest%20in%20the%20proverbial%20haystack%20of%0Aradio-frequency%20interference%20%28RFI%29.%20Current%20search%20frameworks%20find%20an%20enormity%0Aof%20false-positive%20signals%2C%20especially%20in%20large%20surveys%2C%20requiring%20manual%0Afollow-up%20to%20a%20sometimes%20prohibitive%20degree.%20Unsupervised%20learning%20provides%20an%0Aalgorithmic%20way%20to%20winnow%20the%20most%20anomalous%20signals%20from%20the%20chaff%2C%20as%20well%20as%0Agroup%20together%20RFI%20signals%20that%20bear%20morphological%20similarities.%20We%20present%0AGLOBULAR%20%28Grouping%20Low-frequency%20Observations%20By%20Unsupervised%20Learning%20After%0AReduction%29%20clustering%2C%20a%20signal%20processing%20method%20that%20uses%20HDBSCAN%20to%20reduce%0Athe%20false-positive%20rate%20and%20isolate%20outlier%20signals%20for%20further%20analysis.%20When%0Acombined%20with%20a%20standard%20narrowband%20signal%20detection%20and%20spatial%20filtering%0Apipeline%2C%20such%20as%20turboSETI%2C%20GLOBULAR%20clustering%20offers%20significant%0Aimprovements%20in%20the%20false-positive%20rate%20over%20the%20standard%20pipeline%20alone%2C%0Asuggesting%20dramatic%20potential%20for%20the%20amelioration%20of%20manual%20follow-up%0Arequirements%20for%20future%20large%20surveys.%20By%20removing%20RFI%20signals%20in%20regions%20of%0Ahigh%20spectral%20occupancy%2C%20GLOBULAR%20clustering%20may%20also%20enable%20the%20detection%20of%0Asignals%20missed%20by%20the%20standard%20pipeline.%20We%20benchmark%20our%20method%20against%20the%0AChoza%20et%20al.%20%282024%29%20turboSETI-only%20search%20of%2097%20nearby%20galaxies%20at%20L-band%2C%0Ademonstrating%20a%20false-positive%20hit%20reduction%20rate%20of%2093.1%25%20and%20a%20false-positive%0Aevent%20reduction%20rate%20of%2099.3%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomaly%2520Detection%2520and%2520RFI%2520Classification%2520with%2520Unsupervised%2520Learning%2520in%250A%2520%2520Narrowband%2520Radio%2520Technosignature%2520Searches%26entry.906535625%3DBen%2520Jacobson-Bell%2520and%2520Steve%2520Croft%2520and%2520Carmen%2520Choza%2520and%2520Alex%2520Andersson%2520and%2520Daniel%2520Bautista%2520and%2520Vishal%2520Gajjar%2520and%2520Matthew%2520Lebofsky%2520and%2520David%2520H.%2520E.%2520MacMahon%2520and%2520Caleb%2520Painter%2520and%2520Andrew%2520P.%2520V.%2520Siemion%26entry.1292438233%3D%2520%2520The%2520search%2520for%2520radio%2520technosignatures%2520is%2520an%2520anomaly%2520detection%2520problem%253A%250Acandidate%2520signals%2520represent%2520needles%2520of%2520interest%2520in%2520the%2520proverbial%2520haystack%2520of%250Aradio-frequency%2520interference%2520%2528RFI%2529.%2520Current%2520search%2520frameworks%2520find%2520an%2520enormity%250Aof%2520false-positive%2520signals%252C%2520especially%2520in%2520large%2520surveys%252C%2520requiring%2520manual%250Afollow-up%2520to%2520a%2520sometimes%2520prohibitive%2520degree.%2520Unsupervised%2520learning%2520provides%2520an%250Aalgorithmic%2520way%2520to%2520winnow%2520the%2520most%2520anomalous%2520signals%2520from%2520the%2520chaff%252C%2520as%2520well%2520as%250Agroup%2520together%2520RFI%2520signals%2520that%2520bear%2520morphological%2520similarities.%2520We%2520present%250AGLOBULAR%2520%2528Grouping%2520Low-frequency%2520Observations%2520By%2520Unsupervised%2520Learning%2520After%250AReduction%2529%2520clustering%252C%2520a%2520signal%2520processing%2520method%2520that%2520uses%2520HDBSCAN%2520to%2520reduce%250Athe%2520false-positive%2520rate%2520and%2520isolate%2520outlier%2520signals%2520for%2520further%2520analysis.%2520When%250Acombined%2520with%2520a%2520standard%2520narrowband%2520signal%2520detection%2520and%2520spatial%2520filtering%250Apipeline%252C%2520such%2520as%2520turboSETI%252C%2520GLOBULAR%2520clustering%2520offers%2520significant%250Aimprovements%2520in%2520the%2520false-positive%2520rate%2520over%2520the%2520standard%2520pipeline%2520alone%252C%250Asuggesting%2520dramatic%2520potential%2520for%2520the%2520amelioration%2520of%2520manual%2520follow-up%250Arequirements%2520for%2520future%2520large%2520surveys.%2520By%2520removing%2520RFI%2520signals%2520in%2520regions%2520of%250Ahigh%2520spectral%2520occupancy%252C%2520GLOBULAR%2520clustering%2520may%2520also%2520enable%2520the%2520detection%2520of%250Asignals%2520missed%2520by%2520the%2520standard%2520pipeline.%2520We%2520benchmark%2520our%2520method%2520against%2520the%250AChoza%2520et%2520al.%2520%25282024%2529%2520turboSETI-only%2520search%2520of%252097%2520nearby%2520galaxies%2520at%2520L-band%252C%250Ademonstrating%2520a%2520false-positive%2520hit%2520reduction%2520rate%2520of%252093.1%2525%2520and%2520a%2520false-positive%250Aevent%2520reduction%2520rate%2520of%252099.3%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly%20Detection%20and%20RFI%20Classification%20with%20Unsupervised%20Learning%20in%0A%20%20Narrowband%20Radio%20Technosignature%20Searches&entry.906535625=Ben%20Jacobson-Bell%20and%20Steve%20Croft%20and%20Carmen%20Choza%20and%20Alex%20Andersson%20and%20Daniel%20Bautista%20and%20Vishal%20Gajjar%20and%20Matthew%20Lebofsky%20and%20David%20H.%20E.%20MacMahon%20and%20Caleb%20Painter%20and%20Andrew%20P.%20V.%20Siemion&entry.1292438233=%20%20The%20search%20for%20radio%20technosignatures%20is%20an%20anomaly%20detection%20problem%3A%0Acandidate%20signals%20represent%20needles%20of%20interest%20in%20the%20proverbial%20haystack%20of%0Aradio-frequency%20interference%20%28RFI%29.%20Current%20search%20frameworks%20find%20an%20enormity%0Aof%20false-positive%20signals%2C%20especially%20in%20large%20surveys%2C%20requiring%20manual%0Afollow-up%20to%20a%20sometimes%20prohibitive%20degree.%20Unsupervised%20learning%20provides%20an%0Aalgorithmic%20way%20to%20winnow%20the%20most%20anomalous%20signals%20from%20the%20chaff%2C%20as%20well%20as%0Agroup%20together%20RFI%20signals%20that%20bear%20morphological%20similarities.%20We%20present%0AGLOBULAR%20%28Grouping%20Low-frequency%20Observations%20By%20Unsupervised%20Learning%20After%0AReduction%29%20clustering%2C%20a%20signal%20processing%20method%20that%20uses%20HDBSCAN%20to%20reduce%0Athe%20false-positive%20rate%20and%20isolate%20outlier%20signals%20for%20further%20analysis.%20When%0Acombined%20with%20a%20standard%20narrowband%20signal%20detection%20and%20spatial%20filtering%0Apipeline%2C%20such%20as%20turboSETI%2C%20GLOBULAR%20clustering%20offers%20significant%0Aimprovements%20in%20the%20false-positive%20rate%20over%20the%20standard%20pipeline%20alone%2C%0Asuggesting%20dramatic%20potential%20for%20the%20amelioration%20of%20manual%20follow-up%0Arequirements%20for%20future%20large%20surveys.%20By%20removing%20RFI%20signals%20in%20regions%20of%0Ahigh%20spectral%20occupancy%2C%20GLOBULAR%20clustering%20may%20also%20enable%20the%20detection%20of%0Asignals%20missed%20by%20the%20standard%20pipeline.%20We%20benchmark%20our%20method%20against%20the%0AChoza%20et%20al.%20%282024%29%20turboSETI-only%20search%20of%2097%20nearby%20galaxies%20at%20L-band%2C%0Ademonstrating%20a%20false-positive%20hit%20reduction%20rate%20of%2093.1%25%20and%20a%20false-positive%0Aevent%20reduction%20rate%20of%2099.3%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16556v1&entry.124074799=Read"},
{"title": "Factorized Visual Tokenization and Generation", "author": "Zechen Bai and Jianxiong Gao and Ziteng Gao and Pichao Wang and Zheng Zhang and Tong He and Mike Zheng Shou", "abstract": "  Visual tokenizers are fundamental to image generation. They convert visual\ndata into discrete tokens, enabling transformer-based models to excel at image\ngeneration. Despite their success, VQ-based tokenizers like VQGAN face\nsignificant limitations due to constrained vocabulary sizes. Simply expanding\nthe codebook often leads to training instability and diminishing performance\ngains, making scalability a critical challenge. In this work, we introduce\nFactorized Quantization (FQ), a novel approach that revitalizes VQ-based\ntokenizers by decomposing a large codebook into multiple independent\nsub-codebooks. This factorization reduces the lookup complexity of large\ncodebooks, enabling more efficient and scalable visual tokenization. To ensure\neach sub-codebook captures distinct and complementary information, we propose a\ndisentanglement regularization that explicitly reduces redundancy, promoting\ndiversity across the sub-codebooks. Furthermore, we integrate representation\nlearning into the training process, leveraging pretrained vision models like\nCLIP and DINO to infuse semantic richness into the learned representations.\nThis design ensures our tokenizer captures diverse semantic levels, leading to\nmore expressive and disentangled representations. Experiments show that the\nproposed FQGAN model substantially improves the reconstruction quality of\nvisual tokenizers, achieving state-of-the-art performance. We further\ndemonstrate that this tokenizer can be effectively adapted into auto-regressive\nimage generation. https://showlab.github.io/FQGAN\n", "link": "http://arxiv.org/abs/2411.16681v1", "date": "2024-11-25", "relevancy": 2.2672, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6072}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5426}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Factorized%20Visual%20Tokenization%20and%20Generation&body=Title%3A%20Factorized%20Visual%20Tokenization%20and%20Generation%0AAuthor%3A%20Zechen%20Bai%20and%20Jianxiong%20Gao%20and%20Ziteng%20Gao%20and%20Pichao%20Wang%20and%20Zheng%20Zhang%20and%20Tong%20He%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Visual%20tokenizers%20are%20fundamental%20to%20image%20generation.%20They%20convert%20visual%0Adata%20into%20discrete%20tokens%2C%20enabling%20transformer-based%20models%20to%20excel%20at%20image%0Ageneration.%20Despite%20their%20success%2C%20VQ-based%20tokenizers%20like%20VQGAN%20face%0Asignificant%20limitations%20due%20to%20constrained%20vocabulary%20sizes.%20Simply%20expanding%0Athe%20codebook%20often%20leads%20to%20training%20instability%20and%20diminishing%20performance%0Agains%2C%20making%20scalability%20a%20critical%20challenge.%20In%20this%20work%2C%20we%20introduce%0AFactorized%20Quantization%20%28FQ%29%2C%20a%20novel%20approach%20that%20revitalizes%20VQ-based%0Atokenizers%20by%20decomposing%20a%20large%20codebook%20into%20multiple%20independent%0Asub-codebooks.%20This%20factorization%20reduces%20the%20lookup%20complexity%20of%20large%0Acodebooks%2C%20enabling%20more%20efficient%20and%20scalable%20visual%20tokenization.%20To%20ensure%0Aeach%20sub-codebook%20captures%20distinct%20and%20complementary%20information%2C%20we%20propose%20a%0Adisentanglement%20regularization%20that%20explicitly%20reduces%20redundancy%2C%20promoting%0Adiversity%20across%20the%20sub-codebooks.%20Furthermore%2C%20we%20integrate%20representation%0Alearning%20into%20the%20training%20process%2C%20leveraging%20pretrained%20vision%20models%20like%0ACLIP%20and%20DINO%20to%20infuse%20semantic%20richness%20into%20the%20learned%20representations.%0AThis%20design%20ensures%20our%20tokenizer%20captures%20diverse%20semantic%20levels%2C%20leading%20to%0Amore%20expressive%20and%20disentangled%20representations.%20Experiments%20show%20that%20the%0Aproposed%20FQGAN%20model%20substantially%20improves%20the%20reconstruction%20quality%20of%0Avisual%20tokenizers%2C%20achieving%20state-of-the-art%20performance.%20We%20further%0Ademonstrate%20that%20this%20tokenizer%20can%20be%20effectively%20adapted%20into%20auto-regressive%0Aimage%20generation.%20https%3A//showlab.github.io/FQGAN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactorized%2520Visual%2520Tokenization%2520and%2520Generation%26entry.906535625%3DZechen%2520Bai%2520and%2520Jianxiong%2520Gao%2520and%2520Ziteng%2520Gao%2520and%2520Pichao%2520Wang%2520and%2520Zheng%2520Zhang%2520and%2520Tong%2520He%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Visual%2520tokenizers%2520are%2520fundamental%2520to%2520image%2520generation.%2520They%2520convert%2520visual%250Adata%2520into%2520discrete%2520tokens%252C%2520enabling%2520transformer-based%2520models%2520to%2520excel%2520at%2520image%250Ageneration.%2520Despite%2520their%2520success%252C%2520VQ-based%2520tokenizers%2520like%2520VQGAN%2520face%250Asignificant%2520limitations%2520due%2520to%2520constrained%2520vocabulary%2520sizes.%2520Simply%2520expanding%250Athe%2520codebook%2520often%2520leads%2520to%2520training%2520instability%2520and%2520diminishing%2520performance%250Agains%252C%2520making%2520scalability%2520a%2520critical%2520challenge.%2520In%2520this%2520work%252C%2520we%2520introduce%250AFactorized%2520Quantization%2520%2528FQ%2529%252C%2520a%2520novel%2520approach%2520that%2520revitalizes%2520VQ-based%250Atokenizers%2520by%2520decomposing%2520a%2520large%2520codebook%2520into%2520multiple%2520independent%250Asub-codebooks.%2520This%2520factorization%2520reduces%2520the%2520lookup%2520complexity%2520of%2520large%250Acodebooks%252C%2520enabling%2520more%2520efficient%2520and%2520scalable%2520visual%2520tokenization.%2520To%2520ensure%250Aeach%2520sub-codebook%2520captures%2520distinct%2520and%2520complementary%2520information%252C%2520we%2520propose%2520a%250Adisentanglement%2520regularization%2520that%2520explicitly%2520reduces%2520redundancy%252C%2520promoting%250Adiversity%2520across%2520the%2520sub-codebooks.%2520Furthermore%252C%2520we%2520integrate%2520representation%250Alearning%2520into%2520the%2520training%2520process%252C%2520leveraging%2520pretrained%2520vision%2520models%2520like%250ACLIP%2520and%2520DINO%2520to%2520infuse%2520semantic%2520richness%2520into%2520the%2520learned%2520representations.%250AThis%2520design%2520ensures%2520our%2520tokenizer%2520captures%2520diverse%2520semantic%2520levels%252C%2520leading%2520to%250Amore%2520expressive%2520and%2520disentangled%2520representations.%2520Experiments%2520show%2520that%2520the%250Aproposed%2520FQGAN%2520model%2520substantially%2520improves%2520the%2520reconstruction%2520quality%2520of%250Avisual%2520tokenizers%252C%2520achieving%2520state-of-the-art%2520performance.%2520We%2520further%250Ademonstrate%2520that%2520this%2520tokenizer%2520can%2520be%2520effectively%2520adapted%2520into%2520auto-regressive%250Aimage%2520generation.%2520https%253A//showlab.github.io/FQGAN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factorized%20Visual%20Tokenization%20and%20Generation&entry.906535625=Zechen%20Bai%20and%20Jianxiong%20Gao%20and%20Ziteng%20Gao%20and%20Pichao%20Wang%20and%20Zheng%20Zhang%20and%20Tong%20He%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Visual%20tokenizers%20are%20fundamental%20to%20image%20generation.%20They%20convert%20visual%0Adata%20into%20discrete%20tokens%2C%20enabling%20transformer-based%20models%20to%20excel%20at%20image%0Ageneration.%20Despite%20their%20success%2C%20VQ-based%20tokenizers%20like%20VQGAN%20face%0Asignificant%20limitations%20due%20to%20constrained%20vocabulary%20sizes.%20Simply%20expanding%0Athe%20codebook%20often%20leads%20to%20training%20instability%20and%20diminishing%20performance%0Agains%2C%20making%20scalability%20a%20critical%20challenge.%20In%20this%20work%2C%20we%20introduce%0AFactorized%20Quantization%20%28FQ%29%2C%20a%20novel%20approach%20that%20revitalizes%20VQ-based%0Atokenizers%20by%20decomposing%20a%20large%20codebook%20into%20multiple%20independent%0Asub-codebooks.%20This%20factorization%20reduces%20the%20lookup%20complexity%20of%20large%0Acodebooks%2C%20enabling%20more%20efficient%20and%20scalable%20visual%20tokenization.%20To%20ensure%0Aeach%20sub-codebook%20captures%20distinct%20and%20complementary%20information%2C%20we%20propose%20a%0Adisentanglement%20regularization%20that%20explicitly%20reduces%20redundancy%2C%20promoting%0Adiversity%20across%20the%20sub-codebooks.%20Furthermore%2C%20we%20integrate%20representation%0Alearning%20into%20the%20training%20process%2C%20leveraging%20pretrained%20vision%20models%20like%0ACLIP%20and%20DINO%20to%20infuse%20semantic%20richness%20into%20the%20learned%20representations.%0AThis%20design%20ensures%20our%20tokenizer%20captures%20diverse%20semantic%20levels%2C%20leading%20to%0Amore%20expressive%20and%20disentangled%20representations.%20Experiments%20show%20that%20the%0Aproposed%20FQGAN%20model%20substantially%20improves%20the%20reconstruction%20quality%20of%0Avisual%20tokenizers%2C%20achieving%20state-of-the-art%20performance.%20We%20further%0Ademonstrate%20that%20this%20tokenizer%20can%20be%20effectively%20adapted%20into%20auto-regressive%0Aimage%20generation.%20https%3A//showlab.github.io/FQGAN%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16681v1&entry.124074799=Read"},
{"title": "CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance\n  Segmentation", "author": "Leon Sick and Dominik Engel and Sebastian Hartwig and Pedro Hermosilla and Timo Ropinski", "abstract": "  Traditionally, algorithms that learn to segment object instances in 2D images\nhave heavily relied on large amounts of human-annotated data. Only recently,\nnovel approaches have emerged tackling this problem in an unsupervised fashion.\nGenerally, these approaches first generate pseudo-masks and then train a\nclass-agnostic detector. While such methods deliver the current state of the\nart, they often fail to correctly separate instances overlapping in 2D image\nspace since only semantics are considered. To tackle this issue, we instead\npropose to cut the semantic masks in 3D to obtain the final 2D instances by\nutilizing a point cloud representation of the scene. Furthermore, we derive a\nSpatial Importance function, which we use to resharpen the semantics along the\n3D borders of instances. Nevertheless, these pseudo-masks are still subject to\nmask ambiguity. To address this issue, we further propose to augment the\ntraining of a class-agnostic detector with three Spatial Confidence components\naiming to isolate a clean learning signal. With these contributions, our\napproach outperforms competing methods across multiple standard benchmarks for\nunsupervised instance segmentation and object detection.\n", "link": "http://arxiv.org/abs/2411.16319v1", "date": "2024-11-25", "relevancy": 2.2427, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5648}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5637}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CutS3D%3A%20Cutting%20Semantics%20in%203D%20for%202D%20Unsupervised%20Instance%0A%20%20Segmentation&body=Title%3A%20CutS3D%3A%20Cutting%20Semantics%20in%203D%20for%202D%20Unsupervised%20Instance%0A%20%20Segmentation%0AAuthor%3A%20Leon%20Sick%20and%20Dominik%20Engel%20and%20Sebastian%20Hartwig%20and%20Pedro%20Hermosilla%20and%20Timo%20Ropinski%0AAbstract%3A%20%20%20Traditionally%2C%20algorithms%20that%20learn%20to%20segment%20object%20instances%20in%202D%20images%0Ahave%20heavily%20relied%20on%20large%20amounts%20of%20human-annotated%20data.%20Only%20recently%2C%0Anovel%20approaches%20have%20emerged%20tackling%20this%20problem%20in%20an%20unsupervised%20fashion.%0AGenerally%2C%20these%20approaches%20first%20generate%20pseudo-masks%20and%20then%20train%20a%0Aclass-agnostic%20detector.%20While%20such%20methods%20deliver%20the%20current%20state%20of%20the%0Aart%2C%20they%20often%20fail%20to%20correctly%20separate%20instances%20overlapping%20in%202D%20image%0Aspace%20since%20only%20semantics%20are%20considered.%20To%20tackle%20this%20issue%2C%20we%20instead%0Apropose%20to%20cut%20the%20semantic%20masks%20in%203D%20to%20obtain%20the%20final%202D%20instances%20by%0Autilizing%20a%20point%20cloud%20representation%20of%20the%20scene.%20Furthermore%2C%20we%20derive%20a%0ASpatial%20Importance%20function%2C%20which%20we%20use%20to%20resharpen%20the%20semantics%20along%20the%0A3D%20borders%20of%20instances.%20Nevertheless%2C%20these%20pseudo-masks%20are%20still%20subject%20to%0Amask%20ambiguity.%20To%20address%20this%20issue%2C%20we%20further%20propose%20to%20augment%20the%0Atraining%20of%20a%20class-agnostic%20detector%20with%20three%20Spatial%20Confidence%20components%0Aaiming%20to%20isolate%20a%20clean%20learning%20signal.%20With%20these%20contributions%2C%20our%0Aapproach%20outperforms%20competing%20methods%20across%20multiple%20standard%20benchmarks%20for%0Aunsupervised%20instance%20segmentation%20and%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCutS3D%253A%2520Cutting%2520Semantics%2520in%25203D%2520for%25202D%2520Unsupervised%2520Instance%250A%2520%2520Segmentation%26entry.906535625%3DLeon%2520Sick%2520and%2520Dominik%2520Engel%2520and%2520Sebastian%2520Hartwig%2520and%2520Pedro%2520Hermosilla%2520and%2520Timo%2520Ropinski%26entry.1292438233%3D%2520%2520Traditionally%252C%2520algorithms%2520that%2520learn%2520to%2520segment%2520object%2520instances%2520in%25202D%2520images%250Ahave%2520heavily%2520relied%2520on%2520large%2520amounts%2520of%2520human-annotated%2520data.%2520Only%2520recently%252C%250Anovel%2520approaches%2520have%2520emerged%2520tackling%2520this%2520problem%2520in%2520an%2520unsupervised%2520fashion.%250AGenerally%252C%2520these%2520approaches%2520first%2520generate%2520pseudo-masks%2520and%2520then%2520train%2520a%250Aclass-agnostic%2520detector.%2520While%2520such%2520methods%2520deliver%2520the%2520current%2520state%2520of%2520the%250Aart%252C%2520they%2520often%2520fail%2520to%2520correctly%2520separate%2520instances%2520overlapping%2520in%25202D%2520image%250Aspace%2520since%2520only%2520semantics%2520are%2520considered.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520instead%250Apropose%2520to%2520cut%2520the%2520semantic%2520masks%2520in%25203D%2520to%2520obtain%2520the%2520final%25202D%2520instances%2520by%250Autilizing%2520a%2520point%2520cloud%2520representation%2520of%2520the%2520scene.%2520Furthermore%252C%2520we%2520derive%2520a%250ASpatial%2520Importance%2520function%252C%2520which%2520we%2520use%2520to%2520resharpen%2520the%2520semantics%2520along%2520the%250A3D%2520borders%2520of%2520instances.%2520Nevertheless%252C%2520these%2520pseudo-masks%2520are%2520still%2520subject%2520to%250Amask%2520ambiguity.%2520To%2520address%2520this%2520issue%252C%2520we%2520further%2520propose%2520to%2520augment%2520the%250Atraining%2520of%2520a%2520class-agnostic%2520detector%2520with%2520three%2520Spatial%2520Confidence%2520components%250Aaiming%2520to%2520isolate%2520a%2520clean%2520learning%2520signal.%2520With%2520these%2520contributions%252C%2520our%250Aapproach%2520outperforms%2520competing%2520methods%2520across%2520multiple%2520standard%2520benchmarks%2520for%250Aunsupervised%2520instance%2520segmentation%2520and%2520object%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CutS3D%3A%20Cutting%20Semantics%20in%203D%20for%202D%20Unsupervised%20Instance%0A%20%20Segmentation&entry.906535625=Leon%20Sick%20and%20Dominik%20Engel%20and%20Sebastian%20Hartwig%20and%20Pedro%20Hermosilla%20and%20Timo%20Ropinski&entry.1292438233=%20%20Traditionally%2C%20algorithms%20that%20learn%20to%20segment%20object%20instances%20in%202D%20images%0Ahave%20heavily%20relied%20on%20large%20amounts%20of%20human-annotated%20data.%20Only%20recently%2C%0Anovel%20approaches%20have%20emerged%20tackling%20this%20problem%20in%20an%20unsupervised%20fashion.%0AGenerally%2C%20these%20approaches%20first%20generate%20pseudo-masks%20and%20then%20train%20a%0Aclass-agnostic%20detector.%20While%20such%20methods%20deliver%20the%20current%20state%20of%20the%0Aart%2C%20they%20often%20fail%20to%20correctly%20separate%20instances%20overlapping%20in%202D%20image%0Aspace%20since%20only%20semantics%20are%20considered.%20To%20tackle%20this%20issue%2C%20we%20instead%0Apropose%20to%20cut%20the%20semantic%20masks%20in%203D%20to%20obtain%20the%20final%202D%20instances%20by%0Autilizing%20a%20point%20cloud%20representation%20of%20the%20scene.%20Furthermore%2C%20we%20derive%20a%0ASpatial%20Importance%20function%2C%20which%20we%20use%20to%20resharpen%20the%20semantics%20along%20the%0A3D%20borders%20of%20instances.%20Nevertheless%2C%20these%20pseudo-masks%20are%20still%20subject%20to%0Amask%20ambiguity.%20To%20address%20this%20issue%2C%20we%20further%20propose%20to%20augment%20the%0Atraining%20of%20a%20class-agnostic%20detector%20with%20three%20Spatial%20Confidence%20components%0Aaiming%20to%20isolate%20a%20clean%20learning%20signal.%20With%20these%20contributions%2C%20our%0Aapproach%20outperforms%20competing%20methods%20across%20multiple%20standard%20benchmarks%20for%0Aunsupervised%20instance%20segmentation%20and%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16319v1&entry.124074799=Read"},
{"title": "No Identity, no problem: Motion through detection for people tracking", "author": "Martin Engilberge and F. Wilke Grosche and Pascal Fua", "abstract": "  Tracking-by-detection has become the de facto standard approach to people\ntracking. To increase robustness, some approaches incorporate re-identification\nusing appearance models and regressing motion offset, which requires costly\nidentity annotations. In this paper, we propose exploiting motion clues while\nproviding supervision only for the detections, which is much easier to do. Our\nalgorithm predicts detection heatmaps at two different times, along with a 2D\nmotion estimate between the two images. It then warps one heatmap using the\nmotion estimate and enforces consistency with the other one. This provides the\nrequired supervisory signal on the motion without the need for any motion\nannotations. In this manner, we couple the information obtained from different\nimages during training and increase accuracy, especially in crowded scenes and\nwhen using low frame-rate sequences. We show that our approach delivers\nstate-of-the-art results for single- and multi-view multi-target tracking on\nthe MOT17 and WILDTRACK datasets.\n", "link": "http://arxiv.org/abs/2411.16466v1", "date": "2024-11-25", "relevancy": 2.2413, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.575}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5526}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Identity%2C%20no%20problem%3A%20Motion%20through%20detection%20for%20people%20tracking&body=Title%3A%20No%20Identity%2C%20no%20problem%3A%20Motion%20through%20detection%20for%20people%20tracking%0AAuthor%3A%20Martin%20Engilberge%20and%20F.%20Wilke%20Grosche%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Tracking-by-detection%20has%20become%20the%20de%20facto%20standard%20approach%20to%20people%0Atracking.%20To%20increase%20robustness%2C%20some%20approaches%20incorporate%20re-identification%0Ausing%20appearance%20models%20and%20regressing%20motion%20offset%2C%20which%20requires%20costly%0Aidentity%20annotations.%20In%20this%20paper%2C%20we%20propose%20exploiting%20motion%20clues%20while%0Aproviding%20supervision%20only%20for%20the%20detections%2C%20which%20is%20much%20easier%20to%20do.%20Our%0Aalgorithm%20predicts%20detection%20heatmaps%20at%20two%20different%20times%2C%20along%20with%20a%202D%0Amotion%20estimate%20between%20the%20two%20images.%20It%20then%20warps%20one%20heatmap%20using%20the%0Amotion%20estimate%20and%20enforces%20consistency%20with%20the%20other%20one.%20This%20provides%20the%0Arequired%20supervisory%20signal%20on%20the%20motion%20without%20the%20need%20for%20any%20motion%0Aannotations.%20In%20this%20manner%2C%20we%20couple%20the%20information%20obtained%20from%20different%0Aimages%20during%20training%20and%20increase%20accuracy%2C%20especially%20in%20crowded%20scenes%20and%0Awhen%20using%20low%20frame-rate%20sequences.%20We%20show%20that%20our%20approach%20delivers%0Astate-of-the-art%20results%20for%20single-%20and%20multi-view%20multi-target%20tracking%20on%0Athe%20MOT17%20and%20WILDTRACK%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Identity%252C%2520no%2520problem%253A%2520Motion%2520through%2520detection%2520for%2520people%2520tracking%26entry.906535625%3DMartin%2520Engilberge%2520and%2520F.%2520Wilke%2520Grosche%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520Tracking-by-detection%2520has%2520become%2520the%2520de%2520facto%2520standard%2520approach%2520to%2520people%250Atracking.%2520To%2520increase%2520robustness%252C%2520some%2520approaches%2520incorporate%2520re-identification%250Ausing%2520appearance%2520models%2520and%2520regressing%2520motion%2520offset%252C%2520which%2520requires%2520costly%250Aidentity%2520annotations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520exploiting%2520motion%2520clues%2520while%250Aproviding%2520supervision%2520only%2520for%2520the%2520detections%252C%2520which%2520is%2520much%2520easier%2520to%2520do.%2520Our%250Aalgorithm%2520predicts%2520detection%2520heatmaps%2520at%2520two%2520different%2520times%252C%2520along%2520with%2520a%25202D%250Amotion%2520estimate%2520between%2520the%2520two%2520images.%2520It%2520then%2520warps%2520one%2520heatmap%2520using%2520the%250Amotion%2520estimate%2520and%2520enforces%2520consistency%2520with%2520the%2520other%2520one.%2520This%2520provides%2520the%250Arequired%2520supervisory%2520signal%2520on%2520the%2520motion%2520without%2520the%2520need%2520for%2520any%2520motion%250Aannotations.%2520In%2520this%2520manner%252C%2520we%2520couple%2520the%2520information%2520obtained%2520from%2520different%250Aimages%2520during%2520training%2520and%2520increase%2520accuracy%252C%2520especially%2520in%2520crowded%2520scenes%2520and%250Awhen%2520using%2520low%2520frame-rate%2520sequences.%2520We%2520show%2520that%2520our%2520approach%2520delivers%250Astate-of-the-art%2520results%2520for%2520single-%2520and%2520multi-view%2520multi-target%2520tracking%2520on%250Athe%2520MOT17%2520and%2520WILDTRACK%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Identity%2C%20no%20problem%3A%20Motion%20through%20detection%20for%20people%20tracking&entry.906535625=Martin%20Engilberge%20and%20F.%20Wilke%20Grosche%20and%20Pascal%20Fua&entry.1292438233=%20%20Tracking-by-detection%20has%20become%20the%20de%20facto%20standard%20approach%20to%20people%0Atracking.%20To%20increase%20robustness%2C%20some%20approaches%20incorporate%20re-identification%0Ausing%20appearance%20models%20and%20regressing%20motion%20offset%2C%20which%20requires%20costly%0Aidentity%20annotations.%20In%20this%20paper%2C%20we%20propose%20exploiting%20motion%20clues%20while%0Aproviding%20supervision%20only%20for%20the%20detections%2C%20which%20is%20much%20easier%20to%20do.%20Our%0Aalgorithm%20predicts%20detection%20heatmaps%20at%20two%20different%20times%2C%20along%20with%20a%202D%0Amotion%20estimate%20between%20the%20two%20images.%20It%20then%20warps%20one%20heatmap%20using%20the%0Amotion%20estimate%20and%20enforces%20consistency%20with%20the%20other%20one.%20This%20provides%20the%0Arequired%20supervisory%20signal%20on%20the%20motion%20without%20the%20need%20for%20any%20motion%0Aannotations.%20In%20this%20manner%2C%20we%20couple%20the%20information%20obtained%20from%20different%0Aimages%20during%20training%20and%20increase%20accuracy%2C%20especially%20in%20crowded%20scenes%20and%0Awhen%20using%20low%20frame-rate%20sequences.%20We%20show%20that%20our%20approach%20delivers%0Astate-of-the-art%20results%20for%20single-%20and%20multi-view%20multi-target%20tracking%20on%0Athe%20MOT17%20and%20WILDTRACK%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16466v1&entry.124074799=Read"},
{"title": "LaVida Drive: Vision-Text Interaction VLM for Autonomous Driving with\n  Token Selection, Recovery and Enhancement", "author": "Siwen Jiao and Yangyi Fang and Baoyun Peng and Wangqun Chen and Bharadwaj Veeravalli", "abstract": "  Recent advancements in Visual Language Models (VLMs) have made them crucial\nfor visual question answering (VQA) in autonomous driving, enabling natural\nhuman-vehicle interactions. However, existing methods often struggle in dynamic\ndriving environments, as they usually focus on static images or videos and rely\non downsampling to manage computational costs. This results in the loss of\ncritical details and the difficulty in effectively integrating spatial and\ntemporal information, undermining fine-grained perception and temporal\ncoherence essential for effective decision-making. To tackle these challenges,\nwe introduce LaVida Drive, a novel and efficient VQA framework for autonomous\ndriving. LaVida Drive seamlessly integrates temporal data while maintaining\nhigh-resolution inputs for detailed visual perception. It optimizes spatial\nprocessing by retaining high-resolution data for intricate details and using\nlower-resolution inputs for temporal analysis to focus on motion-related\nfeatures, thereby boosting computational efficiency. The core of LaVida Drive\nconsists of two modules: the \\textit{Query-aware Token Selection} module and\nthe \\textit{Spatial-Temporal Token Recovery and Enhancement} module. The former\ndynamically selects the most relevant visual tokens based on semantic alignment\nwith the input query, reducing the token count from high-resolution spatial\ninput. The latter ensures smooth and coherent interactions between spatial and\ntemporal information, preserving contextual continuity across frames. Extensive\nexperiments on various autonomous driving question-answering benchmarks show\nthat LaVida Drive significantly reduces visual tokens, enhances efficiency, and\nimproves overall performance.\n", "link": "http://arxiv.org/abs/2411.12980v2", "date": "2024-11-25", "relevancy": 2.233, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaVida%20Drive%3A%20Vision-Text%20Interaction%20VLM%20for%20Autonomous%20Driving%20with%0A%20%20Token%20Selection%2C%20Recovery%20and%20Enhancement&body=Title%3A%20LaVida%20Drive%3A%20Vision-Text%20Interaction%20VLM%20for%20Autonomous%20Driving%20with%0A%20%20Token%20Selection%2C%20Recovery%20and%20Enhancement%0AAuthor%3A%20Siwen%20Jiao%20and%20Yangyi%20Fang%20and%20Baoyun%20Peng%20and%20Wangqun%20Chen%20and%20Bharadwaj%20Veeravalli%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Visual%20Language%20Models%20%28VLMs%29%20have%20made%20them%20crucial%0Afor%20visual%20question%20answering%20%28VQA%29%20in%20autonomous%20driving%2C%20enabling%20natural%0Ahuman-vehicle%20interactions.%20However%2C%20existing%20methods%20often%20struggle%20in%20dynamic%0Adriving%20environments%2C%20as%20they%20usually%20focus%20on%20static%20images%20or%20videos%20and%20rely%0Aon%20downsampling%20to%20manage%20computational%20costs.%20This%20results%20in%20the%20loss%20of%0Acritical%20details%20and%20the%20difficulty%20in%20effectively%20integrating%20spatial%20and%0Atemporal%20information%2C%20undermining%20fine-grained%20perception%20and%20temporal%0Acoherence%20essential%20for%20effective%20decision-making.%20To%20tackle%20these%20challenges%2C%0Awe%20introduce%20LaVida%20Drive%2C%20a%20novel%20and%20efficient%20VQA%20framework%20for%20autonomous%0Adriving.%20LaVida%20Drive%20seamlessly%20integrates%20temporal%20data%20while%20maintaining%0Ahigh-resolution%20inputs%20for%20detailed%20visual%20perception.%20It%20optimizes%20spatial%0Aprocessing%20by%20retaining%20high-resolution%20data%20for%20intricate%20details%20and%20using%0Alower-resolution%20inputs%20for%20temporal%20analysis%20to%20focus%20on%20motion-related%0Afeatures%2C%20thereby%20boosting%20computational%20efficiency.%20The%20core%20of%20LaVida%20Drive%0Aconsists%20of%20two%20modules%3A%20the%20%5Ctextit%7BQuery-aware%20Token%20Selection%7D%20module%20and%0Athe%20%5Ctextit%7BSpatial-Temporal%20Token%20Recovery%20and%20Enhancement%7D%20module.%20The%20former%0Adynamically%20selects%20the%20most%20relevant%20visual%20tokens%20based%20on%20semantic%20alignment%0Awith%20the%20input%20query%2C%20reducing%20the%20token%20count%20from%20high-resolution%20spatial%0Ainput.%20The%20latter%20ensures%20smooth%20and%20coherent%20interactions%20between%20spatial%20and%0Atemporal%20information%2C%20preserving%20contextual%20continuity%20across%20frames.%20Extensive%0Aexperiments%20on%20various%20autonomous%20driving%20question-answering%20benchmarks%20show%0Athat%20LaVida%20Drive%20significantly%20reduces%20visual%20tokens%2C%20enhances%20efficiency%2C%20and%0Aimproves%20overall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaVida%2520Drive%253A%2520Vision-Text%2520Interaction%2520VLM%2520for%2520Autonomous%2520Driving%2520with%250A%2520%2520Token%2520Selection%252C%2520Recovery%2520and%2520Enhancement%26entry.906535625%3DSiwen%2520Jiao%2520and%2520Yangyi%2520Fang%2520and%2520Baoyun%2520Peng%2520and%2520Wangqun%2520Chen%2520and%2520Bharadwaj%2520Veeravalli%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520made%2520them%2520crucial%250Afor%2520visual%2520question%2520answering%2520%2528VQA%2529%2520in%2520autonomous%2520driving%252C%2520enabling%2520natural%250Ahuman-vehicle%2520interactions.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520in%2520dynamic%250Adriving%2520environments%252C%2520as%2520they%2520usually%2520focus%2520on%2520static%2520images%2520or%2520videos%2520and%2520rely%250Aon%2520downsampling%2520to%2520manage%2520computational%2520costs.%2520This%2520results%2520in%2520the%2520loss%2520of%250Acritical%2520details%2520and%2520the%2520difficulty%2520in%2520effectively%2520integrating%2520spatial%2520and%250Atemporal%2520information%252C%2520undermining%2520fine-grained%2520perception%2520and%2520temporal%250Acoherence%2520essential%2520for%2520effective%2520decision-making.%2520To%2520tackle%2520these%2520challenges%252C%250Awe%2520introduce%2520LaVida%2520Drive%252C%2520a%2520novel%2520and%2520efficient%2520VQA%2520framework%2520for%2520autonomous%250Adriving.%2520LaVida%2520Drive%2520seamlessly%2520integrates%2520temporal%2520data%2520while%2520maintaining%250Ahigh-resolution%2520inputs%2520for%2520detailed%2520visual%2520perception.%2520It%2520optimizes%2520spatial%250Aprocessing%2520by%2520retaining%2520high-resolution%2520data%2520for%2520intricate%2520details%2520and%2520using%250Alower-resolution%2520inputs%2520for%2520temporal%2520analysis%2520to%2520focus%2520on%2520motion-related%250Afeatures%252C%2520thereby%2520boosting%2520computational%2520efficiency.%2520The%2520core%2520of%2520LaVida%2520Drive%250Aconsists%2520of%2520two%2520modules%253A%2520the%2520%255Ctextit%257BQuery-aware%2520Token%2520Selection%257D%2520module%2520and%250Athe%2520%255Ctextit%257BSpatial-Temporal%2520Token%2520Recovery%2520and%2520Enhancement%257D%2520module.%2520The%2520former%250Adynamically%2520selects%2520the%2520most%2520relevant%2520visual%2520tokens%2520based%2520on%2520semantic%2520alignment%250Awith%2520the%2520input%2520query%252C%2520reducing%2520the%2520token%2520count%2520from%2520high-resolution%2520spatial%250Ainput.%2520The%2520latter%2520ensures%2520smooth%2520and%2520coherent%2520interactions%2520between%2520spatial%2520and%250Atemporal%2520information%252C%2520preserving%2520contextual%2520continuity%2520across%2520frames.%2520Extensive%250Aexperiments%2520on%2520various%2520autonomous%2520driving%2520question-answering%2520benchmarks%2520show%250Athat%2520LaVida%2520Drive%2520significantly%2520reduces%2520visual%2520tokens%252C%2520enhances%2520efficiency%252C%2520and%250Aimproves%2520overall%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaVida%20Drive%3A%20Vision-Text%20Interaction%20VLM%20for%20Autonomous%20Driving%20with%0A%20%20Token%20Selection%2C%20Recovery%20and%20Enhancement&entry.906535625=Siwen%20Jiao%20and%20Yangyi%20Fang%20and%20Baoyun%20Peng%20and%20Wangqun%20Chen%20and%20Bharadwaj%20Veeravalli&entry.1292438233=%20%20Recent%20advancements%20in%20Visual%20Language%20Models%20%28VLMs%29%20have%20made%20them%20crucial%0Afor%20visual%20question%20answering%20%28VQA%29%20in%20autonomous%20driving%2C%20enabling%20natural%0Ahuman-vehicle%20interactions.%20However%2C%20existing%20methods%20often%20struggle%20in%20dynamic%0Adriving%20environments%2C%20as%20they%20usually%20focus%20on%20static%20images%20or%20videos%20and%20rely%0Aon%20downsampling%20to%20manage%20computational%20costs.%20This%20results%20in%20the%20loss%20of%0Acritical%20details%20and%20the%20difficulty%20in%20effectively%20integrating%20spatial%20and%0Atemporal%20information%2C%20undermining%20fine-grained%20perception%20and%20temporal%0Acoherence%20essential%20for%20effective%20decision-making.%20To%20tackle%20these%20challenges%2C%0Awe%20introduce%20LaVida%20Drive%2C%20a%20novel%20and%20efficient%20VQA%20framework%20for%20autonomous%0Adriving.%20LaVida%20Drive%20seamlessly%20integrates%20temporal%20data%20while%20maintaining%0Ahigh-resolution%20inputs%20for%20detailed%20visual%20perception.%20It%20optimizes%20spatial%0Aprocessing%20by%20retaining%20high-resolution%20data%20for%20intricate%20details%20and%20using%0Alower-resolution%20inputs%20for%20temporal%20analysis%20to%20focus%20on%20motion-related%0Afeatures%2C%20thereby%20boosting%20computational%20efficiency.%20The%20core%20of%20LaVida%20Drive%0Aconsists%20of%20two%20modules%3A%20the%20%5Ctextit%7BQuery-aware%20Token%20Selection%7D%20module%20and%0Athe%20%5Ctextit%7BSpatial-Temporal%20Token%20Recovery%20and%20Enhancement%7D%20module.%20The%20former%0Adynamically%20selects%20the%20most%20relevant%20visual%20tokens%20based%20on%20semantic%20alignment%0Awith%20the%20input%20query%2C%20reducing%20the%20token%20count%20from%20high-resolution%20spatial%0Ainput.%20The%20latter%20ensures%20smooth%20and%20coherent%20interactions%20between%20spatial%20and%0Atemporal%20information%2C%20preserving%20contextual%20continuity%20across%20frames.%20Extensive%0Aexperiments%20on%20various%20autonomous%20driving%20question-answering%20benchmarks%20show%0Athat%20LaVida%20Drive%20significantly%20reduces%20visual%20tokens%2C%20enhances%20efficiency%2C%20and%0Aimproves%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12980v2&entry.124074799=Read"},
{"title": "GeoFormer: A Multi-Polygon Segmentation Transformer", "author": "Maxim Khomiakov and Michael Riis Andersen and Jes Frellsen", "abstract": "  In remote sensing there exists a common need for learning scale invariant\nshapes of objects like buildings. Prior works relies on tweaking multiple loss\nfunctions to convert segmentation maps into the final scale invariant\nrepresentation, necessitating arduous design and optimization. For this purpose\nwe introduce the GeoFormer, a novel architecture which presents a remedy to the\nsaid challenges, learning to generate multipolygons end-to-end. By modeling\nkeypoints as spatially dependent tokens in an auto-regressive manner, the\nGeoFormer outperforms existing works in delineating building objects from\nsatellite imagery. We evaluate the robustness of the GeoFormer against former\nmethods through a variety of parameter ablations and highlight the advantages\nof optimizing a single likelihood function. Our study presents the first\nsuccessful application of auto-regressive transformer models for multi-polygon\npredictions in remote sensing, suggesting a promising methodological\nalternative for building vectorization.\n", "link": "http://arxiv.org/abs/2411.16616v1", "date": "2024-11-25", "relevancy": 2.2325, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5656}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5634}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoFormer%3A%20A%20Multi-Polygon%20Segmentation%20Transformer&body=Title%3A%20GeoFormer%3A%20A%20Multi-Polygon%20Segmentation%20Transformer%0AAuthor%3A%20Maxim%20Khomiakov%20and%20Michael%20Riis%20Andersen%20and%20Jes%20Frellsen%0AAbstract%3A%20%20%20In%20remote%20sensing%20there%20exists%20a%20common%20need%20for%20learning%20scale%20invariant%0Ashapes%20of%20objects%20like%20buildings.%20Prior%20works%20relies%20on%20tweaking%20multiple%20loss%0Afunctions%20to%20convert%20segmentation%20maps%20into%20the%20final%20scale%20invariant%0Arepresentation%2C%20necessitating%20arduous%20design%20and%20optimization.%20For%20this%20purpose%0Awe%20introduce%20the%20GeoFormer%2C%20a%20novel%20architecture%20which%20presents%20a%20remedy%20to%20the%0Asaid%20challenges%2C%20learning%20to%20generate%20multipolygons%20end-to-end.%20By%20modeling%0Akeypoints%20as%20spatially%20dependent%20tokens%20in%20an%20auto-regressive%20manner%2C%20the%0AGeoFormer%20outperforms%20existing%20works%20in%20delineating%20building%20objects%20from%0Asatellite%20imagery.%20We%20evaluate%20the%20robustness%20of%20the%20GeoFormer%20against%20former%0Amethods%20through%20a%20variety%20of%20parameter%20ablations%20and%20highlight%20the%20advantages%0Aof%20optimizing%20a%20single%20likelihood%20function.%20Our%20study%20presents%20the%20first%0Asuccessful%20application%20of%20auto-regressive%20transformer%20models%20for%20multi-polygon%0Apredictions%20in%20remote%20sensing%2C%20suggesting%20a%20promising%20methodological%0Aalternative%20for%20building%20vectorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoFormer%253A%2520A%2520Multi-Polygon%2520Segmentation%2520Transformer%26entry.906535625%3DMaxim%2520Khomiakov%2520and%2520Michael%2520Riis%2520Andersen%2520and%2520Jes%2520Frellsen%26entry.1292438233%3D%2520%2520In%2520remote%2520sensing%2520there%2520exists%2520a%2520common%2520need%2520for%2520learning%2520scale%2520invariant%250Ashapes%2520of%2520objects%2520like%2520buildings.%2520Prior%2520works%2520relies%2520on%2520tweaking%2520multiple%2520loss%250Afunctions%2520to%2520convert%2520segmentation%2520maps%2520into%2520the%2520final%2520scale%2520invariant%250Arepresentation%252C%2520necessitating%2520arduous%2520design%2520and%2520optimization.%2520For%2520this%2520purpose%250Awe%2520introduce%2520the%2520GeoFormer%252C%2520a%2520novel%2520architecture%2520which%2520presents%2520a%2520remedy%2520to%2520the%250Asaid%2520challenges%252C%2520learning%2520to%2520generate%2520multipolygons%2520end-to-end.%2520By%2520modeling%250Akeypoints%2520as%2520spatially%2520dependent%2520tokens%2520in%2520an%2520auto-regressive%2520manner%252C%2520the%250AGeoFormer%2520outperforms%2520existing%2520works%2520in%2520delineating%2520building%2520objects%2520from%250Asatellite%2520imagery.%2520We%2520evaluate%2520the%2520robustness%2520of%2520the%2520GeoFormer%2520against%2520former%250Amethods%2520through%2520a%2520variety%2520of%2520parameter%2520ablations%2520and%2520highlight%2520the%2520advantages%250Aof%2520optimizing%2520a%2520single%2520likelihood%2520function.%2520Our%2520study%2520presents%2520the%2520first%250Asuccessful%2520application%2520of%2520auto-regressive%2520transformer%2520models%2520for%2520multi-polygon%250Apredictions%2520in%2520remote%2520sensing%252C%2520suggesting%2520a%2520promising%2520methodological%250Aalternative%2520for%2520building%2520vectorization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoFormer%3A%20A%20Multi-Polygon%20Segmentation%20Transformer&entry.906535625=Maxim%20Khomiakov%20and%20Michael%20Riis%20Andersen%20and%20Jes%20Frellsen&entry.1292438233=%20%20In%20remote%20sensing%20there%20exists%20a%20common%20need%20for%20learning%20scale%20invariant%0Ashapes%20of%20objects%20like%20buildings.%20Prior%20works%20relies%20on%20tweaking%20multiple%20loss%0Afunctions%20to%20convert%20segmentation%20maps%20into%20the%20final%20scale%20invariant%0Arepresentation%2C%20necessitating%20arduous%20design%20and%20optimization.%20For%20this%20purpose%0Awe%20introduce%20the%20GeoFormer%2C%20a%20novel%20architecture%20which%20presents%20a%20remedy%20to%20the%0Asaid%20challenges%2C%20learning%20to%20generate%20multipolygons%20end-to-end.%20By%20modeling%0Akeypoints%20as%20spatially%20dependent%20tokens%20in%20an%20auto-regressive%20manner%2C%20the%0AGeoFormer%20outperforms%20existing%20works%20in%20delineating%20building%20objects%20from%0Asatellite%20imagery.%20We%20evaluate%20the%20robustness%20of%20the%20GeoFormer%20against%20former%0Amethods%20through%20a%20variety%20of%20parameter%20ablations%20and%20highlight%20the%20advantages%0Aof%20optimizing%20a%20single%20likelihood%20function.%20Our%20study%20presents%20the%20first%0Asuccessful%20application%20of%20auto-regressive%20transformer%20models%20for%20multi-polygon%0Apredictions%20in%20remote%20sensing%2C%20suggesting%20a%20promising%20methodological%0Aalternative%20for%20building%20vectorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16616v1&entry.124074799=Read"},
{"title": "LegoPET: Hierarchical Feature Guided Conditional Diffusion for PET Image\n  Reconstruction", "author": "Yiran Sun and Osama Mawlawi", "abstract": "  Positron emission tomography (PET) is widely utilized for cancer detection\ndue to its ability to visualize functional and biological processes in vivo.\nPET images are usually reconstructed from histogrammed raw data (sinograms)\nusing traditional iterative techniques (e.g., OSEM, MLEM). Recently, deep\nlearning (DL) methods have shown promise by directly mapping raw sinogram data\nto PET images. However, DL approaches that are regression-based or GAN-based\noften produce overly smoothed images or introduce various artifacts\nrespectively. Image-conditioned diffusion probabilistic models (cDPMs) are\nanother class of likelihood-based DL techniques capable of generating highly\nrealistic and controllable images. While cDPMs have notable strengths, they\nstill face challenges such as maintain correspondence and consistency between\ninput and output images when they are from different domains (e.g., sinogram\nvs. image domain) as well as slow convergence rates. To address these\nlimitations, we introduce LegoPET, a hierarchical feature guided conditional\ndiffusion model for high-perceptual quality PET image reconstruction from\nsinograms. We conducted several experiments demonstrating that LegoPET not only\nimproves the performance of cDPMs but also surpasses recent DL-based PET image\nreconstruction techniques in terms of visual quality and pixel-level PSNR/SSIM\nmetrics. Our code is available at https://github.com/yransun/LegoPET.\n", "link": "http://arxiv.org/abs/2411.16629v1", "date": "2024-11-25", "relevancy": 2.2176, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5886}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LegoPET%3A%20Hierarchical%20Feature%20Guided%20Conditional%20Diffusion%20for%20PET%20Image%0A%20%20Reconstruction&body=Title%3A%20LegoPET%3A%20Hierarchical%20Feature%20Guided%20Conditional%20Diffusion%20for%20PET%20Image%0A%20%20Reconstruction%0AAuthor%3A%20Yiran%20Sun%20and%20Osama%20Mawlawi%0AAbstract%3A%20%20%20Positron%20emission%20tomography%20%28PET%29%20is%20widely%20utilized%20for%20cancer%20detection%0Adue%20to%20its%20ability%20to%20visualize%20functional%20and%20biological%20processes%20in%20vivo.%0APET%20images%20are%20usually%20reconstructed%20from%20histogrammed%20raw%20data%20%28sinograms%29%0Ausing%20traditional%20iterative%20techniques%20%28e.g.%2C%20OSEM%2C%20MLEM%29.%20Recently%2C%20deep%0Alearning%20%28DL%29%20methods%20have%20shown%20promise%20by%20directly%20mapping%20raw%20sinogram%20data%0Ato%20PET%20images.%20However%2C%20DL%20approaches%20that%20are%20regression-based%20or%20GAN-based%0Aoften%20produce%20overly%20smoothed%20images%20or%20introduce%20various%20artifacts%0Arespectively.%20Image-conditioned%20diffusion%20probabilistic%20models%20%28cDPMs%29%20are%0Aanother%20class%20of%20likelihood-based%20DL%20techniques%20capable%20of%20generating%20highly%0Arealistic%20and%20controllable%20images.%20While%20cDPMs%20have%20notable%20strengths%2C%20they%0Astill%20face%20challenges%20such%20as%20maintain%20correspondence%20and%20consistency%20between%0Ainput%20and%20output%20images%20when%20they%20are%20from%20different%20domains%20%28e.g.%2C%20sinogram%0Avs.%20image%20domain%29%20as%20well%20as%20slow%20convergence%20rates.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20LegoPET%2C%20a%20hierarchical%20feature%20guided%20conditional%0Adiffusion%20model%20for%20high-perceptual%20quality%20PET%20image%20reconstruction%20from%0Asinograms.%20We%20conducted%20several%20experiments%20demonstrating%20that%20LegoPET%20not%20only%0Aimproves%20the%20performance%20of%20cDPMs%20but%20also%20surpasses%20recent%20DL-based%20PET%20image%0Areconstruction%20techniques%20in%20terms%20of%20visual%20quality%20and%20pixel-level%20PSNR/SSIM%0Ametrics.%20Our%20code%20is%20available%20at%20https%3A//github.com/yransun/LegoPET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLegoPET%253A%2520Hierarchical%2520Feature%2520Guided%2520Conditional%2520Diffusion%2520for%2520PET%2520Image%250A%2520%2520Reconstruction%26entry.906535625%3DYiran%2520Sun%2520and%2520Osama%2520Mawlawi%26entry.1292438233%3D%2520%2520Positron%2520emission%2520tomography%2520%2528PET%2529%2520is%2520widely%2520utilized%2520for%2520cancer%2520detection%250Adue%2520to%2520its%2520ability%2520to%2520visualize%2520functional%2520and%2520biological%2520processes%2520in%2520vivo.%250APET%2520images%2520are%2520usually%2520reconstructed%2520from%2520histogrammed%2520raw%2520data%2520%2528sinograms%2529%250Ausing%2520traditional%2520iterative%2520techniques%2520%2528e.g.%252C%2520OSEM%252C%2520MLEM%2529.%2520Recently%252C%2520deep%250Alearning%2520%2528DL%2529%2520methods%2520have%2520shown%2520promise%2520by%2520directly%2520mapping%2520raw%2520sinogram%2520data%250Ato%2520PET%2520images.%2520However%252C%2520DL%2520approaches%2520that%2520are%2520regression-based%2520or%2520GAN-based%250Aoften%2520produce%2520overly%2520smoothed%2520images%2520or%2520introduce%2520various%2520artifacts%250Arespectively.%2520Image-conditioned%2520diffusion%2520probabilistic%2520models%2520%2528cDPMs%2529%2520are%250Aanother%2520class%2520of%2520likelihood-based%2520DL%2520techniques%2520capable%2520of%2520generating%2520highly%250Arealistic%2520and%2520controllable%2520images.%2520While%2520cDPMs%2520have%2520notable%2520strengths%252C%2520they%250Astill%2520face%2520challenges%2520such%2520as%2520maintain%2520correspondence%2520and%2520consistency%2520between%250Ainput%2520and%2520output%2520images%2520when%2520they%2520are%2520from%2520different%2520domains%2520%2528e.g.%252C%2520sinogram%250Avs.%2520image%2520domain%2529%2520as%2520well%2520as%2520slow%2520convergence%2520rates.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520LegoPET%252C%2520a%2520hierarchical%2520feature%2520guided%2520conditional%250Adiffusion%2520model%2520for%2520high-perceptual%2520quality%2520PET%2520image%2520reconstruction%2520from%250Asinograms.%2520We%2520conducted%2520several%2520experiments%2520demonstrating%2520that%2520LegoPET%2520not%2520only%250Aimproves%2520the%2520performance%2520of%2520cDPMs%2520but%2520also%2520surpasses%2520recent%2520DL-based%2520PET%2520image%250Areconstruction%2520techniques%2520in%2520terms%2520of%2520visual%2520quality%2520and%2520pixel-level%2520PSNR/SSIM%250Ametrics.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/yransun/LegoPET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LegoPET%3A%20Hierarchical%20Feature%20Guided%20Conditional%20Diffusion%20for%20PET%20Image%0A%20%20Reconstruction&entry.906535625=Yiran%20Sun%20and%20Osama%20Mawlawi&entry.1292438233=%20%20Positron%20emission%20tomography%20%28PET%29%20is%20widely%20utilized%20for%20cancer%20detection%0Adue%20to%20its%20ability%20to%20visualize%20functional%20and%20biological%20processes%20in%20vivo.%0APET%20images%20are%20usually%20reconstructed%20from%20histogrammed%20raw%20data%20%28sinograms%29%0Ausing%20traditional%20iterative%20techniques%20%28e.g.%2C%20OSEM%2C%20MLEM%29.%20Recently%2C%20deep%0Alearning%20%28DL%29%20methods%20have%20shown%20promise%20by%20directly%20mapping%20raw%20sinogram%20data%0Ato%20PET%20images.%20However%2C%20DL%20approaches%20that%20are%20regression-based%20or%20GAN-based%0Aoften%20produce%20overly%20smoothed%20images%20or%20introduce%20various%20artifacts%0Arespectively.%20Image-conditioned%20diffusion%20probabilistic%20models%20%28cDPMs%29%20are%0Aanother%20class%20of%20likelihood-based%20DL%20techniques%20capable%20of%20generating%20highly%0Arealistic%20and%20controllable%20images.%20While%20cDPMs%20have%20notable%20strengths%2C%20they%0Astill%20face%20challenges%20such%20as%20maintain%20correspondence%20and%20consistency%20between%0Ainput%20and%20output%20images%20when%20they%20are%20from%20different%20domains%20%28e.g.%2C%20sinogram%0Avs.%20image%20domain%29%20as%20well%20as%20slow%20convergence%20rates.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20LegoPET%2C%20a%20hierarchical%20feature%20guided%20conditional%0Adiffusion%20model%20for%20high-perceptual%20quality%20PET%20image%20reconstruction%20from%0Asinograms.%20We%20conducted%20several%20experiments%20demonstrating%20that%20LegoPET%20not%20only%0Aimproves%20the%20performance%20of%20cDPMs%20but%20also%20surpasses%20recent%20DL-based%20PET%20image%0Areconstruction%20techniques%20in%20terms%20of%20visual%20quality%20and%20pixel-level%20PSNR/SSIM%0Ametrics.%20Our%20code%20is%20available%20at%20https%3A//github.com/yransun/LegoPET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16629v1&entry.124074799=Read"},
{"title": "Object Augmentation Algorithm: Computing virtual object motion and\n  object induced interaction wrench from optical markers", "author": "Christopher Herneth and Junnan Li and Muhammad Hilman Fatoni and Amartya Ganguly and Sami Haddadin", "abstract": "  This study addresses the critical need for diverse and comprehensive data\nfocused on human arm joint torques while performing activities of daily living\n(ADL). Previous studies have often overlooked the influence of objects on joint\ntorques during ADL, resulting in limited datasets for analysis. To address this\ngap, we propose an Object Augmentation Algorithm (OAA) capable of augmenting\nexisting marker-based databases with virtual object motions and object-induced\njoint torque estimations. The OAA consists of five phases: (1) computing hand\ncoordinate systems from optical markers, (2) characterising object movements\nwith virtual markers, (3) calculating object motions through inverse kinematics\n(IK), (4) determining the wrench necessary for prescribed object motion using\ninverse dynamics (ID), and (5) computing joint torques resulting from object\nmanipulation. The algorithm's accuracy is validated through trajectory tracking\nand torque analysis on a 5+4 degree of freedom (DoF) robotic hand-arm system,\nmanipulating three unique objects. The results show that the OAA can accurately\nand precisely estimate 6 DoF object motion and object-induced joint torques.\nCorrelations between computed and measured quantities were > 0.99 for object\ntrajectories and > 0.93 for joint torques. The OAA was further shown to be\nrobust to variations in the number and placement of input markers, which are\nexpected between databases. Differences between repeated experiments were minor\nbut significant (p < 0.05). The algorithm expands the scope of available data\nand facilitates more comprehensive analyses of human-object interaction\ndynamics.\n", "link": "http://arxiv.org/abs/2408.07434v3", "date": "2024-11-25", "relevancy": 2.2029, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5606}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5479}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Augmentation%20Algorithm%3A%20Computing%20virtual%20object%20motion%20and%0A%20%20object%20induced%20interaction%20wrench%20from%20optical%20markers&body=Title%3A%20Object%20Augmentation%20Algorithm%3A%20Computing%20virtual%20object%20motion%20and%0A%20%20object%20induced%20interaction%20wrench%20from%20optical%20markers%0AAuthor%3A%20Christopher%20Herneth%20and%20Junnan%20Li%20and%20Muhammad%20Hilman%20Fatoni%20and%20Amartya%20Ganguly%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20critical%20need%20for%20diverse%20and%20comprehensive%20data%0Afocused%20on%20human%20arm%20joint%20torques%20while%20performing%20activities%20of%20daily%20living%0A%28ADL%29.%20Previous%20studies%20have%20often%20overlooked%20the%20influence%20of%20objects%20on%20joint%0Atorques%20during%20ADL%2C%20resulting%20in%20limited%20datasets%20for%20analysis.%20To%20address%20this%0Agap%2C%20we%20propose%20an%20Object%20Augmentation%20Algorithm%20%28OAA%29%20capable%20of%20augmenting%0Aexisting%20marker-based%20databases%20with%20virtual%20object%20motions%20and%20object-induced%0Ajoint%20torque%20estimations.%20The%20OAA%20consists%20of%20five%20phases%3A%20%281%29%20computing%20hand%0Acoordinate%20systems%20from%20optical%20markers%2C%20%282%29%20characterising%20object%20movements%0Awith%20virtual%20markers%2C%20%283%29%20calculating%20object%20motions%20through%20inverse%20kinematics%0A%28IK%29%2C%20%284%29%20determining%20the%20wrench%20necessary%20for%20prescribed%20object%20motion%20using%0Ainverse%20dynamics%20%28ID%29%2C%20and%20%285%29%20computing%20joint%20torques%20resulting%20from%20object%0Amanipulation.%20The%20algorithm%27s%20accuracy%20is%20validated%20through%20trajectory%20tracking%0Aand%20torque%20analysis%20on%20a%205%2B4%20degree%20of%20freedom%20%28DoF%29%20robotic%20hand-arm%20system%2C%0Amanipulating%20three%20unique%20objects.%20The%20results%20show%20that%20the%20OAA%20can%20accurately%0Aand%20precisely%20estimate%206%20DoF%20object%20motion%20and%20object-induced%20joint%20torques.%0ACorrelations%20between%20computed%20and%20measured%20quantities%20were%20%3E%200.99%20for%20object%0Atrajectories%20and%20%3E%200.93%20for%20joint%20torques.%20The%20OAA%20was%20further%20shown%20to%20be%0Arobust%20to%20variations%20in%20the%20number%20and%20placement%20of%20input%20markers%2C%20which%20are%0Aexpected%20between%20databases.%20Differences%20between%20repeated%20experiments%20were%20minor%0Abut%20significant%20%28p%20%3C%200.05%29.%20The%20algorithm%20expands%20the%20scope%20of%20available%20data%0Aand%20facilitates%20more%20comprehensive%20analyses%20of%20human-object%20interaction%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07434v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Augmentation%2520Algorithm%253A%2520Computing%2520virtual%2520object%2520motion%2520and%250A%2520%2520object%2520induced%2520interaction%2520wrench%2520from%2520optical%2520markers%26entry.906535625%3DChristopher%2520Herneth%2520and%2520Junnan%2520Li%2520and%2520Muhammad%2520Hilman%2520Fatoni%2520and%2520Amartya%2520Ganguly%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520critical%2520need%2520for%2520diverse%2520and%2520comprehensive%2520data%250Afocused%2520on%2520human%2520arm%2520joint%2520torques%2520while%2520performing%2520activities%2520of%2520daily%2520living%250A%2528ADL%2529.%2520Previous%2520studies%2520have%2520often%2520overlooked%2520the%2520influence%2520of%2520objects%2520on%2520joint%250Atorques%2520during%2520ADL%252C%2520resulting%2520in%2520limited%2520datasets%2520for%2520analysis.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520an%2520Object%2520Augmentation%2520Algorithm%2520%2528OAA%2529%2520capable%2520of%2520augmenting%250Aexisting%2520marker-based%2520databases%2520with%2520virtual%2520object%2520motions%2520and%2520object-induced%250Ajoint%2520torque%2520estimations.%2520The%2520OAA%2520consists%2520of%2520five%2520phases%253A%2520%25281%2529%2520computing%2520hand%250Acoordinate%2520systems%2520from%2520optical%2520markers%252C%2520%25282%2529%2520characterising%2520object%2520movements%250Awith%2520virtual%2520markers%252C%2520%25283%2529%2520calculating%2520object%2520motions%2520through%2520inverse%2520kinematics%250A%2528IK%2529%252C%2520%25284%2529%2520determining%2520the%2520wrench%2520necessary%2520for%2520prescribed%2520object%2520motion%2520using%250Ainverse%2520dynamics%2520%2528ID%2529%252C%2520and%2520%25285%2529%2520computing%2520joint%2520torques%2520resulting%2520from%2520object%250Amanipulation.%2520The%2520algorithm%2527s%2520accuracy%2520is%2520validated%2520through%2520trajectory%2520tracking%250Aand%2520torque%2520analysis%2520on%2520a%25205%252B4%2520degree%2520of%2520freedom%2520%2528DoF%2529%2520robotic%2520hand-arm%2520system%252C%250Amanipulating%2520three%2520unique%2520objects.%2520The%2520results%2520show%2520that%2520the%2520OAA%2520can%2520accurately%250Aand%2520precisely%2520estimate%25206%2520DoF%2520object%2520motion%2520and%2520object-induced%2520joint%2520torques.%250ACorrelations%2520between%2520computed%2520and%2520measured%2520quantities%2520were%2520%253E%25200.99%2520for%2520object%250Atrajectories%2520and%2520%253E%25200.93%2520for%2520joint%2520torques.%2520The%2520OAA%2520was%2520further%2520shown%2520to%2520be%250Arobust%2520to%2520variations%2520in%2520the%2520number%2520and%2520placement%2520of%2520input%2520markers%252C%2520which%2520are%250Aexpected%2520between%2520databases.%2520Differences%2520between%2520repeated%2520experiments%2520were%2520minor%250Abut%2520significant%2520%2528p%2520%253C%25200.05%2529.%2520The%2520algorithm%2520expands%2520the%2520scope%2520of%2520available%2520data%250Aand%2520facilitates%2520more%2520comprehensive%2520analyses%2520of%2520human-object%2520interaction%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07434v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Augmentation%20Algorithm%3A%20Computing%20virtual%20object%20motion%20and%0A%20%20object%20induced%20interaction%20wrench%20from%20optical%20markers&entry.906535625=Christopher%20Herneth%20and%20Junnan%20Li%20and%20Muhammad%20Hilman%20Fatoni%20and%20Amartya%20Ganguly%20and%20Sami%20Haddadin&entry.1292438233=%20%20This%20study%20addresses%20the%20critical%20need%20for%20diverse%20and%20comprehensive%20data%0Afocused%20on%20human%20arm%20joint%20torques%20while%20performing%20activities%20of%20daily%20living%0A%28ADL%29.%20Previous%20studies%20have%20often%20overlooked%20the%20influence%20of%20objects%20on%20joint%0Atorques%20during%20ADL%2C%20resulting%20in%20limited%20datasets%20for%20analysis.%20To%20address%20this%0Agap%2C%20we%20propose%20an%20Object%20Augmentation%20Algorithm%20%28OAA%29%20capable%20of%20augmenting%0Aexisting%20marker-based%20databases%20with%20virtual%20object%20motions%20and%20object-induced%0Ajoint%20torque%20estimations.%20The%20OAA%20consists%20of%20five%20phases%3A%20%281%29%20computing%20hand%0Acoordinate%20systems%20from%20optical%20markers%2C%20%282%29%20characterising%20object%20movements%0Awith%20virtual%20markers%2C%20%283%29%20calculating%20object%20motions%20through%20inverse%20kinematics%0A%28IK%29%2C%20%284%29%20determining%20the%20wrench%20necessary%20for%20prescribed%20object%20motion%20using%0Ainverse%20dynamics%20%28ID%29%2C%20and%20%285%29%20computing%20joint%20torques%20resulting%20from%20object%0Amanipulation.%20The%20algorithm%27s%20accuracy%20is%20validated%20through%20trajectory%20tracking%0Aand%20torque%20analysis%20on%20a%205%2B4%20degree%20of%20freedom%20%28DoF%29%20robotic%20hand-arm%20system%2C%0Amanipulating%20three%20unique%20objects.%20The%20results%20show%20that%20the%20OAA%20can%20accurately%0Aand%20precisely%20estimate%206%20DoF%20object%20motion%20and%20object-induced%20joint%20torques.%0ACorrelations%20between%20computed%20and%20measured%20quantities%20were%20%3E%200.99%20for%20object%0Atrajectories%20and%20%3E%200.93%20for%20joint%20torques.%20The%20OAA%20was%20further%20shown%20to%20be%0Arobust%20to%20variations%20in%20the%20number%20and%20placement%20of%20input%20markers%2C%20which%20are%0Aexpected%20between%20databases.%20Differences%20between%20repeated%20experiments%20were%20minor%0Abut%20significant%20%28p%20%3C%200.05%29.%20The%20algorithm%20expands%20the%20scope%20of%20available%20data%0Aand%20facilitates%20more%20comprehensive%20analyses%20of%20human-object%20interaction%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07434v3&entry.124074799=Read"},
{"title": "EPS: Efficient Patch Sampling for Video Overfitting in Deep\n  Super-Resolution Model Training", "author": "Yiying Wei and Hadi Amirpour and Jong Hwan Ko and Christian Timmerer", "abstract": "  Leveraging the overfitting property of deep neural networks (DNNs) is\ntrending in video delivery systems to enhance quality within bandwidth limits.\nExisting approaches transmit overfitted super-resolution (SR) model streams for\nlow-resolution (LR) bitstreams, which are used to reconstruct high-resolution\n(HR) videos at the decoder. Although these approaches show promising results,\nthe huge computational costs of training a large number of video frames limit\ntheir practical applications. To overcome this challenge, we propose an\nefficient patch sampling method named EPS for video SR network overfitting,\nwhich identifies the most valuable training patches from video frames. To this\nend, we first present two low-complexity Discrete Cosine Transform (DCT)-based\nspatial-temporal features to measure the complexity score of each patch\ndirectly. By analyzing the histogram distribution of these features, we then\ncategorize all possible patches into different clusters and select training\npatches from the cluster with the highest spatial-temporal information. The\nnumber of sampled patches is adaptive based on the video content, addressing\nthe trade-off between training complexity and efficiency. Our method reduces\nthe number of patches for the training to 4% to 25%, depending on the\nresolution and number of clusters, while maintaining high video quality and\nsignificantly enhancing training efficiency. Compared to the state-of-the-art\npatch sampling method, EMT, our approach achieves an 83% decrease in overall\nrun time.\n", "link": "http://arxiv.org/abs/2411.16312v1", "date": "2024-11-25", "relevancy": 2.1806, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5563}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5476}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EPS%3A%20Efficient%20Patch%20Sampling%20for%20Video%20Overfitting%20in%20Deep%0A%20%20Super-Resolution%20Model%20Training&body=Title%3A%20EPS%3A%20Efficient%20Patch%20Sampling%20for%20Video%20Overfitting%20in%20Deep%0A%20%20Super-Resolution%20Model%20Training%0AAuthor%3A%20Yiying%20Wei%20and%20Hadi%20Amirpour%20and%20Jong%20Hwan%20Ko%20and%20Christian%20Timmerer%0AAbstract%3A%20%20%20Leveraging%20the%20overfitting%20property%20of%20deep%20neural%20networks%20%28DNNs%29%20is%0Atrending%20in%20video%20delivery%20systems%20to%20enhance%20quality%20within%20bandwidth%20limits.%0AExisting%20approaches%20transmit%20overfitted%20super-resolution%20%28SR%29%20model%20streams%20for%0Alow-resolution%20%28LR%29%20bitstreams%2C%20which%20are%20used%20to%20reconstruct%20high-resolution%0A%28HR%29%20videos%20at%20the%20decoder.%20Although%20these%20approaches%20show%20promising%20results%2C%0Athe%20huge%20computational%20costs%20of%20training%20a%20large%20number%20of%20video%20frames%20limit%0Atheir%20practical%20applications.%20To%20overcome%20this%20challenge%2C%20we%20propose%20an%0Aefficient%20patch%20sampling%20method%20named%20EPS%20for%20video%20SR%20network%20overfitting%2C%0Awhich%20identifies%20the%20most%20valuable%20training%20patches%20from%20video%20frames.%20To%20this%0Aend%2C%20we%20first%20present%20two%20low-complexity%20Discrete%20Cosine%20Transform%20%28DCT%29-based%0Aspatial-temporal%20features%20to%20measure%20the%20complexity%20score%20of%20each%20patch%0Adirectly.%20By%20analyzing%20the%20histogram%20distribution%20of%20these%20features%2C%20we%20then%0Acategorize%20all%20possible%20patches%20into%20different%20clusters%20and%20select%20training%0Apatches%20from%20the%20cluster%20with%20the%20highest%20spatial-temporal%20information.%20The%0Anumber%20of%20sampled%20patches%20is%20adaptive%20based%20on%20the%20video%20content%2C%20addressing%0Athe%20trade-off%20between%20training%20complexity%20and%20efficiency.%20Our%20method%20reduces%0Athe%20number%20of%20patches%20for%20the%20training%20to%204%25%20to%2025%25%2C%20depending%20on%20the%0Aresolution%20and%20number%20of%20clusters%2C%20while%20maintaining%20high%20video%20quality%20and%0Asignificantly%20enhancing%20training%20efficiency.%20Compared%20to%20the%20state-of-the-art%0Apatch%20sampling%20method%2C%20EMT%2C%20our%20approach%20achieves%20an%2083%25%20decrease%20in%20overall%0Arun%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEPS%253A%2520Efficient%2520Patch%2520Sampling%2520for%2520Video%2520Overfitting%2520in%2520Deep%250A%2520%2520Super-Resolution%2520Model%2520Training%26entry.906535625%3DYiying%2520Wei%2520and%2520Hadi%2520Amirpour%2520and%2520Jong%2520Hwan%2520Ko%2520and%2520Christian%2520Timmerer%26entry.1292438233%3D%2520%2520Leveraging%2520the%2520overfitting%2520property%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520is%250Atrending%2520in%2520video%2520delivery%2520systems%2520to%2520enhance%2520quality%2520within%2520bandwidth%2520limits.%250AExisting%2520approaches%2520transmit%2520overfitted%2520super-resolution%2520%2528SR%2529%2520model%2520streams%2520for%250Alow-resolution%2520%2528LR%2529%2520bitstreams%252C%2520which%2520are%2520used%2520to%2520reconstruct%2520high-resolution%250A%2528HR%2529%2520videos%2520at%2520the%2520decoder.%2520Although%2520these%2520approaches%2520show%2520promising%2520results%252C%250Athe%2520huge%2520computational%2520costs%2520of%2520training%2520a%2520large%2520number%2520of%2520video%2520frames%2520limit%250Atheir%2520practical%2520applications.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520an%250Aefficient%2520patch%2520sampling%2520method%2520named%2520EPS%2520for%2520video%2520SR%2520network%2520overfitting%252C%250Awhich%2520identifies%2520the%2520most%2520valuable%2520training%2520patches%2520from%2520video%2520frames.%2520To%2520this%250Aend%252C%2520we%2520first%2520present%2520two%2520low-complexity%2520Discrete%2520Cosine%2520Transform%2520%2528DCT%2529-based%250Aspatial-temporal%2520features%2520to%2520measure%2520the%2520complexity%2520score%2520of%2520each%2520patch%250Adirectly.%2520By%2520analyzing%2520the%2520histogram%2520distribution%2520of%2520these%2520features%252C%2520we%2520then%250Acategorize%2520all%2520possible%2520patches%2520into%2520different%2520clusters%2520and%2520select%2520training%250Apatches%2520from%2520the%2520cluster%2520with%2520the%2520highest%2520spatial-temporal%2520information.%2520The%250Anumber%2520of%2520sampled%2520patches%2520is%2520adaptive%2520based%2520on%2520the%2520video%2520content%252C%2520addressing%250Athe%2520trade-off%2520between%2520training%2520complexity%2520and%2520efficiency.%2520Our%2520method%2520reduces%250Athe%2520number%2520of%2520patches%2520for%2520the%2520training%2520to%25204%2525%2520to%252025%2525%252C%2520depending%2520on%2520the%250Aresolution%2520and%2520number%2520of%2520clusters%252C%2520while%2520maintaining%2520high%2520video%2520quality%2520and%250Asignificantly%2520enhancing%2520training%2520efficiency.%2520Compared%2520to%2520the%2520state-of-the-art%250Apatch%2520sampling%2520method%252C%2520EMT%252C%2520our%2520approach%2520achieves%2520an%252083%2525%2520decrease%2520in%2520overall%250Arun%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPS%3A%20Efficient%20Patch%20Sampling%20for%20Video%20Overfitting%20in%20Deep%0A%20%20Super-Resolution%20Model%20Training&entry.906535625=Yiying%20Wei%20and%20Hadi%20Amirpour%20and%20Jong%20Hwan%20Ko%20and%20Christian%20Timmerer&entry.1292438233=%20%20Leveraging%20the%20overfitting%20property%20of%20deep%20neural%20networks%20%28DNNs%29%20is%0Atrending%20in%20video%20delivery%20systems%20to%20enhance%20quality%20within%20bandwidth%20limits.%0AExisting%20approaches%20transmit%20overfitted%20super-resolution%20%28SR%29%20model%20streams%20for%0Alow-resolution%20%28LR%29%20bitstreams%2C%20which%20are%20used%20to%20reconstruct%20high-resolution%0A%28HR%29%20videos%20at%20the%20decoder.%20Although%20these%20approaches%20show%20promising%20results%2C%0Athe%20huge%20computational%20costs%20of%20training%20a%20large%20number%20of%20video%20frames%20limit%0Atheir%20practical%20applications.%20To%20overcome%20this%20challenge%2C%20we%20propose%20an%0Aefficient%20patch%20sampling%20method%20named%20EPS%20for%20video%20SR%20network%20overfitting%2C%0Awhich%20identifies%20the%20most%20valuable%20training%20patches%20from%20video%20frames.%20To%20this%0Aend%2C%20we%20first%20present%20two%20low-complexity%20Discrete%20Cosine%20Transform%20%28DCT%29-based%0Aspatial-temporal%20features%20to%20measure%20the%20complexity%20score%20of%20each%20patch%0Adirectly.%20By%20analyzing%20the%20histogram%20distribution%20of%20these%20features%2C%20we%20then%0Acategorize%20all%20possible%20patches%20into%20different%20clusters%20and%20select%20training%0Apatches%20from%20the%20cluster%20with%20the%20highest%20spatial-temporal%20information.%20The%0Anumber%20of%20sampled%20patches%20is%20adaptive%20based%20on%20the%20video%20content%2C%20addressing%0Athe%20trade-off%20between%20training%20complexity%20and%20efficiency.%20Our%20method%20reduces%0Athe%20number%20of%20patches%20for%20the%20training%20to%204%25%20to%2025%25%2C%20depending%20on%20the%0Aresolution%20and%20number%20of%20clusters%2C%20while%20maintaining%20high%20video%20quality%20and%0Asignificantly%20enhancing%20training%20efficiency.%20Compared%20to%20the%20state-of-the-art%0Apatch%20sampling%20method%2C%20EMT%2C%20our%20approach%20achieves%20an%2083%25%20decrease%20in%20overall%0Arun%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16312v1&entry.124074799=Read"},
{"title": "Exploring Discrete Flow Matching for 3D De Novo Molecule Generation", "author": "Ian Dunn and David R. Koes", "abstract": "  Deep generative models that produce novel molecular structures have the\npotential to facilitate chemical discovery. Flow matching is a recently\nproposed generative modeling framework that has achieved impressive performance\non a variety of tasks including those on biomolecular structures. The seminal\nflow matching framework was developed only for continuous data. However, de\nnovo molecular design tasks require generating discrete data such as atomic\nelements or sequences of amino acid residues. Several discrete flow matching\nmethods have been proposed recently to address this gap. In this work we\nbenchmark the performance of existing discrete flow matching methods for 3D de\nnovo small molecule generation and provide explanations of their differing\nbehavior. As a result we present FlowMol-CTMC, an open-source model that\nachieves state of the art performance for 3D de novo design with fewer\nlearnable parameters than existing methods. Additionally, we propose the use of\nmetrics that capture molecule quality beyond local chemical valency constraints\nand towards higher-order structural motifs. These metrics show that even though\nbasic constraints are satisfied, the models tend to produce unusual and\npotentially problematic functional groups outside of the training data\ndistribution. Code and trained models for reproducing this work are available\nat \\url{https://github.com/dunni3/FlowMol}.\n", "link": "http://arxiv.org/abs/2411.16644v1", "date": "2024-11-25", "relevancy": 2.1782, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5474}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.544}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Discrete%20Flow%20Matching%20for%203D%20De%20Novo%20Molecule%20Generation&body=Title%3A%20Exploring%20Discrete%20Flow%20Matching%20for%203D%20De%20Novo%20Molecule%20Generation%0AAuthor%3A%20Ian%20Dunn%20and%20David%20R.%20Koes%0AAbstract%3A%20%20%20Deep%20generative%20models%20that%20produce%20novel%20molecular%20structures%20have%20the%0Apotential%20to%20facilitate%20chemical%20discovery.%20Flow%20matching%20is%20a%20recently%0Aproposed%20generative%20modeling%20framework%20that%20has%20achieved%20impressive%20performance%0Aon%20a%20variety%20of%20tasks%20including%20those%20on%20biomolecular%20structures.%20The%20seminal%0Aflow%20matching%20framework%20was%20developed%20only%20for%20continuous%20data.%20However%2C%20de%0Anovo%20molecular%20design%20tasks%20require%20generating%20discrete%20data%20such%20as%20atomic%0Aelements%20or%20sequences%20of%20amino%20acid%20residues.%20Several%20discrete%20flow%20matching%0Amethods%20have%20been%20proposed%20recently%20to%20address%20this%20gap.%20In%20this%20work%20we%0Abenchmark%20the%20performance%20of%20existing%20discrete%20flow%20matching%20methods%20for%203D%20de%0Anovo%20small%20molecule%20generation%20and%20provide%20explanations%20of%20their%20differing%0Abehavior.%20As%20a%20result%20we%20present%20FlowMol-CTMC%2C%20an%20open-source%20model%20that%0Aachieves%20state%20of%20the%20art%20performance%20for%203D%20de%20novo%20design%20with%20fewer%0Alearnable%20parameters%20than%20existing%20methods.%20Additionally%2C%20we%20propose%20the%20use%20of%0Ametrics%20that%20capture%20molecule%20quality%20beyond%20local%20chemical%20valency%20constraints%0Aand%20towards%20higher-order%20structural%20motifs.%20These%20metrics%20show%20that%20even%20though%0Abasic%20constraints%20are%20satisfied%2C%20the%20models%20tend%20to%20produce%20unusual%20and%0Apotentially%20problematic%20functional%20groups%20outside%20of%20the%20training%20data%0Adistribution.%20Code%20and%20trained%20models%20for%20reproducing%20this%20work%20are%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/dunni3/FlowMol%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Discrete%2520Flow%2520Matching%2520for%25203D%2520De%2520Novo%2520Molecule%2520Generation%26entry.906535625%3DIan%2520Dunn%2520and%2520David%2520R.%2520Koes%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520that%2520produce%2520novel%2520molecular%2520structures%2520have%2520the%250Apotential%2520to%2520facilitate%2520chemical%2520discovery.%2520Flow%2520matching%2520is%2520a%2520recently%250Aproposed%2520generative%2520modeling%2520framework%2520that%2520has%2520achieved%2520impressive%2520performance%250Aon%2520a%2520variety%2520of%2520tasks%2520including%2520those%2520on%2520biomolecular%2520structures.%2520The%2520seminal%250Aflow%2520matching%2520framework%2520was%2520developed%2520only%2520for%2520continuous%2520data.%2520However%252C%2520de%250Anovo%2520molecular%2520design%2520tasks%2520require%2520generating%2520discrete%2520data%2520such%2520as%2520atomic%250Aelements%2520or%2520sequences%2520of%2520amino%2520acid%2520residues.%2520Several%2520discrete%2520flow%2520matching%250Amethods%2520have%2520been%2520proposed%2520recently%2520to%2520address%2520this%2520gap.%2520In%2520this%2520work%2520we%250Abenchmark%2520the%2520performance%2520of%2520existing%2520discrete%2520flow%2520matching%2520methods%2520for%25203D%2520de%250Anovo%2520small%2520molecule%2520generation%2520and%2520provide%2520explanations%2520of%2520their%2520differing%250Abehavior.%2520As%2520a%2520result%2520we%2520present%2520FlowMol-CTMC%252C%2520an%2520open-source%2520model%2520that%250Aachieves%2520state%2520of%2520the%2520art%2520performance%2520for%25203D%2520de%2520novo%2520design%2520with%2520fewer%250Alearnable%2520parameters%2520than%2520existing%2520methods.%2520Additionally%252C%2520we%2520propose%2520the%2520use%2520of%250Ametrics%2520that%2520capture%2520molecule%2520quality%2520beyond%2520local%2520chemical%2520valency%2520constraints%250Aand%2520towards%2520higher-order%2520structural%2520motifs.%2520These%2520metrics%2520show%2520that%2520even%2520though%250Abasic%2520constraints%2520are%2520satisfied%252C%2520the%2520models%2520tend%2520to%2520produce%2520unusual%2520and%250Apotentially%2520problematic%2520functional%2520groups%2520outside%2520of%2520the%2520training%2520data%250Adistribution.%2520Code%2520and%2520trained%2520models%2520for%2520reproducing%2520this%2520work%2520are%2520available%250Aat%2520%255Curl%257Bhttps%253A//github.com/dunni3/FlowMol%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Discrete%20Flow%20Matching%20for%203D%20De%20Novo%20Molecule%20Generation&entry.906535625=Ian%20Dunn%20and%20David%20R.%20Koes&entry.1292438233=%20%20Deep%20generative%20models%20that%20produce%20novel%20molecular%20structures%20have%20the%0Apotential%20to%20facilitate%20chemical%20discovery.%20Flow%20matching%20is%20a%20recently%0Aproposed%20generative%20modeling%20framework%20that%20has%20achieved%20impressive%20performance%0Aon%20a%20variety%20of%20tasks%20including%20those%20on%20biomolecular%20structures.%20The%20seminal%0Aflow%20matching%20framework%20was%20developed%20only%20for%20continuous%20data.%20However%2C%20de%0Anovo%20molecular%20design%20tasks%20require%20generating%20discrete%20data%20such%20as%20atomic%0Aelements%20or%20sequences%20of%20amino%20acid%20residues.%20Several%20discrete%20flow%20matching%0Amethods%20have%20been%20proposed%20recently%20to%20address%20this%20gap.%20In%20this%20work%20we%0Abenchmark%20the%20performance%20of%20existing%20discrete%20flow%20matching%20methods%20for%203D%20de%0Anovo%20small%20molecule%20generation%20and%20provide%20explanations%20of%20their%20differing%0Abehavior.%20As%20a%20result%20we%20present%20FlowMol-CTMC%2C%20an%20open-source%20model%20that%0Aachieves%20state%20of%20the%20art%20performance%20for%203D%20de%20novo%20design%20with%20fewer%0Alearnable%20parameters%20than%20existing%20methods.%20Additionally%2C%20we%20propose%20the%20use%20of%0Ametrics%20that%20capture%20molecule%20quality%20beyond%20local%20chemical%20valency%20constraints%0Aand%20towards%20higher-order%20structural%20motifs.%20These%20metrics%20show%20that%20even%20though%0Abasic%20constraints%20are%20satisfied%2C%20the%20models%20tend%20to%20produce%20unusual%20and%0Apotentially%20problematic%20functional%20groups%20outside%20of%20the%20training%20data%0Adistribution.%20Code%20and%20trained%20models%20for%20reproducing%20this%20work%20are%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/dunni3/FlowMol%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16644v1&entry.124074799=Read"},
{"title": "Graph Pooling with Local Cluster Selection", "author": "Yizhu Chen", "abstract": "  Graph poolings in GNNs are a family of operations which take graphs as inputs\nand produce coarsened graphs as output. Modern graph poolings are trainable and\nclosely related to GNNs, which learn to pool graphs under different\nassumptions. Though there are various assumptions, the procedure of generating\npooled graphs is relatively similar and limited. This work formalizes a novel\nprocedure of pooling graphs, along with a graph pooling approach for average\nsituations.\n", "link": "http://arxiv.org/abs/2411.16615v1", "date": "2024-11-25", "relevancy": 2.1746, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4372}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4359}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Pooling%20with%20Local%20Cluster%20Selection&body=Title%3A%20Graph%20Pooling%20with%20Local%20Cluster%20Selection%0AAuthor%3A%20Yizhu%20Chen%0AAbstract%3A%20%20%20Graph%20poolings%20in%20GNNs%20are%20a%20family%20of%20operations%20which%20take%20graphs%20as%20inputs%0Aand%20produce%20coarsened%20graphs%20as%20output.%20Modern%20graph%20poolings%20are%20trainable%20and%0Aclosely%20related%20to%20GNNs%2C%20which%20learn%20to%20pool%20graphs%20under%20different%0Aassumptions.%20Though%20there%20are%20various%20assumptions%2C%20the%20procedure%20of%20generating%0Apooled%20graphs%20is%20relatively%20similar%20and%20limited.%20This%20work%20formalizes%20a%20novel%0Aprocedure%20of%20pooling%20graphs%2C%20along%20with%20a%20graph%20pooling%20approach%20for%20average%0Asituations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Pooling%2520with%2520Local%2520Cluster%2520Selection%26entry.906535625%3DYizhu%2520Chen%26entry.1292438233%3D%2520%2520Graph%2520poolings%2520in%2520GNNs%2520are%2520a%2520family%2520of%2520operations%2520which%2520take%2520graphs%2520as%2520inputs%250Aand%2520produce%2520coarsened%2520graphs%2520as%2520output.%2520Modern%2520graph%2520poolings%2520are%2520trainable%2520and%250Aclosely%2520related%2520to%2520GNNs%252C%2520which%2520learn%2520to%2520pool%2520graphs%2520under%2520different%250Aassumptions.%2520Though%2520there%2520are%2520various%2520assumptions%252C%2520the%2520procedure%2520of%2520generating%250Apooled%2520graphs%2520is%2520relatively%2520similar%2520and%2520limited.%2520This%2520work%2520formalizes%2520a%2520novel%250Aprocedure%2520of%2520pooling%2520graphs%252C%2520along%2520with%2520a%2520graph%2520pooling%2520approach%2520for%2520average%250Asituations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Pooling%20with%20Local%20Cluster%20Selection&entry.906535625=Yizhu%20Chen&entry.1292438233=%20%20Graph%20poolings%20in%20GNNs%20are%20a%20family%20of%20operations%20which%20take%20graphs%20as%20inputs%0Aand%20produce%20coarsened%20graphs%20as%20output.%20Modern%20graph%20poolings%20are%20trainable%20and%0Aclosely%20related%20to%20GNNs%2C%20which%20learn%20to%20pool%20graphs%20under%20different%0Aassumptions.%20Though%20there%20are%20various%20assumptions%2C%20the%20procedure%20of%20generating%0Apooled%20graphs%20is%20relatively%20similar%20and%20limited.%20This%20work%20formalizes%20a%20novel%0Aprocedure%20of%20pooling%20graphs%2C%20along%20with%20a%20graph%20pooling%20approach%20for%20average%0Asituations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16615v1&entry.124074799=Read"},
{"title": "Intelligent Anomaly Detection for Lane Rendering Using Transformer with\n  Self-Supervised Pre-Training and Customized Fine-Tuning", "author": "Yongqi Dong and Xingmin Lu and Ruohan Li and Wei Song and Bart van Arem and Haneen Farah", "abstract": "  The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.\n", "link": "http://arxiv.org/abs/2312.04398v3", "date": "2024-11-25", "relevancy": 2.1638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5462}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5386}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Anomaly%20Detection%20for%20Lane%20Rendering%20Using%20Transformer%20with%0A%20%20Self-Supervised%20Pre-Training%20and%20Customized%20Fine-Tuning&body=Title%3A%20Intelligent%20Anomaly%20Detection%20for%20Lane%20Rendering%20Using%20Transformer%20with%0A%20%20Self-Supervised%20Pre-Training%20and%20Customized%20Fine-Tuning%0AAuthor%3A%20Yongqi%20Dong%20and%20Xingmin%20Lu%20and%20Ruohan%20Li%20and%20Wei%20Song%20and%20Bart%20van%20Arem%20and%20Haneen%20Farah%0AAbstract%3A%20%20%20The%20burgeoning%20navigation%20services%20using%20digital%20maps%20provide%20great%0Aconvenience%20to%20drivers.%20Nevertheless%2C%20the%20presence%20of%20anomalies%20in%20lane%0Arendering%20map%20images%20occasionally%20introduces%20potential%20hazards%2C%20as%20such%0Aanomalies%20can%20be%20misleading%20to%20human%20drivers%20and%20consequently%20contribute%20to%0Aunsafe%20driving%20conditions.%20In%20response%20to%20this%20concern%20and%20to%20accurately%20and%0Aeffectively%20detect%20the%20anomalies%2C%20this%20paper%20transforms%20lane%20rendering%20image%0Aanomaly%20detection%20into%20a%20classification%20problem%20and%20proposes%20a%20four-phase%0Apipeline%20consisting%20of%20data%20pre-processing%2C%20self-supervised%20pre-training%20with%0Athe%20masked%20image%20modeling%20%28MiM%29%20method%2C%20customized%20fine-tuning%20using%0Across-entropy%20based%20loss%20with%20label%20smoothing%2C%20and%20post-processing%20to%20tackle%20it%0Aleveraging%20state-of-the-art%20deep%20learning%20techniques%2C%20especially%20those%0Ainvolving%20Transformer%20models.%20Various%20experiments%20verify%20the%20effectiveness%20of%0Athe%20proposed%20pipeline.%20Results%20indicate%20that%20the%20proposed%20pipeline%20exhibits%0Asuperior%20performance%20in%20lane%20rendering%20image%20anomaly%20detection%2C%20and%20notably%2C%0Athe%20self-supervised%20pre-training%20with%20MiM%20can%20greatly%20enhance%20the%20detection%0Aaccuracy%20while%20significantly%20reducing%20the%20total%20training%20time.%20For%20instance%2C%0Aemploying%20the%20Swin%20Transformer%20with%20Uniform%20Masking%20as%20self-supervised%0Apretraining%20%28Swin-Trans-UM%29%20yielded%20a%20heightened%20accuracy%20at%2094.77%25%20and%20an%0Aimproved%20Area%20Under%20The%20Curve%20%28AUC%29%20score%20of%200.9743%20compared%20with%20the%20pure%20Swin%0ATransformer%20without%20pre-training%20%28Swin-Trans%29%20with%20an%20accuracy%20of%2094.01%25%20and%20an%0AAUC%20of%200.9498.%20The%20fine-tuning%20epochs%20were%20dramatically%20reduced%20to%2041%20from%20the%0Aoriginal%20280.%20In%20conclusion%2C%20the%20proposed%20pipeline%2C%20with%20its%20incorporation%20of%0Aself-supervised%20pre-training%20using%20MiM%20and%20other%20advanced%20deep%20learning%0Atechniques%2C%20emerges%20as%20a%20robust%20solution%20for%20enhancing%20the%20accuracy%20and%0Aefficiency%20of%20lane%20rendering%20image%20anomaly%20detection%20in%20digital%20navigation%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04398v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Anomaly%2520Detection%2520for%2520Lane%2520Rendering%2520Using%2520Transformer%2520with%250A%2520%2520Self-Supervised%2520Pre-Training%2520and%2520Customized%2520Fine-Tuning%26entry.906535625%3DYongqi%2520Dong%2520and%2520Xingmin%2520Lu%2520and%2520Ruohan%2520Li%2520and%2520Wei%2520Song%2520and%2520Bart%2520van%2520Arem%2520and%2520Haneen%2520Farah%26entry.1292438233%3D%2520%2520The%2520burgeoning%2520navigation%2520services%2520using%2520digital%2520maps%2520provide%2520great%250Aconvenience%2520to%2520drivers.%2520Nevertheless%252C%2520the%2520presence%2520of%2520anomalies%2520in%2520lane%250Arendering%2520map%2520images%2520occasionally%2520introduces%2520potential%2520hazards%252C%2520as%2520such%250Aanomalies%2520can%2520be%2520misleading%2520to%2520human%2520drivers%2520and%2520consequently%2520contribute%2520to%250Aunsafe%2520driving%2520conditions.%2520In%2520response%2520to%2520this%2520concern%2520and%2520to%2520accurately%2520and%250Aeffectively%2520detect%2520the%2520anomalies%252C%2520this%2520paper%2520transforms%2520lane%2520rendering%2520image%250Aanomaly%2520detection%2520into%2520a%2520classification%2520problem%2520and%2520proposes%2520a%2520four-phase%250Apipeline%2520consisting%2520of%2520data%2520pre-processing%252C%2520self-supervised%2520pre-training%2520with%250Athe%2520masked%2520image%2520modeling%2520%2528MiM%2529%2520method%252C%2520customized%2520fine-tuning%2520using%250Across-entropy%2520based%2520loss%2520with%2520label%2520smoothing%252C%2520and%2520post-processing%2520to%2520tackle%2520it%250Aleveraging%2520state-of-the-art%2520deep%2520learning%2520techniques%252C%2520especially%2520those%250Ainvolving%2520Transformer%2520models.%2520Various%2520experiments%2520verify%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520pipeline.%2520Results%2520indicate%2520that%2520the%2520proposed%2520pipeline%2520exhibits%250Asuperior%2520performance%2520in%2520lane%2520rendering%2520image%2520anomaly%2520detection%252C%2520and%2520notably%252C%250Athe%2520self-supervised%2520pre-training%2520with%2520MiM%2520can%2520greatly%2520enhance%2520the%2520detection%250Aaccuracy%2520while%2520significantly%2520reducing%2520the%2520total%2520training%2520time.%2520For%2520instance%252C%250Aemploying%2520the%2520Swin%2520Transformer%2520with%2520Uniform%2520Masking%2520as%2520self-supervised%250Apretraining%2520%2528Swin-Trans-UM%2529%2520yielded%2520a%2520heightened%2520accuracy%2520at%252094.77%2525%2520and%2520an%250Aimproved%2520Area%2520Under%2520The%2520Curve%2520%2528AUC%2529%2520score%2520of%25200.9743%2520compared%2520with%2520the%2520pure%2520Swin%250ATransformer%2520without%2520pre-training%2520%2528Swin-Trans%2529%2520with%2520an%2520accuracy%2520of%252094.01%2525%2520and%2520an%250AAUC%2520of%25200.9498.%2520The%2520fine-tuning%2520epochs%2520were%2520dramatically%2520reduced%2520to%252041%2520from%2520the%250Aoriginal%2520280.%2520In%2520conclusion%252C%2520the%2520proposed%2520pipeline%252C%2520with%2520its%2520incorporation%2520of%250Aself-supervised%2520pre-training%2520using%2520MiM%2520and%2520other%2520advanced%2520deep%2520learning%250Atechniques%252C%2520emerges%2520as%2520a%2520robust%2520solution%2520for%2520enhancing%2520the%2520accuracy%2520and%250Aefficiency%2520of%2520lane%2520rendering%2520image%2520anomaly%2520detection%2520in%2520digital%2520navigation%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04398v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Anomaly%20Detection%20for%20Lane%20Rendering%20Using%20Transformer%20with%0A%20%20Self-Supervised%20Pre-Training%20and%20Customized%20Fine-Tuning&entry.906535625=Yongqi%20Dong%20and%20Xingmin%20Lu%20and%20Ruohan%20Li%20and%20Wei%20Song%20and%20Bart%20van%20Arem%20and%20Haneen%20Farah&entry.1292438233=%20%20The%20burgeoning%20navigation%20services%20using%20digital%20maps%20provide%20great%0Aconvenience%20to%20drivers.%20Nevertheless%2C%20the%20presence%20of%20anomalies%20in%20lane%0Arendering%20map%20images%20occasionally%20introduces%20potential%20hazards%2C%20as%20such%0Aanomalies%20can%20be%20misleading%20to%20human%20drivers%20and%20consequently%20contribute%20to%0Aunsafe%20driving%20conditions.%20In%20response%20to%20this%20concern%20and%20to%20accurately%20and%0Aeffectively%20detect%20the%20anomalies%2C%20this%20paper%20transforms%20lane%20rendering%20image%0Aanomaly%20detection%20into%20a%20classification%20problem%20and%20proposes%20a%20four-phase%0Apipeline%20consisting%20of%20data%20pre-processing%2C%20self-supervised%20pre-training%20with%0Athe%20masked%20image%20modeling%20%28MiM%29%20method%2C%20customized%20fine-tuning%20using%0Across-entropy%20based%20loss%20with%20label%20smoothing%2C%20and%20post-processing%20to%20tackle%20it%0Aleveraging%20state-of-the-art%20deep%20learning%20techniques%2C%20especially%20those%0Ainvolving%20Transformer%20models.%20Various%20experiments%20verify%20the%20effectiveness%20of%0Athe%20proposed%20pipeline.%20Results%20indicate%20that%20the%20proposed%20pipeline%20exhibits%0Asuperior%20performance%20in%20lane%20rendering%20image%20anomaly%20detection%2C%20and%20notably%2C%0Athe%20self-supervised%20pre-training%20with%20MiM%20can%20greatly%20enhance%20the%20detection%0Aaccuracy%20while%20significantly%20reducing%20the%20total%20training%20time.%20For%20instance%2C%0Aemploying%20the%20Swin%20Transformer%20with%20Uniform%20Masking%20as%20self-supervised%0Apretraining%20%28Swin-Trans-UM%29%20yielded%20a%20heightened%20accuracy%20at%2094.77%25%20and%20an%0Aimproved%20Area%20Under%20The%20Curve%20%28AUC%29%20score%20of%200.9743%20compared%20with%20the%20pure%20Swin%0ATransformer%20without%20pre-training%20%28Swin-Trans%29%20with%20an%20accuracy%20of%2094.01%25%20and%20an%0AAUC%20of%200.9498.%20The%20fine-tuning%20epochs%20were%20dramatically%20reduced%20to%2041%20from%20the%0Aoriginal%20280.%20In%20conclusion%2C%20the%20proposed%20pipeline%2C%20with%20its%20incorporation%20of%0Aself-supervised%20pre-training%20using%20MiM%20and%20other%20advanced%20deep%20learning%0Atechniques%2C%20emerges%20as%20a%20robust%20solution%20for%20enhancing%20the%20accuracy%20and%0Aefficiency%20of%20lane%20rendering%20image%20anomaly%20detection%20in%20digital%20navigation%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04398v3&entry.124074799=Read"},
{"title": "FakeFormer: Efficient Vulnerability-Driven Transformers for\n  Generalisable Deepfake Detection", "author": "Dat Nguyen and Marcella Astrid and Enjie Ghorbel and Djamila Aouada", "abstract": "  Recently, Vision Transformers (ViTs) have achieved unprecedented\neffectiveness in the general domain of image classification. Nonetheless, these\nmodels remain underexplored in the field of deepfake detection, given their\nlower performance as compared to Convolution Neural Networks (CNNs) in that\nspecific context. In this paper, we start by investigating why plain ViT\narchitectures exhibit a suboptimal performance when dealing with the detection\nof facial forgeries. Our analysis reveals that, as compared to CNNs, ViT\nstruggles to model localized forgery artifacts that typically characterize\ndeepfakes. Based on this observation, we propose a deepfake detection framework\ncalled FakeFormer, which extends ViTs to enforce the extraction of subtle\ninconsistency-prone information. For that purpose, an explicit attention\nlearning guided by artifact-vulnerable patches and tailored to ViTs is\nintroduced. Extensive experiments are conducted on diverse well-known datasets,\nincluding FF++, Celeb-DF, WildDeepfake, DFD, DFDCP, and DFDC. The results show\nthat FakeFormer outperforms the state-of-the-art in terms of generalization and\ncomputational cost, without the need for large-scale training datasets. The\ncode is available at \\url{https://github.com/10Ring/FakeFormer}.\n", "link": "http://arxiv.org/abs/2410.21964v2", "date": "2024-11-25", "relevancy": 2.1535, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5492}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5318}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakeFormer%3A%20Efficient%20Vulnerability-Driven%20Transformers%20for%0A%20%20Generalisable%20Deepfake%20Detection&body=Title%3A%20FakeFormer%3A%20Efficient%20Vulnerability-Driven%20Transformers%20for%0A%20%20Generalisable%20Deepfake%20Detection%0AAuthor%3A%20Dat%20Nguyen%20and%20Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Recently%2C%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20unprecedented%0Aeffectiveness%20in%20the%20general%20domain%20of%20image%20classification.%20Nonetheless%2C%20these%0Amodels%20remain%20underexplored%20in%20the%20field%20of%20deepfake%20detection%2C%20given%20their%0Alower%20performance%20as%20compared%20to%20Convolution%20Neural%20Networks%20%28CNNs%29%20in%20that%0Aspecific%20context.%20In%20this%20paper%2C%20we%20start%20by%20investigating%20why%20plain%20ViT%0Aarchitectures%20exhibit%20a%20suboptimal%20performance%20when%20dealing%20with%20the%20detection%0Aof%20facial%20forgeries.%20Our%20analysis%20reveals%20that%2C%20as%20compared%20to%20CNNs%2C%20ViT%0Astruggles%20to%20model%20localized%20forgery%20artifacts%20that%20typically%20characterize%0Adeepfakes.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20deepfake%20detection%20framework%0Acalled%20FakeFormer%2C%20which%20extends%20ViTs%20to%20enforce%20the%20extraction%20of%20subtle%0Ainconsistency-prone%20information.%20For%20that%20purpose%2C%20an%20explicit%20attention%0Alearning%20guided%20by%20artifact-vulnerable%20patches%20and%20tailored%20to%20ViTs%20is%0Aintroduced.%20Extensive%20experiments%20are%20conducted%20on%20diverse%20well-known%20datasets%2C%0Aincluding%20FF%2B%2B%2C%20Celeb-DF%2C%20WildDeepfake%2C%20DFD%2C%20DFDCP%2C%20and%20DFDC.%20The%20results%20show%0Athat%20FakeFormer%20outperforms%20the%20state-of-the-art%20in%20terms%20of%20generalization%20and%0Acomputational%20cost%2C%20without%20the%20need%20for%20large-scale%20training%20datasets.%20The%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/10Ring/FakeFormer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21964v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakeFormer%253A%2520Efficient%2520Vulnerability-Driven%2520Transformers%2520for%250A%2520%2520Generalisable%2520Deepfake%2520Detection%26entry.906535625%3DDat%2520Nguyen%2520and%2520Marcella%2520Astrid%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Recently%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520achieved%2520unprecedented%250Aeffectiveness%2520in%2520the%2520general%2520domain%2520of%2520image%2520classification.%2520Nonetheless%252C%2520these%250Amodels%2520remain%2520underexplored%2520in%2520the%2520field%2520of%2520deepfake%2520detection%252C%2520given%2520their%250Alower%2520performance%2520as%2520compared%2520to%2520Convolution%2520Neural%2520Networks%2520%2528CNNs%2529%2520in%2520that%250Aspecific%2520context.%2520In%2520this%2520paper%252C%2520we%2520start%2520by%2520investigating%2520why%2520plain%2520ViT%250Aarchitectures%2520exhibit%2520a%2520suboptimal%2520performance%2520when%2520dealing%2520with%2520the%2520detection%250Aof%2520facial%2520forgeries.%2520Our%2520analysis%2520reveals%2520that%252C%2520as%2520compared%2520to%2520CNNs%252C%2520ViT%250Astruggles%2520to%2520model%2520localized%2520forgery%2520artifacts%2520that%2520typically%2520characterize%250Adeepfakes.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520a%2520deepfake%2520detection%2520framework%250Acalled%2520FakeFormer%252C%2520which%2520extends%2520ViTs%2520to%2520enforce%2520the%2520extraction%2520of%2520subtle%250Ainconsistency-prone%2520information.%2520For%2520that%2520purpose%252C%2520an%2520explicit%2520attention%250Alearning%2520guided%2520by%2520artifact-vulnerable%2520patches%2520and%2520tailored%2520to%2520ViTs%2520is%250Aintroduced.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520diverse%2520well-known%2520datasets%252C%250Aincluding%2520FF%252B%252B%252C%2520Celeb-DF%252C%2520WildDeepfake%252C%2520DFD%252C%2520DFDCP%252C%2520and%2520DFDC.%2520The%2520results%2520show%250Athat%2520FakeFormer%2520outperforms%2520the%2520state-of-the-art%2520in%2520terms%2520of%2520generalization%2520and%250Acomputational%2520cost%252C%2520without%2520the%2520need%2520for%2520large-scale%2520training%2520datasets.%2520The%250Acode%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/10Ring/FakeFormer%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21964v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakeFormer%3A%20Efficient%20Vulnerability-Driven%20Transformers%20for%0A%20%20Generalisable%20Deepfake%20Detection&entry.906535625=Dat%20Nguyen%20and%20Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20Recently%2C%20Vision%20Transformers%20%28ViTs%29%20have%20achieved%20unprecedented%0Aeffectiveness%20in%20the%20general%20domain%20of%20image%20classification.%20Nonetheless%2C%20these%0Amodels%20remain%20underexplored%20in%20the%20field%20of%20deepfake%20detection%2C%20given%20their%0Alower%20performance%20as%20compared%20to%20Convolution%20Neural%20Networks%20%28CNNs%29%20in%20that%0Aspecific%20context.%20In%20this%20paper%2C%20we%20start%20by%20investigating%20why%20plain%20ViT%0Aarchitectures%20exhibit%20a%20suboptimal%20performance%20when%20dealing%20with%20the%20detection%0Aof%20facial%20forgeries.%20Our%20analysis%20reveals%20that%2C%20as%20compared%20to%20CNNs%2C%20ViT%0Astruggles%20to%20model%20localized%20forgery%20artifacts%20that%20typically%20characterize%0Adeepfakes.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20deepfake%20detection%20framework%0Acalled%20FakeFormer%2C%20which%20extends%20ViTs%20to%20enforce%20the%20extraction%20of%20subtle%0Ainconsistency-prone%20information.%20For%20that%20purpose%2C%20an%20explicit%20attention%0Alearning%20guided%20by%20artifact-vulnerable%20patches%20and%20tailored%20to%20ViTs%20is%0Aintroduced.%20Extensive%20experiments%20are%20conducted%20on%20diverse%20well-known%20datasets%2C%0Aincluding%20FF%2B%2B%2C%20Celeb-DF%2C%20WildDeepfake%2C%20DFD%2C%20DFDCP%2C%20and%20DFDC.%20The%20results%20show%0Athat%20FakeFormer%20outperforms%20the%20state-of-the-art%20in%20terms%20of%20generalization%20and%0Acomputational%20cost%2C%20without%20the%20need%20for%20large-scale%20training%20datasets.%20The%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/10Ring/FakeFormer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21964v2&entry.124074799=Read"},
{"title": "Edge Weight Prediction For Category-Agnostic Pose Estimation", "author": "Or Hirschorn and Shai Avidan", "abstract": "  Category-Agnostic Pose Estimation (CAPE) localizes keypoints across diverse\nobject categories with a single model, using one or a few annotated support\nimages. Recent works have shown that using a pose graph (i.e., treating\nkeypoints as nodes in a graph rather than isolated points) helps handle\nocclusions and break symmetry. However, these methods assume a static pose\ngraph with equal-weight edges, leading to suboptimal results. We introduce\nEdgeCape, a novel framework that overcomes these limitations by predicting the\ngraph's edge weights which optimizes localization. To further leverage\nstructural priors, we propose integrating Markovian Structural Bias, which\nmodulates the self-attention interaction between nodes based on the number of\nhops between them. We show that this improves the model's ability to capture\nglobal spatial dependencies. Evaluated on the MP-100 benchmark, which includes\n100 categories and over 20K images, EdgeCape achieves state-of-the-art results\nin the 1-shot setting and leads among similar-sized methods in the 5-shot\nsetting, significantly improving keypoint localization accuracy. Our code is\npublicly available.\n", "link": "http://arxiv.org/abs/2411.16665v1", "date": "2024-11-25", "relevancy": 2.1365, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5263}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge%20Weight%20Prediction%20For%20Category-Agnostic%20Pose%20Estimation&body=Title%3A%20Edge%20Weight%20Prediction%20For%20Category-Agnostic%20Pose%20Estimation%0AAuthor%3A%20Or%20Hirschorn%20and%20Shai%20Avidan%0AAbstract%3A%20%20%20Category-Agnostic%20Pose%20Estimation%20%28CAPE%29%20localizes%20keypoints%20across%20diverse%0Aobject%20categories%20with%20a%20single%20model%2C%20using%20one%20or%20a%20few%20annotated%20support%0Aimages.%20Recent%20works%20have%20shown%20that%20using%20a%20pose%20graph%20%28i.e.%2C%20treating%0Akeypoints%20as%20nodes%20in%20a%20graph%20rather%20than%20isolated%20points%29%20helps%20handle%0Aocclusions%20and%20break%20symmetry.%20However%2C%20these%20methods%20assume%20a%20static%20pose%0Agraph%20with%20equal-weight%20edges%2C%20leading%20to%20suboptimal%20results.%20We%20introduce%0AEdgeCape%2C%20a%20novel%20framework%20that%20overcomes%20these%20limitations%20by%20predicting%20the%0Agraph%27s%20edge%20weights%20which%20optimizes%20localization.%20To%20further%20leverage%0Astructural%20priors%2C%20we%20propose%20integrating%20Markovian%20Structural%20Bias%2C%20which%0Amodulates%20the%20self-attention%20interaction%20between%20nodes%20based%20on%20the%20number%20of%0Ahops%20between%20them.%20We%20show%20that%20this%20improves%20the%20model%27s%20ability%20to%20capture%0Aglobal%20spatial%20dependencies.%20Evaluated%20on%20the%20MP-100%20benchmark%2C%20which%20includes%0A100%20categories%20and%20over%2020K%20images%2C%20EdgeCape%20achieves%20state-of-the-art%20results%0Ain%20the%201-shot%20setting%20and%20leads%20among%20similar-sized%20methods%20in%20the%205-shot%0Asetting%2C%20significantly%20improving%20keypoint%20localization%20accuracy.%20Our%20code%20is%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge%2520Weight%2520Prediction%2520For%2520Category-Agnostic%2520Pose%2520Estimation%26entry.906535625%3DOr%2520Hirschorn%2520and%2520Shai%2520Avidan%26entry.1292438233%3D%2520%2520Category-Agnostic%2520Pose%2520Estimation%2520%2528CAPE%2529%2520localizes%2520keypoints%2520across%2520diverse%250Aobject%2520categories%2520with%2520a%2520single%2520model%252C%2520using%2520one%2520or%2520a%2520few%2520annotated%2520support%250Aimages.%2520Recent%2520works%2520have%2520shown%2520that%2520using%2520a%2520pose%2520graph%2520%2528i.e.%252C%2520treating%250Akeypoints%2520as%2520nodes%2520in%2520a%2520graph%2520rather%2520than%2520isolated%2520points%2529%2520helps%2520handle%250Aocclusions%2520and%2520break%2520symmetry.%2520However%252C%2520these%2520methods%2520assume%2520a%2520static%2520pose%250Agraph%2520with%2520equal-weight%2520edges%252C%2520leading%2520to%2520suboptimal%2520results.%2520We%2520introduce%250AEdgeCape%252C%2520a%2520novel%2520framework%2520that%2520overcomes%2520these%2520limitations%2520by%2520predicting%2520the%250Agraph%2527s%2520edge%2520weights%2520which%2520optimizes%2520localization.%2520To%2520further%2520leverage%250Astructural%2520priors%252C%2520we%2520propose%2520integrating%2520Markovian%2520Structural%2520Bias%252C%2520which%250Amodulates%2520the%2520self-attention%2520interaction%2520between%2520nodes%2520based%2520on%2520the%2520number%2520of%250Ahops%2520between%2520them.%2520We%2520show%2520that%2520this%2520improves%2520the%2520model%2527s%2520ability%2520to%2520capture%250Aglobal%2520spatial%2520dependencies.%2520Evaluated%2520on%2520the%2520MP-100%2520benchmark%252C%2520which%2520includes%250A100%2520categories%2520and%2520over%252020K%2520images%252C%2520EdgeCape%2520achieves%2520state-of-the-art%2520results%250Ain%2520the%25201-shot%2520setting%2520and%2520leads%2520among%2520similar-sized%2520methods%2520in%2520the%25205-shot%250Asetting%252C%2520significantly%2520improving%2520keypoint%2520localization%2520accuracy.%2520Our%2520code%2520is%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20Weight%20Prediction%20For%20Category-Agnostic%20Pose%20Estimation&entry.906535625=Or%20Hirschorn%20and%20Shai%20Avidan&entry.1292438233=%20%20Category-Agnostic%20Pose%20Estimation%20%28CAPE%29%20localizes%20keypoints%20across%20diverse%0Aobject%20categories%20with%20a%20single%20model%2C%20using%20one%20or%20a%20few%20annotated%20support%0Aimages.%20Recent%20works%20have%20shown%20that%20using%20a%20pose%20graph%20%28i.e.%2C%20treating%0Akeypoints%20as%20nodes%20in%20a%20graph%20rather%20than%20isolated%20points%29%20helps%20handle%0Aocclusions%20and%20break%20symmetry.%20However%2C%20these%20methods%20assume%20a%20static%20pose%0Agraph%20with%20equal-weight%20edges%2C%20leading%20to%20suboptimal%20results.%20We%20introduce%0AEdgeCape%2C%20a%20novel%20framework%20that%20overcomes%20these%20limitations%20by%20predicting%20the%0Agraph%27s%20edge%20weights%20which%20optimizes%20localization.%20To%20further%20leverage%0Astructural%20priors%2C%20we%20propose%20integrating%20Markovian%20Structural%20Bias%2C%20which%0Amodulates%20the%20self-attention%20interaction%20between%20nodes%20based%20on%20the%20number%20of%0Ahops%20between%20them.%20We%20show%20that%20this%20improves%20the%20model%27s%20ability%20to%20capture%0Aglobal%20spatial%20dependencies.%20Evaluated%20on%20the%20MP-100%20benchmark%2C%20which%20includes%0A100%20categories%20and%20over%2020K%20images%2C%20EdgeCape%20achieves%20state-of-the-art%20results%0Ain%20the%201-shot%20setting%20and%20leads%20among%20similar-sized%20methods%20in%20the%205-shot%0Asetting%2C%20significantly%20improving%20keypoint%20localization%20accuracy.%20Our%20code%20is%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16665v1&entry.124074799=Read"},
{"title": "Enhancing Few-Shot Learning with Integrated Data and GAN Model\n  Approaches", "author": "Yinqiu Feng and Aoran Shen and Jiacheng Hu and Yingbin Liang and Shiru Wang and Junliang Du", "abstract": "  This paper presents an innovative approach to enhancing few-shot learning by\nintegrating data augmentation with model fine-tuning in a framework designed to\ntackle the challenges posed by small-sample data. Recognizing the critical\nlimitations of traditional machine learning models that require large\ndatasets-especially in fields such as drug discovery, target recognition, and\nmalicious traffic detection-this study proposes a novel strategy that leverages\nGenerative Adversarial Networks (GANs) and advanced optimization techniques to\nimprove model performance with limited data. Specifically, the paper addresses\nthe noise and bias issues introduced by data augmentation methods, contrasting\nthem with model-based approaches, such as fine-tuning and metric learning,\nwhich rely heavily on related datasets. By combining Markov Chain Monte Carlo\n(MCMC) sampling and discriminative model ensemble strategies within a GAN\nframework, the proposed model adjusts generative and discriminative\ndistributions to simulate a broader range of relevant data. Furthermore, it\nemploys MHLoss and a reparameterized GAN ensemble to enhance stability and\naccelerate convergence, ultimately leading to improved classification\nperformance on small-sample images and structured datasets. Results confirm\nthat the MhERGAN algorithm developed in this research is highly effective for\nfew-shot learning, offering a practical solution that bridges data scarcity\nwith high-performing model adaptability and generalization.\n", "link": "http://arxiv.org/abs/2411.16567v1", "date": "2024-11-25", "relevancy": 2.1316, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5518}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5244}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Few-Shot%20Learning%20with%20Integrated%20Data%20and%20GAN%20Model%0A%20%20Approaches&body=Title%3A%20Enhancing%20Few-Shot%20Learning%20with%20Integrated%20Data%20and%20GAN%20Model%0A%20%20Approaches%0AAuthor%3A%20Yinqiu%20Feng%20and%20Aoran%20Shen%20and%20Jiacheng%20Hu%20and%20Yingbin%20Liang%20and%20Shiru%20Wang%20and%20Junliang%20Du%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20innovative%20approach%20to%20enhancing%20few-shot%20learning%20by%0Aintegrating%20data%20augmentation%20with%20model%20fine-tuning%20in%20a%20framework%20designed%20to%0Atackle%20the%20challenges%20posed%20by%20small-sample%20data.%20Recognizing%20the%20critical%0Alimitations%20of%20traditional%20machine%20learning%20models%20that%20require%20large%0Adatasets-especially%20in%20fields%20such%20as%20drug%20discovery%2C%20target%20recognition%2C%20and%0Amalicious%20traffic%20detection-this%20study%20proposes%20a%20novel%20strategy%20that%20leverages%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20and%20advanced%20optimization%20techniques%20to%0Aimprove%20model%20performance%20with%20limited%20data.%20Specifically%2C%20the%20paper%20addresses%0Athe%20noise%20and%20bias%20issues%20introduced%20by%20data%20augmentation%20methods%2C%20contrasting%0Athem%20with%20model-based%20approaches%2C%20such%20as%20fine-tuning%20and%20metric%20learning%2C%0Awhich%20rely%20heavily%20on%20related%20datasets.%20By%20combining%20Markov%20Chain%20Monte%20Carlo%0A%28MCMC%29%20sampling%20and%20discriminative%20model%20ensemble%20strategies%20within%20a%20GAN%0Aframework%2C%20the%20proposed%20model%20adjusts%20generative%20and%20discriminative%0Adistributions%20to%20simulate%20a%20broader%20range%20of%20relevant%20data.%20Furthermore%2C%20it%0Aemploys%20MHLoss%20and%20a%20reparameterized%20GAN%20ensemble%20to%20enhance%20stability%20and%0Aaccelerate%20convergence%2C%20ultimately%20leading%20to%20improved%20classification%0Aperformance%20on%20small-sample%20images%20and%20structured%20datasets.%20Results%20confirm%0Athat%20the%20MhERGAN%20algorithm%20developed%20in%20this%20research%20is%20highly%20effective%20for%0Afew-shot%20learning%2C%20offering%20a%20practical%20solution%20that%20bridges%20data%20scarcity%0Awith%20high-performing%20model%20adaptability%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Few-Shot%2520Learning%2520with%2520Integrated%2520Data%2520and%2520GAN%2520Model%250A%2520%2520Approaches%26entry.906535625%3DYinqiu%2520Feng%2520and%2520Aoran%2520Shen%2520and%2520Jiacheng%2520Hu%2520and%2520Yingbin%2520Liang%2520and%2520Shiru%2520Wang%2520and%2520Junliang%2520Du%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520innovative%2520approach%2520to%2520enhancing%2520few-shot%2520learning%2520by%250Aintegrating%2520data%2520augmentation%2520with%2520model%2520fine-tuning%2520in%2520a%2520framework%2520designed%2520to%250Atackle%2520the%2520challenges%2520posed%2520by%2520small-sample%2520data.%2520Recognizing%2520the%2520critical%250Alimitations%2520of%2520traditional%2520machine%2520learning%2520models%2520that%2520require%2520large%250Adatasets-especially%2520in%2520fields%2520such%2520as%2520drug%2520discovery%252C%2520target%2520recognition%252C%2520and%250Amalicious%2520traffic%2520detection-this%2520study%2520proposes%2520a%2520novel%2520strategy%2520that%2520leverages%250AGenerative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520and%2520advanced%2520optimization%2520techniques%2520to%250Aimprove%2520model%2520performance%2520with%2520limited%2520data.%2520Specifically%252C%2520the%2520paper%2520addresses%250Athe%2520noise%2520and%2520bias%2520issues%2520introduced%2520by%2520data%2520augmentation%2520methods%252C%2520contrasting%250Athem%2520with%2520model-based%2520approaches%252C%2520such%2520as%2520fine-tuning%2520and%2520metric%2520learning%252C%250Awhich%2520rely%2520heavily%2520on%2520related%2520datasets.%2520By%2520combining%2520Markov%2520Chain%2520Monte%2520Carlo%250A%2528MCMC%2529%2520sampling%2520and%2520discriminative%2520model%2520ensemble%2520strategies%2520within%2520a%2520GAN%250Aframework%252C%2520the%2520proposed%2520model%2520adjusts%2520generative%2520and%2520discriminative%250Adistributions%2520to%2520simulate%2520a%2520broader%2520range%2520of%2520relevant%2520data.%2520Furthermore%252C%2520it%250Aemploys%2520MHLoss%2520and%2520a%2520reparameterized%2520GAN%2520ensemble%2520to%2520enhance%2520stability%2520and%250Aaccelerate%2520convergence%252C%2520ultimately%2520leading%2520to%2520improved%2520classification%250Aperformance%2520on%2520small-sample%2520images%2520and%2520structured%2520datasets.%2520Results%2520confirm%250Athat%2520the%2520MhERGAN%2520algorithm%2520developed%2520in%2520this%2520research%2520is%2520highly%2520effective%2520for%250Afew-shot%2520learning%252C%2520offering%2520a%2520practical%2520solution%2520that%2520bridges%2520data%2520scarcity%250Awith%2520high-performing%2520model%2520adaptability%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Few-Shot%20Learning%20with%20Integrated%20Data%20and%20GAN%20Model%0A%20%20Approaches&entry.906535625=Yinqiu%20Feng%20and%20Aoran%20Shen%20and%20Jiacheng%20Hu%20and%20Yingbin%20Liang%20and%20Shiru%20Wang%20and%20Junliang%20Du&entry.1292438233=%20%20This%20paper%20presents%20an%20innovative%20approach%20to%20enhancing%20few-shot%20learning%20by%0Aintegrating%20data%20augmentation%20with%20model%20fine-tuning%20in%20a%20framework%20designed%20to%0Atackle%20the%20challenges%20posed%20by%20small-sample%20data.%20Recognizing%20the%20critical%0Alimitations%20of%20traditional%20machine%20learning%20models%20that%20require%20large%0Adatasets-especially%20in%20fields%20such%20as%20drug%20discovery%2C%20target%20recognition%2C%20and%0Amalicious%20traffic%20detection-this%20study%20proposes%20a%20novel%20strategy%20that%20leverages%0AGenerative%20Adversarial%20Networks%20%28GANs%29%20and%20advanced%20optimization%20techniques%20to%0Aimprove%20model%20performance%20with%20limited%20data.%20Specifically%2C%20the%20paper%20addresses%0Athe%20noise%20and%20bias%20issues%20introduced%20by%20data%20augmentation%20methods%2C%20contrasting%0Athem%20with%20model-based%20approaches%2C%20such%20as%20fine-tuning%20and%20metric%20learning%2C%0Awhich%20rely%20heavily%20on%20related%20datasets.%20By%20combining%20Markov%20Chain%20Monte%20Carlo%0A%28MCMC%29%20sampling%20and%20discriminative%20model%20ensemble%20strategies%20within%20a%20GAN%0Aframework%2C%20the%20proposed%20model%20adjusts%20generative%20and%20discriminative%0Adistributions%20to%20simulate%20a%20broader%20range%20of%20relevant%20data.%20Furthermore%2C%20it%0Aemploys%20MHLoss%20and%20a%20reparameterized%20GAN%20ensemble%20to%20enhance%20stability%20and%0Aaccelerate%20convergence%2C%20ultimately%20leading%20to%20improved%20classification%0Aperformance%20on%20small-sample%20images%20and%20structured%20datasets.%20Results%20confirm%0Athat%20the%20MhERGAN%20algorithm%20developed%20in%20this%20research%20is%20highly%20effective%20for%0Afew-shot%20learning%2C%20offering%20a%20practical%20solution%20that%20bridges%20data%20scarcity%0Awith%20high-performing%20model%20adaptability%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16567v1&entry.124074799=Read"},
{"title": "A Performance Increment Strategy for Semantic Segmentation of\n  Low-Resolution Images from Damaged Roads", "author": "Rafael S. Toledo and Cristiano S. Oliveira and Vitor H. T. Oliveira and Eric A. Antonelo and Aldo von Wangenheim", "abstract": "  Autonomous driving needs good roads, but 85% of Brazilian roads have damages\nthat deep learning models may not regard as most semantic segmentation datasets\nfor autonomous driving are high-resolution images of well-maintained urban\nroads. A representative dataset for emerging countries consists of\nlow-resolution images of poorly maintained roads and includes labels of damage\nclasses; in this scenario, three challenges arise: objects with few pixels,\nobjects with undefined shapes, and highly underrepresented classes. To tackle\nthese challenges, this work proposes the Performance Increment Strategy for\nSemantic Segmentation (PISSS) as a methodology of 14 training experiments to\nboost performance. With PISSS, we reached state-of-the-art results of 79.8 and\n68.8 mIoU on the Road Traversing Knowledge (RTK) and Technik Autonomer Systeme\n500 (TAS500) test sets, respectively. Furthermore, we also offer an analysis of\nDeepLabV3+ pitfalls for small object segmentation.\n", "link": "http://arxiv.org/abs/2411.16295v1", "date": "2024-11-25", "relevancy": 2.1298, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5587}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5337}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Performance%20Increment%20Strategy%20for%20Semantic%20Segmentation%20of%0A%20%20Low-Resolution%20Images%20from%20Damaged%20Roads&body=Title%3A%20A%20Performance%20Increment%20Strategy%20for%20Semantic%20Segmentation%20of%0A%20%20Low-Resolution%20Images%20from%20Damaged%20Roads%0AAuthor%3A%20Rafael%20S.%20Toledo%20and%20Cristiano%20S.%20Oliveira%20and%20Vitor%20H.%20T.%20Oliveira%20and%20Eric%20A.%20Antonelo%20and%20Aldo%20von%20Wangenheim%0AAbstract%3A%20%20%20Autonomous%20driving%20needs%20good%20roads%2C%20but%2085%25%20of%20Brazilian%20roads%20have%20damages%0Athat%20deep%20learning%20models%20may%20not%20regard%20as%20most%20semantic%20segmentation%20datasets%0Afor%20autonomous%20driving%20are%20high-resolution%20images%20of%20well-maintained%20urban%0Aroads.%20A%20representative%20dataset%20for%20emerging%20countries%20consists%20of%0Alow-resolution%20images%20of%20poorly%20maintained%20roads%20and%20includes%20labels%20of%20damage%0Aclasses%3B%20in%20this%20scenario%2C%20three%20challenges%20arise%3A%20objects%20with%20few%20pixels%2C%0Aobjects%20with%20undefined%20shapes%2C%20and%20highly%20underrepresented%20classes.%20To%20tackle%0Athese%20challenges%2C%20this%20work%20proposes%20the%20Performance%20Increment%20Strategy%20for%0ASemantic%20Segmentation%20%28PISSS%29%20as%20a%20methodology%20of%2014%20training%20experiments%20to%0Aboost%20performance.%20With%20PISSS%2C%20we%20reached%20state-of-the-art%20results%20of%2079.8%20and%0A68.8%20mIoU%20on%20the%20Road%20Traversing%20Knowledge%20%28RTK%29%20and%20Technik%20Autonomer%20Systeme%0A500%20%28TAS500%29%20test%20sets%2C%20respectively.%20Furthermore%2C%20we%20also%20offer%20an%20analysis%20of%0ADeepLabV3%2B%20pitfalls%20for%20small%20object%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Performance%2520Increment%2520Strategy%2520for%2520Semantic%2520Segmentation%2520of%250A%2520%2520Low-Resolution%2520Images%2520from%2520Damaged%2520Roads%26entry.906535625%3DRafael%2520S.%2520Toledo%2520and%2520Cristiano%2520S.%2520Oliveira%2520and%2520Vitor%2520H.%2520T.%2520Oliveira%2520and%2520Eric%2520A.%2520Antonelo%2520and%2520Aldo%2520von%2520Wangenheim%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520needs%2520good%2520roads%252C%2520but%252085%2525%2520of%2520Brazilian%2520roads%2520have%2520damages%250Athat%2520deep%2520learning%2520models%2520may%2520not%2520regard%2520as%2520most%2520semantic%2520segmentation%2520datasets%250Afor%2520autonomous%2520driving%2520are%2520high-resolution%2520images%2520of%2520well-maintained%2520urban%250Aroads.%2520A%2520representative%2520dataset%2520for%2520emerging%2520countries%2520consists%2520of%250Alow-resolution%2520images%2520of%2520poorly%2520maintained%2520roads%2520and%2520includes%2520labels%2520of%2520damage%250Aclasses%253B%2520in%2520this%2520scenario%252C%2520three%2520challenges%2520arise%253A%2520objects%2520with%2520few%2520pixels%252C%250Aobjects%2520with%2520undefined%2520shapes%252C%2520and%2520highly%2520underrepresented%2520classes.%2520To%2520tackle%250Athese%2520challenges%252C%2520this%2520work%2520proposes%2520the%2520Performance%2520Increment%2520Strategy%2520for%250ASemantic%2520Segmentation%2520%2528PISSS%2529%2520as%2520a%2520methodology%2520of%252014%2520training%2520experiments%2520to%250Aboost%2520performance.%2520With%2520PISSS%252C%2520we%2520reached%2520state-of-the-art%2520results%2520of%252079.8%2520and%250A68.8%2520mIoU%2520on%2520the%2520Road%2520Traversing%2520Knowledge%2520%2528RTK%2529%2520and%2520Technik%2520Autonomer%2520Systeme%250A500%2520%2528TAS500%2529%2520test%2520sets%252C%2520respectively.%2520Furthermore%252C%2520we%2520also%2520offer%2520an%2520analysis%2520of%250ADeepLabV3%252B%2520pitfalls%2520for%2520small%2520object%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Performance%20Increment%20Strategy%20for%20Semantic%20Segmentation%20of%0A%20%20Low-Resolution%20Images%20from%20Damaged%20Roads&entry.906535625=Rafael%20S.%20Toledo%20and%20Cristiano%20S.%20Oliveira%20and%20Vitor%20H.%20T.%20Oliveira%20and%20Eric%20A.%20Antonelo%20and%20Aldo%20von%20Wangenheim&entry.1292438233=%20%20Autonomous%20driving%20needs%20good%20roads%2C%20but%2085%25%20of%20Brazilian%20roads%20have%20damages%0Athat%20deep%20learning%20models%20may%20not%20regard%20as%20most%20semantic%20segmentation%20datasets%0Afor%20autonomous%20driving%20are%20high-resolution%20images%20of%20well-maintained%20urban%0Aroads.%20A%20representative%20dataset%20for%20emerging%20countries%20consists%20of%0Alow-resolution%20images%20of%20poorly%20maintained%20roads%20and%20includes%20labels%20of%20damage%0Aclasses%3B%20in%20this%20scenario%2C%20three%20challenges%20arise%3A%20objects%20with%20few%20pixels%2C%0Aobjects%20with%20undefined%20shapes%2C%20and%20highly%20underrepresented%20classes.%20To%20tackle%0Athese%20challenges%2C%20this%20work%20proposes%20the%20Performance%20Increment%20Strategy%20for%0ASemantic%20Segmentation%20%28PISSS%29%20as%20a%20methodology%20of%2014%20training%20experiments%20to%0Aboost%20performance.%20With%20PISSS%2C%20we%20reached%20state-of-the-art%20results%20of%2079.8%20and%0A68.8%20mIoU%20on%20the%20Road%20Traversing%20Knowledge%20%28RTK%29%20and%20Technik%20Autonomer%20Systeme%0A500%20%28TAS500%29%20test%20sets%2C%20respectively.%20Furthermore%2C%20we%20also%20offer%20an%20analysis%20of%0ADeepLabV3%2B%20pitfalls%20for%20small%20object%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16295v1&entry.124074799=Read"},
{"title": "Multi-Scale Direction-Aware Network for Infrared Small Target Detection", "author": "Jinmiao Zhao and Zelin Shi and Chuang Yu and Yunpeng Liu", "abstract": "  Infrared small target detection faces the problem that it is difficult to\neffectively separate the background and the target. Existing deep\nlearning-based methods focus on appearance features and ignore high-frequency\ndirectional features. Therefore, we propose a multi-scale direction-aware\nnetwork (MSDA-Net), which is the first attempt to integrate the high-frequency\ndirectional features of infrared small targets as domain prior knowledge into\nneural networks. Specifically, an innovative multi-directional feature\nawareness (MDFA) module is constructed, which fully utilizes the prior\nknowledge of targets and emphasizes the focus on high-frequency directional\nfeatures. On this basis, combined with the multi-scale local relation learning\n(MLRL) module, a multi-scale direction-aware (MSDA) module is further\nconstructed. The MSDA module promotes the full extraction of local relations at\ndifferent scales and the full perception of key features in different\ndirections. Meanwhile, a high-frequency direction injection (HFDI) module\nwithout training parameters is constructed to inject the high-frequency\ndirectional information of the original image into the network. This helps\nguide the network to pay attention to detailed information such as target edges\nand shapes. In addition, we propose a feature aggregation (FA) structure that\naggregates multi-level features to solve the problem of small targets\ndisappearing in deep feature maps. Furthermore, a lightweight feature alignment\nfusion (FAF) module is constructed, which can effectively alleviate the pixel\noffset existing in multi-level feature map fusion. Extensive experimental\nresults show that our MSDA-Net achieves state-of-the-art (SOTA) results on the\npublic NUDT-SIRST, SIRST and IRSTD-1k datasets.\n", "link": "http://arxiv.org/abs/2406.02037v2", "date": "2024-11-25", "relevancy": 2.1272, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5523}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5388}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Direction-Aware%20Network%20for%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Multi-Scale%20Direction-Aware%20Network%20for%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu%0AAbstract%3A%20%20%20Infrared%20small%20target%20detection%20faces%20the%20problem%20that%20it%20is%20difficult%20to%0Aeffectively%20separate%20the%20background%20and%20the%20target.%20Existing%20deep%0Alearning-based%20methods%20focus%20on%20appearance%20features%20and%20ignore%20high-frequency%0Adirectional%20features.%20Therefore%2C%20we%20propose%20a%20multi-scale%20direction-aware%0Anetwork%20%28MSDA-Net%29%2C%20which%20is%20the%20first%20attempt%20to%20integrate%20the%20high-frequency%0Adirectional%20features%20of%20infrared%20small%20targets%20as%20domain%20prior%20knowledge%20into%0Aneural%20networks.%20Specifically%2C%20an%20innovative%20multi-directional%20feature%0Aawareness%20%28MDFA%29%20module%20is%20constructed%2C%20which%20fully%20utilizes%20the%20prior%0Aknowledge%20of%20targets%20and%20emphasizes%20the%20focus%20on%20high-frequency%20directional%0Afeatures.%20On%20this%20basis%2C%20combined%20with%20the%20multi-scale%20local%20relation%20learning%0A%28MLRL%29%20module%2C%20a%20multi-scale%20direction-aware%20%28MSDA%29%20module%20is%20further%0Aconstructed.%20The%20MSDA%20module%20promotes%20the%20full%20extraction%20of%20local%20relations%20at%0Adifferent%20scales%20and%20the%20full%20perception%20of%20key%20features%20in%20different%0Adirections.%20Meanwhile%2C%20a%20high-frequency%20direction%20injection%20%28HFDI%29%20module%0Awithout%20training%20parameters%20is%20constructed%20to%20inject%20the%20high-frequency%0Adirectional%20information%20of%20the%20original%20image%20into%20the%20network.%20This%20helps%0Aguide%20the%20network%20to%20pay%20attention%20to%20detailed%20information%20such%20as%20target%20edges%0Aand%20shapes.%20In%20addition%2C%20we%20propose%20a%20feature%20aggregation%20%28FA%29%20structure%20that%0Aaggregates%20multi-level%20features%20to%20solve%20the%20problem%20of%20small%20targets%0Adisappearing%20in%20deep%20feature%20maps.%20Furthermore%2C%20a%20lightweight%20feature%20alignment%0Afusion%20%28FAF%29%20module%20is%20constructed%2C%20which%20can%20effectively%20alleviate%20the%20pixel%0Aoffset%20existing%20in%20multi-level%20feature%20map%20fusion.%20Extensive%20experimental%0Aresults%20show%20that%20our%20MSDA-Net%20achieves%20state-of-the-art%20%28SOTA%29%20results%20on%20the%0Apublic%20NUDT-SIRST%2C%20SIRST%20and%20IRSTD-1k%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Direction-Aware%2520Network%2520for%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DJinmiao%2520Zhao%2520and%2520Zelin%2520Shi%2520and%2520Chuang%2520Yu%2520and%2520Yunpeng%2520Liu%26entry.1292438233%3D%2520%2520Infrared%2520small%2520target%2520detection%2520faces%2520the%2520problem%2520that%2520it%2520is%2520difficult%2520to%250Aeffectively%2520separate%2520the%2520background%2520and%2520the%2520target.%2520Existing%2520deep%250Alearning-based%2520methods%2520focus%2520on%2520appearance%2520features%2520and%2520ignore%2520high-frequency%250Adirectional%2520features.%2520Therefore%252C%2520we%2520propose%2520a%2520multi-scale%2520direction-aware%250Anetwork%2520%2528MSDA-Net%2529%252C%2520which%2520is%2520the%2520first%2520attempt%2520to%2520integrate%2520the%2520high-frequency%250Adirectional%2520features%2520of%2520infrared%2520small%2520targets%2520as%2520domain%2520prior%2520knowledge%2520into%250Aneural%2520networks.%2520Specifically%252C%2520an%2520innovative%2520multi-directional%2520feature%250Aawareness%2520%2528MDFA%2529%2520module%2520is%2520constructed%252C%2520which%2520fully%2520utilizes%2520the%2520prior%250Aknowledge%2520of%2520targets%2520and%2520emphasizes%2520the%2520focus%2520on%2520high-frequency%2520directional%250Afeatures.%2520On%2520this%2520basis%252C%2520combined%2520with%2520the%2520multi-scale%2520local%2520relation%2520learning%250A%2528MLRL%2529%2520module%252C%2520a%2520multi-scale%2520direction-aware%2520%2528MSDA%2529%2520module%2520is%2520further%250Aconstructed.%2520The%2520MSDA%2520module%2520promotes%2520the%2520full%2520extraction%2520of%2520local%2520relations%2520at%250Adifferent%2520scales%2520and%2520the%2520full%2520perception%2520of%2520key%2520features%2520in%2520different%250Adirections.%2520Meanwhile%252C%2520a%2520high-frequency%2520direction%2520injection%2520%2528HFDI%2529%2520module%250Awithout%2520training%2520parameters%2520is%2520constructed%2520to%2520inject%2520the%2520high-frequency%250Adirectional%2520information%2520of%2520the%2520original%2520image%2520into%2520the%2520network.%2520This%2520helps%250Aguide%2520the%2520network%2520to%2520pay%2520attention%2520to%2520detailed%2520information%2520such%2520as%2520target%2520edges%250Aand%2520shapes.%2520In%2520addition%252C%2520we%2520propose%2520a%2520feature%2520aggregation%2520%2528FA%2529%2520structure%2520that%250Aaggregates%2520multi-level%2520features%2520to%2520solve%2520the%2520problem%2520of%2520small%2520targets%250Adisappearing%2520in%2520deep%2520feature%2520maps.%2520Furthermore%252C%2520a%2520lightweight%2520feature%2520alignment%250Afusion%2520%2528FAF%2529%2520module%2520is%2520constructed%252C%2520which%2520can%2520effectively%2520alleviate%2520the%2520pixel%250Aoffset%2520existing%2520in%2520multi-level%2520feature%2520map%2520fusion.%2520Extensive%2520experimental%250Aresults%2520show%2520that%2520our%2520MSDA-Net%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520on%2520the%250Apublic%2520NUDT-SIRST%252C%2520SIRST%2520and%2520IRSTD-1k%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Direction-Aware%20Network%20for%20Infrared%20Small%20Target%20Detection&entry.906535625=Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu&entry.1292438233=%20%20Infrared%20small%20target%20detection%20faces%20the%20problem%20that%20it%20is%20difficult%20to%0Aeffectively%20separate%20the%20background%20and%20the%20target.%20Existing%20deep%0Alearning-based%20methods%20focus%20on%20appearance%20features%20and%20ignore%20high-frequency%0Adirectional%20features.%20Therefore%2C%20we%20propose%20a%20multi-scale%20direction-aware%0Anetwork%20%28MSDA-Net%29%2C%20which%20is%20the%20first%20attempt%20to%20integrate%20the%20high-frequency%0Adirectional%20features%20of%20infrared%20small%20targets%20as%20domain%20prior%20knowledge%20into%0Aneural%20networks.%20Specifically%2C%20an%20innovative%20multi-directional%20feature%0Aawareness%20%28MDFA%29%20module%20is%20constructed%2C%20which%20fully%20utilizes%20the%20prior%0Aknowledge%20of%20targets%20and%20emphasizes%20the%20focus%20on%20high-frequency%20directional%0Afeatures.%20On%20this%20basis%2C%20combined%20with%20the%20multi-scale%20local%20relation%20learning%0A%28MLRL%29%20module%2C%20a%20multi-scale%20direction-aware%20%28MSDA%29%20module%20is%20further%0Aconstructed.%20The%20MSDA%20module%20promotes%20the%20full%20extraction%20of%20local%20relations%20at%0Adifferent%20scales%20and%20the%20full%20perception%20of%20key%20features%20in%20different%0Adirections.%20Meanwhile%2C%20a%20high-frequency%20direction%20injection%20%28HFDI%29%20module%0Awithout%20training%20parameters%20is%20constructed%20to%20inject%20the%20high-frequency%0Adirectional%20information%20of%20the%20original%20image%20into%20the%20network.%20This%20helps%0Aguide%20the%20network%20to%20pay%20attention%20to%20detailed%20information%20such%20as%20target%20edges%0Aand%20shapes.%20In%20addition%2C%20we%20propose%20a%20feature%20aggregation%20%28FA%29%20structure%20that%0Aaggregates%20multi-level%20features%20to%20solve%20the%20problem%20of%20small%20targets%0Adisappearing%20in%20deep%20feature%20maps.%20Furthermore%2C%20a%20lightweight%20feature%20alignment%0Afusion%20%28FAF%29%20module%20is%20constructed%2C%20which%20can%20effectively%20alleviate%20the%20pixel%0Aoffset%20existing%20in%20multi-level%20feature%20map%20fusion.%20Extensive%20experimental%0Aresults%20show%20that%20our%20MSDA-Net%20achieves%20state-of-the-art%20%28SOTA%29%20results%20on%20the%0Apublic%20NUDT-SIRST%2C%20SIRST%20and%20IRSTD-1k%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02037v2&entry.124074799=Read"},
{"title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100\n  Languages", "author": "Ashmal Vayani and Dinura Dissanayake and Hasindri Watawana and Noor Ahsan and Nevasini Sasikumar and Omkar Thawakar and Henok Biadglign Ademtew and Yahya Hmaiti and Amandeep Kumar and Kartik Kuckreja and Mykola Maslych and Wafa Al Ghallabi and Mihail Mihaylov and Chao Qin and Abdelrahman M Shaker and Mike Zhang and Mahardika Krisna Ihsani and Amiel Esplana and Monil Gokani and Shachar Mirkin and Harsh Singh and Ashay Srivastava and Endre Hamerlik and Fathinah Asma Izzati and Fadillah Adamsyah Maani and Sebastian Cavada and Jenny Chim and Rohit Gupta and Sanjay Manjunath and Kamila Zhumakhanova and Feno Heriniaina Rabevohitra and Azril Amirudin and Muhammad Ridzuan and Daniya Kareem and Ketan More and Kunyang Li and Pramesh Shakya and Muhammad Saad and Amirpouya Ghasemaghaei and Amirbek Djanibekov and Dilshod Azizov and Branislava Jankovic and Naman Bhatia and Alvaro Cabrera and Johan Obando-Ceron and Olympiah Otieno and Fabian Farestam and Muztoba Rabbani and Sanoojan Baliah and Santosh Sanjeev and Abduragim Shtanchaev and Maheen Fatima and Thao Nguyen and Amrin Kareem and Toluwani Aremu and Nathan Xavier and Amit Bhatkal and Hawau Toyin and Aman Chadha and Hisham Cholakkal and Rao Muhammad Anwer and Michael Felsberg and Jorma Laaksonen and Thamar Solorio and Monojit Choudhury and Ivan Laptev and Mubarak Shah and Salman Khan and Fahad Khan", "abstract": "  Existing Large Multimodal Models (LMMs) generally focus on only a few regions\nand languages. As LMMs continue to improve, it is increasingly important to\nensure they understand cultural contexts, respect local sensitivities, and\nsupport low-resource languages, all while effectively integrating corresponding\nvisual cues. In pursuit of culturally diverse global multimodal models, our\nproposed All Languages Matter Benchmark (ALM-bench) represents the largest and\nmost comprehensive effort to date for evaluating LMMs across 100 languages.\nALM-bench challenges existing models by testing their ability to understand and\nreason about culturally diverse images paired with text in various languages,\nincluding many low-resource languages traditionally underrepresented in LMM\nresearch. The benchmark offers a robust and nuanced evaluation framework\nfeaturing various question formats, including true/false, multiple choice, and\nopen-ended questions, which are further divided into short and long-answer\ncategories. ALM-bench design ensures a comprehensive assessment of a model's\nability to handle varied levels of difficulty in visual and linguistic\nreasoning. To capture the rich tapestry of global cultures, ALM-bench carefully\ncurates content from 13 distinct cultural aspects, ranging from traditions and\nrituals to famous personalities and celebrations. Through this, ALM-bench not\nonly provides a rigorous testing ground for state-of-the-art open and\nclosed-source LMMs but also highlights the importance of cultural and\nlinguistic inclusivity, encouraging the development of models that can serve\ndiverse global populations effectively. Our benchmark is publicly available.\n", "link": "http://arxiv.org/abs/2411.16508v1", "date": "2024-11-25", "relevancy": 2.1194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All%20Languages%20Matter%3A%20Evaluating%20LMMs%20on%20Culturally%20Diverse%20100%0A%20%20Languages&body=Title%3A%20All%20Languages%20Matter%3A%20Evaluating%20LMMs%20on%20Culturally%20Diverse%20100%0A%20%20Languages%0AAuthor%3A%20Ashmal%20Vayani%20and%20Dinura%20Dissanayake%20and%20Hasindri%20Watawana%20and%20Noor%20Ahsan%20and%20Nevasini%20Sasikumar%20and%20Omkar%20Thawakar%20and%20Henok%20Biadglign%20Ademtew%20and%20Yahya%20Hmaiti%20and%20Amandeep%20Kumar%20and%20Kartik%20Kuckreja%20and%20Mykola%20Maslych%20and%20Wafa%20Al%20Ghallabi%20and%20Mihail%20Mihaylov%20and%20Chao%20Qin%20and%20Abdelrahman%20M%20Shaker%20and%20Mike%20Zhang%20and%20Mahardika%20Krisna%20Ihsani%20and%20Amiel%20Esplana%20and%20Monil%20Gokani%20and%20Shachar%20Mirkin%20and%20Harsh%20Singh%20and%20Ashay%20Srivastava%20and%20Endre%20Hamerlik%20and%20Fathinah%20Asma%20Izzati%20and%20Fadillah%20Adamsyah%20Maani%20and%20Sebastian%20Cavada%20and%20Jenny%20Chim%20and%20Rohit%20Gupta%20and%20Sanjay%20Manjunath%20and%20Kamila%20Zhumakhanova%20and%20Feno%20Heriniaina%20Rabevohitra%20and%20Azril%20Amirudin%20and%20Muhammad%20Ridzuan%20and%20Daniya%20Kareem%20and%20Ketan%20More%20and%20Kunyang%20Li%20and%20Pramesh%20Shakya%20and%20Muhammad%20Saad%20and%20Amirpouya%20Ghasemaghaei%20and%20Amirbek%20Djanibekov%20and%20Dilshod%20Azizov%20and%20Branislava%20Jankovic%20and%20Naman%20Bhatia%20and%20Alvaro%20Cabrera%20and%20Johan%20Obando-Ceron%20and%20Olympiah%20Otieno%20and%20Fabian%20Farestam%20and%20Muztoba%20Rabbani%20and%20Sanoojan%20Baliah%20and%20Santosh%20Sanjeev%20and%20Abduragim%20Shtanchaev%20and%20Maheen%20Fatima%20and%20Thao%20Nguyen%20and%20Amrin%20Kareem%20and%20Toluwani%20Aremu%20and%20Nathan%20Xavier%20and%20Amit%20Bhatkal%20and%20Hawau%20Toyin%20and%20Aman%20Chadha%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Michael%20Felsberg%20and%20Jorma%20Laaksonen%20and%20Thamar%20Solorio%20and%20Monojit%20Choudhury%20and%20Ivan%20Laptev%20and%20Mubarak%20Shah%20and%20Salman%20Khan%20and%20Fahad%20Khan%0AAbstract%3A%20%20%20Existing%20Large%20Multimodal%20Models%20%28LMMs%29%20generally%20focus%20on%20only%20a%20few%20regions%0Aand%20languages.%20As%20LMMs%20continue%20to%20improve%2C%20it%20is%20increasingly%20important%20to%0Aensure%20they%20understand%20cultural%20contexts%2C%20respect%20local%20sensitivities%2C%20and%0Asupport%20low-resource%20languages%2C%20all%20while%20effectively%20integrating%20corresponding%0Avisual%20cues.%20In%20pursuit%20of%20culturally%20diverse%20global%20multimodal%20models%2C%20our%0Aproposed%20All%20Languages%20Matter%20Benchmark%20%28ALM-bench%29%20represents%20the%20largest%20and%0Amost%20comprehensive%20effort%20to%20date%20for%20evaluating%20LMMs%20across%20100%20languages.%0AALM-bench%20challenges%20existing%20models%20by%20testing%20their%20ability%20to%20understand%20and%0Areason%20about%20culturally%20diverse%20images%20paired%20with%20text%20in%20various%20languages%2C%0Aincluding%20many%20low-resource%20languages%20traditionally%20underrepresented%20in%20LMM%0Aresearch.%20The%20benchmark%20offers%20a%20robust%20and%20nuanced%20evaluation%20framework%0Afeaturing%20various%20question%20formats%2C%20including%20true/false%2C%20multiple%20choice%2C%20and%0Aopen-ended%20questions%2C%20which%20are%20further%20divided%20into%20short%20and%20long-answer%0Acategories.%20ALM-bench%20design%20ensures%20a%20comprehensive%20assessment%20of%20a%20model%27s%0Aability%20to%20handle%20varied%20levels%20of%20difficulty%20in%20visual%20and%20linguistic%0Areasoning.%20To%20capture%20the%20rich%20tapestry%20of%20global%20cultures%2C%20ALM-bench%20carefully%0Acurates%20content%20from%2013%20distinct%20cultural%20aspects%2C%20ranging%20from%20traditions%20and%0Arituals%20to%20famous%20personalities%20and%20celebrations.%20Through%20this%2C%20ALM-bench%20not%0Aonly%20provides%20a%20rigorous%20testing%20ground%20for%20state-of-the-art%20open%20and%0Aclosed-source%20LMMs%20but%20also%20highlights%20the%20importance%20of%20cultural%20and%0Alinguistic%20inclusivity%2C%20encouraging%20the%20development%20of%20models%20that%20can%20serve%0Adiverse%20global%20populations%20effectively.%20Our%20benchmark%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll%2520Languages%2520Matter%253A%2520Evaluating%2520LMMs%2520on%2520Culturally%2520Diverse%2520100%250A%2520%2520Languages%26entry.906535625%3DAshmal%2520Vayani%2520and%2520Dinura%2520Dissanayake%2520and%2520Hasindri%2520Watawana%2520and%2520Noor%2520Ahsan%2520and%2520Nevasini%2520Sasikumar%2520and%2520Omkar%2520Thawakar%2520and%2520Henok%2520Biadglign%2520Ademtew%2520and%2520Yahya%2520Hmaiti%2520and%2520Amandeep%2520Kumar%2520and%2520Kartik%2520Kuckreja%2520and%2520Mykola%2520Maslych%2520and%2520Wafa%2520Al%2520Ghallabi%2520and%2520Mihail%2520Mihaylov%2520and%2520Chao%2520Qin%2520and%2520Abdelrahman%2520M%2520Shaker%2520and%2520Mike%2520Zhang%2520and%2520Mahardika%2520Krisna%2520Ihsani%2520and%2520Amiel%2520Esplana%2520and%2520Monil%2520Gokani%2520and%2520Shachar%2520Mirkin%2520and%2520Harsh%2520Singh%2520and%2520Ashay%2520Srivastava%2520and%2520Endre%2520Hamerlik%2520and%2520Fathinah%2520Asma%2520Izzati%2520and%2520Fadillah%2520Adamsyah%2520Maani%2520and%2520Sebastian%2520Cavada%2520and%2520Jenny%2520Chim%2520and%2520Rohit%2520Gupta%2520and%2520Sanjay%2520Manjunath%2520and%2520Kamila%2520Zhumakhanova%2520and%2520Feno%2520Heriniaina%2520Rabevohitra%2520and%2520Azril%2520Amirudin%2520and%2520Muhammad%2520Ridzuan%2520and%2520Daniya%2520Kareem%2520and%2520Ketan%2520More%2520and%2520Kunyang%2520Li%2520and%2520Pramesh%2520Shakya%2520and%2520Muhammad%2520Saad%2520and%2520Amirpouya%2520Ghasemaghaei%2520and%2520Amirbek%2520Djanibekov%2520and%2520Dilshod%2520Azizov%2520and%2520Branislava%2520Jankovic%2520and%2520Naman%2520Bhatia%2520and%2520Alvaro%2520Cabrera%2520and%2520Johan%2520Obando-Ceron%2520and%2520Olympiah%2520Otieno%2520and%2520Fabian%2520Farestam%2520and%2520Muztoba%2520Rabbani%2520and%2520Sanoojan%2520Baliah%2520and%2520Santosh%2520Sanjeev%2520and%2520Abduragim%2520Shtanchaev%2520and%2520Maheen%2520Fatima%2520and%2520Thao%2520Nguyen%2520and%2520Amrin%2520Kareem%2520and%2520Toluwani%2520Aremu%2520and%2520Nathan%2520Xavier%2520and%2520Amit%2520Bhatkal%2520and%2520Hawau%2520Toyin%2520and%2520Aman%2520Chadha%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Michael%2520Felsberg%2520and%2520Jorma%2520Laaksonen%2520and%2520Thamar%2520Solorio%2520and%2520Monojit%2520Choudhury%2520and%2520Ivan%2520Laptev%2520and%2520Mubarak%2520Shah%2520and%2520Salman%2520Khan%2520and%2520Fahad%2520Khan%26entry.1292438233%3D%2520%2520Existing%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520generally%2520focus%2520on%2520only%2520a%2520few%2520regions%250Aand%2520languages.%2520As%2520LMMs%2520continue%2520to%2520improve%252C%2520it%2520is%2520increasingly%2520important%2520to%250Aensure%2520they%2520understand%2520cultural%2520contexts%252C%2520respect%2520local%2520sensitivities%252C%2520and%250Asupport%2520low-resource%2520languages%252C%2520all%2520while%2520effectively%2520integrating%2520corresponding%250Avisual%2520cues.%2520In%2520pursuit%2520of%2520culturally%2520diverse%2520global%2520multimodal%2520models%252C%2520our%250Aproposed%2520All%2520Languages%2520Matter%2520Benchmark%2520%2528ALM-bench%2529%2520represents%2520the%2520largest%2520and%250Amost%2520comprehensive%2520effort%2520to%2520date%2520for%2520evaluating%2520LMMs%2520across%2520100%2520languages.%250AALM-bench%2520challenges%2520existing%2520models%2520by%2520testing%2520their%2520ability%2520to%2520understand%2520and%250Areason%2520about%2520culturally%2520diverse%2520images%2520paired%2520with%2520text%2520in%2520various%2520languages%252C%250Aincluding%2520many%2520low-resource%2520languages%2520traditionally%2520underrepresented%2520in%2520LMM%250Aresearch.%2520The%2520benchmark%2520offers%2520a%2520robust%2520and%2520nuanced%2520evaluation%2520framework%250Afeaturing%2520various%2520question%2520formats%252C%2520including%2520true/false%252C%2520multiple%2520choice%252C%2520and%250Aopen-ended%2520questions%252C%2520which%2520are%2520further%2520divided%2520into%2520short%2520and%2520long-answer%250Acategories.%2520ALM-bench%2520design%2520ensures%2520a%2520comprehensive%2520assessment%2520of%2520a%2520model%2527s%250Aability%2520to%2520handle%2520varied%2520levels%2520of%2520difficulty%2520in%2520visual%2520and%2520linguistic%250Areasoning.%2520To%2520capture%2520the%2520rich%2520tapestry%2520of%2520global%2520cultures%252C%2520ALM-bench%2520carefully%250Acurates%2520content%2520from%252013%2520distinct%2520cultural%2520aspects%252C%2520ranging%2520from%2520traditions%2520and%250Arituals%2520to%2520famous%2520personalities%2520and%2520celebrations.%2520Through%2520this%252C%2520ALM-bench%2520not%250Aonly%2520provides%2520a%2520rigorous%2520testing%2520ground%2520for%2520state-of-the-art%2520open%2520and%250Aclosed-source%2520LMMs%2520but%2520also%2520highlights%2520the%2520importance%2520of%2520cultural%2520and%250Alinguistic%2520inclusivity%252C%2520encouraging%2520the%2520development%2520of%2520models%2520that%2520can%2520serve%250Adiverse%2520global%2520populations%2520effectively.%2520Our%2520benchmark%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All%20Languages%20Matter%3A%20Evaluating%20LMMs%20on%20Culturally%20Diverse%20100%0A%20%20Languages&entry.906535625=Ashmal%20Vayani%20and%20Dinura%20Dissanayake%20and%20Hasindri%20Watawana%20and%20Noor%20Ahsan%20and%20Nevasini%20Sasikumar%20and%20Omkar%20Thawakar%20and%20Henok%20Biadglign%20Ademtew%20and%20Yahya%20Hmaiti%20and%20Amandeep%20Kumar%20and%20Kartik%20Kuckreja%20and%20Mykola%20Maslych%20and%20Wafa%20Al%20Ghallabi%20and%20Mihail%20Mihaylov%20and%20Chao%20Qin%20and%20Abdelrahman%20M%20Shaker%20and%20Mike%20Zhang%20and%20Mahardika%20Krisna%20Ihsani%20and%20Amiel%20Esplana%20and%20Monil%20Gokani%20and%20Shachar%20Mirkin%20and%20Harsh%20Singh%20and%20Ashay%20Srivastava%20and%20Endre%20Hamerlik%20and%20Fathinah%20Asma%20Izzati%20and%20Fadillah%20Adamsyah%20Maani%20and%20Sebastian%20Cavada%20and%20Jenny%20Chim%20and%20Rohit%20Gupta%20and%20Sanjay%20Manjunath%20and%20Kamila%20Zhumakhanova%20and%20Feno%20Heriniaina%20Rabevohitra%20and%20Azril%20Amirudin%20and%20Muhammad%20Ridzuan%20and%20Daniya%20Kareem%20and%20Ketan%20More%20and%20Kunyang%20Li%20and%20Pramesh%20Shakya%20and%20Muhammad%20Saad%20and%20Amirpouya%20Ghasemaghaei%20and%20Amirbek%20Djanibekov%20and%20Dilshod%20Azizov%20and%20Branislava%20Jankovic%20and%20Naman%20Bhatia%20and%20Alvaro%20Cabrera%20and%20Johan%20Obando-Ceron%20and%20Olympiah%20Otieno%20and%20Fabian%20Farestam%20and%20Muztoba%20Rabbani%20and%20Sanoojan%20Baliah%20and%20Santosh%20Sanjeev%20and%20Abduragim%20Shtanchaev%20and%20Maheen%20Fatima%20and%20Thao%20Nguyen%20and%20Amrin%20Kareem%20and%20Toluwani%20Aremu%20and%20Nathan%20Xavier%20and%20Amit%20Bhatkal%20and%20Hawau%20Toyin%20and%20Aman%20Chadha%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Michael%20Felsberg%20and%20Jorma%20Laaksonen%20and%20Thamar%20Solorio%20and%20Monojit%20Choudhury%20and%20Ivan%20Laptev%20and%20Mubarak%20Shah%20and%20Salman%20Khan%20and%20Fahad%20Khan&entry.1292438233=%20%20Existing%20Large%20Multimodal%20Models%20%28LMMs%29%20generally%20focus%20on%20only%20a%20few%20regions%0Aand%20languages.%20As%20LMMs%20continue%20to%20improve%2C%20it%20is%20increasingly%20important%20to%0Aensure%20they%20understand%20cultural%20contexts%2C%20respect%20local%20sensitivities%2C%20and%0Asupport%20low-resource%20languages%2C%20all%20while%20effectively%20integrating%20corresponding%0Avisual%20cues.%20In%20pursuit%20of%20culturally%20diverse%20global%20multimodal%20models%2C%20our%0Aproposed%20All%20Languages%20Matter%20Benchmark%20%28ALM-bench%29%20represents%20the%20largest%20and%0Amost%20comprehensive%20effort%20to%20date%20for%20evaluating%20LMMs%20across%20100%20languages.%0AALM-bench%20challenges%20existing%20models%20by%20testing%20their%20ability%20to%20understand%20and%0Areason%20about%20culturally%20diverse%20images%20paired%20with%20text%20in%20various%20languages%2C%0Aincluding%20many%20low-resource%20languages%20traditionally%20underrepresented%20in%20LMM%0Aresearch.%20The%20benchmark%20offers%20a%20robust%20and%20nuanced%20evaluation%20framework%0Afeaturing%20various%20question%20formats%2C%20including%20true/false%2C%20multiple%20choice%2C%20and%0Aopen-ended%20questions%2C%20which%20are%20further%20divided%20into%20short%20and%20long-answer%0Acategories.%20ALM-bench%20design%20ensures%20a%20comprehensive%20assessment%20of%20a%20model%27s%0Aability%20to%20handle%20varied%20levels%20of%20difficulty%20in%20visual%20and%20linguistic%0Areasoning.%20To%20capture%20the%20rich%20tapestry%20of%20global%20cultures%2C%20ALM-bench%20carefully%0Acurates%20content%20from%2013%20distinct%20cultural%20aspects%2C%20ranging%20from%20traditions%20and%0Arituals%20to%20famous%20personalities%20and%20celebrations.%20Through%20this%2C%20ALM-bench%20not%0Aonly%20provides%20a%20rigorous%20testing%20ground%20for%20state-of-the-art%20open%20and%0Aclosed-source%20LMMs%20but%20also%20highlights%20the%20importance%20of%20cultural%20and%0Alinguistic%20inclusivity%2C%20encouraging%20the%20development%20of%20models%20that%20can%20serve%0Adiverse%20global%20populations%20effectively.%20Our%20benchmark%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16508v1&entry.124074799=Read"},
{"title": "A Review of Mechanistic Models of Event Comprehension", "author": "Tan T. Nguyen", "abstract": "  This review examines theoretical assumptions and computational models of\nevent comprehension, tracing the evolution from discourse comprehension\ntheories to contemporary event cognition frameworks. The review covers key\ndiscourse comprehension accounts, including Construction-Integration, Event\nIndexing, Causal Network, and Resonance models, highlighting their\ncontributions to understanding cognitive processes in comprehension. I then\ndiscuss contemporary theoretical frameworks of event comprehension, including\nEvent Segmentation Theory (Zacks et al., 2007), the Event Horizon Model\n(Radvansky & Zacks, 2014), and Hierarchical Generative Framework (Kuperberg,\n2021), which emphasize prediction, causality, and multilevel representations in\nevent understanding. Building on these theories, I evaluate five computational\nmodels of event comprehension: REPRISE (Butz et al., 2019), Structured Event\nMemory (SEM; Franklin et al., 2020), the Lu model (Lu et al., 2022), the\nGumbsch model (Gumbsch et al., 2022), and the Elman and McRae model (2019). The\nanalysis focuses on their approaches to hierarchical processing, prediction\nmechanisms, and representation learning. Key themes that emerge include the use\nof hierarchical structures as inductive biases, the importance of prediction in\ncomprehension, and diverse strategies for learning event dynamics. The review\nidentifies critical areas for future research, including the need for more\nsophisticated approaches to learning structured representations, integrating\nepisodic memory mechanisms, and developing adaptive updating algorithms for\nworking event models. By synthesizing insights from both theoretical frameworks\nand computational implementations, this review aims to advance our\nunderstanding of human event comprehension and guide future modeling efforts in\ncognitive science.\n", "link": "http://arxiv.org/abs/2409.18992v2", "date": "2024-11-25", "relevancy": 2.1143, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%20Mechanistic%20Models%20of%20Event%20Comprehension&body=Title%3A%20A%20Review%20of%20Mechanistic%20Models%20of%20Event%20Comprehension%0AAuthor%3A%20Tan%20T.%20Nguyen%0AAbstract%3A%20%20%20This%20review%20examines%20theoretical%20assumptions%20and%20computational%20models%20of%0Aevent%20comprehension%2C%20tracing%20the%20evolution%20from%20discourse%20comprehension%0Atheories%20to%20contemporary%20event%20cognition%20frameworks.%20The%20review%20covers%20key%0Adiscourse%20comprehension%20accounts%2C%20including%20Construction-Integration%2C%20Event%0AIndexing%2C%20Causal%20Network%2C%20and%20Resonance%20models%2C%20highlighting%20their%0Acontributions%20to%20understanding%20cognitive%20processes%20in%20comprehension.%20I%20then%0Adiscuss%20contemporary%20theoretical%20frameworks%20of%20event%20comprehension%2C%20including%0AEvent%20Segmentation%20Theory%20%28Zacks%20et%20al.%2C%202007%29%2C%20the%20Event%20Horizon%20Model%0A%28Radvansky%20%26%20Zacks%2C%202014%29%2C%20and%20Hierarchical%20Generative%20Framework%20%28Kuperberg%2C%0A2021%29%2C%20which%20emphasize%20prediction%2C%20causality%2C%20and%20multilevel%20representations%20in%0Aevent%20understanding.%20Building%20on%20these%20theories%2C%20I%20evaluate%20five%20computational%0Amodels%20of%20event%20comprehension%3A%20REPRISE%20%28Butz%20et%20al.%2C%202019%29%2C%20Structured%20Event%0AMemory%20%28SEM%3B%20Franklin%20et%20al.%2C%202020%29%2C%20the%20Lu%20model%20%28Lu%20et%20al.%2C%202022%29%2C%20the%0AGumbsch%20model%20%28Gumbsch%20et%20al.%2C%202022%29%2C%20and%20the%20Elman%20and%20McRae%20model%20%282019%29.%20The%0Aanalysis%20focuses%20on%20their%20approaches%20to%20hierarchical%20processing%2C%20prediction%0Amechanisms%2C%20and%20representation%20learning.%20Key%20themes%20that%20emerge%20include%20the%20use%0Aof%20hierarchical%20structures%20as%20inductive%20biases%2C%20the%20importance%20of%20prediction%20in%0Acomprehension%2C%20and%20diverse%20strategies%20for%20learning%20event%20dynamics.%20The%20review%0Aidentifies%20critical%20areas%20for%20future%20research%2C%20including%20the%20need%20for%20more%0Asophisticated%20approaches%20to%20learning%20structured%20representations%2C%20integrating%0Aepisodic%20memory%20mechanisms%2C%20and%20developing%20adaptive%20updating%20algorithms%20for%0Aworking%20event%20models.%20By%20synthesizing%20insights%20from%20both%20theoretical%20frameworks%0Aand%20computational%20implementations%2C%20this%20review%20aims%20to%20advance%20our%0Aunderstanding%20of%20human%20event%20comprehension%20and%20guide%20future%20modeling%20efforts%20in%0Acognitive%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18992v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%2520Mechanistic%2520Models%2520of%2520Event%2520Comprehension%26entry.906535625%3DTan%2520T.%2520Nguyen%26entry.1292438233%3D%2520%2520This%2520review%2520examines%2520theoretical%2520assumptions%2520and%2520computational%2520models%2520of%250Aevent%2520comprehension%252C%2520tracing%2520the%2520evolution%2520from%2520discourse%2520comprehension%250Atheories%2520to%2520contemporary%2520event%2520cognition%2520frameworks.%2520The%2520review%2520covers%2520key%250Adiscourse%2520comprehension%2520accounts%252C%2520including%2520Construction-Integration%252C%2520Event%250AIndexing%252C%2520Causal%2520Network%252C%2520and%2520Resonance%2520models%252C%2520highlighting%2520their%250Acontributions%2520to%2520understanding%2520cognitive%2520processes%2520in%2520comprehension.%2520I%2520then%250Adiscuss%2520contemporary%2520theoretical%2520frameworks%2520of%2520event%2520comprehension%252C%2520including%250AEvent%2520Segmentation%2520Theory%2520%2528Zacks%2520et%2520al.%252C%25202007%2529%252C%2520the%2520Event%2520Horizon%2520Model%250A%2528Radvansky%2520%2526%2520Zacks%252C%25202014%2529%252C%2520and%2520Hierarchical%2520Generative%2520Framework%2520%2528Kuperberg%252C%250A2021%2529%252C%2520which%2520emphasize%2520prediction%252C%2520causality%252C%2520and%2520multilevel%2520representations%2520in%250Aevent%2520understanding.%2520Building%2520on%2520these%2520theories%252C%2520I%2520evaluate%2520five%2520computational%250Amodels%2520of%2520event%2520comprehension%253A%2520REPRISE%2520%2528Butz%2520et%2520al.%252C%25202019%2529%252C%2520Structured%2520Event%250AMemory%2520%2528SEM%253B%2520Franklin%2520et%2520al.%252C%25202020%2529%252C%2520the%2520Lu%2520model%2520%2528Lu%2520et%2520al.%252C%25202022%2529%252C%2520the%250AGumbsch%2520model%2520%2528Gumbsch%2520et%2520al.%252C%25202022%2529%252C%2520and%2520the%2520Elman%2520and%2520McRae%2520model%2520%25282019%2529.%2520The%250Aanalysis%2520focuses%2520on%2520their%2520approaches%2520to%2520hierarchical%2520processing%252C%2520prediction%250Amechanisms%252C%2520and%2520representation%2520learning.%2520Key%2520themes%2520that%2520emerge%2520include%2520the%2520use%250Aof%2520hierarchical%2520structures%2520as%2520inductive%2520biases%252C%2520the%2520importance%2520of%2520prediction%2520in%250Acomprehension%252C%2520and%2520diverse%2520strategies%2520for%2520learning%2520event%2520dynamics.%2520The%2520review%250Aidentifies%2520critical%2520areas%2520for%2520future%2520research%252C%2520including%2520the%2520need%2520for%2520more%250Asophisticated%2520approaches%2520to%2520learning%2520structured%2520representations%252C%2520integrating%250Aepisodic%2520memory%2520mechanisms%252C%2520and%2520developing%2520adaptive%2520updating%2520algorithms%2520for%250Aworking%2520event%2520models.%2520By%2520synthesizing%2520insights%2520from%2520both%2520theoretical%2520frameworks%250Aand%2520computational%2520implementations%252C%2520this%2520review%2520aims%2520to%2520advance%2520our%250Aunderstanding%2520of%2520human%2520event%2520comprehension%2520and%2520guide%2520future%2520modeling%2520efforts%2520in%250Acognitive%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18992v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%20Mechanistic%20Models%20of%20Event%20Comprehension&entry.906535625=Tan%20T.%20Nguyen&entry.1292438233=%20%20This%20review%20examines%20theoretical%20assumptions%20and%20computational%20models%20of%0Aevent%20comprehension%2C%20tracing%20the%20evolution%20from%20discourse%20comprehension%0Atheories%20to%20contemporary%20event%20cognition%20frameworks.%20The%20review%20covers%20key%0Adiscourse%20comprehension%20accounts%2C%20including%20Construction-Integration%2C%20Event%0AIndexing%2C%20Causal%20Network%2C%20and%20Resonance%20models%2C%20highlighting%20their%0Acontributions%20to%20understanding%20cognitive%20processes%20in%20comprehension.%20I%20then%0Adiscuss%20contemporary%20theoretical%20frameworks%20of%20event%20comprehension%2C%20including%0AEvent%20Segmentation%20Theory%20%28Zacks%20et%20al.%2C%202007%29%2C%20the%20Event%20Horizon%20Model%0A%28Radvansky%20%26%20Zacks%2C%202014%29%2C%20and%20Hierarchical%20Generative%20Framework%20%28Kuperberg%2C%0A2021%29%2C%20which%20emphasize%20prediction%2C%20causality%2C%20and%20multilevel%20representations%20in%0Aevent%20understanding.%20Building%20on%20these%20theories%2C%20I%20evaluate%20five%20computational%0Amodels%20of%20event%20comprehension%3A%20REPRISE%20%28Butz%20et%20al.%2C%202019%29%2C%20Structured%20Event%0AMemory%20%28SEM%3B%20Franklin%20et%20al.%2C%202020%29%2C%20the%20Lu%20model%20%28Lu%20et%20al.%2C%202022%29%2C%20the%0AGumbsch%20model%20%28Gumbsch%20et%20al.%2C%202022%29%2C%20and%20the%20Elman%20and%20McRae%20model%20%282019%29.%20The%0Aanalysis%20focuses%20on%20their%20approaches%20to%20hierarchical%20processing%2C%20prediction%0Amechanisms%2C%20and%20representation%20learning.%20Key%20themes%20that%20emerge%20include%20the%20use%0Aof%20hierarchical%20structures%20as%20inductive%20biases%2C%20the%20importance%20of%20prediction%20in%0Acomprehension%2C%20and%20diverse%20strategies%20for%20learning%20event%20dynamics.%20The%20review%0Aidentifies%20critical%20areas%20for%20future%20research%2C%20including%20the%20need%20for%20more%0Asophisticated%20approaches%20to%20learning%20structured%20representations%2C%20integrating%0Aepisodic%20memory%20mechanisms%2C%20and%20developing%20adaptive%20updating%20algorithms%20for%0Aworking%20event%20models.%20By%20synthesizing%20insights%20from%20both%20theoretical%20frameworks%0Aand%20computational%20implementations%2C%20this%20review%20aims%20to%20advance%20our%0Aunderstanding%20of%20human%20event%20comprehension%20and%20guide%20future%20modeling%20efforts%20in%0Acognitive%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18992v2&entry.124074799=Read"},
{"title": "Enhancing Autonomous Driving Safety through World Model-Based Predictive\n  Navigation and Adaptive Learning Algorithms for 5G Wireless Applications", "author": "Hong Ding and Ziming Wang and Yi Ding and Hongjie Lin and SuYang Xi and Chia Chao Kang", "abstract": "  Addressing the challenge of ensuring safety in ever-changing and\nunpredictable environments, particularly in the swiftly advancing realm of\nautonomous driving in today's 5G wireless communication world, we present\nNavigation Secure (NavSecure). This vision-based navigation framework merges\nthe strengths of world models with crucial safety-focused decision-making\ncapabilities, enabling autonomous vehicles to navigate real-world complexities\nsecurely. Our approach anticipates potential threats and formulates safer\nroutes by harnessing the predictive capabilities of world models, thus\nsignificantly reducing the need for extensive real-world trial-and-error\nlearning. Additionally, our method empowers vehicles to autonomously learn and\ndevelop through continuous practice, ensuring the system evolves and adapts to\nnew challenges. Incorporating radio frequency technology, NavSecure leverages\n5G networks to enhance real-time data exchange, improving communication and\nresponsiveness. Validated through rigorous experiments under simulation-to-real\ndriving conditions, NavSecure has shown exceptional performance in\nsafety-critical scenarios, such as sudden obstacle avoidance. Results indicate\nthat NavSecure excels in key safety metrics, including collision prevention and\nrisk reduction, surpassing other end-to-end methodologies. This framework not\nonly advances autonomous driving safety but also demonstrates how world models\ncan enhance decision-making in critical applications. NavSecure sets a new\nstandard for developing more robust and trustworthy autonomous driving systems,\ncapable of handling the inherent dynamics and uncertainties of real-world\nenvironments.\n", "link": "http://arxiv.org/abs/2411.15042v2", "date": "2024-11-25", "relevancy": 2.1135, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5367}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5268}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Autonomous%20Driving%20Safety%20through%20World%20Model-Based%20Predictive%0A%20%20Navigation%20and%20Adaptive%20Learning%20Algorithms%20for%205G%20Wireless%20Applications&body=Title%3A%20Enhancing%20Autonomous%20Driving%20Safety%20through%20World%20Model-Based%20Predictive%0A%20%20Navigation%20and%20Adaptive%20Learning%20Algorithms%20for%205G%20Wireless%20Applications%0AAuthor%3A%20Hong%20Ding%20and%20Ziming%20Wang%20and%20Yi%20Ding%20and%20Hongjie%20Lin%20and%20SuYang%20Xi%20and%20Chia%20Chao%20Kang%0AAbstract%3A%20%20%20Addressing%20the%20challenge%20of%20ensuring%20safety%20in%20ever-changing%20and%0Aunpredictable%20environments%2C%20particularly%20in%20the%20swiftly%20advancing%20realm%20of%0Aautonomous%20driving%20in%20today%27s%205G%20wireless%20communication%20world%2C%20we%20present%0ANavigation%20Secure%20%28NavSecure%29.%20This%20vision-based%20navigation%20framework%20merges%0Athe%20strengths%20of%20world%20models%20with%20crucial%20safety-focused%20decision-making%0Acapabilities%2C%20enabling%20autonomous%20vehicles%20to%20navigate%20real-world%20complexities%0Asecurely.%20Our%20approach%20anticipates%20potential%20threats%20and%20formulates%20safer%0Aroutes%20by%20harnessing%20the%20predictive%20capabilities%20of%20world%20models%2C%20thus%0Asignificantly%20reducing%20the%20need%20for%20extensive%20real-world%20trial-and-error%0Alearning.%20Additionally%2C%20our%20method%20empowers%20vehicles%20to%20autonomously%20learn%20and%0Adevelop%20through%20continuous%20practice%2C%20ensuring%20the%20system%20evolves%20and%20adapts%20to%0Anew%20challenges.%20Incorporating%20radio%20frequency%20technology%2C%20NavSecure%20leverages%0A5G%20networks%20to%20enhance%20real-time%20data%20exchange%2C%20improving%20communication%20and%0Aresponsiveness.%20Validated%20through%20rigorous%20experiments%20under%20simulation-to-real%0Adriving%20conditions%2C%20NavSecure%20has%20shown%20exceptional%20performance%20in%0Asafety-critical%20scenarios%2C%20such%20as%20sudden%20obstacle%20avoidance.%20Results%20indicate%0Athat%20NavSecure%20excels%20in%20key%20safety%20metrics%2C%20including%20collision%20prevention%20and%0Arisk%20reduction%2C%20surpassing%20other%20end-to-end%20methodologies.%20This%20framework%20not%0Aonly%20advances%20autonomous%20driving%20safety%20but%20also%20demonstrates%20how%20world%20models%0Acan%20enhance%20decision-making%20in%20critical%20applications.%20NavSecure%20sets%20a%20new%0Astandard%20for%20developing%20more%20robust%20and%20trustworthy%20autonomous%20driving%20systems%2C%0Acapable%20of%20handling%20the%20inherent%20dynamics%20and%20uncertainties%20of%20real-world%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Autonomous%2520Driving%2520Safety%2520through%2520World%2520Model-Based%2520Predictive%250A%2520%2520Navigation%2520and%2520Adaptive%2520Learning%2520Algorithms%2520for%25205G%2520Wireless%2520Applications%26entry.906535625%3DHong%2520Ding%2520and%2520Ziming%2520Wang%2520and%2520Yi%2520Ding%2520and%2520Hongjie%2520Lin%2520and%2520SuYang%2520Xi%2520and%2520Chia%2520Chao%2520Kang%26entry.1292438233%3D%2520%2520Addressing%2520the%2520challenge%2520of%2520ensuring%2520safety%2520in%2520ever-changing%2520and%250Aunpredictable%2520environments%252C%2520particularly%2520in%2520the%2520swiftly%2520advancing%2520realm%2520of%250Aautonomous%2520driving%2520in%2520today%2527s%25205G%2520wireless%2520communication%2520world%252C%2520we%2520present%250ANavigation%2520Secure%2520%2528NavSecure%2529.%2520This%2520vision-based%2520navigation%2520framework%2520merges%250Athe%2520strengths%2520of%2520world%2520models%2520with%2520crucial%2520safety-focused%2520decision-making%250Acapabilities%252C%2520enabling%2520autonomous%2520vehicles%2520to%2520navigate%2520real-world%2520complexities%250Asecurely.%2520Our%2520approach%2520anticipates%2520potential%2520threats%2520and%2520formulates%2520safer%250Aroutes%2520by%2520harnessing%2520the%2520predictive%2520capabilities%2520of%2520world%2520models%252C%2520thus%250Asignificantly%2520reducing%2520the%2520need%2520for%2520extensive%2520real-world%2520trial-and-error%250Alearning.%2520Additionally%252C%2520our%2520method%2520empowers%2520vehicles%2520to%2520autonomously%2520learn%2520and%250Adevelop%2520through%2520continuous%2520practice%252C%2520ensuring%2520the%2520system%2520evolves%2520and%2520adapts%2520to%250Anew%2520challenges.%2520Incorporating%2520radio%2520frequency%2520technology%252C%2520NavSecure%2520leverages%250A5G%2520networks%2520to%2520enhance%2520real-time%2520data%2520exchange%252C%2520improving%2520communication%2520and%250Aresponsiveness.%2520Validated%2520through%2520rigorous%2520experiments%2520under%2520simulation-to-real%250Adriving%2520conditions%252C%2520NavSecure%2520has%2520shown%2520exceptional%2520performance%2520in%250Asafety-critical%2520scenarios%252C%2520such%2520as%2520sudden%2520obstacle%2520avoidance.%2520Results%2520indicate%250Athat%2520NavSecure%2520excels%2520in%2520key%2520safety%2520metrics%252C%2520including%2520collision%2520prevention%2520and%250Arisk%2520reduction%252C%2520surpassing%2520other%2520end-to-end%2520methodologies.%2520This%2520framework%2520not%250Aonly%2520advances%2520autonomous%2520driving%2520safety%2520but%2520also%2520demonstrates%2520how%2520world%2520models%250Acan%2520enhance%2520decision-making%2520in%2520critical%2520applications.%2520NavSecure%2520sets%2520a%2520new%250Astandard%2520for%2520developing%2520more%2520robust%2520and%2520trustworthy%2520autonomous%2520driving%2520systems%252C%250Acapable%2520of%2520handling%2520the%2520inherent%2520dynamics%2520and%2520uncertainties%2520of%2520real-world%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Autonomous%20Driving%20Safety%20through%20World%20Model-Based%20Predictive%0A%20%20Navigation%20and%20Adaptive%20Learning%20Algorithms%20for%205G%20Wireless%20Applications&entry.906535625=Hong%20Ding%20and%20Ziming%20Wang%20and%20Yi%20Ding%20and%20Hongjie%20Lin%20and%20SuYang%20Xi%20and%20Chia%20Chao%20Kang&entry.1292438233=%20%20Addressing%20the%20challenge%20of%20ensuring%20safety%20in%20ever-changing%20and%0Aunpredictable%20environments%2C%20particularly%20in%20the%20swiftly%20advancing%20realm%20of%0Aautonomous%20driving%20in%20today%27s%205G%20wireless%20communication%20world%2C%20we%20present%0ANavigation%20Secure%20%28NavSecure%29.%20This%20vision-based%20navigation%20framework%20merges%0Athe%20strengths%20of%20world%20models%20with%20crucial%20safety-focused%20decision-making%0Acapabilities%2C%20enabling%20autonomous%20vehicles%20to%20navigate%20real-world%20complexities%0Asecurely.%20Our%20approach%20anticipates%20potential%20threats%20and%20formulates%20safer%0Aroutes%20by%20harnessing%20the%20predictive%20capabilities%20of%20world%20models%2C%20thus%0Asignificantly%20reducing%20the%20need%20for%20extensive%20real-world%20trial-and-error%0Alearning.%20Additionally%2C%20our%20method%20empowers%20vehicles%20to%20autonomously%20learn%20and%0Adevelop%20through%20continuous%20practice%2C%20ensuring%20the%20system%20evolves%20and%20adapts%20to%0Anew%20challenges.%20Incorporating%20radio%20frequency%20technology%2C%20NavSecure%20leverages%0A5G%20networks%20to%20enhance%20real-time%20data%20exchange%2C%20improving%20communication%20and%0Aresponsiveness.%20Validated%20through%20rigorous%20experiments%20under%20simulation-to-real%0Adriving%20conditions%2C%20NavSecure%20has%20shown%20exceptional%20performance%20in%0Asafety-critical%20scenarios%2C%20such%20as%20sudden%20obstacle%20avoidance.%20Results%20indicate%0Athat%20NavSecure%20excels%20in%20key%20safety%20metrics%2C%20including%20collision%20prevention%20and%0Arisk%20reduction%2C%20surpassing%20other%20end-to-end%20methodologies.%20This%20framework%20not%0Aonly%20advances%20autonomous%20driving%20safety%20but%20also%20demonstrates%20how%20world%20models%0Acan%20enhance%20decision-making%20in%20critical%20applications.%20NavSecure%20sets%20a%20new%0Astandard%20for%20developing%20more%20robust%20and%20trustworthy%20autonomous%20driving%20systems%2C%0Acapable%20of%20handling%20the%20inherent%20dynamics%20and%20uncertainties%20of%20real-world%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15042v2&entry.124074799=Read"},
{"title": "CapHDR2IR: Caption-Driven Transfer from Visible Light to Infrared Domain", "author": "Jingchao Peng and Thomas Bashford-Rogers and Zhuang Shao and Haitao Zhao and Aru Ranjan Singh and Abhishek Goswami and Kurt Debattista", "abstract": "  Infrared (IR) imaging offers advantages in several fields due to its unique\nability of capturing content in extreme light conditions. However, the\ndemanding hardware requirements of high-resolution IR sensors limit its\nwidespread application. As an alternative, visible light can be used to\nsynthesize IR images but this causes a loss of fidelity in image details and\nintroduces inconsistencies due to lack of contextual awareness of the scene.\nThis stems from a combination of using visible light with a standard dynamic\nrange, especially under extreme lighting, and a lack of contextual awareness\ncan result in pseudo-thermal-crossover artifacts. This occurs when multiple\nobjects with similar temperatures appear indistinguishable in the training\ndata, further exacerbating the loss of fidelity. To solve this challenge, this\npaper proposes CapHDR2IR, a novel framework incorporating vision-language\nmodels using high dynamic range (HDR) images as inputs to generate IR images.\nHDR images capture a wider range of luminance variations, ensuring reliable IR\nimage generation in different light conditions. Additionally, a dense caption\nbranch integrates semantic understanding, resulting in more meaningful and\ndiscernible IR outputs. Extensive experiments on the HDRT dataset show that the\nproposed CapHDR2IR achieves state-of-the-art performance compared with existing\ngeneral domain transfer methods and those tailored for visible-to-infrared\nimage translation.\n", "link": "http://arxiv.org/abs/2411.16327v1", "date": "2024-11-25", "relevancy": 2.1134, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CapHDR2IR%3A%20Caption-Driven%20Transfer%20from%20Visible%20Light%20to%20Infrared%20Domain&body=Title%3A%20CapHDR2IR%3A%20Caption-Driven%20Transfer%20from%20Visible%20Light%20to%20Infrared%20Domain%0AAuthor%3A%20Jingchao%20Peng%20and%20Thomas%20Bashford-Rogers%20and%20Zhuang%20Shao%20and%20Haitao%20Zhao%20and%20Aru%20Ranjan%20Singh%20and%20Abhishek%20Goswami%20and%20Kurt%20Debattista%0AAbstract%3A%20%20%20Infrared%20%28IR%29%20imaging%20offers%20advantages%20in%20several%20fields%20due%20to%20its%20unique%0Aability%20of%20capturing%20content%20in%20extreme%20light%20conditions.%20However%2C%20the%0Ademanding%20hardware%20requirements%20of%20high-resolution%20IR%20sensors%20limit%20its%0Awidespread%20application.%20As%20an%20alternative%2C%20visible%20light%20can%20be%20used%20to%0Asynthesize%20IR%20images%20but%20this%20causes%20a%20loss%20of%20fidelity%20in%20image%20details%20and%0Aintroduces%20inconsistencies%20due%20to%20lack%20of%20contextual%20awareness%20of%20the%20scene.%0AThis%20stems%20from%20a%20combination%20of%20using%20visible%20light%20with%20a%20standard%20dynamic%0Arange%2C%20especially%20under%20extreme%20lighting%2C%20and%20a%20lack%20of%20contextual%20awareness%0Acan%20result%20in%20pseudo-thermal-crossover%20artifacts.%20This%20occurs%20when%20multiple%0Aobjects%20with%20similar%20temperatures%20appear%20indistinguishable%20in%20the%20training%0Adata%2C%20further%20exacerbating%20the%20loss%20of%20fidelity.%20To%20solve%20this%20challenge%2C%20this%0Apaper%20proposes%20CapHDR2IR%2C%20a%20novel%20framework%20incorporating%20vision-language%0Amodels%20using%20high%20dynamic%20range%20%28HDR%29%20images%20as%20inputs%20to%20generate%20IR%20images.%0AHDR%20images%20capture%20a%20wider%20range%20of%20luminance%20variations%2C%20ensuring%20reliable%20IR%0Aimage%20generation%20in%20different%20light%20conditions.%20Additionally%2C%20a%20dense%20caption%0Abranch%20integrates%20semantic%20understanding%2C%20resulting%20in%20more%20meaningful%20and%0Adiscernible%20IR%20outputs.%20Extensive%20experiments%20on%20the%20HDRT%20dataset%20show%20that%20the%0Aproposed%20CapHDR2IR%20achieves%20state-of-the-art%20performance%20compared%20with%20existing%0Ageneral%20domain%20transfer%20methods%20and%20those%20tailored%20for%20visible-to-infrared%0Aimage%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapHDR2IR%253A%2520Caption-Driven%2520Transfer%2520from%2520Visible%2520Light%2520to%2520Infrared%2520Domain%26entry.906535625%3DJingchao%2520Peng%2520and%2520Thomas%2520Bashford-Rogers%2520and%2520Zhuang%2520Shao%2520and%2520Haitao%2520Zhao%2520and%2520Aru%2520Ranjan%2520Singh%2520and%2520Abhishek%2520Goswami%2520and%2520Kurt%2520Debattista%26entry.1292438233%3D%2520%2520Infrared%2520%2528IR%2529%2520imaging%2520offers%2520advantages%2520in%2520several%2520fields%2520due%2520to%2520its%2520unique%250Aability%2520of%2520capturing%2520content%2520in%2520extreme%2520light%2520conditions.%2520However%252C%2520the%250Ademanding%2520hardware%2520requirements%2520of%2520high-resolution%2520IR%2520sensors%2520limit%2520its%250Awidespread%2520application.%2520As%2520an%2520alternative%252C%2520visible%2520light%2520can%2520be%2520used%2520to%250Asynthesize%2520IR%2520images%2520but%2520this%2520causes%2520a%2520loss%2520of%2520fidelity%2520in%2520image%2520details%2520and%250Aintroduces%2520inconsistencies%2520due%2520to%2520lack%2520of%2520contextual%2520awareness%2520of%2520the%2520scene.%250AThis%2520stems%2520from%2520a%2520combination%2520of%2520using%2520visible%2520light%2520with%2520a%2520standard%2520dynamic%250Arange%252C%2520especially%2520under%2520extreme%2520lighting%252C%2520and%2520a%2520lack%2520of%2520contextual%2520awareness%250Acan%2520result%2520in%2520pseudo-thermal-crossover%2520artifacts.%2520This%2520occurs%2520when%2520multiple%250Aobjects%2520with%2520similar%2520temperatures%2520appear%2520indistinguishable%2520in%2520the%2520training%250Adata%252C%2520further%2520exacerbating%2520the%2520loss%2520of%2520fidelity.%2520To%2520solve%2520this%2520challenge%252C%2520this%250Apaper%2520proposes%2520CapHDR2IR%252C%2520a%2520novel%2520framework%2520incorporating%2520vision-language%250Amodels%2520using%2520high%2520dynamic%2520range%2520%2528HDR%2529%2520images%2520as%2520inputs%2520to%2520generate%2520IR%2520images.%250AHDR%2520images%2520capture%2520a%2520wider%2520range%2520of%2520luminance%2520variations%252C%2520ensuring%2520reliable%2520IR%250Aimage%2520generation%2520in%2520different%2520light%2520conditions.%2520Additionally%252C%2520a%2520dense%2520caption%250Abranch%2520integrates%2520semantic%2520understanding%252C%2520resulting%2520in%2520more%2520meaningful%2520and%250Adiscernible%2520IR%2520outputs.%2520Extensive%2520experiments%2520on%2520the%2520HDRT%2520dataset%2520show%2520that%2520the%250Aproposed%2520CapHDR2IR%2520achieves%2520state-of-the-art%2520performance%2520compared%2520with%2520existing%250Ageneral%2520domain%2520transfer%2520methods%2520and%2520those%2520tailored%2520for%2520visible-to-infrared%250Aimage%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CapHDR2IR%3A%20Caption-Driven%20Transfer%20from%20Visible%20Light%20to%20Infrared%20Domain&entry.906535625=Jingchao%20Peng%20and%20Thomas%20Bashford-Rogers%20and%20Zhuang%20Shao%20and%20Haitao%20Zhao%20and%20Aru%20Ranjan%20Singh%20and%20Abhishek%20Goswami%20and%20Kurt%20Debattista&entry.1292438233=%20%20Infrared%20%28IR%29%20imaging%20offers%20advantages%20in%20several%20fields%20due%20to%20its%20unique%0Aability%20of%20capturing%20content%20in%20extreme%20light%20conditions.%20However%2C%20the%0Ademanding%20hardware%20requirements%20of%20high-resolution%20IR%20sensors%20limit%20its%0Awidespread%20application.%20As%20an%20alternative%2C%20visible%20light%20can%20be%20used%20to%0Asynthesize%20IR%20images%20but%20this%20causes%20a%20loss%20of%20fidelity%20in%20image%20details%20and%0Aintroduces%20inconsistencies%20due%20to%20lack%20of%20contextual%20awareness%20of%20the%20scene.%0AThis%20stems%20from%20a%20combination%20of%20using%20visible%20light%20with%20a%20standard%20dynamic%0Arange%2C%20especially%20under%20extreme%20lighting%2C%20and%20a%20lack%20of%20contextual%20awareness%0Acan%20result%20in%20pseudo-thermal-crossover%20artifacts.%20This%20occurs%20when%20multiple%0Aobjects%20with%20similar%20temperatures%20appear%20indistinguishable%20in%20the%20training%0Adata%2C%20further%20exacerbating%20the%20loss%20of%20fidelity.%20To%20solve%20this%20challenge%2C%20this%0Apaper%20proposes%20CapHDR2IR%2C%20a%20novel%20framework%20incorporating%20vision-language%0Amodels%20using%20high%20dynamic%20range%20%28HDR%29%20images%20as%20inputs%20to%20generate%20IR%20images.%0AHDR%20images%20capture%20a%20wider%20range%20of%20luminance%20variations%2C%20ensuring%20reliable%20IR%0Aimage%20generation%20in%20different%20light%20conditions.%20Additionally%2C%20a%20dense%20caption%0Abranch%20integrates%20semantic%20understanding%2C%20resulting%20in%20more%20meaningful%20and%0Adiscernible%20IR%20outputs.%20Extensive%20experiments%20on%20the%20HDRT%20dataset%20show%20that%20the%0Aproposed%20CapHDR2IR%20achieves%20state-of-the-art%20performance%20compared%20with%20existing%0Ageneral%20domain%20transfer%20methods%20and%20those%20tailored%20for%20visible-to-infrared%0Aimage%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16327v1&entry.124074799=Read"},
{"title": "Generating Out-Of-Distribution Scenarios Using Language Models", "author": "Erfan Aasi and Phat Nguyen and Shiva Sreeram and Guy Rosman and Sertac Karaman and Daniela Rus", "abstract": "  The deployment of autonomous vehicles controlled by machine learning\ntechniques requires extensive testing in diverse real-world environments,\nrobust handling of edge cases and out-of-distribution scenarios, and\ncomprehensive safety validation to ensure that these systems can navigate\nsafely and effectively under unpredictable conditions. Addressing\nOut-Of-Distribution (OOD) driving scenarios is essential for enhancing safety,\nas OOD scenarios help validate the reliability of the models within the\nvehicle's autonomy stack. However, generating OOD scenarios is challenging due\nto their long-tailed distribution and rarity in urban driving dataset.\nRecently, Large Language Models (LLMs) have shown promise in autonomous\ndriving, particularly for their zero-shot generalization and common-sense\nreasoning capabilities. In this paper, we leverage these LLM strengths to\nintroduce a framework for generating diverse OOD driving scenarios. Our\napproach uses LLMs to construct a branching tree, where each branch represents\na unique OOD scenario. These scenarios are then simulated in the CARLA\nsimulator using an automated framework that aligns scene augmentation with the\ncorresponding textual descriptions. We evaluate our framework through extensive\nsimulations, and assess its performance via a diversity metric that measures\nthe richness of the scenarios. Additionally, we introduce a new \"OOD-ness\"\nmetric, which quantifies how much the generated scenarios deviate from typical\nurban driving conditions. Furthermore, we explore the capacity of modern\nVision-Language Models (VLMs) to interpret and safely navigate through the\nsimulated OOD scenarios. Our findings offer valuable insights into the\nreliability of language models in addressing OOD scenarios within the context\nof urban driving.\n", "link": "http://arxiv.org/abs/2411.16554v1", "date": "2024-11-25", "relevancy": 2.1081, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Out-Of-Distribution%20Scenarios%20Using%20Language%20Models&body=Title%3A%20Generating%20Out-Of-Distribution%20Scenarios%20Using%20Language%20Models%0AAuthor%3A%20Erfan%20Aasi%20and%20Phat%20Nguyen%20and%20Shiva%20Sreeram%20and%20Guy%20Rosman%20and%20Sertac%20Karaman%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20The%20deployment%20of%20autonomous%20vehicles%20controlled%20by%20machine%20learning%0Atechniques%20requires%20extensive%20testing%20in%20diverse%20real-world%20environments%2C%0Arobust%20handling%20of%20edge%20cases%20and%20out-of-distribution%20scenarios%2C%20and%0Acomprehensive%20safety%20validation%20to%20ensure%20that%20these%20systems%20can%20navigate%0Asafely%20and%20effectively%20under%20unpredictable%20conditions.%20Addressing%0AOut-Of-Distribution%20%28OOD%29%20driving%20scenarios%20is%20essential%20for%20enhancing%20safety%2C%0Aas%20OOD%20scenarios%20help%20validate%20the%20reliability%20of%20the%20models%20within%20the%0Avehicle%27s%20autonomy%20stack.%20However%2C%20generating%20OOD%20scenarios%20is%20challenging%20due%0Ato%20their%20long-tailed%20distribution%20and%20rarity%20in%20urban%20driving%20dataset.%0ARecently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20promise%20in%20autonomous%0Adriving%2C%20particularly%20for%20their%20zero-shot%20generalization%20and%20common-sense%0Areasoning%20capabilities.%20In%20this%20paper%2C%20we%20leverage%20these%20LLM%20strengths%20to%0Aintroduce%20a%20framework%20for%20generating%20diverse%20OOD%20driving%20scenarios.%20Our%0Aapproach%20uses%20LLMs%20to%20construct%20a%20branching%20tree%2C%20where%20each%20branch%20represents%0Aa%20unique%20OOD%20scenario.%20These%20scenarios%20are%20then%20simulated%20in%20the%20CARLA%0Asimulator%20using%20an%20automated%20framework%20that%20aligns%20scene%20augmentation%20with%20the%0Acorresponding%20textual%20descriptions.%20We%20evaluate%20our%20framework%20through%20extensive%0Asimulations%2C%20and%20assess%20its%20performance%20via%20a%20diversity%20metric%20that%20measures%0Athe%20richness%20of%20the%20scenarios.%20Additionally%2C%20we%20introduce%20a%20new%20%22OOD-ness%22%0Ametric%2C%20which%20quantifies%20how%20much%20the%20generated%20scenarios%20deviate%20from%20typical%0Aurban%20driving%20conditions.%20Furthermore%2C%20we%20explore%20the%20capacity%20of%20modern%0AVision-Language%20Models%20%28VLMs%29%20to%20interpret%20and%20safely%20navigate%20through%20the%0Asimulated%20OOD%20scenarios.%20Our%20findings%20offer%20valuable%20insights%20into%20the%0Areliability%20of%20language%20models%20in%20addressing%20OOD%20scenarios%20within%20the%20context%0Aof%20urban%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Out-Of-Distribution%2520Scenarios%2520Using%2520Language%2520Models%26entry.906535625%3DErfan%2520Aasi%2520and%2520Phat%2520Nguyen%2520and%2520Shiva%2520Sreeram%2520and%2520Guy%2520Rosman%2520and%2520Sertac%2520Karaman%2520and%2520Daniela%2520Rus%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520autonomous%2520vehicles%2520controlled%2520by%2520machine%2520learning%250Atechniques%2520requires%2520extensive%2520testing%2520in%2520diverse%2520real-world%2520environments%252C%250Arobust%2520handling%2520of%2520edge%2520cases%2520and%2520out-of-distribution%2520scenarios%252C%2520and%250Acomprehensive%2520safety%2520validation%2520to%2520ensure%2520that%2520these%2520systems%2520can%2520navigate%250Asafely%2520and%2520effectively%2520under%2520unpredictable%2520conditions.%2520Addressing%250AOut-Of-Distribution%2520%2528OOD%2529%2520driving%2520scenarios%2520is%2520essential%2520for%2520enhancing%2520safety%252C%250Aas%2520OOD%2520scenarios%2520help%2520validate%2520the%2520reliability%2520of%2520the%2520models%2520within%2520the%250Avehicle%2527s%2520autonomy%2520stack.%2520However%252C%2520generating%2520OOD%2520scenarios%2520is%2520challenging%2520due%250Ato%2520their%2520long-tailed%2520distribution%2520and%2520rarity%2520in%2520urban%2520driving%2520dataset.%250ARecently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520in%2520autonomous%250Adriving%252C%2520particularly%2520for%2520their%2520zero-shot%2520generalization%2520and%2520common-sense%250Areasoning%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520these%2520LLM%2520strengths%2520to%250Aintroduce%2520a%2520framework%2520for%2520generating%2520diverse%2520OOD%2520driving%2520scenarios.%2520Our%250Aapproach%2520uses%2520LLMs%2520to%2520construct%2520a%2520branching%2520tree%252C%2520where%2520each%2520branch%2520represents%250Aa%2520unique%2520OOD%2520scenario.%2520These%2520scenarios%2520are%2520then%2520simulated%2520in%2520the%2520CARLA%250Asimulator%2520using%2520an%2520automated%2520framework%2520that%2520aligns%2520scene%2520augmentation%2520with%2520the%250Acorresponding%2520textual%2520descriptions.%2520We%2520evaluate%2520our%2520framework%2520through%2520extensive%250Asimulations%252C%2520and%2520assess%2520its%2520performance%2520via%2520a%2520diversity%2520metric%2520that%2520measures%250Athe%2520richness%2520of%2520the%2520scenarios.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%2520%2522OOD-ness%2522%250Ametric%252C%2520which%2520quantifies%2520how%2520much%2520the%2520generated%2520scenarios%2520deviate%2520from%2520typical%250Aurban%2520driving%2520conditions.%2520Furthermore%252C%2520we%2520explore%2520the%2520capacity%2520of%2520modern%250AVision-Language%2520Models%2520%2528VLMs%2529%2520to%2520interpret%2520and%2520safely%2520navigate%2520through%2520the%250Asimulated%2520OOD%2520scenarios.%2520Our%2520findings%2520offer%2520valuable%2520insights%2520into%2520the%250Areliability%2520of%2520language%2520models%2520in%2520addressing%2520OOD%2520scenarios%2520within%2520the%2520context%250Aof%2520urban%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Out-Of-Distribution%20Scenarios%20Using%20Language%20Models&entry.906535625=Erfan%20Aasi%20and%20Phat%20Nguyen%20and%20Shiva%20Sreeram%20and%20Guy%20Rosman%20and%20Sertac%20Karaman%20and%20Daniela%20Rus&entry.1292438233=%20%20The%20deployment%20of%20autonomous%20vehicles%20controlled%20by%20machine%20learning%0Atechniques%20requires%20extensive%20testing%20in%20diverse%20real-world%20environments%2C%0Arobust%20handling%20of%20edge%20cases%20and%20out-of-distribution%20scenarios%2C%20and%0Acomprehensive%20safety%20validation%20to%20ensure%20that%20these%20systems%20can%20navigate%0Asafely%20and%20effectively%20under%20unpredictable%20conditions.%20Addressing%0AOut-Of-Distribution%20%28OOD%29%20driving%20scenarios%20is%20essential%20for%20enhancing%20safety%2C%0Aas%20OOD%20scenarios%20help%20validate%20the%20reliability%20of%20the%20models%20within%20the%0Avehicle%27s%20autonomy%20stack.%20However%2C%20generating%20OOD%20scenarios%20is%20challenging%20due%0Ato%20their%20long-tailed%20distribution%20and%20rarity%20in%20urban%20driving%20dataset.%0ARecently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20promise%20in%20autonomous%0Adriving%2C%20particularly%20for%20their%20zero-shot%20generalization%20and%20common-sense%0Areasoning%20capabilities.%20In%20this%20paper%2C%20we%20leverage%20these%20LLM%20strengths%20to%0Aintroduce%20a%20framework%20for%20generating%20diverse%20OOD%20driving%20scenarios.%20Our%0Aapproach%20uses%20LLMs%20to%20construct%20a%20branching%20tree%2C%20where%20each%20branch%20represents%0Aa%20unique%20OOD%20scenario.%20These%20scenarios%20are%20then%20simulated%20in%20the%20CARLA%0Asimulator%20using%20an%20automated%20framework%20that%20aligns%20scene%20augmentation%20with%20the%0Acorresponding%20textual%20descriptions.%20We%20evaluate%20our%20framework%20through%20extensive%0Asimulations%2C%20and%20assess%20its%20performance%20via%20a%20diversity%20metric%20that%20measures%0Athe%20richness%20of%20the%20scenarios.%20Additionally%2C%20we%20introduce%20a%20new%20%22OOD-ness%22%0Ametric%2C%20which%20quantifies%20how%20much%20the%20generated%20scenarios%20deviate%20from%20typical%0Aurban%20driving%20conditions.%20Furthermore%2C%20we%20explore%20the%20capacity%20of%20modern%0AVision-Language%20Models%20%28VLMs%29%20to%20interpret%20and%20safely%20navigate%20through%20the%0Asimulated%20OOD%20scenarios.%20Our%20findings%20offer%20valuable%20insights%20into%20the%0Areliability%20of%20language%20models%20in%20addressing%20OOD%20scenarios%20within%20the%20context%0Aof%20urban%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16554v1&entry.124074799=Read"},
{"title": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion", "author": "Jiacheng Ruan and Yebin Yang and Zehao Lin and Yuchen Feng and Feiyu Xiong and Zeyun Tang and Zhiyu Li", "abstract": "  Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.\n", "link": "http://arxiv.org/abs/2410.12564v2", "date": "2024-11-25", "relevancy": 2.1069, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FTII-Bench%3A%20A%20Comprehensive%20Multimodal%20Benchmark%20for%20Flow%20Text%20with%0A%20%20Image%20Insertion&body=Title%3A%20FTII-Bench%3A%20A%20Comprehensive%20Multimodal%20Benchmark%20for%20Flow%20Text%20with%0A%20%20Image%20Insertion%0AAuthor%3A%20Jiacheng%20Ruan%20and%20Yebin%20Yang%20and%20Zehao%20Lin%20and%20Yuchen%20Feng%20and%20Feiyu%20Xiong%20and%20Zeyun%20Tang%20and%20Zhiyu%20Li%0AAbstract%3A%20%20%20Benefiting%20from%20the%20revolutionary%20advances%20in%20large%20language%20models%20%28LLMs%29%0Aand%20foundational%20vision%20models%2C%20large%20vision-language%20models%20%28LVLMs%29%20have%20also%0Amade%20significant%20progress.%20However%2C%20current%20benchmarks%20focus%20on%20tasks%20that%0Aevaluating%20only%20a%20single%20aspect%20of%20LVLM%20capabilities%20%28e.g.%2C%20recognition%2C%0Adetection%2C%20understanding%29.%20These%20tasks%20fail%20to%20fully%20demonstrate%20LVLMs%27%0Apotential%20in%20complex%20application%20scenarios.%20To%20comprehensively%20assess%20the%0Aperformance%20of%20existing%20LVLMs%2C%20we%20propose%20a%20more%20challenging%20task%20called%20the%0AFlow%20Text%20with%20Image%20Insertion%20task%20%28FTII%29.%20This%20task%20requires%20LVLMs%20to%0Asimultaneously%20possess%20outstanding%20abilities%20in%20image%20comprehension%2C%0Ainstruction%20understanding%2C%20and%20long-text%20interpretation.%20Specifically%2C%20given%0Aseveral%20text%20paragraphs%20and%20a%20set%20of%20candidate%20images%2C%20as%20the%20text%20paragraphs%0Aaccumulate%2C%20the%20LVLMs%20are%20required%20to%20select%20the%20most%20suitable%20image%20from%20the%0Acandidates%20to%20insert%20after%20the%20corresponding%20paragraph.%20Constructing%20a%0Abenchmark%20for%20such%20a%20task%20is%20highly%20challenging%2C%20particularly%20in%20determining%0Athe%20sequence%20of%20flowing%20text%20and%20images.%20To%20address%20this%20challenge%2C%20we%20turn%20to%0Aprofessional%20news%20reports%2C%20which%20naturally%20contain%20a%20gold%20standard%20for%0Aimage-text%20sequences.%20Based%20on%20this%2C%20we%20introduce%20the%20Flow%20Text%20with%20Image%0AInsertion%20Benchmark%20%28FTII-Bench%29%2C%20which%20includes%20318%20high-quality%20Chinese%0Aimage-text%20news%20articles%20and%20307%20high-quality%20English%20image-text%20news%20articles%2C%0Acovering%2010%20different%20news%20domains.%20Using%20these%20625%20high-quality%20articles%2C%20we%0Aconstruct%20problems%20of%20two%20different%20types%20with%20multiple%20levels%20of%20difficulty.%0AFurthermore%2C%20we%20establish%20two%20different%20evaluation%20pipelines%20based%20on%20the%20CLIP%0Amodel%20and%20existing%20LVLMs.%20We%20evaluate%209%20open-source%20and%202%20closed-source%20LVLMs%0Aas%20well%20as%202%20CLIP-based%20models.%20Results%20indicate%20that%20even%20the%20most%20advanced%0Amodels%20%28e.g.%2C%20GPT-4o%29%20face%20significant%20challenges%20when%20tackling%20the%20FTII%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12564v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFTII-Bench%253A%2520A%2520Comprehensive%2520Multimodal%2520Benchmark%2520for%2520Flow%2520Text%2520with%250A%2520%2520Image%2520Insertion%26entry.906535625%3DJiacheng%2520Ruan%2520and%2520Yebin%2520Yang%2520and%2520Zehao%2520Lin%2520and%2520Yuchen%2520Feng%2520and%2520Feiyu%2520Xiong%2520and%2520Zeyun%2520Tang%2520and%2520Zhiyu%2520Li%26entry.1292438233%3D%2520%2520Benefiting%2520from%2520the%2520revolutionary%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%250Aand%2520foundational%2520vision%2520models%252C%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520also%250Amade%2520significant%2520progress.%2520However%252C%2520current%2520benchmarks%2520focus%2520on%2520tasks%2520that%250Aevaluating%2520only%2520a%2520single%2520aspect%2520of%2520LVLM%2520capabilities%2520%2528e.g.%252C%2520recognition%252C%250Adetection%252C%2520understanding%2529.%2520These%2520tasks%2520fail%2520to%2520fully%2520demonstrate%2520LVLMs%2527%250Apotential%2520in%2520complex%2520application%2520scenarios.%2520To%2520comprehensively%2520assess%2520the%250Aperformance%2520of%2520existing%2520LVLMs%252C%2520we%2520propose%2520a%2520more%2520challenging%2520task%2520called%2520the%250AFlow%2520Text%2520with%2520Image%2520Insertion%2520task%2520%2528FTII%2529.%2520This%2520task%2520requires%2520LVLMs%2520to%250Asimultaneously%2520possess%2520outstanding%2520abilities%2520in%2520image%2520comprehension%252C%250Ainstruction%2520understanding%252C%2520and%2520long-text%2520interpretation.%2520Specifically%252C%2520given%250Aseveral%2520text%2520paragraphs%2520and%2520a%2520set%2520of%2520candidate%2520images%252C%2520as%2520the%2520text%2520paragraphs%250Aaccumulate%252C%2520the%2520LVLMs%2520are%2520required%2520to%2520select%2520the%2520most%2520suitable%2520image%2520from%2520the%250Acandidates%2520to%2520insert%2520after%2520the%2520corresponding%2520paragraph.%2520Constructing%2520a%250Abenchmark%2520for%2520such%2520a%2520task%2520is%2520highly%2520challenging%252C%2520particularly%2520in%2520determining%250Athe%2520sequence%2520of%2520flowing%2520text%2520and%2520images.%2520To%2520address%2520this%2520challenge%252C%2520we%2520turn%2520to%250Aprofessional%2520news%2520reports%252C%2520which%2520naturally%2520contain%2520a%2520gold%2520standard%2520for%250Aimage-text%2520sequences.%2520Based%2520on%2520this%252C%2520we%2520introduce%2520the%2520Flow%2520Text%2520with%2520Image%250AInsertion%2520Benchmark%2520%2528FTII-Bench%2529%252C%2520which%2520includes%2520318%2520high-quality%2520Chinese%250Aimage-text%2520news%2520articles%2520and%2520307%2520high-quality%2520English%2520image-text%2520news%2520articles%252C%250Acovering%252010%2520different%2520news%2520domains.%2520Using%2520these%2520625%2520high-quality%2520articles%252C%2520we%250Aconstruct%2520problems%2520of%2520two%2520different%2520types%2520with%2520multiple%2520levels%2520of%2520difficulty.%250AFurthermore%252C%2520we%2520establish%2520two%2520different%2520evaluation%2520pipelines%2520based%2520on%2520the%2520CLIP%250Amodel%2520and%2520existing%2520LVLMs.%2520We%2520evaluate%25209%2520open-source%2520and%25202%2520closed-source%2520LVLMs%250Aas%2520well%2520as%25202%2520CLIP-based%2520models.%2520Results%2520indicate%2520that%2520even%2520the%2520most%2520advanced%250Amodels%2520%2528e.g.%252C%2520GPT-4o%2529%2520face%2520significant%2520challenges%2520when%2520tackling%2520the%2520FTII%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12564v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FTII-Bench%3A%20A%20Comprehensive%20Multimodal%20Benchmark%20for%20Flow%20Text%20with%0A%20%20Image%20Insertion&entry.906535625=Jiacheng%20Ruan%20and%20Yebin%20Yang%20and%20Zehao%20Lin%20and%20Yuchen%20Feng%20and%20Feiyu%20Xiong%20and%20Zeyun%20Tang%20and%20Zhiyu%20Li&entry.1292438233=%20%20Benefiting%20from%20the%20revolutionary%20advances%20in%20large%20language%20models%20%28LLMs%29%0Aand%20foundational%20vision%20models%2C%20large%20vision-language%20models%20%28LVLMs%29%20have%20also%0Amade%20significant%20progress.%20However%2C%20current%20benchmarks%20focus%20on%20tasks%20that%0Aevaluating%20only%20a%20single%20aspect%20of%20LVLM%20capabilities%20%28e.g.%2C%20recognition%2C%0Adetection%2C%20understanding%29.%20These%20tasks%20fail%20to%20fully%20demonstrate%20LVLMs%27%0Apotential%20in%20complex%20application%20scenarios.%20To%20comprehensively%20assess%20the%0Aperformance%20of%20existing%20LVLMs%2C%20we%20propose%20a%20more%20challenging%20task%20called%20the%0AFlow%20Text%20with%20Image%20Insertion%20task%20%28FTII%29.%20This%20task%20requires%20LVLMs%20to%0Asimultaneously%20possess%20outstanding%20abilities%20in%20image%20comprehension%2C%0Ainstruction%20understanding%2C%20and%20long-text%20interpretation.%20Specifically%2C%20given%0Aseveral%20text%20paragraphs%20and%20a%20set%20of%20candidate%20images%2C%20as%20the%20text%20paragraphs%0Aaccumulate%2C%20the%20LVLMs%20are%20required%20to%20select%20the%20most%20suitable%20image%20from%20the%0Acandidates%20to%20insert%20after%20the%20corresponding%20paragraph.%20Constructing%20a%0Abenchmark%20for%20such%20a%20task%20is%20highly%20challenging%2C%20particularly%20in%20determining%0Athe%20sequence%20of%20flowing%20text%20and%20images.%20To%20address%20this%20challenge%2C%20we%20turn%20to%0Aprofessional%20news%20reports%2C%20which%20naturally%20contain%20a%20gold%20standard%20for%0Aimage-text%20sequences.%20Based%20on%20this%2C%20we%20introduce%20the%20Flow%20Text%20with%20Image%0AInsertion%20Benchmark%20%28FTII-Bench%29%2C%20which%20includes%20318%20high-quality%20Chinese%0Aimage-text%20news%20articles%20and%20307%20high-quality%20English%20image-text%20news%20articles%2C%0Acovering%2010%20different%20news%20domains.%20Using%20these%20625%20high-quality%20articles%2C%20we%0Aconstruct%20problems%20of%20two%20different%20types%20with%20multiple%20levels%20of%20difficulty.%0AFurthermore%2C%20we%20establish%20two%20different%20evaluation%20pipelines%20based%20on%20the%20CLIP%0Amodel%20and%20existing%20LVLMs.%20We%20evaluate%209%20open-source%20and%202%20closed-source%20LVLMs%0Aas%20well%20as%202%20CLIP-based%20models.%20Results%20indicate%20that%20even%20the%20most%20advanced%0Amodels%20%28e.g.%2C%20GPT-4o%29%20face%20significant%20challenges%20when%20tackling%20the%20FTII%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12564v2&entry.124074799=Read"},
{"title": "Human-Calibrated Automated Testing and Validation of Generative Language\n  Models", "author": "Agus Sudjianto and Aijun Zhang and Srinivas Neppalli and Tarun Joshi and Michal Malohlava", "abstract": "  This paper introduces a comprehensive framework for the evaluation and\nvalidation of generative language models (GLMs), with a focus on\nRetrieval-Augmented Generation (RAG) systems deployed in high-stakes domains\nsuch as banking. GLM evaluation is challenging due to open-ended outputs and\nsubjective quality assessments. Leveraging the structured nature of RAG\nsystems, where generated responses are grounded in a predefined document\ncollection, we propose the Human-Calibrated Automated Testing (HCAT) framework.\nHCAT integrates a) automated test generation using stratified sampling, b)\nembedding-based metrics for explainable assessment of functionality, risk and\nsafety attributes, and c) a two-stage calibration approach that aligns\nmachine-generated evaluations with human judgments through probability\ncalibration and conformal prediction.\n  In addition, the framework includes robustness testing to evaluate model\nperformance against adversarial, out-of-distribution, and varied input\nconditions, as well as targeted weakness identification using marginal and\nbivariate analysis to pinpoint specific areas for improvement. This\nhuman-calibrated, multi-layered evaluation framework offers a scalable,\ntransparent, and interpretable approach to GLM assessment, providing a\npractical and reliable solution for deploying GLMs in applications where\naccuracy, transparency, and regulatory compliance are paramount.\n", "link": "http://arxiv.org/abs/2411.16391v1", "date": "2024-11-25", "relevancy": 2.1052, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5328}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5294}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Calibrated%20Automated%20Testing%20and%20Validation%20of%20Generative%20Language%0A%20%20Models&body=Title%3A%20Human-Calibrated%20Automated%20Testing%20and%20Validation%20of%20Generative%20Language%0A%20%20Models%0AAuthor%3A%20Agus%20Sudjianto%20and%20Aijun%20Zhang%20and%20Srinivas%20Neppalli%20and%20Tarun%20Joshi%20and%20Michal%20Malohlava%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20comprehensive%20framework%20for%20the%20evaluation%20and%0Avalidation%20of%20generative%20language%20models%20%28GLMs%29%2C%20with%20a%20focus%20on%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems%20deployed%20in%20high-stakes%20domains%0Asuch%20as%20banking.%20GLM%20evaluation%20is%20challenging%20due%20to%20open-ended%20outputs%20and%0Asubjective%20quality%20assessments.%20Leveraging%20the%20structured%20nature%20of%20RAG%0Asystems%2C%20where%20generated%20responses%20are%20grounded%20in%20a%20predefined%20document%0Acollection%2C%20we%20propose%20the%20Human-Calibrated%20Automated%20Testing%20%28HCAT%29%20framework.%0AHCAT%20integrates%20a%29%20automated%20test%20generation%20using%20stratified%20sampling%2C%20b%29%0Aembedding-based%20metrics%20for%20explainable%20assessment%20of%20functionality%2C%20risk%20and%0Asafety%20attributes%2C%20and%20c%29%20a%20two-stage%20calibration%20approach%20that%20aligns%0Amachine-generated%20evaluations%20with%20human%20judgments%20through%20probability%0Acalibration%20and%20conformal%20prediction.%0A%20%20In%20addition%2C%20the%20framework%20includes%20robustness%20testing%20to%20evaluate%20model%0Aperformance%20against%20adversarial%2C%20out-of-distribution%2C%20and%20varied%20input%0Aconditions%2C%20as%20well%20as%20targeted%20weakness%20identification%20using%20marginal%20and%0Abivariate%20analysis%20to%20pinpoint%20specific%20areas%20for%20improvement.%20This%0Ahuman-calibrated%2C%20multi-layered%20evaluation%20framework%20offers%20a%20scalable%2C%0Atransparent%2C%20and%20interpretable%20approach%20to%20GLM%20assessment%2C%20providing%20a%0Apractical%20and%20reliable%20solution%20for%20deploying%20GLMs%20in%20applications%20where%0Aaccuracy%2C%20transparency%2C%20and%20regulatory%20compliance%20are%20paramount.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Calibrated%2520Automated%2520Testing%2520and%2520Validation%2520of%2520Generative%2520Language%250A%2520%2520Models%26entry.906535625%3DAgus%2520Sudjianto%2520and%2520Aijun%2520Zhang%2520and%2520Srinivas%2520Neppalli%2520and%2520Tarun%2520Joshi%2520and%2520Michal%2520Malohlava%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520comprehensive%2520framework%2520for%2520the%2520evaluation%2520and%250Avalidation%2520of%2520generative%2520language%2520models%2520%2528GLMs%2529%252C%2520with%2520a%2520focus%2520on%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520deployed%2520in%2520high-stakes%2520domains%250Asuch%2520as%2520banking.%2520GLM%2520evaluation%2520is%2520challenging%2520due%2520to%2520open-ended%2520outputs%2520and%250Asubjective%2520quality%2520assessments.%2520Leveraging%2520the%2520structured%2520nature%2520of%2520RAG%250Asystems%252C%2520where%2520generated%2520responses%2520are%2520grounded%2520in%2520a%2520predefined%2520document%250Acollection%252C%2520we%2520propose%2520the%2520Human-Calibrated%2520Automated%2520Testing%2520%2528HCAT%2529%2520framework.%250AHCAT%2520integrates%2520a%2529%2520automated%2520test%2520generation%2520using%2520stratified%2520sampling%252C%2520b%2529%250Aembedding-based%2520metrics%2520for%2520explainable%2520assessment%2520of%2520functionality%252C%2520risk%2520and%250Asafety%2520attributes%252C%2520and%2520c%2529%2520a%2520two-stage%2520calibration%2520approach%2520that%2520aligns%250Amachine-generated%2520evaluations%2520with%2520human%2520judgments%2520through%2520probability%250Acalibration%2520and%2520conformal%2520prediction.%250A%2520%2520In%2520addition%252C%2520the%2520framework%2520includes%2520robustness%2520testing%2520to%2520evaluate%2520model%250Aperformance%2520against%2520adversarial%252C%2520out-of-distribution%252C%2520and%2520varied%2520input%250Aconditions%252C%2520as%2520well%2520as%2520targeted%2520weakness%2520identification%2520using%2520marginal%2520and%250Abivariate%2520analysis%2520to%2520pinpoint%2520specific%2520areas%2520for%2520improvement.%2520This%250Ahuman-calibrated%252C%2520multi-layered%2520evaluation%2520framework%2520offers%2520a%2520scalable%252C%250Atransparent%252C%2520and%2520interpretable%2520approach%2520to%2520GLM%2520assessment%252C%2520providing%2520a%250Apractical%2520and%2520reliable%2520solution%2520for%2520deploying%2520GLMs%2520in%2520applications%2520where%250Aaccuracy%252C%2520transparency%252C%2520and%2520regulatory%2520compliance%2520are%2520paramount.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Calibrated%20Automated%20Testing%20and%20Validation%20of%20Generative%20Language%0A%20%20Models&entry.906535625=Agus%20Sudjianto%20and%20Aijun%20Zhang%20and%20Srinivas%20Neppalli%20and%20Tarun%20Joshi%20and%20Michal%20Malohlava&entry.1292438233=%20%20This%20paper%20introduces%20a%20comprehensive%20framework%20for%20the%20evaluation%20and%0Avalidation%20of%20generative%20language%20models%20%28GLMs%29%2C%20with%20a%20focus%20on%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems%20deployed%20in%20high-stakes%20domains%0Asuch%20as%20banking.%20GLM%20evaluation%20is%20challenging%20due%20to%20open-ended%20outputs%20and%0Asubjective%20quality%20assessments.%20Leveraging%20the%20structured%20nature%20of%20RAG%0Asystems%2C%20where%20generated%20responses%20are%20grounded%20in%20a%20predefined%20document%0Acollection%2C%20we%20propose%20the%20Human-Calibrated%20Automated%20Testing%20%28HCAT%29%20framework.%0AHCAT%20integrates%20a%29%20automated%20test%20generation%20using%20stratified%20sampling%2C%20b%29%0Aembedding-based%20metrics%20for%20explainable%20assessment%20of%20functionality%2C%20risk%20and%0Asafety%20attributes%2C%20and%20c%29%20a%20two-stage%20calibration%20approach%20that%20aligns%0Amachine-generated%20evaluations%20with%20human%20judgments%20through%20probability%0Acalibration%20and%20conformal%20prediction.%0A%20%20In%20addition%2C%20the%20framework%20includes%20robustness%20testing%20to%20evaluate%20model%0Aperformance%20against%20adversarial%2C%20out-of-distribution%2C%20and%20varied%20input%0Aconditions%2C%20as%20well%20as%20targeted%20weakness%20identification%20using%20marginal%20and%0Abivariate%20analysis%20to%20pinpoint%20specific%20areas%20for%20improvement.%20This%0Ahuman-calibrated%2C%20multi-layered%20evaluation%20framework%20offers%20a%20scalable%2C%0Atransparent%2C%20and%20interpretable%20approach%20to%20GLM%20assessment%2C%20providing%20a%0Apractical%20and%20reliable%20solution%20for%20deploying%20GLMs%20in%20applications%20where%0Aaccuracy%2C%20transparency%2C%20and%20regulatory%20compliance%20are%20paramount.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16391v1&entry.124074799=Read"},
{"title": "CoHD: A Counting-Aware Hierarchical Decoding Framework for Generalized\n  Referring Expression Segmentation", "author": "Zhuoyan Luo and Yinghao Wu and Tianheng Cheng and Yong Liu and Yicheng Xiao and Hongfa Wang and Xiao-Ping Zhang and Yujiu Yang", "abstract": "  The newly proposed Generalized Referring Expression Segmentation (GRES)\namplifies the formulation of classic RES by involving complex\nmultiple/non-target scenarios. Recent approaches address GRES by directly\nextending the well-adopted RES frameworks with object-existence identification.\nHowever, these approaches tend to encode multi-granularity object information\ninto a single representation, which makes it difficult to precisely represent\ncomprehensive objects of different granularity. Moreover, the simple binary\nobject-existence identification across all referent scenarios fails to specify\ntheir inherent differences, incurring ambiguity in object understanding. To\ntackle the above issues, we propose a \\textbf{Co}unting-Aware\n\\textbf{H}ierarchical \\textbf{D}ecoding framework (CoHD) for GRES. By\ndecoupling the intricate referring semantics into different granularity with a\nvisual-linguistic hierarchy, and dynamic aggregating it with intra- and\ninter-selection, CoHD boosts multi-granularity comprehension with the\nreciprocal benefit of the hierarchical nature. Furthermore, we incorporate the\ncounting ability by embodying multiple/single/non-target scenarios into count-\nand category-level supervision, facilitating comprehensive object perception.\nExperimental results on gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO benchmarks\ndemonstrate the effectiveness and rationality of CoHD which outperforms\nstate-of-the-art GRES methods by a remarkable margin. Code is available at\n\\href{https://github.com/RobertLuo1/CoHD}{here}.\n", "link": "http://arxiv.org/abs/2405.15658v2", "date": "2024-11-25", "relevancy": 2.101, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoHD%3A%20A%20Counting-Aware%20Hierarchical%20Decoding%20Framework%20for%20Generalized%0A%20%20Referring%20Expression%20Segmentation&body=Title%3A%20CoHD%3A%20A%20Counting-Aware%20Hierarchical%20Decoding%20Framework%20for%20Generalized%0A%20%20Referring%20Expression%20Segmentation%0AAuthor%3A%20Zhuoyan%20Luo%20and%20Yinghao%20Wu%20and%20Tianheng%20Cheng%20and%20Yong%20Liu%20and%20Yicheng%20Xiao%20and%20Hongfa%20Wang%20and%20Xiao-Ping%20Zhang%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20The%20newly%20proposed%20Generalized%20Referring%20Expression%20Segmentation%20%28GRES%29%0Aamplifies%20the%20formulation%20of%20classic%20RES%20by%20involving%20complex%0Amultiple/non-target%20scenarios.%20Recent%20approaches%20address%20GRES%20by%20directly%0Aextending%20the%20well-adopted%20RES%20frameworks%20with%20object-existence%20identification.%0AHowever%2C%20these%20approaches%20tend%20to%20encode%20multi-granularity%20object%20information%0Ainto%20a%20single%20representation%2C%20which%20makes%20it%20difficult%20to%20precisely%20represent%0Acomprehensive%20objects%20of%20different%20granularity.%20Moreover%2C%20the%20simple%20binary%0Aobject-existence%20identification%20across%20all%20referent%20scenarios%20fails%20to%20specify%0Atheir%20inherent%20differences%2C%20incurring%20ambiguity%20in%20object%20understanding.%20To%0Atackle%20the%20above%20issues%2C%20we%20propose%20a%20%5Ctextbf%7BCo%7Dunting-Aware%0A%5Ctextbf%7BH%7Dierarchical%20%5Ctextbf%7BD%7Decoding%20framework%20%28CoHD%29%20for%20GRES.%20By%0Adecoupling%20the%20intricate%20referring%20semantics%20into%20different%20granularity%20with%20a%0Avisual-linguistic%20hierarchy%2C%20and%20dynamic%20aggregating%20it%20with%20intra-%20and%0Ainter-selection%2C%20CoHD%20boosts%20multi-granularity%20comprehension%20with%20the%0Areciprocal%20benefit%20of%20the%20hierarchical%20nature.%20Furthermore%2C%20we%20incorporate%20the%0Acounting%20ability%20by%20embodying%20multiple/single/non-target%20scenarios%20into%20count-%0Aand%20category-level%20supervision%2C%20facilitating%20comprehensive%20object%20perception.%0AExperimental%20results%20on%20gRefCOCO%2C%20Ref-ZOM%2C%20R-RefCOCO%2C%20and%20RefCOCO%20benchmarks%0Ademonstrate%20the%20effectiveness%20and%20rationality%20of%20CoHD%20which%20outperforms%0Astate-of-the-art%20GRES%20methods%20by%20a%20remarkable%20margin.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/RobertLuo1/CoHD%7D%7Bhere%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15658v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoHD%253A%2520A%2520Counting-Aware%2520Hierarchical%2520Decoding%2520Framework%2520for%2520Generalized%250A%2520%2520Referring%2520Expression%2520Segmentation%26entry.906535625%3DZhuoyan%2520Luo%2520and%2520Yinghao%2520Wu%2520and%2520Tianheng%2520Cheng%2520and%2520Yong%2520Liu%2520and%2520Yicheng%2520Xiao%2520and%2520Hongfa%2520Wang%2520and%2520Xiao-Ping%2520Zhang%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520The%2520newly%2520proposed%2520Generalized%2520Referring%2520Expression%2520Segmentation%2520%2528GRES%2529%250Aamplifies%2520the%2520formulation%2520of%2520classic%2520RES%2520by%2520involving%2520complex%250Amultiple/non-target%2520scenarios.%2520Recent%2520approaches%2520address%2520GRES%2520by%2520directly%250Aextending%2520the%2520well-adopted%2520RES%2520frameworks%2520with%2520object-existence%2520identification.%250AHowever%252C%2520these%2520approaches%2520tend%2520to%2520encode%2520multi-granularity%2520object%2520information%250Ainto%2520a%2520single%2520representation%252C%2520which%2520makes%2520it%2520difficult%2520to%2520precisely%2520represent%250Acomprehensive%2520objects%2520of%2520different%2520granularity.%2520Moreover%252C%2520the%2520simple%2520binary%250Aobject-existence%2520identification%2520across%2520all%2520referent%2520scenarios%2520fails%2520to%2520specify%250Atheir%2520inherent%2520differences%252C%2520incurring%2520ambiguity%2520in%2520object%2520understanding.%2520To%250Atackle%2520the%2520above%2520issues%252C%2520we%2520propose%2520a%2520%255Ctextbf%257BCo%257Dunting-Aware%250A%255Ctextbf%257BH%257Dierarchical%2520%255Ctextbf%257BD%257Decoding%2520framework%2520%2528CoHD%2529%2520for%2520GRES.%2520By%250Adecoupling%2520the%2520intricate%2520referring%2520semantics%2520into%2520different%2520granularity%2520with%2520a%250Avisual-linguistic%2520hierarchy%252C%2520and%2520dynamic%2520aggregating%2520it%2520with%2520intra-%2520and%250Ainter-selection%252C%2520CoHD%2520boosts%2520multi-granularity%2520comprehension%2520with%2520the%250Areciprocal%2520benefit%2520of%2520the%2520hierarchical%2520nature.%2520Furthermore%252C%2520we%2520incorporate%2520the%250Acounting%2520ability%2520by%2520embodying%2520multiple/single/non-target%2520scenarios%2520into%2520count-%250Aand%2520category-level%2520supervision%252C%2520facilitating%2520comprehensive%2520object%2520perception.%250AExperimental%2520results%2520on%2520gRefCOCO%252C%2520Ref-ZOM%252C%2520R-RefCOCO%252C%2520and%2520RefCOCO%2520benchmarks%250Ademonstrate%2520the%2520effectiveness%2520and%2520rationality%2520of%2520CoHD%2520which%2520outperforms%250Astate-of-the-art%2520GRES%2520methods%2520by%2520a%2520remarkable%2520margin.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/RobertLuo1/CoHD%257D%257Bhere%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15658v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoHD%3A%20A%20Counting-Aware%20Hierarchical%20Decoding%20Framework%20for%20Generalized%0A%20%20Referring%20Expression%20Segmentation&entry.906535625=Zhuoyan%20Luo%20and%20Yinghao%20Wu%20and%20Tianheng%20Cheng%20and%20Yong%20Liu%20and%20Yicheng%20Xiao%20and%20Hongfa%20Wang%20and%20Xiao-Ping%20Zhang%20and%20Yujiu%20Yang&entry.1292438233=%20%20The%20newly%20proposed%20Generalized%20Referring%20Expression%20Segmentation%20%28GRES%29%0Aamplifies%20the%20formulation%20of%20classic%20RES%20by%20involving%20complex%0Amultiple/non-target%20scenarios.%20Recent%20approaches%20address%20GRES%20by%20directly%0Aextending%20the%20well-adopted%20RES%20frameworks%20with%20object-existence%20identification.%0AHowever%2C%20these%20approaches%20tend%20to%20encode%20multi-granularity%20object%20information%0Ainto%20a%20single%20representation%2C%20which%20makes%20it%20difficult%20to%20precisely%20represent%0Acomprehensive%20objects%20of%20different%20granularity.%20Moreover%2C%20the%20simple%20binary%0Aobject-existence%20identification%20across%20all%20referent%20scenarios%20fails%20to%20specify%0Atheir%20inherent%20differences%2C%20incurring%20ambiguity%20in%20object%20understanding.%20To%0Atackle%20the%20above%20issues%2C%20we%20propose%20a%20%5Ctextbf%7BCo%7Dunting-Aware%0A%5Ctextbf%7BH%7Dierarchical%20%5Ctextbf%7BD%7Decoding%20framework%20%28CoHD%29%20for%20GRES.%20By%0Adecoupling%20the%20intricate%20referring%20semantics%20into%20different%20granularity%20with%20a%0Avisual-linguistic%20hierarchy%2C%20and%20dynamic%20aggregating%20it%20with%20intra-%20and%0Ainter-selection%2C%20CoHD%20boosts%20multi-granularity%20comprehension%20with%20the%0Areciprocal%20benefit%20of%20the%20hierarchical%20nature.%20Furthermore%2C%20we%20incorporate%20the%0Acounting%20ability%20by%20embodying%20multiple/single/non-target%20scenarios%20into%20count-%0Aand%20category-level%20supervision%2C%20facilitating%20comprehensive%20object%20perception.%0AExperimental%20results%20on%20gRefCOCO%2C%20Ref-ZOM%2C%20R-RefCOCO%2C%20and%20RefCOCO%20benchmarks%0Ademonstrate%20the%20effectiveness%20and%20rationality%20of%20CoHD%20which%20outperforms%0Astate-of-the-art%20GRES%20methods%20by%20a%20remarkable%20margin.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/RobertLuo1/CoHD%7D%7Bhere%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15658v2&entry.124074799=Read"},
{"title": "VQ-SGen: A Vector Quantized Stroke Representation for Sketch Generation", "author": "Jiawei Wang and Zhiming Cui and Changjian Li", "abstract": "  This paper presents VQ-SGen, a novel algorithm for high-quality sketch\ngeneration. Recent approaches have often framed the task as pixel-based\ngeneration either as a whole or part-by-part, neglecting the intrinsic and\ncontextual relationships among individual strokes, such as the shape and\nspatial positioning of both proximal and distant strokes. To overcome these\nlimitations, we propose treating each stroke within a sketch as an entity and\nintroducing a vector-quantized (VQ) stroke representation for fine-grained\nsketch generation. Our method follows a two-stage framework - in the first\nstage, we decouple each stroke's shape and location information to ensure the\nVQ representation prioritizes stroke shape learning. In the second stage, we\nfeed the precise and compact representation into an auto-decoding Transformer\nto incorporate stroke semantics, positions, and shapes into the generation\nprocess. By utilizing tokenized stroke representation, our approach generates\nstrokes with high fidelity and facilitates novel applications, such as\nconditional generation and semantic-aware stroke editing. Comprehensive\nexperiments demonstrate our method surpasses existing state-of-the-art\ntechniques, underscoring its effectiveness. The code and model will be made\npublicly available upon publication.\n", "link": "http://arxiv.org/abs/2411.16446v1", "date": "2024-11-25", "relevancy": 2.0945, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5345}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5235}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQ-SGen%3A%20A%20Vector%20Quantized%20Stroke%20Representation%20for%20Sketch%20Generation&body=Title%3A%20VQ-SGen%3A%20A%20Vector%20Quantized%20Stroke%20Representation%20for%20Sketch%20Generation%0AAuthor%3A%20Jiawei%20Wang%20and%20Zhiming%20Cui%20and%20Changjian%20Li%0AAbstract%3A%20%20%20This%20paper%20presents%20VQ-SGen%2C%20a%20novel%20algorithm%20for%20high-quality%20sketch%0Ageneration.%20Recent%20approaches%20have%20often%20framed%20the%20task%20as%20pixel-based%0Ageneration%20either%20as%20a%20whole%20or%20part-by-part%2C%20neglecting%20the%20intrinsic%20and%0Acontextual%20relationships%20among%20individual%20strokes%2C%20such%20as%20the%20shape%20and%0Aspatial%20positioning%20of%20both%20proximal%20and%20distant%20strokes.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20treating%20each%20stroke%20within%20a%20sketch%20as%20an%20entity%20and%0Aintroducing%20a%20vector-quantized%20%28VQ%29%20stroke%20representation%20for%20fine-grained%0Asketch%20generation.%20Our%20method%20follows%20a%20two-stage%20framework%20-%20in%20the%20first%0Astage%2C%20we%20decouple%20each%20stroke%27s%20shape%20and%20location%20information%20to%20ensure%20the%0AVQ%20representation%20prioritizes%20stroke%20shape%20learning.%20In%20the%20second%20stage%2C%20we%0Afeed%20the%20precise%20and%20compact%20representation%20into%20an%20auto-decoding%20Transformer%0Ato%20incorporate%20stroke%20semantics%2C%20positions%2C%20and%20shapes%20into%20the%20generation%0Aprocess.%20By%20utilizing%20tokenized%20stroke%20representation%2C%20our%20approach%20generates%0Astrokes%20with%20high%20fidelity%20and%20facilitates%20novel%20applications%2C%20such%20as%0Aconditional%20generation%20and%20semantic-aware%20stroke%20editing.%20Comprehensive%0Aexperiments%20demonstrate%20our%20method%20surpasses%20existing%20state-of-the-art%0Atechniques%2C%20underscoring%20its%20effectiveness.%20The%20code%20and%20model%20will%20be%20made%0Apublicly%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQ-SGen%253A%2520A%2520Vector%2520Quantized%2520Stroke%2520Representation%2520for%2520Sketch%2520Generation%26entry.906535625%3DJiawei%2520Wang%2520and%2520Zhiming%2520Cui%2520and%2520Changjian%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520VQ-SGen%252C%2520a%2520novel%2520algorithm%2520for%2520high-quality%2520sketch%250Ageneration.%2520Recent%2520approaches%2520have%2520often%2520framed%2520the%2520task%2520as%2520pixel-based%250Ageneration%2520either%2520as%2520a%2520whole%2520or%2520part-by-part%252C%2520neglecting%2520the%2520intrinsic%2520and%250Acontextual%2520relationships%2520among%2520individual%2520strokes%252C%2520such%2520as%2520the%2520shape%2520and%250Aspatial%2520positioning%2520of%2520both%2520proximal%2520and%2520distant%2520strokes.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520treating%2520each%2520stroke%2520within%2520a%2520sketch%2520as%2520an%2520entity%2520and%250Aintroducing%2520a%2520vector-quantized%2520%2528VQ%2529%2520stroke%2520representation%2520for%2520fine-grained%250Asketch%2520generation.%2520Our%2520method%2520follows%2520a%2520two-stage%2520framework%2520-%2520in%2520the%2520first%250Astage%252C%2520we%2520decouple%2520each%2520stroke%2527s%2520shape%2520and%2520location%2520information%2520to%2520ensure%2520the%250AVQ%2520representation%2520prioritizes%2520stroke%2520shape%2520learning.%2520In%2520the%2520second%2520stage%252C%2520we%250Afeed%2520the%2520precise%2520and%2520compact%2520representation%2520into%2520an%2520auto-decoding%2520Transformer%250Ato%2520incorporate%2520stroke%2520semantics%252C%2520positions%252C%2520and%2520shapes%2520into%2520the%2520generation%250Aprocess.%2520By%2520utilizing%2520tokenized%2520stroke%2520representation%252C%2520our%2520approach%2520generates%250Astrokes%2520with%2520high%2520fidelity%2520and%2520facilitates%2520novel%2520applications%252C%2520such%2520as%250Aconditional%2520generation%2520and%2520semantic-aware%2520stroke%2520editing.%2520Comprehensive%250Aexperiments%2520demonstrate%2520our%2520method%2520surpasses%2520existing%2520state-of-the-art%250Atechniques%252C%2520underscoring%2520its%2520effectiveness.%2520The%2520code%2520and%2520model%2520will%2520be%2520made%250Apublicly%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQ-SGen%3A%20A%20Vector%20Quantized%20Stroke%20Representation%20for%20Sketch%20Generation&entry.906535625=Jiawei%20Wang%20and%20Zhiming%20Cui%20and%20Changjian%20Li&entry.1292438233=%20%20This%20paper%20presents%20VQ-SGen%2C%20a%20novel%20algorithm%20for%20high-quality%20sketch%0Ageneration.%20Recent%20approaches%20have%20often%20framed%20the%20task%20as%20pixel-based%0Ageneration%20either%20as%20a%20whole%20or%20part-by-part%2C%20neglecting%20the%20intrinsic%20and%0Acontextual%20relationships%20among%20individual%20strokes%2C%20such%20as%20the%20shape%20and%0Aspatial%20positioning%20of%20both%20proximal%20and%20distant%20strokes.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20treating%20each%20stroke%20within%20a%20sketch%20as%20an%20entity%20and%0Aintroducing%20a%20vector-quantized%20%28VQ%29%20stroke%20representation%20for%20fine-grained%0Asketch%20generation.%20Our%20method%20follows%20a%20two-stage%20framework%20-%20in%20the%20first%0Astage%2C%20we%20decouple%20each%20stroke%27s%20shape%20and%20location%20information%20to%20ensure%20the%0AVQ%20representation%20prioritizes%20stroke%20shape%20learning.%20In%20the%20second%20stage%2C%20we%0Afeed%20the%20precise%20and%20compact%20representation%20into%20an%20auto-decoding%20Transformer%0Ato%20incorporate%20stroke%20semantics%2C%20positions%2C%20and%20shapes%20into%20the%20generation%0Aprocess.%20By%20utilizing%20tokenized%20stroke%20representation%2C%20our%20approach%20generates%0Astrokes%20with%20high%20fidelity%20and%20facilitates%20novel%20applications%2C%20such%20as%0Aconditional%20generation%20and%20semantic-aware%20stroke%20editing.%20Comprehensive%0Aexperiments%20demonstrate%20our%20method%20surpasses%20existing%20state-of-the-art%0Atechniques%2C%20underscoring%20its%20effectiveness.%20The%20code%20and%20model%20will%20be%20made%0Apublicly%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16446v1&entry.124074799=Read"},
{"title": "WTDUN: Wavelet Tree-Structured Sampling and Deep Unfolding Network for\n  Image Compressed Sensing", "author": "Kai Han and Jin Wang and Yunhui Shi and Hanqin Cai and Nam Ling and Baocai Yin", "abstract": "  Deep unfolding networks have gained increasing attention in the field of\ncompressed sensing (CS) owing to their theoretical interpretability and\nsuperior reconstruction performance. However, most existing deep unfolding\nmethods often face the following issues: 1) they learn directly from\nsingle-channel images, leading to a simple feature representation that does not\nfully capture complex features; and 2) they treat various image components\nuniformly, ignoring the characteristics of different components. To address\nthese issues, we propose a novel wavelet-domain deep unfolding framework named\nWTDUN, which operates directly on the multi-scale wavelet subbands. Our method\nutilizes the intrinsic sparsity and multi-scale structure of wavelet\ncoefficients to achieve a tree-structured sampling and reconstruction,\neffectively capturing and highlighting the most important features within\nimages. Specifically, the design of tree-structured reconstruction aims to\ncapture the inter-dependencies among the multi-scale subbands, enabling the\nidentification of both fine and coarse features, which can lead to a marked\nimprovement in reconstruction quality. Furthermore, a wavelet domain adaptive\nsampling method is proposed to greatly improve the sampling capability, which\nis realized by assigning measurements to each wavelet subband based on its\nimportance. Unlike pure deep learning methods that treat all components\nuniformly, our method introduces a targeted focus on important subbands,\nconsidering their energy and sparsity. This targeted strategy lets us capture\nkey information more efficiently while discarding less important information,\nresulting in a more effective and detailed reconstruction. Extensive\nexperimental results on various datasets validate the superior performance of\nour proposed method.\n", "link": "http://arxiv.org/abs/2411.16336v1", "date": "2024-11-25", "relevancy": 2.0862, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5445}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5225}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WTDUN%3A%20Wavelet%20Tree-Structured%20Sampling%20and%20Deep%20Unfolding%20Network%20for%0A%20%20Image%20Compressed%20Sensing&body=Title%3A%20WTDUN%3A%20Wavelet%20Tree-Structured%20Sampling%20and%20Deep%20Unfolding%20Network%20for%0A%20%20Image%20Compressed%20Sensing%0AAuthor%3A%20Kai%20Han%20and%20Jin%20Wang%20and%20Yunhui%20Shi%20and%20Hanqin%20Cai%20and%20Nam%20Ling%20and%20Baocai%20Yin%0AAbstract%3A%20%20%20Deep%20unfolding%20networks%20have%20gained%20increasing%20attention%20in%20the%20field%20of%0Acompressed%20sensing%20%28CS%29%20owing%20to%20their%20theoretical%20interpretability%20and%0Asuperior%20reconstruction%20performance.%20However%2C%20most%20existing%20deep%20unfolding%0Amethods%20often%20face%20the%20following%20issues%3A%201%29%20they%20learn%20directly%20from%0Asingle-channel%20images%2C%20leading%20to%20a%20simple%20feature%20representation%20that%20does%20not%0Afully%20capture%20complex%20features%3B%20and%202%29%20they%20treat%20various%20image%20components%0Auniformly%2C%20ignoring%20the%20characteristics%20of%20different%20components.%20To%20address%0Athese%20issues%2C%20we%20propose%20a%20novel%20wavelet-domain%20deep%20unfolding%20framework%20named%0AWTDUN%2C%20which%20operates%20directly%20on%20the%20multi-scale%20wavelet%20subbands.%20Our%20method%0Autilizes%20the%20intrinsic%20sparsity%20and%20multi-scale%20structure%20of%20wavelet%0Acoefficients%20to%20achieve%20a%20tree-structured%20sampling%20and%20reconstruction%2C%0Aeffectively%20capturing%20and%20highlighting%20the%20most%20important%20features%20within%0Aimages.%20Specifically%2C%20the%20design%20of%20tree-structured%20reconstruction%20aims%20to%0Acapture%20the%20inter-dependencies%20among%20the%20multi-scale%20subbands%2C%20enabling%20the%0Aidentification%20of%20both%20fine%20and%20coarse%20features%2C%20which%20can%20lead%20to%20a%20marked%0Aimprovement%20in%20reconstruction%20quality.%20Furthermore%2C%20a%20wavelet%20domain%20adaptive%0Asampling%20method%20is%20proposed%20to%20greatly%20improve%20the%20sampling%20capability%2C%20which%0Ais%20realized%20by%20assigning%20measurements%20to%20each%20wavelet%20subband%20based%20on%20its%0Aimportance.%20Unlike%20pure%20deep%20learning%20methods%20that%20treat%20all%20components%0Auniformly%2C%20our%20method%20introduces%20a%20targeted%20focus%20on%20important%20subbands%2C%0Aconsidering%20their%20energy%20and%20sparsity.%20This%20targeted%20strategy%20lets%20us%20capture%0Akey%20information%20more%20efficiently%20while%20discarding%20less%20important%20information%2C%0Aresulting%20in%20a%20more%20effective%20and%20detailed%20reconstruction.%20Extensive%0Aexperimental%20results%20on%20various%20datasets%20validate%20the%20superior%20performance%20of%0Aour%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWTDUN%253A%2520Wavelet%2520Tree-Structured%2520Sampling%2520and%2520Deep%2520Unfolding%2520Network%2520for%250A%2520%2520Image%2520Compressed%2520Sensing%26entry.906535625%3DKai%2520Han%2520and%2520Jin%2520Wang%2520and%2520Yunhui%2520Shi%2520and%2520Hanqin%2520Cai%2520and%2520Nam%2520Ling%2520and%2520Baocai%2520Yin%26entry.1292438233%3D%2520%2520Deep%2520unfolding%2520networks%2520have%2520gained%2520increasing%2520attention%2520in%2520the%2520field%2520of%250Acompressed%2520sensing%2520%2528CS%2529%2520owing%2520to%2520their%2520theoretical%2520interpretability%2520and%250Asuperior%2520reconstruction%2520performance.%2520However%252C%2520most%2520existing%2520deep%2520unfolding%250Amethods%2520often%2520face%2520the%2520following%2520issues%253A%25201%2529%2520they%2520learn%2520directly%2520from%250Asingle-channel%2520images%252C%2520leading%2520to%2520a%2520simple%2520feature%2520representation%2520that%2520does%2520not%250Afully%2520capture%2520complex%2520features%253B%2520and%25202%2529%2520they%2520treat%2520various%2520image%2520components%250Auniformly%252C%2520ignoring%2520the%2520characteristics%2520of%2520different%2520components.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520a%2520novel%2520wavelet-domain%2520deep%2520unfolding%2520framework%2520named%250AWTDUN%252C%2520which%2520operates%2520directly%2520on%2520the%2520multi-scale%2520wavelet%2520subbands.%2520Our%2520method%250Autilizes%2520the%2520intrinsic%2520sparsity%2520and%2520multi-scale%2520structure%2520of%2520wavelet%250Acoefficients%2520to%2520achieve%2520a%2520tree-structured%2520sampling%2520and%2520reconstruction%252C%250Aeffectively%2520capturing%2520and%2520highlighting%2520the%2520most%2520important%2520features%2520within%250Aimages.%2520Specifically%252C%2520the%2520design%2520of%2520tree-structured%2520reconstruction%2520aims%2520to%250Acapture%2520the%2520inter-dependencies%2520among%2520the%2520multi-scale%2520subbands%252C%2520enabling%2520the%250Aidentification%2520of%2520both%2520fine%2520and%2520coarse%2520features%252C%2520which%2520can%2520lead%2520to%2520a%2520marked%250Aimprovement%2520in%2520reconstruction%2520quality.%2520Furthermore%252C%2520a%2520wavelet%2520domain%2520adaptive%250Asampling%2520method%2520is%2520proposed%2520to%2520greatly%2520improve%2520the%2520sampling%2520capability%252C%2520which%250Ais%2520realized%2520by%2520assigning%2520measurements%2520to%2520each%2520wavelet%2520subband%2520based%2520on%2520its%250Aimportance.%2520Unlike%2520pure%2520deep%2520learning%2520methods%2520that%2520treat%2520all%2520components%250Auniformly%252C%2520our%2520method%2520introduces%2520a%2520targeted%2520focus%2520on%2520important%2520subbands%252C%250Aconsidering%2520their%2520energy%2520and%2520sparsity.%2520This%2520targeted%2520strategy%2520lets%2520us%2520capture%250Akey%2520information%2520more%2520efficiently%2520while%2520discarding%2520less%2520important%2520information%252C%250Aresulting%2520in%2520a%2520more%2520effective%2520and%2520detailed%2520reconstruction.%2520Extensive%250Aexperimental%2520results%2520on%2520various%2520datasets%2520validate%2520the%2520superior%2520performance%2520of%250Aour%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WTDUN%3A%20Wavelet%20Tree-Structured%20Sampling%20and%20Deep%20Unfolding%20Network%20for%0A%20%20Image%20Compressed%20Sensing&entry.906535625=Kai%20Han%20and%20Jin%20Wang%20and%20Yunhui%20Shi%20and%20Hanqin%20Cai%20and%20Nam%20Ling%20and%20Baocai%20Yin&entry.1292438233=%20%20Deep%20unfolding%20networks%20have%20gained%20increasing%20attention%20in%20the%20field%20of%0Acompressed%20sensing%20%28CS%29%20owing%20to%20their%20theoretical%20interpretability%20and%0Asuperior%20reconstruction%20performance.%20However%2C%20most%20existing%20deep%20unfolding%0Amethods%20often%20face%20the%20following%20issues%3A%201%29%20they%20learn%20directly%20from%0Asingle-channel%20images%2C%20leading%20to%20a%20simple%20feature%20representation%20that%20does%20not%0Afully%20capture%20complex%20features%3B%20and%202%29%20they%20treat%20various%20image%20components%0Auniformly%2C%20ignoring%20the%20characteristics%20of%20different%20components.%20To%20address%0Athese%20issues%2C%20we%20propose%20a%20novel%20wavelet-domain%20deep%20unfolding%20framework%20named%0AWTDUN%2C%20which%20operates%20directly%20on%20the%20multi-scale%20wavelet%20subbands.%20Our%20method%0Autilizes%20the%20intrinsic%20sparsity%20and%20multi-scale%20structure%20of%20wavelet%0Acoefficients%20to%20achieve%20a%20tree-structured%20sampling%20and%20reconstruction%2C%0Aeffectively%20capturing%20and%20highlighting%20the%20most%20important%20features%20within%0Aimages.%20Specifically%2C%20the%20design%20of%20tree-structured%20reconstruction%20aims%20to%0Acapture%20the%20inter-dependencies%20among%20the%20multi-scale%20subbands%2C%20enabling%20the%0Aidentification%20of%20both%20fine%20and%20coarse%20features%2C%20which%20can%20lead%20to%20a%20marked%0Aimprovement%20in%20reconstruction%20quality.%20Furthermore%2C%20a%20wavelet%20domain%20adaptive%0Asampling%20method%20is%20proposed%20to%20greatly%20improve%20the%20sampling%20capability%2C%20which%0Ais%20realized%20by%20assigning%20measurements%20to%20each%20wavelet%20subband%20based%20on%20its%0Aimportance.%20Unlike%20pure%20deep%20learning%20methods%20that%20treat%20all%20components%0Auniformly%2C%20our%20method%20introduces%20a%20targeted%20focus%20on%20important%20subbands%2C%0Aconsidering%20their%20energy%20and%20sparsity.%20This%20targeted%20strategy%20lets%20us%20capture%0Akey%20information%20more%20efficiently%20while%20discarding%20less%20important%20information%2C%0Aresulting%20in%20a%20more%20effective%20and%20detailed%20reconstruction.%20Extensive%0Aexperimental%20results%20on%20various%20datasets%20validate%20the%20superior%20performance%20of%0Aour%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16336v1&entry.124074799=Read"},
{"title": "Optimized Kalman Filter based State Estimation and Height Control in\n  Hopping Robots", "author": "Samuel Burns and Matthew Woodward", "abstract": "  Quadrotor-based multimodal hopping and flying locomotion significantly\nimproves efficiency and operation time as compared to purely flying systems.\nHowever, effective control necessitates continuous estimation of the vertical\nstates, as thrust (insufficient for flight) in the aerial phase creates\nnon-ballistic behavior. A single hopping continuous state estimator has been\nshown (Kang 2024), in which two vertical states (position, acceleration) are\nmeasured and velocity is estimated through a technique requiring multiple\nsensors (IMU, lidar, depth camera, contact force sensor), and computationally\nintensive calculations (12-core, 5 GHz processor), for a maximum hop height of\n~0.6 m at 3.65 kg. This poses a significant challenge to the development of\nlight-weight, high-performance, low observable, jamming and electronic\ninterference resistant hopping systems; especially in perceptually degraded\nenvironments (e.g., dust, smoke). Here we show a trained Kalman filter based\nhopping vertical state estimator (HVSE), requiring only vertical acceleration\nmeasurements. The training uses hopping data from the robot and a motion\ncapture system to adapt a general framework to the specific system; including\nhigh impact behaviors. Our results show the HVSE can estimate more states\n(position, velocity) with 32% of the mean-absolute-percentage-error in the hop\napex height (height error/ground truth) of the next best inertial navigation\ntechnique (12.5%), running ~4.2x faster (840 Hz) on a substantially less\npowerful processor (dual-core 240 MHz) with over ~6.7x the hopping height (4.02\nm) at 20% of the mass (672 g). The presented general HVSE, and training\nprocedure make the methodology broadly applicable to other robots.\n", "link": "http://arxiv.org/abs/2408.11978v2", "date": "2024-11-25", "relevancy": 2.0803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5952}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5279}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimized%20Kalman%20Filter%20based%20State%20Estimation%20and%20Height%20Control%20in%0A%20%20Hopping%20Robots&body=Title%3A%20Optimized%20Kalman%20Filter%20based%20State%20Estimation%20and%20Height%20Control%20in%0A%20%20Hopping%20Robots%0AAuthor%3A%20Samuel%20Burns%20and%20Matthew%20Woodward%0AAbstract%3A%20%20%20Quadrotor-based%20multimodal%20hopping%20and%20flying%20locomotion%20significantly%0Aimproves%20efficiency%20and%20operation%20time%20as%20compared%20to%20purely%20flying%20systems.%0AHowever%2C%20effective%20control%20necessitates%20continuous%20estimation%20of%20the%20vertical%0Astates%2C%20as%20thrust%20%28insufficient%20for%20flight%29%20in%20the%20aerial%20phase%20creates%0Anon-ballistic%20behavior.%20A%20single%20hopping%20continuous%20state%20estimator%20has%20been%0Ashown%20%28Kang%202024%29%2C%20in%20which%20two%20vertical%20states%20%28position%2C%20acceleration%29%20are%0Ameasured%20and%20velocity%20is%20estimated%20through%20a%20technique%20requiring%20multiple%0Asensors%20%28IMU%2C%20lidar%2C%20depth%20camera%2C%20contact%20force%20sensor%29%2C%20and%20computationally%0Aintensive%20calculations%20%2812-core%2C%205%20GHz%20processor%29%2C%20for%20a%20maximum%20hop%20height%20of%0A~0.6%20m%20at%203.65%20kg.%20This%20poses%20a%20significant%20challenge%20to%20the%20development%20of%0Alight-weight%2C%20high-performance%2C%20low%20observable%2C%20jamming%20and%20electronic%0Ainterference%20resistant%20hopping%20systems%3B%20especially%20in%20perceptually%20degraded%0Aenvironments%20%28e.g.%2C%20dust%2C%20smoke%29.%20Here%20we%20show%20a%20trained%20Kalman%20filter%20based%0Ahopping%20vertical%20state%20estimator%20%28HVSE%29%2C%20requiring%20only%20vertical%20acceleration%0Ameasurements.%20The%20training%20uses%20hopping%20data%20from%20the%20robot%20and%20a%20motion%0Acapture%20system%20to%20adapt%20a%20general%20framework%20to%20the%20specific%20system%3B%20including%0Ahigh%20impact%20behaviors.%20Our%20results%20show%20the%20HVSE%20can%20estimate%20more%20states%0A%28position%2C%20velocity%29%20with%2032%25%20of%20the%20mean-absolute-percentage-error%20in%20the%20hop%0Aapex%20height%20%28height%20error/ground%20truth%29%20of%20the%20next%20best%20inertial%20navigation%0Atechnique%20%2812.5%25%29%2C%20running%20~4.2x%20faster%20%28840%20Hz%29%20on%20a%20substantially%20less%0Apowerful%20processor%20%28dual-core%20240%20MHz%29%20with%20over%20~6.7x%20the%20hopping%20height%20%284.02%0Am%29%20at%2020%25%20of%20the%20mass%20%28672%20g%29.%20The%20presented%20general%20HVSE%2C%20and%20training%0Aprocedure%20make%20the%20methodology%20broadly%20applicable%20to%20other%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimized%2520Kalman%2520Filter%2520based%2520State%2520Estimation%2520and%2520Height%2520Control%2520in%250A%2520%2520Hopping%2520Robots%26entry.906535625%3DSamuel%2520Burns%2520and%2520Matthew%2520Woodward%26entry.1292438233%3D%2520%2520Quadrotor-based%2520multimodal%2520hopping%2520and%2520flying%2520locomotion%2520significantly%250Aimproves%2520efficiency%2520and%2520operation%2520time%2520as%2520compared%2520to%2520purely%2520flying%2520systems.%250AHowever%252C%2520effective%2520control%2520necessitates%2520continuous%2520estimation%2520of%2520the%2520vertical%250Astates%252C%2520as%2520thrust%2520%2528insufficient%2520for%2520flight%2529%2520in%2520the%2520aerial%2520phase%2520creates%250Anon-ballistic%2520behavior.%2520A%2520single%2520hopping%2520continuous%2520state%2520estimator%2520has%2520been%250Ashown%2520%2528Kang%25202024%2529%252C%2520in%2520which%2520two%2520vertical%2520states%2520%2528position%252C%2520acceleration%2529%2520are%250Ameasured%2520and%2520velocity%2520is%2520estimated%2520through%2520a%2520technique%2520requiring%2520multiple%250Asensors%2520%2528IMU%252C%2520lidar%252C%2520depth%2520camera%252C%2520contact%2520force%2520sensor%2529%252C%2520and%2520computationally%250Aintensive%2520calculations%2520%252812-core%252C%25205%2520GHz%2520processor%2529%252C%2520for%2520a%2520maximum%2520hop%2520height%2520of%250A~0.6%2520m%2520at%25203.65%2520kg.%2520This%2520poses%2520a%2520significant%2520challenge%2520to%2520the%2520development%2520of%250Alight-weight%252C%2520high-performance%252C%2520low%2520observable%252C%2520jamming%2520and%2520electronic%250Ainterference%2520resistant%2520hopping%2520systems%253B%2520especially%2520in%2520perceptually%2520degraded%250Aenvironments%2520%2528e.g.%252C%2520dust%252C%2520smoke%2529.%2520Here%2520we%2520show%2520a%2520trained%2520Kalman%2520filter%2520based%250Ahopping%2520vertical%2520state%2520estimator%2520%2528HVSE%2529%252C%2520requiring%2520only%2520vertical%2520acceleration%250Ameasurements.%2520The%2520training%2520uses%2520hopping%2520data%2520from%2520the%2520robot%2520and%2520a%2520motion%250Acapture%2520system%2520to%2520adapt%2520a%2520general%2520framework%2520to%2520the%2520specific%2520system%253B%2520including%250Ahigh%2520impact%2520behaviors.%2520Our%2520results%2520show%2520the%2520HVSE%2520can%2520estimate%2520more%2520states%250A%2528position%252C%2520velocity%2529%2520with%252032%2525%2520of%2520the%2520mean-absolute-percentage-error%2520in%2520the%2520hop%250Aapex%2520height%2520%2528height%2520error/ground%2520truth%2529%2520of%2520the%2520next%2520best%2520inertial%2520navigation%250Atechnique%2520%252812.5%2525%2529%252C%2520running%2520~4.2x%2520faster%2520%2528840%2520Hz%2529%2520on%2520a%2520substantially%2520less%250Apowerful%2520processor%2520%2528dual-core%2520240%2520MHz%2529%2520with%2520over%2520~6.7x%2520the%2520hopping%2520height%2520%25284.02%250Am%2529%2520at%252020%2525%2520of%2520the%2520mass%2520%2528672%2520g%2529.%2520The%2520presented%2520general%2520HVSE%252C%2520and%2520training%250Aprocedure%2520make%2520the%2520methodology%2520broadly%2520applicable%2520to%2520other%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimized%20Kalman%20Filter%20based%20State%20Estimation%20and%20Height%20Control%20in%0A%20%20Hopping%20Robots&entry.906535625=Samuel%20Burns%20and%20Matthew%20Woodward&entry.1292438233=%20%20Quadrotor-based%20multimodal%20hopping%20and%20flying%20locomotion%20significantly%0Aimproves%20efficiency%20and%20operation%20time%20as%20compared%20to%20purely%20flying%20systems.%0AHowever%2C%20effective%20control%20necessitates%20continuous%20estimation%20of%20the%20vertical%0Astates%2C%20as%20thrust%20%28insufficient%20for%20flight%29%20in%20the%20aerial%20phase%20creates%0Anon-ballistic%20behavior.%20A%20single%20hopping%20continuous%20state%20estimator%20has%20been%0Ashown%20%28Kang%202024%29%2C%20in%20which%20two%20vertical%20states%20%28position%2C%20acceleration%29%20are%0Ameasured%20and%20velocity%20is%20estimated%20through%20a%20technique%20requiring%20multiple%0Asensors%20%28IMU%2C%20lidar%2C%20depth%20camera%2C%20contact%20force%20sensor%29%2C%20and%20computationally%0Aintensive%20calculations%20%2812-core%2C%205%20GHz%20processor%29%2C%20for%20a%20maximum%20hop%20height%20of%0A~0.6%20m%20at%203.65%20kg.%20This%20poses%20a%20significant%20challenge%20to%20the%20development%20of%0Alight-weight%2C%20high-performance%2C%20low%20observable%2C%20jamming%20and%20electronic%0Ainterference%20resistant%20hopping%20systems%3B%20especially%20in%20perceptually%20degraded%0Aenvironments%20%28e.g.%2C%20dust%2C%20smoke%29.%20Here%20we%20show%20a%20trained%20Kalman%20filter%20based%0Ahopping%20vertical%20state%20estimator%20%28HVSE%29%2C%20requiring%20only%20vertical%20acceleration%0Ameasurements.%20The%20training%20uses%20hopping%20data%20from%20the%20robot%20and%20a%20motion%0Acapture%20system%20to%20adapt%20a%20general%20framework%20to%20the%20specific%20system%3B%20including%0Ahigh%20impact%20behaviors.%20Our%20results%20show%20the%20HVSE%20can%20estimate%20more%20states%0A%28position%2C%20velocity%29%20with%2032%25%20of%20the%20mean-absolute-percentage-error%20in%20the%20hop%0Aapex%20height%20%28height%20error/ground%20truth%29%20of%20the%20next%20best%20inertial%20navigation%0Atechnique%20%2812.5%25%29%2C%20running%20~4.2x%20faster%20%28840%20Hz%29%20on%20a%20substantially%20less%0Apowerful%20processor%20%28dual-core%20240%20MHz%29%20with%20over%20~6.7x%20the%20hopping%20height%20%284.02%0Am%29%20at%2020%25%20of%20the%20mass%20%28672%20g%29.%20The%20presented%20general%20HVSE%2C%20and%20training%0Aprocedure%20make%20the%20methodology%20broadly%20applicable%20to%20other%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11978v2&entry.124074799=Read"},
{"title": "Long-Tailed Out-of-Distribution Detection via Normalized Outlier\n  Distribution Adaptation", "author": "Wenjun Miao and Guansong Pang and Jin Zheng and Xiao Bai", "abstract": "  One key challenge in Out-of-Distribution (OOD) detection is the absence of\nground-truth OOD samples during training. One principled approach to address\nthis issue is to use samples from external datasets as outliers (i.e., pseudo\nOOD samples) to train OOD detectors. However, we find empirically that the\noutlier samples often present a distribution shift compared to the true OOD\nsamples, especially in Long-Tailed Recognition (LTR) scenarios, where ID\nclasses are heavily imbalanced, \\ie, the true OOD samples exhibit very\ndifferent probability distribution to the head and tailed ID classes from the\noutliers. In this work, we propose a novel approach, namely normalized outlier\ndistribution adaptation (AdaptOD), to tackle this distribution shift problem.\nOne of its key components is dynamic outlier distribution adaptation that\neffectively adapts a vanilla outlier distribution based on the outlier samples\nto the true OOD distribution by utilizing the OOD knowledge in the predicted\nOOD samples during inference. Further, to obtain a more reliable set of\npredicted OOD samples on long-tailed ID data, a novel dual-normalized energy\nloss is introduced in AdaptOD, which leverages class- and sample-wise\nnormalized energy to enforce a more balanced prediction energy on imbalanced ID\nsamples. This helps avoid bias toward the head samples and learn a\nsubstantially better vanilla outlier distribution than existing energy losses\nduring training. It also eliminates the need of manually tuning the sensitive\nmargin hyperparameters in energy losses. Empirical results on three popular\nbenchmarks for OOD detection in LTR show the superior performance of AdaptOD\nover state-of-the-art methods. Code is available at\nhttps://github.com/mala-lab/AdaptOD.\n", "link": "http://arxiv.org/abs/2410.20807v2", "date": "2024-11-25", "relevancy": 2.072, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5773}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4861}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Tailed%20Out-of-Distribution%20Detection%20via%20Normalized%20Outlier%0A%20%20Distribution%20Adaptation&body=Title%3A%20Long-Tailed%20Out-of-Distribution%20Detection%20via%20Normalized%20Outlier%0A%20%20Distribution%20Adaptation%0AAuthor%3A%20Wenjun%20Miao%20and%20Guansong%20Pang%20and%20Jin%20Zheng%20and%20Xiao%20Bai%0AAbstract%3A%20%20%20One%20key%20challenge%20in%20Out-of-Distribution%20%28OOD%29%20detection%20is%20the%20absence%20of%0Aground-truth%20OOD%20samples%20during%20training.%20One%20principled%20approach%20to%20address%0Athis%20issue%20is%20to%20use%20samples%20from%20external%20datasets%20as%20outliers%20%28i.e.%2C%20pseudo%0AOOD%20samples%29%20to%20train%20OOD%20detectors.%20However%2C%20we%20find%20empirically%20that%20the%0Aoutlier%20samples%20often%20present%20a%20distribution%20shift%20compared%20to%20the%20true%20OOD%0Asamples%2C%20especially%20in%20Long-Tailed%20Recognition%20%28LTR%29%20scenarios%2C%20where%20ID%0Aclasses%20are%20heavily%20imbalanced%2C%20%5Cie%2C%20the%20true%20OOD%20samples%20exhibit%20very%0Adifferent%20probability%20distribution%20to%20the%20head%20and%20tailed%20ID%20classes%20from%20the%0Aoutliers.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%2C%20namely%20normalized%20outlier%0Adistribution%20adaptation%20%28AdaptOD%29%2C%20to%20tackle%20this%20distribution%20shift%20problem.%0AOne%20of%20its%20key%20components%20is%20dynamic%20outlier%20distribution%20adaptation%20that%0Aeffectively%20adapts%20a%20vanilla%20outlier%20distribution%20based%20on%20the%20outlier%20samples%0Ato%20the%20true%20OOD%20distribution%20by%20utilizing%20the%20OOD%20knowledge%20in%20the%20predicted%0AOOD%20samples%20during%20inference.%20Further%2C%20to%20obtain%20a%20more%20reliable%20set%20of%0Apredicted%20OOD%20samples%20on%20long-tailed%20ID%20data%2C%20a%20novel%20dual-normalized%20energy%0Aloss%20is%20introduced%20in%20AdaptOD%2C%20which%20leverages%20class-%20and%20sample-wise%0Anormalized%20energy%20to%20enforce%20a%20more%20balanced%20prediction%20energy%20on%20imbalanced%20ID%0Asamples.%20This%20helps%20avoid%20bias%20toward%20the%20head%20samples%20and%20learn%20a%0Asubstantially%20better%20vanilla%20outlier%20distribution%20than%20existing%20energy%20losses%0Aduring%20training.%20It%20also%20eliminates%20the%20need%20of%20manually%20tuning%20the%20sensitive%0Amargin%20hyperparameters%20in%20energy%20losses.%20Empirical%20results%20on%20three%20popular%0Abenchmarks%20for%20OOD%20detection%20in%20LTR%20show%20the%20superior%20performance%20of%20AdaptOD%0Aover%20state-of-the-art%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mala-lab/AdaptOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Tailed%2520Out-of-Distribution%2520Detection%2520via%2520Normalized%2520Outlier%250A%2520%2520Distribution%2520Adaptation%26entry.906535625%3DWenjun%2520Miao%2520and%2520Guansong%2520Pang%2520and%2520Jin%2520Zheng%2520and%2520Xiao%2520Bai%26entry.1292438233%3D%2520%2520One%2520key%2520challenge%2520in%2520Out-of-Distribution%2520%2528OOD%2529%2520detection%2520is%2520the%2520absence%2520of%250Aground-truth%2520OOD%2520samples%2520during%2520training.%2520One%2520principled%2520approach%2520to%2520address%250Athis%2520issue%2520is%2520to%2520use%2520samples%2520from%2520external%2520datasets%2520as%2520outliers%2520%2528i.e.%252C%2520pseudo%250AOOD%2520samples%2529%2520to%2520train%2520OOD%2520detectors.%2520However%252C%2520we%2520find%2520empirically%2520that%2520the%250Aoutlier%2520samples%2520often%2520present%2520a%2520distribution%2520shift%2520compared%2520to%2520the%2520true%2520OOD%250Asamples%252C%2520especially%2520in%2520Long-Tailed%2520Recognition%2520%2528LTR%2529%2520scenarios%252C%2520where%2520ID%250Aclasses%2520are%2520heavily%2520imbalanced%252C%2520%255Cie%252C%2520the%2520true%2520OOD%2520samples%2520exhibit%2520very%250Adifferent%2520probability%2520distribution%2520to%2520the%2520head%2520and%2520tailed%2520ID%2520classes%2520from%2520the%250Aoutliers.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%252C%2520namely%2520normalized%2520outlier%250Adistribution%2520adaptation%2520%2528AdaptOD%2529%252C%2520to%2520tackle%2520this%2520distribution%2520shift%2520problem.%250AOne%2520of%2520its%2520key%2520components%2520is%2520dynamic%2520outlier%2520distribution%2520adaptation%2520that%250Aeffectively%2520adapts%2520a%2520vanilla%2520outlier%2520distribution%2520based%2520on%2520the%2520outlier%2520samples%250Ato%2520the%2520true%2520OOD%2520distribution%2520by%2520utilizing%2520the%2520OOD%2520knowledge%2520in%2520the%2520predicted%250AOOD%2520samples%2520during%2520inference.%2520Further%252C%2520to%2520obtain%2520a%2520more%2520reliable%2520set%2520of%250Apredicted%2520OOD%2520samples%2520on%2520long-tailed%2520ID%2520data%252C%2520a%2520novel%2520dual-normalized%2520energy%250Aloss%2520is%2520introduced%2520in%2520AdaptOD%252C%2520which%2520leverages%2520class-%2520and%2520sample-wise%250Anormalized%2520energy%2520to%2520enforce%2520a%2520more%2520balanced%2520prediction%2520energy%2520on%2520imbalanced%2520ID%250Asamples.%2520This%2520helps%2520avoid%2520bias%2520toward%2520the%2520head%2520samples%2520and%2520learn%2520a%250Asubstantially%2520better%2520vanilla%2520outlier%2520distribution%2520than%2520existing%2520energy%2520losses%250Aduring%2520training.%2520It%2520also%2520eliminates%2520the%2520need%2520of%2520manually%2520tuning%2520the%2520sensitive%250Amargin%2520hyperparameters%2520in%2520energy%2520losses.%2520Empirical%2520results%2520on%2520three%2520popular%250Abenchmarks%2520for%2520OOD%2520detection%2520in%2520LTR%2520show%2520the%2520superior%2520performance%2520of%2520AdaptOD%250Aover%2520state-of-the-art%2520methods.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/mala-lab/AdaptOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Tailed%20Out-of-Distribution%20Detection%20via%20Normalized%20Outlier%0A%20%20Distribution%20Adaptation&entry.906535625=Wenjun%20Miao%20and%20Guansong%20Pang%20and%20Jin%20Zheng%20and%20Xiao%20Bai&entry.1292438233=%20%20One%20key%20challenge%20in%20Out-of-Distribution%20%28OOD%29%20detection%20is%20the%20absence%20of%0Aground-truth%20OOD%20samples%20during%20training.%20One%20principled%20approach%20to%20address%0Athis%20issue%20is%20to%20use%20samples%20from%20external%20datasets%20as%20outliers%20%28i.e.%2C%20pseudo%0AOOD%20samples%29%20to%20train%20OOD%20detectors.%20However%2C%20we%20find%20empirically%20that%20the%0Aoutlier%20samples%20often%20present%20a%20distribution%20shift%20compared%20to%20the%20true%20OOD%0Asamples%2C%20especially%20in%20Long-Tailed%20Recognition%20%28LTR%29%20scenarios%2C%20where%20ID%0Aclasses%20are%20heavily%20imbalanced%2C%20%5Cie%2C%20the%20true%20OOD%20samples%20exhibit%20very%0Adifferent%20probability%20distribution%20to%20the%20head%20and%20tailed%20ID%20classes%20from%20the%0Aoutliers.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%2C%20namely%20normalized%20outlier%0Adistribution%20adaptation%20%28AdaptOD%29%2C%20to%20tackle%20this%20distribution%20shift%20problem.%0AOne%20of%20its%20key%20components%20is%20dynamic%20outlier%20distribution%20adaptation%20that%0Aeffectively%20adapts%20a%20vanilla%20outlier%20distribution%20based%20on%20the%20outlier%20samples%0Ato%20the%20true%20OOD%20distribution%20by%20utilizing%20the%20OOD%20knowledge%20in%20the%20predicted%0AOOD%20samples%20during%20inference.%20Further%2C%20to%20obtain%20a%20more%20reliable%20set%20of%0Apredicted%20OOD%20samples%20on%20long-tailed%20ID%20data%2C%20a%20novel%20dual-normalized%20energy%0Aloss%20is%20introduced%20in%20AdaptOD%2C%20which%20leverages%20class-%20and%20sample-wise%0Anormalized%20energy%20to%20enforce%20a%20more%20balanced%20prediction%20energy%20on%20imbalanced%20ID%0Asamples.%20This%20helps%20avoid%20bias%20toward%20the%20head%20samples%20and%20learn%20a%0Asubstantially%20better%20vanilla%20outlier%20distribution%20than%20existing%20energy%20losses%0Aduring%20training.%20It%20also%20eliminates%20the%20need%20of%20manually%20tuning%20the%20sensitive%0Amargin%20hyperparameters%20in%20energy%20losses.%20Empirical%20results%20on%20three%20popular%0Abenchmarks%20for%20OOD%20detection%20in%20LTR%20show%20the%20superior%20performance%20of%20AdaptOD%0Aover%20state-of-the-art%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mala-lab/AdaptOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20807v2&entry.124074799=Read"},
{"title": "Hyperspectral and multispectral image fusion with arbitrary resolution\n  through self-supervised representations", "author": "Ting Wang and Zipei Yan and Jizhou Li and Xile Zhao and Chao Wang and Michael Ng", "abstract": "  The fusion of a low-resolution hyperspectral image (LR-HSI) with a\nhigh-resolution multispectral image (HR-MSI) has emerged as an effective\ntechnique for achieving HSI super-resolution (SR). Previous studies have mainly\nconcentrated on estimating the posterior distribution of the latent\nhigh-resolution hyperspectral image (HR-HSI), leveraging an appropriate image\nprior and likelihood computed from the discrepancy between the latent HSI and\nobserved images. Low rankness stands out for preserving latent HSI\ncharacteristics through matrix factorization among the various priors. However,\nthe primary limitation in previous studies lies in the generalization of a\nfusion model with fixed resolution scales, which necessitates retraining\nwhenever output resolutions are changed. To overcome this limitation, we\npropose a novel continuous low-rank factorization (CLoRF) by integrating two\nneural representations into the matrix factorization, capturing spatial and\nspectral information, respectively. This approach enables us to harness both\nthe low rankness from the matrix factorization and the continuity from neural\nrepresentation in a self-supervised manner.Theoretically, we prove the low-rank\nproperty and Lipschitz continuity in the proposed continuous low-rank\nfactorization. Experimentally, our method significantly surpasses existing\ntechniques and achieves user-desired resolutions without the need for neural\nnetwork retraining. Code is available at\nhttps://github.com/wangting1907/CLoRF-Fusion.\n", "link": "http://arxiv.org/abs/2405.17818v2", "date": "2024-11-25", "relevancy": 2.0688, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.52}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.517}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20and%20multispectral%20image%20fusion%20with%20arbitrary%20resolution%0A%20%20through%20self-supervised%20representations&body=Title%3A%20Hyperspectral%20and%20multispectral%20image%20fusion%20with%20arbitrary%20resolution%0A%20%20through%20self-supervised%20representations%0AAuthor%3A%20Ting%20Wang%20and%20Zipei%20Yan%20and%20Jizhou%20Li%20and%20Xile%20Zhao%20and%20Chao%20Wang%20and%20Michael%20Ng%0AAbstract%3A%20%20%20The%20fusion%20of%20a%20low-resolution%20hyperspectral%20image%20%28LR-HSI%29%20with%20a%0Ahigh-resolution%20multispectral%20image%20%28HR-MSI%29%20has%20emerged%20as%20an%20effective%0Atechnique%20for%20achieving%20HSI%20super-resolution%20%28SR%29.%20Previous%20studies%20have%20mainly%0Aconcentrated%20on%20estimating%20the%20posterior%20distribution%20of%20the%20latent%0Ahigh-resolution%20hyperspectral%20image%20%28HR-HSI%29%2C%20leveraging%20an%20appropriate%20image%0Aprior%20and%20likelihood%20computed%20from%20the%20discrepancy%20between%20the%20latent%20HSI%20and%0Aobserved%20images.%20Low%20rankness%20stands%20out%20for%20preserving%20latent%20HSI%0Acharacteristics%20through%20matrix%20factorization%20among%20the%20various%20priors.%20However%2C%0Athe%20primary%20limitation%20in%20previous%20studies%20lies%20in%20the%20generalization%20of%20a%0Afusion%20model%20with%20fixed%20resolution%20scales%2C%20which%20necessitates%20retraining%0Awhenever%20output%20resolutions%20are%20changed.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20novel%20continuous%20low-rank%20factorization%20%28CLoRF%29%20by%20integrating%20two%0Aneural%20representations%20into%20the%20matrix%20factorization%2C%20capturing%20spatial%20and%0Aspectral%20information%2C%20respectively.%20This%20approach%20enables%20us%20to%20harness%20both%0Athe%20low%20rankness%20from%20the%20matrix%20factorization%20and%20the%20continuity%20from%20neural%0Arepresentation%20in%20a%20self-supervised%20manner.Theoretically%2C%20we%20prove%20the%20low-rank%0Aproperty%20and%20Lipschitz%20continuity%20in%20the%20proposed%20continuous%20low-rank%0Afactorization.%20Experimentally%2C%20our%20method%20significantly%20surpasses%20existing%0Atechniques%20and%20achieves%20user-desired%20resolutions%20without%20the%20need%20for%20neural%0Anetwork%20retraining.%20Code%20is%20available%20at%0Ahttps%3A//github.com/wangting1907/CLoRF-Fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17818v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520and%2520multispectral%2520image%2520fusion%2520with%2520arbitrary%2520resolution%250A%2520%2520through%2520self-supervised%2520representations%26entry.906535625%3DTing%2520Wang%2520and%2520Zipei%2520Yan%2520and%2520Jizhou%2520Li%2520and%2520Xile%2520Zhao%2520and%2520Chao%2520Wang%2520and%2520Michael%2520Ng%26entry.1292438233%3D%2520%2520The%2520fusion%2520of%2520a%2520low-resolution%2520hyperspectral%2520image%2520%2528LR-HSI%2529%2520with%2520a%250Ahigh-resolution%2520multispectral%2520image%2520%2528HR-MSI%2529%2520has%2520emerged%2520as%2520an%2520effective%250Atechnique%2520for%2520achieving%2520HSI%2520super-resolution%2520%2528SR%2529.%2520Previous%2520studies%2520have%2520mainly%250Aconcentrated%2520on%2520estimating%2520the%2520posterior%2520distribution%2520of%2520the%2520latent%250Ahigh-resolution%2520hyperspectral%2520image%2520%2528HR-HSI%2529%252C%2520leveraging%2520an%2520appropriate%2520image%250Aprior%2520and%2520likelihood%2520computed%2520from%2520the%2520discrepancy%2520between%2520the%2520latent%2520HSI%2520and%250Aobserved%2520images.%2520Low%2520rankness%2520stands%2520out%2520for%2520preserving%2520latent%2520HSI%250Acharacteristics%2520through%2520matrix%2520factorization%2520among%2520the%2520various%2520priors.%2520However%252C%250Athe%2520primary%2520limitation%2520in%2520previous%2520studies%2520lies%2520in%2520the%2520generalization%2520of%2520a%250Afusion%2520model%2520with%2520fixed%2520resolution%2520scales%252C%2520which%2520necessitates%2520retraining%250Awhenever%2520output%2520resolutions%2520are%2520changed.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520a%2520novel%2520continuous%2520low-rank%2520factorization%2520%2528CLoRF%2529%2520by%2520integrating%2520two%250Aneural%2520representations%2520into%2520the%2520matrix%2520factorization%252C%2520capturing%2520spatial%2520and%250Aspectral%2520information%252C%2520respectively.%2520This%2520approach%2520enables%2520us%2520to%2520harness%2520both%250Athe%2520low%2520rankness%2520from%2520the%2520matrix%2520factorization%2520and%2520the%2520continuity%2520from%2520neural%250Arepresentation%2520in%2520a%2520self-supervised%2520manner.Theoretically%252C%2520we%2520prove%2520the%2520low-rank%250Aproperty%2520and%2520Lipschitz%2520continuity%2520in%2520the%2520proposed%2520continuous%2520low-rank%250Afactorization.%2520Experimentally%252C%2520our%2520method%2520significantly%2520surpasses%2520existing%250Atechniques%2520and%2520achieves%2520user-desired%2520resolutions%2520without%2520the%2520need%2520for%2520neural%250Anetwork%2520retraining.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/wangting1907/CLoRF-Fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17818v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20and%20multispectral%20image%20fusion%20with%20arbitrary%20resolution%0A%20%20through%20self-supervised%20representations&entry.906535625=Ting%20Wang%20and%20Zipei%20Yan%20and%20Jizhou%20Li%20and%20Xile%20Zhao%20and%20Chao%20Wang%20and%20Michael%20Ng&entry.1292438233=%20%20The%20fusion%20of%20a%20low-resolution%20hyperspectral%20image%20%28LR-HSI%29%20with%20a%0Ahigh-resolution%20multispectral%20image%20%28HR-MSI%29%20has%20emerged%20as%20an%20effective%0Atechnique%20for%20achieving%20HSI%20super-resolution%20%28SR%29.%20Previous%20studies%20have%20mainly%0Aconcentrated%20on%20estimating%20the%20posterior%20distribution%20of%20the%20latent%0Ahigh-resolution%20hyperspectral%20image%20%28HR-HSI%29%2C%20leveraging%20an%20appropriate%20image%0Aprior%20and%20likelihood%20computed%20from%20the%20discrepancy%20between%20the%20latent%20HSI%20and%0Aobserved%20images.%20Low%20rankness%20stands%20out%20for%20preserving%20latent%20HSI%0Acharacteristics%20through%20matrix%20factorization%20among%20the%20various%20priors.%20However%2C%0Athe%20primary%20limitation%20in%20previous%20studies%20lies%20in%20the%20generalization%20of%20a%0Afusion%20model%20with%20fixed%20resolution%20scales%2C%20which%20necessitates%20retraining%0Awhenever%20output%20resolutions%20are%20changed.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20novel%20continuous%20low-rank%20factorization%20%28CLoRF%29%20by%20integrating%20two%0Aneural%20representations%20into%20the%20matrix%20factorization%2C%20capturing%20spatial%20and%0Aspectral%20information%2C%20respectively.%20This%20approach%20enables%20us%20to%20harness%20both%0Athe%20low%20rankness%20from%20the%20matrix%20factorization%20and%20the%20continuity%20from%20neural%0Arepresentation%20in%20a%20self-supervised%20manner.Theoretically%2C%20we%20prove%20the%20low-rank%0Aproperty%20and%20Lipschitz%20continuity%20in%20the%20proposed%20continuous%20low-rank%0Afactorization.%20Experimentally%2C%20our%20method%20significantly%20surpasses%20existing%0Atechniques%20and%20achieves%20user-desired%20resolutions%20without%20the%20need%20for%20neural%0Anetwork%20retraining.%20Code%20is%20available%20at%0Ahttps%3A//github.com/wangting1907/CLoRF-Fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17818v2&entry.124074799=Read"},
{"title": "DoubleCCA: Improving Foundation Model Group Robustness with Random\n  Sentence Embeddings", "author": "Hong Liu and Yitong Lu", "abstract": "  This paper presents a novel method to improve the robustness of foundation\nmodels to group-based biases. We propose a simple yet effective method, called\nDoubleCCA, that leverages random sentences and Canonical Correlation Analysis\n(CCA) to enrich the text embeddings of the foundation model. First, we generate\nvarious random sentences that augment the original prompts, which extends the\noriginal prompts with random words or character sequences. Second, we use an\nadditional sentence embedding model to generate different text embeddings with\nrespect to these random sentences. We then use CCA double twice to align the\nrepresentations and reconstruct them back to the original representation space.\nWe demonstrate the effectiveness of our method on a variety of tasks and\ndatasets, showing that it outperforms existing methods in terms of both\nperformance and robustness. Our method is simple to implement and can be easily\nintegrated into existing models, making it a practical solution for improving\nthe robustness of foundation models to group-based biases.\n", "link": "http://arxiv.org/abs/2411.16236v1", "date": "2024-11-25", "relevancy": 2.0572, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DoubleCCA%3A%20Improving%20Foundation%20Model%20Group%20Robustness%20with%20Random%0A%20%20Sentence%20Embeddings&body=Title%3A%20DoubleCCA%3A%20Improving%20Foundation%20Model%20Group%20Robustness%20with%20Random%0A%20%20Sentence%20Embeddings%0AAuthor%3A%20Hong%20Liu%20and%20Yitong%20Lu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20method%20to%20improve%20the%20robustness%20of%20foundation%0Amodels%20to%20group-based%20biases.%20We%20propose%20a%20simple%20yet%20effective%20method%2C%20called%0ADoubleCCA%2C%20that%20leverages%20random%20sentences%20and%20Canonical%20Correlation%20Analysis%0A%28CCA%29%20to%20enrich%20the%20text%20embeddings%20of%20the%20foundation%20model.%20First%2C%20we%20generate%0Avarious%20random%20sentences%20that%20augment%20the%20original%20prompts%2C%20which%20extends%20the%0Aoriginal%20prompts%20with%20random%20words%20or%20character%20sequences.%20Second%2C%20we%20use%20an%0Aadditional%20sentence%20embedding%20model%20to%20generate%20different%20text%20embeddings%20with%0Arespect%20to%20these%20random%20sentences.%20We%20then%20use%20CCA%20double%20twice%20to%20align%20the%0Arepresentations%20and%20reconstruct%20them%20back%20to%20the%20original%20representation%20space.%0AWe%20demonstrate%20the%20effectiveness%20of%20our%20method%20on%20a%20variety%20of%20tasks%20and%0Adatasets%2C%20showing%20that%20it%20outperforms%20existing%20methods%20in%20terms%20of%20both%0Aperformance%20and%20robustness.%20Our%20method%20is%20simple%20to%20implement%20and%20can%20be%20easily%0Aintegrated%20into%20existing%20models%2C%20making%20it%20a%20practical%20solution%20for%20improving%0Athe%20robustness%20of%20foundation%20models%20to%20group-based%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoubleCCA%253A%2520Improving%2520Foundation%2520Model%2520Group%2520Robustness%2520with%2520Random%250A%2520%2520Sentence%2520Embeddings%26entry.906535625%3DHong%2520Liu%2520and%2520Yitong%2520Lu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520method%2520to%2520improve%2520the%2520robustness%2520of%2520foundation%250Amodels%2520to%2520group-based%2520biases.%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520called%250ADoubleCCA%252C%2520that%2520leverages%2520random%2520sentences%2520and%2520Canonical%2520Correlation%2520Analysis%250A%2528CCA%2529%2520to%2520enrich%2520the%2520text%2520embeddings%2520of%2520the%2520foundation%2520model.%2520First%252C%2520we%2520generate%250Avarious%2520random%2520sentences%2520that%2520augment%2520the%2520original%2520prompts%252C%2520which%2520extends%2520the%250Aoriginal%2520prompts%2520with%2520random%2520words%2520or%2520character%2520sequences.%2520Second%252C%2520we%2520use%2520an%250Aadditional%2520sentence%2520embedding%2520model%2520to%2520generate%2520different%2520text%2520embeddings%2520with%250Arespect%2520to%2520these%2520random%2520sentences.%2520We%2520then%2520use%2520CCA%2520double%2520twice%2520to%2520align%2520the%250Arepresentations%2520and%2520reconstruct%2520them%2520back%2520to%2520the%2520original%2520representation%2520space.%250AWe%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520a%2520variety%2520of%2520tasks%2520and%250Adatasets%252C%2520showing%2520that%2520it%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520both%250Aperformance%2520and%2520robustness.%2520Our%2520method%2520is%2520simple%2520to%2520implement%2520and%2520can%2520be%2520easily%250Aintegrated%2520into%2520existing%2520models%252C%2520making%2520it%2520a%2520practical%2520solution%2520for%2520improving%250Athe%2520robustness%2520of%2520foundation%2520models%2520to%2520group-based%2520biases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoubleCCA%3A%20Improving%20Foundation%20Model%20Group%20Robustness%20with%20Random%0A%20%20Sentence%20Embeddings&entry.906535625=Hong%20Liu%20and%20Yitong%20Lu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20method%20to%20improve%20the%20robustness%20of%20foundation%0Amodels%20to%20group-based%20biases.%20We%20propose%20a%20simple%20yet%20effective%20method%2C%20called%0ADoubleCCA%2C%20that%20leverages%20random%20sentences%20and%20Canonical%20Correlation%20Analysis%0A%28CCA%29%20to%20enrich%20the%20text%20embeddings%20of%20the%20foundation%20model.%20First%2C%20we%20generate%0Avarious%20random%20sentences%20that%20augment%20the%20original%20prompts%2C%20which%20extends%20the%0Aoriginal%20prompts%20with%20random%20words%20or%20character%20sequences.%20Second%2C%20we%20use%20an%0Aadditional%20sentence%20embedding%20model%20to%20generate%20different%20text%20embeddings%20with%0Arespect%20to%20these%20random%20sentences.%20We%20then%20use%20CCA%20double%20twice%20to%20align%20the%0Arepresentations%20and%20reconstruct%20them%20back%20to%20the%20original%20representation%20space.%0AWe%20demonstrate%20the%20effectiveness%20of%20our%20method%20on%20a%20variety%20of%20tasks%20and%0Adatasets%2C%20showing%20that%20it%20outperforms%20existing%20methods%20in%20terms%20of%20both%0Aperformance%20and%20robustness.%20Our%20method%20is%20simple%20to%20implement%20and%20can%20be%20easily%0Aintegrated%20into%20existing%20models%2C%20making%20it%20a%20practical%20solution%20for%20improving%0Athe%20robustness%20of%20foundation%20models%20to%20group-based%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16236v1&entry.124074799=Read"},
{"title": "Probing for Consciousness in Machines", "author": "Mathis Immertreu and Achim Schilling and Andreas Maier and Patrick Krauss", "abstract": "  This study explores the potential for artificial agents to develop core\nconsciousness, as proposed by Antonio Damasio's theory of consciousness.\nAccording to Damasio, the emergence of core consciousness relies on the\nintegration of a self model, informed by representations of emotions and\nfeelings, and a world model. We hypothesize that an artificial agent, trained\nvia reinforcement learning (RL) in a virtual environment, can develop\npreliminary forms of these models as a byproduct of its primary task. The\nagent's main objective is to learn to play a video game and explore the\nenvironment. To evaluate the emergence of world and self models, we employ\nprobes-feedforward classifiers that use the activations of the trained agent's\nneural networks to predict the spatial positions of the agent itself. Our\nresults demonstrate that the agent can form rudimentary world and self models,\nsuggesting a pathway toward developing machine consciousness. This research\nprovides foundational insights into the capabilities of artificial agents in\nmirroring aspects of human consciousness, with implications for future\nadvancements in artificial intelligence.\n", "link": "http://arxiv.org/abs/2411.16262v1", "date": "2024-11-25", "relevancy": 2.0545, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20for%20Consciousness%20in%20Machines&body=Title%3A%20Probing%20for%20Consciousness%20in%20Machines%0AAuthor%3A%20Mathis%20Immertreu%20and%20Achim%20Schilling%20and%20Andreas%20Maier%20and%20Patrick%20Krauss%0AAbstract%3A%20%20%20This%20study%20explores%20the%20potential%20for%20artificial%20agents%20to%20develop%20core%0Aconsciousness%2C%20as%20proposed%20by%20Antonio%20Damasio%27s%20theory%20of%20consciousness.%0AAccording%20to%20Damasio%2C%20the%20emergence%20of%20core%20consciousness%20relies%20on%20the%0Aintegration%20of%20a%20self%20model%2C%20informed%20by%20representations%20of%20emotions%20and%0Afeelings%2C%20and%20a%20world%20model.%20We%20hypothesize%20that%20an%20artificial%20agent%2C%20trained%0Avia%20reinforcement%20learning%20%28RL%29%20in%20a%20virtual%20environment%2C%20can%20develop%0Apreliminary%20forms%20of%20these%20models%20as%20a%20byproduct%20of%20its%20primary%20task.%20The%0Aagent%27s%20main%20objective%20is%20to%20learn%20to%20play%20a%20video%20game%20and%20explore%20the%0Aenvironment.%20To%20evaluate%20the%20emergence%20of%20world%20and%20self%20models%2C%20we%20employ%0Aprobes-feedforward%20classifiers%20that%20use%20the%20activations%20of%20the%20trained%20agent%27s%0Aneural%20networks%20to%20predict%20the%20spatial%20positions%20of%20the%20agent%20itself.%20Our%0Aresults%20demonstrate%20that%20the%20agent%20can%20form%20rudimentary%20world%20and%20self%20models%2C%0Asuggesting%20a%20pathway%20toward%20developing%20machine%20consciousness.%20This%20research%0Aprovides%20foundational%20insights%20into%20the%20capabilities%20of%20artificial%20agents%20in%0Amirroring%20aspects%20of%20human%20consciousness%2C%20with%20implications%20for%20future%0Aadvancements%20in%20artificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520for%2520Consciousness%2520in%2520Machines%26entry.906535625%3DMathis%2520Immertreu%2520and%2520Achim%2520Schilling%2520and%2520Andreas%2520Maier%2520and%2520Patrick%2520Krauss%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520potential%2520for%2520artificial%2520agents%2520to%2520develop%2520core%250Aconsciousness%252C%2520as%2520proposed%2520by%2520Antonio%2520Damasio%2527s%2520theory%2520of%2520consciousness.%250AAccording%2520to%2520Damasio%252C%2520the%2520emergence%2520of%2520core%2520consciousness%2520relies%2520on%2520the%250Aintegration%2520of%2520a%2520self%2520model%252C%2520informed%2520by%2520representations%2520of%2520emotions%2520and%250Afeelings%252C%2520and%2520a%2520world%2520model.%2520We%2520hypothesize%2520that%2520an%2520artificial%2520agent%252C%2520trained%250Avia%2520reinforcement%2520learning%2520%2528RL%2529%2520in%2520a%2520virtual%2520environment%252C%2520can%2520develop%250Apreliminary%2520forms%2520of%2520these%2520models%2520as%2520a%2520byproduct%2520of%2520its%2520primary%2520task.%2520The%250Aagent%2527s%2520main%2520objective%2520is%2520to%2520learn%2520to%2520play%2520a%2520video%2520game%2520and%2520explore%2520the%250Aenvironment.%2520To%2520evaluate%2520the%2520emergence%2520of%2520world%2520and%2520self%2520models%252C%2520we%2520employ%250Aprobes-feedforward%2520classifiers%2520that%2520use%2520the%2520activations%2520of%2520the%2520trained%2520agent%2527s%250Aneural%2520networks%2520to%2520predict%2520the%2520spatial%2520positions%2520of%2520the%2520agent%2520itself.%2520Our%250Aresults%2520demonstrate%2520that%2520the%2520agent%2520can%2520form%2520rudimentary%2520world%2520and%2520self%2520models%252C%250Asuggesting%2520a%2520pathway%2520toward%2520developing%2520machine%2520consciousness.%2520This%2520research%250Aprovides%2520foundational%2520insights%2520into%2520the%2520capabilities%2520of%2520artificial%2520agents%2520in%250Amirroring%2520aspects%2520of%2520human%2520consciousness%252C%2520with%2520implications%2520for%2520future%250Aadvancements%2520in%2520artificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20for%20Consciousness%20in%20Machines&entry.906535625=Mathis%20Immertreu%20and%20Achim%20Schilling%20and%20Andreas%20Maier%20and%20Patrick%20Krauss&entry.1292438233=%20%20This%20study%20explores%20the%20potential%20for%20artificial%20agents%20to%20develop%20core%0Aconsciousness%2C%20as%20proposed%20by%20Antonio%20Damasio%27s%20theory%20of%20consciousness.%0AAccording%20to%20Damasio%2C%20the%20emergence%20of%20core%20consciousness%20relies%20on%20the%0Aintegration%20of%20a%20self%20model%2C%20informed%20by%20representations%20of%20emotions%20and%0Afeelings%2C%20and%20a%20world%20model.%20We%20hypothesize%20that%20an%20artificial%20agent%2C%20trained%0Avia%20reinforcement%20learning%20%28RL%29%20in%20a%20virtual%20environment%2C%20can%20develop%0Apreliminary%20forms%20of%20these%20models%20as%20a%20byproduct%20of%20its%20primary%20task.%20The%0Aagent%27s%20main%20objective%20is%20to%20learn%20to%20play%20a%20video%20game%20and%20explore%20the%0Aenvironment.%20To%20evaluate%20the%20emergence%20of%20world%20and%20self%20models%2C%20we%20employ%0Aprobes-feedforward%20classifiers%20that%20use%20the%20activations%20of%20the%20trained%20agent%27s%0Aneural%20networks%20to%20predict%20the%20spatial%20positions%20of%20the%20agent%20itself.%20Our%0Aresults%20demonstrate%20that%20the%20agent%20can%20form%20rudimentary%20world%20and%20self%20models%2C%0Asuggesting%20a%20pathway%20toward%20developing%20machine%20consciousness.%20This%20research%0Aprovides%20foundational%20insights%20into%20the%20capabilities%20of%20artificial%20agents%20in%0Amirroring%20aspects%20of%20human%20consciousness%2C%20with%20implications%20for%20future%0Aadvancements%20in%20artificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16262v1&entry.124074799=Read"},
{"title": "LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation", "author": "Steven Song and Anirudh Subramanyam and Irene Madejski and Robert L. Grossman", "abstract": "  In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage.\n", "link": "http://arxiv.org/abs/2411.16523v1", "date": "2024-11-25", "relevancy": 2.0479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5216}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5109}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaB-RAG%3A%20Label%20Boosted%20Retrieval%20Augmented%20Generation%20for%20Radiology%0A%20%20Report%20Generation&body=Title%3A%20LaB-RAG%3A%20Label%20Boosted%20Retrieval%20Augmented%20Generation%20for%20Radiology%0A%20%20Report%20Generation%0AAuthor%3A%20Steven%20Song%20and%20Anirudh%20Subramanyam%20and%20Irene%20Madejski%20and%20Robert%20L.%20Grossman%0AAbstract%3A%20%20%20In%20the%20current%20paradigm%20of%20image%20captioning%2C%20deep%20learning%20models%20are%20trained%0Ato%20generate%20text%20from%20image%20embeddings%20of%20latent%20features.%20We%20challenge%20the%0Aassumption%20that%20these%20latent%20features%20ought%20to%20be%20high-dimensional%20vectors%0Awhich%20require%20model%20fine%20tuning%20to%20handle.%20Here%20we%20propose%20Label%20Boosted%0ARetrieval%20Augmented%20Generation%20%28LaB-RAG%29%2C%20a%20text-based%20approach%20to%20image%0Acaptioning%20that%20leverages%20image%20descriptors%20in%20the%20form%20of%20categorical%20labels%0Ato%20boost%20standard%20retrieval%20augmented%20generation%20%28RAG%29%20with%20pretrained%20large%0Alanguage%20models%20%28LLMs%29.%20We%20study%20our%20method%20in%20the%20context%20of%20radiology%20report%0Ageneration%20%28RRG%29%2C%20where%20the%20task%20is%20to%20generate%20a%20clinician%27s%20report%20detailing%0Atheir%20observations%20from%20a%20set%20of%20radiological%20images%2C%20such%20as%20X-rays.%20We%20argue%0Athat%20simple%20linear%20classifiers%20over%20extracted%20image%20embeddings%20can%20effectively%0Atransform%20X-rays%20into%20text-space%20as%20radiology-specific%20labels.%20In%20combination%0Awith%20standard%20RAG%2C%20we%20show%20that%20these%20derived%20text%20labels%20can%20be%20used%20with%0Ageneral-domain%20LLMs%20to%20generate%20radiology%20reports.%20Without%20ever%20training%20our%0Agenerative%20language%20model%20or%20image%20feature%20encoder%20models%2C%20and%20without%20ever%0Adirectly%20%22showing%22%20the%20LLM%20an%20X-ray%2C%20we%20demonstrate%20that%20LaB-RAG%20achieves%0Abetter%20results%20across%20natural%20language%20and%20radiology%20language%20metrics%20compared%0Awith%20other%20retrieval-based%20RRG%20methods%2C%20while%20attaining%20competitive%20results%0Acompared%20to%20other%20fine-tuned%20vision-language%20RRG%20models.%20We%20further%20present%0Aresults%20of%20our%20experiments%20with%20various%20components%20of%20LaB-RAG%20to%20better%0Aunderstand%20our%20method.%20Finally%2C%20we%20critique%20the%20use%20of%20a%20popular%20RRG%20metric%2C%0Aarguing%20it%20is%20possible%20to%20artificially%20inflate%20its%20results%20without%20true%0Adata-leakage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaB-RAG%253A%2520Label%2520Boosted%2520Retrieval%2520Augmented%2520Generation%2520for%2520Radiology%250A%2520%2520Report%2520Generation%26entry.906535625%3DSteven%2520Song%2520and%2520Anirudh%2520Subramanyam%2520and%2520Irene%2520Madejski%2520and%2520Robert%2520L.%2520Grossman%26entry.1292438233%3D%2520%2520In%2520the%2520current%2520paradigm%2520of%2520image%2520captioning%252C%2520deep%2520learning%2520models%2520are%2520trained%250Ato%2520generate%2520text%2520from%2520image%2520embeddings%2520of%2520latent%2520features.%2520We%2520challenge%2520the%250Aassumption%2520that%2520these%2520latent%2520features%2520ought%2520to%2520be%2520high-dimensional%2520vectors%250Awhich%2520require%2520model%2520fine%2520tuning%2520to%2520handle.%2520Here%2520we%2520propose%2520Label%2520Boosted%250ARetrieval%2520Augmented%2520Generation%2520%2528LaB-RAG%2529%252C%2520a%2520text-based%2520approach%2520to%2520image%250Acaptioning%2520that%2520leverages%2520image%2520descriptors%2520in%2520the%2520form%2520of%2520categorical%2520labels%250Ato%2520boost%2520standard%2520retrieval%2520augmented%2520generation%2520%2528RAG%2529%2520with%2520pretrained%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520We%2520study%2520our%2520method%2520in%2520the%2520context%2520of%2520radiology%2520report%250Ageneration%2520%2528RRG%2529%252C%2520where%2520the%2520task%2520is%2520to%2520generate%2520a%2520clinician%2527s%2520report%2520detailing%250Atheir%2520observations%2520from%2520a%2520set%2520of%2520radiological%2520images%252C%2520such%2520as%2520X-rays.%2520We%2520argue%250Athat%2520simple%2520linear%2520classifiers%2520over%2520extracted%2520image%2520embeddings%2520can%2520effectively%250Atransform%2520X-rays%2520into%2520text-space%2520as%2520radiology-specific%2520labels.%2520In%2520combination%250Awith%2520standard%2520RAG%252C%2520we%2520show%2520that%2520these%2520derived%2520text%2520labels%2520can%2520be%2520used%2520with%250Ageneral-domain%2520LLMs%2520to%2520generate%2520radiology%2520reports.%2520Without%2520ever%2520training%2520our%250Agenerative%2520language%2520model%2520or%2520image%2520feature%2520encoder%2520models%252C%2520and%2520without%2520ever%250Adirectly%2520%2522showing%2522%2520the%2520LLM%2520an%2520X-ray%252C%2520we%2520demonstrate%2520that%2520LaB-RAG%2520achieves%250Abetter%2520results%2520across%2520natural%2520language%2520and%2520radiology%2520language%2520metrics%2520compared%250Awith%2520other%2520retrieval-based%2520RRG%2520methods%252C%2520while%2520attaining%2520competitive%2520results%250Acompared%2520to%2520other%2520fine-tuned%2520vision-language%2520RRG%2520models.%2520We%2520further%2520present%250Aresults%2520of%2520our%2520experiments%2520with%2520various%2520components%2520of%2520LaB-RAG%2520to%2520better%250Aunderstand%2520our%2520method.%2520Finally%252C%2520we%2520critique%2520the%2520use%2520of%2520a%2520popular%2520RRG%2520metric%252C%250Aarguing%2520it%2520is%2520possible%2520to%2520artificially%2520inflate%2520its%2520results%2520without%2520true%250Adata-leakage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaB-RAG%3A%20Label%20Boosted%20Retrieval%20Augmented%20Generation%20for%20Radiology%0A%20%20Report%20Generation&entry.906535625=Steven%20Song%20and%20Anirudh%20Subramanyam%20and%20Irene%20Madejski%20and%20Robert%20L.%20Grossman&entry.1292438233=%20%20In%20the%20current%20paradigm%20of%20image%20captioning%2C%20deep%20learning%20models%20are%20trained%0Ato%20generate%20text%20from%20image%20embeddings%20of%20latent%20features.%20We%20challenge%20the%0Aassumption%20that%20these%20latent%20features%20ought%20to%20be%20high-dimensional%20vectors%0Awhich%20require%20model%20fine%20tuning%20to%20handle.%20Here%20we%20propose%20Label%20Boosted%0ARetrieval%20Augmented%20Generation%20%28LaB-RAG%29%2C%20a%20text-based%20approach%20to%20image%0Acaptioning%20that%20leverages%20image%20descriptors%20in%20the%20form%20of%20categorical%20labels%0Ato%20boost%20standard%20retrieval%20augmented%20generation%20%28RAG%29%20with%20pretrained%20large%0Alanguage%20models%20%28LLMs%29.%20We%20study%20our%20method%20in%20the%20context%20of%20radiology%20report%0Ageneration%20%28RRG%29%2C%20where%20the%20task%20is%20to%20generate%20a%20clinician%27s%20report%20detailing%0Atheir%20observations%20from%20a%20set%20of%20radiological%20images%2C%20such%20as%20X-rays.%20We%20argue%0Athat%20simple%20linear%20classifiers%20over%20extracted%20image%20embeddings%20can%20effectively%0Atransform%20X-rays%20into%20text-space%20as%20radiology-specific%20labels.%20In%20combination%0Awith%20standard%20RAG%2C%20we%20show%20that%20these%20derived%20text%20labels%20can%20be%20used%20with%0Ageneral-domain%20LLMs%20to%20generate%20radiology%20reports.%20Without%20ever%20training%20our%0Agenerative%20language%20model%20or%20image%20feature%20encoder%20models%2C%20and%20without%20ever%0Adirectly%20%22showing%22%20the%20LLM%20an%20X-ray%2C%20we%20demonstrate%20that%20LaB-RAG%20achieves%0Abetter%20results%20across%20natural%20language%20and%20radiology%20language%20metrics%20compared%0Awith%20other%20retrieval-based%20RRG%20methods%2C%20while%20attaining%20competitive%20results%0Acompared%20to%20other%20fine-tuned%20vision-language%20RRG%20models.%20We%20further%20present%0Aresults%20of%20our%20experiments%20with%20various%20components%20of%20LaB-RAG%20to%20better%0Aunderstand%20our%20method.%20Finally%2C%20we%20critique%20the%20use%20of%20a%20popular%20RRG%20metric%2C%0Aarguing%20it%20is%20possible%20to%20artificially%20inflate%20its%20results%20without%20true%0Adata-leakage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16523v1&entry.124074799=Read"},
{"title": "Towards Foundation Models for Critical Care Time Series", "author": "Manuel Burger and Fedor Sergeev and Malte Londschien and Daphn\u00e9 Chopard and Hugo Y\u00e8che and Eike Gerdes and Polina Leshetkina and Alexander Morgenroth and Zeynep Bab\u00fcr and Jasmina Bogojeska and Martin Faltys and Rita Kuznetsova and Gunnar R\u00e4tsch", "abstract": "  Notable progress has been made in generalist medical large language models\nacross various healthcare areas. However, large-scale modeling of in-hospital\ntime series data - such as vital signs, lab results, and treatments in critical\ncare - remains underexplored. Existing datasets are relatively small, but\ncombining them can enhance patient diversity and improve model robustness. To\neffectively utilize these combined datasets for large-scale modeling, it is\nessential to address the distribution shifts caused by varying treatment\npolicies, necessitating the harmonization of treatment variables across the\ndifferent datasets. This work aims to establish a foundation for training\nlarge-scale multi-variate time series models on critical care data and to\nprovide a benchmark for machine learning models in transfer learning across\nhospitals to study and address distribution shift challenges. We introduce a\nharmonized dataset for sequence modeling and transfer learning research,\nrepresenting the first large-scale collection to include core treatment\nvariables. Future plans involve expanding this dataset to support further\nadvancements in transfer learning and the development of scalable,\ngeneralizable models for critical healthcare applications.\n", "link": "http://arxiv.org/abs/2411.16346v1", "date": "2024-11-25", "relevancy": 1.8746, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Foundation%20Models%20for%20Critical%20Care%20Time%20Series&body=Title%3A%20Towards%20Foundation%20Models%20for%20Critical%20Care%20Time%20Series%0AAuthor%3A%20Manuel%20Burger%20and%20Fedor%20Sergeev%20and%20Malte%20Londschien%20and%20Daphn%C3%A9%20Chopard%20and%20Hugo%20Y%C3%A8che%20and%20Eike%20Gerdes%20and%20Polina%20Leshetkina%20and%20Alexander%20Morgenroth%20and%20Zeynep%20Bab%C3%BCr%20and%20Jasmina%20Bogojeska%20and%20Martin%20Faltys%20and%20Rita%20Kuznetsova%20and%20Gunnar%20R%C3%A4tsch%0AAbstract%3A%20%20%20Notable%20progress%20has%20been%20made%20in%20generalist%20medical%20large%20language%20models%0Aacross%20various%20healthcare%20areas.%20However%2C%20large-scale%20modeling%20of%20in-hospital%0Atime%20series%20data%20-%20such%20as%20vital%20signs%2C%20lab%20results%2C%20and%20treatments%20in%20critical%0Acare%20-%20remains%20underexplored.%20Existing%20datasets%20are%20relatively%20small%2C%20but%0Acombining%20them%20can%20enhance%20patient%20diversity%20and%20improve%20model%20robustness.%20To%0Aeffectively%20utilize%20these%20combined%20datasets%20for%20large-scale%20modeling%2C%20it%20is%0Aessential%20to%20address%20the%20distribution%20shifts%20caused%20by%20varying%20treatment%0Apolicies%2C%20necessitating%20the%20harmonization%20of%20treatment%20variables%20across%20the%0Adifferent%20datasets.%20This%20work%20aims%20to%20establish%20a%20foundation%20for%20training%0Alarge-scale%20multi-variate%20time%20series%20models%20on%20critical%20care%20data%20and%20to%0Aprovide%20a%20benchmark%20for%20machine%20learning%20models%20in%20transfer%20learning%20across%0Ahospitals%20to%20study%20and%20address%20distribution%20shift%20challenges.%20We%20introduce%20a%0Aharmonized%20dataset%20for%20sequence%20modeling%20and%20transfer%20learning%20research%2C%0Arepresenting%20the%20first%20large-scale%20collection%20to%20include%20core%20treatment%0Avariables.%20Future%20plans%20involve%20expanding%20this%20dataset%20to%20support%20further%0Aadvancements%20in%20transfer%20learning%20and%20the%20development%20of%20scalable%2C%0Ageneralizable%20models%20for%20critical%20healthcare%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Foundation%2520Models%2520for%2520Critical%2520Care%2520Time%2520Series%26entry.906535625%3DManuel%2520Burger%2520and%2520Fedor%2520Sergeev%2520and%2520Malte%2520Londschien%2520and%2520Daphn%25C3%25A9%2520Chopard%2520and%2520Hugo%2520Y%25C3%25A8che%2520and%2520Eike%2520Gerdes%2520and%2520Polina%2520Leshetkina%2520and%2520Alexander%2520Morgenroth%2520and%2520Zeynep%2520Bab%25C3%25BCr%2520and%2520Jasmina%2520Bogojeska%2520and%2520Martin%2520Faltys%2520and%2520Rita%2520Kuznetsova%2520and%2520Gunnar%2520R%25C3%25A4tsch%26entry.1292438233%3D%2520%2520Notable%2520progress%2520has%2520been%2520made%2520in%2520generalist%2520medical%2520large%2520language%2520models%250Aacross%2520various%2520healthcare%2520areas.%2520However%252C%2520large-scale%2520modeling%2520of%2520in-hospital%250Atime%2520series%2520data%2520-%2520such%2520as%2520vital%2520signs%252C%2520lab%2520results%252C%2520and%2520treatments%2520in%2520critical%250Acare%2520-%2520remains%2520underexplored.%2520Existing%2520datasets%2520are%2520relatively%2520small%252C%2520but%250Acombining%2520them%2520can%2520enhance%2520patient%2520diversity%2520and%2520improve%2520model%2520robustness.%2520To%250Aeffectively%2520utilize%2520these%2520combined%2520datasets%2520for%2520large-scale%2520modeling%252C%2520it%2520is%250Aessential%2520to%2520address%2520the%2520distribution%2520shifts%2520caused%2520by%2520varying%2520treatment%250Apolicies%252C%2520necessitating%2520the%2520harmonization%2520of%2520treatment%2520variables%2520across%2520the%250Adifferent%2520datasets.%2520This%2520work%2520aims%2520to%2520establish%2520a%2520foundation%2520for%2520training%250Alarge-scale%2520multi-variate%2520time%2520series%2520models%2520on%2520critical%2520care%2520data%2520and%2520to%250Aprovide%2520a%2520benchmark%2520for%2520machine%2520learning%2520models%2520in%2520transfer%2520learning%2520across%250Ahospitals%2520to%2520study%2520and%2520address%2520distribution%2520shift%2520challenges.%2520We%2520introduce%2520a%250Aharmonized%2520dataset%2520for%2520sequence%2520modeling%2520and%2520transfer%2520learning%2520research%252C%250Arepresenting%2520the%2520first%2520large-scale%2520collection%2520to%2520include%2520core%2520treatment%250Avariables.%2520Future%2520plans%2520involve%2520expanding%2520this%2520dataset%2520to%2520support%2520further%250Aadvancements%2520in%2520transfer%2520learning%2520and%2520the%2520development%2520of%2520scalable%252C%250Ageneralizable%2520models%2520for%2520critical%2520healthcare%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Foundation%20Models%20for%20Critical%20Care%20Time%20Series&entry.906535625=Manuel%20Burger%20and%20Fedor%20Sergeev%20and%20Malte%20Londschien%20and%20Daphn%C3%A9%20Chopard%20and%20Hugo%20Y%C3%A8che%20and%20Eike%20Gerdes%20and%20Polina%20Leshetkina%20and%20Alexander%20Morgenroth%20and%20Zeynep%20Bab%C3%BCr%20and%20Jasmina%20Bogojeska%20and%20Martin%20Faltys%20and%20Rita%20Kuznetsova%20and%20Gunnar%20R%C3%A4tsch&entry.1292438233=%20%20Notable%20progress%20has%20been%20made%20in%20generalist%20medical%20large%20language%20models%0Aacross%20various%20healthcare%20areas.%20However%2C%20large-scale%20modeling%20of%20in-hospital%0Atime%20series%20data%20-%20such%20as%20vital%20signs%2C%20lab%20results%2C%20and%20treatments%20in%20critical%0Acare%20-%20remains%20underexplored.%20Existing%20datasets%20are%20relatively%20small%2C%20but%0Acombining%20them%20can%20enhance%20patient%20diversity%20and%20improve%20model%20robustness.%20To%0Aeffectively%20utilize%20these%20combined%20datasets%20for%20large-scale%20modeling%2C%20it%20is%0Aessential%20to%20address%20the%20distribution%20shifts%20caused%20by%20varying%20treatment%0Apolicies%2C%20necessitating%20the%20harmonization%20of%20treatment%20variables%20across%20the%0Adifferent%20datasets.%20This%20work%20aims%20to%20establish%20a%20foundation%20for%20training%0Alarge-scale%20multi-variate%20time%20series%20models%20on%20critical%20care%20data%20and%20to%0Aprovide%20a%20benchmark%20for%20machine%20learning%20models%20in%20transfer%20learning%20across%0Ahospitals%20to%20study%20and%20address%20distribution%20shift%20challenges.%20We%20introduce%20a%0Aharmonized%20dataset%20for%20sequence%20modeling%20and%20transfer%20learning%20research%2C%0Arepresenting%20the%20first%20large-scale%20collection%20to%20include%20core%20treatment%0Avariables.%20Future%20plans%20involve%20expanding%20this%20dataset%20to%20support%20further%0Aadvancements%20in%20transfer%20learning%20and%20the%20development%20of%20scalable%2C%0Ageneralizable%20models%20for%20critical%20healthcare%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16346v1&entry.124074799=Read"},
{"title": "Auditing for Human Expertise", "author": "Rohan Alur and Loren Laine and Darrick K. Li and Manish Raghavan and Devavrat Shah and Dennis Shung", "abstract": "  High-stakes prediction tasks (e.g., patient diagnosis) are often handled by\ntrained human experts. A common source of concern about automation in these\nsettings is that experts may exercise intuition that is difficult to model\nand/or have access to information (e.g., conversations with a patient) that is\nsimply unavailable to a would-be algorithm. This raises a natural question\nwhether human experts add value which could not be captured by an algorithmic\npredictor. We develop a statistical framework under which we can pose this\nquestion as a natural hypothesis test. Indeed, as our framework highlights,\ndetecting human expertise is more subtle than simply comparing the accuracy of\nexpert predictions to those made by a particular learning algorithm. Instead,\nwe propose a simple procedure which tests whether expert predictions are\nstatistically independent from the outcomes of interest after conditioning on\nthe available inputs (`features'). A rejection of our test thus suggests that\nhuman experts may add value to any algorithm trained on the available data, and\nhas direct implications for whether human-AI `complementarity' is achievable in\na given prediction task. We highlight the utility of our procedure using\nadmissions data collected from the emergency department of a large academic\nhospital system, where we show that physicians' admit/discharge decisions for\npatients with acute gastrointestinal bleeding (AGIB) appear to be incorporating\ninformation that is not available to a standard algorithmic screening tool.\nThis is despite the fact that the screening tool is arguably more accurate than\nphysicians' discretionary decisions, highlighting that -- even absent normative\nconcerns about accountability or interpretability -- accuracy is insufficient\nto justify algorithmic automation.\n", "link": "http://arxiv.org/abs/2306.01646v3", "date": "2024-11-25", "relevancy": 1.4252, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4928}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4737}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auditing%20for%20Human%20Expertise&body=Title%3A%20Auditing%20for%20Human%20Expertise%0AAuthor%3A%20Rohan%20Alur%20and%20Loren%20Laine%20and%20Darrick%20K.%20Li%20and%20Manish%20Raghavan%20and%20Devavrat%20Shah%20and%20Dennis%20Shung%0AAbstract%3A%20%20%20High-stakes%20prediction%20tasks%20%28e.g.%2C%20patient%20diagnosis%29%20are%20often%20handled%20by%0Atrained%20human%20experts.%20A%20common%20source%20of%20concern%20about%20automation%20in%20these%0Asettings%20is%20that%20experts%20may%20exercise%20intuition%20that%20is%20difficult%20to%20model%0Aand/or%20have%20access%20to%20information%20%28e.g.%2C%20conversations%20with%20a%20patient%29%20that%20is%0Asimply%20unavailable%20to%20a%20would-be%20algorithm.%20This%20raises%20a%20natural%20question%0Awhether%20human%20experts%20add%20value%20which%20could%20not%20be%20captured%20by%20an%20algorithmic%0Apredictor.%20We%20develop%20a%20statistical%20framework%20under%20which%20we%20can%20pose%20this%0Aquestion%20as%20a%20natural%20hypothesis%20test.%20Indeed%2C%20as%20our%20framework%20highlights%2C%0Adetecting%20human%20expertise%20is%20more%20subtle%20than%20simply%20comparing%20the%20accuracy%20of%0Aexpert%20predictions%20to%20those%20made%20by%20a%20particular%20learning%20algorithm.%20Instead%2C%0Awe%20propose%20a%20simple%20procedure%20which%20tests%20whether%20expert%20predictions%20are%0Astatistically%20independent%20from%20the%20outcomes%20of%20interest%20after%20conditioning%20on%0Athe%20available%20inputs%20%28%60features%27%29.%20A%20rejection%20of%20our%20test%20thus%20suggests%20that%0Ahuman%20experts%20may%20add%20value%20to%20any%20algorithm%20trained%20on%20the%20available%20data%2C%20and%0Ahas%20direct%20implications%20for%20whether%20human-AI%20%60complementarity%27%20is%20achievable%20in%0Aa%20given%20prediction%20task.%20We%20highlight%20the%20utility%20of%20our%20procedure%20using%0Aadmissions%20data%20collected%20from%20the%20emergency%20department%20of%20a%20large%20academic%0Ahospital%20system%2C%20where%20we%20show%20that%20physicians%27%20admit/discharge%20decisions%20for%0Apatients%20with%20acute%20gastrointestinal%20bleeding%20%28AGIB%29%20appear%20to%20be%20incorporating%0Ainformation%20that%20is%20not%20available%20to%20a%20standard%20algorithmic%20screening%20tool.%0AThis%20is%20despite%20the%20fact%20that%20the%20screening%20tool%20is%20arguably%20more%20accurate%20than%0Aphysicians%27%20discretionary%20decisions%2C%20highlighting%20that%20--%20even%20absent%20normative%0Aconcerns%20about%20accountability%20or%20interpretability%20--%20accuracy%20is%20insufficient%0Ato%20justify%20algorithmic%20automation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.01646v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuditing%2520for%2520Human%2520Expertise%26entry.906535625%3DRohan%2520Alur%2520and%2520Loren%2520Laine%2520and%2520Darrick%2520K.%2520Li%2520and%2520Manish%2520Raghavan%2520and%2520Devavrat%2520Shah%2520and%2520Dennis%2520Shung%26entry.1292438233%3D%2520%2520High-stakes%2520prediction%2520tasks%2520%2528e.g.%252C%2520patient%2520diagnosis%2529%2520are%2520often%2520handled%2520by%250Atrained%2520human%2520experts.%2520A%2520common%2520source%2520of%2520concern%2520about%2520automation%2520in%2520these%250Asettings%2520is%2520that%2520experts%2520may%2520exercise%2520intuition%2520that%2520is%2520difficult%2520to%2520model%250Aand/or%2520have%2520access%2520to%2520information%2520%2528e.g.%252C%2520conversations%2520with%2520a%2520patient%2529%2520that%2520is%250Asimply%2520unavailable%2520to%2520a%2520would-be%2520algorithm.%2520This%2520raises%2520a%2520natural%2520question%250Awhether%2520human%2520experts%2520add%2520value%2520which%2520could%2520not%2520be%2520captured%2520by%2520an%2520algorithmic%250Apredictor.%2520We%2520develop%2520a%2520statistical%2520framework%2520under%2520which%2520we%2520can%2520pose%2520this%250Aquestion%2520as%2520a%2520natural%2520hypothesis%2520test.%2520Indeed%252C%2520as%2520our%2520framework%2520highlights%252C%250Adetecting%2520human%2520expertise%2520is%2520more%2520subtle%2520than%2520simply%2520comparing%2520the%2520accuracy%2520of%250Aexpert%2520predictions%2520to%2520those%2520made%2520by%2520a%2520particular%2520learning%2520algorithm.%2520Instead%252C%250Awe%2520propose%2520a%2520simple%2520procedure%2520which%2520tests%2520whether%2520expert%2520predictions%2520are%250Astatistically%2520independent%2520from%2520the%2520outcomes%2520of%2520interest%2520after%2520conditioning%2520on%250Athe%2520available%2520inputs%2520%2528%2560features%2527%2529.%2520A%2520rejection%2520of%2520our%2520test%2520thus%2520suggests%2520that%250Ahuman%2520experts%2520may%2520add%2520value%2520to%2520any%2520algorithm%2520trained%2520on%2520the%2520available%2520data%252C%2520and%250Ahas%2520direct%2520implications%2520for%2520whether%2520human-AI%2520%2560complementarity%2527%2520is%2520achievable%2520in%250Aa%2520given%2520prediction%2520task.%2520We%2520highlight%2520the%2520utility%2520of%2520our%2520procedure%2520using%250Aadmissions%2520data%2520collected%2520from%2520the%2520emergency%2520department%2520of%2520a%2520large%2520academic%250Ahospital%2520system%252C%2520where%2520we%2520show%2520that%2520physicians%2527%2520admit/discharge%2520decisions%2520for%250Apatients%2520with%2520acute%2520gastrointestinal%2520bleeding%2520%2528AGIB%2529%2520appear%2520to%2520be%2520incorporating%250Ainformation%2520that%2520is%2520not%2520available%2520to%2520a%2520standard%2520algorithmic%2520screening%2520tool.%250AThis%2520is%2520despite%2520the%2520fact%2520that%2520the%2520screening%2520tool%2520is%2520arguably%2520more%2520accurate%2520than%250Aphysicians%2527%2520discretionary%2520decisions%252C%2520highlighting%2520that%2520--%2520even%2520absent%2520normative%250Aconcerns%2520about%2520accountability%2520or%2520interpretability%2520--%2520accuracy%2520is%2520insufficient%250Ato%2520justify%2520algorithmic%2520automation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.01646v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auditing%20for%20Human%20Expertise&entry.906535625=Rohan%20Alur%20and%20Loren%20Laine%20and%20Darrick%20K.%20Li%20and%20Manish%20Raghavan%20and%20Devavrat%20Shah%20and%20Dennis%20Shung&entry.1292438233=%20%20High-stakes%20prediction%20tasks%20%28e.g.%2C%20patient%20diagnosis%29%20are%20often%20handled%20by%0Atrained%20human%20experts.%20A%20common%20source%20of%20concern%20about%20automation%20in%20these%0Asettings%20is%20that%20experts%20may%20exercise%20intuition%20that%20is%20difficult%20to%20model%0Aand/or%20have%20access%20to%20information%20%28e.g.%2C%20conversations%20with%20a%20patient%29%20that%20is%0Asimply%20unavailable%20to%20a%20would-be%20algorithm.%20This%20raises%20a%20natural%20question%0Awhether%20human%20experts%20add%20value%20which%20could%20not%20be%20captured%20by%20an%20algorithmic%0Apredictor.%20We%20develop%20a%20statistical%20framework%20under%20which%20we%20can%20pose%20this%0Aquestion%20as%20a%20natural%20hypothesis%20test.%20Indeed%2C%20as%20our%20framework%20highlights%2C%0Adetecting%20human%20expertise%20is%20more%20subtle%20than%20simply%20comparing%20the%20accuracy%20of%0Aexpert%20predictions%20to%20those%20made%20by%20a%20particular%20learning%20algorithm.%20Instead%2C%0Awe%20propose%20a%20simple%20procedure%20which%20tests%20whether%20expert%20predictions%20are%0Astatistically%20independent%20from%20the%20outcomes%20of%20interest%20after%20conditioning%20on%0Athe%20available%20inputs%20%28%60features%27%29.%20A%20rejection%20of%20our%20test%20thus%20suggests%20that%0Ahuman%20experts%20may%20add%20value%20to%20any%20algorithm%20trained%20on%20the%20available%20data%2C%20and%0Ahas%20direct%20implications%20for%20whether%20human-AI%20%60complementarity%27%20is%20achievable%20in%0Aa%20given%20prediction%20task.%20We%20highlight%20the%20utility%20of%20our%20procedure%20using%0Aadmissions%20data%20collected%20from%20the%20emergency%20department%20of%20a%20large%20academic%0Ahospital%20system%2C%20where%20we%20show%20that%20physicians%27%20admit/discharge%20decisions%20for%0Apatients%20with%20acute%20gastrointestinal%20bleeding%20%28AGIB%29%20appear%20to%20be%20incorporating%0Ainformation%20that%20is%20not%20available%20to%20a%20standard%20algorithmic%20screening%20tool.%0AThis%20is%20despite%20the%20fact%20that%20the%20screening%20tool%20is%20arguably%20more%20accurate%20than%0Aphysicians%27%20discretionary%20decisions%2C%20highlighting%20that%20--%20even%20absent%20normative%0Aconcerns%20about%20accountability%20or%20interpretability%20--%20accuracy%20is%20insufficient%0Ato%20justify%20algorithmic%20automation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.01646v3&entry.124074799=Read"},
{"title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing", "author": "Kaifeng Gao and Jiaxin Shi and Hanwang Zhang and Chunping Wang and Jun Xiao and Long Chen", "abstract": "  With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM\n", "link": "http://arxiv.org/abs/2411.16375v1", "date": "2024-11-25", "relevancy": 1.9164, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6646}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6111}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ca2-VDM%3A%20Efficient%20Autoregressive%20Video%20Diffusion%20Model%20with%20Causal%0A%20%20Generation%20and%20Cache%20Sharing&body=Title%3A%20Ca2-VDM%3A%20Efficient%20Autoregressive%20Video%20Diffusion%20Model%20with%20Causal%0A%20%20Generation%20and%20Cache%20Sharing%0AAuthor%3A%20Kaifeng%20Gao%20and%20Jiaxin%20Shi%20and%20Hanwang%20Zhang%20and%20Chunping%20Wang%20and%20Jun%20Xiao%20and%20Long%20Chen%0AAbstract%3A%20%20%20With%20the%20advance%20of%20diffusion%20models%2C%20today%27s%20video%20generation%20has%20achieved%0Aimpressive%20quality.%20To%20extend%20the%20generation%20length%20and%20facilitate%20real-world%0Aapplications%2C%20a%20majority%20of%20video%20diffusion%20models%20%28VDMs%29%20generate%20videos%20in%20an%0Aautoregressive%20manner%2C%20i.e.%2C%20generating%20subsequent%20clips%20conditioned%20on%20the%0Alast%20frame%28s%29%20of%20the%20previous%20clip.%20However%2C%20existing%20autoregressive%20VDMs%20are%0Ahighly%20inefficient%20and%20redundant%3A%20The%20model%20must%20re-compute%20all%20the%20conditional%0Aframes%20that%20are%20overlapped%20between%20adjacent%20clips.%20This%20issue%20is%20exacerbated%0Awhen%20the%20conditional%20frames%20are%20extended%20autoregressively%20to%20provide%20the%20model%0Awith%20long-term%20context.%20In%20such%20cases%2C%20the%20computational%20demands%20increase%0Asignificantly%20%28i.e.%2C%20with%20a%20quadratic%20complexity%20w.r.t.%20the%20autoregression%0Astep%29.%20In%20this%20paper%2C%20we%20propose%20Ca2-VDM%2C%20an%20efficient%20autoregressive%20VDM%20with%0ACausal%20generation%20and%20Cache%20sharing.%20For%20causal%20generation%2C%20it%20introduces%0Aunidirectional%20feature%20computation%2C%20which%20ensures%20that%20the%20cache%20of%20conditional%0Aframes%20can%20be%20precomputed%20in%20previous%20autoregression%20steps%20and%20reused%20in%20every%0Asubsequent%20step%2C%20eliminating%20redundant%20computations.%20For%20cache%20sharing%2C%20it%0Ashares%20the%20cache%20across%20all%20denoising%20steps%20to%20avoid%20the%20huge%20cache%20storage%0Acost.%20Extensive%20experiments%20demonstrated%20that%20our%20Ca2-VDM%20achieves%0Astate-of-the-art%20quantitative%20and%20qualitative%20video%20generation%20results%20and%0Asignificantly%20improves%20the%20generation%20speed.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Dawn-LX/CausalCache-VDM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCa2-VDM%253A%2520Efficient%2520Autoregressive%2520Video%2520Diffusion%2520Model%2520with%2520Causal%250A%2520%2520Generation%2520and%2520Cache%2520Sharing%26entry.906535625%3DKaifeng%2520Gao%2520and%2520Jiaxin%2520Shi%2520and%2520Hanwang%2520Zhang%2520and%2520Chunping%2520Wang%2520and%2520Jun%2520Xiao%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520advance%2520of%2520diffusion%2520models%252C%2520today%2527s%2520video%2520generation%2520has%2520achieved%250Aimpressive%2520quality.%2520To%2520extend%2520the%2520generation%2520length%2520and%2520facilitate%2520real-world%250Aapplications%252C%2520a%2520majority%2520of%2520video%2520diffusion%2520models%2520%2528VDMs%2529%2520generate%2520videos%2520in%2520an%250Aautoregressive%2520manner%252C%2520i.e.%252C%2520generating%2520subsequent%2520clips%2520conditioned%2520on%2520the%250Alast%2520frame%2528s%2529%2520of%2520the%2520previous%2520clip.%2520However%252C%2520existing%2520autoregressive%2520VDMs%2520are%250Ahighly%2520inefficient%2520and%2520redundant%253A%2520The%2520model%2520must%2520re-compute%2520all%2520the%2520conditional%250Aframes%2520that%2520are%2520overlapped%2520between%2520adjacent%2520clips.%2520This%2520issue%2520is%2520exacerbated%250Awhen%2520the%2520conditional%2520frames%2520are%2520extended%2520autoregressively%2520to%2520provide%2520the%2520model%250Awith%2520long-term%2520context.%2520In%2520such%2520cases%252C%2520the%2520computational%2520demands%2520increase%250Asignificantly%2520%2528i.e.%252C%2520with%2520a%2520quadratic%2520complexity%2520w.r.t.%2520the%2520autoregression%250Astep%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Ca2-VDM%252C%2520an%2520efficient%2520autoregressive%2520VDM%2520with%250ACausal%2520generation%2520and%2520Cache%2520sharing.%2520For%2520causal%2520generation%252C%2520it%2520introduces%250Aunidirectional%2520feature%2520computation%252C%2520which%2520ensures%2520that%2520the%2520cache%2520of%2520conditional%250Aframes%2520can%2520be%2520precomputed%2520in%2520previous%2520autoregression%2520steps%2520and%2520reused%2520in%2520every%250Asubsequent%2520step%252C%2520eliminating%2520redundant%2520computations.%2520For%2520cache%2520sharing%252C%2520it%250Ashares%2520the%2520cache%2520across%2520all%2520denoising%2520steps%2520to%2520avoid%2520the%2520huge%2520cache%2520storage%250Acost.%2520Extensive%2520experiments%2520demonstrated%2520that%2520our%2520Ca2-VDM%2520achieves%250Astate-of-the-art%2520quantitative%2520and%2520qualitative%2520video%2520generation%2520results%2520and%250Asignificantly%2520improves%2520the%2520generation%2520speed.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Dawn-LX/CausalCache-VDM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ca2-VDM%3A%20Efficient%20Autoregressive%20Video%20Diffusion%20Model%20with%20Causal%0A%20%20Generation%20and%20Cache%20Sharing&entry.906535625=Kaifeng%20Gao%20and%20Jiaxin%20Shi%20and%20Hanwang%20Zhang%20and%20Chunping%20Wang%20and%20Jun%20Xiao%20and%20Long%20Chen&entry.1292438233=%20%20With%20the%20advance%20of%20diffusion%20models%2C%20today%27s%20video%20generation%20has%20achieved%0Aimpressive%20quality.%20To%20extend%20the%20generation%20length%20and%20facilitate%20real-world%0Aapplications%2C%20a%20majority%20of%20video%20diffusion%20models%20%28VDMs%29%20generate%20videos%20in%20an%0Aautoregressive%20manner%2C%20i.e.%2C%20generating%20subsequent%20clips%20conditioned%20on%20the%0Alast%20frame%28s%29%20of%20the%20previous%20clip.%20However%2C%20existing%20autoregressive%20VDMs%20are%0Ahighly%20inefficient%20and%20redundant%3A%20The%20model%20must%20re-compute%20all%20the%20conditional%0Aframes%20that%20are%20overlapped%20between%20adjacent%20clips.%20This%20issue%20is%20exacerbated%0Awhen%20the%20conditional%20frames%20are%20extended%20autoregressively%20to%20provide%20the%20model%0Awith%20long-term%20context.%20In%20such%20cases%2C%20the%20computational%20demands%20increase%0Asignificantly%20%28i.e.%2C%20with%20a%20quadratic%20complexity%20w.r.t.%20the%20autoregression%0Astep%29.%20In%20this%20paper%2C%20we%20propose%20Ca2-VDM%2C%20an%20efficient%20autoregressive%20VDM%20with%0ACausal%20generation%20and%20Cache%20sharing.%20For%20causal%20generation%2C%20it%20introduces%0Aunidirectional%20feature%20computation%2C%20which%20ensures%20that%20the%20cache%20of%20conditional%0Aframes%20can%20be%20precomputed%20in%20previous%20autoregression%20steps%20and%20reused%20in%20every%0Asubsequent%20step%2C%20eliminating%20redundant%20computations.%20For%20cache%20sharing%2C%20it%0Ashares%20the%20cache%20across%20all%20denoising%20steps%20to%20avoid%20the%20huge%20cache%20storage%0Acost.%20Extensive%20experiments%20demonstrated%20that%20our%20Ca2-VDM%20achieves%0Astate-of-the-art%20quantitative%20and%20qualitative%20video%20generation%20results%20and%0Asignificantly%20improves%20the%20generation%20speed.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Dawn-LX/CausalCache-VDM%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16375v1&entry.124074799=Read"},
{"title": "A Survey of Event Causality Identification: Principles, Taxonomy,\n  Challenges, and Assessment", "author": "Qing Cheng and Zefan Zeng and Xingchen Hu and Yuehang Si and Zhong Liu", "abstract": "  Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications.\n", "link": "http://arxiv.org/abs/2411.10371v2", "date": "2024-11-25", "relevancy": 1.9194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Event%20Causality%20Identification%3A%20Principles%2C%20Taxonomy%2C%0A%20%20Challenges%2C%20and%20Assessment&body=Title%3A%20A%20Survey%20of%20Event%20Causality%20Identification%3A%20Principles%2C%20Taxonomy%2C%0A%20%20Challenges%2C%20and%20Assessment%0AAuthor%3A%20Qing%20Cheng%20and%20Zefan%20Zeng%20and%20Xingchen%20Hu%20and%20Yuehang%20Si%20and%20Zhong%20Liu%0AAbstract%3A%20%20%20Event%20Causality%20Identification%20%28ECI%29%20has%20become%20a%20crucial%20task%20in%20Natural%0ALanguage%20Processing%20%28NLP%29%2C%20aimed%20at%20automatically%20extracting%20causalities%20from%0Atextual%20data.%20In%20this%20survey%2C%20we%20systematically%20address%20the%20foundational%0Aprinciples%2C%20technical%20frameworks%2C%20and%20challenges%20of%20ECI%2C%20offering%20a%0Acomprehensive%20taxonomy%20to%20categorize%20and%20clarify%20current%20research%0Amethodologies%2C%20as%20well%20as%20a%20quantitative%20assessment%20of%20existing%20models.%20We%0Afirst%20establish%20a%20conceptual%20framework%20for%20ECI%2C%20outlining%20key%20definitions%2C%0Aproblem%20formulations%2C%20and%20evaluation%20standards.%20Our%20taxonomy%20classifies%20ECI%0Amethods%20according%20to%20the%20two%20primary%20tasks%20of%20sentence-level%20%28SECI%29%20and%0Adocument-level%20%28DECI%29%20event%20causality%20identification.%20For%20SECI%2C%20we%20examine%0Afeature%20pattern-based%20matching%2C%20deep%20semantic%20encoding%2C%20causal%20knowledge%0Apre-training%20and%20prompt-based%20fine-tuning%2C%20and%20external%20knowledge%20enhancement%0Amethods.%20For%20DECI%2C%20we%20highlight%20approaches%20focused%20on%20event%20graph%20reasoning%20and%0Aprompt-based%20techniques%20to%20address%20the%20complexity%20of%20cross-sentence%20causal%0Ainference.%20Additionally%2C%20we%20analyze%20the%20strengths%2C%20limitations%2C%20and%20open%0Achallenges%20of%20each%20approach.%20We%20further%20conduct%20an%20extensive%20quantitative%0Aevaluation%20of%20various%20ECI%20methods%20on%20two%20benchmark%20datasets.%20Finally%2C%20we%0Aexplore%20future%20research%20directions%2C%20highlighting%20promising%20pathways%20to%20overcome%0Acurrent%20limitations%20and%20broaden%20ECI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Event%2520Causality%2520Identification%253A%2520Principles%252C%2520Taxonomy%252C%250A%2520%2520Challenges%252C%2520and%2520Assessment%26entry.906535625%3DQing%2520Cheng%2520and%2520Zefan%2520Zeng%2520and%2520Xingchen%2520Hu%2520and%2520Yuehang%2520Si%2520and%2520Zhong%2520Liu%26entry.1292438233%3D%2520%2520Event%2520Causality%2520Identification%2520%2528ECI%2529%2520has%2520become%2520a%2520crucial%2520task%2520in%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%252C%2520aimed%2520at%2520automatically%2520extracting%2520causalities%2520from%250Atextual%2520data.%2520In%2520this%2520survey%252C%2520we%2520systematically%2520address%2520the%2520foundational%250Aprinciples%252C%2520technical%2520frameworks%252C%2520and%2520challenges%2520of%2520ECI%252C%2520offering%2520a%250Acomprehensive%2520taxonomy%2520to%2520categorize%2520and%2520clarify%2520current%2520research%250Amethodologies%252C%2520as%2520well%2520as%2520a%2520quantitative%2520assessment%2520of%2520existing%2520models.%2520We%250Afirst%2520establish%2520a%2520conceptual%2520framework%2520for%2520ECI%252C%2520outlining%2520key%2520definitions%252C%250Aproblem%2520formulations%252C%2520and%2520evaluation%2520standards.%2520Our%2520taxonomy%2520classifies%2520ECI%250Amethods%2520according%2520to%2520the%2520two%2520primary%2520tasks%2520of%2520sentence-level%2520%2528SECI%2529%2520and%250Adocument-level%2520%2528DECI%2529%2520event%2520causality%2520identification.%2520For%2520SECI%252C%2520we%2520examine%250Afeature%2520pattern-based%2520matching%252C%2520deep%2520semantic%2520encoding%252C%2520causal%2520knowledge%250Apre-training%2520and%2520prompt-based%2520fine-tuning%252C%2520and%2520external%2520knowledge%2520enhancement%250Amethods.%2520For%2520DECI%252C%2520we%2520highlight%2520approaches%2520focused%2520on%2520event%2520graph%2520reasoning%2520and%250Aprompt-based%2520techniques%2520to%2520address%2520the%2520complexity%2520of%2520cross-sentence%2520causal%250Ainference.%2520Additionally%252C%2520we%2520analyze%2520the%2520strengths%252C%2520limitations%252C%2520and%2520open%250Achallenges%2520of%2520each%2520approach.%2520We%2520further%2520conduct%2520an%2520extensive%2520quantitative%250Aevaluation%2520of%2520various%2520ECI%2520methods%2520on%2520two%2520benchmark%2520datasets.%2520Finally%252C%2520we%250Aexplore%2520future%2520research%2520directions%252C%2520highlighting%2520promising%2520pathways%2520to%2520overcome%250Acurrent%2520limitations%2520and%2520broaden%2520ECI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Event%20Causality%20Identification%3A%20Principles%2C%20Taxonomy%2C%0A%20%20Challenges%2C%20and%20Assessment&entry.906535625=Qing%20Cheng%20and%20Zefan%20Zeng%20and%20Xingchen%20Hu%20and%20Yuehang%20Si%20and%20Zhong%20Liu&entry.1292438233=%20%20Event%20Causality%20Identification%20%28ECI%29%20has%20become%20a%20crucial%20task%20in%20Natural%0ALanguage%20Processing%20%28NLP%29%2C%20aimed%20at%20automatically%20extracting%20causalities%20from%0Atextual%20data.%20In%20this%20survey%2C%20we%20systematically%20address%20the%20foundational%0Aprinciples%2C%20technical%20frameworks%2C%20and%20challenges%20of%20ECI%2C%20offering%20a%0Acomprehensive%20taxonomy%20to%20categorize%20and%20clarify%20current%20research%0Amethodologies%2C%20as%20well%20as%20a%20quantitative%20assessment%20of%20existing%20models.%20We%0Afirst%20establish%20a%20conceptual%20framework%20for%20ECI%2C%20outlining%20key%20definitions%2C%0Aproblem%20formulations%2C%20and%20evaluation%20standards.%20Our%20taxonomy%20classifies%20ECI%0Amethods%20according%20to%20the%20two%20primary%20tasks%20of%20sentence-level%20%28SECI%29%20and%0Adocument-level%20%28DECI%29%20event%20causality%20identification.%20For%20SECI%2C%20we%20examine%0Afeature%20pattern-based%20matching%2C%20deep%20semantic%20encoding%2C%20causal%20knowledge%0Apre-training%20and%20prompt-based%20fine-tuning%2C%20and%20external%20knowledge%20enhancement%0Amethods.%20For%20DECI%2C%20we%20highlight%20approaches%20focused%20on%20event%20graph%20reasoning%20and%0Aprompt-based%20techniques%20to%20address%20the%20complexity%20of%20cross-sentence%20causal%0Ainference.%20Additionally%2C%20we%20analyze%20the%20strengths%2C%20limitations%2C%20and%20open%0Achallenges%20of%20each%20approach.%20We%20further%20conduct%20an%20extensive%20quantitative%0Aevaluation%20of%20various%20ECI%20methods%20on%20two%20benchmark%20datasets.%20Finally%2C%20we%0Aexplore%20future%20research%20directions%2C%20highlighting%20promising%20pathways%20to%20overcome%0Acurrent%20limitations%20and%20broaden%20ECI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10371v2&entry.124074799=Read"},
{"title": "Use-Inspired Mobile Robot to Improve Safety of Building Retrofit\n  Workforce in Constrained Spaces", "author": "Smruti Suresh and Michael Angelo Carvajal and Nathaniel Hanson and Ethan Holand and Samuel Hibbard and Taskin Padir", "abstract": "  The inspection of confined critical infrastructure such as attics or\ncrawlspaces is challenging for human operators due to insufficient task space,\nlimited visibility, and the presence of hazardous materials. This paper\nintroduces a prototype of PARIS (Precision Application Robot for Inaccessible\nSpaces): a use-inspired teleoperated mobile robot manipulator system that was\nconceived, developed, and tested for and selected as a Phase I winner of the\nU.S. Department of Energy's E-ROBOT Prize. To improve the thermal efficiency of\nbuildings, the PARIS platform supports: 1) teleoperated mapping and navigation,\nenabling the human operator to explore compact spaces; 2) inspection and\nsensing, facilitating the identification and localization of under-insulated\nareas; and 3) air-sealing targeted gaps and cracks through which thermal energy\nis lost. The resulting versatile platform can also be tailored for targeted\napplication of treatments and remediation in constrained spaces.\n", "link": "http://arxiv.org/abs/2411.16511v1", "date": "2024-11-25", "relevancy": 1.532, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5407}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Use-Inspired%20Mobile%20Robot%20to%20Improve%20Safety%20of%20Building%20Retrofit%0A%20%20Workforce%20in%20Constrained%20Spaces&body=Title%3A%20Use-Inspired%20Mobile%20Robot%20to%20Improve%20Safety%20of%20Building%20Retrofit%0A%20%20Workforce%20in%20Constrained%20Spaces%0AAuthor%3A%20Smruti%20Suresh%20and%20Michael%20Angelo%20Carvajal%20and%20Nathaniel%20Hanson%20and%20Ethan%20Holand%20and%20Samuel%20Hibbard%20and%20Taskin%20Padir%0AAbstract%3A%20%20%20The%20inspection%20of%20confined%20critical%20infrastructure%20such%20as%20attics%20or%0Acrawlspaces%20is%20challenging%20for%20human%20operators%20due%20to%20insufficient%20task%20space%2C%0Alimited%20visibility%2C%20and%20the%20presence%20of%20hazardous%20materials.%20This%20paper%0Aintroduces%20a%20prototype%20of%20PARIS%20%28Precision%20Application%20Robot%20for%20Inaccessible%0ASpaces%29%3A%20a%20use-inspired%20teleoperated%20mobile%20robot%20manipulator%20system%20that%20was%0Aconceived%2C%20developed%2C%20and%20tested%20for%20and%20selected%20as%20a%20Phase%20I%20winner%20of%20the%0AU.S.%20Department%20of%20Energy%27s%20E-ROBOT%20Prize.%20To%20improve%20the%20thermal%20efficiency%20of%0Abuildings%2C%20the%20PARIS%20platform%20supports%3A%201%29%20teleoperated%20mapping%20and%20navigation%2C%0Aenabling%20the%20human%20operator%20to%20explore%20compact%20spaces%3B%202%29%20inspection%20and%0Asensing%2C%20facilitating%20the%20identification%20and%20localization%20of%20under-insulated%0Aareas%3B%20and%203%29%20air-sealing%20targeted%20gaps%20and%20cracks%20through%20which%20thermal%20energy%0Ais%20lost.%20The%20resulting%20versatile%20platform%20can%20also%20be%20tailored%20for%20targeted%0Aapplication%20of%20treatments%20and%20remediation%20in%20constrained%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUse-Inspired%2520Mobile%2520Robot%2520to%2520Improve%2520Safety%2520of%2520Building%2520Retrofit%250A%2520%2520Workforce%2520in%2520Constrained%2520Spaces%26entry.906535625%3DSmruti%2520Suresh%2520and%2520Michael%2520Angelo%2520Carvajal%2520and%2520Nathaniel%2520Hanson%2520and%2520Ethan%2520Holand%2520and%2520Samuel%2520Hibbard%2520and%2520Taskin%2520Padir%26entry.1292438233%3D%2520%2520The%2520inspection%2520of%2520confined%2520critical%2520infrastructure%2520such%2520as%2520attics%2520or%250Acrawlspaces%2520is%2520challenging%2520for%2520human%2520operators%2520due%2520to%2520insufficient%2520task%2520space%252C%250Alimited%2520visibility%252C%2520and%2520the%2520presence%2520of%2520hazardous%2520materials.%2520This%2520paper%250Aintroduces%2520a%2520prototype%2520of%2520PARIS%2520%2528Precision%2520Application%2520Robot%2520for%2520Inaccessible%250ASpaces%2529%253A%2520a%2520use-inspired%2520teleoperated%2520mobile%2520robot%2520manipulator%2520system%2520that%2520was%250Aconceived%252C%2520developed%252C%2520and%2520tested%2520for%2520and%2520selected%2520as%2520a%2520Phase%2520I%2520winner%2520of%2520the%250AU.S.%2520Department%2520of%2520Energy%2527s%2520E-ROBOT%2520Prize.%2520To%2520improve%2520the%2520thermal%2520efficiency%2520of%250Abuildings%252C%2520the%2520PARIS%2520platform%2520supports%253A%25201%2529%2520teleoperated%2520mapping%2520and%2520navigation%252C%250Aenabling%2520the%2520human%2520operator%2520to%2520explore%2520compact%2520spaces%253B%25202%2529%2520inspection%2520and%250Asensing%252C%2520facilitating%2520the%2520identification%2520and%2520localization%2520of%2520under-insulated%250Aareas%253B%2520and%25203%2529%2520air-sealing%2520targeted%2520gaps%2520and%2520cracks%2520through%2520which%2520thermal%2520energy%250Ais%2520lost.%2520The%2520resulting%2520versatile%2520platform%2520can%2520also%2520be%2520tailored%2520for%2520targeted%250Aapplication%2520of%2520treatments%2520and%2520remediation%2520in%2520constrained%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Use-Inspired%20Mobile%20Robot%20to%20Improve%20Safety%20of%20Building%20Retrofit%0A%20%20Workforce%20in%20Constrained%20Spaces&entry.906535625=Smruti%20Suresh%20and%20Michael%20Angelo%20Carvajal%20and%20Nathaniel%20Hanson%20and%20Ethan%20Holand%20and%20Samuel%20Hibbard%20and%20Taskin%20Padir&entry.1292438233=%20%20The%20inspection%20of%20confined%20critical%20infrastructure%20such%20as%20attics%20or%0Acrawlspaces%20is%20challenging%20for%20human%20operators%20due%20to%20insufficient%20task%20space%2C%0Alimited%20visibility%2C%20and%20the%20presence%20of%20hazardous%20materials.%20This%20paper%0Aintroduces%20a%20prototype%20of%20PARIS%20%28Precision%20Application%20Robot%20for%20Inaccessible%0ASpaces%29%3A%20a%20use-inspired%20teleoperated%20mobile%20robot%20manipulator%20system%20that%20was%0Aconceived%2C%20developed%2C%20and%20tested%20for%20and%20selected%20as%20a%20Phase%20I%20winner%20of%20the%0AU.S.%20Department%20of%20Energy%27s%20E-ROBOT%20Prize.%20To%20improve%20the%20thermal%20efficiency%20of%0Abuildings%2C%20the%20PARIS%20platform%20supports%3A%201%29%20teleoperated%20mapping%20and%20navigation%2C%0Aenabling%20the%20human%20operator%20to%20explore%20compact%20spaces%3B%202%29%20inspection%20and%0Asensing%2C%20facilitating%20the%20identification%20and%20localization%20of%20under-insulated%0Aareas%3B%20and%203%29%20air-sealing%20targeted%20gaps%20and%20cracks%20through%20which%20thermal%20energy%0Ais%20lost.%20The%20resulting%20versatile%20platform%20can%20also%20be%20tailored%20for%20targeted%0Aapplication%20of%20treatments%20and%20remediation%20in%20constrained%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16511v1&entry.124074799=Read"},
{"title": "Statistical inference for quantum singular models", "author": "Hiroshi Yano and Yota Maeda and Naoki Yamamoto", "abstract": "  Deep learning has seen substantial achievements, with numerical and\ntheoretical evidence suggesting that singularities of statistical models are\nconsidered a contributing factor to its performance. From this remarkable\nsuccess of classical statistical models, it is naturally expected that quantum\nsingular models will play a vital role in many quantum statistical tasks.\nHowever, while the theory of quantum statistical models in regular cases has\nbeen established, theoretical understanding of quantum singular models is still\nlimited. To investigate the statistical properties of quantum singular models,\nwe focus on two prominent tasks in quantum statistical inference: quantum state\nestimation and model selection. In particular, we base our study on classical\nsingular learning theory and seek to extend it within the framework of Bayesian\nquantum state estimation. To this end, we define quantum generalization and\ntraining loss functions and give their asymptotic expansions through algebraic\ngeometrical methods. The key idea of the proof is the introduction of a quantum\nanalog of the likelihood function using classical shadows. Consequently, we\nconstruct an asymptotically unbiased estimator of the quantum generalization\nloss, the quantum widely applicable information criterion (QWAIC), as a\ncomputable model selection metric from given measurement outcomes.\n", "link": "http://arxiv.org/abs/2411.16396v1", "date": "2024-11-25", "relevancy": 1.2491, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4501}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4263}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20inference%20for%20quantum%20singular%20models&body=Title%3A%20Statistical%20inference%20for%20quantum%20singular%20models%0AAuthor%3A%20Hiroshi%20Yano%20and%20Yota%20Maeda%20and%20Naoki%20Yamamoto%0AAbstract%3A%20%20%20Deep%20learning%20has%20seen%20substantial%20achievements%2C%20with%20numerical%20and%0Atheoretical%20evidence%20suggesting%20that%20singularities%20of%20statistical%20models%20are%0Aconsidered%20a%20contributing%20factor%20to%20its%20performance.%20From%20this%20remarkable%0Asuccess%20of%20classical%20statistical%20models%2C%20it%20is%20naturally%20expected%20that%20quantum%0Asingular%20models%20will%20play%20a%20vital%20role%20in%20many%20quantum%20statistical%20tasks.%0AHowever%2C%20while%20the%20theory%20of%20quantum%20statistical%20models%20in%20regular%20cases%20has%0Abeen%20established%2C%20theoretical%20understanding%20of%20quantum%20singular%20models%20is%20still%0Alimited.%20To%20investigate%20the%20statistical%20properties%20of%20quantum%20singular%20models%2C%0Awe%20focus%20on%20two%20prominent%20tasks%20in%20quantum%20statistical%20inference%3A%20quantum%20state%0Aestimation%20and%20model%20selection.%20In%20particular%2C%20we%20base%20our%20study%20on%20classical%0Asingular%20learning%20theory%20and%20seek%20to%20extend%20it%20within%20the%20framework%20of%20Bayesian%0Aquantum%20state%20estimation.%20To%20this%20end%2C%20we%20define%20quantum%20generalization%20and%0Atraining%20loss%20functions%20and%20give%20their%20asymptotic%20expansions%20through%20algebraic%0Ageometrical%20methods.%20The%20key%20idea%20of%20the%20proof%20is%20the%20introduction%20of%20a%20quantum%0Aanalog%20of%20the%20likelihood%20function%20using%20classical%20shadows.%20Consequently%2C%20we%0Aconstruct%20an%20asymptotically%20unbiased%20estimator%20of%20the%20quantum%20generalization%0Aloss%2C%20the%20quantum%20widely%20applicable%20information%20criterion%20%28QWAIC%29%2C%20as%20a%0Acomputable%20model%20selection%20metric%20from%20given%20measurement%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520inference%2520for%2520quantum%2520singular%2520models%26entry.906535625%3DHiroshi%2520Yano%2520and%2520Yota%2520Maeda%2520and%2520Naoki%2520Yamamoto%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520seen%2520substantial%2520achievements%252C%2520with%2520numerical%2520and%250Atheoretical%2520evidence%2520suggesting%2520that%2520singularities%2520of%2520statistical%2520models%2520are%250Aconsidered%2520a%2520contributing%2520factor%2520to%2520its%2520performance.%2520From%2520this%2520remarkable%250Asuccess%2520of%2520classical%2520statistical%2520models%252C%2520it%2520is%2520naturally%2520expected%2520that%2520quantum%250Asingular%2520models%2520will%2520play%2520a%2520vital%2520role%2520in%2520many%2520quantum%2520statistical%2520tasks.%250AHowever%252C%2520while%2520the%2520theory%2520of%2520quantum%2520statistical%2520models%2520in%2520regular%2520cases%2520has%250Abeen%2520established%252C%2520theoretical%2520understanding%2520of%2520quantum%2520singular%2520models%2520is%2520still%250Alimited.%2520To%2520investigate%2520the%2520statistical%2520properties%2520of%2520quantum%2520singular%2520models%252C%250Awe%2520focus%2520on%2520two%2520prominent%2520tasks%2520in%2520quantum%2520statistical%2520inference%253A%2520quantum%2520state%250Aestimation%2520and%2520model%2520selection.%2520In%2520particular%252C%2520we%2520base%2520our%2520study%2520on%2520classical%250Asingular%2520learning%2520theory%2520and%2520seek%2520to%2520extend%2520it%2520within%2520the%2520framework%2520of%2520Bayesian%250Aquantum%2520state%2520estimation.%2520To%2520this%2520end%252C%2520we%2520define%2520quantum%2520generalization%2520and%250Atraining%2520loss%2520functions%2520and%2520give%2520their%2520asymptotic%2520expansions%2520through%2520algebraic%250Ageometrical%2520methods.%2520The%2520key%2520idea%2520of%2520the%2520proof%2520is%2520the%2520introduction%2520of%2520a%2520quantum%250Aanalog%2520of%2520the%2520likelihood%2520function%2520using%2520classical%2520shadows.%2520Consequently%252C%2520we%250Aconstruct%2520an%2520asymptotically%2520unbiased%2520estimator%2520of%2520the%2520quantum%2520generalization%250Aloss%252C%2520the%2520quantum%2520widely%2520applicable%2520information%2520criterion%2520%2528QWAIC%2529%252C%2520as%2520a%250Acomputable%2520model%2520selection%2520metric%2520from%2520given%2520measurement%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20inference%20for%20quantum%20singular%20models&entry.906535625=Hiroshi%20Yano%20and%20Yota%20Maeda%20and%20Naoki%20Yamamoto&entry.1292438233=%20%20Deep%20learning%20has%20seen%20substantial%20achievements%2C%20with%20numerical%20and%0Atheoretical%20evidence%20suggesting%20that%20singularities%20of%20statistical%20models%20are%0Aconsidered%20a%20contributing%20factor%20to%20its%20performance.%20From%20this%20remarkable%0Asuccess%20of%20classical%20statistical%20models%2C%20it%20is%20naturally%20expected%20that%20quantum%0Asingular%20models%20will%20play%20a%20vital%20role%20in%20many%20quantum%20statistical%20tasks.%0AHowever%2C%20while%20the%20theory%20of%20quantum%20statistical%20models%20in%20regular%20cases%20has%0Abeen%20established%2C%20theoretical%20understanding%20of%20quantum%20singular%20models%20is%20still%0Alimited.%20To%20investigate%20the%20statistical%20properties%20of%20quantum%20singular%20models%2C%0Awe%20focus%20on%20two%20prominent%20tasks%20in%20quantum%20statistical%20inference%3A%20quantum%20state%0Aestimation%20and%20model%20selection.%20In%20particular%2C%20we%20base%20our%20study%20on%20classical%0Asingular%20learning%20theory%20and%20seek%20to%20extend%20it%20within%20the%20framework%20of%20Bayesian%0Aquantum%20state%20estimation.%20To%20this%20end%2C%20we%20define%20quantum%20generalization%20and%0Atraining%20loss%20functions%20and%20give%20their%20asymptotic%20expansions%20through%20algebraic%0Ageometrical%20methods.%20The%20key%20idea%20of%20the%20proof%20is%20the%20introduction%20of%20a%20quantum%0Aanalog%20of%20the%20likelihood%20function%20using%20classical%20shadows.%20Consequently%2C%20we%0Aconstruct%20an%20asymptotically%20unbiased%20estimator%20of%20the%20quantum%20generalization%0Aloss%2C%20the%20quantum%20widely%20applicable%20information%20criterion%20%28QWAIC%29%2C%20as%20a%0Acomputable%20model%20selection%20metric%20from%20given%20measurement%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16396v1&entry.124074799=Read"},
{"title": "Offline reinforcement learning for job-shop scheduling problems", "author": "Imanol Echeverria and Maialen Murua and Roberto Santana", "abstract": "  Recent advances in deep learning have shown significant potential for solving\ncombinatorial optimization problems in real-time. Unlike traditional methods,\ndeep learning can generate high-quality solutions efficiently, which is crucial\nfor applications like routing and scheduling. However, existing approaches like\ndeep reinforcement learning (RL) and behavioral cloning have notable\nlimitations, with deep RL suffering from slow learning and behavioral cloning\nrelying solely on expert actions, which can lead to generalization issues and\nneglect of the optimization objective. This paper introduces a novel offline RL\nmethod designed for combinatorial optimization problems with complex\nconstraints, where the state is represented as a heterogeneous graph and the\naction space is variable. Our approach encodes actions in edge attributes and\nbalances expected rewards with the imitation of expert solutions. We\ndemonstrate the effectiveness of this method on job-shop scheduling and\nflexible job-shop scheduling benchmarks, achieving superior performance\ncompared to state-of-the-art techniques.\n", "link": "http://arxiv.org/abs/2410.15714v2", "date": "2024-11-25", "relevancy": 1.9702, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5191}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4897}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Offline%20reinforcement%20learning%20for%20job-shop%20scheduling%20problems&body=Title%3A%20Offline%20reinforcement%20learning%20for%20job-shop%20scheduling%20problems%0AAuthor%3A%20Imanol%20Echeverria%20and%20Maialen%20Murua%20and%20Roberto%20Santana%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20learning%20have%20shown%20significant%20potential%20for%20solving%0Acombinatorial%20optimization%20problems%20in%20real-time.%20Unlike%20traditional%20methods%2C%0Adeep%20learning%20can%20generate%20high-quality%20solutions%20efficiently%2C%20which%20is%20crucial%0Afor%20applications%20like%20routing%20and%20scheduling.%20However%2C%20existing%20approaches%20like%0Adeep%20reinforcement%20learning%20%28RL%29%20and%20behavioral%20cloning%20have%20notable%0Alimitations%2C%20with%20deep%20RL%20suffering%20from%20slow%20learning%20and%20behavioral%20cloning%0Arelying%20solely%20on%20expert%20actions%2C%20which%20can%20lead%20to%20generalization%20issues%20and%0Aneglect%20of%20the%20optimization%20objective.%20This%20paper%20introduces%20a%20novel%20offline%20RL%0Amethod%20designed%20for%20combinatorial%20optimization%20problems%20with%20complex%0Aconstraints%2C%20where%20the%20state%20is%20represented%20as%20a%20heterogeneous%20graph%20and%20the%0Aaction%20space%20is%20variable.%20Our%20approach%20encodes%20actions%20in%20edge%20attributes%20and%0Abalances%20expected%20rewards%20with%20the%20imitation%20of%20expert%20solutions.%20We%0Ademonstrate%20the%20effectiveness%20of%20this%20method%20on%20job-shop%20scheduling%20and%0Aflexible%20job-shop%20scheduling%20benchmarks%2C%20achieving%20superior%20performance%0Acompared%20to%20state-of-the-art%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOffline%2520reinforcement%2520learning%2520for%2520job-shop%2520scheduling%2520problems%26entry.906535625%3DImanol%2520Echeverria%2520and%2520Maialen%2520Murua%2520and%2520Roberto%2520Santana%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520deep%2520learning%2520have%2520shown%2520significant%2520potential%2520for%2520solving%250Acombinatorial%2520optimization%2520problems%2520in%2520real-time.%2520Unlike%2520traditional%2520methods%252C%250Adeep%2520learning%2520can%2520generate%2520high-quality%2520solutions%2520efficiently%252C%2520which%2520is%2520crucial%250Afor%2520applications%2520like%2520routing%2520and%2520scheduling.%2520However%252C%2520existing%2520approaches%2520like%250Adeep%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520behavioral%2520cloning%2520have%2520notable%250Alimitations%252C%2520with%2520deep%2520RL%2520suffering%2520from%2520slow%2520learning%2520and%2520behavioral%2520cloning%250Arelying%2520solely%2520on%2520expert%2520actions%252C%2520which%2520can%2520lead%2520to%2520generalization%2520issues%2520and%250Aneglect%2520of%2520the%2520optimization%2520objective.%2520This%2520paper%2520introduces%2520a%2520novel%2520offline%2520RL%250Amethod%2520designed%2520for%2520combinatorial%2520optimization%2520problems%2520with%2520complex%250Aconstraints%252C%2520where%2520the%2520state%2520is%2520represented%2520as%2520a%2520heterogeneous%2520graph%2520and%2520the%250Aaction%2520space%2520is%2520variable.%2520Our%2520approach%2520encodes%2520actions%2520in%2520edge%2520attributes%2520and%250Abalances%2520expected%2520rewards%2520with%2520the%2520imitation%2520of%2520expert%2520solutions.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520this%2520method%2520on%2520job-shop%2520scheduling%2520and%250Aflexible%2520job-shop%2520scheduling%2520benchmarks%252C%2520achieving%2520superior%2520performance%250Acompared%2520to%2520state-of-the-art%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Offline%20reinforcement%20learning%20for%20job-shop%20scheduling%20problems&entry.906535625=Imanol%20Echeverria%20and%20Maialen%20Murua%20and%20Roberto%20Santana&entry.1292438233=%20%20Recent%20advances%20in%20deep%20learning%20have%20shown%20significant%20potential%20for%20solving%0Acombinatorial%20optimization%20problems%20in%20real-time.%20Unlike%20traditional%20methods%2C%0Adeep%20learning%20can%20generate%20high-quality%20solutions%20efficiently%2C%20which%20is%20crucial%0Afor%20applications%20like%20routing%20and%20scheduling.%20However%2C%20existing%20approaches%20like%0Adeep%20reinforcement%20learning%20%28RL%29%20and%20behavioral%20cloning%20have%20notable%0Alimitations%2C%20with%20deep%20RL%20suffering%20from%20slow%20learning%20and%20behavioral%20cloning%0Arelying%20solely%20on%20expert%20actions%2C%20which%20can%20lead%20to%20generalization%20issues%20and%0Aneglect%20of%20the%20optimization%20objective.%20This%20paper%20introduces%20a%20novel%20offline%20RL%0Amethod%20designed%20for%20combinatorial%20optimization%20problems%20with%20complex%0Aconstraints%2C%20where%20the%20state%20is%20represented%20as%20a%20heterogeneous%20graph%20and%20the%0Aaction%20space%20is%20variable.%20Our%20approach%20encodes%20actions%20in%20edge%20attributes%20and%0Abalances%20expected%20rewards%20with%20the%20imitation%20of%20expert%20solutions.%20We%0Ademonstrate%20the%20effectiveness%20of%20this%20method%20on%20job-shop%20scheduling%20and%0Aflexible%20job-shop%20scheduling%20benchmarks%2C%20achieving%20superior%20performance%0Acompared%20to%20state-of-the-art%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15714v2&entry.124074799=Read"},
{"title": "OminiControl: Minimal and Universal Control for Diffusion Transformer", "author": "Zhenxiong Tan and Songhua Liu and Xingyi Yang and Qiaochu Xue and Xinchao Wang", "abstract": "  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n", "link": "http://arxiv.org/abs/2411.15098v2", "date": "2024-11-25", "relevancy": 1.7639, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6327}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.577}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OminiControl%3A%20Minimal%20and%20Universal%20Control%20for%20Diffusion%20Transformer&body=Title%3A%20OminiControl%3A%20Minimal%20and%20Universal%20Control%20for%20Diffusion%20Transformer%0AAuthor%3A%20Zhenxiong%20Tan%20and%20Songhua%20Liu%20and%20Xingyi%20Yang%20and%20Qiaochu%20Xue%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20OminiControl%2C%20a%20highly%20versatile%20and%0Aparameter-efficient%20framework%20that%20integrates%20image%20conditions%20into%20pre-trained%0ADiffusion%20Transformer%20%28DiT%29%20models.%20At%20its%20core%2C%20OminiControl%20leverages%20a%0Aparameter%20reuse%20mechanism%2C%20enabling%20the%20DiT%20to%20encode%20image%20conditions%20using%0Aitself%20as%20a%20powerful%20backbone%20and%20process%20them%20with%20its%20flexible%20multi-modal%0Aattention%20processors.%20Unlike%20existing%20methods%2C%20which%20rely%20heavily%20on%20additional%0Aencoder%20modules%20with%20complex%20architectures%2C%20OminiControl%20%281%29%20effectively%20and%0Aefficiently%20incorporates%20injected%20image%20conditions%20with%20only%20~0.1%25%20additional%0Aparameters%2C%20and%20%282%29%20addresses%20a%20wide%20range%20of%20image%20conditioning%20tasks%20in%20a%0Aunified%20manner%2C%20including%20subject-driven%20generation%20and%20spatially-aligned%0Aconditions%20such%20as%20edges%2C%20depth%2C%20and%20more.%20Remarkably%2C%20these%20capabilities%20are%0Aachieved%20by%20training%20on%20images%20generated%20by%20the%20DiT%20itself%2C%20which%20is%0Aparticularly%20beneficial%20for%20subject-driven%20generation.%20Extensive%20evaluations%0Ademonstrate%20that%20OminiControl%20outperforms%20existing%20UNet-based%20and%20DiT-adapted%0Amodels%20in%20both%20subject-driven%20and%20spatially-aligned%20conditional%20generation.%0AAdditionally%2C%20we%20release%20our%20training%20dataset%2C%20Subjects200K%2C%20a%20diverse%0Acollection%20of%20over%20200%2C000%20identity-consistent%20images%2C%20along%20with%20an%20efficient%0Adata%20synthesis%20pipeline%20to%20advance%20research%20in%20subject-consistent%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOminiControl%253A%2520Minimal%2520and%2520Universal%2520Control%2520for%2520Diffusion%2520Transformer%26entry.906535625%3DZhenxiong%2520Tan%2520and%2520Songhua%2520Liu%2520and%2520Xingyi%2520Yang%2520and%2520Qiaochu%2520Xue%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520OminiControl%252C%2520a%2520highly%2520versatile%2520and%250Aparameter-efficient%2520framework%2520that%2520integrates%2520image%2520conditions%2520into%2520pre-trained%250ADiffusion%2520Transformer%2520%2528DiT%2529%2520models.%2520At%2520its%2520core%252C%2520OminiControl%2520leverages%2520a%250Aparameter%2520reuse%2520mechanism%252C%2520enabling%2520the%2520DiT%2520to%2520encode%2520image%2520conditions%2520using%250Aitself%2520as%2520a%2520powerful%2520backbone%2520and%2520process%2520them%2520with%2520its%2520flexible%2520multi-modal%250Aattention%2520processors.%2520Unlike%2520existing%2520methods%252C%2520which%2520rely%2520heavily%2520on%2520additional%250Aencoder%2520modules%2520with%2520complex%2520architectures%252C%2520OminiControl%2520%25281%2529%2520effectively%2520and%250Aefficiently%2520incorporates%2520injected%2520image%2520conditions%2520with%2520only%2520~0.1%2525%2520additional%250Aparameters%252C%2520and%2520%25282%2529%2520addresses%2520a%2520wide%2520range%2520of%2520image%2520conditioning%2520tasks%2520in%2520a%250Aunified%2520manner%252C%2520including%2520subject-driven%2520generation%2520and%2520spatially-aligned%250Aconditions%2520such%2520as%2520edges%252C%2520depth%252C%2520and%2520more.%2520Remarkably%252C%2520these%2520capabilities%2520are%250Aachieved%2520by%2520training%2520on%2520images%2520generated%2520by%2520the%2520DiT%2520itself%252C%2520which%2520is%250Aparticularly%2520beneficial%2520for%2520subject-driven%2520generation.%2520Extensive%2520evaluations%250Ademonstrate%2520that%2520OminiControl%2520outperforms%2520existing%2520UNet-based%2520and%2520DiT-adapted%250Amodels%2520in%2520both%2520subject-driven%2520and%2520spatially-aligned%2520conditional%2520generation.%250AAdditionally%252C%2520we%2520release%2520our%2520training%2520dataset%252C%2520Subjects200K%252C%2520a%2520diverse%250Acollection%2520of%2520over%2520200%252C000%2520identity-consistent%2520images%252C%2520along%2520with%2520an%2520efficient%250Adata%2520synthesis%2520pipeline%2520to%2520advance%2520research%2520in%2520subject-consistent%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OminiControl%3A%20Minimal%20and%20Universal%20Control%20for%20Diffusion%20Transformer&entry.906535625=Zhenxiong%20Tan%20and%20Songhua%20Liu%20and%20Xingyi%20Yang%20and%20Qiaochu%20Xue%20and%20Xinchao%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20OminiControl%2C%20a%20highly%20versatile%20and%0Aparameter-efficient%20framework%20that%20integrates%20image%20conditions%20into%20pre-trained%0ADiffusion%20Transformer%20%28DiT%29%20models.%20At%20its%20core%2C%20OminiControl%20leverages%20a%0Aparameter%20reuse%20mechanism%2C%20enabling%20the%20DiT%20to%20encode%20image%20conditions%20using%0Aitself%20as%20a%20powerful%20backbone%20and%20process%20them%20with%20its%20flexible%20multi-modal%0Aattention%20processors.%20Unlike%20existing%20methods%2C%20which%20rely%20heavily%20on%20additional%0Aencoder%20modules%20with%20complex%20architectures%2C%20OminiControl%20%281%29%20effectively%20and%0Aefficiently%20incorporates%20injected%20image%20conditions%20with%20only%20~0.1%25%20additional%0Aparameters%2C%20and%20%282%29%20addresses%20a%20wide%20range%20of%20image%20conditioning%20tasks%20in%20a%0Aunified%20manner%2C%20including%20subject-driven%20generation%20and%20spatially-aligned%0Aconditions%20such%20as%20edges%2C%20depth%2C%20and%20more.%20Remarkably%2C%20these%20capabilities%20are%0Aachieved%20by%20training%20on%20images%20generated%20by%20the%20DiT%20itself%2C%20which%20is%0Aparticularly%20beneficial%20for%20subject-driven%20generation.%20Extensive%20evaluations%0Ademonstrate%20that%20OminiControl%20outperforms%20existing%20UNet-based%20and%20DiT-adapted%0Amodels%20in%20both%20subject-driven%20and%20spatially-aligned%20conditional%20generation.%0AAdditionally%2C%20we%20release%20our%20training%20dataset%2C%20Subjects200K%2C%20a%20diverse%0Acollection%20of%20over%20200%2C000%20identity-consistent%20images%2C%20along%20with%20an%20efficient%0Adata%20synthesis%20pipeline%20to%20advance%20research%20in%20subject-consistent%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15098v2&entry.124074799=Read"},
{"title": "Tuning Synaptic Connections instead of Weights by Genetic Algorithm in\n  Spiking Policy Network", "author": "Duzhen Zhang and Tielin Zhang and Shuncheng Jia and Qingyu Wang and Bo Xu", "abstract": "  Learning from interaction is the primary way that biological agents acquire\nknowledge about their environment and themselves. Modern deep reinforcement\nlearning (DRL) explores a computational approach to learning from interaction\nand has made significant progress in solving various tasks. However, despite\nits power, DRL still falls short of biological agents in terms of energy\nefficiency. Although the underlying mechanisms are not fully understood, we\nbelieve that the integration of spiking communication between neurons and\nbiologically-plausible synaptic plasticity plays a prominent role in achieving\ngreater energy efficiency. Following this biological intuition, we optimized a\nspiking policy network (SPN) using a genetic algorithm as an energy-efficient\nalternative to DRL. Our SPN mimics the sensorimotor neuron pathway of insects\nand communicates through event-based spikes. Inspired by biological research\nshowing that the brain forms memories by creating new synaptic connections and\nrewiring these connections based on new experiences, we tuned the synaptic\nconnections instead of weights in the SPN to solve given tasks. Experimental\nresults on several robotic control tasks demonstrate that our method can\nachieve the same level of performance as mainstream DRL methods while\nexhibiting significantly higher energy efficiency.\n", "link": "http://arxiv.org/abs/2301.10292v2", "date": "2024-11-25", "relevancy": 1.4578, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4867}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning%20Synaptic%20Connections%20instead%20of%20Weights%20by%20Genetic%20Algorithm%20in%0A%20%20Spiking%20Policy%20Network&body=Title%3A%20Tuning%20Synaptic%20Connections%20instead%20of%20Weights%20by%20Genetic%20Algorithm%20in%0A%20%20Spiking%20Policy%20Network%0AAuthor%3A%20Duzhen%20Zhang%20and%20Tielin%20Zhang%20and%20Shuncheng%20Jia%20and%20Qingyu%20Wang%20and%20Bo%20Xu%0AAbstract%3A%20%20%20Learning%20from%20interaction%20is%20the%20primary%20way%20that%20biological%20agents%20acquire%0Aknowledge%20about%20their%20environment%20and%20themselves.%20Modern%20deep%20reinforcement%0Alearning%20%28DRL%29%20explores%20a%20computational%20approach%20to%20learning%20from%20interaction%0Aand%20has%20made%20significant%20progress%20in%20solving%20various%20tasks.%20However%2C%20despite%0Aits%20power%2C%20DRL%20still%20falls%20short%20of%20biological%20agents%20in%20terms%20of%20energy%0Aefficiency.%20Although%20the%20underlying%20mechanisms%20are%20not%20fully%20understood%2C%20we%0Abelieve%20that%20the%20integration%20of%20spiking%20communication%20between%20neurons%20and%0Abiologically-plausible%20synaptic%20plasticity%20plays%20a%20prominent%20role%20in%20achieving%0Agreater%20energy%20efficiency.%20Following%20this%20biological%20intuition%2C%20we%20optimized%20a%0Aspiking%20policy%20network%20%28SPN%29%20using%20a%20genetic%20algorithm%20as%20an%20energy-efficient%0Aalternative%20to%20DRL.%20Our%20SPN%20mimics%20the%20sensorimotor%20neuron%20pathway%20of%20insects%0Aand%20communicates%20through%20event-based%20spikes.%20Inspired%20by%20biological%20research%0Ashowing%20that%20the%20brain%20forms%20memories%20by%20creating%20new%20synaptic%20connections%20and%0Arewiring%20these%20connections%20based%20on%20new%20experiences%2C%20we%20tuned%20the%20synaptic%0Aconnections%20instead%20of%20weights%20in%20the%20SPN%20to%20solve%20given%20tasks.%20Experimental%0Aresults%20on%20several%20robotic%20control%20tasks%20demonstrate%20that%20our%20method%20can%0Aachieve%20the%20same%20level%20of%20performance%20as%20mainstream%20DRL%20methods%20while%0Aexhibiting%20significantly%20higher%20energy%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.10292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning%2520Synaptic%2520Connections%2520instead%2520of%2520Weights%2520by%2520Genetic%2520Algorithm%2520in%250A%2520%2520Spiking%2520Policy%2520Network%26entry.906535625%3DDuzhen%2520Zhang%2520and%2520Tielin%2520Zhang%2520and%2520Shuncheng%2520Jia%2520and%2520Qingyu%2520Wang%2520and%2520Bo%2520Xu%26entry.1292438233%3D%2520%2520Learning%2520from%2520interaction%2520is%2520the%2520primary%2520way%2520that%2520biological%2520agents%2520acquire%250Aknowledge%2520about%2520their%2520environment%2520and%2520themselves.%2520Modern%2520deep%2520reinforcement%250Alearning%2520%2528DRL%2529%2520explores%2520a%2520computational%2520approach%2520to%2520learning%2520from%2520interaction%250Aand%2520has%2520made%2520significant%2520progress%2520in%2520solving%2520various%2520tasks.%2520However%252C%2520despite%250Aits%2520power%252C%2520DRL%2520still%2520falls%2520short%2520of%2520biological%2520agents%2520in%2520terms%2520of%2520energy%250Aefficiency.%2520Although%2520the%2520underlying%2520mechanisms%2520are%2520not%2520fully%2520understood%252C%2520we%250Abelieve%2520that%2520the%2520integration%2520of%2520spiking%2520communication%2520between%2520neurons%2520and%250Abiologically-plausible%2520synaptic%2520plasticity%2520plays%2520a%2520prominent%2520role%2520in%2520achieving%250Agreater%2520energy%2520efficiency.%2520Following%2520this%2520biological%2520intuition%252C%2520we%2520optimized%2520a%250Aspiking%2520policy%2520network%2520%2528SPN%2529%2520using%2520a%2520genetic%2520algorithm%2520as%2520an%2520energy-efficient%250Aalternative%2520to%2520DRL.%2520Our%2520SPN%2520mimics%2520the%2520sensorimotor%2520neuron%2520pathway%2520of%2520insects%250Aand%2520communicates%2520through%2520event-based%2520spikes.%2520Inspired%2520by%2520biological%2520research%250Ashowing%2520that%2520the%2520brain%2520forms%2520memories%2520by%2520creating%2520new%2520synaptic%2520connections%2520and%250Arewiring%2520these%2520connections%2520based%2520on%2520new%2520experiences%252C%2520we%2520tuned%2520the%2520synaptic%250Aconnections%2520instead%2520of%2520weights%2520in%2520the%2520SPN%2520to%2520solve%2520given%2520tasks.%2520Experimental%250Aresults%2520on%2520several%2520robotic%2520control%2520tasks%2520demonstrate%2520that%2520our%2520method%2520can%250Aachieve%2520the%2520same%2520level%2520of%2520performance%2520as%2520mainstream%2520DRL%2520methods%2520while%250Aexhibiting%2520significantly%2520higher%2520energy%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.10292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning%20Synaptic%20Connections%20instead%20of%20Weights%20by%20Genetic%20Algorithm%20in%0A%20%20Spiking%20Policy%20Network&entry.906535625=Duzhen%20Zhang%20and%20Tielin%20Zhang%20and%20Shuncheng%20Jia%20and%20Qingyu%20Wang%20and%20Bo%20Xu&entry.1292438233=%20%20Learning%20from%20interaction%20is%20the%20primary%20way%20that%20biological%20agents%20acquire%0Aknowledge%20about%20their%20environment%20and%20themselves.%20Modern%20deep%20reinforcement%0Alearning%20%28DRL%29%20explores%20a%20computational%20approach%20to%20learning%20from%20interaction%0Aand%20has%20made%20significant%20progress%20in%20solving%20various%20tasks.%20However%2C%20despite%0Aits%20power%2C%20DRL%20still%20falls%20short%20of%20biological%20agents%20in%20terms%20of%20energy%0Aefficiency.%20Although%20the%20underlying%20mechanisms%20are%20not%20fully%20understood%2C%20we%0Abelieve%20that%20the%20integration%20of%20spiking%20communication%20between%20neurons%20and%0Abiologically-plausible%20synaptic%20plasticity%20plays%20a%20prominent%20role%20in%20achieving%0Agreater%20energy%20efficiency.%20Following%20this%20biological%20intuition%2C%20we%20optimized%20a%0Aspiking%20policy%20network%20%28SPN%29%20using%20a%20genetic%20algorithm%20as%20an%20energy-efficient%0Aalternative%20to%20DRL.%20Our%20SPN%20mimics%20the%20sensorimotor%20neuron%20pathway%20of%20insects%0Aand%20communicates%20through%20event-based%20spikes.%20Inspired%20by%20biological%20research%0Ashowing%20that%20the%20brain%20forms%20memories%20by%20creating%20new%20synaptic%20connections%20and%0Arewiring%20these%20connections%20based%20on%20new%20experiences%2C%20we%20tuned%20the%20synaptic%0Aconnections%20instead%20of%20weights%20in%20the%20SPN%20to%20solve%20given%20tasks.%20Experimental%0Aresults%20on%20several%20robotic%20control%20tasks%20demonstrate%20that%20our%20method%20can%0Aachieve%20the%20same%20level%20of%20performance%20as%20mainstream%20DRL%20methods%20while%0Aexhibiting%20significantly%20higher%20energy%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.10292v2&entry.124074799=Read"},
{"title": "Guarding the Gate: ConceptGuard Battles Concept-Level Backdoors in\n  Concept Bottleneck Models", "author": "Songning Lai and Yu Huang and Jiayu Yang and Gaoxiang Huang and Wenshuo Chen and Yutao Yue", "abstract": "  The increasing complexity of AI models, especially in deep learning, has\nraised concerns about transparency and accountability, particularly in\nhigh-stakes applications like medical diagnostics, where opaque models can\nundermine trust. Explainable Artificial Intelligence (XAI) aims to address\nthese issues by providing clear, interpretable models. Among XAI techniques,\nConcept Bottleneck Models (CBMs) enhance transparency by using high-level\nsemantic concepts. However, CBMs are vulnerable to concept-level backdoor\nattacks, which inject hidden triggers into these concepts, leading to\nundetectable anomalous behavior. To address this critical security gap, we\nintroduce ConceptGuard, a novel defense framework specifically designed to\nprotect CBMs from concept-level backdoor attacks. ConceptGuard employs a\nmulti-stage approach, including concept clustering based on text distance\nmeasurements and a voting mechanism among classifiers trained on different\nconcept subgroups, to isolate and mitigate potential triggers. Our\ncontributions are threefold: (i) we present ConceptGuard as the first defense\nmechanism tailored for concept-level backdoor attacks in CBMs; (ii) we provide\ntheoretical guarantees that ConceptGuard can effectively defend against such\nattacks within a certain trigger size threshold, ensuring robustness; and (iii)\nwe demonstrate that ConceptGuard maintains the high performance and\ninterpretability of CBMs, crucial for trustworthiness. Through comprehensive\nexperiments and theoretical proofs, we show that ConceptGuard significantly\nenhances the security and trustworthiness of CBMs, paving the way for their\nsecure deployment in critical applications.\n", "link": "http://arxiv.org/abs/2411.16512v1", "date": "2024-11-25", "relevancy": 1.3444, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guarding%20the%20Gate%3A%20ConceptGuard%20Battles%20Concept-Level%20Backdoors%20in%0A%20%20Concept%20Bottleneck%20Models&body=Title%3A%20Guarding%20the%20Gate%3A%20ConceptGuard%20Battles%20Concept-Level%20Backdoors%20in%0A%20%20Concept%20Bottleneck%20Models%0AAuthor%3A%20Songning%20Lai%20and%20Yu%20Huang%20and%20Jiayu%20Yang%20and%20Gaoxiang%20Huang%20and%20Wenshuo%20Chen%20and%20Yutao%20Yue%0AAbstract%3A%20%20%20The%20increasing%20complexity%20of%20AI%20models%2C%20especially%20in%20deep%20learning%2C%20has%0Araised%20concerns%20about%20transparency%20and%20accountability%2C%20particularly%20in%0Ahigh-stakes%20applications%20like%20medical%20diagnostics%2C%20where%20opaque%20models%20can%0Aundermine%20trust.%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20aims%20to%20address%0Athese%20issues%20by%20providing%20clear%2C%20interpretable%20models.%20Among%20XAI%20techniques%2C%0AConcept%20Bottleneck%20Models%20%28CBMs%29%20enhance%20transparency%20by%20using%20high-level%0Asemantic%20concepts.%20However%2C%20CBMs%20are%20vulnerable%20to%20concept-level%20backdoor%0Aattacks%2C%20which%20inject%20hidden%20triggers%20into%20these%20concepts%2C%20leading%20to%0Aundetectable%20anomalous%20behavior.%20To%20address%20this%20critical%20security%20gap%2C%20we%0Aintroduce%20ConceptGuard%2C%20a%20novel%20defense%20framework%20specifically%20designed%20to%0Aprotect%20CBMs%20from%20concept-level%20backdoor%20attacks.%20ConceptGuard%20employs%20a%0Amulti-stage%20approach%2C%20including%20concept%20clustering%20based%20on%20text%20distance%0Ameasurements%20and%20a%20voting%20mechanism%20among%20classifiers%20trained%20on%20different%0Aconcept%20subgroups%2C%20to%20isolate%20and%20mitigate%20potential%20triggers.%20Our%0Acontributions%20are%20threefold%3A%20%28i%29%20we%20present%20ConceptGuard%20as%20the%20first%20defense%0Amechanism%20tailored%20for%20concept-level%20backdoor%20attacks%20in%20CBMs%3B%20%28ii%29%20we%20provide%0Atheoretical%20guarantees%20that%20ConceptGuard%20can%20effectively%20defend%20against%20such%0Aattacks%20within%20a%20certain%20trigger%20size%20threshold%2C%20ensuring%20robustness%3B%20and%20%28iii%29%0Awe%20demonstrate%20that%20ConceptGuard%20maintains%20the%20high%20performance%20and%0Ainterpretability%20of%20CBMs%2C%20crucial%20for%20trustworthiness.%20Through%20comprehensive%0Aexperiments%20and%20theoretical%20proofs%2C%20we%20show%20that%20ConceptGuard%20significantly%0Aenhances%20the%20security%20and%20trustworthiness%20of%20CBMs%2C%20paving%20the%20way%20for%20their%0Asecure%20deployment%20in%20critical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuarding%2520the%2520Gate%253A%2520ConceptGuard%2520Battles%2520Concept-Level%2520Backdoors%2520in%250A%2520%2520Concept%2520Bottleneck%2520Models%26entry.906535625%3DSongning%2520Lai%2520and%2520Yu%2520Huang%2520and%2520Jiayu%2520Yang%2520and%2520Gaoxiang%2520Huang%2520and%2520Wenshuo%2520Chen%2520and%2520Yutao%2520Yue%26entry.1292438233%3D%2520%2520The%2520increasing%2520complexity%2520of%2520AI%2520models%252C%2520especially%2520in%2520deep%2520learning%252C%2520has%250Araised%2520concerns%2520about%2520transparency%2520and%2520accountability%252C%2520particularly%2520in%250Ahigh-stakes%2520applications%2520like%2520medical%2520diagnostics%252C%2520where%2520opaque%2520models%2520can%250Aundermine%2520trust.%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520aims%2520to%2520address%250Athese%2520issues%2520by%2520providing%2520clear%252C%2520interpretable%2520models.%2520Among%2520XAI%2520techniques%252C%250AConcept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520enhance%2520transparency%2520by%2520using%2520high-level%250Asemantic%2520concepts.%2520However%252C%2520CBMs%2520are%2520vulnerable%2520to%2520concept-level%2520backdoor%250Aattacks%252C%2520which%2520inject%2520hidden%2520triggers%2520into%2520these%2520concepts%252C%2520leading%2520to%250Aundetectable%2520anomalous%2520behavior.%2520To%2520address%2520this%2520critical%2520security%2520gap%252C%2520we%250Aintroduce%2520ConceptGuard%252C%2520a%2520novel%2520defense%2520framework%2520specifically%2520designed%2520to%250Aprotect%2520CBMs%2520from%2520concept-level%2520backdoor%2520attacks.%2520ConceptGuard%2520employs%2520a%250Amulti-stage%2520approach%252C%2520including%2520concept%2520clustering%2520based%2520on%2520text%2520distance%250Ameasurements%2520and%2520a%2520voting%2520mechanism%2520among%2520classifiers%2520trained%2520on%2520different%250Aconcept%2520subgroups%252C%2520to%2520isolate%2520and%2520mitigate%2520potential%2520triggers.%2520Our%250Acontributions%2520are%2520threefold%253A%2520%2528i%2529%2520we%2520present%2520ConceptGuard%2520as%2520the%2520first%2520defense%250Amechanism%2520tailored%2520for%2520concept-level%2520backdoor%2520attacks%2520in%2520CBMs%253B%2520%2528ii%2529%2520we%2520provide%250Atheoretical%2520guarantees%2520that%2520ConceptGuard%2520can%2520effectively%2520defend%2520against%2520such%250Aattacks%2520within%2520a%2520certain%2520trigger%2520size%2520threshold%252C%2520ensuring%2520robustness%253B%2520and%2520%2528iii%2529%250Awe%2520demonstrate%2520that%2520ConceptGuard%2520maintains%2520the%2520high%2520performance%2520and%250Ainterpretability%2520of%2520CBMs%252C%2520crucial%2520for%2520trustworthiness.%2520Through%2520comprehensive%250Aexperiments%2520and%2520theoretical%2520proofs%252C%2520we%2520show%2520that%2520ConceptGuard%2520significantly%250Aenhances%2520the%2520security%2520and%2520trustworthiness%2520of%2520CBMs%252C%2520paving%2520the%2520way%2520for%2520their%250Asecure%2520deployment%2520in%2520critical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guarding%20the%20Gate%3A%20ConceptGuard%20Battles%20Concept-Level%20Backdoors%20in%0A%20%20Concept%20Bottleneck%20Models&entry.906535625=Songning%20Lai%20and%20Yu%20Huang%20and%20Jiayu%20Yang%20and%20Gaoxiang%20Huang%20and%20Wenshuo%20Chen%20and%20Yutao%20Yue&entry.1292438233=%20%20The%20increasing%20complexity%20of%20AI%20models%2C%20especially%20in%20deep%20learning%2C%20has%0Araised%20concerns%20about%20transparency%20and%20accountability%2C%20particularly%20in%0Ahigh-stakes%20applications%20like%20medical%20diagnostics%2C%20where%20opaque%20models%20can%0Aundermine%20trust.%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20aims%20to%20address%0Athese%20issues%20by%20providing%20clear%2C%20interpretable%20models.%20Among%20XAI%20techniques%2C%0AConcept%20Bottleneck%20Models%20%28CBMs%29%20enhance%20transparency%20by%20using%20high-level%0Asemantic%20concepts.%20However%2C%20CBMs%20are%20vulnerable%20to%20concept-level%20backdoor%0Aattacks%2C%20which%20inject%20hidden%20triggers%20into%20these%20concepts%2C%20leading%20to%0Aundetectable%20anomalous%20behavior.%20To%20address%20this%20critical%20security%20gap%2C%20we%0Aintroduce%20ConceptGuard%2C%20a%20novel%20defense%20framework%20specifically%20designed%20to%0Aprotect%20CBMs%20from%20concept-level%20backdoor%20attacks.%20ConceptGuard%20employs%20a%0Amulti-stage%20approach%2C%20including%20concept%20clustering%20based%20on%20text%20distance%0Ameasurements%20and%20a%20voting%20mechanism%20among%20classifiers%20trained%20on%20different%0Aconcept%20subgroups%2C%20to%20isolate%20and%20mitigate%20potential%20triggers.%20Our%0Acontributions%20are%20threefold%3A%20%28i%29%20we%20present%20ConceptGuard%20as%20the%20first%20defense%0Amechanism%20tailored%20for%20concept-level%20backdoor%20attacks%20in%20CBMs%3B%20%28ii%29%20we%20provide%0Atheoretical%20guarantees%20that%20ConceptGuard%20can%20effectively%20defend%20against%20such%0Aattacks%20within%20a%20certain%20trigger%20size%20threshold%2C%20ensuring%20robustness%3B%20and%20%28iii%29%0Awe%20demonstrate%20that%20ConceptGuard%20maintains%20the%20high%20performance%20and%0Ainterpretability%20of%20CBMs%2C%20crucial%20for%20trustworthiness.%20Through%20comprehensive%0Aexperiments%20and%20theoretical%20proofs%2C%20we%20show%20that%20ConceptGuard%20significantly%0Aenhances%20the%20security%20and%20trustworthiness%20of%20CBMs%2C%20paving%20the%20way%20for%20their%0Asecure%20deployment%20in%20critical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16512v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


