<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241106.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with\n  Enhanced Generalization and Personalization Abilities", "author": "Peizhi Yan and Rabab Ward and Qiang Tang and Shan Du", "abstract": "  Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant\npotential for modeling 3D head avatars, providing greater flexibility than\nmesh-based methods and more efficient rendering compared to NeRF-based\napproaches. Despite these advancements, the creation of controllable 3DGS-based\nhead avatars remains time-intensive, often requiring tens of minutes to hours.\nTo expedite this process, we here introduce the \"Gaussian Deja-vu\" framework,\nwhich first obtains a generalized model of the head avatar and then\npersonalizes the result. The generalized model is trained on large 2D\n(synthetic and real) image datasets. This model provides a well-initialized 3D\nGaussian head that is further refined using a monocular video to achieve the\npersonalized head avatar. For personalizing, we propose learnable\nexpression-aware rectification blendmaps to correct the initial 3D Gaussians,\nensuring rapid convergence without the reliance on neural networks. Experiments\ndemonstrate that the proposed method meets its objectives. It outperforms\nstate-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as\nwell as reduces training time consumption to at least a quarter of the existing\nmethods, producing the avatar in minutes.\n", "link": "http://arxiv.org/abs/2409.16147v3", "date": "2024-11-06", "relevancy": 3.9128, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8259}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8259}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Deja-vu%3A%20Creating%20Controllable%203D%20Gaussian%20Head-Avatars%20with%0A%20%20Enhanced%20Generalization%20and%20Personalization%20Abilities&body=Title%3A%20Gaussian%20Deja-vu%3A%20Creating%20Controllable%203D%20Gaussian%20Head-Avatars%20with%0A%20%20Enhanced%20Generalization%20and%20Personalization%20Abilities%0AAuthor%3A%20Peizhi%20Yan%20and%20Rabab%20Ward%20and%20Qiang%20Tang%20and%20Shan%20Du%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20unlocked%20significant%0Apotential%20for%20modeling%203D%20head%20avatars%2C%20providing%20greater%20flexibility%20than%0Amesh-based%20methods%20and%20more%20efficient%20rendering%20compared%20to%20NeRF-based%0Aapproaches.%20Despite%20these%20advancements%2C%20the%20creation%20of%20controllable%203DGS-based%0Ahead%20avatars%20remains%20time-intensive%2C%20often%20requiring%20tens%20of%20minutes%20to%20hours.%0ATo%20expedite%20this%20process%2C%20we%20here%20introduce%20the%20%22Gaussian%20Deja-vu%22%20framework%2C%0Awhich%20first%20obtains%20a%20generalized%20model%20of%20the%20head%20avatar%20and%20then%0Apersonalizes%20the%20result.%20The%20generalized%20model%20is%20trained%20on%20large%202D%0A%28synthetic%20and%20real%29%20image%20datasets.%20This%20model%20provides%20a%20well-initialized%203D%0AGaussian%20head%20that%20is%20further%20refined%20using%20a%20monocular%20video%20to%20achieve%20the%0Apersonalized%20head%20avatar.%20For%20personalizing%2C%20we%20propose%20learnable%0Aexpression-aware%20rectification%20blendmaps%20to%20correct%20the%20initial%203D%20Gaussians%2C%0Aensuring%20rapid%20convergence%20without%20the%20reliance%20on%20neural%20networks.%20Experiments%0Ademonstrate%20that%20the%20proposed%20method%20meets%20its%20objectives.%20It%20outperforms%0Astate-of-the-art%203D%20Gaussian%20head%20avatars%20in%20terms%20of%20photorealistic%20quality%20as%0Awell%20as%20reduces%20training%20time%20consumption%20to%20at%20least%20a%20quarter%20of%20the%20existing%0Amethods%2C%20producing%20the%20avatar%20in%20minutes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16147v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Deja-vu%253A%2520Creating%2520Controllable%25203D%2520Gaussian%2520Head-Avatars%2520with%250A%2520%2520Enhanced%2520Generalization%2520and%2520Personalization%2520Abilities%26entry.906535625%3DPeizhi%2520Yan%2520and%2520Rabab%2520Ward%2520and%2520Qiang%2520Tang%2520and%2520Shan%2520Du%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520unlocked%2520significant%250Apotential%2520for%2520modeling%25203D%2520head%2520avatars%252C%2520providing%2520greater%2520flexibility%2520than%250Amesh-based%2520methods%2520and%2520more%2520efficient%2520rendering%2520compared%2520to%2520NeRF-based%250Aapproaches.%2520Despite%2520these%2520advancements%252C%2520the%2520creation%2520of%2520controllable%25203DGS-based%250Ahead%2520avatars%2520remains%2520time-intensive%252C%2520often%2520requiring%2520tens%2520of%2520minutes%2520to%2520hours.%250ATo%2520expedite%2520this%2520process%252C%2520we%2520here%2520introduce%2520the%2520%2522Gaussian%2520Deja-vu%2522%2520framework%252C%250Awhich%2520first%2520obtains%2520a%2520generalized%2520model%2520of%2520the%2520head%2520avatar%2520and%2520then%250Apersonalizes%2520the%2520result.%2520The%2520generalized%2520model%2520is%2520trained%2520on%2520large%25202D%250A%2528synthetic%2520and%2520real%2529%2520image%2520datasets.%2520This%2520model%2520provides%2520a%2520well-initialized%25203D%250AGaussian%2520head%2520that%2520is%2520further%2520refined%2520using%2520a%2520monocular%2520video%2520to%2520achieve%2520the%250Apersonalized%2520head%2520avatar.%2520For%2520personalizing%252C%2520we%2520propose%2520learnable%250Aexpression-aware%2520rectification%2520blendmaps%2520to%2520correct%2520the%2520initial%25203D%2520Gaussians%252C%250Aensuring%2520rapid%2520convergence%2520without%2520the%2520reliance%2520on%2520neural%2520networks.%2520Experiments%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520meets%2520its%2520objectives.%2520It%2520outperforms%250Astate-of-the-art%25203D%2520Gaussian%2520head%2520avatars%2520in%2520terms%2520of%2520photorealistic%2520quality%2520as%250Awell%2520as%2520reduces%2520training%2520time%2520consumption%2520to%2520at%2520least%2520a%2520quarter%2520of%2520the%2520existing%250Amethods%252C%2520producing%2520the%2520avatar%2520in%2520minutes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16147v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Deja-vu%3A%20Creating%20Controllable%203D%20Gaussian%20Head-Avatars%20with%0A%20%20Enhanced%20Generalization%20and%20Personalization%20Abilities&entry.906535625=Peizhi%20Yan%20and%20Rabab%20Ward%20and%20Qiang%20Tang%20and%20Shan%20Du&entry.1292438233=%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20unlocked%20significant%0Apotential%20for%20modeling%203D%20head%20avatars%2C%20providing%20greater%20flexibility%20than%0Amesh-based%20methods%20and%20more%20efficient%20rendering%20compared%20to%20NeRF-based%0Aapproaches.%20Despite%20these%20advancements%2C%20the%20creation%20of%20controllable%203DGS-based%0Ahead%20avatars%20remains%20time-intensive%2C%20often%20requiring%20tens%20of%20minutes%20to%20hours.%0ATo%20expedite%20this%20process%2C%20we%20here%20introduce%20the%20%22Gaussian%20Deja-vu%22%20framework%2C%0Awhich%20first%20obtains%20a%20generalized%20model%20of%20the%20head%20avatar%20and%20then%0Apersonalizes%20the%20result.%20The%20generalized%20model%20is%20trained%20on%20large%202D%0A%28synthetic%20and%20real%29%20image%20datasets.%20This%20model%20provides%20a%20well-initialized%203D%0AGaussian%20head%20that%20is%20further%20refined%20using%20a%20monocular%20video%20to%20achieve%20the%0Apersonalized%20head%20avatar.%20For%20personalizing%2C%20we%20propose%20learnable%0Aexpression-aware%20rectification%20blendmaps%20to%20correct%20the%20initial%203D%20Gaussians%2C%0Aensuring%20rapid%20convergence%20without%20the%20reliance%20on%20neural%20networks.%20Experiments%0Ademonstrate%20that%20the%20proposed%20method%20meets%20its%20objectives.%20It%20outperforms%0Astate-of-the-art%203D%20Gaussian%20head%20avatars%20in%20terms%20of%20photorealistic%20quality%20as%0Awell%20as%20reduces%20training%20time%20consumption%20to%20at%20least%20a%20quarter%20of%20the%20existing%0Amethods%2C%20producing%20the%20avatar%20in%20minutes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16147v3&entry.124074799=Read"},
{"title": "GS2Pose: Tow-stage 6D Object Pose Estimation Guided by Gaussian\n  Splatting", "author": "Jilan Mei and Junbo Li and Cai Meng", "abstract": "  This paper proposes a new method for accurate and robust 6D pose estimation\nof novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose\ncan utilize the reconstruction results without requiring a high-quality CAD\nmodel, which means it only requires segmented RGBD images as input.\nSpecifically, GS2Pose employs a two-stage structure consisting of coarse\nestimation followed by refined estimation. In the coarse stage, a lightweight\nU-Net network with a polarization attention mechanism, called Pose-Net, is\ndesigned. By using the 3DGS model for supervised training, Pose-Net can\ngenerate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose\nformulates a pose regression algorithm following the idea of reprojection or\nBundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to\nextend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that\nrefines the coarse pose by comparing the input images with the rendered images.\nGS-Refiner also selectively updates parameters in the 3DGS model to achieve\nenvironmental adaptation, thereby enhancing the algorithm's robustness and\nflexibility to illuminative variation, occlusion, and other challenging\ndisruptive factors. GS2Pose was evaluated through experiments conducted on the\nLineMod dataset, where it was compared with similar algorithms, yielding highly\ncompetitive results. The code for GS2Pose will soon be released on GitHub.\n", "link": "http://arxiv.org/abs/2411.03807v1", "date": "2024-11-06", "relevancy": 3.4245, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7379}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6904}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS2Pose%3A%20Tow-stage%206D%20Object%20Pose%20Estimation%20Guided%20by%20Gaussian%0A%20%20Splatting&body=Title%3A%20GS2Pose%3A%20Tow-stage%206D%20Object%20Pose%20Estimation%20Guided%20by%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Jilan%20Mei%20and%20Junbo%20Li%20and%20Cai%20Meng%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20new%20method%20for%20accurate%20and%20robust%206D%20pose%20estimation%0Aof%20novel%20objects%2C%20named%20GS2Pose.%20By%20introducing%203D%20Gaussian%20splatting%2C%20GS2Pose%0Acan%20utilize%20the%20reconstruction%20results%20without%20requiring%20a%20high-quality%20CAD%0Amodel%2C%20which%20means%20it%20only%20requires%20segmented%20RGBD%20images%20as%20input.%0ASpecifically%2C%20GS2Pose%20employs%20a%20two-stage%20structure%20consisting%20of%20coarse%0Aestimation%20followed%20by%20refined%20estimation.%20In%20the%20coarse%20stage%2C%20a%20lightweight%0AU-Net%20network%20with%20a%20polarization%20attention%20mechanism%2C%20called%20Pose-Net%2C%20is%0Adesigned.%20By%20using%20the%203DGS%20model%20for%20supervised%20training%2C%20Pose-Net%20can%0Agenerate%20NOCS%20images%20to%20compute%20a%20coarse%20pose.%20In%20the%20refinement%20stage%2C%20GS2Pose%0Aformulates%20a%20pose%20regression%20algorithm%20following%20the%20idea%20of%20reprojection%20or%0ABundle%20Adjustment%20%28BA%29%2C%20referred%20to%20as%20GS-Refiner.%20By%20leveraging%20Lie%20algebra%20to%0Aextend%203DGS%2C%20GS-Refiner%20obtains%20a%20pose-differentiable%20rendering%20pipeline%20that%0Arefines%20the%20coarse%20pose%20by%20comparing%20the%20input%20images%20with%20the%20rendered%20images.%0AGS-Refiner%20also%20selectively%20updates%20parameters%20in%20the%203DGS%20model%20to%20achieve%0Aenvironmental%20adaptation%2C%20thereby%20enhancing%20the%20algorithm%27s%20robustness%20and%0Aflexibility%20to%20illuminative%20variation%2C%20occlusion%2C%20and%20other%20challenging%0Adisruptive%20factors.%20GS2Pose%20was%20evaluated%20through%20experiments%20conducted%20on%20the%0ALineMod%20dataset%2C%20where%20it%20was%20compared%20with%20similar%20algorithms%2C%20yielding%20highly%0Acompetitive%20results.%20The%20code%20for%20GS2Pose%20will%20soon%20be%20released%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS2Pose%253A%2520Tow-stage%25206D%2520Object%2520Pose%2520Estimation%2520Guided%2520by%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DJilan%2520Mei%2520and%2520Junbo%2520Li%2520and%2520Cai%2520Meng%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520new%2520method%2520for%2520accurate%2520and%2520robust%25206D%2520pose%2520estimation%250Aof%2520novel%2520objects%252C%2520named%2520GS2Pose.%2520By%2520introducing%25203D%2520Gaussian%2520splatting%252C%2520GS2Pose%250Acan%2520utilize%2520the%2520reconstruction%2520results%2520without%2520requiring%2520a%2520high-quality%2520CAD%250Amodel%252C%2520which%2520means%2520it%2520only%2520requires%2520segmented%2520RGBD%2520images%2520as%2520input.%250ASpecifically%252C%2520GS2Pose%2520employs%2520a%2520two-stage%2520structure%2520consisting%2520of%2520coarse%250Aestimation%2520followed%2520by%2520refined%2520estimation.%2520In%2520the%2520coarse%2520stage%252C%2520a%2520lightweight%250AU-Net%2520network%2520with%2520a%2520polarization%2520attention%2520mechanism%252C%2520called%2520Pose-Net%252C%2520is%250Adesigned.%2520By%2520using%2520the%25203DGS%2520model%2520for%2520supervised%2520training%252C%2520Pose-Net%2520can%250Agenerate%2520NOCS%2520images%2520to%2520compute%2520a%2520coarse%2520pose.%2520In%2520the%2520refinement%2520stage%252C%2520GS2Pose%250Aformulates%2520a%2520pose%2520regression%2520algorithm%2520following%2520the%2520idea%2520of%2520reprojection%2520or%250ABundle%2520Adjustment%2520%2528BA%2529%252C%2520referred%2520to%2520as%2520GS-Refiner.%2520By%2520leveraging%2520Lie%2520algebra%2520to%250Aextend%25203DGS%252C%2520GS-Refiner%2520obtains%2520a%2520pose-differentiable%2520rendering%2520pipeline%2520that%250Arefines%2520the%2520coarse%2520pose%2520by%2520comparing%2520the%2520input%2520images%2520with%2520the%2520rendered%2520images.%250AGS-Refiner%2520also%2520selectively%2520updates%2520parameters%2520in%2520the%25203DGS%2520model%2520to%2520achieve%250Aenvironmental%2520adaptation%252C%2520thereby%2520enhancing%2520the%2520algorithm%2527s%2520robustness%2520and%250Aflexibility%2520to%2520illuminative%2520variation%252C%2520occlusion%252C%2520and%2520other%2520challenging%250Adisruptive%2520factors.%2520GS2Pose%2520was%2520evaluated%2520through%2520experiments%2520conducted%2520on%2520the%250ALineMod%2520dataset%252C%2520where%2520it%2520was%2520compared%2520with%2520similar%2520algorithms%252C%2520yielding%2520highly%250Acompetitive%2520results.%2520The%2520code%2520for%2520GS2Pose%2520will%2520soon%2520be%2520released%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS2Pose%3A%20Tow-stage%206D%20Object%20Pose%20Estimation%20Guided%20by%20Gaussian%0A%20%20Splatting&entry.906535625=Jilan%20Mei%20and%20Junbo%20Li%20and%20Cai%20Meng&entry.1292438233=%20%20This%20paper%20proposes%20a%20new%20method%20for%20accurate%20and%20robust%206D%20pose%20estimation%0Aof%20novel%20objects%2C%20named%20GS2Pose.%20By%20introducing%203D%20Gaussian%20splatting%2C%20GS2Pose%0Acan%20utilize%20the%20reconstruction%20results%20without%20requiring%20a%20high-quality%20CAD%0Amodel%2C%20which%20means%20it%20only%20requires%20segmented%20RGBD%20images%20as%20input.%0ASpecifically%2C%20GS2Pose%20employs%20a%20two-stage%20structure%20consisting%20of%20coarse%0Aestimation%20followed%20by%20refined%20estimation.%20In%20the%20coarse%20stage%2C%20a%20lightweight%0AU-Net%20network%20with%20a%20polarization%20attention%20mechanism%2C%20called%20Pose-Net%2C%20is%0Adesigned.%20By%20using%20the%203DGS%20model%20for%20supervised%20training%2C%20Pose-Net%20can%0Agenerate%20NOCS%20images%20to%20compute%20a%20coarse%20pose.%20In%20the%20refinement%20stage%2C%20GS2Pose%0Aformulates%20a%20pose%20regression%20algorithm%20following%20the%20idea%20of%20reprojection%20or%0ABundle%20Adjustment%20%28BA%29%2C%20referred%20to%20as%20GS-Refiner.%20By%20leveraging%20Lie%20algebra%20to%0Aextend%203DGS%2C%20GS-Refiner%20obtains%20a%20pose-differentiable%20rendering%20pipeline%20that%0Arefines%20the%20coarse%20pose%20by%20comparing%20the%20input%20images%20with%20the%20rendered%20images.%0AGS-Refiner%20also%20selectively%20updates%20parameters%20in%20the%203DGS%20model%20to%20achieve%0Aenvironmental%20adaptation%2C%20thereby%20enhancing%20the%20algorithm%27s%20robustness%20and%0Aflexibility%20to%20illuminative%20variation%2C%20occlusion%2C%20and%20other%20challenging%0Adisruptive%20factors.%20GS2Pose%20was%20evaluated%20through%20experiments%20conducted%20on%20the%0ALineMod%20dataset%2C%20where%20it%20was%20compared%20with%20similar%20algorithms%2C%20yielding%20highly%0Acompetitive%20results.%20The%20code%20for%20GS2Pose%20will%20soon%20be%20released%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03807v1&entry.124074799=Read"},
{"title": "GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface\n  Reconstruction in Open Scenes", "author": "Gaochao Song and Chong Cheng and Hao Wang", "abstract": "  In this paper we present a novel method for efficient and effective 3D\nsurface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)\nbased works typically require extensive training and rendering time due to the\nadopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)\nuses an explicit and discrete representation, hence the reconstructed surface\nis built by the huge number of Gaussian primitives, which leads to excessive\nmemory consumption and rough surface details in sparse Gaussian areas. To\naddress these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which\nestablish a continuous scene representation based on discrete 3DGS through\nkernel regression. The GVKF integrates fast 3DGS rasterization and highly\neffective scene implicit representations, achieving high-fidelity open scene\nsurface reconstruction. Experiments on challenging scene datasets demonstrate\nthe efficiency and effectiveness of our proposed GVKF, featuring with high\nreconstruction quality, real-time rendering speed, significant savings in\nstorage and training memory consumption.\n", "link": "http://arxiv.org/abs/2411.01853v2", "date": "2024-11-06", "relevancy": 3.1988, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6861}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6275}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GVKF%3A%20Gaussian%20Voxel%20Kernel%20Functions%20for%20Highly%20Efficient%20Surface%0A%20%20Reconstruction%20in%20Open%20Scenes&body=Title%3A%20GVKF%3A%20Gaussian%20Voxel%20Kernel%20Functions%20for%20Highly%20Efficient%20Surface%0A%20%20Reconstruction%20in%20Open%20Scenes%0AAuthor%3A%20Gaochao%20Song%20and%20Chong%20Cheng%20and%20Hao%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%20we%20present%20a%20novel%20method%20for%20efficient%20and%20effective%203D%0Asurface%20reconstruction%20in%20open%20scenes.%20Existing%20Neural%20Radiance%20Fields%20%28NeRF%29%0Abased%20works%20typically%20require%20extensive%20training%20and%20rendering%20time%20due%20to%20the%0Aadopted%20implicit%20representations.%20In%20contrast%2C%203D%20Gaussian%20splatting%20%283DGS%29%0Auses%20an%20explicit%20and%20discrete%20representation%2C%20hence%20the%20reconstructed%20surface%0Ais%20built%20by%20the%20huge%20number%20of%20Gaussian%20primitives%2C%20which%20leads%20to%20excessive%0Amemory%20consumption%20and%20rough%20surface%20details%20in%20sparse%20Gaussian%20areas.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20Gaussian%20Voxel%20Kernel%20Functions%20%28GVKF%29%2C%20which%0Aestablish%20a%20continuous%20scene%20representation%20based%20on%20discrete%203DGS%20through%0Akernel%20regression.%20The%20GVKF%20integrates%20fast%203DGS%20rasterization%20and%20highly%0Aeffective%20scene%20implicit%20representations%2C%20achieving%20high-fidelity%20open%20scene%0Asurface%20reconstruction.%20Experiments%20on%20challenging%20scene%20datasets%20demonstrate%0Athe%20efficiency%20and%20effectiveness%20of%20our%20proposed%20GVKF%2C%20featuring%20with%20high%0Areconstruction%20quality%2C%20real-time%20rendering%20speed%2C%20significant%20savings%20in%0Astorage%20and%20training%20memory%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGVKF%253A%2520Gaussian%2520Voxel%2520Kernel%2520Functions%2520for%2520Highly%2520Efficient%2520Surface%250A%2520%2520Reconstruction%2520in%2520Open%2520Scenes%26entry.906535625%3DGaochao%2520Song%2520and%2520Chong%2520Cheng%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520present%2520a%2520novel%2520method%2520for%2520efficient%2520and%2520effective%25203D%250Asurface%2520reconstruction%2520in%2520open%2520scenes.%2520Existing%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%250Abased%2520works%2520typically%2520require%2520extensive%2520training%2520and%2520rendering%2520time%2520due%2520to%2520the%250Aadopted%2520implicit%2520representations.%2520In%2520contrast%252C%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%250Auses%2520an%2520explicit%2520and%2520discrete%2520representation%252C%2520hence%2520the%2520reconstructed%2520surface%250Ais%2520built%2520by%2520the%2520huge%2520number%2520of%2520Gaussian%2520primitives%252C%2520which%2520leads%2520to%2520excessive%250Amemory%2520consumption%2520and%2520rough%2520surface%2520details%2520in%2520sparse%2520Gaussian%2520areas.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520Gaussian%2520Voxel%2520Kernel%2520Functions%2520%2528GVKF%2529%252C%2520which%250Aestablish%2520a%2520continuous%2520scene%2520representation%2520based%2520on%2520discrete%25203DGS%2520through%250Akernel%2520regression.%2520The%2520GVKF%2520integrates%2520fast%25203DGS%2520rasterization%2520and%2520highly%250Aeffective%2520scene%2520implicit%2520representations%252C%2520achieving%2520high-fidelity%2520open%2520scene%250Asurface%2520reconstruction.%2520Experiments%2520on%2520challenging%2520scene%2520datasets%2520demonstrate%250Athe%2520efficiency%2520and%2520effectiveness%2520of%2520our%2520proposed%2520GVKF%252C%2520featuring%2520with%2520high%250Areconstruction%2520quality%252C%2520real-time%2520rendering%2520speed%252C%2520significant%2520savings%2520in%250Astorage%2520and%2520training%2520memory%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GVKF%3A%20Gaussian%20Voxel%20Kernel%20Functions%20for%20Highly%20Efficient%20Surface%0A%20%20Reconstruction%20in%20Open%20Scenes&entry.906535625=Gaochao%20Song%20and%20Chong%20Cheng%20and%20Hao%20Wang&entry.1292438233=%20%20In%20this%20paper%20we%20present%20a%20novel%20method%20for%20efficient%20and%20effective%203D%0Asurface%20reconstruction%20in%20open%20scenes.%20Existing%20Neural%20Radiance%20Fields%20%28NeRF%29%0Abased%20works%20typically%20require%20extensive%20training%20and%20rendering%20time%20due%20to%20the%0Aadopted%20implicit%20representations.%20In%20contrast%2C%203D%20Gaussian%20splatting%20%283DGS%29%0Auses%20an%20explicit%20and%20discrete%20representation%2C%20hence%20the%20reconstructed%20surface%0Ais%20built%20by%20the%20huge%20number%20of%20Gaussian%20primitives%2C%20which%20leads%20to%20excessive%0Amemory%20consumption%20and%20rough%20surface%20details%20in%20sparse%20Gaussian%20areas.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20Gaussian%20Voxel%20Kernel%20Functions%20%28GVKF%29%2C%20which%0Aestablish%20a%20continuous%20scene%20representation%20based%20on%20discrete%203DGS%20through%0Akernel%20regression.%20The%20GVKF%20integrates%20fast%203DGS%20rasterization%20and%20highly%0Aeffective%20scene%20implicit%20representations%2C%20achieving%20high-fidelity%20open%20scene%0Asurface%20reconstruction.%20Experiments%20on%20challenging%20scene%20datasets%20demonstrate%0Athe%20efficiency%20and%20effectiveness%20of%20our%20proposed%20GVKF%2C%20featuring%20with%20high%0Areconstruction%20quality%2C%20real-time%20rendering%20speed%2C%20significant%20savings%20in%0Astorage%20and%20training%20memory%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01853v2&entry.124074799=Read"},
{"title": "SA3DIP: Segment Any 3D Instance with Potential 3D Priors", "author": "Xi Yang and Xu Gu and Xingyilang Yin and Xinbo Gao", "abstract": "  The proliferation of 2D foundation models has sparked research into adapting\nthem for open-world 3D instance segmentation. Recent methods introduce a\nparadigm that leverages superpoints as geometric primitives and incorporates 2D\nmulti-view masks from Segment Anything model (SAM) as merging guidance,\nachieving outstanding zero-shot instance segmentation results. However, the\nlimited use of 3D priors restricts the segmentation performance. Previous\nmethods calculate the 3D superpoints solely based on estimated normal from\nspatial coordinates, resulting in under-segmentation for instances with similar\ngeometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D\nspace suffers from over-segmentation due to SAM's inherent part-level\nsegmentation tendency. To address these issues, we propose SA3DIP, a novel\nmethod for Segmenting Any 3D Instances via exploiting potential 3D Priors.\nSpecifically, on one hand, we generate complementary 3D primitives based on\nboth geometric and textural priors, which reduces the initial errors that\naccumulate in subsequent procedures. On the other hand, we introduce\nsupplemental constraints from the 3D space by using a 3D detector to guide a\nfurther merging process. Furthermore, we notice a considerable portion of\nlow-quality ground truth annotations in ScanNetV2 benchmark, which affect the\nfair evaluations. Thus, we present ScanNetV2-INS with complete ground truth\nlabels and supplement additional instances for 3D class-agnostic instance\nsegmentation. Experimental evaluations on various 2D-3D datasets demonstrate\nthe effectiveness and robustness of our approach. Our code and proposed\nScanNetV2-INS dataset are available HERE.\n", "link": "http://arxiv.org/abs/2411.03819v1", "date": "2024-11-06", "relevancy": 3.0325, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6109}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SA3DIP%3A%20Segment%20Any%203D%20Instance%20with%20Potential%203D%20Priors&body=Title%3A%20SA3DIP%3A%20Segment%20Any%203D%20Instance%20with%20Potential%203D%20Priors%0AAuthor%3A%20Xi%20Yang%20and%20Xu%20Gu%20and%20Xingyilang%20Yin%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20The%20proliferation%20of%202D%20foundation%20models%20has%20sparked%20research%20into%20adapting%0Athem%20for%20open-world%203D%20instance%20segmentation.%20Recent%20methods%20introduce%20a%0Aparadigm%20that%20leverages%20superpoints%20as%20geometric%20primitives%20and%20incorporates%202D%0Amulti-view%20masks%20from%20Segment%20Anything%20model%20%28SAM%29%20as%20merging%20guidance%2C%0Aachieving%20outstanding%20zero-shot%20instance%20segmentation%20results.%20However%2C%20the%0Alimited%20use%20of%203D%20priors%20restricts%20the%20segmentation%20performance.%20Previous%0Amethods%20calculate%20the%203D%20superpoints%20solely%20based%20on%20estimated%20normal%20from%0Aspatial%20coordinates%2C%20resulting%20in%20under-segmentation%20for%20instances%20with%20similar%0Ageometry.%20Besides%2C%20the%20heavy%20reliance%20on%20SAM%20and%20hand-crafted%20algorithms%20in%202D%0Aspace%20suffers%20from%20over-segmentation%20due%20to%20SAM%27s%20inherent%20part-level%0Asegmentation%20tendency.%20To%20address%20these%20issues%2C%20we%20propose%20SA3DIP%2C%20a%20novel%0Amethod%20for%20Segmenting%20Any%203D%20Instances%20via%20exploiting%20potential%203D%20Priors.%0ASpecifically%2C%20on%20one%20hand%2C%20we%20generate%20complementary%203D%20primitives%20based%20on%0Aboth%20geometric%20and%20textural%20priors%2C%20which%20reduces%20the%20initial%20errors%20that%0Aaccumulate%20in%20subsequent%20procedures.%20On%20the%20other%20hand%2C%20we%20introduce%0Asupplemental%20constraints%20from%20the%203D%20space%20by%20using%20a%203D%20detector%20to%20guide%20a%0Afurther%20merging%20process.%20Furthermore%2C%20we%20notice%20a%20considerable%20portion%20of%0Alow-quality%20ground%20truth%20annotations%20in%20ScanNetV2%20benchmark%2C%20which%20affect%20the%0Afair%20evaluations.%20Thus%2C%20we%20present%20ScanNetV2-INS%20with%20complete%20ground%20truth%0Alabels%20and%20supplement%20additional%20instances%20for%203D%20class-agnostic%20instance%0Asegmentation.%20Experimental%20evaluations%20on%20various%202D-3D%20datasets%20demonstrate%0Athe%20effectiveness%20and%20robustness%20of%20our%20approach.%20Our%20code%20and%20proposed%0AScanNetV2-INS%20dataset%20are%20available%20HERE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSA3DIP%253A%2520Segment%2520Any%25203D%2520Instance%2520with%2520Potential%25203D%2520Priors%26entry.906535625%3DXi%2520Yang%2520and%2520Xu%2520Gu%2520and%2520Xingyilang%2520Yin%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%25202D%2520foundation%2520models%2520has%2520sparked%2520research%2520into%2520adapting%250Athem%2520for%2520open-world%25203D%2520instance%2520segmentation.%2520Recent%2520methods%2520introduce%2520a%250Aparadigm%2520that%2520leverages%2520superpoints%2520as%2520geometric%2520primitives%2520and%2520incorporates%25202D%250Amulti-view%2520masks%2520from%2520Segment%2520Anything%2520model%2520%2528SAM%2529%2520as%2520merging%2520guidance%252C%250Aachieving%2520outstanding%2520zero-shot%2520instance%2520segmentation%2520results.%2520However%252C%2520the%250Alimited%2520use%2520of%25203D%2520priors%2520restricts%2520the%2520segmentation%2520performance.%2520Previous%250Amethods%2520calculate%2520the%25203D%2520superpoints%2520solely%2520based%2520on%2520estimated%2520normal%2520from%250Aspatial%2520coordinates%252C%2520resulting%2520in%2520under-segmentation%2520for%2520instances%2520with%2520similar%250Ageometry.%2520Besides%252C%2520the%2520heavy%2520reliance%2520on%2520SAM%2520and%2520hand-crafted%2520algorithms%2520in%25202D%250Aspace%2520suffers%2520from%2520over-segmentation%2520due%2520to%2520SAM%2527s%2520inherent%2520part-level%250Asegmentation%2520tendency.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520SA3DIP%252C%2520a%2520novel%250Amethod%2520for%2520Segmenting%2520Any%25203D%2520Instances%2520via%2520exploiting%2520potential%25203D%2520Priors.%250ASpecifically%252C%2520on%2520one%2520hand%252C%2520we%2520generate%2520complementary%25203D%2520primitives%2520based%2520on%250Aboth%2520geometric%2520and%2520textural%2520priors%252C%2520which%2520reduces%2520the%2520initial%2520errors%2520that%250Aaccumulate%2520in%2520subsequent%2520procedures.%2520On%2520the%2520other%2520hand%252C%2520we%2520introduce%250Asupplemental%2520constraints%2520from%2520the%25203D%2520space%2520by%2520using%2520a%25203D%2520detector%2520to%2520guide%2520a%250Afurther%2520merging%2520process.%2520Furthermore%252C%2520we%2520notice%2520a%2520considerable%2520portion%2520of%250Alow-quality%2520ground%2520truth%2520annotations%2520in%2520ScanNetV2%2520benchmark%252C%2520which%2520affect%2520the%250Afair%2520evaluations.%2520Thus%252C%2520we%2520present%2520ScanNetV2-INS%2520with%2520complete%2520ground%2520truth%250Alabels%2520and%2520supplement%2520additional%2520instances%2520for%25203D%2520class-agnostic%2520instance%250Asegmentation.%2520Experimental%2520evaluations%2520on%2520various%25202D-3D%2520datasets%2520demonstrate%250Athe%2520effectiveness%2520and%2520robustness%2520of%2520our%2520approach.%2520Our%2520code%2520and%2520proposed%250AScanNetV2-INS%2520dataset%2520are%2520available%2520HERE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SA3DIP%3A%20Segment%20Any%203D%20Instance%20with%20Potential%203D%20Priors&entry.906535625=Xi%20Yang%20and%20Xu%20Gu%20and%20Xingyilang%20Yin%20and%20Xinbo%20Gao&entry.1292438233=%20%20The%20proliferation%20of%202D%20foundation%20models%20has%20sparked%20research%20into%20adapting%0Athem%20for%20open-world%203D%20instance%20segmentation.%20Recent%20methods%20introduce%20a%0Aparadigm%20that%20leverages%20superpoints%20as%20geometric%20primitives%20and%20incorporates%202D%0Amulti-view%20masks%20from%20Segment%20Anything%20model%20%28SAM%29%20as%20merging%20guidance%2C%0Aachieving%20outstanding%20zero-shot%20instance%20segmentation%20results.%20However%2C%20the%0Alimited%20use%20of%203D%20priors%20restricts%20the%20segmentation%20performance.%20Previous%0Amethods%20calculate%20the%203D%20superpoints%20solely%20based%20on%20estimated%20normal%20from%0Aspatial%20coordinates%2C%20resulting%20in%20under-segmentation%20for%20instances%20with%20similar%0Ageometry.%20Besides%2C%20the%20heavy%20reliance%20on%20SAM%20and%20hand-crafted%20algorithms%20in%202D%0Aspace%20suffers%20from%20over-segmentation%20due%20to%20SAM%27s%20inherent%20part-level%0Asegmentation%20tendency.%20To%20address%20these%20issues%2C%20we%20propose%20SA3DIP%2C%20a%20novel%0Amethod%20for%20Segmenting%20Any%203D%20Instances%20via%20exploiting%20potential%203D%20Priors.%0ASpecifically%2C%20on%20one%20hand%2C%20we%20generate%20complementary%203D%20primitives%20based%20on%0Aboth%20geometric%20and%20textural%20priors%2C%20which%20reduces%20the%20initial%20errors%20that%0Aaccumulate%20in%20subsequent%20procedures.%20On%20the%20other%20hand%2C%20we%20introduce%0Asupplemental%20constraints%20from%20the%203D%20space%20by%20using%20a%203D%20detector%20to%20guide%20a%0Afurther%20merging%20process.%20Furthermore%2C%20we%20notice%20a%20considerable%20portion%20of%0Alow-quality%20ground%20truth%20annotations%20in%20ScanNetV2%20benchmark%2C%20which%20affect%20the%0Afair%20evaluations.%20Thus%2C%20we%20present%20ScanNetV2-INS%20with%20complete%20ground%20truth%0Alabels%20and%20supplement%20additional%20instances%20for%203D%20class-agnostic%20instance%0Asegmentation.%20Experimental%20evaluations%20on%20various%202D-3D%20datasets%20demonstrate%0Athe%20effectiveness%20and%20robustness%20of%20our%20approach.%20Our%20code%20and%20proposed%0AScanNetV2-INS%20dataset%20are%20available%20HERE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03819v1&entry.124074799=Read"},
{"title": "Select2Plan: Training-Free ICL-Based Planning through VQA and Memory\n  Retrieval", "author": "Davide Buoso and Luke Robinson and Giuseppe Averta and Philip Torr and Tim Franzmeyer and Daniele De Martini", "abstract": "  This study explores the potential of off-the-shelf Vision-Language Models\n(VLMs) for high-level robot planning in the context of autonomous navigation.\nIndeed, while most of existing learning-based approaches for path planning\nrequire extensive task-specific training/fine-tuning, we demonstrate how such\ntraining can be avoided for most practical cases. To do this, we introduce\nSelect2Plan (S2P), a novel training-free framework for high-level robot\nplanning which completely eliminates the need for fine-tuning or specialised\ntraining. By leveraging structured Visual Question-Answering (VQA) and\nIn-Context Learning (ICL), our approach drastically reduces the need for data\ncollection, requiring a fraction of the task-specific data typically used by\ntrained models, or even relying only on online data. Our method facilitates the\neffective use of a generally trained VLM in a flexible and cost-efficient way,\nand does not require additional sensing except for a simple monocular camera.\nWe demonstrate its adaptability across various scene types, context sources,\nand sensing setups. We evaluate our approach in two distinct scenarios:\ntraditional First-Person View (FPV) and infrastructure-driven Third-Person View\n(TPV) navigation, demonstrating the flexibility and simplicity of our method.\nOur technique significantly enhances the navigational capabilities of a\nbaseline VLM of approximately 50% in TPV scenario, and is comparable to trained\nmodels in the FPV one, with as few as 20 demonstrations.\n", "link": "http://arxiv.org/abs/2411.04006v1", "date": "2024-11-06", "relevancy": 2.9805, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.61}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Select2Plan%3A%20Training-Free%20ICL-Based%20Planning%20through%20VQA%20and%20Memory%0A%20%20Retrieval&body=Title%3A%20Select2Plan%3A%20Training-Free%20ICL-Based%20Planning%20through%20VQA%20and%20Memory%0A%20%20Retrieval%0AAuthor%3A%20Davide%20Buoso%20and%20Luke%20Robinson%20and%20Giuseppe%20Averta%20and%20Philip%20Torr%20and%20Tim%20Franzmeyer%20and%20Daniele%20De%20Martini%0AAbstract%3A%20%20%20This%20study%20explores%20the%20potential%20of%20off-the-shelf%20Vision-Language%20Models%0A%28VLMs%29%20for%20high-level%20robot%20planning%20in%20the%20context%20of%20autonomous%20navigation.%0AIndeed%2C%20while%20most%20of%20existing%20learning-based%20approaches%20for%20path%20planning%0Arequire%20extensive%20task-specific%20training/fine-tuning%2C%20we%20demonstrate%20how%20such%0Atraining%20can%20be%20avoided%20for%20most%20practical%20cases.%20To%20do%20this%2C%20we%20introduce%0ASelect2Plan%20%28S2P%29%2C%20a%20novel%20training-free%20framework%20for%20high-level%20robot%0Aplanning%20which%20completely%20eliminates%20the%20need%20for%20fine-tuning%20or%20specialised%0Atraining.%20By%20leveraging%20structured%20Visual%20Question-Answering%20%28VQA%29%20and%0AIn-Context%20Learning%20%28ICL%29%2C%20our%20approach%20drastically%20reduces%20the%20need%20for%20data%0Acollection%2C%20requiring%20a%20fraction%20of%20the%20task-specific%20data%20typically%20used%20by%0Atrained%20models%2C%20or%20even%20relying%20only%20on%20online%20data.%20Our%20method%20facilitates%20the%0Aeffective%20use%20of%20a%20generally%20trained%20VLM%20in%20a%20flexible%20and%20cost-efficient%20way%2C%0Aand%20does%20not%20require%20additional%20sensing%20except%20for%20a%20simple%20monocular%20camera.%0AWe%20demonstrate%20its%20adaptability%20across%20various%20scene%20types%2C%20context%20sources%2C%0Aand%20sensing%20setups.%20We%20evaluate%20our%20approach%20in%20two%20distinct%20scenarios%3A%0Atraditional%20First-Person%20View%20%28FPV%29%20and%20infrastructure-driven%20Third-Person%20View%0A%28TPV%29%20navigation%2C%20demonstrating%20the%20flexibility%20and%20simplicity%20of%20our%20method.%0AOur%20technique%20significantly%20enhances%20the%20navigational%20capabilities%20of%20a%0Abaseline%20VLM%20of%20approximately%2050%25%20in%20TPV%20scenario%2C%20and%20is%20comparable%20to%20trained%0Amodels%20in%20the%20FPV%20one%2C%20with%20as%20few%20as%2020%20demonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelect2Plan%253A%2520Training-Free%2520ICL-Based%2520Planning%2520through%2520VQA%2520and%2520Memory%250A%2520%2520Retrieval%26entry.906535625%3DDavide%2520Buoso%2520and%2520Luke%2520Robinson%2520and%2520Giuseppe%2520Averta%2520and%2520Philip%2520Torr%2520and%2520Tim%2520Franzmeyer%2520and%2520Daniele%2520De%2520Martini%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520potential%2520of%2520off-the-shelf%2520Vision-Language%2520Models%250A%2528VLMs%2529%2520for%2520high-level%2520robot%2520planning%2520in%2520the%2520context%2520of%2520autonomous%2520navigation.%250AIndeed%252C%2520while%2520most%2520of%2520existing%2520learning-based%2520approaches%2520for%2520path%2520planning%250Arequire%2520extensive%2520task-specific%2520training/fine-tuning%252C%2520we%2520demonstrate%2520how%2520such%250Atraining%2520can%2520be%2520avoided%2520for%2520most%2520practical%2520cases.%2520To%2520do%2520this%252C%2520we%2520introduce%250ASelect2Plan%2520%2528S2P%2529%252C%2520a%2520novel%2520training-free%2520framework%2520for%2520high-level%2520robot%250Aplanning%2520which%2520completely%2520eliminates%2520the%2520need%2520for%2520fine-tuning%2520or%2520specialised%250Atraining.%2520By%2520leveraging%2520structured%2520Visual%2520Question-Answering%2520%2528VQA%2529%2520and%250AIn-Context%2520Learning%2520%2528ICL%2529%252C%2520our%2520approach%2520drastically%2520reduces%2520the%2520need%2520for%2520data%250Acollection%252C%2520requiring%2520a%2520fraction%2520of%2520the%2520task-specific%2520data%2520typically%2520used%2520by%250Atrained%2520models%252C%2520or%2520even%2520relying%2520only%2520on%2520online%2520data.%2520Our%2520method%2520facilitates%2520the%250Aeffective%2520use%2520of%2520a%2520generally%2520trained%2520VLM%2520in%2520a%2520flexible%2520and%2520cost-efficient%2520way%252C%250Aand%2520does%2520not%2520require%2520additional%2520sensing%2520except%2520for%2520a%2520simple%2520monocular%2520camera.%250AWe%2520demonstrate%2520its%2520adaptability%2520across%2520various%2520scene%2520types%252C%2520context%2520sources%252C%250Aand%2520sensing%2520setups.%2520We%2520evaluate%2520our%2520approach%2520in%2520two%2520distinct%2520scenarios%253A%250Atraditional%2520First-Person%2520View%2520%2528FPV%2529%2520and%2520infrastructure-driven%2520Third-Person%2520View%250A%2528TPV%2529%2520navigation%252C%2520demonstrating%2520the%2520flexibility%2520and%2520simplicity%2520of%2520our%2520method.%250AOur%2520technique%2520significantly%2520enhances%2520the%2520navigational%2520capabilities%2520of%2520a%250Abaseline%2520VLM%2520of%2520approximately%252050%2525%2520in%2520TPV%2520scenario%252C%2520and%2520is%2520comparable%2520to%2520trained%250Amodels%2520in%2520the%2520FPV%2520one%252C%2520with%2520as%2520few%2520as%252020%2520demonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Select2Plan%3A%20Training-Free%20ICL-Based%20Planning%20through%20VQA%20and%20Memory%0A%20%20Retrieval&entry.906535625=Davide%20Buoso%20and%20Luke%20Robinson%20and%20Giuseppe%20Averta%20and%20Philip%20Torr%20and%20Tim%20Franzmeyer%20and%20Daniele%20De%20Martini&entry.1292438233=%20%20This%20study%20explores%20the%20potential%20of%20off-the-shelf%20Vision-Language%20Models%0A%28VLMs%29%20for%20high-level%20robot%20planning%20in%20the%20context%20of%20autonomous%20navigation.%0AIndeed%2C%20while%20most%20of%20existing%20learning-based%20approaches%20for%20path%20planning%0Arequire%20extensive%20task-specific%20training/fine-tuning%2C%20we%20demonstrate%20how%20such%0Atraining%20can%20be%20avoided%20for%20most%20practical%20cases.%20To%20do%20this%2C%20we%20introduce%0ASelect2Plan%20%28S2P%29%2C%20a%20novel%20training-free%20framework%20for%20high-level%20robot%0Aplanning%20which%20completely%20eliminates%20the%20need%20for%20fine-tuning%20or%20specialised%0Atraining.%20By%20leveraging%20structured%20Visual%20Question-Answering%20%28VQA%29%20and%0AIn-Context%20Learning%20%28ICL%29%2C%20our%20approach%20drastically%20reduces%20the%20need%20for%20data%0Acollection%2C%20requiring%20a%20fraction%20of%20the%20task-specific%20data%20typically%20used%20by%0Atrained%20models%2C%20or%20even%20relying%20only%20on%20online%20data.%20Our%20method%20facilitates%20the%0Aeffective%20use%20of%20a%20generally%20trained%20VLM%20in%20a%20flexible%20and%20cost-efficient%20way%2C%0Aand%20does%20not%20require%20additional%20sensing%20except%20for%20a%20simple%20monocular%20camera.%0AWe%20demonstrate%20its%20adaptability%20across%20various%20scene%20types%2C%20context%20sources%2C%0Aand%20sensing%20setups.%20We%20evaluate%20our%20approach%20in%20two%20distinct%20scenarios%3A%0Atraditional%20First-Person%20View%20%28FPV%29%20and%20infrastructure-driven%20Third-Person%20View%0A%28TPV%29%20navigation%2C%20demonstrating%20the%20flexibility%20and%20simplicity%20of%20our%20method.%0AOur%20technique%20significantly%20enhances%20the%20navigational%20capabilities%20of%20a%0Abaseline%20VLM%20of%20approximately%2050%25%20in%20TPV%20scenario%2C%20and%20is%20comparable%20to%20trained%0Amodels%20in%20the%20FPV%20one%2C%20with%20as%20few%20as%2020%20demonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04006v1&entry.124074799=Read"},
{"title": "Self-supervised 3D Point Cloud Completion via Multi-view Adversarial\n  Learning", "author": "Lintai Wu and Xianjing Cheng and Yong Xu and Huanqiang Zeng and Junhui Hou", "abstract": "  In real-world scenarios, scanned point clouds are often incomplete due to\nocclusion issues. The task of self-supervised point cloud completion involves\nreconstructing missing regions of these incomplete objects without the\nsupervision of complete ground truth. Current self-supervised methods either\nrely on multiple views of partial observations for supervision or overlook the\nintrinsic geometric similarity that can be identified and utilized from the\ngiven partial point clouds. In this paper, we propose MAL-SPC, a framework that\neffectively leverages both object-level and category-specific geometric\nsimilarities to complete missing structures. Our MAL-SPC does not require any\n3D complete supervision and only necessitates a single partial point cloud for\neach object. Specifically, we first introduce a Pattern Retrieval Network to\nretrieve similar position and curvature patterns between the partial input and\nthe predicted shape, then leverage these similarities to densify and refine the\nreconstructed results. Additionally, we render the reconstructed complete shape\ninto multi-view depth maps and design an adversarial learning module to learn\nthe geometry of the target shape from category-specific single-view depth\nimages. To achieve anisotropic rendering, we design a density-aware radius\nestimation algorithm to improve the quality of the rendered images. Our MAL-SPC\nyields the best results compared to current state-of-the-art methods.We will\nmake the source code publicly available at \\url{https://github.com/ltwu6/malspc\n", "link": "http://arxiv.org/abs/2407.09786v2", "date": "2024-11-06", "relevancy": 2.9632, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6016}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5901}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%203D%20Point%20Cloud%20Completion%20via%20Multi-view%20Adversarial%0A%20%20Learning&body=Title%3A%20Self-supervised%203D%20Point%20Cloud%20Completion%20via%20Multi-view%20Adversarial%0A%20%20Learning%0AAuthor%3A%20Lintai%20Wu%20and%20Xianjing%20Cheng%20and%20Yong%20Xu%20and%20Huanqiang%20Zeng%20and%20Junhui%20Hou%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20scanned%20point%20clouds%20are%20often%20incomplete%20due%20to%0Aocclusion%20issues.%20The%20task%20of%20self-supervised%20point%20cloud%20completion%20involves%0Areconstructing%20missing%20regions%20of%20these%20incomplete%20objects%20without%20the%0Asupervision%20of%20complete%20ground%20truth.%20Current%20self-supervised%20methods%20either%0Arely%20on%20multiple%20views%20of%20partial%20observations%20for%20supervision%20or%20overlook%20the%0Aintrinsic%20geometric%20similarity%20that%20can%20be%20identified%20and%20utilized%20from%20the%0Agiven%20partial%20point%20clouds.%20In%20this%20paper%2C%20we%20propose%20MAL-SPC%2C%20a%20framework%20that%0Aeffectively%20leverages%20both%20object-level%20and%20category-specific%20geometric%0Asimilarities%20to%20complete%20missing%20structures.%20Our%20MAL-SPC%20does%20not%20require%20any%0A3D%20complete%20supervision%20and%20only%20necessitates%20a%20single%20partial%20point%20cloud%20for%0Aeach%20object.%20Specifically%2C%20we%20first%20introduce%20a%20Pattern%20Retrieval%20Network%20to%0Aretrieve%20similar%20position%20and%20curvature%20patterns%20between%20the%20partial%20input%20and%0Athe%20predicted%20shape%2C%20then%20leverage%20these%20similarities%20to%20densify%20and%20refine%20the%0Areconstructed%20results.%20Additionally%2C%20we%20render%20the%20reconstructed%20complete%20shape%0Ainto%20multi-view%20depth%20maps%20and%20design%20an%20adversarial%20learning%20module%20to%20learn%0Athe%20geometry%20of%20the%20target%20shape%20from%20category-specific%20single-view%20depth%0Aimages.%20To%20achieve%20anisotropic%20rendering%2C%20we%20design%20a%20density-aware%20radius%0Aestimation%20algorithm%20to%20improve%20the%20quality%20of%20the%20rendered%20images.%20Our%20MAL-SPC%0Ayields%20the%20best%20results%20compared%20to%20current%20state-of-the-art%20methods.We%20will%0Amake%20the%20source%20code%20publicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/ltwu6/malspc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09786v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%25203D%2520Point%2520Cloud%2520Completion%2520via%2520Multi-view%2520Adversarial%250A%2520%2520Learning%26entry.906535625%3DLintai%2520Wu%2520and%2520Xianjing%2520Cheng%2520and%2520Yong%2520Xu%2520and%2520Huanqiang%2520Zeng%2520and%2520Junhui%2520Hou%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520scanned%2520point%2520clouds%2520are%2520often%2520incomplete%2520due%2520to%250Aocclusion%2520issues.%2520The%2520task%2520of%2520self-supervised%2520point%2520cloud%2520completion%2520involves%250Areconstructing%2520missing%2520regions%2520of%2520these%2520incomplete%2520objects%2520without%2520the%250Asupervision%2520of%2520complete%2520ground%2520truth.%2520Current%2520self-supervised%2520methods%2520either%250Arely%2520on%2520multiple%2520views%2520of%2520partial%2520observations%2520for%2520supervision%2520or%2520overlook%2520the%250Aintrinsic%2520geometric%2520similarity%2520that%2520can%2520be%2520identified%2520and%2520utilized%2520from%2520the%250Agiven%2520partial%2520point%2520clouds.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MAL-SPC%252C%2520a%2520framework%2520that%250Aeffectively%2520leverages%2520both%2520object-level%2520and%2520category-specific%2520geometric%250Asimilarities%2520to%2520complete%2520missing%2520structures.%2520Our%2520MAL-SPC%2520does%2520not%2520require%2520any%250A3D%2520complete%2520supervision%2520and%2520only%2520necessitates%2520a%2520single%2520partial%2520point%2520cloud%2520for%250Aeach%2520object.%2520Specifically%252C%2520we%2520first%2520introduce%2520a%2520Pattern%2520Retrieval%2520Network%2520to%250Aretrieve%2520similar%2520position%2520and%2520curvature%2520patterns%2520between%2520the%2520partial%2520input%2520and%250Athe%2520predicted%2520shape%252C%2520then%2520leverage%2520these%2520similarities%2520to%2520densify%2520and%2520refine%2520the%250Areconstructed%2520results.%2520Additionally%252C%2520we%2520render%2520the%2520reconstructed%2520complete%2520shape%250Ainto%2520multi-view%2520depth%2520maps%2520and%2520design%2520an%2520adversarial%2520learning%2520module%2520to%2520learn%250Athe%2520geometry%2520of%2520the%2520target%2520shape%2520from%2520category-specific%2520single-view%2520depth%250Aimages.%2520To%2520achieve%2520anisotropic%2520rendering%252C%2520we%2520design%2520a%2520density-aware%2520radius%250Aestimation%2520algorithm%2520to%2520improve%2520the%2520quality%2520of%2520the%2520rendered%2520images.%2520Our%2520MAL-SPC%250Ayields%2520the%2520best%2520results%2520compared%2520to%2520current%2520state-of-the-art%2520methods.We%2520will%250Amake%2520the%2520source%2520code%2520publicly%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/ltwu6/malspc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09786v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%203D%20Point%20Cloud%20Completion%20via%20Multi-view%20Adversarial%0A%20%20Learning&entry.906535625=Lintai%20Wu%20and%20Xianjing%20Cheng%20and%20Yong%20Xu%20and%20Huanqiang%20Zeng%20and%20Junhui%20Hou&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20scanned%20point%20clouds%20are%20often%20incomplete%20due%20to%0Aocclusion%20issues.%20The%20task%20of%20self-supervised%20point%20cloud%20completion%20involves%0Areconstructing%20missing%20regions%20of%20these%20incomplete%20objects%20without%20the%0Asupervision%20of%20complete%20ground%20truth.%20Current%20self-supervised%20methods%20either%0Arely%20on%20multiple%20views%20of%20partial%20observations%20for%20supervision%20or%20overlook%20the%0Aintrinsic%20geometric%20similarity%20that%20can%20be%20identified%20and%20utilized%20from%20the%0Agiven%20partial%20point%20clouds.%20In%20this%20paper%2C%20we%20propose%20MAL-SPC%2C%20a%20framework%20that%0Aeffectively%20leverages%20both%20object-level%20and%20category-specific%20geometric%0Asimilarities%20to%20complete%20missing%20structures.%20Our%20MAL-SPC%20does%20not%20require%20any%0A3D%20complete%20supervision%20and%20only%20necessitates%20a%20single%20partial%20point%20cloud%20for%0Aeach%20object.%20Specifically%2C%20we%20first%20introduce%20a%20Pattern%20Retrieval%20Network%20to%0Aretrieve%20similar%20position%20and%20curvature%20patterns%20between%20the%20partial%20input%20and%0Athe%20predicted%20shape%2C%20then%20leverage%20these%20similarities%20to%20densify%20and%20refine%20the%0Areconstructed%20results.%20Additionally%2C%20we%20render%20the%20reconstructed%20complete%20shape%0Ainto%20multi-view%20depth%20maps%20and%20design%20an%20adversarial%20learning%20module%20to%20learn%0Athe%20geometry%20of%20the%20target%20shape%20from%20category-specific%20single-view%20depth%0Aimages.%20To%20achieve%20anisotropic%20rendering%2C%20we%20design%20a%20density-aware%20radius%0Aestimation%20algorithm%20to%20improve%20the%20quality%20of%20the%20rendered%20images.%20Our%20MAL-SPC%0Ayields%20the%20best%20results%20compared%20to%20current%20state-of-the-art%20methods.We%20will%0Amake%20the%20source%20code%20publicly%20available%20at%20%5Curl%7Bhttps%3A//github.com/ltwu6/malspc%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09786v2&entry.124074799=Read"},
{"title": "TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic\n  Scene", "author": "Sandika Biswas and Qianyi Wu and Biplab Banerjee and Hamid Rezatofighi", "abstract": "  Despite advancements in Neural Implicit models for 3D surface reconstruction,\nhandling dynamic environments with arbitrary rigid, non-rigid, or deformable\nentities remains challenging. Many template-based methods are entity-specific,\nfocusing on humans, while generic reconstruction methods adaptable to such\ndynamic scenes often require additional inputs like depth or optical flow or\nrely on pre-trained image features for reasonable outcomes. These methods\ntypically use latent codes to capture frame-by-frame deformations. In contrast,\nsome template-free methods bypass these requirements and adopt traditional LBS\n(Linear Blend Skinning) weights for a detailed representation of deformable\nobject motions, although they involve complex optimizations leading to lengthy\ntraining times. To this end, as a remedy, this paper introduces TFS-NeRF, a\ntemplate-free 3D semantic NeRF for dynamic scenes captured from sparse or\nsingle-view RGB videos, featuring interactions among various entities and more\ntime-efficient than other LBS-based approaches. Our framework uses an\nInvertible Neural Network (INN) for LBS prediction, simplifying the training\nprocess. By disentangling the motions of multiple entities and optimizing\nper-entity skinning weights, our method efficiently generates accurate,\nsemantically separable geometries. Extensive experiments demonstrate that our\napproach produces high-quality reconstructions of both deformable and\nnon-deformable objects in complex interactions, with improved training\nefficiency compared to existing methods.\n", "link": "http://arxiv.org/abs/2409.17459v2", "date": "2024-11-06", "relevancy": 2.9527, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TFS-NeRF%3A%20Template-Free%20NeRF%20for%20Semantic%203D%20Reconstruction%20of%20Dynamic%0A%20%20Scene&body=Title%3A%20TFS-NeRF%3A%20Template-Free%20NeRF%20for%20Semantic%203D%20Reconstruction%20of%20Dynamic%0A%20%20Scene%0AAuthor%3A%20Sandika%20Biswas%20and%20Qianyi%20Wu%20and%20Biplab%20Banerjee%20and%20Hamid%20Rezatofighi%0AAbstract%3A%20%20%20Despite%20advancements%20in%20Neural%20Implicit%20models%20for%203D%20surface%20reconstruction%2C%0Ahandling%20dynamic%20environments%20with%20arbitrary%20rigid%2C%20non-rigid%2C%20or%20deformable%0Aentities%20remains%20challenging.%20Many%20template-based%20methods%20are%20entity-specific%2C%0Afocusing%20on%20humans%2C%20while%20generic%20reconstruction%20methods%20adaptable%20to%20such%0Adynamic%20scenes%20often%20require%20additional%20inputs%20like%20depth%20or%20optical%20flow%20or%0Arely%20on%20pre-trained%20image%20features%20for%20reasonable%20outcomes.%20These%20methods%0Atypically%20use%20latent%20codes%20to%20capture%20frame-by-frame%20deformations.%20In%20contrast%2C%0Asome%20template-free%20methods%20bypass%20these%20requirements%20and%20adopt%20traditional%20LBS%0A%28Linear%20Blend%20Skinning%29%20weights%20for%20a%20detailed%20representation%20of%20deformable%0Aobject%20motions%2C%20although%20they%20involve%20complex%20optimizations%20leading%20to%20lengthy%0Atraining%20times.%20To%20this%20end%2C%20as%20a%20remedy%2C%20this%20paper%20introduces%20TFS-NeRF%2C%20a%0Atemplate-free%203D%20semantic%20NeRF%20for%20dynamic%20scenes%20captured%20from%20sparse%20or%0Asingle-view%20RGB%20videos%2C%20featuring%20interactions%20among%20various%20entities%20and%20more%0Atime-efficient%20than%20other%20LBS-based%20approaches.%20Our%20framework%20uses%20an%0AInvertible%20Neural%20Network%20%28INN%29%20for%20LBS%20prediction%2C%20simplifying%20the%20training%0Aprocess.%20By%20disentangling%20the%20motions%20of%20multiple%20entities%20and%20optimizing%0Aper-entity%20skinning%20weights%2C%20our%20method%20efficiently%20generates%20accurate%2C%0Asemantically%20separable%20geometries.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20produces%20high-quality%20reconstructions%20of%20both%20deformable%20and%0Anon-deformable%20objects%20in%20complex%20interactions%2C%20with%20improved%20training%0Aefficiency%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTFS-NeRF%253A%2520Template-Free%2520NeRF%2520for%2520Semantic%25203D%2520Reconstruction%2520of%2520Dynamic%250A%2520%2520Scene%26entry.906535625%3DSandika%2520Biswas%2520and%2520Qianyi%2520Wu%2520and%2520Biplab%2520Banerjee%2520and%2520Hamid%2520Rezatofighi%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520Neural%2520Implicit%2520models%2520for%25203D%2520surface%2520reconstruction%252C%250Ahandling%2520dynamic%2520environments%2520with%2520arbitrary%2520rigid%252C%2520non-rigid%252C%2520or%2520deformable%250Aentities%2520remains%2520challenging.%2520Many%2520template-based%2520methods%2520are%2520entity-specific%252C%250Afocusing%2520on%2520humans%252C%2520while%2520generic%2520reconstruction%2520methods%2520adaptable%2520to%2520such%250Adynamic%2520scenes%2520often%2520require%2520additional%2520inputs%2520like%2520depth%2520or%2520optical%2520flow%2520or%250Arely%2520on%2520pre-trained%2520image%2520features%2520for%2520reasonable%2520outcomes.%2520These%2520methods%250Atypically%2520use%2520latent%2520codes%2520to%2520capture%2520frame-by-frame%2520deformations.%2520In%2520contrast%252C%250Asome%2520template-free%2520methods%2520bypass%2520these%2520requirements%2520and%2520adopt%2520traditional%2520LBS%250A%2528Linear%2520Blend%2520Skinning%2529%2520weights%2520for%2520a%2520detailed%2520representation%2520of%2520deformable%250Aobject%2520motions%252C%2520although%2520they%2520involve%2520complex%2520optimizations%2520leading%2520to%2520lengthy%250Atraining%2520times.%2520To%2520this%2520end%252C%2520as%2520a%2520remedy%252C%2520this%2520paper%2520introduces%2520TFS-NeRF%252C%2520a%250Atemplate-free%25203D%2520semantic%2520NeRF%2520for%2520dynamic%2520scenes%2520captured%2520from%2520sparse%2520or%250Asingle-view%2520RGB%2520videos%252C%2520featuring%2520interactions%2520among%2520various%2520entities%2520and%2520more%250Atime-efficient%2520than%2520other%2520LBS-based%2520approaches.%2520Our%2520framework%2520uses%2520an%250AInvertible%2520Neural%2520Network%2520%2528INN%2529%2520for%2520LBS%2520prediction%252C%2520simplifying%2520the%2520training%250Aprocess.%2520By%2520disentangling%2520the%2520motions%2520of%2520multiple%2520entities%2520and%2520optimizing%250Aper-entity%2520skinning%2520weights%252C%2520our%2520method%2520efficiently%2520generates%2520accurate%252C%250Asemantically%2520separable%2520geometries.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520produces%2520high-quality%2520reconstructions%2520of%2520both%2520deformable%2520and%250Anon-deformable%2520objects%2520in%2520complex%2520interactions%252C%2520with%2520improved%2520training%250Aefficiency%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TFS-NeRF%3A%20Template-Free%20NeRF%20for%20Semantic%203D%20Reconstruction%20of%20Dynamic%0A%20%20Scene&entry.906535625=Sandika%20Biswas%20and%20Qianyi%20Wu%20and%20Biplab%20Banerjee%20and%20Hamid%20Rezatofighi&entry.1292438233=%20%20Despite%20advancements%20in%20Neural%20Implicit%20models%20for%203D%20surface%20reconstruction%2C%0Ahandling%20dynamic%20environments%20with%20arbitrary%20rigid%2C%20non-rigid%2C%20or%20deformable%0Aentities%20remains%20challenging.%20Many%20template-based%20methods%20are%20entity-specific%2C%0Afocusing%20on%20humans%2C%20while%20generic%20reconstruction%20methods%20adaptable%20to%20such%0Adynamic%20scenes%20often%20require%20additional%20inputs%20like%20depth%20or%20optical%20flow%20or%0Arely%20on%20pre-trained%20image%20features%20for%20reasonable%20outcomes.%20These%20methods%0Atypically%20use%20latent%20codes%20to%20capture%20frame-by-frame%20deformations.%20In%20contrast%2C%0Asome%20template-free%20methods%20bypass%20these%20requirements%20and%20adopt%20traditional%20LBS%0A%28Linear%20Blend%20Skinning%29%20weights%20for%20a%20detailed%20representation%20of%20deformable%0Aobject%20motions%2C%20although%20they%20involve%20complex%20optimizations%20leading%20to%20lengthy%0Atraining%20times.%20To%20this%20end%2C%20as%20a%20remedy%2C%20this%20paper%20introduces%20TFS-NeRF%2C%20a%0Atemplate-free%203D%20semantic%20NeRF%20for%20dynamic%20scenes%20captured%20from%20sparse%20or%0Asingle-view%20RGB%20videos%2C%20featuring%20interactions%20among%20various%20entities%20and%20more%0Atime-efficient%20than%20other%20LBS-based%20approaches.%20Our%20framework%20uses%20an%0AInvertible%20Neural%20Network%20%28INN%29%20for%20LBS%20prediction%2C%20simplifying%20the%20training%0Aprocess.%20By%20disentangling%20the%20motions%20of%20multiple%20entities%20and%20optimizing%0Aper-entity%20skinning%20weights%2C%20our%20method%20efficiently%20generates%20accurate%2C%0Asemantically%20separable%20geometries.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20produces%20high-quality%20reconstructions%20of%20both%20deformable%20and%0Anon-deformable%20objects%20in%20complex%20interactions%2C%20with%20improved%20training%0Aefficiency%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17459v2&entry.124074799=Read"},
{"title": "Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?", "author": "Daniel P. Jeong and Saurabh Garg and Zachary C. Lipton and Michael Oberst", "abstract": "  Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.\n", "link": "http://arxiv.org/abs/2411.04118v1", "date": "2024-11-06", "relevancy": 2.8834, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Medical%20Adaptation%20of%20Large%20Language%20and%20Vision-Language%20Models%3A%20Are%20We%0A%20%20Making%20Progress%3F&body=Title%3A%20Medical%20Adaptation%20of%20Large%20Language%20and%20Vision-Language%20Models%3A%20Are%20We%0A%20%20Making%20Progress%3F%0AAuthor%3A%20Daniel%20P.%20Jeong%20and%20Saurabh%20Garg%20and%20Zachary%20C.%20Lipton%20and%20Michael%20Oberst%0AAbstract%3A%20%20%20Several%20recent%20works%20seek%20to%20develop%20foundation%20models%20specifically%20for%0Amedical%20applications%2C%20adapting%20general-purpose%20large%20language%20models%20%28LLMs%29%20and%0Avision-language%20models%20%28VLMs%29%20via%20continued%20pretraining%20on%20publicly%20available%0Abiomedical%20corpora.%20These%20works%20typically%20claim%20that%20such%20domain-adaptive%0Apretraining%20%28DAPT%29%20improves%20performance%20on%20downstream%20medical%20tasks%2C%20such%20as%0Aanswering%20medical%20licensing%20exam%20questions.%20In%20this%20paper%2C%20we%20compare%20seven%0Apublic%20%22medical%22%20LLMs%20and%20two%20VLMs%20against%20their%20corresponding%20base%20models%2C%0Aarriving%20at%20a%20different%20conclusion%3A%20all%20medical%20VLMs%20and%20nearly%20all%20medical%0ALLMs%20fail%20to%20consistently%20improve%20over%20their%20base%20models%20in%20the%20zero-/few-shot%0Aprompting%20regime%20for%20medical%20question-answering%20%28QA%29%20tasks.%20For%20instance%2C%0Aacross%20the%20tasks%20and%20model%20pairs%20we%20consider%20in%20the%203-shot%20setting%2C%20medical%0ALLMs%20only%20outperform%20their%20base%20models%20in%2012.1%25%20of%20cases%2C%20reach%20a%20%28statistical%29%0Atie%20in%2049.8%25%20of%20cases%2C%20and%20are%20significantly%20worse%20than%20their%20base%20models%20in%0Athe%20remaining%2038.2%25%20of%20cases.%20Our%20conclusions%20are%20based%20on%20%28i%29%20comparing%20each%0Amedical%20model%20head-to-head%2C%20directly%20against%20the%20corresponding%20base%20model%3B%20%28ii%29%0Aoptimizing%20the%20prompts%20for%20each%20model%20separately%3B%20and%20%28iii%29%20accounting%20for%0Astatistical%20uncertainty%20in%20comparisons.%20While%20these%20basic%20practices%20are%20not%0Aconsistently%20adopted%20in%20the%20literature%2C%20our%20ablations%20show%20that%20they%0Asubstantially%20impact%20conclusions.%20Our%20findings%20suggest%20that%20state-of-the-art%0Ageneral-domain%20models%20may%20already%20exhibit%20strong%20medical%20knowledge%20and%0Areasoning%20capabilities%2C%20and%20offer%20recommendations%20to%20strengthen%20the%20conclusions%0Aof%20future%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedical%2520Adaptation%2520of%2520Large%2520Language%2520and%2520Vision-Language%2520Models%253A%2520Are%2520We%250A%2520%2520Making%2520Progress%253F%26entry.906535625%3DDaniel%2520P.%2520Jeong%2520and%2520Saurabh%2520Garg%2520and%2520Zachary%2520C.%2520Lipton%2520and%2520Michael%2520Oberst%26entry.1292438233%3D%2520%2520Several%2520recent%2520works%2520seek%2520to%2520develop%2520foundation%2520models%2520specifically%2520for%250Amedical%2520applications%252C%2520adapting%2520general-purpose%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Avision-language%2520models%2520%2528VLMs%2529%2520via%2520continued%2520pretraining%2520on%2520publicly%2520available%250Abiomedical%2520corpora.%2520These%2520works%2520typically%2520claim%2520that%2520such%2520domain-adaptive%250Apretraining%2520%2528DAPT%2529%2520improves%2520performance%2520on%2520downstream%2520medical%2520tasks%252C%2520such%2520as%250Aanswering%2520medical%2520licensing%2520exam%2520questions.%2520In%2520this%2520paper%252C%2520we%2520compare%2520seven%250Apublic%2520%2522medical%2522%2520LLMs%2520and%2520two%2520VLMs%2520against%2520their%2520corresponding%2520base%2520models%252C%250Aarriving%2520at%2520a%2520different%2520conclusion%253A%2520all%2520medical%2520VLMs%2520and%2520nearly%2520all%2520medical%250ALLMs%2520fail%2520to%2520consistently%2520improve%2520over%2520their%2520base%2520models%2520in%2520the%2520zero-/few-shot%250Aprompting%2520regime%2520for%2520medical%2520question-answering%2520%2528QA%2529%2520tasks.%2520For%2520instance%252C%250Aacross%2520the%2520tasks%2520and%2520model%2520pairs%2520we%2520consider%2520in%2520the%25203-shot%2520setting%252C%2520medical%250ALLMs%2520only%2520outperform%2520their%2520base%2520models%2520in%252012.1%2525%2520of%2520cases%252C%2520reach%2520a%2520%2528statistical%2529%250Atie%2520in%252049.8%2525%2520of%2520cases%252C%2520and%2520are%2520significantly%2520worse%2520than%2520their%2520base%2520models%2520in%250Athe%2520remaining%252038.2%2525%2520of%2520cases.%2520Our%2520conclusions%2520are%2520based%2520on%2520%2528i%2529%2520comparing%2520each%250Amedical%2520model%2520head-to-head%252C%2520directly%2520against%2520the%2520corresponding%2520base%2520model%253B%2520%2528ii%2529%250Aoptimizing%2520the%2520prompts%2520for%2520each%2520model%2520separately%253B%2520and%2520%2528iii%2529%2520accounting%2520for%250Astatistical%2520uncertainty%2520in%2520comparisons.%2520While%2520these%2520basic%2520practices%2520are%2520not%250Aconsistently%2520adopted%2520in%2520the%2520literature%252C%2520our%2520ablations%2520show%2520that%2520they%250Asubstantially%2520impact%2520conclusions.%2520Our%2520findings%2520suggest%2520that%2520state-of-the-art%250Ageneral-domain%2520models%2520may%2520already%2520exhibit%2520strong%2520medical%2520knowledge%2520and%250Areasoning%2520capabilities%252C%2520and%2520offer%2520recommendations%2520to%2520strengthen%2520the%2520conclusions%250Aof%2520future%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medical%20Adaptation%20of%20Large%20Language%20and%20Vision-Language%20Models%3A%20Are%20We%0A%20%20Making%20Progress%3F&entry.906535625=Daniel%20P.%20Jeong%20and%20Saurabh%20Garg%20and%20Zachary%20C.%20Lipton%20and%20Michael%20Oberst&entry.1292438233=%20%20Several%20recent%20works%20seek%20to%20develop%20foundation%20models%20specifically%20for%0Amedical%20applications%2C%20adapting%20general-purpose%20large%20language%20models%20%28LLMs%29%20and%0Avision-language%20models%20%28VLMs%29%20via%20continued%20pretraining%20on%20publicly%20available%0Abiomedical%20corpora.%20These%20works%20typically%20claim%20that%20such%20domain-adaptive%0Apretraining%20%28DAPT%29%20improves%20performance%20on%20downstream%20medical%20tasks%2C%20such%20as%0Aanswering%20medical%20licensing%20exam%20questions.%20In%20this%20paper%2C%20we%20compare%20seven%0Apublic%20%22medical%22%20LLMs%20and%20two%20VLMs%20against%20their%20corresponding%20base%20models%2C%0Aarriving%20at%20a%20different%20conclusion%3A%20all%20medical%20VLMs%20and%20nearly%20all%20medical%0ALLMs%20fail%20to%20consistently%20improve%20over%20their%20base%20models%20in%20the%20zero-/few-shot%0Aprompting%20regime%20for%20medical%20question-answering%20%28QA%29%20tasks.%20For%20instance%2C%0Aacross%20the%20tasks%20and%20model%20pairs%20we%20consider%20in%20the%203-shot%20setting%2C%20medical%0ALLMs%20only%20outperform%20their%20base%20models%20in%2012.1%25%20of%20cases%2C%20reach%20a%20%28statistical%29%0Atie%20in%2049.8%25%20of%20cases%2C%20and%20are%20significantly%20worse%20than%20their%20base%20models%20in%0Athe%20remaining%2038.2%25%20of%20cases.%20Our%20conclusions%20are%20based%20on%20%28i%29%20comparing%20each%0Amedical%20model%20head-to-head%2C%20directly%20against%20the%20corresponding%20base%20model%3B%20%28ii%29%0Aoptimizing%20the%20prompts%20for%20each%20model%20separately%3B%20and%20%28iii%29%20accounting%20for%0Astatistical%20uncertainty%20in%20comparisons.%20While%20these%20basic%20practices%20are%20not%0Aconsistently%20adopted%20in%20the%20literature%2C%20our%20ablations%20show%20that%20they%0Asubstantially%20impact%20conclusions.%20Our%20findings%20suggest%20that%20state-of-the-art%0Ageneral-domain%20models%20may%20already%20exhibit%20strong%20medical%20knowledge%20and%0Areasoning%20capabilities%2C%20and%20offer%20recommendations%20to%20strengthen%20the%20conclusions%0Aof%20future%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04118v1&entry.124074799=Read"},
{"title": "H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations\n  in Large Vision-Language Models", "author": "Nhi Pham and Michael Schott", "abstract": "  By leveraging both texts and images, large vision language models (LVLMs)\nhave shown significant progress in various multi-modal tasks. Nevertheless,\nthese models often suffer from hallucinations, e.g., they exhibit\ninconsistencies between the visual input and the textual output. To address\nthis, we propose H-POPE, a coarse-to-fine-grained benchmark that systematically\nassesses hallucination in object existence and attributes. Our evaluation shows\nthat models are prone to hallucinations on object existence, and even more so\non fine-grained attributes. We further investigate whether these models rely on\nvisual input to formulate the output texts.\n", "link": "http://arxiv.org/abs/2411.04077v1", "date": "2024-11-06", "relevancy": 2.7967, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5819}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H-POPE%3A%20Hierarchical%20Polling-based%20Probing%20Evaluation%20of%20Hallucinations%0A%20%20in%20Large%20Vision-Language%20Models&body=Title%3A%20H-POPE%3A%20Hierarchical%20Polling-based%20Probing%20Evaluation%20of%20Hallucinations%0A%20%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Nhi%20Pham%20and%20Michael%20Schott%0AAbstract%3A%20%20%20By%20leveraging%20both%20texts%20and%20images%2C%20large%20vision%20language%20models%20%28LVLMs%29%0Ahave%20shown%20significant%20progress%20in%20various%20multi-modal%20tasks.%20Nevertheless%2C%0Athese%20models%20often%20suffer%20from%20hallucinations%2C%20e.g.%2C%20they%20exhibit%0Ainconsistencies%20between%20the%20visual%20input%20and%20the%20textual%20output.%20To%20address%0Athis%2C%20we%20propose%20H-POPE%2C%20a%20coarse-to-fine-grained%20benchmark%20that%20systematically%0Aassesses%20hallucination%20in%20object%20existence%20and%20attributes.%20Our%20evaluation%20shows%0Athat%20models%20are%20prone%20to%20hallucinations%20on%20object%20existence%2C%20and%20even%20more%20so%0Aon%20fine-grained%20attributes.%20We%20further%20investigate%20whether%20these%20models%20rely%20on%0Avisual%20input%20to%20formulate%20the%20output%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH-POPE%253A%2520Hierarchical%2520Polling-based%2520Probing%2520Evaluation%2520of%2520Hallucinations%250A%2520%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DNhi%2520Pham%2520and%2520Michael%2520Schott%26entry.1292438233%3D%2520%2520By%2520leveraging%2520both%2520texts%2520and%2520images%252C%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%250Ahave%2520shown%2520significant%2520progress%2520in%2520various%2520multi-modal%2520tasks.%2520Nevertheless%252C%250Athese%2520models%2520often%2520suffer%2520from%2520hallucinations%252C%2520e.g.%252C%2520they%2520exhibit%250Ainconsistencies%2520between%2520the%2520visual%2520input%2520and%2520the%2520textual%2520output.%2520To%2520address%250Athis%252C%2520we%2520propose%2520H-POPE%252C%2520a%2520coarse-to-fine-grained%2520benchmark%2520that%2520systematically%250Aassesses%2520hallucination%2520in%2520object%2520existence%2520and%2520attributes.%2520Our%2520evaluation%2520shows%250Athat%2520models%2520are%2520prone%2520to%2520hallucinations%2520on%2520object%2520existence%252C%2520and%2520even%2520more%2520so%250Aon%2520fine-grained%2520attributes.%2520We%2520further%2520investigate%2520whether%2520these%2520models%2520rely%2520on%250Avisual%2520input%2520to%2520formulate%2520the%2520output%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H-POPE%3A%20Hierarchical%20Polling-based%20Probing%20Evaluation%20of%20Hallucinations%0A%20%20in%20Large%20Vision-Language%20Models&entry.906535625=Nhi%20Pham%20and%20Michael%20Schott&entry.1292438233=%20%20By%20leveraging%20both%20texts%20and%20images%2C%20large%20vision%20language%20models%20%28LVLMs%29%0Ahave%20shown%20significant%20progress%20in%20various%20multi-modal%20tasks.%20Nevertheless%2C%0Athese%20models%20often%20suffer%20from%20hallucinations%2C%20e.g.%2C%20they%20exhibit%0Ainconsistencies%20between%20the%20visual%20input%20and%20the%20textual%20output.%20To%20address%0Athis%2C%20we%20propose%20H-POPE%2C%20a%20coarse-to-fine-grained%20benchmark%20that%20systematically%0Aassesses%20hallucination%20in%20object%20existence%20and%20attributes.%20Our%20evaluation%20shows%0Athat%20models%20are%20prone%20to%20hallucinations%20on%20object%20existence%2C%20and%20even%20more%20so%0Aon%20fine-grained%20attributes.%20We%20further%20investigate%20whether%20these%20models%20rely%20on%0Avisual%20input%20to%20formulate%20the%20output%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04077v1&entry.124074799=Read"},
{"title": "CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale", "author": "ZeMing Gong and Austin T. Wang and Xiaoliang Huo and Joakim Bruslund Haurum and Scott C. Lowe and Graham W. Taylor and Angel X. Chang", "abstract": "  Measuring biodiversity is crucial for understanding ecosystem health. While\nprior works have developed machine learning models for taxonomic classification\nof photographic images and DNA separately, in this work, we introduce a\nmultimodal approach combining both, using CLIP-style contrastive learning to\nalign images, barcode DNA, and text-based representations of taxonomic labels\nin a unified embedding space. This allows for accurate classification of both\nknown and unknown insect species without task-specific fine-tuning, leveraging\ncontrastive learning for the first time to fuse DNA and image data. Our method\nsurpasses previous single-modality approaches in accuracy by over 8% on\nzero-shot learning tasks, showcasing its effectiveness in biodiversity studies.\n", "link": "http://arxiv.org/abs/2405.17537v3", "date": "2024-11-06", "relevancy": 2.7615, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5388}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIBD%3A%20Bridging%20Vision%20and%20Genomics%20for%20Biodiversity%20Monitoring%20at%20Scale&body=Title%3A%20CLIBD%3A%20Bridging%20Vision%20and%20Genomics%20for%20Biodiversity%20Monitoring%20at%20Scale%0AAuthor%3A%20ZeMing%20Gong%20and%20Austin%20T.%20Wang%20and%20Xiaoliang%20Huo%20and%20Joakim%20Bruslund%20Haurum%20and%20Scott%20C.%20Lowe%20and%20Graham%20W.%20Taylor%20and%20Angel%20X.%20Chang%0AAbstract%3A%20%20%20Measuring%20biodiversity%20is%20crucial%20for%20understanding%20ecosystem%20health.%20While%0Aprior%20works%20have%20developed%20machine%20learning%20models%20for%20taxonomic%20classification%0Aof%20photographic%20images%20and%20DNA%20separately%2C%20in%20this%20work%2C%20we%20introduce%20a%0Amultimodal%20approach%20combining%20both%2C%20using%20CLIP-style%20contrastive%20learning%20to%0Aalign%20images%2C%20barcode%20DNA%2C%20and%20text-based%20representations%20of%20taxonomic%20labels%0Ain%20a%20unified%20embedding%20space.%20This%20allows%20for%20accurate%20classification%20of%20both%0Aknown%20and%20unknown%20insect%20species%20without%20task-specific%20fine-tuning%2C%20leveraging%0Acontrastive%20learning%20for%20the%20first%20time%20to%20fuse%20DNA%20and%20image%20data.%20Our%20method%0Asurpasses%20previous%20single-modality%20approaches%20in%20accuracy%20by%20over%208%25%20on%0Azero-shot%20learning%20tasks%2C%20showcasing%20its%20effectiveness%20in%20biodiversity%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17537v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIBD%253A%2520Bridging%2520Vision%2520and%2520Genomics%2520for%2520Biodiversity%2520Monitoring%2520at%2520Scale%26entry.906535625%3DZeMing%2520Gong%2520and%2520Austin%2520T.%2520Wang%2520and%2520Xiaoliang%2520Huo%2520and%2520Joakim%2520Bruslund%2520Haurum%2520and%2520Scott%2520C.%2520Lowe%2520and%2520Graham%2520W.%2520Taylor%2520and%2520Angel%2520X.%2520Chang%26entry.1292438233%3D%2520%2520Measuring%2520biodiversity%2520is%2520crucial%2520for%2520understanding%2520ecosystem%2520health.%2520While%250Aprior%2520works%2520have%2520developed%2520machine%2520learning%2520models%2520for%2520taxonomic%2520classification%250Aof%2520photographic%2520images%2520and%2520DNA%2520separately%252C%2520in%2520this%2520work%252C%2520we%2520introduce%2520a%250Amultimodal%2520approach%2520combining%2520both%252C%2520using%2520CLIP-style%2520contrastive%2520learning%2520to%250Aalign%2520images%252C%2520barcode%2520DNA%252C%2520and%2520text-based%2520representations%2520of%2520taxonomic%2520labels%250Ain%2520a%2520unified%2520embedding%2520space.%2520This%2520allows%2520for%2520accurate%2520classification%2520of%2520both%250Aknown%2520and%2520unknown%2520insect%2520species%2520without%2520task-specific%2520fine-tuning%252C%2520leveraging%250Acontrastive%2520learning%2520for%2520the%2520first%2520time%2520to%2520fuse%2520DNA%2520and%2520image%2520data.%2520Our%2520method%250Asurpasses%2520previous%2520single-modality%2520approaches%2520in%2520accuracy%2520by%2520over%25208%2525%2520on%250Azero-shot%2520learning%2520tasks%252C%2520showcasing%2520its%2520effectiveness%2520in%2520biodiversity%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17537v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIBD%3A%20Bridging%20Vision%20and%20Genomics%20for%20Biodiversity%20Monitoring%20at%20Scale&entry.906535625=ZeMing%20Gong%20and%20Austin%20T.%20Wang%20and%20Xiaoliang%20Huo%20and%20Joakim%20Bruslund%20Haurum%20and%20Scott%20C.%20Lowe%20and%20Graham%20W.%20Taylor%20and%20Angel%20X.%20Chang&entry.1292438233=%20%20Measuring%20biodiversity%20is%20crucial%20for%20understanding%20ecosystem%20health.%20While%0Aprior%20works%20have%20developed%20machine%20learning%20models%20for%20taxonomic%20classification%0Aof%20photographic%20images%20and%20DNA%20separately%2C%20in%20this%20work%2C%20we%20introduce%20a%0Amultimodal%20approach%20combining%20both%2C%20using%20CLIP-style%20contrastive%20learning%20to%0Aalign%20images%2C%20barcode%20DNA%2C%20and%20text-based%20representations%20of%20taxonomic%20labels%0Ain%20a%20unified%20embedding%20space.%20This%20allows%20for%20accurate%20classification%20of%20both%0Aknown%20and%20unknown%20insect%20species%20without%20task-specific%20fine-tuning%2C%20leveraging%0Acontrastive%20learning%20for%20the%20first%20time%20to%20fuse%20DNA%20and%20image%20data.%20Our%20method%0Asurpasses%20previous%20single-modality%20approaches%20in%20accuracy%20by%20over%208%25%20on%0Azero-shot%20learning%20tasks%2C%20showcasing%20its%20effectiveness%20in%20biodiversity%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17537v3&entry.124074799=Read"},
{"title": "Local vs distributed representations: What is the right basis for\n  interpretability?", "author": "Julien Colin and Lore Goetschalckx and Thomas Fel and Victor Boutin and Jay Gopal and Thomas Serre and Nuria Oliver", "abstract": "  Much of the research on the interpretability of deep neural networks has\nfocused on studying the visual features that maximally activate individual\nneurons. However, recent work has cast doubts on the usefulness of such local\nrepresentations for understanding the behavior of deep neural networks because\nindividual neurons tend to respond to multiple unrelated visual patterns, a\nphenomenon referred to as \"superposition\". A promising alternative to\ndisentangle these complex patterns is learning sparsely distributed vector\nrepresentations from entire network layers, as the resulting basis vectors\nseemingly encode single identifiable visual patterns consistently. Thus, one\nwould expect the resulting code to align better with human perceivable visual\npatterns, but supporting evidence remains, at best, anecdotal. To fill this\ngap, we conducted three large-scale psychophysics experiments collected from a\npool of 560 participants. Our findings provide (i) strong evidence that\nfeatures obtained from sparse distributed representations are easier to\ninterpret by human observers and (ii) that this effect is more pronounced in\nthe deepest layers of a neural network. Complementary analyses also reveal that\n(iii) features derived from sparse distributed representations contribute more\nto the model's decision. Overall, our results highlight that distributed\nrepresentations constitute a superior basis for interpretability, underscoring\na need for the field to move beyond the interpretation of local neural codes in\nfavor of sparsely distributed ones.\n", "link": "http://arxiv.org/abs/2411.03993v1", "date": "2024-11-06", "relevancy": 2.7528, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20vs%20distributed%20representations%3A%20What%20is%20the%20right%20basis%20for%0A%20%20interpretability%3F&body=Title%3A%20Local%20vs%20distributed%20representations%3A%20What%20is%20the%20right%20basis%20for%0A%20%20interpretability%3F%0AAuthor%3A%20Julien%20Colin%20and%20Lore%20Goetschalckx%20and%20Thomas%20Fel%20and%20Victor%20Boutin%20and%20Jay%20Gopal%20and%20Thomas%20Serre%20and%20Nuria%20Oliver%0AAbstract%3A%20%20%20Much%20of%20the%20research%20on%20the%20interpretability%20of%20deep%20neural%20networks%20has%0Afocused%20on%20studying%20the%20visual%20features%20that%20maximally%20activate%20individual%0Aneurons.%20However%2C%20recent%20work%20has%20cast%20doubts%20on%20the%20usefulness%20of%20such%20local%0Arepresentations%20for%20understanding%20the%20behavior%20of%20deep%20neural%20networks%20because%0Aindividual%20neurons%20tend%20to%20respond%20to%20multiple%20unrelated%20visual%20patterns%2C%20a%0Aphenomenon%20referred%20to%20as%20%22superposition%22.%20A%20promising%20alternative%20to%0Adisentangle%20these%20complex%20patterns%20is%20learning%20sparsely%20distributed%20vector%0Arepresentations%20from%20entire%20network%20layers%2C%20as%20the%20resulting%20basis%20vectors%0Aseemingly%20encode%20single%20identifiable%20visual%20patterns%20consistently.%20Thus%2C%20one%0Awould%20expect%20the%20resulting%20code%20to%20align%20better%20with%20human%20perceivable%20visual%0Apatterns%2C%20but%20supporting%20evidence%20remains%2C%20at%20best%2C%20anecdotal.%20To%20fill%20this%0Agap%2C%20we%20conducted%20three%20large-scale%20psychophysics%20experiments%20collected%20from%20a%0Apool%20of%20560%20participants.%20Our%20findings%20provide%20%28i%29%20strong%20evidence%20that%0Afeatures%20obtained%20from%20sparse%20distributed%20representations%20are%20easier%20to%0Ainterpret%20by%20human%20observers%20and%20%28ii%29%20that%20this%20effect%20is%20more%20pronounced%20in%0Athe%20deepest%20layers%20of%20a%20neural%20network.%20Complementary%20analyses%20also%20reveal%20that%0A%28iii%29%20features%20derived%20from%20sparse%20distributed%20representations%20contribute%20more%0Ato%20the%20model%27s%20decision.%20Overall%2C%20our%20results%20highlight%20that%20distributed%0Arepresentations%20constitute%20a%20superior%20basis%20for%20interpretability%2C%20underscoring%0Aa%20need%20for%20the%20field%20to%20move%20beyond%20the%20interpretation%20of%20local%20neural%20codes%20in%0Afavor%20of%20sparsely%20distributed%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520vs%2520distributed%2520representations%253A%2520What%2520is%2520the%2520right%2520basis%2520for%250A%2520%2520interpretability%253F%26entry.906535625%3DJulien%2520Colin%2520and%2520Lore%2520Goetschalckx%2520and%2520Thomas%2520Fel%2520and%2520Victor%2520Boutin%2520and%2520Jay%2520Gopal%2520and%2520Thomas%2520Serre%2520and%2520Nuria%2520Oliver%26entry.1292438233%3D%2520%2520Much%2520of%2520the%2520research%2520on%2520the%2520interpretability%2520of%2520deep%2520neural%2520networks%2520has%250Afocused%2520on%2520studying%2520the%2520visual%2520features%2520that%2520maximally%2520activate%2520individual%250Aneurons.%2520However%252C%2520recent%2520work%2520has%2520cast%2520doubts%2520on%2520the%2520usefulness%2520of%2520such%2520local%250Arepresentations%2520for%2520understanding%2520the%2520behavior%2520of%2520deep%2520neural%2520networks%2520because%250Aindividual%2520neurons%2520tend%2520to%2520respond%2520to%2520multiple%2520unrelated%2520visual%2520patterns%252C%2520a%250Aphenomenon%2520referred%2520to%2520as%2520%2522superposition%2522.%2520A%2520promising%2520alternative%2520to%250Adisentangle%2520these%2520complex%2520patterns%2520is%2520learning%2520sparsely%2520distributed%2520vector%250Arepresentations%2520from%2520entire%2520network%2520layers%252C%2520as%2520the%2520resulting%2520basis%2520vectors%250Aseemingly%2520encode%2520single%2520identifiable%2520visual%2520patterns%2520consistently.%2520Thus%252C%2520one%250Awould%2520expect%2520the%2520resulting%2520code%2520to%2520align%2520better%2520with%2520human%2520perceivable%2520visual%250Apatterns%252C%2520but%2520supporting%2520evidence%2520remains%252C%2520at%2520best%252C%2520anecdotal.%2520To%2520fill%2520this%250Agap%252C%2520we%2520conducted%2520three%2520large-scale%2520psychophysics%2520experiments%2520collected%2520from%2520a%250Apool%2520of%2520560%2520participants.%2520Our%2520findings%2520provide%2520%2528i%2529%2520strong%2520evidence%2520that%250Afeatures%2520obtained%2520from%2520sparse%2520distributed%2520representations%2520are%2520easier%2520to%250Ainterpret%2520by%2520human%2520observers%2520and%2520%2528ii%2529%2520that%2520this%2520effect%2520is%2520more%2520pronounced%2520in%250Athe%2520deepest%2520layers%2520of%2520a%2520neural%2520network.%2520Complementary%2520analyses%2520also%2520reveal%2520that%250A%2528iii%2529%2520features%2520derived%2520from%2520sparse%2520distributed%2520representations%2520contribute%2520more%250Ato%2520the%2520model%2527s%2520decision.%2520Overall%252C%2520our%2520results%2520highlight%2520that%2520distributed%250Arepresentations%2520constitute%2520a%2520superior%2520basis%2520for%2520interpretability%252C%2520underscoring%250Aa%2520need%2520for%2520the%2520field%2520to%2520move%2520beyond%2520the%2520interpretation%2520of%2520local%2520neural%2520codes%2520in%250Afavor%2520of%2520sparsely%2520distributed%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20vs%20distributed%20representations%3A%20What%20is%20the%20right%20basis%20for%0A%20%20interpretability%3F&entry.906535625=Julien%20Colin%20and%20Lore%20Goetschalckx%20and%20Thomas%20Fel%20and%20Victor%20Boutin%20and%20Jay%20Gopal%20and%20Thomas%20Serre%20and%20Nuria%20Oliver&entry.1292438233=%20%20Much%20of%20the%20research%20on%20the%20interpretability%20of%20deep%20neural%20networks%20has%0Afocused%20on%20studying%20the%20visual%20features%20that%20maximally%20activate%20individual%0Aneurons.%20However%2C%20recent%20work%20has%20cast%20doubts%20on%20the%20usefulness%20of%20such%20local%0Arepresentations%20for%20understanding%20the%20behavior%20of%20deep%20neural%20networks%20because%0Aindividual%20neurons%20tend%20to%20respond%20to%20multiple%20unrelated%20visual%20patterns%2C%20a%0Aphenomenon%20referred%20to%20as%20%22superposition%22.%20A%20promising%20alternative%20to%0Adisentangle%20these%20complex%20patterns%20is%20learning%20sparsely%20distributed%20vector%0Arepresentations%20from%20entire%20network%20layers%2C%20as%20the%20resulting%20basis%20vectors%0Aseemingly%20encode%20single%20identifiable%20visual%20patterns%20consistently.%20Thus%2C%20one%0Awould%20expect%20the%20resulting%20code%20to%20align%20better%20with%20human%20perceivable%20visual%0Apatterns%2C%20but%20supporting%20evidence%20remains%2C%20at%20best%2C%20anecdotal.%20To%20fill%20this%0Agap%2C%20we%20conducted%20three%20large-scale%20psychophysics%20experiments%20collected%20from%20a%0Apool%20of%20560%20participants.%20Our%20findings%20provide%20%28i%29%20strong%20evidence%20that%0Afeatures%20obtained%20from%20sparse%20distributed%20representations%20are%20easier%20to%0Ainterpret%20by%20human%20observers%20and%20%28ii%29%20that%20this%20effect%20is%20more%20pronounced%20in%0Athe%20deepest%20layers%20of%20a%20neural%20network.%20Complementary%20analyses%20also%20reveal%20that%0A%28iii%29%20features%20derived%20from%20sparse%20distributed%20representations%20contribute%20more%0Ato%20the%20model%27s%20decision.%20Overall%2C%20our%20results%20highlight%20that%20distributed%0Arepresentations%20constitute%20a%20superior%20basis%20for%20interpretability%2C%20underscoring%0Aa%20need%20for%20the%20field%20to%20move%20beyond%20the%20interpretation%20of%20local%20neural%20codes%20in%0Afavor%20of%20sparsely%20distributed%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03993v1&entry.124074799=Read"},
{"title": "Self-supervised Representation Learning for Cell Event Recognition\n  through Time Arrow Prediction", "author": "Cangxiong Chen and Vinay P. Namboodiri and Julia E. Sero", "abstract": "  The spatio-temporal nature of live-cell microscopy data poses challenges in\nthe analysis of cell states which is fundamental in bioimaging. Deep-learning\nbased segmentation or tracking methods rely on large amount of high quality\nannotations to work effectively. In this work, we explore an alternative\nsolution: using feature maps obtained from self-supervised representation\nlearning (SSRL) on time arrow prediction (TAP) for the downstream supervised\ntask of cell event recognition. We demonstrate through extensive experiments\nand analysis that this approach can achieve better performance with limited\nannotation compared to models trained from end to end using fully supervised\napproach. Our analysis also provides insight into applications of the SSRL\nusing TAP in live-cell microscopy.\n", "link": "http://arxiv.org/abs/2411.03924v1", "date": "2024-11-06", "relevancy": 2.6907, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5691}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5247}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Representation%20Learning%20for%20Cell%20Event%20Recognition%0A%20%20through%20Time%20Arrow%20Prediction&body=Title%3A%20Self-supervised%20Representation%20Learning%20for%20Cell%20Event%20Recognition%0A%20%20through%20Time%20Arrow%20Prediction%0AAuthor%3A%20Cangxiong%20Chen%20and%20Vinay%20P.%20Namboodiri%20and%20Julia%20E.%20Sero%0AAbstract%3A%20%20%20The%20spatio-temporal%20nature%20of%20live-cell%20microscopy%20data%20poses%20challenges%20in%0Athe%20analysis%20of%20cell%20states%20which%20is%20fundamental%20in%20bioimaging.%20Deep-learning%0Abased%20segmentation%20or%20tracking%20methods%20rely%20on%20large%20amount%20of%20high%20quality%0Aannotations%20to%20work%20effectively.%20In%20this%20work%2C%20we%20explore%20an%20alternative%0Asolution%3A%20using%20feature%20maps%20obtained%20from%20self-supervised%20representation%0Alearning%20%28SSRL%29%20on%20time%20arrow%20prediction%20%28TAP%29%20for%20the%20downstream%20supervised%0Atask%20of%20cell%20event%20recognition.%20We%20demonstrate%20through%20extensive%20experiments%0Aand%20analysis%20that%20this%20approach%20can%20achieve%20better%20performance%20with%20limited%0Aannotation%20compared%20to%20models%20trained%20from%20end%20to%20end%20using%20fully%20supervised%0Aapproach.%20Our%20analysis%20also%20provides%20insight%20into%20applications%20of%20the%20SSRL%0Ausing%20TAP%20in%20live-cell%20microscopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Representation%2520Learning%2520for%2520Cell%2520Event%2520Recognition%250A%2520%2520through%2520Time%2520Arrow%2520Prediction%26entry.906535625%3DCangxiong%2520Chen%2520and%2520Vinay%2520P.%2520Namboodiri%2520and%2520Julia%2520E.%2520Sero%26entry.1292438233%3D%2520%2520The%2520spatio-temporal%2520nature%2520of%2520live-cell%2520microscopy%2520data%2520poses%2520challenges%2520in%250Athe%2520analysis%2520of%2520cell%2520states%2520which%2520is%2520fundamental%2520in%2520bioimaging.%2520Deep-learning%250Abased%2520segmentation%2520or%2520tracking%2520methods%2520rely%2520on%2520large%2520amount%2520of%2520high%2520quality%250Aannotations%2520to%2520work%2520effectively.%2520In%2520this%2520work%252C%2520we%2520explore%2520an%2520alternative%250Asolution%253A%2520using%2520feature%2520maps%2520obtained%2520from%2520self-supervised%2520representation%250Alearning%2520%2528SSRL%2529%2520on%2520time%2520arrow%2520prediction%2520%2528TAP%2529%2520for%2520the%2520downstream%2520supervised%250Atask%2520of%2520cell%2520event%2520recognition.%2520We%2520demonstrate%2520through%2520extensive%2520experiments%250Aand%2520analysis%2520that%2520this%2520approach%2520can%2520achieve%2520better%2520performance%2520with%2520limited%250Aannotation%2520compared%2520to%2520models%2520trained%2520from%2520end%2520to%2520end%2520using%2520fully%2520supervised%250Aapproach.%2520Our%2520analysis%2520also%2520provides%2520insight%2520into%2520applications%2520of%2520the%2520SSRL%250Ausing%2520TAP%2520in%2520live-cell%2520microscopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Representation%20Learning%20for%20Cell%20Event%20Recognition%0A%20%20through%20Time%20Arrow%20Prediction&entry.906535625=Cangxiong%20Chen%20and%20Vinay%20P.%20Namboodiri%20and%20Julia%20E.%20Sero&entry.1292438233=%20%20The%20spatio-temporal%20nature%20of%20live-cell%20microscopy%20data%20poses%20challenges%20in%0Athe%20analysis%20of%20cell%20states%20which%20is%20fundamental%20in%20bioimaging.%20Deep-learning%0Abased%20segmentation%20or%20tracking%20methods%20rely%20on%20large%20amount%20of%20high%20quality%0Aannotations%20to%20work%20effectively.%20In%20this%20work%2C%20we%20explore%20an%20alternative%0Asolution%3A%20using%20feature%20maps%20obtained%20from%20self-supervised%20representation%0Alearning%20%28SSRL%29%20on%20time%20arrow%20prediction%20%28TAP%29%20for%20the%20downstream%20supervised%0Atask%20of%20cell%20event%20recognition.%20We%20demonstrate%20through%20extensive%20experiments%0Aand%20analysis%20that%20this%20approach%20can%20achieve%20better%20performance%20with%20limited%0Aannotation%20compared%20to%20models%20trained%20from%20end%20to%20end%20using%20fully%20supervised%0Aapproach.%20Our%20analysis%20also%20provides%20insight%20into%20applications%20of%20the%20SSRL%0Ausing%20TAP%20in%20live-cell%20microscopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03924v1&entry.124074799=Read"},
{"title": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward", "author": "Shashi Kumar and Iuliia Thorbecke and Sergio Burdisso and Esa\u00fa Villatoro-Tello and Manjunath K E and Kadri Hacio\u011flu and Pradeep Rangappa and Petr Motlicek and Aravind Ganapathiraju and Andreas Stolcke", "abstract": "  Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\ndifferent speech perturbations. In this paper, we address these questions by\nconducting various ablation experiments using a recent and widely adopted\napproach called SLAM-ASR. We present novel empirical findings that offer\ninsights on how to effectively utilize the SLAM-ASR architecture across a wide\nrange of settings. Our main findings indicate that the SLAM-ASR exhibits poor\nperformance in cross-domain evaluation settings. Additionally, speech\nperturbations within in-domain data, such as changes in speed or the presence\nof additive noise, can significantly impact performance. Our findings offer\ncritical insights for fine-tuning and configuring robust LLM-based ASR models,\ntailored to different data characteristics and computational resources.\n", "link": "http://arxiv.org/abs/2411.03866v1", "date": "2024-11-06", "relevancy": 2.6804, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20evaluation%20of%20SLAM-ASR%3A%20The%20Good%2C%20the%20Bad%2C%20the%20Ugly%2C%20and%20the%0A%20%20Way%20Forward&body=Title%3A%20Performance%20evaluation%20of%20SLAM-ASR%3A%20The%20Good%2C%20the%20Bad%2C%20the%20Ugly%2C%20and%20the%0A%20%20Way%20Forward%0AAuthor%3A%20Shashi%20Kumar%20and%20Iuliia%20Thorbecke%20and%20Sergio%20Burdisso%20and%20Esa%C3%BA%20Villatoro-Tello%20and%20Manjunath%20K%20E%20and%20Kadri%20Hacio%C4%9Flu%20and%20Pradeep%20Rangappa%20and%20Petr%20Motlicek%20and%20Aravind%20Ganapathiraju%20and%20Andreas%20Stolcke%0AAbstract%3A%20%20%20Recent%20research%20has%20demonstrated%20that%20training%20a%20linear%20connector%20between%0Aspeech%20foundation%20encoders%20and%20large%20language%20models%20%28LLMs%29%20enables%20this%0Aarchitecture%20to%20achieve%20strong%20ASR%20capabilities.%20Despite%20the%20impressive%0Aresults%2C%20it%20remains%20unclear%20whether%20these%20simple%20approaches%20are%20robust%20enough%0Aacross%20different%20scenarios%20and%20speech%20conditions%2C%20such%20as%20domain%20shifts%20and%0Adifferent%20speech%20perturbations.%20In%20this%20paper%2C%20we%20address%20these%20questions%20by%0Aconducting%20various%20ablation%20experiments%20using%20a%20recent%20and%20widely%20adopted%0Aapproach%20called%20SLAM-ASR.%20We%20present%20novel%20empirical%20findings%20that%20offer%0Ainsights%20on%20how%20to%20effectively%20utilize%20the%20SLAM-ASR%20architecture%20across%20a%20wide%0Arange%20of%20settings.%20Our%20main%20findings%20indicate%20that%20the%20SLAM-ASR%20exhibits%20poor%0Aperformance%20in%20cross-domain%20evaluation%20settings.%20Additionally%2C%20speech%0Aperturbations%20within%20in-domain%20data%2C%20such%20as%20changes%20in%20speed%20or%20the%20presence%0Aof%20additive%20noise%2C%20can%20significantly%20impact%20performance.%20Our%20findings%20offer%0Acritical%20insights%20for%20fine-tuning%20and%20configuring%20robust%20LLM-based%20ASR%20models%2C%0Atailored%20to%20different%20data%20characteristics%20and%20computational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520evaluation%2520of%2520SLAM-ASR%253A%2520The%2520Good%252C%2520the%2520Bad%252C%2520the%2520Ugly%252C%2520and%2520the%250A%2520%2520Way%2520Forward%26entry.906535625%3DShashi%2520Kumar%2520and%2520Iuliia%2520Thorbecke%2520and%2520Sergio%2520Burdisso%2520and%2520Esa%25C3%25BA%2520Villatoro-Tello%2520and%2520Manjunath%2520K%2520E%2520and%2520Kadri%2520Hacio%25C4%259Flu%2520and%2520Pradeep%2520Rangappa%2520and%2520Petr%2520Motlicek%2520and%2520Aravind%2520Ganapathiraju%2520and%2520Andreas%2520Stolcke%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520demonstrated%2520that%2520training%2520a%2520linear%2520connector%2520between%250Aspeech%2520foundation%2520encoders%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520enables%2520this%250Aarchitecture%2520to%2520achieve%2520strong%2520ASR%2520capabilities.%2520Despite%2520the%2520impressive%250Aresults%252C%2520it%2520remains%2520unclear%2520whether%2520these%2520simple%2520approaches%2520are%2520robust%2520enough%250Aacross%2520different%2520scenarios%2520and%2520speech%2520conditions%252C%2520such%2520as%2520domain%2520shifts%2520and%250Adifferent%2520speech%2520perturbations.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520questions%2520by%250Aconducting%2520various%2520ablation%2520experiments%2520using%2520a%2520recent%2520and%2520widely%2520adopted%250Aapproach%2520called%2520SLAM-ASR.%2520We%2520present%2520novel%2520empirical%2520findings%2520that%2520offer%250Ainsights%2520on%2520how%2520to%2520effectively%2520utilize%2520the%2520SLAM-ASR%2520architecture%2520across%2520a%2520wide%250Arange%2520of%2520settings.%2520Our%2520main%2520findings%2520indicate%2520that%2520the%2520SLAM-ASR%2520exhibits%2520poor%250Aperformance%2520in%2520cross-domain%2520evaluation%2520settings.%2520Additionally%252C%2520speech%250Aperturbations%2520within%2520in-domain%2520data%252C%2520such%2520as%2520changes%2520in%2520speed%2520or%2520the%2520presence%250Aof%2520additive%2520noise%252C%2520can%2520significantly%2520impact%2520performance.%2520Our%2520findings%2520offer%250Acritical%2520insights%2520for%2520fine-tuning%2520and%2520configuring%2520robust%2520LLM-based%2520ASR%2520models%252C%250Atailored%2520to%2520different%2520data%2520characteristics%2520and%2520computational%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20evaluation%20of%20SLAM-ASR%3A%20The%20Good%2C%20the%20Bad%2C%20the%20Ugly%2C%20and%20the%0A%20%20Way%20Forward&entry.906535625=Shashi%20Kumar%20and%20Iuliia%20Thorbecke%20and%20Sergio%20Burdisso%20and%20Esa%C3%BA%20Villatoro-Tello%20and%20Manjunath%20K%20E%20and%20Kadri%20Hacio%C4%9Flu%20and%20Pradeep%20Rangappa%20and%20Petr%20Motlicek%20and%20Aravind%20Ganapathiraju%20and%20Andreas%20Stolcke&entry.1292438233=%20%20Recent%20research%20has%20demonstrated%20that%20training%20a%20linear%20connector%20between%0Aspeech%20foundation%20encoders%20and%20large%20language%20models%20%28LLMs%29%20enables%20this%0Aarchitecture%20to%20achieve%20strong%20ASR%20capabilities.%20Despite%20the%20impressive%0Aresults%2C%20it%20remains%20unclear%20whether%20these%20simple%20approaches%20are%20robust%20enough%0Aacross%20different%20scenarios%20and%20speech%20conditions%2C%20such%20as%20domain%20shifts%20and%0Adifferent%20speech%20perturbations.%20In%20this%20paper%2C%20we%20address%20these%20questions%20by%0Aconducting%20various%20ablation%20experiments%20using%20a%20recent%20and%20widely%20adopted%0Aapproach%20called%20SLAM-ASR.%20We%20present%20novel%20empirical%20findings%20that%20offer%0Ainsights%20on%20how%20to%20effectively%20utilize%20the%20SLAM-ASR%20architecture%20across%20a%20wide%0Arange%20of%20settings.%20Our%20main%20findings%20indicate%20that%20the%20SLAM-ASR%20exhibits%20poor%0Aperformance%20in%20cross-domain%20evaluation%20settings.%20Additionally%2C%20speech%0Aperturbations%20within%20in-domain%20data%2C%20such%20as%20changes%20in%20speed%20or%20the%20presence%0Aof%20additive%20noise%2C%20can%20significantly%20impact%20performance.%20Our%20findings%20offer%0Acritical%20insights%20for%20fine-tuning%20and%20configuring%20robust%20LLM-based%20ASR%20models%2C%0Atailored%20to%20different%20data%20characteristics%20and%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03866v1&entry.124074799=Read"},
{"title": "EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention", "author": "Yulong Shi and Mingwei Sun and Yongshuai Wang and Jiahao Ma and Zengqiang Chen", "abstract": "  Owing to advancements in deep learning technology, Vision Transformers (ViTs)\nhave demonstrated impressive performance in various computer vision tasks.\nNonetheless, ViTs still face some challenges, such as high computational\ncomplexity and the absence of desirable inductive biases. To alleviate these\nissues, {the potential advantages of combining eagle vision with ViTs are\nexplored. We summarize a Bi-Fovea Visual Interaction (BFVI) structure inspired\nby the unique physiological and visual characteristics of eagle eyes. A novel\nBi-Fovea Self-Attention (BFSA) mechanism and Bi-Fovea Feedforward Network\n(BFFN) are proposed based on this structural design approach, which can be used\nto mimic the hierarchical and parallel information processing scheme of the\nbiological visual cortex, enabling networks to learn feature representations of\ntargets in a coarse-to-fine manner. Furthermore, a Bionic Eagle Vision (BEV)\nblock is designed as the basic building unit based on the BFSA mechanism and\nBFFN. By stacking BEV blocks, a unified and efficient family of pyramid\nbackbone networks called Eagle Vision Transformers (EViTs) is developed.\nExperimental results show that EViTs exhibit highly competitive performance in\nvarious computer vision tasks, such as image classification, object detection\nand semantic segmentation. Compared with other approaches, EViTs have\nsignificant advantages, especially in terms of performance and computational\nefficiency. Code is available at https://github.com/nkusyl/EViT\n", "link": "http://arxiv.org/abs/2310.06629v4", "date": "2024-11-06", "relevancy": 2.6533, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EViT%3A%20An%20Eagle%20Vision%20Transformer%20with%20Bi-Fovea%20Self-Attention&body=Title%3A%20EViT%3A%20An%20Eagle%20Vision%20Transformer%20with%20Bi-Fovea%20Self-Attention%0AAuthor%3A%20Yulong%20Shi%20and%20Mingwei%20Sun%20and%20Yongshuai%20Wang%20and%20Jiahao%20Ma%20and%20Zengqiang%20Chen%0AAbstract%3A%20%20%20Owing%20to%20advancements%20in%20deep%20learning%20technology%2C%20Vision%20Transformers%20%28ViTs%29%0Ahave%20demonstrated%20impressive%20performance%20in%20various%20computer%20vision%20tasks.%0ANonetheless%2C%20ViTs%20still%20face%20some%20challenges%2C%20such%20as%20high%20computational%0Acomplexity%20and%20the%20absence%20of%20desirable%20inductive%20biases.%20To%20alleviate%20these%0Aissues%2C%20%7Bthe%20potential%20advantages%20of%20combining%20eagle%20vision%20with%20ViTs%20are%0Aexplored.%20We%20summarize%20a%20Bi-Fovea%20Visual%20Interaction%20%28BFVI%29%20structure%20inspired%0Aby%20the%20unique%20physiological%20and%20visual%20characteristics%20of%20eagle%20eyes.%20A%20novel%0ABi-Fovea%20Self-Attention%20%28BFSA%29%20mechanism%20and%20Bi-Fovea%20Feedforward%20Network%0A%28BFFN%29%20are%20proposed%20based%20on%20this%20structural%20design%20approach%2C%20which%20can%20be%20used%0Ato%20mimic%20the%20hierarchical%20and%20parallel%20information%20processing%20scheme%20of%20the%0Abiological%20visual%20cortex%2C%20enabling%20networks%20to%20learn%20feature%20representations%20of%0Atargets%20in%20a%20coarse-to-fine%20manner.%20Furthermore%2C%20a%20Bionic%20Eagle%20Vision%20%28BEV%29%0Ablock%20is%20designed%20as%20the%20basic%20building%20unit%20based%20on%20the%20BFSA%20mechanism%20and%0ABFFN.%20By%20stacking%20BEV%20blocks%2C%20a%20unified%20and%20efficient%20family%20of%20pyramid%0Abackbone%20networks%20called%20Eagle%20Vision%20Transformers%20%28EViTs%29%20is%20developed.%0AExperimental%20results%20show%20that%20EViTs%20exhibit%20highly%20competitive%20performance%20in%0Avarious%20computer%20vision%20tasks%2C%20such%20as%20image%20classification%2C%20object%20detection%0Aand%20semantic%20segmentation.%20Compared%20with%20other%20approaches%2C%20EViTs%20have%0Asignificant%20advantages%2C%20especially%20in%20terms%20of%20performance%20and%20computational%0Aefficiency.%20Code%20is%20available%20at%20https%3A//github.com/nkusyl/EViT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06629v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEViT%253A%2520An%2520Eagle%2520Vision%2520Transformer%2520with%2520Bi-Fovea%2520Self-Attention%26entry.906535625%3DYulong%2520Shi%2520and%2520Mingwei%2520Sun%2520and%2520Yongshuai%2520Wang%2520and%2520Jiahao%2520Ma%2520and%2520Zengqiang%2520Chen%26entry.1292438233%3D%2520%2520Owing%2520to%2520advancements%2520in%2520deep%2520learning%2520technology%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%250Ahave%2520demonstrated%2520impressive%2520performance%2520in%2520various%2520computer%2520vision%2520tasks.%250ANonetheless%252C%2520ViTs%2520still%2520face%2520some%2520challenges%252C%2520such%2520as%2520high%2520computational%250Acomplexity%2520and%2520the%2520absence%2520of%2520desirable%2520inductive%2520biases.%2520To%2520alleviate%2520these%250Aissues%252C%2520%257Bthe%2520potential%2520advantages%2520of%2520combining%2520eagle%2520vision%2520with%2520ViTs%2520are%250Aexplored.%2520We%2520summarize%2520a%2520Bi-Fovea%2520Visual%2520Interaction%2520%2528BFVI%2529%2520structure%2520inspired%250Aby%2520the%2520unique%2520physiological%2520and%2520visual%2520characteristics%2520of%2520eagle%2520eyes.%2520A%2520novel%250ABi-Fovea%2520Self-Attention%2520%2528BFSA%2529%2520mechanism%2520and%2520Bi-Fovea%2520Feedforward%2520Network%250A%2528BFFN%2529%2520are%2520proposed%2520based%2520on%2520this%2520structural%2520design%2520approach%252C%2520which%2520can%2520be%2520used%250Ato%2520mimic%2520the%2520hierarchical%2520and%2520parallel%2520information%2520processing%2520scheme%2520of%2520the%250Abiological%2520visual%2520cortex%252C%2520enabling%2520networks%2520to%2520learn%2520feature%2520representations%2520of%250Atargets%2520in%2520a%2520coarse-to-fine%2520manner.%2520Furthermore%252C%2520a%2520Bionic%2520Eagle%2520Vision%2520%2528BEV%2529%250Ablock%2520is%2520designed%2520as%2520the%2520basic%2520building%2520unit%2520based%2520on%2520the%2520BFSA%2520mechanism%2520and%250ABFFN.%2520By%2520stacking%2520BEV%2520blocks%252C%2520a%2520unified%2520and%2520efficient%2520family%2520of%2520pyramid%250Abackbone%2520networks%2520called%2520Eagle%2520Vision%2520Transformers%2520%2528EViTs%2529%2520is%2520developed.%250AExperimental%2520results%2520show%2520that%2520EViTs%2520exhibit%2520highly%2520competitive%2520performance%2520in%250Avarious%2520computer%2520vision%2520tasks%252C%2520such%2520as%2520image%2520classification%252C%2520object%2520detection%250Aand%2520semantic%2520segmentation.%2520Compared%2520with%2520other%2520approaches%252C%2520EViTs%2520have%250Asignificant%2520advantages%252C%2520especially%2520in%2520terms%2520of%2520performance%2520and%2520computational%250Aefficiency.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/nkusyl/EViT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06629v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EViT%3A%20An%20Eagle%20Vision%20Transformer%20with%20Bi-Fovea%20Self-Attention&entry.906535625=Yulong%20Shi%20and%20Mingwei%20Sun%20and%20Yongshuai%20Wang%20and%20Jiahao%20Ma%20and%20Zengqiang%20Chen&entry.1292438233=%20%20Owing%20to%20advancements%20in%20deep%20learning%20technology%2C%20Vision%20Transformers%20%28ViTs%29%0Ahave%20demonstrated%20impressive%20performance%20in%20various%20computer%20vision%20tasks.%0ANonetheless%2C%20ViTs%20still%20face%20some%20challenges%2C%20such%20as%20high%20computational%0Acomplexity%20and%20the%20absence%20of%20desirable%20inductive%20biases.%20To%20alleviate%20these%0Aissues%2C%20%7Bthe%20potential%20advantages%20of%20combining%20eagle%20vision%20with%20ViTs%20are%0Aexplored.%20We%20summarize%20a%20Bi-Fovea%20Visual%20Interaction%20%28BFVI%29%20structure%20inspired%0Aby%20the%20unique%20physiological%20and%20visual%20characteristics%20of%20eagle%20eyes.%20A%20novel%0ABi-Fovea%20Self-Attention%20%28BFSA%29%20mechanism%20and%20Bi-Fovea%20Feedforward%20Network%0A%28BFFN%29%20are%20proposed%20based%20on%20this%20structural%20design%20approach%2C%20which%20can%20be%20used%0Ato%20mimic%20the%20hierarchical%20and%20parallel%20information%20processing%20scheme%20of%20the%0Abiological%20visual%20cortex%2C%20enabling%20networks%20to%20learn%20feature%20representations%20of%0Atargets%20in%20a%20coarse-to-fine%20manner.%20Furthermore%2C%20a%20Bionic%20Eagle%20Vision%20%28BEV%29%0Ablock%20is%20designed%20as%20the%20basic%20building%20unit%20based%20on%20the%20BFSA%20mechanism%20and%0ABFFN.%20By%20stacking%20BEV%20blocks%2C%20a%20unified%20and%20efficient%20family%20of%20pyramid%0Abackbone%20networks%20called%20Eagle%20Vision%20Transformers%20%28EViTs%29%20is%20developed.%0AExperimental%20results%20show%20that%20EViTs%20exhibit%20highly%20competitive%20performance%20in%0Avarious%20computer%20vision%20tasks%2C%20such%20as%20image%20classification%2C%20object%20detection%0Aand%20semantic%20segmentation.%20Compared%20with%20other%20approaches%2C%20EViTs%20have%0Asignificant%20advantages%2C%20especially%20in%20terms%20of%20performance%20and%20computational%0Aefficiency.%20Code%20is%20available%20at%20https%3A//github.com/nkusyl/EViT%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06629v4&entry.124074799=Read"},
{"title": "GUIDE-VAE: Advancing Data Generation with User Information and Pattern\n  Dictionaries", "author": "Kutay B\u00f6lat and Simon Tindemans", "abstract": "  Generative modelling of multi-user datasets has become prominent in science\nand engineering. Generating a data point for a given user requires employing\nuser information, and conventional generative models, including variational\nautoencoders (VAEs), often ignore that. This paper introduces GUIDE-VAE, a\nnovel conditional generative model that leverages user embeddings to generate\nuser-guided data. By allowing the model to benefit from shared patterns across\nusers, GUIDE-VAE enhances performance in multi-user settings, even under\nsignificant data imbalance. In addition to integrating user information,\nGUIDE-VAE incorporates a pattern dictionary-based covariance composition (PDCC)\nto improve the realism of generated samples by capturing complex feature\ndependencies. While user embeddings drive performance gains, PDCC addresses\ncommon issues such as noise and over-smoothing typically seen in VAEs.\n  The proposed GUIDE-VAE was evaluated on a multi-user smart meter dataset\ncharacterized by substantial data imbalance across users. Quantitative results\nshow that GUIDE-VAE performs effectively in both synthetic data generation and\nmissing record imputation tasks, while qualitative evaluations reveal that\nGUIDE-VAE produces more plausible and less noisy data. These results establish\nGUIDE-VAE as a promising tool for controlled, realistic data generation in\nmulti-user datasets, with potential applications across various domains\nrequiring user-informed modelling.\n", "link": "http://arxiv.org/abs/2411.03936v1", "date": "2024-11-06", "relevancy": 2.6445, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5397}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5251}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUIDE-VAE%3A%20Advancing%20Data%20Generation%20with%20User%20Information%20and%20Pattern%0A%20%20Dictionaries&body=Title%3A%20GUIDE-VAE%3A%20Advancing%20Data%20Generation%20with%20User%20Information%20and%20Pattern%0A%20%20Dictionaries%0AAuthor%3A%20Kutay%20B%C3%B6lat%20and%20Simon%20Tindemans%0AAbstract%3A%20%20%20Generative%20modelling%20of%20multi-user%20datasets%20has%20become%20prominent%20in%20science%0Aand%20engineering.%20Generating%20a%20data%20point%20for%20a%20given%20user%20requires%20employing%0Auser%20information%2C%20and%20conventional%20generative%20models%2C%20including%20variational%0Aautoencoders%20%28VAEs%29%2C%20often%20ignore%20that.%20This%20paper%20introduces%20GUIDE-VAE%2C%20a%0Anovel%20conditional%20generative%20model%20that%20leverages%20user%20embeddings%20to%20generate%0Auser-guided%20data.%20By%20allowing%20the%20model%20to%20benefit%20from%20shared%20patterns%20across%0Ausers%2C%20GUIDE-VAE%20enhances%20performance%20in%20multi-user%20settings%2C%20even%20under%0Asignificant%20data%20imbalance.%20In%20addition%20to%20integrating%20user%20information%2C%0AGUIDE-VAE%20incorporates%20a%20pattern%20dictionary-based%20covariance%20composition%20%28PDCC%29%0Ato%20improve%20the%20realism%20of%20generated%20samples%20by%20capturing%20complex%20feature%0Adependencies.%20While%20user%20embeddings%20drive%20performance%20gains%2C%20PDCC%20addresses%0Acommon%20issues%20such%20as%20noise%20and%20over-smoothing%20typically%20seen%20in%20VAEs.%0A%20%20The%20proposed%20GUIDE-VAE%20was%20evaluated%20on%20a%20multi-user%20smart%20meter%20dataset%0Acharacterized%20by%20substantial%20data%20imbalance%20across%20users.%20Quantitative%20results%0Ashow%20that%20GUIDE-VAE%20performs%20effectively%20in%20both%20synthetic%20data%20generation%20and%0Amissing%20record%20imputation%20tasks%2C%20while%20qualitative%20evaluations%20reveal%20that%0AGUIDE-VAE%20produces%20more%20plausible%20and%20less%20noisy%20data.%20These%20results%20establish%0AGUIDE-VAE%20as%20a%20promising%20tool%20for%20controlled%2C%20realistic%20data%20generation%20in%0Amulti-user%20datasets%2C%20with%20potential%20applications%20across%20various%20domains%0Arequiring%20user-informed%20modelling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUIDE-VAE%253A%2520Advancing%2520Data%2520Generation%2520with%2520User%2520Information%2520and%2520Pattern%250A%2520%2520Dictionaries%26entry.906535625%3DKutay%2520B%25C3%25B6lat%2520and%2520Simon%2520Tindemans%26entry.1292438233%3D%2520%2520Generative%2520modelling%2520of%2520multi-user%2520datasets%2520has%2520become%2520prominent%2520in%2520science%250Aand%2520engineering.%2520Generating%2520a%2520data%2520point%2520for%2520a%2520given%2520user%2520requires%2520employing%250Auser%2520information%252C%2520and%2520conventional%2520generative%2520models%252C%2520including%2520variational%250Aautoencoders%2520%2528VAEs%2529%252C%2520often%2520ignore%2520that.%2520This%2520paper%2520introduces%2520GUIDE-VAE%252C%2520a%250Anovel%2520conditional%2520generative%2520model%2520that%2520leverages%2520user%2520embeddings%2520to%2520generate%250Auser-guided%2520data.%2520By%2520allowing%2520the%2520model%2520to%2520benefit%2520from%2520shared%2520patterns%2520across%250Ausers%252C%2520GUIDE-VAE%2520enhances%2520performance%2520in%2520multi-user%2520settings%252C%2520even%2520under%250Asignificant%2520data%2520imbalance.%2520In%2520addition%2520to%2520integrating%2520user%2520information%252C%250AGUIDE-VAE%2520incorporates%2520a%2520pattern%2520dictionary-based%2520covariance%2520composition%2520%2528PDCC%2529%250Ato%2520improve%2520the%2520realism%2520of%2520generated%2520samples%2520by%2520capturing%2520complex%2520feature%250Adependencies.%2520While%2520user%2520embeddings%2520drive%2520performance%2520gains%252C%2520PDCC%2520addresses%250Acommon%2520issues%2520such%2520as%2520noise%2520and%2520over-smoothing%2520typically%2520seen%2520in%2520VAEs.%250A%2520%2520The%2520proposed%2520GUIDE-VAE%2520was%2520evaluated%2520on%2520a%2520multi-user%2520smart%2520meter%2520dataset%250Acharacterized%2520by%2520substantial%2520data%2520imbalance%2520across%2520users.%2520Quantitative%2520results%250Ashow%2520that%2520GUIDE-VAE%2520performs%2520effectively%2520in%2520both%2520synthetic%2520data%2520generation%2520and%250Amissing%2520record%2520imputation%2520tasks%252C%2520while%2520qualitative%2520evaluations%2520reveal%2520that%250AGUIDE-VAE%2520produces%2520more%2520plausible%2520and%2520less%2520noisy%2520data.%2520These%2520results%2520establish%250AGUIDE-VAE%2520as%2520a%2520promising%2520tool%2520for%2520controlled%252C%2520realistic%2520data%2520generation%2520in%250Amulti-user%2520datasets%252C%2520with%2520potential%2520applications%2520across%2520various%2520domains%250Arequiring%2520user-informed%2520modelling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUIDE-VAE%3A%20Advancing%20Data%20Generation%20with%20User%20Information%20and%20Pattern%0A%20%20Dictionaries&entry.906535625=Kutay%20B%C3%B6lat%20and%20Simon%20Tindemans&entry.1292438233=%20%20Generative%20modelling%20of%20multi-user%20datasets%20has%20become%20prominent%20in%20science%0Aand%20engineering.%20Generating%20a%20data%20point%20for%20a%20given%20user%20requires%20employing%0Auser%20information%2C%20and%20conventional%20generative%20models%2C%20including%20variational%0Aautoencoders%20%28VAEs%29%2C%20often%20ignore%20that.%20This%20paper%20introduces%20GUIDE-VAE%2C%20a%0Anovel%20conditional%20generative%20model%20that%20leverages%20user%20embeddings%20to%20generate%0Auser-guided%20data.%20By%20allowing%20the%20model%20to%20benefit%20from%20shared%20patterns%20across%0Ausers%2C%20GUIDE-VAE%20enhances%20performance%20in%20multi-user%20settings%2C%20even%20under%0Asignificant%20data%20imbalance.%20In%20addition%20to%20integrating%20user%20information%2C%0AGUIDE-VAE%20incorporates%20a%20pattern%20dictionary-based%20covariance%20composition%20%28PDCC%29%0Ato%20improve%20the%20realism%20of%20generated%20samples%20by%20capturing%20complex%20feature%0Adependencies.%20While%20user%20embeddings%20drive%20performance%20gains%2C%20PDCC%20addresses%0Acommon%20issues%20such%20as%20noise%20and%20over-smoothing%20typically%20seen%20in%20VAEs.%0A%20%20The%20proposed%20GUIDE-VAE%20was%20evaluated%20on%20a%20multi-user%20smart%20meter%20dataset%0Acharacterized%20by%20substantial%20data%20imbalance%20across%20users.%20Quantitative%20results%0Ashow%20that%20GUIDE-VAE%20performs%20effectively%20in%20both%20synthetic%20data%20generation%20and%0Amissing%20record%20imputation%20tasks%2C%20while%20qualitative%20evaluations%20reveal%20that%0AGUIDE-VAE%20produces%20more%20plausible%20and%20less%20noisy%20data.%20These%20results%20establish%0AGUIDE-VAE%20as%20a%20promising%20tool%20for%20controlled%2C%20realistic%20data%20generation%20in%0Amulti-user%20datasets%2C%20with%20potential%20applications%20across%20various%20domains%0Arequiring%20user-informed%20modelling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03936v1&entry.124074799=Read"},
{"title": "Classification Done Right for Vision-Language Pre-Training", "author": "Zilong Huang and Qinghao Ye and Bingyi Kang and Jiashi Feng and Haoqi Fan", "abstract": "  We introduce SuperClass, a super simple classification method for\nvision-language pre-training on image-text data. Unlike its contrastive\ncounterpart CLIP who contrast with a text encoder, SuperClass directly utilizes\ntokenized raw text as supervised classification labels, without the need for\nadditional text filtering or selection. Due to the absence of the text encoding\nas contrastive target, SuperClass does not require a text encoder and does not\nneed to maintain a large batch size as CLIP does. SuperClass demonstrated\nsuperior performance on various downstream tasks, including classic computer\nvision benchmarks and vision language downstream tasks. We further explored the\nscaling behavior of SuperClass on model size, training length, or data size,\nand reported encouraging results and comparisons to CLIP.\nhttps://github.com/x-cls/superclass\n", "link": "http://arxiv.org/abs/2411.03313v2", "date": "2024-11-06", "relevancy": 2.6078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5408}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20Done%20Right%20for%20Vision-Language%20Pre-Training&body=Title%3A%20Classification%20Done%20Right%20for%20Vision-Language%20Pre-Training%0AAuthor%3A%20Zilong%20Huang%20and%20Qinghao%20Ye%20and%20Bingyi%20Kang%20and%20Jiashi%20Feng%20and%20Haoqi%20Fan%0AAbstract%3A%20%20%20We%20introduce%20SuperClass%2C%20a%20super%20simple%20classification%20method%20for%0Avision-language%20pre-training%20on%20image-text%20data.%20Unlike%20its%20contrastive%0Acounterpart%20CLIP%20who%20contrast%20with%20a%20text%20encoder%2C%20SuperClass%20directly%20utilizes%0Atokenized%20raw%20text%20as%20supervised%20classification%20labels%2C%20without%20the%20need%20for%0Aadditional%20text%20filtering%20or%20selection.%20Due%20to%20the%20absence%20of%20the%20text%20encoding%0Aas%20contrastive%20target%2C%20SuperClass%20does%20not%20require%20a%20text%20encoder%20and%20does%20not%0Aneed%20to%20maintain%20a%20large%20batch%20size%20as%20CLIP%20does.%20SuperClass%20demonstrated%0Asuperior%20performance%20on%20various%20downstream%20tasks%2C%20including%20classic%20computer%0Avision%20benchmarks%20and%20vision%20language%20downstream%20tasks.%20We%20further%20explored%20the%0Ascaling%20behavior%20of%20SuperClass%20on%20model%20size%2C%20training%20length%2C%20or%20data%20size%2C%0Aand%20reported%20encouraging%20results%20and%20comparisons%20to%20CLIP.%0Ahttps%3A//github.com/x-cls/superclass%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520Done%2520Right%2520for%2520Vision-Language%2520Pre-Training%26entry.906535625%3DZilong%2520Huang%2520and%2520Qinghao%2520Ye%2520and%2520Bingyi%2520Kang%2520and%2520Jiashi%2520Feng%2520and%2520Haoqi%2520Fan%26entry.1292438233%3D%2520%2520We%2520introduce%2520SuperClass%252C%2520a%2520super%2520simple%2520classification%2520method%2520for%250Avision-language%2520pre-training%2520on%2520image-text%2520data.%2520Unlike%2520its%2520contrastive%250Acounterpart%2520CLIP%2520who%2520contrast%2520with%2520a%2520text%2520encoder%252C%2520SuperClass%2520directly%2520utilizes%250Atokenized%2520raw%2520text%2520as%2520supervised%2520classification%2520labels%252C%2520without%2520the%2520need%2520for%250Aadditional%2520text%2520filtering%2520or%2520selection.%2520Due%2520to%2520the%2520absence%2520of%2520the%2520text%2520encoding%250Aas%2520contrastive%2520target%252C%2520SuperClass%2520does%2520not%2520require%2520a%2520text%2520encoder%2520and%2520does%2520not%250Aneed%2520to%2520maintain%2520a%2520large%2520batch%2520size%2520as%2520CLIP%2520does.%2520SuperClass%2520demonstrated%250Asuperior%2520performance%2520on%2520various%2520downstream%2520tasks%252C%2520including%2520classic%2520computer%250Avision%2520benchmarks%2520and%2520vision%2520language%2520downstream%2520tasks.%2520We%2520further%2520explored%2520the%250Ascaling%2520behavior%2520of%2520SuperClass%2520on%2520model%2520size%252C%2520training%2520length%252C%2520or%2520data%2520size%252C%250Aand%2520reported%2520encouraging%2520results%2520and%2520comparisons%2520to%2520CLIP.%250Ahttps%253A//github.com/x-cls/superclass%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20Done%20Right%20for%20Vision-Language%20Pre-Training&entry.906535625=Zilong%20Huang%20and%20Qinghao%20Ye%20and%20Bingyi%20Kang%20and%20Jiashi%20Feng%20and%20Haoqi%20Fan&entry.1292438233=%20%20We%20introduce%20SuperClass%2C%20a%20super%20simple%20classification%20method%20for%0Avision-language%20pre-training%20on%20image-text%20data.%20Unlike%20its%20contrastive%0Acounterpart%20CLIP%20who%20contrast%20with%20a%20text%20encoder%2C%20SuperClass%20directly%20utilizes%0Atokenized%20raw%20text%20as%20supervised%20classification%20labels%2C%20without%20the%20need%20for%0Aadditional%20text%20filtering%20or%20selection.%20Due%20to%20the%20absence%20of%20the%20text%20encoding%0Aas%20contrastive%20target%2C%20SuperClass%20does%20not%20require%20a%20text%20encoder%20and%20does%20not%0Aneed%20to%20maintain%20a%20large%20batch%20size%20as%20CLIP%20does.%20SuperClass%20demonstrated%0Asuperior%20performance%20on%20various%20downstream%20tasks%2C%20including%20classic%20computer%0Avision%20benchmarks%20and%20vision%20language%20downstream%20tasks.%20We%20further%20explored%20the%0Ascaling%20behavior%20of%20SuperClass%20on%20model%20size%2C%20training%20length%2C%20or%20data%20size%2C%0Aand%20reported%20encouraging%20results%20and%20comparisons%20to%20CLIP.%0Ahttps%3A//github.com/x-cls/superclass%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03313v2&entry.124074799=Read"},
{"title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models", "author": "Yiming Chen and Xianghu Yue and Xiaoxue Gao and Chen Zhang and Luis Fernando D'Haro and Robby T. Tan and Haizhou Li", "abstract": "  Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.\n", "link": "http://arxiv.org/abs/2409.18680v3", "date": "2024-11-06", "relevancy": 2.5978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Single-Audio%3A%20Advancing%20Multi-Audio%20Processing%20in%20Audio%20Large%0A%20%20Language%20Models&body=Title%3A%20Beyond%20Single-Audio%3A%20Advancing%20Multi-Audio%20Processing%20in%20Audio%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yiming%20Chen%20and%20Xianghu%20Yue%20and%20Xiaoxue%20Gao%20and%20Chen%20Zhang%20and%20Luis%20Fernando%20D%27Haro%20and%20Robby%20T.%20Tan%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20Various%20audio-LLMs%20%28ALLMs%29%20have%20been%20explored%20recently%20for%20tackling%20different%0Aaudio%20tasks%20simultaneously%20using%20a%20single%2C%20unified%20model.%20While%20existing%0Aevaluations%20of%20ALLMs%20primarily%20focus%20on%20single-audio%20tasks%2C%20real-world%0Aapplications%20often%20involve%20processing%20multiple%20audio%20streams%20simultaneously.%20To%0Abridge%20this%20gap%2C%20we%20propose%20the%20first%20multi-audio%20evaluation%20%28MAE%29%20benchmark%0Athat%20consists%20of%2020%20datasets%20from%2011%20multi-audio%20tasks%20encompassing%20both%20speech%0Aand%20sound%20scenarios.%20Comprehensive%20experiments%20on%20MAE%20demonstrate%20that%20the%0Aexisting%20ALLMs%2C%20while%20being%20powerful%20in%20comprehending%20primary%20audio%20elements%20in%0Aindividual%20audio%20inputs%2C%20struggling%20to%20handle%20multi-audio%20scenarios.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20multi-audio-LLM%20%28MALLM%29%20to%20capture%20audio%20context%20among%0Amultiple%20similar%20audios%20using%20discriminative%20learning%20on%20our%20proposed%20synthetic%0Adata.%20The%20results%20demonstrate%20that%20the%20proposed%20MALLM%20outperforms%20all%20baselines%0Aand%20achieves%20high%20data%20efficiency%20using%20synthetic%20data%20without%20requiring%20human%0Aannotations.%20The%20proposed%20MALLM%20opens%20the%20door%20for%20ALLMs%20towards%20multi-audio%0Aprocessing%20era%20and%20brings%20us%20closer%20to%20replicating%20human%20auditory%20capabilities%0Ain%20machines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18680v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Single-Audio%253A%2520Advancing%2520Multi-Audio%2520Processing%2520in%2520Audio%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYiming%2520Chen%2520and%2520Xianghu%2520Yue%2520and%2520Xiaoxue%2520Gao%2520and%2520Chen%2520Zhang%2520and%2520Luis%2520Fernando%2520D%2527Haro%2520and%2520Robby%2520T.%2520Tan%2520and%2520Haizhou%2520Li%26entry.1292438233%3D%2520%2520Various%2520audio-LLMs%2520%2528ALLMs%2529%2520have%2520been%2520explored%2520recently%2520for%2520tackling%2520different%250Aaudio%2520tasks%2520simultaneously%2520using%2520a%2520single%252C%2520unified%2520model.%2520While%2520existing%250Aevaluations%2520of%2520ALLMs%2520primarily%2520focus%2520on%2520single-audio%2520tasks%252C%2520real-world%250Aapplications%2520often%2520involve%2520processing%2520multiple%2520audio%2520streams%2520simultaneously.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520the%2520first%2520multi-audio%2520evaluation%2520%2528MAE%2529%2520benchmark%250Athat%2520consists%2520of%252020%2520datasets%2520from%252011%2520multi-audio%2520tasks%2520encompassing%2520both%2520speech%250Aand%2520sound%2520scenarios.%2520Comprehensive%2520experiments%2520on%2520MAE%2520demonstrate%2520that%2520the%250Aexisting%2520ALLMs%252C%2520while%2520being%2520powerful%2520in%2520comprehending%2520primary%2520audio%2520elements%2520in%250Aindividual%2520audio%2520inputs%252C%2520struggling%2520to%2520handle%2520multi-audio%2520scenarios.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520novel%2520multi-audio-LLM%2520%2528MALLM%2529%2520to%2520capture%2520audio%2520context%2520among%250Amultiple%2520similar%2520audios%2520using%2520discriminative%2520learning%2520on%2520our%2520proposed%2520synthetic%250Adata.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520MALLM%2520outperforms%2520all%2520baselines%250Aand%2520achieves%2520high%2520data%2520efficiency%2520using%2520synthetic%2520data%2520without%2520requiring%2520human%250Aannotations.%2520The%2520proposed%2520MALLM%2520opens%2520the%2520door%2520for%2520ALLMs%2520towards%2520multi-audio%250Aprocessing%2520era%2520and%2520brings%2520us%2520closer%2520to%2520replicating%2520human%2520auditory%2520capabilities%250Ain%2520machines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18680v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Single-Audio%3A%20Advancing%20Multi-Audio%20Processing%20in%20Audio%20Large%0A%20%20Language%20Models&entry.906535625=Yiming%20Chen%20and%20Xianghu%20Yue%20and%20Xiaoxue%20Gao%20and%20Chen%20Zhang%20and%20Luis%20Fernando%20D%27Haro%20and%20Robby%20T.%20Tan%20and%20Haizhou%20Li&entry.1292438233=%20%20Various%20audio-LLMs%20%28ALLMs%29%20have%20been%20explored%20recently%20for%20tackling%20different%0Aaudio%20tasks%20simultaneously%20using%20a%20single%2C%20unified%20model.%20While%20existing%0Aevaluations%20of%20ALLMs%20primarily%20focus%20on%20single-audio%20tasks%2C%20real-world%0Aapplications%20often%20involve%20processing%20multiple%20audio%20streams%20simultaneously.%20To%0Abridge%20this%20gap%2C%20we%20propose%20the%20first%20multi-audio%20evaluation%20%28MAE%29%20benchmark%0Athat%20consists%20of%2020%20datasets%20from%2011%20multi-audio%20tasks%20encompassing%20both%20speech%0Aand%20sound%20scenarios.%20Comprehensive%20experiments%20on%20MAE%20demonstrate%20that%20the%0Aexisting%20ALLMs%2C%20while%20being%20powerful%20in%20comprehending%20primary%20audio%20elements%20in%0Aindividual%20audio%20inputs%2C%20struggling%20to%20handle%20multi-audio%20scenarios.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20multi-audio-LLM%20%28MALLM%29%20to%20capture%20audio%20context%20among%0Amultiple%20similar%20audios%20using%20discriminative%20learning%20on%20our%20proposed%20synthetic%0Adata.%20The%20results%20demonstrate%20that%20the%20proposed%20MALLM%20outperforms%20all%20baselines%0Aand%20achieves%20high%20data%20efficiency%20using%20synthetic%20data%20without%20requiring%20human%0Aannotations.%20The%20proposed%20MALLM%20opens%20the%20door%20for%20ALLMs%20towards%20multi-audio%0Aprocessing%20era%20and%20brings%20us%20closer%20to%20replicating%20human%20auditory%20capabilities%0Ain%20machines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18680v3&entry.124074799=Read"},
{"title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned\n  Vision-Language Models", "author": "Maya Varma and Jean-Benoit Delbrouck and Zhihong Chen and Akshay Chaudhari and Curtis Langlotz", "abstract": "  Fine-tuned vision-language models (VLMs) often capture spurious correlations\nbetween image features and textual attributes, resulting in degraded zero-shot\nperformance at test time. Existing approaches for addressing spurious\ncorrelations (i) primarily operate at the global image-level rather than\nintervening directly on fine-grained image features and (ii) are predominantly\ndesigned for unimodal settings. In this work, we present RaVL, which takes a\nfine-grained perspective on VLM robustness by discovering and mitigating\nspurious correlations using local image features rather than operating at the\nglobal image level. Given a fine-tuned VLM, RaVL first discovers spurious\ncorrelations by leveraging a region-level clustering approach to identify\nprecise image features contributing to zero-shot classification errors. Then,\nRaVL mitigates the identified spurious correlation with a novel region-aware\nloss function that enables the VLM to focus on relevant regions and ignore\nspurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with\nvarious model architectures, data domains, and learned spurious correlations.\nOur results show that RaVL accurately discovers (191% improvement over the\nclosest baseline) and mitigates (8.2% improvement on worst-group image\nclassification accuracy) spurious correlations. Qualitative evaluations on\ngeneral-domain and medical-domain VLMs confirm our findings.\n", "link": "http://arxiv.org/abs/2411.04097v1", "date": "2024-11-06", "relevancy": 2.5563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaVL%3A%20Discovering%20and%20Mitigating%20Spurious%20Correlations%20in%20Fine-Tuned%0A%20%20Vision-Language%20Models&body=Title%3A%20RaVL%3A%20Discovering%20and%20Mitigating%20Spurious%20Correlations%20in%20Fine-Tuned%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Maya%20Varma%20and%20Jean-Benoit%20Delbrouck%20and%20Zhihong%20Chen%20and%20Akshay%20Chaudhari%20and%20Curtis%20Langlotz%0AAbstract%3A%20%20%20Fine-tuned%20vision-language%20models%20%28VLMs%29%20often%20capture%20spurious%20correlations%0Abetween%20image%20features%20and%20textual%20attributes%2C%20resulting%20in%20degraded%20zero-shot%0Aperformance%20at%20test%20time.%20Existing%20approaches%20for%20addressing%20spurious%0Acorrelations%20%28i%29%20primarily%20operate%20at%20the%20global%20image-level%20rather%20than%0Aintervening%20directly%20on%20fine-grained%20image%20features%20and%20%28ii%29%20are%20predominantly%0Adesigned%20for%20unimodal%20settings.%20In%20this%20work%2C%20we%20present%20RaVL%2C%20which%20takes%20a%0Afine-grained%20perspective%20on%20VLM%20robustness%20by%20discovering%20and%20mitigating%0Aspurious%20correlations%20using%20local%20image%20features%20rather%20than%20operating%20at%20the%0Aglobal%20image%20level.%20Given%20a%20fine-tuned%20VLM%2C%20RaVL%20first%20discovers%20spurious%0Acorrelations%20by%20leveraging%20a%20region-level%20clustering%20approach%20to%20identify%0Aprecise%20image%20features%20contributing%20to%20zero-shot%20classification%20errors.%20Then%2C%0ARaVL%20mitigates%20the%20identified%20spurious%20correlation%20with%20a%20novel%20region-aware%0Aloss%20function%20that%20enables%20the%20VLM%20to%20focus%20on%20relevant%20regions%20and%20ignore%0Aspurious%20relationships%20during%20fine-tuning.%20We%20evaluate%20RaVL%20on%20654%20VLMs%20with%0Avarious%20model%20architectures%2C%20data%20domains%2C%20and%20learned%20spurious%20correlations.%0AOur%20results%20show%20that%20RaVL%20accurately%20discovers%20%28191%25%20improvement%20over%20the%0Aclosest%20baseline%29%20and%20mitigates%20%288.2%25%20improvement%20on%20worst-group%20image%0Aclassification%20accuracy%29%20spurious%20correlations.%20Qualitative%20evaluations%20on%0Ageneral-domain%20and%20medical-domain%20VLMs%20confirm%20our%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaVL%253A%2520Discovering%2520and%2520Mitigating%2520Spurious%2520Correlations%2520in%2520Fine-Tuned%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DMaya%2520Varma%2520and%2520Jean-Benoit%2520Delbrouck%2520and%2520Zhihong%2520Chen%2520and%2520Akshay%2520Chaudhari%2520and%2520Curtis%2520Langlotz%26entry.1292438233%3D%2520%2520Fine-tuned%2520vision-language%2520models%2520%2528VLMs%2529%2520often%2520capture%2520spurious%2520correlations%250Abetween%2520image%2520features%2520and%2520textual%2520attributes%252C%2520resulting%2520in%2520degraded%2520zero-shot%250Aperformance%2520at%2520test%2520time.%2520Existing%2520approaches%2520for%2520addressing%2520spurious%250Acorrelations%2520%2528i%2529%2520primarily%2520operate%2520at%2520the%2520global%2520image-level%2520rather%2520than%250Aintervening%2520directly%2520on%2520fine-grained%2520image%2520features%2520and%2520%2528ii%2529%2520are%2520predominantly%250Adesigned%2520for%2520unimodal%2520settings.%2520In%2520this%2520work%252C%2520we%2520present%2520RaVL%252C%2520which%2520takes%2520a%250Afine-grained%2520perspective%2520on%2520VLM%2520robustness%2520by%2520discovering%2520and%2520mitigating%250Aspurious%2520correlations%2520using%2520local%2520image%2520features%2520rather%2520than%2520operating%2520at%2520the%250Aglobal%2520image%2520level.%2520Given%2520a%2520fine-tuned%2520VLM%252C%2520RaVL%2520first%2520discovers%2520spurious%250Acorrelations%2520by%2520leveraging%2520a%2520region-level%2520clustering%2520approach%2520to%2520identify%250Aprecise%2520image%2520features%2520contributing%2520to%2520zero-shot%2520classification%2520errors.%2520Then%252C%250ARaVL%2520mitigates%2520the%2520identified%2520spurious%2520correlation%2520with%2520a%2520novel%2520region-aware%250Aloss%2520function%2520that%2520enables%2520the%2520VLM%2520to%2520focus%2520on%2520relevant%2520regions%2520and%2520ignore%250Aspurious%2520relationships%2520during%2520fine-tuning.%2520We%2520evaluate%2520RaVL%2520on%2520654%2520VLMs%2520with%250Avarious%2520model%2520architectures%252C%2520data%2520domains%252C%2520and%2520learned%2520spurious%2520correlations.%250AOur%2520results%2520show%2520that%2520RaVL%2520accurately%2520discovers%2520%2528191%2525%2520improvement%2520over%2520the%250Aclosest%2520baseline%2529%2520and%2520mitigates%2520%25288.2%2525%2520improvement%2520on%2520worst-group%2520image%250Aclassification%2520accuracy%2529%2520spurious%2520correlations.%2520Qualitative%2520evaluations%2520on%250Ageneral-domain%2520and%2520medical-domain%2520VLMs%2520confirm%2520our%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaVL%3A%20Discovering%20and%20Mitigating%20Spurious%20Correlations%20in%20Fine-Tuned%0A%20%20Vision-Language%20Models&entry.906535625=Maya%20Varma%20and%20Jean-Benoit%20Delbrouck%20and%20Zhihong%20Chen%20and%20Akshay%20Chaudhari%20and%20Curtis%20Langlotz&entry.1292438233=%20%20Fine-tuned%20vision-language%20models%20%28VLMs%29%20often%20capture%20spurious%20correlations%0Abetween%20image%20features%20and%20textual%20attributes%2C%20resulting%20in%20degraded%20zero-shot%0Aperformance%20at%20test%20time.%20Existing%20approaches%20for%20addressing%20spurious%0Acorrelations%20%28i%29%20primarily%20operate%20at%20the%20global%20image-level%20rather%20than%0Aintervening%20directly%20on%20fine-grained%20image%20features%20and%20%28ii%29%20are%20predominantly%0Adesigned%20for%20unimodal%20settings.%20In%20this%20work%2C%20we%20present%20RaVL%2C%20which%20takes%20a%0Afine-grained%20perspective%20on%20VLM%20robustness%20by%20discovering%20and%20mitigating%0Aspurious%20correlations%20using%20local%20image%20features%20rather%20than%20operating%20at%20the%0Aglobal%20image%20level.%20Given%20a%20fine-tuned%20VLM%2C%20RaVL%20first%20discovers%20spurious%0Acorrelations%20by%20leveraging%20a%20region-level%20clustering%20approach%20to%20identify%0Aprecise%20image%20features%20contributing%20to%20zero-shot%20classification%20errors.%20Then%2C%0ARaVL%20mitigates%20the%20identified%20spurious%20correlation%20with%20a%20novel%20region-aware%0Aloss%20function%20that%20enables%20the%20VLM%20to%20focus%20on%20relevant%20regions%20and%20ignore%0Aspurious%20relationships%20during%20fine-tuning.%20We%20evaluate%20RaVL%20on%20654%20VLMs%20with%0Avarious%20model%20architectures%2C%20data%20domains%2C%20and%20learned%20spurious%20correlations.%0AOur%20results%20show%20that%20RaVL%20accurately%20discovers%20%28191%25%20improvement%20over%20the%0Aclosest%20baseline%29%20and%20mitigates%20%288.2%25%20improvement%20on%20worst-group%20image%0Aclassification%20accuracy%29%20spurious%20correlations.%20Qualitative%20evaluations%20on%0Ageneral-domain%20and%20medical-domain%20VLMs%20confirm%20our%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04097v1&entry.124074799=Read"},
{"title": "UniTraj: Universal Human Trajectory Modeling from Billion-Scale\n  Worldwide Traces", "author": "Yuanshao Zhu and James Jianqiao Yu and Xiangyu Zhao and Xuetao Wei and Yuxuan Liang", "abstract": "  Human trajectory modeling is essential for deciphering movement patterns and\nsupporting advanced applications across various domains. However, existing\nmethods are often tailored to specific tasks and regions, resulting in\nlimitations related to task specificity, regional dependency, and data quality\nsensitivity. Addressing these challenges requires a universal human trajectory\nfoundation model capable of generalizing and scaling across diverse tasks and\ngeographic contexts. To this end, we propose UniTraj, a Universal human\nTrajectory foundation model that is task-adaptive, region-independent, and\nhighly generalizable. To further enhance performance, we construct WorldTrace,\nthe first large-scale, high-quality, globally distributed dataset sourced from\nopen web platforms, encompassing 2.45 million trajectories with billions of\npoints across 70 countries. Through multiple resampling and masking strategies\ndesigned for pre-training, UniTraj effectively overcomes geographic and task\nconstraints, adapting to heterogeneous data quality. Extensive experiments\nacross multiple trajectory analysis tasks and real-world datasets demonstrate\nthat UniTraj consistently outperforms existing approaches in terms of\nscalability and adaptability. These results underscore the potential of UniTraj\nas a versatile, robust solution for a wide range of trajectory analysis\napplications, with WorldTrace serving as an ideal but non-exclusive foundation\nfor training.\n", "link": "http://arxiv.org/abs/2411.03859v1", "date": "2024-11-06", "relevancy": 2.5154, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5079}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5057}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTraj%3A%20Universal%20Human%20Trajectory%20Modeling%20from%20Billion-Scale%0A%20%20Worldwide%20Traces&body=Title%3A%20UniTraj%3A%20Universal%20Human%20Trajectory%20Modeling%20from%20Billion-Scale%0A%20%20Worldwide%20Traces%0AAuthor%3A%20Yuanshao%20Zhu%20and%20James%20Jianqiao%20Yu%20and%20Xiangyu%20Zhao%20and%20Xuetao%20Wei%20and%20Yuxuan%20Liang%0AAbstract%3A%20%20%20Human%20trajectory%20modeling%20is%20essential%20for%20deciphering%20movement%20patterns%20and%0Asupporting%20advanced%20applications%20across%20various%20domains.%20However%2C%20existing%0Amethods%20are%20often%20tailored%20to%20specific%20tasks%20and%20regions%2C%20resulting%20in%0Alimitations%20related%20to%20task%20specificity%2C%20regional%20dependency%2C%20and%20data%20quality%0Asensitivity.%20Addressing%20these%20challenges%20requires%20a%20universal%20human%20trajectory%0Afoundation%20model%20capable%20of%20generalizing%20and%20scaling%20across%20diverse%20tasks%20and%0Ageographic%20contexts.%20To%20this%20end%2C%20we%20propose%20UniTraj%2C%20a%20Universal%20human%0ATrajectory%20foundation%20model%20that%20is%20task-adaptive%2C%20region-independent%2C%20and%0Ahighly%20generalizable.%20To%20further%20enhance%20performance%2C%20we%20construct%20WorldTrace%2C%0Athe%20first%20large-scale%2C%20high-quality%2C%20globally%20distributed%20dataset%20sourced%20from%0Aopen%20web%20platforms%2C%20encompassing%202.45%20million%20trajectories%20with%20billions%20of%0Apoints%20across%2070%20countries.%20Through%20multiple%20resampling%20and%20masking%20strategies%0Adesigned%20for%20pre-training%2C%20UniTraj%20effectively%20overcomes%20geographic%20and%20task%0Aconstraints%2C%20adapting%20to%20heterogeneous%20data%20quality.%20Extensive%20experiments%0Aacross%20multiple%20trajectory%20analysis%20tasks%20and%20real-world%20datasets%20demonstrate%0Athat%20UniTraj%20consistently%20outperforms%20existing%20approaches%20in%20terms%20of%0Ascalability%20and%20adaptability.%20These%20results%20underscore%20the%20potential%20of%20UniTraj%0Aas%20a%20versatile%2C%20robust%20solution%20for%20a%20wide%20range%20of%20trajectory%20analysis%0Aapplications%2C%20with%20WorldTrace%20serving%20as%20an%20ideal%20but%20non-exclusive%20foundation%0Afor%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTraj%253A%2520Universal%2520Human%2520Trajectory%2520Modeling%2520from%2520Billion-Scale%250A%2520%2520Worldwide%2520Traces%26entry.906535625%3DYuanshao%2520Zhu%2520and%2520James%2520Jianqiao%2520Yu%2520and%2520Xiangyu%2520Zhao%2520and%2520Xuetao%2520Wei%2520and%2520Yuxuan%2520Liang%26entry.1292438233%3D%2520%2520Human%2520trajectory%2520modeling%2520is%2520essential%2520for%2520deciphering%2520movement%2520patterns%2520and%250Asupporting%2520advanced%2520applications%2520across%2520various%2520domains.%2520However%252C%2520existing%250Amethods%2520are%2520often%2520tailored%2520to%2520specific%2520tasks%2520and%2520regions%252C%2520resulting%2520in%250Alimitations%2520related%2520to%2520task%2520specificity%252C%2520regional%2520dependency%252C%2520and%2520data%2520quality%250Asensitivity.%2520Addressing%2520these%2520challenges%2520requires%2520a%2520universal%2520human%2520trajectory%250Afoundation%2520model%2520capable%2520of%2520generalizing%2520and%2520scaling%2520across%2520diverse%2520tasks%2520and%250Ageographic%2520contexts.%2520To%2520this%2520end%252C%2520we%2520propose%2520UniTraj%252C%2520a%2520Universal%2520human%250ATrajectory%2520foundation%2520model%2520that%2520is%2520task-adaptive%252C%2520region-independent%252C%2520and%250Ahighly%2520generalizable.%2520To%2520further%2520enhance%2520performance%252C%2520we%2520construct%2520WorldTrace%252C%250Athe%2520first%2520large-scale%252C%2520high-quality%252C%2520globally%2520distributed%2520dataset%2520sourced%2520from%250Aopen%2520web%2520platforms%252C%2520encompassing%25202.45%2520million%2520trajectories%2520with%2520billions%2520of%250Apoints%2520across%252070%2520countries.%2520Through%2520multiple%2520resampling%2520and%2520masking%2520strategies%250Adesigned%2520for%2520pre-training%252C%2520UniTraj%2520effectively%2520overcomes%2520geographic%2520and%2520task%250Aconstraints%252C%2520adapting%2520to%2520heterogeneous%2520data%2520quality.%2520Extensive%2520experiments%250Aacross%2520multiple%2520trajectory%2520analysis%2520tasks%2520and%2520real-world%2520datasets%2520demonstrate%250Athat%2520UniTraj%2520consistently%2520outperforms%2520existing%2520approaches%2520in%2520terms%2520of%250Ascalability%2520and%2520adaptability.%2520These%2520results%2520underscore%2520the%2520potential%2520of%2520UniTraj%250Aas%2520a%2520versatile%252C%2520robust%2520solution%2520for%2520a%2520wide%2520range%2520of%2520trajectory%2520analysis%250Aapplications%252C%2520with%2520WorldTrace%2520serving%2520as%2520an%2520ideal%2520but%2520non-exclusive%2520foundation%250Afor%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTraj%3A%20Universal%20Human%20Trajectory%20Modeling%20from%20Billion-Scale%0A%20%20Worldwide%20Traces&entry.906535625=Yuanshao%20Zhu%20and%20James%20Jianqiao%20Yu%20and%20Xiangyu%20Zhao%20and%20Xuetao%20Wei%20and%20Yuxuan%20Liang&entry.1292438233=%20%20Human%20trajectory%20modeling%20is%20essential%20for%20deciphering%20movement%20patterns%20and%0Asupporting%20advanced%20applications%20across%20various%20domains.%20However%2C%20existing%0Amethods%20are%20often%20tailored%20to%20specific%20tasks%20and%20regions%2C%20resulting%20in%0Alimitations%20related%20to%20task%20specificity%2C%20regional%20dependency%2C%20and%20data%20quality%0Asensitivity.%20Addressing%20these%20challenges%20requires%20a%20universal%20human%20trajectory%0Afoundation%20model%20capable%20of%20generalizing%20and%20scaling%20across%20diverse%20tasks%20and%0Ageographic%20contexts.%20To%20this%20end%2C%20we%20propose%20UniTraj%2C%20a%20Universal%20human%0ATrajectory%20foundation%20model%20that%20is%20task-adaptive%2C%20region-independent%2C%20and%0Ahighly%20generalizable.%20To%20further%20enhance%20performance%2C%20we%20construct%20WorldTrace%2C%0Athe%20first%20large-scale%2C%20high-quality%2C%20globally%20distributed%20dataset%20sourced%20from%0Aopen%20web%20platforms%2C%20encompassing%202.45%20million%20trajectories%20with%20billions%20of%0Apoints%20across%2070%20countries.%20Through%20multiple%20resampling%20and%20masking%20strategies%0Adesigned%20for%20pre-training%2C%20UniTraj%20effectively%20overcomes%20geographic%20and%20task%0Aconstraints%2C%20adapting%20to%20heterogeneous%20data%20quality.%20Extensive%20experiments%0Aacross%20multiple%20trajectory%20analysis%20tasks%20and%20real-world%20datasets%20demonstrate%0Athat%20UniTraj%20consistently%20outperforms%20existing%20approaches%20in%20terms%20of%0Ascalability%20and%20adaptability.%20These%20results%20underscore%20the%20potential%20of%20UniTraj%0Aas%20a%20versatile%2C%20robust%20solution%20for%20a%20wide%20range%20of%20trajectory%20analysis%0Aapplications%2C%20with%20WorldTrace%20serving%20as%20an%20ideal%20but%20non-exclusive%20foundation%0Afor%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03859v1&entry.124074799=Read"},
{"title": "Opening the Black-Box: A Systematic Review on Explainable AI in Remote\n  Sensing", "author": "Adrian H\u00f6hl and Ivica Obadic and Miguel \u00c1ngel Fern\u00e1ndez Torres and Hiba Najjar and Dario Oliveira and Zeynep Akata and Andreas Dengel and Xiao Xiang Zhu", "abstract": "  In recent years, black-box machine learning approaches have become a dominant\nmodeling paradigm for knowledge extraction in remote sensing. Despite the\npotential benefits of uncovering the inner workings of these models with\nexplainable AI, a comprehensive overview summarizing the explainable AI methods\nused and their objectives, findings, and challenges in remote sensing\napplications is still missing. In this paper, we address this gap by performing\na systematic review to identify the key trends in the field and shed light on\nnovel explainable AI approaches and emerging directions that tackle specific\nremote sensing challenges. We also reveal the common patterns of explanation\ninterpretation, discuss the extracted scientific insights, and reflect on the\napproaches used for the evaluation of explainable AI methods. As such, our\nreview provides a complete summary of the state-of-the-art of explainable AI in\nremote sensing. Further, we give a detailed outlook on the challenges and\npromising research directions, representing a basis for novel methodological\ndevelopment and a useful starting point for new researchers in the field.\n", "link": "http://arxiv.org/abs/2402.13791v2", "date": "2024-11-06", "relevancy": 2.5086, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opening%20the%20Black-Box%3A%20A%20Systematic%20Review%20on%20Explainable%20AI%20in%20Remote%0A%20%20Sensing&body=Title%3A%20Opening%20the%20Black-Box%3A%20A%20Systematic%20Review%20on%20Explainable%20AI%20in%20Remote%0A%20%20Sensing%0AAuthor%3A%20Adrian%20H%C3%B6hl%20and%20Ivica%20Obadic%20and%20Miguel%20%C3%81ngel%20Fern%C3%A1ndez%20Torres%20and%20Hiba%20Najjar%20and%20Dario%20Oliveira%20and%20Zeynep%20Akata%20and%20Andreas%20Dengel%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20black-box%20machine%20learning%20approaches%20have%20become%20a%20dominant%0Amodeling%20paradigm%20for%20knowledge%20extraction%20in%20remote%20sensing.%20Despite%20the%0Apotential%20benefits%20of%20uncovering%20the%20inner%20workings%20of%20these%20models%20with%0Aexplainable%20AI%2C%20a%20comprehensive%20overview%20summarizing%20the%20explainable%20AI%20methods%0Aused%20and%20their%20objectives%2C%20findings%2C%20and%20challenges%20in%20remote%20sensing%0Aapplications%20is%20still%20missing.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20performing%0Aa%20systematic%20review%20to%20identify%20the%20key%20trends%20in%20the%20field%20and%20shed%20light%20on%0Anovel%20explainable%20AI%20approaches%20and%20emerging%20directions%20that%20tackle%20specific%0Aremote%20sensing%20challenges.%20We%20also%20reveal%20the%20common%20patterns%20of%20explanation%0Ainterpretation%2C%20discuss%20the%20extracted%20scientific%20insights%2C%20and%20reflect%20on%20the%0Aapproaches%20used%20for%20the%20evaluation%20of%20explainable%20AI%20methods.%20As%20such%2C%20our%0Areview%20provides%20a%20complete%20summary%20of%20the%20state-of-the-art%20of%20explainable%20AI%20in%0Aremote%20sensing.%20Further%2C%20we%20give%20a%20detailed%20outlook%20on%20the%20challenges%20and%0Apromising%20research%20directions%2C%20representing%20a%20basis%20for%20novel%20methodological%0Adevelopment%20and%20a%20useful%20starting%20point%20for%20new%20researchers%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpening%2520the%2520Black-Box%253A%2520A%2520Systematic%2520Review%2520on%2520Explainable%2520AI%2520in%2520Remote%250A%2520%2520Sensing%26entry.906535625%3DAdrian%2520H%25C3%25B6hl%2520and%2520Ivica%2520Obadic%2520and%2520Miguel%2520%25C3%2581ngel%2520Fern%25C3%25A1ndez%2520Torres%2520and%2520Hiba%2520Najjar%2520and%2520Dario%2520Oliveira%2520and%2520Zeynep%2520Akata%2520and%2520Andreas%2520Dengel%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520black-box%2520machine%2520learning%2520approaches%2520have%2520become%2520a%2520dominant%250Amodeling%2520paradigm%2520for%2520knowledge%2520extraction%2520in%2520remote%2520sensing.%2520Despite%2520the%250Apotential%2520benefits%2520of%2520uncovering%2520the%2520inner%2520workings%2520of%2520these%2520models%2520with%250Aexplainable%2520AI%252C%2520a%2520comprehensive%2520overview%2520summarizing%2520the%2520explainable%2520AI%2520methods%250Aused%2520and%2520their%2520objectives%252C%2520findings%252C%2520and%2520challenges%2520in%2520remote%2520sensing%250Aapplications%2520is%2520still%2520missing.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520performing%250Aa%2520systematic%2520review%2520to%2520identify%2520the%2520key%2520trends%2520in%2520the%2520field%2520and%2520shed%2520light%2520on%250Anovel%2520explainable%2520AI%2520approaches%2520and%2520emerging%2520directions%2520that%2520tackle%2520specific%250Aremote%2520sensing%2520challenges.%2520We%2520also%2520reveal%2520the%2520common%2520patterns%2520of%2520explanation%250Ainterpretation%252C%2520discuss%2520the%2520extracted%2520scientific%2520insights%252C%2520and%2520reflect%2520on%2520the%250Aapproaches%2520used%2520for%2520the%2520evaluation%2520of%2520explainable%2520AI%2520methods.%2520As%2520such%252C%2520our%250Areview%2520provides%2520a%2520complete%2520summary%2520of%2520the%2520state-of-the-art%2520of%2520explainable%2520AI%2520in%250Aremote%2520sensing.%2520Further%252C%2520we%2520give%2520a%2520detailed%2520outlook%2520on%2520the%2520challenges%2520and%250Apromising%2520research%2520directions%252C%2520representing%2520a%2520basis%2520for%2520novel%2520methodological%250Adevelopment%2520and%2520a%2520useful%2520starting%2520point%2520for%2520new%2520researchers%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opening%20the%20Black-Box%3A%20A%20Systematic%20Review%20on%20Explainable%20AI%20in%20Remote%0A%20%20Sensing&entry.906535625=Adrian%20H%C3%B6hl%20and%20Ivica%20Obadic%20and%20Miguel%20%C3%81ngel%20Fern%C3%A1ndez%20Torres%20and%20Hiba%20Najjar%20and%20Dario%20Oliveira%20and%20Zeynep%20Akata%20and%20Andreas%20Dengel%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20In%20recent%20years%2C%20black-box%20machine%20learning%20approaches%20have%20become%20a%20dominant%0Amodeling%20paradigm%20for%20knowledge%20extraction%20in%20remote%20sensing.%20Despite%20the%0Apotential%20benefits%20of%20uncovering%20the%20inner%20workings%20of%20these%20models%20with%0Aexplainable%20AI%2C%20a%20comprehensive%20overview%20summarizing%20the%20explainable%20AI%20methods%0Aused%20and%20their%20objectives%2C%20findings%2C%20and%20challenges%20in%20remote%20sensing%0Aapplications%20is%20still%20missing.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20performing%0Aa%20systematic%20review%20to%20identify%20the%20key%20trends%20in%20the%20field%20and%20shed%20light%20on%0Anovel%20explainable%20AI%20approaches%20and%20emerging%20directions%20that%20tackle%20specific%0Aremote%20sensing%20challenges.%20We%20also%20reveal%20the%20common%20patterns%20of%20explanation%0Ainterpretation%2C%20discuss%20the%20extracted%20scientific%20insights%2C%20and%20reflect%20on%20the%0Aapproaches%20used%20for%20the%20evaluation%20of%20explainable%20AI%20methods.%20As%20such%2C%20our%0Areview%20provides%20a%20complete%20summary%20of%20the%20state-of-the-art%20of%20explainable%20AI%20in%0Aremote%20sensing.%20Further%2C%20we%20give%20a%20detailed%20outlook%20on%20the%20challenges%20and%0Apromising%20research%20directions%2C%20representing%20a%20basis%20for%20novel%20methodological%0Adevelopment%20and%20a%20useful%20starting%20point%20for%20new%20researchers%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13791v2&entry.124074799=Read"},
{"title": "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level\n  Chart Question Answering", "author": "Yifan Wu and Lutao Yan and Leixian Shen and Yunhai Wang and Nan Tang and Yuyu Luo", "abstract": "  Chart question answering (ChartQA) tasks play a critical role in interpreting\nand extracting insights from visualization charts. While recent advancements in\nmultimodal large language models (MLLMs) like GPT-4o have shown promise in\nhigh-level ChartQA tasks, such as chart captioning, their effectiveness in\nlow-level ChartQA tasks (e.g., identifying correlations) remains underexplored.\nIn this paper, we address this gap by evaluating MLLMs on low-level ChartQA\nusing a newly curated dataset, ChartInsights, which consists of 22,347 (chart,\ntask, query, answer) covering 10 data analysis tasks across 7 chart types. We\nsystematically evaluate 19 advanced MLLMs, including 12 open-source and 7\nclosed-source models. The average accuracy rate across these models is 39.8%,\nwith GPT-4o achieving the highest accuracy at 69.17%. To further explore the\nlimitations of MLLMs in low-level ChartQA, we conduct experiments that alter\nvisual elements of charts (e.g., changing color schemes, adding image noise) to\nassess their impact on the task effectiveness. Furthermore, we propose a new\ntextual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks,\nwhich boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally,\nincorporating a visual prompt strategy that directs attention to relevant\nvisual elements further improves accuracy to 84.32%.\n", "link": "http://arxiv.org/abs/2405.07001v4", "date": "2024-11-06", "relevancy": 2.4913, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartInsights%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20for%20Low-Level%0A%20%20Chart%20Question%20Answering&body=Title%3A%20ChartInsights%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20for%20Low-Level%0A%20%20Chart%20Question%20Answering%0AAuthor%3A%20Yifan%20Wu%20and%20Lutao%20Yan%20and%20Leixian%20Shen%20and%20Yunhai%20Wang%20and%20Nan%20Tang%20and%20Yuyu%20Luo%0AAbstract%3A%20%20%20Chart%20question%20answering%20%28ChartQA%29%20tasks%20play%20a%20critical%20role%20in%20interpreting%0Aand%20extracting%20insights%20from%20visualization%20charts.%20While%20recent%20advancements%20in%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20like%20GPT-4o%20have%20shown%20promise%20in%0Ahigh-level%20ChartQA%20tasks%2C%20such%20as%20chart%20captioning%2C%20their%20effectiveness%20in%0Alow-level%20ChartQA%20tasks%20%28e.g.%2C%20identifying%20correlations%29%20remains%20underexplored.%0AIn%20this%20paper%2C%20we%20address%20this%20gap%20by%20evaluating%20MLLMs%20on%20low-level%20ChartQA%0Ausing%20a%20newly%20curated%20dataset%2C%20ChartInsights%2C%20which%20consists%20of%2022%2C347%20%28chart%2C%0Atask%2C%20query%2C%20answer%29%20covering%2010%20data%20analysis%20tasks%20across%207%20chart%20types.%20We%0Asystematically%20evaluate%2019%20advanced%20MLLMs%2C%20including%2012%20open-source%20and%207%0Aclosed-source%20models.%20The%20average%20accuracy%20rate%20across%20these%20models%20is%2039.8%25%2C%0Awith%20GPT-4o%20achieving%20the%20highest%20accuracy%20at%2069.17%25.%20To%20further%20explore%20the%0Alimitations%20of%20MLLMs%20in%20low-level%20ChartQA%2C%20we%20conduct%20experiments%20that%20alter%0Avisual%20elements%20of%20charts%20%28e.g.%2C%20changing%20color%20schemes%2C%20adding%20image%20noise%29%20to%0Aassess%20their%20impact%20on%20the%20task%20effectiveness.%20Furthermore%2C%20we%20propose%20a%20new%0Atextual%20prompt%20strategy%2C%20Chain-of-Charts%2C%20tailored%20for%20low-level%20ChartQA%20tasks%2C%0Awhich%20boosts%20performance%20by%2014.41%25%2C%20achieving%20an%20accuracy%20of%2083.58%25.%20Finally%2C%0Aincorporating%20a%20visual%20prompt%20strategy%20that%20directs%20attention%20to%20relevant%0Avisual%20elements%20further%20improves%20accuracy%20to%2084.32%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07001v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartInsights%253A%2520Evaluating%2520Multimodal%2520Large%2520Language%2520Models%2520for%2520Low-Level%250A%2520%2520Chart%2520Question%2520Answering%26entry.906535625%3DYifan%2520Wu%2520and%2520Lutao%2520Yan%2520and%2520Leixian%2520Shen%2520and%2520Yunhai%2520Wang%2520and%2520Nan%2520Tang%2520and%2520Yuyu%2520Luo%26entry.1292438233%3D%2520%2520Chart%2520question%2520answering%2520%2528ChartQA%2529%2520tasks%2520play%2520a%2520critical%2520role%2520in%2520interpreting%250Aand%2520extracting%2520insights%2520from%2520visualization%2520charts.%2520While%2520recent%2520advancements%2520in%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520like%2520GPT-4o%2520have%2520shown%2520promise%2520in%250Ahigh-level%2520ChartQA%2520tasks%252C%2520such%2520as%2520chart%2520captioning%252C%2520their%2520effectiveness%2520in%250Alow-level%2520ChartQA%2520tasks%2520%2528e.g.%252C%2520identifying%2520correlations%2529%2520remains%2520underexplored.%250AIn%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520evaluating%2520MLLMs%2520on%2520low-level%2520ChartQA%250Ausing%2520a%2520newly%2520curated%2520dataset%252C%2520ChartInsights%252C%2520which%2520consists%2520of%252022%252C347%2520%2528chart%252C%250Atask%252C%2520query%252C%2520answer%2529%2520covering%252010%2520data%2520analysis%2520tasks%2520across%25207%2520chart%2520types.%2520We%250Asystematically%2520evaluate%252019%2520advanced%2520MLLMs%252C%2520including%252012%2520open-source%2520and%25207%250Aclosed-source%2520models.%2520The%2520average%2520accuracy%2520rate%2520across%2520these%2520models%2520is%252039.8%2525%252C%250Awith%2520GPT-4o%2520achieving%2520the%2520highest%2520accuracy%2520at%252069.17%2525.%2520To%2520further%2520explore%2520the%250Alimitations%2520of%2520MLLMs%2520in%2520low-level%2520ChartQA%252C%2520we%2520conduct%2520experiments%2520that%2520alter%250Avisual%2520elements%2520of%2520charts%2520%2528e.g.%252C%2520changing%2520color%2520schemes%252C%2520adding%2520image%2520noise%2529%2520to%250Aassess%2520their%2520impact%2520on%2520the%2520task%2520effectiveness.%2520Furthermore%252C%2520we%2520propose%2520a%2520new%250Atextual%2520prompt%2520strategy%252C%2520Chain-of-Charts%252C%2520tailored%2520for%2520low-level%2520ChartQA%2520tasks%252C%250Awhich%2520boosts%2520performance%2520by%252014.41%2525%252C%2520achieving%2520an%2520accuracy%2520of%252083.58%2525.%2520Finally%252C%250Aincorporating%2520a%2520visual%2520prompt%2520strategy%2520that%2520directs%2520attention%2520to%2520relevant%250Avisual%2520elements%2520further%2520improves%2520accuracy%2520to%252084.32%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07001v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartInsights%3A%20Evaluating%20Multimodal%20Large%20Language%20Models%20for%20Low-Level%0A%20%20Chart%20Question%20Answering&entry.906535625=Yifan%20Wu%20and%20Lutao%20Yan%20and%20Leixian%20Shen%20and%20Yunhai%20Wang%20and%20Nan%20Tang%20and%20Yuyu%20Luo&entry.1292438233=%20%20Chart%20question%20answering%20%28ChartQA%29%20tasks%20play%20a%20critical%20role%20in%20interpreting%0Aand%20extracting%20insights%20from%20visualization%20charts.%20While%20recent%20advancements%20in%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20like%20GPT-4o%20have%20shown%20promise%20in%0Ahigh-level%20ChartQA%20tasks%2C%20such%20as%20chart%20captioning%2C%20their%20effectiveness%20in%0Alow-level%20ChartQA%20tasks%20%28e.g.%2C%20identifying%20correlations%29%20remains%20underexplored.%0AIn%20this%20paper%2C%20we%20address%20this%20gap%20by%20evaluating%20MLLMs%20on%20low-level%20ChartQA%0Ausing%20a%20newly%20curated%20dataset%2C%20ChartInsights%2C%20which%20consists%20of%2022%2C347%20%28chart%2C%0Atask%2C%20query%2C%20answer%29%20covering%2010%20data%20analysis%20tasks%20across%207%20chart%20types.%20We%0Asystematically%20evaluate%2019%20advanced%20MLLMs%2C%20including%2012%20open-source%20and%207%0Aclosed-source%20models.%20The%20average%20accuracy%20rate%20across%20these%20models%20is%2039.8%25%2C%0Awith%20GPT-4o%20achieving%20the%20highest%20accuracy%20at%2069.17%25.%20To%20further%20explore%20the%0Alimitations%20of%20MLLMs%20in%20low-level%20ChartQA%2C%20we%20conduct%20experiments%20that%20alter%0Avisual%20elements%20of%20charts%20%28e.g.%2C%20changing%20color%20schemes%2C%20adding%20image%20noise%29%20to%0Aassess%20their%20impact%20on%20the%20task%20effectiveness.%20Furthermore%2C%20we%20propose%20a%20new%0Atextual%20prompt%20strategy%2C%20Chain-of-Charts%2C%20tailored%20for%20low-level%20ChartQA%20tasks%2C%0Awhich%20boosts%20performance%20by%2014.41%25%2C%20achieving%20an%20accuracy%20of%2083.58%25.%20Finally%2C%0Aincorporating%20a%20visual%20prompt%20strategy%20that%20directs%20attention%20to%20relevant%0Avisual%20elements%20further%20improves%20accuracy%20to%2084.32%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07001v4&entry.124074799=Read"},
{"title": "What Really is Commonsense Knowledge?", "author": "Quyet V. Do and Junze Li and Tung-Duong Vuong and Zhaowei Wang and Yangqiu Song and Xiaojuan Ma", "abstract": "  Commonsense datasets have been well developed in Natural Language Processing,\nmainly through crowdsource human annotation. However, there are debates on the\ngenuineness of commonsense reasoning benchmarks. In specific, a significant\nportion of instances in some commonsense benchmarks do not concern commonsense\nknowledge. That problem would undermine the measurement of the true commonsense\nreasoning ability of evaluated models. It is also suggested that the problem\noriginated from a blurry concept of commonsense knowledge, as distinguished\nfrom other types of knowledge. To demystify all of the above claims, in this\nstudy, we survey existing definitions of commonsense knowledge, ground into the\nthree frameworks for defining concepts, and consolidate them into a\nmulti-framework unified definition of commonsense knowledge (so-called\nconsolidated definition). We then use the consolidated definition for\nannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets\nto examine the above claims. Our study shows that there exists a large portion\nof non-commonsense-knowledge instances in the two datasets, and a large\nperformance gap on these two subsets where Large Language Models (LLMs) perform\nworse on commonsense-knowledge instances.\n", "link": "http://arxiv.org/abs/2411.03964v1", "date": "2024-11-06", "relevancy": 2.4831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Really%20is%20Commonsense%20Knowledge%3F&body=Title%3A%20What%20Really%20is%20Commonsense%20Knowledge%3F%0AAuthor%3A%20Quyet%20V.%20Do%20and%20Junze%20Li%20and%20Tung-Duong%20Vuong%20and%20Zhaowei%20Wang%20and%20Yangqiu%20Song%20and%20Xiaojuan%20Ma%0AAbstract%3A%20%20%20Commonsense%20datasets%20have%20been%20well%20developed%20in%20Natural%20Language%20Processing%2C%0Amainly%20through%20crowdsource%20human%20annotation.%20However%2C%20there%20are%20debates%20on%20the%0Agenuineness%20of%20commonsense%20reasoning%20benchmarks.%20In%20specific%2C%20a%20significant%0Aportion%20of%20instances%20in%20some%20commonsense%20benchmarks%20do%20not%20concern%20commonsense%0Aknowledge.%20That%20problem%20would%20undermine%20the%20measurement%20of%20the%20true%20commonsense%0Areasoning%20ability%20of%20evaluated%20models.%20It%20is%20also%20suggested%20that%20the%20problem%0Aoriginated%20from%20a%20blurry%20concept%20of%20commonsense%20knowledge%2C%20as%20distinguished%0Afrom%20other%20types%20of%20knowledge.%20To%20demystify%20all%20of%20the%20above%20claims%2C%20in%20this%0Astudy%2C%20we%20survey%20existing%20definitions%20of%20commonsense%20knowledge%2C%20ground%20into%20the%0Athree%20frameworks%20for%20defining%20concepts%2C%20and%20consolidate%20them%20into%20a%0Amulti-framework%20unified%20definition%20of%20commonsense%20knowledge%20%28so-called%0Aconsolidated%20definition%29.%20We%20then%20use%20the%20consolidated%20definition%20for%0Aannotations%20and%20experiments%20on%20the%20CommonsenseQA%20and%20CommonsenseQA%202.0%20datasets%0Ato%20examine%20the%20above%20claims.%20Our%20study%20shows%20that%20there%20exists%20a%20large%20portion%0Aof%20non-commonsense-knowledge%20instances%20in%20the%20two%20datasets%2C%20and%20a%20large%0Aperformance%20gap%20on%20these%20two%20subsets%20where%20Large%20Language%20Models%20%28LLMs%29%20perform%0Aworse%20on%20commonsense-knowledge%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Really%2520is%2520Commonsense%2520Knowledge%253F%26entry.906535625%3DQuyet%2520V.%2520Do%2520and%2520Junze%2520Li%2520and%2520Tung-Duong%2520Vuong%2520and%2520Zhaowei%2520Wang%2520and%2520Yangqiu%2520Song%2520and%2520Xiaojuan%2520Ma%26entry.1292438233%3D%2520%2520Commonsense%2520datasets%2520have%2520been%2520well%2520developed%2520in%2520Natural%2520Language%2520Processing%252C%250Amainly%2520through%2520crowdsource%2520human%2520annotation.%2520However%252C%2520there%2520are%2520debates%2520on%2520the%250Agenuineness%2520of%2520commonsense%2520reasoning%2520benchmarks.%2520In%2520specific%252C%2520a%2520significant%250Aportion%2520of%2520instances%2520in%2520some%2520commonsense%2520benchmarks%2520do%2520not%2520concern%2520commonsense%250Aknowledge.%2520That%2520problem%2520would%2520undermine%2520the%2520measurement%2520of%2520the%2520true%2520commonsense%250Areasoning%2520ability%2520of%2520evaluated%2520models.%2520It%2520is%2520also%2520suggested%2520that%2520the%2520problem%250Aoriginated%2520from%2520a%2520blurry%2520concept%2520of%2520commonsense%2520knowledge%252C%2520as%2520distinguished%250Afrom%2520other%2520types%2520of%2520knowledge.%2520To%2520demystify%2520all%2520of%2520the%2520above%2520claims%252C%2520in%2520this%250Astudy%252C%2520we%2520survey%2520existing%2520definitions%2520of%2520commonsense%2520knowledge%252C%2520ground%2520into%2520the%250Athree%2520frameworks%2520for%2520defining%2520concepts%252C%2520and%2520consolidate%2520them%2520into%2520a%250Amulti-framework%2520unified%2520definition%2520of%2520commonsense%2520knowledge%2520%2528so-called%250Aconsolidated%2520definition%2529.%2520We%2520then%2520use%2520the%2520consolidated%2520definition%2520for%250Aannotations%2520and%2520experiments%2520on%2520the%2520CommonsenseQA%2520and%2520CommonsenseQA%25202.0%2520datasets%250Ato%2520examine%2520the%2520above%2520claims.%2520Our%2520study%2520shows%2520that%2520there%2520exists%2520a%2520large%2520portion%250Aof%2520non-commonsense-knowledge%2520instances%2520in%2520the%2520two%2520datasets%252C%2520and%2520a%2520large%250Aperformance%2520gap%2520on%2520these%2520two%2520subsets%2520where%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520perform%250Aworse%2520on%2520commonsense-knowledge%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Really%20is%20Commonsense%20Knowledge%3F&entry.906535625=Quyet%20V.%20Do%20and%20Junze%20Li%20and%20Tung-Duong%20Vuong%20and%20Zhaowei%20Wang%20and%20Yangqiu%20Song%20and%20Xiaojuan%20Ma&entry.1292438233=%20%20Commonsense%20datasets%20have%20been%20well%20developed%20in%20Natural%20Language%20Processing%2C%0Amainly%20through%20crowdsource%20human%20annotation.%20However%2C%20there%20are%20debates%20on%20the%0Agenuineness%20of%20commonsense%20reasoning%20benchmarks.%20In%20specific%2C%20a%20significant%0Aportion%20of%20instances%20in%20some%20commonsense%20benchmarks%20do%20not%20concern%20commonsense%0Aknowledge.%20That%20problem%20would%20undermine%20the%20measurement%20of%20the%20true%20commonsense%0Areasoning%20ability%20of%20evaluated%20models.%20It%20is%20also%20suggested%20that%20the%20problem%0Aoriginated%20from%20a%20blurry%20concept%20of%20commonsense%20knowledge%2C%20as%20distinguished%0Afrom%20other%20types%20of%20knowledge.%20To%20demystify%20all%20of%20the%20above%20claims%2C%20in%20this%0Astudy%2C%20we%20survey%20existing%20definitions%20of%20commonsense%20knowledge%2C%20ground%20into%20the%0Athree%20frameworks%20for%20defining%20concepts%2C%20and%20consolidate%20them%20into%20a%0Amulti-framework%20unified%20definition%20of%20commonsense%20knowledge%20%28so-called%0Aconsolidated%20definition%29.%20We%20then%20use%20the%20consolidated%20definition%20for%0Aannotations%20and%20experiments%20on%20the%20CommonsenseQA%20and%20CommonsenseQA%202.0%20datasets%0Ato%20examine%20the%20above%20claims.%20Our%20study%20shows%20that%20there%20exists%20a%20large%20portion%0Aof%20non-commonsense-knowledge%20instances%20in%20the%20two%20datasets%2C%20and%20a%20large%0Aperformance%20gap%20on%20these%20two%20subsets%20where%20Large%20Language%20Models%20%28LLMs%29%20perform%0Aworse%20on%20commonsense-knowledge%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03964v1&entry.124074799=Read"},
{"title": "Self-Consistency Preference Optimization", "author": "Archiki Prasad and Weizhe Yuan and Richard Yuanzhe Pang and Jing Xu and Maryam Fazel-Zarandi and Mohit Bansal and Sainbayar Sukhbaatar and Jason Weston and Jane Yu", "abstract": "  Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku.\n", "link": "http://arxiv.org/abs/2411.04109v1", "date": "2024-11-06", "relevancy": 2.4822, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Consistency%20Preference%20Optimization&body=Title%3A%20Self-Consistency%20Preference%20Optimization%0AAuthor%3A%20Archiki%20Prasad%20and%20Weizhe%20Yuan%20and%20Richard%20Yuanzhe%20Pang%20and%20Jing%20Xu%20and%20Maryam%20Fazel-Zarandi%20and%20Mohit%20Bansal%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%20and%20Jane%20Yu%0AAbstract%3A%20%20%20Self-alignment%2C%20whereby%20models%20learn%20to%20improve%20themselves%20without%20human%0Aannotation%2C%20is%20a%20rapidly%20growing%20research%20area.%20However%2C%20existing%20techniques%0Aoften%20fail%20to%20improve%20complex%20reasoning%20tasks%20due%20to%20the%20difficulty%20of%0Aassigning%20correct%20rewards.%20An%20orthogonal%20approach%20that%20is%20known%20to%20improve%0Acorrectness%20is%20self-consistency%2C%20a%20method%20applied%20at%20inference%20time%20based%20on%0Amultiple%20sampling%20in%20order%20to%20find%20the%20most%20consistent%20answer.%20In%20this%20work%2C%20we%0Aextend%20the%20self-consistency%20concept%20to%20help%20train%20models.%20We%20thus%20introduce%0Aself-consistency%20preference%20optimization%20%28ScPO%29%2C%20which%20iteratively%20trains%0Aconsistent%20answers%20to%20be%20preferred%20over%20inconsistent%20ones%20on%20unsupervised%20new%0Aproblems.%20We%20show%20ScPO%20leads%20to%20large%20improvements%20over%20conventional%20reward%0Amodel%20training%20on%20reasoning%20tasks%20such%20as%20GSM8K%20and%20MATH%2C%20closing%20the%20gap%20with%0Asupervised%20training%20with%20gold%20answers%20or%20preferences%2C%20and%20that%20combining%20ScPO%0Awith%20standard%20supervised%20learning%20improves%20results%20even%20further.%20On%20ZebraLogic%2C%0AScPO%20finetunes%20Llama-3%208B%20to%20be%20superior%20to%20Llama-3%2070B%2C%20Gemma-2%2027B%2C%20and%0AClaude-3%20Haiku.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Consistency%2520Preference%2520Optimization%26entry.906535625%3DArchiki%2520Prasad%2520and%2520Weizhe%2520Yuan%2520and%2520Richard%2520Yuanzhe%2520Pang%2520and%2520Jing%2520Xu%2520and%2520Maryam%2520Fazel-Zarandi%2520and%2520Mohit%2520Bansal%2520and%2520Sainbayar%2520Sukhbaatar%2520and%2520Jason%2520Weston%2520and%2520Jane%2520Yu%26entry.1292438233%3D%2520%2520Self-alignment%252C%2520whereby%2520models%2520learn%2520to%2520improve%2520themselves%2520without%2520human%250Aannotation%252C%2520is%2520a%2520rapidly%2520growing%2520research%2520area.%2520However%252C%2520existing%2520techniques%250Aoften%2520fail%2520to%2520improve%2520complex%2520reasoning%2520tasks%2520due%2520to%2520the%2520difficulty%2520of%250Aassigning%2520correct%2520rewards.%2520An%2520orthogonal%2520approach%2520that%2520is%2520known%2520to%2520improve%250Acorrectness%2520is%2520self-consistency%252C%2520a%2520method%2520applied%2520at%2520inference%2520time%2520based%2520on%250Amultiple%2520sampling%2520in%2520order%2520to%2520find%2520the%2520most%2520consistent%2520answer.%2520In%2520this%2520work%252C%2520we%250Aextend%2520the%2520self-consistency%2520concept%2520to%2520help%2520train%2520models.%2520We%2520thus%2520introduce%250Aself-consistency%2520preference%2520optimization%2520%2528ScPO%2529%252C%2520which%2520iteratively%2520trains%250Aconsistent%2520answers%2520to%2520be%2520preferred%2520over%2520inconsistent%2520ones%2520on%2520unsupervised%2520new%250Aproblems.%2520We%2520show%2520ScPO%2520leads%2520to%2520large%2520improvements%2520over%2520conventional%2520reward%250Amodel%2520training%2520on%2520reasoning%2520tasks%2520such%2520as%2520GSM8K%2520and%2520MATH%252C%2520closing%2520the%2520gap%2520with%250Asupervised%2520training%2520with%2520gold%2520answers%2520or%2520preferences%252C%2520and%2520that%2520combining%2520ScPO%250Awith%2520standard%2520supervised%2520learning%2520improves%2520results%2520even%2520further.%2520On%2520ZebraLogic%252C%250AScPO%2520finetunes%2520Llama-3%25208B%2520to%2520be%2520superior%2520to%2520Llama-3%252070B%252C%2520Gemma-2%252027B%252C%2520and%250AClaude-3%2520Haiku.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Consistency%20Preference%20Optimization&entry.906535625=Archiki%20Prasad%20and%20Weizhe%20Yuan%20and%20Richard%20Yuanzhe%20Pang%20and%20Jing%20Xu%20and%20Maryam%20Fazel-Zarandi%20and%20Mohit%20Bansal%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%20and%20Jane%20Yu&entry.1292438233=%20%20Self-alignment%2C%20whereby%20models%20learn%20to%20improve%20themselves%20without%20human%0Aannotation%2C%20is%20a%20rapidly%20growing%20research%20area.%20However%2C%20existing%20techniques%0Aoften%20fail%20to%20improve%20complex%20reasoning%20tasks%20due%20to%20the%20difficulty%20of%0Aassigning%20correct%20rewards.%20An%20orthogonal%20approach%20that%20is%20known%20to%20improve%0Acorrectness%20is%20self-consistency%2C%20a%20method%20applied%20at%20inference%20time%20based%20on%0Amultiple%20sampling%20in%20order%20to%20find%20the%20most%20consistent%20answer.%20In%20this%20work%2C%20we%0Aextend%20the%20self-consistency%20concept%20to%20help%20train%20models.%20We%20thus%20introduce%0Aself-consistency%20preference%20optimization%20%28ScPO%29%2C%20which%20iteratively%20trains%0Aconsistent%20answers%20to%20be%20preferred%20over%20inconsistent%20ones%20on%20unsupervised%20new%0Aproblems.%20We%20show%20ScPO%20leads%20to%20large%20improvements%20over%20conventional%20reward%0Amodel%20training%20on%20reasoning%20tasks%20such%20as%20GSM8K%20and%20MATH%2C%20closing%20the%20gap%20with%0Asupervised%20training%20with%20gold%20answers%20or%20preferences%2C%20and%20that%20combining%20ScPO%0Awith%20standard%20supervised%20learning%20improves%20results%20even%20further.%20On%20ZebraLogic%2C%0AScPO%20finetunes%20Llama-3%208B%20to%20be%20superior%20to%20Llama-3%2070B%2C%20Gemma-2%2027B%2C%20and%0AClaude-3%20Haiku.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04109v1&entry.124074799=Read"},
{"title": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System", "author": "David Maria Schmidt and Mohammad Fazleh Elahi and Philipp Cimiano", "abstract": "  In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality.\n", "link": "http://arxiv.org/abs/2411.03906v1", "date": "2024-11-06", "relevancy": 2.474, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lexicalization%20Is%20All%20You%20Need%3A%20Examining%20the%20Impact%20of%20Lexical%0A%20%20Knowledge%20in%20a%20Compositional%20QALD%20System&body=Title%3A%20Lexicalization%20Is%20All%20You%20Need%3A%20Examining%20the%20Impact%20of%20Lexical%0A%20%20Knowledge%20in%20a%20Compositional%20QALD%20System%0AAuthor%3A%20David%20Maria%20Schmidt%20and%20Mohammad%20Fazleh%20Elahi%20and%20Philipp%20Cimiano%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20examine%20the%20impact%20of%20lexicalization%20on%20Question%20Answering%0Aover%20Linked%20Data%20%28QALD%29.%20It%20is%20well%20known%20that%20one%20of%20the%20key%20challenges%20in%0Ainterpreting%20natural%20language%20questions%20with%20respect%20to%20SPARQL%20lies%20in%20bridging%0Athe%20lexical%20gap%2C%20that%20is%20mapping%20the%20words%20in%20the%20query%20to%20the%20correct%0Avocabulary%20elements.%20We%20argue%20in%20this%20paper%20that%20lexicalization%2C%20that%20is%0Aexplicit%20knowledge%20about%20the%20potential%20interpretations%20of%20a%20word%20with%20respect%0Ato%20the%20given%20vocabulary%2C%20significantly%20eases%20the%20task%20and%20increases%20the%0Aperformance%20of%20QA%20systems.%20Towards%20this%20goal%2C%20we%20present%20a%20compositional%20QA%0Asystem%20that%20can%20leverage%20explicit%20lexical%20knowledge%20in%20a%20compositional%20manner%0Ato%20infer%20the%20meaning%20of%20a%20question%20in%20terms%20of%20a%20SPARQL%20query.%20We%20show%20that%0Asuch%20a%20system%2C%20given%20lexical%20knowledge%2C%20has%20a%20performance%20well%20beyond%20current%0AQA%20systems%2C%20achieving%20up%20to%20a%20%2435.8%5C%25%24%20increase%20in%20the%20micro%20%24F_1%24%20score%0Acompared%20to%20the%20best%20QA%20system%20on%20QALD-9.%20This%20shows%20the%20importance%20and%0Apotential%20of%20including%20explicit%20lexical%20knowledge.%20In%20contrast%2C%20we%20show%20that%0ALLMs%20have%20limited%20abilities%20to%20exploit%20lexical%20knowledge%2C%20with%20only%20marginal%0Aimprovements%20compared%20to%20a%20version%20without%20lexical%20knowledge.%20This%20shows%20that%0ALLMs%20have%20no%20ability%20to%20compositionally%20interpret%20a%20question%20on%20the%20basis%20of%0Athe%20meaning%20of%20its%20parts%2C%20a%20key%20feature%20of%20compositional%20approaches.%20Taken%0Atogether%2C%20our%20work%20shows%20new%20avenues%20for%20QALD%20research%2C%20emphasizing%20the%0Aimportance%20of%20lexicalization%20and%20compositionality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLexicalization%2520Is%2520All%2520You%2520Need%253A%2520Examining%2520the%2520Impact%2520of%2520Lexical%250A%2520%2520Knowledge%2520in%2520a%2520Compositional%2520QALD%2520System%26entry.906535625%3DDavid%2520Maria%2520Schmidt%2520and%2520Mohammad%2520Fazleh%2520Elahi%2520and%2520Philipp%2520Cimiano%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520examine%2520the%2520impact%2520of%2520lexicalization%2520on%2520Question%2520Answering%250Aover%2520Linked%2520Data%2520%2528QALD%2529.%2520It%2520is%2520well%2520known%2520that%2520one%2520of%2520the%2520key%2520challenges%2520in%250Ainterpreting%2520natural%2520language%2520questions%2520with%2520respect%2520to%2520SPARQL%2520lies%2520in%2520bridging%250Athe%2520lexical%2520gap%252C%2520that%2520is%2520mapping%2520the%2520words%2520in%2520the%2520query%2520to%2520the%2520correct%250Avocabulary%2520elements.%2520We%2520argue%2520in%2520this%2520paper%2520that%2520lexicalization%252C%2520that%2520is%250Aexplicit%2520knowledge%2520about%2520the%2520potential%2520interpretations%2520of%2520a%2520word%2520with%2520respect%250Ato%2520the%2520given%2520vocabulary%252C%2520significantly%2520eases%2520the%2520task%2520and%2520increases%2520the%250Aperformance%2520of%2520QA%2520systems.%2520Towards%2520this%2520goal%252C%2520we%2520present%2520a%2520compositional%2520QA%250Asystem%2520that%2520can%2520leverage%2520explicit%2520lexical%2520knowledge%2520in%2520a%2520compositional%2520manner%250Ato%2520infer%2520the%2520meaning%2520of%2520a%2520question%2520in%2520terms%2520of%2520a%2520SPARQL%2520query.%2520We%2520show%2520that%250Asuch%2520a%2520system%252C%2520given%2520lexical%2520knowledge%252C%2520has%2520a%2520performance%2520well%2520beyond%2520current%250AQA%2520systems%252C%2520achieving%2520up%2520to%2520a%2520%252435.8%255C%2525%2524%2520increase%2520in%2520the%2520micro%2520%2524F_1%2524%2520score%250Acompared%2520to%2520the%2520best%2520QA%2520system%2520on%2520QALD-9.%2520This%2520shows%2520the%2520importance%2520and%250Apotential%2520of%2520including%2520explicit%2520lexical%2520knowledge.%2520In%2520contrast%252C%2520we%2520show%2520that%250ALLMs%2520have%2520limited%2520abilities%2520to%2520exploit%2520lexical%2520knowledge%252C%2520with%2520only%2520marginal%250Aimprovements%2520compared%2520to%2520a%2520version%2520without%2520lexical%2520knowledge.%2520This%2520shows%2520that%250ALLMs%2520have%2520no%2520ability%2520to%2520compositionally%2520interpret%2520a%2520question%2520on%2520the%2520basis%2520of%250Athe%2520meaning%2520of%2520its%2520parts%252C%2520a%2520key%2520feature%2520of%2520compositional%2520approaches.%2520Taken%250Atogether%252C%2520our%2520work%2520shows%2520new%2520avenues%2520for%2520QALD%2520research%252C%2520emphasizing%2520the%250Aimportance%2520of%2520lexicalization%2520and%2520compositionality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lexicalization%20Is%20All%20You%20Need%3A%20Examining%20the%20Impact%20of%20Lexical%0A%20%20Knowledge%20in%20a%20Compositional%20QALD%20System&entry.906535625=David%20Maria%20Schmidt%20and%20Mohammad%20Fazleh%20Elahi%20and%20Philipp%20Cimiano&entry.1292438233=%20%20In%20this%20paper%2C%20we%20examine%20the%20impact%20of%20lexicalization%20on%20Question%20Answering%0Aover%20Linked%20Data%20%28QALD%29.%20It%20is%20well%20known%20that%20one%20of%20the%20key%20challenges%20in%0Ainterpreting%20natural%20language%20questions%20with%20respect%20to%20SPARQL%20lies%20in%20bridging%0Athe%20lexical%20gap%2C%20that%20is%20mapping%20the%20words%20in%20the%20query%20to%20the%20correct%0Avocabulary%20elements.%20We%20argue%20in%20this%20paper%20that%20lexicalization%2C%20that%20is%0Aexplicit%20knowledge%20about%20the%20potential%20interpretations%20of%20a%20word%20with%20respect%0Ato%20the%20given%20vocabulary%2C%20significantly%20eases%20the%20task%20and%20increases%20the%0Aperformance%20of%20QA%20systems.%20Towards%20this%20goal%2C%20we%20present%20a%20compositional%20QA%0Asystem%20that%20can%20leverage%20explicit%20lexical%20knowledge%20in%20a%20compositional%20manner%0Ato%20infer%20the%20meaning%20of%20a%20question%20in%20terms%20of%20a%20SPARQL%20query.%20We%20show%20that%0Asuch%20a%20system%2C%20given%20lexical%20knowledge%2C%20has%20a%20performance%20well%20beyond%20current%0AQA%20systems%2C%20achieving%20up%20to%20a%20%2435.8%5C%25%24%20increase%20in%20the%20micro%20%24F_1%24%20score%0Acompared%20to%20the%20best%20QA%20system%20on%20QALD-9.%20This%20shows%20the%20importance%20and%0Apotential%20of%20including%20explicit%20lexical%20knowledge.%20In%20contrast%2C%20we%20show%20that%0ALLMs%20have%20limited%20abilities%20to%20exploit%20lexical%20knowledge%2C%20with%20only%20marginal%0Aimprovements%20compared%20to%20a%20version%20without%20lexical%20knowledge.%20This%20shows%20that%0ALLMs%20have%20no%20ability%20to%20compositionally%20interpret%20a%20question%20on%20the%20basis%20of%0Athe%20meaning%20of%20its%20parts%2C%20a%20key%20feature%20of%20compositional%20approaches.%20Taken%0Atogether%2C%20our%20work%20shows%20new%20avenues%20for%20QALD%20research%2C%20emphasizing%20the%0Aimportance%20of%20lexicalization%20and%20compositionality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03906v1&entry.124074799=Read"},
{"title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration", "author": "Aofeng Su and Aowen Wang and Chao Ye and Chen Zhou and Ga Zhang and Guangcheng Zhu and Haobo Wang and Haokai Xu and Hao Chen and Haoze Li and Haoxuan Lan and Jiaming Tian and Jing Yuan and Junbo Zhao and Junlin Zhou and Kaizhe Shou and Liangyu Zha and Lin Long and Liyao Li and Pengzuo Wu and Qi Zhang and Qingyi Huang and Saisai Yang and Tao Zhang and Wentao Ye and Wufang Zhu and Xiaomeng Hu and Xijun Gu and Xinjie Sun and Xiang Li and Yuhang Yang and Zhiqing Xiao", "abstract": "  The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.\n", "link": "http://arxiv.org/abs/2411.02059v2", "date": "2024-11-06", "relevancy": 2.4687, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TableGPT2%3A%20A%20Large%20Multimodal%20Model%20with%20Tabular%20Data%20Integration&body=Title%3A%20TableGPT2%3A%20A%20Large%20Multimodal%20Model%20with%20Tabular%20Data%20Integration%0AAuthor%3A%20Aofeng%20Su%20and%20Aowen%20Wang%20and%20Chao%20Ye%20and%20Chen%20Zhou%20and%20Ga%20Zhang%20and%20Guangcheng%20Zhu%20and%20Haobo%20Wang%20and%20Haokai%20Xu%20and%20Hao%20Chen%20and%20Haoze%20Li%20and%20Haoxuan%20Lan%20and%20Jiaming%20Tian%20and%20Jing%20Yuan%20and%20Junbo%20Zhao%20and%20Junlin%20Zhou%20and%20Kaizhe%20Shou%20and%20Liangyu%20Zha%20and%20Lin%20Long%20and%20Liyao%20Li%20and%20Pengzuo%20Wu%20and%20Qi%20Zhang%20and%20Qingyi%20Huang%20and%20Saisai%20Yang%20and%20Tao%20Zhang%20and%20Wentao%20Ye%20and%20Wufang%20Zhu%20and%20Xiaomeng%20Hu%20and%20Xijun%20Gu%20and%20Xinjie%20Sun%20and%20Xiang%20Li%20and%20Yuhang%20Yang%20and%20Zhiqing%20Xiao%0AAbstract%3A%20%20%20The%20emergence%20of%20models%20like%20GPTs%2C%20Claude%2C%20LLaMA%2C%20and%20Qwen%20has%20reshaped%20AI%0Aapplications%2C%20presenting%20vast%20new%20opportunities%20across%20industries.%20Yet%2C%20the%0Aintegration%20of%20tabular%20data%20remains%20notably%20underdeveloped%2C%20despite%20its%0Afoundational%20role%20in%20numerous%20real-world%20domains.%0A%20%20This%20gap%20is%20critical%20for%20three%20main%20reasons.%20First%2C%20database%20or%20data%0Awarehouse%20data%20integration%20is%20essential%20for%20advanced%20applications%3B%20second%2C%20the%0Avast%20and%20largely%20untapped%20resource%20of%20tabular%20data%20offers%20immense%20potential%20for%0Aanalysis%3B%20and%20third%2C%20the%20business%20intelligence%20domain%20specifically%20demands%0Aadaptable%2C%20precise%20solutions%20that%20many%20current%20LLMs%20may%20struggle%20to%20provide.%0A%20%20In%20response%2C%20we%20introduce%20TableGPT2%2C%20a%20model%20rigorously%20pre-trained%20and%0Afine-tuned%20with%20over%20593.8K%20tables%20and%202.36M%20high-quality%20query-table-output%0Atuples%2C%20a%20scale%20of%20table-related%20data%20unprecedented%20in%20prior%20research.%20This%0Aextensive%20training%20enables%20TableGPT2%20to%20excel%20in%20table-centric%20tasks%20while%0Amaintaining%20strong%20general%20language%20and%20coding%20abilities.%0A%20%20One%20of%20TableGPT2%27s%20key%20innovations%20is%20its%20novel%20table%20encoder%2C%20specifically%0Adesigned%20to%20capture%20schema-level%20and%20cell-level%20information.%20This%20encoder%0Astrengthens%20the%20model%27s%20ability%20to%20handle%20ambiguous%20queries%2C%20missing%20column%0Anames%2C%20and%20irregular%20tables%20commonly%20encountered%20in%20real-world%20applications.%0ASimilar%20to%20visual%20language%20models%2C%20this%20pioneering%20approach%20integrates%20with%20the%0Adecoder%20to%20form%20a%20robust%20large%20multimodal%20model.%0A%20%20We%20believe%20the%20results%20are%20compelling%3A%20over%2023%20benchmarking%20metrics%2C%0ATableGPT2%20achieves%20an%20average%20performance%20improvement%20of%2035.20%25%20in%20the%207B%20model%0Aand%2049.32%25%20in%20the%2072B%20model%20over%20prior%20benchmark-neutral%20LLMs%2C%20with%20robust%0Ageneral-purpose%20capabilities%20intact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTableGPT2%253A%2520A%2520Large%2520Multimodal%2520Model%2520with%2520Tabular%2520Data%2520Integration%26entry.906535625%3DAofeng%2520Su%2520and%2520Aowen%2520Wang%2520and%2520Chao%2520Ye%2520and%2520Chen%2520Zhou%2520and%2520Ga%2520Zhang%2520and%2520Guangcheng%2520Zhu%2520and%2520Haobo%2520Wang%2520and%2520Haokai%2520Xu%2520and%2520Hao%2520Chen%2520and%2520Haoze%2520Li%2520and%2520Haoxuan%2520Lan%2520and%2520Jiaming%2520Tian%2520and%2520Jing%2520Yuan%2520and%2520Junbo%2520Zhao%2520and%2520Junlin%2520Zhou%2520and%2520Kaizhe%2520Shou%2520and%2520Liangyu%2520Zha%2520and%2520Lin%2520Long%2520and%2520Liyao%2520Li%2520and%2520Pengzuo%2520Wu%2520and%2520Qi%2520Zhang%2520and%2520Qingyi%2520Huang%2520and%2520Saisai%2520Yang%2520and%2520Tao%2520Zhang%2520and%2520Wentao%2520Ye%2520and%2520Wufang%2520Zhu%2520and%2520Xiaomeng%2520Hu%2520and%2520Xijun%2520Gu%2520and%2520Xinjie%2520Sun%2520and%2520Xiang%2520Li%2520and%2520Yuhang%2520Yang%2520and%2520Zhiqing%2520Xiao%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520models%2520like%2520GPTs%252C%2520Claude%252C%2520LLaMA%252C%2520and%2520Qwen%2520has%2520reshaped%2520AI%250Aapplications%252C%2520presenting%2520vast%2520new%2520opportunities%2520across%2520industries.%2520Yet%252C%2520the%250Aintegration%2520of%2520tabular%2520data%2520remains%2520notably%2520underdeveloped%252C%2520despite%2520its%250Afoundational%2520role%2520in%2520numerous%2520real-world%2520domains.%250A%2520%2520This%2520gap%2520is%2520critical%2520for%2520three%2520main%2520reasons.%2520First%252C%2520database%2520or%2520data%250Awarehouse%2520data%2520integration%2520is%2520essential%2520for%2520advanced%2520applications%253B%2520second%252C%2520the%250Avast%2520and%2520largely%2520untapped%2520resource%2520of%2520tabular%2520data%2520offers%2520immense%2520potential%2520for%250Aanalysis%253B%2520and%2520third%252C%2520the%2520business%2520intelligence%2520domain%2520specifically%2520demands%250Aadaptable%252C%2520precise%2520solutions%2520that%2520many%2520current%2520LLMs%2520may%2520struggle%2520to%2520provide.%250A%2520%2520In%2520response%252C%2520we%2520introduce%2520TableGPT2%252C%2520a%2520model%2520rigorously%2520pre-trained%2520and%250Afine-tuned%2520with%2520over%2520593.8K%2520tables%2520and%25202.36M%2520high-quality%2520query-table-output%250Atuples%252C%2520a%2520scale%2520of%2520table-related%2520data%2520unprecedented%2520in%2520prior%2520research.%2520This%250Aextensive%2520training%2520enables%2520TableGPT2%2520to%2520excel%2520in%2520table-centric%2520tasks%2520while%250Amaintaining%2520strong%2520general%2520language%2520and%2520coding%2520abilities.%250A%2520%2520One%2520of%2520TableGPT2%2527s%2520key%2520innovations%2520is%2520its%2520novel%2520table%2520encoder%252C%2520specifically%250Adesigned%2520to%2520capture%2520schema-level%2520and%2520cell-level%2520information.%2520This%2520encoder%250Astrengthens%2520the%2520model%2527s%2520ability%2520to%2520handle%2520ambiguous%2520queries%252C%2520missing%2520column%250Anames%252C%2520and%2520irregular%2520tables%2520commonly%2520encountered%2520in%2520real-world%2520applications.%250ASimilar%2520to%2520visual%2520language%2520models%252C%2520this%2520pioneering%2520approach%2520integrates%2520with%2520the%250Adecoder%2520to%2520form%2520a%2520robust%2520large%2520multimodal%2520model.%250A%2520%2520We%2520believe%2520the%2520results%2520are%2520compelling%253A%2520over%252023%2520benchmarking%2520metrics%252C%250ATableGPT2%2520achieves%2520an%2520average%2520performance%2520improvement%2520of%252035.20%2525%2520in%2520the%25207B%2520model%250Aand%252049.32%2525%2520in%2520the%252072B%2520model%2520over%2520prior%2520benchmark-neutral%2520LLMs%252C%2520with%2520robust%250Ageneral-purpose%2520capabilities%2520intact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TableGPT2%3A%20A%20Large%20Multimodal%20Model%20with%20Tabular%20Data%20Integration&entry.906535625=Aofeng%20Su%20and%20Aowen%20Wang%20and%20Chao%20Ye%20and%20Chen%20Zhou%20and%20Ga%20Zhang%20and%20Guangcheng%20Zhu%20and%20Haobo%20Wang%20and%20Haokai%20Xu%20and%20Hao%20Chen%20and%20Haoze%20Li%20and%20Haoxuan%20Lan%20and%20Jiaming%20Tian%20and%20Jing%20Yuan%20and%20Junbo%20Zhao%20and%20Junlin%20Zhou%20and%20Kaizhe%20Shou%20and%20Liangyu%20Zha%20and%20Lin%20Long%20and%20Liyao%20Li%20and%20Pengzuo%20Wu%20and%20Qi%20Zhang%20and%20Qingyi%20Huang%20and%20Saisai%20Yang%20and%20Tao%20Zhang%20and%20Wentao%20Ye%20and%20Wufang%20Zhu%20and%20Xiaomeng%20Hu%20and%20Xijun%20Gu%20and%20Xinjie%20Sun%20and%20Xiang%20Li%20and%20Yuhang%20Yang%20and%20Zhiqing%20Xiao&entry.1292438233=%20%20The%20emergence%20of%20models%20like%20GPTs%2C%20Claude%2C%20LLaMA%2C%20and%20Qwen%20has%20reshaped%20AI%0Aapplications%2C%20presenting%20vast%20new%20opportunities%20across%20industries.%20Yet%2C%20the%0Aintegration%20of%20tabular%20data%20remains%20notably%20underdeveloped%2C%20despite%20its%0Afoundational%20role%20in%20numerous%20real-world%20domains.%0A%20%20This%20gap%20is%20critical%20for%20three%20main%20reasons.%20First%2C%20database%20or%20data%0Awarehouse%20data%20integration%20is%20essential%20for%20advanced%20applications%3B%20second%2C%20the%0Avast%20and%20largely%20untapped%20resource%20of%20tabular%20data%20offers%20immense%20potential%20for%0Aanalysis%3B%20and%20third%2C%20the%20business%20intelligence%20domain%20specifically%20demands%0Aadaptable%2C%20precise%20solutions%20that%20many%20current%20LLMs%20may%20struggle%20to%20provide.%0A%20%20In%20response%2C%20we%20introduce%20TableGPT2%2C%20a%20model%20rigorously%20pre-trained%20and%0Afine-tuned%20with%20over%20593.8K%20tables%20and%202.36M%20high-quality%20query-table-output%0Atuples%2C%20a%20scale%20of%20table-related%20data%20unprecedented%20in%20prior%20research.%20This%0Aextensive%20training%20enables%20TableGPT2%20to%20excel%20in%20table-centric%20tasks%20while%0Amaintaining%20strong%20general%20language%20and%20coding%20abilities.%0A%20%20One%20of%20TableGPT2%27s%20key%20innovations%20is%20its%20novel%20table%20encoder%2C%20specifically%0Adesigned%20to%20capture%20schema-level%20and%20cell-level%20information.%20This%20encoder%0Astrengthens%20the%20model%27s%20ability%20to%20handle%20ambiguous%20queries%2C%20missing%20column%0Anames%2C%20and%20irregular%20tables%20commonly%20encountered%20in%20real-world%20applications.%0ASimilar%20to%20visual%20language%20models%2C%20this%20pioneering%20approach%20integrates%20with%20the%0Adecoder%20to%20form%20a%20robust%20large%20multimodal%20model.%0A%20%20We%20believe%20the%20results%20are%20compelling%3A%20over%2023%20benchmarking%20metrics%2C%0ATableGPT2%20achieves%20an%20average%20performance%20improvement%20of%2035.20%25%20in%20the%207B%20model%0Aand%2049.32%25%20in%20the%2072B%20model%20over%20prior%20benchmark-neutral%20LLMs%2C%20with%20robust%0Ageneral-purpose%20capabilities%20intact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02059v2&entry.124074799=Read"},
{"title": "Object-Centric Dexterous Manipulation from Human Motion Data", "author": "Yuanpei Chen and Chen Wang and Yaodong Yang and C. Karen Liu", "abstract": "  Manipulating objects to achieve desired goal states is a basic but important\nskill for dexterous manipulation. Human hand motions demonstrate proficient\nmanipulation capability, providing valuable data for training robots with\nmulti-finger hands. Despite this potential, substantial challenges arise due to\nthe embodiment gap between human and robot hands. In this work, we introduce a\nhierarchical policy learning framework that uses human hand motion data for\ntraining object-centric dexterous robot manipulation. At the core of our method\nis a high-level trajectory generative model, learned with a large-scale human\nhand motion capture dataset, to synthesize human-like wrist motions conditioned\non the desired object goal states. Guided by the generated wrist motions, deep\nreinforcement learning is further used to train a low-level finger controller\nthat is grounded in the robot's embodiment to physically interact with the\nobject to achieve the goal. Through extensive evaluation across 10 household\nobjects, our approach not only demonstrates superior performance but also\nshowcases generalization capability to novel object geometries and goal states.\nFurthermore, we transfer the learned policies from simulation to a real-world\nbimanual dexterous robot system, further demonstrating its applicability in\nreal-world scenarios. Project website:\nhttps://cypypccpy.github.io/obj-dex.github.io/.\n", "link": "http://arxiv.org/abs/2411.04005v1", "date": "2024-11-06", "relevancy": 2.4581, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6437}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6167}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Centric%20Dexterous%20Manipulation%20from%20Human%20Motion%20Data&body=Title%3A%20Object-Centric%20Dexterous%20Manipulation%20from%20Human%20Motion%20Data%0AAuthor%3A%20Yuanpei%20Chen%20and%20Chen%20Wang%20and%20Yaodong%20Yang%20and%20C.%20Karen%20Liu%0AAbstract%3A%20%20%20Manipulating%20objects%20to%20achieve%20desired%20goal%20states%20is%20a%20basic%20but%20important%0Askill%20for%20dexterous%20manipulation.%20Human%20hand%20motions%20demonstrate%20proficient%0Amanipulation%20capability%2C%20providing%20valuable%20data%20for%20training%20robots%20with%0Amulti-finger%20hands.%20Despite%20this%20potential%2C%20substantial%20challenges%20arise%20due%20to%0Athe%20embodiment%20gap%20between%20human%20and%20robot%20hands.%20In%20this%20work%2C%20we%20introduce%20a%0Ahierarchical%20policy%20learning%20framework%20that%20uses%20human%20hand%20motion%20data%20for%0Atraining%20object-centric%20dexterous%20robot%20manipulation.%20At%20the%20core%20of%20our%20method%0Ais%20a%20high-level%20trajectory%20generative%20model%2C%20learned%20with%20a%20large-scale%20human%0Ahand%20motion%20capture%20dataset%2C%20to%20synthesize%20human-like%20wrist%20motions%20conditioned%0Aon%20the%20desired%20object%20goal%20states.%20Guided%20by%20the%20generated%20wrist%20motions%2C%20deep%0Areinforcement%20learning%20is%20further%20used%20to%20train%20a%20low-level%20finger%20controller%0Athat%20is%20grounded%20in%20the%20robot%27s%20embodiment%20to%20physically%20interact%20with%20the%0Aobject%20to%20achieve%20the%20goal.%20Through%20extensive%20evaluation%20across%2010%20household%0Aobjects%2C%20our%20approach%20not%20only%20demonstrates%20superior%20performance%20but%20also%0Ashowcases%20generalization%20capability%20to%20novel%20object%20geometries%20and%20goal%20states.%0AFurthermore%2C%20we%20transfer%20the%20learned%20policies%20from%20simulation%20to%20a%20real-world%0Abimanual%20dexterous%20robot%20system%2C%20further%20demonstrating%20its%20applicability%20in%0Areal-world%20scenarios.%20Project%20website%3A%0Ahttps%3A//cypypccpy.github.io/obj-dex.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Centric%2520Dexterous%2520Manipulation%2520from%2520Human%2520Motion%2520Data%26entry.906535625%3DYuanpei%2520Chen%2520and%2520Chen%2520Wang%2520and%2520Yaodong%2520Yang%2520and%2520C.%2520Karen%2520Liu%26entry.1292438233%3D%2520%2520Manipulating%2520objects%2520to%2520achieve%2520desired%2520goal%2520states%2520is%2520a%2520basic%2520but%2520important%250Askill%2520for%2520dexterous%2520manipulation.%2520Human%2520hand%2520motions%2520demonstrate%2520proficient%250Amanipulation%2520capability%252C%2520providing%2520valuable%2520data%2520for%2520training%2520robots%2520with%250Amulti-finger%2520hands.%2520Despite%2520this%2520potential%252C%2520substantial%2520challenges%2520arise%2520due%2520to%250Athe%2520embodiment%2520gap%2520between%2520human%2520and%2520robot%2520hands.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Ahierarchical%2520policy%2520learning%2520framework%2520that%2520uses%2520human%2520hand%2520motion%2520data%2520for%250Atraining%2520object-centric%2520dexterous%2520robot%2520manipulation.%2520At%2520the%2520core%2520of%2520our%2520method%250Ais%2520a%2520high-level%2520trajectory%2520generative%2520model%252C%2520learned%2520with%2520a%2520large-scale%2520human%250Ahand%2520motion%2520capture%2520dataset%252C%2520to%2520synthesize%2520human-like%2520wrist%2520motions%2520conditioned%250Aon%2520the%2520desired%2520object%2520goal%2520states.%2520Guided%2520by%2520the%2520generated%2520wrist%2520motions%252C%2520deep%250Areinforcement%2520learning%2520is%2520further%2520used%2520to%2520train%2520a%2520low-level%2520finger%2520controller%250Athat%2520is%2520grounded%2520in%2520the%2520robot%2527s%2520embodiment%2520to%2520physically%2520interact%2520with%2520the%250Aobject%2520to%2520achieve%2520the%2520goal.%2520Through%2520extensive%2520evaluation%2520across%252010%2520household%250Aobjects%252C%2520our%2520approach%2520not%2520only%2520demonstrates%2520superior%2520performance%2520but%2520also%250Ashowcases%2520generalization%2520capability%2520to%2520novel%2520object%2520geometries%2520and%2520goal%2520states.%250AFurthermore%252C%2520we%2520transfer%2520the%2520learned%2520policies%2520from%2520simulation%2520to%2520a%2520real-world%250Abimanual%2520dexterous%2520robot%2520system%252C%2520further%2520demonstrating%2520its%2520applicability%2520in%250Areal-world%2520scenarios.%2520Project%2520website%253A%250Ahttps%253A//cypypccpy.github.io/obj-dex.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Centric%20Dexterous%20Manipulation%20from%20Human%20Motion%20Data&entry.906535625=Yuanpei%20Chen%20and%20Chen%20Wang%20and%20Yaodong%20Yang%20and%20C.%20Karen%20Liu&entry.1292438233=%20%20Manipulating%20objects%20to%20achieve%20desired%20goal%20states%20is%20a%20basic%20but%20important%0Askill%20for%20dexterous%20manipulation.%20Human%20hand%20motions%20demonstrate%20proficient%0Amanipulation%20capability%2C%20providing%20valuable%20data%20for%20training%20robots%20with%0Amulti-finger%20hands.%20Despite%20this%20potential%2C%20substantial%20challenges%20arise%20due%20to%0Athe%20embodiment%20gap%20between%20human%20and%20robot%20hands.%20In%20this%20work%2C%20we%20introduce%20a%0Ahierarchical%20policy%20learning%20framework%20that%20uses%20human%20hand%20motion%20data%20for%0Atraining%20object-centric%20dexterous%20robot%20manipulation.%20At%20the%20core%20of%20our%20method%0Ais%20a%20high-level%20trajectory%20generative%20model%2C%20learned%20with%20a%20large-scale%20human%0Ahand%20motion%20capture%20dataset%2C%20to%20synthesize%20human-like%20wrist%20motions%20conditioned%0Aon%20the%20desired%20object%20goal%20states.%20Guided%20by%20the%20generated%20wrist%20motions%2C%20deep%0Areinforcement%20learning%20is%20further%20used%20to%20train%20a%20low-level%20finger%20controller%0Athat%20is%20grounded%20in%20the%20robot%27s%20embodiment%20to%20physically%20interact%20with%20the%0Aobject%20to%20achieve%20the%20goal.%20Through%20extensive%20evaluation%20across%2010%20household%0Aobjects%2C%20our%20approach%20not%20only%20demonstrates%20superior%20performance%20but%20also%0Ashowcases%20generalization%20capability%20to%20novel%20object%20geometries%20and%20goal%20states.%0AFurthermore%2C%20we%20transfer%20the%20learned%20policies%20from%20simulation%20to%20a%20real-world%0Abimanual%20dexterous%20robot%20system%2C%20further%20demonstrating%20its%20applicability%20in%0Areal-world%20scenarios.%20Project%20website%3A%0Ahttps%3A//cypypccpy.github.io/obj-dex.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04005v1&entry.124074799=Read"},
{"title": "DexDiffuser: Generating Dexterous Grasps with Diffusion Models", "author": "Zehang Weng and Haofei Lu and Danica Kragic and Jens Lundell", "abstract": "  We introduce DexDiffuser, a novel dexterous grasping method that generates,\nevaluates, and refines grasps on partial object point clouds. DexDiffuser\nincludes the conditional diffusion-based grasp sampler DexSampler and the\ndexterous grasp evaluator DexEvaluator. DexSampler generates high-quality\ngrasps conditioned on object point clouds by iterative denoising of randomly\nsampled grasps. We also introduce two grasp refinement strategies:\nEvaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR).\nThe experiment results demonstrate that DexDiffuser consistently outperforms\nthe state-of-the-art multi-finger grasp generation method FFHNet with an, on\naverage, 9.12% and 19.44% higher grasp success rate in simulation and real\nrobot experiments, respectively. Supplementary materials are available at\nhttps://yulihn.github.io/DexDiffuser_page/\n", "link": "http://arxiv.org/abs/2402.02989v3", "date": "2024-11-06", "relevancy": 2.4552, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7236}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5601}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexDiffuser%3A%20Generating%20Dexterous%20Grasps%20with%20Diffusion%20Models&body=Title%3A%20DexDiffuser%3A%20Generating%20Dexterous%20Grasps%20with%20Diffusion%20Models%0AAuthor%3A%20Zehang%20Weng%20and%20Haofei%20Lu%20and%20Danica%20Kragic%20and%20Jens%20Lundell%0AAbstract%3A%20%20%20We%20introduce%20DexDiffuser%2C%20a%20novel%20dexterous%20grasping%20method%20that%20generates%2C%0Aevaluates%2C%20and%20refines%20grasps%20on%20partial%20object%20point%20clouds.%20DexDiffuser%0Aincludes%20the%20conditional%20diffusion-based%20grasp%20sampler%20DexSampler%20and%20the%0Adexterous%20grasp%20evaluator%20DexEvaluator.%20DexSampler%20generates%20high-quality%0Agrasps%20conditioned%20on%20object%20point%20clouds%20by%20iterative%20denoising%20of%20randomly%0Asampled%20grasps.%20We%20also%20introduce%20two%20grasp%20refinement%20strategies%3A%0AEvaluator-Guided%20Diffusion%20%28EGD%29%20and%20Evaluator-based%20Sampling%20Refinement%20%28ESR%29.%0AThe%20experiment%20results%20demonstrate%20that%20DexDiffuser%20consistently%20outperforms%0Athe%20state-of-the-art%20multi-finger%20grasp%20generation%20method%20FFHNet%20with%20an%2C%20on%0Aaverage%2C%209.12%25%20and%2019.44%25%20higher%20grasp%20success%20rate%20in%20simulation%20and%20real%0Arobot%20experiments%2C%20respectively.%20Supplementary%20materials%20are%20available%20at%0Ahttps%3A//yulihn.github.io/DexDiffuser_page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexDiffuser%253A%2520Generating%2520Dexterous%2520Grasps%2520with%2520Diffusion%2520Models%26entry.906535625%3DZehang%2520Weng%2520and%2520Haofei%2520Lu%2520and%2520Danica%2520Kragic%2520and%2520Jens%2520Lundell%26entry.1292438233%3D%2520%2520We%2520introduce%2520DexDiffuser%252C%2520a%2520novel%2520dexterous%2520grasping%2520method%2520that%2520generates%252C%250Aevaluates%252C%2520and%2520refines%2520grasps%2520on%2520partial%2520object%2520point%2520clouds.%2520DexDiffuser%250Aincludes%2520the%2520conditional%2520diffusion-based%2520grasp%2520sampler%2520DexSampler%2520and%2520the%250Adexterous%2520grasp%2520evaluator%2520DexEvaluator.%2520DexSampler%2520generates%2520high-quality%250Agrasps%2520conditioned%2520on%2520object%2520point%2520clouds%2520by%2520iterative%2520denoising%2520of%2520randomly%250Asampled%2520grasps.%2520We%2520also%2520introduce%2520two%2520grasp%2520refinement%2520strategies%253A%250AEvaluator-Guided%2520Diffusion%2520%2528EGD%2529%2520and%2520Evaluator-based%2520Sampling%2520Refinement%2520%2528ESR%2529.%250AThe%2520experiment%2520results%2520demonstrate%2520that%2520DexDiffuser%2520consistently%2520outperforms%250Athe%2520state-of-the-art%2520multi-finger%2520grasp%2520generation%2520method%2520FFHNet%2520with%2520an%252C%2520on%250Aaverage%252C%25209.12%2525%2520and%252019.44%2525%2520higher%2520grasp%2520success%2520rate%2520in%2520simulation%2520and%2520real%250Arobot%2520experiments%252C%2520respectively.%2520Supplementary%2520materials%2520are%2520available%2520at%250Ahttps%253A//yulihn.github.io/DexDiffuser_page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexDiffuser%3A%20Generating%20Dexterous%20Grasps%20with%20Diffusion%20Models&entry.906535625=Zehang%20Weng%20and%20Haofei%20Lu%20and%20Danica%20Kragic%20and%20Jens%20Lundell&entry.1292438233=%20%20We%20introduce%20DexDiffuser%2C%20a%20novel%20dexterous%20grasping%20method%20that%20generates%2C%0Aevaluates%2C%20and%20refines%20grasps%20on%20partial%20object%20point%20clouds.%20DexDiffuser%0Aincludes%20the%20conditional%20diffusion-based%20grasp%20sampler%20DexSampler%20and%20the%0Adexterous%20grasp%20evaluator%20DexEvaluator.%20DexSampler%20generates%20high-quality%0Agrasps%20conditioned%20on%20object%20point%20clouds%20by%20iterative%20denoising%20of%20randomly%0Asampled%20grasps.%20We%20also%20introduce%20two%20grasp%20refinement%20strategies%3A%0AEvaluator-Guided%20Diffusion%20%28EGD%29%20and%20Evaluator-based%20Sampling%20Refinement%20%28ESR%29.%0AThe%20experiment%20results%20demonstrate%20that%20DexDiffuser%20consistently%20outperforms%0Athe%20state-of-the-art%20multi-finger%20grasp%20generation%20method%20FFHNet%20with%20an%2C%20on%0Aaverage%2C%209.12%25%20and%2019.44%25%20higher%20grasp%20success%20rate%20in%20simulation%20and%20real%0Arobot%20experiments%2C%20respectively.%20Supplementary%20materials%20are%20available%20at%0Ahttps%3A//yulihn.github.io/DexDiffuser_page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02989v3&entry.124074799=Read"},
{"title": "Face Reconstruction from Face Embeddings using Adapter to a Face\n  Foundation Model", "author": "Hatef Otroshi Shahreza and Anjith George and S\u00e9bastien Marcel", "abstract": "  Face recognition systems extract embedding vectors from face images and use\nthese embeddings to verify or identify individuals. Face reconstruction attack\n(also known as template inversion) refers to reconstructing face images from\nface embeddings and using the reconstructed face image to enter a face\nrecognition system. In this paper, we propose to use a face foundation model to\nreconstruct face images from the embeddings of a blackbox face recognition\nmodel. The foundation model is trained with 42M images to generate face images\nfrom the facial embeddings of a fixed face recognition model. We propose to use\nan adapter to translate target embeddings into the embedding space of the\nfoundation model. The generated images are evaluated on different face\nrecognition models and different datasets, demonstrating the effectiveness of\nour method to translate embeddings of different face recognition models. We\nalso evaluate the transferability of reconstructed face images when attacking\ndifferent face recognition models. Our experimental results show that our\nreconstructed face images outperform previous reconstruction attacks against\nface recognition models.\n", "link": "http://arxiv.org/abs/2411.03960v1", "date": "2024-11-06", "relevancy": 2.443, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Face%20Reconstruction%20from%20Face%20Embeddings%20using%20Adapter%20to%20a%20Face%0A%20%20Foundation%20Model&body=Title%3A%20Face%20Reconstruction%20from%20Face%20Embeddings%20using%20Adapter%20to%20a%20Face%0A%20%20Foundation%20Model%0AAuthor%3A%20Hatef%20Otroshi%20Shahreza%20and%20Anjith%20George%20and%20S%C3%A9bastien%20Marcel%0AAbstract%3A%20%20%20Face%20recognition%20systems%20extract%20embedding%20vectors%20from%20face%20images%20and%20use%0Athese%20embeddings%20to%20verify%20or%20identify%20individuals.%20Face%20reconstruction%20attack%0A%28also%20known%20as%20template%20inversion%29%20refers%20to%20reconstructing%20face%20images%20from%0Aface%20embeddings%20and%20using%20the%20reconstructed%20face%20image%20to%20enter%20a%20face%0Arecognition%20system.%20In%20this%20paper%2C%20we%20propose%20to%20use%20a%20face%20foundation%20model%20to%0Areconstruct%20face%20images%20from%20the%20embeddings%20of%20a%20blackbox%20face%20recognition%0Amodel.%20The%20foundation%20model%20is%20trained%20with%2042M%20images%20to%20generate%20face%20images%0Afrom%20the%20facial%20embeddings%20of%20a%20fixed%20face%20recognition%20model.%20We%20propose%20to%20use%0Aan%20adapter%20to%20translate%20target%20embeddings%20into%20the%20embedding%20space%20of%20the%0Afoundation%20model.%20The%20generated%20images%20are%20evaluated%20on%20different%20face%0Arecognition%20models%20and%20different%20datasets%2C%20demonstrating%20the%20effectiveness%20of%0Aour%20method%20to%20translate%20embeddings%20of%20different%20face%20recognition%20models.%20We%0Aalso%20evaluate%20the%20transferability%20of%20reconstructed%20face%20images%20when%20attacking%0Adifferent%20face%20recognition%20models.%20Our%20experimental%20results%20show%20that%20our%0Areconstructed%20face%20images%20outperform%20previous%20reconstruction%20attacks%20against%0Aface%20recognition%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFace%2520Reconstruction%2520from%2520Face%2520Embeddings%2520using%2520Adapter%2520to%2520a%2520Face%250A%2520%2520Foundation%2520Model%26entry.906535625%3DHatef%2520Otroshi%2520Shahreza%2520and%2520Anjith%2520George%2520and%2520S%25C3%25A9bastien%2520Marcel%26entry.1292438233%3D%2520%2520Face%2520recognition%2520systems%2520extract%2520embedding%2520vectors%2520from%2520face%2520images%2520and%2520use%250Athese%2520embeddings%2520to%2520verify%2520or%2520identify%2520individuals.%2520Face%2520reconstruction%2520attack%250A%2528also%2520known%2520as%2520template%2520inversion%2529%2520refers%2520to%2520reconstructing%2520face%2520images%2520from%250Aface%2520embeddings%2520and%2520using%2520the%2520reconstructed%2520face%2520image%2520to%2520enter%2520a%2520face%250Arecognition%2520system.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520use%2520a%2520face%2520foundation%2520model%2520to%250Areconstruct%2520face%2520images%2520from%2520the%2520embeddings%2520of%2520a%2520blackbox%2520face%2520recognition%250Amodel.%2520The%2520foundation%2520model%2520is%2520trained%2520with%252042M%2520images%2520to%2520generate%2520face%2520images%250Afrom%2520the%2520facial%2520embeddings%2520of%2520a%2520fixed%2520face%2520recognition%2520model.%2520We%2520propose%2520to%2520use%250Aan%2520adapter%2520to%2520translate%2520target%2520embeddings%2520into%2520the%2520embedding%2520space%2520of%2520the%250Afoundation%2520model.%2520The%2520generated%2520images%2520are%2520evaluated%2520on%2520different%2520face%250Arecognition%2520models%2520and%2520different%2520datasets%252C%2520demonstrating%2520the%2520effectiveness%2520of%250Aour%2520method%2520to%2520translate%2520embeddings%2520of%2520different%2520face%2520recognition%2520models.%2520We%250Aalso%2520evaluate%2520the%2520transferability%2520of%2520reconstructed%2520face%2520images%2520when%2520attacking%250Adifferent%2520face%2520recognition%2520models.%2520Our%2520experimental%2520results%2520show%2520that%2520our%250Areconstructed%2520face%2520images%2520outperform%2520previous%2520reconstruction%2520attacks%2520against%250Aface%2520recognition%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face%20Reconstruction%20from%20Face%20Embeddings%20using%20Adapter%20to%20a%20Face%0A%20%20Foundation%20Model&entry.906535625=Hatef%20Otroshi%20Shahreza%20and%20Anjith%20George%20and%20S%C3%A9bastien%20Marcel&entry.1292438233=%20%20Face%20recognition%20systems%20extract%20embedding%20vectors%20from%20face%20images%20and%20use%0Athese%20embeddings%20to%20verify%20or%20identify%20individuals.%20Face%20reconstruction%20attack%0A%28also%20known%20as%20template%20inversion%29%20refers%20to%20reconstructing%20face%20images%20from%0Aface%20embeddings%20and%20using%20the%20reconstructed%20face%20image%20to%20enter%20a%20face%0Arecognition%20system.%20In%20this%20paper%2C%20we%20propose%20to%20use%20a%20face%20foundation%20model%20to%0Areconstruct%20face%20images%20from%20the%20embeddings%20of%20a%20blackbox%20face%20recognition%0Amodel.%20The%20foundation%20model%20is%20trained%20with%2042M%20images%20to%20generate%20face%20images%0Afrom%20the%20facial%20embeddings%20of%20a%20fixed%20face%20recognition%20model.%20We%20propose%20to%20use%0Aan%20adapter%20to%20translate%20target%20embeddings%20into%20the%20embedding%20space%20of%20the%0Afoundation%20model.%20The%20generated%20images%20are%20evaluated%20on%20different%20face%0Arecognition%20models%20and%20different%20datasets%2C%20demonstrating%20the%20effectiveness%20of%0Aour%20method%20to%20translate%20embeddings%20of%20different%20face%20recognition%20models.%20We%0Aalso%20evaluate%20the%20transferability%20of%20reconstructed%20face%20images%20when%20attacking%0Adifferent%20face%20recognition%20models.%20Our%20experimental%20results%20show%20that%20our%0Areconstructed%20face%20images%20outperform%20previous%20reconstruction%20attacks%20against%0Aface%20recognition%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03960v1&entry.124074799=Read"},
{"title": "Robustifying automatic speech recognition by extracting slowly varying\n  features", "author": "Mat\u00edas Pizarro and Dorothea Kolossa and Asja Fischer", "abstract": "  In the past few years, it has been shown that deep learning systems are\nhighly vulnerable under attacks with adversarial examples. Neural-network-based\nautomatic speech recognition (ASR) systems are no exception. Targeted and\nuntargeted attacks can modify an audio input signal in such a way that humans\nstill recognise the same words, while ASR systems are steered to predict a\ndifferent transcription. In this paper, we propose a defense mechanism against\ntargeted adversarial attacks consisting in removing fast-changing features from\nthe audio signals, either by applying slow feature analysis, a low-pass filter,\nor both, before feeding the input to the ASR system. We perform an empirical\nanalysis of hybrid ASR models trained on data pre-processed in such a way.\nWhile the resulting models perform quite well on benign data, they are\nsignificantly more robust against targeted adversarial attacks: Our final,\nproposed model shows a performance on clean data similar to the baseline model,\nwhile being more than four times more robust.\n", "link": "http://arxiv.org/abs/2112.07400v3", "date": "2024-11-06", "relevancy": 2.4134, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5293}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4674}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustifying%20automatic%20speech%20recognition%20by%20extracting%20slowly%20varying%0A%20%20features&body=Title%3A%20Robustifying%20automatic%20speech%20recognition%20by%20extracting%20slowly%20varying%0A%20%20features%0AAuthor%3A%20Mat%C3%ADas%20Pizarro%20and%20Dorothea%20Kolossa%20and%20Asja%20Fischer%0AAbstract%3A%20%20%20In%20the%20past%20few%20years%2C%20it%20has%20been%20shown%20that%20deep%20learning%20systems%20are%0Ahighly%20vulnerable%20under%20attacks%20with%20adversarial%20examples.%20Neural-network-based%0Aautomatic%20speech%20recognition%20%28ASR%29%20systems%20are%20no%20exception.%20Targeted%20and%0Auntargeted%20attacks%20can%20modify%20an%20audio%20input%20signal%20in%20such%20a%20way%20that%20humans%0Astill%20recognise%20the%20same%20words%2C%20while%20ASR%20systems%20are%20steered%20to%20predict%20a%0Adifferent%20transcription.%20In%20this%20paper%2C%20we%20propose%20a%20defense%20mechanism%20against%0Atargeted%20adversarial%20attacks%20consisting%20in%20removing%20fast-changing%20features%20from%0Athe%20audio%20signals%2C%20either%20by%20applying%20slow%20feature%20analysis%2C%20a%20low-pass%20filter%2C%0Aor%20both%2C%20before%20feeding%20the%20input%20to%20the%20ASR%20system.%20We%20perform%20an%20empirical%0Aanalysis%20of%20hybrid%20ASR%20models%20trained%20on%20data%20pre-processed%20in%20such%20a%20way.%0AWhile%20the%20resulting%20models%20perform%20quite%20well%20on%20benign%20data%2C%20they%20are%0Asignificantly%20more%20robust%20against%20targeted%20adversarial%20attacks%3A%20Our%20final%2C%0Aproposed%20model%20shows%20a%20performance%20on%20clean%20data%20similar%20to%20the%20baseline%20model%2C%0Awhile%20being%20more%20than%20four%20times%20more%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2112.07400v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustifying%2520automatic%2520speech%2520recognition%2520by%2520extracting%2520slowly%2520varying%250A%2520%2520features%26entry.906535625%3DMat%25C3%25ADas%2520Pizarro%2520and%2520Dorothea%2520Kolossa%2520and%2520Asja%2520Fischer%26entry.1292438233%3D%2520%2520In%2520the%2520past%2520few%2520years%252C%2520it%2520has%2520been%2520shown%2520that%2520deep%2520learning%2520systems%2520are%250Ahighly%2520vulnerable%2520under%2520attacks%2520with%2520adversarial%2520examples.%2520Neural-network-based%250Aautomatic%2520speech%2520recognition%2520%2528ASR%2529%2520systems%2520are%2520no%2520exception.%2520Targeted%2520and%250Auntargeted%2520attacks%2520can%2520modify%2520an%2520audio%2520input%2520signal%2520in%2520such%2520a%2520way%2520that%2520humans%250Astill%2520recognise%2520the%2520same%2520words%252C%2520while%2520ASR%2520systems%2520are%2520steered%2520to%2520predict%2520a%250Adifferent%2520transcription.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520defense%2520mechanism%2520against%250Atargeted%2520adversarial%2520attacks%2520consisting%2520in%2520removing%2520fast-changing%2520features%2520from%250Athe%2520audio%2520signals%252C%2520either%2520by%2520applying%2520slow%2520feature%2520analysis%252C%2520a%2520low-pass%2520filter%252C%250Aor%2520both%252C%2520before%2520feeding%2520the%2520input%2520to%2520the%2520ASR%2520system.%2520We%2520perform%2520an%2520empirical%250Aanalysis%2520of%2520hybrid%2520ASR%2520models%2520trained%2520on%2520data%2520pre-processed%2520in%2520such%2520a%2520way.%250AWhile%2520the%2520resulting%2520models%2520perform%2520quite%2520well%2520on%2520benign%2520data%252C%2520they%2520are%250Asignificantly%2520more%2520robust%2520against%2520targeted%2520adversarial%2520attacks%253A%2520Our%2520final%252C%250Aproposed%2520model%2520shows%2520a%2520performance%2520on%2520clean%2520data%2520similar%2520to%2520the%2520baseline%2520model%252C%250Awhile%2520being%2520more%2520than%2520four%2520times%2520more%2520robust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2112.07400v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustifying%20automatic%20speech%20recognition%20by%20extracting%20slowly%20varying%0A%20%20features&entry.906535625=Mat%C3%ADas%20Pizarro%20and%20Dorothea%20Kolossa%20and%20Asja%20Fischer&entry.1292438233=%20%20In%20the%20past%20few%20years%2C%20it%20has%20been%20shown%20that%20deep%20learning%20systems%20are%0Ahighly%20vulnerable%20under%20attacks%20with%20adversarial%20examples.%20Neural-network-based%0Aautomatic%20speech%20recognition%20%28ASR%29%20systems%20are%20no%20exception.%20Targeted%20and%0Auntargeted%20attacks%20can%20modify%20an%20audio%20input%20signal%20in%20such%20a%20way%20that%20humans%0Astill%20recognise%20the%20same%20words%2C%20while%20ASR%20systems%20are%20steered%20to%20predict%20a%0Adifferent%20transcription.%20In%20this%20paper%2C%20we%20propose%20a%20defense%20mechanism%20against%0Atargeted%20adversarial%20attacks%20consisting%20in%20removing%20fast-changing%20features%20from%0Athe%20audio%20signals%2C%20either%20by%20applying%20slow%20feature%20analysis%2C%20a%20low-pass%20filter%2C%0Aor%20both%2C%20before%20feeding%20the%20input%20to%20the%20ASR%20system.%20We%20perform%20an%20empirical%0Aanalysis%20of%20hybrid%20ASR%20models%20trained%20on%20data%20pre-processed%20in%20such%20a%20way.%0AWhile%20the%20resulting%20models%20perform%20quite%20well%20on%20benign%20data%2C%20they%20are%0Asignificantly%20more%20robust%20against%20targeted%20adversarial%20attacks%3A%20Our%20final%2C%0Aproposed%20model%20shows%20a%20performance%20on%20clean%20data%20similar%20to%20the%20baseline%20model%2C%0Awhile%20being%20more%20than%20four%20times%20more%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.07400v3&entry.124074799=Read"},
{"title": "DEIO: Deep Event Inertial Odometry", "author": "Weipeng Guan and Fuling Lin and Peiyu Chen and Peng Lu", "abstract": "  Event cameras are bio-inspired, motion-activated sensors that demonstrate\nimpressive potential in handling challenging situations, such as motion blur\nand high-dynamic range. Despite their promise, existing event-based\nsimultaneous localization and mapping (SLAM) approaches exhibit limited\nperformance in real-world applications. On the other hand, state-of-the-art\nSLAM approaches that incorporate deep neural networks for better robustness and\napplicability. However, these is a lack of research in fusing learning-based\nevent SLAM methods with IMU, which could be indispensable to push the\nevent-based SLAM to large-scale, low-texture or complex scenarios. In this\npaper, we propose DEIO, the first monocular deep event-inertial odometry\nframework that combines learning-based method with traditional nonlinear\ngraph-based optimization. Specifically, we tightly integrate a trainable\nevent-based differentiable bundle adjustment (e-DBA) with the IMU\npre-integration in a factor graph which employs keyframe-based sliding window\noptimization. Numerical Experiments in nine public challenge datasets show that\nour method can achieve superior performance compared with the image-based and\nevent-based benchmarks. The source code is available at:\nhttps://github.com/arclab-hku/DEIO.\n", "link": "http://arxiv.org/abs/2411.03928v1", "date": "2024-11-06", "relevancy": 2.3863, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6068}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEIO%3A%20Deep%20Event%20Inertial%20Odometry&body=Title%3A%20DEIO%3A%20Deep%20Event%20Inertial%20Odometry%0AAuthor%3A%20Weipeng%20Guan%20and%20Fuling%20Lin%20and%20Peiyu%20Chen%20and%20Peng%20Lu%0AAbstract%3A%20%20%20Event%20cameras%20are%20bio-inspired%2C%20motion-activated%20sensors%20that%20demonstrate%0Aimpressive%20potential%20in%20handling%20challenging%20situations%2C%20such%20as%20motion%20blur%0Aand%20high-dynamic%20range.%20Despite%20their%20promise%2C%20existing%20event-based%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%20approaches%20exhibit%20limited%0Aperformance%20in%20real-world%20applications.%20On%20the%20other%20hand%2C%20state-of-the-art%0ASLAM%20approaches%20that%20incorporate%20deep%20neural%20networks%20for%20better%20robustness%20and%0Aapplicability.%20However%2C%20these%20is%20a%20lack%20of%20research%20in%20fusing%20learning-based%0Aevent%20SLAM%20methods%20with%20IMU%2C%20which%20could%20be%20indispensable%20to%20push%20the%0Aevent-based%20SLAM%20to%20large-scale%2C%20low-texture%20or%20complex%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20DEIO%2C%20the%20first%20monocular%20deep%20event-inertial%20odometry%0Aframework%20that%20combines%20learning-based%20method%20with%20traditional%20nonlinear%0Agraph-based%20optimization.%20Specifically%2C%20we%20tightly%20integrate%20a%20trainable%0Aevent-based%20differentiable%20bundle%20adjustment%20%28e-DBA%29%20with%20the%20IMU%0Apre-integration%20in%20a%20factor%20graph%20which%20employs%20keyframe-based%20sliding%20window%0Aoptimization.%20Numerical%20Experiments%20in%20nine%20public%20challenge%20datasets%20show%20that%0Aour%20method%20can%20achieve%20superior%20performance%20compared%20with%20the%20image-based%20and%0Aevent-based%20benchmarks.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/arclab-hku/DEIO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEIO%253A%2520Deep%2520Event%2520Inertial%2520Odometry%26entry.906535625%3DWeipeng%2520Guan%2520and%2520Fuling%2520Lin%2520and%2520Peiyu%2520Chen%2520and%2520Peng%2520Lu%26entry.1292438233%3D%2520%2520Event%2520cameras%2520are%2520bio-inspired%252C%2520motion-activated%2520sensors%2520that%2520demonstrate%250Aimpressive%2520potential%2520in%2520handling%2520challenging%2520situations%252C%2520such%2520as%2520motion%2520blur%250Aand%2520high-dynamic%2520range.%2520Despite%2520their%2520promise%252C%2520existing%2520event-based%250Asimultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520approaches%2520exhibit%2520limited%250Aperformance%2520in%2520real-world%2520applications.%2520On%2520the%2520other%2520hand%252C%2520state-of-the-art%250ASLAM%2520approaches%2520that%2520incorporate%2520deep%2520neural%2520networks%2520for%2520better%2520robustness%2520and%250Aapplicability.%2520However%252C%2520these%2520is%2520a%2520lack%2520of%2520research%2520in%2520fusing%2520learning-based%250Aevent%2520SLAM%2520methods%2520with%2520IMU%252C%2520which%2520could%2520be%2520indispensable%2520to%2520push%2520the%250Aevent-based%2520SLAM%2520to%2520large-scale%252C%2520low-texture%2520or%2520complex%2520scenarios.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520DEIO%252C%2520the%2520first%2520monocular%2520deep%2520event-inertial%2520odometry%250Aframework%2520that%2520combines%2520learning-based%2520method%2520with%2520traditional%2520nonlinear%250Agraph-based%2520optimization.%2520Specifically%252C%2520we%2520tightly%2520integrate%2520a%2520trainable%250Aevent-based%2520differentiable%2520bundle%2520adjustment%2520%2528e-DBA%2529%2520with%2520the%2520IMU%250Apre-integration%2520in%2520a%2520factor%2520graph%2520which%2520employs%2520keyframe-based%2520sliding%2520window%250Aoptimization.%2520Numerical%2520Experiments%2520in%2520nine%2520public%2520challenge%2520datasets%2520show%2520that%250Aour%2520method%2520can%2520achieve%2520superior%2520performance%2520compared%2520with%2520the%2520image-based%2520and%250Aevent-based%2520benchmarks.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/arclab-hku/DEIO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEIO%3A%20Deep%20Event%20Inertial%20Odometry&entry.906535625=Weipeng%20Guan%20and%20Fuling%20Lin%20and%20Peiyu%20Chen%20and%20Peng%20Lu&entry.1292438233=%20%20Event%20cameras%20are%20bio-inspired%2C%20motion-activated%20sensors%20that%20demonstrate%0Aimpressive%20potential%20in%20handling%20challenging%20situations%2C%20such%20as%20motion%20blur%0Aand%20high-dynamic%20range.%20Despite%20their%20promise%2C%20existing%20event-based%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%20approaches%20exhibit%20limited%0Aperformance%20in%20real-world%20applications.%20On%20the%20other%20hand%2C%20state-of-the-art%0ASLAM%20approaches%20that%20incorporate%20deep%20neural%20networks%20for%20better%20robustness%20and%0Aapplicability.%20However%2C%20these%20is%20a%20lack%20of%20research%20in%20fusing%20learning-based%0Aevent%20SLAM%20methods%20with%20IMU%2C%20which%20could%20be%20indispensable%20to%20push%20the%0Aevent-based%20SLAM%20to%20large-scale%2C%20low-texture%20or%20complex%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20DEIO%2C%20the%20first%20monocular%20deep%20event-inertial%20odometry%0Aframework%20that%20combines%20learning-based%20method%20with%20traditional%20nonlinear%0Agraph-based%20optimization.%20Specifically%2C%20we%20tightly%20integrate%20a%20trainable%0Aevent-based%20differentiable%20bundle%20adjustment%20%28e-DBA%29%20with%20the%20IMU%0Apre-integration%20in%20a%20factor%20graph%20which%20employs%20keyframe-based%20sliding%20window%0Aoptimization.%20Numerical%20Experiments%20in%20nine%20public%20challenge%20datasets%20show%20that%0Aour%20method%20can%20achieve%20superior%20performance%20compared%20with%20the%20image-based%20and%0Aevent-based%20benchmarks.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/arclab-hku/DEIO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03928v1&entry.124074799=Read"},
{"title": "Reconsidering the Performance of GAE in Link Prediction", "author": "Weishuo Ma and Yanbo Wang and Xiyuan Wang and Muhan Zhang", "abstract": "  Various graph neural networks (GNNs) with advanced training techniques and\nmodel designs have been proposed for link prediction tasks. However, outdated\nbaseline models may lead to an overestimation of the benefits provided by these\nnovel approaches. To address this, we systematically investigate the potential\nof Graph Autoencoders (GAE) by meticulously tuning hyperparameters and\nutilizing the trick of orthogonal embedding and linear propagation. Our\nfindings reveal that a well-optimized GAE can match the performance of more\ncomplex models while offering greater computational efficiency.\n", "link": "http://arxiv.org/abs/2411.03845v1", "date": "2024-11-06", "relevancy": 2.3673, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4933}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.478}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconsidering%20the%20Performance%20of%20GAE%20in%20Link%20Prediction&body=Title%3A%20Reconsidering%20the%20Performance%20of%20GAE%20in%20Link%20Prediction%0AAuthor%3A%20Weishuo%20Ma%20and%20Yanbo%20Wang%20and%20Xiyuan%20Wang%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20Various%20graph%20neural%20networks%20%28GNNs%29%20with%20advanced%20training%20techniques%20and%0Amodel%20designs%20have%20been%20proposed%20for%20link%20prediction%20tasks.%20However%2C%20outdated%0Abaseline%20models%20may%20lead%20to%20an%20overestimation%20of%20the%20benefits%20provided%20by%20these%0Anovel%20approaches.%20To%20address%20this%2C%20we%20systematically%20investigate%20the%20potential%0Aof%20Graph%20Autoencoders%20%28GAE%29%20by%20meticulously%20tuning%20hyperparameters%20and%0Autilizing%20the%20trick%20of%20orthogonal%20embedding%20and%20linear%20propagation.%20Our%0Afindings%20reveal%20that%20a%20well-optimized%20GAE%20can%20match%20the%20performance%20of%20more%0Acomplex%20models%20while%20offering%20greater%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconsidering%2520the%2520Performance%2520of%2520GAE%2520in%2520Link%2520Prediction%26entry.906535625%3DWeishuo%2520Ma%2520and%2520Yanbo%2520Wang%2520and%2520Xiyuan%2520Wang%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520Various%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520with%2520advanced%2520training%2520techniques%2520and%250Amodel%2520designs%2520have%2520been%2520proposed%2520for%2520link%2520prediction%2520tasks.%2520However%252C%2520outdated%250Abaseline%2520models%2520may%2520lead%2520to%2520an%2520overestimation%2520of%2520the%2520benefits%2520provided%2520by%2520these%250Anovel%2520approaches.%2520To%2520address%2520this%252C%2520we%2520systematically%2520investigate%2520the%2520potential%250Aof%2520Graph%2520Autoencoders%2520%2528GAE%2529%2520by%2520meticulously%2520tuning%2520hyperparameters%2520and%250Autilizing%2520the%2520trick%2520of%2520orthogonal%2520embedding%2520and%2520linear%2520propagation.%2520Our%250Afindings%2520reveal%2520that%2520a%2520well-optimized%2520GAE%2520can%2520match%2520the%2520performance%2520of%2520more%250Acomplex%2520models%2520while%2520offering%2520greater%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconsidering%20the%20Performance%20of%20GAE%20in%20Link%20Prediction&entry.906535625=Weishuo%20Ma%20and%20Yanbo%20Wang%20and%20Xiyuan%20Wang%20and%20Muhan%20Zhang&entry.1292438233=%20%20Various%20graph%20neural%20networks%20%28GNNs%29%20with%20advanced%20training%20techniques%20and%0Amodel%20designs%20have%20been%20proposed%20for%20link%20prediction%20tasks.%20However%2C%20outdated%0Abaseline%20models%20may%20lead%20to%20an%20overestimation%20of%20the%20benefits%20provided%20by%20these%0Anovel%20approaches.%20To%20address%20this%2C%20we%20systematically%20investigate%20the%20potential%0Aof%20Graph%20Autoencoders%20%28GAE%29%20by%20meticulously%20tuning%20hyperparameters%20and%0Autilizing%20the%20trick%20of%20orthogonal%20embedding%20and%20linear%20propagation.%20Our%0Afindings%20reveal%20that%20a%20well-optimized%20GAE%20can%20match%20the%20performance%20of%20more%0Acomplex%20models%20while%20offering%20greater%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03845v1&entry.124074799=Read"},
{"title": "Robust Perception-Informed Navigation using PAC-NMPC with a Learned\n  Value Function", "author": "Adam Polevoy and Mark Gonzales and Marin Kobilarov and Joseph Moore", "abstract": "  Nonlinear model predictive control (NMPC) is typically restricted to short,\nfinite horizons to limit the computational burden of online optimization. As a\nresult, global planning frameworks are frequently necessary to avoid local\nminima when using NMPC for navigation in complex environments. By contrast,\nreinforcement learning (RL) can generate policies that minimize the expected\ncost over an infinite-horizon and can often avoid local minima, even when\noperating only on current sensor measurements. However, these learned policies\nare usually unable to provide performance guarantees (e.g., on collision\navoidance), especially when outside of the training distribution. In this\npaper, we augment Probably Approximately Correct NMPC (PAC-NMPC), a\nsampling-based stochastic NMPC algorithm capable of providing statistical\nguarantees of performance and safety, with an approximate perception-dependent\nvalue function trained via RL. We demonstrate in simulation that our algorithm\ncan improve the long-term behavior of PAC-NMPC while outperforming other\napproaches with regards to safety for both planar car dynamics and more\ncomplex, high-dimensional fixed-wing aerial vehicle dynamics. We also\ndemonstrate that, even when our value function is trained in simulation, our\nalgorithm can successfully achieve statistically safe navigation on hardware\nusing a 1/10th scale rally car in cluttered real-world environments using only\ncurrent sensor information.\n", "link": "http://arxiv.org/abs/2309.13171v2", "date": "2024-11-06", "relevancy": 2.3546, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6158}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6029}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Perception-Informed%20Navigation%20using%20PAC-NMPC%20with%20a%20Learned%0A%20%20Value%20Function&body=Title%3A%20Robust%20Perception-Informed%20Navigation%20using%20PAC-NMPC%20with%20a%20Learned%0A%20%20Value%20Function%0AAuthor%3A%20Adam%20Polevoy%20and%20Mark%20Gonzales%20and%20Marin%20Kobilarov%20and%20Joseph%20Moore%0AAbstract%3A%20%20%20Nonlinear%20model%20predictive%20control%20%28NMPC%29%20is%20typically%20restricted%20to%20short%2C%0Afinite%20horizons%20to%20limit%20the%20computational%20burden%20of%20online%20optimization.%20As%20a%0Aresult%2C%20global%20planning%20frameworks%20are%20frequently%20necessary%20to%20avoid%20local%0Aminima%20when%20using%20NMPC%20for%20navigation%20in%20complex%20environments.%20By%20contrast%2C%0Areinforcement%20learning%20%28RL%29%20can%20generate%20policies%20that%20minimize%20the%20expected%0Acost%20over%20an%20infinite-horizon%20and%20can%20often%20avoid%20local%20minima%2C%20even%20when%0Aoperating%20only%20on%20current%20sensor%20measurements.%20However%2C%20these%20learned%20policies%0Aare%20usually%20unable%20to%20provide%20performance%20guarantees%20%28e.g.%2C%20on%20collision%0Aavoidance%29%2C%20especially%20when%20outside%20of%20the%20training%20distribution.%20In%20this%0Apaper%2C%20we%20augment%20Probably%20Approximately%20Correct%20NMPC%20%28PAC-NMPC%29%2C%20a%0Asampling-based%20stochastic%20NMPC%20algorithm%20capable%20of%20providing%20statistical%0Aguarantees%20of%20performance%20and%20safety%2C%20with%20an%20approximate%20perception-dependent%0Avalue%20function%20trained%20via%20RL.%20We%20demonstrate%20in%20simulation%20that%20our%20algorithm%0Acan%20improve%20the%20long-term%20behavior%20of%20PAC-NMPC%20while%20outperforming%20other%0Aapproaches%20with%20regards%20to%20safety%20for%20both%20planar%20car%20dynamics%20and%20more%0Acomplex%2C%20high-dimensional%20fixed-wing%20aerial%20vehicle%20dynamics.%20We%20also%0Ademonstrate%20that%2C%20even%20when%20our%20value%20function%20is%20trained%20in%20simulation%2C%20our%0Aalgorithm%20can%20successfully%20achieve%20statistically%20safe%20navigation%20on%20hardware%0Ausing%20a%201/10th%20scale%20rally%20car%20in%20cluttered%20real-world%20environments%20using%20only%0Acurrent%20sensor%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.13171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Perception-Informed%2520Navigation%2520using%2520PAC-NMPC%2520with%2520a%2520Learned%250A%2520%2520Value%2520Function%26entry.906535625%3DAdam%2520Polevoy%2520and%2520Mark%2520Gonzales%2520and%2520Marin%2520Kobilarov%2520and%2520Joseph%2520Moore%26entry.1292438233%3D%2520%2520Nonlinear%2520model%2520predictive%2520control%2520%2528NMPC%2529%2520is%2520typically%2520restricted%2520to%2520short%252C%250Afinite%2520horizons%2520to%2520limit%2520the%2520computational%2520burden%2520of%2520online%2520optimization.%2520As%2520a%250Aresult%252C%2520global%2520planning%2520frameworks%2520are%2520frequently%2520necessary%2520to%2520avoid%2520local%250Aminima%2520when%2520using%2520NMPC%2520for%2520navigation%2520in%2520complex%2520environments.%2520By%2520contrast%252C%250Areinforcement%2520learning%2520%2528RL%2529%2520can%2520generate%2520policies%2520that%2520minimize%2520the%2520expected%250Acost%2520over%2520an%2520infinite-horizon%2520and%2520can%2520often%2520avoid%2520local%2520minima%252C%2520even%2520when%250Aoperating%2520only%2520on%2520current%2520sensor%2520measurements.%2520However%252C%2520these%2520learned%2520policies%250Aare%2520usually%2520unable%2520to%2520provide%2520performance%2520guarantees%2520%2528e.g.%252C%2520on%2520collision%250Aavoidance%2529%252C%2520especially%2520when%2520outside%2520of%2520the%2520training%2520distribution.%2520In%2520this%250Apaper%252C%2520we%2520augment%2520Probably%2520Approximately%2520Correct%2520NMPC%2520%2528PAC-NMPC%2529%252C%2520a%250Asampling-based%2520stochastic%2520NMPC%2520algorithm%2520capable%2520of%2520providing%2520statistical%250Aguarantees%2520of%2520performance%2520and%2520safety%252C%2520with%2520an%2520approximate%2520perception-dependent%250Avalue%2520function%2520trained%2520via%2520RL.%2520We%2520demonstrate%2520in%2520simulation%2520that%2520our%2520algorithm%250Acan%2520improve%2520the%2520long-term%2520behavior%2520of%2520PAC-NMPC%2520while%2520outperforming%2520other%250Aapproaches%2520with%2520regards%2520to%2520safety%2520for%2520both%2520planar%2520car%2520dynamics%2520and%2520more%250Acomplex%252C%2520high-dimensional%2520fixed-wing%2520aerial%2520vehicle%2520dynamics.%2520We%2520also%250Ademonstrate%2520that%252C%2520even%2520when%2520our%2520value%2520function%2520is%2520trained%2520in%2520simulation%252C%2520our%250Aalgorithm%2520can%2520successfully%2520achieve%2520statistically%2520safe%2520navigation%2520on%2520hardware%250Ausing%2520a%25201/10th%2520scale%2520rally%2520car%2520in%2520cluttered%2520real-world%2520environments%2520using%2520only%250Acurrent%2520sensor%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.13171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Perception-Informed%20Navigation%20using%20PAC-NMPC%20with%20a%20Learned%0A%20%20Value%20Function&entry.906535625=Adam%20Polevoy%20and%20Mark%20Gonzales%20and%20Marin%20Kobilarov%20and%20Joseph%20Moore&entry.1292438233=%20%20Nonlinear%20model%20predictive%20control%20%28NMPC%29%20is%20typically%20restricted%20to%20short%2C%0Afinite%20horizons%20to%20limit%20the%20computational%20burden%20of%20online%20optimization.%20As%20a%0Aresult%2C%20global%20planning%20frameworks%20are%20frequently%20necessary%20to%20avoid%20local%0Aminima%20when%20using%20NMPC%20for%20navigation%20in%20complex%20environments.%20By%20contrast%2C%0Areinforcement%20learning%20%28RL%29%20can%20generate%20policies%20that%20minimize%20the%20expected%0Acost%20over%20an%20infinite-horizon%20and%20can%20often%20avoid%20local%20minima%2C%20even%20when%0Aoperating%20only%20on%20current%20sensor%20measurements.%20However%2C%20these%20learned%20policies%0Aare%20usually%20unable%20to%20provide%20performance%20guarantees%20%28e.g.%2C%20on%20collision%0Aavoidance%29%2C%20especially%20when%20outside%20of%20the%20training%20distribution.%20In%20this%0Apaper%2C%20we%20augment%20Probably%20Approximately%20Correct%20NMPC%20%28PAC-NMPC%29%2C%20a%0Asampling-based%20stochastic%20NMPC%20algorithm%20capable%20of%20providing%20statistical%0Aguarantees%20of%20performance%20and%20safety%2C%20with%20an%20approximate%20perception-dependent%0Avalue%20function%20trained%20via%20RL.%20We%20demonstrate%20in%20simulation%20that%20our%20algorithm%0Acan%20improve%20the%20long-term%20behavior%20of%20PAC-NMPC%20while%20outperforming%20other%0Aapproaches%20with%20regards%20to%20safety%20for%20both%20planar%20car%20dynamics%20and%20more%0Acomplex%2C%20high-dimensional%20fixed-wing%20aerial%20vehicle%20dynamics.%20We%20also%0Ademonstrate%20that%2C%20even%20when%20our%20value%20function%20is%20trained%20in%20simulation%2C%20our%0Aalgorithm%20can%20successfully%20achieve%20statistically%20safe%20navigation%20on%20hardware%0Ausing%20a%201/10th%20scale%20rally%20car%20in%20cluttered%20real-world%20environments%20using%20only%0Acurrent%20sensor%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.13171v2&entry.124074799=Read"},
{"title": "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "author": "Kiran Purohit and Venktesh V and Raghuram Devalla and Krishna Mohan Yerragorla and Sourangshu Bhattacharya and Avishek Anand", "abstract": "  Answering reasoning-based complex questions over text and hybrid sources,\nincluding tables, is a challenging task. Recent advances in large language\nmodels (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire\nproficiency in a specific task using only a few demonstration samples\n(exemplars). A critical challenge in ICL is the selection of optimal exemplars,\nwhich can be either task-specific (static) or test-example-specific (dynamic).\nStatic exemplars provide faster inference times and increased robustness across\na distribution of test examples. In this paper, we propose an algorithm for\nstatic exemplar subset selection for complex reasoning tasks. We introduce\nEXPLORA, a novel exploration method designed to estimate the parameters of the\nscoring function, which evaluates exemplar subsets without incorporating\nconfidence information. EXPLORA significantly reduces the number of LLM calls\nto ~11% of those required by state-of-the-art methods and achieves a\nsubstantial performance improvement of 12.24%. We open-source our code and data\n(https://github.com/kiranpurohit/EXPLORA).\n", "link": "http://arxiv.org/abs/2411.03877v1", "date": "2024-11-06", "relevancy": 2.3094, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EXPLORA%3A%20Efficient%20Exemplar%20Subset%20Selection%20for%20Complex%20Reasoning&body=Title%3A%20EXPLORA%3A%20Efficient%20Exemplar%20Subset%20Selection%20for%20Complex%20Reasoning%0AAuthor%3A%20Kiran%20Purohit%20and%20Venktesh%20V%20and%20Raghuram%20Devalla%20and%20Krishna%20Mohan%20Yerragorla%20and%20Sourangshu%20Bhattacharya%20and%20Avishek%20Anand%0AAbstract%3A%20%20%20Answering%20reasoning-based%20complex%20questions%20over%20text%20and%20hybrid%20sources%2C%0Aincluding%20tables%2C%20is%20a%20challenging%20task.%20Recent%20advances%20in%20large%20language%0Amodels%20%28LLMs%29%20have%20enabled%20in-context%20learning%20%28ICL%29%2C%20allowing%20LLMs%20to%20acquire%0Aproficiency%20in%20a%20specific%20task%20using%20only%20a%20few%20demonstration%20samples%0A%28exemplars%29.%20A%20critical%20challenge%20in%20ICL%20is%20the%20selection%20of%20optimal%20exemplars%2C%0Awhich%20can%20be%20either%20task-specific%20%28static%29%20or%20test-example-specific%20%28dynamic%29.%0AStatic%20exemplars%20provide%20faster%20inference%20times%20and%20increased%20robustness%20across%0Aa%20distribution%20of%20test%20examples.%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20for%0Astatic%20exemplar%20subset%20selection%20for%20complex%20reasoning%20tasks.%20We%20introduce%0AEXPLORA%2C%20a%20novel%20exploration%20method%20designed%20to%20estimate%20the%20parameters%20of%20the%0Ascoring%20function%2C%20which%20evaluates%20exemplar%20subsets%20without%20incorporating%0Aconfidence%20information.%20EXPLORA%20significantly%20reduces%20the%20number%20of%20LLM%20calls%0Ato%20~11%25%20of%20those%20required%20by%20state-of-the-art%20methods%20and%20achieves%20a%0Asubstantial%20performance%20improvement%20of%2012.24%25.%20We%20open-source%20our%20code%20and%20data%0A%28https%3A//github.com/kiranpurohit/EXPLORA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEXPLORA%253A%2520Efficient%2520Exemplar%2520Subset%2520Selection%2520for%2520Complex%2520Reasoning%26entry.906535625%3DKiran%2520Purohit%2520and%2520Venktesh%2520V%2520and%2520Raghuram%2520Devalla%2520and%2520Krishna%2520Mohan%2520Yerragorla%2520and%2520Sourangshu%2520Bhattacharya%2520and%2520Avishek%2520Anand%26entry.1292438233%3D%2520%2520Answering%2520reasoning-based%2520complex%2520questions%2520over%2520text%2520and%2520hybrid%2520sources%252C%250Aincluding%2520tables%252C%2520is%2520a%2520challenging%2520task.%2520Recent%2520advances%2520in%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520have%2520enabled%2520in-context%2520learning%2520%2528ICL%2529%252C%2520allowing%2520LLMs%2520to%2520acquire%250Aproficiency%2520in%2520a%2520specific%2520task%2520using%2520only%2520a%2520few%2520demonstration%2520samples%250A%2528exemplars%2529.%2520A%2520critical%2520challenge%2520in%2520ICL%2520is%2520the%2520selection%2520of%2520optimal%2520exemplars%252C%250Awhich%2520can%2520be%2520either%2520task-specific%2520%2528static%2529%2520or%2520test-example-specific%2520%2528dynamic%2529.%250AStatic%2520exemplars%2520provide%2520faster%2520inference%2520times%2520and%2520increased%2520robustness%2520across%250Aa%2520distribution%2520of%2520test%2520examples.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520algorithm%2520for%250Astatic%2520exemplar%2520subset%2520selection%2520for%2520complex%2520reasoning%2520tasks.%2520We%2520introduce%250AEXPLORA%252C%2520a%2520novel%2520exploration%2520method%2520designed%2520to%2520estimate%2520the%2520parameters%2520of%2520the%250Ascoring%2520function%252C%2520which%2520evaluates%2520exemplar%2520subsets%2520without%2520incorporating%250Aconfidence%2520information.%2520EXPLORA%2520significantly%2520reduces%2520the%2520number%2520of%2520LLM%2520calls%250Ato%2520~11%2525%2520of%2520those%2520required%2520by%2520state-of-the-art%2520methods%2520and%2520achieves%2520a%250Asubstantial%2520performance%2520improvement%2520of%252012.24%2525.%2520We%2520open-source%2520our%2520code%2520and%2520data%250A%2528https%253A//github.com/kiranpurohit/EXPLORA%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EXPLORA%3A%20Efficient%20Exemplar%20Subset%20Selection%20for%20Complex%20Reasoning&entry.906535625=Kiran%20Purohit%20and%20Venktesh%20V%20and%20Raghuram%20Devalla%20and%20Krishna%20Mohan%20Yerragorla%20and%20Sourangshu%20Bhattacharya%20and%20Avishek%20Anand&entry.1292438233=%20%20Answering%20reasoning-based%20complex%20questions%20over%20text%20and%20hybrid%20sources%2C%0Aincluding%20tables%2C%20is%20a%20challenging%20task.%20Recent%20advances%20in%20large%20language%0Amodels%20%28LLMs%29%20have%20enabled%20in-context%20learning%20%28ICL%29%2C%20allowing%20LLMs%20to%20acquire%0Aproficiency%20in%20a%20specific%20task%20using%20only%20a%20few%20demonstration%20samples%0A%28exemplars%29.%20A%20critical%20challenge%20in%20ICL%20is%20the%20selection%20of%20optimal%20exemplars%2C%0Awhich%20can%20be%20either%20task-specific%20%28static%29%20or%20test-example-specific%20%28dynamic%29.%0AStatic%20exemplars%20provide%20faster%20inference%20times%20and%20increased%20robustness%20across%0Aa%20distribution%20of%20test%20examples.%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20for%0Astatic%20exemplar%20subset%20selection%20for%20complex%20reasoning%20tasks.%20We%20introduce%0AEXPLORA%2C%20a%20novel%20exploration%20method%20designed%20to%20estimate%20the%20parameters%20of%20the%0Ascoring%20function%2C%20which%20evaluates%20exemplar%20subsets%20without%20incorporating%0Aconfidence%20information.%20EXPLORA%20significantly%20reduces%20the%20number%20of%20LLM%20calls%0Ato%20~11%25%20of%20those%20required%20by%20state-of-the-art%20methods%20and%20achieves%20a%0Asubstantial%20performance%20improvement%20of%2012.24%25.%20We%20open-source%20our%20code%20and%20data%0A%28https%3A//github.com/kiranpurohit/EXPLORA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03877v1&entry.124074799=Read"},
{"title": "Act in Collusion: A Persistent Distributed Multi-Target Backdoor in\n  Federated Learning", "author": "Tao Liu and Wu Yang and Chen Xu and Jiguang Lv and Huanran Wang and Yuhang Zhang and Shuchun Xu and Dapeng Man", "abstract": "  Federated learning, a novel paradigm designed to protect data privacy, is\nvulnerable to backdoor attacks due to its distributed nature. Current research\noften designs attacks based on a single attacker with a single backdoor,\noverlooking more realistic and complex threats in federated learning. We\npropose a more practical threat model for federated learning: the distributed\nmulti-target backdoor. In this model, multiple attackers control different\nclients, embedding various triggers and targeting different classes,\ncollaboratively implanting backdoors into the global model via central\naggregation. Empirical validation shows that existing methods struggle to\nmaintain the effectiveness of multiple backdoors in the global model. Our key\ninsight is that similar backdoor triggers cause parameter conflicts and\ninjecting new backdoors disrupts gradient directions, significantly weakening\nsome backdoors performance. To solve this, we propose a Distributed\nMulti-Target Backdoor Attack (DMBA), ensuring efficiency and persistence of\nbackdoors from different malicious clients. To avoid parameter conflicts, we\ndesign a multi-channel dispersed frequency trigger strategy to maximize trigger\ndifferences. To mitigate gradient interference, we introduce backdoor replay in\nlocal training to neutralize conflicting gradients. Extensive validation shows\nthat 30 rounds after the attack, Attack Success Rates of three different\nbackdoors from various clients remain above 93%. The code will be made publicly\navailable after the review period.\n", "link": "http://arxiv.org/abs/2411.03926v1", "date": "2024-11-06", "relevancy": 2.3086, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4633}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4625}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Act%20in%20Collusion%3A%20A%20Persistent%20Distributed%20Multi-Target%20Backdoor%20in%0A%20%20Federated%20Learning&body=Title%3A%20Act%20in%20Collusion%3A%20A%20Persistent%20Distributed%20Multi-Target%20Backdoor%20in%0A%20%20Federated%20Learning%0AAuthor%3A%20Tao%20Liu%20and%20Wu%20Yang%20and%20Chen%20Xu%20and%20Jiguang%20Lv%20and%20Huanran%20Wang%20and%20Yuhang%20Zhang%20and%20Shuchun%20Xu%20and%20Dapeng%20Man%0AAbstract%3A%20%20%20Federated%20learning%2C%20a%20novel%20paradigm%20designed%20to%20protect%20data%20privacy%2C%20is%0Avulnerable%20to%20backdoor%20attacks%20due%20to%20its%20distributed%20nature.%20Current%20research%0Aoften%20designs%20attacks%20based%20on%20a%20single%20attacker%20with%20a%20single%20backdoor%2C%0Aoverlooking%20more%20realistic%20and%20complex%20threats%20in%20federated%20learning.%20We%0Apropose%20a%20more%20practical%20threat%20model%20for%20federated%20learning%3A%20the%20distributed%0Amulti-target%20backdoor.%20In%20this%20model%2C%20multiple%20attackers%20control%20different%0Aclients%2C%20embedding%20various%20triggers%20and%20targeting%20different%20classes%2C%0Acollaboratively%20implanting%20backdoors%20into%20the%20global%20model%20via%20central%0Aaggregation.%20Empirical%20validation%20shows%20that%20existing%20methods%20struggle%20to%0Amaintain%20the%20effectiveness%20of%20multiple%20backdoors%20in%20the%20global%20model.%20Our%20key%0Ainsight%20is%20that%20similar%20backdoor%20triggers%20cause%20parameter%20conflicts%20and%0Ainjecting%20new%20backdoors%20disrupts%20gradient%20directions%2C%20significantly%20weakening%0Asome%20backdoors%20performance.%20To%20solve%20this%2C%20we%20propose%20a%20Distributed%0AMulti-Target%20Backdoor%20Attack%20%28DMBA%29%2C%20ensuring%20efficiency%20and%20persistence%20of%0Abackdoors%20from%20different%20malicious%20clients.%20To%20avoid%20parameter%20conflicts%2C%20we%0Adesign%20a%20multi-channel%20dispersed%20frequency%20trigger%20strategy%20to%20maximize%20trigger%0Adifferences.%20To%20mitigate%20gradient%20interference%2C%20we%20introduce%20backdoor%20replay%20in%0Alocal%20training%20to%20neutralize%20conflicting%20gradients.%20Extensive%20validation%20shows%0Athat%2030%20rounds%20after%20the%20attack%2C%20Attack%20Success%20Rates%20of%20three%20different%0Abackdoors%20from%20various%20clients%20remain%20above%2093%25.%20The%20code%20will%20be%20made%20publicly%0Aavailable%20after%20the%20review%20period.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAct%2520in%2520Collusion%253A%2520A%2520Persistent%2520Distributed%2520Multi-Target%2520Backdoor%2520in%250A%2520%2520Federated%2520Learning%26entry.906535625%3DTao%2520Liu%2520and%2520Wu%2520Yang%2520and%2520Chen%2520Xu%2520and%2520Jiguang%2520Lv%2520and%2520Huanran%2520Wang%2520and%2520Yuhang%2520Zhang%2520and%2520Shuchun%2520Xu%2520and%2520Dapeng%2520Man%26entry.1292438233%3D%2520%2520Federated%2520learning%252C%2520a%2520novel%2520paradigm%2520designed%2520to%2520protect%2520data%2520privacy%252C%2520is%250Avulnerable%2520to%2520backdoor%2520attacks%2520due%2520to%2520its%2520distributed%2520nature.%2520Current%2520research%250Aoften%2520designs%2520attacks%2520based%2520on%2520a%2520single%2520attacker%2520with%2520a%2520single%2520backdoor%252C%250Aoverlooking%2520more%2520realistic%2520and%2520complex%2520threats%2520in%2520federated%2520learning.%2520We%250Apropose%2520a%2520more%2520practical%2520threat%2520model%2520for%2520federated%2520learning%253A%2520the%2520distributed%250Amulti-target%2520backdoor.%2520In%2520this%2520model%252C%2520multiple%2520attackers%2520control%2520different%250Aclients%252C%2520embedding%2520various%2520triggers%2520and%2520targeting%2520different%2520classes%252C%250Acollaboratively%2520implanting%2520backdoors%2520into%2520the%2520global%2520model%2520via%2520central%250Aaggregation.%2520Empirical%2520validation%2520shows%2520that%2520existing%2520methods%2520struggle%2520to%250Amaintain%2520the%2520effectiveness%2520of%2520multiple%2520backdoors%2520in%2520the%2520global%2520model.%2520Our%2520key%250Ainsight%2520is%2520that%2520similar%2520backdoor%2520triggers%2520cause%2520parameter%2520conflicts%2520and%250Ainjecting%2520new%2520backdoors%2520disrupts%2520gradient%2520directions%252C%2520significantly%2520weakening%250Asome%2520backdoors%2520performance.%2520To%2520solve%2520this%252C%2520we%2520propose%2520a%2520Distributed%250AMulti-Target%2520Backdoor%2520Attack%2520%2528DMBA%2529%252C%2520ensuring%2520efficiency%2520and%2520persistence%2520of%250Abackdoors%2520from%2520different%2520malicious%2520clients.%2520To%2520avoid%2520parameter%2520conflicts%252C%2520we%250Adesign%2520a%2520multi-channel%2520dispersed%2520frequency%2520trigger%2520strategy%2520to%2520maximize%2520trigger%250Adifferences.%2520To%2520mitigate%2520gradient%2520interference%252C%2520we%2520introduce%2520backdoor%2520replay%2520in%250Alocal%2520training%2520to%2520neutralize%2520conflicting%2520gradients.%2520Extensive%2520validation%2520shows%250Athat%252030%2520rounds%2520after%2520the%2520attack%252C%2520Attack%2520Success%2520Rates%2520of%2520three%2520different%250Abackdoors%2520from%2520various%2520clients%2520remain%2520above%252093%2525.%2520The%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520after%2520the%2520review%2520period.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Act%20in%20Collusion%3A%20A%20Persistent%20Distributed%20Multi-Target%20Backdoor%20in%0A%20%20Federated%20Learning&entry.906535625=Tao%20Liu%20and%20Wu%20Yang%20and%20Chen%20Xu%20and%20Jiguang%20Lv%20and%20Huanran%20Wang%20and%20Yuhang%20Zhang%20and%20Shuchun%20Xu%20and%20Dapeng%20Man&entry.1292438233=%20%20Federated%20learning%2C%20a%20novel%20paradigm%20designed%20to%20protect%20data%20privacy%2C%20is%0Avulnerable%20to%20backdoor%20attacks%20due%20to%20its%20distributed%20nature.%20Current%20research%0Aoften%20designs%20attacks%20based%20on%20a%20single%20attacker%20with%20a%20single%20backdoor%2C%0Aoverlooking%20more%20realistic%20and%20complex%20threats%20in%20federated%20learning.%20We%0Apropose%20a%20more%20practical%20threat%20model%20for%20federated%20learning%3A%20the%20distributed%0Amulti-target%20backdoor.%20In%20this%20model%2C%20multiple%20attackers%20control%20different%0Aclients%2C%20embedding%20various%20triggers%20and%20targeting%20different%20classes%2C%0Acollaboratively%20implanting%20backdoors%20into%20the%20global%20model%20via%20central%0Aaggregation.%20Empirical%20validation%20shows%20that%20existing%20methods%20struggle%20to%0Amaintain%20the%20effectiveness%20of%20multiple%20backdoors%20in%20the%20global%20model.%20Our%20key%0Ainsight%20is%20that%20similar%20backdoor%20triggers%20cause%20parameter%20conflicts%20and%0Ainjecting%20new%20backdoors%20disrupts%20gradient%20directions%2C%20significantly%20weakening%0Asome%20backdoors%20performance.%20To%20solve%20this%2C%20we%20propose%20a%20Distributed%0AMulti-Target%20Backdoor%20Attack%20%28DMBA%29%2C%20ensuring%20efficiency%20and%20persistence%20of%0Abackdoors%20from%20different%20malicious%20clients.%20To%20avoid%20parameter%20conflicts%2C%20we%0Adesign%20a%20multi-channel%20dispersed%20frequency%20trigger%20strategy%20to%20maximize%20trigger%0Adifferences.%20To%20mitigate%20gradient%20interference%2C%20we%20introduce%20backdoor%20replay%20in%0Alocal%20training%20to%20neutralize%20conflicting%20gradients.%20Extensive%20validation%20shows%0Athat%2030%20rounds%20after%20the%20attack%2C%20Attack%20Success%20Rates%20of%20three%20different%0Abackdoors%20from%20various%20clients%20remain%20above%2093%25.%20The%20code%20will%20be%20made%20publicly%0Aavailable%20after%20the%20review%20period.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03926v1&entry.124074799=Read"},
{"title": "Gradient Descent Finds Over-Parameterized Neural Networks with Sharp\n  Generalization for Nonparametric Regression: A Distribution-Free Analysis", "author": "Yingzhen Yang and Ping Li", "abstract": "  We study nonparametric regression by an over-parameterized two-layer neural\nnetwork trained by gradient descent (GD) in this paper. We show that, if the\nneural network is trained by GD with early stopping, then the trained network\nrenders a sharp rate of the nonparametric regression risk of $\\cO(\\eps_n^2)$,\nwhich is the same rate as that for the classical kernel regression trained by\nGD with early stopping, where $\\eps_n$ is the critical population rate of the\nNeural Tangent Kernel (NTK) associated with the network and $n$ is the size of\nthe training data. It is remarked that our result does not require\ndistributional assumptions on the training data, in a strong contrast with many\nexisting results which rely on specific distributions such as the spherical\nuniform data distribution or distributions satisfying certain restrictive\nconditions. The rate $\\cO(\\eps_n^2)$ is known to be minimax optimal for\nspecific cases, such as the case that the NTK has a polynomial eigenvalue decay\nrate which happens under certain distributional assumptions. Our result\nformally fills the gap between training a classical kernel regression model and\ntraining an over-parameterized but finite-width neural network by GD for\nnonparametric regression without distributional assumptions. We also provide\nconfirmative answers to certain open questions or address particular concerns\nin the literature of training over-parameterized neural networks by GD with\nearly stopping for nonparametric regression, including the characterization of\nthe stopping time, the lower bound for the network width, and the constant\nlearning rate used in GD.\n", "link": "http://arxiv.org/abs/2411.02904v2", "date": "2024-11-06", "relevancy": 2.3053, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5085}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4394}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Descent%20Finds%20Over-Parameterized%20Neural%20Networks%20with%20Sharp%0A%20%20Generalization%20for%20Nonparametric%20Regression%3A%20A%20Distribution-Free%20Analysis&body=Title%3A%20Gradient%20Descent%20Finds%20Over-Parameterized%20Neural%20Networks%20with%20Sharp%0A%20%20Generalization%20for%20Nonparametric%20Regression%3A%20A%20Distribution-Free%20Analysis%0AAuthor%3A%20Yingzhen%20Yang%20and%20Ping%20Li%0AAbstract%3A%20%20%20We%20study%20nonparametric%20regression%20by%20an%20over-parameterized%20two-layer%20neural%0Anetwork%20trained%20by%20gradient%20descent%20%28GD%29%20in%20this%20paper.%20We%20show%20that%2C%20if%20the%0Aneural%20network%20is%20trained%20by%20GD%20with%20early%20stopping%2C%20then%20the%20trained%20network%0Arenders%20a%20sharp%20rate%20of%20the%20nonparametric%20regression%20risk%20of%20%24%5CcO%28%5Ceps_n%5E2%29%24%2C%0Awhich%20is%20the%20same%20rate%20as%20that%20for%20the%20classical%20kernel%20regression%20trained%20by%0AGD%20with%20early%20stopping%2C%20where%20%24%5Ceps_n%24%20is%20the%20critical%20population%20rate%20of%20the%0ANeural%20Tangent%20Kernel%20%28NTK%29%20associated%20with%20the%20network%20and%20%24n%24%20is%20the%20size%20of%0Athe%20training%20data.%20It%20is%20remarked%20that%20our%20result%20does%20not%20require%0Adistributional%20assumptions%20on%20the%20training%20data%2C%20in%20a%20strong%20contrast%20with%20many%0Aexisting%20results%20which%20rely%20on%20specific%20distributions%20such%20as%20the%20spherical%0Auniform%20data%20distribution%20or%20distributions%20satisfying%20certain%20restrictive%0Aconditions.%20The%20rate%20%24%5CcO%28%5Ceps_n%5E2%29%24%20is%20known%20to%20be%20minimax%20optimal%20for%0Aspecific%20cases%2C%20such%20as%20the%20case%20that%20the%20NTK%20has%20a%20polynomial%20eigenvalue%20decay%0Arate%20which%20happens%20under%20certain%20distributional%20assumptions.%20Our%20result%0Aformally%20fills%20the%20gap%20between%20training%20a%20classical%20kernel%20regression%20model%20and%0Atraining%20an%20over-parameterized%20but%20finite-width%20neural%20network%20by%20GD%20for%0Anonparametric%20regression%20without%20distributional%20assumptions.%20We%20also%20provide%0Aconfirmative%20answers%20to%20certain%20open%20questions%20or%20address%20particular%20concerns%0Ain%20the%20literature%20of%20training%20over-parameterized%20neural%20networks%20by%20GD%20with%0Aearly%20stopping%20for%20nonparametric%20regression%2C%20including%20the%20characterization%20of%0Athe%20stopping%20time%2C%20the%20lower%20bound%20for%20the%20network%20width%2C%20and%20the%20constant%0Alearning%20rate%20used%20in%20GD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Descent%2520Finds%2520Over-Parameterized%2520Neural%2520Networks%2520with%2520Sharp%250A%2520%2520Generalization%2520for%2520Nonparametric%2520Regression%253A%2520A%2520Distribution-Free%2520Analysis%26entry.906535625%3DYingzhen%2520Yang%2520and%2520Ping%2520Li%26entry.1292438233%3D%2520%2520We%2520study%2520nonparametric%2520regression%2520by%2520an%2520over-parameterized%2520two-layer%2520neural%250Anetwork%2520trained%2520by%2520gradient%2520descent%2520%2528GD%2529%2520in%2520this%2520paper.%2520We%2520show%2520that%252C%2520if%2520the%250Aneural%2520network%2520is%2520trained%2520by%2520GD%2520with%2520early%2520stopping%252C%2520then%2520the%2520trained%2520network%250Arenders%2520a%2520sharp%2520rate%2520of%2520the%2520nonparametric%2520regression%2520risk%2520of%2520%2524%255CcO%2528%255Ceps_n%255E2%2529%2524%252C%250Awhich%2520is%2520the%2520same%2520rate%2520as%2520that%2520for%2520the%2520classical%2520kernel%2520regression%2520trained%2520by%250AGD%2520with%2520early%2520stopping%252C%2520where%2520%2524%255Ceps_n%2524%2520is%2520the%2520critical%2520population%2520rate%2520of%2520the%250ANeural%2520Tangent%2520Kernel%2520%2528NTK%2529%2520associated%2520with%2520the%2520network%2520and%2520%2524n%2524%2520is%2520the%2520size%2520of%250Athe%2520training%2520data.%2520It%2520is%2520remarked%2520that%2520our%2520result%2520does%2520not%2520require%250Adistributional%2520assumptions%2520on%2520the%2520training%2520data%252C%2520in%2520a%2520strong%2520contrast%2520with%2520many%250Aexisting%2520results%2520which%2520rely%2520on%2520specific%2520distributions%2520such%2520as%2520the%2520spherical%250Auniform%2520data%2520distribution%2520or%2520distributions%2520satisfying%2520certain%2520restrictive%250Aconditions.%2520The%2520rate%2520%2524%255CcO%2528%255Ceps_n%255E2%2529%2524%2520is%2520known%2520to%2520be%2520minimax%2520optimal%2520for%250Aspecific%2520cases%252C%2520such%2520as%2520the%2520case%2520that%2520the%2520NTK%2520has%2520a%2520polynomial%2520eigenvalue%2520decay%250Arate%2520which%2520happens%2520under%2520certain%2520distributional%2520assumptions.%2520Our%2520result%250Aformally%2520fills%2520the%2520gap%2520between%2520training%2520a%2520classical%2520kernel%2520regression%2520model%2520and%250Atraining%2520an%2520over-parameterized%2520but%2520finite-width%2520neural%2520network%2520by%2520GD%2520for%250Anonparametric%2520regression%2520without%2520distributional%2520assumptions.%2520We%2520also%2520provide%250Aconfirmative%2520answers%2520to%2520certain%2520open%2520questions%2520or%2520address%2520particular%2520concerns%250Ain%2520the%2520literature%2520of%2520training%2520over-parameterized%2520neural%2520networks%2520by%2520GD%2520with%250Aearly%2520stopping%2520for%2520nonparametric%2520regression%252C%2520including%2520the%2520characterization%2520of%250Athe%2520stopping%2520time%252C%2520the%2520lower%2520bound%2520for%2520the%2520network%2520width%252C%2520and%2520the%2520constant%250Alearning%2520rate%2520used%2520in%2520GD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Descent%20Finds%20Over-Parameterized%20Neural%20Networks%20with%20Sharp%0A%20%20Generalization%20for%20Nonparametric%20Regression%3A%20A%20Distribution-Free%20Analysis&entry.906535625=Yingzhen%20Yang%20and%20Ping%20Li&entry.1292438233=%20%20We%20study%20nonparametric%20regression%20by%20an%20over-parameterized%20two-layer%20neural%0Anetwork%20trained%20by%20gradient%20descent%20%28GD%29%20in%20this%20paper.%20We%20show%20that%2C%20if%20the%0Aneural%20network%20is%20trained%20by%20GD%20with%20early%20stopping%2C%20then%20the%20trained%20network%0Arenders%20a%20sharp%20rate%20of%20the%20nonparametric%20regression%20risk%20of%20%24%5CcO%28%5Ceps_n%5E2%29%24%2C%0Awhich%20is%20the%20same%20rate%20as%20that%20for%20the%20classical%20kernel%20regression%20trained%20by%0AGD%20with%20early%20stopping%2C%20where%20%24%5Ceps_n%24%20is%20the%20critical%20population%20rate%20of%20the%0ANeural%20Tangent%20Kernel%20%28NTK%29%20associated%20with%20the%20network%20and%20%24n%24%20is%20the%20size%20of%0Athe%20training%20data.%20It%20is%20remarked%20that%20our%20result%20does%20not%20require%0Adistributional%20assumptions%20on%20the%20training%20data%2C%20in%20a%20strong%20contrast%20with%20many%0Aexisting%20results%20which%20rely%20on%20specific%20distributions%20such%20as%20the%20spherical%0Auniform%20data%20distribution%20or%20distributions%20satisfying%20certain%20restrictive%0Aconditions.%20The%20rate%20%24%5CcO%28%5Ceps_n%5E2%29%24%20is%20known%20to%20be%20minimax%20optimal%20for%0Aspecific%20cases%2C%20such%20as%20the%20case%20that%20the%20NTK%20has%20a%20polynomial%20eigenvalue%20decay%0Arate%20which%20happens%20under%20certain%20distributional%20assumptions.%20Our%20result%0Aformally%20fills%20the%20gap%20between%20training%20a%20classical%20kernel%20regression%20model%20and%0Atraining%20an%20over-parameterized%20but%20finite-width%20neural%20network%20by%20GD%20for%0Anonparametric%20regression%20without%20distributional%20assumptions.%20We%20also%20provide%0Aconfirmative%20answers%20to%20certain%20open%20questions%20or%20address%20particular%20concerns%0Ain%20the%20literature%20of%20training%20over-parameterized%20neural%20networks%20by%20GD%20with%0Aearly%20stopping%20for%20nonparametric%20regression%2C%20including%20the%20characterization%20of%0Athe%20stopping%20time%2C%20the%20lower%20bound%20for%20the%20network%20width%2C%20and%20the%20constant%0Alearning%20rate%20used%20in%20GD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02904v2&entry.124074799=Read"},
{"title": "The Selective G-Bispectrum and its Inversion: Applications to\n  G-Invariant Networks", "author": "Simon Mataigne and Johan Mathe and Sophia Sanborn and Christopher Hillar and Nina Miolane", "abstract": "  An important problem in signal processing and deep learning is to achieve\n\\textit{invariance} to nuisance factors not relevant for the task. Since many\nof these factors are describable as the action of a group $G$ (e.g. rotations,\ntranslations, scalings), we want methods to be $G$-invariant. The\n$G$-Bispectrum extracts every characteristic of a given signal up to group\naction: for example, the shape of an object in an image, but not its\norientation. Consequently, the $G$-Bispectrum has been incorporated into deep\nneural network architectures as a computational primitive for\n$G$-invariance\\textemdash akin to a pooling mechanism, but with greater\nselectivity and robustness. However, the computational cost of the\n$G$-Bispectrum ($\\mathcal{O}(|G|^2)$, with $|G|$ the size of the group) has\nlimited its widespread adoption. Here, we show that the $G$-Bispectrum\ncomputation contains redundancies that can be reduced into a \\textit{selective\n$G$-Bispectrum} with $\\mathcal{O}(|G|)$ complexity. We prove desirable\nmathematical properties of the selective $G$-Bispectrum and demonstrate how its\nintegration in neural networks enhances accuracy and robustness compared to\ntraditional approaches, while enjoying considerable speeds-up compared to the\nfull $G$-Bispectrum.\n", "link": "http://arxiv.org/abs/2407.07655v2", "date": "2024-11-06", "relevancy": 2.3043, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4788}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4552}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Selective%20G-Bispectrum%20and%20its%20Inversion%3A%20Applications%20to%0A%20%20G-Invariant%20Networks&body=Title%3A%20The%20Selective%20G-Bispectrum%20and%20its%20Inversion%3A%20Applications%20to%0A%20%20G-Invariant%20Networks%0AAuthor%3A%20Simon%20Mataigne%20and%20Johan%20Mathe%20and%20Sophia%20Sanborn%20and%20Christopher%20Hillar%20and%20Nina%20Miolane%0AAbstract%3A%20%20%20An%20important%20problem%20in%20signal%20processing%20and%20deep%20learning%20is%20to%20achieve%0A%5Ctextit%7Binvariance%7D%20to%20nuisance%20factors%20not%20relevant%20for%20the%20task.%20Since%20many%0Aof%20these%20factors%20are%20describable%20as%20the%20action%20of%20a%20group%20%24G%24%20%28e.g.%20rotations%2C%0Atranslations%2C%20scalings%29%2C%20we%20want%20methods%20to%20be%20%24G%24-invariant.%20The%0A%24G%24-Bispectrum%20extracts%20every%20characteristic%20of%20a%20given%20signal%20up%20to%20group%0Aaction%3A%20for%20example%2C%20the%20shape%20of%20an%20object%20in%20an%20image%2C%20but%20not%20its%0Aorientation.%20Consequently%2C%20the%20%24G%24-Bispectrum%20has%20been%20incorporated%20into%20deep%0Aneural%20network%20architectures%20as%20a%20computational%20primitive%20for%0A%24G%24-invariance%5Ctextemdash%20akin%20to%20a%20pooling%20mechanism%2C%20but%20with%20greater%0Aselectivity%20and%20robustness.%20However%2C%20the%20computational%20cost%20of%20the%0A%24G%24-Bispectrum%20%28%24%5Cmathcal%7BO%7D%28%7CG%7C%5E2%29%24%2C%20with%20%24%7CG%7C%24%20the%20size%20of%20the%20group%29%20has%0Alimited%20its%20widespread%20adoption.%20Here%2C%20we%20show%20that%20the%20%24G%24-Bispectrum%0Acomputation%20contains%20redundancies%20that%20can%20be%20reduced%20into%20a%20%5Ctextit%7Bselective%0A%24G%24-Bispectrum%7D%20with%20%24%5Cmathcal%7BO%7D%28%7CG%7C%29%24%20complexity.%20We%20prove%20desirable%0Amathematical%20properties%20of%20the%20selective%20%24G%24-Bispectrum%20and%20demonstrate%20how%20its%0Aintegration%20in%20neural%20networks%20enhances%20accuracy%20and%20robustness%20compared%20to%0Atraditional%20approaches%2C%20while%20enjoying%20considerable%20speeds-up%20compared%20to%20the%0Afull%20%24G%24-Bispectrum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Selective%2520G-Bispectrum%2520and%2520its%2520Inversion%253A%2520Applications%2520to%250A%2520%2520G-Invariant%2520Networks%26entry.906535625%3DSimon%2520Mataigne%2520and%2520Johan%2520Mathe%2520and%2520Sophia%2520Sanborn%2520and%2520Christopher%2520Hillar%2520and%2520Nina%2520Miolane%26entry.1292438233%3D%2520%2520An%2520important%2520problem%2520in%2520signal%2520processing%2520and%2520deep%2520learning%2520is%2520to%2520achieve%250A%255Ctextit%257Binvariance%257D%2520to%2520nuisance%2520factors%2520not%2520relevant%2520for%2520the%2520task.%2520Since%2520many%250Aof%2520these%2520factors%2520are%2520describable%2520as%2520the%2520action%2520of%2520a%2520group%2520%2524G%2524%2520%2528e.g.%2520rotations%252C%250Atranslations%252C%2520scalings%2529%252C%2520we%2520want%2520methods%2520to%2520be%2520%2524G%2524-invariant.%2520The%250A%2524G%2524-Bispectrum%2520extracts%2520every%2520characteristic%2520of%2520a%2520given%2520signal%2520up%2520to%2520group%250Aaction%253A%2520for%2520example%252C%2520the%2520shape%2520of%2520an%2520object%2520in%2520an%2520image%252C%2520but%2520not%2520its%250Aorientation.%2520Consequently%252C%2520the%2520%2524G%2524-Bispectrum%2520has%2520been%2520incorporated%2520into%2520deep%250Aneural%2520network%2520architectures%2520as%2520a%2520computational%2520primitive%2520for%250A%2524G%2524-invariance%255Ctextemdash%2520akin%2520to%2520a%2520pooling%2520mechanism%252C%2520but%2520with%2520greater%250Aselectivity%2520and%2520robustness.%2520However%252C%2520the%2520computational%2520cost%2520of%2520the%250A%2524G%2524-Bispectrum%2520%2528%2524%255Cmathcal%257BO%257D%2528%257CG%257C%255E2%2529%2524%252C%2520with%2520%2524%257CG%257C%2524%2520the%2520size%2520of%2520the%2520group%2529%2520has%250Alimited%2520its%2520widespread%2520adoption.%2520Here%252C%2520we%2520show%2520that%2520the%2520%2524G%2524-Bispectrum%250Acomputation%2520contains%2520redundancies%2520that%2520can%2520be%2520reduced%2520into%2520a%2520%255Ctextit%257Bselective%250A%2524G%2524-Bispectrum%257D%2520with%2520%2524%255Cmathcal%257BO%257D%2528%257CG%257C%2529%2524%2520complexity.%2520We%2520prove%2520desirable%250Amathematical%2520properties%2520of%2520the%2520selective%2520%2524G%2524-Bispectrum%2520and%2520demonstrate%2520how%2520its%250Aintegration%2520in%2520neural%2520networks%2520enhances%2520accuracy%2520and%2520robustness%2520compared%2520to%250Atraditional%2520approaches%252C%2520while%2520enjoying%2520considerable%2520speeds-up%2520compared%2520to%2520the%250Afull%2520%2524G%2524-Bispectrum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Selective%20G-Bispectrum%20and%20its%20Inversion%3A%20Applications%20to%0A%20%20G-Invariant%20Networks&entry.906535625=Simon%20Mataigne%20and%20Johan%20Mathe%20and%20Sophia%20Sanborn%20and%20Christopher%20Hillar%20and%20Nina%20Miolane&entry.1292438233=%20%20An%20important%20problem%20in%20signal%20processing%20and%20deep%20learning%20is%20to%20achieve%0A%5Ctextit%7Binvariance%7D%20to%20nuisance%20factors%20not%20relevant%20for%20the%20task.%20Since%20many%0Aof%20these%20factors%20are%20describable%20as%20the%20action%20of%20a%20group%20%24G%24%20%28e.g.%20rotations%2C%0Atranslations%2C%20scalings%29%2C%20we%20want%20methods%20to%20be%20%24G%24-invariant.%20The%0A%24G%24-Bispectrum%20extracts%20every%20characteristic%20of%20a%20given%20signal%20up%20to%20group%0Aaction%3A%20for%20example%2C%20the%20shape%20of%20an%20object%20in%20an%20image%2C%20but%20not%20its%0Aorientation.%20Consequently%2C%20the%20%24G%24-Bispectrum%20has%20been%20incorporated%20into%20deep%0Aneural%20network%20architectures%20as%20a%20computational%20primitive%20for%0A%24G%24-invariance%5Ctextemdash%20akin%20to%20a%20pooling%20mechanism%2C%20but%20with%20greater%0Aselectivity%20and%20robustness.%20However%2C%20the%20computational%20cost%20of%20the%0A%24G%24-Bispectrum%20%28%24%5Cmathcal%7BO%7D%28%7CG%7C%5E2%29%24%2C%20with%20%24%7CG%7C%24%20the%20size%20of%20the%20group%29%20has%0Alimited%20its%20widespread%20adoption.%20Here%2C%20we%20show%20that%20the%20%24G%24-Bispectrum%0Acomputation%20contains%20redundancies%20that%20can%20be%20reduced%20into%20a%20%5Ctextit%7Bselective%0A%24G%24-Bispectrum%7D%20with%20%24%5Cmathcal%7BO%7D%28%7CG%7C%29%24%20complexity.%20We%20prove%20desirable%0Amathematical%20properties%20of%20the%20selective%20%24G%24-Bispectrum%20and%20demonstrate%20how%20its%0Aintegration%20in%20neural%20networks%20enhances%20accuracy%20and%20robustness%20compared%20to%0Atraditional%20approaches%2C%20while%20enjoying%20considerable%20speeds-up%20compared%20to%20the%0Afull%20%24G%24-Bispectrum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07655v2&entry.124074799=Read"},
{"title": "Quantum Algorithm for Sparse Online Learning with Truncated Gradient\n  Descent", "author": "Debbie Lim and Yixian Qiu and Patrick Rebentrost and Qisheng Wang", "abstract": "  Logistic regression, the Support Vector Machine (SVM), and least squares are\nwell-studied methods in the statistical and computer science community, with\nvarious practical applications. High-dimensional data arriving on a real-time\nbasis makes the design of online learning algorithms that produce sparse\nsolutions essential. The seminal work of\n\\hyperlink{cite.langford2009sparse}{Langford, Li, and Zhang (2009)} developed a\nmethod to obtain sparsity via truncated gradient descent, showing a\nnear-optimal online regret bound. Based on this method, we develop a quantum\nsparse online learning algorithm for logistic regression, the SVM, and least\nsquares. Given efficient quantum access to the inputs, we show that a quadratic\nspeedup in the time complexity with respect to the dimension of the problem is\nachievable, while maintaining a regret of $O(1/\\sqrt{T})$, where $T$ is the\nnumber of iterations.\n", "link": "http://arxiv.org/abs/2411.03925v1", "date": "2024-11-06", "relevancy": 2.2989, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4663}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.463}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Algorithm%20for%20Sparse%20Online%20Learning%20with%20Truncated%20Gradient%0A%20%20Descent&body=Title%3A%20Quantum%20Algorithm%20for%20Sparse%20Online%20Learning%20with%20Truncated%20Gradient%0A%20%20Descent%0AAuthor%3A%20Debbie%20Lim%20and%20Yixian%20Qiu%20and%20Patrick%20Rebentrost%20and%20Qisheng%20Wang%0AAbstract%3A%20%20%20Logistic%20regression%2C%20the%20Support%20Vector%20Machine%20%28SVM%29%2C%20and%20least%20squares%20are%0Awell-studied%20methods%20in%20the%20statistical%20and%20computer%20science%20community%2C%20with%0Avarious%20practical%20applications.%20High-dimensional%20data%20arriving%20on%20a%20real-time%0Abasis%20makes%20the%20design%20of%20online%20learning%20algorithms%20that%20produce%20sparse%0Asolutions%20essential.%20The%20seminal%20work%20of%0A%5Chyperlink%7Bcite.langford2009sparse%7D%7BLangford%2C%20Li%2C%20and%20Zhang%20%282009%29%7D%20developed%20a%0Amethod%20to%20obtain%20sparsity%20via%20truncated%20gradient%20descent%2C%20showing%20a%0Anear-optimal%20online%20regret%20bound.%20Based%20on%20this%20method%2C%20we%20develop%20a%20quantum%0Asparse%20online%20learning%20algorithm%20for%20logistic%20regression%2C%20the%20SVM%2C%20and%20least%0Asquares.%20Given%20efficient%20quantum%20access%20to%20the%20inputs%2C%20we%20show%20that%20a%20quadratic%0Aspeedup%20in%20the%20time%20complexity%20with%20respect%20to%20the%20dimension%20of%20the%20problem%20is%0Aachievable%2C%20while%20maintaining%20a%20regret%20of%20%24O%281/%5Csqrt%7BT%7D%29%24%2C%20where%20%24T%24%20is%20the%0Anumber%20of%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Algorithm%2520for%2520Sparse%2520Online%2520Learning%2520with%2520Truncated%2520Gradient%250A%2520%2520Descent%26entry.906535625%3DDebbie%2520Lim%2520and%2520Yixian%2520Qiu%2520and%2520Patrick%2520Rebentrost%2520and%2520Qisheng%2520Wang%26entry.1292438233%3D%2520%2520Logistic%2520regression%252C%2520the%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%2520and%2520least%2520squares%2520are%250Awell-studied%2520methods%2520in%2520the%2520statistical%2520and%2520computer%2520science%2520community%252C%2520with%250Avarious%2520practical%2520applications.%2520High-dimensional%2520data%2520arriving%2520on%2520a%2520real-time%250Abasis%2520makes%2520the%2520design%2520of%2520online%2520learning%2520algorithms%2520that%2520produce%2520sparse%250Asolutions%2520essential.%2520The%2520seminal%2520work%2520of%250A%255Chyperlink%257Bcite.langford2009sparse%257D%257BLangford%252C%2520Li%252C%2520and%2520Zhang%2520%25282009%2529%257D%2520developed%2520a%250Amethod%2520to%2520obtain%2520sparsity%2520via%2520truncated%2520gradient%2520descent%252C%2520showing%2520a%250Anear-optimal%2520online%2520regret%2520bound.%2520Based%2520on%2520this%2520method%252C%2520we%2520develop%2520a%2520quantum%250Asparse%2520online%2520learning%2520algorithm%2520for%2520logistic%2520regression%252C%2520the%2520SVM%252C%2520and%2520least%250Asquares.%2520Given%2520efficient%2520quantum%2520access%2520to%2520the%2520inputs%252C%2520we%2520show%2520that%2520a%2520quadratic%250Aspeedup%2520in%2520the%2520time%2520complexity%2520with%2520respect%2520to%2520the%2520dimension%2520of%2520the%2520problem%2520is%250Aachievable%252C%2520while%2520maintaining%2520a%2520regret%2520of%2520%2524O%25281/%255Csqrt%257BT%257D%2529%2524%252C%2520where%2520%2524T%2524%2520is%2520the%250Anumber%2520of%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Algorithm%20for%20Sparse%20Online%20Learning%20with%20Truncated%20Gradient%0A%20%20Descent&entry.906535625=Debbie%20Lim%20and%20Yixian%20Qiu%20and%20Patrick%20Rebentrost%20and%20Qisheng%20Wang&entry.1292438233=%20%20Logistic%20regression%2C%20the%20Support%20Vector%20Machine%20%28SVM%29%2C%20and%20least%20squares%20are%0Awell-studied%20methods%20in%20the%20statistical%20and%20computer%20science%20community%2C%20with%0Avarious%20practical%20applications.%20High-dimensional%20data%20arriving%20on%20a%20real-time%0Abasis%20makes%20the%20design%20of%20online%20learning%20algorithms%20that%20produce%20sparse%0Asolutions%20essential.%20The%20seminal%20work%20of%0A%5Chyperlink%7Bcite.langford2009sparse%7D%7BLangford%2C%20Li%2C%20and%20Zhang%20%282009%29%7D%20developed%20a%0Amethod%20to%20obtain%20sparsity%20via%20truncated%20gradient%20descent%2C%20showing%20a%0Anear-optimal%20online%20regret%20bound.%20Based%20on%20this%20method%2C%20we%20develop%20a%20quantum%0Asparse%20online%20learning%20algorithm%20for%20logistic%20regression%2C%20the%20SVM%2C%20and%20least%0Asquares.%20Given%20efficient%20quantum%20access%20to%20the%20inputs%2C%20we%20show%20that%20a%20quadratic%0Aspeedup%20in%20the%20time%20complexity%20with%20respect%20to%20the%20dimension%20of%20the%20problem%20is%0Aachievable%2C%20while%20maintaining%20a%20regret%20of%20%24O%281/%5Csqrt%7BT%7D%29%24%2C%20where%20%24T%24%20is%20the%0Anumber%20of%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03925v1&entry.124074799=Read"},
{"title": "SynCode: LLM Generation with Grammar Augmentation", "author": "Shubham Ugare and Tarun Suresh and Hangoo Kang and Sasa Misailovic and Gagandeep Singh", "abstract": "  LLMs are widely used in complex AI applications. These applications\nunderscore the need for LLM outputs to adhere to a specific format, for their\nintegration with other components in the systems. Typically the format rules\ne.g., for data serialization formats such as JSON, YAML, or Code in Programming\nLanguage are expressed as context-free grammar (CFG). Due to the hallucinations\nand unreliability of LLMs, instructing LLMs to adhere to specified syntax\nbecomes an increasingly important challenge.\n  We present SynCode, a novel framework for efficient and general syntactical\ndecoding with LLMs, to address this challenge. SynCode ensures soundness and\ncompleteness with respect to the CFG of a formal language, effectively\nretaining valid tokens while filtering out invalid ones. SynCode uses an\noffline-constructed, efficient lookup table, the DFA mask store, derived from\nthe DFA of the language's grammar for efficient generation. SynCode seamlessly\nintegrates with any language defined by CFG, as evidenced by experiments\nfocusing on generating JSON, Python, and Go outputs. Our experiments evaluating\nthe effectiveness of SynCode for JSON generation demonstrate that SynCode\neliminates all syntax errors and significantly outperforms state-of-the-art\nbaselines. Furthermore, our results underscore how SynCode significantly\nreduces 96.07% of syntax errors in generated Python and Go code, showcasing its\nsubstantial impact on enhancing syntactical precision in LLM generation. Our\ncode is available at https://github.com/uiuc-focal-lab/syncode\n", "link": "http://arxiv.org/abs/2403.01632v4", "date": "2024-11-06", "relevancy": 2.2777, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynCode%3A%20LLM%20Generation%20with%20Grammar%20Augmentation&body=Title%3A%20SynCode%3A%20LLM%20Generation%20with%20Grammar%20Augmentation%0AAuthor%3A%20Shubham%20Ugare%20and%20Tarun%20Suresh%20and%20Hangoo%20Kang%20and%20Sasa%20Misailovic%20and%20Gagandeep%20Singh%0AAbstract%3A%20%20%20LLMs%20are%20widely%20used%20in%20complex%20AI%20applications.%20These%20applications%0Aunderscore%20the%20need%20for%20LLM%20outputs%20to%20adhere%20to%20a%20specific%20format%2C%20for%20their%0Aintegration%20with%20other%20components%20in%20the%20systems.%20Typically%20the%20format%20rules%0Ae.g.%2C%20for%20data%20serialization%20formats%20such%20as%20JSON%2C%20YAML%2C%20or%20Code%20in%20Programming%0ALanguage%20are%20expressed%20as%20context-free%20grammar%20%28CFG%29.%20Due%20to%20the%20hallucinations%0Aand%20unreliability%20of%20LLMs%2C%20instructing%20LLMs%20to%20adhere%20to%20specified%20syntax%0Abecomes%20an%20increasingly%20important%20challenge.%0A%20%20We%20present%20SynCode%2C%20a%20novel%20framework%20for%20efficient%20and%20general%20syntactical%0Adecoding%20with%20LLMs%2C%20to%20address%20this%20challenge.%20SynCode%20ensures%20soundness%20and%0Acompleteness%20with%20respect%20to%20the%20CFG%20of%20a%20formal%20language%2C%20effectively%0Aretaining%20valid%20tokens%20while%20filtering%20out%20invalid%20ones.%20SynCode%20uses%20an%0Aoffline-constructed%2C%20efficient%20lookup%20table%2C%20the%20DFA%20mask%20store%2C%20derived%20from%0Athe%20DFA%20of%20the%20language%27s%20grammar%20for%20efficient%20generation.%20SynCode%20seamlessly%0Aintegrates%20with%20any%20language%20defined%20by%20CFG%2C%20as%20evidenced%20by%20experiments%0Afocusing%20on%20generating%20JSON%2C%20Python%2C%20and%20Go%20outputs.%20Our%20experiments%20evaluating%0Athe%20effectiveness%20of%20SynCode%20for%20JSON%20generation%20demonstrate%20that%20SynCode%0Aeliminates%20all%20syntax%20errors%20and%20significantly%20outperforms%20state-of-the-art%0Abaselines.%20Furthermore%2C%20our%20results%20underscore%20how%20SynCode%20significantly%0Areduces%2096.07%25%20of%20syntax%20errors%20in%20generated%20Python%20and%20Go%20code%2C%20showcasing%20its%0Asubstantial%20impact%20on%20enhancing%20syntactical%20precision%20in%20LLM%20generation.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/uiuc-focal-lab/syncode%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01632v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynCode%253A%2520LLM%2520Generation%2520with%2520Grammar%2520Augmentation%26entry.906535625%3DShubham%2520Ugare%2520and%2520Tarun%2520Suresh%2520and%2520Hangoo%2520Kang%2520and%2520Sasa%2520Misailovic%2520and%2520Gagandeep%2520Singh%26entry.1292438233%3D%2520%2520LLMs%2520are%2520widely%2520used%2520in%2520complex%2520AI%2520applications.%2520These%2520applications%250Aunderscore%2520the%2520need%2520for%2520LLM%2520outputs%2520to%2520adhere%2520to%2520a%2520specific%2520format%252C%2520for%2520their%250Aintegration%2520with%2520other%2520components%2520in%2520the%2520systems.%2520Typically%2520the%2520format%2520rules%250Ae.g.%252C%2520for%2520data%2520serialization%2520formats%2520such%2520as%2520JSON%252C%2520YAML%252C%2520or%2520Code%2520in%2520Programming%250ALanguage%2520are%2520expressed%2520as%2520context-free%2520grammar%2520%2528CFG%2529.%2520Due%2520to%2520the%2520hallucinations%250Aand%2520unreliability%2520of%2520LLMs%252C%2520instructing%2520LLMs%2520to%2520adhere%2520to%2520specified%2520syntax%250Abecomes%2520an%2520increasingly%2520important%2520challenge.%250A%2520%2520We%2520present%2520SynCode%252C%2520a%2520novel%2520framework%2520for%2520efficient%2520and%2520general%2520syntactical%250Adecoding%2520with%2520LLMs%252C%2520to%2520address%2520this%2520challenge.%2520SynCode%2520ensures%2520soundness%2520and%250Acompleteness%2520with%2520respect%2520to%2520the%2520CFG%2520of%2520a%2520formal%2520language%252C%2520effectively%250Aretaining%2520valid%2520tokens%2520while%2520filtering%2520out%2520invalid%2520ones.%2520SynCode%2520uses%2520an%250Aoffline-constructed%252C%2520efficient%2520lookup%2520table%252C%2520the%2520DFA%2520mask%2520store%252C%2520derived%2520from%250Athe%2520DFA%2520of%2520the%2520language%2527s%2520grammar%2520for%2520efficient%2520generation.%2520SynCode%2520seamlessly%250Aintegrates%2520with%2520any%2520language%2520defined%2520by%2520CFG%252C%2520as%2520evidenced%2520by%2520experiments%250Afocusing%2520on%2520generating%2520JSON%252C%2520Python%252C%2520and%2520Go%2520outputs.%2520Our%2520experiments%2520evaluating%250Athe%2520effectiveness%2520of%2520SynCode%2520for%2520JSON%2520generation%2520demonstrate%2520that%2520SynCode%250Aeliminates%2520all%2520syntax%2520errors%2520and%2520significantly%2520outperforms%2520state-of-the-art%250Abaselines.%2520Furthermore%252C%2520our%2520results%2520underscore%2520how%2520SynCode%2520significantly%250Areduces%252096.07%2525%2520of%2520syntax%2520errors%2520in%2520generated%2520Python%2520and%2520Go%2520code%252C%2520showcasing%2520its%250Asubstantial%2520impact%2520on%2520enhancing%2520syntactical%2520precision%2520in%2520LLM%2520generation.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/uiuc-focal-lab/syncode%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01632v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynCode%3A%20LLM%20Generation%20with%20Grammar%20Augmentation&entry.906535625=Shubham%20Ugare%20and%20Tarun%20Suresh%20and%20Hangoo%20Kang%20and%20Sasa%20Misailovic%20and%20Gagandeep%20Singh&entry.1292438233=%20%20LLMs%20are%20widely%20used%20in%20complex%20AI%20applications.%20These%20applications%0Aunderscore%20the%20need%20for%20LLM%20outputs%20to%20adhere%20to%20a%20specific%20format%2C%20for%20their%0Aintegration%20with%20other%20components%20in%20the%20systems.%20Typically%20the%20format%20rules%0Ae.g.%2C%20for%20data%20serialization%20formats%20such%20as%20JSON%2C%20YAML%2C%20or%20Code%20in%20Programming%0ALanguage%20are%20expressed%20as%20context-free%20grammar%20%28CFG%29.%20Due%20to%20the%20hallucinations%0Aand%20unreliability%20of%20LLMs%2C%20instructing%20LLMs%20to%20adhere%20to%20specified%20syntax%0Abecomes%20an%20increasingly%20important%20challenge.%0A%20%20We%20present%20SynCode%2C%20a%20novel%20framework%20for%20efficient%20and%20general%20syntactical%0Adecoding%20with%20LLMs%2C%20to%20address%20this%20challenge.%20SynCode%20ensures%20soundness%20and%0Acompleteness%20with%20respect%20to%20the%20CFG%20of%20a%20formal%20language%2C%20effectively%0Aretaining%20valid%20tokens%20while%20filtering%20out%20invalid%20ones.%20SynCode%20uses%20an%0Aoffline-constructed%2C%20efficient%20lookup%20table%2C%20the%20DFA%20mask%20store%2C%20derived%20from%0Athe%20DFA%20of%20the%20language%27s%20grammar%20for%20efficient%20generation.%20SynCode%20seamlessly%0Aintegrates%20with%20any%20language%20defined%20by%20CFG%2C%20as%20evidenced%20by%20experiments%0Afocusing%20on%20generating%20JSON%2C%20Python%2C%20and%20Go%20outputs.%20Our%20experiments%20evaluating%0Athe%20effectiveness%20of%20SynCode%20for%20JSON%20generation%20demonstrate%20that%20SynCode%0Aeliminates%20all%20syntax%20errors%20and%20significantly%20outperforms%20state-of-the-art%0Abaselines.%20Furthermore%2C%20our%20results%20underscore%20how%20SynCode%20significantly%0Areduces%2096.07%25%20of%20syntax%20errors%20in%20generated%20Python%20and%20Go%20code%2C%20showcasing%20its%0Asubstantial%20impact%20on%20enhancing%20syntactical%20precision%20in%20LLM%20generation.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/uiuc-focal-lab/syncode%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01632v4&entry.124074799=Read"},
{"title": "PersianRAG: A Retrieval-Augmented Generation System for Persian Language", "author": "Hossein Hosseini and Mohammad Sobhan Zare and Amir Hossein Mohammadi and Arefeh Kazemi and Zahra Zojaji and Mohammad Ali Nematbakhsh", "abstract": "  Retrieval augmented generation (RAG) models, which integrate large-scale\npre-trained generative models with external retrieval mechanisms, have shown\nsignificant success in various natural language processing (NLP) tasks.\nHowever, applying RAG models in Persian language as a low-resource language,\nposes distinct challenges. These challenges primarily involve the\npreprocessing, embedding, retrieval, prompt construction, language modeling,\nand response evaluation of the system. In this paper, we address the challenges\ntowards implementing a real-world RAG system for Persian language called\nPersianRAG. We propose novel solutions to overcome these obstacles and evaluate\nour approach using several Persian benchmark datasets. Our experimental results\ndemonstrate the capability of the PersianRAG framework to enhance question\nanswering task in Persian.\n", "link": "http://arxiv.org/abs/2411.02832v2", "date": "2024-11-06", "relevancy": 2.2554, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4636}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4518}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PersianRAG%3A%20A%20Retrieval-Augmented%20Generation%20System%20for%20Persian%20Language&body=Title%3A%20PersianRAG%3A%20A%20Retrieval-Augmented%20Generation%20System%20for%20Persian%20Language%0AAuthor%3A%20Hossein%20Hosseini%20and%20Mohammad%20Sobhan%20Zare%20and%20Amir%20Hossein%20Mohammadi%20and%20Arefeh%20Kazemi%20and%20Zahra%20Zojaji%20and%20Mohammad%20Ali%20Nematbakhsh%0AAbstract%3A%20%20%20Retrieval%20augmented%20generation%20%28RAG%29%20models%2C%20which%20integrate%20large-scale%0Apre-trained%20generative%20models%20with%20external%20retrieval%20mechanisms%2C%20have%20shown%0Asignificant%20success%20in%20various%20natural%20language%20processing%20%28NLP%29%20tasks.%0AHowever%2C%20applying%20RAG%20models%20in%20Persian%20language%20as%20a%20low-resource%20language%2C%0Aposes%20distinct%20challenges.%20These%20challenges%20primarily%20involve%20the%0Apreprocessing%2C%20embedding%2C%20retrieval%2C%20prompt%20construction%2C%20language%20modeling%2C%0Aand%20response%20evaluation%20of%20the%20system.%20In%20this%20paper%2C%20we%20address%20the%20challenges%0Atowards%20implementing%20a%20real-world%20RAG%20system%20for%20Persian%20language%20called%0APersianRAG.%20We%20propose%20novel%20solutions%20to%20overcome%20these%20obstacles%20and%20evaluate%0Aour%20approach%20using%20several%20Persian%20benchmark%20datasets.%20Our%20experimental%20results%0Ademonstrate%20the%20capability%20of%20the%20PersianRAG%20framework%20to%20enhance%20question%0Aanswering%20task%20in%20Persian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02832v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersianRAG%253A%2520A%2520Retrieval-Augmented%2520Generation%2520System%2520for%2520Persian%2520Language%26entry.906535625%3DHossein%2520Hosseini%2520and%2520Mohammad%2520Sobhan%2520Zare%2520and%2520Amir%2520Hossein%2520Mohammadi%2520and%2520Arefeh%2520Kazemi%2520and%2520Zahra%2520Zojaji%2520and%2520Mohammad%2520Ali%2520Nematbakhsh%26entry.1292438233%3D%2520%2520Retrieval%2520augmented%2520generation%2520%2528RAG%2529%2520models%252C%2520which%2520integrate%2520large-scale%250Apre-trained%2520generative%2520models%2520with%2520external%2520retrieval%2520mechanisms%252C%2520have%2520shown%250Asignificant%2520success%2520in%2520various%2520natural%2520language%2520processing%2520%2528NLP%2529%2520tasks.%250AHowever%252C%2520applying%2520RAG%2520models%2520in%2520Persian%2520language%2520as%2520a%2520low-resource%2520language%252C%250Aposes%2520distinct%2520challenges.%2520These%2520challenges%2520primarily%2520involve%2520the%250Apreprocessing%252C%2520embedding%252C%2520retrieval%252C%2520prompt%2520construction%252C%2520language%2520modeling%252C%250Aand%2520response%2520evaluation%2520of%2520the%2520system.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenges%250Atowards%2520implementing%2520a%2520real-world%2520RAG%2520system%2520for%2520Persian%2520language%2520called%250APersianRAG.%2520We%2520propose%2520novel%2520solutions%2520to%2520overcome%2520these%2520obstacles%2520and%2520evaluate%250Aour%2520approach%2520using%2520several%2520Persian%2520benchmark%2520datasets.%2520Our%2520experimental%2520results%250Ademonstrate%2520the%2520capability%2520of%2520the%2520PersianRAG%2520framework%2520to%2520enhance%2520question%250Aanswering%2520task%2520in%2520Persian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02832v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PersianRAG%3A%20A%20Retrieval-Augmented%20Generation%20System%20for%20Persian%20Language&entry.906535625=Hossein%20Hosseini%20and%20Mohammad%20Sobhan%20Zare%20and%20Amir%20Hossein%20Mohammadi%20and%20Arefeh%20Kazemi%20and%20Zahra%20Zojaji%20and%20Mohammad%20Ali%20Nematbakhsh&entry.1292438233=%20%20Retrieval%20augmented%20generation%20%28RAG%29%20models%2C%20which%20integrate%20large-scale%0Apre-trained%20generative%20models%20with%20external%20retrieval%20mechanisms%2C%20have%20shown%0Asignificant%20success%20in%20various%20natural%20language%20processing%20%28NLP%29%20tasks.%0AHowever%2C%20applying%20RAG%20models%20in%20Persian%20language%20as%20a%20low-resource%20language%2C%0Aposes%20distinct%20challenges.%20These%20challenges%20primarily%20involve%20the%0Apreprocessing%2C%20embedding%2C%20retrieval%2C%20prompt%20construction%2C%20language%20modeling%2C%0Aand%20response%20evaluation%20of%20the%20system.%20In%20this%20paper%2C%20we%20address%20the%20challenges%0Atowards%20implementing%20a%20real-world%20RAG%20system%20for%20Persian%20language%20called%0APersianRAG.%20We%20propose%20novel%20solutions%20to%20overcome%20these%20obstacles%20and%20evaluate%0Aour%20approach%20using%20several%20Persian%20benchmark%20datasets.%20Our%20experimental%20results%0Ademonstrate%20the%20capability%20of%20the%20PersianRAG%20framework%20to%20enhance%20question%0Aanswering%20task%20in%20Persian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02832v2&entry.124074799=Read"},
{"title": "Degradation Oriented and Regularized Network for Blind Depth\n  Super-Resolution", "author": "Zhengxue Wang and Zhiqiang Yan and Jinshan Pan and Guangwei Gao and Kai Zhang and Jian Yang", "abstract": "  Recent RGB-guided depth super-resolution methods have achieved impressive\nperformance under the assumption of fixed and known degradation (e.g., bicubic\ndownsampling). However, in real-world scenarios, captured depth data often\nsuffer from unconventional and unknown degradation due to sensor limitations\nand complex imaging environments (e.g., low reflective surfaces, varying\nillumination). Consequently, the performance of these methods significantly\ndeclines when real-world degradation deviate from their assumptions. In this\npaper, we propose the Degradation Oriented and Regularized Network (DORNet), a\nnovel framework designed to adaptively address unknown degradation in\nreal-world scenes through implicit degradation representations. Our approach\nbegins with the development of a self-supervised degradation learning strategy,\nwhich models the degradation representations of low-resolution depth data using\nrouting selection-based degradation regularization. To facilitate effective\nRGB-D fusion, we further introduce a degradation-oriented feature\ntransformation module that selectively propagates RGB content into the depth\ndata based on the learned degradation priors. Extensive experimental results on\nboth real and synthetic datasets demonstrate the superiority of our DORNet in\nhandling unknown degradation, outperforming existing methods. The code is\navailable at https://github.com/yanzq95/DORNet.\n", "link": "http://arxiv.org/abs/2410.11666v3", "date": "2024-11-06", "relevancy": 2.2523, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5671}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5615}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degradation%20Oriented%20and%20Regularized%20Network%20for%20Blind%20Depth%0A%20%20Super-Resolution&body=Title%3A%20Degradation%20Oriented%20and%20Regularized%20Network%20for%20Blind%20Depth%0A%20%20Super-Resolution%0AAuthor%3A%20Zhengxue%20Wang%20and%20Zhiqiang%20Yan%20and%20Jinshan%20Pan%20and%20Guangwei%20Gao%20and%20Kai%20Zhang%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Recent%20RGB-guided%20depth%20super-resolution%20methods%20have%20achieved%20impressive%0Aperformance%20under%20the%20assumption%20of%20fixed%20and%20known%20degradation%20%28e.g.%2C%20bicubic%0Adownsampling%29.%20However%2C%20in%20real-world%20scenarios%2C%20captured%20depth%20data%20often%0Asuffer%20from%20unconventional%20and%20unknown%20degradation%20due%20to%20sensor%20limitations%0Aand%20complex%20imaging%20environments%20%28e.g.%2C%20low%20reflective%20surfaces%2C%20varying%0Aillumination%29.%20Consequently%2C%20the%20performance%20of%20these%20methods%20significantly%0Adeclines%20when%20real-world%20degradation%20deviate%20from%20their%20assumptions.%20In%20this%0Apaper%2C%20we%20propose%20the%20Degradation%20Oriented%20and%20Regularized%20Network%20%28DORNet%29%2C%20a%0Anovel%20framework%20designed%20to%20adaptively%20address%20unknown%20degradation%20in%0Areal-world%20scenes%20through%20implicit%20degradation%20representations.%20Our%20approach%0Abegins%20with%20the%20development%20of%20a%20self-supervised%20degradation%20learning%20strategy%2C%0Awhich%20models%20the%20degradation%20representations%20of%20low-resolution%20depth%20data%20using%0Arouting%20selection-based%20degradation%20regularization.%20To%20facilitate%20effective%0ARGB-D%20fusion%2C%20we%20further%20introduce%20a%20degradation-oriented%20feature%0Atransformation%20module%20that%20selectively%20propagates%20RGB%20content%20into%20the%20depth%0Adata%20based%20on%20the%20learned%20degradation%20priors.%20Extensive%20experimental%20results%20on%0Aboth%20real%20and%20synthetic%20datasets%20demonstrate%20the%20superiority%20of%20our%20DORNet%20in%0Ahandling%20unknown%20degradation%2C%20outperforming%20existing%20methods.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/yanzq95/DORNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11666v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegradation%2520Oriented%2520and%2520Regularized%2520Network%2520for%2520Blind%2520Depth%250A%2520%2520Super-Resolution%26entry.906535625%3DZhengxue%2520Wang%2520and%2520Zhiqiang%2520Yan%2520and%2520Jinshan%2520Pan%2520and%2520Guangwei%2520Gao%2520and%2520Kai%2520Zhang%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520RGB-guided%2520depth%2520super-resolution%2520methods%2520have%2520achieved%2520impressive%250Aperformance%2520under%2520the%2520assumption%2520of%2520fixed%2520and%2520known%2520degradation%2520%2528e.g.%252C%2520bicubic%250Adownsampling%2529.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520captured%2520depth%2520data%2520often%250Asuffer%2520from%2520unconventional%2520and%2520unknown%2520degradation%2520due%2520to%2520sensor%2520limitations%250Aand%2520complex%2520imaging%2520environments%2520%2528e.g.%252C%2520low%2520reflective%2520surfaces%252C%2520varying%250Aillumination%2529.%2520Consequently%252C%2520the%2520performance%2520of%2520these%2520methods%2520significantly%250Adeclines%2520when%2520real-world%2520degradation%2520deviate%2520from%2520their%2520assumptions.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520Degradation%2520Oriented%2520and%2520Regularized%2520Network%2520%2528DORNet%2529%252C%2520a%250Anovel%2520framework%2520designed%2520to%2520adaptively%2520address%2520unknown%2520degradation%2520in%250Areal-world%2520scenes%2520through%2520implicit%2520degradation%2520representations.%2520Our%2520approach%250Abegins%2520with%2520the%2520development%2520of%2520a%2520self-supervised%2520degradation%2520learning%2520strategy%252C%250Awhich%2520models%2520the%2520degradation%2520representations%2520of%2520low-resolution%2520depth%2520data%2520using%250Arouting%2520selection-based%2520degradation%2520regularization.%2520To%2520facilitate%2520effective%250ARGB-D%2520fusion%252C%2520we%2520further%2520introduce%2520a%2520degradation-oriented%2520feature%250Atransformation%2520module%2520that%2520selectively%2520propagates%2520RGB%2520content%2520into%2520the%2520depth%250Adata%2520based%2520on%2520the%2520learned%2520degradation%2520priors.%2520Extensive%2520experimental%2520results%2520on%250Aboth%2520real%2520and%2520synthetic%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520our%2520DORNet%2520in%250Ahandling%2520unknown%2520degradation%252C%2520outperforming%2520existing%2520methods.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/yanzq95/DORNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11666v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degradation%20Oriented%20and%20Regularized%20Network%20for%20Blind%20Depth%0A%20%20Super-Resolution&entry.906535625=Zhengxue%20Wang%20and%20Zhiqiang%20Yan%20and%20Jinshan%20Pan%20and%20Guangwei%20Gao%20and%20Kai%20Zhang%20and%20Jian%20Yang&entry.1292438233=%20%20Recent%20RGB-guided%20depth%20super-resolution%20methods%20have%20achieved%20impressive%0Aperformance%20under%20the%20assumption%20of%20fixed%20and%20known%20degradation%20%28e.g.%2C%20bicubic%0Adownsampling%29.%20However%2C%20in%20real-world%20scenarios%2C%20captured%20depth%20data%20often%0Asuffer%20from%20unconventional%20and%20unknown%20degradation%20due%20to%20sensor%20limitations%0Aand%20complex%20imaging%20environments%20%28e.g.%2C%20low%20reflective%20surfaces%2C%20varying%0Aillumination%29.%20Consequently%2C%20the%20performance%20of%20these%20methods%20significantly%0Adeclines%20when%20real-world%20degradation%20deviate%20from%20their%20assumptions.%20In%20this%0Apaper%2C%20we%20propose%20the%20Degradation%20Oriented%20and%20Regularized%20Network%20%28DORNet%29%2C%20a%0Anovel%20framework%20designed%20to%20adaptively%20address%20unknown%20degradation%20in%0Areal-world%20scenes%20through%20implicit%20degradation%20representations.%20Our%20approach%0Abegins%20with%20the%20development%20of%20a%20self-supervised%20degradation%20learning%20strategy%2C%0Awhich%20models%20the%20degradation%20representations%20of%20low-resolution%20depth%20data%20using%0Arouting%20selection-based%20degradation%20regularization.%20To%20facilitate%20effective%0ARGB-D%20fusion%2C%20we%20further%20introduce%20a%20degradation-oriented%20feature%0Atransformation%20module%20that%20selectively%20propagates%20RGB%20content%20into%20the%20depth%0Adata%20based%20on%20the%20learned%20degradation%20priors.%20Extensive%20experimental%20results%20on%0Aboth%20real%20and%20synthetic%20datasets%20demonstrate%20the%20superiority%20of%20our%20DORNet%20in%0Ahandling%20unknown%20degradation%2C%20outperforming%20existing%20methods.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/yanzq95/DORNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11666v3&entry.124074799=Read"},
{"title": "ParaGAN: A Scalable Distributed Training Framework for Generative\n  Adversarial Networks", "author": "Ziji Shi and Jialin Li and Yang You", "abstract": "  Recent advances in Generative Artificial Intelligence have fueled numerous\napplications, particularly those involving Generative Adversarial Networks\n(GANs), which are essential for synthesizing realistic photos and videos.\nHowever, efficiently training GANs remains a critical challenge due to their\ncomputationally intensive and numerically unstable nature. Existing methods\noften require days or even weeks for training, posing significant resource and\ntime constraints.\n  In this work, we introduce ParaGAN, a scalable distributed GAN training\nframework that leverages asynchronous training and an asymmetric optimization\npolicy to accelerate GAN training. ParaGAN employs a congestion-aware data\npipeline and hardware-aware layout transformation to enhance accelerator\nutilization, resulting in over 30% improvements in throughput. With ParaGAN, we\nreduce the training time of BigGAN from 15 days to 14 hours while achieving 91%\nscaling efficiency. Additionally, ParaGAN enables unprecedented high-resolution\nimage generation using BigGAN.\n", "link": "http://arxiv.org/abs/2411.03999v1", "date": "2024-11-06", "relevancy": 2.2415, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.571}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5672}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ParaGAN%3A%20A%20Scalable%20Distributed%20Training%20Framework%20for%20Generative%0A%20%20Adversarial%20Networks&body=Title%3A%20ParaGAN%3A%20A%20Scalable%20Distributed%20Training%20Framework%20for%20Generative%0A%20%20Adversarial%20Networks%0AAuthor%3A%20Ziji%20Shi%20and%20Jialin%20Li%20and%20Yang%20You%0AAbstract%3A%20%20%20Recent%20advances%20in%20Generative%20Artificial%20Intelligence%20have%20fueled%20numerous%0Aapplications%2C%20particularly%20those%20involving%20Generative%20Adversarial%20Networks%0A%28GANs%29%2C%20which%20are%20essential%20for%20synthesizing%20realistic%20photos%20and%20videos.%0AHowever%2C%20efficiently%20training%20GANs%20remains%20a%20critical%20challenge%20due%20to%20their%0Acomputationally%20intensive%20and%20numerically%20unstable%20nature.%20Existing%20methods%0Aoften%20require%20days%20or%20even%20weeks%20for%20training%2C%20posing%20significant%20resource%20and%0Atime%20constraints.%0A%20%20In%20this%20work%2C%20we%20introduce%20ParaGAN%2C%20a%20scalable%20distributed%20GAN%20training%0Aframework%20that%20leverages%20asynchronous%20training%20and%20an%20asymmetric%20optimization%0Apolicy%20to%20accelerate%20GAN%20training.%20ParaGAN%20employs%20a%20congestion-aware%20data%0Apipeline%20and%20hardware-aware%20layout%20transformation%20to%20enhance%20accelerator%0Autilization%2C%20resulting%20in%20over%2030%25%20improvements%20in%20throughput.%20With%20ParaGAN%2C%20we%0Areduce%20the%20training%20time%20of%20BigGAN%20from%2015%20days%20to%2014%20hours%20while%20achieving%2091%25%0Ascaling%20efficiency.%20Additionally%2C%20ParaGAN%20enables%20unprecedented%20high-resolution%0Aimage%20generation%20using%20BigGAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParaGAN%253A%2520A%2520Scalable%2520Distributed%2520Training%2520Framework%2520for%2520Generative%250A%2520%2520Adversarial%2520Networks%26entry.906535625%3DZiji%2520Shi%2520and%2520Jialin%2520Li%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Generative%2520Artificial%2520Intelligence%2520have%2520fueled%2520numerous%250Aapplications%252C%2520particularly%2520those%2520involving%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%252C%2520which%2520are%2520essential%2520for%2520synthesizing%2520realistic%2520photos%2520and%2520videos.%250AHowever%252C%2520efficiently%2520training%2520GANs%2520remains%2520a%2520critical%2520challenge%2520due%2520to%2520their%250Acomputationally%2520intensive%2520and%2520numerically%2520unstable%2520nature.%2520Existing%2520methods%250Aoften%2520require%2520days%2520or%2520even%2520weeks%2520for%2520training%252C%2520posing%2520significant%2520resource%2520and%250Atime%2520constraints.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520ParaGAN%252C%2520a%2520scalable%2520distributed%2520GAN%2520training%250Aframework%2520that%2520leverages%2520asynchronous%2520training%2520and%2520an%2520asymmetric%2520optimization%250Apolicy%2520to%2520accelerate%2520GAN%2520training.%2520ParaGAN%2520employs%2520a%2520congestion-aware%2520data%250Apipeline%2520and%2520hardware-aware%2520layout%2520transformation%2520to%2520enhance%2520accelerator%250Autilization%252C%2520resulting%2520in%2520over%252030%2525%2520improvements%2520in%2520throughput.%2520With%2520ParaGAN%252C%2520we%250Areduce%2520the%2520training%2520time%2520of%2520BigGAN%2520from%252015%2520days%2520to%252014%2520hours%2520while%2520achieving%252091%2525%250Ascaling%2520efficiency.%2520Additionally%252C%2520ParaGAN%2520enables%2520unprecedented%2520high-resolution%250Aimage%2520generation%2520using%2520BigGAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ParaGAN%3A%20A%20Scalable%20Distributed%20Training%20Framework%20for%20Generative%0A%20%20Adversarial%20Networks&entry.906535625=Ziji%20Shi%20and%20Jialin%20Li%20and%20Yang%20You&entry.1292438233=%20%20Recent%20advances%20in%20Generative%20Artificial%20Intelligence%20have%20fueled%20numerous%0Aapplications%2C%20particularly%20those%20involving%20Generative%20Adversarial%20Networks%0A%28GANs%29%2C%20which%20are%20essential%20for%20synthesizing%20realistic%20photos%20and%20videos.%0AHowever%2C%20efficiently%20training%20GANs%20remains%20a%20critical%20challenge%20due%20to%20their%0Acomputationally%20intensive%20and%20numerically%20unstable%20nature.%20Existing%20methods%0Aoften%20require%20days%20or%20even%20weeks%20for%20training%2C%20posing%20significant%20resource%20and%0Atime%20constraints.%0A%20%20In%20this%20work%2C%20we%20introduce%20ParaGAN%2C%20a%20scalable%20distributed%20GAN%20training%0Aframework%20that%20leverages%20asynchronous%20training%20and%20an%20asymmetric%20optimization%0Apolicy%20to%20accelerate%20GAN%20training.%20ParaGAN%20employs%20a%20congestion-aware%20data%0Apipeline%20and%20hardware-aware%20layout%20transformation%20to%20enhance%20accelerator%0Autilization%2C%20resulting%20in%20over%2030%25%20improvements%20in%20throughput.%20With%20ParaGAN%2C%20we%0Areduce%20the%20training%20time%20of%20BigGAN%20from%2015%20days%20to%2014%20hours%20while%20achieving%2091%25%0Ascaling%20efficiency.%20Additionally%2C%20ParaGAN%20enables%20unprecedented%20high-resolution%0Aimage%20generation%20using%20BigGAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03999v1&entry.124074799=Read"},
{"title": "Stepping Forward on the Last Mile", "author": "Chen Feng and Shaojie Zhuo and Xiaopeng Zhang and Ramchalam Kinattinkara Ramakrishnan and Zhaocong Yuan and Andrew Zou Li", "abstract": "  Continuously adapting pre-trained models to local data on resource\nconstrained edge devices is the $\\emph{last mile}$ for model deployment.\nHowever, as models increase in size and depth, backpropagation requires a large\namount of memory, which becomes prohibitive for edge devices. In addition, most\nexisting low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are\ndesigned as fixed-point inference accelerators, without training capabilities.\nForward gradients, solely based on directional derivatives computed from two\nforward calls, have been recently used for model training, with substantial\nsavings in computation and memory. However, the performance of quantized\ntraining with fixed-point forward gradients remains unclear. In this paper, we\ninvestigate the feasibility of on-device training using fixed-point forward\ngradients, by conducting comprehensive experiments across a variety of deep\nlearning benchmark tasks in both vision and audio domains. We propose a series\nof algorithm enhancements that further reduce the memory footprint, and the\naccuracy gap compared to backpropagation. An empirical study on how training\nwith forward gradients navigates in the loss landscape is further explored. Our\nresults demonstrate that on the last mile of model customization on edge\ndevices, training with fixed-point forward gradients is a feasible and\npractical approach.\n", "link": "http://arxiv.org/abs/2411.04036v1", "date": "2024-11-06", "relevancy": 2.2129, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5663}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5532}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stepping%20Forward%20on%20the%20Last%20Mile&body=Title%3A%20Stepping%20Forward%20on%20the%20Last%20Mile%0AAuthor%3A%20Chen%20Feng%20and%20Shaojie%20Zhuo%20and%20Xiaopeng%20Zhang%20and%20Ramchalam%20Kinattinkara%20Ramakrishnan%20and%20Zhaocong%20Yuan%20and%20Andrew%20Zou%20Li%0AAbstract%3A%20%20%20Continuously%20adapting%20pre-trained%20models%20to%20local%20data%20on%20resource%0Aconstrained%20edge%20devices%20is%20the%20%24%5Cemph%7Blast%20mile%7D%24%20for%20model%20deployment.%0AHowever%2C%20as%20models%20increase%20in%20size%20and%20depth%2C%20backpropagation%20requires%20a%20large%0Aamount%20of%20memory%2C%20which%20becomes%20prohibitive%20for%20edge%20devices.%20In%20addition%2C%20most%0Aexisting%20low%20power%20neural%20processing%20engines%20%28e.g.%2C%20NPUs%2C%20DSPs%2C%20MCUs%2C%20etc.%29%20are%0Adesigned%20as%20fixed-point%20inference%20accelerators%2C%20without%20training%20capabilities.%0AForward%20gradients%2C%20solely%20based%20on%20directional%20derivatives%20computed%20from%20two%0Aforward%20calls%2C%20have%20been%20recently%20used%20for%20model%20training%2C%20with%20substantial%0Asavings%20in%20computation%20and%20memory.%20However%2C%20the%20performance%20of%20quantized%0Atraining%20with%20fixed-point%20forward%20gradients%20remains%20unclear.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20feasibility%20of%20on-device%20training%20using%20fixed-point%20forward%0Agradients%2C%20by%20conducting%20comprehensive%20experiments%20across%20a%20variety%20of%20deep%0Alearning%20benchmark%20tasks%20in%20both%20vision%20and%20audio%20domains.%20We%20propose%20a%20series%0Aof%20algorithm%20enhancements%20that%20further%20reduce%20the%20memory%20footprint%2C%20and%20the%0Aaccuracy%20gap%20compared%20to%20backpropagation.%20An%20empirical%20study%20on%20how%20training%0Awith%20forward%20gradients%20navigates%20in%20the%20loss%20landscape%20is%20further%20explored.%20Our%0Aresults%20demonstrate%20that%20on%20the%20last%20mile%20of%20model%20customization%20on%20edge%0Adevices%2C%20training%20with%20fixed-point%20forward%20gradients%20is%20a%20feasible%20and%0Apractical%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepping%2520Forward%2520on%2520the%2520Last%2520Mile%26entry.906535625%3DChen%2520Feng%2520and%2520Shaojie%2520Zhuo%2520and%2520Xiaopeng%2520Zhang%2520and%2520Ramchalam%2520Kinattinkara%2520Ramakrishnan%2520and%2520Zhaocong%2520Yuan%2520and%2520Andrew%2520Zou%2520Li%26entry.1292438233%3D%2520%2520Continuously%2520adapting%2520pre-trained%2520models%2520to%2520local%2520data%2520on%2520resource%250Aconstrained%2520edge%2520devices%2520is%2520the%2520%2524%255Cemph%257Blast%2520mile%257D%2524%2520for%2520model%2520deployment.%250AHowever%252C%2520as%2520models%2520increase%2520in%2520size%2520and%2520depth%252C%2520backpropagation%2520requires%2520a%2520large%250Aamount%2520of%2520memory%252C%2520which%2520becomes%2520prohibitive%2520for%2520edge%2520devices.%2520In%2520addition%252C%2520most%250Aexisting%2520low%2520power%2520neural%2520processing%2520engines%2520%2528e.g.%252C%2520NPUs%252C%2520DSPs%252C%2520MCUs%252C%2520etc.%2529%2520are%250Adesigned%2520as%2520fixed-point%2520inference%2520accelerators%252C%2520without%2520training%2520capabilities.%250AForward%2520gradients%252C%2520solely%2520based%2520on%2520directional%2520derivatives%2520computed%2520from%2520two%250Aforward%2520calls%252C%2520have%2520been%2520recently%2520used%2520for%2520model%2520training%252C%2520with%2520substantial%250Asavings%2520in%2520computation%2520and%2520memory.%2520However%252C%2520the%2520performance%2520of%2520quantized%250Atraining%2520with%2520fixed-point%2520forward%2520gradients%2520remains%2520unclear.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520the%2520feasibility%2520of%2520on-device%2520training%2520using%2520fixed-point%2520forward%250Agradients%252C%2520by%2520conducting%2520comprehensive%2520experiments%2520across%2520a%2520variety%2520of%2520deep%250Alearning%2520benchmark%2520tasks%2520in%2520both%2520vision%2520and%2520audio%2520domains.%2520We%2520propose%2520a%2520series%250Aof%2520algorithm%2520enhancements%2520that%2520further%2520reduce%2520the%2520memory%2520footprint%252C%2520and%2520the%250Aaccuracy%2520gap%2520compared%2520to%2520backpropagation.%2520An%2520empirical%2520study%2520on%2520how%2520training%250Awith%2520forward%2520gradients%2520navigates%2520in%2520the%2520loss%2520landscape%2520is%2520further%2520explored.%2520Our%250Aresults%2520demonstrate%2520that%2520on%2520the%2520last%2520mile%2520of%2520model%2520customization%2520on%2520edge%250Adevices%252C%2520training%2520with%2520fixed-point%2520forward%2520gradients%2520is%2520a%2520feasible%2520and%250Apractical%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stepping%20Forward%20on%20the%20Last%20Mile&entry.906535625=Chen%20Feng%20and%20Shaojie%20Zhuo%20and%20Xiaopeng%20Zhang%20and%20Ramchalam%20Kinattinkara%20Ramakrishnan%20and%20Zhaocong%20Yuan%20and%20Andrew%20Zou%20Li&entry.1292438233=%20%20Continuously%20adapting%20pre-trained%20models%20to%20local%20data%20on%20resource%0Aconstrained%20edge%20devices%20is%20the%20%24%5Cemph%7Blast%20mile%7D%24%20for%20model%20deployment.%0AHowever%2C%20as%20models%20increase%20in%20size%20and%20depth%2C%20backpropagation%20requires%20a%20large%0Aamount%20of%20memory%2C%20which%20becomes%20prohibitive%20for%20edge%20devices.%20In%20addition%2C%20most%0Aexisting%20low%20power%20neural%20processing%20engines%20%28e.g.%2C%20NPUs%2C%20DSPs%2C%20MCUs%2C%20etc.%29%20are%0Adesigned%20as%20fixed-point%20inference%20accelerators%2C%20without%20training%20capabilities.%0AForward%20gradients%2C%20solely%20based%20on%20directional%20derivatives%20computed%20from%20two%0Aforward%20calls%2C%20have%20been%20recently%20used%20for%20model%20training%2C%20with%20substantial%0Asavings%20in%20computation%20and%20memory.%20However%2C%20the%20performance%20of%20quantized%0Atraining%20with%20fixed-point%20forward%20gradients%20remains%20unclear.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20feasibility%20of%20on-device%20training%20using%20fixed-point%20forward%0Agradients%2C%20by%20conducting%20comprehensive%20experiments%20across%20a%20variety%20of%20deep%0Alearning%20benchmark%20tasks%20in%20both%20vision%20and%20audio%20domains.%20We%20propose%20a%20series%0Aof%20algorithm%20enhancements%20that%20further%20reduce%20the%20memory%20footprint%2C%20and%20the%0Aaccuracy%20gap%20compared%20to%20backpropagation.%20An%20empirical%20study%20on%20how%20training%0Awith%20forward%20gradients%20navigates%20in%20the%20loss%20landscape%20is%20further%20explored.%20Our%0Aresults%20demonstrate%20that%20on%20the%20last%20mile%20of%20model%20customization%20on%20edge%0Adevices%2C%20training%20with%20fixed-point%20forward%20gradients%20is%20a%20feasible%20and%0Apractical%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04036v1&entry.124074799=Read"},
{"title": "bit2bit: 1-bit quanta video reconstruction via self-supervised photon\n  prediction", "author": "Yehe Liu and Alexander Krull and Hector Basevi and Ales Leonardis and Michael W. Jenkins", "abstract": "  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,\nproducing 1-bit arrays representing photon detection events over exposures as\nshort as a few nanoseconds. In practice, raw data are post-processed using\nheavy spatiotemporal binning to create more useful and interpretable images at\nthe cost of degrading spatiotemporal resolution. In this work, we propose\nbit2bit, a new method for reconstructing high-quality image stacks at the\noriginal spatiotemporal resolution from sparse binary quanta image data.\nInspired by recent work on Poisson denoising, we developed an algorithm that\ncreates a dense image sequence from sparse binary photon data by predicting the\nphoton arrival location probability distribution. However, due to the binary\nnature of the data, we show that the assumption of a Poisson distribution is\ninadequate. Instead, we model the process with a Bernoulli lattice process from\nthe truncated Poisson. This leads to the proposal of a novel self-supervised\nsolution based on a masked loss function. We evaluate our method using both\nsimulated and real data. On simulated data from a conventional video, we\nachieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06\nphotons per pixel per frame). We also present a novel dataset containing a wide\nrange of real SPAD high-speed videos under various challenging imaging\nconditions. The scenes cover strong/weak ambient light, strong motion,\nultra-fast events, etc., which will be made available to the community, on\nwhich we demonstrate the promise of our approach. Both reconstruction quality\nand throughput substantially surpass the state-of-the-art methods (e.g., Quanta\nBurst Photography (QBP)). Our approach significantly enhances the visualization\nand usability of the data, enabling the application of existing analysis\ntechniques.\n", "link": "http://arxiv.org/abs/2410.23247v2", "date": "2024-11-06", "relevancy": 2.2066, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5567}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5556}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20bit2bit%3A%201-bit%20quanta%20video%20reconstruction%20via%20self-supervised%20photon%0A%20%20prediction&body=Title%3A%20bit2bit%3A%201-bit%20quanta%20video%20reconstruction%20via%20self-supervised%20photon%0A%20%20prediction%0AAuthor%3A%20Yehe%20Liu%20and%20Alexander%20Krull%20and%20Hector%20Basevi%20and%20Ales%20Leonardis%20and%20Michael%20W.%20Jenkins%0AAbstract%3A%20%20%20Quanta%20image%20sensors%2C%20such%20as%20SPAD%20arrays%2C%20are%20an%20emerging%20sensor%20technology%2C%0Aproducing%201-bit%20arrays%20representing%20photon%20detection%20events%20over%20exposures%20as%0Ashort%20as%20a%20few%20nanoseconds.%20In%20practice%2C%20raw%20data%20are%20post-processed%20using%0Aheavy%20spatiotemporal%20binning%20to%20create%20more%20useful%20and%20interpretable%20images%20at%0Athe%20cost%20of%20degrading%20spatiotemporal%20resolution.%20In%20this%20work%2C%20we%20propose%0Abit2bit%2C%20a%20new%20method%20for%20reconstructing%20high-quality%20image%20stacks%20at%20the%0Aoriginal%20spatiotemporal%20resolution%20from%20sparse%20binary%20quanta%20image%20data.%0AInspired%20by%20recent%20work%20on%20Poisson%20denoising%2C%20we%20developed%20an%20algorithm%20that%0Acreates%20a%20dense%20image%20sequence%20from%20sparse%20binary%20photon%20data%20by%20predicting%20the%0Aphoton%20arrival%20location%20probability%20distribution.%20However%2C%20due%20to%20the%20binary%0Anature%20of%20the%20data%2C%20we%20show%20that%20the%20assumption%20of%20a%20Poisson%20distribution%20is%0Ainadequate.%20Instead%2C%20we%20model%20the%20process%20with%20a%20Bernoulli%20lattice%20process%20from%0Athe%20truncated%20Poisson.%20This%20leads%20to%20the%20proposal%20of%20a%20novel%20self-supervised%0Asolution%20based%20on%20a%20masked%20loss%20function.%20We%20evaluate%20our%20method%20using%20both%0Asimulated%20and%20real%20data.%20On%20simulated%20data%20from%20a%20conventional%20video%2C%20we%0Aachieve%2034.35%20mean%20PSNR%20with%20extremely%20photon-sparse%20binary%20input%20%28%3C0.06%0Aphotons%20per%20pixel%20per%20frame%29.%20We%20also%20present%20a%20novel%20dataset%20containing%20a%20wide%0Arange%20of%20real%20SPAD%20high-speed%20videos%20under%20various%20challenging%20imaging%0Aconditions.%20The%20scenes%20cover%20strong/weak%20ambient%20light%2C%20strong%20motion%2C%0Aultra-fast%20events%2C%20etc.%2C%20which%20will%20be%20made%20available%20to%20the%20community%2C%20on%0Awhich%20we%20demonstrate%20the%20promise%20of%20our%20approach.%20Both%20reconstruction%20quality%0Aand%20throughput%20substantially%20surpass%20the%20state-of-the-art%20methods%20%28e.g.%2C%20Quanta%0ABurst%20Photography%20%28QBP%29%29.%20Our%20approach%20significantly%20enhances%20the%20visualization%0Aand%20usability%20of%20the%20data%2C%20enabling%20the%20application%20of%20existing%20analysis%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dbit2bit%253A%25201-bit%2520quanta%2520video%2520reconstruction%2520via%2520self-supervised%2520photon%250A%2520%2520prediction%26entry.906535625%3DYehe%2520Liu%2520and%2520Alexander%2520Krull%2520and%2520Hector%2520Basevi%2520and%2520Ales%2520Leonardis%2520and%2520Michael%2520W.%2520Jenkins%26entry.1292438233%3D%2520%2520Quanta%2520image%2520sensors%252C%2520such%2520as%2520SPAD%2520arrays%252C%2520are%2520an%2520emerging%2520sensor%2520technology%252C%250Aproducing%25201-bit%2520arrays%2520representing%2520photon%2520detection%2520events%2520over%2520exposures%2520as%250Ashort%2520as%2520a%2520few%2520nanoseconds.%2520In%2520practice%252C%2520raw%2520data%2520are%2520post-processed%2520using%250Aheavy%2520spatiotemporal%2520binning%2520to%2520create%2520more%2520useful%2520and%2520interpretable%2520images%2520at%250Athe%2520cost%2520of%2520degrading%2520spatiotemporal%2520resolution.%2520In%2520this%2520work%252C%2520we%2520propose%250Abit2bit%252C%2520a%2520new%2520method%2520for%2520reconstructing%2520high-quality%2520image%2520stacks%2520at%2520the%250Aoriginal%2520spatiotemporal%2520resolution%2520from%2520sparse%2520binary%2520quanta%2520image%2520data.%250AInspired%2520by%2520recent%2520work%2520on%2520Poisson%2520denoising%252C%2520we%2520developed%2520an%2520algorithm%2520that%250Acreates%2520a%2520dense%2520image%2520sequence%2520from%2520sparse%2520binary%2520photon%2520data%2520by%2520predicting%2520the%250Aphoton%2520arrival%2520location%2520probability%2520distribution.%2520However%252C%2520due%2520to%2520the%2520binary%250Anature%2520of%2520the%2520data%252C%2520we%2520show%2520that%2520the%2520assumption%2520of%2520a%2520Poisson%2520distribution%2520is%250Ainadequate.%2520Instead%252C%2520we%2520model%2520the%2520process%2520with%2520a%2520Bernoulli%2520lattice%2520process%2520from%250Athe%2520truncated%2520Poisson.%2520This%2520leads%2520to%2520the%2520proposal%2520of%2520a%2520novel%2520self-supervised%250Asolution%2520based%2520on%2520a%2520masked%2520loss%2520function.%2520We%2520evaluate%2520our%2520method%2520using%2520both%250Asimulated%2520and%2520real%2520data.%2520On%2520simulated%2520data%2520from%2520a%2520conventional%2520video%252C%2520we%250Aachieve%252034.35%2520mean%2520PSNR%2520with%2520extremely%2520photon-sparse%2520binary%2520input%2520%2528%253C0.06%250Aphotons%2520per%2520pixel%2520per%2520frame%2529.%2520We%2520also%2520present%2520a%2520novel%2520dataset%2520containing%2520a%2520wide%250Arange%2520of%2520real%2520SPAD%2520high-speed%2520videos%2520under%2520various%2520challenging%2520imaging%250Aconditions.%2520The%2520scenes%2520cover%2520strong/weak%2520ambient%2520light%252C%2520strong%2520motion%252C%250Aultra-fast%2520events%252C%2520etc.%252C%2520which%2520will%2520be%2520made%2520available%2520to%2520the%2520community%252C%2520on%250Awhich%2520we%2520demonstrate%2520the%2520promise%2520of%2520our%2520approach.%2520Both%2520reconstruction%2520quality%250Aand%2520throughput%2520substantially%2520surpass%2520the%2520state-of-the-art%2520methods%2520%2528e.g.%252C%2520Quanta%250ABurst%2520Photography%2520%2528QBP%2529%2529.%2520Our%2520approach%2520significantly%2520enhances%2520the%2520visualization%250Aand%2520usability%2520of%2520the%2520data%252C%2520enabling%2520the%2520application%2520of%2520existing%2520analysis%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=bit2bit%3A%201-bit%20quanta%20video%20reconstruction%20via%20self-supervised%20photon%0A%20%20prediction&entry.906535625=Yehe%20Liu%20and%20Alexander%20Krull%20and%20Hector%20Basevi%20and%20Ales%20Leonardis%20and%20Michael%20W.%20Jenkins&entry.1292438233=%20%20Quanta%20image%20sensors%2C%20such%20as%20SPAD%20arrays%2C%20are%20an%20emerging%20sensor%20technology%2C%0Aproducing%201-bit%20arrays%20representing%20photon%20detection%20events%20over%20exposures%20as%0Ashort%20as%20a%20few%20nanoseconds.%20In%20practice%2C%20raw%20data%20are%20post-processed%20using%0Aheavy%20spatiotemporal%20binning%20to%20create%20more%20useful%20and%20interpretable%20images%20at%0Athe%20cost%20of%20degrading%20spatiotemporal%20resolution.%20In%20this%20work%2C%20we%20propose%0Abit2bit%2C%20a%20new%20method%20for%20reconstructing%20high-quality%20image%20stacks%20at%20the%0Aoriginal%20spatiotemporal%20resolution%20from%20sparse%20binary%20quanta%20image%20data.%0AInspired%20by%20recent%20work%20on%20Poisson%20denoising%2C%20we%20developed%20an%20algorithm%20that%0Acreates%20a%20dense%20image%20sequence%20from%20sparse%20binary%20photon%20data%20by%20predicting%20the%0Aphoton%20arrival%20location%20probability%20distribution.%20However%2C%20due%20to%20the%20binary%0Anature%20of%20the%20data%2C%20we%20show%20that%20the%20assumption%20of%20a%20Poisson%20distribution%20is%0Ainadequate.%20Instead%2C%20we%20model%20the%20process%20with%20a%20Bernoulli%20lattice%20process%20from%0Athe%20truncated%20Poisson.%20This%20leads%20to%20the%20proposal%20of%20a%20novel%20self-supervised%0Asolution%20based%20on%20a%20masked%20loss%20function.%20We%20evaluate%20our%20method%20using%20both%0Asimulated%20and%20real%20data.%20On%20simulated%20data%20from%20a%20conventional%20video%2C%20we%0Aachieve%2034.35%20mean%20PSNR%20with%20extremely%20photon-sparse%20binary%20input%20%28%3C0.06%0Aphotons%20per%20pixel%20per%20frame%29.%20We%20also%20present%20a%20novel%20dataset%20containing%20a%20wide%0Arange%20of%20real%20SPAD%20high-speed%20videos%20under%20various%20challenging%20imaging%0Aconditions.%20The%20scenes%20cover%20strong/weak%20ambient%20light%2C%20strong%20motion%2C%0Aultra-fast%20events%2C%20etc.%2C%20which%20will%20be%20made%20available%20to%20the%20community%2C%20on%0Awhich%20we%20demonstrate%20the%20promise%20of%20our%20approach.%20Both%20reconstruction%20quality%0Aand%20throughput%20substantially%20surpass%20the%20state-of-the-art%20methods%20%28e.g.%2C%20Quanta%0ABurst%20Photography%20%28QBP%29%29.%20Our%20approach%20significantly%20enhances%20the%20visualization%0Aand%20usability%20of%20the%20data%2C%20enabling%20the%20application%20of%20existing%20analysis%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23247v2&entry.124074799=Read"},
{"title": "Improving Context-Aware Preference Modeling for Language Models", "author": "Silviu Pitis and Ziang Xiao and Nicolas Le Roux and Alessandro Sordoni", "abstract": "  While finetuning language models from pairwise preferences has proven\nremarkably effective, the underspecified nature of natural language presents\ncritical challenges. Direct preference feedback is uninterpretable, difficult\nto provide where multidimensional criteria may apply, and often inconsistent,\neither because it is based on incomplete instructions or provided by diverse\nprincipals. To address these challenges, we consider the two-step preference\nmodeling procedure that first resolves the under-specification by selecting a\ncontext, and then evaluates preference with respect to the chosen context. We\ndecompose reward modeling error according to these two steps, which suggests\nthat supervising context in addition to context-specific preference may be a\nviable approach to aligning models with diverse human preferences. For this to\nwork, the ability of models to evaluate context-specific preference is\ncritical. To this end, we contribute context-conditioned preference datasets\nand accompanying experiments that investigate the ability of language models to\nevaluate context-specific preference. We use our datasets to (1) show that\nexisting preference models benefit from, but fail to fully consider, added\ncontext, (2) finetune a context-aware reward model with context-specific\nperformance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)\ninvestigate the value of context-aware preference modeling.\n", "link": "http://arxiv.org/abs/2407.14916v2", "date": "2024-11-06", "relevancy": 2.2064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Context-Aware%20Preference%20Modeling%20for%20Language%20Models&body=Title%3A%20Improving%20Context-Aware%20Preference%20Modeling%20for%20Language%20Models%0AAuthor%3A%20Silviu%20Pitis%20and%20Ziang%20Xiao%20and%20Nicolas%20Le%20Roux%20and%20Alessandro%20Sordoni%0AAbstract%3A%20%20%20While%20finetuning%20language%20models%20from%20pairwise%20preferences%20has%20proven%0Aremarkably%20effective%2C%20the%20underspecified%20nature%20of%20natural%20language%20presents%0Acritical%20challenges.%20Direct%20preference%20feedback%20is%20uninterpretable%2C%20difficult%0Ato%20provide%20where%20multidimensional%20criteria%20may%20apply%2C%20and%20often%20inconsistent%2C%0Aeither%20because%20it%20is%20based%20on%20incomplete%20instructions%20or%20provided%20by%20diverse%0Aprincipals.%20To%20address%20these%20challenges%2C%20we%20consider%20the%20two-step%20preference%0Amodeling%20procedure%20that%20first%20resolves%20the%20under-specification%20by%20selecting%20a%0Acontext%2C%20and%20then%20evaluates%20preference%20with%20respect%20to%20the%20chosen%20context.%20We%0Adecompose%20reward%20modeling%20error%20according%20to%20these%20two%20steps%2C%20which%20suggests%0Athat%20supervising%20context%20in%20addition%20to%20context-specific%20preference%20may%20be%20a%0Aviable%20approach%20to%20aligning%20models%20with%20diverse%20human%20preferences.%20For%20this%20to%0Awork%2C%20the%20ability%20of%20models%20to%20evaluate%20context-specific%20preference%20is%0Acritical.%20To%20this%20end%2C%20we%20contribute%20context-conditioned%20preference%20datasets%0Aand%20accompanying%20experiments%20that%20investigate%20the%20ability%20of%20language%20models%20to%0Aevaluate%20context-specific%20preference.%20We%20use%20our%20datasets%20to%20%281%29%20show%20that%0Aexisting%20preference%20models%20benefit%20from%2C%20but%20fail%20to%20fully%20consider%2C%20added%0Acontext%2C%20%282%29%20finetune%20a%20context-aware%20reward%20model%20with%20context-specific%0Aperformance%20exceeding%20that%20of%20GPT-4%20and%20Llama%203%2070B%20on%20tested%20datasets%2C%20and%20%283%29%0Ainvestigate%20the%20value%20of%20context-aware%20preference%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14916v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Context-Aware%2520Preference%2520Modeling%2520for%2520Language%2520Models%26entry.906535625%3DSilviu%2520Pitis%2520and%2520Ziang%2520Xiao%2520and%2520Nicolas%2520Le%2520Roux%2520and%2520Alessandro%2520Sordoni%26entry.1292438233%3D%2520%2520While%2520finetuning%2520language%2520models%2520from%2520pairwise%2520preferences%2520has%2520proven%250Aremarkably%2520effective%252C%2520the%2520underspecified%2520nature%2520of%2520natural%2520language%2520presents%250Acritical%2520challenges.%2520Direct%2520preference%2520feedback%2520is%2520uninterpretable%252C%2520difficult%250Ato%2520provide%2520where%2520multidimensional%2520criteria%2520may%2520apply%252C%2520and%2520often%2520inconsistent%252C%250Aeither%2520because%2520it%2520is%2520based%2520on%2520incomplete%2520instructions%2520or%2520provided%2520by%2520diverse%250Aprincipals.%2520To%2520address%2520these%2520challenges%252C%2520we%2520consider%2520the%2520two-step%2520preference%250Amodeling%2520procedure%2520that%2520first%2520resolves%2520the%2520under-specification%2520by%2520selecting%2520a%250Acontext%252C%2520and%2520then%2520evaluates%2520preference%2520with%2520respect%2520to%2520the%2520chosen%2520context.%2520We%250Adecompose%2520reward%2520modeling%2520error%2520according%2520to%2520these%2520two%2520steps%252C%2520which%2520suggests%250Athat%2520supervising%2520context%2520in%2520addition%2520to%2520context-specific%2520preference%2520may%2520be%2520a%250Aviable%2520approach%2520to%2520aligning%2520models%2520with%2520diverse%2520human%2520preferences.%2520For%2520this%2520to%250Awork%252C%2520the%2520ability%2520of%2520models%2520to%2520evaluate%2520context-specific%2520preference%2520is%250Acritical.%2520To%2520this%2520end%252C%2520we%2520contribute%2520context-conditioned%2520preference%2520datasets%250Aand%2520accompanying%2520experiments%2520that%2520investigate%2520the%2520ability%2520of%2520language%2520models%2520to%250Aevaluate%2520context-specific%2520preference.%2520We%2520use%2520our%2520datasets%2520to%2520%25281%2529%2520show%2520that%250Aexisting%2520preference%2520models%2520benefit%2520from%252C%2520but%2520fail%2520to%2520fully%2520consider%252C%2520added%250Acontext%252C%2520%25282%2529%2520finetune%2520a%2520context-aware%2520reward%2520model%2520with%2520context-specific%250Aperformance%2520exceeding%2520that%2520of%2520GPT-4%2520and%2520Llama%25203%252070B%2520on%2520tested%2520datasets%252C%2520and%2520%25283%2529%250Ainvestigate%2520the%2520value%2520of%2520context-aware%2520preference%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14916v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Context-Aware%20Preference%20Modeling%20for%20Language%20Models&entry.906535625=Silviu%20Pitis%20and%20Ziang%20Xiao%20and%20Nicolas%20Le%20Roux%20and%20Alessandro%20Sordoni&entry.1292438233=%20%20While%20finetuning%20language%20models%20from%20pairwise%20preferences%20has%20proven%0Aremarkably%20effective%2C%20the%20underspecified%20nature%20of%20natural%20language%20presents%0Acritical%20challenges.%20Direct%20preference%20feedback%20is%20uninterpretable%2C%20difficult%0Ato%20provide%20where%20multidimensional%20criteria%20may%20apply%2C%20and%20often%20inconsistent%2C%0Aeither%20because%20it%20is%20based%20on%20incomplete%20instructions%20or%20provided%20by%20diverse%0Aprincipals.%20To%20address%20these%20challenges%2C%20we%20consider%20the%20two-step%20preference%0Amodeling%20procedure%20that%20first%20resolves%20the%20under-specification%20by%20selecting%20a%0Acontext%2C%20and%20then%20evaluates%20preference%20with%20respect%20to%20the%20chosen%20context.%20We%0Adecompose%20reward%20modeling%20error%20according%20to%20these%20two%20steps%2C%20which%20suggests%0Athat%20supervising%20context%20in%20addition%20to%20context-specific%20preference%20may%20be%20a%0Aviable%20approach%20to%20aligning%20models%20with%20diverse%20human%20preferences.%20For%20this%20to%0Awork%2C%20the%20ability%20of%20models%20to%20evaluate%20context-specific%20preference%20is%0Acritical.%20To%20this%20end%2C%20we%20contribute%20context-conditioned%20preference%20datasets%0Aand%20accompanying%20experiments%20that%20investigate%20the%20ability%20of%20language%20models%20to%0Aevaluate%20context-specific%20preference.%20We%20use%20our%20datasets%20to%20%281%29%20show%20that%0Aexisting%20preference%20models%20benefit%20from%2C%20but%20fail%20to%20fully%20consider%2C%20added%0Acontext%2C%20%282%29%20finetune%20a%20context-aware%20reward%20model%20with%20context-specific%0Aperformance%20exceeding%20that%20of%20GPT-4%20and%20Llama%203%2070B%20on%20tested%20datasets%2C%20and%20%283%29%0Ainvestigate%20the%20value%20of%20context-aware%20preference%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14916v2&entry.124074799=Read"},
{"title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for\n  Evaluating Foundation Models", "author": "Chuhan Li and Ziyao Shangguan and Yilun Zhao and Deyuan Li and Yixin Liu and Arman Cohan", "abstract": "  Existing benchmarks for evaluating foundation models mainly focus on\nsingle-document, text-only tasks. However, they often fail to fully capture the\ncomplexity of research workflows, which typically involve interpreting\nnon-textual data and gathering information across multiple documents. To\naddress this gap, we introduce M3SciQA, a multi-modal, multi-document\nscientific question answering benchmark designed for a more comprehensive\nevaluation of foundation models. M3SciQA consists of 1,452 expert-annotated\nquestions spanning 70 natural language processing paper clusters, where each\ncluster represents a primary paper along with all its cited documents,\nmirroring the workflow of comprehending a single paper by requiring multi-modal\nand multi-document data. With M3SciQA, we conduct a comprehensive evaluation of\n18 foundation models. Our results indicate that current foundation models still\nsignificantly underperform compared to human experts in multi-modal information\nretrieval and in reasoning across multiple scientific documents. Additionally,\nwe explore the implications of these findings for the future advancement of\napplying foundation models in multi-modal scientific literature analysis.\n", "link": "http://arxiv.org/abs/2411.04075v1", "date": "2024-11-06", "relevancy": 2.1952, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3SciQA%3A%20A%20Multi-Modal%20Multi-Document%20Scientific%20QA%20Benchmark%20for%0A%20%20Evaluating%20Foundation%20Models&body=Title%3A%20M3SciQA%3A%20A%20Multi-Modal%20Multi-Document%20Scientific%20QA%20Benchmark%20for%0A%20%20Evaluating%20Foundation%20Models%0AAuthor%3A%20Chuhan%20Li%20and%20Ziyao%20Shangguan%20and%20Yilun%20Zhao%20and%20Deyuan%20Li%20and%20Yixin%20Liu%20and%20Arman%20Cohan%0AAbstract%3A%20%20%20Existing%20benchmarks%20for%20evaluating%20foundation%20models%20mainly%20focus%20on%0Asingle-document%2C%20text-only%20tasks.%20However%2C%20they%20often%20fail%20to%20fully%20capture%20the%0Acomplexity%20of%20research%20workflows%2C%20which%20typically%20involve%20interpreting%0Anon-textual%20data%20and%20gathering%20information%20across%20multiple%20documents.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20M3SciQA%2C%20a%20multi-modal%2C%20multi-document%0Ascientific%20question%20answering%20benchmark%20designed%20for%20a%20more%20comprehensive%0Aevaluation%20of%20foundation%20models.%20M3SciQA%20consists%20of%201%2C452%20expert-annotated%0Aquestions%20spanning%2070%20natural%20language%20processing%20paper%20clusters%2C%20where%20each%0Acluster%20represents%20a%20primary%20paper%20along%20with%20all%20its%20cited%20documents%2C%0Amirroring%20the%20workflow%20of%20comprehending%20a%20single%20paper%20by%20requiring%20multi-modal%0Aand%20multi-document%20data.%20With%20M3SciQA%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%0A18%20foundation%20models.%20Our%20results%20indicate%20that%20current%20foundation%20models%20still%0Asignificantly%20underperform%20compared%20to%20human%20experts%20in%20multi-modal%20information%0Aretrieval%20and%20in%20reasoning%20across%20multiple%20scientific%20documents.%20Additionally%2C%0Awe%20explore%20the%20implications%20of%20these%20findings%20for%20the%20future%20advancement%20of%0Aapplying%20foundation%20models%20in%20multi-modal%20scientific%20literature%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3SciQA%253A%2520A%2520Multi-Modal%2520Multi-Document%2520Scientific%2520QA%2520Benchmark%2520for%250A%2520%2520Evaluating%2520Foundation%2520Models%26entry.906535625%3DChuhan%2520Li%2520and%2520Ziyao%2520Shangguan%2520and%2520Yilun%2520Zhao%2520and%2520Deyuan%2520Li%2520and%2520Yixin%2520Liu%2520and%2520Arman%2520Cohan%26entry.1292438233%3D%2520%2520Existing%2520benchmarks%2520for%2520evaluating%2520foundation%2520models%2520mainly%2520focus%2520on%250Asingle-document%252C%2520text-only%2520tasks.%2520However%252C%2520they%2520often%2520fail%2520to%2520fully%2520capture%2520the%250Acomplexity%2520of%2520research%2520workflows%252C%2520which%2520typically%2520involve%2520interpreting%250Anon-textual%2520data%2520and%2520gathering%2520information%2520across%2520multiple%2520documents.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520M3SciQA%252C%2520a%2520multi-modal%252C%2520multi-document%250Ascientific%2520question%2520answering%2520benchmark%2520designed%2520for%2520a%2520more%2520comprehensive%250Aevaluation%2520of%2520foundation%2520models.%2520M3SciQA%2520consists%2520of%25201%252C452%2520expert-annotated%250Aquestions%2520spanning%252070%2520natural%2520language%2520processing%2520paper%2520clusters%252C%2520where%2520each%250Acluster%2520represents%2520a%2520primary%2520paper%2520along%2520with%2520all%2520its%2520cited%2520documents%252C%250Amirroring%2520the%2520workflow%2520of%2520comprehending%2520a%2520single%2520paper%2520by%2520requiring%2520multi-modal%250Aand%2520multi-document%2520data.%2520With%2520M3SciQA%252C%2520we%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%250A18%2520foundation%2520models.%2520Our%2520results%2520indicate%2520that%2520current%2520foundation%2520models%2520still%250Asignificantly%2520underperform%2520compared%2520to%2520human%2520experts%2520in%2520multi-modal%2520information%250Aretrieval%2520and%2520in%2520reasoning%2520across%2520multiple%2520scientific%2520documents.%2520Additionally%252C%250Awe%2520explore%2520the%2520implications%2520of%2520these%2520findings%2520for%2520the%2520future%2520advancement%2520of%250Aapplying%2520foundation%2520models%2520in%2520multi-modal%2520scientific%2520literature%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3SciQA%3A%20A%20Multi-Modal%20Multi-Document%20Scientific%20QA%20Benchmark%20for%0A%20%20Evaluating%20Foundation%20Models&entry.906535625=Chuhan%20Li%20and%20Ziyao%20Shangguan%20and%20Yilun%20Zhao%20and%20Deyuan%20Li%20and%20Yixin%20Liu%20and%20Arman%20Cohan&entry.1292438233=%20%20Existing%20benchmarks%20for%20evaluating%20foundation%20models%20mainly%20focus%20on%0Asingle-document%2C%20text-only%20tasks.%20However%2C%20they%20often%20fail%20to%20fully%20capture%20the%0Acomplexity%20of%20research%20workflows%2C%20which%20typically%20involve%20interpreting%0Anon-textual%20data%20and%20gathering%20information%20across%20multiple%20documents.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20M3SciQA%2C%20a%20multi-modal%2C%20multi-document%0Ascientific%20question%20answering%20benchmark%20designed%20for%20a%20more%20comprehensive%0Aevaluation%20of%20foundation%20models.%20M3SciQA%20consists%20of%201%2C452%20expert-annotated%0Aquestions%20spanning%2070%20natural%20language%20processing%20paper%20clusters%2C%20where%20each%0Acluster%20represents%20a%20primary%20paper%20along%20with%20all%20its%20cited%20documents%2C%0Amirroring%20the%20workflow%20of%20comprehending%20a%20single%20paper%20by%20requiring%20multi-modal%0Aand%20multi-document%20data.%20With%20M3SciQA%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%0A18%20foundation%20models.%20Our%20results%20indicate%20that%20current%20foundation%20models%20still%0Asignificantly%20underperform%20compared%20to%20human%20experts%20in%20multi-modal%20information%0Aretrieval%20and%20in%20reasoning%20across%20multiple%20scientific%20documents.%20Additionally%2C%0Awe%20explore%20the%20implications%20of%20these%20findings%20for%20the%20future%20advancement%20of%0Aapplying%20foundation%20models%20in%20multi-modal%20scientific%20literature%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04075v1&entry.124074799=Read"},
{"title": "Cross-Task Affinity Learning for Multitask Dense Scene Predictions", "author": "Dimitrios Sinodinos and Narges Armanfard", "abstract": "  Multitask learning (MTL) has become prominent for its ability to predict\nmultiple tasks jointly, achieving better per-task performance with fewer\nparameters than single-task learning. Recently, decoder-focused architectures\nhave significantly improved multitask performance by refining task predictions\nusing features from related tasks. However, most refinement methods struggle to\nefficiently capture both local and long-range dependencies between\ntask-specific representations and cross-task patterns. In this paper, we\nintroduce the Cross-Task Affinity Learning (CTAL) module, a lightweight\nframework that enhances task refinement in multitask networks. CTAL effectively\ncaptures local and long-range cross-task interactions by optimizing task\naffinity matrices for parameter-efficient grouped convolutions without concern\nfor information loss. Our results demonstrate state-of-the-art MTL performance\nfor both CNN and transformer backbones, using significantly fewer parameters\nthan single-task learning. Our code is publicly available at\nhttps://github.com/Armanfard-Lab/EMA-Net.\n", "link": "http://arxiv.org/abs/2401.11124v2", "date": "2024-11-06", "relevancy": 2.1945, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5927}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5207}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Task%20Affinity%20Learning%20for%20Multitask%20Dense%20Scene%20Predictions&body=Title%3A%20Cross-Task%20Affinity%20Learning%20for%20Multitask%20Dense%20Scene%20Predictions%0AAuthor%3A%20Dimitrios%20Sinodinos%20and%20Narges%20Armanfard%0AAbstract%3A%20%20%20Multitask%20learning%20%28MTL%29%20has%20become%20prominent%20for%20its%20ability%20to%20predict%0Amultiple%20tasks%20jointly%2C%20achieving%20better%20per-task%20performance%20with%20fewer%0Aparameters%20than%20single-task%20learning.%20Recently%2C%20decoder-focused%20architectures%0Ahave%20significantly%20improved%20multitask%20performance%20by%20refining%20task%20predictions%0Ausing%20features%20from%20related%20tasks.%20However%2C%20most%20refinement%20methods%20struggle%20to%0Aefficiently%20capture%20both%20local%20and%20long-range%20dependencies%20between%0Atask-specific%20representations%20and%20cross-task%20patterns.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20Cross-Task%20Affinity%20Learning%20%28CTAL%29%20module%2C%20a%20lightweight%0Aframework%20that%20enhances%20task%20refinement%20in%20multitask%20networks.%20CTAL%20effectively%0Acaptures%20local%20and%20long-range%20cross-task%20interactions%20by%20optimizing%20task%0Aaffinity%20matrices%20for%20parameter-efficient%20grouped%20convolutions%20without%20concern%0Afor%20information%20loss.%20Our%20results%20demonstrate%20state-of-the-art%20MTL%20performance%0Afor%20both%20CNN%20and%20transformer%20backbones%2C%20using%20significantly%20fewer%20parameters%0Athan%20single-task%20learning.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Armanfard-Lab/EMA-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11124v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Task%2520Affinity%2520Learning%2520for%2520Multitask%2520Dense%2520Scene%2520Predictions%26entry.906535625%3DDimitrios%2520Sinodinos%2520and%2520Narges%2520Armanfard%26entry.1292438233%3D%2520%2520Multitask%2520learning%2520%2528MTL%2529%2520has%2520become%2520prominent%2520for%2520its%2520ability%2520to%2520predict%250Amultiple%2520tasks%2520jointly%252C%2520achieving%2520better%2520per-task%2520performance%2520with%2520fewer%250Aparameters%2520than%2520single-task%2520learning.%2520Recently%252C%2520decoder-focused%2520architectures%250Ahave%2520significantly%2520improved%2520multitask%2520performance%2520by%2520refining%2520task%2520predictions%250Ausing%2520features%2520from%2520related%2520tasks.%2520However%252C%2520most%2520refinement%2520methods%2520struggle%2520to%250Aefficiently%2520capture%2520both%2520local%2520and%2520long-range%2520dependencies%2520between%250Atask-specific%2520representations%2520and%2520cross-task%2520patterns.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520Cross-Task%2520Affinity%2520Learning%2520%2528CTAL%2529%2520module%252C%2520a%2520lightweight%250Aframework%2520that%2520enhances%2520task%2520refinement%2520in%2520multitask%2520networks.%2520CTAL%2520effectively%250Acaptures%2520local%2520and%2520long-range%2520cross-task%2520interactions%2520by%2520optimizing%2520task%250Aaffinity%2520matrices%2520for%2520parameter-efficient%2520grouped%2520convolutions%2520without%2520concern%250Afor%2520information%2520loss.%2520Our%2520results%2520demonstrate%2520state-of-the-art%2520MTL%2520performance%250Afor%2520both%2520CNN%2520and%2520transformer%2520backbones%252C%2520using%2520significantly%2520fewer%2520parameters%250Athan%2520single-task%2520learning.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Armanfard-Lab/EMA-Net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11124v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Task%20Affinity%20Learning%20for%20Multitask%20Dense%20Scene%20Predictions&entry.906535625=Dimitrios%20Sinodinos%20and%20Narges%20Armanfard&entry.1292438233=%20%20Multitask%20learning%20%28MTL%29%20has%20become%20prominent%20for%20its%20ability%20to%20predict%0Amultiple%20tasks%20jointly%2C%20achieving%20better%20per-task%20performance%20with%20fewer%0Aparameters%20than%20single-task%20learning.%20Recently%2C%20decoder-focused%20architectures%0Ahave%20significantly%20improved%20multitask%20performance%20by%20refining%20task%20predictions%0Ausing%20features%20from%20related%20tasks.%20However%2C%20most%20refinement%20methods%20struggle%20to%0Aefficiently%20capture%20both%20local%20and%20long-range%20dependencies%20between%0Atask-specific%20representations%20and%20cross-task%20patterns.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20Cross-Task%20Affinity%20Learning%20%28CTAL%29%20module%2C%20a%20lightweight%0Aframework%20that%20enhances%20task%20refinement%20in%20multitask%20networks.%20CTAL%20effectively%0Acaptures%20local%20and%20long-range%20cross-task%20interactions%20by%20optimizing%20task%0Aaffinity%20matrices%20for%20parameter-efficient%20grouped%20convolutions%20without%20concern%0Afor%20information%20loss.%20Our%20results%20demonstrate%20state-of-the-art%20MTL%20performance%0Afor%20both%20CNN%20and%20transformer%20backbones%2C%20using%20significantly%20fewer%20parameters%0Athan%20single-task%20learning.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Armanfard-Lab/EMA-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11124v2&entry.124074799=Read"},
{"title": "Memorized action chunking with Transformers: Imitation learning for\n  vision-based tissue surface scanning", "author": "Bochen Yang and Kaizhong Deng and Christopher J Peters and George Mylonas and Daniel S. Elson", "abstract": "  Optical sensing technologies are emerging technologies used in cancer\nsurgeries to ensure the complete removal of cancerous tissue. While point-wise\nassessment has many potential applications, incorporating automated large area\nscanning would enable holistic tissue sampling. However, such scanning tasks\nare challenging due to their long-horizon dependency and the requirement for\nfine-grained motion. To address these issues, we introduce Memorized Action\nChunking with Transformers (MACT), an intuitive yet efficient imitation\nlearning method for tissue surface scanning tasks. It utilizes a sequence of\npast images as historical information to predict near-future action sequences.\nIn addition, hybrid temporal-spatial positional embeddings were employed to\nfacilitate learning. In various simulation settings, MACT demonstrated\nsignificant improvements in contour scanning and area scanning over the\nbaseline model. In real-world testing, with only 50 demonstration trajectories,\nMACT surpassed the baseline model by achieving a 60-80% success rate on all\nscanning tasks. Our findings suggest that MACT is a promising model for\nadaptive scanning in surgical settings.\n", "link": "http://arxiv.org/abs/2411.04050v1", "date": "2024-11-06", "relevancy": 2.1915, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5647}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5407}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorized%20action%20chunking%20with%20Transformers%3A%20Imitation%20learning%20for%0A%20%20vision-based%20tissue%20surface%20scanning&body=Title%3A%20Memorized%20action%20chunking%20with%20Transformers%3A%20Imitation%20learning%20for%0A%20%20vision-based%20tissue%20surface%20scanning%0AAuthor%3A%20Bochen%20Yang%20and%20Kaizhong%20Deng%20and%20Christopher%20J%20Peters%20and%20George%20Mylonas%20and%20Daniel%20S.%20Elson%0AAbstract%3A%20%20%20Optical%20sensing%20technologies%20are%20emerging%20technologies%20used%20in%20cancer%0Asurgeries%20to%20ensure%20the%20complete%20removal%20of%20cancerous%20tissue.%20While%20point-wise%0Aassessment%20has%20many%20potential%20applications%2C%20incorporating%20automated%20large%20area%0Ascanning%20would%20enable%20holistic%20tissue%20sampling.%20However%2C%20such%20scanning%20tasks%0Aare%20challenging%20due%20to%20their%20long-horizon%20dependency%20and%20the%20requirement%20for%0Afine-grained%20motion.%20To%20address%20these%20issues%2C%20we%20introduce%20Memorized%20Action%0AChunking%20with%20Transformers%20%28MACT%29%2C%20an%20intuitive%20yet%20efficient%20imitation%0Alearning%20method%20for%20tissue%20surface%20scanning%20tasks.%20It%20utilizes%20a%20sequence%20of%0Apast%20images%20as%20historical%20information%20to%20predict%20near-future%20action%20sequences.%0AIn%20addition%2C%20hybrid%20temporal-spatial%20positional%20embeddings%20were%20employed%20to%0Afacilitate%20learning.%20In%20various%20simulation%20settings%2C%20MACT%20demonstrated%0Asignificant%20improvements%20in%20contour%20scanning%20and%20area%20scanning%20over%20the%0Abaseline%20model.%20In%20real-world%20testing%2C%20with%20only%2050%20demonstration%20trajectories%2C%0AMACT%20surpassed%20the%20baseline%20model%20by%20achieving%20a%2060-80%25%20success%20rate%20on%20all%0Ascanning%20tasks.%20Our%20findings%20suggest%20that%20MACT%20is%20a%20promising%20model%20for%0Aadaptive%20scanning%20in%20surgical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorized%2520action%2520chunking%2520with%2520Transformers%253A%2520Imitation%2520learning%2520for%250A%2520%2520vision-based%2520tissue%2520surface%2520scanning%26entry.906535625%3DBochen%2520Yang%2520and%2520Kaizhong%2520Deng%2520and%2520Christopher%2520J%2520Peters%2520and%2520George%2520Mylonas%2520and%2520Daniel%2520S.%2520Elson%26entry.1292438233%3D%2520%2520Optical%2520sensing%2520technologies%2520are%2520emerging%2520technologies%2520used%2520in%2520cancer%250Asurgeries%2520to%2520ensure%2520the%2520complete%2520removal%2520of%2520cancerous%2520tissue.%2520While%2520point-wise%250Aassessment%2520has%2520many%2520potential%2520applications%252C%2520incorporating%2520automated%2520large%2520area%250Ascanning%2520would%2520enable%2520holistic%2520tissue%2520sampling.%2520However%252C%2520such%2520scanning%2520tasks%250Aare%2520challenging%2520due%2520to%2520their%2520long-horizon%2520dependency%2520and%2520the%2520requirement%2520for%250Afine-grained%2520motion.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Memorized%2520Action%250AChunking%2520with%2520Transformers%2520%2528MACT%2529%252C%2520an%2520intuitive%2520yet%2520efficient%2520imitation%250Alearning%2520method%2520for%2520tissue%2520surface%2520scanning%2520tasks.%2520It%2520utilizes%2520a%2520sequence%2520of%250Apast%2520images%2520as%2520historical%2520information%2520to%2520predict%2520near-future%2520action%2520sequences.%250AIn%2520addition%252C%2520hybrid%2520temporal-spatial%2520positional%2520embeddings%2520were%2520employed%2520to%250Afacilitate%2520learning.%2520In%2520various%2520simulation%2520settings%252C%2520MACT%2520demonstrated%250Asignificant%2520improvements%2520in%2520contour%2520scanning%2520and%2520area%2520scanning%2520over%2520the%250Abaseline%2520model.%2520In%2520real-world%2520testing%252C%2520with%2520only%252050%2520demonstration%2520trajectories%252C%250AMACT%2520surpassed%2520the%2520baseline%2520model%2520by%2520achieving%2520a%252060-80%2525%2520success%2520rate%2520on%2520all%250Ascanning%2520tasks.%2520Our%2520findings%2520suggest%2520that%2520MACT%2520is%2520a%2520promising%2520model%2520for%250Aadaptive%2520scanning%2520in%2520surgical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorized%20action%20chunking%20with%20Transformers%3A%20Imitation%20learning%20for%0A%20%20vision-based%20tissue%20surface%20scanning&entry.906535625=Bochen%20Yang%20and%20Kaizhong%20Deng%20and%20Christopher%20J%20Peters%20and%20George%20Mylonas%20and%20Daniel%20S.%20Elson&entry.1292438233=%20%20Optical%20sensing%20technologies%20are%20emerging%20technologies%20used%20in%20cancer%0Asurgeries%20to%20ensure%20the%20complete%20removal%20of%20cancerous%20tissue.%20While%20point-wise%0Aassessment%20has%20many%20potential%20applications%2C%20incorporating%20automated%20large%20area%0Ascanning%20would%20enable%20holistic%20tissue%20sampling.%20However%2C%20such%20scanning%20tasks%0Aare%20challenging%20due%20to%20their%20long-horizon%20dependency%20and%20the%20requirement%20for%0Afine-grained%20motion.%20To%20address%20these%20issues%2C%20we%20introduce%20Memorized%20Action%0AChunking%20with%20Transformers%20%28MACT%29%2C%20an%20intuitive%20yet%20efficient%20imitation%0Alearning%20method%20for%20tissue%20surface%20scanning%20tasks.%20It%20utilizes%20a%20sequence%20of%0Apast%20images%20as%20historical%20information%20to%20predict%20near-future%20action%20sequences.%0AIn%20addition%2C%20hybrid%20temporal-spatial%20positional%20embeddings%20were%20employed%20to%0Afacilitate%20learning.%20In%20various%20simulation%20settings%2C%20MACT%20demonstrated%0Asignificant%20improvements%20in%20contour%20scanning%20and%20area%20scanning%20over%20the%0Abaseline%20model.%20In%20real-world%20testing%2C%20with%20only%2050%20demonstration%20trajectories%2C%0AMACT%20surpassed%20the%20baseline%20model%20by%20achieving%20a%2060-80%25%20success%20rate%20on%20all%0Ascanning%20tasks.%20Our%20findings%20suggest%20that%20MACT%20is%20a%20promising%20model%20for%0Aadaptive%20scanning%20in%20surgical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04050v1&entry.124074799=Read"},
{"title": "Navigating Extremes: Dynamic Sparsity in Large Output Space", "author": "Nasib Ullah and Erik Schultheis and Mike Lasby and Yani Ioannou and Rohit Babbar", "abstract": "  In recent years, Dynamic Sparse Training (DST) has emerged as an alternative\nto post-training pruning for generating efficient models. In principle, DST\nallows for a more memory efficient training process, as it maintains sparsity\nthroughout the entire training run. However, current DST implementations fail\nto capitalize on this in practice. Because sparse matrix multiplication is much\nless efficient than dense matrix multiplication on GPUs, most implementations\nsimulate sparsity by masking weights. In this paper, we leverage recent\nadvances in semi-structured sparse training to apply DST in the domain of\nclassification with large output spaces, where memory-efficiency is paramount.\nWith a label space of possibly millions of candidates, the classification layer\nalone will consume several gigabytes of memory. Switching from a dense to a\nfixed fan-in sparse layer updated with sparse evolutionary training (SET);\nhowever, severely hampers training convergence, especially at the largest label\nspaces. We find that poor gradient flow from the sparse classifier to the dense\ntext encoder make it difficult to learn good input representations. By\nemploying an intermediate layer or adding an auxiliary training objective, we\nrecover most of the generalisation performance of the dense model. Overall, we\ndemonstrate the applicability and practical benefits of DST in a challenging\ndomain -- characterized by a highly skewed label distribution that differs\nsubstantially from typical DST benchmark datasets -- which enables end-to-end\ntraining with millions of labels on commodity hardware.\n", "link": "http://arxiv.org/abs/2411.03171v2", "date": "2024-11-06", "relevancy": 2.1656, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5461}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5451}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20Extremes%3A%20Dynamic%20Sparsity%20in%20Large%20Output%20Space&body=Title%3A%20Navigating%20Extremes%3A%20Dynamic%20Sparsity%20in%20Large%20Output%20Space%0AAuthor%3A%20Nasib%20Ullah%20and%20Erik%20Schultheis%20and%20Mike%20Lasby%20and%20Yani%20Ioannou%20and%20Rohit%20Babbar%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Dynamic%20Sparse%20Training%20%28DST%29%20has%20emerged%20as%20an%20alternative%0Ato%20post-training%20pruning%20for%20generating%20efficient%20models.%20In%20principle%2C%20DST%0Aallows%20for%20a%20more%20memory%20efficient%20training%20process%2C%20as%20it%20maintains%20sparsity%0Athroughout%20the%20entire%20training%20run.%20However%2C%20current%20DST%20implementations%20fail%0Ato%20capitalize%20on%20this%20in%20practice.%20Because%20sparse%20matrix%20multiplication%20is%20much%0Aless%20efficient%20than%20dense%20matrix%20multiplication%20on%20GPUs%2C%20most%20implementations%0Asimulate%20sparsity%20by%20masking%20weights.%20In%20this%20paper%2C%20we%20leverage%20recent%0Aadvances%20in%20semi-structured%20sparse%20training%20to%20apply%20DST%20in%20the%20domain%20of%0Aclassification%20with%20large%20output%20spaces%2C%20where%20memory-efficiency%20is%20paramount.%0AWith%20a%20label%20space%20of%20possibly%20millions%20of%20candidates%2C%20the%20classification%20layer%0Aalone%20will%20consume%20several%20gigabytes%20of%20memory.%20Switching%20from%20a%20dense%20to%20a%0Afixed%20fan-in%20sparse%20layer%20updated%20with%20sparse%20evolutionary%20training%20%28SET%29%3B%0Ahowever%2C%20severely%20hampers%20training%20convergence%2C%20especially%20at%20the%20largest%20label%0Aspaces.%20We%20find%20that%20poor%20gradient%20flow%20from%20the%20sparse%20classifier%20to%20the%20dense%0Atext%20encoder%20make%20it%20difficult%20to%20learn%20good%20input%20representations.%20By%0Aemploying%20an%20intermediate%20layer%20or%20adding%20an%20auxiliary%20training%20objective%2C%20we%0Arecover%20most%20of%20the%20generalisation%20performance%20of%20the%20dense%20model.%20Overall%2C%20we%0Ademonstrate%20the%20applicability%20and%20practical%20benefits%20of%20DST%20in%20a%20challenging%0Adomain%20--%20characterized%20by%20a%20highly%20skewed%20label%20distribution%20that%20differs%0Asubstantially%20from%20typical%20DST%20benchmark%20datasets%20--%20which%20enables%20end-to-end%0Atraining%20with%20millions%20of%20labels%20on%20commodity%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520Extremes%253A%2520Dynamic%2520Sparsity%2520in%2520Large%2520Output%2520Space%26entry.906535625%3DNasib%2520Ullah%2520and%2520Erik%2520Schultheis%2520and%2520Mike%2520Lasby%2520and%2520Yani%2520Ioannou%2520and%2520Rohit%2520Babbar%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Dynamic%2520Sparse%2520Training%2520%2528DST%2529%2520has%2520emerged%2520as%2520an%2520alternative%250Ato%2520post-training%2520pruning%2520for%2520generating%2520efficient%2520models.%2520In%2520principle%252C%2520DST%250Aallows%2520for%2520a%2520more%2520memory%2520efficient%2520training%2520process%252C%2520as%2520it%2520maintains%2520sparsity%250Athroughout%2520the%2520entire%2520training%2520run.%2520However%252C%2520current%2520DST%2520implementations%2520fail%250Ato%2520capitalize%2520on%2520this%2520in%2520practice.%2520Because%2520sparse%2520matrix%2520multiplication%2520is%2520much%250Aless%2520efficient%2520than%2520dense%2520matrix%2520multiplication%2520on%2520GPUs%252C%2520most%2520implementations%250Asimulate%2520sparsity%2520by%2520masking%2520weights.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520recent%250Aadvances%2520in%2520semi-structured%2520sparse%2520training%2520to%2520apply%2520DST%2520in%2520the%2520domain%2520of%250Aclassification%2520with%2520large%2520output%2520spaces%252C%2520where%2520memory-efficiency%2520is%2520paramount.%250AWith%2520a%2520label%2520space%2520of%2520possibly%2520millions%2520of%2520candidates%252C%2520the%2520classification%2520layer%250Aalone%2520will%2520consume%2520several%2520gigabytes%2520of%2520memory.%2520Switching%2520from%2520a%2520dense%2520to%2520a%250Afixed%2520fan-in%2520sparse%2520layer%2520updated%2520with%2520sparse%2520evolutionary%2520training%2520%2528SET%2529%253B%250Ahowever%252C%2520severely%2520hampers%2520training%2520convergence%252C%2520especially%2520at%2520the%2520largest%2520label%250Aspaces.%2520We%2520find%2520that%2520poor%2520gradient%2520flow%2520from%2520the%2520sparse%2520classifier%2520to%2520the%2520dense%250Atext%2520encoder%2520make%2520it%2520difficult%2520to%2520learn%2520good%2520input%2520representations.%2520By%250Aemploying%2520an%2520intermediate%2520layer%2520or%2520adding%2520an%2520auxiliary%2520training%2520objective%252C%2520we%250Arecover%2520most%2520of%2520the%2520generalisation%2520performance%2520of%2520the%2520dense%2520model.%2520Overall%252C%2520we%250Ademonstrate%2520the%2520applicability%2520and%2520practical%2520benefits%2520of%2520DST%2520in%2520a%2520challenging%250Adomain%2520--%2520characterized%2520by%2520a%2520highly%2520skewed%2520label%2520distribution%2520that%2520differs%250Asubstantially%2520from%2520typical%2520DST%2520benchmark%2520datasets%2520--%2520which%2520enables%2520end-to-end%250Atraining%2520with%2520millions%2520of%2520labels%2520on%2520commodity%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20Extremes%3A%20Dynamic%20Sparsity%20in%20Large%20Output%20Space&entry.906535625=Nasib%20Ullah%20and%20Erik%20Schultheis%20and%20Mike%20Lasby%20and%20Yani%20Ioannou%20and%20Rohit%20Babbar&entry.1292438233=%20%20In%20recent%20years%2C%20Dynamic%20Sparse%20Training%20%28DST%29%20has%20emerged%20as%20an%20alternative%0Ato%20post-training%20pruning%20for%20generating%20efficient%20models.%20In%20principle%2C%20DST%0Aallows%20for%20a%20more%20memory%20efficient%20training%20process%2C%20as%20it%20maintains%20sparsity%0Athroughout%20the%20entire%20training%20run.%20However%2C%20current%20DST%20implementations%20fail%0Ato%20capitalize%20on%20this%20in%20practice.%20Because%20sparse%20matrix%20multiplication%20is%20much%0Aless%20efficient%20than%20dense%20matrix%20multiplication%20on%20GPUs%2C%20most%20implementations%0Asimulate%20sparsity%20by%20masking%20weights.%20In%20this%20paper%2C%20we%20leverage%20recent%0Aadvances%20in%20semi-structured%20sparse%20training%20to%20apply%20DST%20in%20the%20domain%20of%0Aclassification%20with%20large%20output%20spaces%2C%20where%20memory-efficiency%20is%20paramount.%0AWith%20a%20label%20space%20of%20possibly%20millions%20of%20candidates%2C%20the%20classification%20layer%0Aalone%20will%20consume%20several%20gigabytes%20of%20memory.%20Switching%20from%20a%20dense%20to%20a%0Afixed%20fan-in%20sparse%20layer%20updated%20with%20sparse%20evolutionary%20training%20%28SET%29%3B%0Ahowever%2C%20severely%20hampers%20training%20convergence%2C%20especially%20at%20the%20largest%20label%0Aspaces.%20We%20find%20that%20poor%20gradient%20flow%20from%20the%20sparse%20classifier%20to%20the%20dense%0Atext%20encoder%20make%20it%20difficult%20to%20learn%20good%20input%20representations.%20By%0Aemploying%20an%20intermediate%20layer%20or%20adding%20an%20auxiliary%20training%20objective%2C%20we%0Arecover%20most%20of%20the%20generalisation%20performance%20of%20the%20dense%20model.%20Overall%2C%20we%0Ademonstrate%20the%20applicability%20and%20practical%20benefits%20of%20DST%20in%20a%20challenging%0Adomain%20--%20characterized%20by%20a%20highly%20skewed%20label%20distribution%20that%20differs%0Asubstantially%20from%20typical%20DST%20benchmark%20datasets%20--%20which%20enables%20end-to-end%0Atraining%20with%20millions%20of%20labels%20on%20commodity%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03171v2&entry.124074799=Read"},
{"title": "An Enhancement of Haar Cascade Algorithm Applied to Face Recognition for\n  Gate Pass Security", "author": "Clarence A. Antipona and Romeo R. Magsino and Raymund M. Dioses and Khatalyn E. Mata", "abstract": "  This study is focused on enhancing the Haar Cascade Algorithm to decrease the\nfalse positive and false negative rate in face matching and face detection to\nincrease the accuracy rate even under challenging conditions. The face\nrecognition library was implemented with Haar Cascade Algorithm in which the\n128-dimensional vectors representing the unique features of a face are encoded.\nA subprocess was applied where the grayscale image from Haar Cascade was\nconverted to RGB to improve the face encoding. Logical process and face\nfiltering are also used to decrease non-face detection. The Enhanced Haar\nCascade Algorithm produced a 98.39% accuracy rate (21.39% increase), 63.59%\nprecision rate, 98.30% recall rate, and 72.23% in F1 Score. In comparison, the\nHaar Cascade Algorithm achieved a 46.70% to 77.00% accuracy rate, 44.15%\nprecision rate, 98.61% recall rate, and 47.01% in F1 Score. Both algorithms\nused the Confusion Matrix Test with 301,950 comparisons using the same dataset\nof 550 images. The 98.39% accuracy rate shows a significant decrease in false\npositive and false negative rates in facial recognition. Face matching and face\ndetection are more accurate in images with complex backgrounds, lighting\nvariations, and occlusions, or even those with similar attributes.\n", "link": "http://arxiv.org/abs/2411.03831v1", "date": "2024-11-06", "relevancy": 2.1561, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4382}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.432}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Enhancement%20of%20Haar%20Cascade%20Algorithm%20Applied%20to%20Face%20Recognition%20for%0A%20%20Gate%20Pass%20Security&body=Title%3A%20An%20Enhancement%20of%20Haar%20Cascade%20Algorithm%20Applied%20to%20Face%20Recognition%20for%0A%20%20Gate%20Pass%20Security%0AAuthor%3A%20Clarence%20A.%20Antipona%20and%20Romeo%20R.%20Magsino%20and%20Raymund%20M.%20Dioses%20and%20Khatalyn%20E.%20Mata%0AAbstract%3A%20%20%20This%20study%20is%20focused%20on%20enhancing%20the%20Haar%20Cascade%20Algorithm%20to%20decrease%20the%0Afalse%20positive%20and%20false%20negative%20rate%20in%20face%20matching%20and%20face%20detection%20to%0Aincrease%20the%20accuracy%20rate%20even%20under%20challenging%20conditions.%20The%20face%0Arecognition%20library%20was%20implemented%20with%20Haar%20Cascade%20Algorithm%20in%20which%20the%0A128-dimensional%20vectors%20representing%20the%20unique%20features%20of%20a%20face%20are%20encoded.%0AA%20subprocess%20was%20applied%20where%20the%20grayscale%20image%20from%20Haar%20Cascade%20was%0Aconverted%20to%20RGB%20to%20improve%20the%20face%20encoding.%20Logical%20process%20and%20face%0Afiltering%20are%20also%20used%20to%20decrease%20non-face%20detection.%20The%20Enhanced%20Haar%0ACascade%20Algorithm%20produced%20a%2098.39%25%20accuracy%20rate%20%2821.39%25%20increase%29%2C%2063.59%25%0Aprecision%20rate%2C%2098.30%25%20recall%20rate%2C%20and%2072.23%25%20in%20F1%20Score.%20In%20comparison%2C%20the%0AHaar%20Cascade%20Algorithm%20achieved%20a%2046.70%25%20to%2077.00%25%20accuracy%20rate%2C%2044.15%25%0Aprecision%20rate%2C%2098.61%25%20recall%20rate%2C%20and%2047.01%25%20in%20F1%20Score.%20Both%20algorithms%0Aused%20the%20Confusion%20Matrix%20Test%20with%20301%2C950%20comparisons%20using%20the%20same%20dataset%0Aof%20550%20images.%20The%2098.39%25%20accuracy%20rate%20shows%20a%20significant%20decrease%20in%20false%0Apositive%20and%20false%20negative%20rates%20in%20facial%20recognition.%20Face%20matching%20and%20face%0Adetection%20are%20more%20accurate%20in%20images%20with%20complex%20backgrounds%2C%20lighting%0Avariations%2C%20and%20occlusions%2C%20or%20even%20those%20with%20similar%20attributes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Enhancement%2520of%2520Haar%2520Cascade%2520Algorithm%2520Applied%2520to%2520Face%2520Recognition%2520for%250A%2520%2520Gate%2520Pass%2520Security%26entry.906535625%3DClarence%2520A.%2520Antipona%2520and%2520Romeo%2520R.%2520Magsino%2520and%2520Raymund%2520M.%2520Dioses%2520and%2520Khatalyn%2520E.%2520Mata%26entry.1292438233%3D%2520%2520This%2520study%2520is%2520focused%2520on%2520enhancing%2520the%2520Haar%2520Cascade%2520Algorithm%2520to%2520decrease%2520the%250Afalse%2520positive%2520and%2520false%2520negative%2520rate%2520in%2520face%2520matching%2520and%2520face%2520detection%2520to%250Aincrease%2520the%2520accuracy%2520rate%2520even%2520under%2520challenging%2520conditions.%2520The%2520face%250Arecognition%2520library%2520was%2520implemented%2520with%2520Haar%2520Cascade%2520Algorithm%2520in%2520which%2520the%250A128-dimensional%2520vectors%2520representing%2520the%2520unique%2520features%2520of%2520a%2520face%2520are%2520encoded.%250AA%2520subprocess%2520was%2520applied%2520where%2520the%2520grayscale%2520image%2520from%2520Haar%2520Cascade%2520was%250Aconverted%2520to%2520RGB%2520to%2520improve%2520the%2520face%2520encoding.%2520Logical%2520process%2520and%2520face%250Afiltering%2520are%2520also%2520used%2520to%2520decrease%2520non-face%2520detection.%2520The%2520Enhanced%2520Haar%250ACascade%2520Algorithm%2520produced%2520a%252098.39%2525%2520accuracy%2520rate%2520%252821.39%2525%2520increase%2529%252C%252063.59%2525%250Aprecision%2520rate%252C%252098.30%2525%2520recall%2520rate%252C%2520and%252072.23%2525%2520in%2520F1%2520Score.%2520In%2520comparison%252C%2520the%250AHaar%2520Cascade%2520Algorithm%2520achieved%2520a%252046.70%2525%2520to%252077.00%2525%2520accuracy%2520rate%252C%252044.15%2525%250Aprecision%2520rate%252C%252098.61%2525%2520recall%2520rate%252C%2520and%252047.01%2525%2520in%2520F1%2520Score.%2520Both%2520algorithms%250Aused%2520the%2520Confusion%2520Matrix%2520Test%2520with%2520301%252C950%2520comparisons%2520using%2520the%2520same%2520dataset%250Aof%2520550%2520images.%2520The%252098.39%2525%2520accuracy%2520rate%2520shows%2520a%2520significant%2520decrease%2520in%2520false%250Apositive%2520and%2520false%2520negative%2520rates%2520in%2520facial%2520recognition.%2520Face%2520matching%2520and%2520face%250Adetection%2520are%2520more%2520accurate%2520in%2520images%2520with%2520complex%2520backgrounds%252C%2520lighting%250Avariations%252C%2520and%2520occlusions%252C%2520or%2520even%2520those%2520with%2520similar%2520attributes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Enhancement%20of%20Haar%20Cascade%20Algorithm%20Applied%20to%20Face%20Recognition%20for%0A%20%20Gate%20Pass%20Security&entry.906535625=Clarence%20A.%20Antipona%20and%20Romeo%20R.%20Magsino%20and%20Raymund%20M.%20Dioses%20and%20Khatalyn%20E.%20Mata&entry.1292438233=%20%20This%20study%20is%20focused%20on%20enhancing%20the%20Haar%20Cascade%20Algorithm%20to%20decrease%20the%0Afalse%20positive%20and%20false%20negative%20rate%20in%20face%20matching%20and%20face%20detection%20to%0Aincrease%20the%20accuracy%20rate%20even%20under%20challenging%20conditions.%20The%20face%0Arecognition%20library%20was%20implemented%20with%20Haar%20Cascade%20Algorithm%20in%20which%20the%0A128-dimensional%20vectors%20representing%20the%20unique%20features%20of%20a%20face%20are%20encoded.%0AA%20subprocess%20was%20applied%20where%20the%20grayscale%20image%20from%20Haar%20Cascade%20was%0Aconverted%20to%20RGB%20to%20improve%20the%20face%20encoding.%20Logical%20process%20and%20face%0Afiltering%20are%20also%20used%20to%20decrease%20non-face%20detection.%20The%20Enhanced%20Haar%0ACascade%20Algorithm%20produced%20a%2098.39%25%20accuracy%20rate%20%2821.39%25%20increase%29%2C%2063.59%25%0Aprecision%20rate%2C%2098.30%25%20recall%20rate%2C%20and%2072.23%25%20in%20F1%20Score.%20In%20comparison%2C%20the%0AHaar%20Cascade%20Algorithm%20achieved%20a%2046.70%25%20to%2077.00%25%20accuracy%20rate%2C%2044.15%25%0Aprecision%20rate%2C%2098.61%25%20recall%20rate%2C%20and%2047.01%25%20in%20F1%20Score.%20Both%20algorithms%0Aused%20the%20Confusion%20Matrix%20Test%20with%20301%2C950%20comparisons%20using%20the%20same%20dataset%0Aof%20550%20images.%20The%2098.39%25%20accuracy%20rate%20shows%20a%20significant%20decrease%20in%20false%0Apositive%20and%20false%20negative%20rates%20in%20facial%20recognition.%20Face%20matching%20and%20face%0Adetection%20are%20more%20accurate%20in%20images%20with%20complex%20backgrounds%2C%20lighting%0Avariations%2C%20and%20occlusions%2C%20or%20even%20those%20with%20similar%20attributes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03831v1&entry.124074799=Read"},
{"title": "Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For\n  Autonomous Visual Robot Navigation", "author": "Shreya Gummadi and Mateus V. Gasparino and Deepak Vasisht and Girish Chowdhary", "abstract": "  Centralized learning requires data to be aggregated at a central server,\nwhich poses significant challenges in terms of data privacy and bandwidth\nconsumption. Federated learning presents a compelling alternative, however,\nvanilla federated learning methods deployed in robotics aim to learn a single\nglobal model across robots that works ideally for all. But in practice one\nmodel may not be well suited for robots deployed in various environments. This\npaper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated\nlearning framework that is deployed with vision based autonomous robot\nnavigation in diverse outdoor environments. The framework addresses the key\nfederated learning challenge of deteriorating model performance of a single\nglobal model due to the presence of non-IID data across real-world robots.\nExtensive real-world experiments validate that Fed-EC reduces the communication\nsize by 23x for each robot while matching the performance of centralized\nlearning for goal-oriented navigation and outperforms local learning. Fed-EC\ncan transfer previously learnt models to new robots that join the cluster.\n", "link": "http://arxiv.org/abs/2411.04112v1", "date": "2024-11-06", "relevancy": 2.1524, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.542}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5392}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fed-EC%3A%20Bandwidth-Efficient%20Clustering-Based%20Federated%20Learning%20For%0A%20%20Autonomous%20Visual%20Robot%20Navigation&body=Title%3A%20Fed-EC%3A%20Bandwidth-Efficient%20Clustering-Based%20Federated%20Learning%20For%0A%20%20Autonomous%20Visual%20Robot%20Navigation%0AAuthor%3A%20Shreya%20Gummadi%20and%20Mateus%20V.%20Gasparino%20and%20Deepak%20Vasisht%20and%20Girish%20Chowdhary%0AAbstract%3A%20%20%20Centralized%20learning%20requires%20data%20to%20be%20aggregated%20at%20a%20central%20server%2C%0Awhich%20poses%20significant%20challenges%20in%20terms%20of%20data%20privacy%20and%20bandwidth%0Aconsumption.%20Federated%20learning%20presents%20a%20compelling%20alternative%2C%20however%2C%0Avanilla%20federated%20learning%20methods%20deployed%20in%20robotics%20aim%20to%20learn%20a%20single%0Aglobal%20model%20across%20robots%20that%20works%20ideally%20for%20all.%20But%20in%20practice%20one%0Amodel%20may%20not%20be%20well%20suited%20for%20robots%20deployed%20in%20various%20environments.%20This%0Apaper%20proposes%20Federated-EmbedCluster%20%28Fed-EC%29%2C%20a%20clustering-based%20federated%0Alearning%20framework%20that%20is%20deployed%20with%20vision%20based%20autonomous%20robot%0Anavigation%20in%20diverse%20outdoor%20environments.%20The%20framework%20addresses%20the%20key%0Afederated%20learning%20challenge%20of%20deteriorating%20model%20performance%20of%20a%20single%0Aglobal%20model%20due%20to%20the%20presence%20of%20non-IID%20data%20across%20real-world%20robots.%0AExtensive%20real-world%20experiments%20validate%20that%20Fed-EC%20reduces%20the%20communication%0Asize%20by%2023x%20for%20each%20robot%20while%20matching%20the%20performance%20of%20centralized%0Alearning%20for%20goal-oriented%20navigation%20and%20outperforms%20local%20learning.%20Fed-EC%0Acan%20transfer%20previously%20learnt%20models%20to%20new%20robots%20that%20join%20the%20cluster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFed-EC%253A%2520Bandwidth-Efficient%2520Clustering-Based%2520Federated%2520Learning%2520For%250A%2520%2520Autonomous%2520Visual%2520Robot%2520Navigation%26entry.906535625%3DShreya%2520Gummadi%2520and%2520Mateus%2520V.%2520Gasparino%2520and%2520Deepak%2520Vasisht%2520and%2520Girish%2520Chowdhary%26entry.1292438233%3D%2520%2520Centralized%2520learning%2520requires%2520data%2520to%2520be%2520aggregated%2520at%2520a%2520central%2520server%252C%250Awhich%2520poses%2520significant%2520challenges%2520in%2520terms%2520of%2520data%2520privacy%2520and%2520bandwidth%250Aconsumption.%2520Federated%2520learning%2520presents%2520a%2520compelling%2520alternative%252C%2520however%252C%250Avanilla%2520federated%2520learning%2520methods%2520deployed%2520in%2520robotics%2520aim%2520to%2520learn%2520a%2520single%250Aglobal%2520model%2520across%2520robots%2520that%2520works%2520ideally%2520for%2520all.%2520But%2520in%2520practice%2520one%250Amodel%2520may%2520not%2520be%2520well%2520suited%2520for%2520robots%2520deployed%2520in%2520various%2520environments.%2520This%250Apaper%2520proposes%2520Federated-EmbedCluster%2520%2528Fed-EC%2529%252C%2520a%2520clustering-based%2520federated%250Alearning%2520framework%2520that%2520is%2520deployed%2520with%2520vision%2520based%2520autonomous%2520robot%250Anavigation%2520in%2520diverse%2520outdoor%2520environments.%2520The%2520framework%2520addresses%2520the%2520key%250Afederated%2520learning%2520challenge%2520of%2520deteriorating%2520model%2520performance%2520of%2520a%2520single%250Aglobal%2520model%2520due%2520to%2520the%2520presence%2520of%2520non-IID%2520data%2520across%2520real-world%2520robots.%250AExtensive%2520real-world%2520experiments%2520validate%2520that%2520Fed-EC%2520reduces%2520the%2520communication%250Asize%2520by%252023x%2520for%2520each%2520robot%2520while%2520matching%2520the%2520performance%2520of%2520centralized%250Alearning%2520for%2520goal-oriented%2520navigation%2520and%2520outperforms%2520local%2520learning.%2520Fed-EC%250Acan%2520transfer%2520previously%2520learnt%2520models%2520to%2520new%2520robots%2520that%2520join%2520the%2520cluster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fed-EC%3A%20Bandwidth-Efficient%20Clustering-Based%20Federated%20Learning%20For%0A%20%20Autonomous%20Visual%20Robot%20Navigation&entry.906535625=Shreya%20Gummadi%20and%20Mateus%20V.%20Gasparino%20and%20Deepak%20Vasisht%20and%20Girish%20Chowdhary&entry.1292438233=%20%20Centralized%20learning%20requires%20data%20to%20be%20aggregated%20at%20a%20central%20server%2C%0Awhich%20poses%20significant%20challenges%20in%20terms%20of%20data%20privacy%20and%20bandwidth%0Aconsumption.%20Federated%20learning%20presents%20a%20compelling%20alternative%2C%20however%2C%0Avanilla%20federated%20learning%20methods%20deployed%20in%20robotics%20aim%20to%20learn%20a%20single%0Aglobal%20model%20across%20robots%20that%20works%20ideally%20for%20all.%20But%20in%20practice%20one%0Amodel%20may%20not%20be%20well%20suited%20for%20robots%20deployed%20in%20various%20environments.%20This%0Apaper%20proposes%20Federated-EmbedCluster%20%28Fed-EC%29%2C%20a%20clustering-based%20federated%0Alearning%20framework%20that%20is%20deployed%20with%20vision%20based%20autonomous%20robot%0Anavigation%20in%20diverse%20outdoor%20environments.%20The%20framework%20addresses%20the%20key%0Afederated%20learning%20challenge%20of%20deteriorating%20model%20performance%20of%20a%20single%0Aglobal%20model%20due%20to%20the%20presence%20of%20non-IID%20data%20across%20real-world%20robots.%0AExtensive%20real-world%20experiments%20validate%20that%20Fed-EC%20reduces%20the%20communication%0Asize%20by%2023x%20for%20each%20robot%20while%20matching%20the%20performance%20of%20centralized%0Alearning%20for%20goal-oriented%20navigation%20and%20outperforms%20local%20learning.%20Fed-EC%0Acan%20transfer%20previously%20learnt%20models%20to%20new%20robots%20that%20join%20the%20cluster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04112v1&entry.124074799=Read"},
{"title": "BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack", "author": "Yuri Kuratov and Aydar Bulatov and Petr Anokhin and Ivan Rodkin and Dmitry Sorokin and Artyom Sorokin and Mikhail Burtsev", "abstract": "  In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers after\nfine-tuning, enabling the processing of lengths up to 50 million tokens. The\nBABILong benchmark is extendable to any length to support the evaluation of new\nupcoming models with increased capabilities, and we provide splits up to 10\nmillion token lengths.\n", "link": "http://arxiv.org/abs/2406.10149v2", "date": "2024-11-06", "relevancy": 2.1522, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BABILong%3A%20Testing%20the%20Limits%20of%20LLMs%20with%20Long%20Context%0A%20%20Reasoning-in-a-Haystack&body=Title%3A%20BABILong%3A%20Testing%20the%20Limits%20of%20LLMs%20with%20Long%20Context%0A%20%20Reasoning-in-a-Haystack%0AAuthor%3A%20Yuri%20Kuratov%20and%20Aydar%20Bulatov%20and%20Petr%20Anokhin%20and%20Ivan%20Rodkin%20and%20Dmitry%20Sorokin%20and%20Artyom%20Sorokin%20and%20Mikhail%20Burtsev%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20input%20context%20sizes%20of%20large%20language%20models%20%28LLMs%29%20have%0Aincreased%20dramatically.%20However%2C%20existing%20evaluation%20methods%20have%20not%20kept%0Apace%2C%20failing%20to%20comprehensively%20assess%20the%20efficiency%20of%20models%20in%20handling%0Along%20contexts.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20BABILong%20benchmark%2C%0Adesigned%20to%20test%20language%20models%27%20ability%20to%20reason%20across%20facts%20distributed%20in%0Aextremely%20long%20documents.%20BABILong%20includes%20a%20diverse%20set%20of%2020%20reasoning%0Atasks%2C%20including%20fact%20chaining%2C%20simple%20induction%2C%20deduction%2C%20counting%2C%20and%0Ahandling%20lists/sets.%20These%20tasks%20are%20challenging%20on%20their%20own%2C%20and%20even%20more%0Ademanding%20when%20the%20required%20facts%20are%20scattered%20across%20long%20natural%20text.%20Our%0Aevaluations%20show%20that%20popular%20LLMs%20effectively%20utilize%20only%2010-20%5C%25%20of%20the%0Acontext%20and%20their%20performance%20declines%20sharply%20with%20increased%20reasoning%0Acomplexity.%20Among%20alternatives%20to%20in-context%20reasoning%2C%20Retrieval-Augmented%0AGeneration%20methods%20achieve%20a%20modest%2060%5C%25%20accuracy%20on%20single-fact%20question%0Aanswering%2C%20independent%20of%20context%20length.%20Among%20context%20extension%20methods%2C%20the%0Ahighest%20performance%20is%20demonstrated%20by%20recurrent%20memory%20transformers%20after%0Afine-tuning%2C%20enabling%20the%20processing%20of%20lengths%20up%20to%2050%20million%20tokens.%20The%0ABABILong%20benchmark%20is%20extendable%20to%20any%20length%20to%20support%20the%20evaluation%20of%20new%0Aupcoming%20models%20with%20increased%20capabilities%2C%20and%20we%20provide%20splits%20up%20to%2010%0Amillion%20token%20lengths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10149v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBABILong%253A%2520Testing%2520the%2520Limits%2520of%2520LLMs%2520with%2520Long%2520Context%250A%2520%2520Reasoning-in-a-Haystack%26entry.906535625%3DYuri%2520Kuratov%2520and%2520Aydar%2520Bulatov%2520and%2520Petr%2520Anokhin%2520and%2520Ivan%2520Rodkin%2520and%2520Dmitry%2520Sorokin%2520and%2520Artyom%2520Sorokin%2520and%2520Mikhail%2520Burtsev%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520input%2520context%2520sizes%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Aincreased%2520dramatically.%2520However%252C%2520existing%2520evaluation%2520methods%2520have%2520not%2520kept%250Apace%252C%2520failing%2520to%2520comprehensively%2520assess%2520the%2520efficiency%2520of%2520models%2520in%2520handling%250Along%2520contexts.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520the%2520BABILong%2520benchmark%252C%250Adesigned%2520to%2520test%2520language%2520models%2527%2520ability%2520to%2520reason%2520across%2520facts%2520distributed%2520in%250Aextremely%2520long%2520documents.%2520BABILong%2520includes%2520a%2520diverse%2520set%2520of%252020%2520reasoning%250Atasks%252C%2520including%2520fact%2520chaining%252C%2520simple%2520induction%252C%2520deduction%252C%2520counting%252C%2520and%250Ahandling%2520lists/sets.%2520These%2520tasks%2520are%2520challenging%2520on%2520their%2520own%252C%2520and%2520even%2520more%250Ademanding%2520when%2520the%2520required%2520facts%2520are%2520scattered%2520across%2520long%2520natural%2520text.%2520Our%250Aevaluations%2520show%2520that%2520popular%2520LLMs%2520effectively%2520utilize%2520only%252010-20%255C%2525%2520of%2520the%250Acontext%2520and%2520their%2520performance%2520declines%2520sharply%2520with%2520increased%2520reasoning%250Acomplexity.%2520Among%2520alternatives%2520to%2520in-context%2520reasoning%252C%2520Retrieval-Augmented%250AGeneration%2520methods%2520achieve%2520a%2520modest%252060%255C%2525%2520accuracy%2520on%2520single-fact%2520question%250Aanswering%252C%2520independent%2520of%2520context%2520length.%2520Among%2520context%2520extension%2520methods%252C%2520the%250Ahighest%2520performance%2520is%2520demonstrated%2520by%2520recurrent%2520memory%2520transformers%2520after%250Afine-tuning%252C%2520enabling%2520the%2520processing%2520of%2520lengths%2520up%2520to%252050%2520million%2520tokens.%2520The%250ABABILong%2520benchmark%2520is%2520extendable%2520to%2520any%2520length%2520to%2520support%2520the%2520evaluation%2520of%2520new%250Aupcoming%2520models%2520with%2520increased%2520capabilities%252C%2520and%2520we%2520provide%2520splits%2520up%2520to%252010%250Amillion%2520token%2520lengths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10149v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BABILong%3A%20Testing%20the%20Limits%20of%20LLMs%20with%20Long%20Context%0A%20%20Reasoning-in-a-Haystack&entry.906535625=Yuri%20Kuratov%20and%20Aydar%20Bulatov%20and%20Petr%20Anokhin%20and%20Ivan%20Rodkin%20and%20Dmitry%20Sorokin%20and%20Artyom%20Sorokin%20and%20Mikhail%20Burtsev&entry.1292438233=%20%20In%20recent%20years%2C%20the%20input%20context%20sizes%20of%20large%20language%20models%20%28LLMs%29%20have%0Aincreased%20dramatically.%20However%2C%20existing%20evaluation%20methods%20have%20not%20kept%0Apace%2C%20failing%20to%20comprehensively%20assess%20the%20efficiency%20of%20models%20in%20handling%0Along%20contexts.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20BABILong%20benchmark%2C%0Adesigned%20to%20test%20language%20models%27%20ability%20to%20reason%20across%20facts%20distributed%20in%0Aextremely%20long%20documents.%20BABILong%20includes%20a%20diverse%20set%20of%2020%20reasoning%0Atasks%2C%20including%20fact%20chaining%2C%20simple%20induction%2C%20deduction%2C%20counting%2C%20and%0Ahandling%20lists/sets.%20These%20tasks%20are%20challenging%20on%20their%20own%2C%20and%20even%20more%0Ademanding%20when%20the%20required%20facts%20are%20scattered%20across%20long%20natural%20text.%20Our%0Aevaluations%20show%20that%20popular%20LLMs%20effectively%20utilize%20only%2010-20%5C%25%20of%20the%0Acontext%20and%20their%20performance%20declines%20sharply%20with%20increased%20reasoning%0Acomplexity.%20Among%20alternatives%20to%20in-context%20reasoning%2C%20Retrieval-Augmented%0AGeneration%20methods%20achieve%20a%20modest%2060%5C%25%20accuracy%20on%20single-fact%20question%0Aanswering%2C%20independent%20of%20context%20length.%20Among%20context%20extension%20methods%2C%20the%0Ahighest%20performance%20is%20demonstrated%20by%20recurrent%20memory%20transformers%20after%0Afine-tuning%2C%20enabling%20the%20processing%20of%20lengths%20up%20to%2050%20million%20tokens.%20The%0ABABILong%20benchmark%20is%20extendable%20to%20any%20length%20to%20support%20the%20evaluation%20of%20new%0Aupcoming%20models%20with%20increased%20capabilities%2C%20and%20we%20provide%20splits%20up%20to%2010%0Amillion%20token%20lengths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10149v2&entry.124074799=Read"},
{"title": "Large Generative Model-assisted Talking-face Semantic Communication\n  System", "author": "Feibo Jiang and Siwei Tu and Li Dong and Cunhua Pan and Jiangzhou Wang and Xiaohu You", "abstract": "  The rapid development of generative Artificial Intelligence (AI) continually\nunveils the potential of Semantic Communication (SemCom). However, current\ntalking-face SemCom systems still encounter challenges such as low bandwidth\nutilization, semantic ambiguity, and diminished Quality of Experience (QoE).\nThis study introduces a Large Generative Model-assisted Talking-face Semantic\nCommunication (LGM-TSC) System tailored for the talking-face video\ncommunication. Firstly, we introduce a Generative Semantic Extractor (GSE) at\nthe transmitter based on the FunASR model to convert semantically sparse\ntalking-face videos into texts with high information density. Secondly, we\nestablish a private Knowledge Base (KB) based on the Large Language Model (LLM)\nfor semantic disambiguation and correction, complemented by a joint knowledge\nbase-semantic-channel coding scheme. Finally, at the receiver, we propose a\nGenerative Semantic Reconstructor (GSR) that utilizes BERT-VITS2 and SadTalker\nmodels to transform text back into a high-QoE talking-face video matching the\nuser's timbre. Simulation results demonstrate the feasibility and effectiveness\nof the proposed LGM-TSC system.\n", "link": "http://arxiv.org/abs/2411.03876v1", "date": "2024-11-06", "relevancy": 2.1466, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5549}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5296}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Generative%20Model-assisted%20Talking-face%20Semantic%20Communication%0A%20%20System&body=Title%3A%20Large%20Generative%20Model-assisted%20Talking-face%20Semantic%20Communication%0A%20%20System%0AAuthor%3A%20Feibo%20Jiang%20and%20Siwei%20Tu%20and%20Li%20Dong%20and%20Cunhua%20Pan%20and%20Jiangzhou%20Wang%20and%20Xiaohu%20You%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20generative%20Artificial%20Intelligence%20%28AI%29%20continually%0Aunveils%20the%20potential%20of%20Semantic%20Communication%20%28SemCom%29.%20However%2C%20current%0Atalking-face%20SemCom%20systems%20still%20encounter%20challenges%20such%20as%20low%20bandwidth%0Autilization%2C%20semantic%20ambiguity%2C%20and%20diminished%20Quality%20of%20Experience%20%28QoE%29.%0AThis%20study%20introduces%20a%20Large%20Generative%20Model-assisted%20Talking-face%20Semantic%0ACommunication%20%28LGM-TSC%29%20System%20tailored%20for%20the%20talking-face%20video%0Acommunication.%20Firstly%2C%20we%20introduce%20a%20Generative%20Semantic%20Extractor%20%28GSE%29%20at%0Athe%20transmitter%20based%20on%20the%20FunASR%20model%20to%20convert%20semantically%20sparse%0Atalking-face%20videos%20into%20texts%20with%20high%20information%20density.%20Secondly%2C%20we%0Aestablish%20a%20private%20Knowledge%20Base%20%28KB%29%20based%20on%20the%20Large%20Language%20Model%20%28LLM%29%0Afor%20semantic%20disambiguation%20and%20correction%2C%20complemented%20by%20a%20joint%20knowledge%0Abase-semantic-channel%20coding%20scheme.%20Finally%2C%20at%20the%20receiver%2C%20we%20propose%20a%0AGenerative%20Semantic%20Reconstructor%20%28GSR%29%20that%20utilizes%20BERT-VITS2%20and%20SadTalker%0Amodels%20to%20transform%20text%20back%20into%20a%20high-QoE%20talking-face%20video%20matching%20the%0Auser%27s%20timbre.%20Simulation%20results%20demonstrate%20the%20feasibility%20and%20effectiveness%0Aof%20the%20proposed%20LGM-TSC%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Generative%2520Model-assisted%2520Talking-face%2520Semantic%2520Communication%250A%2520%2520System%26entry.906535625%3DFeibo%2520Jiang%2520and%2520Siwei%2520Tu%2520and%2520Li%2520Dong%2520and%2520Cunhua%2520Pan%2520and%2520Jiangzhou%2520Wang%2520and%2520Xiaohu%2520You%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520generative%2520Artificial%2520Intelligence%2520%2528AI%2529%2520continually%250Aunveils%2520the%2520potential%2520of%2520Semantic%2520Communication%2520%2528SemCom%2529.%2520However%252C%2520current%250Atalking-face%2520SemCom%2520systems%2520still%2520encounter%2520challenges%2520such%2520as%2520low%2520bandwidth%250Autilization%252C%2520semantic%2520ambiguity%252C%2520and%2520diminished%2520Quality%2520of%2520Experience%2520%2528QoE%2529.%250AThis%2520study%2520introduces%2520a%2520Large%2520Generative%2520Model-assisted%2520Talking-face%2520Semantic%250ACommunication%2520%2528LGM-TSC%2529%2520System%2520tailored%2520for%2520the%2520talking-face%2520video%250Acommunication.%2520Firstly%252C%2520we%2520introduce%2520a%2520Generative%2520Semantic%2520Extractor%2520%2528GSE%2529%2520at%250Athe%2520transmitter%2520based%2520on%2520the%2520FunASR%2520model%2520to%2520convert%2520semantically%2520sparse%250Atalking-face%2520videos%2520into%2520texts%2520with%2520high%2520information%2520density.%2520Secondly%252C%2520we%250Aestablish%2520a%2520private%2520Knowledge%2520Base%2520%2528KB%2529%2520based%2520on%2520the%2520Large%2520Language%2520Model%2520%2528LLM%2529%250Afor%2520semantic%2520disambiguation%2520and%2520correction%252C%2520complemented%2520by%2520a%2520joint%2520knowledge%250Abase-semantic-channel%2520coding%2520scheme.%2520Finally%252C%2520at%2520the%2520receiver%252C%2520we%2520propose%2520a%250AGenerative%2520Semantic%2520Reconstructor%2520%2528GSR%2529%2520that%2520utilizes%2520BERT-VITS2%2520and%2520SadTalker%250Amodels%2520to%2520transform%2520text%2520back%2520into%2520a%2520high-QoE%2520talking-face%2520video%2520matching%2520the%250Auser%2527s%2520timbre.%2520Simulation%2520results%2520demonstrate%2520the%2520feasibility%2520and%2520effectiveness%250Aof%2520the%2520proposed%2520LGM-TSC%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Generative%20Model-assisted%20Talking-face%20Semantic%20Communication%0A%20%20System&entry.906535625=Feibo%20Jiang%20and%20Siwei%20Tu%20and%20Li%20Dong%20and%20Cunhua%20Pan%20and%20Jiangzhou%20Wang%20and%20Xiaohu%20You&entry.1292438233=%20%20The%20rapid%20development%20of%20generative%20Artificial%20Intelligence%20%28AI%29%20continually%0Aunveils%20the%20potential%20of%20Semantic%20Communication%20%28SemCom%29.%20However%2C%20current%0Atalking-face%20SemCom%20systems%20still%20encounter%20challenges%20such%20as%20low%20bandwidth%0Autilization%2C%20semantic%20ambiguity%2C%20and%20diminished%20Quality%20of%20Experience%20%28QoE%29.%0AThis%20study%20introduces%20a%20Large%20Generative%20Model-assisted%20Talking-face%20Semantic%0ACommunication%20%28LGM-TSC%29%20System%20tailored%20for%20the%20talking-face%20video%0Acommunication.%20Firstly%2C%20we%20introduce%20a%20Generative%20Semantic%20Extractor%20%28GSE%29%20at%0Athe%20transmitter%20based%20on%20the%20FunASR%20model%20to%20convert%20semantically%20sparse%0Atalking-face%20videos%20into%20texts%20with%20high%20information%20density.%20Secondly%2C%20we%0Aestablish%20a%20private%20Knowledge%20Base%20%28KB%29%20based%20on%20the%20Large%20Language%20Model%20%28LLM%29%0Afor%20semantic%20disambiguation%20and%20correction%2C%20complemented%20by%20a%20joint%20knowledge%0Abase-semantic-channel%20coding%20scheme.%20Finally%2C%20at%20the%20receiver%2C%20we%20propose%20a%0AGenerative%20Semantic%20Reconstructor%20%28GSR%29%20that%20utilizes%20BERT-VITS2%20and%20SadTalker%0Amodels%20to%20transform%20text%20back%20into%20a%20high-QoE%20talking-face%20video%20matching%20the%0Auser%27s%20timbre.%20Simulation%20results%20demonstrate%20the%20feasibility%20and%20effectiveness%0Aof%20the%20proposed%20LGM-TSC%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03876v1&entry.124074799=Read"},
{"title": "HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion\n  Segmentation", "author": "Ziyuan Ding and Yixiong Liang and Shichao Kan and Qing Liu", "abstract": "  High resolution is crucial for precise segmentation in fundus images, yet\nhandling high-resolution inputs incurs considerable GPU memory costs, with\ndiminishing performance gains as overhead increases. To address this issue\nwhile tackling the challenge of segmenting tiny objects, recent studies have\nexplored local-global fusion methods. These methods preserve fine details using\nlocal regions and capture long-range context information from downscaled global\nimages. However, the necessity of multiple forward passes inevitably incurs\nsignificant computational overhead, adversely affecting inference speed. In\nthis paper, we propose HRDecoder, a simple High-Resolution Decoder network for\nfundus lesion segmentation. It integrates a high-resolution representation\nlearning module to capture fine-grained local features and a high-resolution\nfusion module to fuse multi-scale predictions. Our method effectively improves\nthe overall segmentation accuracy of fundus lesions while consuming reasonable\nmemory and computational overhead, and maintaining satisfying inference speed.\nExperimental results on the IDRID and DDR datasets demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/CVIU-CSU/HRDecoder.\n", "link": "http://arxiv.org/abs/2411.03976v1", "date": "2024-11-06", "relevancy": 2.1367, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5405}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HRDecoder%3A%20High-Resolution%20Decoder%20Network%20for%20Fundus%20Image%20Lesion%0A%20%20Segmentation&body=Title%3A%20HRDecoder%3A%20High-Resolution%20Decoder%20Network%20for%20Fundus%20Image%20Lesion%0A%20%20Segmentation%0AAuthor%3A%20Ziyuan%20Ding%20and%20Yixiong%20Liang%20and%20Shichao%20Kan%20and%20Qing%20Liu%0AAbstract%3A%20%20%20High%20resolution%20is%20crucial%20for%20precise%20segmentation%20in%20fundus%20images%2C%20yet%0Ahandling%20high-resolution%20inputs%20incurs%20considerable%20GPU%20memory%20costs%2C%20with%0Adiminishing%20performance%20gains%20as%20overhead%20increases.%20To%20address%20this%20issue%0Awhile%20tackling%20the%20challenge%20of%20segmenting%20tiny%20objects%2C%20recent%20studies%20have%0Aexplored%20local-global%20fusion%20methods.%20These%20methods%20preserve%20fine%20details%20using%0Alocal%20regions%20and%20capture%20long-range%20context%20information%20from%20downscaled%20global%0Aimages.%20However%2C%20the%20necessity%20of%20multiple%20forward%20passes%20inevitably%20incurs%0Asignificant%20computational%20overhead%2C%20adversely%20affecting%20inference%20speed.%20In%0Athis%20paper%2C%20we%20propose%20HRDecoder%2C%20a%20simple%20High-Resolution%20Decoder%20network%20for%0Afundus%20lesion%20segmentation.%20It%20integrates%20a%20high-resolution%20representation%0Alearning%20module%20to%20capture%20fine-grained%20local%20features%20and%20a%20high-resolution%0Afusion%20module%20to%20fuse%20multi-scale%20predictions.%20Our%20method%20effectively%20improves%0Athe%20overall%20segmentation%20accuracy%20of%20fundus%20lesions%20while%20consuming%20reasonable%0Amemory%20and%20computational%20overhead%2C%20and%20maintaining%20satisfying%20inference%20speed.%0AExperimental%20results%20on%20the%20IDRID%20and%20DDR%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%20Code%20is%20available%20at%0Ahttps%3A//github.com/CVIU-CSU/HRDecoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHRDecoder%253A%2520High-Resolution%2520Decoder%2520Network%2520for%2520Fundus%2520Image%2520Lesion%250A%2520%2520Segmentation%26entry.906535625%3DZiyuan%2520Ding%2520and%2520Yixiong%2520Liang%2520and%2520Shichao%2520Kan%2520and%2520Qing%2520Liu%26entry.1292438233%3D%2520%2520High%2520resolution%2520is%2520crucial%2520for%2520precise%2520segmentation%2520in%2520fundus%2520images%252C%2520yet%250Ahandling%2520high-resolution%2520inputs%2520incurs%2520considerable%2520GPU%2520memory%2520costs%252C%2520with%250Adiminishing%2520performance%2520gains%2520as%2520overhead%2520increases.%2520To%2520address%2520this%2520issue%250Awhile%2520tackling%2520the%2520challenge%2520of%2520segmenting%2520tiny%2520objects%252C%2520recent%2520studies%2520have%250Aexplored%2520local-global%2520fusion%2520methods.%2520These%2520methods%2520preserve%2520fine%2520details%2520using%250Alocal%2520regions%2520and%2520capture%2520long-range%2520context%2520information%2520from%2520downscaled%2520global%250Aimages.%2520However%252C%2520the%2520necessity%2520of%2520multiple%2520forward%2520passes%2520inevitably%2520incurs%250Asignificant%2520computational%2520overhead%252C%2520adversely%2520affecting%2520inference%2520speed.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520HRDecoder%252C%2520a%2520simple%2520High-Resolution%2520Decoder%2520network%2520for%250Afundus%2520lesion%2520segmentation.%2520It%2520integrates%2520a%2520high-resolution%2520representation%250Alearning%2520module%2520to%2520capture%2520fine-grained%2520local%2520features%2520and%2520a%2520high-resolution%250Afusion%2520module%2520to%2520fuse%2520multi-scale%2520predictions.%2520Our%2520method%2520effectively%2520improves%250Athe%2520overall%2520segmentation%2520accuracy%2520of%2520fundus%2520lesions%2520while%2520consuming%2520reasonable%250Amemory%2520and%2520computational%2520overhead%252C%2520and%2520maintaining%2520satisfying%2520inference%2520speed.%250AExperimental%2520results%2520on%2520the%2520IDRID%2520and%2520DDR%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/CVIU-CSU/HRDecoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HRDecoder%3A%20High-Resolution%20Decoder%20Network%20for%20Fundus%20Image%20Lesion%0A%20%20Segmentation&entry.906535625=Ziyuan%20Ding%20and%20Yixiong%20Liang%20and%20Shichao%20Kan%20and%20Qing%20Liu&entry.1292438233=%20%20High%20resolution%20is%20crucial%20for%20precise%20segmentation%20in%20fundus%20images%2C%20yet%0Ahandling%20high-resolution%20inputs%20incurs%20considerable%20GPU%20memory%20costs%2C%20with%0Adiminishing%20performance%20gains%20as%20overhead%20increases.%20To%20address%20this%20issue%0Awhile%20tackling%20the%20challenge%20of%20segmenting%20tiny%20objects%2C%20recent%20studies%20have%0Aexplored%20local-global%20fusion%20methods.%20These%20methods%20preserve%20fine%20details%20using%0Alocal%20regions%20and%20capture%20long-range%20context%20information%20from%20downscaled%20global%0Aimages.%20However%2C%20the%20necessity%20of%20multiple%20forward%20passes%20inevitably%20incurs%0Asignificant%20computational%20overhead%2C%20adversely%20affecting%20inference%20speed.%20In%0Athis%20paper%2C%20we%20propose%20HRDecoder%2C%20a%20simple%20High-Resolution%20Decoder%20network%20for%0Afundus%20lesion%20segmentation.%20It%20integrates%20a%20high-resolution%20representation%0Alearning%20module%20to%20capture%20fine-grained%20local%20features%20and%20a%20high-resolution%0Afusion%20module%20to%20fuse%20multi-scale%20predictions.%20Our%20method%20effectively%20improves%0Athe%20overall%20segmentation%20accuracy%20of%20fundus%20lesions%20while%20consuming%20reasonable%0Amemory%20and%20computational%20overhead%2C%20and%20maintaining%20satisfying%20inference%20speed.%0AExperimental%20results%20on%20the%20IDRID%20and%20DDR%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%20Code%20is%20available%20at%0Ahttps%3A//github.com/CVIU-CSU/HRDecoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03976v1&entry.124074799=Read"},
{"title": "Aligning Characteristic Descriptors with Images for Human-Expert-like\n  Explainability", "author": "Bharat Chandra Yalavarthi and Nalini Ratha", "abstract": "  In mission-critical domains such as law enforcement and medical diagnosis,\nthe ability to explain and interpret the outputs of deep learning models is\ncrucial for ensuring user trust and supporting informed decision-making.\nDespite advancements in explainability, existing methods often fall short in\nproviding explanations that mirror the depth and clarity of those given by\nhuman experts. Such expert-level explanations are essential for the dependable\napplication of deep learning models in law enforcement and medical contexts.\nAdditionally, we recognize that most explanations in real-world scenarios are\ncommunicated primarily through natural language. Addressing these needs, we\npropose a novel approach that utilizes characteristic descriptors to explain\nmodel decisions by identifying their presence in images, thereby generating\nexpert-like explanations. Our method incorporates a concept bottleneck layer\nwithin the model architecture, which calculates the similarity between image\nand descriptor encodings to deliver inherent and faithful explanations. Through\nexperiments in face recognition and chest X-ray diagnosis, we demonstrate that\nour approach offers a significant contrast over existing techniques, which are\noften limited to the use of saliency maps. We believe our approach represents a\nsignificant step toward making deep learning systems more accountable,\ntransparent, and trustworthy in the critical domains of face recognition and\nmedical diagnosis.\n", "link": "http://arxiv.org/abs/2411.04008v1", "date": "2024-11-06", "relevancy": 2.1336, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Characteristic%20Descriptors%20with%20Images%20for%20Human-Expert-like%0A%20%20Explainability&body=Title%3A%20Aligning%20Characteristic%20Descriptors%20with%20Images%20for%20Human-Expert-like%0A%20%20Explainability%0AAuthor%3A%20Bharat%20Chandra%20Yalavarthi%20and%20Nalini%20Ratha%0AAbstract%3A%20%20%20In%20mission-critical%20domains%20such%20as%20law%20enforcement%20and%20medical%20diagnosis%2C%0Athe%20ability%20to%20explain%20and%20interpret%20the%20outputs%20of%20deep%20learning%20models%20is%0Acrucial%20for%20ensuring%20user%20trust%20and%20supporting%20informed%20decision-making.%0ADespite%20advancements%20in%20explainability%2C%20existing%20methods%20often%20fall%20short%20in%0Aproviding%20explanations%20that%20mirror%20the%20depth%20and%20clarity%20of%20those%20given%20by%0Ahuman%20experts.%20Such%20expert-level%20explanations%20are%20essential%20for%20the%20dependable%0Aapplication%20of%20deep%20learning%20models%20in%20law%20enforcement%20and%20medical%20contexts.%0AAdditionally%2C%20we%20recognize%20that%20most%20explanations%20in%20real-world%20scenarios%20are%0Acommunicated%20primarily%20through%20natural%20language.%20Addressing%20these%20needs%2C%20we%0Apropose%20a%20novel%20approach%20that%20utilizes%20characteristic%20descriptors%20to%20explain%0Amodel%20decisions%20by%20identifying%20their%20presence%20in%20images%2C%20thereby%20generating%0Aexpert-like%20explanations.%20Our%20method%20incorporates%20a%20concept%20bottleneck%20layer%0Awithin%20the%20model%20architecture%2C%20which%20calculates%20the%20similarity%20between%20image%0Aand%20descriptor%20encodings%20to%20deliver%20inherent%20and%20faithful%20explanations.%20Through%0Aexperiments%20in%20face%20recognition%20and%20chest%20X-ray%20diagnosis%2C%20we%20demonstrate%20that%0Aour%20approach%20offers%20a%20significant%20contrast%20over%20existing%20techniques%2C%20which%20are%0Aoften%20limited%20to%20the%20use%20of%20saliency%20maps.%20We%20believe%20our%20approach%20represents%20a%0Asignificant%20step%20toward%20making%20deep%20learning%20systems%20more%20accountable%2C%0Atransparent%2C%20and%20trustworthy%20in%20the%20critical%20domains%20of%20face%20recognition%20and%0Amedical%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Characteristic%2520Descriptors%2520with%2520Images%2520for%2520Human-Expert-like%250A%2520%2520Explainability%26entry.906535625%3DBharat%2520Chandra%2520Yalavarthi%2520and%2520Nalini%2520Ratha%26entry.1292438233%3D%2520%2520In%2520mission-critical%2520domains%2520such%2520as%2520law%2520enforcement%2520and%2520medical%2520diagnosis%252C%250Athe%2520ability%2520to%2520explain%2520and%2520interpret%2520the%2520outputs%2520of%2520deep%2520learning%2520models%2520is%250Acrucial%2520for%2520ensuring%2520user%2520trust%2520and%2520supporting%2520informed%2520decision-making.%250ADespite%2520advancements%2520in%2520explainability%252C%2520existing%2520methods%2520often%2520fall%2520short%2520in%250Aproviding%2520explanations%2520that%2520mirror%2520the%2520depth%2520and%2520clarity%2520of%2520those%2520given%2520by%250Ahuman%2520experts.%2520Such%2520expert-level%2520explanations%2520are%2520essential%2520for%2520the%2520dependable%250Aapplication%2520of%2520deep%2520learning%2520models%2520in%2520law%2520enforcement%2520and%2520medical%2520contexts.%250AAdditionally%252C%2520we%2520recognize%2520that%2520most%2520explanations%2520in%2520real-world%2520scenarios%2520are%250Acommunicated%2520primarily%2520through%2520natural%2520language.%2520Addressing%2520these%2520needs%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520that%2520utilizes%2520characteristic%2520descriptors%2520to%2520explain%250Amodel%2520decisions%2520by%2520identifying%2520their%2520presence%2520in%2520images%252C%2520thereby%2520generating%250Aexpert-like%2520explanations.%2520Our%2520method%2520incorporates%2520a%2520concept%2520bottleneck%2520layer%250Awithin%2520the%2520model%2520architecture%252C%2520which%2520calculates%2520the%2520similarity%2520between%2520image%250Aand%2520descriptor%2520encodings%2520to%2520deliver%2520inherent%2520and%2520faithful%2520explanations.%2520Through%250Aexperiments%2520in%2520face%2520recognition%2520and%2520chest%2520X-ray%2520diagnosis%252C%2520we%2520demonstrate%2520that%250Aour%2520approach%2520offers%2520a%2520significant%2520contrast%2520over%2520existing%2520techniques%252C%2520which%2520are%250Aoften%2520limited%2520to%2520the%2520use%2520of%2520saliency%2520maps.%2520We%2520believe%2520our%2520approach%2520represents%2520a%250Asignificant%2520step%2520toward%2520making%2520deep%2520learning%2520systems%2520more%2520accountable%252C%250Atransparent%252C%2520and%2520trustworthy%2520in%2520the%2520critical%2520domains%2520of%2520face%2520recognition%2520and%250Amedical%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Characteristic%20Descriptors%20with%20Images%20for%20Human-Expert-like%0A%20%20Explainability&entry.906535625=Bharat%20Chandra%20Yalavarthi%20and%20Nalini%20Ratha&entry.1292438233=%20%20In%20mission-critical%20domains%20such%20as%20law%20enforcement%20and%20medical%20diagnosis%2C%0Athe%20ability%20to%20explain%20and%20interpret%20the%20outputs%20of%20deep%20learning%20models%20is%0Acrucial%20for%20ensuring%20user%20trust%20and%20supporting%20informed%20decision-making.%0ADespite%20advancements%20in%20explainability%2C%20existing%20methods%20often%20fall%20short%20in%0Aproviding%20explanations%20that%20mirror%20the%20depth%20and%20clarity%20of%20those%20given%20by%0Ahuman%20experts.%20Such%20expert-level%20explanations%20are%20essential%20for%20the%20dependable%0Aapplication%20of%20deep%20learning%20models%20in%20law%20enforcement%20and%20medical%20contexts.%0AAdditionally%2C%20we%20recognize%20that%20most%20explanations%20in%20real-world%20scenarios%20are%0Acommunicated%20primarily%20through%20natural%20language.%20Addressing%20these%20needs%2C%20we%0Apropose%20a%20novel%20approach%20that%20utilizes%20characteristic%20descriptors%20to%20explain%0Amodel%20decisions%20by%20identifying%20their%20presence%20in%20images%2C%20thereby%20generating%0Aexpert-like%20explanations.%20Our%20method%20incorporates%20a%20concept%20bottleneck%20layer%0Awithin%20the%20model%20architecture%2C%20which%20calculates%20the%20similarity%20between%20image%0Aand%20descriptor%20encodings%20to%20deliver%20inherent%20and%20faithful%20explanations.%20Through%0Aexperiments%20in%20face%20recognition%20and%20chest%20X-ray%20diagnosis%2C%20we%20demonstrate%20that%0Aour%20approach%20offers%20a%20significant%20contrast%20over%20existing%20techniques%2C%20which%20are%0Aoften%20limited%20to%20the%20use%20of%20saliency%20maps.%20We%20believe%20our%20approach%20represents%20a%0Asignificant%20step%20toward%20making%20deep%20learning%20systems%20more%20accountable%2C%0Atransparent%2C%20and%20trustworthy%20in%20the%20critical%20domains%20of%20face%20recognition%20and%0Amedical%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04008v1&entry.124074799=Read"},
{"title": "IFAdapter: Instance Feature Control for Grounded Text-to-Image\n  Generation", "author": "Yinwei Wu and Xianpan Zhou and Bing Ma and Xuefeng Su and Kai Ma and Xinchao Wang", "abstract": "  While Text-to-Image (T2I) diffusion models excel at generating visually\nappealing images of individual instances, they struggle to accurately position\nand control the features generation of multiple instances. The Layout-to-Image\n(L2I) task was introduced to address the positioning challenges by\nincorporating bounding boxes as spatial control signals, but it still falls\nshort in generating precise instance features. In response, we propose the\nInstance Feature Generation (IFG) task, which aims to ensure both positional\naccuracy and feature fidelity in generated instances. To address the IFG task,\nwe introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances\nfeature depiction by incorporating additional appearance tokens and utilizing\nan Instance Semantic Map to align instance-level features with spatial\nlocations. The IFAdapter guides the diffusion process as a plug-and-play\nmodule, making it adaptable to various community models. For evaluation, we\ncontribute an IFG benchmark and develop a verification pipeline to objectively\ncompare models' abilities to generate instances with accurate positioning and\nfeatures. Experimental results demonstrate that IFAdapter outperforms other\nmodels in both quantitative and qualitative evaluations.\n", "link": "http://arxiv.org/abs/2409.08240v3", "date": "2024-11-06", "relevancy": 2.1336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5269}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IFAdapter%3A%20Instance%20Feature%20Control%20for%20Grounded%20Text-to-Image%0A%20%20Generation&body=Title%3A%20IFAdapter%3A%20Instance%20Feature%20Control%20for%20Grounded%20Text-to-Image%0A%20%20Generation%0AAuthor%3A%20Yinwei%20Wu%20and%20Xianpan%20Zhou%20and%20Bing%20Ma%20and%20Xuefeng%20Su%20and%20Kai%20Ma%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20While%20Text-to-Image%20%28T2I%29%20diffusion%20models%20excel%20at%20generating%20visually%0Aappealing%20images%20of%20individual%20instances%2C%20they%20struggle%20to%20accurately%20position%0Aand%20control%20the%20features%20generation%20of%20multiple%20instances.%20The%20Layout-to-Image%0A%28L2I%29%20task%20was%20introduced%20to%20address%20the%20positioning%20challenges%20by%0Aincorporating%20bounding%20boxes%20as%20spatial%20control%20signals%2C%20but%20it%20still%20falls%0Ashort%20in%20generating%20precise%20instance%20features.%20In%20response%2C%20we%20propose%20the%0AInstance%20Feature%20Generation%20%28IFG%29%20task%2C%20which%20aims%20to%20ensure%20both%20positional%0Aaccuracy%20and%20feature%20fidelity%20in%20generated%20instances.%20To%20address%20the%20IFG%20task%2C%0Awe%20introduce%20the%20Instance%20Feature%20Adapter%20%28IFAdapter%29.%20The%20IFAdapter%20enhances%0Afeature%20depiction%20by%20incorporating%20additional%20appearance%20tokens%20and%20utilizing%0Aan%20Instance%20Semantic%20Map%20to%20align%20instance-level%20features%20with%20spatial%0Alocations.%20The%20IFAdapter%20guides%20the%20diffusion%20process%20as%20a%20plug-and-play%0Amodule%2C%20making%20it%20adaptable%20to%20various%20community%20models.%20For%20evaluation%2C%20we%0Acontribute%20an%20IFG%20benchmark%20and%20develop%20a%20verification%20pipeline%20to%20objectively%0Acompare%20models%27%20abilities%20to%20generate%20instances%20with%20accurate%20positioning%20and%0Afeatures.%20Experimental%20results%20demonstrate%20that%20IFAdapter%20outperforms%20other%0Amodels%20in%20both%20quantitative%20and%20qualitative%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08240v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIFAdapter%253A%2520Instance%2520Feature%2520Control%2520for%2520Grounded%2520Text-to-Image%250A%2520%2520Generation%26entry.906535625%3DYinwei%2520Wu%2520and%2520Xianpan%2520Zhou%2520and%2520Bing%2520Ma%2520and%2520Xuefeng%2520Su%2520and%2520Kai%2520Ma%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520While%2520Text-to-Image%2520%2528T2I%2529%2520diffusion%2520models%2520excel%2520at%2520generating%2520visually%250Aappealing%2520images%2520of%2520individual%2520instances%252C%2520they%2520struggle%2520to%2520accurately%2520position%250Aand%2520control%2520the%2520features%2520generation%2520of%2520multiple%2520instances.%2520The%2520Layout-to-Image%250A%2528L2I%2529%2520task%2520was%2520introduced%2520to%2520address%2520the%2520positioning%2520challenges%2520by%250Aincorporating%2520bounding%2520boxes%2520as%2520spatial%2520control%2520signals%252C%2520but%2520it%2520still%2520falls%250Ashort%2520in%2520generating%2520precise%2520instance%2520features.%2520In%2520response%252C%2520we%2520propose%2520the%250AInstance%2520Feature%2520Generation%2520%2528IFG%2529%2520task%252C%2520which%2520aims%2520to%2520ensure%2520both%2520positional%250Aaccuracy%2520and%2520feature%2520fidelity%2520in%2520generated%2520instances.%2520To%2520address%2520the%2520IFG%2520task%252C%250Awe%2520introduce%2520the%2520Instance%2520Feature%2520Adapter%2520%2528IFAdapter%2529.%2520The%2520IFAdapter%2520enhances%250Afeature%2520depiction%2520by%2520incorporating%2520additional%2520appearance%2520tokens%2520and%2520utilizing%250Aan%2520Instance%2520Semantic%2520Map%2520to%2520align%2520instance-level%2520features%2520with%2520spatial%250Alocations.%2520The%2520IFAdapter%2520guides%2520the%2520diffusion%2520process%2520as%2520a%2520plug-and-play%250Amodule%252C%2520making%2520it%2520adaptable%2520to%2520various%2520community%2520models.%2520For%2520evaluation%252C%2520we%250Acontribute%2520an%2520IFG%2520benchmark%2520and%2520develop%2520a%2520verification%2520pipeline%2520to%2520objectively%250Acompare%2520models%2527%2520abilities%2520to%2520generate%2520instances%2520with%2520accurate%2520positioning%2520and%250Afeatures.%2520Experimental%2520results%2520demonstrate%2520that%2520IFAdapter%2520outperforms%2520other%250Amodels%2520in%2520both%2520quantitative%2520and%2520qualitative%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08240v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IFAdapter%3A%20Instance%20Feature%20Control%20for%20Grounded%20Text-to-Image%0A%20%20Generation&entry.906535625=Yinwei%20Wu%20and%20Xianpan%20Zhou%20and%20Bing%20Ma%20and%20Xuefeng%20Su%20and%20Kai%20Ma%20and%20Xinchao%20Wang&entry.1292438233=%20%20While%20Text-to-Image%20%28T2I%29%20diffusion%20models%20excel%20at%20generating%20visually%0Aappealing%20images%20of%20individual%20instances%2C%20they%20struggle%20to%20accurately%20position%0Aand%20control%20the%20features%20generation%20of%20multiple%20instances.%20The%20Layout-to-Image%0A%28L2I%29%20task%20was%20introduced%20to%20address%20the%20positioning%20challenges%20by%0Aincorporating%20bounding%20boxes%20as%20spatial%20control%20signals%2C%20but%20it%20still%20falls%0Ashort%20in%20generating%20precise%20instance%20features.%20In%20response%2C%20we%20propose%20the%0AInstance%20Feature%20Generation%20%28IFG%29%20task%2C%20which%20aims%20to%20ensure%20both%20positional%0Aaccuracy%20and%20feature%20fidelity%20in%20generated%20instances.%20To%20address%20the%20IFG%20task%2C%0Awe%20introduce%20the%20Instance%20Feature%20Adapter%20%28IFAdapter%29.%20The%20IFAdapter%20enhances%0Afeature%20depiction%20by%20incorporating%20additional%20appearance%20tokens%20and%20utilizing%0Aan%20Instance%20Semantic%20Map%20to%20align%20instance-level%20features%20with%20spatial%0Alocations.%20The%20IFAdapter%20guides%20the%20diffusion%20process%20as%20a%20plug-and-play%0Amodule%2C%20making%20it%20adaptable%20to%20various%20community%20models.%20For%20evaluation%2C%20we%0Acontribute%20an%20IFG%20benchmark%20and%20develop%20a%20verification%20pipeline%20to%20objectively%0Acompare%20models%27%20abilities%20to%20generate%20instances%20with%20accurate%20positioning%20and%0Afeatures.%20Experimental%20results%20demonstrate%20that%20IFAdapter%20outperforms%20other%0Amodels%20in%20both%20quantitative%20and%20qualitative%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08240v3&entry.124074799=Read"},
{"title": "DeNetDM: Debiasing by Network Depth Modulation", "author": "Silpa Vadakkeeveetil Sreelatha and Adarsh Kappiyath and Abhra Chaudhuri and Anjan Dutta", "abstract": "  Neural networks trained on biased datasets tend to inadvertently learn\nspurious correlations, hindering generalization. We formally prove that (1)\nsamples that exhibit spurious correlations lie on a lower rank manifold\nrelative to the ones that do not; and (2) the depth of a network acts as an\nimplicit regularizer on the rank of the attribute subspace that is encoded in\nits representations. Leveraging these insights, we present DeNetDM, a novel\ndebiasing method that uses network depth modulation as a way of developing\nrobustness to spurious correlations. Using a training paradigm derived from\nProduct of Experts, we create both biased and debiased branches with deep and\nshallow architectures and then distill knowledge to produce the target debiased\nmodel. Our method requires no bias annotations or explicit data augmentation\nwhile performing on par with approaches that require either or both. We\ndemonstrate that DeNetDM outperforms existing debiasing techniques on both\nsynthetic and real-world datasets by 5\\%. The project page is available at\nhttps://vssilpa.github.io/denetdm/.\n", "link": "http://arxiv.org/abs/2403.19863v4", "date": "2024-11-06", "relevancy": 2.1204, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5683}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5101}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeNetDM%3A%20Debiasing%20by%20Network%20Depth%20Modulation&body=Title%3A%20DeNetDM%3A%20Debiasing%20by%20Network%20Depth%20Modulation%0AAuthor%3A%20Silpa%20Vadakkeeveetil%20Sreelatha%20and%20Adarsh%20Kappiyath%20and%20Abhra%20Chaudhuri%20and%20Anjan%20Dutta%0AAbstract%3A%20%20%20Neural%20networks%20trained%20on%20biased%20datasets%20tend%20to%20inadvertently%20learn%0Aspurious%20correlations%2C%20hindering%20generalization.%20We%20formally%20prove%20that%20%281%29%0Asamples%20that%20exhibit%20spurious%20correlations%20lie%20on%20a%20lower%20rank%20manifold%0Arelative%20to%20the%20ones%20that%20do%20not%3B%20and%20%282%29%20the%20depth%20of%20a%20network%20acts%20as%20an%0Aimplicit%20regularizer%20on%20the%20rank%20of%20the%20attribute%20subspace%20that%20is%20encoded%20in%0Aits%20representations.%20Leveraging%20these%20insights%2C%20we%20present%20DeNetDM%2C%20a%20novel%0Adebiasing%20method%20that%20uses%20network%20depth%20modulation%20as%20a%20way%20of%20developing%0Arobustness%20to%20spurious%20correlations.%20Using%20a%20training%20paradigm%20derived%20from%0AProduct%20of%20Experts%2C%20we%20create%20both%20biased%20and%20debiased%20branches%20with%20deep%20and%0Ashallow%20architectures%20and%20then%20distill%20knowledge%20to%20produce%20the%20target%20debiased%0Amodel.%20Our%20method%20requires%20no%20bias%20annotations%20or%20explicit%20data%20augmentation%0Awhile%20performing%20on%20par%20with%20approaches%20that%20require%20either%20or%20both.%20We%0Ademonstrate%20that%20DeNetDM%20outperforms%20existing%20debiasing%20techniques%20on%20both%0Asynthetic%20and%20real-world%20datasets%20by%205%5C%25.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//vssilpa.github.io/denetdm/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19863v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeNetDM%253A%2520Debiasing%2520by%2520Network%2520Depth%2520Modulation%26entry.906535625%3DSilpa%2520Vadakkeeveetil%2520Sreelatha%2520and%2520Adarsh%2520Kappiyath%2520and%2520Abhra%2520Chaudhuri%2520and%2520Anjan%2520Dutta%26entry.1292438233%3D%2520%2520Neural%2520networks%2520trained%2520on%2520biased%2520datasets%2520tend%2520to%2520inadvertently%2520learn%250Aspurious%2520correlations%252C%2520hindering%2520generalization.%2520We%2520formally%2520prove%2520that%2520%25281%2529%250Asamples%2520that%2520exhibit%2520spurious%2520correlations%2520lie%2520on%2520a%2520lower%2520rank%2520manifold%250Arelative%2520to%2520the%2520ones%2520that%2520do%2520not%253B%2520and%2520%25282%2529%2520the%2520depth%2520of%2520a%2520network%2520acts%2520as%2520an%250Aimplicit%2520regularizer%2520on%2520the%2520rank%2520of%2520the%2520attribute%2520subspace%2520that%2520is%2520encoded%2520in%250Aits%2520representations.%2520Leveraging%2520these%2520insights%252C%2520we%2520present%2520DeNetDM%252C%2520a%2520novel%250Adebiasing%2520method%2520that%2520uses%2520network%2520depth%2520modulation%2520as%2520a%2520way%2520of%2520developing%250Arobustness%2520to%2520spurious%2520correlations.%2520Using%2520a%2520training%2520paradigm%2520derived%2520from%250AProduct%2520of%2520Experts%252C%2520we%2520create%2520both%2520biased%2520and%2520debiased%2520branches%2520with%2520deep%2520and%250Ashallow%2520architectures%2520and%2520then%2520distill%2520knowledge%2520to%2520produce%2520the%2520target%2520debiased%250Amodel.%2520Our%2520method%2520requires%2520no%2520bias%2520annotations%2520or%2520explicit%2520data%2520augmentation%250Awhile%2520performing%2520on%2520par%2520with%2520approaches%2520that%2520require%2520either%2520or%2520both.%2520We%250Ademonstrate%2520that%2520DeNetDM%2520outperforms%2520existing%2520debiasing%2520techniques%2520on%2520both%250Asynthetic%2520and%2520real-world%2520datasets%2520by%25205%255C%2525.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//vssilpa.github.io/denetdm/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19863v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeNetDM%3A%20Debiasing%20by%20Network%20Depth%20Modulation&entry.906535625=Silpa%20Vadakkeeveetil%20Sreelatha%20and%20Adarsh%20Kappiyath%20and%20Abhra%20Chaudhuri%20and%20Anjan%20Dutta&entry.1292438233=%20%20Neural%20networks%20trained%20on%20biased%20datasets%20tend%20to%20inadvertently%20learn%0Aspurious%20correlations%2C%20hindering%20generalization.%20We%20formally%20prove%20that%20%281%29%0Asamples%20that%20exhibit%20spurious%20correlations%20lie%20on%20a%20lower%20rank%20manifold%0Arelative%20to%20the%20ones%20that%20do%20not%3B%20and%20%282%29%20the%20depth%20of%20a%20network%20acts%20as%20an%0Aimplicit%20regularizer%20on%20the%20rank%20of%20the%20attribute%20subspace%20that%20is%20encoded%20in%0Aits%20representations.%20Leveraging%20these%20insights%2C%20we%20present%20DeNetDM%2C%20a%20novel%0Adebiasing%20method%20that%20uses%20network%20depth%20modulation%20as%20a%20way%20of%20developing%0Arobustness%20to%20spurious%20correlations.%20Using%20a%20training%20paradigm%20derived%20from%0AProduct%20of%20Experts%2C%20we%20create%20both%20biased%20and%20debiased%20branches%20with%20deep%20and%0Ashallow%20architectures%20and%20then%20distill%20knowledge%20to%20produce%20the%20target%20debiased%0Amodel.%20Our%20method%20requires%20no%20bias%20annotations%20or%20explicit%20data%20augmentation%0Awhile%20performing%20on%20par%20with%20approaches%20that%20require%20either%20or%20both.%20We%0Ademonstrate%20that%20DeNetDM%20outperforms%20existing%20debiasing%20techniques%20on%20both%0Asynthetic%20and%20real-world%20datasets%20by%205%5C%25.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//vssilpa.github.io/denetdm/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19863v4&entry.124074799=Read"},
{"title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis", "author": "Temitope Akinboyewa and Zhenlong Li and Huan Ning and M. Naser Lessani", "abstract": "  Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes.\n", "link": "http://arxiv.org/abs/2411.03205v2", "date": "2024-11-06", "relevancy": 2.1201, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5466}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIS%20Copilot%3A%20Towards%20an%20Autonomous%20GIS%20Agent%20for%20Spatial%20Analysis&body=Title%3A%20GIS%20Copilot%3A%20Towards%20an%20Autonomous%20GIS%20Agent%20for%20Spatial%20Analysis%0AAuthor%3A%20Temitope%20Akinboyewa%20and%20Zhenlong%20Li%20and%20Huan%20Ning%20and%20M.%20Naser%20Lessani%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Generative%20AI%20offer%20promising%20capabilities%20for%20spatial%0Aanalysis.%20Despite%20their%20potential%2C%20the%20integration%20of%20generative%20AI%20with%0Aestablished%20GIS%20platforms%20remains%20underexplored.%20In%20this%20study%2C%20we%20propose%20a%0Aframework%20for%20integrating%20LLMs%20directly%20into%20existing%20GIS%20platforms%2C%20using%20QGIS%0Aas%20an%20example.%20Our%20approach%20leverages%20the%20reasoning%20and%20programming%0Acapabilities%20of%20LLMs%20to%20autonomously%20generate%20spatial%20analysis%20workflows%20and%0Acode%20through%20an%20informed%20agent%20that%20has%20comprehensive%20documentation%20of%20key%20GIS%0Atools%20and%20parameters.%20The%20implementation%20of%20this%20framework%20resulted%20in%20the%0Adevelopment%20of%20a%20%22GIS%20Copilot%22%20that%20allows%20GIS%20users%20to%20interact%20with%20QGIS%0Ausing%20natural%20language%20commands%20for%20spatial%20analysis.%20The%20GIS%20Copilot%20was%0Aevaluated%20based%20on%20three%20complexity%20levels%3A%20basic%20tasks%20that%20require%20one%20GIS%0Atool%20and%20typically%20involve%20one%20data%20layer%20to%20perform%20simple%20operations%3B%0Aintermediate%20tasks%20involving%20multi-step%20processes%20with%20multiple%20tools%2C%20guided%0Aby%20user%20instructions%3B%20and%20advanced%20tasks%20which%20involve%20multi-step%20processes%0Athat%20require%20multiple%20tools%20but%20not%20guided%20by%20user%20instructions%2C%20necessitating%0Athe%20agent%20to%20independently%20decide%20on%20and%20executes%20the%20necessary%20steps.%20The%0Aevaluation%20reveals%20that%20the%20GIS%20Copilot%20demonstrates%20strong%20potential%20in%0Aautomating%20foundational%20GIS%20operations%2C%20with%20a%20high%20success%20rate%20in%20tool%0Aselection%20and%20code%20generation%20for%20basic%20and%20intermediate%20tasks%2C%20while%0Achallenges%20remain%20in%20achieving%20full%20autonomy%20for%20more%20complex%20tasks.%20This%20study%0Acontributes%20to%20the%20emerging%20vision%20of%20Autonomous%20GIS%2C%20providing%20a%20pathway%20for%0Anon-experts%20to%20engage%20with%20geospatial%20analysis%20with%20minimal%20prior%20expertise.%0AWhile%20full%20autonomy%20is%20yet%20to%20be%20achieved%2C%20the%20GIS%20Copilot%20demonstrates%0Asignificant%20potential%20for%20simplifying%20GIS%20workflows%20and%20enhancing%0Adecision-making%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIS%2520Copilot%253A%2520Towards%2520an%2520Autonomous%2520GIS%2520Agent%2520for%2520Spatial%2520Analysis%26entry.906535625%3DTemitope%2520Akinboyewa%2520and%2520Zhenlong%2520Li%2520and%2520Huan%2520Ning%2520and%2520M.%2520Naser%2520Lessani%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Generative%2520AI%2520offer%2520promising%2520capabilities%2520for%2520spatial%250Aanalysis.%2520Despite%2520their%2520potential%252C%2520the%2520integration%2520of%2520generative%2520AI%2520with%250Aestablished%2520GIS%2520platforms%2520remains%2520underexplored.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250Aframework%2520for%2520integrating%2520LLMs%2520directly%2520into%2520existing%2520GIS%2520platforms%252C%2520using%2520QGIS%250Aas%2520an%2520example.%2520Our%2520approach%2520leverages%2520the%2520reasoning%2520and%2520programming%250Acapabilities%2520of%2520LLMs%2520to%2520autonomously%2520generate%2520spatial%2520analysis%2520workflows%2520and%250Acode%2520through%2520an%2520informed%2520agent%2520that%2520has%2520comprehensive%2520documentation%2520of%2520key%2520GIS%250Atools%2520and%2520parameters.%2520The%2520implementation%2520of%2520this%2520framework%2520resulted%2520in%2520the%250Adevelopment%2520of%2520a%2520%2522GIS%2520Copilot%2522%2520that%2520allows%2520GIS%2520users%2520to%2520interact%2520with%2520QGIS%250Ausing%2520natural%2520language%2520commands%2520for%2520spatial%2520analysis.%2520The%2520GIS%2520Copilot%2520was%250Aevaluated%2520based%2520on%2520three%2520complexity%2520levels%253A%2520basic%2520tasks%2520that%2520require%2520one%2520GIS%250Atool%2520and%2520typically%2520involve%2520one%2520data%2520layer%2520to%2520perform%2520simple%2520operations%253B%250Aintermediate%2520tasks%2520involving%2520multi-step%2520processes%2520with%2520multiple%2520tools%252C%2520guided%250Aby%2520user%2520instructions%253B%2520and%2520advanced%2520tasks%2520which%2520involve%2520multi-step%2520processes%250Athat%2520require%2520multiple%2520tools%2520but%2520not%2520guided%2520by%2520user%2520instructions%252C%2520necessitating%250Athe%2520agent%2520to%2520independently%2520decide%2520on%2520and%2520executes%2520the%2520necessary%2520steps.%2520The%250Aevaluation%2520reveals%2520that%2520the%2520GIS%2520Copilot%2520demonstrates%2520strong%2520potential%2520in%250Aautomating%2520foundational%2520GIS%2520operations%252C%2520with%2520a%2520high%2520success%2520rate%2520in%2520tool%250Aselection%2520and%2520code%2520generation%2520for%2520basic%2520and%2520intermediate%2520tasks%252C%2520while%250Achallenges%2520remain%2520in%2520achieving%2520full%2520autonomy%2520for%2520more%2520complex%2520tasks.%2520This%2520study%250Acontributes%2520to%2520the%2520emerging%2520vision%2520of%2520Autonomous%2520GIS%252C%2520providing%2520a%2520pathway%2520for%250Anon-experts%2520to%2520engage%2520with%2520geospatial%2520analysis%2520with%2520minimal%2520prior%2520expertise.%250AWhile%2520full%2520autonomy%2520is%2520yet%2520to%2520be%2520achieved%252C%2520the%2520GIS%2520Copilot%2520demonstrates%250Asignificant%2520potential%2520for%2520simplifying%2520GIS%2520workflows%2520and%2520enhancing%250Adecision-making%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIS%20Copilot%3A%20Towards%20an%20Autonomous%20GIS%20Agent%20for%20Spatial%20Analysis&entry.906535625=Temitope%20Akinboyewa%20and%20Zhenlong%20Li%20and%20Huan%20Ning%20and%20M.%20Naser%20Lessani&entry.1292438233=%20%20Recent%20advancements%20in%20Generative%20AI%20offer%20promising%20capabilities%20for%20spatial%0Aanalysis.%20Despite%20their%20potential%2C%20the%20integration%20of%20generative%20AI%20with%0Aestablished%20GIS%20platforms%20remains%20underexplored.%20In%20this%20study%2C%20we%20propose%20a%0Aframework%20for%20integrating%20LLMs%20directly%20into%20existing%20GIS%20platforms%2C%20using%20QGIS%0Aas%20an%20example.%20Our%20approach%20leverages%20the%20reasoning%20and%20programming%0Acapabilities%20of%20LLMs%20to%20autonomously%20generate%20spatial%20analysis%20workflows%20and%0Acode%20through%20an%20informed%20agent%20that%20has%20comprehensive%20documentation%20of%20key%20GIS%0Atools%20and%20parameters.%20The%20implementation%20of%20this%20framework%20resulted%20in%20the%0Adevelopment%20of%20a%20%22GIS%20Copilot%22%20that%20allows%20GIS%20users%20to%20interact%20with%20QGIS%0Ausing%20natural%20language%20commands%20for%20spatial%20analysis.%20The%20GIS%20Copilot%20was%0Aevaluated%20based%20on%20three%20complexity%20levels%3A%20basic%20tasks%20that%20require%20one%20GIS%0Atool%20and%20typically%20involve%20one%20data%20layer%20to%20perform%20simple%20operations%3B%0Aintermediate%20tasks%20involving%20multi-step%20processes%20with%20multiple%20tools%2C%20guided%0Aby%20user%20instructions%3B%20and%20advanced%20tasks%20which%20involve%20multi-step%20processes%0Athat%20require%20multiple%20tools%20but%20not%20guided%20by%20user%20instructions%2C%20necessitating%0Athe%20agent%20to%20independently%20decide%20on%20and%20executes%20the%20necessary%20steps.%20The%0Aevaluation%20reveals%20that%20the%20GIS%20Copilot%20demonstrates%20strong%20potential%20in%0Aautomating%20foundational%20GIS%20operations%2C%20with%20a%20high%20success%20rate%20in%20tool%0Aselection%20and%20code%20generation%20for%20basic%20and%20intermediate%20tasks%2C%20while%0Achallenges%20remain%20in%20achieving%20full%20autonomy%20for%20more%20complex%20tasks.%20This%20study%0Acontributes%20to%20the%20emerging%20vision%20of%20Autonomous%20GIS%2C%20providing%20a%20pathway%20for%0Anon-experts%20to%20engage%20with%20geospatial%20analysis%20with%20minimal%20prior%20expertise.%0AWhile%20full%20autonomy%20is%20yet%20to%20be%20achieved%2C%20the%20GIS%20Copilot%20demonstrates%0Asignificant%20potential%20for%20simplifying%20GIS%20workflows%20and%20enhancing%0Adecision-making%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03205v2&entry.124074799=Read"},
{"title": "MT2ST: Adaptive Multi-Task to Single-Task Learning", "author": "Dong Liu", "abstract": "  The conventional training approaches often face challenges in balancing the\nbreadth of multi-task learning (MTL) with the depth of single-task learning\n(STL). To address this issue, we introduce the Multi-Task to Single-Task\n(MT2ST) framework, a groundbreaking approach that can combine the\ngeneralizability of MTL with the precision of STL. Our work include two\nstrategies: 'Diminish' and 'Switch'. 'Diminish' Strategy will gradually reduce\nthe influence of auxiliary tasks, while the 'Switch' strategy involves a shift\nfrom multi-tasking to single-tasking at a specific timepoint at the training\nprocess.\n  In this paper, we propose the Multi-Task to Single-Task (MT2ST) framework, a\nnovel approach that significantly enhances the efficiency and accuracy of word\nembedding training while concurrently addressing prevalent issues such as\noverfitting. Our empirical studies demonstrate that MT2ST can reduce training\ntime by 67% when contrasted with single-task learning approaches, and by 13%\ncompared to traditional multi-task learning methods. These findings underscore\nMT2ST's potential to be a powerful tools for word embedding training\nacceleration. The code implementation is can be found at:\nhttps://github.com/NoakLiu/MT2ST-Word-Embeddings-Acceleration.\n", "link": "http://arxiv.org/abs/2406.18038v2", "date": "2024-11-06", "relevancy": 2.1119, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5747}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4999}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MT2ST%3A%20Adaptive%20Multi-Task%20to%20Single-Task%20Learning&body=Title%3A%20MT2ST%3A%20Adaptive%20Multi-Task%20to%20Single-Task%20Learning%0AAuthor%3A%20Dong%20Liu%0AAbstract%3A%20%20%20The%20conventional%20training%20approaches%20often%20face%20challenges%20in%20balancing%20the%0Abreadth%20of%20multi-task%20learning%20%28MTL%29%20with%20the%20depth%20of%20single-task%20learning%0A%28STL%29.%20To%20address%20this%20issue%2C%20we%20introduce%20the%20Multi-Task%20to%20Single-Task%0A%28MT2ST%29%20framework%2C%20a%20groundbreaking%20approach%20that%20can%20combine%20the%0Ageneralizability%20of%20MTL%20with%20the%20precision%20of%20STL.%20Our%20work%20include%20two%0Astrategies%3A%20%27Diminish%27%20and%20%27Switch%27.%20%27Diminish%27%20Strategy%20will%20gradually%20reduce%0Athe%20influence%20of%20auxiliary%20tasks%2C%20while%20the%20%27Switch%27%20strategy%20involves%20a%20shift%0Afrom%20multi-tasking%20to%20single-tasking%20at%20a%20specific%20timepoint%20at%20the%20training%0Aprocess.%0A%20%20In%20this%20paper%2C%20we%20propose%20the%20Multi-Task%20to%20Single-Task%20%28MT2ST%29%20framework%2C%20a%0Anovel%20approach%20that%20significantly%20enhances%20the%20efficiency%20and%20accuracy%20of%20word%0Aembedding%20training%20while%20concurrently%20addressing%20prevalent%20issues%20such%20as%0Aoverfitting.%20Our%20empirical%20studies%20demonstrate%20that%20MT2ST%20can%20reduce%20training%0Atime%20by%2067%25%20when%20contrasted%20with%20single-task%20learning%20approaches%2C%20and%20by%2013%25%0Acompared%20to%20traditional%20multi-task%20learning%20methods.%20These%20findings%20underscore%0AMT2ST%27s%20potential%20to%20be%20a%20powerful%20tools%20for%20word%20embedding%20training%0Aacceleration.%20The%20code%20implementation%20is%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/NoakLiu/MT2ST-Word-Embeddings-Acceleration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18038v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMT2ST%253A%2520Adaptive%2520Multi-Task%2520to%2520Single-Task%2520Learning%26entry.906535625%3DDong%2520Liu%26entry.1292438233%3D%2520%2520The%2520conventional%2520training%2520approaches%2520often%2520face%2520challenges%2520in%2520balancing%2520the%250Abreadth%2520of%2520multi-task%2520learning%2520%2528MTL%2529%2520with%2520the%2520depth%2520of%2520single-task%2520learning%250A%2528STL%2529.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520the%2520Multi-Task%2520to%2520Single-Task%250A%2528MT2ST%2529%2520framework%252C%2520a%2520groundbreaking%2520approach%2520that%2520can%2520combine%2520the%250Ageneralizability%2520of%2520MTL%2520with%2520the%2520precision%2520of%2520STL.%2520Our%2520work%2520include%2520two%250Astrategies%253A%2520%2527Diminish%2527%2520and%2520%2527Switch%2527.%2520%2527Diminish%2527%2520Strategy%2520will%2520gradually%2520reduce%250Athe%2520influence%2520of%2520auxiliary%2520tasks%252C%2520while%2520the%2520%2527Switch%2527%2520strategy%2520involves%2520a%2520shift%250Afrom%2520multi-tasking%2520to%2520single-tasking%2520at%2520a%2520specific%2520timepoint%2520at%2520the%2520training%250Aprocess.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Multi-Task%2520to%2520Single-Task%2520%2528MT2ST%2529%2520framework%252C%2520a%250Anovel%2520approach%2520that%2520significantly%2520enhances%2520the%2520efficiency%2520and%2520accuracy%2520of%2520word%250Aembedding%2520training%2520while%2520concurrently%2520addressing%2520prevalent%2520issues%2520such%2520as%250Aoverfitting.%2520Our%2520empirical%2520studies%2520demonstrate%2520that%2520MT2ST%2520can%2520reduce%2520training%250Atime%2520by%252067%2525%2520when%2520contrasted%2520with%2520single-task%2520learning%2520approaches%252C%2520and%2520by%252013%2525%250Acompared%2520to%2520traditional%2520multi-task%2520learning%2520methods.%2520These%2520findings%2520underscore%250AMT2ST%2527s%2520potential%2520to%2520be%2520a%2520powerful%2520tools%2520for%2520word%2520embedding%2520training%250Aacceleration.%2520The%2520code%2520implementation%2520is%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/NoakLiu/MT2ST-Word-Embeddings-Acceleration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18038v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT2ST%3A%20Adaptive%20Multi-Task%20to%20Single-Task%20Learning&entry.906535625=Dong%20Liu&entry.1292438233=%20%20The%20conventional%20training%20approaches%20often%20face%20challenges%20in%20balancing%20the%0Abreadth%20of%20multi-task%20learning%20%28MTL%29%20with%20the%20depth%20of%20single-task%20learning%0A%28STL%29.%20To%20address%20this%20issue%2C%20we%20introduce%20the%20Multi-Task%20to%20Single-Task%0A%28MT2ST%29%20framework%2C%20a%20groundbreaking%20approach%20that%20can%20combine%20the%0Ageneralizability%20of%20MTL%20with%20the%20precision%20of%20STL.%20Our%20work%20include%20two%0Astrategies%3A%20%27Diminish%27%20and%20%27Switch%27.%20%27Diminish%27%20Strategy%20will%20gradually%20reduce%0Athe%20influence%20of%20auxiliary%20tasks%2C%20while%20the%20%27Switch%27%20strategy%20involves%20a%20shift%0Afrom%20multi-tasking%20to%20single-tasking%20at%20a%20specific%20timepoint%20at%20the%20training%0Aprocess.%0A%20%20In%20this%20paper%2C%20we%20propose%20the%20Multi-Task%20to%20Single-Task%20%28MT2ST%29%20framework%2C%20a%0Anovel%20approach%20that%20significantly%20enhances%20the%20efficiency%20and%20accuracy%20of%20word%0Aembedding%20training%20while%20concurrently%20addressing%20prevalent%20issues%20such%20as%0Aoverfitting.%20Our%20empirical%20studies%20demonstrate%20that%20MT2ST%20can%20reduce%20training%0Atime%20by%2067%25%20when%20contrasted%20with%20single-task%20learning%20approaches%2C%20and%20by%2013%25%0Acompared%20to%20traditional%20multi-task%20learning%20methods.%20These%20findings%20underscore%0AMT2ST%27s%20potential%20to%20be%20a%20powerful%20tools%20for%20word%20embedding%20training%0Aacceleration.%20The%20code%20implementation%20is%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/NoakLiu/MT2ST-Word-Embeddings-Acceleration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18038v2&entry.124074799=Read"},
{"title": "Pseudo-labeling with Keyword Refining for Few-Supervised Video\n  Captioning", "author": "Ping Li and Tao Wang and Xinkui Zhao and Xianghua Xu and Mingli Song", "abstract": "  Video captioning generate a sentence that describes the video content.\nExisting methods always require a number of captions (\\eg, 10 or 20) per video\nto train the model, which is quite costly. In this work, we explore the\npossibility of using only one or very few ground-truth sentences, and introduce\na new task named few-supervised video captioning. Specifically, we propose a\nfew-supervised video captioning framework that consists of lexically\nconstrained pseudo-labeling module and keyword-refined captioning module.\nUnlike the random sampling in natural language processing that may cause\ninvalid modifications (\\ie, edit words), the former module guides the model to\nedit words using some actions (\\eg, copy, replace, insert, and delete) by a\npretrained token-level classifier, and then fine-tunes candidate sentences by a\npretrained language model. Meanwhile, the former employs the repetition\npenalized sampling to encourage the model to yield concise pseudo-labeled\nsentences with less repetition, and selects the most relevant sentences upon a\npretrained video-text model. Moreover, to keep semantic consistency between\npseudo-labeled sentences and video content, we develop the transformer-based\nkeyword refiner with the video-keyword gated fusion strategy to emphasize more\non relevant words. Extensive experiments on several benchmarks demonstrate the\nadvantages of the proposed approach in both few-supervised and fully-supervised\nscenarios. The code implementation is available at\nhttps://github.com/mlvccn/PKG_VidCap\n", "link": "http://arxiv.org/abs/2411.04059v1", "date": "2024-11-06", "relevancy": 2.1111, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5346}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.53}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-labeling%20with%20Keyword%20Refining%20for%20Few-Supervised%20Video%0A%20%20Captioning&body=Title%3A%20Pseudo-labeling%20with%20Keyword%20Refining%20for%20Few-Supervised%20Video%0A%20%20Captioning%0AAuthor%3A%20Ping%20Li%20and%20Tao%20Wang%20and%20Xinkui%20Zhao%20and%20Xianghua%20Xu%20and%20Mingli%20Song%0AAbstract%3A%20%20%20Video%20captioning%20generate%20a%20sentence%20that%20describes%20the%20video%20content.%0AExisting%20methods%20always%20require%20a%20number%20of%20captions%20%28%5Ceg%2C%2010%20or%2020%29%20per%20video%0Ato%20train%20the%20model%2C%20which%20is%20quite%20costly.%20In%20this%20work%2C%20we%20explore%20the%0Apossibility%20of%20using%20only%20one%20or%20very%20few%20ground-truth%20sentences%2C%20and%20introduce%0Aa%20new%20task%20named%20few-supervised%20video%20captioning.%20Specifically%2C%20we%20propose%20a%0Afew-supervised%20video%20captioning%20framework%20that%20consists%20of%20lexically%0Aconstrained%20pseudo-labeling%20module%20and%20keyword-refined%20captioning%20module.%0AUnlike%20the%20random%20sampling%20in%20natural%20language%20processing%20that%20may%20cause%0Ainvalid%20modifications%20%28%5Cie%2C%20edit%20words%29%2C%20the%20former%20module%20guides%20the%20model%20to%0Aedit%20words%20using%20some%20actions%20%28%5Ceg%2C%20copy%2C%20replace%2C%20insert%2C%20and%20delete%29%20by%20a%0Apretrained%20token-level%20classifier%2C%20and%20then%20fine-tunes%20candidate%20sentences%20by%20a%0Apretrained%20language%20model.%20Meanwhile%2C%20the%20former%20employs%20the%20repetition%0Apenalized%20sampling%20to%20encourage%20the%20model%20to%20yield%20concise%20pseudo-labeled%0Asentences%20with%20less%20repetition%2C%20and%20selects%20the%20most%20relevant%20sentences%20upon%20a%0Apretrained%20video-text%20model.%20Moreover%2C%20to%20keep%20semantic%20consistency%20between%0Apseudo-labeled%20sentences%20and%20video%20content%2C%20we%20develop%20the%20transformer-based%0Akeyword%20refiner%20with%20the%20video-keyword%20gated%20fusion%20strategy%20to%20emphasize%20more%0Aon%20relevant%20words.%20Extensive%20experiments%20on%20several%20benchmarks%20demonstrate%20the%0Aadvantages%20of%20the%20proposed%20approach%20in%20both%20few-supervised%20and%20fully-supervised%0Ascenarios.%20The%20code%20implementation%20is%20available%20at%0Ahttps%3A//github.com/mlvccn/PKG_VidCap%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-labeling%2520with%2520Keyword%2520Refining%2520for%2520Few-Supervised%2520Video%250A%2520%2520Captioning%26entry.906535625%3DPing%2520Li%2520and%2520Tao%2520Wang%2520and%2520Xinkui%2520Zhao%2520and%2520Xianghua%2520Xu%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520Video%2520captioning%2520generate%2520a%2520sentence%2520that%2520describes%2520the%2520video%2520content.%250AExisting%2520methods%2520always%2520require%2520a%2520number%2520of%2520captions%2520%2528%255Ceg%252C%252010%2520or%252020%2529%2520per%2520video%250Ato%2520train%2520the%2520model%252C%2520which%2520is%2520quite%2520costly.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Apossibility%2520of%2520using%2520only%2520one%2520or%2520very%2520few%2520ground-truth%2520sentences%252C%2520and%2520introduce%250Aa%2520new%2520task%2520named%2520few-supervised%2520video%2520captioning.%2520Specifically%252C%2520we%2520propose%2520a%250Afew-supervised%2520video%2520captioning%2520framework%2520that%2520consists%2520of%2520lexically%250Aconstrained%2520pseudo-labeling%2520module%2520and%2520keyword-refined%2520captioning%2520module.%250AUnlike%2520the%2520random%2520sampling%2520in%2520natural%2520language%2520processing%2520that%2520may%2520cause%250Ainvalid%2520modifications%2520%2528%255Cie%252C%2520edit%2520words%2529%252C%2520the%2520former%2520module%2520guides%2520the%2520model%2520to%250Aedit%2520words%2520using%2520some%2520actions%2520%2528%255Ceg%252C%2520copy%252C%2520replace%252C%2520insert%252C%2520and%2520delete%2529%2520by%2520a%250Apretrained%2520token-level%2520classifier%252C%2520and%2520then%2520fine-tunes%2520candidate%2520sentences%2520by%2520a%250Apretrained%2520language%2520model.%2520Meanwhile%252C%2520the%2520former%2520employs%2520the%2520repetition%250Apenalized%2520sampling%2520to%2520encourage%2520the%2520model%2520to%2520yield%2520concise%2520pseudo-labeled%250Asentences%2520with%2520less%2520repetition%252C%2520and%2520selects%2520the%2520most%2520relevant%2520sentences%2520upon%2520a%250Apretrained%2520video-text%2520model.%2520Moreover%252C%2520to%2520keep%2520semantic%2520consistency%2520between%250Apseudo-labeled%2520sentences%2520and%2520video%2520content%252C%2520we%2520develop%2520the%2520transformer-based%250Akeyword%2520refiner%2520with%2520the%2520video-keyword%2520gated%2520fusion%2520strategy%2520to%2520emphasize%2520more%250Aon%2520relevant%2520words.%2520Extensive%2520experiments%2520on%2520several%2520benchmarks%2520demonstrate%2520the%250Aadvantages%2520of%2520the%2520proposed%2520approach%2520in%2520both%2520few-supervised%2520and%2520fully-supervised%250Ascenarios.%2520The%2520code%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/mlvccn/PKG_VidCap%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-labeling%20with%20Keyword%20Refining%20for%20Few-Supervised%20Video%0A%20%20Captioning&entry.906535625=Ping%20Li%20and%20Tao%20Wang%20and%20Xinkui%20Zhao%20and%20Xianghua%20Xu%20and%20Mingli%20Song&entry.1292438233=%20%20Video%20captioning%20generate%20a%20sentence%20that%20describes%20the%20video%20content.%0AExisting%20methods%20always%20require%20a%20number%20of%20captions%20%28%5Ceg%2C%2010%20or%2020%29%20per%20video%0Ato%20train%20the%20model%2C%20which%20is%20quite%20costly.%20In%20this%20work%2C%20we%20explore%20the%0Apossibility%20of%20using%20only%20one%20or%20very%20few%20ground-truth%20sentences%2C%20and%20introduce%0Aa%20new%20task%20named%20few-supervised%20video%20captioning.%20Specifically%2C%20we%20propose%20a%0Afew-supervised%20video%20captioning%20framework%20that%20consists%20of%20lexically%0Aconstrained%20pseudo-labeling%20module%20and%20keyword-refined%20captioning%20module.%0AUnlike%20the%20random%20sampling%20in%20natural%20language%20processing%20that%20may%20cause%0Ainvalid%20modifications%20%28%5Cie%2C%20edit%20words%29%2C%20the%20former%20module%20guides%20the%20model%20to%0Aedit%20words%20using%20some%20actions%20%28%5Ceg%2C%20copy%2C%20replace%2C%20insert%2C%20and%20delete%29%20by%20a%0Apretrained%20token-level%20classifier%2C%20and%20then%20fine-tunes%20candidate%20sentences%20by%20a%0Apretrained%20language%20model.%20Meanwhile%2C%20the%20former%20employs%20the%20repetition%0Apenalized%20sampling%20to%20encourage%20the%20model%20to%20yield%20concise%20pseudo-labeled%0Asentences%20with%20less%20repetition%2C%20and%20selects%20the%20most%20relevant%20sentences%20upon%20a%0Apretrained%20video-text%20model.%20Moreover%2C%20to%20keep%20semantic%20consistency%20between%0Apseudo-labeled%20sentences%20and%20video%20content%2C%20we%20develop%20the%20transformer-based%0Akeyword%20refiner%20with%20the%20video-keyword%20gated%20fusion%20strategy%20to%20emphasize%20more%0Aon%20relevant%20words.%20Extensive%20experiments%20on%20several%20benchmarks%20demonstrate%20the%0Aadvantages%20of%20the%20proposed%20approach%20in%20both%20few-supervised%20and%20fully-supervised%0Ascenarios.%20The%20code%20implementation%20is%20available%20at%0Ahttps%3A//github.com/mlvccn/PKG_VidCap%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04059v1&entry.124074799=Read"},
{"title": "No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen\n  Representations", "author": "Walter Simoncini and Spyros Gidaris and Andrei Bursuc and Yuki M. Asano", "abstract": "  This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of transformer encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These\ngradients are projected to a lower dimension and then concatenated with the\nmodel's output embedding. The resulting features are evaluated on k-nearest\nneighbor classification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification, clustering and image retrieval, and that they\nsignificantly improve the retrieval-based in-context scene understanding\nabilities of pretrained models, for example improving upon DINO by +17% for\nsemantic segmentation - without any training.\n", "link": "http://arxiv.org/abs/2407.10964v2", "date": "2024-11-06", "relevancy": 2.0931, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5452}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5141}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Train%2C%20all%20Gain%3A%20Self-Supervised%20Gradients%20Improve%20Deep%20Frozen%0A%20%20Representations&body=Title%3A%20No%20Train%2C%20all%20Gain%3A%20Self-Supervised%20Gradients%20Improve%20Deep%20Frozen%0A%20%20Representations%0AAuthor%3A%20Walter%20Simoncini%20and%20Spyros%20Gidaris%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20This%20paper%20introduces%20FUNGI%2C%20Features%20from%20UNsupervised%20GradIents%2C%20a%20method%0Ato%20enhance%20the%20features%20of%20transformer%20encoders%20by%20leveraging%20self-supervised%0Agradients.%20Our%20method%20is%20simple%3A%20given%20any%20pretrained%20model%2C%20we%20first%20compute%0Agradients%20from%20various%20self-supervised%20objectives%20for%20each%20input.%20These%0Agradients%20are%20projected%20to%20a%20lower%20dimension%20and%20then%20concatenated%20with%20the%0Amodel%27s%20output%20embedding.%20The%20resulting%20features%20are%20evaluated%20on%20k-nearest%0Aneighbor%20classification%20over%2011%20datasets%20from%20vision%2C%205%20from%20natural%20language%0Aprocessing%2C%20and%202%20from%20audio.%20Across%20backbones%20spanning%20various%20sizes%20and%0Apretraining%20strategies%2C%20FUNGI%20features%20provide%20consistent%20performance%0Aimprovements%20over%20the%20embeddings.%20We%20also%20show%20that%20using%20FUNGI%20features%20can%0Abenefit%20linear%20classification%2C%20clustering%20and%20image%20retrieval%2C%20and%20that%20they%0Asignificantly%20improve%20the%20retrieval-based%20in-context%20scene%20understanding%0Aabilities%20of%20pretrained%20models%2C%20for%20example%20improving%20upon%20DINO%20by%20%2B17%25%20for%0Asemantic%20segmentation%20-%20without%20any%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10964v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Train%252C%2520all%2520Gain%253A%2520Self-Supervised%2520Gradients%2520Improve%2520Deep%2520Frozen%250A%2520%2520Representations%26entry.906535625%3DWalter%2520Simoncini%2520and%2520Spyros%2520Gidaris%2520and%2520Andrei%2520Bursuc%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520FUNGI%252C%2520Features%2520from%2520UNsupervised%2520GradIents%252C%2520a%2520method%250Ato%2520enhance%2520the%2520features%2520of%2520transformer%2520encoders%2520by%2520leveraging%2520self-supervised%250Agradients.%2520Our%2520method%2520is%2520simple%253A%2520given%2520any%2520pretrained%2520model%252C%2520we%2520first%2520compute%250Agradients%2520from%2520various%2520self-supervised%2520objectives%2520for%2520each%2520input.%2520These%250Agradients%2520are%2520projected%2520to%2520a%2520lower%2520dimension%2520and%2520then%2520concatenated%2520with%2520the%250Amodel%2527s%2520output%2520embedding.%2520The%2520resulting%2520features%2520are%2520evaluated%2520on%2520k-nearest%250Aneighbor%2520classification%2520over%252011%2520datasets%2520from%2520vision%252C%25205%2520from%2520natural%2520language%250Aprocessing%252C%2520and%25202%2520from%2520audio.%2520Across%2520backbones%2520spanning%2520various%2520sizes%2520and%250Apretraining%2520strategies%252C%2520FUNGI%2520features%2520provide%2520consistent%2520performance%250Aimprovements%2520over%2520the%2520embeddings.%2520We%2520also%2520show%2520that%2520using%2520FUNGI%2520features%2520can%250Abenefit%2520linear%2520classification%252C%2520clustering%2520and%2520image%2520retrieval%252C%2520and%2520that%2520they%250Asignificantly%2520improve%2520the%2520retrieval-based%2520in-context%2520scene%2520understanding%250Aabilities%2520of%2520pretrained%2520models%252C%2520for%2520example%2520improving%2520upon%2520DINO%2520by%2520%252B17%2525%2520for%250Asemantic%2520segmentation%2520-%2520without%2520any%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10964v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Train%2C%20all%20Gain%3A%20Self-Supervised%20Gradients%20Improve%20Deep%20Frozen%0A%20%20Representations&entry.906535625=Walter%20Simoncini%20and%20Spyros%20Gidaris%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20This%20paper%20introduces%20FUNGI%2C%20Features%20from%20UNsupervised%20GradIents%2C%20a%20method%0Ato%20enhance%20the%20features%20of%20transformer%20encoders%20by%20leveraging%20self-supervised%0Agradients.%20Our%20method%20is%20simple%3A%20given%20any%20pretrained%20model%2C%20we%20first%20compute%0Agradients%20from%20various%20self-supervised%20objectives%20for%20each%20input.%20These%0Agradients%20are%20projected%20to%20a%20lower%20dimension%20and%20then%20concatenated%20with%20the%0Amodel%27s%20output%20embedding.%20The%20resulting%20features%20are%20evaluated%20on%20k-nearest%0Aneighbor%20classification%20over%2011%20datasets%20from%20vision%2C%205%20from%20natural%20language%0Aprocessing%2C%20and%202%20from%20audio.%20Across%20backbones%20spanning%20various%20sizes%20and%0Apretraining%20strategies%2C%20FUNGI%20features%20provide%20consistent%20performance%0Aimprovements%20over%20the%20embeddings.%20We%20also%20show%20that%20using%20FUNGI%20features%20can%0Abenefit%20linear%20classification%2C%20clustering%20and%20image%20retrieval%2C%20and%20that%20they%0Asignificantly%20improve%20the%20retrieval-based%20in-context%20scene%20understanding%0Aabilities%20of%20pretrained%20models%2C%20for%20example%20improving%20upon%20DINO%20by%20%2B17%25%20for%0Asemantic%20segmentation%20-%20without%20any%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10964v2&entry.124074799=Read"},
{"title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable\n  Transformers", "author": "Themistoklis Haris", "abstract": "  Despite their power, Transformers face challenges with long sequences due to\nthe quadratic complexity of self-attention. To address this limitation, methods\nlike $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy, Saffar,\nVaswani, Grangier, 2021] enabling each token to attend to only its $k$ closest\ntokens. While $k$NN attention has shown empirical success in making\nTransformers more efficient, its exact approximation guarantees have not been\ntheoretically analyzed. In this work, we establish a theoretical framework for\n$k$NN attention, reformulating self-attention as expectations over softmax\ndistributions and leveraging lazy Gumbel sampling [Mussmann, Levy, Ermon, 2017]\nwith $k$NN indices for efficient approximation. Building on this framework, we\nalso propose novel sub-quadratic algorithms that approximate self-attention\ngradients by leveraging efficient sampling techniques, such as Markov\nChain-based estimation. Finally, we demonstrate the practical effectiveness of\nthese algorithms through empirical experiments, showcasing their benefits in\nboth training and inference.\n", "link": "http://arxiv.org/abs/2411.04013v1", "date": "2024-11-06", "relevancy": 2.0884, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5487}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.512}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24k%24NN%20Attention%20Demystified%3A%20A%20Theoretical%20Exploration%20for%20Scalable%0A%20%20Transformers&body=Title%3A%20%24k%24NN%20Attention%20Demystified%3A%20A%20Theoretical%20Exploration%20for%20Scalable%0A%20%20Transformers%0AAuthor%3A%20Themistoklis%20Haris%0AAbstract%3A%20%20%20Despite%20their%20power%2C%20Transformers%20face%20challenges%20with%20long%20sequences%20due%20to%0Athe%20quadratic%20complexity%20of%20self-attention.%20To%20address%20this%20limitation%2C%20methods%0Alike%20%24k%24-Nearest-Neighbor%20%28%24k%24NN%29%20attention%20have%20been%20introduced%20%5BRoy%2C%20Saffar%2C%0AVaswani%2C%20Grangier%2C%202021%5D%20enabling%20each%20token%20to%20attend%20to%20only%20its%20%24k%24%20closest%0Atokens.%20While%20%24k%24NN%20attention%20has%20shown%20empirical%20success%20in%20making%0ATransformers%20more%20efficient%2C%20its%20exact%20approximation%20guarantees%20have%20not%20been%0Atheoretically%20analyzed.%20In%20this%20work%2C%20we%20establish%20a%20theoretical%20framework%20for%0A%24k%24NN%20attention%2C%20reformulating%20self-attention%20as%20expectations%20over%20softmax%0Adistributions%20and%20leveraging%20lazy%20Gumbel%20sampling%20%5BMussmann%2C%20Levy%2C%20Ermon%2C%202017%5D%0Awith%20%24k%24NN%20indices%20for%20efficient%20approximation.%20Building%20on%20this%20framework%2C%20we%0Aalso%20propose%20novel%20sub-quadratic%20algorithms%20that%20approximate%20self-attention%0Agradients%20by%20leveraging%20efficient%20sampling%20techniques%2C%20such%20as%20Markov%0AChain-based%20estimation.%20Finally%2C%20we%20demonstrate%20the%20practical%20effectiveness%20of%0Athese%20algorithms%20through%20empirical%20experiments%2C%20showcasing%20their%20benefits%20in%0Aboth%20training%20and%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524k%2524NN%2520Attention%2520Demystified%253A%2520A%2520Theoretical%2520Exploration%2520for%2520Scalable%250A%2520%2520Transformers%26entry.906535625%3DThemistoklis%2520Haris%26entry.1292438233%3D%2520%2520Despite%2520their%2520power%252C%2520Transformers%2520face%2520challenges%2520with%2520long%2520sequences%2520due%2520to%250Athe%2520quadratic%2520complexity%2520of%2520self-attention.%2520To%2520address%2520this%2520limitation%252C%2520methods%250Alike%2520%2524k%2524-Nearest-Neighbor%2520%2528%2524k%2524NN%2529%2520attention%2520have%2520been%2520introduced%2520%255BRoy%252C%2520Saffar%252C%250AVaswani%252C%2520Grangier%252C%25202021%255D%2520enabling%2520each%2520token%2520to%2520attend%2520to%2520only%2520its%2520%2524k%2524%2520closest%250Atokens.%2520While%2520%2524k%2524NN%2520attention%2520has%2520shown%2520empirical%2520success%2520in%2520making%250ATransformers%2520more%2520efficient%252C%2520its%2520exact%2520approximation%2520guarantees%2520have%2520not%2520been%250Atheoretically%2520analyzed.%2520In%2520this%2520work%252C%2520we%2520establish%2520a%2520theoretical%2520framework%2520for%250A%2524k%2524NN%2520attention%252C%2520reformulating%2520self-attention%2520as%2520expectations%2520over%2520softmax%250Adistributions%2520and%2520leveraging%2520lazy%2520Gumbel%2520sampling%2520%255BMussmann%252C%2520Levy%252C%2520Ermon%252C%25202017%255D%250Awith%2520%2524k%2524NN%2520indices%2520for%2520efficient%2520approximation.%2520Building%2520on%2520this%2520framework%252C%2520we%250Aalso%2520propose%2520novel%2520sub-quadratic%2520algorithms%2520that%2520approximate%2520self-attention%250Agradients%2520by%2520leveraging%2520efficient%2520sampling%2520techniques%252C%2520such%2520as%2520Markov%250AChain-based%2520estimation.%2520Finally%252C%2520we%2520demonstrate%2520the%2520practical%2520effectiveness%2520of%250Athese%2520algorithms%2520through%2520empirical%2520experiments%252C%2520showcasing%2520their%2520benefits%2520in%250Aboth%2520training%2520and%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24k%24NN%20Attention%20Demystified%3A%20A%20Theoretical%20Exploration%20for%20Scalable%0A%20%20Transformers&entry.906535625=Themistoklis%20Haris&entry.1292438233=%20%20Despite%20their%20power%2C%20Transformers%20face%20challenges%20with%20long%20sequences%20due%20to%0Athe%20quadratic%20complexity%20of%20self-attention.%20To%20address%20this%20limitation%2C%20methods%0Alike%20%24k%24-Nearest-Neighbor%20%28%24k%24NN%29%20attention%20have%20been%20introduced%20%5BRoy%2C%20Saffar%2C%0AVaswani%2C%20Grangier%2C%202021%5D%20enabling%20each%20token%20to%20attend%20to%20only%20its%20%24k%24%20closest%0Atokens.%20While%20%24k%24NN%20attention%20has%20shown%20empirical%20success%20in%20making%0ATransformers%20more%20efficient%2C%20its%20exact%20approximation%20guarantees%20have%20not%20been%0Atheoretically%20analyzed.%20In%20this%20work%2C%20we%20establish%20a%20theoretical%20framework%20for%0A%24k%24NN%20attention%2C%20reformulating%20self-attention%20as%20expectations%20over%20softmax%0Adistributions%20and%20leveraging%20lazy%20Gumbel%20sampling%20%5BMussmann%2C%20Levy%2C%20Ermon%2C%202017%5D%0Awith%20%24k%24NN%20indices%20for%20efficient%20approximation.%20Building%20on%20this%20framework%2C%20we%0Aalso%20propose%20novel%20sub-quadratic%20algorithms%20that%20approximate%20self-attention%0Agradients%20by%20leveraging%20efficient%20sampling%20techniques%2C%20such%20as%20Markov%0AChain-based%20estimation.%20Finally%2C%20we%20demonstrate%20the%20practical%20effectiveness%20of%0Athese%20algorithms%20through%20empirical%20experiments%2C%20showcasing%20their%20benefits%20in%0Aboth%20training%20and%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04013v1&entry.124074799=Read"},
{"title": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis", "author": "Guan Zhe Hong and Nishanth Dikkala and Enming Luo and Cyrus Rashtchian and Rina Panigrahy", "abstract": "  Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.\n", "link": "http://arxiv.org/abs/2411.04105v1", "date": "2024-11-06", "relevancy": 2.0816, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Transformers%20Solve%20Propositional%20Logic%20Problems%3A%20A%20Mechanistic%0A%20%20Analysis&body=Title%3A%20How%20Transformers%20Solve%20Propositional%20Logic%20Problems%3A%20A%20Mechanistic%0A%20%20Analysis%0AAuthor%3A%20Guan%20Zhe%20Hong%20and%20Nishanth%20Dikkala%20and%20Enming%20Luo%20and%20Cyrus%20Rashtchian%20and%20Rina%20Panigrahy%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20amazing%20performance%20on%20tasks%20that%0Arequire%20planning%20and%20reasoning.%20Motivated%20by%20this%2C%20we%20investigate%20the%20internal%0Amechanisms%20that%20underpin%20a%20network%27s%20ability%20to%20perform%20complex%20logical%0Areasoning.%20We%20first%20construct%20a%20synthetic%20propositional%20logic%20problem%20that%0Aserves%20as%20a%20concrete%20test-bed%20for%20network%20training%20and%20evaluation.%20Crucially%2C%0Athis%20problem%20demands%20nontrivial%20planning%20to%20solve%2C%20but%20we%20can%20train%20a%20small%0Atransformer%20to%20achieve%20perfect%20accuracy.%20Building%20on%20our%20set-up%2C%20we%20then%20pursue%0Aan%20understanding%20of%20precisely%20how%20a%20three-layer%20transformer%2C%20trained%20from%0Ascratch%2C%20solves%20this%20problem.%20We%20are%20able%20to%20identify%20certain%20%22planning%22%20and%0A%22reasoning%22%20circuits%20in%20the%20network%20that%20necessitate%20cooperation%20between%20the%0Aattention%20blocks%20to%20implement%20the%20desired%20logic.%20To%20expand%20our%20findings%2C%20we%0Athen%20study%20a%20larger%20model%2C%20Mistral%207B.%20Using%20activation%20patching%2C%20we%0Acharacterize%20internal%20components%20that%20are%20critical%20in%20solving%20our%20logic%0Aproblem.%20Overall%2C%20our%20work%20systemically%20uncovers%20novel%20aspects%20of%20small%20and%0Alarge%20transformers%2C%20and%20continues%20the%20study%20of%20how%20they%20plan%20and%20reason.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Transformers%2520Solve%2520Propositional%2520Logic%2520Problems%253A%2520A%2520Mechanistic%250A%2520%2520Analysis%26entry.906535625%3DGuan%2520Zhe%2520Hong%2520and%2520Nishanth%2520Dikkala%2520and%2520Enming%2520Luo%2520and%2520Cyrus%2520Rashtchian%2520and%2520Rina%2520Panigrahy%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520amazing%2520performance%2520on%2520tasks%2520that%250Arequire%2520planning%2520and%2520reasoning.%2520Motivated%2520by%2520this%252C%2520we%2520investigate%2520the%2520internal%250Amechanisms%2520that%2520underpin%2520a%2520network%2527s%2520ability%2520to%2520perform%2520complex%2520logical%250Areasoning.%2520We%2520first%2520construct%2520a%2520synthetic%2520propositional%2520logic%2520problem%2520that%250Aserves%2520as%2520a%2520concrete%2520test-bed%2520for%2520network%2520training%2520and%2520evaluation.%2520Crucially%252C%250Athis%2520problem%2520demands%2520nontrivial%2520planning%2520to%2520solve%252C%2520but%2520we%2520can%2520train%2520a%2520small%250Atransformer%2520to%2520achieve%2520perfect%2520accuracy.%2520Building%2520on%2520our%2520set-up%252C%2520we%2520then%2520pursue%250Aan%2520understanding%2520of%2520precisely%2520how%2520a%2520three-layer%2520transformer%252C%2520trained%2520from%250Ascratch%252C%2520solves%2520this%2520problem.%2520We%2520are%2520able%2520to%2520identify%2520certain%2520%2522planning%2522%2520and%250A%2522reasoning%2522%2520circuits%2520in%2520the%2520network%2520that%2520necessitate%2520cooperation%2520between%2520the%250Aattention%2520blocks%2520to%2520implement%2520the%2520desired%2520logic.%2520To%2520expand%2520our%2520findings%252C%2520we%250Athen%2520study%2520a%2520larger%2520model%252C%2520Mistral%25207B.%2520Using%2520activation%2520patching%252C%2520we%250Acharacterize%2520internal%2520components%2520that%2520are%2520critical%2520in%2520solving%2520our%2520logic%250Aproblem.%2520Overall%252C%2520our%2520work%2520systemically%2520uncovers%2520novel%2520aspects%2520of%2520small%2520and%250Alarge%2520transformers%252C%2520and%2520continues%2520the%2520study%2520of%2520how%2520they%2520plan%2520and%2520reason.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Transformers%20Solve%20Propositional%20Logic%20Problems%3A%20A%20Mechanistic%0A%20%20Analysis&entry.906535625=Guan%20Zhe%20Hong%20and%20Nishanth%20Dikkala%20and%20Enming%20Luo%20and%20Cyrus%20Rashtchian%20and%20Rina%20Panigrahy&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20amazing%20performance%20on%20tasks%20that%0Arequire%20planning%20and%20reasoning.%20Motivated%20by%20this%2C%20we%20investigate%20the%20internal%0Amechanisms%20that%20underpin%20a%20network%27s%20ability%20to%20perform%20complex%20logical%0Areasoning.%20We%20first%20construct%20a%20synthetic%20propositional%20logic%20problem%20that%0Aserves%20as%20a%20concrete%20test-bed%20for%20network%20training%20and%20evaluation.%20Crucially%2C%0Athis%20problem%20demands%20nontrivial%20planning%20to%20solve%2C%20but%20we%20can%20train%20a%20small%0Atransformer%20to%20achieve%20perfect%20accuracy.%20Building%20on%20our%20set-up%2C%20we%20then%20pursue%0Aan%20understanding%20of%20precisely%20how%20a%20three-layer%20transformer%2C%20trained%20from%0Ascratch%2C%20solves%20this%20problem.%20We%20are%20able%20to%20identify%20certain%20%22planning%22%20and%0A%22reasoning%22%20circuits%20in%20the%20network%20that%20necessitate%20cooperation%20between%20the%0Aattention%20blocks%20to%20implement%20the%20desired%20logic.%20To%20expand%20our%20findings%2C%20we%0Athen%20study%20a%20larger%20model%2C%20Mistral%207B.%20Using%20activation%20patching%2C%20we%0Acharacterize%20internal%20components%20that%20are%20critical%20in%20solving%20our%20logic%0Aproblem.%20Overall%2C%20our%20work%20systemically%20uncovers%20novel%20aspects%20of%20small%20and%0Alarge%20transformers%2C%20and%20continues%20the%20study%20of%20how%20they%20plan%20and%20reason.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04105v1&entry.124074799=Read"},
{"title": "On the Decomposition of Differential Game", "author": "Nanxiang Zhou and Jing Dong and Yutian Li and Baoxiang Wang", "abstract": "  To understand the complexity of the dynamic of learning in differential\ngames, we decompose the game into components where the dynamic is well\nunderstood. One of the possible tools is Helmholtz's theorem, which can\ndecompose a vector field into a potential and a harmonic component. This has\nbeen shown to be effective in finite and normal-form games. However, applying\nHelmholtz's theorem by connecting it with the Hodge theorem on $\\mathbb{R}^n$\n(which is the strategy space of differential game) is non-trivial due to the\nnon-compactness of $\\mathbb{R}^n$. Bridging the dynamic-strategic disconnect\nthrough Hodge/Helmoltz's theorem in differential games is then left as an open\nproblem \\cite{letcher2019differentiable}. In this work, we provide two\ndecompositions of differential games to answer this question: the first as an\nexact scalar potential part, a near vector potential part, and a non-strategic\npart; the second as a near scalar potential part, an exact vector potential\npart, and a non-strategic part. We show that scalar potential games coincide\nwith potential games proposed by \\cite{monderer1996potential}, where the\ngradient descent dynamic can successfully find the Nash equilibrium. For the\nvector potential game, we show that the individual gradient field is\ndivergence-free, in which case the gradient descent dynamic may either be\ndivergent or recurrent.\n", "link": "http://arxiv.org/abs/2411.03802v1", "date": "2024-11-06", "relevancy": 2.0783, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.453}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4004}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Decomposition%20of%20Differential%20Game&body=Title%3A%20On%20the%20Decomposition%20of%20Differential%20Game%0AAuthor%3A%20Nanxiang%20Zhou%20and%20Jing%20Dong%20and%20Yutian%20Li%20and%20Baoxiang%20Wang%0AAbstract%3A%20%20%20To%20understand%20the%20complexity%20of%20the%20dynamic%20of%20learning%20in%20differential%0Agames%2C%20we%20decompose%20the%20game%20into%20components%20where%20the%20dynamic%20is%20well%0Aunderstood.%20One%20of%20the%20possible%20tools%20is%20Helmholtz%27s%20theorem%2C%20which%20can%0Adecompose%20a%20vector%20field%20into%20a%20potential%20and%20a%20harmonic%20component.%20This%20has%0Abeen%20shown%20to%20be%20effective%20in%20finite%20and%20normal-form%20games.%20However%2C%20applying%0AHelmholtz%27s%20theorem%20by%20connecting%20it%20with%20the%20Hodge%20theorem%20on%20%24%5Cmathbb%7BR%7D%5En%24%0A%28which%20is%20the%20strategy%20space%20of%20differential%20game%29%20is%20non-trivial%20due%20to%20the%0Anon-compactness%20of%20%24%5Cmathbb%7BR%7D%5En%24.%20Bridging%20the%20dynamic-strategic%20disconnect%0Athrough%20Hodge/Helmoltz%27s%20theorem%20in%20differential%20games%20is%20then%20left%20as%20an%20open%0Aproblem%20%5Ccite%7Bletcher2019differentiable%7D.%20In%20this%20work%2C%20we%20provide%20two%0Adecompositions%20of%20differential%20games%20to%20answer%20this%20question%3A%20the%20first%20as%20an%0Aexact%20scalar%20potential%20part%2C%20a%20near%20vector%20potential%20part%2C%20and%20a%20non-strategic%0Apart%3B%20the%20second%20as%20a%20near%20scalar%20potential%20part%2C%20an%20exact%20vector%20potential%0Apart%2C%20and%20a%20non-strategic%20part.%20We%20show%20that%20scalar%20potential%20games%20coincide%0Awith%20potential%20games%20proposed%20by%20%5Ccite%7Bmonderer1996potential%7D%2C%20where%20the%0Agradient%20descent%20dynamic%20can%20successfully%20find%20the%20Nash%20equilibrium.%20For%20the%0Avector%20potential%20game%2C%20we%20show%20that%20the%20individual%20gradient%20field%20is%0Adivergence-free%2C%20in%20which%20case%20the%20gradient%20descent%20dynamic%20may%20either%20be%0Adivergent%20or%20recurrent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Decomposition%2520of%2520Differential%2520Game%26entry.906535625%3DNanxiang%2520Zhou%2520and%2520Jing%2520Dong%2520and%2520Yutian%2520Li%2520and%2520Baoxiang%2520Wang%26entry.1292438233%3D%2520%2520To%2520understand%2520the%2520complexity%2520of%2520the%2520dynamic%2520of%2520learning%2520in%2520differential%250Agames%252C%2520we%2520decompose%2520the%2520game%2520into%2520components%2520where%2520the%2520dynamic%2520is%2520well%250Aunderstood.%2520One%2520of%2520the%2520possible%2520tools%2520is%2520Helmholtz%2527s%2520theorem%252C%2520which%2520can%250Adecompose%2520a%2520vector%2520field%2520into%2520a%2520potential%2520and%2520a%2520harmonic%2520component.%2520This%2520has%250Abeen%2520shown%2520to%2520be%2520effective%2520in%2520finite%2520and%2520normal-form%2520games.%2520However%252C%2520applying%250AHelmholtz%2527s%2520theorem%2520by%2520connecting%2520it%2520with%2520the%2520Hodge%2520theorem%2520on%2520%2524%255Cmathbb%257BR%257D%255En%2524%250A%2528which%2520is%2520the%2520strategy%2520space%2520of%2520differential%2520game%2529%2520is%2520non-trivial%2520due%2520to%2520the%250Anon-compactness%2520of%2520%2524%255Cmathbb%257BR%257D%255En%2524.%2520Bridging%2520the%2520dynamic-strategic%2520disconnect%250Athrough%2520Hodge/Helmoltz%2527s%2520theorem%2520in%2520differential%2520games%2520is%2520then%2520left%2520as%2520an%2520open%250Aproblem%2520%255Ccite%257Bletcher2019differentiable%257D.%2520In%2520this%2520work%252C%2520we%2520provide%2520two%250Adecompositions%2520of%2520differential%2520games%2520to%2520answer%2520this%2520question%253A%2520the%2520first%2520as%2520an%250Aexact%2520scalar%2520potential%2520part%252C%2520a%2520near%2520vector%2520potential%2520part%252C%2520and%2520a%2520non-strategic%250Apart%253B%2520the%2520second%2520as%2520a%2520near%2520scalar%2520potential%2520part%252C%2520an%2520exact%2520vector%2520potential%250Apart%252C%2520and%2520a%2520non-strategic%2520part.%2520We%2520show%2520that%2520scalar%2520potential%2520games%2520coincide%250Awith%2520potential%2520games%2520proposed%2520by%2520%255Ccite%257Bmonderer1996potential%257D%252C%2520where%2520the%250Agradient%2520descent%2520dynamic%2520can%2520successfully%2520find%2520the%2520Nash%2520equilibrium.%2520For%2520the%250Avector%2520potential%2520game%252C%2520we%2520show%2520that%2520the%2520individual%2520gradient%2520field%2520is%250Adivergence-free%252C%2520in%2520which%2520case%2520the%2520gradient%2520descent%2520dynamic%2520may%2520either%2520be%250Adivergent%2520or%2520recurrent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Decomposition%20of%20Differential%20Game&entry.906535625=Nanxiang%20Zhou%20and%20Jing%20Dong%20and%20Yutian%20Li%20and%20Baoxiang%20Wang&entry.1292438233=%20%20To%20understand%20the%20complexity%20of%20the%20dynamic%20of%20learning%20in%20differential%0Agames%2C%20we%20decompose%20the%20game%20into%20components%20where%20the%20dynamic%20is%20well%0Aunderstood.%20One%20of%20the%20possible%20tools%20is%20Helmholtz%27s%20theorem%2C%20which%20can%0Adecompose%20a%20vector%20field%20into%20a%20potential%20and%20a%20harmonic%20component.%20This%20has%0Abeen%20shown%20to%20be%20effective%20in%20finite%20and%20normal-form%20games.%20However%2C%20applying%0AHelmholtz%27s%20theorem%20by%20connecting%20it%20with%20the%20Hodge%20theorem%20on%20%24%5Cmathbb%7BR%7D%5En%24%0A%28which%20is%20the%20strategy%20space%20of%20differential%20game%29%20is%20non-trivial%20due%20to%20the%0Anon-compactness%20of%20%24%5Cmathbb%7BR%7D%5En%24.%20Bridging%20the%20dynamic-strategic%20disconnect%0Athrough%20Hodge/Helmoltz%27s%20theorem%20in%20differential%20games%20is%20then%20left%20as%20an%20open%0Aproblem%20%5Ccite%7Bletcher2019differentiable%7D.%20In%20this%20work%2C%20we%20provide%20two%0Adecompositions%20of%20differential%20games%20to%20answer%20this%20question%3A%20the%20first%20as%20an%0Aexact%20scalar%20potential%20part%2C%20a%20near%20vector%20potential%20part%2C%20and%20a%20non-strategic%0Apart%3B%20the%20second%20as%20a%20near%20scalar%20potential%20part%2C%20an%20exact%20vector%20potential%0Apart%2C%20and%20a%20non-strategic%20part.%20We%20show%20that%20scalar%20potential%20games%20coincide%0Awith%20potential%20games%20proposed%20by%20%5Ccite%7Bmonderer1996potential%7D%2C%20where%20the%0Agradient%20descent%20dynamic%20can%20successfully%20find%20the%20Nash%20equilibrium.%20For%20the%0Avector%20potential%20game%2C%20we%20show%20that%20the%20individual%20gradient%20field%20is%0Adivergence-free%2C%20in%20which%20case%20the%20gradient%20descent%20dynamic%20may%20either%20be%0Adivergent%20or%20recurrent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03802v1&entry.124074799=Read"},
{"title": "Topology-guided Hypergraph Transformer Network: Unveiling Structural\n  Insights for Improved Representation", "author": "Khaled Mohammed Saifuddin and Mehmet Emin Aktas and Esra Akbas", "abstract": "  Hypergraphs, with their capacity to depict high-order relationships, have\nemerged as a significant extension of traditional graphs. Although Graph Neural\nNetworks (GNNs) have remarkable performance in graph representation learning,\ntheir extension to hypergraphs encounters challenges due to their intricate\nstructures. Furthermore, current hypergraph transformers, a special variant of\nGNN, utilize semantic feature-based self-attention, ignoring topological\nattributes of nodes and hyperedges. To address these challenges, we propose a\nTopology-guided Hypergraph Transformer Network (THTN). In this model, we first\nformulate a hypergraph from a graph while retaining its structural essence to\nlearn higher-order relations within the graph. Then, we design a simple yet\neffective structural and spatial encoding module to incorporate the topological\nand spatial information of the nodes into their representation. Further, we\npresent a structure-aware self-attention mechanism that discovers the important\nnodes and hyperedges from both semantic and structural viewpoints. By\nleveraging these two modules, THTN crafts an improved node representation,\ncapturing both local and global topological expressions. Extensive experiments\nconducted on node classification tasks demonstrate that the performance of the\nproposed model consistently exceeds that of the existing approaches.\n", "link": "http://arxiv.org/abs/2310.09657v4", "date": "2024-11-06", "relevancy": 2.0648, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5383}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5229}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-guided%20Hypergraph%20Transformer%20Network%3A%20Unveiling%20Structural%0A%20%20Insights%20for%20Improved%20Representation&body=Title%3A%20Topology-guided%20Hypergraph%20Transformer%20Network%3A%20Unveiling%20Structural%0A%20%20Insights%20for%20Improved%20Representation%0AAuthor%3A%20Khaled%20Mohammed%20Saifuddin%20and%20Mehmet%20Emin%20Aktas%20and%20Esra%20Akbas%0AAbstract%3A%20%20%20Hypergraphs%2C%20with%20their%20capacity%20to%20depict%20high-order%20relationships%2C%20have%0Aemerged%20as%20a%20significant%20extension%20of%20traditional%20graphs.%20Although%20Graph%20Neural%0ANetworks%20%28GNNs%29%20have%20remarkable%20performance%20in%20graph%20representation%20learning%2C%0Atheir%20extension%20to%20hypergraphs%20encounters%20challenges%20due%20to%20their%20intricate%0Astructures.%20Furthermore%2C%20current%20hypergraph%20transformers%2C%20a%20special%20variant%20of%0AGNN%2C%20utilize%20semantic%20feature-based%20self-attention%2C%20ignoring%20topological%0Aattributes%20of%20nodes%20and%20hyperedges.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0ATopology-guided%20Hypergraph%20Transformer%20Network%20%28THTN%29.%20In%20this%20model%2C%20we%20first%0Aformulate%20a%20hypergraph%20from%20a%20graph%20while%20retaining%20its%20structural%20essence%20to%0Alearn%20higher-order%20relations%20within%20the%20graph.%20Then%2C%20we%20design%20a%20simple%20yet%0Aeffective%20structural%20and%20spatial%20encoding%20module%20to%20incorporate%20the%20topological%0Aand%20spatial%20information%20of%20the%20nodes%20into%20their%20representation.%20Further%2C%20we%0Apresent%20a%20structure-aware%20self-attention%20mechanism%20that%20discovers%20the%20important%0Anodes%20and%20hyperedges%20from%20both%20semantic%20and%20structural%20viewpoints.%20By%0Aleveraging%20these%20two%20modules%2C%20THTN%20crafts%20an%20improved%20node%20representation%2C%0Acapturing%20both%20local%20and%20global%20topological%20expressions.%20Extensive%20experiments%0Aconducted%20on%20node%20classification%20tasks%20demonstrate%20that%20the%20performance%20of%20the%0Aproposed%20model%20consistently%20exceeds%20that%20of%20the%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09657v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-guided%2520Hypergraph%2520Transformer%2520Network%253A%2520Unveiling%2520Structural%250A%2520%2520Insights%2520for%2520Improved%2520Representation%26entry.906535625%3DKhaled%2520Mohammed%2520Saifuddin%2520and%2520Mehmet%2520Emin%2520Aktas%2520and%2520Esra%2520Akbas%26entry.1292438233%3D%2520%2520Hypergraphs%252C%2520with%2520their%2520capacity%2520to%2520depict%2520high-order%2520relationships%252C%2520have%250Aemerged%2520as%2520a%2520significant%2520extension%2520of%2520traditional%2520graphs.%2520Although%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520have%2520remarkable%2520performance%2520in%2520graph%2520representation%2520learning%252C%250Atheir%2520extension%2520to%2520hypergraphs%2520encounters%2520challenges%2520due%2520to%2520their%2520intricate%250Astructures.%2520Furthermore%252C%2520current%2520hypergraph%2520transformers%252C%2520a%2520special%2520variant%2520of%250AGNN%252C%2520utilize%2520semantic%2520feature-based%2520self-attention%252C%2520ignoring%2520topological%250Aattributes%2520of%2520nodes%2520and%2520hyperedges.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250ATopology-guided%2520Hypergraph%2520Transformer%2520Network%2520%2528THTN%2529.%2520In%2520this%2520model%252C%2520we%2520first%250Aformulate%2520a%2520hypergraph%2520from%2520a%2520graph%2520while%2520retaining%2520its%2520structural%2520essence%2520to%250Alearn%2520higher-order%2520relations%2520within%2520the%2520graph.%2520Then%252C%2520we%2520design%2520a%2520simple%2520yet%250Aeffective%2520structural%2520and%2520spatial%2520encoding%2520module%2520to%2520incorporate%2520the%2520topological%250Aand%2520spatial%2520information%2520of%2520the%2520nodes%2520into%2520their%2520representation.%2520Further%252C%2520we%250Apresent%2520a%2520structure-aware%2520self-attention%2520mechanism%2520that%2520discovers%2520the%2520important%250Anodes%2520and%2520hyperedges%2520from%2520both%2520semantic%2520and%2520structural%2520viewpoints.%2520By%250Aleveraging%2520these%2520two%2520modules%252C%2520THTN%2520crafts%2520an%2520improved%2520node%2520representation%252C%250Acapturing%2520both%2520local%2520and%2520global%2520topological%2520expressions.%2520Extensive%2520experiments%250Aconducted%2520on%2520node%2520classification%2520tasks%2520demonstrate%2520that%2520the%2520performance%2520of%2520the%250Aproposed%2520model%2520consistently%2520exceeds%2520that%2520of%2520the%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09657v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-guided%20Hypergraph%20Transformer%20Network%3A%20Unveiling%20Structural%0A%20%20Insights%20for%20Improved%20Representation&entry.906535625=Khaled%20Mohammed%20Saifuddin%20and%20Mehmet%20Emin%20Aktas%20and%20Esra%20Akbas&entry.1292438233=%20%20Hypergraphs%2C%20with%20their%20capacity%20to%20depict%20high-order%20relationships%2C%20have%0Aemerged%20as%20a%20significant%20extension%20of%20traditional%20graphs.%20Although%20Graph%20Neural%0ANetworks%20%28GNNs%29%20have%20remarkable%20performance%20in%20graph%20representation%20learning%2C%0Atheir%20extension%20to%20hypergraphs%20encounters%20challenges%20due%20to%20their%20intricate%0Astructures.%20Furthermore%2C%20current%20hypergraph%20transformers%2C%20a%20special%20variant%20of%0AGNN%2C%20utilize%20semantic%20feature-based%20self-attention%2C%20ignoring%20topological%0Aattributes%20of%20nodes%20and%20hyperedges.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0ATopology-guided%20Hypergraph%20Transformer%20Network%20%28THTN%29.%20In%20this%20model%2C%20we%20first%0Aformulate%20a%20hypergraph%20from%20a%20graph%20while%20retaining%20its%20structural%20essence%20to%0Alearn%20higher-order%20relations%20within%20the%20graph.%20Then%2C%20we%20design%20a%20simple%20yet%0Aeffective%20structural%20and%20spatial%20encoding%20module%20to%20incorporate%20the%20topological%0Aand%20spatial%20information%20of%20the%20nodes%20into%20their%20representation.%20Further%2C%20we%0Apresent%20a%20structure-aware%20self-attention%20mechanism%20that%20discovers%20the%20important%0Anodes%20and%20hyperedges%20from%20both%20semantic%20and%20structural%20viewpoints.%20By%0Aleveraging%20these%20two%20modules%2C%20THTN%20crafts%20an%20improved%20node%20representation%2C%0Acapturing%20both%20local%20and%20global%20topological%20expressions.%20Extensive%20experiments%0Aconducted%20on%20node%20classification%20tasks%20demonstrate%20that%20the%20performance%20of%20the%0Aproposed%20model%20consistently%20exceeds%20that%20of%20the%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09657v4&entry.124074799=Read"},
{"title": "Fine-tuning -- a Transfer Learning approach", "author": "Joseph Arul Raj and Linglong Qian and Zina Ibrahim", "abstract": "  Secondary research use of Electronic Health Records (EHRs) is often hampered\nby the abundance of missing data in this valuable resource. Missingness in EHRs\noccurs naturally as a result of the data recording practices during routine\nclinical care, but handling it is crucial to the precision of medical analysis\nand the decision-making that follows. The literature contains a variety of\nimputation methodologies based on deep neural networks. Those aim to overcome\nthe dynamic, heterogeneous and multivariate missingness patterns of EHRs, which\ncannot be handled by classical and statistical imputation methods. However, all\nexisting deep imputation methods rely on end-to-end pipelines that incorporate\nboth imputation and downstream analyses, e.g. classification. This coupling\nmakes it difficult to assess the quality of imputation and takes away the\nflexibility of re-using the imputer for a different task. Furthermore, most\nend-to-end deep architectures tend to use complex networks to perform the\ndownstream task, in addition to the already sophisticated deep imputation\nnetwork. We, therefore ask if the high performance reported in the literature\nis due to the imputer or the classifier and further ask if an optimised\nstate-of-the-art imputer is used, a simpler classifier can achieve comparable\nperformance. This paper explores the development of a modular, deep\nlearning-based imputation and classification pipeline, specifically built to\nleverage the capabilities of state-of-the-art imputation models for downstream\nclassification tasks. Such a modular approach enables a) objective assessment\nof the quality of the imputer and classifier independently, and b) enables the\nexploration of the performance of simpler classification architectures using an\noptimised imputer.\n", "link": "http://arxiv.org/abs/2411.03941v1", "date": "2024-11-06", "relevancy": 2.056, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5397}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4964}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20--%20a%20Transfer%20Learning%20approach&body=Title%3A%20Fine-tuning%20--%20a%20Transfer%20Learning%20approach%0AAuthor%3A%20Joseph%20Arul%20Raj%20and%20Linglong%20Qian%20and%20Zina%20Ibrahim%0AAbstract%3A%20%20%20Secondary%20research%20use%20of%20Electronic%20Health%20Records%20%28EHRs%29%20is%20often%20hampered%0Aby%20the%20abundance%20of%20missing%20data%20in%20this%20valuable%20resource.%20Missingness%20in%20EHRs%0Aoccurs%20naturally%20as%20a%20result%20of%20the%20data%20recording%20practices%20during%20routine%0Aclinical%20care%2C%20but%20handling%20it%20is%20crucial%20to%20the%20precision%20of%20medical%20analysis%0Aand%20the%20decision-making%20that%20follows.%20The%20literature%20contains%20a%20variety%20of%0Aimputation%20methodologies%20based%20on%20deep%20neural%20networks.%20Those%20aim%20to%20overcome%0Athe%20dynamic%2C%20heterogeneous%20and%20multivariate%20missingness%20patterns%20of%20EHRs%2C%20which%0Acannot%20be%20handled%20by%20classical%20and%20statistical%20imputation%20methods.%20However%2C%20all%0Aexisting%20deep%20imputation%20methods%20rely%20on%20end-to-end%20pipelines%20that%20incorporate%0Aboth%20imputation%20and%20downstream%20analyses%2C%20e.g.%20classification.%20This%20coupling%0Amakes%20it%20difficult%20to%20assess%20the%20quality%20of%20imputation%20and%20takes%20away%20the%0Aflexibility%20of%20re-using%20the%20imputer%20for%20a%20different%20task.%20Furthermore%2C%20most%0Aend-to-end%20deep%20architectures%20tend%20to%20use%20complex%20networks%20to%20perform%20the%0Adownstream%20task%2C%20in%20addition%20to%20the%20already%20sophisticated%20deep%20imputation%0Anetwork.%20We%2C%20therefore%20ask%20if%20the%20high%20performance%20reported%20in%20the%20literature%0Ais%20due%20to%20the%20imputer%20or%20the%20classifier%20and%20further%20ask%20if%20an%20optimised%0Astate-of-the-art%20imputer%20is%20used%2C%20a%20simpler%20classifier%20can%20achieve%20comparable%0Aperformance.%20This%20paper%20explores%20the%20development%20of%20a%20modular%2C%20deep%0Alearning-based%20imputation%20and%20classification%20pipeline%2C%20specifically%20built%20to%0Aleverage%20the%20capabilities%20of%20state-of-the-art%20imputation%20models%20for%20downstream%0Aclassification%20tasks.%20Such%20a%20modular%20approach%20enables%20a%29%20objective%20assessment%0Aof%20the%20quality%20of%20the%20imputer%20and%20classifier%20independently%2C%20and%20b%29%20enables%20the%0Aexploration%20of%20the%20performance%20of%20simpler%20classification%20architectures%20using%20an%0Aoptimised%20imputer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tuning%2520--%2520a%2520Transfer%2520Learning%2520approach%26entry.906535625%3DJoseph%2520Arul%2520Raj%2520and%2520Linglong%2520Qian%2520and%2520Zina%2520Ibrahim%26entry.1292438233%3D%2520%2520Secondary%2520research%2520use%2520of%2520Electronic%2520Health%2520Records%2520%2528EHRs%2529%2520is%2520often%2520hampered%250Aby%2520the%2520abundance%2520of%2520missing%2520data%2520in%2520this%2520valuable%2520resource.%2520Missingness%2520in%2520EHRs%250Aoccurs%2520naturally%2520as%2520a%2520result%2520of%2520the%2520data%2520recording%2520practices%2520during%2520routine%250Aclinical%2520care%252C%2520but%2520handling%2520it%2520is%2520crucial%2520to%2520the%2520precision%2520of%2520medical%2520analysis%250Aand%2520the%2520decision-making%2520that%2520follows.%2520The%2520literature%2520contains%2520a%2520variety%2520of%250Aimputation%2520methodologies%2520based%2520on%2520deep%2520neural%2520networks.%2520Those%2520aim%2520to%2520overcome%250Athe%2520dynamic%252C%2520heterogeneous%2520and%2520multivariate%2520missingness%2520patterns%2520of%2520EHRs%252C%2520which%250Acannot%2520be%2520handled%2520by%2520classical%2520and%2520statistical%2520imputation%2520methods.%2520However%252C%2520all%250Aexisting%2520deep%2520imputation%2520methods%2520rely%2520on%2520end-to-end%2520pipelines%2520that%2520incorporate%250Aboth%2520imputation%2520and%2520downstream%2520analyses%252C%2520e.g.%2520classification.%2520This%2520coupling%250Amakes%2520it%2520difficult%2520to%2520assess%2520the%2520quality%2520of%2520imputation%2520and%2520takes%2520away%2520the%250Aflexibility%2520of%2520re-using%2520the%2520imputer%2520for%2520a%2520different%2520task.%2520Furthermore%252C%2520most%250Aend-to-end%2520deep%2520architectures%2520tend%2520to%2520use%2520complex%2520networks%2520to%2520perform%2520the%250Adownstream%2520task%252C%2520in%2520addition%2520to%2520the%2520already%2520sophisticated%2520deep%2520imputation%250Anetwork.%2520We%252C%2520therefore%2520ask%2520if%2520the%2520high%2520performance%2520reported%2520in%2520the%2520literature%250Ais%2520due%2520to%2520the%2520imputer%2520or%2520the%2520classifier%2520and%2520further%2520ask%2520if%2520an%2520optimised%250Astate-of-the-art%2520imputer%2520is%2520used%252C%2520a%2520simpler%2520classifier%2520can%2520achieve%2520comparable%250Aperformance.%2520This%2520paper%2520explores%2520the%2520development%2520of%2520a%2520modular%252C%2520deep%250Alearning-based%2520imputation%2520and%2520classification%2520pipeline%252C%2520specifically%2520built%2520to%250Aleverage%2520the%2520capabilities%2520of%2520state-of-the-art%2520imputation%2520models%2520for%2520downstream%250Aclassification%2520tasks.%2520Such%2520a%2520modular%2520approach%2520enables%2520a%2529%2520objective%2520assessment%250Aof%2520the%2520quality%2520of%2520the%2520imputer%2520and%2520classifier%2520independently%252C%2520and%2520b%2529%2520enables%2520the%250Aexploration%2520of%2520the%2520performance%2520of%2520simpler%2520classification%2520architectures%2520using%2520an%250Aoptimised%2520imputer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20--%20a%20Transfer%20Learning%20approach&entry.906535625=Joseph%20Arul%20Raj%20and%20Linglong%20Qian%20and%20Zina%20Ibrahim&entry.1292438233=%20%20Secondary%20research%20use%20of%20Electronic%20Health%20Records%20%28EHRs%29%20is%20often%20hampered%0Aby%20the%20abundance%20of%20missing%20data%20in%20this%20valuable%20resource.%20Missingness%20in%20EHRs%0Aoccurs%20naturally%20as%20a%20result%20of%20the%20data%20recording%20practices%20during%20routine%0Aclinical%20care%2C%20but%20handling%20it%20is%20crucial%20to%20the%20precision%20of%20medical%20analysis%0Aand%20the%20decision-making%20that%20follows.%20The%20literature%20contains%20a%20variety%20of%0Aimputation%20methodologies%20based%20on%20deep%20neural%20networks.%20Those%20aim%20to%20overcome%0Athe%20dynamic%2C%20heterogeneous%20and%20multivariate%20missingness%20patterns%20of%20EHRs%2C%20which%0Acannot%20be%20handled%20by%20classical%20and%20statistical%20imputation%20methods.%20However%2C%20all%0Aexisting%20deep%20imputation%20methods%20rely%20on%20end-to-end%20pipelines%20that%20incorporate%0Aboth%20imputation%20and%20downstream%20analyses%2C%20e.g.%20classification.%20This%20coupling%0Amakes%20it%20difficult%20to%20assess%20the%20quality%20of%20imputation%20and%20takes%20away%20the%0Aflexibility%20of%20re-using%20the%20imputer%20for%20a%20different%20task.%20Furthermore%2C%20most%0Aend-to-end%20deep%20architectures%20tend%20to%20use%20complex%20networks%20to%20perform%20the%0Adownstream%20task%2C%20in%20addition%20to%20the%20already%20sophisticated%20deep%20imputation%0Anetwork.%20We%2C%20therefore%20ask%20if%20the%20high%20performance%20reported%20in%20the%20literature%0Ais%20due%20to%20the%20imputer%20or%20the%20classifier%20and%20further%20ask%20if%20an%20optimised%0Astate-of-the-art%20imputer%20is%20used%2C%20a%20simpler%20classifier%20can%20achieve%20comparable%0Aperformance.%20This%20paper%20explores%20the%20development%20of%20a%20modular%2C%20deep%0Alearning-based%20imputation%20and%20classification%20pipeline%2C%20specifically%20built%20to%0Aleverage%20the%20capabilities%20of%20state-of-the-art%20imputation%20models%20for%20downstream%0Aclassification%20tasks.%20Such%20a%20modular%20approach%20enables%20a%29%20objective%20assessment%0Aof%20the%20quality%20of%20the%20imputer%20and%20classifier%20independently%2C%20and%20b%29%20enables%20the%0Aexploration%20of%20the%20performance%20of%20simpler%20classification%20architectures%20using%20an%0Aoptimised%20imputer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03941v1&entry.124074799=Read"},
{"title": "ATM: Improving Model Merging by Alternating Tuning and Merging", "author": "Luca Zhou and Daniele Solombrino and Donato Crisostomi and Maria Sofia Bucarelli and Fabrizio Silvestri and Emanuele Rodol\u00e0", "abstract": "  Model merging has recently emerged as a cost-efficient paradigm for\nmulti-task learning. Among current approaches, task arithmetic stands out for\nits simplicity and effectiveness. In this paper, we motivate the effectiveness\nof task vectors by linking them to multi-task gradients. We show that in a\nsingle-epoch scenario, task vectors are mathematically equivalent to the\ngradients obtained via gradient descent in a multi-task setting, and still\napproximate these gradients in subsequent epochs. Furthermore, we show that\ntask vectors perform optimally when equality is maintained, and their\neffectiveness is largely driven by the first epoch's gradient. Building on this\ninsight, we propose viewing model merging as a single step in an iterative\nprocess that Alternates between Tuning and Merging (ATM). This method acts as a\nbridge between model merging and multi-task gradient descent, achieving\nstate-of-the-art results with the same data and computational requirements. We\nextensively evaluate ATM across diverse settings, achieving up to 20% higher\naccuracy in computer vision and NLP tasks, compared to the best baselines.\nFinally, we provide both empirical and theoretical support for its\neffectiveness, demonstrating increased orthogonality between task vectors and\nproving that ATM minimizes an upper bound on the loss obtained by jointly\nfinetuning all tasks.\n", "link": "http://arxiv.org/abs/2411.03055v2", "date": "2024-11-06", "relevancy": 2.0558, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5249}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5215}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging&body=Title%3A%20ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging%0AAuthor%3A%20Luca%20Zhou%20and%20Daniele%20Solombrino%20and%20Donato%20Crisostomi%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Model%20merging%20has%20recently%20emerged%20as%20a%20cost-efficient%20paradigm%20for%0Amulti-task%20learning.%20Among%20current%20approaches%2C%20task%20arithmetic%20stands%20out%20for%0Aits%20simplicity%20and%20effectiveness.%20In%20this%20paper%2C%20we%20motivate%20the%20effectiveness%0Aof%20task%20vectors%20by%20linking%20them%20to%20multi-task%20gradients.%20We%20show%20that%20in%20a%0Asingle-epoch%20scenario%2C%20task%20vectors%20are%20mathematically%20equivalent%20to%20the%0Agradients%20obtained%20via%20gradient%20descent%20in%20a%20multi-task%20setting%2C%20and%20still%0Aapproximate%20these%20gradients%20in%20subsequent%20epochs.%20Furthermore%2C%20we%20show%20that%0Atask%20vectors%20perform%20optimally%20when%20equality%20is%20maintained%2C%20and%20their%0Aeffectiveness%20is%20largely%20driven%20by%20the%20first%20epoch%27s%20gradient.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20viewing%20model%20merging%20as%20a%20single%20step%20in%20an%20iterative%0Aprocess%20that%20Alternates%20between%20Tuning%20and%20Merging%20%28ATM%29.%20This%20method%20acts%20as%20a%0Abridge%20between%20model%20merging%20and%20multi-task%20gradient%20descent%2C%20achieving%0Astate-of-the-art%20results%20with%20the%20same%20data%20and%20computational%20requirements.%20We%0Aextensively%20evaluate%20ATM%20across%20diverse%20settings%2C%20achieving%20up%20to%2020%25%20higher%0Aaccuracy%20in%20computer%20vision%20and%20NLP%20tasks%2C%20compared%20to%20the%20best%20baselines.%0AFinally%2C%20we%20provide%20both%20empirical%20and%20theoretical%20support%20for%20its%0Aeffectiveness%2C%20demonstrating%20increased%20orthogonality%20between%20task%20vectors%20and%0Aproving%20that%20ATM%20minimizes%20an%20upper%20bound%20on%20the%20loss%20obtained%20by%20jointly%0Afinetuning%20all%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATM%253A%2520Improving%2520Model%2520Merging%2520by%2520Alternating%2520Tuning%2520and%2520Merging%26entry.906535625%3DLuca%2520Zhou%2520and%2520Daniele%2520Solombrino%2520and%2520Donato%2520Crisostomi%2520and%2520Maria%2520Sofia%2520Bucarelli%2520and%2520Fabrizio%2520Silvestri%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520Model%2520merging%2520has%2520recently%2520emerged%2520as%2520a%2520cost-efficient%2520paradigm%2520for%250Amulti-task%2520learning.%2520Among%2520current%2520approaches%252C%2520task%2520arithmetic%2520stands%2520out%2520for%250Aits%2520simplicity%2520and%2520effectiveness.%2520In%2520this%2520paper%252C%2520we%2520motivate%2520the%2520effectiveness%250Aof%2520task%2520vectors%2520by%2520linking%2520them%2520to%2520multi-task%2520gradients.%2520We%2520show%2520that%2520in%2520a%250Asingle-epoch%2520scenario%252C%2520task%2520vectors%2520are%2520mathematically%2520equivalent%2520to%2520the%250Agradients%2520obtained%2520via%2520gradient%2520descent%2520in%2520a%2520multi-task%2520setting%252C%2520and%2520still%250Aapproximate%2520these%2520gradients%2520in%2520subsequent%2520epochs.%2520Furthermore%252C%2520we%2520show%2520that%250Atask%2520vectors%2520perform%2520optimally%2520when%2520equality%2520is%2520maintained%252C%2520and%2520their%250Aeffectiveness%2520is%2520largely%2520driven%2520by%2520the%2520first%2520epoch%2527s%2520gradient.%2520Building%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520viewing%2520model%2520merging%2520as%2520a%2520single%2520step%2520in%2520an%2520iterative%250Aprocess%2520that%2520Alternates%2520between%2520Tuning%2520and%2520Merging%2520%2528ATM%2529.%2520This%2520method%2520acts%2520as%2520a%250Abridge%2520between%2520model%2520merging%2520and%2520multi-task%2520gradient%2520descent%252C%2520achieving%250Astate-of-the-art%2520results%2520with%2520the%2520same%2520data%2520and%2520computational%2520requirements.%2520We%250Aextensively%2520evaluate%2520ATM%2520across%2520diverse%2520settings%252C%2520achieving%2520up%2520to%252020%2525%2520higher%250Aaccuracy%2520in%2520computer%2520vision%2520and%2520NLP%2520tasks%252C%2520compared%2520to%2520the%2520best%2520baselines.%250AFinally%252C%2520we%2520provide%2520both%2520empirical%2520and%2520theoretical%2520support%2520for%2520its%250Aeffectiveness%252C%2520demonstrating%2520increased%2520orthogonality%2520between%2520task%2520vectors%2520and%250Aproving%2520that%2520ATM%2520minimizes%2520an%2520upper%2520bound%2520on%2520the%2520loss%2520obtained%2520by%2520jointly%250Afinetuning%2520all%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATM%3A%20Improving%20Model%20Merging%20by%20Alternating%20Tuning%20and%20Merging&entry.906535625=Luca%20Zhou%20and%20Daniele%20Solombrino%20and%20Donato%20Crisostomi%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Model%20merging%20has%20recently%20emerged%20as%20a%20cost-efficient%20paradigm%20for%0Amulti-task%20learning.%20Among%20current%20approaches%2C%20task%20arithmetic%20stands%20out%20for%0Aits%20simplicity%20and%20effectiveness.%20In%20this%20paper%2C%20we%20motivate%20the%20effectiveness%0Aof%20task%20vectors%20by%20linking%20them%20to%20multi-task%20gradients.%20We%20show%20that%20in%20a%0Asingle-epoch%20scenario%2C%20task%20vectors%20are%20mathematically%20equivalent%20to%20the%0Agradients%20obtained%20via%20gradient%20descent%20in%20a%20multi-task%20setting%2C%20and%20still%0Aapproximate%20these%20gradients%20in%20subsequent%20epochs.%20Furthermore%2C%20we%20show%20that%0Atask%20vectors%20perform%20optimally%20when%20equality%20is%20maintained%2C%20and%20their%0Aeffectiveness%20is%20largely%20driven%20by%20the%20first%20epoch%27s%20gradient.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20viewing%20model%20merging%20as%20a%20single%20step%20in%20an%20iterative%0Aprocess%20that%20Alternates%20between%20Tuning%20and%20Merging%20%28ATM%29.%20This%20method%20acts%20as%20a%0Abridge%20between%20model%20merging%20and%20multi-task%20gradient%20descent%2C%20achieving%0Astate-of-the-art%20results%20with%20the%20same%20data%20and%20computational%20requirements.%20We%0Aextensively%20evaluate%20ATM%20across%20diverse%20settings%2C%20achieving%20up%20to%2020%25%20higher%0Aaccuracy%20in%20computer%20vision%20and%20NLP%20tasks%2C%20compared%20to%20the%20best%20baselines.%0AFinally%2C%20we%20provide%20both%20empirical%20and%20theoretical%20support%20for%20its%0Aeffectiveness%2C%20demonstrating%20increased%20orthogonality%20between%20task%20vectors%20and%0Aproving%20that%20ATM%20minimizes%20an%20upper%20bound%20on%20the%20loss%20obtained%20by%20jointly%0Afinetuning%20all%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03055v2&entry.124074799=Read"},
{"title": "LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics", "author": "Thomas Robert and Mher Safaryan and Ionut-Vlad Modoranu and Dan Alistarh", "abstract": "  We introduce LDAdam, a memory-efficient optimizer for training large models,\nthat performs adaptive optimization steps within lower dimensional subspaces,\nwhile consistently exploring the full parameter space during training. This\nstrategy keeps the optimizer's memory footprint to a fraction of the model\nsize. LDAdam relies on a new projection-aware update rule for the optimizer\nstates that allows for transitioning between subspaces, i.e., estimation of the\nstatistics of the projected gradients. To mitigate the errors due to low-rank\nprojection, LDAdam integrates a new generalized error feedback mechanism, which\nexplicitly accounts for both gradient and optimizer state compression. We prove\nthe convergence of LDAdam under standard assumptions, and show that LDAdam\nallows for accurate and efficient fine-tuning and pre-training of language\nmodels.\n", "link": "http://arxiv.org/abs/2410.16103v2", "date": "2024-11-06", "relevancy": 2.0548, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5181}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5126}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LDAdam%3A%20Adaptive%20Optimization%20from%20Low-Dimensional%20Gradient%20Statistics&body=Title%3A%20LDAdam%3A%20Adaptive%20Optimization%20from%20Low-Dimensional%20Gradient%20Statistics%0AAuthor%3A%20Thomas%20Robert%20and%20Mher%20Safaryan%20and%20Ionut-Vlad%20Modoranu%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20We%20introduce%20LDAdam%2C%20a%20memory-efficient%20optimizer%20for%20training%20large%20models%2C%0Athat%20performs%20adaptive%20optimization%20steps%20within%20lower%20dimensional%20subspaces%2C%0Awhile%20consistently%20exploring%20the%20full%20parameter%20space%20during%20training.%20This%0Astrategy%20keeps%20the%20optimizer%27s%20memory%20footprint%20to%20a%20fraction%20of%20the%20model%0Asize.%20LDAdam%20relies%20on%20a%20new%20projection-aware%20update%20rule%20for%20the%20optimizer%0Astates%20that%20allows%20for%20transitioning%20between%20subspaces%2C%20i.e.%2C%20estimation%20of%20the%0Astatistics%20of%20the%20projected%20gradients.%20To%20mitigate%20the%20errors%20due%20to%20low-rank%0Aprojection%2C%20LDAdam%20integrates%20a%20new%20generalized%20error%20feedback%20mechanism%2C%20which%0Aexplicitly%20accounts%20for%20both%20gradient%20and%20optimizer%20state%20compression.%20We%20prove%0Athe%20convergence%20of%20LDAdam%20under%20standard%20assumptions%2C%20and%20show%20that%20LDAdam%0Aallows%20for%20accurate%20and%20efficient%20fine-tuning%20and%20pre-training%20of%20language%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLDAdam%253A%2520Adaptive%2520Optimization%2520from%2520Low-Dimensional%2520Gradient%2520Statistics%26entry.906535625%3DThomas%2520Robert%2520and%2520Mher%2520Safaryan%2520and%2520Ionut-Vlad%2520Modoranu%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520We%2520introduce%2520LDAdam%252C%2520a%2520memory-efficient%2520optimizer%2520for%2520training%2520large%2520models%252C%250Athat%2520performs%2520adaptive%2520optimization%2520steps%2520within%2520lower%2520dimensional%2520subspaces%252C%250Awhile%2520consistently%2520exploring%2520the%2520full%2520parameter%2520space%2520during%2520training.%2520This%250Astrategy%2520keeps%2520the%2520optimizer%2527s%2520memory%2520footprint%2520to%2520a%2520fraction%2520of%2520the%2520model%250Asize.%2520LDAdam%2520relies%2520on%2520a%2520new%2520projection-aware%2520update%2520rule%2520for%2520the%2520optimizer%250Astates%2520that%2520allows%2520for%2520transitioning%2520between%2520subspaces%252C%2520i.e.%252C%2520estimation%2520of%2520the%250Astatistics%2520of%2520the%2520projected%2520gradients.%2520To%2520mitigate%2520the%2520errors%2520due%2520to%2520low-rank%250Aprojection%252C%2520LDAdam%2520integrates%2520a%2520new%2520generalized%2520error%2520feedback%2520mechanism%252C%2520which%250Aexplicitly%2520accounts%2520for%2520both%2520gradient%2520and%2520optimizer%2520state%2520compression.%2520We%2520prove%250Athe%2520convergence%2520of%2520LDAdam%2520under%2520standard%2520assumptions%252C%2520and%2520show%2520that%2520LDAdam%250Aallows%2520for%2520accurate%2520and%2520efficient%2520fine-tuning%2520and%2520pre-training%2520of%2520language%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LDAdam%3A%20Adaptive%20Optimization%20from%20Low-Dimensional%20Gradient%20Statistics&entry.906535625=Thomas%20Robert%20and%20Mher%20Safaryan%20and%20Ionut-Vlad%20Modoranu%20and%20Dan%20Alistarh&entry.1292438233=%20%20We%20introduce%20LDAdam%2C%20a%20memory-efficient%20optimizer%20for%20training%20large%20models%2C%0Athat%20performs%20adaptive%20optimization%20steps%20within%20lower%20dimensional%20subspaces%2C%0Awhile%20consistently%20exploring%20the%20full%20parameter%20space%20during%20training.%20This%0Astrategy%20keeps%20the%20optimizer%27s%20memory%20footprint%20to%20a%20fraction%20of%20the%20model%0Asize.%20LDAdam%20relies%20on%20a%20new%20projection-aware%20update%20rule%20for%20the%20optimizer%0Astates%20that%20allows%20for%20transitioning%20between%20subspaces%2C%20i.e.%2C%20estimation%20of%20the%0Astatistics%20of%20the%20projected%20gradients.%20To%20mitigate%20the%20errors%20due%20to%20low-rank%0Aprojection%2C%20LDAdam%20integrates%20a%20new%20generalized%20error%20feedback%20mechanism%2C%20which%0Aexplicitly%20accounts%20for%20both%20gradient%20and%20optimizer%20state%20compression.%20We%20prove%0Athe%20convergence%20of%20LDAdam%20under%20standard%20assumptions%2C%20and%20show%20that%20LDAdam%0Aallows%20for%20accurate%20and%20efficient%20fine-tuning%20and%20pre-training%20of%20language%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16103v2&entry.124074799=Read"},
{"title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in\n  Retrieval-Augmented Generation", "author": "Yuhang Liu and Xueyu Hu and Shengyu Zhang and Jingyuan Chen and Fan Wu and Fei Wu", "abstract": "  Retrieval-Augmented Generation (RAG) has proven to be an effective method for\nmitigating hallucination issues inherent in large language models (LLMs).\nPrevious approaches typically train retrievers based on semantic similarity,\nlacking optimization for RAG. More recent works have proposed aligning\nretrievers with the preference signals of LLMs. However, these preference\nsignals are often difficult for dense retrievers, which typically have weaker\nlanguage capabilities, to understand and learn effectively. Drawing inspiration\nfrom pedagogical theories like Guided Discovery Learning, we propose a novel\nframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the\nlanguage capabilities of LLMs to construct examples from a more granular,\ninformation-centric perspective to guide the learning of retrievers.\nSpecifically, our method utilizes LLMs to construct easy-to-understand examples\nfrom samples where the retriever performs poorly, focusing on three learning\nobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,\nand purity. These examples serve as scaffolding to ultimately align the\nretriever with the LLM's preferences. Furthermore, we employ a dual curriculum\nlearning strategy and leverage the reciprocal feedback between LLM and\nretriever to further enhance the performance of the RAG system. A series of\nexperiments demonstrate that our proposed framework enhances the performance of\nRAG systems equipped with different retrievers and is applicable to various\nLLMs.\n", "link": "http://arxiv.org/abs/2411.03957v1", "date": "2024-11-06", "relevancy": 2.0482, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5321}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5039}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Guidance%20for%20Retrievers%3A%20Leveraging%20LLMs%27%20Feedback%20in%0A%20%20Retrieval-Augmented%20Generation&body=Title%3A%20Fine-Grained%20Guidance%20for%20Retrievers%3A%20Leveraging%20LLMs%27%20Feedback%20in%0A%20%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Yuhang%20Liu%20and%20Xueyu%20Hu%20and%20Shengyu%20Zhang%20and%20Jingyuan%20Chen%20and%20Fan%20Wu%20and%20Fei%20Wu%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20proven%20to%20be%20an%20effective%20method%20for%0Amitigating%20hallucination%20issues%20inherent%20in%20large%20language%20models%20%28LLMs%29.%0APrevious%20approaches%20typically%20train%20retrievers%20based%20on%20semantic%20similarity%2C%0Alacking%20optimization%20for%20RAG.%20More%20recent%20works%20have%20proposed%20aligning%0Aretrievers%20with%20the%20preference%20signals%20of%20LLMs.%20However%2C%20these%20preference%0Asignals%20are%20often%20difficult%20for%20dense%20retrievers%2C%20which%20typically%20have%20weaker%0Alanguage%20capabilities%2C%20to%20understand%20and%20learn%20effectively.%20Drawing%20inspiration%0Afrom%20pedagogical%20theories%20like%20Guided%20Discovery%20Learning%2C%20we%20propose%20a%20novel%0Aframework%2C%20FiGRet%20%28Fine-grained%20Guidance%20for%20Retrievers%29%2C%20which%20leverages%20the%0Alanguage%20capabilities%20of%20LLMs%20to%20construct%20examples%20from%20a%20more%20granular%2C%0Ainformation-centric%20perspective%20to%20guide%20the%20learning%20of%20retrievers.%0ASpecifically%2C%20our%20method%20utilizes%20LLMs%20to%20construct%20easy-to-understand%20examples%0Afrom%20samples%20where%20the%20retriever%20performs%20poorly%2C%20focusing%20on%20three%20learning%0Aobjectives%20highly%20relevant%20to%20the%20RAG%20scenario%3A%20relevance%2C%20comprehensiveness%2C%0Aand%20purity.%20These%20examples%20serve%20as%20scaffolding%20to%20ultimately%20align%20the%0Aretriever%20with%20the%20LLM%27s%20preferences.%20Furthermore%2C%20we%20employ%20a%20dual%20curriculum%0Alearning%20strategy%20and%20leverage%20the%20reciprocal%20feedback%20between%20LLM%20and%0Aretriever%20to%20further%20enhance%20the%20performance%20of%20the%20RAG%20system.%20A%20series%20of%0Aexperiments%20demonstrate%20that%20our%20proposed%20framework%20enhances%20the%20performance%20of%0ARAG%20systems%20equipped%20with%20different%20retrievers%20and%20is%20applicable%20to%20various%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Guidance%2520for%2520Retrievers%253A%2520Leveraging%2520LLMs%2527%2520Feedback%2520in%250A%2520%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DYuhang%2520Liu%2520and%2520Xueyu%2520Hu%2520and%2520Shengyu%2520Zhang%2520and%2520Jingyuan%2520Chen%2520and%2520Fan%2520Wu%2520and%2520Fei%2520Wu%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520proven%2520to%2520be%2520an%2520effective%2520method%2520for%250Amitigating%2520hallucination%2520issues%2520inherent%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%250APrevious%2520approaches%2520typically%2520train%2520retrievers%2520based%2520on%2520semantic%2520similarity%252C%250Alacking%2520optimization%2520for%2520RAG.%2520More%2520recent%2520works%2520have%2520proposed%2520aligning%250Aretrievers%2520with%2520the%2520preference%2520signals%2520of%2520LLMs.%2520However%252C%2520these%2520preference%250Asignals%2520are%2520often%2520difficult%2520for%2520dense%2520retrievers%252C%2520which%2520typically%2520have%2520weaker%250Alanguage%2520capabilities%252C%2520to%2520understand%2520and%2520learn%2520effectively.%2520Drawing%2520inspiration%250Afrom%2520pedagogical%2520theories%2520like%2520Guided%2520Discovery%2520Learning%252C%2520we%2520propose%2520a%2520novel%250Aframework%252C%2520FiGRet%2520%2528Fine-grained%2520Guidance%2520for%2520Retrievers%2529%252C%2520which%2520leverages%2520the%250Alanguage%2520capabilities%2520of%2520LLMs%2520to%2520construct%2520examples%2520from%2520a%2520more%2520granular%252C%250Ainformation-centric%2520perspective%2520to%2520guide%2520the%2520learning%2520of%2520retrievers.%250ASpecifically%252C%2520our%2520method%2520utilizes%2520LLMs%2520to%2520construct%2520easy-to-understand%2520examples%250Afrom%2520samples%2520where%2520the%2520retriever%2520performs%2520poorly%252C%2520focusing%2520on%2520three%2520learning%250Aobjectives%2520highly%2520relevant%2520to%2520the%2520RAG%2520scenario%253A%2520relevance%252C%2520comprehensiveness%252C%250Aand%2520purity.%2520These%2520examples%2520serve%2520as%2520scaffolding%2520to%2520ultimately%2520align%2520the%250Aretriever%2520with%2520the%2520LLM%2527s%2520preferences.%2520Furthermore%252C%2520we%2520employ%2520a%2520dual%2520curriculum%250Alearning%2520strategy%2520and%2520leverage%2520the%2520reciprocal%2520feedback%2520between%2520LLM%2520and%250Aretriever%2520to%2520further%2520enhance%2520the%2520performance%2520of%2520the%2520RAG%2520system.%2520A%2520series%2520of%250Aexperiments%2520demonstrate%2520that%2520our%2520proposed%2520framework%2520enhances%2520the%2520performance%2520of%250ARAG%2520systems%2520equipped%2520with%2520different%2520retrievers%2520and%2520is%2520applicable%2520to%2520various%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Guidance%20for%20Retrievers%3A%20Leveraging%20LLMs%27%20Feedback%20in%0A%20%20Retrieval-Augmented%20Generation&entry.906535625=Yuhang%20Liu%20and%20Xueyu%20Hu%20and%20Shengyu%20Zhang%20and%20Jingyuan%20Chen%20and%20Fan%20Wu%20and%20Fei%20Wu&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20proven%20to%20be%20an%20effective%20method%20for%0Amitigating%20hallucination%20issues%20inherent%20in%20large%20language%20models%20%28LLMs%29.%0APrevious%20approaches%20typically%20train%20retrievers%20based%20on%20semantic%20similarity%2C%0Alacking%20optimization%20for%20RAG.%20More%20recent%20works%20have%20proposed%20aligning%0Aretrievers%20with%20the%20preference%20signals%20of%20LLMs.%20However%2C%20these%20preference%0Asignals%20are%20often%20difficult%20for%20dense%20retrievers%2C%20which%20typically%20have%20weaker%0Alanguage%20capabilities%2C%20to%20understand%20and%20learn%20effectively.%20Drawing%20inspiration%0Afrom%20pedagogical%20theories%20like%20Guided%20Discovery%20Learning%2C%20we%20propose%20a%20novel%0Aframework%2C%20FiGRet%20%28Fine-grained%20Guidance%20for%20Retrievers%29%2C%20which%20leverages%20the%0Alanguage%20capabilities%20of%20LLMs%20to%20construct%20examples%20from%20a%20more%20granular%2C%0Ainformation-centric%20perspective%20to%20guide%20the%20learning%20of%20retrievers.%0ASpecifically%2C%20our%20method%20utilizes%20LLMs%20to%20construct%20easy-to-understand%20examples%0Afrom%20samples%20where%20the%20retriever%20performs%20poorly%2C%20focusing%20on%20three%20learning%0Aobjectives%20highly%20relevant%20to%20the%20RAG%20scenario%3A%20relevance%2C%20comprehensiveness%2C%0Aand%20purity.%20These%20examples%20serve%20as%20scaffolding%20to%20ultimately%20align%20the%0Aretriever%20with%20the%20LLM%27s%20preferences.%20Furthermore%2C%20we%20employ%20a%20dual%20curriculum%0Alearning%20strategy%20and%20leverage%20the%20reciprocal%20feedback%20between%20LLM%20and%0Aretriever%20to%20further%20enhance%20the%20performance%20of%20the%20RAG%20system.%20A%20series%20of%0Aexperiments%20demonstrate%20that%20our%20proposed%20framework%20enhances%20the%20performance%20of%0ARAG%20systems%20equipped%20with%20different%20retrievers%20and%20is%20applicable%20to%20various%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03957v1&entry.124074799=Read"},
{"title": "TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific\n  Energy-Based Models", "author": "Andrei Margeloiu and Xiangjian Jiang and Nikola Simidjievski and Mateja Jamnik", "abstract": "  Data collection is often difficult in critical fields such as medicine,\nphysics, and chemistry. As a result, classification methods usually perform\npoorly with these small datasets, leading to weak predictive performance.\nIncreasing the training set with additional synthetic data, similar to data\naugmentation in images, is commonly believed to improve downstream\nclassification performance. However, current tabular generative methods that\nlearn either the joint distribution $ p(\\mathbf{x}, y) $ or the\nclass-conditional distribution $ p(\\mathbf{x} \\mid y) $ often overfit on small\ndatasets, resulting in poor-quality synthetic data, usually worsening\nclassification performance compared to using real data alone. To solve these\nchallenges, we introduce TabEBM, a novel class-conditional generative method\nusing Energy-Based Models (EBMs). Unlike existing methods that use a shared\nmodel to approximate all class-conditional densities, our key innovation is to\ncreate distinct EBM generative models for each class, each modelling its\nclass-specific data distribution individually. This approach creates robust\nenergy landscapes, even in ambiguous class distributions. Our experiments show\nthat TabEBM generates synthetic data with higher quality and better statistical\nfidelity than existing methods. When used for data augmentation, our synthetic\ndata consistently improves the classification performance across diverse\ndatasets of various sizes, especially small ones. Code is available at\nhttps://github.com/andreimargeloiu/TabEBM.\n", "link": "http://arxiv.org/abs/2409.16118v3", "date": "2024-11-06", "relevancy": 2.0236, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5319}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5043}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabEBM%3A%20A%20Tabular%20Data%20Augmentation%20Method%20with%20Distinct%20Class-Specific%0A%20%20Energy-Based%20Models&body=Title%3A%20TabEBM%3A%20A%20Tabular%20Data%20Augmentation%20Method%20with%20Distinct%20Class-Specific%0A%20%20Energy-Based%20Models%0AAuthor%3A%20Andrei%20Margeloiu%20and%20Xiangjian%20Jiang%20and%20Nikola%20Simidjievski%20and%20Mateja%20Jamnik%0AAbstract%3A%20%20%20Data%20collection%20is%20often%20difficult%20in%20critical%20fields%20such%20as%20medicine%2C%0Aphysics%2C%20and%20chemistry.%20As%20a%20result%2C%20classification%20methods%20usually%20perform%0Apoorly%20with%20these%20small%20datasets%2C%20leading%20to%20weak%20predictive%20performance.%0AIncreasing%20the%20training%20set%20with%20additional%20synthetic%20data%2C%20similar%20to%20data%0Aaugmentation%20in%20images%2C%20is%20commonly%20believed%20to%20improve%20downstream%0Aclassification%20performance.%20However%2C%20current%20tabular%20generative%20methods%20that%0Alearn%20either%20the%20joint%20distribution%20%24%20p%28%5Cmathbf%7Bx%7D%2C%20y%29%20%24%20or%20the%0Aclass-conditional%20distribution%20%24%20p%28%5Cmathbf%7Bx%7D%20%5Cmid%20y%29%20%24%20often%20overfit%20on%20small%0Adatasets%2C%20resulting%20in%20poor-quality%20synthetic%20data%2C%20usually%20worsening%0Aclassification%20performance%20compared%20to%20using%20real%20data%20alone.%20To%20solve%20these%0Achallenges%2C%20we%20introduce%20TabEBM%2C%20a%20novel%20class-conditional%20generative%20method%0Ausing%20Energy-Based%20Models%20%28EBMs%29.%20Unlike%20existing%20methods%20that%20use%20a%20shared%0Amodel%20to%20approximate%20all%20class-conditional%20densities%2C%20our%20key%20innovation%20is%20to%0Acreate%20distinct%20EBM%20generative%20models%20for%20each%20class%2C%20each%20modelling%20its%0Aclass-specific%20data%20distribution%20individually.%20This%20approach%20creates%20robust%0Aenergy%20landscapes%2C%20even%20in%20ambiguous%20class%20distributions.%20Our%20experiments%20show%0Athat%20TabEBM%20generates%20synthetic%20data%20with%20higher%20quality%20and%20better%20statistical%0Afidelity%20than%20existing%20methods.%20When%20used%20for%20data%20augmentation%2C%20our%20synthetic%0Adata%20consistently%20improves%20the%20classification%20performance%20across%20diverse%0Adatasets%20of%20various%20sizes%2C%20especially%20small%20ones.%20Code%20is%20available%20at%0Ahttps%3A//github.com/andreimargeloiu/TabEBM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16118v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabEBM%253A%2520A%2520Tabular%2520Data%2520Augmentation%2520Method%2520with%2520Distinct%2520Class-Specific%250A%2520%2520Energy-Based%2520Models%26entry.906535625%3DAndrei%2520Margeloiu%2520and%2520Xiangjian%2520Jiang%2520and%2520Nikola%2520Simidjievski%2520and%2520Mateja%2520Jamnik%26entry.1292438233%3D%2520%2520Data%2520collection%2520is%2520often%2520difficult%2520in%2520critical%2520fields%2520such%2520as%2520medicine%252C%250Aphysics%252C%2520and%2520chemistry.%2520As%2520a%2520result%252C%2520classification%2520methods%2520usually%2520perform%250Apoorly%2520with%2520these%2520small%2520datasets%252C%2520leading%2520to%2520weak%2520predictive%2520performance.%250AIncreasing%2520the%2520training%2520set%2520with%2520additional%2520synthetic%2520data%252C%2520similar%2520to%2520data%250Aaugmentation%2520in%2520images%252C%2520is%2520commonly%2520believed%2520to%2520improve%2520downstream%250Aclassification%2520performance.%2520However%252C%2520current%2520tabular%2520generative%2520methods%2520that%250Alearn%2520either%2520the%2520joint%2520distribution%2520%2524%2520p%2528%255Cmathbf%257Bx%257D%252C%2520y%2529%2520%2524%2520or%2520the%250Aclass-conditional%2520distribution%2520%2524%2520p%2528%255Cmathbf%257Bx%257D%2520%255Cmid%2520y%2529%2520%2524%2520often%2520overfit%2520on%2520small%250Adatasets%252C%2520resulting%2520in%2520poor-quality%2520synthetic%2520data%252C%2520usually%2520worsening%250Aclassification%2520performance%2520compared%2520to%2520using%2520real%2520data%2520alone.%2520To%2520solve%2520these%250Achallenges%252C%2520we%2520introduce%2520TabEBM%252C%2520a%2520novel%2520class-conditional%2520generative%2520method%250Ausing%2520Energy-Based%2520Models%2520%2528EBMs%2529.%2520Unlike%2520existing%2520methods%2520that%2520use%2520a%2520shared%250Amodel%2520to%2520approximate%2520all%2520class-conditional%2520densities%252C%2520our%2520key%2520innovation%2520is%2520to%250Acreate%2520distinct%2520EBM%2520generative%2520models%2520for%2520each%2520class%252C%2520each%2520modelling%2520its%250Aclass-specific%2520data%2520distribution%2520individually.%2520This%2520approach%2520creates%2520robust%250Aenergy%2520landscapes%252C%2520even%2520in%2520ambiguous%2520class%2520distributions.%2520Our%2520experiments%2520show%250Athat%2520TabEBM%2520generates%2520synthetic%2520data%2520with%2520higher%2520quality%2520and%2520better%2520statistical%250Afidelity%2520than%2520existing%2520methods.%2520When%2520used%2520for%2520data%2520augmentation%252C%2520our%2520synthetic%250Adata%2520consistently%2520improves%2520the%2520classification%2520performance%2520across%2520diverse%250Adatasets%2520of%2520various%2520sizes%252C%2520especially%2520small%2520ones.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/andreimargeloiu/TabEBM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16118v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabEBM%3A%20A%20Tabular%20Data%20Augmentation%20Method%20with%20Distinct%20Class-Specific%0A%20%20Energy-Based%20Models&entry.906535625=Andrei%20Margeloiu%20and%20Xiangjian%20Jiang%20and%20Nikola%20Simidjievski%20and%20Mateja%20Jamnik&entry.1292438233=%20%20Data%20collection%20is%20often%20difficult%20in%20critical%20fields%20such%20as%20medicine%2C%0Aphysics%2C%20and%20chemistry.%20As%20a%20result%2C%20classification%20methods%20usually%20perform%0Apoorly%20with%20these%20small%20datasets%2C%20leading%20to%20weak%20predictive%20performance.%0AIncreasing%20the%20training%20set%20with%20additional%20synthetic%20data%2C%20similar%20to%20data%0Aaugmentation%20in%20images%2C%20is%20commonly%20believed%20to%20improve%20downstream%0Aclassification%20performance.%20However%2C%20current%20tabular%20generative%20methods%20that%0Alearn%20either%20the%20joint%20distribution%20%24%20p%28%5Cmathbf%7Bx%7D%2C%20y%29%20%24%20or%20the%0Aclass-conditional%20distribution%20%24%20p%28%5Cmathbf%7Bx%7D%20%5Cmid%20y%29%20%24%20often%20overfit%20on%20small%0Adatasets%2C%20resulting%20in%20poor-quality%20synthetic%20data%2C%20usually%20worsening%0Aclassification%20performance%20compared%20to%20using%20real%20data%20alone.%20To%20solve%20these%0Achallenges%2C%20we%20introduce%20TabEBM%2C%20a%20novel%20class-conditional%20generative%20method%0Ausing%20Energy-Based%20Models%20%28EBMs%29.%20Unlike%20existing%20methods%20that%20use%20a%20shared%0Amodel%20to%20approximate%20all%20class-conditional%20densities%2C%20our%20key%20innovation%20is%20to%0Acreate%20distinct%20EBM%20generative%20models%20for%20each%20class%2C%20each%20modelling%20its%0Aclass-specific%20data%20distribution%20individually.%20This%20approach%20creates%20robust%0Aenergy%20landscapes%2C%20even%20in%20ambiguous%20class%20distributions.%20Our%20experiments%20show%0Athat%20TabEBM%20generates%20synthetic%20data%20with%20higher%20quality%20and%20better%20statistical%0Afidelity%20than%20existing%20methods.%20When%20used%20for%20data%20augmentation%2C%20our%20synthetic%0Adata%20consistently%20improves%20the%20classification%20performance%20across%20diverse%0Adatasets%20of%20various%20sizes%2C%20especially%20small%20ones.%20Code%20is%20available%20at%0Ahttps%3A//github.com/andreimargeloiu/TabEBM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16118v3&entry.124074799=Read"},
{"title": "Calibrating for the Future:Enhancing Calorimeter Longevity with Deep\n  Learning", "author": "S. Ali and A. S. Ryzhikov and D. A. Derkach and F. D. Ratnikov and V. O. Bocharnikov", "abstract": "  In the realm of high-energy physics, the longevity of calorimeters is\nparamount. Our research introduces a deep learning strategy to refine the\ncalibration process of calorimeters used in particle physics experiments. We\ndevelop a Wasserstein GAN inspired methodology that adeptly calibrates the\nmisalignment in calorimeter data due to aging or other factors. Leveraging the\nWasserstein distance for loss calculation, this innovative approach requires a\nsignificantly lower number of events and resources to achieve high precision,\nminimizing absolute errors effectively. Our work extends the operational\nlifespan of calorimeters, thereby ensuring the accuracy and reliability of data\nin the long term, and is particularly beneficial for experiments where data\nintegrity is crucial for scientific discovery.\n", "link": "http://arxiv.org/abs/2411.03891v1", "date": "2024-11-06", "relevancy": 2.0197, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5114}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5044}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrating%20for%20the%20Future%3AEnhancing%20Calorimeter%20Longevity%20with%20Deep%0A%20%20Learning&body=Title%3A%20Calibrating%20for%20the%20Future%3AEnhancing%20Calorimeter%20Longevity%20with%20Deep%0A%20%20Learning%0AAuthor%3A%20S.%20Ali%20and%20A.%20S.%20Ryzhikov%20and%20D.%20A.%20Derkach%20and%20F.%20D.%20Ratnikov%20and%20V.%20O.%20Bocharnikov%0AAbstract%3A%20%20%20In%20the%20realm%20of%20high-energy%20physics%2C%20the%20longevity%20of%20calorimeters%20is%0Aparamount.%20Our%20research%20introduces%20a%20deep%20learning%20strategy%20to%20refine%20the%0Acalibration%20process%20of%20calorimeters%20used%20in%20particle%20physics%20experiments.%20We%0Adevelop%20a%20Wasserstein%20GAN%20inspired%20methodology%20that%20adeptly%20calibrates%20the%0Amisalignment%20in%20calorimeter%20data%20due%20to%20aging%20or%20other%20factors.%20Leveraging%20the%0AWasserstein%20distance%20for%20loss%20calculation%2C%20this%20innovative%20approach%20requires%20a%0Asignificantly%20lower%20number%20of%20events%20and%20resources%20to%20achieve%20high%20precision%2C%0Aminimizing%20absolute%20errors%20effectively.%20Our%20work%20extends%20the%20operational%0Alifespan%20of%20calorimeters%2C%20thereby%20ensuring%20the%20accuracy%20and%20reliability%20of%20data%0Ain%20the%20long%20term%2C%20and%20is%20particularly%20beneficial%20for%20experiments%20where%20data%0Aintegrity%20is%20crucial%20for%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrating%2520for%2520the%2520Future%253AEnhancing%2520Calorimeter%2520Longevity%2520with%2520Deep%250A%2520%2520Learning%26entry.906535625%3DS.%2520Ali%2520and%2520A.%2520S.%2520Ryzhikov%2520and%2520D.%2520A.%2520Derkach%2520and%2520F.%2520D.%2520Ratnikov%2520and%2520V.%2520O.%2520Bocharnikov%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520high-energy%2520physics%252C%2520the%2520longevity%2520of%2520calorimeters%2520is%250Aparamount.%2520Our%2520research%2520introduces%2520a%2520deep%2520learning%2520strategy%2520to%2520refine%2520the%250Acalibration%2520process%2520of%2520calorimeters%2520used%2520in%2520particle%2520physics%2520experiments.%2520We%250Adevelop%2520a%2520Wasserstein%2520GAN%2520inspired%2520methodology%2520that%2520adeptly%2520calibrates%2520the%250Amisalignment%2520in%2520calorimeter%2520data%2520due%2520to%2520aging%2520or%2520other%2520factors.%2520Leveraging%2520the%250AWasserstein%2520distance%2520for%2520loss%2520calculation%252C%2520this%2520innovative%2520approach%2520requires%2520a%250Asignificantly%2520lower%2520number%2520of%2520events%2520and%2520resources%2520to%2520achieve%2520high%2520precision%252C%250Aminimizing%2520absolute%2520errors%2520effectively.%2520Our%2520work%2520extends%2520the%2520operational%250Alifespan%2520of%2520calorimeters%252C%2520thereby%2520ensuring%2520the%2520accuracy%2520and%2520reliability%2520of%2520data%250Ain%2520the%2520long%2520term%252C%2520and%2520is%2520particularly%2520beneficial%2520for%2520experiments%2520where%2520data%250Aintegrity%2520is%2520crucial%2520for%2520scientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrating%20for%20the%20Future%3AEnhancing%20Calorimeter%20Longevity%20with%20Deep%0A%20%20Learning&entry.906535625=S.%20Ali%20and%20A.%20S.%20Ryzhikov%20and%20D.%20A.%20Derkach%20and%20F.%20D.%20Ratnikov%20and%20V.%20O.%20Bocharnikov&entry.1292438233=%20%20In%20the%20realm%20of%20high-energy%20physics%2C%20the%20longevity%20of%20calorimeters%20is%0Aparamount.%20Our%20research%20introduces%20a%20deep%20learning%20strategy%20to%20refine%20the%0Acalibration%20process%20of%20calorimeters%20used%20in%20particle%20physics%20experiments.%20We%0Adevelop%20a%20Wasserstein%20GAN%20inspired%20methodology%20that%20adeptly%20calibrates%20the%0Amisalignment%20in%20calorimeter%20data%20due%20to%20aging%20or%20other%20factors.%20Leveraging%20the%0AWasserstein%20distance%20for%20loss%20calculation%2C%20this%20innovative%20approach%20requires%20a%0Asignificantly%20lower%20number%20of%20events%20and%20resources%20to%20achieve%20high%20precision%2C%0Aminimizing%20absolute%20errors%20effectively.%20Our%20work%20extends%20the%20operational%0Alifespan%20of%20calorimeters%2C%20thereby%20ensuring%20the%20accuracy%20and%20reliability%20of%20data%0Ain%20the%20long%20term%2C%20and%20is%20particularly%20beneficial%20for%20experiments%20where%20data%0Aintegrity%20is%20crucial%20for%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03891v1&entry.124074799=Read"},
{"title": "Can Custom Models Learn In-Context? An Exploration of Hybrid\n  Architecture Performance on In-Context Learning Tasks", "author": "Ryan Campbell and Nelson Lojo and Kesava Viswanadha and Christoffer Grondal Tryggestad and Derrick Han Sun and Sriteja Vijapurapu and August Rolfsen and Anant Sahai", "abstract": "  In-Context Learning (ICL) is a phenomenon where task learning occurs through\na prompt sequence without the necessity of parameter updates. ICL in\nMulti-Headed Attention (MHA) with absolute positional embedding has been the\nfocus of more study than other sequence model varieties. We examine\nimplications of architectural differences between GPT-2 and LLaMa as well as\nLlaMa and Mamba. We extend work done by Garg et al. (2022) and Park et al.\n(2024) to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models - examining the\ninterplay between sequence transformation blocks and regressive performance\nin-context. We note that certain architectural changes cause degraded training\nefficiency/ICL accuracy by converging to suboptimal predictors or converging\nslower. We also find certain hybrids showing optimistic performance\nimprovements, informing potential future ICL-focused architecture\nmodifications. Additionally, we propose the \"ICL regression score\", a scalar\nmetric describing a model's whole performance on a specific task. Compute\nlimitations impose restrictions on our architecture-space, training duration,\nnumber of training runs, function class complexity, and benchmark complexity.\nTo foster reproducible and extensible research, we provide a typed, modular,\nand extensible Python package on which we run all experiments.\n", "link": "http://arxiv.org/abs/2411.03945v1", "date": "2024-11-06", "relevancy": 2.0173, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5274}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Custom%20Models%20Learn%20In-Context%3F%20An%20Exploration%20of%20Hybrid%0A%20%20Architecture%20Performance%20on%20In-Context%20Learning%20Tasks&body=Title%3A%20Can%20Custom%20Models%20Learn%20In-Context%3F%20An%20Exploration%20of%20Hybrid%0A%20%20Architecture%20Performance%20on%20In-Context%20Learning%20Tasks%0AAuthor%3A%20Ryan%20Campbell%20and%20Nelson%20Lojo%20and%20Kesava%20Viswanadha%20and%20Christoffer%20Grondal%20Tryggestad%20and%20Derrick%20Han%20Sun%20and%20Sriteja%20Vijapurapu%20and%20August%20Rolfsen%20and%20Anant%20Sahai%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20is%20a%20phenomenon%20where%20task%20learning%20occurs%20through%0Aa%20prompt%20sequence%20without%20the%20necessity%20of%20parameter%20updates.%20ICL%20in%0AMulti-Headed%20Attention%20%28MHA%29%20with%20absolute%20positional%20embedding%20has%20been%20the%0Afocus%20of%20more%20study%20than%20other%20sequence%20model%20varieties.%20We%20examine%0Aimplications%20of%20architectural%20differences%20between%20GPT-2%20and%20LLaMa%20as%20well%20as%0ALlaMa%20and%20Mamba.%20We%20extend%20work%20done%20by%20Garg%20et%20al.%20%282022%29%20and%20Park%20et%20al.%0A%282024%29%20to%20GPT-2/LLaMa%20hybrid%20and%20LLaMa/Mamba%20hybrid%20models%20-%20examining%20the%0Ainterplay%20between%20sequence%20transformation%20blocks%20and%20regressive%20performance%0Ain-context.%20We%20note%20that%20certain%20architectural%20changes%20cause%20degraded%20training%0Aefficiency/ICL%20accuracy%20by%20converging%20to%20suboptimal%20predictors%20or%20converging%0Aslower.%20We%20also%20find%20certain%20hybrids%20showing%20optimistic%20performance%0Aimprovements%2C%20informing%20potential%20future%20ICL-focused%20architecture%0Amodifications.%20Additionally%2C%20we%20propose%20the%20%22ICL%20regression%20score%22%2C%20a%20scalar%0Ametric%20describing%20a%20model%27s%20whole%20performance%20on%20a%20specific%20task.%20Compute%0Alimitations%20impose%20restrictions%20on%20our%20architecture-space%2C%20training%20duration%2C%0Anumber%20of%20training%20runs%2C%20function%20class%20complexity%2C%20and%20benchmark%20complexity.%0ATo%20foster%20reproducible%20and%20extensible%20research%2C%20we%20provide%20a%20typed%2C%20modular%2C%0Aand%20extensible%20Python%20package%20on%20which%20we%20run%20all%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Custom%2520Models%2520Learn%2520In-Context%253F%2520An%2520Exploration%2520of%2520Hybrid%250A%2520%2520Architecture%2520Performance%2520on%2520In-Context%2520Learning%2520Tasks%26entry.906535625%3DRyan%2520Campbell%2520and%2520Nelson%2520Lojo%2520and%2520Kesava%2520Viswanadha%2520and%2520Christoffer%2520Grondal%2520Tryggestad%2520and%2520Derrick%2520Han%2520Sun%2520and%2520Sriteja%2520Vijapurapu%2520and%2520August%2520Rolfsen%2520and%2520Anant%2520Sahai%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520is%2520a%2520phenomenon%2520where%2520task%2520learning%2520occurs%2520through%250Aa%2520prompt%2520sequence%2520without%2520the%2520necessity%2520of%2520parameter%2520updates.%2520ICL%2520in%250AMulti-Headed%2520Attention%2520%2528MHA%2529%2520with%2520absolute%2520positional%2520embedding%2520has%2520been%2520the%250Afocus%2520of%2520more%2520study%2520than%2520other%2520sequence%2520model%2520varieties.%2520We%2520examine%250Aimplications%2520of%2520architectural%2520differences%2520between%2520GPT-2%2520and%2520LLaMa%2520as%2520well%2520as%250ALlaMa%2520and%2520Mamba.%2520We%2520extend%2520work%2520done%2520by%2520Garg%2520et%2520al.%2520%25282022%2529%2520and%2520Park%2520et%2520al.%250A%25282024%2529%2520to%2520GPT-2/LLaMa%2520hybrid%2520and%2520LLaMa/Mamba%2520hybrid%2520models%2520-%2520examining%2520the%250Ainterplay%2520between%2520sequence%2520transformation%2520blocks%2520and%2520regressive%2520performance%250Ain-context.%2520We%2520note%2520that%2520certain%2520architectural%2520changes%2520cause%2520degraded%2520training%250Aefficiency/ICL%2520accuracy%2520by%2520converging%2520to%2520suboptimal%2520predictors%2520or%2520converging%250Aslower.%2520We%2520also%2520find%2520certain%2520hybrids%2520showing%2520optimistic%2520performance%250Aimprovements%252C%2520informing%2520potential%2520future%2520ICL-focused%2520architecture%250Amodifications.%2520Additionally%252C%2520we%2520propose%2520the%2520%2522ICL%2520regression%2520score%2522%252C%2520a%2520scalar%250Ametric%2520describing%2520a%2520model%2527s%2520whole%2520performance%2520on%2520a%2520specific%2520task.%2520Compute%250Alimitations%2520impose%2520restrictions%2520on%2520our%2520architecture-space%252C%2520training%2520duration%252C%250Anumber%2520of%2520training%2520runs%252C%2520function%2520class%2520complexity%252C%2520and%2520benchmark%2520complexity.%250ATo%2520foster%2520reproducible%2520and%2520extensible%2520research%252C%2520we%2520provide%2520a%2520typed%252C%2520modular%252C%250Aand%2520extensible%2520Python%2520package%2520on%2520which%2520we%2520run%2520all%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Custom%20Models%20Learn%20In-Context%3F%20An%20Exploration%20of%20Hybrid%0A%20%20Architecture%20Performance%20on%20In-Context%20Learning%20Tasks&entry.906535625=Ryan%20Campbell%20and%20Nelson%20Lojo%20and%20Kesava%20Viswanadha%20and%20Christoffer%20Grondal%20Tryggestad%20and%20Derrick%20Han%20Sun%20and%20Sriteja%20Vijapurapu%20and%20August%20Rolfsen%20and%20Anant%20Sahai&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20is%20a%20phenomenon%20where%20task%20learning%20occurs%20through%0Aa%20prompt%20sequence%20without%20the%20necessity%20of%20parameter%20updates.%20ICL%20in%0AMulti-Headed%20Attention%20%28MHA%29%20with%20absolute%20positional%20embedding%20has%20been%20the%0Afocus%20of%20more%20study%20than%20other%20sequence%20model%20varieties.%20We%20examine%0Aimplications%20of%20architectural%20differences%20between%20GPT-2%20and%20LLaMa%20as%20well%20as%0ALlaMa%20and%20Mamba.%20We%20extend%20work%20done%20by%20Garg%20et%20al.%20%282022%29%20and%20Park%20et%20al.%0A%282024%29%20to%20GPT-2/LLaMa%20hybrid%20and%20LLaMa/Mamba%20hybrid%20models%20-%20examining%20the%0Ainterplay%20between%20sequence%20transformation%20blocks%20and%20regressive%20performance%0Ain-context.%20We%20note%20that%20certain%20architectural%20changes%20cause%20degraded%20training%0Aefficiency/ICL%20accuracy%20by%20converging%20to%20suboptimal%20predictors%20or%20converging%0Aslower.%20We%20also%20find%20certain%20hybrids%20showing%20optimistic%20performance%0Aimprovements%2C%20informing%20potential%20future%20ICL-focused%20architecture%0Amodifications.%20Additionally%2C%20we%20propose%20the%20%22ICL%20regression%20score%22%2C%20a%20scalar%0Ametric%20describing%20a%20model%27s%20whole%20performance%20on%20a%20specific%20task.%20Compute%0Alimitations%20impose%20restrictions%20on%20our%20architecture-space%2C%20training%20duration%2C%0Anumber%20of%20training%20runs%2C%20function%20class%20complexity%2C%20and%20benchmark%20complexity.%0ATo%20foster%20reproducible%20and%20extensible%20research%2C%20we%20provide%20a%20typed%2C%20modular%2C%0Aand%20extensible%20Python%20package%20on%20which%20we%20run%20all%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03945v1&entry.124074799=Read"},
{"title": "Deep neural network-based detection of counterfeit products from\n  smartphone images", "author": "Hugo Garcia-Cotte and Dorra Mellouli and Abdul Rehman and Li Wang and David G. Stork", "abstract": "  Counterfeit products such as drugs and vaccines as well as luxury items such\nas high-fashion handbags, watches, jewelry, garments, and cosmetics, represent\nsignificant direct losses of revenue to legitimate manufacturers and vendors,\nas well as indirect costs to societies at large. We present the world's first\npurely computer-vision-based system to combat such counterfeiting-one that does\nnot require special security tags or other alterations to the products or\nmodifications to supply chain tracking. Our deep neural network system shows\nhigh accuracy on branded garments from our first manufacturer tested (99.71%\nafter 3.06% rejections) using images captured under natural, weakly controlled\nconditions, such as in retail stores, customs checkpoints, warehouses, and\noutdoors. Our system, suitably transfer trained on a small number of fake and\ngenuine articles, should find application in additional product categories as\nwell, for example fashion accessories, perfume boxes, medicines, and more.\n", "link": "http://arxiv.org/abs/2410.05969v2", "date": "2024-11-06", "relevancy": 2.0153, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.509}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5047}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20neural%20network-based%20detection%20of%20counterfeit%20products%20from%0A%20%20smartphone%20images&body=Title%3A%20Deep%20neural%20network-based%20detection%20of%20counterfeit%20products%20from%0A%20%20smartphone%20images%0AAuthor%3A%20Hugo%20Garcia-Cotte%20and%20Dorra%20Mellouli%20and%20Abdul%20Rehman%20and%20Li%20Wang%20and%20David%20G.%20Stork%0AAbstract%3A%20%20%20Counterfeit%20products%20such%20as%20drugs%20and%20vaccines%20as%20well%20as%20luxury%20items%20such%0Aas%20high-fashion%20handbags%2C%20watches%2C%20jewelry%2C%20garments%2C%20and%20cosmetics%2C%20represent%0Asignificant%20direct%20losses%20of%20revenue%20to%20legitimate%20manufacturers%20and%20vendors%2C%0Aas%20well%20as%20indirect%20costs%20to%20societies%20at%20large.%20We%20present%20the%20world%27s%20first%0Apurely%20computer-vision-based%20system%20to%20combat%20such%20counterfeiting-one%20that%20does%0Anot%20require%20special%20security%20tags%20or%20other%20alterations%20to%20the%20products%20or%0Amodifications%20to%20supply%20chain%20tracking.%20Our%20deep%20neural%20network%20system%20shows%0Ahigh%20accuracy%20on%20branded%20garments%20from%20our%20first%20manufacturer%20tested%20%2899.71%25%0Aafter%203.06%25%20rejections%29%20using%20images%20captured%20under%20natural%2C%20weakly%20controlled%0Aconditions%2C%20such%20as%20in%20retail%20stores%2C%20customs%20checkpoints%2C%20warehouses%2C%20and%0Aoutdoors.%20Our%20system%2C%20suitably%20transfer%20trained%20on%20a%20small%20number%20of%20fake%20and%0Agenuine%20articles%2C%20should%20find%20application%20in%20additional%20product%20categories%20as%0Awell%2C%20for%20example%20fashion%20accessories%2C%20perfume%20boxes%2C%20medicines%2C%20and%20more.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520neural%2520network-based%2520detection%2520of%2520counterfeit%2520products%2520from%250A%2520%2520smartphone%2520images%26entry.906535625%3DHugo%2520Garcia-Cotte%2520and%2520Dorra%2520Mellouli%2520and%2520Abdul%2520Rehman%2520and%2520Li%2520Wang%2520and%2520David%2520G.%2520Stork%26entry.1292438233%3D%2520%2520Counterfeit%2520products%2520such%2520as%2520drugs%2520and%2520vaccines%2520as%2520well%2520as%2520luxury%2520items%2520such%250Aas%2520high-fashion%2520handbags%252C%2520watches%252C%2520jewelry%252C%2520garments%252C%2520and%2520cosmetics%252C%2520represent%250Asignificant%2520direct%2520losses%2520of%2520revenue%2520to%2520legitimate%2520manufacturers%2520and%2520vendors%252C%250Aas%2520well%2520as%2520indirect%2520costs%2520to%2520societies%2520at%2520large.%2520We%2520present%2520the%2520world%2527s%2520first%250Apurely%2520computer-vision-based%2520system%2520to%2520combat%2520such%2520counterfeiting-one%2520that%2520does%250Anot%2520require%2520special%2520security%2520tags%2520or%2520other%2520alterations%2520to%2520the%2520products%2520or%250Amodifications%2520to%2520supply%2520chain%2520tracking.%2520Our%2520deep%2520neural%2520network%2520system%2520shows%250Ahigh%2520accuracy%2520on%2520branded%2520garments%2520from%2520our%2520first%2520manufacturer%2520tested%2520%252899.71%2525%250Aafter%25203.06%2525%2520rejections%2529%2520using%2520images%2520captured%2520under%2520natural%252C%2520weakly%2520controlled%250Aconditions%252C%2520such%2520as%2520in%2520retail%2520stores%252C%2520customs%2520checkpoints%252C%2520warehouses%252C%2520and%250Aoutdoors.%2520Our%2520system%252C%2520suitably%2520transfer%2520trained%2520on%2520a%2520small%2520number%2520of%2520fake%2520and%250Agenuine%2520articles%252C%2520should%2520find%2520application%2520in%2520additional%2520product%2520categories%2520as%250Awell%252C%2520for%2520example%2520fashion%2520accessories%252C%2520perfume%2520boxes%252C%2520medicines%252C%2520and%2520more.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20neural%20network-based%20detection%20of%20counterfeit%20products%20from%0A%20%20smartphone%20images&entry.906535625=Hugo%20Garcia-Cotte%20and%20Dorra%20Mellouli%20and%20Abdul%20Rehman%20and%20Li%20Wang%20and%20David%20G.%20Stork&entry.1292438233=%20%20Counterfeit%20products%20such%20as%20drugs%20and%20vaccines%20as%20well%20as%20luxury%20items%20such%0Aas%20high-fashion%20handbags%2C%20watches%2C%20jewelry%2C%20garments%2C%20and%20cosmetics%2C%20represent%0Asignificant%20direct%20losses%20of%20revenue%20to%20legitimate%20manufacturers%20and%20vendors%2C%0Aas%20well%20as%20indirect%20costs%20to%20societies%20at%20large.%20We%20present%20the%20world%27s%20first%0Apurely%20computer-vision-based%20system%20to%20combat%20such%20counterfeiting-one%20that%20does%0Anot%20require%20special%20security%20tags%20or%20other%20alterations%20to%20the%20products%20or%0Amodifications%20to%20supply%20chain%20tracking.%20Our%20deep%20neural%20network%20system%20shows%0Ahigh%20accuracy%20on%20branded%20garments%20from%20our%20first%20manufacturer%20tested%20%2899.71%25%0Aafter%203.06%25%20rejections%29%20using%20images%20captured%20under%20natural%2C%20weakly%20controlled%0Aconditions%2C%20such%20as%20in%20retail%20stores%2C%20customs%20checkpoints%2C%20warehouses%2C%20and%0Aoutdoors.%20Our%20system%2C%20suitably%20transfer%20trained%20on%20a%20small%20number%20of%20fake%20and%0Agenuine%20articles%2C%20should%20find%20application%20in%20additional%20product%20categories%20as%0Awell%2C%20for%20example%20fashion%20accessories%2C%20perfume%20boxes%2C%20medicines%2C%20and%20more.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05969v2&entry.124074799=Read"},
{"title": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering", "author": "Laura Cabello and Carmen Martin-Turrero and Uchenna Akujuobi and Anders S\u00f8gaard and Carlos Bobed", "abstract": "  Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.\n", "link": "http://arxiv.org/abs/2411.03883v1", "date": "2024-11-06", "relevancy": 2.0076, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEG%3A%20Medical%20Knowledge-Augmented%20Large%20Language%20Models%20for%20Question%0A%20%20Answering&body=Title%3A%20MEG%3A%20Medical%20Knowledge-Augmented%20Large%20Language%20Models%20for%20Question%0A%20%20Answering%0AAuthor%3A%20Laura%20Cabello%20and%20Carmen%20Martin-Turrero%20and%20Uchenna%20Akujuobi%20and%20Anders%20S%C3%B8gaard%20and%20Carlos%20Bobed%0AAbstract%3A%20%20%20Question%20answering%20is%20a%20natural%20language%20understanding%20task%20that%20involves%0Areasoning%20over%20both%20explicit%20context%20and%20unstated%2C%20relevant%20domain%20knowledge.%0ALarge%20language%20models%20%28LLMs%29%2C%20which%20underpin%20most%20contemporary%20question%0Aanswering%20systems%2C%20struggle%20to%20induce%20how%20concepts%20relate%20in%20specialized%0Adomains%20such%20as%20medicine.%20Existing%20medical%20LLMs%20are%20also%20costly%20to%20train.%20In%0Athis%20work%2C%20we%20present%20MEG%2C%20a%20parameter-efficient%20approach%20for%20medical%0Aknowledge-augmented%20LLMs.%20MEG%20uses%20a%20lightweight%20mapping%20network%20to%20integrate%0Agraph%20embeddings%20into%20the%20LLM%2C%20enabling%20it%20to%20leverage%20external%20knowledge%20in%20a%0Acost-effective%20way.%20We%20evaluate%20our%20method%20on%20four%20popular%20medical%0Amultiple-choice%20datasets%20and%20show%20that%20LLMs%20greatly%20benefit%20from%20the%20factual%0Agrounding%20provided%20by%20knowledge%20graph%20embeddings.%20MEG%20attains%20an%20average%20of%0A%2B10.2%25%20accuracy%20over%20the%20Mistral-Instruct%20baseline%2C%20and%20%2B6.7%25%20over%20specialized%0Amodels%20like%20BioMistral.%20We%20also%20show%20results%20based%20on%20Llama-3.%20Finally%2C%20we%20show%0Athat%20MEG%27s%20performance%20remains%20robust%20to%20the%20choice%20of%20graph%20encoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEG%253A%2520Medical%2520Knowledge-Augmented%2520Large%2520Language%2520Models%2520for%2520Question%250A%2520%2520Answering%26entry.906535625%3DLaura%2520Cabello%2520and%2520Carmen%2520Martin-Turrero%2520and%2520Uchenna%2520Akujuobi%2520and%2520Anders%2520S%25C3%25B8gaard%2520and%2520Carlos%2520Bobed%26entry.1292438233%3D%2520%2520Question%2520answering%2520is%2520a%2520natural%2520language%2520understanding%2520task%2520that%2520involves%250Areasoning%2520over%2520both%2520explicit%2520context%2520and%2520unstated%252C%2520relevant%2520domain%2520knowledge.%250ALarge%2520language%2520models%2520%2528LLMs%2529%252C%2520which%2520underpin%2520most%2520contemporary%2520question%250Aanswering%2520systems%252C%2520struggle%2520to%2520induce%2520how%2520concepts%2520relate%2520in%2520specialized%250Adomains%2520such%2520as%2520medicine.%2520Existing%2520medical%2520LLMs%2520are%2520also%2520costly%2520to%2520train.%2520In%250Athis%2520work%252C%2520we%2520present%2520MEG%252C%2520a%2520parameter-efficient%2520approach%2520for%2520medical%250Aknowledge-augmented%2520LLMs.%2520MEG%2520uses%2520a%2520lightweight%2520mapping%2520network%2520to%2520integrate%250Agraph%2520embeddings%2520into%2520the%2520LLM%252C%2520enabling%2520it%2520to%2520leverage%2520external%2520knowledge%2520in%2520a%250Acost-effective%2520way.%2520We%2520evaluate%2520our%2520method%2520on%2520four%2520popular%2520medical%250Amultiple-choice%2520datasets%2520and%2520show%2520that%2520LLMs%2520greatly%2520benefit%2520from%2520the%2520factual%250Agrounding%2520provided%2520by%2520knowledge%2520graph%2520embeddings.%2520MEG%2520attains%2520an%2520average%2520of%250A%252B10.2%2525%2520accuracy%2520over%2520the%2520Mistral-Instruct%2520baseline%252C%2520and%2520%252B6.7%2525%2520over%2520specialized%250Amodels%2520like%2520BioMistral.%2520We%2520also%2520show%2520results%2520based%2520on%2520Llama-3.%2520Finally%252C%2520we%2520show%250Athat%2520MEG%2527s%2520performance%2520remains%2520robust%2520to%2520the%2520choice%2520of%2520graph%2520encoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEG%3A%20Medical%20Knowledge-Augmented%20Large%20Language%20Models%20for%20Question%0A%20%20Answering&entry.906535625=Laura%20Cabello%20and%20Carmen%20Martin-Turrero%20and%20Uchenna%20Akujuobi%20and%20Anders%20S%C3%B8gaard%20and%20Carlos%20Bobed&entry.1292438233=%20%20Question%20answering%20is%20a%20natural%20language%20understanding%20task%20that%20involves%0Areasoning%20over%20both%20explicit%20context%20and%20unstated%2C%20relevant%20domain%20knowledge.%0ALarge%20language%20models%20%28LLMs%29%2C%20which%20underpin%20most%20contemporary%20question%0Aanswering%20systems%2C%20struggle%20to%20induce%20how%20concepts%20relate%20in%20specialized%0Adomains%20such%20as%20medicine.%20Existing%20medical%20LLMs%20are%20also%20costly%20to%20train.%20In%0Athis%20work%2C%20we%20present%20MEG%2C%20a%20parameter-efficient%20approach%20for%20medical%0Aknowledge-augmented%20LLMs.%20MEG%20uses%20a%20lightweight%20mapping%20network%20to%20integrate%0Agraph%20embeddings%20into%20the%20LLM%2C%20enabling%20it%20to%20leverage%20external%20knowledge%20in%20a%0Acost-effective%20way.%20We%20evaluate%20our%20method%20on%20four%20popular%20medical%0Amultiple-choice%20datasets%20and%20show%20that%20LLMs%20greatly%20benefit%20from%20the%20factual%0Agrounding%20provided%20by%20knowledge%20graph%20embeddings.%20MEG%20attains%20an%20average%20of%0A%2B10.2%25%20accuracy%20over%20the%20Mistral-Instruct%20baseline%2C%20and%20%2B6.7%25%20over%20specialized%0Amodels%20like%20BioMistral.%20We%20also%20show%20results%20based%20on%20Llama-3.%20Finally%2C%20we%20show%0Athat%20MEG%27s%20performance%20remains%20robust%20to%20the%20choice%20of%20graph%20encoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03883v1&entry.124074799=Read"},
{"title": "BCDNet: A Fast Residual Neural Network For Invasive Ductal Carcinoma\n  Detection", "author": "Yujia Lin and Aiwei Lian and Mingyu Liao and Shuangjie Yuan", "abstract": "  It is of great significance to diagnose Invasive Ductal Carcinoma (IDC) in\nearly stage, which is the most common subtype of breast cancer. Although the\npowerful models in the Computer-Aided Diagnosis (CAD) systems provide promising\nresults, it is still difficult to integrate them into other medical devices or\nuse them without sufficient computation resource. In this paper, we propose\nBCDNet, which firstly upsamples the input image by the residual block and use\nsmaller convolutional block and a special MLP to learn features. BCDNet is\nproofed to effectively detect IDC in histopathological RGB images with an\naverage accuracy of 91.6% and reduce training consumption effectively compared\nto ResNet 50 and ViT-B-16.\n", "link": "http://arxiv.org/abs/2408.13800v3", "date": "2024-11-06", "relevancy": 2.004, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5354}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5153}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BCDNet%3A%20A%20Fast%20Residual%20Neural%20Network%20For%20Invasive%20Ductal%20Carcinoma%0A%20%20Detection&body=Title%3A%20BCDNet%3A%20A%20Fast%20Residual%20Neural%20Network%20For%20Invasive%20Ductal%20Carcinoma%0A%20%20Detection%0AAuthor%3A%20Yujia%20Lin%20and%20Aiwei%20Lian%20and%20Mingyu%20Liao%20and%20Shuangjie%20Yuan%0AAbstract%3A%20%20%20It%20is%20of%20great%20significance%20to%20diagnose%20Invasive%20Ductal%20Carcinoma%20%28IDC%29%20in%0Aearly%20stage%2C%20which%20is%20the%20most%20common%20subtype%20of%20breast%20cancer.%20Although%20the%0Apowerful%20models%20in%20the%20Computer-Aided%20Diagnosis%20%28CAD%29%20systems%20provide%20promising%0Aresults%2C%20it%20is%20still%20difficult%20to%20integrate%20them%20into%20other%20medical%20devices%20or%0Ause%20them%20without%20sufficient%20computation%20resource.%20In%20this%20paper%2C%20we%20propose%0ABCDNet%2C%20which%20firstly%20upsamples%20the%20input%20image%20by%20the%20residual%20block%20and%20use%0Asmaller%20convolutional%20block%20and%20a%20special%20MLP%20to%20learn%20features.%20BCDNet%20is%0Aproofed%20to%20effectively%20detect%20IDC%20in%20histopathological%20RGB%20images%20with%20an%0Aaverage%20accuracy%20of%2091.6%25%20and%20reduce%20training%20consumption%20effectively%20compared%0Ato%20ResNet%2050%20and%20ViT-B-16.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13800v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBCDNet%253A%2520A%2520Fast%2520Residual%2520Neural%2520Network%2520For%2520Invasive%2520Ductal%2520Carcinoma%250A%2520%2520Detection%26entry.906535625%3DYujia%2520Lin%2520and%2520Aiwei%2520Lian%2520and%2520Mingyu%2520Liao%2520and%2520Shuangjie%2520Yuan%26entry.1292438233%3D%2520%2520It%2520is%2520of%2520great%2520significance%2520to%2520diagnose%2520Invasive%2520Ductal%2520Carcinoma%2520%2528IDC%2529%2520in%250Aearly%2520stage%252C%2520which%2520is%2520the%2520most%2520common%2520subtype%2520of%2520breast%2520cancer.%2520Although%2520the%250Apowerful%2520models%2520in%2520the%2520Computer-Aided%2520Diagnosis%2520%2528CAD%2529%2520systems%2520provide%2520promising%250Aresults%252C%2520it%2520is%2520still%2520difficult%2520to%2520integrate%2520them%2520into%2520other%2520medical%2520devices%2520or%250Ause%2520them%2520without%2520sufficient%2520computation%2520resource.%2520In%2520this%2520paper%252C%2520we%2520propose%250ABCDNet%252C%2520which%2520firstly%2520upsamples%2520the%2520input%2520image%2520by%2520the%2520residual%2520block%2520and%2520use%250Asmaller%2520convolutional%2520block%2520and%2520a%2520special%2520MLP%2520to%2520learn%2520features.%2520BCDNet%2520is%250Aproofed%2520to%2520effectively%2520detect%2520IDC%2520in%2520histopathological%2520RGB%2520images%2520with%2520an%250Aaverage%2520accuracy%2520of%252091.6%2525%2520and%2520reduce%2520training%2520consumption%2520effectively%2520compared%250Ato%2520ResNet%252050%2520and%2520ViT-B-16.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13800v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BCDNet%3A%20A%20Fast%20Residual%20Neural%20Network%20For%20Invasive%20Ductal%20Carcinoma%0A%20%20Detection&entry.906535625=Yujia%20Lin%20and%20Aiwei%20Lian%20and%20Mingyu%20Liao%20and%20Shuangjie%20Yuan&entry.1292438233=%20%20It%20is%20of%20great%20significance%20to%20diagnose%20Invasive%20Ductal%20Carcinoma%20%28IDC%29%20in%0Aearly%20stage%2C%20which%20is%20the%20most%20common%20subtype%20of%20breast%20cancer.%20Although%20the%0Apowerful%20models%20in%20the%20Computer-Aided%20Diagnosis%20%28CAD%29%20systems%20provide%20promising%0Aresults%2C%20it%20is%20still%20difficult%20to%20integrate%20them%20into%20other%20medical%20devices%20or%0Ause%20them%20without%20sufficient%20computation%20resource.%20In%20this%20paper%2C%20we%20propose%0ABCDNet%2C%20which%20firstly%20upsamples%20the%20input%20image%20by%20the%20residual%20block%20and%20use%0Asmaller%20convolutional%20block%20and%20a%20special%20MLP%20to%20learn%20features.%20BCDNet%20is%0Aproofed%20to%20effectively%20detect%20IDC%20in%20histopathological%20RGB%20images%20with%20an%0Aaverage%20accuracy%20of%2091.6%25%20and%20reduce%20training%20consumption%20effectively%20compared%0Ato%20ResNet%2050%20and%20ViT-B-16.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13800v3&entry.124074799=Read"},
{"title": "Counterfactual Token Generation in Large Language Models", "author": "Ivi Chatzi and Nina Corvelo Benz and Eleni Straitouri and Stratis Tsirtsis and Manuel Gomez-Rodriguez", "abstract": "  \"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm\nof her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]\nLyra's eyes welled up with tears as she realized the bitter truth - she had\nsacrificed everything for fleeting riches, and lost the love of her crew, her\nfamily, and herself.\" Although this story, generated by a large language model,\nis captivating, one may wonder -- how would the story have unfolded if the\nmodel had chosen \"Captain Maeve\" as the protagonist instead? We cannot know.\nState-of-the-art large language models are stateless -- they maintain no\ninternal memory or state. Given a prompt, they generate a sequence of tokens as\nan output using an autoregressive process. As a consequence, they cannot reason\nabout counterfactual alternatives to tokens they have generated in the past. In\nthis work, our goal is to enhance them with this functionality. To this end, we\ndevelop a causal model of token generation that builds upon the Gumbel-Max\nstructural causal model. Our model allows any large language model to perform\ncounterfactual token generation at almost no cost in comparison with vanilla\ntoken generation, it is embarrassingly simple to implement, and it does not\nrequire any fine-tuning nor prompt engineering. We implement our model on Llama\n3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a\nquantitative analysis of counterfactually generated text. We conclude with a\ndemonstrative application of counterfactual token generation for bias\ndetection, unveiling interesting insights about the model of the world\nconstructed by large language models.\n", "link": "http://arxiv.org/abs/2409.17027v2", "date": "2024-11-06", "relevancy": 2.0013, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5164}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4889}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Token%20Generation%20in%20Large%20Language%20Models&body=Title%3A%20Counterfactual%20Token%20Generation%20in%20Large%20Language%20Models%0AAuthor%3A%20Ivi%20Chatzi%20and%20Nina%20Corvelo%20Benz%20and%20Eleni%20Straitouri%20and%20Stratis%20Tsirtsis%20and%20Manuel%20Gomez-Rodriguez%0AAbstract%3A%20%20%20%22Sure%2C%20I%20am%20happy%20to%20generate%20a%20story%20for%20you%3A%20Captain%20Lyra%20stood%20at%20the%20helm%0Aof%20her%20trusty%20ship%2C%20the%20Maelstrom%27s%20Fury%2C%20gazing%20out%20at%20the%20endless%20sea.%20%5B...%5D%0ALyra%27s%20eyes%20welled%20up%20with%20tears%20as%20she%20realized%20the%20bitter%20truth%20-%20she%20had%0Asacrificed%20everything%20for%20fleeting%20riches%2C%20and%20lost%20the%20love%20of%20her%20crew%2C%20her%0Afamily%2C%20and%20herself.%22%20Although%20this%20story%2C%20generated%20by%20a%20large%20language%20model%2C%0Ais%20captivating%2C%20one%20may%20wonder%20--%20how%20would%20the%20story%20have%20unfolded%20if%20the%0Amodel%20had%20chosen%20%22Captain%20Maeve%22%20as%20the%20protagonist%20instead%3F%20We%20cannot%20know.%0AState-of-the-art%20large%20language%20models%20are%20stateless%20--%20they%20maintain%20no%0Ainternal%20memory%20or%20state.%20Given%20a%20prompt%2C%20they%20generate%20a%20sequence%20of%20tokens%20as%0Aan%20output%20using%20an%20autoregressive%20process.%20As%20a%20consequence%2C%20they%20cannot%20reason%0Aabout%20counterfactual%20alternatives%20to%20tokens%20they%20have%20generated%20in%20the%20past.%20In%0Athis%20work%2C%20our%20goal%20is%20to%20enhance%20them%20with%20this%20functionality.%20To%20this%20end%2C%20we%0Adevelop%20a%20causal%20model%20of%20token%20generation%20that%20builds%20upon%20the%20Gumbel-Max%0Astructural%20causal%20model.%20Our%20model%20allows%20any%20large%20language%20model%20to%20perform%0Acounterfactual%20token%20generation%20at%20almost%20no%20cost%20in%20comparison%20with%20vanilla%0Atoken%20generation%2C%20it%20is%20embarrassingly%20simple%20to%20implement%2C%20and%20it%20does%20not%0Arequire%20any%20fine-tuning%20nor%20prompt%20engineering.%20We%20implement%20our%20model%20on%20Llama%0A3%208B-Instruct%20and%20Ministral-8B-Instruct%20and%20conduct%20a%20qualitative%20and%20a%0Aquantitative%20analysis%20of%20counterfactually%20generated%20text.%20We%20conclude%20with%20a%0Ademonstrative%20application%20of%20counterfactual%20token%20generation%20for%20bias%0Adetection%2C%20unveiling%20interesting%20insights%20about%20the%20model%20of%20the%20world%0Aconstructed%20by%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Token%2520Generation%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DIvi%2520Chatzi%2520and%2520Nina%2520Corvelo%2520Benz%2520and%2520Eleni%2520Straitouri%2520and%2520Stratis%2520Tsirtsis%2520and%2520Manuel%2520Gomez-Rodriguez%26entry.1292438233%3D%2520%2520%2522Sure%252C%2520I%2520am%2520happy%2520to%2520generate%2520a%2520story%2520for%2520you%253A%2520Captain%2520Lyra%2520stood%2520at%2520the%2520helm%250Aof%2520her%2520trusty%2520ship%252C%2520the%2520Maelstrom%2527s%2520Fury%252C%2520gazing%2520out%2520at%2520the%2520endless%2520sea.%2520%255B...%255D%250ALyra%2527s%2520eyes%2520welled%2520up%2520with%2520tears%2520as%2520she%2520realized%2520the%2520bitter%2520truth%2520-%2520she%2520had%250Asacrificed%2520everything%2520for%2520fleeting%2520riches%252C%2520and%2520lost%2520the%2520love%2520of%2520her%2520crew%252C%2520her%250Afamily%252C%2520and%2520herself.%2522%2520Although%2520this%2520story%252C%2520generated%2520by%2520a%2520large%2520language%2520model%252C%250Ais%2520captivating%252C%2520one%2520may%2520wonder%2520--%2520how%2520would%2520the%2520story%2520have%2520unfolded%2520if%2520the%250Amodel%2520had%2520chosen%2520%2522Captain%2520Maeve%2522%2520as%2520the%2520protagonist%2520instead%253F%2520We%2520cannot%2520know.%250AState-of-the-art%2520large%2520language%2520models%2520are%2520stateless%2520--%2520they%2520maintain%2520no%250Ainternal%2520memory%2520or%2520state.%2520Given%2520a%2520prompt%252C%2520they%2520generate%2520a%2520sequence%2520of%2520tokens%2520as%250Aan%2520output%2520using%2520an%2520autoregressive%2520process.%2520As%2520a%2520consequence%252C%2520they%2520cannot%2520reason%250Aabout%2520counterfactual%2520alternatives%2520to%2520tokens%2520they%2520have%2520generated%2520in%2520the%2520past.%2520In%250Athis%2520work%252C%2520our%2520goal%2520is%2520to%2520enhance%2520them%2520with%2520this%2520functionality.%2520To%2520this%2520end%252C%2520we%250Adevelop%2520a%2520causal%2520model%2520of%2520token%2520generation%2520that%2520builds%2520upon%2520the%2520Gumbel-Max%250Astructural%2520causal%2520model.%2520Our%2520model%2520allows%2520any%2520large%2520language%2520model%2520to%2520perform%250Acounterfactual%2520token%2520generation%2520at%2520almost%2520no%2520cost%2520in%2520comparison%2520with%2520vanilla%250Atoken%2520generation%252C%2520it%2520is%2520embarrassingly%2520simple%2520to%2520implement%252C%2520and%2520it%2520does%2520not%250Arequire%2520any%2520fine-tuning%2520nor%2520prompt%2520engineering.%2520We%2520implement%2520our%2520model%2520on%2520Llama%250A3%25208B-Instruct%2520and%2520Ministral-8B-Instruct%2520and%2520conduct%2520a%2520qualitative%2520and%2520a%250Aquantitative%2520analysis%2520of%2520counterfactually%2520generated%2520text.%2520We%2520conclude%2520with%2520a%250Ademonstrative%2520application%2520of%2520counterfactual%2520token%2520generation%2520for%2520bias%250Adetection%252C%2520unveiling%2520interesting%2520insights%2520about%2520the%2520model%2520of%2520the%2520world%250Aconstructed%2520by%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Token%20Generation%20in%20Large%20Language%20Models&entry.906535625=Ivi%20Chatzi%20and%20Nina%20Corvelo%20Benz%20and%20Eleni%20Straitouri%20and%20Stratis%20Tsirtsis%20and%20Manuel%20Gomez-Rodriguez&entry.1292438233=%20%20%22Sure%2C%20I%20am%20happy%20to%20generate%20a%20story%20for%20you%3A%20Captain%20Lyra%20stood%20at%20the%20helm%0Aof%20her%20trusty%20ship%2C%20the%20Maelstrom%27s%20Fury%2C%20gazing%20out%20at%20the%20endless%20sea.%20%5B...%5D%0ALyra%27s%20eyes%20welled%20up%20with%20tears%20as%20she%20realized%20the%20bitter%20truth%20-%20she%20had%0Asacrificed%20everything%20for%20fleeting%20riches%2C%20and%20lost%20the%20love%20of%20her%20crew%2C%20her%0Afamily%2C%20and%20herself.%22%20Although%20this%20story%2C%20generated%20by%20a%20large%20language%20model%2C%0Ais%20captivating%2C%20one%20may%20wonder%20--%20how%20would%20the%20story%20have%20unfolded%20if%20the%0Amodel%20had%20chosen%20%22Captain%20Maeve%22%20as%20the%20protagonist%20instead%3F%20We%20cannot%20know.%0AState-of-the-art%20large%20language%20models%20are%20stateless%20--%20they%20maintain%20no%0Ainternal%20memory%20or%20state.%20Given%20a%20prompt%2C%20they%20generate%20a%20sequence%20of%20tokens%20as%0Aan%20output%20using%20an%20autoregressive%20process.%20As%20a%20consequence%2C%20they%20cannot%20reason%0Aabout%20counterfactual%20alternatives%20to%20tokens%20they%20have%20generated%20in%20the%20past.%20In%0Athis%20work%2C%20our%20goal%20is%20to%20enhance%20them%20with%20this%20functionality.%20To%20this%20end%2C%20we%0Adevelop%20a%20causal%20model%20of%20token%20generation%20that%20builds%20upon%20the%20Gumbel-Max%0Astructural%20causal%20model.%20Our%20model%20allows%20any%20large%20language%20model%20to%20perform%0Acounterfactual%20token%20generation%20at%20almost%20no%20cost%20in%20comparison%20with%20vanilla%0Atoken%20generation%2C%20it%20is%20embarrassingly%20simple%20to%20implement%2C%20and%20it%20does%20not%0Arequire%20any%20fine-tuning%20nor%20prompt%20engineering.%20We%20implement%20our%20model%20on%20Llama%0A3%208B-Instruct%20and%20Ministral-8B-Instruct%20and%20conduct%20a%20qualitative%20and%20a%0Aquantitative%20analysis%20of%20counterfactually%20generated%20text.%20We%20conclude%20with%20a%0Ademonstrative%20application%20of%20counterfactual%20token%20generation%20for%20bias%0Adetection%2C%20unveiling%20interesting%20insights%20about%20the%20model%20of%20the%20world%0Aconstructed%20by%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17027v2&entry.124074799=Read"},
{"title": "Improving Causal Reasoning in Large Language Models: A Survey", "author": "Longxuan Yu and Delin Chen and Siheng Xiong and Qingyang Wu and Qingzhen Liu and Dawei Li and Zhikai Chen and Xiaoze Liu and Liangming Pan", "abstract": "  Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning.\n", "link": "http://arxiv.org/abs/2410.16676v3", "date": "2024-11-06", "relevancy": 1.9798, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Causal%20Reasoning%20in%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Improving%20Causal%20Reasoning%20in%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Longxuan%20Yu%20and%20Delin%20Chen%20and%20Siheng%20Xiong%20and%20Qingyang%20Wu%20and%20Qingzhen%20Liu%20and%20Dawei%20Li%20and%20Zhikai%20Chen%20and%20Xiaoze%20Liu%20and%20Liangming%20Pan%0AAbstract%3A%20%20%20Causal%20reasoning%20%28CR%29%20is%20a%20crucial%20aspect%20of%20intelligence%2C%20essential%20for%0Aproblem-solving%2C%20decision-making%2C%20and%20understanding%20the%20world.%20While%20large%0Alanguage%20models%20%28LLMs%29%20can%20generate%20rationales%20for%20their%20outputs%2C%20their%20ability%0Ato%20reliably%20perform%20causal%20reasoning%20remains%20uncertain%2C%20often%20falling%20short%20in%0Atasks%20requiring%20a%20deep%20understanding%20of%20causality.%20In%20this%20survey%2C%20we%20provide%20a%0Acomprehensive%20review%20of%20research%20aimed%20at%20enhancing%20LLMs%20for%20causal%20reasoning.%0AWe%20categorize%20existing%20methods%20based%20on%20the%20role%20of%20LLMs%3A%20either%20as%20reasoning%0Aengines%20or%20as%20helpers%20providing%20knowledge%20or%20data%20to%20traditional%20CR%20methods%2C%0Afollowed%20by%20a%20detailed%20discussion%20of%20the%20methodologies%20in%20each%20category.%20We%0Athen%20evaluate%20the%20performance%20of%20LLMs%20on%20various%20causal%20reasoning%20tasks%2C%0Aproviding%20key%20findings%20and%20in-depth%20analysis.%20Finally%2C%20we%20provide%20insights%20from%0Acurrent%20studies%20and%20highlight%20promising%20directions%20for%20future%20research.%20We%20aim%0Afor%20this%20work%20to%20serve%20as%20a%20comprehensive%20resource%2C%20fostering%20further%0Aadvancements%20in%20causal%20reasoning%20with%20LLMs.%20Resources%20are%20available%20at%0Ahttps%3A//github.com/chendl02/Awesome-LLM-causal-reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16676v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Causal%2520Reasoning%2520in%2520Large%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DLongxuan%2520Yu%2520and%2520Delin%2520Chen%2520and%2520Siheng%2520Xiong%2520and%2520Qingyang%2520Wu%2520and%2520Qingzhen%2520Liu%2520and%2520Dawei%2520Li%2520and%2520Zhikai%2520Chen%2520and%2520Xiaoze%2520Liu%2520and%2520Liangming%2520Pan%26entry.1292438233%3D%2520%2520Causal%2520reasoning%2520%2528CR%2529%2520is%2520a%2520crucial%2520aspect%2520of%2520intelligence%252C%2520essential%2520for%250Aproblem-solving%252C%2520decision-making%252C%2520and%2520understanding%2520the%2520world.%2520While%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520can%2520generate%2520rationales%2520for%2520their%2520outputs%252C%2520their%2520ability%250Ato%2520reliably%2520perform%2520causal%2520reasoning%2520remains%2520uncertain%252C%2520often%2520falling%2520short%2520in%250Atasks%2520requiring%2520a%2520deep%2520understanding%2520of%2520causality.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%250Acomprehensive%2520review%2520of%2520research%2520aimed%2520at%2520enhancing%2520LLMs%2520for%2520causal%2520reasoning.%250AWe%2520categorize%2520existing%2520methods%2520based%2520on%2520the%2520role%2520of%2520LLMs%253A%2520either%2520as%2520reasoning%250Aengines%2520or%2520as%2520helpers%2520providing%2520knowledge%2520or%2520data%2520to%2520traditional%2520CR%2520methods%252C%250Afollowed%2520by%2520a%2520detailed%2520discussion%2520of%2520the%2520methodologies%2520in%2520each%2520category.%2520We%250Athen%2520evaluate%2520the%2520performance%2520of%2520LLMs%2520on%2520various%2520causal%2520reasoning%2520tasks%252C%250Aproviding%2520key%2520findings%2520and%2520in-depth%2520analysis.%2520Finally%252C%2520we%2520provide%2520insights%2520from%250Acurrent%2520studies%2520and%2520highlight%2520promising%2520directions%2520for%2520future%2520research.%2520We%2520aim%250Afor%2520this%2520work%2520to%2520serve%2520as%2520a%2520comprehensive%2520resource%252C%2520fostering%2520further%250Aadvancements%2520in%2520causal%2520reasoning%2520with%2520LLMs.%2520Resources%2520are%2520available%2520at%250Ahttps%253A//github.com/chendl02/Awesome-LLM-causal-reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16676v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Causal%20Reasoning%20in%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Longxuan%20Yu%20and%20Delin%20Chen%20and%20Siheng%20Xiong%20and%20Qingyang%20Wu%20and%20Qingzhen%20Liu%20and%20Dawei%20Li%20and%20Zhikai%20Chen%20and%20Xiaoze%20Liu%20and%20Liangming%20Pan&entry.1292438233=%20%20Causal%20reasoning%20%28CR%29%20is%20a%20crucial%20aspect%20of%20intelligence%2C%20essential%20for%0Aproblem-solving%2C%20decision-making%2C%20and%20understanding%20the%20world.%20While%20large%0Alanguage%20models%20%28LLMs%29%20can%20generate%20rationales%20for%20their%20outputs%2C%20their%20ability%0Ato%20reliably%20perform%20causal%20reasoning%20remains%20uncertain%2C%20often%20falling%20short%20in%0Atasks%20requiring%20a%20deep%20understanding%20of%20causality.%20In%20this%20survey%2C%20we%20provide%20a%0Acomprehensive%20review%20of%20research%20aimed%20at%20enhancing%20LLMs%20for%20causal%20reasoning.%0AWe%20categorize%20existing%20methods%20based%20on%20the%20role%20of%20LLMs%3A%20either%20as%20reasoning%0Aengines%20or%20as%20helpers%20providing%20knowledge%20or%20data%20to%20traditional%20CR%20methods%2C%0Afollowed%20by%20a%20detailed%20discussion%20of%20the%20methodologies%20in%20each%20category.%20We%0Athen%20evaluate%20the%20performance%20of%20LLMs%20on%20various%20causal%20reasoning%20tasks%2C%0Aproviding%20key%20findings%20and%20in-depth%20analysis.%20Finally%2C%20we%20provide%20insights%20from%0Acurrent%20studies%20and%20highlight%20promising%20directions%20for%20future%20research.%20We%20aim%0Afor%20this%20work%20to%20serve%20as%20a%20comprehensive%20resource%2C%20fostering%20further%0Aadvancements%20in%20causal%20reasoning%20with%20LLMs.%20Resources%20are%20available%20at%0Ahttps%3A//github.com/chendl02/Awesome-LLM-causal-reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16676v3&entry.124074799=Read"},
{"title": "Confidence Calibration of Classifiers with Many Classes", "author": "Adrien LeCoz and St\u00e9phane Herbin and Faouzi Adjed", "abstract": "  For classification models based on neural networks, the maximum predicted\nclass probability is often used as a confidence score. This score rarely\npredicts well the probability of making a correct prediction and requires a\npost-processing calibration step. However, many confidence calibration methods\nfail for problems with many classes. To address this issue, we transform the\nproblem of calibrating a multiclass classifier into calibrating a single\nsurrogate binary classifier. This approach allows for more efficient use of\nstandard calibration methods. We evaluate our approach on numerous neural\nnetworks used for image or text classification and show that it significantly\nenhances existing calibration methods.\n", "link": "http://arxiv.org/abs/2411.02988v2", "date": "2024-11-06", "relevancy": 1.9715, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5079}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4917}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Calibration%20of%20Classifiers%20with%20Many%20Classes&body=Title%3A%20Confidence%20Calibration%20of%20Classifiers%20with%20Many%20Classes%0AAuthor%3A%20Adrien%20LeCoz%20and%20St%C3%A9phane%20Herbin%20and%20Faouzi%20Adjed%0AAbstract%3A%20%20%20For%20classification%20models%20based%20on%20neural%20networks%2C%20the%20maximum%20predicted%0Aclass%20probability%20is%20often%20used%20as%20a%20confidence%20score.%20This%20score%20rarely%0Apredicts%20well%20the%20probability%20of%20making%20a%20correct%20prediction%20and%20requires%20a%0Apost-processing%20calibration%20step.%20However%2C%20many%20confidence%20calibration%20methods%0Afail%20for%20problems%20with%20many%20classes.%20To%20address%20this%20issue%2C%20we%20transform%20the%0Aproblem%20of%20calibrating%20a%20multiclass%20classifier%20into%20calibrating%20a%20single%0Asurrogate%20binary%20classifier.%20This%20approach%20allows%20for%20more%20efficient%20use%20of%0Astandard%20calibration%20methods.%20We%20evaluate%20our%20approach%20on%20numerous%20neural%0Anetworks%20used%20for%20image%20or%20text%20classification%20and%20show%20that%20it%20significantly%0Aenhances%20existing%20calibration%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02988v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Calibration%2520of%2520Classifiers%2520with%2520Many%2520Classes%26entry.906535625%3DAdrien%2520LeCoz%2520and%2520St%25C3%25A9phane%2520Herbin%2520and%2520Faouzi%2520Adjed%26entry.1292438233%3D%2520%2520For%2520classification%2520models%2520based%2520on%2520neural%2520networks%252C%2520the%2520maximum%2520predicted%250Aclass%2520probability%2520is%2520often%2520used%2520as%2520a%2520confidence%2520score.%2520This%2520score%2520rarely%250Apredicts%2520well%2520the%2520probability%2520of%2520making%2520a%2520correct%2520prediction%2520and%2520requires%2520a%250Apost-processing%2520calibration%2520step.%2520However%252C%2520many%2520confidence%2520calibration%2520methods%250Afail%2520for%2520problems%2520with%2520many%2520classes.%2520To%2520address%2520this%2520issue%252C%2520we%2520transform%2520the%250Aproblem%2520of%2520calibrating%2520a%2520multiclass%2520classifier%2520into%2520calibrating%2520a%2520single%250Asurrogate%2520binary%2520classifier.%2520This%2520approach%2520allows%2520for%2520more%2520efficient%2520use%2520of%250Astandard%2520calibration%2520methods.%2520We%2520evaluate%2520our%2520approach%2520on%2520numerous%2520neural%250Anetworks%2520used%2520for%2520image%2520or%2520text%2520classification%2520and%2520show%2520that%2520it%2520significantly%250Aenhances%2520existing%2520calibration%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02988v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Calibration%20of%20Classifiers%20with%20Many%20Classes&entry.906535625=Adrien%20LeCoz%20and%20St%C3%A9phane%20Herbin%20and%20Faouzi%20Adjed&entry.1292438233=%20%20For%20classification%20models%20based%20on%20neural%20networks%2C%20the%20maximum%20predicted%0Aclass%20probability%20is%20often%20used%20as%20a%20confidence%20score.%20This%20score%20rarely%0Apredicts%20well%20the%20probability%20of%20making%20a%20correct%20prediction%20and%20requires%20a%0Apost-processing%20calibration%20step.%20However%2C%20many%20confidence%20calibration%20methods%0Afail%20for%20problems%20with%20many%20classes.%20To%20address%20this%20issue%2C%20we%20transform%20the%0Aproblem%20of%20calibrating%20a%20multiclass%20classifier%20into%20calibrating%20a%20single%0Asurrogate%20binary%20classifier.%20This%20approach%20allows%20for%20more%20efficient%20use%20of%0Astandard%20calibration%20methods.%20We%20evaluate%20our%20approach%20on%20numerous%20neural%0Anetworks%20used%20for%20image%20or%20text%20classification%20and%20show%20that%20it%20significantly%0Aenhances%20existing%20calibration%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02988v2&entry.124074799=Read"},
{"title": "Hybrid Transfer Reinforcement Learning: Provable Sample Efficiency from\n  Shifted-Dynamics Data", "author": "Chengrui Qu and Laixi Shi and Kishan Panaganti and Pengcheng You and Adam Wierman", "abstract": "  Online Reinforcement learning (RL) typically requires high-stakes online\ninteraction data to learn a policy for a target task. This prompts interest in\nleveraging historical data to improve sample efficiency. The historical data\nmay come from outdated or related source environments with different dynamics.\nIt remains unclear how to effectively use such data in the target task to\nprovably enhance learning and sample efficiency. To address this, we propose a\nhybrid transfer RL (HTRL) setting, where an agent learns in a target\nenvironment while accessing offline data from a source environment with shifted\ndynamics. We show that -- without information on the dynamics shift -- general\nshifted-dynamics data, even with subtle shifts, does not reduce sample\ncomplexity in the target environment. However, with prior information on the\ndegree of the dynamics shift, we design HySRL, a transfer algorithm that\nachieves problem-dependent sample complexity and outperforms pure online RL.\nFinally, our experimental results demonstrate that HySRL surpasses\nstate-of-the-art online RL baseline.\n", "link": "http://arxiv.org/abs/2411.03810v1", "date": "2024-11-06", "relevancy": 1.9713, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4994}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4933}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Transfer%20Reinforcement%20Learning%3A%20Provable%20Sample%20Efficiency%20from%0A%20%20Shifted-Dynamics%20Data&body=Title%3A%20Hybrid%20Transfer%20Reinforcement%20Learning%3A%20Provable%20Sample%20Efficiency%20from%0A%20%20Shifted-Dynamics%20Data%0AAuthor%3A%20Chengrui%20Qu%20and%20Laixi%20Shi%20and%20Kishan%20Panaganti%20and%20Pengcheng%20You%20and%20Adam%20Wierman%0AAbstract%3A%20%20%20Online%20Reinforcement%20learning%20%28RL%29%20typically%20requires%20high-stakes%20online%0Ainteraction%20data%20to%20learn%20a%20policy%20for%20a%20target%20task.%20This%20prompts%20interest%20in%0Aleveraging%20historical%20data%20to%20improve%20sample%20efficiency.%20The%20historical%20data%0Amay%20come%20from%20outdated%20or%20related%20source%20environments%20with%20different%20dynamics.%0AIt%20remains%20unclear%20how%20to%20effectively%20use%20such%20data%20in%20the%20target%20task%20to%0Aprovably%20enhance%20learning%20and%20sample%20efficiency.%20To%20address%20this%2C%20we%20propose%20a%0Ahybrid%20transfer%20RL%20%28HTRL%29%20setting%2C%20where%20an%20agent%20learns%20in%20a%20target%0Aenvironment%20while%20accessing%20offline%20data%20from%20a%20source%20environment%20with%20shifted%0Adynamics.%20We%20show%20that%20--%20without%20information%20on%20the%20dynamics%20shift%20--%20general%0Ashifted-dynamics%20data%2C%20even%20with%20subtle%20shifts%2C%20does%20not%20reduce%20sample%0Acomplexity%20in%20the%20target%20environment.%20However%2C%20with%20prior%20information%20on%20the%0Adegree%20of%20the%20dynamics%20shift%2C%20we%20design%20HySRL%2C%20a%20transfer%20algorithm%20that%0Aachieves%20problem-dependent%20sample%20complexity%20and%20outperforms%20pure%20online%20RL.%0AFinally%2C%20our%20experimental%20results%20demonstrate%20that%20HySRL%20surpasses%0Astate-of-the-art%20online%20RL%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Transfer%2520Reinforcement%2520Learning%253A%2520Provable%2520Sample%2520Efficiency%2520from%250A%2520%2520Shifted-Dynamics%2520Data%26entry.906535625%3DChengrui%2520Qu%2520and%2520Laixi%2520Shi%2520and%2520Kishan%2520Panaganti%2520and%2520Pengcheng%2520You%2520and%2520Adam%2520Wierman%26entry.1292438233%3D%2520%2520Online%2520Reinforcement%2520learning%2520%2528RL%2529%2520typically%2520requires%2520high-stakes%2520online%250Ainteraction%2520data%2520to%2520learn%2520a%2520policy%2520for%2520a%2520target%2520task.%2520This%2520prompts%2520interest%2520in%250Aleveraging%2520historical%2520data%2520to%2520improve%2520sample%2520efficiency.%2520The%2520historical%2520data%250Amay%2520come%2520from%2520outdated%2520or%2520related%2520source%2520environments%2520with%2520different%2520dynamics.%250AIt%2520remains%2520unclear%2520how%2520to%2520effectively%2520use%2520such%2520data%2520in%2520the%2520target%2520task%2520to%250Aprovably%2520enhance%2520learning%2520and%2520sample%2520efficiency.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Ahybrid%2520transfer%2520RL%2520%2528HTRL%2529%2520setting%252C%2520where%2520an%2520agent%2520learns%2520in%2520a%2520target%250Aenvironment%2520while%2520accessing%2520offline%2520data%2520from%2520a%2520source%2520environment%2520with%2520shifted%250Adynamics.%2520We%2520show%2520that%2520--%2520without%2520information%2520on%2520the%2520dynamics%2520shift%2520--%2520general%250Ashifted-dynamics%2520data%252C%2520even%2520with%2520subtle%2520shifts%252C%2520does%2520not%2520reduce%2520sample%250Acomplexity%2520in%2520the%2520target%2520environment.%2520However%252C%2520with%2520prior%2520information%2520on%2520the%250Adegree%2520of%2520the%2520dynamics%2520shift%252C%2520we%2520design%2520HySRL%252C%2520a%2520transfer%2520algorithm%2520that%250Aachieves%2520problem-dependent%2520sample%2520complexity%2520and%2520outperforms%2520pure%2520online%2520RL.%250AFinally%252C%2520our%2520experimental%2520results%2520demonstrate%2520that%2520HySRL%2520surpasses%250Astate-of-the-art%2520online%2520RL%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Transfer%20Reinforcement%20Learning%3A%20Provable%20Sample%20Efficiency%20from%0A%20%20Shifted-Dynamics%20Data&entry.906535625=Chengrui%20Qu%20and%20Laixi%20Shi%20and%20Kishan%20Panaganti%20and%20Pengcheng%20You%20and%20Adam%20Wierman&entry.1292438233=%20%20Online%20Reinforcement%20learning%20%28RL%29%20typically%20requires%20high-stakes%20online%0Ainteraction%20data%20to%20learn%20a%20policy%20for%20a%20target%20task.%20This%20prompts%20interest%20in%0Aleveraging%20historical%20data%20to%20improve%20sample%20efficiency.%20The%20historical%20data%0Amay%20come%20from%20outdated%20or%20related%20source%20environments%20with%20different%20dynamics.%0AIt%20remains%20unclear%20how%20to%20effectively%20use%20such%20data%20in%20the%20target%20task%20to%0Aprovably%20enhance%20learning%20and%20sample%20efficiency.%20To%20address%20this%2C%20we%20propose%20a%0Ahybrid%20transfer%20RL%20%28HTRL%29%20setting%2C%20where%20an%20agent%20learns%20in%20a%20target%0Aenvironment%20while%20accessing%20offline%20data%20from%20a%20source%20environment%20with%20shifted%0Adynamics.%20We%20show%20that%20--%20without%20information%20on%20the%20dynamics%20shift%20--%20general%0Ashifted-dynamics%20data%2C%20even%20with%20subtle%20shifts%2C%20does%20not%20reduce%20sample%0Acomplexity%20in%20the%20target%20environment.%20However%2C%20with%20prior%20information%20on%20the%0Adegree%20of%20the%20dynamics%20shift%2C%20we%20design%20HySRL%2C%20a%20transfer%20algorithm%20that%0Aachieves%20problem-dependent%20sample%20complexity%20and%20outperforms%20pure%20online%20RL.%0AFinally%2C%20our%20experimental%20results%20demonstrate%20that%20HySRL%20surpasses%0Astate-of-the-art%20online%20RL%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03810v1&entry.124074799=Read"},
{"title": "Diversity Progress for Goal Selection in Discriminability-Motivated RL", "author": "Erik M. Lintunen and Nadia M. Ady and Christian Guckelsberger", "abstract": "  Non-uniform goal selection has the potential to improve the reinforcement\nlearning (RL) of skills over uniform-random selection. In this paper, we\nintroduce a method for learning a goal-selection policy in\nintrinsically-motivated goal-conditioned RL: \"Diversity Progress\" (DP). The\nlearner forms a curriculum based on observed improvement in discriminability\nover its set of goals. Our proposed method is applicable to the class of\ndiscriminability-motivated agents, where the intrinsic reward is computed as a\nfunction of the agent's certainty of following the true goal being pursued.\nThis reward can motivate the agent to learn a set of diverse skills without\nextrinsic rewards. We demonstrate empirically that a DP-motivated agent can\nlearn a set of distinguishable skills faster than previous approaches, and do\nso without suffering from a collapse of the goal distribution -- a known issue\nwith some prior approaches. We end with plans to take this proof-of-concept\nforward.\n", "link": "http://arxiv.org/abs/2411.01521v2", "date": "2024-11-06", "relevancy": 1.9624, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.506}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.48}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversity%20Progress%20for%20Goal%20Selection%20in%20Discriminability-Motivated%20RL&body=Title%3A%20Diversity%20Progress%20for%20Goal%20Selection%20in%20Discriminability-Motivated%20RL%0AAuthor%3A%20Erik%20M.%20Lintunen%20and%20Nadia%20M.%20Ady%20and%20Christian%20Guckelsberger%0AAbstract%3A%20%20%20Non-uniform%20goal%20selection%20has%20the%20potential%20to%20improve%20the%20reinforcement%0Alearning%20%28RL%29%20of%20skills%20over%20uniform-random%20selection.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20method%20for%20learning%20a%20goal-selection%20policy%20in%0Aintrinsically-motivated%20goal-conditioned%20RL%3A%20%22Diversity%20Progress%22%20%28DP%29.%20The%0Alearner%20forms%20a%20curriculum%20based%20on%20observed%20improvement%20in%20discriminability%0Aover%20its%20set%20of%20goals.%20Our%20proposed%20method%20is%20applicable%20to%20the%20class%20of%0Adiscriminability-motivated%20agents%2C%20where%20the%20intrinsic%20reward%20is%20computed%20as%20a%0Afunction%20of%20the%20agent%27s%20certainty%20of%20following%20the%20true%20goal%20being%20pursued.%0AThis%20reward%20can%20motivate%20the%20agent%20to%20learn%20a%20set%20of%20diverse%20skills%20without%0Aextrinsic%20rewards.%20We%20demonstrate%20empirically%20that%20a%20DP-motivated%20agent%20can%0Alearn%20a%20set%20of%20distinguishable%20skills%20faster%20than%20previous%20approaches%2C%20and%20do%0Aso%20without%20suffering%20from%20a%20collapse%20of%20the%20goal%20distribution%20--%20a%20known%20issue%0Awith%20some%20prior%20approaches.%20We%20end%20with%20plans%20to%20take%20this%20proof-of-concept%0Aforward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversity%2520Progress%2520for%2520Goal%2520Selection%2520in%2520Discriminability-Motivated%2520RL%26entry.906535625%3DErik%2520M.%2520Lintunen%2520and%2520Nadia%2520M.%2520Ady%2520and%2520Christian%2520Guckelsberger%26entry.1292438233%3D%2520%2520Non-uniform%2520goal%2520selection%2520has%2520the%2520potential%2520to%2520improve%2520the%2520reinforcement%250Alearning%2520%2528RL%2529%2520of%2520skills%2520over%2520uniform-random%2520selection.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520method%2520for%2520learning%2520a%2520goal-selection%2520policy%2520in%250Aintrinsically-motivated%2520goal-conditioned%2520RL%253A%2520%2522Diversity%2520Progress%2522%2520%2528DP%2529.%2520The%250Alearner%2520forms%2520a%2520curriculum%2520based%2520on%2520observed%2520improvement%2520in%2520discriminability%250Aover%2520its%2520set%2520of%2520goals.%2520Our%2520proposed%2520method%2520is%2520applicable%2520to%2520the%2520class%2520of%250Adiscriminability-motivated%2520agents%252C%2520where%2520the%2520intrinsic%2520reward%2520is%2520computed%2520as%2520a%250Afunction%2520of%2520the%2520agent%2527s%2520certainty%2520of%2520following%2520the%2520true%2520goal%2520being%2520pursued.%250AThis%2520reward%2520can%2520motivate%2520the%2520agent%2520to%2520learn%2520a%2520set%2520of%2520diverse%2520skills%2520without%250Aextrinsic%2520rewards.%2520We%2520demonstrate%2520empirically%2520that%2520a%2520DP-motivated%2520agent%2520can%250Alearn%2520a%2520set%2520of%2520distinguishable%2520skills%2520faster%2520than%2520previous%2520approaches%252C%2520and%2520do%250Aso%2520without%2520suffering%2520from%2520a%2520collapse%2520of%2520the%2520goal%2520distribution%2520--%2520a%2520known%2520issue%250Awith%2520some%2520prior%2520approaches.%2520We%2520end%2520with%2520plans%2520to%2520take%2520this%2520proof-of-concept%250Aforward.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversity%20Progress%20for%20Goal%20Selection%20in%20Discriminability-Motivated%20RL&entry.906535625=Erik%20M.%20Lintunen%20and%20Nadia%20M.%20Ady%20and%20Christian%20Guckelsberger&entry.1292438233=%20%20Non-uniform%20goal%20selection%20has%20the%20potential%20to%20improve%20the%20reinforcement%0Alearning%20%28RL%29%20of%20skills%20over%20uniform-random%20selection.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20method%20for%20learning%20a%20goal-selection%20policy%20in%0Aintrinsically-motivated%20goal-conditioned%20RL%3A%20%22Diversity%20Progress%22%20%28DP%29.%20The%0Alearner%20forms%20a%20curriculum%20based%20on%20observed%20improvement%20in%20discriminability%0Aover%20its%20set%20of%20goals.%20Our%20proposed%20method%20is%20applicable%20to%20the%20class%20of%0Adiscriminability-motivated%20agents%2C%20where%20the%20intrinsic%20reward%20is%20computed%20as%20a%0Afunction%20of%20the%20agent%27s%20certainty%20of%20following%20the%20true%20goal%20being%20pursued.%0AThis%20reward%20can%20motivate%20the%20agent%20to%20learn%20a%20set%20of%20diverse%20skills%20without%0Aextrinsic%20rewards.%20We%20demonstrate%20empirically%20that%20a%20DP-motivated%20agent%20can%0Alearn%20a%20set%20of%20distinguishable%20skills%20faster%20than%20previous%20approaches%2C%20and%20do%0Aso%20without%20suffering%20from%20a%20collapse%20of%20the%20goal%20distribution%20--%20a%20known%20issue%0Awith%20some%20prior%20approaches.%20We%20end%20with%20plans%20to%20take%20this%20proof-of-concept%0Aforward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01521v2&entry.124074799=Read"},
{"title": "Interactions Across Blocks in Post-Training Quantization of Large\n  Language Models", "author": "Khasmamad Shabanovi and Lukas Wiest and Vladimir Golkov and Daniel Cremers and Thomas Pfeil", "abstract": "  Post-training quantization is widely employed to reduce the computational\ndemands of neural networks. Typically, individual substructures, such as layers\nor blocks of layers, are quantized with the objective of minimizing\nquantization errors in their pre-activations by fine-tuning the corresponding\nweights. Deriving this local objective from the global objective of minimizing\ntask loss involves two key simplifications: assuming substructures are mutually\nindependent and ignoring the knowledge of subsequent substructures as well as\nthe task loss. In this work, we assess the effects of these simplifications on\nweight-only quantization of large language models. We introduce two multi-block\nfine-tuning strategies and compare them against the baseline of fine-tuning\nsingle transformer blocks. The first captures correlations of weights across\nblocks by jointly optimizing multiple quantized blocks. The second incorporates\nknowledge of subsequent blocks by minimizing the error in downstream\npre-activations rather than focusing solely on the quantized block. Our\nfindings indicate that the effectiveness of these methods depends on the\nspecific network model, with no impact on some models but demonstrating\nsignificant benefits for others.\n", "link": "http://arxiv.org/abs/2411.03934v1", "date": "2024-11-06", "relevancy": 1.961, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5123}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactions%20Across%20Blocks%20in%20Post-Training%20Quantization%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20Interactions%20Across%20Blocks%20in%20Post-Training%20Quantization%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Khasmamad%20Shabanovi%20and%20Lukas%20Wiest%20and%20Vladimir%20Golkov%20and%20Daniel%20Cremers%20and%20Thomas%20Pfeil%0AAbstract%3A%20%20%20Post-training%20quantization%20is%20widely%20employed%20to%20reduce%20the%20computational%0Ademands%20of%20neural%20networks.%20Typically%2C%20individual%20substructures%2C%20such%20as%20layers%0Aor%20blocks%20of%20layers%2C%20are%20quantized%20with%20the%20objective%20of%20minimizing%0Aquantization%20errors%20in%20their%20pre-activations%20by%20fine-tuning%20the%20corresponding%0Aweights.%20Deriving%20this%20local%20objective%20from%20the%20global%20objective%20of%20minimizing%0Atask%20loss%20involves%20two%20key%20simplifications%3A%20assuming%20substructures%20are%20mutually%0Aindependent%20and%20ignoring%20the%20knowledge%20of%20subsequent%20substructures%20as%20well%20as%0Athe%20task%20loss.%20In%20this%20work%2C%20we%20assess%20the%20effects%20of%20these%20simplifications%20on%0Aweight-only%20quantization%20of%20large%20language%20models.%20We%20introduce%20two%20multi-block%0Afine-tuning%20strategies%20and%20compare%20them%20against%20the%20baseline%20of%20fine-tuning%0Asingle%20transformer%20blocks.%20The%20first%20captures%20correlations%20of%20weights%20across%0Ablocks%20by%20jointly%20optimizing%20multiple%20quantized%20blocks.%20The%20second%20incorporates%0Aknowledge%20of%20subsequent%20blocks%20by%20minimizing%20the%20error%20in%20downstream%0Apre-activations%20rather%20than%20focusing%20solely%20on%20the%20quantized%20block.%20Our%0Afindings%20indicate%20that%20the%20effectiveness%20of%20these%20methods%20depends%20on%20the%0Aspecific%20network%20model%2C%20with%20no%20impact%20on%20some%20models%20but%20demonstrating%0Asignificant%20benefits%20for%20others.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractions%2520Across%2520Blocks%2520in%2520Post-Training%2520Quantization%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DKhasmamad%2520Shabanovi%2520and%2520Lukas%2520Wiest%2520and%2520Vladimir%2520Golkov%2520and%2520Daniel%2520Cremers%2520and%2520Thomas%2520Pfeil%26entry.1292438233%3D%2520%2520Post-training%2520quantization%2520is%2520widely%2520employed%2520to%2520reduce%2520the%2520computational%250Ademands%2520of%2520neural%2520networks.%2520Typically%252C%2520individual%2520substructures%252C%2520such%2520as%2520layers%250Aor%2520blocks%2520of%2520layers%252C%2520are%2520quantized%2520with%2520the%2520objective%2520of%2520minimizing%250Aquantization%2520errors%2520in%2520their%2520pre-activations%2520by%2520fine-tuning%2520the%2520corresponding%250Aweights.%2520Deriving%2520this%2520local%2520objective%2520from%2520the%2520global%2520objective%2520of%2520minimizing%250Atask%2520loss%2520involves%2520two%2520key%2520simplifications%253A%2520assuming%2520substructures%2520are%2520mutually%250Aindependent%2520and%2520ignoring%2520the%2520knowledge%2520of%2520subsequent%2520substructures%2520as%2520well%2520as%250Athe%2520task%2520loss.%2520In%2520this%2520work%252C%2520we%2520assess%2520the%2520effects%2520of%2520these%2520simplifications%2520on%250Aweight-only%2520quantization%2520of%2520large%2520language%2520models.%2520We%2520introduce%2520two%2520multi-block%250Afine-tuning%2520strategies%2520and%2520compare%2520them%2520against%2520the%2520baseline%2520of%2520fine-tuning%250Asingle%2520transformer%2520blocks.%2520The%2520first%2520captures%2520correlations%2520of%2520weights%2520across%250Ablocks%2520by%2520jointly%2520optimizing%2520multiple%2520quantized%2520blocks.%2520The%2520second%2520incorporates%250Aknowledge%2520of%2520subsequent%2520blocks%2520by%2520minimizing%2520the%2520error%2520in%2520downstream%250Apre-activations%2520rather%2520than%2520focusing%2520solely%2520on%2520the%2520quantized%2520block.%2520Our%250Afindings%2520indicate%2520that%2520the%2520effectiveness%2520of%2520these%2520methods%2520depends%2520on%2520the%250Aspecific%2520network%2520model%252C%2520with%2520no%2520impact%2520on%2520some%2520models%2520but%2520demonstrating%250Asignificant%2520benefits%2520for%2520others.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactions%20Across%20Blocks%20in%20Post-Training%20Quantization%20of%20Large%0A%20%20Language%20Models&entry.906535625=Khasmamad%20Shabanovi%20and%20Lukas%20Wiest%20and%20Vladimir%20Golkov%20and%20Daniel%20Cremers%20and%20Thomas%20Pfeil&entry.1292438233=%20%20Post-training%20quantization%20is%20widely%20employed%20to%20reduce%20the%20computational%0Ademands%20of%20neural%20networks.%20Typically%2C%20individual%20substructures%2C%20such%20as%20layers%0Aor%20blocks%20of%20layers%2C%20are%20quantized%20with%20the%20objective%20of%20minimizing%0Aquantization%20errors%20in%20their%20pre-activations%20by%20fine-tuning%20the%20corresponding%0Aweights.%20Deriving%20this%20local%20objective%20from%20the%20global%20objective%20of%20minimizing%0Atask%20loss%20involves%20two%20key%20simplifications%3A%20assuming%20substructures%20are%20mutually%0Aindependent%20and%20ignoring%20the%20knowledge%20of%20subsequent%20substructures%20as%20well%20as%0Athe%20task%20loss.%20In%20this%20work%2C%20we%20assess%20the%20effects%20of%20these%20simplifications%20on%0Aweight-only%20quantization%20of%20large%20language%20models.%20We%20introduce%20two%20multi-block%0Afine-tuning%20strategies%20and%20compare%20them%20against%20the%20baseline%20of%20fine-tuning%0Asingle%20transformer%20blocks.%20The%20first%20captures%20correlations%20of%20weights%20across%0Ablocks%20by%20jointly%20optimizing%20multiple%20quantized%20blocks.%20The%20second%20incorporates%0Aknowledge%20of%20subsequent%20blocks%20by%20minimizing%20the%20error%20in%20downstream%0Apre-activations%20rather%20than%20focusing%20solely%20on%20the%20quantized%20block.%20Our%0Afindings%20indicate%20that%20the%20effectiveness%20of%20these%20methods%20depends%20on%20the%0Aspecific%20network%20model%2C%20with%20no%20impact%20on%20some%20models%20but%20demonstrating%0Asignificant%20benefits%20for%20others.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03934v1&entry.124074799=Read"},
{"title": "Evaluating Morphological Compositional Generalization in Large Language\n  Models", "author": "Mete Ismayilzada and Defne Circi and Jonne S\u00e4lev\u00e4 and Hale Sirin and Abdullatif K\u00f6ksal and Bhuwan Dhingra and Antoine Bosselut and Lonneke van der Plas and Duygu Ataman", "abstract": "  Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.\n", "link": "http://arxiv.org/abs/2410.12656v2", "date": "2024-11-06", "relevancy": 1.9527, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4887}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Morphological%20Compositional%20Generalization%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20Evaluating%20Morphological%20Compositional%20Generalization%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Mete%20Ismayilzada%20and%20Defne%20Circi%20and%20Jonne%20S%C3%A4lev%C3%A4%20and%20Hale%20Sirin%20and%20Abdullatif%20K%C3%B6ksal%20and%20Bhuwan%20Dhingra%20and%20Antoine%20Bosselut%20and%20Lonneke%20van%20der%20Plas%20and%20Duygu%20Ataman%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20significant%20progress%20in%0Avarious%20natural%20language%20generation%20and%20understanding%20tasks.%20However%2C%20their%0Alinguistic%20generalization%20capabilities%20remain%20questionable%2C%20raising%20doubts%0Aabout%20whether%20these%20models%20learn%20language%20similarly%20to%20humans.%20While%20humans%0Aexhibit%20compositional%20generalization%20and%20linguistic%20creativity%20in%20language%20use%2C%0Athe%20extent%20to%20which%20LLMs%20replicate%20these%20abilities%2C%20particularly%20in%20morphology%2C%0Ais%20under-explored.%20In%20this%20work%2C%20we%20systematically%20investigate%20the%0Amorphological%20generalization%20abilities%20of%20LLMs%20through%20the%20lens%20of%0Acompositionality.%20We%20define%20morphemes%20as%20compositional%20primitives%20and%20design%20a%0Anovel%20suite%20of%20generative%20and%20discriminative%20tasks%20to%20assess%20morphological%0Aproductivity%20and%20systematicity.%20Focusing%20on%20agglutinative%20languages%20such%20as%0ATurkish%20and%20Finnish%2C%20we%20evaluate%20several%20state-of-the-art%20instruction-finetuned%0Amultilingual%20models%2C%20including%20GPT-4%20and%20Gemini.%20Our%20analysis%20shows%20that%20LLMs%0Astruggle%20with%20morphological%20compositional%20generalization%20particularly%20when%0Aapplied%20to%20novel%20word%20roots%2C%20with%20performance%20declining%20sharply%20as%0Amorphological%20complexity%20increases.%20While%20models%20can%20identify%20individual%0Amorphological%20combinations%20better%20than%20chance%2C%20their%20performance%20lacks%0Asystematicity%2C%20leading%20to%20significant%20accuracy%20gaps%20compared%20to%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12656v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Morphological%2520Compositional%2520Generalization%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DMete%2520Ismayilzada%2520and%2520Defne%2520Circi%2520and%2520Jonne%2520S%25C3%25A4lev%25C3%25A4%2520and%2520Hale%2520Sirin%2520and%2520Abdullatif%2520K%25C3%25B6ksal%2520and%2520Bhuwan%2520Dhingra%2520and%2520Antoine%2520Bosselut%2520and%2520Lonneke%2520van%2520der%2520Plas%2520and%2520Duygu%2520Ataman%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%2520progress%2520in%250Avarious%2520natural%2520language%2520generation%2520and%2520understanding%2520tasks.%2520However%252C%2520their%250Alinguistic%2520generalization%2520capabilities%2520remain%2520questionable%252C%2520raising%2520doubts%250Aabout%2520whether%2520these%2520models%2520learn%2520language%2520similarly%2520to%2520humans.%2520While%2520humans%250Aexhibit%2520compositional%2520generalization%2520and%2520linguistic%2520creativity%2520in%2520language%2520use%252C%250Athe%2520extent%2520to%2520which%2520LLMs%2520replicate%2520these%2520abilities%252C%2520particularly%2520in%2520morphology%252C%250Ais%2520under-explored.%2520In%2520this%2520work%252C%2520we%2520systematically%2520investigate%2520the%250Amorphological%2520generalization%2520abilities%2520of%2520LLMs%2520through%2520the%2520lens%2520of%250Acompositionality.%2520We%2520define%2520morphemes%2520as%2520compositional%2520primitives%2520and%2520design%2520a%250Anovel%2520suite%2520of%2520generative%2520and%2520discriminative%2520tasks%2520to%2520assess%2520morphological%250Aproductivity%2520and%2520systematicity.%2520Focusing%2520on%2520agglutinative%2520languages%2520such%2520as%250ATurkish%2520and%2520Finnish%252C%2520we%2520evaluate%2520several%2520state-of-the-art%2520instruction-finetuned%250Amultilingual%2520models%252C%2520including%2520GPT-4%2520and%2520Gemini.%2520Our%2520analysis%2520shows%2520that%2520LLMs%250Astruggle%2520with%2520morphological%2520compositional%2520generalization%2520particularly%2520when%250Aapplied%2520to%2520novel%2520word%2520roots%252C%2520with%2520performance%2520declining%2520sharply%2520as%250Amorphological%2520complexity%2520increases.%2520While%2520models%2520can%2520identify%2520individual%250Amorphological%2520combinations%2520better%2520than%2520chance%252C%2520their%2520performance%2520lacks%250Asystematicity%252C%2520leading%2520to%2520significant%2520accuracy%2520gaps%2520compared%2520to%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12656v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Morphological%20Compositional%20Generalization%20in%20Large%20Language%0A%20%20Models&entry.906535625=Mete%20Ismayilzada%20and%20Defne%20Circi%20and%20Jonne%20S%C3%A4lev%C3%A4%20and%20Hale%20Sirin%20and%20Abdullatif%20K%C3%B6ksal%20and%20Bhuwan%20Dhingra%20and%20Antoine%20Bosselut%20and%20Lonneke%20van%20der%20Plas%20and%20Duygu%20Ataman&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20significant%20progress%20in%0Avarious%20natural%20language%20generation%20and%20understanding%20tasks.%20However%2C%20their%0Alinguistic%20generalization%20capabilities%20remain%20questionable%2C%20raising%20doubts%0Aabout%20whether%20these%20models%20learn%20language%20similarly%20to%20humans.%20While%20humans%0Aexhibit%20compositional%20generalization%20and%20linguistic%20creativity%20in%20language%20use%2C%0Athe%20extent%20to%20which%20LLMs%20replicate%20these%20abilities%2C%20particularly%20in%20morphology%2C%0Ais%20under-explored.%20In%20this%20work%2C%20we%20systematically%20investigate%20the%0Amorphological%20generalization%20abilities%20of%20LLMs%20through%20the%20lens%20of%0Acompositionality.%20We%20define%20morphemes%20as%20compositional%20primitives%20and%20design%20a%0Anovel%20suite%20of%20generative%20and%20discriminative%20tasks%20to%20assess%20morphological%0Aproductivity%20and%20systematicity.%20Focusing%20on%20agglutinative%20languages%20such%20as%0ATurkish%20and%20Finnish%2C%20we%20evaluate%20several%20state-of-the-art%20instruction-finetuned%0Amultilingual%20models%2C%20including%20GPT-4%20and%20Gemini.%20Our%20analysis%20shows%20that%20LLMs%0Astruggle%20with%20morphological%20compositional%20generalization%20particularly%20when%0Aapplied%20to%20novel%20word%20roots%2C%20with%20performance%20declining%20sharply%20as%0Amorphological%20complexity%20increases.%20While%20models%20can%20identify%20individual%0Amorphological%20combinations%20better%20than%20chance%2C%20their%20performance%20lacks%0Asystematicity%2C%20leading%20to%20significant%20accuracy%20gaps%20compared%20to%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12656v2&entry.124074799=Read"},
{"title": "Reassessing Noise Augmentation Methods in the Context of Adversarial\n  Speech", "author": "Karla Pizzi and Mat\u00edas Pizarro and Asja Fischer", "abstract": "  In this study, we investigate if noise-augmented training can concurrently\nimprove adversarial robustness in automatic speech recognition (ASR) systems.\nWe conduct a comparative analysis of the adversarial robustness of four\ndifferent state-of-the-art ASR architectures, where each of the ASR\narchitectures is trained under three different augmentation conditions: one\nsubject to background noise, speed variations, and reverberations, another\nsubject to speed variations only, and a third without any form of data\naugmentation. The results demonstrate that noise augmentation not only improves\nmodel performance on noisy speech but also the model's robustness to\nadversarial attacks.\n", "link": "http://arxiv.org/abs/2409.01813v3", "date": "2024-11-06", "relevancy": 1.9477, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4959}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4861}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reassessing%20Noise%20Augmentation%20Methods%20in%20the%20Context%20of%20Adversarial%0A%20%20Speech&body=Title%3A%20Reassessing%20Noise%20Augmentation%20Methods%20in%20the%20Context%20of%20Adversarial%0A%20%20Speech%0AAuthor%3A%20Karla%20Pizzi%20and%20Mat%C3%ADas%20Pizarro%20and%20Asja%20Fischer%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20investigate%20if%20noise-augmented%20training%20can%20concurrently%0Aimprove%20adversarial%20robustness%20in%20automatic%20speech%20recognition%20%28ASR%29%20systems.%0AWe%20conduct%20a%20comparative%20analysis%20of%20the%20adversarial%20robustness%20of%20four%0Adifferent%20state-of-the-art%20ASR%20architectures%2C%20where%20each%20of%20the%20ASR%0Aarchitectures%20is%20trained%20under%20three%20different%20augmentation%20conditions%3A%20one%0Asubject%20to%20background%20noise%2C%20speed%20variations%2C%20and%20reverberations%2C%20another%0Asubject%20to%20speed%20variations%20only%2C%20and%20a%20third%20without%20any%20form%20of%20data%0Aaugmentation.%20The%20results%20demonstrate%20that%20noise%20augmentation%20not%20only%20improves%0Amodel%20performance%20on%20noisy%20speech%20but%20also%20the%20model%27s%20robustness%20to%0Aadversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01813v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReassessing%2520Noise%2520Augmentation%2520Methods%2520in%2520the%2520Context%2520of%2520Adversarial%250A%2520%2520Speech%26entry.906535625%3DKarla%2520Pizzi%2520and%2520Mat%25C3%25ADas%2520Pizarro%2520and%2520Asja%2520Fischer%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520investigate%2520if%2520noise-augmented%2520training%2520can%2520concurrently%250Aimprove%2520adversarial%2520robustness%2520in%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520systems.%250AWe%2520conduct%2520a%2520comparative%2520analysis%2520of%2520the%2520adversarial%2520robustness%2520of%2520four%250Adifferent%2520state-of-the-art%2520ASR%2520architectures%252C%2520where%2520each%2520of%2520the%2520ASR%250Aarchitectures%2520is%2520trained%2520under%2520three%2520different%2520augmentation%2520conditions%253A%2520one%250Asubject%2520to%2520background%2520noise%252C%2520speed%2520variations%252C%2520and%2520reverberations%252C%2520another%250Asubject%2520to%2520speed%2520variations%2520only%252C%2520and%2520a%2520third%2520without%2520any%2520form%2520of%2520data%250Aaugmentation.%2520The%2520results%2520demonstrate%2520that%2520noise%2520augmentation%2520not%2520only%2520improves%250Amodel%2520performance%2520on%2520noisy%2520speech%2520but%2520also%2520the%2520model%2527s%2520robustness%2520to%250Aadversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01813v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reassessing%20Noise%20Augmentation%20Methods%20in%20the%20Context%20of%20Adversarial%0A%20%20Speech&entry.906535625=Karla%20Pizzi%20and%20Mat%C3%ADas%20Pizarro%20and%20Asja%20Fischer&entry.1292438233=%20%20In%20this%20study%2C%20we%20investigate%20if%20noise-augmented%20training%20can%20concurrently%0Aimprove%20adversarial%20robustness%20in%20automatic%20speech%20recognition%20%28ASR%29%20systems.%0AWe%20conduct%20a%20comparative%20analysis%20of%20the%20adversarial%20robustness%20of%20four%0Adifferent%20state-of-the-art%20ASR%20architectures%2C%20where%20each%20of%20the%20ASR%0Aarchitectures%20is%20trained%20under%20three%20different%20augmentation%20conditions%3A%20one%0Asubject%20to%20background%20noise%2C%20speed%20variations%2C%20and%20reverberations%2C%20another%0Asubject%20to%20speed%20variations%20only%2C%20and%20a%20third%20without%20any%20form%20of%20data%0Aaugmentation.%20The%20results%20demonstrate%20that%20noise%20augmentation%20not%20only%20improves%0Amodel%20performance%20on%20noisy%20speech%20but%20also%20the%20model%27s%20robustness%20to%0Aadversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01813v3&entry.124074799=Read"},
{"title": "Overcoming label shift in targeted federated learning", "author": "Edvin Listo Zec and Adam Breitholtz and Fredrik D. Johansson", "abstract": "  Federated learning enables multiple actors to collaboratively train models\nwithout sharing private data. This unlocks the potential for scaling machine\nlearning to diverse applications. Existing algorithms for this task are\nwell-justified when clients and the intended target domain share the same\ndistribution of features and labels, but this assumption is often violated in\nreal-world scenarios. One common violation is label shift, where the label\ndistributions differ across clients or between clients and the target domain,\nwhich can significantly degrade model performance. To address this problem, we\npropose FedPALS, a novel model aggregation scheme that adapts to label shifts\nby leveraging knowledge of the target label distribution at the central server.\nOur approach ensures unbiased updates under stochastic gradient descent,\nensuring robust generalization across clients with diverse, label-shifted data.\nExtensive experiments on image classification demonstrate that FedPALS\nconsistently outperforms standard baselines by aligning model aggregation with\nthe target domain. Our findings reveal that conventional federated learning\nmethods suffer severely in cases of extreme client sparsity, highlighting the\ncritical need for target-aware aggregation. FedPALS offers a principled and\npractical solution to mitigate label distribution mismatch, ensuring models\ntrained in federated settings can generalize effectively to label-shifted\ntarget domains.\n", "link": "http://arxiv.org/abs/2411.03799v1", "date": "2024-11-06", "relevancy": 1.938, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4882}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4835}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20label%20shift%20in%20targeted%20federated%20learning&body=Title%3A%20Overcoming%20label%20shift%20in%20targeted%20federated%20learning%0AAuthor%3A%20Edvin%20Listo%20Zec%20and%20Adam%20Breitholtz%20and%20Fredrik%20D.%20Johansson%0AAbstract%3A%20%20%20Federated%20learning%20enables%20multiple%20actors%20to%20collaboratively%20train%20models%0Awithout%20sharing%20private%20data.%20This%20unlocks%20the%20potential%20for%20scaling%20machine%0Alearning%20to%20diverse%20applications.%20Existing%20algorithms%20for%20this%20task%20are%0Awell-justified%20when%20clients%20and%20the%20intended%20target%20domain%20share%20the%20same%0Adistribution%20of%20features%20and%20labels%2C%20but%20this%20assumption%20is%20often%20violated%20in%0Areal-world%20scenarios.%20One%20common%20violation%20is%20label%20shift%2C%20where%20the%20label%0Adistributions%20differ%20across%20clients%20or%20between%20clients%20and%20the%20target%20domain%2C%0Awhich%20can%20significantly%20degrade%20model%20performance.%20To%20address%20this%20problem%2C%20we%0Apropose%20FedPALS%2C%20a%20novel%20model%20aggregation%20scheme%20that%20adapts%20to%20label%20shifts%0Aby%20leveraging%20knowledge%20of%20the%20target%20label%20distribution%20at%20the%20central%20server.%0AOur%20approach%20ensures%20unbiased%20updates%20under%20stochastic%20gradient%20descent%2C%0Aensuring%20robust%20generalization%20across%20clients%20with%20diverse%2C%20label-shifted%20data.%0AExtensive%20experiments%20on%20image%20classification%20demonstrate%20that%20FedPALS%0Aconsistently%20outperforms%20standard%20baselines%20by%20aligning%20model%20aggregation%20with%0Athe%20target%20domain.%20Our%20findings%20reveal%20that%20conventional%20federated%20learning%0Amethods%20suffer%20severely%20in%20cases%20of%20extreme%20client%20sparsity%2C%20highlighting%20the%0Acritical%20need%20for%20target-aware%20aggregation.%20FedPALS%20offers%20a%20principled%20and%0Apractical%20solution%20to%20mitigate%20label%20distribution%20mismatch%2C%20ensuring%20models%0Atrained%20in%20federated%20settings%20can%20generalize%20effectively%20to%20label-shifted%0Atarget%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520label%2520shift%2520in%2520targeted%2520federated%2520learning%26entry.906535625%3DEdvin%2520Listo%2520Zec%2520and%2520Adam%2520Breitholtz%2520and%2520Fredrik%2520D.%2520Johansson%26entry.1292438233%3D%2520%2520Federated%2520learning%2520enables%2520multiple%2520actors%2520to%2520collaboratively%2520train%2520models%250Awithout%2520sharing%2520private%2520data.%2520This%2520unlocks%2520the%2520potential%2520for%2520scaling%2520machine%250Alearning%2520to%2520diverse%2520applications.%2520Existing%2520algorithms%2520for%2520this%2520task%2520are%250Awell-justified%2520when%2520clients%2520and%2520the%2520intended%2520target%2520domain%2520share%2520the%2520same%250Adistribution%2520of%2520features%2520and%2520labels%252C%2520but%2520this%2520assumption%2520is%2520often%2520violated%2520in%250Areal-world%2520scenarios.%2520One%2520common%2520violation%2520is%2520label%2520shift%252C%2520where%2520the%2520label%250Adistributions%2520differ%2520across%2520clients%2520or%2520between%2520clients%2520and%2520the%2520target%2520domain%252C%250Awhich%2520can%2520significantly%2520degrade%2520model%2520performance.%2520To%2520address%2520this%2520problem%252C%2520we%250Apropose%2520FedPALS%252C%2520a%2520novel%2520model%2520aggregation%2520scheme%2520that%2520adapts%2520to%2520label%2520shifts%250Aby%2520leveraging%2520knowledge%2520of%2520the%2520target%2520label%2520distribution%2520at%2520the%2520central%2520server.%250AOur%2520approach%2520ensures%2520unbiased%2520updates%2520under%2520stochastic%2520gradient%2520descent%252C%250Aensuring%2520robust%2520generalization%2520across%2520clients%2520with%2520diverse%252C%2520label-shifted%2520data.%250AExtensive%2520experiments%2520on%2520image%2520classification%2520demonstrate%2520that%2520FedPALS%250Aconsistently%2520outperforms%2520standard%2520baselines%2520by%2520aligning%2520model%2520aggregation%2520with%250Athe%2520target%2520domain.%2520Our%2520findings%2520reveal%2520that%2520conventional%2520federated%2520learning%250Amethods%2520suffer%2520severely%2520in%2520cases%2520of%2520extreme%2520client%2520sparsity%252C%2520highlighting%2520the%250Acritical%2520need%2520for%2520target-aware%2520aggregation.%2520FedPALS%2520offers%2520a%2520principled%2520and%250Apractical%2520solution%2520to%2520mitigate%2520label%2520distribution%2520mismatch%252C%2520ensuring%2520models%250Atrained%2520in%2520federated%2520settings%2520can%2520generalize%2520effectively%2520to%2520label-shifted%250Atarget%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20label%20shift%20in%20targeted%20federated%20learning&entry.906535625=Edvin%20Listo%20Zec%20and%20Adam%20Breitholtz%20and%20Fredrik%20D.%20Johansson&entry.1292438233=%20%20Federated%20learning%20enables%20multiple%20actors%20to%20collaboratively%20train%20models%0Awithout%20sharing%20private%20data.%20This%20unlocks%20the%20potential%20for%20scaling%20machine%0Alearning%20to%20diverse%20applications.%20Existing%20algorithms%20for%20this%20task%20are%0Awell-justified%20when%20clients%20and%20the%20intended%20target%20domain%20share%20the%20same%0Adistribution%20of%20features%20and%20labels%2C%20but%20this%20assumption%20is%20often%20violated%20in%0Areal-world%20scenarios.%20One%20common%20violation%20is%20label%20shift%2C%20where%20the%20label%0Adistributions%20differ%20across%20clients%20or%20between%20clients%20and%20the%20target%20domain%2C%0Awhich%20can%20significantly%20degrade%20model%20performance.%20To%20address%20this%20problem%2C%20we%0Apropose%20FedPALS%2C%20a%20novel%20model%20aggregation%20scheme%20that%20adapts%20to%20label%20shifts%0Aby%20leveraging%20knowledge%20of%20the%20target%20label%20distribution%20at%20the%20central%20server.%0AOur%20approach%20ensures%20unbiased%20updates%20under%20stochastic%20gradient%20descent%2C%0Aensuring%20robust%20generalization%20across%20clients%20with%20diverse%2C%20label-shifted%20data.%0AExtensive%20experiments%20on%20image%20classification%20demonstrate%20that%20FedPALS%0Aconsistently%20outperforms%20standard%20baselines%20by%20aligning%20model%20aggregation%20with%0Athe%20target%20domain.%20Our%20findings%20reveal%20that%20conventional%20federated%20learning%0Amethods%20suffer%20severely%20in%20cases%20of%20extreme%20client%20sparsity%2C%20highlighting%20the%0Acritical%20need%20for%20target-aware%20aggregation.%20FedPALS%20offers%20a%20principled%20and%0Apractical%20solution%20to%20mitigate%20label%20distribution%20mismatch%2C%20ensuring%20models%0Atrained%20in%20federated%20settings%20can%20generalize%20effectively%20to%20label-shifted%0Atarget%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03799v1&entry.124074799=Read"},
{"title": "Beyond The Rainbow: High Performance Deep Reinforcement Learning On A\n  Desktop PC", "author": "Tyler Clark and Mark Towers and Christine Evers and Jonathon Hare", "abstract": "  Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent\nenhancements could significantly boost a reinforcement learning (RL) agent's\nperformance. In this paper, we present \"Beyond The Rainbow\" (BTR), a novel\nalgorithm that integrates six improvements from across the RL literature to\nRainbow DQN, establishing a new state-of-the-art for RL using a desktop PC,\nwith a human-normalized interquartile mean (IQM) of 7.4 on atari-60. Beyond\nAtari, we demonstrate BTR's capability to handle complex 3D games, successfully\ntraining agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with\nminimal algorithmic changes. Designing BTR with computational efficiency in\nmind, agents can be trained using a desktop PC on 200 million Atari frames\nwithin 12 hours. Additionally, we conduct detailed ablation studies of each\ncomponent, analzying the performance and impact using numerous measures.\n", "link": "http://arxiv.org/abs/2411.03820v1", "date": "2024-11-06", "relevancy": 1.9233, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4837}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20The%20Rainbow%3A%20High%20Performance%20Deep%20Reinforcement%20Learning%20On%20A%0A%20%20Desktop%20PC&body=Title%3A%20Beyond%20The%20Rainbow%3A%20High%20Performance%20Deep%20Reinforcement%20Learning%20On%20A%0A%20%20Desktop%20PC%0AAuthor%3A%20Tyler%20Clark%20and%20Mark%20Towers%20and%20Christine%20Evers%20and%20Jonathon%20Hare%0AAbstract%3A%20%20%20Rainbow%20Deep%20Q-Network%20%28DQN%29%20demonstrated%20combining%20multiple%20independent%0Aenhancements%20could%20significantly%20boost%20a%20reinforcement%20learning%20%28RL%29%20agent%27s%0Aperformance.%20In%20this%20paper%2C%20we%20present%20%22Beyond%20The%20Rainbow%22%20%28BTR%29%2C%20a%20novel%0Aalgorithm%20that%20integrates%20six%20improvements%20from%20across%20the%20RL%20literature%20to%0ARainbow%20DQN%2C%20establishing%20a%20new%20state-of-the-art%20for%20RL%20using%20a%20desktop%20PC%2C%0Awith%20a%20human-normalized%20interquartile%20mean%20%28IQM%29%20of%207.4%20on%20atari-60.%20Beyond%0AAtari%2C%20we%20demonstrate%20BTR%27s%20capability%20to%20handle%20complex%203D%20games%2C%20successfully%0Atraining%20agents%20to%20play%20Super%20Mario%20Galaxy%2C%20Mario%20Kart%2C%20and%20Mortal%20Kombat%20with%0Aminimal%20algorithmic%20changes.%20Designing%20BTR%20with%20computational%20efficiency%20in%0Amind%2C%20agents%20can%20be%20trained%20using%20a%20desktop%20PC%20on%20200%20million%20Atari%20frames%0Awithin%2012%20hours.%20Additionally%2C%20we%20conduct%20detailed%20ablation%20studies%20of%20each%0Acomponent%2C%20analzying%20the%20performance%20and%20impact%20using%20numerous%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520The%2520Rainbow%253A%2520High%2520Performance%2520Deep%2520Reinforcement%2520Learning%2520On%2520A%250A%2520%2520Desktop%2520PC%26entry.906535625%3DTyler%2520Clark%2520and%2520Mark%2520Towers%2520and%2520Christine%2520Evers%2520and%2520Jonathon%2520Hare%26entry.1292438233%3D%2520%2520Rainbow%2520Deep%2520Q-Network%2520%2528DQN%2529%2520demonstrated%2520combining%2520multiple%2520independent%250Aenhancements%2520could%2520significantly%2520boost%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520agent%2527s%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520present%2520%2522Beyond%2520The%2520Rainbow%2522%2520%2528BTR%2529%252C%2520a%2520novel%250Aalgorithm%2520that%2520integrates%2520six%2520improvements%2520from%2520across%2520the%2520RL%2520literature%2520to%250ARainbow%2520DQN%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520for%2520RL%2520using%2520a%2520desktop%2520PC%252C%250Awith%2520a%2520human-normalized%2520interquartile%2520mean%2520%2528IQM%2529%2520of%25207.4%2520on%2520atari-60.%2520Beyond%250AAtari%252C%2520we%2520demonstrate%2520BTR%2527s%2520capability%2520to%2520handle%2520complex%25203D%2520games%252C%2520successfully%250Atraining%2520agents%2520to%2520play%2520Super%2520Mario%2520Galaxy%252C%2520Mario%2520Kart%252C%2520and%2520Mortal%2520Kombat%2520with%250Aminimal%2520algorithmic%2520changes.%2520Designing%2520BTR%2520with%2520computational%2520efficiency%2520in%250Amind%252C%2520agents%2520can%2520be%2520trained%2520using%2520a%2520desktop%2520PC%2520on%2520200%2520million%2520Atari%2520frames%250Awithin%252012%2520hours.%2520Additionally%252C%2520we%2520conduct%2520detailed%2520ablation%2520studies%2520of%2520each%250Acomponent%252C%2520analzying%2520the%2520performance%2520and%2520impact%2520using%2520numerous%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20The%20Rainbow%3A%20High%20Performance%20Deep%20Reinforcement%20Learning%20On%20A%0A%20%20Desktop%20PC&entry.906535625=Tyler%20Clark%20and%20Mark%20Towers%20and%20Christine%20Evers%20and%20Jonathon%20Hare&entry.1292438233=%20%20Rainbow%20Deep%20Q-Network%20%28DQN%29%20demonstrated%20combining%20multiple%20independent%0Aenhancements%20could%20significantly%20boost%20a%20reinforcement%20learning%20%28RL%29%20agent%27s%0Aperformance.%20In%20this%20paper%2C%20we%20present%20%22Beyond%20The%20Rainbow%22%20%28BTR%29%2C%20a%20novel%0Aalgorithm%20that%20integrates%20six%20improvements%20from%20across%20the%20RL%20literature%20to%0ARainbow%20DQN%2C%20establishing%20a%20new%20state-of-the-art%20for%20RL%20using%20a%20desktop%20PC%2C%0Awith%20a%20human-normalized%20interquartile%20mean%20%28IQM%29%20of%207.4%20on%20atari-60.%20Beyond%0AAtari%2C%20we%20demonstrate%20BTR%27s%20capability%20to%20handle%20complex%203D%20games%2C%20successfully%0Atraining%20agents%20to%20play%20Super%20Mario%20Galaxy%2C%20Mario%20Kart%2C%20and%20Mortal%20Kombat%20with%0Aminimal%20algorithmic%20changes.%20Designing%20BTR%20with%20computational%20efficiency%20in%0Amind%2C%20agents%20can%20be%20trained%20using%20a%20desktop%20PC%20on%20200%20million%20Atari%20frames%0Awithin%2012%20hours.%20Additionally%2C%20we%20conduct%20detailed%20ablation%20studies%20of%20each%0Acomponent%2C%20analzying%20the%20performance%20and%20impact%20using%20numerous%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03820v1&entry.124074799=Read"},
{"title": "Efficient Message Passing Architecture for GCN Training on HBM-based\n  FPGAs with Orthogonal Topology On-Chip Networks", "author": "Qizhe Wu and Letian Zhao and Yuchen Gui and Huawen Liang Xiaotian Wang", "abstract": "  Graph Convolutional Networks (GCNs) are state-of-the-art deep learning models\nfor representation learning on graphs. However, the efficient training of GCNs\nis hampered by constraints in memory capacity and bandwidth, compounded by the\nirregular data flow that results in communication bottlenecks. To address these\nchallenges, we propose a message-passing architecture that leverages NUMA-based\nmemory access properties and employs a parallel multicast routing algorithm\nbased on a 4-D hypercube network within the accelerator for efficient message\npassing in graphs. Additionally, we have re-engineered the backpropagation\nalgorithm specific to GCNs within our proposed accelerator. This redesign\nstrategically mitigates the memory demands prevalent during the training phase\nand diminishes the computational overhead associated with the transposition of\nextensive matrices. Compared to the state-of-the-art HP-GNN architecture we\nachieved a performance improvement of $1.03\\times \\sim 1.81\\times$.\n", "link": "http://arxiv.org/abs/2411.03857v1", "date": "2024-11-06", "relevancy": 1.9157, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4978}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4655}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Message%20Passing%20Architecture%20for%20GCN%20Training%20on%20HBM-based%0A%20%20FPGAs%20with%20Orthogonal%20Topology%20On-Chip%20Networks&body=Title%3A%20Efficient%20Message%20Passing%20Architecture%20for%20GCN%20Training%20on%20HBM-based%0A%20%20FPGAs%20with%20Orthogonal%20Topology%20On-Chip%20Networks%0AAuthor%3A%20Qizhe%20Wu%20and%20Letian%20Zhao%20and%20Yuchen%20Gui%20and%20Huawen%20Liang%20Xiaotian%20Wang%0AAbstract%3A%20%20%20Graph%20Convolutional%20Networks%20%28GCNs%29%20are%20state-of-the-art%20deep%20learning%20models%0Afor%20representation%20learning%20on%20graphs.%20However%2C%20the%20efficient%20training%20of%20GCNs%0Ais%20hampered%20by%20constraints%20in%20memory%20capacity%20and%20bandwidth%2C%20compounded%20by%20the%0Airregular%20data%20flow%20that%20results%20in%20communication%20bottlenecks.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20message-passing%20architecture%20that%20leverages%20NUMA-based%0Amemory%20access%20properties%20and%20employs%20a%20parallel%20multicast%20routing%20algorithm%0Abased%20on%20a%204-D%20hypercube%20network%20within%20the%20accelerator%20for%20efficient%20message%0Apassing%20in%20graphs.%20Additionally%2C%20we%20have%20re-engineered%20the%20backpropagation%0Aalgorithm%20specific%20to%20GCNs%20within%20our%20proposed%20accelerator.%20This%20redesign%0Astrategically%20mitigates%20the%20memory%20demands%20prevalent%20during%20the%20training%20phase%0Aand%20diminishes%20the%20computational%20overhead%20associated%20with%20the%20transposition%20of%0Aextensive%20matrices.%20Compared%20to%20the%20state-of-the-art%20HP-GNN%20architecture%20we%0Aachieved%20a%20performance%20improvement%20of%20%241.03%5Ctimes%20%5Csim%201.81%5Ctimes%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Message%2520Passing%2520Architecture%2520for%2520GCN%2520Training%2520on%2520HBM-based%250A%2520%2520FPGAs%2520with%2520Orthogonal%2520Topology%2520On-Chip%2520Networks%26entry.906535625%3DQizhe%2520Wu%2520and%2520Letian%2520Zhao%2520and%2520Yuchen%2520Gui%2520and%2520Huawen%2520Liang%2520Xiaotian%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520are%2520state-of-the-art%2520deep%2520learning%2520models%250Afor%2520representation%2520learning%2520on%2520graphs.%2520However%252C%2520the%2520efficient%2520training%2520of%2520GCNs%250Ais%2520hampered%2520by%2520constraints%2520in%2520memory%2520capacity%2520and%2520bandwidth%252C%2520compounded%2520by%2520the%250Airregular%2520data%2520flow%2520that%2520results%2520in%2520communication%2520bottlenecks.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520message-passing%2520architecture%2520that%2520leverages%2520NUMA-based%250Amemory%2520access%2520properties%2520and%2520employs%2520a%2520parallel%2520multicast%2520routing%2520algorithm%250Abased%2520on%2520a%25204-D%2520hypercube%2520network%2520within%2520the%2520accelerator%2520for%2520efficient%2520message%250Apassing%2520in%2520graphs.%2520Additionally%252C%2520we%2520have%2520re-engineered%2520the%2520backpropagation%250Aalgorithm%2520specific%2520to%2520GCNs%2520within%2520our%2520proposed%2520accelerator.%2520This%2520redesign%250Astrategically%2520mitigates%2520the%2520memory%2520demands%2520prevalent%2520during%2520the%2520training%2520phase%250Aand%2520diminishes%2520the%2520computational%2520overhead%2520associated%2520with%2520the%2520transposition%2520of%250Aextensive%2520matrices.%2520Compared%2520to%2520the%2520state-of-the-art%2520HP-GNN%2520architecture%2520we%250Aachieved%2520a%2520performance%2520improvement%2520of%2520%25241.03%255Ctimes%2520%255Csim%25201.81%255Ctimes%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Message%20Passing%20Architecture%20for%20GCN%20Training%20on%20HBM-based%0A%20%20FPGAs%20with%20Orthogonal%20Topology%20On-Chip%20Networks&entry.906535625=Qizhe%20Wu%20and%20Letian%20Zhao%20and%20Yuchen%20Gui%20and%20Huawen%20Liang%20Xiaotian%20Wang&entry.1292438233=%20%20Graph%20Convolutional%20Networks%20%28GCNs%29%20are%20state-of-the-art%20deep%20learning%20models%0Afor%20representation%20learning%20on%20graphs.%20However%2C%20the%20efficient%20training%20of%20GCNs%0Ais%20hampered%20by%20constraints%20in%20memory%20capacity%20and%20bandwidth%2C%20compounded%20by%20the%0Airregular%20data%20flow%20that%20results%20in%20communication%20bottlenecks.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20message-passing%20architecture%20that%20leverages%20NUMA-based%0Amemory%20access%20properties%20and%20employs%20a%20parallel%20multicast%20routing%20algorithm%0Abased%20on%20a%204-D%20hypercube%20network%20within%20the%20accelerator%20for%20efficient%20message%0Apassing%20in%20graphs.%20Additionally%2C%20we%20have%20re-engineered%20the%20backpropagation%0Aalgorithm%20specific%20to%20GCNs%20within%20our%20proposed%20accelerator.%20This%20redesign%0Astrategically%20mitigates%20the%20memory%20demands%20prevalent%20during%20the%20training%20phase%0Aand%20diminishes%20the%20computational%20overhead%20associated%20with%20the%20transposition%20of%0Aextensive%20matrices.%20Compared%20to%20the%20state-of-the-art%20HP-GNN%20architecture%20we%0Aachieved%20a%20performance%20improvement%20of%20%241.03%5Ctimes%20%5Csim%201.81%5Ctimes%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03857v1&entry.124074799=Read"},
{"title": "DistriBlock: Identifying adversarial audio samples by leveraging\n  characteristics of the output distribution", "author": "Mat\u00edas Pizarro and Dorothea Kolossa and Asja Fischer", "abstract": "  Adversarial attacks can mislead automatic speech recognition (ASR) systems\ninto predicting an arbitrary target text, thus posing a clear security threat.\nTo prevent such attacks, we propose DistriBlock, an efficient detection\nstrategy applicable to any ASR system that predicts a probability distribution\nover output tokens in each time step. We measure a set of characteristics of\nthis distribution: the median, maximum, and minimum over the output\nprobabilities, the entropy of the distribution, as well as the Kullback-Leibler\nand the Jensen-Shannon divergence with respect to the distributions of the\nsubsequent time step. Then, by leveraging the characteristics observed for both\nbenign and adversarial data, we apply binary classifiers, including simple\nthreshold-based classification, ensembles of such classifiers, and neural\nnetworks. Through extensive analysis across different state-of-the-art ASR\nsystems and language data sets, we demonstrate the supreme performance of this\napproach, with a mean area under the receiver operating characteristic curve\nfor distinguishing target adversarial examples against clean and noisy data of\n99% and 97%, respectively. To assess the robustness of our method, we show that\nadaptive adversarial examples that can circumvent DistriBlock are much noisier,\nwhich makes them easier to detect through filtering and creates another avenue\nfor preserving the system's robustness.\n", "link": "http://arxiv.org/abs/2305.17000v8", "date": "2024-11-06", "relevancy": 1.9, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5494}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.428}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DistriBlock%3A%20Identifying%20adversarial%20audio%20samples%20by%20leveraging%0A%20%20characteristics%20of%20the%20output%20distribution&body=Title%3A%20DistriBlock%3A%20Identifying%20adversarial%20audio%20samples%20by%20leveraging%0A%20%20characteristics%20of%20the%20output%20distribution%0AAuthor%3A%20Mat%C3%ADas%20Pizarro%20and%20Dorothea%20Kolossa%20and%20Asja%20Fischer%0AAbstract%3A%20%20%20Adversarial%20attacks%20can%20mislead%20automatic%20speech%20recognition%20%28ASR%29%20systems%0Ainto%20predicting%20an%20arbitrary%20target%20text%2C%20thus%20posing%20a%20clear%20security%20threat.%0ATo%20prevent%20such%20attacks%2C%20we%20propose%20DistriBlock%2C%20an%20efficient%20detection%0Astrategy%20applicable%20to%20any%20ASR%20system%20that%20predicts%20a%20probability%20distribution%0Aover%20output%20tokens%20in%20each%20time%20step.%20We%20measure%20a%20set%20of%20characteristics%20of%0Athis%20distribution%3A%20the%20median%2C%20maximum%2C%20and%20minimum%20over%20the%20output%0Aprobabilities%2C%20the%20entropy%20of%20the%20distribution%2C%20as%20well%20as%20the%20Kullback-Leibler%0Aand%20the%20Jensen-Shannon%20divergence%20with%20respect%20to%20the%20distributions%20of%20the%0Asubsequent%20time%20step.%20Then%2C%20by%20leveraging%20the%20characteristics%20observed%20for%20both%0Abenign%20and%20adversarial%20data%2C%20we%20apply%20binary%20classifiers%2C%20including%20simple%0Athreshold-based%20classification%2C%20ensembles%20of%20such%20classifiers%2C%20and%20neural%0Anetworks.%20Through%20extensive%20analysis%20across%20different%20state-of-the-art%20ASR%0Asystems%20and%20language%20data%20sets%2C%20we%20demonstrate%20the%20supreme%20performance%20of%20this%0Aapproach%2C%20with%20a%20mean%20area%20under%20the%20receiver%20operating%20characteristic%20curve%0Afor%20distinguishing%20target%20adversarial%20examples%20against%20clean%20and%20noisy%20data%20of%0A99%25%20and%2097%25%2C%20respectively.%20To%20assess%20the%20robustness%20of%20our%20method%2C%20we%20show%20that%0Aadaptive%20adversarial%20examples%20that%20can%20circumvent%20DistriBlock%20are%20much%20noisier%2C%0Awhich%20makes%20them%20easier%20to%20detect%20through%20filtering%20and%20creates%20another%20avenue%0Afor%20preserving%20the%20system%27s%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17000v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistriBlock%253A%2520Identifying%2520adversarial%2520audio%2520samples%2520by%2520leveraging%250A%2520%2520characteristics%2520of%2520the%2520output%2520distribution%26entry.906535625%3DMat%25C3%25ADas%2520Pizarro%2520and%2520Dorothea%2520Kolossa%2520and%2520Asja%2520Fischer%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520can%2520mislead%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520systems%250Ainto%2520predicting%2520an%2520arbitrary%2520target%2520text%252C%2520thus%2520posing%2520a%2520clear%2520security%2520threat.%250ATo%2520prevent%2520such%2520attacks%252C%2520we%2520propose%2520DistriBlock%252C%2520an%2520efficient%2520detection%250Astrategy%2520applicable%2520to%2520any%2520ASR%2520system%2520that%2520predicts%2520a%2520probability%2520distribution%250Aover%2520output%2520tokens%2520in%2520each%2520time%2520step.%2520We%2520measure%2520a%2520set%2520of%2520characteristics%2520of%250Athis%2520distribution%253A%2520the%2520median%252C%2520maximum%252C%2520and%2520minimum%2520over%2520the%2520output%250Aprobabilities%252C%2520the%2520entropy%2520of%2520the%2520distribution%252C%2520as%2520well%2520as%2520the%2520Kullback-Leibler%250Aand%2520the%2520Jensen-Shannon%2520divergence%2520with%2520respect%2520to%2520the%2520distributions%2520of%2520the%250Asubsequent%2520time%2520step.%2520Then%252C%2520by%2520leveraging%2520the%2520characteristics%2520observed%2520for%2520both%250Abenign%2520and%2520adversarial%2520data%252C%2520we%2520apply%2520binary%2520classifiers%252C%2520including%2520simple%250Athreshold-based%2520classification%252C%2520ensembles%2520of%2520such%2520classifiers%252C%2520and%2520neural%250Anetworks.%2520Through%2520extensive%2520analysis%2520across%2520different%2520state-of-the-art%2520ASR%250Asystems%2520and%2520language%2520data%2520sets%252C%2520we%2520demonstrate%2520the%2520supreme%2520performance%2520of%2520this%250Aapproach%252C%2520with%2520a%2520mean%2520area%2520under%2520the%2520receiver%2520operating%2520characteristic%2520curve%250Afor%2520distinguishing%2520target%2520adversarial%2520examples%2520against%2520clean%2520and%2520noisy%2520data%2520of%250A99%2525%2520and%252097%2525%252C%2520respectively.%2520To%2520assess%2520the%2520robustness%2520of%2520our%2520method%252C%2520we%2520show%2520that%250Aadaptive%2520adversarial%2520examples%2520that%2520can%2520circumvent%2520DistriBlock%2520are%2520much%2520noisier%252C%250Awhich%2520makes%2520them%2520easier%2520to%2520detect%2520through%2520filtering%2520and%2520creates%2520another%2520avenue%250Afor%2520preserving%2520the%2520system%2527s%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.17000v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DistriBlock%3A%20Identifying%20adversarial%20audio%20samples%20by%20leveraging%0A%20%20characteristics%20of%20the%20output%20distribution&entry.906535625=Mat%C3%ADas%20Pizarro%20and%20Dorothea%20Kolossa%20and%20Asja%20Fischer&entry.1292438233=%20%20Adversarial%20attacks%20can%20mislead%20automatic%20speech%20recognition%20%28ASR%29%20systems%0Ainto%20predicting%20an%20arbitrary%20target%20text%2C%20thus%20posing%20a%20clear%20security%20threat.%0ATo%20prevent%20such%20attacks%2C%20we%20propose%20DistriBlock%2C%20an%20efficient%20detection%0Astrategy%20applicable%20to%20any%20ASR%20system%20that%20predicts%20a%20probability%20distribution%0Aover%20output%20tokens%20in%20each%20time%20step.%20We%20measure%20a%20set%20of%20characteristics%20of%0Athis%20distribution%3A%20the%20median%2C%20maximum%2C%20and%20minimum%20over%20the%20output%0Aprobabilities%2C%20the%20entropy%20of%20the%20distribution%2C%20as%20well%20as%20the%20Kullback-Leibler%0Aand%20the%20Jensen-Shannon%20divergence%20with%20respect%20to%20the%20distributions%20of%20the%0Asubsequent%20time%20step.%20Then%2C%20by%20leveraging%20the%20characteristics%20observed%20for%20both%0Abenign%20and%20adversarial%20data%2C%20we%20apply%20binary%20classifiers%2C%20including%20simple%0Athreshold-based%20classification%2C%20ensembles%20of%20such%20classifiers%2C%20and%20neural%0Anetworks.%20Through%20extensive%20analysis%20across%20different%20state-of-the-art%20ASR%0Asystems%20and%20language%20data%20sets%2C%20we%20demonstrate%20the%20supreme%20performance%20of%20this%0Aapproach%2C%20with%20a%20mean%20area%20under%20the%20receiver%20operating%20characteristic%20curve%0Afor%20distinguishing%20target%20adversarial%20examples%20against%20clean%20and%20noisy%20data%20of%0A99%25%20and%2097%25%2C%20respectively.%20To%20assess%20the%20robustness%20of%20our%20method%2C%20we%20show%20that%0Aadaptive%20adversarial%20examples%20that%20can%20circumvent%20DistriBlock%20are%20much%20noisier%2C%0Awhich%20makes%20them%20easier%20to%20detect%20through%20filtering%20and%20creates%20another%20avenue%0Afor%20preserving%20the%20system%27s%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17000v8&entry.124074799=Read"},
{"title": "Pretraining and Updates of Domain-Specific LLM: A Case Study in the\n  Japanese Business Domain", "author": "Kosuke Takahashi and Takahiro Omi and Kosuke Arima and Tatsuya Ishigaki", "abstract": "  The development of Large Language Models (LLMs) in various languages has been\nadvancing, but the combination of non-English languages with domain-specific\ncontexts remains underexplored. This paper presents our findings from training\nand evaluating a Japanese business domain-specific LLM designed to better\nunderstand business-related documents, such as the news on current affairs,\ntechnical reports, and patents. Additionally, LLMs in this domain require\nregular updates to incorporate the most recent knowledge. Therefore, we also\nreport our findings from the first experiments and evaluations involving\nupdates to this LLM using the latest article data, which is an important\nproblem setting that has not been addressed in previous research. From our\nexperiments on a newly created benchmark dataset for question answering in the\ntarget domain, we found that (1) our pretrained model improves QA accuracy\nwithout losing general knowledge, and (2) a proper mixture of the latest and\nolder texts in the training data for the update is necessary. Our pretrained\nmodel and business domain benchmark are publicly available to support further\nstudies.\n", "link": "http://arxiv.org/abs/2404.08262v3", "date": "2024-11-06", "relevancy": 1.8949, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretraining%20and%20Updates%20of%20Domain-Specific%20LLM%3A%20A%20Case%20Study%20in%20the%0A%20%20Japanese%20Business%20Domain&body=Title%3A%20Pretraining%20and%20Updates%20of%20Domain-Specific%20LLM%3A%20A%20Case%20Study%20in%20the%0A%20%20Japanese%20Business%20Domain%0AAuthor%3A%20Kosuke%20Takahashi%20and%20Takahiro%20Omi%20and%20Kosuke%20Arima%20and%20Tatsuya%20Ishigaki%0AAbstract%3A%20%20%20The%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20various%20languages%20has%20been%0Aadvancing%2C%20but%20the%20combination%20of%20non-English%20languages%20with%20domain-specific%0Acontexts%20remains%20underexplored.%20This%20paper%20presents%20our%20findings%20from%20training%0Aand%20evaluating%20a%20Japanese%20business%20domain-specific%20LLM%20designed%20to%20better%0Aunderstand%20business-related%20documents%2C%20such%20as%20the%20news%20on%20current%20affairs%2C%0Atechnical%20reports%2C%20and%20patents.%20Additionally%2C%20LLMs%20in%20this%20domain%20require%0Aregular%20updates%20to%20incorporate%20the%20most%20recent%20knowledge.%20Therefore%2C%20we%20also%0Areport%20our%20findings%20from%20the%20first%20experiments%20and%20evaluations%20involving%0Aupdates%20to%20this%20LLM%20using%20the%20latest%20article%20data%2C%20which%20is%20an%20important%0Aproblem%20setting%20that%20has%20not%20been%20addressed%20in%20previous%20research.%20From%20our%0Aexperiments%20on%20a%20newly%20created%20benchmark%20dataset%20for%20question%20answering%20in%20the%0Atarget%20domain%2C%20we%20found%20that%20%281%29%20our%20pretrained%20model%20improves%20QA%20accuracy%0Awithout%20losing%20general%20knowledge%2C%20and%20%282%29%20a%20proper%20mixture%20of%20the%20latest%20and%0Aolder%20texts%20in%20the%20training%20data%20for%20the%20update%20is%20necessary.%20Our%20pretrained%0Amodel%20and%20business%20domain%20benchmark%20are%20publicly%20available%20to%20support%20further%0Astudies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08262v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretraining%2520and%2520Updates%2520of%2520Domain-Specific%2520LLM%253A%2520A%2520Case%2520Study%2520in%2520the%250A%2520%2520Japanese%2520Business%2520Domain%26entry.906535625%3DKosuke%2520Takahashi%2520and%2520Takahiro%2520Omi%2520and%2520Kosuke%2520Arima%2520and%2520Tatsuya%2520Ishigaki%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520various%2520languages%2520has%2520been%250Aadvancing%252C%2520but%2520the%2520combination%2520of%2520non-English%2520languages%2520with%2520domain-specific%250Acontexts%2520remains%2520underexplored.%2520This%2520paper%2520presents%2520our%2520findings%2520from%2520training%250Aand%2520evaluating%2520a%2520Japanese%2520business%2520domain-specific%2520LLM%2520designed%2520to%2520better%250Aunderstand%2520business-related%2520documents%252C%2520such%2520as%2520the%2520news%2520on%2520current%2520affairs%252C%250Atechnical%2520reports%252C%2520and%2520patents.%2520Additionally%252C%2520LLMs%2520in%2520this%2520domain%2520require%250Aregular%2520updates%2520to%2520incorporate%2520the%2520most%2520recent%2520knowledge.%2520Therefore%252C%2520we%2520also%250Areport%2520our%2520findings%2520from%2520the%2520first%2520experiments%2520and%2520evaluations%2520involving%250Aupdates%2520to%2520this%2520LLM%2520using%2520the%2520latest%2520article%2520data%252C%2520which%2520is%2520an%2520important%250Aproblem%2520setting%2520that%2520has%2520not%2520been%2520addressed%2520in%2520previous%2520research.%2520From%2520our%250Aexperiments%2520on%2520a%2520newly%2520created%2520benchmark%2520dataset%2520for%2520question%2520answering%2520in%2520the%250Atarget%2520domain%252C%2520we%2520found%2520that%2520%25281%2529%2520our%2520pretrained%2520model%2520improves%2520QA%2520accuracy%250Awithout%2520losing%2520general%2520knowledge%252C%2520and%2520%25282%2529%2520a%2520proper%2520mixture%2520of%2520the%2520latest%2520and%250Aolder%2520texts%2520in%2520the%2520training%2520data%2520for%2520the%2520update%2520is%2520necessary.%2520Our%2520pretrained%250Amodel%2520and%2520business%2520domain%2520benchmark%2520are%2520publicly%2520available%2520to%2520support%2520further%250Astudies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08262v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretraining%20and%20Updates%20of%20Domain-Specific%20LLM%3A%20A%20Case%20Study%20in%20the%0A%20%20Japanese%20Business%20Domain&entry.906535625=Kosuke%20Takahashi%20and%20Takahiro%20Omi%20and%20Kosuke%20Arima%20and%20Tatsuya%20Ishigaki&entry.1292438233=%20%20The%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20various%20languages%20has%20been%0Aadvancing%2C%20but%20the%20combination%20of%20non-English%20languages%20with%20domain-specific%0Acontexts%20remains%20underexplored.%20This%20paper%20presents%20our%20findings%20from%20training%0Aand%20evaluating%20a%20Japanese%20business%20domain-specific%20LLM%20designed%20to%20better%0Aunderstand%20business-related%20documents%2C%20such%20as%20the%20news%20on%20current%20affairs%2C%0Atechnical%20reports%2C%20and%20patents.%20Additionally%2C%20LLMs%20in%20this%20domain%20require%0Aregular%20updates%20to%20incorporate%20the%20most%20recent%20knowledge.%20Therefore%2C%20we%20also%0Areport%20our%20findings%20from%20the%20first%20experiments%20and%20evaluations%20involving%0Aupdates%20to%20this%20LLM%20using%20the%20latest%20article%20data%2C%20which%20is%20an%20important%0Aproblem%20setting%20that%20has%20not%20been%20addressed%20in%20previous%20research.%20From%20our%0Aexperiments%20on%20a%20newly%20created%20benchmark%20dataset%20for%20question%20answering%20in%20the%0Atarget%20domain%2C%20we%20found%20that%20%281%29%20our%20pretrained%20model%20improves%20QA%20accuracy%0Awithout%20losing%20general%20knowledge%2C%20and%20%282%29%20a%20proper%20mixture%20of%20the%20latest%20and%0Aolder%20texts%20in%20the%20training%20data%20for%20the%20update%20is%20necessary.%20Our%20pretrained%0Amodel%20and%20business%20domain%20benchmark%20are%20publicly%20available%20to%20support%20further%0Astudies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08262v3&entry.124074799=Read"},
{"title": "Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage", "author": "Hengzhu Liu and Tianqing Zhu and Lefeng Zhang and Ping Xiong", "abstract": "  With the extensive use of machine learning technologies, data providers\nencounter increasing privacy risks. Recent legislation, such as GDPR, obligates\norganizations to remove requested data and its influence from a trained model.\nMachine unlearning is an emerging technique designed to enable machine learning\nmodels to erase users' private information. Although several efficient machine\nunlearning schemes have been proposed, these methods still have limitations.\nFirst, removing the contributions of partial data may lead to model performance\ndegradation. Second, discrepancies between the original and generated unlearned\nmodels can be exploited by attackers to obtain target sample's information,\nresulting in additional privacy leakage risks. To address above challenges, we\nproposed a game-theoretic machine unlearning algorithm that simulates the\ncompetitive relationship between unlearning performance and privacy protection.\nThis algorithm comprises unlearning and privacy modules. The unlearning module\npossesses a loss function composed of model distance and classification error,\nwhich is used to derive the optimal strategy. The privacy module aims to make\nit difficult for an attacker to infer membership information from the unlearned\ndata, thereby reducing the privacy leakage risk during the unlearning process.\nAdditionally, the experimental results on real-world datasets demonstrate that\nthis game-theoretic unlearning algorithm's effectiveness and its ability to\ngenerate an unlearned model with a performance similar to that of the retrained\none while mitigating extra privacy leakage risks.\n", "link": "http://arxiv.org/abs/2411.03914v1", "date": "2024-11-06", "relevancy": 1.8895, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5247}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4652}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Game-Theoretic%20Machine%20Unlearning%3A%20Mitigating%20Extra%20Privacy%20Leakage&body=Title%3A%20Game-Theoretic%20Machine%20Unlearning%3A%20Mitigating%20Extra%20Privacy%20Leakage%0AAuthor%3A%20Hengzhu%20Liu%20and%20Tianqing%20Zhu%20and%20Lefeng%20Zhang%20and%20Ping%20Xiong%0AAbstract%3A%20%20%20With%20the%20extensive%20use%20of%20machine%20learning%20technologies%2C%20data%20providers%0Aencounter%20increasing%20privacy%20risks.%20Recent%20legislation%2C%20such%20as%20GDPR%2C%20obligates%0Aorganizations%20to%20remove%20requested%20data%20and%20its%20influence%20from%20a%20trained%20model.%0AMachine%20unlearning%20is%20an%20emerging%20technique%20designed%20to%20enable%20machine%20learning%0Amodels%20to%20erase%20users%27%20private%20information.%20Although%20several%20efficient%20machine%0Aunlearning%20schemes%20have%20been%20proposed%2C%20these%20methods%20still%20have%20limitations.%0AFirst%2C%20removing%20the%20contributions%20of%20partial%20data%20may%20lead%20to%20model%20performance%0Adegradation.%20Second%2C%20discrepancies%20between%20the%20original%20and%20generated%20unlearned%0Amodels%20can%20be%20exploited%20by%20attackers%20to%20obtain%20target%20sample%27s%20information%2C%0Aresulting%20in%20additional%20privacy%20leakage%20risks.%20To%20address%20above%20challenges%2C%20we%0Aproposed%20a%20game-theoretic%20machine%20unlearning%20algorithm%20that%20simulates%20the%0Acompetitive%20relationship%20between%20unlearning%20performance%20and%20privacy%20protection.%0AThis%20algorithm%20comprises%20unlearning%20and%20privacy%20modules.%20The%20unlearning%20module%0Apossesses%20a%20loss%20function%20composed%20of%20model%20distance%20and%20classification%20error%2C%0Awhich%20is%20used%20to%20derive%20the%20optimal%20strategy.%20The%20privacy%20module%20aims%20to%20make%0Ait%20difficult%20for%20an%20attacker%20to%20infer%20membership%20information%20from%20the%20unlearned%0Adata%2C%20thereby%20reducing%20the%20privacy%20leakage%20risk%20during%20the%20unlearning%20process.%0AAdditionally%2C%20the%20experimental%20results%20on%20real-world%20datasets%20demonstrate%20that%0Athis%20game-theoretic%20unlearning%20algorithm%27s%20effectiveness%20and%20its%20ability%20to%0Agenerate%20an%20unlearned%20model%20with%20a%20performance%20similar%20to%20that%20of%20the%20retrained%0Aone%20while%20mitigating%20extra%20privacy%20leakage%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGame-Theoretic%2520Machine%2520Unlearning%253A%2520Mitigating%2520Extra%2520Privacy%2520Leakage%26entry.906535625%3DHengzhu%2520Liu%2520and%2520Tianqing%2520Zhu%2520and%2520Lefeng%2520Zhang%2520and%2520Ping%2520Xiong%26entry.1292438233%3D%2520%2520With%2520the%2520extensive%2520use%2520of%2520machine%2520learning%2520technologies%252C%2520data%2520providers%250Aencounter%2520increasing%2520privacy%2520risks.%2520Recent%2520legislation%252C%2520such%2520as%2520GDPR%252C%2520obligates%250Aorganizations%2520to%2520remove%2520requested%2520data%2520and%2520its%2520influence%2520from%2520a%2520trained%2520model.%250AMachine%2520unlearning%2520is%2520an%2520emerging%2520technique%2520designed%2520to%2520enable%2520machine%2520learning%250Amodels%2520to%2520erase%2520users%2527%2520private%2520information.%2520Although%2520several%2520efficient%2520machine%250Aunlearning%2520schemes%2520have%2520been%2520proposed%252C%2520these%2520methods%2520still%2520have%2520limitations.%250AFirst%252C%2520removing%2520the%2520contributions%2520of%2520partial%2520data%2520may%2520lead%2520to%2520model%2520performance%250Adegradation.%2520Second%252C%2520discrepancies%2520between%2520the%2520original%2520and%2520generated%2520unlearned%250Amodels%2520can%2520be%2520exploited%2520by%2520attackers%2520to%2520obtain%2520target%2520sample%2527s%2520information%252C%250Aresulting%2520in%2520additional%2520privacy%2520leakage%2520risks.%2520To%2520address%2520above%2520challenges%252C%2520we%250Aproposed%2520a%2520game-theoretic%2520machine%2520unlearning%2520algorithm%2520that%2520simulates%2520the%250Acompetitive%2520relationship%2520between%2520unlearning%2520performance%2520and%2520privacy%2520protection.%250AThis%2520algorithm%2520comprises%2520unlearning%2520and%2520privacy%2520modules.%2520The%2520unlearning%2520module%250Apossesses%2520a%2520loss%2520function%2520composed%2520of%2520model%2520distance%2520and%2520classification%2520error%252C%250Awhich%2520is%2520used%2520to%2520derive%2520the%2520optimal%2520strategy.%2520The%2520privacy%2520module%2520aims%2520to%2520make%250Ait%2520difficult%2520for%2520an%2520attacker%2520to%2520infer%2520membership%2520information%2520from%2520the%2520unlearned%250Adata%252C%2520thereby%2520reducing%2520the%2520privacy%2520leakage%2520risk%2520during%2520the%2520unlearning%2520process.%250AAdditionally%252C%2520the%2520experimental%2520results%2520on%2520real-world%2520datasets%2520demonstrate%2520that%250Athis%2520game-theoretic%2520unlearning%2520algorithm%2527s%2520effectiveness%2520and%2520its%2520ability%2520to%250Agenerate%2520an%2520unlearned%2520model%2520with%2520a%2520performance%2520similar%2520to%2520that%2520of%2520the%2520retrained%250Aone%2520while%2520mitigating%2520extra%2520privacy%2520leakage%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Game-Theoretic%20Machine%20Unlearning%3A%20Mitigating%20Extra%20Privacy%20Leakage&entry.906535625=Hengzhu%20Liu%20and%20Tianqing%20Zhu%20and%20Lefeng%20Zhang%20and%20Ping%20Xiong&entry.1292438233=%20%20With%20the%20extensive%20use%20of%20machine%20learning%20technologies%2C%20data%20providers%0Aencounter%20increasing%20privacy%20risks.%20Recent%20legislation%2C%20such%20as%20GDPR%2C%20obligates%0Aorganizations%20to%20remove%20requested%20data%20and%20its%20influence%20from%20a%20trained%20model.%0AMachine%20unlearning%20is%20an%20emerging%20technique%20designed%20to%20enable%20machine%20learning%0Amodels%20to%20erase%20users%27%20private%20information.%20Although%20several%20efficient%20machine%0Aunlearning%20schemes%20have%20been%20proposed%2C%20these%20methods%20still%20have%20limitations.%0AFirst%2C%20removing%20the%20contributions%20of%20partial%20data%20may%20lead%20to%20model%20performance%0Adegradation.%20Second%2C%20discrepancies%20between%20the%20original%20and%20generated%20unlearned%0Amodels%20can%20be%20exploited%20by%20attackers%20to%20obtain%20target%20sample%27s%20information%2C%0Aresulting%20in%20additional%20privacy%20leakage%20risks.%20To%20address%20above%20challenges%2C%20we%0Aproposed%20a%20game-theoretic%20machine%20unlearning%20algorithm%20that%20simulates%20the%0Acompetitive%20relationship%20between%20unlearning%20performance%20and%20privacy%20protection.%0AThis%20algorithm%20comprises%20unlearning%20and%20privacy%20modules.%20The%20unlearning%20module%0Apossesses%20a%20loss%20function%20composed%20of%20model%20distance%20and%20classification%20error%2C%0Awhich%20is%20used%20to%20derive%20the%20optimal%20strategy.%20The%20privacy%20module%20aims%20to%20make%0Ait%20difficult%20for%20an%20attacker%20to%20infer%20membership%20information%20from%20the%20unlearned%0Adata%2C%20thereby%20reducing%20the%20privacy%20leakage%20risk%20during%20the%20unlearning%20process.%0AAdditionally%2C%20the%20experimental%20results%20on%20real-world%20datasets%20demonstrate%20that%0Athis%20game-theoretic%20unlearning%20algorithm%27s%20effectiveness%20and%20its%20ability%20to%0Agenerate%20an%20unlearned%20model%20with%20a%20performance%20similar%20to%20that%20of%20the%20retrained%0Aone%20while%20mitigating%20extra%20privacy%20leakage%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03914v1&entry.124074799=Read"},
{"title": "Advantages of Neural Population Coding for Deep Learning", "author": "Heiko Hoffmann", "abstract": "  Scalar variables, e.g., the orientation of a shape in an image, are commonly\npredicted using a single output neuron in a neural network. In contrast, the\nmammalian cortex represents variables with a population of neurons. In this\npopulation code, each neuron is most active at its preferred value and shows\npartial activity for other values. Here, we investigate the benefit of using a\npopulation code for the output layer of a neural network. We compare population\ncodes against single-neuron outputs and one-hot vectors. First, we show\ntheoretically and in experiments with synthetic data that population codes\nimprove robustness to input noise in networks of stacked linear layers. Second,\nwe demonstrate the benefit of using population codes to encode ambiguous\noutputs, such as the pose of symmetric objects. Using the T-LESS dataset of\nfeature-less real-world objects, we show that population codes improve the\naccuracy of predicting 3D object orientation from image input.\n", "link": "http://arxiv.org/abs/2411.00393v3", "date": "2024-11-06", "relevancy": 1.8811, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4755}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4713}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advantages%20of%20Neural%20Population%20Coding%20for%20Deep%20Learning&body=Title%3A%20Advantages%20of%20Neural%20Population%20Coding%20for%20Deep%20Learning%0AAuthor%3A%20Heiko%20Hoffmann%0AAbstract%3A%20%20%20Scalar%20variables%2C%20e.g.%2C%20the%20orientation%20of%20a%20shape%20in%20an%20image%2C%20are%20commonly%0Apredicted%20using%20a%20single%20output%20neuron%20in%20a%20neural%20network.%20In%20contrast%2C%20the%0Amammalian%20cortex%20represents%20variables%20with%20a%20population%20of%20neurons.%20In%20this%0Apopulation%20code%2C%20each%20neuron%20is%20most%20active%20at%20its%20preferred%20value%20and%20shows%0Apartial%20activity%20for%20other%20values.%20Here%2C%20we%20investigate%20the%20benefit%20of%20using%20a%0Apopulation%20code%20for%20the%20output%20layer%20of%20a%20neural%20network.%20We%20compare%20population%0Acodes%20against%20single-neuron%20outputs%20and%20one-hot%20vectors.%20First%2C%20we%20show%0Atheoretically%20and%20in%20experiments%20with%20synthetic%20data%20that%20population%20codes%0Aimprove%20robustness%20to%20input%20noise%20in%20networks%20of%20stacked%20linear%20layers.%20Second%2C%0Awe%20demonstrate%20the%20benefit%20of%20using%20population%20codes%20to%20encode%20ambiguous%0Aoutputs%2C%20such%20as%20the%20pose%20of%20symmetric%20objects.%20Using%20the%20T-LESS%20dataset%20of%0Afeature-less%20real-world%20objects%2C%20we%20show%20that%20population%20codes%20improve%20the%0Aaccuracy%20of%20predicting%203D%20object%20orientation%20from%20image%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00393v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvantages%2520of%2520Neural%2520Population%2520Coding%2520for%2520Deep%2520Learning%26entry.906535625%3DHeiko%2520Hoffmann%26entry.1292438233%3D%2520%2520Scalar%2520variables%252C%2520e.g.%252C%2520the%2520orientation%2520of%2520a%2520shape%2520in%2520an%2520image%252C%2520are%2520commonly%250Apredicted%2520using%2520a%2520single%2520output%2520neuron%2520in%2520a%2520neural%2520network.%2520In%2520contrast%252C%2520the%250Amammalian%2520cortex%2520represents%2520variables%2520with%2520a%2520population%2520of%2520neurons.%2520In%2520this%250Apopulation%2520code%252C%2520each%2520neuron%2520is%2520most%2520active%2520at%2520its%2520preferred%2520value%2520and%2520shows%250Apartial%2520activity%2520for%2520other%2520values.%2520Here%252C%2520we%2520investigate%2520the%2520benefit%2520of%2520using%2520a%250Apopulation%2520code%2520for%2520the%2520output%2520layer%2520of%2520a%2520neural%2520network.%2520We%2520compare%2520population%250Acodes%2520against%2520single-neuron%2520outputs%2520and%2520one-hot%2520vectors.%2520First%252C%2520we%2520show%250Atheoretically%2520and%2520in%2520experiments%2520with%2520synthetic%2520data%2520that%2520population%2520codes%250Aimprove%2520robustness%2520to%2520input%2520noise%2520in%2520networks%2520of%2520stacked%2520linear%2520layers.%2520Second%252C%250Awe%2520demonstrate%2520the%2520benefit%2520of%2520using%2520population%2520codes%2520to%2520encode%2520ambiguous%250Aoutputs%252C%2520such%2520as%2520the%2520pose%2520of%2520symmetric%2520objects.%2520Using%2520the%2520T-LESS%2520dataset%2520of%250Afeature-less%2520real-world%2520objects%252C%2520we%2520show%2520that%2520population%2520codes%2520improve%2520the%250Aaccuracy%2520of%2520predicting%25203D%2520object%2520orientation%2520from%2520image%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00393v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advantages%20of%20Neural%20Population%20Coding%20for%20Deep%20Learning&entry.906535625=Heiko%20Hoffmann&entry.1292438233=%20%20Scalar%20variables%2C%20e.g.%2C%20the%20orientation%20of%20a%20shape%20in%20an%20image%2C%20are%20commonly%0Apredicted%20using%20a%20single%20output%20neuron%20in%20a%20neural%20network.%20In%20contrast%2C%20the%0Amammalian%20cortex%20represents%20variables%20with%20a%20population%20of%20neurons.%20In%20this%0Apopulation%20code%2C%20each%20neuron%20is%20most%20active%20at%20its%20preferred%20value%20and%20shows%0Apartial%20activity%20for%20other%20values.%20Here%2C%20we%20investigate%20the%20benefit%20of%20using%20a%0Apopulation%20code%20for%20the%20output%20layer%20of%20a%20neural%20network.%20We%20compare%20population%0Acodes%20against%20single-neuron%20outputs%20and%20one-hot%20vectors.%20First%2C%20we%20show%0Atheoretically%20and%20in%20experiments%20with%20synthetic%20data%20that%20population%20codes%0Aimprove%20robustness%20to%20input%20noise%20in%20networks%20of%20stacked%20linear%20layers.%20Second%2C%0Awe%20demonstrate%20the%20benefit%20of%20using%20population%20codes%20to%20encode%20ambiguous%0Aoutputs%2C%20such%20as%20the%20pose%20of%20symmetric%20objects.%20Using%20the%20T-LESS%20dataset%20of%0Afeature-less%20real-world%20objects%2C%20we%20show%20that%20population%20codes%20improve%20the%0Aaccuracy%20of%20predicting%203D%20object%20orientation%20from%20image%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00393v3&entry.124074799=Read"},
{"title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs", "author": "Hasan Iqbal and Yuxia Wang and Minghan Wang and Georgi Georgiev and Jiahui Geng and Iryna Gurevych and Preslav Nakov", "abstract": "  The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI.\n", "link": "http://arxiv.org/abs/2408.11832v2", "date": "2024-11-06", "relevancy": 1.8727, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4872}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenFactCheck%3A%20A%20Unified%20Framework%20for%20Factuality%20Evaluation%20of%20LLMs&body=Title%3A%20OpenFactCheck%3A%20A%20Unified%20Framework%20for%20Factuality%20Evaluation%20of%20LLMs%0AAuthor%3A%20Hasan%20Iqbal%20and%20Yuxia%20Wang%20and%20Minghan%20Wang%20and%20Georgi%20Georgiev%20and%20Jiahui%20Geng%20and%20Iryna%20Gurevych%20and%20Preslav%20Nakov%0AAbstract%3A%20%20%20The%20increased%20use%20of%20large%20language%20models%20%28LLMs%29%20across%20a%20variety%20of%0Areal-world%20applications%20calls%20for%20automatic%20tools%20to%20check%20the%20factual%20accuracy%0Aof%20their%20outputs%2C%20as%20LLMs%20often%20hallucinate.%20This%20is%20difficult%20as%20it%20requires%0Aassessing%20the%20factuality%20of%20free-form%20open-domain%20responses.%20While%20there%20has%0Abeen%20a%20lot%20of%20research%20on%20this%20topic%2C%20different%20papers%20use%20different%20evaluation%0Abenchmarks%20and%20measures%2C%20which%20makes%20them%20hard%20to%20compare%20and%20hampers%20future%0Aprogress.%20To%20mitigate%20these%20issues%2C%20we%20developed%20OpenFactCheck%2C%20a%20unified%0Aframework%2C%20with%20three%20modules%3A%20%28i%29%20RESPONSEEVAL%2C%20which%20allows%20users%20to%20easily%0Acustomize%20an%20automatic%20fact-checking%20system%20and%20to%20assess%20the%20factuality%20of%20all%0Aclaims%20in%20an%20input%20document%20using%20that%20system%2C%20%28ii%29%20LLMEVAL%2C%20which%20assesses%20the%0Aoverall%20factuality%20of%20an%20LLM%2C%20and%20%28iii%29%20CHECKEREVAL%2C%20a%20module%20to%20evaluate%0Aautomatic%20fact-checking%20systems.%20OpenFactCheck%20is%20open-sourced%0A%28https%3A//github.com/mbzuai-nlp/openfactcheck%29%20and%20publicly%20released%20as%20a%20Python%0Alibrary%20%28https%3A//pypi.org/project/openfactcheck/%29%20and%20also%20as%20a%20web%20service%0A%28http%3A//app.openfactcheck.com%29.%20A%20video%20describing%20the%20system%20is%20available%20at%0Ahttps%3A//youtu.be/-i9VKL0HleI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11832v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenFactCheck%253A%2520A%2520Unified%2520Framework%2520for%2520Factuality%2520Evaluation%2520of%2520LLMs%26entry.906535625%3DHasan%2520Iqbal%2520and%2520Yuxia%2520Wang%2520and%2520Minghan%2520Wang%2520and%2520Georgi%2520Georgiev%2520and%2520Jiahui%2520Geng%2520and%2520Iryna%2520Gurevych%2520and%2520Preslav%2520Nakov%26entry.1292438233%3D%2520%2520The%2520increased%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520across%2520a%2520variety%2520of%250Areal-world%2520applications%2520calls%2520for%2520automatic%2520tools%2520to%2520check%2520the%2520factual%2520accuracy%250Aof%2520their%2520outputs%252C%2520as%2520LLMs%2520often%2520hallucinate.%2520This%2520is%2520difficult%2520as%2520it%2520requires%250Aassessing%2520the%2520factuality%2520of%2520free-form%2520open-domain%2520responses.%2520While%2520there%2520has%250Abeen%2520a%2520lot%2520of%2520research%2520on%2520this%2520topic%252C%2520different%2520papers%2520use%2520different%2520evaluation%250Abenchmarks%2520and%2520measures%252C%2520which%2520makes%2520them%2520hard%2520to%2520compare%2520and%2520hampers%2520future%250Aprogress.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520developed%2520OpenFactCheck%252C%2520a%2520unified%250Aframework%252C%2520with%2520three%2520modules%253A%2520%2528i%2529%2520RESPONSEEVAL%252C%2520which%2520allows%2520users%2520to%2520easily%250Acustomize%2520an%2520automatic%2520fact-checking%2520system%2520and%2520to%2520assess%2520the%2520factuality%2520of%2520all%250Aclaims%2520in%2520an%2520input%2520document%2520using%2520that%2520system%252C%2520%2528ii%2529%2520LLMEVAL%252C%2520which%2520assesses%2520the%250Aoverall%2520factuality%2520of%2520an%2520LLM%252C%2520and%2520%2528iii%2529%2520CHECKEREVAL%252C%2520a%2520module%2520to%2520evaluate%250Aautomatic%2520fact-checking%2520systems.%2520OpenFactCheck%2520is%2520open-sourced%250A%2528https%253A//github.com/mbzuai-nlp/openfactcheck%2529%2520and%2520publicly%2520released%2520as%2520a%2520Python%250Alibrary%2520%2528https%253A//pypi.org/project/openfactcheck/%2529%2520and%2520also%2520as%2520a%2520web%2520service%250A%2528http%253A//app.openfactcheck.com%2529.%2520A%2520video%2520describing%2520the%2520system%2520is%2520available%2520at%250Ahttps%253A//youtu.be/-i9VKL0HleI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11832v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenFactCheck%3A%20A%20Unified%20Framework%20for%20Factuality%20Evaluation%20of%20LLMs&entry.906535625=Hasan%20Iqbal%20and%20Yuxia%20Wang%20and%20Minghan%20Wang%20and%20Georgi%20Georgiev%20and%20Jiahui%20Geng%20and%20Iryna%20Gurevych%20and%20Preslav%20Nakov&entry.1292438233=%20%20The%20increased%20use%20of%20large%20language%20models%20%28LLMs%29%20across%20a%20variety%20of%0Areal-world%20applications%20calls%20for%20automatic%20tools%20to%20check%20the%20factual%20accuracy%0Aof%20their%20outputs%2C%20as%20LLMs%20often%20hallucinate.%20This%20is%20difficult%20as%20it%20requires%0Aassessing%20the%20factuality%20of%20free-form%20open-domain%20responses.%20While%20there%20has%0Abeen%20a%20lot%20of%20research%20on%20this%20topic%2C%20different%20papers%20use%20different%20evaluation%0Abenchmarks%20and%20measures%2C%20which%20makes%20them%20hard%20to%20compare%20and%20hampers%20future%0Aprogress.%20To%20mitigate%20these%20issues%2C%20we%20developed%20OpenFactCheck%2C%20a%20unified%0Aframework%2C%20with%20three%20modules%3A%20%28i%29%20RESPONSEEVAL%2C%20which%20allows%20users%20to%20easily%0Acustomize%20an%20automatic%20fact-checking%20system%20and%20to%20assess%20the%20factuality%20of%20all%0Aclaims%20in%20an%20input%20document%20using%20that%20system%2C%20%28ii%29%20LLMEVAL%2C%20which%20assesses%20the%0Aoverall%20factuality%20of%20an%20LLM%2C%20and%20%28iii%29%20CHECKEREVAL%2C%20a%20module%20to%20evaluate%0Aautomatic%20fact-checking%20systems.%20OpenFactCheck%20is%20open-sourced%0A%28https%3A//github.com/mbzuai-nlp/openfactcheck%29%20and%20publicly%20released%20as%20a%20Python%0Alibrary%20%28https%3A//pypi.org/project/openfactcheck/%29%20and%20also%20as%20a%20web%20service%0A%28http%3A//app.openfactcheck.com%29.%20A%20video%20describing%20the%20system%20is%20available%20at%0Ahttps%3A//youtu.be/-i9VKL0HleI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11832v2&entry.124074799=Read"},
{"title": "Ensemble-Based Annealed Importance Sampling", "author": "Haoxuan Chen and Lexing Ying", "abstract": "  Sampling from a multimodal distribution is a fundamental and challenging\nproblem in computational science and statistics. Among various approaches\nproposed for this task, one popular method is Annealed Importance Sampling\n(AIS). In this paper, we propose an ensemble-based version of AIS by combining\nit with population-based Monte Carlo methods to improve its efficiency. By\nkeeping track of an ensemble instead of a single particle along some\ncontinuation path between the starting distribution and the target\ndistribution, we take advantage of the interaction within the ensemble to\nencourage the exploration of undiscovered modes. Specifically, our main idea is\nto utilize either the snooker algorithm or the genetic algorithm used in\nEvolutionary Monte Carlo. We discuss how the proposed algorithm can be\nimplemented and derive a partial differential equation governing the evolution\nof the ensemble under the continuous time and mean-field limit. We also test\nthe efficiency of the proposed algorithm on various continuous and discrete\ndistributions.\n", "link": "http://arxiv.org/abs/2401.15645v2", "date": "2024-11-06", "relevancy": 1.7781, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.46}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4479}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble-Based%20Annealed%20Importance%20Sampling&body=Title%3A%20Ensemble-Based%20Annealed%20Importance%20Sampling%0AAuthor%3A%20Haoxuan%20Chen%20and%20Lexing%20Ying%0AAbstract%3A%20%20%20Sampling%20from%20a%20multimodal%20distribution%20is%20a%20fundamental%20and%20challenging%0Aproblem%20in%20computational%20science%20and%20statistics.%20Among%20various%20approaches%0Aproposed%20for%20this%20task%2C%20one%20popular%20method%20is%20Annealed%20Importance%20Sampling%0A%28AIS%29.%20In%20this%20paper%2C%20we%20propose%20an%20ensemble-based%20version%20of%20AIS%20by%20combining%0Ait%20with%20population-based%20Monte%20Carlo%20methods%20to%20improve%20its%20efficiency.%20By%0Akeeping%20track%20of%20an%20ensemble%20instead%20of%20a%20single%20particle%20along%20some%0Acontinuation%20path%20between%20the%20starting%20distribution%20and%20the%20target%0Adistribution%2C%20we%20take%20advantage%20of%20the%20interaction%20within%20the%20ensemble%20to%0Aencourage%20the%20exploration%20of%20undiscovered%20modes.%20Specifically%2C%20our%20main%20idea%20is%0Ato%20utilize%20either%20the%20snooker%20algorithm%20or%20the%20genetic%20algorithm%20used%20in%0AEvolutionary%20Monte%20Carlo.%20We%20discuss%20how%20the%20proposed%20algorithm%20can%20be%0Aimplemented%20and%20derive%20a%20partial%20differential%20equation%20governing%20the%20evolution%0Aof%20the%20ensemble%20under%20the%20continuous%20time%20and%20mean-field%20limit.%20We%20also%20test%0Athe%20efficiency%20of%20the%20proposed%20algorithm%20on%20various%20continuous%20and%20discrete%0Adistributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble-Based%2520Annealed%2520Importance%2520Sampling%26entry.906535625%3DHaoxuan%2520Chen%2520and%2520Lexing%2520Ying%26entry.1292438233%3D%2520%2520Sampling%2520from%2520a%2520multimodal%2520distribution%2520is%2520a%2520fundamental%2520and%2520challenging%250Aproblem%2520in%2520computational%2520science%2520and%2520statistics.%2520Among%2520various%2520approaches%250Aproposed%2520for%2520this%2520task%252C%2520one%2520popular%2520method%2520is%2520Annealed%2520Importance%2520Sampling%250A%2528AIS%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520ensemble-based%2520version%2520of%2520AIS%2520by%2520combining%250Ait%2520with%2520population-based%2520Monte%2520Carlo%2520methods%2520to%2520improve%2520its%2520efficiency.%2520By%250Akeeping%2520track%2520of%2520an%2520ensemble%2520instead%2520of%2520a%2520single%2520particle%2520along%2520some%250Acontinuation%2520path%2520between%2520the%2520starting%2520distribution%2520and%2520the%2520target%250Adistribution%252C%2520we%2520take%2520advantage%2520of%2520the%2520interaction%2520within%2520the%2520ensemble%2520to%250Aencourage%2520the%2520exploration%2520of%2520undiscovered%2520modes.%2520Specifically%252C%2520our%2520main%2520idea%2520is%250Ato%2520utilize%2520either%2520the%2520snooker%2520algorithm%2520or%2520the%2520genetic%2520algorithm%2520used%2520in%250AEvolutionary%2520Monte%2520Carlo.%2520We%2520discuss%2520how%2520the%2520proposed%2520algorithm%2520can%2520be%250Aimplemented%2520and%2520derive%2520a%2520partial%2520differential%2520equation%2520governing%2520the%2520evolution%250Aof%2520the%2520ensemble%2520under%2520the%2520continuous%2520time%2520and%2520mean-field%2520limit.%2520We%2520also%2520test%250Athe%2520efficiency%2520of%2520the%2520proposed%2520algorithm%2520on%2520various%2520continuous%2520and%2520discrete%250Adistributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble-Based%20Annealed%20Importance%20Sampling&entry.906535625=Haoxuan%20Chen%20and%20Lexing%20Ying&entry.1292438233=%20%20Sampling%20from%20a%20multimodal%20distribution%20is%20a%20fundamental%20and%20challenging%0Aproblem%20in%20computational%20science%20and%20statistics.%20Among%20various%20approaches%0Aproposed%20for%20this%20task%2C%20one%20popular%20method%20is%20Annealed%20Importance%20Sampling%0A%28AIS%29.%20In%20this%20paper%2C%20we%20propose%20an%20ensemble-based%20version%20of%20AIS%20by%20combining%0Ait%20with%20population-based%20Monte%20Carlo%20methods%20to%20improve%20its%20efficiency.%20By%0Akeeping%20track%20of%20an%20ensemble%20instead%20of%20a%20single%20particle%20along%20some%0Acontinuation%20path%20between%20the%20starting%20distribution%20and%20the%20target%0Adistribution%2C%20we%20take%20advantage%20of%20the%20interaction%20within%20the%20ensemble%20to%0Aencourage%20the%20exploration%20of%20undiscovered%20modes.%20Specifically%2C%20our%20main%20idea%20is%0Ato%20utilize%20either%20the%20snooker%20algorithm%20or%20the%20genetic%20algorithm%20used%20in%0AEvolutionary%20Monte%20Carlo.%20We%20discuss%20how%20the%20proposed%20algorithm%20can%20be%0Aimplemented%20and%20derive%20a%20partial%20differential%20equation%20governing%20the%20evolution%0Aof%20the%20ensemble%20under%20the%20continuous%20time%20and%20mean-field%20limit.%20We%20also%20test%0Athe%20efficiency%20of%20the%20proposed%20algorithm%20on%20various%20continuous%20and%20discrete%0Adistributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15645v2&entry.124074799=Read"},
{"title": "Agent Design Pattern Catalogue: A Collection of Architectural Patterns\n  for Foundation Model based Agents", "author": "Yue Liu and Sin Kit Lo and Qinghua Lu and Liming Zhu and Dehai Zhao and Xiwei Xu and Stefan Harrer and Jon Whittle", "abstract": "  Foundation model-enabled generative artificial intelligence facilitates the\ndevelopment and implementation of agents, which can leverage distinguished\nreasoning and language processing capabilities to takes a proactive, autonomous\nrole to pursue users' goals. Nevertheless, there is a lack of systematic\nknowledge to guide practitioners in designing the agents considering challenges\nof goal-seeking (including generating instrumental goals and plans), such as\nhallucinations inherent in foundation models, explainability of reasoning\nprocess, complex accountability, etc. To address this issue, we have performed\na systematic literature review to understand the state-of-the-art foundation\nmodel-based agents and the broader ecosystem. In this paper, we present a\npattern catalogue consisting of 18 architectural patterns with analyses of the\ncontext, forces, and trade-offs as the outcomes from the previous literature\nreview. We propose a decision model for selecting the patterns. The proposed\ncatalogue can provide holistic guidance for the effective use of patterns, and\nsupport the architecture design of foundation model-based agents by\nfacilitating goal-seeking and plan generation.\n", "link": "http://arxiv.org/abs/2405.10467v4", "date": "2024-11-06", "relevancy": 1.8031, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4654}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Design%20Pattern%20Catalogue%3A%20A%20Collection%20of%20Architectural%20Patterns%0A%20%20for%20Foundation%20Model%20based%20Agents&body=Title%3A%20Agent%20Design%20Pattern%20Catalogue%3A%20A%20Collection%20of%20Architectural%20Patterns%0A%20%20for%20Foundation%20Model%20based%20Agents%0AAuthor%3A%20Yue%20Liu%20and%20Sin%20Kit%20Lo%20and%20Qinghua%20Lu%20and%20Liming%20Zhu%20and%20Dehai%20Zhao%20and%20Xiwei%20Xu%20and%20Stefan%20Harrer%20and%20Jon%20Whittle%0AAbstract%3A%20%20%20Foundation%20model-enabled%20generative%20artificial%20intelligence%20facilitates%20the%0Adevelopment%20and%20implementation%20of%20agents%2C%20which%20can%20leverage%20distinguished%0Areasoning%20and%20language%20processing%20capabilities%20to%20takes%20a%20proactive%2C%20autonomous%0Arole%20to%20pursue%20users%27%20goals.%20Nevertheless%2C%20there%20is%20a%20lack%20of%20systematic%0Aknowledge%20to%20guide%20practitioners%20in%20designing%20the%20agents%20considering%20challenges%0Aof%20goal-seeking%20%28including%20generating%20instrumental%20goals%20and%20plans%29%2C%20such%20as%0Ahallucinations%20inherent%20in%20foundation%20models%2C%20explainability%20of%20reasoning%0Aprocess%2C%20complex%20accountability%2C%20etc.%20To%20address%20this%20issue%2C%20we%20have%20performed%0Aa%20systematic%20literature%20review%20to%20understand%20the%20state-of-the-art%20foundation%0Amodel-based%20agents%20and%20the%20broader%20ecosystem.%20In%20this%20paper%2C%20we%20present%20a%0Apattern%20catalogue%20consisting%20of%2018%20architectural%20patterns%20with%20analyses%20of%20the%0Acontext%2C%20forces%2C%20and%20trade-offs%20as%20the%20outcomes%20from%20the%20previous%20literature%0Areview.%20We%20propose%20a%20decision%20model%20for%20selecting%20the%20patterns.%20The%20proposed%0Acatalogue%20can%20provide%20holistic%20guidance%20for%20the%20effective%20use%20of%20patterns%2C%20and%0Asupport%20the%20architecture%20design%20of%20foundation%20model-based%20agents%20by%0Afacilitating%20goal-seeking%20and%20plan%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10467v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Design%2520Pattern%2520Catalogue%253A%2520A%2520Collection%2520of%2520Architectural%2520Patterns%250A%2520%2520for%2520Foundation%2520Model%2520based%2520Agents%26entry.906535625%3DYue%2520Liu%2520and%2520Sin%2520Kit%2520Lo%2520and%2520Qinghua%2520Lu%2520and%2520Liming%2520Zhu%2520and%2520Dehai%2520Zhao%2520and%2520Xiwei%2520Xu%2520and%2520Stefan%2520Harrer%2520and%2520Jon%2520Whittle%26entry.1292438233%3D%2520%2520Foundation%2520model-enabled%2520generative%2520artificial%2520intelligence%2520facilitates%2520the%250Adevelopment%2520and%2520implementation%2520of%2520agents%252C%2520which%2520can%2520leverage%2520distinguished%250Areasoning%2520and%2520language%2520processing%2520capabilities%2520to%2520takes%2520a%2520proactive%252C%2520autonomous%250Arole%2520to%2520pursue%2520users%2527%2520goals.%2520Nevertheless%252C%2520there%2520is%2520a%2520lack%2520of%2520systematic%250Aknowledge%2520to%2520guide%2520practitioners%2520in%2520designing%2520the%2520agents%2520considering%2520challenges%250Aof%2520goal-seeking%2520%2528including%2520generating%2520instrumental%2520goals%2520and%2520plans%2529%252C%2520such%2520as%250Ahallucinations%2520inherent%2520in%2520foundation%2520models%252C%2520explainability%2520of%2520reasoning%250Aprocess%252C%2520complex%2520accountability%252C%2520etc.%2520To%2520address%2520this%2520issue%252C%2520we%2520have%2520performed%250Aa%2520systematic%2520literature%2520review%2520to%2520understand%2520the%2520state-of-the-art%2520foundation%250Amodel-based%2520agents%2520and%2520the%2520broader%2520ecosystem.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Apattern%2520catalogue%2520consisting%2520of%252018%2520architectural%2520patterns%2520with%2520analyses%2520of%2520the%250Acontext%252C%2520forces%252C%2520and%2520trade-offs%2520as%2520the%2520outcomes%2520from%2520the%2520previous%2520literature%250Areview.%2520We%2520propose%2520a%2520decision%2520model%2520for%2520selecting%2520the%2520patterns.%2520The%2520proposed%250Acatalogue%2520can%2520provide%2520holistic%2520guidance%2520for%2520the%2520effective%2520use%2520of%2520patterns%252C%2520and%250Asupport%2520the%2520architecture%2520design%2520of%2520foundation%2520model-based%2520agents%2520by%250Afacilitating%2520goal-seeking%2520and%2520plan%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10467v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Design%20Pattern%20Catalogue%3A%20A%20Collection%20of%20Architectural%20Patterns%0A%20%20for%20Foundation%20Model%20based%20Agents&entry.906535625=Yue%20Liu%20and%20Sin%20Kit%20Lo%20and%20Qinghua%20Lu%20and%20Liming%20Zhu%20and%20Dehai%20Zhao%20and%20Xiwei%20Xu%20and%20Stefan%20Harrer%20and%20Jon%20Whittle&entry.1292438233=%20%20Foundation%20model-enabled%20generative%20artificial%20intelligence%20facilitates%20the%0Adevelopment%20and%20implementation%20of%20agents%2C%20which%20can%20leverage%20distinguished%0Areasoning%20and%20language%20processing%20capabilities%20to%20takes%20a%20proactive%2C%20autonomous%0Arole%20to%20pursue%20users%27%20goals.%20Nevertheless%2C%20there%20is%20a%20lack%20of%20systematic%0Aknowledge%20to%20guide%20practitioners%20in%20designing%20the%20agents%20considering%20challenges%0Aof%20goal-seeking%20%28including%20generating%20instrumental%20goals%20and%20plans%29%2C%20such%20as%0Ahallucinations%20inherent%20in%20foundation%20models%2C%20explainability%20of%20reasoning%0Aprocess%2C%20complex%20accountability%2C%20etc.%20To%20address%20this%20issue%2C%20we%20have%20performed%0Aa%20systematic%20literature%20review%20to%20understand%20the%20state-of-the-art%20foundation%0Amodel-based%20agents%20and%20the%20broader%20ecosystem.%20In%20this%20paper%2C%20we%20present%20a%0Apattern%20catalogue%20consisting%20of%2018%20architectural%20patterns%20with%20analyses%20of%20the%0Acontext%2C%20forces%2C%20and%20trade-offs%20as%20the%20outcomes%20from%20the%20previous%20literature%0Areview.%20We%20propose%20a%20decision%20model%20for%20selecting%20the%20patterns.%20The%20proposed%0Acatalogue%20can%20provide%20holistic%20guidance%20for%20the%20effective%20use%20of%20patterns%2C%20and%0Asupport%20the%20architecture%20design%20of%20foundation%20model-based%20agents%20by%0Afacilitating%20goal-seeking%20and%20plan%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10467v4&entry.124074799=Read"},
{"title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue", "author": "Fengxiang Wang and Ranjie Duan and Peng Xiao and Xiaojun Jia and YueFeng Chen and Chongwen Wang and Jialing Tao and Hang Su and Jun Zhu and Hui Xue", "abstract": "  Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon.\n", "link": "http://arxiv.org/abs/2411.03814v1", "date": "2024-11-06", "relevancy": 1.2939, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4505}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4284}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRJ-Agent%3A%20An%20Effective%20Jailbreak%20Agent%20for%20Multi-Round%20Dialogue&body=Title%3A%20MRJ-Agent%3A%20An%20Effective%20Jailbreak%20Agent%20for%20Multi-Round%20Dialogue%0AAuthor%3A%20Fengxiang%20Wang%20and%20Ranjie%20Duan%20and%20Peng%20Xiao%20and%20Xiaojun%20Jia%20and%20YueFeng%20Chen%20and%20Chongwen%20Wang%20and%20Jialing%20Tao%20and%20Hang%20Su%20and%20Jun%20Zhu%20and%20Hui%20Xue%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20outstanding%20performance%20in%20their%0Areservoir%20of%20knowledge%20and%20understanding%20capabilities%2C%20but%20they%20have%20also%20been%0Ashown%20to%20be%20prone%20to%20illegal%20or%20unethical%20reactions%20when%20subjected%20to%20jailbreak%0Aattacks.%20To%20ensure%20their%20responsible%20deployment%20in%20critical%20applications%2C%20it%20is%0Acrucial%20to%20understand%20the%20safety%20capabilities%20and%20vulnerabilities%20of%20LLMs.%0APrevious%20works%20mainly%20focus%20on%20jailbreak%20in%20single-round%20dialogue%2C%20overlooking%0Athe%20potential%20jailbreak%20risks%20in%20multi-round%20dialogues%2C%20which%20are%20a%20vital%20way%0Ahumans%20interact%20with%20and%20extract%20information%20from%20LLMs.%20Some%20studies%20have%0Aincreasingly%20concentrated%20on%20the%20risks%20associated%20with%20jailbreak%20in%20multi-round%0Adialogues.%20These%20efforts%20typically%20involve%20the%20use%20of%20manually%20crafted%0Atemplates%20or%20prompt%20engineering%20techniques.%20However%2C%20due%20to%20the%20inherent%0Acomplexity%20of%20multi-round%20dialogues%2C%20their%20jailbreak%20performance%20is%20limited.%20To%0Asolve%20this%20problem%2C%20we%20propose%20a%20novel%20multi-round%20dialogue%20jailbreaking%20agent%2C%0Aemphasizing%20the%20importance%20of%20stealthiness%20in%20identifying%20and%20mitigating%0Apotential%20threats%20to%20human%20values%20posed%20by%20LLMs.%20We%20propose%20a%20risk%0Adecomposition%20strategy%20that%20distributes%20risks%20across%20multiple%20rounds%20of%20queries%0Aand%20utilizes%20psychological%20strategies%20to%20enhance%20attack%20strength.%20Extensive%0Aexperiments%20show%20that%20our%20proposed%20method%20surpasses%20other%20attack%20methods%20and%0Aachieves%20state-of-the-art%20attack%20success%20rate.%20We%20will%20make%20the%20corresponding%0Acode%20and%20dataset%20available%20for%20future%20research.%20The%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRJ-Agent%253A%2520An%2520Effective%2520Jailbreak%2520Agent%2520for%2520Multi-Round%2520Dialogue%26entry.906535625%3DFengxiang%2520Wang%2520and%2520Ranjie%2520Duan%2520and%2520Peng%2520Xiao%2520and%2520Xiaojun%2520Jia%2520and%2520YueFeng%2520Chen%2520and%2520Chongwen%2520Wang%2520and%2520Jialing%2520Tao%2520and%2520Hang%2520Su%2520and%2520Jun%2520Zhu%2520and%2520Hui%2520Xue%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520outstanding%2520performance%2520in%2520their%250Areservoir%2520of%2520knowledge%2520and%2520understanding%2520capabilities%252C%2520but%2520they%2520have%2520also%2520been%250Ashown%2520to%2520be%2520prone%2520to%2520illegal%2520or%2520unethical%2520reactions%2520when%2520subjected%2520to%2520jailbreak%250Aattacks.%2520To%2520ensure%2520their%2520responsible%2520deployment%2520in%2520critical%2520applications%252C%2520it%2520is%250Acrucial%2520to%2520understand%2520the%2520safety%2520capabilities%2520and%2520vulnerabilities%2520of%2520LLMs.%250APrevious%2520works%2520mainly%2520focus%2520on%2520jailbreak%2520in%2520single-round%2520dialogue%252C%2520overlooking%250Athe%2520potential%2520jailbreak%2520risks%2520in%2520multi-round%2520dialogues%252C%2520which%2520are%2520a%2520vital%2520way%250Ahumans%2520interact%2520with%2520and%2520extract%2520information%2520from%2520LLMs.%2520Some%2520studies%2520have%250Aincreasingly%2520concentrated%2520on%2520the%2520risks%2520associated%2520with%2520jailbreak%2520in%2520multi-round%250Adialogues.%2520These%2520efforts%2520typically%2520involve%2520the%2520use%2520of%2520manually%2520crafted%250Atemplates%2520or%2520prompt%2520engineering%2520techniques.%2520However%252C%2520due%2520to%2520the%2520inherent%250Acomplexity%2520of%2520multi-round%2520dialogues%252C%2520their%2520jailbreak%2520performance%2520is%2520limited.%2520To%250Asolve%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520multi-round%2520dialogue%2520jailbreaking%2520agent%252C%250Aemphasizing%2520the%2520importance%2520of%2520stealthiness%2520in%2520identifying%2520and%2520mitigating%250Apotential%2520threats%2520to%2520human%2520values%2520posed%2520by%2520LLMs.%2520We%2520propose%2520a%2520risk%250Adecomposition%2520strategy%2520that%2520distributes%2520risks%2520across%2520multiple%2520rounds%2520of%2520queries%250Aand%2520utilizes%2520psychological%2520strategies%2520to%2520enhance%2520attack%2520strength.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520proposed%2520method%2520surpasses%2520other%2520attack%2520methods%2520and%250Aachieves%2520state-of-the-art%2520attack%2520success%2520rate.%2520We%2520will%2520make%2520the%2520corresponding%250Acode%2520and%2520dataset%2520available%2520for%2520future%2520research.%2520The%2520code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRJ-Agent%3A%20An%20Effective%20Jailbreak%20Agent%20for%20Multi-Round%20Dialogue&entry.906535625=Fengxiang%20Wang%20and%20Ranjie%20Duan%20and%20Peng%20Xiao%20and%20Xiaojun%20Jia%20and%20YueFeng%20Chen%20and%20Chongwen%20Wang%20and%20Jialing%20Tao%20and%20Hang%20Su%20and%20Jun%20Zhu%20and%20Hui%20Xue&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20outstanding%20performance%20in%20their%0Areservoir%20of%20knowledge%20and%20understanding%20capabilities%2C%20but%20they%20have%20also%20been%0Ashown%20to%20be%20prone%20to%20illegal%20or%20unethical%20reactions%20when%20subjected%20to%20jailbreak%0Aattacks.%20To%20ensure%20their%20responsible%20deployment%20in%20critical%20applications%2C%20it%20is%0Acrucial%20to%20understand%20the%20safety%20capabilities%20and%20vulnerabilities%20of%20LLMs.%0APrevious%20works%20mainly%20focus%20on%20jailbreak%20in%20single-round%20dialogue%2C%20overlooking%0Athe%20potential%20jailbreak%20risks%20in%20multi-round%20dialogues%2C%20which%20are%20a%20vital%20way%0Ahumans%20interact%20with%20and%20extract%20information%20from%20LLMs.%20Some%20studies%20have%0Aincreasingly%20concentrated%20on%20the%20risks%20associated%20with%20jailbreak%20in%20multi-round%0Adialogues.%20These%20efforts%20typically%20involve%20the%20use%20of%20manually%20crafted%0Atemplates%20or%20prompt%20engineering%20techniques.%20However%2C%20due%20to%20the%20inherent%0Acomplexity%20of%20multi-round%20dialogues%2C%20their%20jailbreak%20performance%20is%20limited.%20To%0Asolve%20this%20problem%2C%20we%20propose%20a%20novel%20multi-round%20dialogue%20jailbreaking%20agent%2C%0Aemphasizing%20the%20importance%20of%20stealthiness%20in%20identifying%20and%20mitigating%0Apotential%20threats%20to%20human%20values%20posed%20by%20LLMs.%20We%20propose%20a%20risk%0Adecomposition%20strategy%20that%20distributes%20risks%20across%20multiple%20rounds%20of%20queries%0Aand%20utilizes%20psychological%20strategies%20to%20enhance%20attack%20strength.%20Extensive%0Aexperiments%20show%20that%20our%20proposed%20method%20surpasses%20other%20attack%20methods%20and%0Aachieves%20state-of-the-art%20attack%20success%20rate.%20We%20will%20make%20the%20corresponding%0Acode%20and%20dataset%20available%20for%20future%20research.%20The%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03814v1&entry.124074799=Read"},
{"title": "Textual Decomposition Then Sub-motion-space Scattering for\n  Open-Vocabulary Motion Generation", "author": "Ke Fan and Jiangning Zhang and Ran Yi and Jingyu Gong and Yabiao Wang and Yating Wang and Xin Tan and Chengjie Wang and Lizhuang Ma", "abstract": "  Text-to-motion generation is a crucial task in computer vision, which\ngenerates the target 3D motion by the given text. The existing annotated\ndatasets are limited in scale, resulting in most existing methods overfitting\nto the small datasets and unable to generalize to the motions of the open\ndomain. Some methods attempt to solve the open-vocabulary motion generation\nproblem by aligning to the CLIP space or using the Pretrain-then-Finetuning\nparadigm. However, the current annotated dataset's limited scale only allows\nthem to achieve mapping from sub-text-space to sub-motion-space, instead of\nmapping between full-text-space and full-motion-space (full mapping), which is\nthe key to attaining open-vocabulary motion generation. To this end, this paper\nproposes to leverage the atomic motion (simple body part motions over a short\ntime period) as an intermediate representation, and leverage two orderly\ncoupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to\naddress the full mapping problem. For Textual Decomposition, we design a\nfine-grained description conversion algorithm, and combine it with the\ngeneralization ability of a large language model to convert any given motion\ntext into atomic texts. Sub-motion-space Scattering learns the compositional\nprocess from atomic motions to the target motions, to make the learned\nsub-motion-space scattered to form the full-motion-space. For a given motion of\nthe open domain, it transforms the extrapolation into interpolation and thereby\nsignificantly improves generalization. Our network, $DSO$-Net, combines textual\n$d$ecomposition and sub-motion-space $s$cattering to solve the\n$o$pen-vocabulary motion generation. Extensive experiments demonstrate that our\nDSO-Net achieves significant improvements over the state-of-the-art methods on\nopen-vocabulary motion generation. Code is available at\nhttps://vankouf.github.io/DSONet/.\n", "link": "http://arxiv.org/abs/2411.04079v1", "date": "2024-11-06", "relevancy": 1.6602, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6035}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Textual%20Decomposition%20Then%20Sub-motion-space%20Scattering%20for%0A%20%20Open-Vocabulary%20Motion%20Generation&body=Title%3A%20Textual%20Decomposition%20Then%20Sub-motion-space%20Scattering%20for%0A%20%20Open-Vocabulary%20Motion%20Generation%0AAuthor%3A%20Ke%20Fan%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Jingyu%20Gong%20and%20Yabiao%20Wang%20and%20Yating%20Wang%20and%20Xin%20Tan%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Text-to-motion%20generation%20is%20a%20crucial%20task%20in%20computer%20vision%2C%20which%0Agenerates%20the%20target%203D%20motion%20by%20the%20given%20text.%20The%20existing%20annotated%0Adatasets%20are%20limited%20in%20scale%2C%20resulting%20in%20most%20existing%20methods%20overfitting%0Ato%20the%20small%20datasets%20and%20unable%20to%20generalize%20to%20the%20motions%20of%20the%20open%0Adomain.%20Some%20methods%20attempt%20to%20solve%20the%20open-vocabulary%20motion%20generation%0Aproblem%20by%20aligning%20to%20the%20CLIP%20space%20or%20using%20the%20Pretrain-then-Finetuning%0Aparadigm.%20However%2C%20the%20current%20annotated%20dataset%27s%20limited%20scale%20only%20allows%0Athem%20to%20achieve%20mapping%20from%20sub-text-space%20to%20sub-motion-space%2C%20instead%20of%0Amapping%20between%20full-text-space%20and%20full-motion-space%20%28full%20mapping%29%2C%20which%20is%0Athe%20key%20to%20attaining%20open-vocabulary%20motion%20generation.%20To%20this%20end%2C%20this%20paper%0Aproposes%20to%20leverage%20the%20atomic%20motion%20%28simple%20body%20part%20motions%20over%20a%20short%0Atime%20period%29%20as%20an%20intermediate%20representation%2C%20and%20leverage%20two%20orderly%0Acoupled%20steps%2C%20i.e.%2C%20Textual%20Decomposition%20and%20Sub-motion-space%20Scattering%2C%20to%0Aaddress%20the%20full%20mapping%20problem.%20For%20Textual%20Decomposition%2C%20we%20design%20a%0Afine-grained%20description%20conversion%20algorithm%2C%20and%20combine%20it%20with%20the%0Ageneralization%20ability%20of%20a%20large%20language%20model%20to%20convert%20any%20given%20motion%0Atext%20into%20atomic%20texts.%20Sub-motion-space%20Scattering%20learns%20the%20compositional%0Aprocess%20from%20atomic%20motions%20to%20the%20target%20motions%2C%20to%20make%20the%20learned%0Asub-motion-space%20scattered%20to%20form%20the%20full-motion-space.%20For%20a%20given%20motion%20of%0Athe%20open%20domain%2C%20it%20transforms%20the%20extrapolation%20into%20interpolation%20and%20thereby%0Asignificantly%20improves%20generalization.%20Our%20network%2C%20%24DSO%24-Net%2C%20combines%20textual%0A%24d%24ecomposition%20and%20sub-motion-space%20%24s%24cattering%20to%20solve%20the%0A%24o%24pen-vocabulary%20motion%20generation.%20Extensive%20experiments%20demonstrate%20that%20our%0ADSO-Net%20achieves%20significant%20improvements%20over%20the%20state-of-the-art%20methods%20on%0Aopen-vocabulary%20motion%20generation.%20Code%20is%20available%20at%0Ahttps%3A//vankouf.github.io/DSONet/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextual%2520Decomposition%2520Then%2520Sub-motion-space%2520Scattering%2520for%250A%2520%2520Open-Vocabulary%2520Motion%2520Generation%26entry.906535625%3DKe%2520Fan%2520and%2520Jiangning%2520Zhang%2520and%2520Ran%2520Yi%2520and%2520Jingyu%2520Gong%2520and%2520Yabiao%2520Wang%2520and%2520Yating%2520Wang%2520and%2520Xin%2520Tan%2520and%2520Chengjie%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Text-to-motion%2520generation%2520is%2520a%2520crucial%2520task%2520in%2520computer%2520vision%252C%2520which%250Agenerates%2520the%2520target%25203D%2520motion%2520by%2520the%2520given%2520text.%2520The%2520existing%2520annotated%250Adatasets%2520are%2520limited%2520in%2520scale%252C%2520resulting%2520in%2520most%2520existing%2520methods%2520overfitting%250Ato%2520the%2520small%2520datasets%2520and%2520unable%2520to%2520generalize%2520to%2520the%2520motions%2520of%2520the%2520open%250Adomain.%2520Some%2520methods%2520attempt%2520to%2520solve%2520the%2520open-vocabulary%2520motion%2520generation%250Aproblem%2520by%2520aligning%2520to%2520the%2520CLIP%2520space%2520or%2520using%2520the%2520Pretrain-then-Finetuning%250Aparadigm.%2520However%252C%2520the%2520current%2520annotated%2520dataset%2527s%2520limited%2520scale%2520only%2520allows%250Athem%2520to%2520achieve%2520mapping%2520from%2520sub-text-space%2520to%2520sub-motion-space%252C%2520instead%2520of%250Amapping%2520between%2520full-text-space%2520and%2520full-motion-space%2520%2528full%2520mapping%2529%252C%2520which%2520is%250Athe%2520key%2520to%2520attaining%2520open-vocabulary%2520motion%2520generation.%2520To%2520this%2520end%252C%2520this%2520paper%250Aproposes%2520to%2520leverage%2520the%2520atomic%2520motion%2520%2528simple%2520body%2520part%2520motions%2520over%2520a%2520short%250Atime%2520period%2529%2520as%2520an%2520intermediate%2520representation%252C%2520and%2520leverage%2520two%2520orderly%250Acoupled%2520steps%252C%2520i.e.%252C%2520Textual%2520Decomposition%2520and%2520Sub-motion-space%2520Scattering%252C%2520to%250Aaddress%2520the%2520full%2520mapping%2520problem.%2520For%2520Textual%2520Decomposition%252C%2520we%2520design%2520a%250Afine-grained%2520description%2520conversion%2520algorithm%252C%2520and%2520combine%2520it%2520with%2520the%250Ageneralization%2520ability%2520of%2520a%2520large%2520language%2520model%2520to%2520convert%2520any%2520given%2520motion%250Atext%2520into%2520atomic%2520texts.%2520Sub-motion-space%2520Scattering%2520learns%2520the%2520compositional%250Aprocess%2520from%2520atomic%2520motions%2520to%2520the%2520target%2520motions%252C%2520to%2520make%2520the%2520learned%250Asub-motion-space%2520scattered%2520to%2520form%2520the%2520full-motion-space.%2520For%2520a%2520given%2520motion%2520of%250Athe%2520open%2520domain%252C%2520it%2520transforms%2520the%2520extrapolation%2520into%2520interpolation%2520and%2520thereby%250Asignificantly%2520improves%2520generalization.%2520Our%2520network%252C%2520%2524DSO%2524-Net%252C%2520combines%2520textual%250A%2524d%2524ecomposition%2520and%2520sub-motion-space%2520%2524s%2524cattering%2520to%2520solve%2520the%250A%2524o%2524pen-vocabulary%2520motion%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250ADSO-Net%2520achieves%2520significant%2520improvements%2520over%2520the%2520state-of-the-art%2520methods%2520on%250Aopen-vocabulary%2520motion%2520generation.%2520Code%2520is%2520available%2520at%250Ahttps%253A//vankouf.github.io/DSONet/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textual%20Decomposition%20Then%20Sub-motion-space%20Scattering%20for%0A%20%20Open-Vocabulary%20Motion%20Generation&entry.906535625=Ke%20Fan%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Jingyu%20Gong%20and%20Yabiao%20Wang%20and%20Yating%20Wang%20and%20Xin%20Tan%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Text-to-motion%20generation%20is%20a%20crucial%20task%20in%20computer%20vision%2C%20which%0Agenerates%20the%20target%203D%20motion%20by%20the%20given%20text.%20The%20existing%20annotated%0Adatasets%20are%20limited%20in%20scale%2C%20resulting%20in%20most%20existing%20methods%20overfitting%0Ato%20the%20small%20datasets%20and%20unable%20to%20generalize%20to%20the%20motions%20of%20the%20open%0Adomain.%20Some%20methods%20attempt%20to%20solve%20the%20open-vocabulary%20motion%20generation%0Aproblem%20by%20aligning%20to%20the%20CLIP%20space%20or%20using%20the%20Pretrain-then-Finetuning%0Aparadigm.%20However%2C%20the%20current%20annotated%20dataset%27s%20limited%20scale%20only%20allows%0Athem%20to%20achieve%20mapping%20from%20sub-text-space%20to%20sub-motion-space%2C%20instead%20of%0Amapping%20between%20full-text-space%20and%20full-motion-space%20%28full%20mapping%29%2C%20which%20is%0Athe%20key%20to%20attaining%20open-vocabulary%20motion%20generation.%20To%20this%20end%2C%20this%20paper%0Aproposes%20to%20leverage%20the%20atomic%20motion%20%28simple%20body%20part%20motions%20over%20a%20short%0Atime%20period%29%20as%20an%20intermediate%20representation%2C%20and%20leverage%20two%20orderly%0Acoupled%20steps%2C%20i.e.%2C%20Textual%20Decomposition%20and%20Sub-motion-space%20Scattering%2C%20to%0Aaddress%20the%20full%20mapping%20problem.%20For%20Textual%20Decomposition%2C%20we%20design%20a%0Afine-grained%20description%20conversion%20algorithm%2C%20and%20combine%20it%20with%20the%0Ageneralization%20ability%20of%20a%20large%20language%20model%20to%20convert%20any%20given%20motion%0Atext%20into%20atomic%20texts.%20Sub-motion-space%20Scattering%20learns%20the%20compositional%0Aprocess%20from%20atomic%20motions%20to%20the%20target%20motions%2C%20to%20make%20the%20learned%0Asub-motion-space%20scattered%20to%20form%20the%20full-motion-space.%20For%20a%20given%20motion%20of%0Athe%20open%20domain%2C%20it%20transforms%20the%20extrapolation%20into%20interpolation%20and%20thereby%0Asignificantly%20improves%20generalization.%20Our%20network%2C%20%24DSO%24-Net%2C%20combines%20textual%0A%24d%24ecomposition%20and%20sub-motion-space%20%24s%24cattering%20to%20solve%20the%0A%24o%24pen-vocabulary%20motion%20generation.%20Extensive%20experiments%20demonstrate%20that%20our%0ADSO-Net%20achieves%20significant%20improvements%20over%20the%20state-of-the-art%20methods%20on%0Aopen-vocabulary%20motion%20generation.%20Code%20is%20available%20at%0Ahttps%3A//vankouf.github.io/DSONet/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04079v1&entry.124074799=Read"},
{"title": "UMIRobot: An Open-{Software, Hardware} Low-Cost Robotic Manipulator for\n  Education", "author": "Murilo M. Marinho and Hung-Ching Lin and Jiawei Zhao", "abstract": "  Robot teleoperation has been studied for the past 70 years and is relevant in\nmany contexts, such as in the handling of hazardous materials and telesurgery.\nThe COVID19 pandemic has rekindled interest in this topic, but the existing\nrobotic education kits fall short of being suitable for teleoperated robotic\nmanipulator learning. In addition, the global restrictions of motion motivated\nlarge investments in online/hybrid education. In this work, a newly developed\nrobotics education kit and its ecosystem are presented which is used as the\nbackbone of an online/hybrid course in teleoperated robots. The students are\ndivided into teams. Each team designs, fabricates (3D printing and assembling),\nand implements a control strategy for a master device and gripper. Coupling\nthose with the UMIRobot, provided as a kit, the students compete in a\nteleoperation challenge. The kit is low cost (< 100USD), which allows\nhigher-learning institutions to provide one kit per student and they can learn\nin a risk-free environment. As of now, 73 such kits have been assembled and\nsent to course participants in eight countries. As major success stories, we\nshow an example of gripper and master designed for the proposed course. In\naddition, we show a teleoperated task between Japan and Bangladesh executed by\ncourse participants. Design files, videos, source code, and more information\nare available at https://mmmarinho.github.io/UMIRobot/\n", "link": "http://arxiv.org/abs/2301.06668v3", "date": "2024-11-06", "relevancy": 1.5993, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5448}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5431}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UMIRobot%3A%20An%20Open-%7BSoftware%2C%20Hardware%7D%20Low-Cost%20Robotic%20Manipulator%20for%0A%20%20Education&body=Title%3A%20UMIRobot%3A%20An%20Open-%7BSoftware%2C%20Hardware%7D%20Low-Cost%20Robotic%20Manipulator%20for%0A%20%20Education%0AAuthor%3A%20Murilo%20M.%20Marinho%20and%20Hung-Ching%20Lin%20and%20Jiawei%20Zhao%0AAbstract%3A%20%20%20Robot%20teleoperation%20has%20been%20studied%20for%20the%20past%2070%20years%20and%20is%20relevant%20in%0Amany%20contexts%2C%20such%20as%20in%20the%20handling%20of%20hazardous%20materials%20and%20telesurgery.%0AThe%20COVID19%20pandemic%20has%20rekindled%20interest%20in%20this%20topic%2C%20but%20the%20existing%0Arobotic%20education%20kits%20fall%20short%20of%20being%20suitable%20for%20teleoperated%20robotic%0Amanipulator%20learning.%20In%20addition%2C%20the%20global%20restrictions%20of%20motion%20motivated%0Alarge%20investments%20in%20online/hybrid%20education.%20In%20this%20work%2C%20a%20newly%20developed%0Arobotics%20education%20kit%20and%20its%20ecosystem%20are%20presented%20which%20is%20used%20as%20the%0Abackbone%20of%20an%20online/hybrid%20course%20in%20teleoperated%20robots.%20The%20students%20are%0Adivided%20into%20teams.%20Each%20team%20designs%2C%20fabricates%20%283D%20printing%20and%20assembling%29%2C%0Aand%20implements%20a%20control%20strategy%20for%20a%20master%20device%20and%20gripper.%20Coupling%0Athose%20with%20the%20UMIRobot%2C%20provided%20as%20a%20kit%2C%20the%20students%20compete%20in%20a%0Ateleoperation%20challenge.%20The%20kit%20is%20low%20cost%20%28%3C%20100USD%29%2C%20which%20allows%0Ahigher-learning%20institutions%20to%20provide%20one%20kit%20per%20student%20and%20they%20can%20learn%0Ain%20a%20risk-free%20environment.%20As%20of%20now%2C%2073%20such%20kits%20have%20been%20assembled%20and%0Asent%20to%20course%20participants%20in%20eight%20countries.%20As%20major%20success%20stories%2C%20we%0Ashow%20an%20example%20of%20gripper%20and%20master%20designed%20for%20the%20proposed%20course.%20In%0Aaddition%2C%20we%20show%20a%20teleoperated%20task%20between%20Japan%20and%20Bangladesh%20executed%20by%0Acourse%20participants.%20Design%20files%2C%20videos%2C%20source%20code%2C%20and%20more%20information%0Aare%20available%20at%20https%3A//mmmarinho.github.io/UMIRobot/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.06668v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUMIRobot%253A%2520An%2520Open-%257BSoftware%252C%2520Hardware%257D%2520Low-Cost%2520Robotic%2520Manipulator%2520for%250A%2520%2520Education%26entry.906535625%3DMurilo%2520M.%2520Marinho%2520and%2520Hung-Ching%2520Lin%2520and%2520Jiawei%2520Zhao%26entry.1292438233%3D%2520%2520Robot%2520teleoperation%2520has%2520been%2520studied%2520for%2520the%2520past%252070%2520years%2520and%2520is%2520relevant%2520in%250Amany%2520contexts%252C%2520such%2520as%2520in%2520the%2520handling%2520of%2520hazardous%2520materials%2520and%2520telesurgery.%250AThe%2520COVID19%2520pandemic%2520has%2520rekindled%2520interest%2520in%2520this%2520topic%252C%2520but%2520the%2520existing%250Arobotic%2520education%2520kits%2520fall%2520short%2520of%2520being%2520suitable%2520for%2520teleoperated%2520robotic%250Amanipulator%2520learning.%2520In%2520addition%252C%2520the%2520global%2520restrictions%2520of%2520motion%2520motivated%250Alarge%2520investments%2520in%2520online/hybrid%2520education.%2520In%2520this%2520work%252C%2520a%2520newly%2520developed%250Arobotics%2520education%2520kit%2520and%2520its%2520ecosystem%2520are%2520presented%2520which%2520is%2520used%2520as%2520the%250Abackbone%2520of%2520an%2520online/hybrid%2520course%2520in%2520teleoperated%2520robots.%2520The%2520students%2520are%250Adivided%2520into%2520teams.%2520Each%2520team%2520designs%252C%2520fabricates%2520%25283D%2520printing%2520and%2520assembling%2529%252C%250Aand%2520implements%2520a%2520control%2520strategy%2520for%2520a%2520master%2520device%2520and%2520gripper.%2520Coupling%250Athose%2520with%2520the%2520UMIRobot%252C%2520provided%2520as%2520a%2520kit%252C%2520the%2520students%2520compete%2520in%2520a%250Ateleoperation%2520challenge.%2520The%2520kit%2520is%2520low%2520cost%2520%2528%253C%2520100USD%2529%252C%2520which%2520allows%250Ahigher-learning%2520institutions%2520to%2520provide%2520one%2520kit%2520per%2520student%2520and%2520they%2520can%2520learn%250Ain%2520a%2520risk-free%2520environment.%2520As%2520of%2520now%252C%252073%2520such%2520kits%2520have%2520been%2520assembled%2520and%250Asent%2520to%2520course%2520participants%2520in%2520eight%2520countries.%2520As%2520major%2520success%2520stories%252C%2520we%250Ashow%2520an%2520example%2520of%2520gripper%2520and%2520master%2520designed%2520for%2520the%2520proposed%2520course.%2520In%250Aaddition%252C%2520we%2520show%2520a%2520teleoperated%2520task%2520between%2520Japan%2520and%2520Bangladesh%2520executed%2520by%250Acourse%2520participants.%2520Design%2520files%252C%2520videos%252C%2520source%2520code%252C%2520and%2520more%2520information%250Aare%2520available%2520at%2520https%253A//mmmarinho.github.io/UMIRobot/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.06668v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMIRobot%3A%20An%20Open-%7BSoftware%2C%20Hardware%7D%20Low-Cost%20Robotic%20Manipulator%20for%0A%20%20Education&entry.906535625=Murilo%20M.%20Marinho%20and%20Hung-Ching%20Lin%20and%20Jiawei%20Zhao&entry.1292438233=%20%20Robot%20teleoperation%20has%20been%20studied%20for%20the%20past%2070%20years%20and%20is%20relevant%20in%0Amany%20contexts%2C%20such%20as%20in%20the%20handling%20of%20hazardous%20materials%20and%20telesurgery.%0AThe%20COVID19%20pandemic%20has%20rekindled%20interest%20in%20this%20topic%2C%20but%20the%20existing%0Arobotic%20education%20kits%20fall%20short%20of%20being%20suitable%20for%20teleoperated%20robotic%0Amanipulator%20learning.%20In%20addition%2C%20the%20global%20restrictions%20of%20motion%20motivated%0Alarge%20investments%20in%20online/hybrid%20education.%20In%20this%20work%2C%20a%20newly%20developed%0Arobotics%20education%20kit%20and%20its%20ecosystem%20are%20presented%20which%20is%20used%20as%20the%0Abackbone%20of%20an%20online/hybrid%20course%20in%20teleoperated%20robots.%20The%20students%20are%0Adivided%20into%20teams.%20Each%20team%20designs%2C%20fabricates%20%283D%20printing%20and%20assembling%29%2C%0Aand%20implements%20a%20control%20strategy%20for%20a%20master%20device%20and%20gripper.%20Coupling%0Athose%20with%20the%20UMIRobot%2C%20provided%20as%20a%20kit%2C%20the%20students%20compete%20in%20a%0Ateleoperation%20challenge.%20The%20kit%20is%20low%20cost%20%28%3C%20100USD%29%2C%20which%20allows%0Ahigher-learning%20institutions%20to%20provide%20one%20kit%20per%20student%20and%20they%20can%20learn%0Ain%20a%20risk-free%20environment.%20As%20of%20now%2C%2073%20such%20kits%20have%20been%20assembled%20and%0Asent%20to%20course%20participants%20in%20eight%20countries.%20As%20major%20success%20stories%2C%20we%0Ashow%20an%20example%20of%20gripper%20and%20master%20designed%20for%20the%20proposed%20course.%20In%0Aaddition%2C%20we%20show%20a%20teleoperated%20task%20between%20Japan%20and%20Bangladesh%20executed%20by%0Acourse%20participants.%20Design%20files%2C%20videos%2C%20source%20code%2C%20and%20more%20information%0Aare%20available%20at%20https%3A//mmmarinho.github.io/UMIRobot/%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.06668v3&entry.124074799=Read"},
{"title": "Exponential convergence rates for momentum stochastic gradient descent\n  in the overparametrized setting", "author": "Benjamin Gess and Sebastian Kassing", "abstract": "  We prove explicit bounds on the exponential rate of convergence for the\nmomentum stochastic gradient descent scheme (MSGD) for arbitrary, fixed\nhyperparameters (learning rate, friction parameter) and its continuous-in-time\ncounterpart in the context of non-convex optimization. In the small step-size\nregime and in the case of flat minima or large noise intensities, these bounds\nprove faster convergence of MSGD compared to plain stochastic gradient descent\n(SGD). The results are shown for objective functions satisfying a local\nPolyak-Lojasiewicz inequality and under assumptions on the variance of MSGD\nthat are satisfied in overparametrized settings. Moreover, we analyze the\noptimal choice of the friction parameter and show that the MSGD process almost\nsurely converges to a local minimum.\n", "link": "http://arxiv.org/abs/2302.03550v2", "date": "2024-11-06", "relevancy": 1.3364, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.445}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exponential%20convergence%20rates%20for%20momentum%20stochastic%20gradient%20descent%0A%20%20in%20the%20overparametrized%20setting&body=Title%3A%20Exponential%20convergence%20rates%20for%20momentum%20stochastic%20gradient%20descent%0A%20%20in%20the%20overparametrized%20setting%0AAuthor%3A%20Benjamin%20Gess%20and%20Sebastian%20Kassing%0AAbstract%3A%20%20%20We%20prove%20explicit%20bounds%20on%20the%20exponential%20rate%20of%20convergence%20for%20the%0Amomentum%20stochastic%20gradient%20descent%20scheme%20%28MSGD%29%20for%20arbitrary%2C%20fixed%0Ahyperparameters%20%28learning%20rate%2C%20friction%20parameter%29%20and%20its%20continuous-in-time%0Acounterpart%20in%20the%20context%20of%20non-convex%20optimization.%20In%20the%20small%20step-size%0Aregime%20and%20in%20the%20case%20of%20flat%20minima%20or%20large%20noise%20intensities%2C%20these%20bounds%0Aprove%20faster%20convergence%20of%20MSGD%20compared%20to%20plain%20stochastic%20gradient%20descent%0A%28SGD%29.%20The%20results%20are%20shown%20for%20objective%20functions%20satisfying%20a%20local%0APolyak-Lojasiewicz%20inequality%20and%20under%20assumptions%20on%20the%20variance%20of%20MSGD%0Athat%20are%20satisfied%20in%20overparametrized%20settings.%20Moreover%2C%20we%20analyze%20the%0Aoptimal%20choice%20of%20the%20friction%20parameter%20and%20show%20that%20the%20MSGD%20process%20almost%0Asurely%20converges%20to%20a%20local%20minimum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExponential%2520convergence%2520rates%2520for%2520momentum%2520stochastic%2520gradient%2520descent%250A%2520%2520in%2520the%2520overparametrized%2520setting%26entry.906535625%3DBenjamin%2520Gess%2520and%2520Sebastian%2520Kassing%26entry.1292438233%3D%2520%2520We%2520prove%2520explicit%2520bounds%2520on%2520the%2520exponential%2520rate%2520of%2520convergence%2520for%2520the%250Amomentum%2520stochastic%2520gradient%2520descent%2520scheme%2520%2528MSGD%2529%2520for%2520arbitrary%252C%2520fixed%250Ahyperparameters%2520%2528learning%2520rate%252C%2520friction%2520parameter%2529%2520and%2520its%2520continuous-in-time%250Acounterpart%2520in%2520the%2520context%2520of%2520non-convex%2520optimization.%2520In%2520the%2520small%2520step-size%250Aregime%2520and%2520in%2520the%2520case%2520of%2520flat%2520minima%2520or%2520large%2520noise%2520intensities%252C%2520these%2520bounds%250Aprove%2520faster%2520convergence%2520of%2520MSGD%2520compared%2520to%2520plain%2520stochastic%2520gradient%2520descent%250A%2528SGD%2529.%2520The%2520results%2520are%2520shown%2520for%2520objective%2520functions%2520satisfying%2520a%2520local%250APolyak-Lojasiewicz%2520inequality%2520and%2520under%2520assumptions%2520on%2520the%2520variance%2520of%2520MSGD%250Athat%2520are%2520satisfied%2520in%2520overparametrized%2520settings.%2520Moreover%252C%2520we%2520analyze%2520the%250Aoptimal%2520choice%2520of%2520the%2520friction%2520parameter%2520and%2520show%2520that%2520the%2520MSGD%2520process%2520almost%250Asurely%2520converges%2520to%2520a%2520local%2520minimum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.03550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exponential%20convergence%20rates%20for%20momentum%20stochastic%20gradient%20descent%0A%20%20in%20the%20overparametrized%20setting&entry.906535625=Benjamin%20Gess%20and%20Sebastian%20Kassing&entry.1292438233=%20%20We%20prove%20explicit%20bounds%20on%20the%20exponential%20rate%20of%20convergence%20for%20the%0Amomentum%20stochastic%20gradient%20descent%20scheme%20%28MSGD%29%20for%20arbitrary%2C%20fixed%0Ahyperparameters%20%28learning%20rate%2C%20friction%20parameter%29%20and%20its%20continuous-in-time%0Acounterpart%20in%20the%20context%20of%20non-convex%20optimization.%20In%20the%20small%20step-size%0Aregime%20and%20in%20the%20case%20of%20flat%20minima%20or%20large%20noise%20intensities%2C%20these%20bounds%0Aprove%20faster%20convergence%20of%20MSGD%20compared%20to%20plain%20stochastic%20gradient%20descent%0A%28SGD%29.%20The%20results%20are%20shown%20for%20objective%20functions%20satisfying%20a%20local%0APolyak-Lojasiewicz%20inequality%20and%20under%20assumptions%20on%20the%20variance%20of%20MSGD%0Athat%20are%20satisfied%20in%20overparametrized%20settings.%20Moreover%2C%20we%20analyze%20the%0Aoptimal%20choice%20of%20the%20friction%20parameter%20and%20show%20that%20the%20MSGD%20process%20almost%0Asurely%20converges%20to%20a%20local%20minimum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03550v2&entry.124074799=Read"},
{"title": "Multi-Scale and Multimodal Species Distribution Modeling", "author": "Nina van Tiel and Robin Zbinden and Emanuele Dalsasso and Benjamin Kellenberger and Lo\u00efc Pellissier and Devis Tuia", "abstract": "  Species distribution models (SDMs) aim to predict the distribution of species\nby relating occurrence data with environmental variables. Recent applications\nof deep learning to SDMs have enabled new avenues, specifically the inclusion\nof spatial data (environmental rasters, satellite images) as model predictors,\nallowing the model to consider the spatial context around each species'\nobservations. However, the appropriate spatial extent of the images is not\nstraightforward to determine and may affect the performance of the model, as\nscale is recognized as an important factor in SDMs. We develop a modular\nstructure for SDMs that allows us to test the effect of scale in both single-\nand multi-scale settings. Furthermore, our model enables different scales to be\nconsidered for different modalities, using a late fusion approach. Results on\nthe GeoLifeCLEF 2023 benchmark indicate that considering multimodal data and\nlearning multi-scale representations leads to more accurate models.\n", "link": "http://arxiv.org/abs/2411.04016v1", "date": "2024-11-06", "relevancy": 1.6529, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5583}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20and%20Multimodal%20Species%20Distribution%20Modeling&body=Title%3A%20Multi-Scale%20and%20Multimodal%20Species%20Distribution%20Modeling%0AAuthor%3A%20Nina%20van%20Tiel%20and%20Robin%20Zbinden%20and%20Emanuele%20Dalsasso%20and%20Benjamin%20Kellenberger%20and%20Lo%C3%AFc%20Pellissier%20and%20Devis%20Tuia%0AAbstract%3A%20%20%20Species%20distribution%20models%20%28SDMs%29%20aim%20to%20predict%20the%20distribution%20of%20species%0Aby%20relating%20occurrence%20data%20with%20environmental%20variables.%20Recent%20applications%0Aof%20deep%20learning%20to%20SDMs%20have%20enabled%20new%20avenues%2C%20specifically%20the%20inclusion%0Aof%20spatial%20data%20%28environmental%20rasters%2C%20satellite%20images%29%20as%20model%20predictors%2C%0Aallowing%20the%20model%20to%20consider%20the%20spatial%20context%20around%20each%20species%27%0Aobservations.%20However%2C%20the%20appropriate%20spatial%20extent%20of%20the%20images%20is%20not%0Astraightforward%20to%20determine%20and%20may%20affect%20the%20performance%20of%20the%20model%2C%20as%0Ascale%20is%20recognized%20as%20an%20important%20factor%20in%20SDMs.%20We%20develop%20a%20modular%0Astructure%20for%20SDMs%20that%20allows%20us%20to%20test%20the%20effect%20of%20scale%20in%20both%20single-%0Aand%20multi-scale%20settings.%20Furthermore%2C%20our%20model%20enables%20different%20scales%20to%20be%0Aconsidered%20for%20different%20modalities%2C%20using%20a%20late%20fusion%20approach.%20Results%20on%0Athe%20GeoLifeCLEF%202023%20benchmark%20indicate%20that%20considering%20multimodal%20data%20and%0Alearning%20multi-scale%20representations%20leads%20to%20more%20accurate%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520and%2520Multimodal%2520Species%2520Distribution%2520Modeling%26entry.906535625%3DNina%2520van%2520Tiel%2520and%2520Robin%2520Zbinden%2520and%2520Emanuele%2520Dalsasso%2520and%2520Benjamin%2520Kellenberger%2520and%2520Lo%25C3%25AFc%2520Pellissier%2520and%2520Devis%2520Tuia%26entry.1292438233%3D%2520%2520Species%2520distribution%2520models%2520%2528SDMs%2529%2520aim%2520to%2520predict%2520the%2520distribution%2520of%2520species%250Aby%2520relating%2520occurrence%2520data%2520with%2520environmental%2520variables.%2520Recent%2520applications%250Aof%2520deep%2520learning%2520to%2520SDMs%2520have%2520enabled%2520new%2520avenues%252C%2520specifically%2520the%2520inclusion%250Aof%2520spatial%2520data%2520%2528environmental%2520rasters%252C%2520satellite%2520images%2529%2520as%2520model%2520predictors%252C%250Aallowing%2520the%2520model%2520to%2520consider%2520the%2520spatial%2520context%2520around%2520each%2520species%2527%250Aobservations.%2520However%252C%2520the%2520appropriate%2520spatial%2520extent%2520of%2520the%2520images%2520is%2520not%250Astraightforward%2520to%2520determine%2520and%2520may%2520affect%2520the%2520performance%2520of%2520the%2520model%252C%2520as%250Ascale%2520is%2520recognized%2520as%2520an%2520important%2520factor%2520in%2520SDMs.%2520We%2520develop%2520a%2520modular%250Astructure%2520for%2520SDMs%2520that%2520allows%2520us%2520to%2520test%2520the%2520effect%2520of%2520scale%2520in%2520both%2520single-%250Aand%2520multi-scale%2520settings.%2520Furthermore%252C%2520our%2520model%2520enables%2520different%2520scales%2520to%2520be%250Aconsidered%2520for%2520different%2520modalities%252C%2520using%2520a%2520late%2520fusion%2520approach.%2520Results%2520on%250Athe%2520GeoLifeCLEF%25202023%2520benchmark%2520indicate%2520that%2520considering%2520multimodal%2520data%2520and%250Alearning%2520multi-scale%2520representations%2520leads%2520to%2520more%2520accurate%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20and%20Multimodal%20Species%20Distribution%20Modeling&entry.906535625=Nina%20van%20Tiel%20and%20Robin%20Zbinden%20and%20Emanuele%20Dalsasso%20and%20Benjamin%20Kellenberger%20and%20Lo%C3%AFc%20Pellissier%20and%20Devis%20Tuia&entry.1292438233=%20%20Species%20distribution%20models%20%28SDMs%29%20aim%20to%20predict%20the%20distribution%20of%20species%0Aby%20relating%20occurrence%20data%20with%20environmental%20variables.%20Recent%20applications%0Aof%20deep%20learning%20to%20SDMs%20have%20enabled%20new%20avenues%2C%20specifically%20the%20inclusion%0Aof%20spatial%20data%20%28environmental%20rasters%2C%20satellite%20images%29%20as%20model%20predictors%2C%0Aallowing%20the%20model%20to%20consider%20the%20spatial%20context%20around%20each%20species%27%0Aobservations.%20However%2C%20the%20appropriate%20spatial%20extent%20of%20the%20images%20is%20not%0Astraightforward%20to%20determine%20and%20may%20affect%20the%20performance%20of%20the%20model%2C%20as%0Ascale%20is%20recognized%20as%20an%20important%20factor%20in%20SDMs.%20We%20develop%20a%20modular%0Astructure%20for%20SDMs%20that%20allows%20us%20to%20test%20the%20effect%20of%20scale%20in%20both%20single-%0Aand%20multi-scale%20settings.%20Furthermore%2C%20our%20model%20enables%20different%20scales%20to%20be%0Aconsidered%20for%20different%20modalities%2C%20using%20a%20late%20fusion%20approach.%20Results%20on%0Athe%20GeoLifeCLEF%202023%20benchmark%20indicate%20that%20considering%20multimodal%20data%20and%0Alearning%20multi-scale%20representations%20leads%20to%20more%20accurate%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04016v1&entry.124074799=Read"},
{"title": "A Learnable Prior Improves Inverse Tumor Growth Modeling", "author": "Jonas Weidner and Ivan Ezhov and Michal Balcerak and Marie-Christin Metz and Sergey Litvinov and Sebastian Kaltenbach and Leonhard Feiner and Laurin Lux and Florian Kofler and Jana Lipkova and Jonas Latz and Daniel Rueckert and Bjoern Menze and Benedikt Wiestler", "abstract": "  Biophysical modeling, particularly involving partial differential equations\n(PDEs), offers significant potential for tailoring disease treatment protocols\nto individual patients. However, the inverse problem-solving aspect of these\nmodels presents a substantial challenge, either due to the high computational\nrequirements of model-based approaches or the limited robustness of deep\nlearning (DL) methods. We propose a novel framework that leverages the unique\nstrengths of both approaches in a synergistic manner. Our method incorporates a\nDL ensemble for initial parameter estimation, facilitating efficient downstream\nevolutionary sampling initialized with this DL-based prior. We showcase the\neffectiveness of integrating a rapid deep-learning algorithm with a\nhigh-precision evolution strategy in estimating brain tumor cell concentrations\nfrom magnetic resonance images. The DL-Prior plays a pivotal role,\nsignificantly constraining the effective sampling-parameter space. This\nreduction results in a fivefold convergence acceleration and a Dice-score of\n95%.\n", "link": "http://arxiv.org/abs/2403.04500v2", "date": "2024-11-06", "relevancy": 0.9781, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4882}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Learnable%20Prior%20Improves%20Inverse%20Tumor%20Growth%20Modeling&body=Title%3A%20A%20Learnable%20Prior%20Improves%20Inverse%20Tumor%20Growth%20Modeling%0AAuthor%3A%20Jonas%20Weidner%20and%20Ivan%20Ezhov%20and%20Michal%20Balcerak%20and%20Marie-Christin%20Metz%20and%20Sergey%20Litvinov%20and%20Sebastian%20Kaltenbach%20and%20Leonhard%20Feiner%20and%20Laurin%20Lux%20and%20Florian%20Kofler%20and%20Jana%20Lipkova%20and%20Jonas%20Latz%20and%20Daniel%20Rueckert%20and%20Bjoern%20Menze%20and%20Benedikt%20Wiestler%0AAbstract%3A%20%20%20Biophysical%20modeling%2C%20particularly%20involving%20partial%20differential%20equations%0A%28PDEs%29%2C%20offers%20significant%20potential%20for%20tailoring%20disease%20treatment%20protocols%0Ato%20individual%20patients.%20However%2C%20the%20inverse%20problem-solving%20aspect%20of%20these%0Amodels%20presents%20a%20substantial%20challenge%2C%20either%20due%20to%20the%20high%20computational%0Arequirements%20of%20model-based%20approaches%20or%20the%20limited%20robustness%20of%20deep%0Alearning%20%28DL%29%20methods.%20We%20propose%20a%20novel%20framework%20that%20leverages%20the%20unique%0Astrengths%20of%20both%20approaches%20in%20a%20synergistic%20manner.%20Our%20method%20incorporates%20a%0ADL%20ensemble%20for%20initial%20parameter%20estimation%2C%20facilitating%20efficient%20downstream%0Aevolutionary%20sampling%20initialized%20with%20this%20DL-based%20prior.%20We%20showcase%20the%0Aeffectiveness%20of%20integrating%20a%20rapid%20deep-learning%20algorithm%20with%20a%0Ahigh-precision%20evolution%20strategy%20in%20estimating%20brain%20tumor%20cell%20concentrations%0Afrom%20magnetic%20resonance%20images.%20The%20DL-Prior%20plays%20a%20pivotal%20role%2C%0Asignificantly%20constraining%20the%20effective%20sampling-parameter%20space.%20This%0Areduction%20results%20in%20a%20fivefold%20convergence%20acceleration%20and%20a%20Dice-score%20of%0A95%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Learnable%2520Prior%2520Improves%2520Inverse%2520Tumor%2520Growth%2520Modeling%26entry.906535625%3DJonas%2520Weidner%2520and%2520Ivan%2520Ezhov%2520and%2520Michal%2520Balcerak%2520and%2520Marie-Christin%2520Metz%2520and%2520Sergey%2520Litvinov%2520and%2520Sebastian%2520Kaltenbach%2520and%2520Leonhard%2520Feiner%2520and%2520Laurin%2520Lux%2520and%2520Florian%2520Kofler%2520and%2520Jana%2520Lipkova%2520and%2520Jonas%2520Latz%2520and%2520Daniel%2520Rueckert%2520and%2520Bjoern%2520Menze%2520and%2520Benedikt%2520Wiestler%26entry.1292438233%3D%2520%2520Biophysical%2520modeling%252C%2520particularly%2520involving%2520partial%2520differential%2520equations%250A%2528PDEs%2529%252C%2520offers%2520significant%2520potential%2520for%2520tailoring%2520disease%2520treatment%2520protocols%250Ato%2520individual%2520patients.%2520However%252C%2520the%2520inverse%2520problem-solving%2520aspect%2520of%2520these%250Amodels%2520presents%2520a%2520substantial%2520challenge%252C%2520either%2520due%2520to%2520the%2520high%2520computational%250Arequirements%2520of%2520model-based%2520approaches%2520or%2520the%2520limited%2520robustness%2520of%2520deep%250Alearning%2520%2528DL%2529%2520methods.%2520We%2520propose%2520a%2520novel%2520framework%2520that%2520leverages%2520the%2520unique%250Astrengths%2520of%2520both%2520approaches%2520in%2520a%2520synergistic%2520manner.%2520Our%2520method%2520incorporates%2520a%250ADL%2520ensemble%2520for%2520initial%2520parameter%2520estimation%252C%2520facilitating%2520efficient%2520downstream%250Aevolutionary%2520sampling%2520initialized%2520with%2520this%2520DL-based%2520prior.%2520We%2520showcase%2520the%250Aeffectiveness%2520of%2520integrating%2520a%2520rapid%2520deep-learning%2520algorithm%2520with%2520a%250Ahigh-precision%2520evolution%2520strategy%2520in%2520estimating%2520brain%2520tumor%2520cell%2520concentrations%250Afrom%2520magnetic%2520resonance%2520images.%2520The%2520DL-Prior%2520plays%2520a%2520pivotal%2520role%252C%250Asignificantly%2520constraining%2520the%2520effective%2520sampling-parameter%2520space.%2520This%250Areduction%2520results%2520in%2520a%2520fivefold%2520convergence%2520acceleration%2520and%2520a%2520Dice-score%2520of%250A95%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Learnable%20Prior%20Improves%20Inverse%20Tumor%20Growth%20Modeling&entry.906535625=Jonas%20Weidner%20and%20Ivan%20Ezhov%20and%20Michal%20Balcerak%20and%20Marie-Christin%20Metz%20and%20Sergey%20Litvinov%20and%20Sebastian%20Kaltenbach%20and%20Leonhard%20Feiner%20and%20Laurin%20Lux%20and%20Florian%20Kofler%20and%20Jana%20Lipkova%20and%20Jonas%20Latz%20and%20Daniel%20Rueckert%20and%20Bjoern%20Menze%20and%20Benedikt%20Wiestler&entry.1292438233=%20%20Biophysical%20modeling%2C%20particularly%20involving%20partial%20differential%20equations%0A%28PDEs%29%2C%20offers%20significant%20potential%20for%20tailoring%20disease%20treatment%20protocols%0Ato%20individual%20patients.%20However%2C%20the%20inverse%20problem-solving%20aspect%20of%20these%0Amodels%20presents%20a%20substantial%20challenge%2C%20either%20due%20to%20the%20high%20computational%0Arequirements%20of%20model-based%20approaches%20or%20the%20limited%20robustness%20of%20deep%0Alearning%20%28DL%29%20methods.%20We%20propose%20a%20novel%20framework%20that%20leverages%20the%20unique%0Astrengths%20of%20both%20approaches%20in%20a%20synergistic%20manner.%20Our%20method%20incorporates%20a%0ADL%20ensemble%20for%20initial%20parameter%20estimation%2C%20facilitating%20efficient%20downstream%0Aevolutionary%20sampling%20initialized%20with%20this%20DL-based%20prior.%20We%20showcase%20the%0Aeffectiveness%20of%20integrating%20a%20rapid%20deep-learning%20algorithm%20with%20a%0Ahigh-precision%20evolution%20strategy%20in%20estimating%20brain%20tumor%20cell%20concentrations%0Afrom%20magnetic%20resonance%20images.%20The%20DL-Prior%20plays%20a%20pivotal%20role%2C%0Asignificantly%20constraining%20the%20effective%20sampling-parameter%20space.%20This%0Areduction%20results%20in%20a%20fivefold%20convergence%20acceleration%20and%20a%20Dice-score%20of%0A95%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04500v2&entry.124074799=Read"},
{"title": "From Federated Learning to Quantum Federated Learning for\n  Space-Air-Ground Integrated Networks", "author": "Vu Khanh Quy and Nguyen Minh Quy and Tran Thi Hoai and Shaba Shaon and Md Raihan Uddin and Tien Nguyen and Dinh C. Nguyen and Aryan Kaushik and Periklis Chatzimisios", "abstract": "  6G wireless networks are expected to provide seamless and data-based\nconnections that cover space-air-ground and underwater networks. As a core\npartition of future 6G networks, Space-Air-Ground Integrated Networks (SAGIN)\nhave been envisioned to provide countless real-time intelligent applications.\nTo realize this, promoting AI techniques into SAGIN is an inevitable trend. Due\nto the distributed and heterogeneous architecture of SAGIN, federated learning\n(FL) and then quantum FL are emerging AI model training techniques for enabling\nfuture privacy-enhanced and computation-efficient SAGINs. In this work, we\nexplore the vision of using FL/QFL in SAGINs. We present a few representative\napplications enabled by the integration of FL and QFL in SAGINs. A case study\nof QFL over UAV networks is also given, showing the merit of quantum-enabled\ntraining approach over the conventional FL benchmark. Research challenges along\nwith standardization for QFL adoption in future SAGINs are also highlighted.\n", "link": "http://arxiv.org/abs/2411.01312v2", "date": "2024-11-06", "relevancy": 1.3351, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4578}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.434}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Federated%20Learning%20to%20Quantum%20Federated%20Learning%20for%0A%20%20Space-Air-Ground%20Integrated%20Networks&body=Title%3A%20From%20Federated%20Learning%20to%20Quantum%20Federated%20Learning%20for%0A%20%20Space-Air-Ground%20Integrated%20Networks%0AAuthor%3A%20Vu%20Khanh%20Quy%20and%20Nguyen%20Minh%20Quy%20and%20Tran%20Thi%20Hoai%20and%20Shaba%20Shaon%20and%20Md%20Raihan%20Uddin%20and%20Tien%20Nguyen%20and%20Dinh%20C.%20Nguyen%20and%20Aryan%20Kaushik%20and%20Periklis%20Chatzimisios%0AAbstract%3A%20%20%206G%20wireless%20networks%20are%20expected%20to%20provide%20seamless%20and%20data-based%0Aconnections%20that%20cover%20space-air-ground%20and%20underwater%20networks.%20As%20a%20core%0Apartition%20of%20future%206G%20networks%2C%20Space-Air-Ground%20Integrated%20Networks%20%28SAGIN%29%0Ahave%20been%20envisioned%20to%20provide%20countless%20real-time%20intelligent%20applications.%0ATo%20realize%20this%2C%20promoting%20AI%20techniques%20into%20SAGIN%20is%20an%20inevitable%20trend.%20Due%0Ato%20the%20distributed%20and%20heterogeneous%20architecture%20of%20SAGIN%2C%20federated%20learning%0A%28FL%29%20and%20then%20quantum%20FL%20are%20emerging%20AI%20model%20training%20techniques%20for%20enabling%0Afuture%20privacy-enhanced%20and%20computation-efficient%20SAGINs.%20In%20this%20work%2C%20we%0Aexplore%20the%20vision%20of%20using%20FL/QFL%20in%20SAGINs.%20We%20present%20a%20few%20representative%0Aapplications%20enabled%20by%20the%20integration%20of%20FL%20and%20QFL%20in%20SAGINs.%20A%20case%20study%0Aof%20QFL%20over%20UAV%20networks%20is%20also%20given%2C%20showing%20the%20merit%20of%20quantum-enabled%0Atraining%20approach%20over%20the%20conventional%20FL%20benchmark.%20Research%20challenges%20along%0Awith%20standardization%20for%20QFL%20adoption%20in%20future%20SAGINs%20are%20also%20highlighted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Federated%2520Learning%2520to%2520Quantum%2520Federated%2520Learning%2520for%250A%2520%2520Space-Air-Ground%2520Integrated%2520Networks%26entry.906535625%3DVu%2520Khanh%2520Quy%2520and%2520Nguyen%2520Minh%2520Quy%2520and%2520Tran%2520Thi%2520Hoai%2520and%2520Shaba%2520Shaon%2520and%2520Md%2520Raihan%2520Uddin%2520and%2520Tien%2520Nguyen%2520and%2520Dinh%2520C.%2520Nguyen%2520and%2520Aryan%2520Kaushik%2520and%2520Periklis%2520Chatzimisios%26entry.1292438233%3D%2520%25206G%2520wireless%2520networks%2520are%2520expected%2520to%2520provide%2520seamless%2520and%2520data-based%250Aconnections%2520that%2520cover%2520space-air-ground%2520and%2520underwater%2520networks.%2520As%2520a%2520core%250Apartition%2520of%2520future%25206G%2520networks%252C%2520Space-Air-Ground%2520Integrated%2520Networks%2520%2528SAGIN%2529%250Ahave%2520been%2520envisioned%2520to%2520provide%2520countless%2520real-time%2520intelligent%2520applications.%250ATo%2520realize%2520this%252C%2520promoting%2520AI%2520techniques%2520into%2520SAGIN%2520is%2520an%2520inevitable%2520trend.%2520Due%250Ato%2520the%2520distributed%2520and%2520heterogeneous%2520architecture%2520of%2520SAGIN%252C%2520federated%2520learning%250A%2528FL%2529%2520and%2520then%2520quantum%2520FL%2520are%2520emerging%2520AI%2520model%2520training%2520techniques%2520for%2520enabling%250Afuture%2520privacy-enhanced%2520and%2520computation-efficient%2520SAGINs.%2520In%2520this%2520work%252C%2520we%250Aexplore%2520the%2520vision%2520of%2520using%2520FL/QFL%2520in%2520SAGINs.%2520We%2520present%2520a%2520few%2520representative%250Aapplications%2520enabled%2520by%2520the%2520integration%2520of%2520FL%2520and%2520QFL%2520in%2520SAGINs.%2520A%2520case%2520study%250Aof%2520QFL%2520over%2520UAV%2520networks%2520is%2520also%2520given%252C%2520showing%2520the%2520merit%2520of%2520quantum-enabled%250Atraining%2520approach%2520over%2520the%2520conventional%2520FL%2520benchmark.%2520Research%2520challenges%2520along%250Awith%2520standardization%2520for%2520QFL%2520adoption%2520in%2520future%2520SAGINs%2520are%2520also%2520highlighted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Federated%20Learning%20to%20Quantum%20Federated%20Learning%20for%0A%20%20Space-Air-Ground%20Integrated%20Networks&entry.906535625=Vu%20Khanh%20Quy%20and%20Nguyen%20Minh%20Quy%20and%20Tran%20Thi%20Hoai%20and%20Shaba%20Shaon%20and%20Md%20Raihan%20Uddin%20and%20Tien%20Nguyen%20and%20Dinh%20C.%20Nguyen%20and%20Aryan%20Kaushik%20and%20Periklis%20Chatzimisios&entry.1292438233=%20%206G%20wireless%20networks%20are%20expected%20to%20provide%20seamless%20and%20data-based%0Aconnections%20that%20cover%20space-air-ground%20and%20underwater%20networks.%20As%20a%20core%0Apartition%20of%20future%206G%20networks%2C%20Space-Air-Ground%20Integrated%20Networks%20%28SAGIN%29%0Ahave%20been%20envisioned%20to%20provide%20countless%20real-time%20intelligent%20applications.%0ATo%20realize%20this%2C%20promoting%20AI%20techniques%20into%20SAGIN%20is%20an%20inevitable%20trend.%20Due%0Ato%20the%20distributed%20and%20heterogeneous%20architecture%20of%20SAGIN%2C%20federated%20learning%0A%28FL%29%20and%20then%20quantum%20FL%20are%20emerging%20AI%20model%20training%20techniques%20for%20enabling%0Afuture%20privacy-enhanced%20and%20computation-efficient%20SAGINs.%20In%20this%20work%2C%20we%0Aexplore%20the%20vision%20of%20using%20FL/QFL%20in%20SAGINs.%20We%20present%20a%20few%20representative%0Aapplications%20enabled%20by%20the%20integration%20of%20FL%20and%20QFL%20in%20SAGINs.%20A%20case%20study%0Aof%20QFL%20over%20UAV%20networks%20is%20also%20given%2C%20showing%20the%20merit%20of%20quantum-enabled%0Atraining%20approach%20over%20the%20conventional%20FL%20benchmark.%20Research%20challenges%20along%0Awith%20standardization%20for%20QFL%20adoption%20in%20future%20SAGINs%20are%20also%20highlighted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01312v2&entry.124074799=Read"},
{"title": "Synomaly Noise and Multi-Stage Diffusion: A Novel Approach for\n  Unsupervised Anomaly Detection in Ultrasound Imaging", "author": "Yuan Bi and Lucie Huang and Ricarda Clarenbach and Reza Ghotbi and Angelos Karlas and Nassir Navab and Zhongliang Jiang", "abstract": "  Ultrasound (US) imaging is widely used in routine clinical practice due to\nits advantages of being radiation-free, cost-effective, and portable. However,\nthe low reproducibility and quality of US images, combined with the scarcity of\nexpert-level annotation, make the training of fully supervised segmentation\nmodels challenging. To address these issues, we propose a novel unsupervised\nanomaly detection framework based on a diffusion model that incorporates a\nsynthetic anomaly (Synomaly) noise function and a multi-stage diffusion\nprocess. Synomaly noise introduces synthetic anomalies into healthy images\nduring training, allowing the model to effectively learn anomaly removal. The\nmulti-stage diffusion process is introduced to progressively denoise images,\npreserving fine details while improving the quality of anomaly-free\nreconstructions. The generated high-fidelity counterfactual healthy images can\nfurther enhance the interpretability of the segmentation models, as well as\nprovide a reliable baseline for evaluating the extent of anomalies and\nsupporting clinical decision-making. Notably, the unsupervised anomaly\ndetection model is trained purely on healthy images, eliminating the need for\nanomalous training samples and pixel-level annotations. We validate the\nproposed approach on carotid US, brain MRI, and liver CT datasets. The\nexperimental results demonstrate that the proposed framework outperforms\nexisting state-of-the-art unsupervised anomaly detection methods, achieving\nperformance comparable to fully supervised segmentation models in the US\ndataset. Additionally, ablation studies underline the importance of\nhyperparameter selection for Synomaly noise and the effectiveness of the\nmulti-stage diffusion process in enhancing model performance.\n", "link": "http://arxiv.org/abs/2411.04004v1", "date": "2024-11-06", "relevancy": 1.6642, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5716}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5527}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synomaly%20Noise%20and%20Multi-Stage%20Diffusion%3A%20A%20Novel%20Approach%20for%0A%20%20Unsupervised%20Anomaly%20Detection%20in%20Ultrasound%20Imaging&body=Title%3A%20Synomaly%20Noise%20and%20Multi-Stage%20Diffusion%3A%20A%20Novel%20Approach%20for%0A%20%20Unsupervised%20Anomaly%20Detection%20in%20Ultrasound%20Imaging%0AAuthor%3A%20Yuan%20Bi%20and%20Lucie%20Huang%20and%20Ricarda%20Clarenbach%20and%20Reza%20Ghotbi%20and%20Angelos%20Karlas%20and%20Nassir%20Navab%20and%20Zhongliang%20Jiang%0AAbstract%3A%20%20%20Ultrasound%20%28US%29%20imaging%20is%20widely%20used%20in%20routine%20clinical%20practice%20due%20to%0Aits%20advantages%20of%20being%20radiation-free%2C%20cost-effective%2C%20and%20portable.%20However%2C%0Athe%20low%20reproducibility%20and%20quality%20of%20US%20images%2C%20combined%20with%20the%20scarcity%20of%0Aexpert-level%20annotation%2C%20make%20the%20training%20of%20fully%20supervised%20segmentation%0Amodels%20challenging.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20unsupervised%0Aanomaly%20detection%20framework%20based%20on%20a%20diffusion%20model%20that%20incorporates%20a%0Asynthetic%20anomaly%20%28Synomaly%29%20noise%20function%20and%20a%20multi-stage%20diffusion%0Aprocess.%20Synomaly%20noise%20introduces%20synthetic%20anomalies%20into%20healthy%20images%0Aduring%20training%2C%20allowing%20the%20model%20to%20effectively%20learn%20anomaly%20removal.%20The%0Amulti-stage%20diffusion%20process%20is%20introduced%20to%20progressively%20denoise%20images%2C%0Apreserving%20fine%20details%20while%20improving%20the%20quality%20of%20anomaly-free%0Areconstructions.%20The%20generated%20high-fidelity%20counterfactual%20healthy%20images%20can%0Afurther%20enhance%20the%20interpretability%20of%20the%20segmentation%20models%2C%20as%20well%20as%0Aprovide%20a%20reliable%20baseline%20for%20evaluating%20the%20extent%20of%20anomalies%20and%0Asupporting%20clinical%20decision-making.%20Notably%2C%20the%20unsupervised%20anomaly%0Adetection%20model%20is%20trained%20purely%20on%20healthy%20images%2C%20eliminating%20the%20need%20for%0Aanomalous%20training%20samples%20and%20pixel-level%20annotations.%20We%20validate%20the%0Aproposed%20approach%20on%20carotid%20US%2C%20brain%20MRI%2C%20and%20liver%20CT%20datasets.%20The%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20framework%20outperforms%0Aexisting%20state-of-the-art%20unsupervised%20anomaly%20detection%20methods%2C%20achieving%0Aperformance%20comparable%20to%20fully%20supervised%20segmentation%20models%20in%20the%20US%0Adataset.%20Additionally%2C%20ablation%20studies%20underline%20the%20importance%20of%0Ahyperparameter%20selection%20for%20Synomaly%20noise%20and%20the%20effectiveness%20of%20the%0Amulti-stage%20diffusion%20process%20in%20enhancing%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynomaly%2520Noise%2520and%2520Multi-Stage%2520Diffusion%253A%2520A%2520Novel%2520Approach%2520for%250A%2520%2520Unsupervised%2520Anomaly%2520Detection%2520in%2520Ultrasound%2520Imaging%26entry.906535625%3DYuan%2520Bi%2520and%2520Lucie%2520Huang%2520and%2520Ricarda%2520Clarenbach%2520and%2520Reza%2520Ghotbi%2520and%2520Angelos%2520Karlas%2520and%2520Nassir%2520Navab%2520and%2520Zhongliang%2520Jiang%26entry.1292438233%3D%2520%2520Ultrasound%2520%2528US%2529%2520imaging%2520is%2520widely%2520used%2520in%2520routine%2520clinical%2520practice%2520due%2520to%250Aits%2520advantages%2520of%2520being%2520radiation-free%252C%2520cost-effective%252C%2520and%2520portable.%2520However%252C%250Athe%2520low%2520reproducibility%2520and%2520quality%2520of%2520US%2520images%252C%2520combined%2520with%2520the%2520scarcity%2520of%250Aexpert-level%2520annotation%252C%2520make%2520the%2520training%2520of%2520fully%2520supervised%2520segmentation%250Amodels%2520challenging.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520unsupervised%250Aanomaly%2520detection%2520framework%2520based%2520on%2520a%2520diffusion%2520model%2520that%2520incorporates%2520a%250Asynthetic%2520anomaly%2520%2528Synomaly%2529%2520noise%2520function%2520and%2520a%2520multi-stage%2520diffusion%250Aprocess.%2520Synomaly%2520noise%2520introduces%2520synthetic%2520anomalies%2520into%2520healthy%2520images%250Aduring%2520training%252C%2520allowing%2520the%2520model%2520to%2520effectively%2520learn%2520anomaly%2520removal.%2520The%250Amulti-stage%2520diffusion%2520process%2520is%2520introduced%2520to%2520progressively%2520denoise%2520images%252C%250Apreserving%2520fine%2520details%2520while%2520improving%2520the%2520quality%2520of%2520anomaly-free%250Areconstructions.%2520The%2520generated%2520high-fidelity%2520counterfactual%2520healthy%2520images%2520can%250Afurther%2520enhance%2520the%2520interpretability%2520of%2520the%2520segmentation%2520models%252C%2520as%2520well%2520as%250Aprovide%2520a%2520reliable%2520baseline%2520for%2520evaluating%2520the%2520extent%2520of%2520anomalies%2520and%250Asupporting%2520clinical%2520decision-making.%2520Notably%252C%2520the%2520unsupervised%2520anomaly%250Adetection%2520model%2520is%2520trained%2520purely%2520on%2520healthy%2520images%252C%2520eliminating%2520the%2520need%2520for%250Aanomalous%2520training%2520samples%2520and%2520pixel-level%2520annotations.%2520We%2520validate%2520the%250Aproposed%2520approach%2520on%2520carotid%2520US%252C%2520brain%2520MRI%252C%2520and%2520liver%2520CT%2520datasets.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520framework%2520outperforms%250Aexisting%2520state-of-the-art%2520unsupervised%2520anomaly%2520detection%2520methods%252C%2520achieving%250Aperformance%2520comparable%2520to%2520fully%2520supervised%2520segmentation%2520models%2520in%2520the%2520US%250Adataset.%2520Additionally%252C%2520ablation%2520studies%2520underline%2520the%2520importance%2520of%250Ahyperparameter%2520selection%2520for%2520Synomaly%2520noise%2520and%2520the%2520effectiveness%2520of%2520the%250Amulti-stage%2520diffusion%2520process%2520in%2520enhancing%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synomaly%20Noise%20and%20Multi-Stage%20Diffusion%3A%20A%20Novel%20Approach%20for%0A%20%20Unsupervised%20Anomaly%20Detection%20in%20Ultrasound%20Imaging&entry.906535625=Yuan%20Bi%20and%20Lucie%20Huang%20and%20Ricarda%20Clarenbach%20and%20Reza%20Ghotbi%20and%20Angelos%20Karlas%20and%20Nassir%20Navab%20and%20Zhongliang%20Jiang&entry.1292438233=%20%20Ultrasound%20%28US%29%20imaging%20is%20widely%20used%20in%20routine%20clinical%20practice%20due%20to%0Aits%20advantages%20of%20being%20radiation-free%2C%20cost-effective%2C%20and%20portable.%20However%2C%0Athe%20low%20reproducibility%20and%20quality%20of%20US%20images%2C%20combined%20with%20the%20scarcity%20of%0Aexpert-level%20annotation%2C%20make%20the%20training%20of%20fully%20supervised%20segmentation%0Amodels%20challenging.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20unsupervised%0Aanomaly%20detection%20framework%20based%20on%20a%20diffusion%20model%20that%20incorporates%20a%0Asynthetic%20anomaly%20%28Synomaly%29%20noise%20function%20and%20a%20multi-stage%20diffusion%0Aprocess.%20Synomaly%20noise%20introduces%20synthetic%20anomalies%20into%20healthy%20images%0Aduring%20training%2C%20allowing%20the%20model%20to%20effectively%20learn%20anomaly%20removal.%20The%0Amulti-stage%20diffusion%20process%20is%20introduced%20to%20progressively%20denoise%20images%2C%0Apreserving%20fine%20details%20while%20improving%20the%20quality%20of%20anomaly-free%0Areconstructions.%20The%20generated%20high-fidelity%20counterfactual%20healthy%20images%20can%0Afurther%20enhance%20the%20interpretability%20of%20the%20segmentation%20models%2C%20as%20well%20as%0Aprovide%20a%20reliable%20baseline%20for%20evaluating%20the%20extent%20of%20anomalies%20and%0Asupporting%20clinical%20decision-making.%20Notably%2C%20the%20unsupervised%20anomaly%0Adetection%20model%20is%20trained%20purely%20on%20healthy%20images%2C%20eliminating%20the%20need%20for%0Aanomalous%20training%20samples%20and%20pixel-level%20annotations.%20We%20validate%20the%0Aproposed%20approach%20on%20carotid%20US%2C%20brain%20MRI%2C%20and%20liver%20CT%20datasets.%20The%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20framework%20outperforms%0Aexisting%20state-of-the-art%20unsupervised%20anomaly%20detection%20methods%2C%20achieving%0Aperformance%20comparable%20to%20fully%20supervised%20segmentation%20models%20in%20the%20US%0Adataset.%20Additionally%2C%20ablation%20studies%20underline%20the%20importance%20of%0Ahyperparameter%20selection%20for%20Synomaly%20noise%20and%20the%20effectiveness%20of%20the%0Amulti-stage%20diffusion%20process%20in%20enhancing%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04004v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


