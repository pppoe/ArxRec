<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251021.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Moving Light Adaptive Colonoscopy Reconstruction via\n  Illumination-Attenuation-Aware 3D Gaussian Splatting", "author": "Hao Wang and Ying Zhou and Haoyu Zhao and Rui Wang and Qiang Hu and Xing Zhang and Qiang Li and Zhiwei Wang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time\nview synthesis in colonoscopy, enabling critical applications such as virtual\ncolonoscopy and lesion tracking. However, the vanilla 3DGS assumes static\nillumination and that observed appearance depends solely on viewing angle,\nwhich causes incompatibility with the photometric variations in colonoscopic\nscenes induced by dynamic light source/camera. This mismatch forces most 3DGS\nmethods to introduce structure-violating vaporous Gaussian blobs between the\ncamera and tissues to compensate for illumination attenuation, ultimately\ndegrading the quality of 3D reconstructions. Previous works only consider the\nillumination attenuation caused by light distance, ignoring the physical\ncharacters of light source and camera. In this paper, we propose ColIAGS, an\nimproved 3DGS framework tailored for colonoscopy. To mimic realistic appearance\nunder varying illumination, we introduce an Improved Appearance Modeling with\ntwo types of illumination attenuation factors, which enables Gaussians to adapt\nto photometric variations while preserving geometry accuracy. To ensure the\ngeometry approximation condition of appearance modeling, we propose an Improved\nGeometry Modeling using high-dimensional view embedding to enhance Gaussian\ngeometry attribute prediction. Furthermore, another cosine embedding input is\nleveraged to generate illumination attenuation solutions in an implicit manner.\nComprehensive experimental results on standard benchmarks demonstrate that our\nproposed ColIAGS achieves the dual capabilities of novel view synthesis and\naccurate geometric reconstruction. It notably outperforms other\nstate-of-the-art methods by achieving superior rendering fidelity while\nsignificantly reducing Depth MSE. Code will be available.\n", "link": "http://arxiv.org/abs/2510.18739v1", "date": "2025-10-21", "relevancy": 3.2395, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6757}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6453}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moving%20Light%20Adaptive%20Colonoscopy%20Reconstruction%20via%0A%20%20Illumination-Attenuation-Aware%203D%20Gaussian%20Splatting&body=Title%3A%20Moving%20Light%20Adaptive%20Colonoscopy%20Reconstruction%20via%0A%20%20Illumination-Attenuation-Aware%203D%20Gaussian%20Splatting%0AAuthor%3A%20Hao%20Wang%20and%20Ying%20Zhou%20and%20Haoyu%20Zhao%20and%20Rui%20Wang%20and%20Qiang%20Hu%20and%20Xing%20Zhang%20and%20Qiang%20Li%20and%20Zhiwei%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20pivotal%20technique%20for%20real-time%0Aview%20synthesis%20in%20colonoscopy%2C%20enabling%20critical%20applications%20such%20as%20virtual%0Acolonoscopy%20and%20lesion%20tracking.%20However%2C%20the%20vanilla%203DGS%20assumes%20static%0Aillumination%20and%20that%20observed%20appearance%20depends%20solely%20on%20viewing%20angle%2C%0Awhich%20causes%20incompatibility%20with%20the%20photometric%20variations%20in%20colonoscopic%0Ascenes%20induced%20by%20dynamic%20light%20source/camera.%20This%20mismatch%20forces%20most%203DGS%0Amethods%20to%20introduce%20structure-violating%20vaporous%20Gaussian%20blobs%20between%20the%0Acamera%20and%20tissues%20to%20compensate%20for%20illumination%20attenuation%2C%20ultimately%0Adegrading%20the%20quality%20of%203D%20reconstructions.%20Previous%20works%20only%20consider%20the%0Aillumination%20attenuation%20caused%20by%20light%20distance%2C%20ignoring%20the%20physical%0Acharacters%20of%20light%20source%20and%20camera.%20In%20this%20paper%2C%20we%20propose%20ColIAGS%2C%20an%0Aimproved%203DGS%20framework%20tailored%20for%20colonoscopy.%20To%20mimic%20realistic%20appearance%0Aunder%20varying%20illumination%2C%20we%20introduce%20an%20Improved%20Appearance%20Modeling%20with%0Atwo%20types%20of%20illumination%20attenuation%20factors%2C%20which%20enables%20Gaussians%20to%20adapt%0Ato%20photometric%20variations%20while%20preserving%20geometry%20accuracy.%20To%20ensure%20the%0Ageometry%20approximation%20condition%20of%20appearance%20modeling%2C%20we%20propose%20an%20Improved%0AGeometry%20Modeling%20using%20high-dimensional%20view%20embedding%20to%20enhance%20Gaussian%0Ageometry%20attribute%20prediction.%20Furthermore%2C%20another%20cosine%20embedding%20input%20is%0Aleveraged%20to%20generate%20illumination%20attenuation%20solutions%20in%20an%20implicit%20manner.%0AComprehensive%20experimental%20results%20on%20standard%20benchmarks%20demonstrate%20that%20our%0Aproposed%20ColIAGS%20achieves%20the%20dual%20capabilities%20of%20novel%20view%20synthesis%20and%0Aaccurate%20geometric%20reconstruction.%20It%20notably%20outperforms%20other%0Astate-of-the-art%20methods%20by%20achieving%20superior%20rendering%20fidelity%20while%0Asignificantly%20reducing%20Depth%20MSE.%20Code%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoving%2520Light%2520Adaptive%2520Colonoscopy%2520Reconstruction%2520via%250A%2520%2520Illumination-Attenuation-Aware%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DHao%2520Wang%2520and%2520Ying%2520Zhou%2520and%2520Haoyu%2520Zhao%2520and%2520Rui%2520Wang%2520and%2520Qiang%2520Hu%2520and%2520Xing%2520Zhang%2520and%2520Qiang%2520Li%2520and%2520Zhiwei%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520pivotal%2520technique%2520for%2520real-time%250Aview%2520synthesis%2520in%2520colonoscopy%252C%2520enabling%2520critical%2520applications%2520such%2520as%2520virtual%250Acolonoscopy%2520and%2520lesion%2520tracking.%2520However%252C%2520the%2520vanilla%25203DGS%2520assumes%2520static%250Aillumination%2520and%2520that%2520observed%2520appearance%2520depends%2520solely%2520on%2520viewing%2520angle%252C%250Awhich%2520causes%2520incompatibility%2520with%2520the%2520photometric%2520variations%2520in%2520colonoscopic%250Ascenes%2520induced%2520by%2520dynamic%2520light%2520source/camera.%2520This%2520mismatch%2520forces%2520most%25203DGS%250Amethods%2520to%2520introduce%2520structure-violating%2520vaporous%2520Gaussian%2520blobs%2520between%2520the%250Acamera%2520and%2520tissues%2520to%2520compensate%2520for%2520illumination%2520attenuation%252C%2520ultimately%250Adegrading%2520the%2520quality%2520of%25203D%2520reconstructions.%2520Previous%2520works%2520only%2520consider%2520the%250Aillumination%2520attenuation%2520caused%2520by%2520light%2520distance%252C%2520ignoring%2520the%2520physical%250Acharacters%2520of%2520light%2520source%2520and%2520camera.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ColIAGS%252C%2520an%250Aimproved%25203DGS%2520framework%2520tailored%2520for%2520colonoscopy.%2520To%2520mimic%2520realistic%2520appearance%250Aunder%2520varying%2520illumination%252C%2520we%2520introduce%2520an%2520Improved%2520Appearance%2520Modeling%2520with%250Atwo%2520types%2520of%2520illumination%2520attenuation%2520factors%252C%2520which%2520enables%2520Gaussians%2520to%2520adapt%250Ato%2520photometric%2520variations%2520while%2520preserving%2520geometry%2520accuracy.%2520To%2520ensure%2520the%250Ageometry%2520approximation%2520condition%2520of%2520appearance%2520modeling%252C%2520we%2520propose%2520an%2520Improved%250AGeometry%2520Modeling%2520using%2520high-dimensional%2520view%2520embedding%2520to%2520enhance%2520Gaussian%250Ageometry%2520attribute%2520prediction.%2520Furthermore%252C%2520another%2520cosine%2520embedding%2520input%2520is%250Aleveraged%2520to%2520generate%2520illumination%2520attenuation%2520solutions%2520in%2520an%2520implicit%2520manner.%250AComprehensive%2520experimental%2520results%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520our%250Aproposed%2520ColIAGS%2520achieves%2520the%2520dual%2520capabilities%2520of%2520novel%2520view%2520synthesis%2520and%250Aaccurate%2520geometric%2520reconstruction.%2520It%2520notably%2520outperforms%2520other%250Astate-of-the-art%2520methods%2520by%2520achieving%2520superior%2520rendering%2520fidelity%2520while%250Asignificantly%2520reducing%2520Depth%2520MSE.%2520Code%2520will%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moving%20Light%20Adaptive%20Colonoscopy%20Reconstruction%20via%0A%20%20Illumination-Attenuation-Aware%203D%20Gaussian%20Splatting&entry.906535625=Hao%20Wang%20and%20Ying%20Zhou%20and%20Haoyu%20Zhao%20and%20Rui%20Wang%20and%20Qiang%20Hu%20and%20Xing%20Zhang%20and%20Qiang%20Li%20and%20Zhiwei%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20pivotal%20technique%20for%20real-time%0Aview%20synthesis%20in%20colonoscopy%2C%20enabling%20critical%20applications%20such%20as%20virtual%0Acolonoscopy%20and%20lesion%20tracking.%20However%2C%20the%20vanilla%203DGS%20assumes%20static%0Aillumination%20and%20that%20observed%20appearance%20depends%20solely%20on%20viewing%20angle%2C%0Awhich%20causes%20incompatibility%20with%20the%20photometric%20variations%20in%20colonoscopic%0Ascenes%20induced%20by%20dynamic%20light%20source/camera.%20This%20mismatch%20forces%20most%203DGS%0Amethods%20to%20introduce%20structure-violating%20vaporous%20Gaussian%20blobs%20between%20the%0Acamera%20and%20tissues%20to%20compensate%20for%20illumination%20attenuation%2C%20ultimately%0Adegrading%20the%20quality%20of%203D%20reconstructions.%20Previous%20works%20only%20consider%20the%0Aillumination%20attenuation%20caused%20by%20light%20distance%2C%20ignoring%20the%20physical%0Acharacters%20of%20light%20source%20and%20camera.%20In%20this%20paper%2C%20we%20propose%20ColIAGS%2C%20an%0Aimproved%203DGS%20framework%20tailored%20for%20colonoscopy.%20To%20mimic%20realistic%20appearance%0Aunder%20varying%20illumination%2C%20we%20introduce%20an%20Improved%20Appearance%20Modeling%20with%0Atwo%20types%20of%20illumination%20attenuation%20factors%2C%20which%20enables%20Gaussians%20to%20adapt%0Ato%20photometric%20variations%20while%20preserving%20geometry%20accuracy.%20To%20ensure%20the%0Ageometry%20approximation%20condition%20of%20appearance%20modeling%2C%20we%20propose%20an%20Improved%0AGeometry%20Modeling%20using%20high-dimensional%20view%20embedding%20to%20enhance%20Gaussian%0Ageometry%20attribute%20prediction.%20Furthermore%2C%20another%20cosine%20embedding%20input%20is%0Aleveraged%20to%20generate%20illumination%20attenuation%20solutions%20in%20an%20implicit%20manner.%0AComprehensive%20experimental%20results%20on%20standard%20benchmarks%20demonstrate%20that%20our%0Aproposed%20ColIAGS%20achieves%20the%20dual%20capabilities%20of%20novel%20view%20synthesis%20and%0Aaccurate%20geometric%20reconstruction.%20It%20notably%20outperforms%20other%0Astate-of-the-art%20methods%20by%20achieving%20superior%20rendering%20fidelity%20while%0Asignificantly%20reducing%20Depth%20MSE.%20Code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18739v1&entry.124074799=Read"},
{"title": "PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward\n  Planar Splatting", "author": "Changkun Liu and Bin Tan and Zeran Ke and Shangzhan Zhang and Jiachen Liu and Ming Qian and Nan Xue and Yujun Shen and Tristan Braud", "abstract": "  This paper addresses metric 3D reconstruction of indoor scenes by exploiting\ntheir inherent geometric regularities with compact representations. Using\nplanar 3D primitives - a well-suited representation for man-made environments -\nwe introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction\nfrom unposed two-view images. Our approach employs Vision Transformers to\nextract a set of sparse planar primitives, estimate relative camera poses, and\nsupervise geometry learning via planar splatting, where gradients are\npropagated through high-resolution rendered depth and normal maps of\nprimitives. Unlike prior feedforward methods that require 3D plane annotations\nduring training, PLANA3R learns planar 3D structures without explicit plane\nsupervision, enabling scalable training on large-scale stereo datasets using\nonly depth and normal annotations. We validate PLANA3R on multiple indoor-scene\ndatasets with metric supervision and demonstrate strong generalization to\nout-of-domain indoor environments across diverse tasks under metric evaluation\nprotocols, including 3D surface reconstruction, depth estimation, and relative\npose estimation. Furthermore, by formulating with planar 3D representation, our\nmethod emerges with the ability for accurate plane segmentation. The project\npage is available at https://lck666666.github.io/plana3r\n", "link": "http://arxiv.org/abs/2510.18714v1", "date": "2025-10-21", "relevancy": 3.1765, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6527}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6439}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLANA3R%3A%20Zero-shot%20Metric%20Planar%203D%20Reconstruction%20via%20Feed-Forward%0A%20%20Planar%20Splatting&body=Title%3A%20PLANA3R%3A%20Zero-shot%20Metric%20Planar%203D%20Reconstruction%20via%20Feed-Forward%0A%20%20Planar%20Splatting%0AAuthor%3A%20Changkun%20Liu%20and%20Bin%20Tan%20and%20Zeran%20Ke%20and%20Shangzhan%20Zhang%20and%20Jiachen%20Liu%20and%20Ming%20Qian%20and%20Nan%20Xue%20and%20Yujun%20Shen%20and%20Tristan%20Braud%0AAbstract%3A%20%20%20This%20paper%20addresses%20metric%203D%20reconstruction%20of%20indoor%20scenes%20by%20exploiting%0Atheir%20inherent%20geometric%20regularities%20with%20compact%20representations.%20Using%0Aplanar%203D%20primitives%20-%20a%20well-suited%20representation%20for%20man-made%20environments%20-%0Awe%20introduce%20PLANA3R%2C%20a%20pose-free%20framework%20for%20metric%20Planar%203D%20Reconstruction%0Afrom%20unposed%20two-view%20images.%20Our%20approach%20employs%20Vision%20Transformers%20to%0Aextract%20a%20set%20of%20sparse%20planar%20primitives%2C%20estimate%20relative%20camera%20poses%2C%20and%0Asupervise%20geometry%20learning%20via%20planar%20splatting%2C%20where%20gradients%20are%0Apropagated%20through%20high-resolution%20rendered%20depth%20and%20normal%20maps%20of%0Aprimitives.%20Unlike%20prior%20feedforward%20methods%20that%20require%203D%20plane%20annotations%0Aduring%20training%2C%20PLANA3R%20learns%20planar%203D%20structures%20without%20explicit%20plane%0Asupervision%2C%20enabling%20scalable%20training%20on%20large-scale%20stereo%20datasets%20using%0Aonly%20depth%20and%20normal%20annotations.%20We%20validate%20PLANA3R%20on%20multiple%20indoor-scene%0Adatasets%20with%20metric%20supervision%20and%20demonstrate%20strong%20generalization%20to%0Aout-of-domain%20indoor%20environments%20across%20diverse%20tasks%20under%20metric%20evaluation%0Aprotocols%2C%20including%203D%20surface%20reconstruction%2C%20depth%20estimation%2C%20and%20relative%0Apose%20estimation.%20Furthermore%2C%20by%20formulating%20with%20planar%203D%20representation%2C%20our%0Amethod%20emerges%20with%20the%20ability%20for%20accurate%20plane%20segmentation.%20The%20project%0Apage%20is%20available%20at%20https%3A//lck666666.github.io/plana3r%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLANA3R%253A%2520Zero-shot%2520Metric%2520Planar%25203D%2520Reconstruction%2520via%2520Feed-Forward%250A%2520%2520Planar%2520Splatting%26entry.906535625%3DChangkun%2520Liu%2520and%2520Bin%2520Tan%2520and%2520Zeran%2520Ke%2520and%2520Shangzhan%2520Zhang%2520and%2520Jiachen%2520Liu%2520and%2520Ming%2520Qian%2520and%2520Nan%2520Xue%2520and%2520Yujun%2520Shen%2520and%2520Tristan%2520Braud%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520metric%25203D%2520reconstruction%2520of%2520indoor%2520scenes%2520by%2520exploiting%250Atheir%2520inherent%2520geometric%2520regularities%2520with%2520compact%2520representations.%2520Using%250Aplanar%25203D%2520primitives%2520-%2520a%2520well-suited%2520representation%2520for%2520man-made%2520environments%2520-%250Awe%2520introduce%2520PLANA3R%252C%2520a%2520pose-free%2520framework%2520for%2520metric%2520Planar%25203D%2520Reconstruction%250Afrom%2520unposed%2520two-view%2520images.%2520Our%2520approach%2520employs%2520Vision%2520Transformers%2520to%250Aextract%2520a%2520set%2520of%2520sparse%2520planar%2520primitives%252C%2520estimate%2520relative%2520camera%2520poses%252C%2520and%250Asupervise%2520geometry%2520learning%2520via%2520planar%2520splatting%252C%2520where%2520gradients%2520are%250Apropagated%2520through%2520high-resolution%2520rendered%2520depth%2520and%2520normal%2520maps%2520of%250Aprimitives.%2520Unlike%2520prior%2520feedforward%2520methods%2520that%2520require%25203D%2520plane%2520annotations%250Aduring%2520training%252C%2520PLANA3R%2520learns%2520planar%25203D%2520structures%2520without%2520explicit%2520plane%250Asupervision%252C%2520enabling%2520scalable%2520training%2520on%2520large-scale%2520stereo%2520datasets%2520using%250Aonly%2520depth%2520and%2520normal%2520annotations.%2520We%2520validate%2520PLANA3R%2520on%2520multiple%2520indoor-scene%250Adatasets%2520with%2520metric%2520supervision%2520and%2520demonstrate%2520strong%2520generalization%2520to%250Aout-of-domain%2520indoor%2520environments%2520across%2520diverse%2520tasks%2520under%2520metric%2520evaluation%250Aprotocols%252C%2520including%25203D%2520surface%2520reconstruction%252C%2520depth%2520estimation%252C%2520and%2520relative%250Apose%2520estimation.%2520Furthermore%252C%2520by%2520formulating%2520with%2520planar%25203D%2520representation%252C%2520our%250Amethod%2520emerges%2520with%2520the%2520ability%2520for%2520accurate%2520plane%2520segmentation.%2520The%2520project%250Apage%2520is%2520available%2520at%2520https%253A//lck666666.github.io/plana3r%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLANA3R%3A%20Zero-shot%20Metric%20Planar%203D%20Reconstruction%20via%20Feed-Forward%0A%20%20Planar%20Splatting&entry.906535625=Changkun%20Liu%20and%20Bin%20Tan%20and%20Zeran%20Ke%20and%20Shangzhan%20Zhang%20and%20Jiachen%20Liu%20and%20Ming%20Qian%20and%20Nan%20Xue%20and%20Yujun%20Shen%20and%20Tristan%20Braud&entry.1292438233=%20%20This%20paper%20addresses%20metric%203D%20reconstruction%20of%20indoor%20scenes%20by%20exploiting%0Atheir%20inherent%20geometric%20regularities%20with%20compact%20representations.%20Using%0Aplanar%203D%20primitives%20-%20a%20well-suited%20representation%20for%20man-made%20environments%20-%0Awe%20introduce%20PLANA3R%2C%20a%20pose-free%20framework%20for%20metric%20Planar%203D%20Reconstruction%0Afrom%20unposed%20two-view%20images.%20Our%20approach%20employs%20Vision%20Transformers%20to%0Aextract%20a%20set%20of%20sparse%20planar%20primitives%2C%20estimate%20relative%20camera%20poses%2C%20and%0Asupervise%20geometry%20learning%20via%20planar%20splatting%2C%20where%20gradients%20are%0Apropagated%20through%20high-resolution%20rendered%20depth%20and%20normal%20maps%20of%0Aprimitives.%20Unlike%20prior%20feedforward%20methods%20that%20require%203D%20plane%20annotations%0Aduring%20training%2C%20PLANA3R%20learns%20planar%203D%20structures%20without%20explicit%20plane%0Asupervision%2C%20enabling%20scalable%20training%20on%20large-scale%20stereo%20datasets%20using%0Aonly%20depth%20and%20normal%20annotations.%20We%20validate%20PLANA3R%20on%20multiple%20indoor-scene%0Adatasets%20with%20metric%20supervision%20and%20demonstrate%20strong%20generalization%20to%0Aout-of-domain%20indoor%20environments%20across%20diverse%20tasks%20under%20metric%20evaluation%0Aprotocols%2C%20including%203D%20surface%20reconstruction%2C%20depth%20estimation%2C%20and%20relative%0Apose%20estimation.%20Furthermore%2C%20by%20formulating%20with%20planar%203D%20representation%2C%20our%0Amethod%20emerges%20with%20the%20ability%20for%20accurate%20plane%20segmentation.%20The%20project%0Apage%20is%20available%20at%20https%3A//lck666666.github.io/plana3r%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18714v1&entry.124074799=Read"},
{"title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention", "author": "Teng Hu and Jiangning Zhang and Zihan Su and Ran Yi", "abstract": "  Recent advances in video generation have made it possible to produce visually\ncompelling videos, with wide-ranging applications in content creation,\nentertainment, and virtual reality. However, most existing diffusion\ntransformer based video generation models are limited to low-resolution outputs\n(<=720P) due to the quadratic computational complexity of the attention\nmechanism with respect to the output width and height. This computational\nbottleneck makes native high-resolution video generation (1080P/2K/4K)\nimpractical for both training and inference. To address this challenge, we\npresent UltraGen, a novel video generation framework that enables i) efficient\nand ii) end-to-end native high-resolution video synthesis. Specifically,\nUltraGen features a hierarchical dual-branch attention architecture based on\nglobal-local attention decomposition, which decouples full attention into a\nlocal attention branch for high-fidelity regional content and a global\nattention branch for overall semantic consistency. We further propose a\nspatially compressed global modeling strategy to efficiently learn global\ndependencies, and a hierarchical cross-window local attention mechanism to\nreduce computational costs while enhancing information flow across different\nlocal windows. Extensive experiments demonstrate that UltraGen can effectively\nscale pre-trained low-resolution video models to 1080P and even 4K resolution\nfor the first time, outperforming existing state-of-the-art methods and\nsuper-resolution based two-stage pipelines in both qualitative and quantitative\nevaluations.\n", "link": "http://arxiv.org/abs/2510.18775v1", "date": "2025-10-21", "relevancy": 3.1604, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.653}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6345}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraGen%3A%20High-Resolution%20Video%20Generation%20with%20Hierarchical%20Attention&body=Title%3A%20UltraGen%3A%20High-Resolution%20Video%20Generation%20with%20Hierarchical%20Attention%0AAuthor%3A%20Teng%20Hu%20and%20Jiangning%20Zhang%20and%20Zihan%20Su%20and%20Ran%20Yi%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20generation%20have%20made%20it%20possible%20to%20produce%20visually%0Acompelling%20videos%2C%20with%20wide-ranging%20applications%20in%20content%20creation%2C%0Aentertainment%2C%20and%20virtual%20reality.%20However%2C%20most%20existing%20diffusion%0Atransformer%20based%20video%20generation%20models%20are%20limited%20to%20low-resolution%20outputs%0A%28%3C%3D720P%29%20due%20to%20the%20quadratic%20computational%20complexity%20of%20the%20attention%0Amechanism%20with%20respect%20to%20the%20output%20width%20and%20height.%20This%20computational%0Abottleneck%20makes%20native%20high-resolution%20video%20generation%20%281080P/2K/4K%29%0Aimpractical%20for%20both%20training%20and%20inference.%20To%20address%20this%20challenge%2C%20we%0Apresent%20UltraGen%2C%20a%20novel%20video%20generation%20framework%20that%20enables%20i%29%20efficient%0Aand%20ii%29%20end-to-end%20native%20high-resolution%20video%20synthesis.%20Specifically%2C%0AUltraGen%20features%20a%20hierarchical%20dual-branch%20attention%20architecture%20based%20on%0Aglobal-local%20attention%20decomposition%2C%20which%20decouples%20full%20attention%20into%20a%0Alocal%20attention%20branch%20for%20high-fidelity%20regional%20content%20and%20a%20global%0Aattention%20branch%20for%20overall%20semantic%20consistency.%20We%20further%20propose%20a%0Aspatially%20compressed%20global%20modeling%20strategy%20to%20efficiently%20learn%20global%0Adependencies%2C%20and%20a%20hierarchical%20cross-window%20local%20attention%20mechanism%20to%0Areduce%20computational%20costs%20while%20enhancing%20information%20flow%20across%20different%0Alocal%20windows.%20Extensive%20experiments%20demonstrate%20that%20UltraGen%20can%20effectively%0Ascale%20pre-trained%20low-resolution%20video%20models%20to%201080P%20and%20even%204K%20resolution%0Afor%20the%20first%20time%2C%20outperforming%20existing%20state-of-the-art%20methods%20and%0Asuper-resolution%20based%20two-stage%20pipelines%20in%20both%20qualitative%20and%20quantitative%0Aevaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraGen%253A%2520High-Resolution%2520Video%2520Generation%2520with%2520Hierarchical%2520Attention%26entry.906535625%3DTeng%2520Hu%2520and%2520Jiangning%2520Zhang%2520and%2520Zihan%2520Su%2520and%2520Ran%2520Yi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520generation%2520have%2520made%2520it%2520possible%2520to%2520produce%2520visually%250Acompelling%2520videos%252C%2520with%2520wide-ranging%2520applications%2520in%2520content%2520creation%252C%250Aentertainment%252C%2520and%2520virtual%2520reality.%2520However%252C%2520most%2520existing%2520diffusion%250Atransformer%2520based%2520video%2520generation%2520models%2520are%2520limited%2520to%2520low-resolution%2520outputs%250A%2528%253C%253D720P%2529%2520due%2520to%2520the%2520quadratic%2520computational%2520complexity%2520of%2520the%2520attention%250Amechanism%2520with%2520respect%2520to%2520the%2520output%2520width%2520and%2520height.%2520This%2520computational%250Abottleneck%2520makes%2520native%2520high-resolution%2520video%2520generation%2520%25281080P/2K/4K%2529%250Aimpractical%2520for%2520both%2520training%2520and%2520inference.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apresent%2520UltraGen%252C%2520a%2520novel%2520video%2520generation%2520framework%2520that%2520enables%2520i%2529%2520efficient%250Aand%2520ii%2529%2520end-to-end%2520native%2520high-resolution%2520video%2520synthesis.%2520Specifically%252C%250AUltraGen%2520features%2520a%2520hierarchical%2520dual-branch%2520attention%2520architecture%2520based%2520on%250Aglobal-local%2520attention%2520decomposition%252C%2520which%2520decouples%2520full%2520attention%2520into%2520a%250Alocal%2520attention%2520branch%2520for%2520high-fidelity%2520regional%2520content%2520and%2520a%2520global%250Aattention%2520branch%2520for%2520overall%2520semantic%2520consistency.%2520We%2520further%2520propose%2520a%250Aspatially%2520compressed%2520global%2520modeling%2520strategy%2520to%2520efficiently%2520learn%2520global%250Adependencies%252C%2520and%2520a%2520hierarchical%2520cross-window%2520local%2520attention%2520mechanism%2520to%250Areduce%2520computational%2520costs%2520while%2520enhancing%2520information%2520flow%2520across%2520different%250Alocal%2520windows.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UltraGen%2520can%2520effectively%250Ascale%2520pre-trained%2520low-resolution%2520video%2520models%2520to%25201080P%2520and%2520even%25204K%2520resolution%250Afor%2520the%2520first%2520time%252C%2520outperforming%2520existing%2520state-of-the-art%2520methods%2520and%250Asuper-resolution%2520based%2520two-stage%2520pipelines%2520in%2520both%2520qualitative%2520and%2520quantitative%250Aevaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraGen%3A%20High-Resolution%20Video%20Generation%20with%20Hierarchical%20Attention&entry.906535625=Teng%20Hu%20and%20Jiangning%20Zhang%20and%20Zihan%20Su%20and%20Ran%20Yi&entry.1292438233=%20%20Recent%20advances%20in%20video%20generation%20have%20made%20it%20possible%20to%20produce%20visually%0Acompelling%20videos%2C%20with%20wide-ranging%20applications%20in%20content%20creation%2C%0Aentertainment%2C%20and%20virtual%20reality.%20However%2C%20most%20existing%20diffusion%0Atransformer%20based%20video%20generation%20models%20are%20limited%20to%20low-resolution%20outputs%0A%28%3C%3D720P%29%20due%20to%20the%20quadratic%20computational%20complexity%20of%20the%20attention%0Amechanism%20with%20respect%20to%20the%20output%20width%20and%20height.%20This%20computational%0Abottleneck%20makes%20native%20high-resolution%20video%20generation%20%281080P/2K/4K%29%0Aimpractical%20for%20both%20training%20and%20inference.%20To%20address%20this%20challenge%2C%20we%0Apresent%20UltraGen%2C%20a%20novel%20video%20generation%20framework%20that%20enables%20i%29%20efficient%0Aand%20ii%29%20end-to-end%20native%20high-resolution%20video%20synthesis.%20Specifically%2C%0AUltraGen%20features%20a%20hierarchical%20dual-branch%20attention%20architecture%20based%20on%0Aglobal-local%20attention%20decomposition%2C%20which%20decouples%20full%20attention%20into%20a%0Alocal%20attention%20branch%20for%20high-fidelity%20regional%20content%20and%20a%20global%0Aattention%20branch%20for%20overall%20semantic%20consistency.%20We%20further%20propose%20a%0Aspatially%20compressed%20global%20modeling%20strategy%20to%20efficiently%20learn%20global%0Adependencies%2C%20and%20a%20hierarchical%20cross-window%20local%20attention%20mechanism%20to%0Areduce%20computational%20costs%20while%20enhancing%20information%20flow%20across%20different%0Alocal%20windows.%20Extensive%20experiments%20demonstrate%20that%20UltraGen%20can%20effectively%0Ascale%20pre-trained%20low-resolution%20video%20models%20to%201080P%20and%20even%204K%20resolution%0Afor%20the%20first%20time%2C%20outperforming%20existing%20state-of-the-art%20methods%20and%0Asuper-resolution%20based%20two-stage%20pipelines%20in%20both%20qualitative%20and%20quantitative%0Aevaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18775v1&entry.124074799=Read"},
{"title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from\n  Limited Views", "author": "Zhangquan Chen and Manyuan Zhang and Xinlei Yu and Xufang Luo and Mingze Sun and Zihao Pan and Yan Feng and Peng Pei and Xunliang Cai and Ruqi Huang", "abstract": "  Though recent advances in vision-language models (VLMs) have achieved\nremarkable progress across a wide range of multimodal tasks, understanding 3D\nspatial relationships from limited views remains a significant challenge.\nPrevious reasoning methods typically rely on pure text (e.g., topological\ncognitive maps) or on 2D visual cues. However, their limited representational\ncapacity hinders performance in specific tasks that require 3D spatial\nimagination. To address this limitation, we propose 3DThinker, a framework that\ncan effectively exploits the rich geometric information embedded within images\nwhile reasoning, like humans do. Our framework is the first to enable 3D\nmentaling during reasoning without any 3D prior input, and it does not rely on\nexplicitly labeled 3D data for training. Specifically, our training consists of\ntwo stages. First, we perform supervised training to align the 3D latent\ngenerated by VLM while reasoning with that of a 3D foundation model (e.g.,\nVGGT). Then, we optimize the entire reasoning trajectory solely based on\noutcome signals, thereby refining the underlying 3D mentaling. Extensive\nexperiments across multiple benchmarks show that 3DThinker consistently\noutperforms strong baselines and offers a new perspective toward unifying 3D\nrepresentations into multimodal reasoning. Our code will be available at\nhttps://github.com/zhangquanchen/3DThinker.\n", "link": "http://arxiv.org/abs/2510.18632v1", "date": "2025-10-21", "relevancy": 3.1445, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6352}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20with%203D%3A%20Geometric%20Imagination%20Grounded%20Spatial%20Reasoning%20from%0A%20%20Limited%20Views&body=Title%3A%20Think%20with%203D%3A%20Geometric%20Imagination%20Grounded%20Spatial%20Reasoning%20from%0A%20%20Limited%20Views%0AAuthor%3A%20Zhangquan%20Chen%20and%20Manyuan%20Zhang%20and%20Xinlei%20Yu%20and%20Xufang%20Luo%20and%20Mingze%20Sun%20and%20Zihao%20Pan%20and%20Yan%20Feng%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Ruqi%20Huang%0AAbstract%3A%20%20%20Though%20recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20achieved%0Aremarkable%20progress%20across%20a%20wide%20range%20of%20multimodal%20tasks%2C%20understanding%203D%0Aspatial%20relationships%20from%20limited%20views%20remains%20a%20significant%20challenge.%0APrevious%20reasoning%20methods%20typically%20rely%20on%20pure%20text%20%28e.g.%2C%20topological%0Acognitive%20maps%29%20or%20on%202D%20visual%20cues.%20However%2C%20their%20limited%20representational%0Acapacity%20hinders%20performance%20in%20specific%20tasks%20that%20require%203D%20spatial%0Aimagination.%20To%20address%20this%20limitation%2C%20we%20propose%203DThinker%2C%20a%20framework%20that%0Acan%20effectively%20exploits%20the%20rich%20geometric%20information%20embedded%20within%20images%0Awhile%20reasoning%2C%20like%20humans%20do.%20Our%20framework%20is%20the%20first%20to%20enable%203D%0Amentaling%20during%20reasoning%20without%20any%203D%20prior%20input%2C%20and%20it%20does%20not%20rely%20on%0Aexplicitly%20labeled%203D%20data%20for%20training.%20Specifically%2C%20our%20training%20consists%20of%0Atwo%20stages.%20First%2C%20we%20perform%20supervised%20training%20to%20align%20the%203D%20latent%0Agenerated%20by%20VLM%20while%20reasoning%20with%20that%20of%20a%203D%20foundation%20model%20%28e.g.%2C%0AVGGT%29.%20Then%2C%20we%20optimize%20the%20entire%20reasoning%20trajectory%20solely%20based%20on%0Aoutcome%20signals%2C%20thereby%20refining%20the%20underlying%203D%20mentaling.%20Extensive%0Aexperiments%20across%20multiple%20benchmarks%20show%20that%203DThinker%20consistently%0Aoutperforms%20strong%20baselines%20and%20offers%20a%20new%20perspective%20toward%20unifying%203D%0Arepresentations%20into%20multimodal%20reasoning.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/zhangquanchen/3DThinker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520with%25203D%253A%2520Geometric%2520Imagination%2520Grounded%2520Spatial%2520Reasoning%2520from%250A%2520%2520Limited%2520Views%26entry.906535625%3DZhangquan%2520Chen%2520and%2520Manyuan%2520Zhang%2520and%2520Xinlei%2520Yu%2520and%2520Xufang%2520Luo%2520and%2520Mingze%2520Sun%2520and%2520Zihao%2520Pan%2520and%2520Yan%2520Feng%2520and%2520Peng%2520Pei%2520and%2520Xunliang%2520Cai%2520and%2520Ruqi%2520Huang%26entry.1292438233%3D%2520%2520Though%2520recent%2520advances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520achieved%250Aremarkable%2520progress%2520across%2520a%2520wide%2520range%2520of%2520multimodal%2520tasks%252C%2520understanding%25203D%250Aspatial%2520relationships%2520from%2520limited%2520views%2520remains%2520a%2520significant%2520challenge.%250APrevious%2520reasoning%2520methods%2520typically%2520rely%2520on%2520pure%2520text%2520%2528e.g.%252C%2520topological%250Acognitive%2520maps%2529%2520or%2520on%25202D%2520visual%2520cues.%2520However%252C%2520their%2520limited%2520representational%250Acapacity%2520hinders%2520performance%2520in%2520specific%2520tasks%2520that%2520require%25203D%2520spatial%250Aimagination.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%25203DThinker%252C%2520a%2520framework%2520that%250Acan%2520effectively%2520exploits%2520the%2520rich%2520geometric%2520information%2520embedded%2520within%2520images%250Awhile%2520reasoning%252C%2520like%2520humans%2520do.%2520Our%2520framework%2520is%2520the%2520first%2520to%2520enable%25203D%250Amentaling%2520during%2520reasoning%2520without%2520any%25203D%2520prior%2520input%252C%2520and%2520it%2520does%2520not%2520rely%2520on%250Aexplicitly%2520labeled%25203D%2520data%2520for%2520training.%2520Specifically%252C%2520our%2520training%2520consists%2520of%250Atwo%2520stages.%2520First%252C%2520we%2520perform%2520supervised%2520training%2520to%2520align%2520the%25203D%2520latent%250Agenerated%2520by%2520VLM%2520while%2520reasoning%2520with%2520that%2520of%2520a%25203D%2520foundation%2520model%2520%2528e.g.%252C%250AVGGT%2529.%2520Then%252C%2520we%2520optimize%2520the%2520entire%2520reasoning%2520trajectory%2520solely%2520based%2520on%250Aoutcome%2520signals%252C%2520thereby%2520refining%2520the%2520underlying%25203D%2520mentaling.%2520Extensive%250Aexperiments%2520across%2520multiple%2520benchmarks%2520show%2520that%25203DThinker%2520consistently%250Aoutperforms%2520strong%2520baselines%2520and%2520offers%2520a%2520new%2520perspective%2520toward%2520unifying%25203D%250Arepresentations%2520into%2520multimodal%2520reasoning.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/zhangquanchen/3DThinker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20with%203D%3A%20Geometric%20Imagination%20Grounded%20Spatial%20Reasoning%20from%0A%20%20Limited%20Views&entry.906535625=Zhangquan%20Chen%20and%20Manyuan%20Zhang%20and%20Xinlei%20Yu%20and%20Xufang%20Luo%20and%20Mingze%20Sun%20and%20Zihao%20Pan%20and%20Yan%20Feng%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Ruqi%20Huang&entry.1292438233=%20%20Though%20recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20achieved%0Aremarkable%20progress%20across%20a%20wide%20range%20of%20multimodal%20tasks%2C%20understanding%203D%0Aspatial%20relationships%20from%20limited%20views%20remains%20a%20significant%20challenge.%0APrevious%20reasoning%20methods%20typically%20rely%20on%20pure%20text%20%28e.g.%2C%20topological%0Acognitive%20maps%29%20or%20on%202D%20visual%20cues.%20However%2C%20their%20limited%20representational%0Acapacity%20hinders%20performance%20in%20specific%20tasks%20that%20require%203D%20spatial%0Aimagination.%20To%20address%20this%20limitation%2C%20we%20propose%203DThinker%2C%20a%20framework%20that%0Acan%20effectively%20exploits%20the%20rich%20geometric%20information%20embedded%20within%20images%0Awhile%20reasoning%2C%20like%20humans%20do.%20Our%20framework%20is%20the%20first%20to%20enable%203D%0Amentaling%20during%20reasoning%20without%20any%203D%20prior%20input%2C%20and%20it%20does%20not%20rely%20on%0Aexplicitly%20labeled%203D%20data%20for%20training.%20Specifically%2C%20our%20training%20consists%20of%0Atwo%20stages.%20First%2C%20we%20perform%20supervised%20training%20to%20align%20the%203D%20latent%0Agenerated%20by%20VLM%20while%20reasoning%20with%20that%20of%20a%203D%20foundation%20model%20%28e.g.%2C%0AVGGT%29.%20Then%2C%20we%20optimize%20the%20entire%20reasoning%20trajectory%20solely%20based%20on%0Aoutcome%20signals%2C%20thereby%20refining%20the%20underlying%203D%20mentaling.%20Extensive%0Aexperiments%20across%20multiple%20benchmarks%20show%20that%203DThinker%20consistently%0Aoutperforms%20strong%20baselines%20and%20offers%20a%20new%20perspective%20toward%20unifying%203D%0Arepresentations%20into%20multimodal%20reasoning.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/zhangquanchen/3DThinker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18632v1&entry.124074799=Read"},
{"title": "Exploring a Unified Vision-Centric Contrastive Alternatives on\n  Multi-Modal Web Documents", "author": "Yiqi Lin and Alex Jinpeng Wang and Linjie Li and Zhengyuan Yang and Mike Zheng Shou", "abstract": "  Contrastive vision-language models such as CLIP have demonstrated strong\nperformance across a wide range of multimodal tasks by learning from aligned\nimage-text pairs. However, their ability to handle complex, real-world web\ndocuments remains limited, particularly in scenarios where text and images are\ninterleaved, loosely aligned, or embedded in visual form. To address these\nchallenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified\nframework that models text, images, and their combinations using a single\nvision transformer. VC2L operates entirely in pixel space by rendering all\ninputs, whether textual, visual, or combined, as images, thus eliminating the\nneed for OCR, text tokenization, or modality fusion strategy. To capture\ncomplex cross-modal relationships in multimodal web documents, VC2L employs a\nsnippet-level contrastive learning objective that aligns consecutive multimodal\nsegments, leveraging the inherent coherence of documents without requiring\nexplicitly paired image-text data. To assess the effectiveness of this\napproach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,\ndesigned to evaluate cross-modal retrieval, fine-grained sequential\nunderstanding, and generalization to unseen data, respectively. Empirical\nresults show that VC2L achieves competitive or superior performance compared to\nCLIP-style models on both the proposed benchmarks and established datasets such\nas M-BEIR and MTEB. These findings underscore the potential of multimodal web\ndata as a valuable training resource for contrastive learning and illustrate\nthe scalability of a unified, vision-centric approach for multimodal\nrepresentation learning. Code and models are available at:\nhttps://github.com/showlab/VC2L.\n", "link": "http://arxiv.org/abs/2510.18703v1", "date": "2025-10-21", "relevancy": 3.115, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6233}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20a%20Unified%20Vision-Centric%20Contrastive%20Alternatives%20on%0A%20%20Multi-Modal%20Web%20Documents&body=Title%3A%20Exploring%20a%20Unified%20Vision-Centric%20Contrastive%20Alternatives%20on%0A%20%20Multi-Modal%20Web%20Documents%0AAuthor%3A%20Yiqi%20Lin%20and%20Alex%20Jinpeng%20Wang%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Contrastive%20vision-language%20models%20such%20as%20CLIP%20have%20demonstrated%20strong%0Aperformance%20across%20a%20wide%20range%20of%20multimodal%20tasks%20by%20learning%20from%20aligned%0Aimage-text%20pairs.%20However%2C%20their%20ability%20to%20handle%20complex%2C%20real-world%20web%0Adocuments%20remains%20limited%2C%20particularly%20in%20scenarios%20where%20text%20and%20images%20are%0Ainterleaved%2C%20loosely%20aligned%2C%20or%20embedded%20in%20visual%20form.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Vision-Centric%20Contrastive%20Learning%20%28VC2L%29%2C%20a%20unified%0Aframework%20that%20models%20text%2C%20images%2C%20and%20their%20combinations%20using%20a%20single%0Avision%20transformer.%20VC2L%20operates%20entirely%20in%20pixel%20space%20by%20rendering%20all%0Ainputs%2C%20whether%20textual%2C%20visual%2C%20or%20combined%2C%20as%20images%2C%20thus%20eliminating%20the%0Aneed%20for%20OCR%2C%20text%20tokenization%2C%20or%20modality%20fusion%20strategy.%20To%20capture%0Acomplex%20cross-modal%20relationships%20in%20multimodal%20web%20documents%2C%20VC2L%20employs%20a%0Asnippet-level%20contrastive%20learning%20objective%20that%20aligns%20consecutive%20multimodal%0Asegments%2C%20leveraging%20the%20inherent%20coherence%20of%20documents%20without%20requiring%0Aexplicitly%20paired%20image-text%20data.%20To%20assess%20the%20effectiveness%20of%20this%0Aapproach%2C%20we%20introduce%20three%20retrieval%20benchmarks%2C%20AnyCIR%2C%20SeqCIR%2C%20and%20CSR%2C%0Adesigned%20to%20evaluate%20cross-modal%20retrieval%2C%20fine-grained%20sequential%0Aunderstanding%2C%20and%20generalization%20to%20unseen%20data%2C%20respectively.%20Empirical%0Aresults%20show%20that%20VC2L%20achieves%20competitive%20or%20superior%20performance%20compared%20to%0ACLIP-style%20models%20on%20both%20the%20proposed%20benchmarks%20and%20established%20datasets%20such%0Aas%20M-BEIR%20and%20MTEB.%20These%20findings%20underscore%20the%20potential%20of%20multimodal%20web%0Adata%20as%20a%20valuable%20training%20resource%20for%20contrastive%20learning%20and%20illustrate%0Athe%20scalability%20of%20a%20unified%2C%20vision-centric%20approach%20for%20multimodal%0Arepresentation%20learning.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/showlab/VC2L.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520a%2520Unified%2520Vision-Centric%2520Contrastive%2520Alternatives%2520on%250A%2520%2520Multi-Modal%2520Web%2520Documents%26entry.906535625%3DYiqi%2520Lin%2520and%2520Alex%2520Jinpeng%2520Wang%2520and%2520Linjie%2520Li%2520and%2520Zhengyuan%2520Yang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Contrastive%2520vision-language%2520models%2520such%2520as%2520CLIP%2520have%2520demonstrated%2520strong%250Aperformance%2520across%2520a%2520wide%2520range%2520of%2520multimodal%2520tasks%2520by%2520learning%2520from%2520aligned%250Aimage-text%2520pairs.%2520However%252C%2520their%2520ability%2520to%2520handle%2520complex%252C%2520real-world%2520web%250Adocuments%2520remains%2520limited%252C%2520particularly%2520in%2520scenarios%2520where%2520text%2520and%2520images%2520are%250Ainterleaved%252C%2520loosely%2520aligned%252C%2520or%2520embedded%2520in%2520visual%2520form.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520Vision-Centric%2520Contrastive%2520Learning%2520%2528VC2L%2529%252C%2520a%2520unified%250Aframework%2520that%2520models%2520text%252C%2520images%252C%2520and%2520their%2520combinations%2520using%2520a%2520single%250Avision%2520transformer.%2520VC2L%2520operates%2520entirely%2520in%2520pixel%2520space%2520by%2520rendering%2520all%250Ainputs%252C%2520whether%2520textual%252C%2520visual%252C%2520or%2520combined%252C%2520as%2520images%252C%2520thus%2520eliminating%2520the%250Aneed%2520for%2520OCR%252C%2520text%2520tokenization%252C%2520or%2520modality%2520fusion%2520strategy.%2520To%2520capture%250Acomplex%2520cross-modal%2520relationships%2520in%2520multimodal%2520web%2520documents%252C%2520VC2L%2520employs%2520a%250Asnippet-level%2520contrastive%2520learning%2520objective%2520that%2520aligns%2520consecutive%2520multimodal%250Asegments%252C%2520leveraging%2520the%2520inherent%2520coherence%2520of%2520documents%2520without%2520requiring%250Aexplicitly%2520paired%2520image-text%2520data.%2520To%2520assess%2520the%2520effectiveness%2520of%2520this%250Aapproach%252C%2520we%2520introduce%2520three%2520retrieval%2520benchmarks%252C%2520AnyCIR%252C%2520SeqCIR%252C%2520and%2520CSR%252C%250Adesigned%2520to%2520evaluate%2520cross-modal%2520retrieval%252C%2520fine-grained%2520sequential%250Aunderstanding%252C%2520and%2520generalization%2520to%2520unseen%2520data%252C%2520respectively.%2520Empirical%250Aresults%2520show%2520that%2520VC2L%2520achieves%2520competitive%2520or%2520superior%2520performance%2520compared%2520to%250ACLIP-style%2520models%2520on%2520both%2520the%2520proposed%2520benchmarks%2520and%2520established%2520datasets%2520such%250Aas%2520M-BEIR%2520and%2520MTEB.%2520These%2520findings%2520underscore%2520the%2520potential%2520of%2520multimodal%2520web%250Adata%2520as%2520a%2520valuable%2520training%2520resource%2520for%2520contrastive%2520learning%2520and%2520illustrate%250Athe%2520scalability%2520of%2520a%2520unified%252C%2520vision-centric%2520approach%2520for%2520multimodal%250Arepresentation%2520learning.%2520Code%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/showlab/VC2L.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20a%20Unified%20Vision-Centric%20Contrastive%20Alternatives%20on%0A%20%20Multi-Modal%20Web%20Documents&entry.906535625=Yiqi%20Lin%20and%20Alex%20Jinpeng%20Wang%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Contrastive%20vision-language%20models%20such%20as%20CLIP%20have%20demonstrated%20strong%0Aperformance%20across%20a%20wide%20range%20of%20multimodal%20tasks%20by%20learning%20from%20aligned%0Aimage-text%20pairs.%20However%2C%20their%20ability%20to%20handle%20complex%2C%20real-world%20web%0Adocuments%20remains%20limited%2C%20particularly%20in%20scenarios%20where%20text%20and%20images%20are%0Ainterleaved%2C%20loosely%20aligned%2C%20or%20embedded%20in%20visual%20form.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Vision-Centric%20Contrastive%20Learning%20%28VC2L%29%2C%20a%20unified%0Aframework%20that%20models%20text%2C%20images%2C%20and%20their%20combinations%20using%20a%20single%0Avision%20transformer.%20VC2L%20operates%20entirely%20in%20pixel%20space%20by%20rendering%20all%0Ainputs%2C%20whether%20textual%2C%20visual%2C%20or%20combined%2C%20as%20images%2C%20thus%20eliminating%20the%0Aneed%20for%20OCR%2C%20text%20tokenization%2C%20or%20modality%20fusion%20strategy.%20To%20capture%0Acomplex%20cross-modal%20relationships%20in%20multimodal%20web%20documents%2C%20VC2L%20employs%20a%0Asnippet-level%20contrastive%20learning%20objective%20that%20aligns%20consecutive%20multimodal%0Asegments%2C%20leveraging%20the%20inherent%20coherence%20of%20documents%20without%20requiring%0Aexplicitly%20paired%20image-text%20data.%20To%20assess%20the%20effectiveness%20of%20this%0Aapproach%2C%20we%20introduce%20three%20retrieval%20benchmarks%2C%20AnyCIR%2C%20SeqCIR%2C%20and%20CSR%2C%0Adesigned%20to%20evaluate%20cross-modal%20retrieval%2C%20fine-grained%20sequential%0Aunderstanding%2C%20and%20generalization%20to%20unseen%20data%2C%20respectively.%20Empirical%0Aresults%20show%20that%20VC2L%20achieves%20competitive%20or%20superior%20performance%20compared%20to%0ACLIP-style%20models%20on%20both%20the%20proposed%20benchmarks%20and%20established%20datasets%20such%0Aas%20M-BEIR%20and%20MTEB.%20These%20findings%20underscore%20the%20potential%20of%20multimodal%20web%0Adata%20as%20a%20valuable%20training%20resource%20for%20contrastive%20learning%20and%20illustrate%0Athe%20scalability%20of%20a%20unified%2C%20vision-centric%20approach%20for%20multimodal%0Arepresentation%20learning.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/showlab/VC2L.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18703v1&entry.124074799=Read"},
{"title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs", "author": "Haochen Wang and Yuhao Wang and Tao Zhang and Yikang Zhou and Yanwei Li and Jiacong Wang and Ye Tian and Jiahao Meng and Zilong Huang and Guangcan Mai and Anran Wang and Yunhai Tong and Zhuochen Wang and Xiangtai Li and Zhaoxiang Zhang", "abstract": "  While Multimodal Large Language Models (MLLMs) excel at holistic\nunderstanding, they struggle in capturing the dense world with complex scenes,\nrequiring fine-grained analysis of intricate details and object\ninter-relationships. Region-level MLLMs have been a promising step. However,\nprevious attempts are generally optimized to understand given regions in\nisolation, neglecting crucial global contexts. To address this, we introduce\nGrasp Any Region (GAR) for comprehen- sive region-level visual understanding.\nEmpowered by an effective RoI-aligned feature replay technique, GAR supports\n(1) precise perception by leveraging necessary global contexts, and (2)\nmodeling interactions between multiple prompts. Together, it then naturally\nachieves (3) advanced compositional reasoning to answer specific free-form\nquestions about any region, shifting the paradigm from passive description to\nactive dialogue. Moreover, we construct GAR-Bench, which not only provides a\nmore accurate evaluation of single-region comprehension, but also, more\nimportantly, measures interactions and complex reasoning across multiple\nregions. Extensive experiments have demonstrated that GAR-1B not only maintains\nthe state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5\non DLC-Bench, but also excels at modeling relationships between multiple\nprompts with advanced comprehension capabilities, even surpassing InternVL3-78B\non GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms\nin-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong\ncapabilities can be easily transferred to videos.\n", "link": "http://arxiv.org/abs/2510.18876v1", "date": "2025-10-21", "relevancy": 3.0592, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6281}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grasp%20Any%20Region%3A%20Towards%20Precise%2C%20Contextual%20Pixel%20Understanding%20for%0A%20%20Multimodal%20LLMs&body=Title%3A%20Grasp%20Any%20Region%3A%20Towards%20Precise%2C%20Contextual%20Pixel%20Understanding%20for%0A%20%20Multimodal%20LLMs%0AAuthor%3A%20Haochen%20Wang%20and%20Yuhao%20Wang%20and%20Tao%20Zhang%20and%20Yikang%20Zhou%20and%20Yanwei%20Li%20and%20Jiacong%20Wang%20and%20Ye%20Tian%20and%20Jiahao%20Meng%20and%20Zilong%20Huang%20and%20Guangcan%20Mai%20and%20Anran%20Wang%20and%20Yunhai%20Tong%20and%20Zhuochen%20Wang%20and%20Xiangtai%20Li%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20holistic%0Aunderstanding%2C%20they%20struggle%20in%20capturing%20the%20dense%20world%20with%20complex%20scenes%2C%0Arequiring%20fine-grained%20analysis%20of%20intricate%20details%20and%20object%0Ainter-relationships.%20Region-level%20MLLMs%20have%20been%20a%20promising%20step.%20However%2C%0Aprevious%20attempts%20are%20generally%20optimized%20to%20understand%20given%20regions%20in%0Aisolation%2C%20neglecting%20crucial%20global%20contexts.%20To%20address%20this%2C%20we%20introduce%0AGrasp%20Any%20Region%20%28GAR%29%20for%20comprehen-%20sive%20region-level%20visual%20understanding.%0AEmpowered%20by%20an%20effective%20RoI-aligned%20feature%20replay%20technique%2C%20GAR%20supports%0A%281%29%20precise%20perception%20by%20leveraging%20necessary%20global%20contexts%2C%20and%20%282%29%0Amodeling%20interactions%20between%20multiple%20prompts.%20Together%2C%20it%20then%20naturally%0Aachieves%20%283%29%20advanced%20compositional%20reasoning%20to%20answer%20specific%20free-form%0Aquestions%20about%20any%20region%2C%20shifting%20the%20paradigm%20from%20passive%20description%20to%0Aactive%20dialogue.%20Moreover%2C%20we%20construct%20GAR-Bench%2C%20which%20not%20only%20provides%20a%0Amore%20accurate%20evaluation%20of%20single-region%20comprehension%2C%20but%20also%2C%20more%0Aimportantly%2C%20measures%20interactions%20and%20complex%20reasoning%20across%20multiple%0Aregions.%20Extensive%20experiments%20have%20demonstrated%20that%20GAR-1B%20not%20only%20maintains%0Athe%20state-of-the-art%20captioning%20capabilities%2C%20e.g.%2C%20outperforming%20DAM-3B%20%2B4.5%0Aon%20DLC-Bench%2C%20but%20also%20excels%20at%20modeling%20relationships%20between%20multiple%0Aprompts%20with%20advanced%20comprehension%20capabilities%2C%20even%20surpassing%20InternVL3-78B%0Aon%20GAR-Bench-VQA.%20More%20importantly%2C%20our%20zero-shot%20GAR-8B%20even%20outperforms%0Ain-domain%20VideoRefer-7B%20on%20VideoRefer-BenchQ%2C%20indicating%20its%20strong%0Acapabilities%20can%20be%20easily%20transferred%20to%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrasp%2520Any%2520Region%253A%2520Towards%2520Precise%252C%2520Contextual%2520Pixel%2520Understanding%2520for%250A%2520%2520Multimodal%2520LLMs%26entry.906535625%3DHaochen%2520Wang%2520and%2520Yuhao%2520Wang%2520and%2520Tao%2520Zhang%2520and%2520Yikang%2520Zhou%2520and%2520Yanwei%2520Li%2520and%2520Jiacong%2520Wang%2520and%2520Ye%2520Tian%2520and%2520Jiahao%2520Meng%2520and%2520Zilong%2520Huang%2520and%2520Guangcan%2520Mai%2520and%2520Anran%2520Wang%2520and%2520Yunhai%2520Tong%2520and%2520Zhuochen%2520Wang%2520and%2520Xiangtai%2520Li%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520holistic%250Aunderstanding%252C%2520they%2520struggle%2520in%2520capturing%2520the%2520dense%2520world%2520with%2520complex%2520scenes%252C%250Arequiring%2520fine-grained%2520analysis%2520of%2520intricate%2520details%2520and%2520object%250Ainter-relationships.%2520Region-level%2520MLLMs%2520have%2520been%2520a%2520promising%2520step.%2520However%252C%250Aprevious%2520attempts%2520are%2520generally%2520optimized%2520to%2520understand%2520given%2520regions%2520in%250Aisolation%252C%2520neglecting%2520crucial%2520global%2520contexts.%2520To%2520address%2520this%252C%2520we%2520introduce%250AGrasp%2520Any%2520Region%2520%2528GAR%2529%2520for%2520comprehen-%2520sive%2520region-level%2520visual%2520understanding.%250AEmpowered%2520by%2520an%2520effective%2520RoI-aligned%2520feature%2520replay%2520technique%252C%2520GAR%2520supports%250A%25281%2529%2520precise%2520perception%2520by%2520leveraging%2520necessary%2520global%2520contexts%252C%2520and%2520%25282%2529%250Amodeling%2520interactions%2520between%2520multiple%2520prompts.%2520Together%252C%2520it%2520then%2520naturally%250Aachieves%2520%25283%2529%2520advanced%2520compositional%2520reasoning%2520to%2520answer%2520specific%2520free-form%250Aquestions%2520about%2520any%2520region%252C%2520shifting%2520the%2520paradigm%2520from%2520passive%2520description%2520to%250Aactive%2520dialogue.%2520Moreover%252C%2520we%2520construct%2520GAR-Bench%252C%2520which%2520not%2520only%2520provides%2520a%250Amore%2520accurate%2520evaluation%2520of%2520single-region%2520comprehension%252C%2520but%2520also%252C%2520more%250Aimportantly%252C%2520measures%2520interactions%2520and%2520complex%2520reasoning%2520across%2520multiple%250Aregions.%2520Extensive%2520experiments%2520have%2520demonstrated%2520that%2520GAR-1B%2520not%2520only%2520maintains%250Athe%2520state-of-the-art%2520captioning%2520capabilities%252C%2520e.g.%252C%2520outperforming%2520DAM-3B%2520%252B4.5%250Aon%2520DLC-Bench%252C%2520but%2520also%2520excels%2520at%2520modeling%2520relationships%2520between%2520multiple%250Aprompts%2520with%2520advanced%2520comprehension%2520capabilities%252C%2520even%2520surpassing%2520InternVL3-78B%250Aon%2520GAR-Bench-VQA.%2520More%2520importantly%252C%2520our%2520zero-shot%2520GAR-8B%2520even%2520outperforms%250Ain-domain%2520VideoRefer-7B%2520on%2520VideoRefer-BenchQ%252C%2520indicating%2520its%2520strong%250Acapabilities%2520can%2520be%2520easily%2520transferred%2520to%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grasp%20Any%20Region%3A%20Towards%20Precise%2C%20Contextual%20Pixel%20Understanding%20for%0A%20%20Multimodal%20LLMs&entry.906535625=Haochen%20Wang%20and%20Yuhao%20Wang%20and%20Tao%20Zhang%20and%20Yikang%20Zhou%20and%20Yanwei%20Li%20and%20Jiacong%20Wang%20and%20Ye%20Tian%20and%20Jiahao%20Meng%20and%20Zilong%20Huang%20and%20Guangcan%20Mai%20and%20Anran%20Wang%20and%20Yunhai%20Tong%20and%20Zhuochen%20Wang%20and%20Xiangtai%20Li%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20holistic%0Aunderstanding%2C%20they%20struggle%20in%20capturing%20the%20dense%20world%20with%20complex%20scenes%2C%0Arequiring%20fine-grained%20analysis%20of%20intricate%20details%20and%20object%0Ainter-relationships.%20Region-level%20MLLMs%20have%20been%20a%20promising%20step.%20However%2C%0Aprevious%20attempts%20are%20generally%20optimized%20to%20understand%20given%20regions%20in%0Aisolation%2C%20neglecting%20crucial%20global%20contexts.%20To%20address%20this%2C%20we%20introduce%0AGrasp%20Any%20Region%20%28GAR%29%20for%20comprehen-%20sive%20region-level%20visual%20understanding.%0AEmpowered%20by%20an%20effective%20RoI-aligned%20feature%20replay%20technique%2C%20GAR%20supports%0A%281%29%20precise%20perception%20by%20leveraging%20necessary%20global%20contexts%2C%20and%20%282%29%0Amodeling%20interactions%20between%20multiple%20prompts.%20Together%2C%20it%20then%20naturally%0Aachieves%20%283%29%20advanced%20compositional%20reasoning%20to%20answer%20specific%20free-form%0Aquestions%20about%20any%20region%2C%20shifting%20the%20paradigm%20from%20passive%20description%20to%0Aactive%20dialogue.%20Moreover%2C%20we%20construct%20GAR-Bench%2C%20which%20not%20only%20provides%20a%0Amore%20accurate%20evaluation%20of%20single-region%20comprehension%2C%20but%20also%2C%20more%0Aimportantly%2C%20measures%20interactions%20and%20complex%20reasoning%20across%20multiple%0Aregions.%20Extensive%20experiments%20have%20demonstrated%20that%20GAR-1B%20not%20only%20maintains%0Athe%20state-of-the-art%20captioning%20capabilities%2C%20e.g.%2C%20outperforming%20DAM-3B%20%2B4.5%0Aon%20DLC-Bench%2C%20but%20also%20excels%20at%20modeling%20relationships%20between%20multiple%0Aprompts%20with%20advanced%20comprehension%20capabilities%2C%20even%20surpassing%20InternVL3-78B%0Aon%20GAR-Bench-VQA.%20More%20importantly%2C%20our%20zero-shot%20GAR-8B%20even%20outperforms%0Ain-domain%20VideoRefer-7B%20on%20VideoRefer-BenchQ%2C%20indicating%20its%20strong%0Acapabilities%20can%20be%20easily%20transferred%20to%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18876v1&entry.124074799=Read"},
{"title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning", "author": "Ye Liu and Zongyang Ma and Junfu Pu and Zhongang Qi and Yang Wu and Ying Shan and Chang Wen Chen", "abstract": "  Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.\n", "link": "http://arxiv.org/abs/2509.18094v2", "date": "2025-10-21", "relevancy": 3.0268, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6195}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniPixel%3A%20Unified%20Object%20Referring%20and%20Segmentation%20for%20Pixel-Level%0A%20%20Visual%20Reasoning&body=Title%3A%20UniPixel%3A%20Unified%20Object%20Referring%20and%20Segmentation%20for%20Pixel-Level%0A%20%20Visual%20Reasoning%0AAuthor%3A%20Ye%20Liu%20and%20Zongyang%20Ma%20and%20Junfu%20Pu%20and%20Zhongang%20Qi%20and%20Yang%20Wu%20and%20Ying%20Shan%20and%20Chang%20Wen%20Chen%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Multi-modal%20Models%20%28LMMs%29%20have%20demonstrated%20their%0Aremarkable%20success%20as%20general-purpose%20multi-modal%20assistants%2C%20with%20particular%0Afocuses%20on%20holistic%20image-%20and%20video-language%20understanding.%20Conversely%2C%20less%0Aattention%20has%20been%20given%20to%20scaling%20fine-grained%20pixel-level%20understanding%0Acapabilities%2C%20where%20the%20models%20are%20expected%20to%20realize%20pixel-level%20alignment%0Abetween%20visual%20signals%20and%20language%20semantics.%20Some%20previous%20studies%20have%0Aapplied%20LMMs%20to%20related%20tasks%20such%20as%20region-level%20captioning%20and%20referring%0Aexpression%20segmentation.%20However%2C%20these%20models%20are%20limited%20to%20performing%20either%0Areferring%20or%20segmentation%20tasks%20independently%20and%20fail%20to%20integrate%20these%0Afine-grained%20perception%20capabilities%20into%20visual%20reasoning.%20To%20bridge%20this%20gap%2C%0Awe%20propose%20UniPixel%2C%20a%20large%20multi-modal%20model%20capable%20of%20flexibly%0Acomprehending%20visual%20prompt%20inputs%20and%20generating%20mask-grounded%20responses.%20Our%0Amodel%20distinguishes%20itself%20by%20seamlessly%20integrating%20pixel-level%20perception%0Awith%20general%20visual%20understanding%20capabilities.%20Specifically%2C%20UniPixel%0Aprocesses%20visual%20prompts%20and%20generates%20relevant%20masks%20on%20demand%2C%20and%20performs%0Asubsequent%20reasoning%20conditioning%20on%20these%20intermediate%20pointers%20during%0Ainference%2C%20thereby%20enabling%20fine-grained%20pixel-level%20reasoning.%20The%0Aeffectiveness%20of%20our%20approach%20has%20been%20verified%20on%2010%20benchmarks%20across%20a%0Adiverse%20set%20of%20tasks%2C%20including%20pixel-level%20referring/segmentation%20and%0Aobject-centric%20understanding%20in%20images/videos.%20A%20novel%20PixelQA%20task%20that%0Ajointly%20requires%20referring%2C%20segmentation%2C%20and%20question%20answering%20is%20also%0Adesigned%20to%20verify%20the%20flexibility%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniPixel%253A%2520Unified%2520Object%2520Referring%2520and%2520Segmentation%2520for%2520Pixel-Level%250A%2520%2520Visual%2520Reasoning%26entry.906535625%3DYe%2520Liu%2520and%2520Zongyang%2520Ma%2520and%2520Junfu%2520Pu%2520and%2520Zhongang%2520Qi%2520and%2520Yang%2520Wu%2520and%2520Ying%2520Shan%2520and%2520Chang%2520Wen%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Multi-modal%2520Models%2520%2528LMMs%2529%2520have%2520demonstrated%2520their%250Aremarkable%2520success%2520as%2520general-purpose%2520multi-modal%2520assistants%252C%2520with%2520particular%250Afocuses%2520on%2520holistic%2520image-%2520and%2520video-language%2520understanding.%2520Conversely%252C%2520less%250Aattention%2520has%2520been%2520given%2520to%2520scaling%2520fine-grained%2520pixel-level%2520understanding%250Acapabilities%252C%2520where%2520the%2520models%2520are%2520expected%2520to%2520realize%2520pixel-level%2520alignment%250Abetween%2520visual%2520signals%2520and%2520language%2520semantics.%2520Some%2520previous%2520studies%2520have%250Aapplied%2520LMMs%2520to%2520related%2520tasks%2520such%2520as%2520region-level%2520captioning%2520and%2520referring%250Aexpression%2520segmentation.%2520However%252C%2520these%2520models%2520are%2520limited%2520to%2520performing%2520either%250Areferring%2520or%2520segmentation%2520tasks%2520independently%2520and%2520fail%2520to%2520integrate%2520these%250Afine-grained%2520perception%2520capabilities%2520into%2520visual%2520reasoning.%2520To%2520bridge%2520this%2520gap%252C%250Awe%2520propose%2520UniPixel%252C%2520a%2520large%2520multi-modal%2520model%2520capable%2520of%2520flexibly%250Acomprehending%2520visual%2520prompt%2520inputs%2520and%2520generating%2520mask-grounded%2520responses.%2520Our%250Amodel%2520distinguishes%2520itself%2520by%2520seamlessly%2520integrating%2520pixel-level%2520perception%250Awith%2520general%2520visual%2520understanding%2520capabilities.%2520Specifically%252C%2520UniPixel%250Aprocesses%2520visual%2520prompts%2520and%2520generates%2520relevant%2520masks%2520on%2520demand%252C%2520and%2520performs%250Asubsequent%2520reasoning%2520conditioning%2520on%2520these%2520intermediate%2520pointers%2520during%250Ainference%252C%2520thereby%2520enabling%2520fine-grained%2520pixel-level%2520reasoning.%2520The%250Aeffectiveness%2520of%2520our%2520approach%2520has%2520been%2520verified%2520on%252010%2520benchmarks%2520across%2520a%250Adiverse%2520set%2520of%2520tasks%252C%2520including%2520pixel-level%2520referring/segmentation%2520and%250Aobject-centric%2520understanding%2520in%2520images/videos.%2520A%2520novel%2520PixelQA%2520task%2520that%250Ajointly%2520requires%2520referring%252C%2520segmentation%252C%2520and%2520question%2520answering%2520is%2520also%250Adesigned%2520to%2520verify%2520the%2520flexibility%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPixel%3A%20Unified%20Object%20Referring%20and%20Segmentation%20for%20Pixel-Level%0A%20%20Visual%20Reasoning&entry.906535625=Ye%20Liu%20and%20Zongyang%20Ma%20and%20Junfu%20Pu%20and%20Zhongang%20Qi%20and%20Yang%20Wu%20and%20Ying%20Shan%20and%20Chang%20Wen%20Chen&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Multi-modal%20Models%20%28LMMs%29%20have%20demonstrated%20their%0Aremarkable%20success%20as%20general-purpose%20multi-modal%20assistants%2C%20with%20particular%0Afocuses%20on%20holistic%20image-%20and%20video-language%20understanding.%20Conversely%2C%20less%0Aattention%20has%20been%20given%20to%20scaling%20fine-grained%20pixel-level%20understanding%0Acapabilities%2C%20where%20the%20models%20are%20expected%20to%20realize%20pixel-level%20alignment%0Abetween%20visual%20signals%20and%20language%20semantics.%20Some%20previous%20studies%20have%0Aapplied%20LMMs%20to%20related%20tasks%20such%20as%20region-level%20captioning%20and%20referring%0Aexpression%20segmentation.%20However%2C%20these%20models%20are%20limited%20to%20performing%20either%0Areferring%20or%20segmentation%20tasks%20independently%20and%20fail%20to%20integrate%20these%0Afine-grained%20perception%20capabilities%20into%20visual%20reasoning.%20To%20bridge%20this%20gap%2C%0Awe%20propose%20UniPixel%2C%20a%20large%20multi-modal%20model%20capable%20of%20flexibly%0Acomprehending%20visual%20prompt%20inputs%20and%20generating%20mask-grounded%20responses.%20Our%0Amodel%20distinguishes%20itself%20by%20seamlessly%20integrating%20pixel-level%20perception%0Awith%20general%20visual%20understanding%20capabilities.%20Specifically%2C%20UniPixel%0Aprocesses%20visual%20prompts%20and%20generates%20relevant%20masks%20on%20demand%2C%20and%20performs%0Asubsequent%20reasoning%20conditioning%20on%20these%20intermediate%20pointers%20during%0Ainference%2C%20thereby%20enabling%20fine-grained%20pixel-level%20reasoning.%20The%0Aeffectiveness%20of%20our%20approach%20has%20been%20verified%20on%2010%20benchmarks%20across%20a%0Adiverse%20set%20of%20tasks%2C%20including%20pixel-level%20referring/segmentation%20and%0Aobject-centric%20understanding%20in%20images/videos.%20A%20novel%20PixelQA%20task%20that%0Ajointly%20requires%20referring%2C%20segmentation%2C%20and%20question%20answering%20is%20also%0Adesigned%20to%20verify%20the%20flexibility%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18094v2&entry.124074799=Read"},
{"title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence", "author": "Ziang Zhang and Zehan Wang and Guanghao Zhang and Weilong Dai and Yan Xia and Ziang Yan and Minjie Hong and Zhou Zhao", "abstract": "  Reasoning about dynamic spatial relationships is essential, as both observers\nand objects often move simultaneously. Although vision-language models (VLMs)\nand visual expertise models excel in 2D tasks and static scenarios, their\nability to fully understand dynamic 3D scenarios remains limited. We introduce\nDynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly\n1,000 dynamic videos and over 1,700 manually annotated questions covering nine\ndecoupled motion patterns of observers and objects. Spatially and temporally\nsymmetric designs reduce biases and enable systematic evaluation of models'\nreasoning about self-motion and object motion. Our evaluation of 14 VLMs and\nexpert models reveals key limitations: models often conflate observer and\nobject motion, exhibit semantic biases, and fail to accurately infer relative\nrelationships in dynamic scenarios. Our DSI-Bench provides valuable findings\nand insights about the future development of general and expertise models with\ndynamic spatial intelligence.\n", "link": "http://arxiv.org/abs/2510.18873v1", "date": "2025-10-21", "relevancy": 2.9938, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6168}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSI-Bench%3A%20A%20Benchmark%20for%20Dynamic%20Spatial%20Intelligence&body=Title%3A%20DSI-Bench%3A%20A%20Benchmark%20for%20Dynamic%20Spatial%20Intelligence%0AAuthor%3A%20Ziang%20Zhang%20and%20Zehan%20Wang%20and%20Guanghao%20Zhang%20and%20Weilong%20Dai%20and%20Yan%20Xia%20and%20Ziang%20Yan%20and%20Minjie%20Hong%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Reasoning%20about%20dynamic%20spatial%20relationships%20is%20essential%2C%20as%20both%20observers%0Aand%20objects%20often%20move%20simultaneously.%20Although%20vision-language%20models%20%28VLMs%29%0Aand%20visual%20expertise%20models%20excel%20in%202D%20tasks%20and%20static%20scenarios%2C%20their%0Aability%20to%20fully%20understand%20dynamic%203D%20scenarios%20remains%20limited.%20We%20introduce%0ADynamic%20Spatial%20Intelligence%20and%20propose%20DSI-Bench%2C%20a%20benchmark%20with%20nearly%0A1%2C000%20dynamic%20videos%20and%20over%201%2C700%20manually%20annotated%20questions%20covering%20nine%0Adecoupled%20motion%20patterns%20of%20observers%20and%20objects.%20Spatially%20and%20temporally%0Asymmetric%20designs%20reduce%20biases%20and%20enable%20systematic%20evaluation%20of%20models%27%0Areasoning%20about%20self-motion%20and%20object%20motion.%20Our%20evaluation%20of%2014%20VLMs%20and%0Aexpert%20models%20reveals%20key%20limitations%3A%20models%20often%20conflate%20observer%20and%0Aobject%20motion%2C%20exhibit%20semantic%20biases%2C%20and%20fail%20to%20accurately%20infer%20relative%0Arelationships%20in%20dynamic%20scenarios.%20Our%20DSI-Bench%20provides%20valuable%20findings%0Aand%20insights%20about%20the%20future%20development%20of%20general%20and%20expertise%20models%20with%0Adynamic%20spatial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSI-Bench%253A%2520A%2520Benchmark%2520for%2520Dynamic%2520Spatial%2520Intelligence%26entry.906535625%3DZiang%2520Zhang%2520and%2520Zehan%2520Wang%2520and%2520Guanghao%2520Zhang%2520and%2520Weilong%2520Dai%2520and%2520Yan%2520Xia%2520and%2520Ziang%2520Yan%2520and%2520Minjie%2520Hong%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Reasoning%2520about%2520dynamic%2520spatial%2520relationships%2520is%2520essential%252C%2520as%2520both%2520observers%250Aand%2520objects%2520often%2520move%2520simultaneously.%2520Although%2520vision-language%2520models%2520%2528VLMs%2529%250Aand%2520visual%2520expertise%2520models%2520excel%2520in%25202D%2520tasks%2520and%2520static%2520scenarios%252C%2520their%250Aability%2520to%2520fully%2520understand%2520dynamic%25203D%2520scenarios%2520remains%2520limited.%2520We%2520introduce%250ADynamic%2520Spatial%2520Intelligence%2520and%2520propose%2520DSI-Bench%252C%2520a%2520benchmark%2520with%2520nearly%250A1%252C000%2520dynamic%2520videos%2520and%2520over%25201%252C700%2520manually%2520annotated%2520questions%2520covering%2520nine%250Adecoupled%2520motion%2520patterns%2520of%2520observers%2520and%2520objects.%2520Spatially%2520and%2520temporally%250Asymmetric%2520designs%2520reduce%2520biases%2520and%2520enable%2520systematic%2520evaluation%2520of%2520models%2527%250Areasoning%2520about%2520self-motion%2520and%2520object%2520motion.%2520Our%2520evaluation%2520of%252014%2520VLMs%2520and%250Aexpert%2520models%2520reveals%2520key%2520limitations%253A%2520models%2520often%2520conflate%2520observer%2520and%250Aobject%2520motion%252C%2520exhibit%2520semantic%2520biases%252C%2520and%2520fail%2520to%2520accurately%2520infer%2520relative%250Arelationships%2520in%2520dynamic%2520scenarios.%2520Our%2520DSI-Bench%2520provides%2520valuable%2520findings%250Aand%2520insights%2520about%2520the%2520future%2520development%2520of%2520general%2520and%2520expertise%2520models%2520with%250Adynamic%2520spatial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSI-Bench%3A%20A%20Benchmark%20for%20Dynamic%20Spatial%20Intelligence&entry.906535625=Ziang%20Zhang%20and%20Zehan%20Wang%20and%20Guanghao%20Zhang%20and%20Weilong%20Dai%20and%20Yan%20Xia%20and%20Ziang%20Yan%20and%20Minjie%20Hong%20and%20Zhou%20Zhao&entry.1292438233=%20%20Reasoning%20about%20dynamic%20spatial%20relationships%20is%20essential%2C%20as%20both%20observers%0Aand%20objects%20often%20move%20simultaneously.%20Although%20vision-language%20models%20%28VLMs%29%0Aand%20visual%20expertise%20models%20excel%20in%202D%20tasks%20and%20static%20scenarios%2C%20their%0Aability%20to%20fully%20understand%20dynamic%203D%20scenarios%20remains%20limited.%20We%20introduce%0ADynamic%20Spatial%20Intelligence%20and%20propose%20DSI-Bench%2C%20a%20benchmark%20with%20nearly%0A1%2C000%20dynamic%20videos%20and%20over%201%2C700%20manually%20annotated%20questions%20covering%20nine%0Adecoupled%20motion%20patterns%20of%20observers%20and%20objects.%20Spatially%20and%20temporally%0Asymmetric%20designs%20reduce%20biases%20and%20enable%20systematic%20evaluation%20of%20models%27%0Areasoning%20about%20self-motion%20and%20object%20motion.%20Our%20evaluation%20of%2014%20VLMs%20and%0Aexpert%20models%20reveals%20key%20limitations%3A%20models%20often%20conflate%20observer%20and%0Aobject%20motion%2C%20exhibit%20semantic%20biases%2C%20and%20fail%20to%20accurately%20infer%20relative%0Arelationships%20in%20dynamic%20scenarios.%20Our%20DSI-Bench%20provides%20valuable%20findings%0Aand%20insights%20about%20the%20future%20development%20of%20general%20and%20expertise%20models%20with%0Adynamic%20spatial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18873v1&entry.124074799=Read"},
{"title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder", "author": "Xiaoxing Hu and Kaicheng Yang and Ziyong Feng and Qi Ming and Zonghao Guo and Xiang An and Ziyong Feng and Junchi Yan and Xue Yang", "abstract": "  The original CLIP text encoder is limited by a maximum input length of 77\ntokens, which hampers its ability to effectively process long texts and perform\nfine-grained semantic understanding. In addition, the CLIP text encoder lacks\nsupport for multilingual inputs. All these limitations significantly restrict\nits applicability across a broader range of tasks. Recent studies have\nattempted to replace the CLIP text encoder with an LLM-based embedder to\nenhance its ability in processing long texts, multilingual understanding, and\nfine-grained semantic comprehension. However, because the representation spaces\nof LLMs and the vision-language space of CLIP are pretrained independently\nwithout alignment priors, direct alignment using contrastive learning can\ndisrupt the intrinsic vision-language alignment in the CLIP image encoder,\nleading to an underutilization of the knowledge acquired during pre-training.\nTo address this challenge, we propose ProCLIP, a curriculum learning-based\nprogressive vision-language alignment framework to effectively align the CLIP\nimage encoder with an LLM-based embedder. Specifically, ProCLIP first distills\nknowledge from CLIP's text encoder into the LLM-based embedder to leverage\nCLIP's rich pretrained knowledge while establishing initial alignment between\nthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns\nthe CLIP image encoder with the LLM-based embedder through image-text\ncontrastive tuning, employing self-distillation regularization to avoid\noverfitting. To achieve a more effective alignment, instance semantic alignment\nloss and embedding structure alignment loss are employed during representation\ninheritance and contrastive tuning. The Code is available at\nhttps://github.com/VisionXLab/ProCLIP\n", "link": "http://arxiv.org/abs/2510.18795v1", "date": "2025-10-21", "relevancy": 2.9896, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6181}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProCLIP%3A%20Progressive%20Vision-Language%20Alignment%20via%20LLM-based%20Embedder&body=Title%3A%20ProCLIP%3A%20Progressive%20Vision-Language%20Alignment%20via%20LLM-based%20Embedder%0AAuthor%3A%20Xiaoxing%20Hu%20and%20Kaicheng%20Yang%20and%20Ziyong%20Feng%20and%20Qi%20Ming%20and%20Zonghao%20Guo%20and%20Xiang%20An%20and%20Ziyong%20Feng%20and%20Junchi%20Yan%20and%20Xue%20Yang%0AAbstract%3A%20%20%20The%20original%20CLIP%20text%20encoder%20is%20limited%20by%20a%20maximum%20input%20length%20of%2077%0Atokens%2C%20which%20hampers%20its%20ability%20to%20effectively%20process%20long%20texts%20and%20perform%0Afine-grained%20semantic%20understanding.%20In%20addition%2C%20the%20CLIP%20text%20encoder%20lacks%0Asupport%20for%20multilingual%20inputs.%20All%20these%20limitations%20significantly%20restrict%0Aits%20applicability%20across%20a%20broader%20range%20of%20tasks.%20Recent%20studies%20have%0Aattempted%20to%20replace%20the%20CLIP%20text%20encoder%20with%20an%20LLM-based%20embedder%20to%0Aenhance%20its%20ability%20in%20processing%20long%20texts%2C%20multilingual%20understanding%2C%20and%0Afine-grained%20semantic%20comprehension.%20However%2C%20because%20the%20representation%20spaces%0Aof%20LLMs%20and%20the%20vision-language%20space%20of%20CLIP%20are%20pretrained%20independently%0Awithout%20alignment%20priors%2C%20direct%20alignment%20using%20contrastive%20learning%20can%0Adisrupt%20the%20intrinsic%20vision-language%20alignment%20in%20the%20CLIP%20image%20encoder%2C%0Aleading%20to%20an%20underutilization%20of%20the%20knowledge%20acquired%20during%20pre-training.%0ATo%20address%20this%20challenge%2C%20we%20propose%20ProCLIP%2C%20a%20curriculum%20learning-based%0Aprogressive%20vision-language%20alignment%20framework%20to%20effectively%20align%20the%20CLIP%0Aimage%20encoder%20with%20an%20LLM-based%20embedder.%20Specifically%2C%20ProCLIP%20first%20distills%0Aknowledge%20from%20CLIP%27s%20text%20encoder%20into%20the%20LLM-based%20embedder%20to%20leverage%0ACLIP%27s%20rich%20pretrained%20knowledge%20while%20establishing%20initial%20alignment%20between%0Athe%20LLM%20embedder%20and%20CLIP%20image%20encoder.%20Subsequently%2C%20ProCLIP%20further%20aligns%0Athe%20CLIP%20image%20encoder%20with%20the%20LLM-based%20embedder%20through%20image-text%0Acontrastive%20tuning%2C%20employing%20self-distillation%20regularization%20to%20avoid%0Aoverfitting.%20To%20achieve%20a%20more%20effective%20alignment%2C%20instance%20semantic%20alignment%0Aloss%20and%20embedding%20structure%20alignment%20loss%20are%20employed%20during%20representation%0Ainheritance%20and%20contrastive%20tuning.%20The%20Code%20is%20available%20at%0Ahttps%3A//github.com/VisionXLab/ProCLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProCLIP%253A%2520Progressive%2520Vision-Language%2520Alignment%2520via%2520LLM-based%2520Embedder%26entry.906535625%3DXiaoxing%2520Hu%2520and%2520Kaicheng%2520Yang%2520and%2520Ziyong%2520Feng%2520and%2520Qi%2520Ming%2520and%2520Zonghao%2520Guo%2520and%2520Xiang%2520An%2520and%2520Ziyong%2520Feng%2520and%2520Junchi%2520Yan%2520and%2520Xue%2520Yang%26entry.1292438233%3D%2520%2520The%2520original%2520CLIP%2520text%2520encoder%2520is%2520limited%2520by%2520a%2520maximum%2520input%2520length%2520of%252077%250Atokens%252C%2520which%2520hampers%2520its%2520ability%2520to%2520effectively%2520process%2520long%2520texts%2520and%2520perform%250Afine-grained%2520semantic%2520understanding.%2520In%2520addition%252C%2520the%2520CLIP%2520text%2520encoder%2520lacks%250Asupport%2520for%2520multilingual%2520inputs.%2520All%2520these%2520limitations%2520significantly%2520restrict%250Aits%2520applicability%2520across%2520a%2520broader%2520range%2520of%2520tasks.%2520Recent%2520studies%2520have%250Aattempted%2520to%2520replace%2520the%2520CLIP%2520text%2520encoder%2520with%2520an%2520LLM-based%2520embedder%2520to%250Aenhance%2520its%2520ability%2520in%2520processing%2520long%2520texts%252C%2520multilingual%2520understanding%252C%2520and%250Afine-grained%2520semantic%2520comprehension.%2520However%252C%2520because%2520the%2520representation%2520spaces%250Aof%2520LLMs%2520and%2520the%2520vision-language%2520space%2520of%2520CLIP%2520are%2520pretrained%2520independently%250Awithout%2520alignment%2520priors%252C%2520direct%2520alignment%2520using%2520contrastive%2520learning%2520can%250Adisrupt%2520the%2520intrinsic%2520vision-language%2520alignment%2520in%2520the%2520CLIP%2520image%2520encoder%252C%250Aleading%2520to%2520an%2520underutilization%2520of%2520the%2520knowledge%2520acquired%2520during%2520pre-training.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520propose%2520ProCLIP%252C%2520a%2520curriculum%2520learning-based%250Aprogressive%2520vision-language%2520alignment%2520framework%2520to%2520effectively%2520align%2520the%2520CLIP%250Aimage%2520encoder%2520with%2520an%2520LLM-based%2520embedder.%2520Specifically%252C%2520ProCLIP%2520first%2520distills%250Aknowledge%2520from%2520CLIP%2527s%2520text%2520encoder%2520into%2520the%2520LLM-based%2520embedder%2520to%2520leverage%250ACLIP%2527s%2520rich%2520pretrained%2520knowledge%2520while%2520establishing%2520initial%2520alignment%2520between%250Athe%2520LLM%2520embedder%2520and%2520CLIP%2520image%2520encoder.%2520Subsequently%252C%2520ProCLIP%2520further%2520aligns%250Athe%2520CLIP%2520image%2520encoder%2520with%2520the%2520LLM-based%2520embedder%2520through%2520image-text%250Acontrastive%2520tuning%252C%2520employing%2520self-distillation%2520regularization%2520to%2520avoid%250Aoverfitting.%2520To%2520achieve%2520a%2520more%2520effective%2520alignment%252C%2520instance%2520semantic%2520alignment%250Aloss%2520and%2520embedding%2520structure%2520alignment%2520loss%2520are%2520employed%2520during%2520representation%250Ainheritance%2520and%2520contrastive%2520tuning.%2520The%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/VisionXLab/ProCLIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProCLIP%3A%20Progressive%20Vision-Language%20Alignment%20via%20LLM-based%20Embedder&entry.906535625=Xiaoxing%20Hu%20and%20Kaicheng%20Yang%20and%20Ziyong%20Feng%20and%20Qi%20Ming%20and%20Zonghao%20Guo%20and%20Xiang%20An%20and%20Ziyong%20Feng%20and%20Junchi%20Yan%20and%20Xue%20Yang&entry.1292438233=%20%20The%20original%20CLIP%20text%20encoder%20is%20limited%20by%20a%20maximum%20input%20length%20of%2077%0Atokens%2C%20which%20hampers%20its%20ability%20to%20effectively%20process%20long%20texts%20and%20perform%0Afine-grained%20semantic%20understanding.%20In%20addition%2C%20the%20CLIP%20text%20encoder%20lacks%0Asupport%20for%20multilingual%20inputs.%20All%20these%20limitations%20significantly%20restrict%0Aits%20applicability%20across%20a%20broader%20range%20of%20tasks.%20Recent%20studies%20have%0Aattempted%20to%20replace%20the%20CLIP%20text%20encoder%20with%20an%20LLM-based%20embedder%20to%0Aenhance%20its%20ability%20in%20processing%20long%20texts%2C%20multilingual%20understanding%2C%20and%0Afine-grained%20semantic%20comprehension.%20However%2C%20because%20the%20representation%20spaces%0Aof%20LLMs%20and%20the%20vision-language%20space%20of%20CLIP%20are%20pretrained%20independently%0Awithout%20alignment%20priors%2C%20direct%20alignment%20using%20contrastive%20learning%20can%0Adisrupt%20the%20intrinsic%20vision-language%20alignment%20in%20the%20CLIP%20image%20encoder%2C%0Aleading%20to%20an%20underutilization%20of%20the%20knowledge%20acquired%20during%20pre-training.%0ATo%20address%20this%20challenge%2C%20we%20propose%20ProCLIP%2C%20a%20curriculum%20learning-based%0Aprogressive%20vision-language%20alignment%20framework%20to%20effectively%20align%20the%20CLIP%0Aimage%20encoder%20with%20an%20LLM-based%20embedder.%20Specifically%2C%20ProCLIP%20first%20distills%0Aknowledge%20from%20CLIP%27s%20text%20encoder%20into%20the%20LLM-based%20embedder%20to%20leverage%0ACLIP%27s%20rich%20pretrained%20knowledge%20while%20establishing%20initial%20alignment%20between%0Athe%20LLM%20embedder%20and%20CLIP%20image%20encoder.%20Subsequently%2C%20ProCLIP%20further%20aligns%0Athe%20CLIP%20image%20encoder%20with%20the%20LLM-based%20embedder%20through%20image-text%0Acontrastive%20tuning%2C%20employing%20self-distillation%20regularization%20to%20avoid%0Aoverfitting.%20To%20achieve%20a%20more%20effective%20alignment%2C%20instance%20semantic%20alignment%0Aloss%20and%20embedding%20structure%20alignment%20loss%20are%20employed%20during%20representation%0Ainheritance%20and%20contrastive%20tuning.%20The%20Code%20is%20available%20at%0Ahttps%3A//github.com/VisionXLab/ProCLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18795v1&entry.124074799=Read"},
{"title": "Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model", "author": "Zhenxing Zhang and Jiayan Teng and Zhuoyi Yang and Tiankun Cao and Cheng Wang and Xiaotao Gu and Jie Tang and Dan Guo and Meng Wang", "abstract": "  We present Kaleido, a subject-to-video~(S2V) generation framework, which aims\nto synthesize subject-consistent videos conditioned on multiple reference\nimages of target subjects. Despite recent progress in S2V generation models,\nexisting approaches remain inadequate at maintaining multi-subject consistency\nand at handling background disentanglement, often resulting in lower reference\nfidelity and semantic drift under multi-image conditioning. These shortcomings\ncan be attributed to several factors. Primarily, the training dataset suffers\nfrom a lack of diversity and high-quality samples, as well as cross-paired\ndata, i.e., paired samples whose components originate from different instances.\nIn addition, the current mechanism for integrating multiple reference images is\nsuboptimal, potentially resulting in the confusion of multiple subjects. To\novercome these limitations, we propose a dedicated data construction pipeline,\nincorporating low-quality sample filtering and diverse data synthesis, to\nproduce consistency-preserving training data. Moreover, we introduce Reference\nRotary Positional Encoding (R-RoPE) to process reference images, enabling\nstable and precise multi-image integration. Extensive experiments across\nnumerous benchmarks demonstrate that Kaleido significantly outperforms previous\nmethods in consistency, fidelity, and generalization, marking an advance in S2V\ngeneration.\n", "link": "http://arxiv.org/abs/2510.18573v1", "date": "2025-10-21", "relevancy": 2.9237, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kaleido%3A%20Open-Sourced%20Multi-Subject%20Reference%20Video%20Generation%20Model&body=Title%3A%20Kaleido%3A%20Open-Sourced%20Multi-Subject%20Reference%20Video%20Generation%20Model%0AAuthor%3A%20Zhenxing%20Zhang%20and%20Jiayan%20Teng%20and%20Zhuoyi%20Yang%20and%20Tiankun%20Cao%20and%20Cheng%20Wang%20and%20Xiaotao%20Gu%20and%20Jie%20Tang%20and%20Dan%20Guo%20and%20Meng%20Wang%0AAbstract%3A%20%20%20We%20present%20Kaleido%2C%20a%20subject-to-video~%28S2V%29%20generation%20framework%2C%20which%20aims%0Ato%20synthesize%20subject-consistent%20videos%20conditioned%20on%20multiple%20reference%0Aimages%20of%20target%20subjects.%20Despite%20recent%20progress%20in%20S2V%20generation%20models%2C%0Aexisting%20approaches%20remain%20inadequate%20at%20maintaining%20multi-subject%20consistency%0Aand%20at%20handling%20background%20disentanglement%2C%20often%20resulting%20in%20lower%20reference%0Afidelity%20and%20semantic%20drift%20under%20multi-image%20conditioning.%20These%20shortcomings%0Acan%20be%20attributed%20to%20several%20factors.%20Primarily%2C%20the%20training%20dataset%20suffers%0Afrom%20a%20lack%20of%20diversity%20and%20high-quality%20samples%2C%20as%20well%20as%20cross-paired%0Adata%2C%20i.e.%2C%20paired%20samples%20whose%20components%20originate%20from%20different%20instances.%0AIn%20addition%2C%20the%20current%20mechanism%20for%20integrating%20multiple%20reference%20images%20is%0Asuboptimal%2C%20potentially%20resulting%20in%20the%20confusion%20of%20multiple%20subjects.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20a%20dedicated%20data%20construction%20pipeline%2C%0Aincorporating%20low-quality%20sample%20filtering%20and%20diverse%20data%20synthesis%2C%20to%0Aproduce%20consistency-preserving%20training%20data.%20Moreover%2C%20we%20introduce%20Reference%0ARotary%20Positional%20Encoding%20%28R-RoPE%29%20to%20process%20reference%20images%2C%20enabling%0Astable%20and%20precise%20multi-image%20integration.%20Extensive%20experiments%20across%0Anumerous%20benchmarks%20demonstrate%20that%20Kaleido%20significantly%20outperforms%20previous%0Amethods%20in%20consistency%2C%20fidelity%2C%20and%20generalization%2C%20marking%20an%20advance%20in%20S2V%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKaleido%253A%2520Open-Sourced%2520Multi-Subject%2520Reference%2520Video%2520Generation%2520Model%26entry.906535625%3DZhenxing%2520Zhang%2520and%2520Jiayan%2520Teng%2520and%2520Zhuoyi%2520Yang%2520and%2520Tiankun%2520Cao%2520and%2520Cheng%2520Wang%2520and%2520Xiaotao%2520Gu%2520and%2520Jie%2520Tang%2520and%2520Dan%2520Guo%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520Kaleido%252C%2520a%2520subject-to-video~%2528S2V%2529%2520generation%2520framework%252C%2520which%2520aims%250Ato%2520synthesize%2520subject-consistent%2520videos%2520conditioned%2520on%2520multiple%2520reference%250Aimages%2520of%2520target%2520subjects.%2520Despite%2520recent%2520progress%2520in%2520S2V%2520generation%2520models%252C%250Aexisting%2520approaches%2520remain%2520inadequate%2520at%2520maintaining%2520multi-subject%2520consistency%250Aand%2520at%2520handling%2520background%2520disentanglement%252C%2520often%2520resulting%2520in%2520lower%2520reference%250Afidelity%2520and%2520semantic%2520drift%2520under%2520multi-image%2520conditioning.%2520These%2520shortcomings%250Acan%2520be%2520attributed%2520to%2520several%2520factors.%2520Primarily%252C%2520the%2520training%2520dataset%2520suffers%250Afrom%2520a%2520lack%2520of%2520diversity%2520and%2520high-quality%2520samples%252C%2520as%2520well%2520as%2520cross-paired%250Adata%252C%2520i.e.%252C%2520paired%2520samples%2520whose%2520components%2520originate%2520from%2520different%2520instances.%250AIn%2520addition%252C%2520the%2520current%2520mechanism%2520for%2520integrating%2520multiple%2520reference%2520images%2520is%250Asuboptimal%252C%2520potentially%2520resulting%2520in%2520the%2520confusion%2520of%2520multiple%2520subjects.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520dedicated%2520data%2520construction%2520pipeline%252C%250Aincorporating%2520low-quality%2520sample%2520filtering%2520and%2520diverse%2520data%2520synthesis%252C%2520to%250Aproduce%2520consistency-preserving%2520training%2520data.%2520Moreover%252C%2520we%2520introduce%2520Reference%250ARotary%2520Positional%2520Encoding%2520%2528R-RoPE%2529%2520to%2520process%2520reference%2520images%252C%2520enabling%250Astable%2520and%2520precise%2520multi-image%2520integration.%2520Extensive%2520experiments%2520across%250Anumerous%2520benchmarks%2520demonstrate%2520that%2520Kaleido%2520significantly%2520outperforms%2520previous%250Amethods%2520in%2520consistency%252C%2520fidelity%252C%2520and%2520generalization%252C%2520marking%2520an%2520advance%2520in%2520S2V%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kaleido%3A%20Open-Sourced%20Multi-Subject%20Reference%20Video%20Generation%20Model&entry.906535625=Zhenxing%20Zhang%20and%20Jiayan%20Teng%20and%20Zhuoyi%20Yang%20and%20Tiankun%20Cao%20and%20Cheng%20Wang%20and%20Xiaotao%20Gu%20and%20Jie%20Tang%20and%20Dan%20Guo%20and%20Meng%20Wang&entry.1292438233=%20%20We%20present%20Kaleido%2C%20a%20subject-to-video~%28S2V%29%20generation%20framework%2C%20which%20aims%0Ato%20synthesize%20subject-consistent%20videos%20conditioned%20on%20multiple%20reference%0Aimages%20of%20target%20subjects.%20Despite%20recent%20progress%20in%20S2V%20generation%20models%2C%0Aexisting%20approaches%20remain%20inadequate%20at%20maintaining%20multi-subject%20consistency%0Aand%20at%20handling%20background%20disentanglement%2C%20often%20resulting%20in%20lower%20reference%0Afidelity%20and%20semantic%20drift%20under%20multi-image%20conditioning.%20These%20shortcomings%0Acan%20be%20attributed%20to%20several%20factors.%20Primarily%2C%20the%20training%20dataset%20suffers%0Afrom%20a%20lack%20of%20diversity%20and%20high-quality%20samples%2C%20as%20well%20as%20cross-paired%0Adata%2C%20i.e.%2C%20paired%20samples%20whose%20components%20originate%20from%20different%20instances.%0AIn%20addition%2C%20the%20current%20mechanism%20for%20integrating%20multiple%20reference%20images%20is%0Asuboptimal%2C%20potentially%20resulting%20in%20the%20confusion%20of%20multiple%20subjects.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20a%20dedicated%20data%20construction%20pipeline%2C%0Aincorporating%20low-quality%20sample%20filtering%20and%20diverse%20data%20synthesis%2C%20to%0Aproduce%20consistency-preserving%20training%20data.%20Moreover%2C%20we%20introduce%20Reference%0ARotary%20Positional%20Encoding%20%28R-RoPE%29%20to%20process%20reference%20images%2C%20enabling%0Astable%20and%20precise%20multi-image%20integration.%20Extensive%20experiments%20across%0Anumerous%20benchmarks%20demonstrate%20that%20Kaleido%20significantly%20outperforms%20previous%0Amethods%20in%20consistency%2C%20fidelity%2C%20and%20generalization%2C%20marking%20an%20advance%20in%20S2V%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18573v1&entry.124074799=Read"},
{"title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and\n  Generation via Reinforcement Learning", "author": "Kaihang Pan and Yang Wu and Wendong Bu and Kai Shen and Juncheng Li and Yingting Wang and Yunfei Li and Siliang Tang and Jun Xiao and Fei Wu and Hang Zhao and Yueting Zhuang", "abstract": "  Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation. However, these two capabilities remain\nlargely independent, as if they are two separate functions encapsulated within\nthe same model. Consequently, visual comprehension does not enhance visual\ngeneration, and the reasoning mechanisms of LLMs have not been fully integrated\nto revolutionize image generation. In this paper, we propose to enable the\ncollaborative co-evolution of visual comprehension and generation, advancing\nimage generation into an iterative introspective process. We introduce a\ntwo-stage training approach: supervised fine-tuning teaches the MLLM with the\nfoundational ability to generate genuine CoT for visual generation, while\nreinforcement learning activates its full potential via an\nexploration-exploitation trade-off. Ultimately, we unlock the Aha moment in\nvisual generation, advancing MLLMs from text-to-image tasks to unified image\ngeneration. Extensive experiments demonstrate that our model not only excels in\ntext-to-image generation and image editing, but also functions as a superior\nimage semantic evaluator with enhanced visual comprehension capabilities.\nProject Page: https://janus-pro-r1.github.io.\n", "link": "http://arxiv.org/abs/2506.01480v2", "date": "2025-10-21", "relevancy": 2.9063, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Janus-Pro-R1%3A%20Advancing%20Collaborative%20Visual%20Comprehension%20and%0A%20%20Generation%20via%20Reinforcement%20Learning&body=Title%3A%20Janus-Pro-R1%3A%20Advancing%20Collaborative%20Visual%20Comprehension%20and%0A%20%20Generation%20via%20Reinforcement%20Learning%0AAuthor%3A%20Kaihang%20Pan%20and%20Yang%20Wu%20and%20Wendong%20Bu%20and%20Kai%20Shen%20and%20Juncheng%20Li%20and%20Yingting%20Wang%20and%20Yunfei%20Li%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Fei%20Wu%20and%20Hang%20Zhao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Recent%20endeavors%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20aim%20to%20unify%0Avisual%20comprehension%20and%20generation.%20However%2C%20these%20two%20capabilities%20remain%0Alargely%20independent%2C%20as%20if%20they%20are%20two%20separate%20functions%20encapsulated%20within%0Athe%20same%20model.%20Consequently%2C%20visual%20comprehension%20does%20not%20enhance%20visual%0Ageneration%2C%20and%20the%20reasoning%20mechanisms%20of%20LLMs%20have%20not%20been%20fully%20integrated%0Ato%20revolutionize%20image%20generation.%20In%20this%20paper%2C%20we%20propose%20to%20enable%20the%0Acollaborative%20co-evolution%20of%20visual%20comprehension%20and%20generation%2C%20advancing%0Aimage%20generation%20into%20an%20iterative%20introspective%20process.%20We%20introduce%20a%0Atwo-stage%20training%20approach%3A%20supervised%20fine-tuning%20teaches%20the%20MLLM%20with%20the%0Afoundational%20ability%20to%20generate%20genuine%20CoT%20for%20visual%20generation%2C%20while%0Areinforcement%20learning%20activates%20its%20full%20potential%20via%20an%0Aexploration-exploitation%20trade-off.%20Ultimately%2C%20we%20unlock%20the%20Aha%20moment%20in%0Avisual%20generation%2C%20advancing%20MLLMs%20from%20text-to-image%20tasks%20to%20unified%20image%0Ageneration.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20not%20only%20excels%20in%0Atext-to-image%20generation%20and%20image%20editing%2C%20but%20also%20functions%20as%20a%20superior%0Aimage%20semantic%20evaluator%20with%20enhanced%20visual%20comprehension%20capabilities.%0AProject%20Page%3A%20https%3A//janus-pro-r1.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJanus-Pro-R1%253A%2520Advancing%2520Collaborative%2520Visual%2520Comprehension%2520and%250A%2520%2520Generation%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DKaihang%2520Pan%2520and%2520Yang%2520Wu%2520and%2520Wendong%2520Bu%2520and%2520Kai%2520Shen%2520and%2520Juncheng%2520Li%2520and%2520Yingting%2520Wang%2520and%2520Yunfei%2520Li%2520and%2520Siliang%2520Tang%2520and%2520Jun%2520Xiao%2520and%2520Fei%2520Wu%2520and%2520Hang%2520Zhao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Recent%2520endeavors%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520aim%2520to%2520unify%250Avisual%2520comprehension%2520and%2520generation.%2520However%252C%2520these%2520two%2520capabilities%2520remain%250Alargely%2520independent%252C%2520as%2520if%2520they%2520are%2520two%2520separate%2520functions%2520encapsulated%2520within%250Athe%2520same%2520model.%2520Consequently%252C%2520visual%2520comprehension%2520does%2520not%2520enhance%2520visual%250Ageneration%252C%2520and%2520the%2520reasoning%2520mechanisms%2520of%2520LLMs%2520have%2520not%2520been%2520fully%2520integrated%250Ato%2520revolutionize%2520image%2520generation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520enable%2520the%250Acollaborative%2520co-evolution%2520of%2520visual%2520comprehension%2520and%2520generation%252C%2520advancing%250Aimage%2520generation%2520into%2520an%2520iterative%2520introspective%2520process.%2520We%2520introduce%2520a%250Atwo-stage%2520training%2520approach%253A%2520supervised%2520fine-tuning%2520teaches%2520the%2520MLLM%2520with%2520the%250Afoundational%2520ability%2520to%2520generate%2520genuine%2520CoT%2520for%2520visual%2520generation%252C%2520while%250Areinforcement%2520learning%2520activates%2520its%2520full%2520potential%2520via%2520an%250Aexploration-exploitation%2520trade-off.%2520Ultimately%252C%2520we%2520unlock%2520the%2520Aha%2520moment%2520in%250Avisual%2520generation%252C%2520advancing%2520MLLMs%2520from%2520text-to-image%2520tasks%2520to%2520unified%2520image%250Ageneration.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%2520not%2520only%2520excels%2520in%250Atext-to-image%2520generation%2520and%2520image%2520editing%252C%2520but%2520also%2520functions%2520as%2520a%2520superior%250Aimage%2520semantic%2520evaluator%2520with%2520enhanced%2520visual%2520comprehension%2520capabilities.%250AProject%2520Page%253A%2520https%253A//janus-pro-r1.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Janus-Pro-R1%3A%20Advancing%20Collaborative%20Visual%20Comprehension%20and%0A%20%20Generation%20via%20Reinforcement%20Learning&entry.906535625=Kaihang%20Pan%20and%20Yang%20Wu%20and%20Wendong%20Bu%20and%20Kai%20Shen%20and%20Juncheng%20Li%20and%20Yingting%20Wang%20and%20Yunfei%20Li%20and%20Siliang%20Tang%20and%20Jun%20Xiao%20and%20Fei%20Wu%20and%20Hang%20Zhao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Recent%20endeavors%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20aim%20to%20unify%0Avisual%20comprehension%20and%20generation.%20However%2C%20these%20two%20capabilities%20remain%0Alargely%20independent%2C%20as%20if%20they%20are%20two%20separate%20functions%20encapsulated%20within%0Athe%20same%20model.%20Consequently%2C%20visual%20comprehension%20does%20not%20enhance%20visual%0Ageneration%2C%20and%20the%20reasoning%20mechanisms%20of%20LLMs%20have%20not%20been%20fully%20integrated%0Ato%20revolutionize%20image%20generation.%20In%20this%20paper%2C%20we%20propose%20to%20enable%20the%0Acollaborative%20co-evolution%20of%20visual%20comprehension%20and%20generation%2C%20advancing%0Aimage%20generation%20into%20an%20iterative%20introspective%20process.%20We%20introduce%20a%0Atwo-stage%20training%20approach%3A%20supervised%20fine-tuning%20teaches%20the%20MLLM%20with%20the%0Afoundational%20ability%20to%20generate%20genuine%20CoT%20for%20visual%20generation%2C%20while%0Areinforcement%20learning%20activates%20its%20full%20potential%20via%20an%0Aexploration-exploitation%20trade-off.%20Ultimately%2C%20we%20unlock%20the%20Aha%20moment%20in%0Avisual%20generation%2C%20advancing%20MLLMs%20from%20text-to-image%20tasks%20to%20unified%20image%0Ageneration.%20Extensive%20experiments%20demonstrate%20that%20our%20model%20not%20only%20excels%20in%0Atext-to-image%20generation%20and%20image%20editing%2C%20but%20also%20functions%20as%20a%20superior%0Aimage%20semantic%20evaluator%20with%20enhanced%20visual%20comprehension%20capabilities.%0AProject%20Page%3A%20https%3A//janus-pro-r1.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01480v2&entry.124074799=Read"},
{"title": "IF-VidCap: Can Video Caption Models Follow Instructions?", "author": "Shihao Li and Yuanxing Zhang and Jiangtao Wu and Zhide Lei and Yiwen He and Runzhe Wen and Chenxi Liao and Chengkang Jiang and An Ping and Shuo Gao and Suhan Wang and Zhaozhou Bian and Zijun Zhou and Jingyi Xie and Jiayi Zhou and Jing Wang and Yifan Yao and Weihao Xie and Yingshui Tan and Yanghai Wang and Qianqian Xie and Zhaoxiang Zhang and Jiaheng Liu", "abstract": "  Although Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in video captioning, practical applications require captions that\nfollow specific user instructions rather than generating exhaustive,\nunconstrained descriptions. Current benchmarks, however, primarily assess\ndescriptive comprehensiveness while largely overlooking instruction-following\ncapabilities. To address this gap, we introduce IF-VidCap, a new benchmark for\nevaluating controllable video captioning, which contains 1,400 high-quality\nsamples. Distinct from existing video captioning or general\ninstruction-following benchmarks, IF-VidCap incorporates a systematic framework\nthat assesses captions on two dimensions: format correctness and content\ncorrectness. Our comprehensive evaluation of over 20 prominent models reveals a\nnuanced landscape: despite the continued dominance of proprietary models, the\nperformance gap is closing, with top-tier open-source solutions now achieving\nnear-parity. Furthermore, we find that models specialized for dense captioning\nunderperform general-purpose MLLMs on complex instructions, indicating that\nfuture work should simultaneously advance both descriptive richness and\ninstruction-following fidelity.\n", "link": "http://arxiv.org/abs/2510.18726v1", "date": "2025-10-21", "relevancy": 2.7594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5632}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5632}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IF-VidCap%3A%20Can%20Video%20Caption%20Models%20Follow%20Instructions%3F&body=Title%3A%20IF-VidCap%3A%20Can%20Video%20Caption%20Models%20Follow%20Instructions%3F%0AAuthor%3A%20Shihao%20Li%20and%20Yuanxing%20Zhang%20and%20Jiangtao%20Wu%20and%20Zhide%20Lei%20and%20Yiwen%20He%20and%20Runzhe%20Wen%20and%20Chenxi%20Liao%20and%20Chengkang%20Jiang%20and%20An%20Ping%20and%20Shuo%20Gao%20and%20Suhan%20Wang%20and%20Zhaozhou%20Bian%20and%20Zijun%20Zhou%20and%20Jingyi%20Xie%20and%20Jiayi%20Zhou%20and%20Jing%20Wang%20and%20Yifan%20Yao%20and%20Weihao%20Xie%20and%20Yingshui%20Tan%20and%20Yanghai%20Wang%20and%20Qianqian%20Xie%20and%20Zhaoxiang%20Zhang%20and%20Jiaheng%20Liu%0AAbstract%3A%20%20%20Although%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%0Aproficiency%20in%20video%20captioning%2C%20practical%20applications%20require%20captions%20that%0Afollow%20specific%20user%20instructions%20rather%20than%20generating%20exhaustive%2C%0Aunconstrained%20descriptions.%20Current%20benchmarks%2C%20however%2C%20primarily%20assess%0Adescriptive%20comprehensiveness%20while%20largely%20overlooking%20instruction-following%0Acapabilities.%20To%20address%20this%20gap%2C%20we%20introduce%20IF-VidCap%2C%20a%20new%20benchmark%20for%0Aevaluating%20controllable%20video%20captioning%2C%20which%20contains%201%2C400%20high-quality%0Asamples.%20Distinct%20from%20existing%20video%20captioning%20or%20general%0Ainstruction-following%20benchmarks%2C%20IF-VidCap%20incorporates%20a%20systematic%20framework%0Athat%20assesses%20captions%20on%20two%20dimensions%3A%20format%20correctness%20and%20content%0Acorrectness.%20Our%20comprehensive%20evaluation%20of%20over%2020%20prominent%20models%20reveals%20a%0Anuanced%20landscape%3A%20despite%20the%20continued%20dominance%20of%20proprietary%20models%2C%20the%0Aperformance%20gap%20is%20closing%2C%20with%20top-tier%20open-source%20solutions%20now%20achieving%0Anear-parity.%20Furthermore%2C%20we%20find%20that%20models%20specialized%20for%20dense%20captioning%0Aunderperform%20general-purpose%20MLLMs%20on%20complex%20instructions%2C%20indicating%20that%0Afuture%20work%20should%20simultaneously%20advance%20both%20descriptive%20richness%20and%0Ainstruction-following%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIF-VidCap%253A%2520Can%2520Video%2520Caption%2520Models%2520Follow%2520Instructions%253F%26entry.906535625%3DShihao%2520Li%2520and%2520Yuanxing%2520Zhang%2520and%2520Jiangtao%2520Wu%2520and%2520Zhide%2520Lei%2520and%2520Yiwen%2520He%2520and%2520Runzhe%2520Wen%2520and%2520Chenxi%2520Liao%2520and%2520Chengkang%2520Jiang%2520and%2520An%2520Ping%2520and%2520Shuo%2520Gao%2520and%2520Suhan%2520Wang%2520and%2520Zhaozhou%2520Bian%2520and%2520Zijun%2520Zhou%2520and%2520Jingyi%2520Xie%2520and%2520Jiayi%2520Zhou%2520and%2520Jing%2520Wang%2520and%2520Yifan%2520Yao%2520and%2520Weihao%2520Xie%2520and%2520Yingshui%2520Tan%2520and%2520Yanghai%2520Wang%2520and%2520Qianqian%2520Xie%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Jiaheng%2520Liu%26entry.1292438233%3D%2520%2520Although%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%250Aproficiency%2520in%2520video%2520captioning%252C%2520practical%2520applications%2520require%2520captions%2520that%250Afollow%2520specific%2520user%2520instructions%2520rather%2520than%2520generating%2520exhaustive%252C%250Aunconstrained%2520descriptions.%2520Current%2520benchmarks%252C%2520however%252C%2520primarily%2520assess%250Adescriptive%2520comprehensiveness%2520while%2520largely%2520overlooking%2520instruction-following%250Acapabilities.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520IF-VidCap%252C%2520a%2520new%2520benchmark%2520for%250Aevaluating%2520controllable%2520video%2520captioning%252C%2520which%2520contains%25201%252C400%2520high-quality%250Asamples.%2520Distinct%2520from%2520existing%2520video%2520captioning%2520or%2520general%250Ainstruction-following%2520benchmarks%252C%2520IF-VidCap%2520incorporates%2520a%2520systematic%2520framework%250Athat%2520assesses%2520captions%2520on%2520two%2520dimensions%253A%2520format%2520correctness%2520and%2520content%250Acorrectness.%2520Our%2520comprehensive%2520evaluation%2520of%2520over%252020%2520prominent%2520models%2520reveals%2520a%250Anuanced%2520landscape%253A%2520despite%2520the%2520continued%2520dominance%2520of%2520proprietary%2520models%252C%2520the%250Aperformance%2520gap%2520is%2520closing%252C%2520with%2520top-tier%2520open-source%2520solutions%2520now%2520achieving%250Anear-parity.%2520Furthermore%252C%2520we%2520find%2520that%2520models%2520specialized%2520for%2520dense%2520captioning%250Aunderperform%2520general-purpose%2520MLLMs%2520on%2520complex%2520instructions%252C%2520indicating%2520that%250Afuture%2520work%2520should%2520simultaneously%2520advance%2520both%2520descriptive%2520richness%2520and%250Ainstruction-following%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IF-VidCap%3A%20Can%20Video%20Caption%20Models%20Follow%20Instructions%3F&entry.906535625=Shihao%20Li%20and%20Yuanxing%20Zhang%20and%20Jiangtao%20Wu%20and%20Zhide%20Lei%20and%20Yiwen%20He%20and%20Runzhe%20Wen%20and%20Chenxi%20Liao%20and%20Chengkang%20Jiang%20and%20An%20Ping%20and%20Shuo%20Gao%20and%20Suhan%20Wang%20and%20Zhaozhou%20Bian%20and%20Zijun%20Zhou%20and%20Jingyi%20Xie%20and%20Jiayi%20Zhou%20and%20Jing%20Wang%20and%20Yifan%20Yao%20and%20Weihao%20Xie%20and%20Yingshui%20Tan%20and%20Yanghai%20Wang%20and%20Qianqian%20Xie%20and%20Zhaoxiang%20Zhang%20and%20Jiaheng%20Liu&entry.1292438233=%20%20Although%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%0Aproficiency%20in%20video%20captioning%2C%20practical%20applications%20require%20captions%20that%0Afollow%20specific%20user%20instructions%20rather%20than%20generating%20exhaustive%2C%0Aunconstrained%20descriptions.%20Current%20benchmarks%2C%20however%2C%20primarily%20assess%0Adescriptive%20comprehensiveness%20while%20largely%20overlooking%20instruction-following%0Acapabilities.%20To%20address%20this%20gap%2C%20we%20introduce%20IF-VidCap%2C%20a%20new%20benchmark%20for%0Aevaluating%20controllable%20video%20captioning%2C%20which%20contains%201%2C400%20high-quality%0Asamples.%20Distinct%20from%20existing%20video%20captioning%20or%20general%0Ainstruction-following%20benchmarks%2C%20IF-VidCap%20incorporates%20a%20systematic%20framework%0Athat%20assesses%20captions%20on%20two%20dimensions%3A%20format%20correctness%20and%20content%0Acorrectness.%20Our%20comprehensive%20evaluation%20of%20over%2020%20prominent%20models%20reveals%20a%0Anuanced%20landscape%3A%20despite%20the%20continued%20dominance%20of%20proprietary%20models%2C%20the%0Aperformance%20gap%20is%20closing%2C%20with%20top-tier%20open-source%20solutions%20now%20achieving%0Anear-parity.%20Furthermore%2C%20we%20find%20that%20models%20specialized%20for%20dense%20captioning%0Aunderperform%20general-purpose%20MLLMs%20on%20complex%20instructions%2C%20indicating%20that%0Afuture%20work%20should%20simultaneously%20advance%20both%20descriptive%20richness%20and%0Ainstruction-following%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18726v1&entry.124074799=Read"},
{"title": "See the Text: From Tokenization to Visual Reading", "author": "Ling Xing and Alex Jinpeng Wang and Rui Yan and Hongyu Qu and Zechao Li and Jinhui Tang", "abstract": "  People see text. Humans read by recognizing words as visual objects,\nincluding their shapes, layouts, and patterns, before connecting them to\nmeaning, which enables us to handle typos, distorted fonts, and various scripts\neffectively. Modern large language models (LLMs), however, rely on subword\ntokenization, fragmenting text into pieces from a fixed vocabulary. While\neffective for high-resource languages, this approach over-segments low-resource\nlanguages, yielding long, linguistically meaningless sequences and inflating\ncomputation. In this work, we challenge this entrenched paradigm and move\ntoward a vision-centric alternative. Our method, SeeTok, renders text as images\n(visual-text) and leverages pretrained multimodal LLMs to interpret them,\nreusing strong OCR and text-vision alignment abilities learned from large-scale\nmultimodal training. Across three different language tasks, SeeTok matches or\nsurpasses subword tokenizers while requiring 4.43 times fewer tokens and\nreducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,\nrobustness to typographic noise, and linguistic hierarchy. SeeTok signals a\nshift from symbolic tokenization to human-like visual reading, and takes a step\ntoward more natural and cognitively inspired language models.\n", "link": "http://arxiv.org/abs/2510.18840v1", "date": "2025-10-21", "relevancy": 2.7463, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20the%20Text%3A%20From%20Tokenization%20to%20Visual%20Reading&body=Title%3A%20See%20the%20Text%3A%20From%20Tokenization%20to%20Visual%20Reading%0AAuthor%3A%20Ling%20Xing%20and%20Alex%20Jinpeng%20Wang%20and%20Rui%20Yan%20and%20Hongyu%20Qu%20and%20Zechao%20Li%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20People%20see%20text.%20Humans%20read%20by%20recognizing%20words%20as%20visual%20objects%2C%0Aincluding%20their%20shapes%2C%20layouts%2C%20and%20patterns%2C%20before%20connecting%20them%20to%0Ameaning%2C%20which%20enables%20us%20to%20handle%20typos%2C%20distorted%20fonts%2C%20and%20various%20scripts%0Aeffectively.%20Modern%20large%20language%20models%20%28LLMs%29%2C%20however%2C%20rely%20on%20subword%0Atokenization%2C%20fragmenting%20text%20into%20pieces%20from%20a%20fixed%20vocabulary.%20While%0Aeffective%20for%20high-resource%20languages%2C%20this%20approach%20over-segments%20low-resource%0Alanguages%2C%20yielding%20long%2C%20linguistically%20meaningless%20sequences%20and%20inflating%0Acomputation.%20In%20this%20work%2C%20we%20challenge%20this%20entrenched%20paradigm%20and%20move%0Atoward%20a%20vision-centric%20alternative.%20Our%20method%2C%20SeeTok%2C%20renders%20text%20as%20images%0A%28visual-text%29%20and%20leverages%20pretrained%20multimodal%20LLMs%20to%20interpret%20them%2C%0Areusing%20strong%20OCR%20and%20text-vision%20alignment%20abilities%20learned%20from%20large-scale%0Amultimodal%20training.%20Across%20three%20different%20language%20tasks%2C%20SeeTok%20matches%20or%0Asurpasses%20subword%20tokenizers%20while%20requiring%204.43%20times%20fewer%20tokens%20and%0Areducing%20FLOPs%20by%2070.5%25%2C%20with%20additional%20gains%20in%20cross-lingual%20generalization%2C%0Arobustness%20to%20typographic%20noise%2C%20and%20linguistic%20hierarchy.%20SeeTok%20signals%20a%0Ashift%20from%20symbolic%20tokenization%20to%20human-like%20visual%20reading%2C%20and%20takes%20a%20step%0Atoward%20more%20natural%20and%20cognitively%20inspired%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520the%2520Text%253A%2520From%2520Tokenization%2520to%2520Visual%2520Reading%26entry.906535625%3DLing%2520Xing%2520and%2520Alex%2520Jinpeng%2520Wang%2520and%2520Rui%2520Yan%2520and%2520Hongyu%2520Qu%2520and%2520Zechao%2520Li%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520People%2520see%2520text.%2520Humans%2520read%2520by%2520recognizing%2520words%2520as%2520visual%2520objects%252C%250Aincluding%2520their%2520shapes%252C%2520layouts%252C%2520and%2520patterns%252C%2520before%2520connecting%2520them%2520to%250Ameaning%252C%2520which%2520enables%2520us%2520to%2520handle%2520typos%252C%2520distorted%2520fonts%252C%2520and%2520various%2520scripts%250Aeffectively.%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520however%252C%2520rely%2520on%2520subword%250Atokenization%252C%2520fragmenting%2520text%2520into%2520pieces%2520from%2520a%2520fixed%2520vocabulary.%2520While%250Aeffective%2520for%2520high-resource%2520languages%252C%2520this%2520approach%2520over-segments%2520low-resource%250Alanguages%252C%2520yielding%2520long%252C%2520linguistically%2520meaningless%2520sequences%2520and%2520inflating%250Acomputation.%2520In%2520this%2520work%252C%2520we%2520challenge%2520this%2520entrenched%2520paradigm%2520and%2520move%250Atoward%2520a%2520vision-centric%2520alternative.%2520Our%2520method%252C%2520SeeTok%252C%2520renders%2520text%2520as%2520images%250A%2528visual-text%2529%2520and%2520leverages%2520pretrained%2520multimodal%2520LLMs%2520to%2520interpret%2520them%252C%250Areusing%2520strong%2520OCR%2520and%2520text-vision%2520alignment%2520abilities%2520learned%2520from%2520large-scale%250Amultimodal%2520training.%2520Across%2520three%2520different%2520language%2520tasks%252C%2520SeeTok%2520matches%2520or%250Asurpasses%2520subword%2520tokenizers%2520while%2520requiring%25204.43%2520times%2520fewer%2520tokens%2520and%250Areducing%2520FLOPs%2520by%252070.5%2525%252C%2520with%2520additional%2520gains%2520in%2520cross-lingual%2520generalization%252C%250Arobustness%2520to%2520typographic%2520noise%252C%2520and%2520linguistic%2520hierarchy.%2520SeeTok%2520signals%2520a%250Ashift%2520from%2520symbolic%2520tokenization%2520to%2520human-like%2520visual%2520reading%252C%2520and%2520takes%2520a%2520step%250Atoward%2520more%2520natural%2520and%2520cognitively%2520inspired%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20the%20Text%3A%20From%20Tokenization%20to%20Visual%20Reading&entry.906535625=Ling%20Xing%20and%20Alex%20Jinpeng%20Wang%20and%20Rui%20Yan%20and%20Hongyu%20Qu%20and%20Zechao%20Li%20and%20Jinhui%20Tang&entry.1292438233=%20%20People%20see%20text.%20Humans%20read%20by%20recognizing%20words%20as%20visual%20objects%2C%0Aincluding%20their%20shapes%2C%20layouts%2C%20and%20patterns%2C%20before%20connecting%20them%20to%0Ameaning%2C%20which%20enables%20us%20to%20handle%20typos%2C%20distorted%20fonts%2C%20and%20various%20scripts%0Aeffectively.%20Modern%20large%20language%20models%20%28LLMs%29%2C%20however%2C%20rely%20on%20subword%0Atokenization%2C%20fragmenting%20text%20into%20pieces%20from%20a%20fixed%20vocabulary.%20While%0Aeffective%20for%20high-resource%20languages%2C%20this%20approach%20over-segments%20low-resource%0Alanguages%2C%20yielding%20long%2C%20linguistically%20meaningless%20sequences%20and%20inflating%0Acomputation.%20In%20this%20work%2C%20we%20challenge%20this%20entrenched%20paradigm%20and%20move%0Atoward%20a%20vision-centric%20alternative.%20Our%20method%2C%20SeeTok%2C%20renders%20text%20as%20images%0A%28visual-text%29%20and%20leverages%20pretrained%20multimodal%20LLMs%20to%20interpret%20them%2C%0Areusing%20strong%20OCR%20and%20text-vision%20alignment%20abilities%20learned%20from%20large-scale%0Amultimodal%20training.%20Across%20three%20different%20language%20tasks%2C%20SeeTok%20matches%20or%0Asurpasses%20subword%20tokenizers%20while%20requiring%204.43%20times%20fewer%20tokens%20and%0Areducing%20FLOPs%20by%2070.5%25%2C%20with%20additional%20gains%20in%20cross-lingual%20generalization%2C%0Arobustness%20to%20typographic%20noise%2C%20and%20linguistic%20hierarchy.%20SeeTok%20signals%20a%0Ashift%20from%20symbolic%20tokenization%20to%20human-like%20visual%20reading%2C%20and%20takes%20a%20step%0Atoward%20more%20natural%20and%20cognitively%20inspired%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18840v1&entry.124074799=Read"},
{"title": "ITVTON: Virtual Try-On Diffusion Transformer Based on Integrated Image\n  and Text", "author": "Haifeng Ni and Ming Xu", "abstract": "  Virtual try-on, which aims to seamlessly fit garments onto person images, has\nrecently seen significant progress with diffusion-based models. However,\nexisting methods commonly resort to duplicated backbones or additional image\nencoders to extract garment features, which increases computational overhead\nand network complexity. In this paper, we propose ITVTON, an efficient\nframework that leverages the Diffusion Transformer (DiT) as its single\ngenerator to improve image fidelity. By concatenating garment and person images\nalong the width dimension and incorporating textual descriptions from both,\nITVTON effectively captures garment-person interactions while preserving\nrealism. To further reduce computational cost, we restrict training to the\nattention parameters within a single Diffusion Transformer (Single-DiT) block.\nExtensive experiments demonstrate that ITVTON surpasses baseline methods both\nqualitatively and quantitatively, setting a new standard for virtual try-on.\nMoreover, experiments on 10,257 image pairs from IGPair confirm its robustness\nin real-world scenarios.\n", "link": "http://arxiv.org/abs/2501.16757v3", "date": "2025-10-21", "relevancy": 2.6774, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6956}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6538}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ITVTON%3A%20Virtual%20Try-On%20Diffusion%20Transformer%20Based%20on%20Integrated%20Image%0A%20%20and%20Text&body=Title%3A%20ITVTON%3A%20Virtual%20Try-On%20Diffusion%20Transformer%20Based%20on%20Integrated%20Image%0A%20%20and%20Text%0AAuthor%3A%20Haifeng%20Ni%20and%20Ming%20Xu%0AAbstract%3A%20%20%20Virtual%20try-on%2C%20which%20aims%20to%20seamlessly%20fit%20garments%20onto%20person%20images%2C%20has%0Arecently%20seen%20significant%20progress%20with%20diffusion-based%20models.%20However%2C%0Aexisting%20methods%20commonly%20resort%20to%20duplicated%20backbones%20or%20additional%20image%0Aencoders%20to%20extract%20garment%20features%2C%20which%20increases%20computational%20overhead%0Aand%20network%20complexity.%20In%20this%20paper%2C%20we%20propose%20ITVTON%2C%20an%20efficient%0Aframework%20that%20leverages%20the%20Diffusion%20Transformer%20%28DiT%29%20as%20its%20single%0Agenerator%20to%20improve%20image%20fidelity.%20By%20concatenating%20garment%20and%20person%20images%0Aalong%20the%20width%20dimension%20and%20incorporating%20textual%20descriptions%20from%20both%2C%0AITVTON%20effectively%20captures%20garment-person%20interactions%20while%20preserving%0Arealism.%20To%20further%20reduce%20computational%20cost%2C%20we%20restrict%20training%20to%20the%0Aattention%20parameters%20within%20a%20single%20Diffusion%20Transformer%20%28Single-DiT%29%20block.%0AExtensive%20experiments%20demonstrate%20that%20ITVTON%20surpasses%20baseline%20methods%20both%0Aqualitatively%20and%20quantitatively%2C%20setting%20a%20new%20standard%20for%20virtual%20try-on.%0AMoreover%2C%20experiments%20on%2010%2C257%20image%20pairs%20from%20IGPair%20confirm%20its%20robustness%0Ain%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16757v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DITVTON%253A%2520Virtual%2520Try-On%2520Diffusion%2520Transformer%2520Based%2520on%2520Integrated%2520Image%250A%2520%2520and%2520Text%26entry.906535625%3DHaifeng%2520Ni%2520and%2520Ming%2520Xu%26entry.1292438233%3D%2520%2520Virtual%2520try-on%252C%2520which%2520aims%2520to%2520seamlessly%2520fit%2520garments%2520onto%2520person%2520images%252C%2520has%250Arecently%2520seen%2520significant%2520progress%2520with%2520diffusion-based%2520models.%2520However%252C%250Aexisting%2520methods%2520commonly%2520resort%2520to%2520duplicated%2520backbones%2520or%2520additional%2520image%250Aencoders%2520to%2520extract%2520garment%2520features%252C%2520which%2520increases%2520computational%2520overhead%250Aand%2520network%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ITVTON%252C%2520an%2520efficient%250Aframework%2520that%2520leverages%2520the%2520Diffusion%2520Transformer%2520%2528DiT%2529%2520as%2520its%2520single%250Agenerator%2520to%2520improve%2520image%2520fidelity.%2520By%2520concatenating%2520garment%2520and%2520person%2520images%250Aalong%2520the%2520width%2520dimension%2520and%2520incorporating%2520textual%2520descriptions%2520from%2520both%252C%250AITVTON%2520effectively%2520captures%2520garment-person%2520interactions%2520while%2520preserving%250Arealism.%2520To%2520further%2520reduce%2520computational%2520cost%252C%2520we%2520restrict%2520training%2520to%2520the%250Aattention%2520parameters%2520within%2520a%2520single%2520Diffusion%2520Transformer%2520%2528Single-DiT%2529%2520block.%250AExtensive%2520experiments%2520demonstrate%2520that%2520ITVTON%2520surpasses%2520baseline%2520methods%2520both%250Aqualitatively%2520and%2520quantitatively%252C%2520setting%2520a%2520new%2520standard%2520for%2520virtual%2520try-on.%250AMoreover%252C%2520experiments%2520on%252010%252C257%2520image%2520pairs%2520from%2520IGPair%2520confirm%2520its%2520robustness%250Ain%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16757v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ITVTON%3A%20Virtual%20Try-On%20Diffusion%20Transformer%20Based%20on%20Integrated%20Image%0A%20%20and%20Text&entry.906535625=Haifeng%20Ni%20and%20Ming%20Xu&entry.1292438233=%20%20Virtual%20try-on%2C%20which%20aims%20to%20seamlessly%20fit%20garments%20onto%20person%20images%2C%20has%0Arecently%20seen%20significant%20progress%20with%20diffusion-based%20models.%20However%2C%0Aexisting%20methods%20commonly%20resort%20to%20duplicated%20backbones%20or%20additional%20image%0Aencoders%20to%20extract%20garment%20features%2C%20which%20increases%20computational%20overhead%0Aand%20network%20complexity.%20In%20this%20paper%2C%20we%20propose%20ITVTON%2C%20an%20efficient%0Aframework%20that%20leverages%20the%20Diffusion%20Transformer%20%28DiT%29%20as%20its%20single%0Agenerator%20to%20improve%20image%20fidelity.%20By%20concatenating%20garment%20and%20person%20images%0Aalong%20the%20width%20dimension%20and%20incorporating%20textual%20descriptions%20from%20both%2C%0AITVTON%20effectively%20captures%20garment-person%20interactions%20while%20preserving%0Arealism.%20To%20further%20reduce%20computational%20cost%2C%20we%20restrict%20training%20to%20the%0Aattention%20parameters%20within%20a%20single%20Diffusion%20Transformer%20%28Single-DiT%29%20block.%0AExtensive%20experiments%20demonstrate%20that%20ITVTON%20surpasses%20baseline%20methods%20both%0Aqualitatively%20and%20quantitatively%2C%20setting%20a%20new%20standard%20for%20virtual%20try-on.%0AMoreover%2C%20experiments%20on%2010%2C257%20image%20pairs%20from%20IGPair%20confirm%20its%20robustness%0Ain%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16757v3&entry.124074799=Read"},
{"title": "SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction", "author": "Kaveh Moradkhani and R Jarrett Rushmore and Sylvain Bouix", "abstract": "  Accurate cortical surface reconstruction from magnetic resonance imaging\n(MRI) data is crucial for reliable neuroanatomical analyses. Current methods\nhave to contend with complex cortical geometries, strict topological\nrequirements, and often produce surfaces with overlaps, self-intersections, and\ntopological defects. To overcome these shortcomings, we introduce SimCortex, a\ndeep learning framework that simultaneously reconstructs all brain surfaces\n(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while\npreserving topological properties. Our method first segments the T1w image into\na nine-class tissue label map. From these segmentations, we generate\nsubject-specific, collision-free initial surface meshes. These surfaces serve\nas precise initializations for subsequent multiscale diffeomorphic\ndeformations. Employing stationary velocity fields (SVFs) integrated via\nscaling-and-squaring, our approach ensures smooth, topology-preserving\ntransformations with significantly reduced surface collisions and\nself-intersections. Evaluations on standard datasets demonstrate that SimCortex\ndramatically reduces surface overlaps and self-intersections, surpassing\ncurrent methods while maintaining state-of-the-art geometric accuracy.\n", "link": "http://arxiv.org/abs/2507.06955v2", "date": "2025-10-21", "relevancy": 2.6594, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction&body=Title%3A%20SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction%0AAuthor%3A%20Kaveh%20Moradkhani%20and%20R%20Jarrett%20Rushmore%20and%20Sylvain%20Bouix%0AAbstract%3A%20%20%20Accurate%20cortical%20surface%20reconstruction%20from%20magnetic%20resonance%20imaging%0A%28MRI%29%20data%20is%20crucial%20for%20reliable%20neuroanatomical%20analyses.%20Current%20methods%0Ahave%20to%20contend%20with%20complex%20cortical%20geometries%2C%20strict%20topological%0Arequirements%2C%20and%20often%20produce%20surfaces%20with%20overlaps%2C%20self-intersections%2C%20and%0Atopological%20defects.%20To%20overcome%20these%20shortcomings%2C%20we%20introduce%20SimCortex%2C%20a%0Adeep%20learning%20framework%20that%20simultaneously%20reconstructs%20all%20brain%20surfaces%0A%28left/right%20white-matter%20and%20pial%29%20from%20T1-weighted%28T1w%29%20MRI%20volumes%20while%0Apreserving%20topological%20properties.%20Our%20method%20first%20segments%20the%20T1w%20image%20into%0Aa%20nine-class%20tissue%20label%20map.%20From%20these%20segmentations%2C%20we%20generate%0Asubject-specific%2C%20collision-free%20initial%20surface%20meshes.%20These%20surfaces%20serve%0Aas%20precise%20initializations%20for%20subsequent%20multiscale%20diffeomorphic%0Adeformations.%20Employing%20stationary%20velocity%20fields%20%28SVFs%29%20integrated%20via%0Ascaling-and-squaring%2C%20our%20approach%20ensures%20smooth%2C%20topology-preserving%0Atransformations%20with%20significantly%20reduced%20surface%20collisions%20and%0Aself-intersections.%20Evaluations%20on%20standard%20datasets%20demonstrate%20that%20SimCortex%0Adramatically%20reduces%20surface%20overlaps%20and%20self-intersections%2C%20surpassing%0Acurrent%20methods%20while%20maintaining%20state-of-the-art%20geometric%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimCortex%253A%2520Collision-free%2520Simultaneous%2520Cortical%2520Surfaces%2520Reconstruction%26entry.906535625%3DKaveh%2520Moradkhani%2520and%2520R%2520Jarrett%2520Rushmore%2520and%2520Sylvain%2520Bouix%26entry.1292438233%3D%2520%2520Accurate%2520cortical%2520surface%2520reconstruction%2520from%2520magnetic%2520resonance%2520imaging%250A%2528MRI%2529%2520data%2520is%2520crucial%2520for%2520reliable%2520neuroanatomical%2520analyses.%2520Current%2520methods%250Ahave%2520to%2520contend%2520with%2520complex%2520cortical%2520geometries%252C%2520strict%2520topological%250Arequirements%252C%2520and%2520often%2520produce%2520surfaces%2520with%2520overlaps%252C%2520self-intersections%252C%2520and%250Atopological%2520defects.%2520To%2520overcome%2520these%2520shortcomings%252C%2520we%2520introduce%2520SimCortex%252C%2520a%250Adeep%2520learning%2520framework%2520that%2520simultaneously%2520reconstructs%2520all%2520brain%2520surfaces%250A%2528left/right%2520white-matter%2520and%2520pial%2529%2520from%2520T1-weighted%2528T1w%2529%2520MRI%2520volumes%2520while%250Apreserving%2520topological%2520properties.%2520Our%2520method%2520first%2520segments%2520the%2520T1w%2520image%2520into%250Aa%2520nine-class%2520tissue%2520label%2520map.%2520From%2520these%2520segmentations%252C%2520we%2520generate%250Asubject-specific%252C%2520collision-free%2520initial%2520surface%2520meshes.%2520These%2520surfaces%2520serve%250Aas%2520precise%2520initializations%2520for%2520subsequent%2520multiscale%2520diffeomorphic%250Adeformations.%2520Employing%2520stationary%2520velocity%2520fields%2520%2528SVFs%2529%2520integrated%2520via%250Ascaling-and-squaring%252C%2520our%2520approach%2520ensures%2520smooth%252C%2520topology-preserving%250Atransformations%2520with%2520significantly%2520reduced%2520surface%2520collisions%2520and%250Aself-intersections.%2520Evaluations%2520on%2520standard%2520datasets%2520demonstrate%2520that%2520SimCortex%250Adramatically%2520reduces%2520surface%2520overlaps%2520and%2520self-intersections%252C%2520surpassing%250Acurrent%2520methods%2520while%2520maintaining%2520state-of-the-art%2520geometric%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimCortex%3A%20Collision-free%20Simultaneous%20Cortical%20Surfaces%20Reconstruction&entry.906535625=Kaveh%20Moradkhani%20and%20R%20Jarrett%20Rushmore%20and%20Sylvain%20Bouix&entry.1292438233=%20%20Accurate%20cortical%20surface%20reconstruction%20from%20magnetic%20resonance%20imaging%0A%28MRI%29%20data%20is%20crucial%20for%20reliable%20neuroanatomical%20analyses.%20Current%20methods%0Ahave%20to%20contend%20with%20complex%20cortical%20geometries%2C%20strict%20topological%0Arequirements%2C%20and%20often%20produce%20surfaces%20with%20overlaps%2C%20self-intersections%2C%20and%0Atopological%20defects.%20To%20overcome%20these%20shortcomings%2C%20we%20introduce%20SimCortex%2C%20a%0Adeep%20learning%20framework%20that%20simultaneously%20reconstructs%20all%20brain%20surfaces%0A%28left/right%20white-matter%20and%20pial%29%20from%20T1-weighted%28T1w%29%20MRI%20volumes%20while%0Apreserving%20topological%20properties.%20Our%20method%20first%20segments%20the%20T1w%20image%20into%0Aa%20nine-class%20tissue%20label%20map.%20From%20these%20segmentations%2C%20we%20generate%0Asubject-specific%2C%20collision-free%20initial%20surface%20meshes.%20These%20surfaces%20serve%0Aas%20precise%20initializations%20for%20subsequent%20multiscale%20diffeomorphic%0Adeformations.%20Employing%20stationary%20velocity%20fields%20%28SVFs%29%20integrated%20via%0Ascaling-and-squaring%2C%20our%20approach%20ensures%20smooth%2C%20topology-preserving%0Atransformations%20with%20significantly%20reduced%20surface%20collisions%20and%0Aself-intersections.%20Evaluations%20on%20standard%20datasets%20demonstrate%20that%20SimCortex%0Adramatically%20reduces%20surface%20overlaps%20and%20self-intersections%2C%20surpassing%0Acurrent%20methods%20while%20maintaining%20state-of-the-art%20geometric%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06955v2&entry.124074799=Read"},
{"title": "AstroMMBench: A Benchmark for Evaluating Multimodal Large Language\n  Models Capabilities in Astronomy", "author": "Jinghang Shi and Xiaoyu Tang and Yang Huang and Yuyang Li and Xiao Kong and Yanxia Zhang and Caizhan Yue", "abstract": "  Astronomical image interpretation presents a significant challenge for\napplying multimodal large language models (MLLMs) to specialized scientific\ntasks. Existing benchmarks focus on general multimodal capabilities but fail to\ncapture the complexity of astronomical data. To bridge this gap, we introduce\nAstroMMBench, the first comprehensive benchmark designed to evaluate MLLMs in\nastronomical image understanding. AstroMMBench comprises 621 multiple-choice\nquestions across six astrophysical subfields, curated and reviewed by 15 domain\nexperts for quality and relevance. We conducted an extensive evaluation of 25\ndiverse MLLMs, including 22 open-source and 3 closed-source models, using\nAstroMMBench. The results show that Ovis2-34B achieved the highest overall\naccuracy (70.5%), demonstrating leading capabilities even compared to strong\nclosed-source models. Performance showed variations across the six\nastrophysical subfields, proving particularly challenging in domains like\ncosmology and high-energy astrophysics, while models performed relatively\nbetter in others, such as instrumentation and solar astrophysics. These\nfindings underscore the vital role of domain-specific benchmarks like\nAstroMMBench in critically evaluating MLLM performance and guiding their\ntargeted development for scientific applications. AstroMMBench provides a\nfoundational resource and a dynamic tool to catalyze advancements at the\nintersection of AI and astronomy.\n", "link": "http://arxiv.org/abs/2510.00063v2", "date": "2025-10-21", "relevancy": 2.6014, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AstroMMBench%3A%20A%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%0A%20%20Models%20Capabilities%20in%20Astronomy&body=Title%3A%20AstroMMBench%3A%20A%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%0A%20%20Models%20Capabilities%20in%20Astronomy%0AAuthor%3A%20Jinghang%20Shi%20and%20Xiaoyu%20Tang%20and%20Yang%20Huang%20and%20Yuyang%20Li%20and%20Xiao%20Kong%20and%20Yanxia%20Zhang%20and%20Caizhan%20Yue%0AAbstract%3A%20%20%20Astronomical%20image%20interpretation%20presents%20a%20significant%20challenge%20for%0Aapplying%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20specialized%20scientific%0Atasks.%20Existing%20benchmarks%20focus%20on%20general%20multimodal%20capabilities%20but%20fail%20to%0Acapture%20the%20complexity%20of%20astronomical%20data.%20To%20bridge%20this%20gap%2C%20we%20introduce%0AAstroMMBench%2C%20the%20first%20comprehensive%20benchmark%20designed%20to%20evaluate%20MLLMs%20in%0Aastronomical%20image%20understanding.%20AstroMMBench%20comprises%20621%20multiple-choice%0Aquestions%20across%20six%20astrophysical%20subfields%2C%20curated%20and%20reviewed%20by%2015%20domain%0Aexperts%20for%20quality%20and%20relevance.%20We%20conducted%20an%20extensive%20evaluation%20of%2025%0Adiverse%20MLLMs%2C%20including%2022%20open-source%20and%203%20closed-source%20models%2C%20using%0AAstroMMBench.%20The%20results%20show%20that%20Ovis2-34B%20achieved%20the%20highest%20overall%0Aaccuracy%20%2870.5%25%29%2C%20demonstrating%20leading%20capabilities%20even%20compared%20to%20strong%0Aclosed-source%20models.%20Performance%20showed%20variations%20across%20the%20six%0Aastrophysical%20subfields%2C%20proving%20particularly%20challenging%20in%20domains%20like%0Acosmology%20and%20high-energy%20astrophysics%2C%20while%20models%20performed%20relatively%0Abetter%20in%20others%2C%20such%20as%20instrumentation%20and%20solar%20astrophysics.%20These%0Afindings%20underscore%20the%20vital%20role%20of%20domain-specific%20benchmarks%20like%0AAstroMMBench%20in%20critically%20evaluating%20MLLM%20performance%20and%20guiding%20their%0Atargeted%20development%20for%20scientific%20applications.%20AstroMMBench%20provides%20a%0Afoundational%20resource%20and%20a%20dynamic%20tool%20to%20catalyze%20advancements%20at%20the%0Aintersection%20of%20AI%20and%20astronomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00063v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAstroMMBench%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%2520Capabilities%2520in%2520Astronomy%26entry.906535625%3DJinghang%2520Shi%2520and%2520Xiaoyu%2520Tang%2520and%2520Yang%2520Huang%2520and%2520Yuyang%2520Li%2520and%2520Xiao%2520Kong%2520and%2520Yanxia%2520Zhang%2520and%2520Caizhan%2520Yue%26entry.1292438233%3D%2520%2520Astronomical%2520image%2520interpretation%2520presents%2520a%2520significant%2520challenge%2520for%250Aapplying%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520specialized%2520scientific%250Atasks.%2520Existing%2520benchmarks%2520focus%2520on%2520general%2520multimodal%2520capabilities%2520but%2520fail%2520to%250Acapture%2520the%2520complexity%2520of%2520astronomical%2520data.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%250AAstroMMBench%252C%2520the%2520first%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520MLLMs%2520in%250Aastronomical%2520image%2520understanding.%2520AstroMMBench%2520comprises%2520621%2520multiple-choice%250Aquestions%2520across%2520six%2520astrophysical%2520subfields%252C%2520curated%2520and%2520reviewed%2520by%252015%2520domain%250Aexperts%2520for%2520quality%2520and%2520relevance.%2520We%2520conducted%2520an%2520extensive%2520evaluation%2520of%252025%250Adiverse%2520MLLMs%252C%2520including%252022%2520open-source%2520and%25203%2520closed-source%2520models%252C%2520using%250AAstroMMBench.%2520The%2520results%2520show%2520that%2520Ovis2-34B%2520achieved%2520the%2520highest%2520overall%250Aaccuracy%2520%252870.5%2525%2529%252C%2520demonstrating%2520leading%2520capabilities%2520even%2520compared%2520to%2520strong%250Aclosed-source%2520models.%2520Performance%2520showed%2520variations%2520across%2520the%2520six%250Aastrophysical%2520subfields%252C%2520proving%2520particularly%2520challenging%2520in%2520domains%2520like%250Acosmology%2520and%2520high-energy%2520astrophysics%252C%2520while%2520models%2520performed%2520relatively%250Abetter%2520in%2520others%252C%2520such%2520as%2520instrumentation%2520and%2520solar%2520astrophysics.%2520These%250Afindings%2520underscore%2520the%2520vital%2520role%2520of%2520domain-specific%2520benchmarks%2520like%250AAstroMMBench%2520in%2520critically%2520evaluating%2520MLLM%2520performance%2520and%2520guiding%2520their%250Atargeted%2520development%2520for%2520scientific%2520applications.%2520AstroMMBench%2520provides%2520a%250Afoundational%2520resource%2520and%2520a%2520dynamic%2520tool%2520to%2520catalyze%2520advancements%2520at%2520the%250Aintersection%2520of%2520AI%2520and%2520astronomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00063v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AstroMMBench%3A%20A%20Benchmark%20for%20Evaluating%20Multimodal%20Large%20Language%0A%20%20Models%20Capabilities%20in%20Astronomy&entry.906535625=Jinghang%20Shi%20and%20Xiaoyu%20Tang%20and%20Yang%20Huang%20and%20Yuyang%20Li%20and%20Xiao%20Kong%20and%20Yanxia%20Zhang%20and%20Caizhan%20Yue&entry.1292438233=%20%20Astronomical%20image%20interpretation%20presents%20a%20significant%20challenge%20for%0Aapplying%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20specialized%20scientific%0Atasks.%20Existing%20benchmarks%20focus%20on%20general%20multimodal%20capabilities%20but%20fail%20to%0Acapture%20the%20complexity%20of%20astronomical%20data.%20To%20bridge%20this%20gap%2C%20we%20introduce%0AAstroMMBench%2C%20the%20first%20comprehensive%20benchmark%20designed%20to%20evaluate%20MLLMs%20in%0Aastronomical%20image%20understanding.%20AstroMMBench%20comprises%20621%20multiple-choice%0Aquestions%20across%20six%20astrophysical%20subfields%2C%20curated%20and%20reviewed%20by%2015%20domain%0Aexperts%20for%20quality%20and%20relevance.%20We%20conducted%20an%20extensive%20evaluation%20of%2025%0Adiverse%20MLLMs%2C%20including%2022%20open-source%20and%203%20closed-source%20models%2C%20using%0AAstroMMBench.%20The%20results%20show%20that%20Ovis2-34B%20achieved%20the%20highest%20overall%0Aaccuracy%20%2870.5%25%29%2C%20demonstrating%20leading%20capabilities%20even%20compared%20to%20strong%0Aclosed-source%20models.%20Performance%20showed%20variations%20across%20the%20six%0Aastrophysical%20subfields%2C%20proving%20particularly%20challenging%20in%20domains%20like%0Acosmology%20and%20high-energy%20astrophysics%2C%20while%20models%20performed%20relatively%0Abetter%20in%20others%2C%20such%20as%20instrumentation%20and%20solar%20astrophysics.%20These%0Afindings%20underscore%20the%20vital%20role%20of%20domain-specific%20benchmarks%20like%0AAstroMMBench%20in%20critically%20evaluating%20MLLM%20performance%20and%20guiding%20their%0Atargeted%20development%20for%20scientific%20applications.%20AstroMMBench%20provides%20a%0Afoundational%20resource%20and%20a%20dynamic%20tool%20to%20catalyze%20advancements%20at%20the%0Aintersection%20of%20AI%20and%20astronomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00063v2&entry.124074799=Read"},
{"title": "SEAL: Semantic-Aware Hierarchical Learning for Generalized Category\n  Discovery", "author": "Zhenqi He and Yuanpei Liu and Kai Han", "abstract": "  This paper investigates the problem of Generalized Category Discovery (GCD).\nGiven a partially labelled dataset, GCD aims to categorize all unlabelled\nimages, regardless of whether they belong to known or unknown classes. Existing\napproaches typically depend on either single-level semantics or manually\ndesigned abstract hierarchies, which limit their generalizability and\nscalability. To address these limitations, we introduce a SEmantic-aware\nhierArchical Learning framework (SEAL), guided by naturally occurring and\neasily accessible hierarchical structures. Within SEAL, we propose a\nHierarchical Semantic-Guided Soft Contrastive Learning approach that exploits\nhierarchical similarity to generate informative soft negatives, addressing the\nlimitations of conventional contrastive losses that treat all negatives\nequally. Furthermore, a Cross-Granularity Consistency (CGC) module is designed\nto align the predictions from different levels of granularity. SEAL\nconsistently achieves state-of-the-art performance on fine-grained benchmarks,\nincluding the SSB benchmark, Oxford-Pet, and the Herbarium19 dataset, and\nfurther demonstrates generalization on coarse-grained datasets. Project page:\nhttps://visual-ai.github.io/seal/\n", "link": "http://arxiv.org/abs/2510.18740v1", "date": "2025-10-21", "relevancy": 2.576, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAL%3A%20Semantic-Aware%20Hierarchical%20Learning%20for%20Generalized%20Category%0A%20%20Discovery&body=Title%3A%20SEAL%3A%20Semantic-Aware%20Hierarchical%20Learning%20for%20Generalized%20Category%0A%20%20Discovery%0AAuthor%3A%20Zhenqi%20He%20and%20Yuanpei%20Liu%20and%20Kai%20Han%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20problem%20of%20Generalized%20Category%20Discovery%20%28GCD%29.%0AGiven%20a%20partially%20labelled%20dataset%2C%20GCD%20aims%20to%20categorize%20all%20unlabelled%0Aimages%2C%20regardless%20of%20whether%20they%20belong%20to%20known%20or%20unknown%20classes.%20Existing%0Aapproaches%20typically%20depend%20on%20either%20single-level%20semantics%20or%20manually%0Adesigned%20abstract%20hierarchies%2C%20which%20limit%20their%20generalizability%20and%0Ascalability.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20SEmantic-aware%0AhierArchical%20Learning%20framework%20%28SEAL%29%2C%20guided%20by%20naturally%20occurring%20and%0Aeasily%20accessible%20hierarchical%20structures.%20Within%20SEAL%2C%20we%20propose%20a%0AHierarchical%20Semantic-Guided%20Soft%20Contrastive%20Learning%20approach%20that%20exploits%0Ahierarchical%20similarity%20to%20generate%20informative%20soft%20negatives%2C%20addressing%20the%0Alimitations%20of%20conventional%20contrastive%20losses%20that%20treat%20all%20negatives%0Aequally.%20Furthermore%2C%20a%20Cross-Granularity%20Consistency%20%28CGC%29%20module%20is%20designed%0Ato%20align%20the%20predictions%20from%20different%20levels%20of%20granularity.%20SEAL%0Aconsistently%20achieves%20state-of-the-art%20performance%20on%20fine-grained%20benchmarks%2C%0Aincluding%20the%20SSB%20benchmark%2C%20Oxford-Pet%2C%20and%20the%20Herbarium19%20dataset%2C%20and%0Afurther%20demonstrates%20generalization%20on%20coarse-grained%20datasets.%20Project%20page%3A%0Ahttps%3A//visual-ai.github.io/seal/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAL%253A%2520Semantic-Aware%2520Hierarchical%2520Learning%2520for%2520Generalized%2520Category%250A%2520%2520Discovery%26entry.906535625%3DZhenqi%2520He%2520and%2520Yuanpei%2520Liu%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520problem%2520of%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529.%250AGiven%2520a%2520partially%2520labelled%2520dataset%252C%2520GCD%2520aims%2520to%2520categorize%2520all%2520unlabelled%250Aimages%252C%2520regardless%2520of%2520whether%2520they%2520belong%2520to%2520known%2520or%2520unknown%2520classes.%2520Existing%250Aapproaches%2520typically%2520depend%2520on%2520either%2520single-level%2520semantics%2520or%2520manually%250Adesigned%2520abstract%2520hierarchies%252C%2520which%2520limit%2520their%2520generalizability%2520and%250Ascalability.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520a%2520SEmantic-aware%250AhierArchical%2520Learning%2520framework%2520%2528SEAL%2529%252C%2520guided%2520by%2520naturally%2520occurring%2520and%250Aeasily%2520accessible%2520hierarchical%2520structures.%2520Within%2520SEAL%252C%2520we%2520propose%2520a%250AHierarchical%2520Semantic-Guided%2520Soft%2520Contrastive%2520Learning%2520approach%2520that%2520exploits%250Ahierarchical%2520similarity%2520to%2520generate%2520informative%2520soft%2520negatives%252C%2520addressing%2520the%250Alimitations%2520of%2520conventional%2520contrastive%2520losses%2520that%2520treat%2520all%2520negatives%250Aequally.%2520Furthermore%252C%2520a%2520Cross-Granularity%2520Consistency%2520%2528CGC%2529%2520module%2520is%2520designed%250Ato%2520align%2520the%2520predictions%2520from%2520different%2520levels%2520of%2520granularity.%2520SEAL%250Aconsistently%2520achieves%2520state-of-the-art%2520performance%2520on%2520fine-grained%2520benchmarks%252C%250Aincluding%2520the%2520SSB%2520benchmark%252C%2520Oxford-Pet%252C%2520and%2520the%2520Herbarium19%2520dataset%252C%2520and%250Afurther%2520demonstrates%2520generalization%2520on%2520coarse-grained%2520datasets.%2520Project%2520page%253A%250Ahttps%253A//visual-ai.github.io/seal/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAL%3A%20Semantic-Aware%20Hierarchical%20Learning%20for%20Generalized%20Category%0A%20%20Discovery&entry.906535625=Zhenqi%20He%20and%20Yuanpei%20Liu%20and%20Kai%20Han&entry.1292438233=%20%20This%20paper%20investigates%20the%20problem%20of%20Generalized%20Category%20Discovery%20%28GCD%29.%0AGiven%20a%20partially%20labelled%20dataset%2C%20GCD%20aims%20to%20categorize%20all%20unlabelled%0Aimages%2C%20regardless%20of%20whether%20they%20belong%20to%20known%20or%20unknown%20classes.%20Existing%0Aapproaches%20typically%20depend%20on%20either%20single-level%20semantics%20or%20manually%0Adesigned%20abstract%20hierarchies%2C%20which%20limit%20their%20generalizability%20and%0Ascalability.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20SEmantic-aware%0AhierArchical%20Learning%20framework%20%28SEAL%29%2C%20guided%20by%20naturally%20occurring%20and%0Aeasily%20accessible%20hierarchical%20structures.%20Within%20SEAL%2C%20we%20propose%20a%0AHierarchical%20Semantic-Guided%20Soft%20Contrastive%20Learning%20approach%20that%20exploits%0Ahierarchical%20similarity%20to%20generate%20informative%20soft%20negatives%2C%20addressing%20the%0Alimitations%20of%20conventional%20contrastive%20losses%20that%20treat%20all%20negatives%0Aequally.%20Furthermore%2C%20a%20Cross-Granularity%20Consistency%20%28CGC%29%20module%20is%20designed%0Ato%20align%20the%20predictions%20from%20different%20levels%20of%20granularity.%20SEAL%0Aconsistently%20achieves%20state-of-the-art%20performance%20on%20fine-grained%20benchmarks%2C%0Aincluding%20the%20SSB%20benchmark%2C%20Oxford-Pet%2C%20and%20the%20Herbarium19%20dataset%2C%20and%0Afurther%20demonstrates%20generalization%20on%20coarse-grained%20datasets.%20Project%20page%3A%0Ahttps%3A//visual-ai.github.io/seal/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18740v1&entry.124074799=Read"},
{"title": "Understanding In-Context Learning on Structured Manifolds: Bridging\n  Attention to Kernel Methods", "author": "Zhaiming Shen and Alexander Hsu and Rongjie Lai and Wenjing Liao", "abstract": "  While in-context learning (ICL) has achieved remarkable success in natural\nlanguage and vision domains, its theoretical understanding-particularly in the\ncontext of structured geometric data-remains unexplored. This paper initiates a\ntheoretical study of ICL for regression of H\\\"older functions on manifolds. We\nestablish a novel connection between the attention mechanism and classical\nkernel methods, demonstrating that transformers effectively perform\nkernel-based prediction at a new query through its interaction with the prompt.\nThis connection is validated by numerical experiments, revealing that the\nlearned query-prompt scores for H\\\"older functions are highly correlated with\nthe Gaussian kernel. Building on this insight, we derive generalization error\nbounds in terms of the prompt length and the number of training tasks. When a\nsufficient number of training tasks are observed, transformers give rise to the\nminimax regression rate of H\\\"older functions on manifolds, which scales\nexponentially with the intrinsic dimension of the manifold, rather than the\nambient space dimension. Our result also characterizes how the generalization\nerror scales with the number of training tasks, shedding light on the\ncomplexity of transformers as in-context kernel algorithm learners. Our\nfindings provide foundational insights into the role of geometry in ICL and\nnovels tools to study ICL of nonlinear models.\n", "link": "http://arxiv.org/abs/2506.10959v2", "date": "2025-10-21", "relevancy": 2.5713, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods&body=Title%3A%20Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods%0AAuthor%3A%20Zhaiming%20Shen%20and%20Alexander%20Hsu%20and%20Rongjie%20Lai%20and%20Wenjing%20Liao%0AAbstract%3A%20%20%20While%20in-context%20learning%20%28ICL%29%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20and%20vision%20domains%2C%20its%20theoretical%20understanding-particularly%20in%20the%0Acontext%20of%20structured%20geometric%20data-remains%20unexplored.%20This%20paper%20initiates%20a%0Atheoretical%20study%20of%20ICL%20for%20regression%20of%20H%5C%22older%20functions%20on%20manifolds.%20We%0Aestablish%20a%20novel%20connection%20between%20the%20attention%20mechanism%20and%20classical%0Akernel%20methods%2C%20demonstrating%20that%20transformers%20effectively%20perform%0Akernel-based%20prediction%20at%20a%20new%20query%20through%20its%20interaction%20with%20the%20prompt.%0AThis%20connection%20is%20validated%20by%20numerical%20experiments%2C%20revealing%20that%20the%0Alearned%20query-prompt%20scores%20for%20H%5C%22older%20functions%20are%20highly%20correlated%20with%0Athe%20Gaussian%20kernel.%20Building%20on%20this%20insight%2C%20we%20derive%20generalization%20error%0Abounds%20in%20terms%20of%20the%20prompt%20length%20and%20the%20number%20of%20training%20tasks.%20When%20a%0Asufficient%20number%20of%20training%20tasks%20are%20observed%2C%20transformers%20give%20rise%20to%20the%0Aminimax%20regression%20rate%20of%20H%5C%22older%20functions%20on%20manifolds%2C%20which%20scales%0Aexponentially%20with%20the%20intrinsic%20dimension%20of%20the%20manifold%2C%20rather%20than%20the%0Aambient%20space%20dimension.%20Our%20result%20also%20characterizes%20how%20the%20generalization%0Aerror%20scales%20with%20the%20number%20of%20training%20tasks%2C%20shedding%20light%20on%20the%0Acomplexity%20of%20transformers%20as%20in-context%20kernel%20algorithm%20learners.%20Our%0Afindings%20provide%20foundational%20insights%20into%20the%20role%20of%20geometry%20in%20ICL%20and%0Anovels%20tools%20to%20study%20ICL%20of%20nonlinear%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520In-Context%2520Learning%2520on%2520Structured%2520Manifolds%253A%2520Bridging%250A%2520%2520Attention%2520to%2520Kernel%2520Methods%26entry.906535625%3DZhaiming%2520Shen%2520and%2520Alexander%2520Hsu%2520and%2520Rongjie%2520Lai%2520and%2520Wenjing%2520Liao%26entry.1292438233%3D%2520%2520While%2520in-context%2520learning%2520%2528ICL%2529%2520has%2520achieved%2520remarkable%2520success%2520in%2520natural%250Alanguage%2520and%2520vision%2520domains%252C%2520its%2520theoretical%2520understanding-particularly%2520in%2520the%250Acontext%2520of%2520structured%2520geometric%2520data-remains%2520unexplored.%2520This%2520paper%2520initiates%2520a%250Atheoretical%2520study%2520of%2520ICL%2520for%2520regression%2520of%2520H%255C%2522older%2520functions%2520on%2520manifolds.%2520We%250Aestablish%2520a%2520novel%2520connection%2520between%2520the%2520attention%2520mechanism%2520and%2520classical%250Akernel%2520methods%252C%2520demonstrating%2520that%2520transformers%2520effectively%2520perform%250Akernel-based%2520prediction%2520at%2520a%2520new%2520query%2520through%2520its%2520interaction%2520with%2520the%2520prompt.%250AThis%2520connection%2520is%2520validated%2520by%2520numerical%2520experiments%252C%2520revealing%2520that%2520the%250Alearned%2520query-prompt%2520scores%2520for%2520H%255C%2522older%2520functions%2520are%2520highly%2520correlated%2520with%250Athe%2520Gaussian%2520kernel.%2520Building%2520on%2520this%2520insight%252C%2520we%2520derive%2520generalization%2520error%250Abounds%2520in%2520terms%2520of%2520the%2520prompt%2520length%2520and%2520the%2520number%2520of%2520training%2520tasks.%2520When%2520a%250Asufficient%2520number%2520of%2520training%2520tasks%2520are%2520observed%252C%2520transformers%2520give%2520rise%2520to%2520the%250Aminimax%2520regression%2520rate%2520of%2520H%255C%2522older%2520functions%2520on%2520manifolds%252C%2520which%2520scales%250Aexponentially%2520with%2520the%2520intrinsic%2520dimension%2520of%2520the%2520manifold%252C%2520rather%2520than%2520the%250Aambient%2520space%2520dimension.%2520Our%2520result%2520also%2520characterizes%2520how%2520the%2520generalization%250Aerror%2520scales%2520with%2520the%2520number%2520of%2520training%2520tasks%252C%2520shedding%2520light%2520on%2520the%250Acomplexity%2520of%2520transformers%2520as%2520in-context%2520kernel%2520algorithm%2520learners.%2520Our%250Afindings%2520provide%2520foundational%2520insights%2520into%2520the%2520role%2520of%2520geometry%2520in%2520ICL%2520and%250Anovels%2520tools%2520to%2520study%2520ICL%2520of%2520nonlinear%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20In-Context%20Learning%20on%20Structured%20Manifolds%3A%20Bridging%0A%20%20Attention%20to%20Kernel%20Methods&entry.906535625=Zhaiming%20Shen%20and%20Alexander%20Hsu%20and%20Rongjie%20Lai%20and%20Wenjing%20Liao&entry.1292438233=%20%20While%20in-context%20learning%20%28ICL%29%20has%20achieved%20remarkable%20success%20in%20natural%0Alanguage%20and%20vision%20domains%2C%20its%20theoretical%20understanding-particularly%20in%20the%0Acontext%20of%20structured%20geometric%20data-remains%20unexplored.%20This%20paper%20initiates%20a%0Atheoretical%20study%20of%20ICL%20for%20regression%20of%20H%5C%22older%20functions%20on%20manifolds.%20We%0Aestablish%20a%20novel%20connection%20between%20the%20attention%20mechanism%20and%20classical%0Akernel%20methods%2C%20demonstrating%20that%20transformers%20effectively%20perform%0Akernel-based%20prediction%20at%20a%20new%20query%20through%20its%20interaction%20with%20the%20prompt.%0AThis%20connection%20is%20validated%20by%20numerical%20experiments%2C%20revealing%20that%20the%0Alearned%20query-prompt%20scores%20for%20H%5C%22older%20functions%20are%20highly%20correlated%20with%0Athe%20Gaussian%20kernel.%20Building%20on%20this%20insight%2C%20we%20derive%20generalization%20error%0Abounds%20in%20terms%20of%20the%20prompt%20length%20and%20the%20number%20of%20training%20tasks.%20When%20a%0Asufficient%20number%20of%20training%20tasks%20are%20observed%2C%20transformers%20give%20rise%20to%20the%0Aminimax%20regression%20rate%20of%20H%5C%22older%20functions%20on%20manifolds%2C%20which%20scales%0Aexponentially%20with%20the%20intrinsic%20dimension%20of%20the%20manifold%2C%20rather%20than%20the%0Aambient%20space%20dimension.%20Our%20result%20also%20characterizes%20how%20the%20generalization%0Aerror%20scales%20with%20the%20number%20of%20training%20tasks%2C%20shedding%20light%20on%20the%0Acomplexity%20of%20transformers%20as%20in-context%20kernel%20algorithm%20learners.%20Our%0Afindings%20provide%20foundational%20insights%20into%20the%20role%20of%20geometry%20in%20ICL%20and%0Anovels%20tools%20to%20study%20ICL%20of%20nonlinear%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10959v2&entry.124074799=Read"},
{"title": "Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing\n  for Physics-based Camera Effect Data Generation", "author": "Yi-Ruei Liu and You-Zhe Xie and Yu-Hsiang Hsu and I-Sheng Fang and Yu-Lun Liu and Jun-Cheng Chen", "abstract": "  Common computer vision systems typically assume ideal pinhole cameras but\nfail when facing real-world camera effects such as fisheye distortion and\nrolling shutter, mainly due to the lack of learning from training data with\ncamera effects. Existing data generation approaches suffer from either high\ncosts, sim-to-real gaps or fail to accurately model camera effects. To address\nthis bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage\npipeline that combines 4D Gaussian Splatting with physically-based ray tracing\nfor camera effect simulation. Given multi-view videos, 4D-GRT first\nreconstructs dynamic scenes, then applies ray tracing to generate videos with\ncontrollable, physically accurate camera effects. 4D-GRT achieves the fastest\nrendering speed while performing better or comparable rendering quality\ncompared to existing baselines. Additionally, we construct eight synthetic\ndynamic scenes in indoor environments across four camera effects as a benchmark\nto evaluate generated videos with camera effects.\n", "link": "http://arxiv.org/abs/2509.10759v2", "date": "2025-10-21", "relevancy": 2.5594, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6519}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6321}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Every%20Camera%20Effect%2C%20Every%20Time%2C%20All%20at%20Once%3A%204D%20Gaussian%20Ray%20Tracing%0A%20%20for%20Physics-based%20Camera%20Effect%20Data%20Generation&body=Title%3A%20Every%20Camera%20Effect%2C%20Every%20Time%2C%20All%20at%20Once%3A%204D%20Gaussian%20Ray%20Tracing%0A%20%20for%20Physics-based%20Camera%20Effect%20Data%20Generation%0AAuthor%3A%20Yi-Ruei%20Liu%20and%20You-Zhe%20Xie%20and%20Yu-Hsiang%20Hsu%20and%20I-Sheng%20Fang%20and%20Yu-Lun%20Liu%20and%20Jun-Cheng%20Chen%0AAbstract%3A%20%20%20Common%20computer%20vision%20systems%20typically%20assume%20ideal%20pinhole%20cameras%20but%0Afail%20when%20facing%20real-world%20camera%20effects%20such%20as%20fisheye%20distortion%20and%0Arolling%20shutter%2C%20mainly%20due%20to%20the%20lack%20of%20learning%20from%20training%20data%20with%0Acamera%20effects.%20Existing%20data%20generation%20approaches%20suffer%20from%20either%20high%0Acosts%2C%20sim-to-real%20gaps%20or%20fail%20to%20accurately%20model%20camera%20effects.%20To%20address%0Athis%20bottleneck%2C%20we%20propose%204D%20Gaussian%20Ray%20Tracing%20%284D-GRT%29%2C%20a%20novel%20two-stage%0Apipeline%20that%20combines%204D%20Gaussian%20Splatting%20with%20physically-based%20ray%20tracing%0Afor%20camera%20effect%20simulation.%20Given%20multi-view%20videos%2C%204D-GRT%20first%0Areconstructs%20dynamic%20scenes%2C%20then%20applies%20ray%20tracing%20to%20generate%20videos%20with%0Acontrollable%2C%20physically%20accurate%20camera%20effects.%204D-GRT%20achieves%20the%20fastest%0Arendering%20speed%20while%20performing%20better%20or%20comparable%20rendering%20quality%0Acompared%20to%20existing%20baselines.%20Additionally%2C%20we%20construct%20eight%20synthetic%0Adynamic%20scenes%20in%20indoor%20environments%20across%20four%20camera%20effects%20as%20a%20benchmark%0Ato%20evaluate%20generated%20videos%20with%20camera%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvery%2520Camera%2520Effect%252C%2520Every%2520Time%252C%2520All%2520at%2520Once%253A%25204D%2520Gaussian%2520Ray%2520Tracing%250A%2520%2520for%2520Physics-based%2520Camera%2520Effect%2520Data%2520Generation%26entry.906535625%3DYi-Ruei%2520Liu%2520and%2520You-Zhe%2520Xie%2520and%2520Yu-Hsiang%2520Hsu%2520and%2520I-Sheng%2520Fang%2520and%2520Yu-Lun%2520Liu%2520and%2520Jun-Cheng%2520Chen%26entry.1292438233%3D%2520%2520Common%2520computer%2520vision%2520systems%2520typically%2520assume%2520ideal%2520pinhole%2520cameras%2520but%250Afail%2520when%2520facing%2520real-world%2520camera%2520effects%2520such%2520as%2520fisheye%2520distortion%2520and%250Arolling%2520shutter%252C%2520mainly%2520due%2520to%2520the%2520lack%2520of%2520learning%2520from%2520training%2520data%2520with%250Acamera%2520effects.%2520Existing%2520data%2520generation%2520approaches%2520suffer%2520from%2520either%2520high%250Acosts%252C%2520sim-to-real%2520gaps%2520or%2520fail%2520to%2520accurately%2520model%2520camera%2520effects.%2520To%2520address%250Athis%2520bottleneck%252C%2520we%2520propose%25204D%2520Gaussian%2520Ray%2520Tracing%2520%25284D-GRT%2529%252C%2520a%2520novel%2520two-stage%250Apipeline%2520that%2520combines%25204D%2520Gaussian%2520Splatting%2520with%2520physically-based%2520ray%2520tracing%250Afor%2520camera%2520effect%2520simulation.%2520Given%2520multi-view%2520videos%252C%25204D-GRT%2520first%250Areconstructs%2520dynamic%2520scenes%252C%2520then%2520applies%2520ray%2520tracing%2520to%2520generate%2520videos%2520with%250Acontrollable%252C%2520physically%2520accurate%2520camera%2520effects.%25204D-GRT%2520achieves%2520the%2520fastest%250Arendering%2520speed%2520while%2520performing%2520better%2520or%2520comparable%2520rendering%2520quality%250Acompared%2520to%2520existing%2520baselines.%2520Additionally%252C%2520we%2520construct%2520eight%2520synthetic%250Adynamic%2520scenes%2520in%2520indoor%2520environments%2520across%2520four%2520camera%2520effects%2520as%2520a%2520benchmark%250Ato%2520evaluate%2520generated%2520videos%2520with%2520camera%2520effects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Every%20Camera%20Effect%2C%20Every%20Time%2C%20All%20at%20Once%3A%204D%20Gaussian%20Ray%20Tracing%0A%20%20for%20Physics-based%20Camera%20Effect%20Data%20Generation&entry.906535625=Yi-Ruei%20Liu%20and%20You-Zhe%20Xie%20and%20Yu-Hsiang%20Hsu%20and%20I-Sheng%20Fang%20and%20Yu-Lun%20Liu%20and%20Jun-Cheng%20Chen&entry.1292438233=%20%20Common%20computer%20vision%20systems%20typically%20assume%20ideal%20pinhole%20cameras%20but%0Afail%20when%20facing%20real-world%20camera%20effects%20such%20as%20fisheye%20distortion%20and%0Arolling%20shutter%2C%20mainly%20due%20to%20the%20lack%20of%20learning%20from%20training%20data%20with%0Acamera%20effects.%20Existing%20data%20generation%20approaches%20suffer%20from%20either%20high%0Acosts%2C%20sim-to-real%20gaps%20or%20fail%20to%20accurately%20model%20camera%20effects.%20To%20address%0Athis%20bottleneck%2C%20we%20propose%204D%20Gaussian%20Ray%20Tracing%20%284D-GRT%29%2C%20a%20novel%20two-stage%0Apipeline%20that%20combines%204D%20Gaussian%20Splatting%20with%20physically-based%20ray%20tracing%0Afor%20camera%20effect%20simulation.%20Given%20multi-view%20videos%2C%204D-GRT%20first%0Areconstructs%20dynamic%20scenes%2C%20then%20applies%20ray%20tracing%20to%20generate%20videos%20with%0Acontrollable%2C%20physically%20accurate%20camera%20effects.%204D-GRT%20achieves%20the%20fastest%0Arendering%20speed%20while%20performing%20better%20or%20comparable%20rendering%20quality%0Acompared%20to%20existing%20baselines.%20Additionally%2C%20we%20construct%20eight%20synthetic%0Adynamic%20scenes%20in%20indoor%20environments%20across%20four%20camera%20effects%20as%20a%20benchmark%0Ato%20evaluate%20generated%20videos%20with%20camera%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10759v2&entry.124074799=Read"},
{"title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs", "author": "Yi Zhang and Bolin Ni and Xin-Sheng Chen and Heng-Rui Zhang and Yongming Rao and Houwen Peng and Qinglin Lu and Han Hu and Meng-Hao Guo and Shi-Min Hu", "abstract": "  Fully open multimodal large language models (MLLMs) currently lag behind\nproprietary counterparts, primarily due to a significant gap in data quality\nfor supervised fine-tuning (SFT). Existing open-source datasets are often\nplagued by widespread noise and a critical deficit in complex reasoning data,\nsuch as Chain-of-Thought (CoT), which hinders the development of advanced model\ncapabilities. Addressing these challenges, our work makes three primary\ncontributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising\napproximately 15 million QA pairs, processed through multiple cleaning\ntechniques and enhanced with a novel dual-level (short and long) CoT enrichment\nstrategy. Second, we introduce HoneyPipe, the data curation pipeline, and its\nunderlying framework DataStudio, providing the community with a transparent and\nadaptable methodology for data curation that moves beyond static dataset\nreleases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B\nmodel on Honey-Data-15M. Experiments show that Bee-8B establishes a new\nstate-of-the-art (SOTA) for fully open MLLMs, achieving performance that is\ncompetitive with, and in some cases surpasses, recent semi-open models such as\nInternVL3.5-8B. Our work delivers to the community a suite of foundational\nresources, including: the Honey-Data-15M corpus; the full-stack suite\ncomprising HoneyPipe and DataStudio; training recipes; an evaluation harness;\nand the model weights. This effort demonstrates that a principled focus on data\nquality is a key pathway to developing fully open MLLMs that are highly\ncompetitive with their semi-open counterparts.\n", "link": "http://arxiv.org/abs/2510.13795v2", "date": "2025-10-21", "relevancy": 2.5543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs&body=Title%3A%20Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs%0AAuthor%3A%20Yi%20Zhang%20and%20Bolin%20Ni%20and%20Xin-Sheng%20Chen%20and%20Heng-Rui%20Zhang%20and%20Yongming%20Rao%20and%20Houwen%20Peng%20and%20Qinglin%20Lu%20and%20Han%20Hu%20and%20Meng-Hao%20Guo%20and%20Shi-Min%20Hu%0AAbstract%3A%20%20%20Fully%20open%20multimodal%20large%20language%20models%20%28MLLMs%29%20currently%20lag%20behind%0Aproprietary%20counterparts%2C%20primarily%20due%20to%20a%20significant%20gap%20in%20data%20quality%0Afor%20supervised%20fine-tuning%20%28SFT%29.%20Existing%20open-source%20datasets%20are%20often%0Aplagued%20by%20widespread%20noise%20and%20a%20critical%20deficit%20in%20complex%20reasoning%20data%2C%0Asuch%20as%20Chain-of-Thought%20%28CoT%29%2C%20which%20hinders%20the%20development%20of%20advanced%20model%0Acapabilities.%20Addressing%20these%20challenges%2C%20our%20work%20makes%20three%20primary%0Acontributions.%20First%2C%20we%20introduce%20Honey-Data-15M%2C%20a%20new%20SFT%20dataset%20comprising%0Aapproximately%2015%20million%20QA%20pairs%2C%20processed%20through%20multiple%20cleaning%0Atechniques%20and%20enhanced%20with%20a%20novel%20dual-level%20%28short%20and%20long%29%20CoT%20enrichment%0Astrategy.%20Second%2C%20we%20introduce%20HoneyPipe%2C%20the%20data%20curation%20pipeline%2C%20and%20its%0Aunderlying%20framework%20DataStudio%2C%20providing%20the%20community%20with%20a%20transparent%20and%0Aadaptable%20methodology%20for%20data%20curation%20that%20moves%20beyond%20static%20dataset%0Areleases.%20Finally%2C%20to%20validate%20our%20dataset%20and%20pipeline%2C%20we%20train%20Bee-8B%2C%20an%208B%0Amodel%20on%20Honey-Data-15M.%20Experiments%20show%20that%20Bee-8B%20establishes%20a%20new%0Astate-of-the-art%20%28SOTA%29%20for%20fully%20open%20MLLMs%2C%20achieving%20performance%20that%20is%0Acompetitive%20with%2C%20and%20in%20some%20cases%20surpasses%2C%20recent%20semi-open%20models%20such%20as%0AInternVL3.5-8B.%20Our%20work%20delivers%20to%20the%20community%20a%20suite%20of%20foundational%0Aresources%2C%20including%3A%20the%20Honey-Data-15M%20corpus%3B%20the%20full-stack%20suite%0Acomprising%20HoneyPipe%20and%20DataStudio%3B%20training%20recipes%3B%20an%20evaluation%20harness%3B%0Aand%20the%20model%20weights.%20This%20effort%20demonstrates%20that%20a%20principled%20focus%20on%20data%0Aquality%20is%20a%20key%20pathway%20to%20developing%20fully%20open%20MLLMs%20that%20are%20highly%0Acompetitive%20with%20their%20semi-open%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBee%253A%2520A%2520High-Quality%2520Corpus%2520and%2520Full-Stack%2520Suite%2520to%2520Unlock%2520Advanced%2520Fully%250A%2520%2520Open%2520MLLMs%26entry.906535625%3DYi%2520Zhang%2520and%2520Bolin%2520Ni%2520and%2520Xin-Sheng%2520Chen%2520and%2520Heng-Rui%2520Zhang%2520and%2520Yongming%2520Rao%2520and%2520Houwen%2520Peng%2520and%2520Qinglin%2520Lu%2520and%2520Han%2520Hu%2520and%2520Meng-Hao%2520Guo%2520and%2520Shi-Min%2520Hu%26entry.1292438233%3D%2520%2520Fully%2520open%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520currently%2520lag%2520behind%250Aproprietary%2520counterparts%252C%2520primarily%2520due%2520to%2520a%2520significant%2520gap%2520in%2520data%2520quality%250Afor%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520Existing%2520open-source%2520datasets%2520are%2520often%250Aplagued%2520by%2520widespread%2520noise%2520and%2520a%2520critical%2520deficit%2520in%2520complex%2520reasoning%2520data%252C%250Asuch%2520as%2520Chain-of-Thought%2520%2528CoT%2529%252C%2520which%2520hinders%2520the%2520development%2520of%2520advanced%2520model%250Acapabilities.%2520Addressing%2520these%2520challenges%252C%2520our%2520work%2520makes%2520three%2520primary%250Acontributions.%2520First%252C%2520we%2520introduce%2520Honey-Data-15M%252C%2520a%2520new%2520SFT%2520dataset%2520comprising%250Aapproximately%252015%2520million%2520QA%2520pairs%252C%2520processed%2520through%2520multiple%2520cleaning%250Atechniques%2520and%2520enhanced%2520with%2520a%2520novel%2520dual-level%2520%2528short%2520and%2520long%2529%2520CoT%2520enrichment%250Astrategy.%2520Second%252C%2520we%2520introduce%2520HoneyPipe%252C%2520the%2520data%2520curation%2520pipeline%252C%2520and%2520its%250Aunderlying%2520framework%2520DataStudio%252C%2520providing%2520the%2520community%2520with%2520a%2520transparent%2520and%250Aadaptable%2520methodology%2520for%2520data%2520curation%2520that%2520moves%2520beyond%2520static%2520dataset%250Areleases.%2520Finally%252C%2520to%2520validate%2520our%2520dataset%2520and%2520pipeline%252C%2520we%2520train%2520Bee-8B%252C%2520an%25208B%250Amodel%2520on%2520Honey-Data-15M.%2520Experiments%2520show%2520that%2520Bee-8B%2520establishes%2520a%2520new%250Astate-of-the-art%2520%2528SOTA%2529%2520for%2520fully%2520open%2520MLLMs%252C%2520achieving%2520performance%2520that%2520is%250Acompetitive%2520with%252C%2520and%2520in%2520some%2520cases%2520surpasses%252C%2520recent%2520semi-open%2520models%2520such%2520as%250AInternVL3.5-8B.%2520Our%2520work%2520delivers%2520to%2520the%2520community%2520a%2520suite%2520of%2520foundational%250Aresources%252C%2520including%253A%2520the%2520Honey-Data-15M%2520corpus%253B%2520the%2520full-stack%2520suite%250Acomprising%2520HoneyPipe%2520and%2520DataStudio%253B%2520training%2520recipes%253B%2520an%2520evaluation%2520harness%253B%250Aand%2520the%2520model%2520weights.%2520This%2520effort%2520demonstrates%2520that%2520a%2520principled%2520focus%2520on%2520data%250Aquality%2520is%2520a%2520key%2520pathway%2520to%2520developing%2520fully%2520open%2520MLLMs%2520that%2520are%2520highly%250Acompetitive%2520with%2520their%2520semi-open%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bee%3A%20A%20High-Quality%20Corpus%20and%20Full-Stack%20Suite%20to%20Unlock%20Advanced%20Fully%0A%20%20Open%20MLLMs&entry.906535625=Yi%20Zhang%20and%20Bolin%20Ni%20and%20Xin-Sheng%20Chen%20and%20Heng-Rui%20Zhang%20and%20Yongming%20Rao%20and%20Houwen%20Peng%20and%20Qinglin%20Lu%20and%20Han%20Hu%20and%20Meng-Hao%20Guo%20and%20Shi-Min%20Hu&entry.1292438233=%20%20Fully%20open%20multimodal%20large%20language%20models%20%28MLLMs%29%20currently%20lag%20behind%0Aproprietary%20counterparts%2C%20primarily%20due%20to%20a%20significant%20gap%20in%20data%20quality%0Afor%20supervised%20fine-tuning%20%28SFT%29.%20Existing%20open-source%20datasets%20are%20often%0Aplagued%20by%20widespread%20noise%20and%20a%20critical%20deficit%20in%20complex%20reasoning%20data%2C%0Asuch%20as%20Chain-of-Thought%20%28CoT%29%2C%20which%20hinders%20the%20development%20of%20advanced%20model%0Acapabilities.%20Addressing%20these%20challenges%2C%20our%20work%20makes%20three%20primary%0Acontributions.%20First%2C%20we%20introduce%20Honey-Data-15M%2C%20a%20new%20SFT%20dataset%20comprising%0Aapproximately%2015%20million%20QA%20pairs%2C%20processed%20through%20multiple%20cleaning%0Atechniques%20and%20enhanced%20with%20a%20novel%20dual-level%20%28short%20and%20long%29%20CoT%20enrichment%0Astrategy.%20Second%2C%20we%20introduce%20HoneyPipe%2C%20the%20data%20curation%20pipeline%2C%20and%20its%0Aunderlying%20framework%20DataStudio%2C%20providing%20the%20community%20with%20a%20transparent%20and%0Aadaptable%20methodology%20for%20data%20curation%20that%20moves%20beyond%20static%20dataset%0Areleases.%20Finally%2C%20to%20validate%20our%20dataset%20and%20pipeline%2C%20we%20train%20Bee-8B%2C%20an%208B%0Amodel%20on%20Honey-Data-15M.%20Experiments%20show%20that%20Bee-8B%20establishes%20a%20new%0Astate-of-the-art%20%28SOTA%29%20for%20fully%20open%20MLLMs%2C%20achieving%20performance%20that%20is%0Acompetitive%20with%2C%20and%20in%20some%20cases%20surpasses%2C%20recent%20semi-open%20models%20such%20as%0AInternVL3.5-8B.%20Our%20work%20delivers%20to%20the%20community%20a%20suite%20of%20foundational%0Aresources%2C%20including%3A%20the%20Honey-Data-15M%20corpus%3B%20the%20full-stack%20suite%0Acomprising%20HoneyPipe%20and%20DataStudio%3B%20training%20recipes%3B%20an%20evaluation%20harness%3B%0Aand%20the%20model%20weights.%20This%20effort%20demonstrates%20that%20a%20principled%20focus%20on%20data%0Aquality%20is%20a%20key%20pathway%20to%20developing%20fully%20open%20MLLMs%20that%20are%20highly%0Acompetitive%20with%20their%20semi-open%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13795v2&entry.124074799=Read"},
{"title": "Beyond the Pipeline: Analyzing Key Factors in End-to-End Deep Learning\n  for Historical Writer Identification", "author": "Hanif Rasyidi and Moshiur Farazi", "abstract": "  This paper investigates various factors that influence the performance of\nend-to-end deep learning approaches for historical writer identification (HWI),\na task that remains challenging due to the diversity of handwriting styles,\ndocument degradation, and the limited number of labelled samples per writer.\nThese conditions often make accurate recognition difficult, even for human\nexperts. Traditional HWI methods typically rely on handcrafted image processing\nand clustering techniques, which tend to perform well on small and carefully\ncurated datasets. In contrast, end-to-end pipelines aim to automate the process\nby learning features directly from document images. However, our experiments\nshow that many of these models struggle to generalise in more realistic,\ndocument-level settings, especially under zero-shot scenarios where writers in\nthe test set are not present in the training data. We explore different\ncombinations of pre-processing methods, backbone architectures, and\npost-processing strategies, including text segmentation, patch sampling, and\nfeature aggregation. The results suggest that most configurations perform\npoorly due to weak capture of low-level visual features, inconsistent patch\nrepresentations, and high sensitivity to content noise. Still, we identify one\nend-to-end setup that achieves results comparable to the top-performing system,\ndespite using a simpler design. These findings point to key challenges in\nbuilding robust end-to-end systems and offer insight into design choices that\nimprove performance in historical document writer identification.\n", "link": "http://arxiv.org/abs/2510.18671v1", "date": "2025-10-21", "relevancy": 2.5122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Pipeline%3A%20Analyzing%20Key%20Factors%20in%20End-to-End%20Deep%20Learning%0A%20%20for%20Historical%20Writer%20Identification&body=Title%3A%20Beyond%20the%20Pipeline%3A%20Analyzing%20Key%20Factors%20in%20End-to-End%20Deep%20Learning%0A%20%20for%20Historical%20Writer%20Identification%0AAuthor%3A%20Hanif%20Rasyidi%20and%20Moshiur%20Farazi%0AAbstract%3A%20%20%20This%20paper%20investigates%20various%20factors%20that%20influence%20the%20performance%20of%0Aend-to-end%20deep%20learning%20approaches%20for%20historical%20writer%20identification%20%28HWI%29%2C%0Aa%20task%20that%20remains%20challenging%20due%20to%20the%20diversity%20of%20handwriting%20styles%2C%0Adocument%20degradation%2C%20and%20the%20limited%20number%20of%20labelled%20samples%20per%20writer.%0AThese%20conditions%20often%20make%20accurate%20recognition%20difficult%2C%20even%20for%20human%0Aexperts.%20Traditional%20HWI%20methods%20typically%20rely%20on%20handcrafted%20image%20processing%0Aand%20clustering%20techniques%2C%20which%20tend%20to%20perform%20well%20on%20small%20and%20carefully%0Acurated%20datasets.%20In%20contrast%2C%20end-to-end%20pipelines%20aim%20to%20automate%20the%20process%0Aby%20learning%20features%20directly%20from%20document%20images.%20However%2C%20our%20experiments%0Ashow%20that%20many%20of%20these%20models%20struggle%20to%20generalise%20in%20more%20realistic%2C%0Adocument-level%20settings%2C%20especially%20under%20zero-shot%20scenarios%20where%20writers%20in%0Athe%20test%20set%20are%20not%20present%20in%20the%20training%20data.%20We%20explore%20different%0Acombinations%20of%20pre-processing%20methods%2C%20backbone%20architectures%2C%20and%0Apost-processing%20strategies%2C%20including%20text%20segmentation%2C%20patch%20sampling%2C%20and%0Afeature%20aggregation.%20The%20results%20suggest%20that%20most%20configurations%20perform%0Apoorly%20due%20to%20weak%20capture%20of%20low-level%20visual%20features%2C%20inconsistent%20patch%0Arepresentations%2C%20and%20high%20sensitivity%20to%20content%20noise.%20Still%2C%20we%20identify%20one%0Aend-to-end%20setup%20that%20achieves%20results%20comparable%20to%20the%20top-performing%20system%2C%0Adespite%20using%20a%20simpler%20design.%20These%20findings%20point%20to%20key%20challenges%20in%0Abuilding%20robust%20end-to-end%20systems%20and%20offer%20insight%20into%20design%20choices%20that%0Aimprove%20performance%20in%20historical%20document%20writer%20identification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Pipeline%253A%2520Analyzing%2520Key%2520Factors%2520in%2520End-to-End%2520Deep%2520Learning%250A%2520%2520for%2520Historical%2520Writer%2520Identification%26entry.906535625%3DHanif%2520Rasyidi%2520and%2520Moshiur%2520Farazi%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520various%2520factors%2520that%2520influence%2520the%2520performance%2520of%250Aend-to-end%2520deep%2520learning%2520approaches%2520for%2520historical%2520writer%2520identification%2520%2528HWI%2529%252C%250Aa%2520task%2520that%2520remains%2520challenging%2520due%2520to%2520the%2520diversity%2520of%2520handwriting%2520styles%252C%250Adocument%2520degradation%252C%2520and%2520the%2520limited%2520number%2520of%2520labelled%2520samples%2520per%2520writer.%250AThese%2520conditions%2520often%2520make%2520accurate%2520recognition%2520difficult%252C%2520even%2520for%2520human%250Aexperts.%2520Traditional%2520HWI%2520methods%2520typically%2520rely%2520on%2520handcrafted%2520image%2520processing%250Aand%2520clustering%2520techniques%252C%2520which%2520tend%2520to%2520perform%2520well%2520on%2520small%2520and%2520carefully%250Acurated%2520datasets.%2520In%2520contrast%252C%2520end-to-end%2520pipelines%2520aim%2520to%2520automate%2520the%2520process%250Aby%2520learning%2520features%2520directly%2520from%2520document%2520images.%2520However%252C%2520our%2520experiments%250Ashow%2520that%2520many%2520of%2520these%2520models%2520struggle%2520to%2520generalise%2520in%2520more%2520realistic%252C%250Adocument-level%2520settings%252C%2520especially%2520under%2520zero-shot%2520scenarios%2520where%2520writers%2520in%250Athe%2520test%2520set%2520are%2520not%2520present%2520in%2520the%2520training%2520data.%2520We%2520explore%2520different%250Acombinations%2520of%2520pre-processing%2520methods%252C%2520backbone%2520architectures%252C%2520and%250Apost-processing%2520strategies%252C%2520including%2520text%2520segmentation%252C%2520patch%2520sampling%252C%2520and%250Afeature%2520aggregation.%2520The%2520results%2520suggest%2520that%2520most%2520configurations%2520perform%250Apoorly%2520due%2520to%2520weak%2520capture%2520of%2520low-level%2520visual%2520features%252C%2520inconsistent%2520patch%250Arepresentations%252C%2520and%2520high%2520sensitivity%2520to%2520content%2520noise.%2520Still%252C%2520we%2520identify%2520one%250Aend-to-end%2520setup%2520that%2520achieves%2520results%2520comparable%2520to%2520the%2520top-performing%2520system%252C%250Adespite%2520using%2520a%2520simpler%2520design.%2520These%2520findings%2520point%2520to%2520key%2520challenges%2520in%250Abuilding%2520robust%2520end-to-end%2520systems%2520and%2520offer%2520insight%2520into%2520design%2520choices%2520that%250Aimprove%2520performance%2520in%2520historical%2520document%2520writer%2520identification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Pipeline%3A%20Analyzing%20Key%20Factors%20in%20End-to-End%20Deep%20Learning%0A%20%20for%20Historical%20Writer%20Identification&entry.906535625=Hanif%20Rasyidi%20and%20Moshiur%20Farazi&entry.1292438233=%20%20This%20paper%20investigates%20various%20factors%20that%20influence%20the%20performance%20of%0Aend-to-end%20deep%20learning%20approaches%20for%20historical%20writer%20identification%20%28HWI%29%2C%0Aa%20task%20that%20remains%20challenging%20due%20to%20the%20diversity%20of%20handwriting%20styles%2C%0Adocument%20degradation%2C%20and%20the%20limited%20number%20of%20labelled%20samples%20per%20writer.%0AThese%20conditions%20often%20make%20accurate%20recognition%20difficult%2C%20even%20for%20human%0Aexperts.%20Traditional%20HWI%20methods%20typically%20rely%20on%20handcrafted%20image%20processing%0Aand%20clustering%20techniques%2C%20which%20tend%20to%20perform%20well%20on%20small%20and%20carefully%0Acurated%20datasets.%20In%20contrast%2C%20end-to-end%20pipelines%20aim%20to%20automate%20the%20process%0Aby%20learning%20features%20directly%20from%20document%20images.%20However%2C%20our%20experiments%0Ashow%20that%20many%20of%20these%20models%20struggle%20to%20generalise%20in%20more%20realistic%2C%0Adocument-level%20settings%2C%20especially%20under%20zero-shot%20scenarios%20where%20writers%20in%0Athe%20test%20set%20are%20not%20present%20in%20the%20training%20data.%20We%20explore%20different%0Acombinations%20of%20pre-processing%20methods%2C%20backbone%20architectures%2C%20and%0Apost-processing%20strategies%2C%20including%20text%20segmentation%2C%20patch%20sampling%2C%20and%0Afeature%20aggregation.%20The%20results%20suggest%20that%20most%20configurations%20perform%0Apoorly%20due%20to%20weak%20capture%20of%20low-level%20visual%20features%2C%20inconsistent%20patch%0Arepresentations%2C%20and%20high%20sensitivity%20to%20content%20noise.%20Still%2C%20we%20identify%20one%0Aend-to-end%20setup%20that%20achieves%20results%20comparable%20to%20the%20top-performing%20system%2C%0Adespite%20using%20a%20simpler%20design.%20These%20findings%20point%20to%20key%20challenges%20in%0Abuilding%20robust%20end-to-end%20systems%20and%20offer%20insight%20into%20design%20choices%20that%0Aimprove%20performance%20in%20historical%20document%20writer%20identification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18671v1&entry.124074799=Read"},
{"title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual\n  Navigation", "author": "Yiyuan Pan and Yunzhe Xu and Zhe Liu and Hesheng Wang", "abstract": "  Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents.\n", "link": "http://arxiv.org/abs/2510.00441v3", "date": "2025-10-21", "relevancy": 2.4886, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.639}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20through%20Uncertainty%3A%20Robust%20Task-Oriented%20Optimization%20in%20Visual%0A%20%20Navigation&body=Title%3A%20Seeing%20through%20Uncertainty%3A%20Robust%20Task-Oriented%20Optimization%20in%20Visual%0A%20%20Navigation%0AAuthor%3A%20Yiyuan%20Pan%20and%20Yunzhe%20Xu%20and%20Zhe%20Liu%20and%20Hesheng%20Wang%0AAbstract%3A%20%20%20Visual%20navigation%20is%20a%20fundamental%20problem%20in%20embodied%20AI%2C%20yet%20practical%0Adeployments%20demand%20long-horizon%20planning%20capabilities%20to%20address%0Amulti-objective%20tasks.%20A%20major%20bottleneck%20is%20data%20scarcity%3A%20policies%20learned%0Afrom%20limited%20data%20often%20overfit%20and%20fail%20to%20generalize%20OOD.%20Existing%20neural%0Anetwork-based%20agents%20typically%20increase%20architectural%20complexity%20that%0Aparadoxically%20become%20counterproductive%20in%20the%20small-sample%20regime.%20This%20paper%0Aintroduce%20NeuRO%2C%20a%20integrated%20learning-to-optimize%20framework%20that%20tightly%0Acouples%20perception%20networks%20with%20downstream%20task-level%20robust%20optimization.%0ASpecifically%2C%20NeuRO%20addresses%20core%20difficulties%20in%20this%20integration%3A%20%28i%29%20it%0Atransforms%20noisy%20visual%20predictions%20under%20data%20scarcity%20into%20convex%20uncertainty%0Asets%20using%20Partially%20Input%20Convex%20Neural%20Networks%20%28PICNNs%29%20with%20conformal%0Acalibration%2C%20which%20directly%20parameterize%20the%20optimization%20constraints%3B%20and%20%28ii%29%0Ait%20reformulates%20planning%20under%20partial%20observability%20as%20a%20robust%20optimization%0Aproblem%2C%20enabling%20uncertainty-aware%20policies%20that%20transfer%20across%20environments.%0AExtensive%20experiments%20on%20both%20unordered%20and%20sequential%20multi-object%20navigation%0Atasks%20demonstrate%20that%20NeuRO%20establishes%20SoTA%20performance%2C%20particularly%20in%0Ageneralization%20to%20unseen%20environments.%20Our%20work%20thus%20presents%20a%20significant%0Aadvancement%20for%20developing%20robust%2C%20generalizable%20autonomous%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520through%2520Uncertainty%253A%2520Robust%2520Task-Oriented%2520Optimization%2520in%2520Visual%250A%2520%2520Navigation%26entry.906535625%3DYiyuan%2520Pan%2520and%2520Yunzhe%2520Xu%2520and%2520Zhe%2520Liu%2520and%2520Hesheng%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520navigation%2520is%2520a%2520fundamental%2520problem%2520in%2520embodied%2520AI%252C%2520yet%2520practical%250Adeployments%2520demand%2520long-horizon%2520planning%2520capabilities%2520to%2520address%250Amulti-objective%2520tasks.%2520A%2520major%2520bottleneck%2520is%2520data%2520scarcity%253A%2520policies%2520learned%250Afrom%2520limited%2520data%2520often%2520overfit%2520and%2520fail%2520to%2520generalize%2520OOD.%2520Existing%2520neural%250Anetwork-based%2520agents%2520typically%2520increase%2520architectural%2520complexity%2520that%250Aparadoxically%2520become%2520counterproductive%2520in%2520the%2520small-sample%2520regime.%2520This%2520paper%250Aintroduce%2520NeuRO%252C%2520a%2520integrated%2520learning-to-optimize%2520framework%2520that%2520tightly%250Acouples%2520perception%2520networks%2520with%2520downstream%2520task-level%2520robust%2520optimization.%250ASpecifically%252C%2520NeuRO%2520addresses%2520core%2520difficulties%2520in%2520this%2520integration%253A%2520%2528i%2529%2520it%250Atransforms%2520noisy%2520visual%2520predictions%2520under%2520data%2520scarcity%2520into%2520convex%2520uncertainty%250Asets%2520using%2520Partially%2520Input%2520Convex%2520Neural%2520Networks%2520%2528PICNNs%2529%2520with%2520conformal%250Acalibration%252C%2520which%2520directly%2520parameterize%2520the%2520optimization%2520constraints%253B%2520and%2520%2528ii%2529%250Ait%2520reformulates%2520planning%2520under%2520partial%2520observability%2520as%2520a%2520robust%2520optimization%250Aproblem%252C%2520enabling%2520uncertainty-aware%2520policies%2520that%2520transfer%2520across%2520environments.%250AExtensive%2520experiments%2520on%2520both%2520unordered%2520and%2520sequential%2520multi-object%2520navigation%250Atasks%2520demonstrate%2520that%2520NeuRO%2520establishes%2520SoTA%2520performance%252C%2520particularly%2520in%250Ageneralization%2520to%2520unseen%2520environments.%2520Our%2520work%2520thus%2520presents%2520a%2520significant%250Aadvancement%2520for%2520developing%2520robust%252C%2520generalizable%2520autonomous%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20through%20Uncertainty%3A%20Robust%20Task-Oriented%20Optimization%20in%20Visual%0A%20%20Navigation&entry.906535625=Yiyuan%20Pan%20and%20Yunzhe%20Xu%20and%20Zhe%20Liu%20and%20Hesheng%20Wang&entry.1292438233=%20%20Visual%20navigation%20is%20a%20fundamental%20problem%20in%20embodied%20AI%2C%20yet%20practical%0Adeployments%20demand%20long-horizon%20planning%20capabilities%20to%20address%0Amulti-objective%20tasks.%20A%20major%20bottleneck%20is%20data%20scarcity%3A%20policies%20learned%0Afrom%20limited%20data%20often%20overfit%20and%20fail%20to%20generalize%20OOD.%20Existing%20neural%0Anetwork-based%20agents%20typically%20increase%20architectural%20complexity%20that%0Aparadoxically%20become%20counterproductive%20in%20the%20small-sample%20regime.%20This%20paper%0Aintroduce%20NeuRO%2C%20a%20integrated%20learning-to-optimize%20framework%20that%20tightly%0Acouples%20perception%20networks%20with%20downstream%20task-level%20robust%20optimization.%0ASpecifically%2C%20NeuRO%20addresses%20core%20difficulties%20in%20this%20integration%3A%20%28i%29%20it%0Atransforms%20noisy%20visual%20predictions%20under%20data%20scarcity%20into%20convex%20uncertainty%0Asets%20using%20Partially%20Input%20Convex%20Neural%20Networks%20%28PICNNs%29%20with%20conformal%0Acalibration%2C%20which%20directly%20parameterize%20the%20optimization%20constraints%3B%20and%20%28ii%29%0Ait%20reformulates%20planning%20under%20partial%20observability%20as%20a%20robust%20optimization%0Aproblem%2C%20enabling%20uncertainty-aware%20policies%20that%20transfer%20across%20environments.%0AExtensive%20experiments%20on%20both%20unordered%20and%20sequential%20multi-object%20navigation%0Atasks%20demonstrate%20that%20NeuRO%20establishes%20SoTA%20performance%2C%20particularly%20in%0Ageneralization%20to%20unseen%20environments.%20Our%20work%20thus%20presents%20a%20significant%0Aadvancement%20for%20developing%20robust%2C%20generalizable%20autonomous%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00441v3&entry.124074799=Read"},
{"title": "Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning", "author": "Khurram Yamin and Gaurav Ghosal and Bryan Wilder", "abstract": "  Large Language Models have been shown to contain extensive world knowledge in\ntheir parameters, enabling impressive performance on many knowledge intensive\ntasks. However, when deployed in novel settings, LLMs often encounter\nsituations where they must integrate parametric knowledge with new or\nunfamiliar information. In this work, we explore whether LLMs can combine\nknowledge in-context with their parametric knowledge through the lens of\ncounterfactual reasoning. Through synthetic and real experiments in multi-hop\nreasoning problems, we show that LLMs generally struggle with counterfactual\nreasoning, often resorting to exclusively using their parametric knowledge.\nMoreover, we show that simple post-hoc finetuning can struggle to instill\ncounterfactual reasoning ability -- often leading to degradation in stored\nparametric knowledge. Ultimately, our work reveals important limitations of\ncurrent LLM's abilities to re-purpose parametric knowledge in novel settings.\n", "link": "http://arxiv.org/abs/2506.15732v3", "date": "2025-10-21", "relevancy": 2.4827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Reconcile%20Knowledge%20Conflicts%20in%20Counterfactual%20Reasoning&body=Title%3A%20Can%20LLMs%20Reconcile%20Knowledge%20Conflicts%20in%20Counterfactual%20Reasoning%0AAuthor%3A%20Khurram%20Yamin%20and%20Gaurav%20Ghosal%20and%20Bryan%20Wilder%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20been%20shown%20to%20contain%20extensive%20world%20knowledge%20in%0Atheir%20parameters%2C%20enabling%20impressive%20performance%20on%20many%20knowledge%20intensive%0Atasks.%20However%2C%20when%20deployed%20in%20novel%20settings%2C%20LLMs%20often%20encounter%0Asituations%20where%20they%20must%20integrate%20parametric%20knowledge%20with%20new%20or%0Aunfamiliar%20information.%20In%20this%20work%2C%20we%20explore%20whether%20LLMs%20can%20combine%0Aknowledge%20in-context%20with%20their%20parametric%20knowledge%20through%20the%20lens%20of%0Acounterfactual%20reasoning.%20Through%20synthetic%20and%20real%20experiments%20in%20multi-hop%0Areasoning%20problems%2C%20we%20show%20that%20LLMs%20generally%20struggle%20with%20counterfactual%0Areasoning%2C%20often%20resorting%20to%20exclusively%20using%20their%20parametric%20knowledge.%0AMoreover%2C%20we%20show%20that%20simple%20post-hoc%20finetuning%20can%20struggle%20to%20instill%0Acounterfactual%20reasoning%20ability%20--%20often%20leading%20to%20degradation%20in%20stored%0Aparametric%20knowledge.%20Ultimately%2C%20our%20work%20reveals%20important%20limitations%20of%0Acurrent%20LLM%27s%20abilities%20to%20re-purpose%20parametric%20knowledge%20in%20novel%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15732v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Reconcile%2520Knowledge%2520Conflicts%2520in%2520Counterfactual%2520Reasoning%26entry.906535625%3DKhurram%2520Yamin%2520and%2520Gaurav%2520Ghosal%2520and%2520Bryan%2520Wilder%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520been%2520shown%2520to%2520contain%2520extensive%2520world%2520knowledge%2520in%250Atheir%2520parameters%252C%2520enabling%2520impressive%2520performance%2520on%2520many%2520knowledge%2520intensive%250Atasks.%2520However%252C%2520when%2520deployed%2520in%2520novel%2520settings%252C%2520LLMs%2520often%2520encounter%250Asituations%2520where%2520they%2520must%2520integrate%2520parametric%2520knowledge%2520with%2520new%2520or%250Aunfamiliar%2520information.%2520In%2520this%2520work%252C%2520we%2520explore%2520whether%2520LLMs%2520can%2520combine%250Aknowledge%2520in-context%2520with%2520their%2520parametric%2520knowledge%2520through%2520the%2520lens%2520of%250Acounterfactual%2520reasoning.%2520Through%2520synthetic%2520and%2520real%2520experiments%2520in%2520multi-hop%250Areasoning%2520problems%252C%2520we%2520show%2520that%2520LLMs%2520generally%2520struggle%2520with%2520counterfactual%250Areasoning%252C%2520often%2520resorting%2520to%2520exclusively%2520using%2520their%2520parametric%2520knowledge.%250AMoreover%252C%2520we%2520show%2520that%2520simple%2520post-hoc%2520finetuning%2520can%2520struggle%2520to%2520instill%250Acounterfactual%2520reasoning%2520ability%2520--%2520often%2520leading%2520to%2520degradation%2520in%2520stored%250Aparametric%2520knowledge.%2520Ultimately%252C%2520our%2520work%2520reveals%2520important%2520limitations%2520of%250Acurrent%2520LLM%2527s%2520abilities%2520to%2520re-purpose%2520parametric%2520knowledge%2520in%2520novel%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15732v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Reconcile%20Knowledge%20Conflicts%20in%20Counterfactual%20Reasoning&entry.906535625=Khurram%20Yamin%20and%20Gaurav%20Ghosal%20and%20Bryan%20Wilder&entry.1292438233=%20%20Large%20Language%20Models%20have%20been%20shown%20to%20contain%20extensive%20world%20knowledge%20in%0Atheir%20parameters%2C%20enabling%20impressive%20performance%20on%20many%20knowledge%20intensive%0Atasks.%20However%2C%20when%20deployed%20in%20novel%20settings%2C%20LLMs%20often%20encounter%0Asituations%20where%20they%20must%20integrate%20parametric%20knowledge%20with%20new%20or%0Aunfamiliar%20information.%20In%20this%20work%2C%20we%20explore%20whether%20LLMs%20can%20combine%0Aknowledge%20in-context%20with%20their%20parametric%20knowledge%20through%20the%20lens%20of%0Acounterfactual%20reasoning.%20Through%20synthetic%20and%20real%20experiments%20in%20multi-hop%0Areasoning%20problems%2C%20we%20show%20that%20LLMs%20generally%20struggle%20with%20counterfactual%0Areasoning%2C%20often%20resorting%20to%20exclusively%20using%20their%20parametric%20knowledge.%0AMoreover%2C%20we%20show%20that%20simple%20post-hoc%20finetuning%20can%20struggle%20to%20instill%0Acounterfactual%20reasoning%20ability%20--%20often%20leading%20to%20degradation%20in%20stored%0Aparametric%20knowledge.%20Ultimately%2C%20our%20work%20reveals%20important%20limitations%20of%0Acurrent%20LLM%27s%20abilities%20to%20re-purpose%20parametric%20knowledge%20in%20novel%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15732v3&entry.124074799=Read"},
{"title": "SO(3)-invariant PCA with application to molecular data", "author": "Michael Fraiman and Paulina Hoyos and Tamir Bendory and Joe Kileel and Oscar Mickelin and Nir Sharon and Amit Singer", "abstract": "  Principal component analysis (PCA) is a fundamental technique for\ndimensionality reduction and denoising; however, its application to\nthree-dimensional data with arbitrary orientations -- common in structural\nbiology -- presents significant challenges. A naive approach requires\naugmenting the dataset with many rotated copies of each sample, incurring\nprohibitive computational costs. In this paper, we extend PCA to 3D volumetric\ndatasets with unknown orientations by developing an efficient and principled\nframework for SO(3)-invariant PCA that implicitly accounts for all rotations\nwithout explicit data augmentation. By exploiting underlying algebraic\nstructure, we demonstrate that the computation involves only the square root of\nthe total number of covariance entries, resulting in a substantial reduction in\ncomplexity. We validate the method on real-world molecular datasets,\ndemonstrating its effectiveness and opening up new possibilities for\nlarge-scale, high-dimensional reconstruction problems.\n", "link": "http://arxiv.org/abs/2510.18827v1", "date": "2025-10-21", "relevancy": 2.4823, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5114}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5114}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SO%283%29-invariant%20PCA%20with%20application%20to%20molecular%20data&body=Title%3A%20SO%283%29-invariant%20PCA%20with%20application%20to%20molecular%20data%0AAuthor%3A%20Michael%20Fraiman%20and%20Paulina%20Hoyos%20and%20Tamir%20Bendory%20and%20Joe%20Kileel%20and%20Oscar%20Mickelin%20and%20Nir%20Sharon%20and%20Amit%20Singer%0AAbstract%3A%20%20%20Principal%20component%20analysis%20%28PCA%29%20is%20a%20fundamental%20technique%20for%0Adimensionality%20reduction%20and%20denoising%3B%20however%2C%20its%20application%20to%0Athree-dimensional%20data%20with%20arbitrary%20orientations%20--%20common%20in%20structural%0Abiology%20--%20presents%20significant%20challenges.%20A%20naive%20approach%20requires%0Aaugmenting%20the%20dataset%20with%20many%20rotated%20copies%20of%20each%20sample%2C%20incurring%0Aprohibitive%20computational%20costs.%20In%20this%20paper%2C%20we%20extend%20PCA%20to%203D%20volumetric%0Adatasets%20with%20unknown%20orientations%20by%20developing%20an%20efficient%20and%20principled%0Aframework%20for%20SO%283%29-invariant%20PCA%20that%20implicitly%20accounts%20for%20all%20rotations%0Awithout%20explicit%20data%20augmentation.%20By%20exploiting%20underlying%20algebraic%0Astructure%2C%20we%20demonstrate%20that%20the%20computation%20involves%20only%20the%20square%20root%20of%0Athe%20total%20number%20of%20covariance%20entries%2C%20resulting%20in%20a%20substantial%20reduction%20in%0Acomplexity.%20We%20validate%20the%20method%20on%20real-world%20molecular%20datasets%2C%0Ademonstrating%20its%20effectiveness%20and%20opening%20up%20new%20possibilities%20for%0Alarge-scale%2C%20high-dimensional%20reconstruction%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSO%25283%2529-invariant%2520PCA%2520with%2520application%2520to%2520molecular%2520data%26entry.906535625%3DMichael%2520Fraiman%2520and%2520Paulina%2520Hoyos%2520and%2520Tamir%2520Bendory%2520and%2520Joe%2520Kileel%2520and%2520Oscar%2520Mickelin%2520and%2520Nir%2520Sharon%2520and%2520Amit%2520Singer%26entry.1292438233%3D%2520%2520Principal%2520component%2520analysis%2520%2528PCA%2529%2520is%2520a%2520fundamental%2520technique%2520for%250Adimensionality%2520reduction%2520and%2520denoising%253B%2520however%252C%2520its%2520application%2520to%250Athree-dimensional%2520data%2520with%2520arbitrary%2520orientations%2520--%2520common%2520in%2520structural%250Abiology%2520--%2520presents%2520significant%2520challenges.%2520A%2520naive%2520approach%2520requires%250Aaugmenting%2520the%2520dataset%2520with%2520many%2520rotated%2520copies%2520of%2520each%2520sample%252C%2520incurring%250Aprohibitive%2520computational%2520costs.%2520In%2520this%2520paper%252C%2520we%2520extend%2520PCA%2520to%25203D%2520volumetric%250Adatasets%2520with%2520unknown%2520orientations%2520by%2520developing%2520an%2520efficient%2520and%2520principled%250Aframework%2520for%2520SO%25283%2529-invariant%2520PCA%2520that%2520implicitly%2520accounts%2520for%2520all%2520rotations%250Awithout%2520explicit%2520data%2520augmentation.%2520By%2520exploiting%2520underlying%2520algebraic%250Astructure%252C%2520we%2520demonstrate%2520that%2520the%2520computation%2520involves%2520only%2520the%2520square%2520root%2520of%250Athe%2520total%2520number%2520of%2520covariance%2520entries%252C%2520resulting%2520in%2520a%2520substantial%2520reduction%2520in%250Acomplexity.%2520We%2520validate%2520the%2520method%2520on%2520real-world%2520molecular%2520datasets%252C%250Ademonstrating%2520its%2520effectiveness%2520and%2520opening%2520up%2520new%2520possibilities%2520for%250Alarge-scale%252C%2520high-dimensional%2520reconstruction%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SO%283%29-invariant%20PCA%20with%20application%20to%20molecular%20data&entry.906535625=Michael%20Fraiman%20and%20Paulina%20Hoyos%20and%20Tamir%20Bendory%20and%20Joe%20Kileel%20and%20Oscar%20Mickelin%20and%20Nir%20Sharon%20and%20Amit%20Singer&entry.1292438233=%20%20Principal%20component%20analysis%20%28PCA%29%20is%20a%20fundamental%20technique%20for%0Adimensionality%20reduction%20and%20denoising%3B%20however%2C%20its%20application%20to%0Athree-dimensional%20data%20with%20arbitrary%20orientations%20--%20common%20in%20structural%0Abiology%20--%20presents%20significant%20challenges.%20A%20naive%20approach%20requires%0Aaugmenting%20the%20dataset%20with%20many%20rotated%20copies%20of%20each%20sample%2C%20incurring%0Aprohibitive%20computational%20costs.%20In%20this%20paper%2C%20we%20extend%20PCA%20to%203D%20volumetric%0Adatasets%20with%20unknown%20orientations%20by%20developing%20an%20efficient%20and%20principled%0Aframework%20for%20SO%283%29-invariant%20PCA%20that%20implicitly%20accounts%20for%20all%20rotations%0Awithout%20explicit%20data%20augmentation.%20By%20exploiting%20underlying%20algebraic%0Astructure%2C%20we%20demonstrate%20that%20the%20computation%20involves%20only%20the%20square%20root%20of%0Athe%20total%20number%20of%20covariance%20entries%2C%20resulting%20in%20a%20substantial%20reduction%20in%0Acomplexity.%20We%20validate%20the%20method%20on%20real-world%20molecular%20datasets%2C%0Ademonstrating%20its%20effectiveness%20and%20opening%20up%20new%20possibilities%20for%0Alarge-scale%2C%20high-dimensional%20reconstruction%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18827v1&entry.124074799=Read"},
{"title": "VideoVerse: How Far is Your T2V Generator from a World Model?", "author": "Zeqing Wang and Xinyu Wei and Bairui Li and Zhen Guo and Jinrui Zhang and Hongyang Wei and Keze Wang and Lei Zhang", "abstract": "  The recent rapid advancement of Text-to-Video (T2V) generation technologies,\nwhich are critical to build ``world models'', makes the existing benchmarks\nincreasingly insufficient to evaluate state-of-the-art T2V models. First,\ncurrent evaluation dimensions, such as per-frame aesthetic quality and temporal\nconsistency, are no longer able to differentiate state-of-the-art T2V models.\nSecond, event-level temporal causality, which not only distinguishes video from\nother modalities but also constitutes a crucial component of world models, is\nseverely underexplored in existing benchmarks. Third, existing benchmarks lack\na systematic assessment of world knowledge, which are essential capabilities\nfor building world models. To address these issues, we introduce VideoVerse, a\ncomprehensive benchmark that focuses on evaluating whether a T2V model could\nunderstand complex temporal causality and world knowledge in the real world. We\ncollect representative videos across diverse domains (e.g., natural landscapes,\nsports, indoor scenes, science fiction, chemical and physical experiments) and\nextract their event-level descriptions with inherent temporal causality, which\nare then rewritten into text-to-video prompts by independent annotators. For\neach prompt, we design a suite of binary evaluation questions from the\nperspective of dynamic and static properties, with a total of ten carefully\ndefined evaluation dimensions. In total, our VideoVerse comprises 300 carefully\ncurated prompts, involving 815 events and 793 binary evaluation questions.\nConsequently, a human preference aligned QA-based evaluation pipeline is\ndeveloped by using modern vision-language models. Finally, we perform a\nsystematic evaluation of state-of-the-art open-source and closed-source T2V\nmodels on VideoVerse, providing in-depth analysis on how far the current T2V\ngenerators are from world models.\n", "link": "http://arxiv.org/abs/2510.08398v2", "date": "2025-10-21", "relevancy": 2.4815, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6362}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoVerse%3A%20How%20Far%20is%20Your%20T2V%20Generator%20from%20a%20World%20Model%3F&body=Title%3A%20VideoVerse%3A%20How%20Far%20is%20Your%20T2V%20Generator%20from%20a%20World%20Model%3F%0AAuthor%3A%20Zeqing%20Wang%20and%20Xinyu%20Wei%20and%20Bairui%20Li%20and%20Zhen%20Guo%20and%20Jinrui%20Zhang%20and%20Hongyang%20Wei%20and%20Keze%20Wang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20The%20recent%20rapid%20advancement%20of%20Text-to-Video%20%28T2V%29%20generation%20technologies%2C%0Awhich%20are%20critical%20to%20build%20%60%60world%20models%27%27%2C%20makes%20the%20existing%20benchmarks%0Aincreasingly%20insufficient%20to%20evaluate%20state-of-the-art%20T2V%20models.%20First%2C%0Acurrent%20evaluation%20dimensions%2C%20such%20as%20per-frame%20aesthetic%20quality%20and%20temporal%0Aconsistency%2C%20are%20no%20longer%20able%20to%20differentiate%20state-of-the-art%20T2V%20models.%0ASecond%2C%20event-level%20temporal%20causality%2C%20which%20not%20only%20distinguishes%20video%20from%0Aother%20modalities%20but%20also%20constitutes%20a%20crucial%20component%20of%20world%20models%2C%20is%0Aseverely%20underexplored%20in%20existing%20benchmarks.%20Third%2C%20existing%20benchmarks%20lack%0Aa%20systematic%20assessment%20of%20world%20knowledge%2C%20which%20are%20essential%20capabilities%0Afor%20building%20world%20models.%20To%20address%20these%20issues%2C%20we%20introduce%20VideoVerse%2C%20a%0Acomprehensive%20benchmark%20that%20focuses%20on%20evaluating%20whether%20a%20T2V%20model%20could%0Aunderstand%20complex%20temporal%20causality%20and%20world%20knowledge%20in%20the%20real%20world.%20We%0Acollect%20representative%20videos%20across%20diverse%20domains%20%28e.g.%2C%20natural%20landscapes%2C%0Asports%2C%20indoor%20scenes%2C%20science%20fiction%2C%20chemical%20and%20physical%20experiments%29%20and%0Aextract%20their%20event-level%20descriptions%20with%20inherent%20temporal%20causality%2C%20which%0Aare%20then%20rewritten%20into%20text-to-video%20prompts%20by%20independent%20annotators.%20For%0Aeach%20prompt%2C%20we%20design%20a%20suite%20of%20binary%20evaluation%20questions%20from%20the%0Aperspective%20of%20dynamic%20and%20static%20properties%2C%20with%20a%20total%20of%20ten%20carefully%0Adefined%20evaluation%20dimensions.%20In%20total%2C%20our%20VideoVerse%20comprises%20300%20carefully%0Acurated%20prompts%2C%20involving%20815%20events%20and%20793%20binary%20evaluation%20questions.%0AConsequently%2C%20a%20human%20preference%20aligned%20QA-based%20evaluation%20pipeline%20is%0Adeveloped%20by%20using%20modern%20vision-language%20models.%20Finally%2C%20we%20perform%20a%0Asystematic%20evaluation%20of%20state-of-the-art%20open-source%20and%20closed-source%20T2V%0Amodels%20on%20VideoVerse%2C%20providing%20in-depth%20analysis%20on%20how%20far%20the%20current%20T2V%0Agenerators%20are%20from%20world%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoVerse%253A%2520How%2520Far%2520is%2520Your%2520T2V%2520Generator%2520from%2520a%2520World%2520Model%253F%26entry.906535625%3DZeqing%2520Wang%2520and%2520Xinyu%2520Wei%2520and%2520Bairui%2520Li%2520and%2520Zhen%2520Guo%2520and%2520Jinrui%2520Zhang%2520and%2520Hongyang%2520Wei%2520and%2520Keze%2520Wang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520recent%2520rapid%2520advancement%2520of%2520Text-to-Video%2520%2528T2V%2529%2520generation%2520technologies%252C%250Awhich%2520are%2520critical%2520to%2520build%2520%2560%2560world%2520models%2527%2527%252C%2520makes%2520the%2520existing%2520benchmarks%250Aincreasingly%2520insufficient%2520to%2520evaluate%2520state-of-the-art%2520T2V%2520models.%2520First%252C%250Acurrent%2520evaluation%2520dimensions%252C%2520such%2520as%2520per-frame%2520aesthetic%2520quality%2520and%2520temporal%250Aconsistency%252C%2520are%2520no%2520longer%2520able%2520to%2520differentiate%2520state-of-the-art%2520T2V%2520models.%250ASecond%252C%2520event-level%2520temporal%2520causality%252C%2520which%2520not%2520only%2520distinguishes%2520video%2520from%250Aother%2520modalities%2520but%2520also%2520constitutes%2520a%2520crucial%2520component%2520of%2520world%2520models%252C%2520is%250Aseverely%2520underexplored%2520in%2520existing%2520benchmarks.%2520Third%252C%2520existing%2520benchmarks%2520lack%250Aa%2520systematic%2520assessment%2520of%2520world%2520knowledge%252C%2520which%2520are%2520essential%2520capabilities%250Afor%2520building%2520world%2520models.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520VideoVerse%252C%2520a%250Acomprehensive%2520benchmark%2520that%2520focuses%2520on%2520evaluating%2520whether%2520a%2520T2V%2520model%2520could%250Aunderstand%2520complex%2520temporal%2520causality%2520and%2520world%2520knowledge%2520in%2520the%2520real%2520world.%2520We%250Acollect%2520representative%2520videos%2520across%2520diverse%2520domains%2520%2528e.g.%252C%2520natural%2520landscapes%252C%250Asports%252C%2520indoor%2520scenes%252C%2520science%2520fiction%252C%2520chemical%2520and%2520physical%2520experiments%2529%2520and%250Aextract%2520their%2520event-level%2520descriptions%2520with%2520inherent%2520temporal%2520causality%252C%2520which%250Aare%2520then%2520rewritten%2520into%2520text-to-video%2520prompts%2520by%2520independent%2520annotators.%2520For%250Aeach%2520prompt%252C%2520we%2520design%2520a%2520suite%2520of%2520binary%2520evaluation%2520questions%2520from%2520the%250Aperspective%2520of%2520dynamic%2520and%2520static%2520properties%252C%2520with%2520a%2520total%2520of%2520ten%2520carefully%250Adefined%2520evaluation%2520dimensions.%2520In%2520total%252C%2520our%2520VideoVerse%2520comprises%2520300%2520carefully%250Acurated%2520prompts%252C%2520involving%2520815%2520events%2520and%2520793%2520binary%2520evaluation%2520questions.%250AConsequently%252C%2520a%2520human%2520preference%2520aligned%2520QA-based%2520evaluation%2520pipeline%2520is%250Adeveloped%2520by%2520using%2520modern%2520vision-language%2520models.%2520Finally%252C%2520we%2520perform%2520a%250Asystematic%2520evaluation%2520of%2520state-of-the-art%2520open-source%2520and%2520closed-source%2520T2V%250Amodels%2520on%2520VideoVerse%252C%2520providing%2520in-depth%2520analysis%2520on%2520how%2520far%2520the%2520current%2520T2V%250Agenerators%2520are%2520from%2520world%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoVerse%3A%20How%20Far%20is%20Your%20T2V%20Generator%20from%20a%20World%20Model%3F&entry.906535625=Zeqing%20Wang%20and%20Xinyu%20Wei%20and%20Bairui%20Li%20and%20Zhen%20Guo%20and%20Jinrui%20Zhang%20and%20Hongyang%20Wei%20and%20Keze%20Wang%20and%20Lei%20Zhang&entry.1292438233=%20%20The%20recent%20rapid%20advancement%20of%20Text-to-Video%20%28T2V%29%20generation%20technologies%2C%0Awhich%20are%20critical%20to%20build%20%60%60world%20models%27%27%2C%20makes%20the%20existing%20benchmarks%0Aincreasingly%20insufficient%20to%20evaluate%20state-of-the-art%20T2V%20models.%20First%2C%0Acurrent%20evaluation%20dimensions%2C%20such%20as%20per-frame%20aesthetic%20quality%20and%20temporal%0Aconsistency%2C%20are%20no%20longer%20able%20to%20differentiate%20state-of-the-art%20T2V%20models.%0ASecond%2C%20event-level%20temporal%20causality%2C%20which%20not%20only%20distinguishes%20video%20from%0Aother%20modalities%20but%20also%20constitutes%20a%20crucial%20component%20of%20world%20models%2C%20is%0Aseverely%20underexplored%20in%20existing%20benchmarks.%20Third%2C%20existing%20benchmarks%20lack%0Aa%20systematic%20assessment%20of%20world%20knowledge%2C%20which%20are%20essential%20capabilities%0Afor%20building%20world%20models.%20To%20address%20these%20issues%2C%20we%20introduce%20VideoVerse%2C%20a%0Acomprehensive%20benchmark%20that%20focuses%20on%20evaluating%20whether%20a%20T2V%20model%20could%0Aunderstand%20complex%20temporal%20causality%20and%20world%20knowledge%20in%20the%20real%20world.%20We%0Acollect%20representative%20videos%20across%20diverse%20domains%20%28e.g.%2C%20natural%20landscapes%2C%0Asports%2C%20indoor%20scenes%2C%20science%20fiction%2C%20chemical%20and%20physical%20experiments%29%20and%0Aextract%20their%20event-level%20descriptions%20with%20inherent%20temporal%20causality%2C%20which%0Aare%20then%20rewritten%20into%20text-to-video%20prompts%20by%20independent%20annotators.%20For%0Aeach%20prompt%2C%20we%20design%20a%20suite%20of%20binary%20evaluation%20questions%20from%20the%0Aperspective%20of%20dynamic%20and%20static%20properties%2C%20with%20a%20total%20of%20ten%20carefully%0Adefined%20evaluation%20dimensions.%20In%20total%2C%20our%20VideoVerse%20comprises%20300%20carefully%0Acurated%20prompts%2C%20involving%20815%20events%20and%20793%20binary%20evaluation%20questions.%0AConsequently%2C%20a%20human%20preference%20aligned%20QA-based%20evaluation%20pipeline%20is%0Adeveloped%20by%20using%20modern%20vision-language%20models.%20Finally%2C%20we%20perform%20a%0Asystematic%20evaluation%20of%20state-of-the-art%20open-source%20and%20closed-source%20T2V%0Amodels%20on%20VideoVerse%2C%20providing%20in-depth%20analysis%20on%20how%20far%20the%20current%20T2V%0Agenerators%20are%20from%20world%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08398v2&entry.124074799=Read"},
{"title": "Understanding Reinforcement Learning for Model Training, and future\n  directions with GRAPE", "author": "Rohit Patel", "abstract": "  This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented.\n", "link": "http://arxiv.org/abs/2509.04501v2", "date": "2025-10-21", "relevancy": 2.4551, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Reinforcement%20Learning%20for%20Model%20Training%2C%20and%20future%0A%20%20directions%20with%20GRAPE&body=Title%3A%20Understanding%20Reinforcement%20Learning%20for%20Model%20Training%2C%20and%20future%0A%20%20directions%20with%20GRAPE%0AAuthor%3A%20Rohit%20Patel%0AAbstract%3A%20%20%20This%20paper%20provides%20a%20self-contained%2C%20from-scratch%2C%20exposition%20of%20key%0Aalgorithms%20for%20instruction%20tuning%20of%20models%3A%20SFT%2C%20Rejection%20Sampling%2C%0AREINFORCE%2C%20Trust%20Region%20Policy%20Optimization%20%28TRPO%29%2C%20Proximal%20Policy%0AOptimization%20%28PPO%29%2C%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20and%20Direct%0APreference%20Optimization%20%28DPO%29.%20Explanations%20of%20these%20algorithms%20often%20assume%0Aprior%20knowledge%2C%20lack%20critical%20details%2C%20and/or%20are%20overly%20generalized%20and%0Acomplex.%20Here%2C%20each%20method%20is%20discussed%20and%20developed%20step%20by%20step%20using%0Asimplified%20and%20explicit%20notation%20focused%20on%20LLMs%2C%20aiming%20to%20eliminate%20ambiguity%0Aand%20provide%20a%20clear%20and%20intuitive%20understanding%20of%20the%20concepts.%20By%20minimizing%0Adetours%20into%20the%20broader%20RL%20literature%20and%20connecting%20concepts%20to%20LLMs%2C%20we%0Aeliminate%20superfluous%20abstractions%20and%20reduce%20cognitive%20overhead.%20Following%0Athis%20exposition%2C%20we%20provide%20a%20literature%20review%20of%20new%20techniques%20and%0Aapproaches%20beyond%20those%20detailed.%20Finally%2C%20new%20ideas%20for%20research%20and%0Aexploration%20in%20the%20form%20of%20GRAPE%20%28Generalized%20Relative%20Advantage%20Policy%0AEvolution%29%20are%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Reinforcement%2520Learning%2520for%2520Model%2520Training%252C%2520and%2520future%250A%2520%2520directions%2520with%2520GRAPE%26entry.906535625%3DRohit%2520Patel%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520a%2520self-contained%252C%2520from-scratch%252C%2520exposition%2520of%2520key%250Aalgorithms%2520for%2520instruction%2520tuning%2520of%2520models%253A%2520SFT%252C%2520Rejection%2520Sampling%252C%250AREINFORCE%252C%2520Trust%2520Region%2520Policy%2520Optimization%2520%2528TRPO%2529%252C%2520Proximal%2520Policy%250AOptimization%2520%2528PPO%2529%252C%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520and%2520Direct%250APreference%2520Optimization%2520%2528DPO%2529.%2520Explanations%2520of%2520these%2520algorithms%2520often%2520assume%250Aprior%2520knowledge%252C%2520lack%2520critical%2520details%252C%2520and/or%2520are%2520overly%2520generalized%2520and%250Acomplex.%2520Here%252C%2520each%2520method%2520is%2520discussed%2520and%2520developed%2520step%2520by%2520step%2520using%250Asimplified%2520and%2520explicit%2520notation%2520focused%2520on%2520LLMs%252C%2520aiming%2520to%2520eliminate%2520ambiguity%250Aand%2520provide%2520a%2520clear%2520and%2520intuitive%2520understanding%2520of%2520the%2520concepts.%2520By%2520minimizing%250Adetours%2520into%2520the%2520broader%2520RL%2520literature%2520and%2520connecting%2520concepts%2520to%2520LLMs%252C%2520we%250Aeliminate%2520superfluous%2520abstractions%2520and%2520reduce%2520cognitive%2520overhead.%2520Following%250Athis%2520exposition%252C%2520we%2520provide%2520a%2520literature%2520review%2520of%2520new%2520techniques%2520and%250Aapproaches%2520beyond%2520those%2520detailed.%2520Finally%252C%2520new%2520ideas%2520for%2520research%2520and%250Aexploration%2520in%2520the%2520form%2520of%2520GRAPE%2520%2528Generalized%2520Relative%2520Advantage%2520Policy%250AEvolution%2529%2520are%2520presented.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Reinforcement%20Learning%20for%20Model%20Training%2C%20and%20future%0A%20%20directions%20with%20GRAPE&entry.906535625=Rohit%20Patel&entry.1292438233=%20%20This%20paper%20provides%20a%20self-contained%2C%20from-scratch%2C%20exposition%20of%20key%0Aalgorithms%20for%20instruction%20tuning%20of%20models%3A%20SFT%2C%20Rejection%20Sampling%2C%0AREINFORCE%2C%20Trust%20Region%20Policy%20Optimization%20%28TRPO%29%2C%20Proximal%20Policy%0AOptimization%20%28PPO%29%2C%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20and%20Direct%0APreference%20Optimization%20%28DPO%29.%20Explanations%20of%20these%20algorithms%20often%20assume%0Aprior%20knowledge%2C%20lack%20critical%20details%2C%20and/or%20are%20overly%20generalized%20and%0Acomplex.%20Here%2C%20each%20method%20is%20discussed%20and%20developed%20step%20by%20step%20using%0Asimplified%20and%20explicit%20notation%20focused%20on%20LLMs%2C%20aiming%20to%20eliminate%20ambiguity%0Aand%20provide%20a%20clear%20and%20intuitive%20understanding%20of%20the%20concepts.%20By%20minimizing%0Adetours%20into%20the%20broader%20RL%20literature%20and%20connecting%20concepts%20to%20LLMs%2C%20we%0Aeliminate%20superfluous%20abstractions%20and%20reduce%20cognitive%20overhead.%20Following%0Athis%20exposition%2C%20we%20provide%20a%20literature%20review%20of%20new%20techniques%20and%0Aapproaches%20beyond%20those%20detailed.%20Finally%2C%20new%20ideas%20for%20research%20and%0Aexploration%20in%20the%20form%20of%20GRAPE%20%28Generalized%20Relative%20Advantage%20Policy%0AEvolution%29%20are%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04501v2&entry.124074799=Read"},
{"title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal\n  Unalignment for Large Language Model", "author": "Jinwei Hu and Zhenglin Huang and Xiangyu Yin and Wenjie Ruan and Guangliang Cheng and Yi Dong and Xiaowei Huang", "abstract": "  Large language models have been widely applied, but can inadvertently encode\nsensitive or harmful information, raising significant safety concerns. Machine\nunlearning has emerged to alleviate this concern; however, existing\ntraining-time unlearning approaches, relying on coarse-grained loss\ncombinations, have limitations in precisely separating knowledge and balancing\nremoval effectiveness with model utility. In contrast, we propose Fine-grained\nActivation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel\nrepresentation-guided unlearning approach that leverages information-theoretic\nguidance for efficient parameter selection, employs contrastive mechanisms to\nenhance representation separation, and projects conflict gradients onto\northogonal subspaces to resolve conflicts between forgetting and retention\nobjectives. Extensive experiments demonstrate that FALCON achieves superior\nunlearning effectiveness while maintaining model utility, exhibiting robust\nresistance against knowledge recovery attempts.\n", "link": "http://arxiv.org/abs/2502.01472v3", "date": "2025-10-21", "relevancy": 2.4341, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4916}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model&body=Title%3A%20FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model%0AAuthor%3A%20Jinwei%20Hu%20and%20Zhenglin%20Huang%20and%20Xiangyu%20Yin%20and%20Wenjie%20Ruan%20and%20Guangliang%20Cheng%20and%20Yi%20Dong%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20been%20widely%20applied%2C%20but%20can%20inadvertently%20encode%0Asensitive%20or%20harmful%20information%2C%20raising%20significant%20safety%20concerns.%20Machine%0Aunlearning%20has%20emerged%20to%20alleviate%20this%20concern%3B%20however%2C%20existing%0Atraining-time%20unlearning%20approaches%2C%20relying%20on%20coarse-grained%20loss%0Acombinations%2C%20have%20limitations%20in%20precisely%20separating%20knowledge%20and%20balancing%0Aremoval%20effectiveness%20with%20model%20utility.%20In%20contrast%2C%20we%20propose%20Fine-grained%0AActivation%20manipuLation%20by%20Contrastive%20Orthogonal%20uNalignment%20%28FALCON%29%2C%20a%20novel%0Arepresentation-guided%20unlearning%20approach%20that%20leverages%20information-theoretic%0Aguidance%20for%20efficient%20parameter%20selection%2C%20employs%20contrastive%20mechanisms%20to%0Aenhance%20representation%20separation%2C%20and%20projects%20conflict%20gradients%20onto%0Aorthogonal%20subspaces%20to%20resolve%20conflicts%20between%20forgetting%20and%20retention%0Aobjectives.%20Extensive%20experiments%20demonstrate%20that%20FALCON%20achieves%20superior%0Aunlearning%20effectiveness%20while%20maintaining%20model%20utility%2C%20exhibiting%20robust%0Aresistance%20against%20knowledge%20recovery%20attempts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01472v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFALCON%253A%2520Fine-grained%2520Activation%2520Manipulation%2520by%2520Contrastive%2520Orthogonal%250A%2520%2520Unalignment%2520for%2520Large%2520Language%2520Model%26entry.906535625%3DJinwei%2520Hu%2520and%2520Zhenglin%2520Huang%2520and%2520Xiangyu%2520Yin%2520and%2520Wenjie%2520Ruan%2520and%2520Guangliang%2520Cheng%2520and%2520Yi%2520Dong%2520and%2520Xiaowei%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520been%2520widely%2520applied%252C%2520but%2520can%2520inadvertently%2520encode%250Asensitive%2520or%2520harmful%2520information%252C%2520raising%2520significant%2520safety%2520concerns.%2520Machine%250Aunlearning%2520has%2520emerged%2520to%2520alleviate%2520this%2520concern%253B%2520however%252C%2520existing%250Atraining-time%2520unlearning%2520approaches%252C%2520relying%2520on%2520coarse-grained%2520loss%250Acombinations%252C%2520have%2520limitations%2520in%2520precisely%2520separating%2520knowledge%2520and%2520balancing%250Aremoval%2520effectiveness%2520with%2520model%2520utility.%2520In%2520contrast%252C%2520we%2520propose%2520Fine-grained%250AActivation%2520manipuLation%2520by%2520Contrastive%2520Orthogonal%2520uNalignment%2520%2528FALCON%2529%252C%2520a%2520novel%250Arepresentation-guided%2520unlearning%2520approach%2520that%2520leverages%2520information-theoretic%250Aguidance%2520for%2520efficient%2520parameter%2520selection%252C%2520employs%2520contrastive%2520mechanisms%2520to%250Aenhance%2520representation%2520separation%252C%2520and%2520projects%2520conflict%2520gradients%2520onto%250Aorthogonal%2520subspaces%2520to%2520resolve%2520conflicts%2520between%2520forgetting%2520and%2520retention%250Aobjectives.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FALCON%2520achieves%2520superior%250Aunlearning%2520effectiveness%2520while%2520maintaining%2520model%2520utility%252C%2520exhibiting%2520robust%250Aresistance%2520against%2520knowledge%2520recovery%2520attempts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01472v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALCON%3A%20Fine-grained%20Activation%20Manipulation%20by%20Contrastive%20Orthogonal%0A%20%20Unalignment%20for%20Large%20Language%20Model&entry.906535625=Jinwei%20Hu%20and%20Zhenglin%20Huang%20and%20Xiangyu%20Yin%20and%20Wenjie%20Ruan%20and%20Guangliang%20Cheng%20and%20Yi%20Dong%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Large%20language%20models%20have%20been%20widely%20applied%2C%20but%20can%20inadvertently%20encode%0Asensitive%20or%20harmful%20information%2C%20raising%20significant%20safety%20concerns.%20Machine%0Aunlearning%20has%20emerged%20to%20alleviate%20this%20concern%3B%20however%2C%20existing%0Atraining-time%20unlearning%20approaches%2C%20relying%20on%20coarse-grained%20loss%0Acombinations%2C%20have%20limitations%20in%20precisely%20separating%20knowledge%20and%20balancing%0Aremoval%20effectiveness%20with%20model%20utility.%20In%20contrast%2C%20we%20propose%20Fine-grained%0AActivation%20manipuLation%20by%20Contrastive%20Orthogonal%20uNalignment%20%28FALCON%29%2C%20a%20novel%0Arepresentation-guided%20unlearning%20approach%20that%20leverages%20information-theoretic%0Aguidance%20for%20efficient%20parameter%20selection%2C%20employs%20contrastive%20mechanisms%20to%0Aenhance%20representation%20separation%2C%20and%20projects%20conflict%20gradients%20onto%0Aorthogonal%20subspaces%20to%20resolve%20conflicts%20between%20forgetting%20and%20retention%0Aobjectives.%20Extensive%20experiments%20demonstrate%20that%20FALCON%20achieves%20superior%0Aunlearning%20effectiveness%20while%20maintaining%20model%20utility%2C%20exhibiting%20robust%0Aresistance%20against%20knowledge%20recovery%20attempts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01472v3&entry.124074799=Read"},
{"title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation", "author": "Weinan Jia and Yuning Lu and Mengqi Huang and Hualiang Wang and Binyuan Huang and Nan Chen and Mu Liu and Jidong Jiang and Zhendong Mao", "abstract": "  Long video generation with Diffusion Transformers (DiTs) is bottlenecked by\nthe quadratic scaling of full attention with sequence length. Since attention\nis highly redundant, outputs are dominated by a small subset of query-key\npairs. Existing sparse methods rely on blockwise coarse estimation, whose\naccuracy-efficiency trade-offs are constrained by block size. This paper\nintroduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention\nthat uses a lightweight, learnable token router to precisely match tokens\nwithout blockwise estimation. Through semantic-aware routing, MoGA enables\neffective long-range interactions. As a kernel-free method, MoGA integrates\nseamlessly with modern attention stacks, including FlashAttention and sequence\nparallelism. Building on MoGA, we develop an efficient long video generation\nmodel that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps,\nwith a context length of approximately 580k. Comprehensive experiments on\nvarious video generation tasks validate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2510.18692v1", "date": "2025-10-21", "relevancy": 2.4149, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6156}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6015}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoGA%3A%20Mixture-of-Groups%20Attention%20for%20End-to-End%20Long%20Video%20Generation&body=Title%3A%20MoGA%3A%20Mixture-of-Groups%20Attention%20for%20End-to-End%20Long%20Video%20Generation%0AAuthor%3A%20Weinan%20Jia%20and%20Yuning%20Lu%20and%20Mengqi%20Huang%20and%20Hualiang%20Wang%20and%20Binyuan%20Huang%20and%20Nan%20Chen%20and%20Mu%20Liu%20and%20Jidong%20Jiang%20and%20Zhendong%20Mao%0AAbstract%3A%20%20%20Long%20video%20generation%20with%20Diffusion%20Transformers%20%28DiTs%29%20is%20bottlenecked%20by%0Athe%20quadratic%20scaling%20of%20full%20attention%20with%20sequence%20length.%20Since%20attention%0Ais%20highly%20redundant%2C%20outputs%20are%20dominated%20by%20a%20small%20subset%20of%20query-key%0Apairs.%20Existing%20sparse%20methods%20rely%20on%20blockwise%20coarse%20estimation%2C%20whose%0Aaccuracy-efficiency%20trade-offs%20are%20constrained%20by%20block%20size.%20This%20paper%0Aintroduces%20Mixture-of-Groups%20Attention%20%28MoGA%29%2C%20an%20efficient%20sparse%20attention%0Athat%20uses%20a%20lightweight%2C%20learnable%20token%20router%20to%20precisely%20match%20tokens%0Awithout%20blockwise%20estimation.%20Through%20semantic-aware%20routing%2C%20MoGA%20enables%0Aeffective%20long-range%20interactions.%20As%20a%20kernel-free%20method%2C%20MoGA%20integrates%0Aseamlessly%20with%20modern%20attention%20stacks%2C%20including%20FlashAttention%20and%20sequence%0Aparallelism.%20Building%20on%20MoGA%2C%20we%20develop%20an%20efficient%20long%20video%20generation%0Amodel%20that%20end-to-end%20produces%20minute-level%2C%20multi-shot%2C%20480p%20videos%20at%2024%20fps%2C%0Awith%20a%20context%20length%20of%20approximately%20580k.%20Comprehensive%20experiments%20on%0Avarious%20video%20generation%20tasks%20validate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoGA%253A%2520Mixture-of-Groups%2520Attention%2520for%2520End-to-End%2520Long%2520Video%2520Generation%26entry.906535625%3DWeinan%2520Jia%2520and%2520Yuning%2520Lu%2520and%2520Mengqi%2520Huang%2520and%2520Hualiang%2520Wang%2520and%2520Binyuan%2520Huang%2520and%2520Nan%2520Chen%2520and%2520Mu%2520Liu%2520and%2520Jidong%2520Jiang%2520and%2520Zhendong%2520Mao%26entry.1292438233%3D%2520%2520Long%2520video%2520generation%2520with%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520is%2520bottlenecked%2520by%250Athe%2520quadratic%2520scaling%2520of%2520full%2520attention%2520with%2520sequence%2520length.%2520Since%2520attention%250Ais%2520highly%2520redundant%252C%2520outputs%2520are%2520dominated%2520by%2520a%2520small%2520subset%2520of%2520query-key%250Apairs.%2520Existing%2520sparse%2520methods%2520rely%2520on%2520blockwise%2520coarse%2520estimation%252C%2520whose%250Aaccuracy-efficiency%2520trade-offs%2520are%2520constrained%2520by%2520block%2520size.%2520This%2520paper%250Aintroduces%2520Mixture-of-Groups%2520Attention%2520%2528MoGA%2529%252C%2520an%2520efficient%2520sparse%2520attention%250Athat%2520uses%2520a%2520lightweight%252C%2520learnable%2520token%2520router%2520to%2520precisely%2520match%2520tokens%250Awithout%2520blockwise%2520estimation.%2520Through%2520semantic-aware%2520routing%252C%2520MoGA%2520enables%250Aeffective%2520long-range%2520interactions.%2520As%2520a%2520kernel-free%2520method%252C%2520MoGA%2520integrates%250Aseamlessly%2520with%2520modern%2520attention%2520stacks%252C%2520including%2520FlashAttention%2520and%2520sequence%250Aparallelism.%2520Building%2520on%2520MoGA%252C%2520we%2520develop%2520an%2520efficient%2520long%2520video%2520generation%250Amodel%2520that%2520end-to-end%2520produces%2520minute-level%252C%2520multi-shot%252C%2520480p%2520videos%2520at%252024%2520fps%252C%250Awith%2520a%2520context%2520length%2520of%2520approximately%2520580k.%2520Comprehensive%2520experiments%2520on%250Avarious%2520video%2520generation%2520tasks%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoGA%3A%20Mixture-of-Groups%20Attention%20for%20End-to-End%20Long%20Video%20Generation&entry.906535625=Weinan%20Jia%20and%20Yuning%20Lu%20and%20Mengqi%20Huang%20and%20Hualiang%20Wang%20and%20Binyuan%20Huang%20and%20Nan%20Chen%20and%20Mu%20Liu%20and%20Jidong%20Jiang%20and%20Zhendong%20Mao&entry.1292438233=%20%20Long%20video%20generation%20with%20Diffusion%20Transformers%20%28DiTs%29%20is%20bottlenecked%20by%0Athe%20quadratic%20scaling%20of%20full%20attention%20with%20sequence%20length.%20Since%20attention%0Ais%20highly%20redundant%2C%20outputs%20are%20dominated%20by%20a%20small%20subset%20of%20query-key%0Apairs.%20Existing%20sparse%20methods%20rely%20on%20blockwise%20coarse%20estimation%2C%20whose%0Aaccuracy-efficiency%20trade-offs%20are%20constrained%20by%20block%20size.%20This%20paper%0Aintroduces%20Mixture-of-Groups%20Attention%20%28MoGA%29%2C%20an%20efficient%20sparse%20attention%0Athat%20uses%20a%20lightweight%2C%20learnable%20token%20router%20to%20precisely%20match%20tokens%0Awithout%20blockwise%20estimation.%20Through%20semantic-aware%20routing%2C%20MoGA%20enables%0Aeffective%20long-range%20interactions.%20As%20a%20kernel-free%20method%2C%20MoGA%20integrates%0Aseamlessly%20with%20modern%20attention%20stacks%2C%20including%20FlashAttention%20and%20sequence%0Aparallelism.%20Building%20on%20MoGA%2C%20we%20develop%20an%20efficient%20long%20video%20generation%0Amodel%20that%20end-to-end%20produces%20minute-level%2C%20multi-shot%2C%20480p%20videos%20at%2024%20fps%2C%0Awith%20a%20context%20length%20of%20approximately%20580k.%20Comprehensive%20experiments%20on%0Avarious%20video%20generation%20tasks%20validate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18692v1&entry.124074799=Read"},
{"title": "Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement\n  Learning", "author": "Jongchan Park and Mingyu Park and Donghwan Lee", "abstract": "  Offline reinforcement learning (RL) aims to learn a policy from a static\ndataset without further interactions with the environment. Collecting\nsufficiently large datasets for offline RL is exhausting since this data\ncollection requires colossus interactions with environments and becomes tricky\nwhen the interaction with the environment is restricted. Hence, how an agent\nlearns the best policy with a minimal static dataset is a crucial issue in\noffline RL, similar to the sample efficiency problem in online RL. In this\npaper, we propose a simple yet effective plug-and-play pretraining method to\ninitialize a feature of a Q-network to enhance data efficiency in offline RL.\nSpecifically, we introduce a shared Q-network structure that outputs\npredictions of the next state and Q-value. We pretrain the shared Q-network\nthrough a supervised regression task that predicts a next state and trains the\nshared Q-network using diverse offline RL methods. Through extensive\nexperiments, we empirically demonstrate that our method enhances the\nperformance of existing popular offline RL methods on the D4RL, Robomimic and\nV-D4RL benchmarks. Furthermore, we show that our method significantly boosts\ndata-efficient offline RL across various data qualities and data distributions\ntrough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of\nthe dataset outperforms standard algorithms even with full datasets.\n", "link": "http://arxiv.org/abs/2505.05701v2", "date": "2025-10-21", "relevancy": 2.4132, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4846}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4843}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretraining%20a%20Shared%20Q-Network%20for%20Data-Efficient%20Offline%20Reinforcement%0A%20%20Learning&body=Title%3A%20Pretraining%20a%20Shared%20Q-Network%20for%20Data-Efficient%20Offline%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Jongchan%20Park%20and%20Mingyu%20Park%20and%20Donghwan%20Lee%0AAbstract%3A%20%20%20Offline%20reinforcement%20learning%20%28RL%29%20aims%20to%20learn%20a%20policy%20from%20a%20static%0Adataset%20without%20further%20interactions%20with%20the%20environment.%20Collecting%0Asufficiently%20large%20datasets%20for%20offline%20RL%20is%20exhausting%20since%20this%20data%0Acollection%20requires%20colossus%20interactions%20with%20environments%20and%20becomes%20tricky%0Awhen%20the%20interaction%20with%20the%20environment%20is%20restricted.%20Hence%2C%20how%20an%20agent%0Alearns%20the%20best%20policy%20with%20a%20minimal%20static%20dataset%20is%20a%20crucial%20issue%20in%0Aoffline%20RL%2C%20similar%20to%20the%20sample%20efficiency%20problem%20in%20online%20RL.%20In%20this%0Apaper%2C%20we%20propose%20a%20simple%20yet%20effective%20plug-and-play%20pretraining%20method%20to%0Ainitialize%20a%20feature%20of%20a%20Q-network%20to%20enhance%20data%20efficiency%20in%20offline%20RL.%0ASpecifically%2C%20we%20introduce%20a%20shared%20Q-network%20structure%20that%20outputs%0Apredictions%20of%20the%20next%20state%20and%20Q-value.%20We%20pretrain%20the%20shared%20Q-network%0Athrough%20a%20supervised%20regression%20task%20that%20predicts%20a%20next%20state%20and%20trains%20the%0Ashared%20Q-network%20using%20diverse%20offline%20RL%20methods.%20Through%20extensive%0Aexperiments%2C%20we%20empirically%20demonstrate%20that%20our%20method%20enhances%20the%0Aperformance%20of%20existing%20popular%20offline%20RL%20methods%20on%20the%20D4RL%2C%20Robomimic%20and%0AV-D4RL%20benchmarks.%20Furthermore%2C%20we%20show%20that%20our%20method%20significantly%20boosts%0Adata-efficient%20offline%20RL%20across%20various%20data%20qualities%20and%20data%20distributions%0Atrough%20D4RL%20and%20ExoRL%20benchmarks.%20Notably%2C%20our%20method%20adapted%20with%20only%2010%25%20of%0Athe%20dataset%20outperforms%20standard%20algorithms%20even%20with%20full%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretraining%2520a%2520Shared%2520Q-Network%2520for%2520Data-Efficient%2520Offline%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DJongchan%2520Park%2520and%2520Mingyu%2520Park%2520and%2520Donghwan%2520Lee%26entry.1292438233%3D%2520%2520Offline%2520reinforcement%2520learning%2520%2528RL%2529%2520aims%2520to%2520learn%2520a%2520policy%2520from%2520a%2520static%250Adataset%2520without%2520further%2520interactions%2520with%2520the%2520environment.%2520Collecting%250Asufficiently%2520large%2520datasets%2520for%2520offline%2520RL%2520is%2520exhausting%2520since%2520this%2520data%250Acollection%2520requires%2520colossus%2520interactions%2520with%2520environments%2520and%2520becomes%2520tricky%250Awhen%2520the%2520interaction%2520with%2520the%2520environment%2520is%2520restricted.%2520Hence%252C%2520how%2520an%2520agent%250Alearns%2520the%2520best%2520policy%2520with%2520a%2520minimal%2520static%2520dataset%2520is%2520a%2520crucial%2520issue%2520in%250Aoffline%2520RL%252C%2520similar%2520to%2520the%2520sample%2520efficiency%2520problem%2520in%2520online%2520RL.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520plug-and-play%2520pretraining%2520method%2520to%250Ainitialize%2520a%2520feature%2520of%2520a%2520Q-network%2520to%2520enhance%2520data%2520efficiency%2520in%2520offline%2520RL.%250ASpecifically%252C%2520we%2520introduce%2520a%2520shared%2520Q-network%2520structure%2520that%2520outputs%250Apredictions%2520of%2520the%2520next%2520state%2520and%2520Q-value.%2520We%2520pretrain%2520the%2520shared%2520Q-network%250Athrough%2520a%2520supervised%2520regression%2520task%2520that%2520predicts%2520a%2520next%2520state%2520and%2520trains%2520the%250Ashared%2520Q-network%2520using%2520diverse%2520offline%2520RL%2520methods.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520empirically%2520demonstrate%2520that%2520our%2520method%2520enhances%2520the%250Aperformance%2520of%2520existing%2520popular%2520offline%2520RL%2520methods%2520on%2520the%2520D4RL%252C%2520Robomimic%2520and%250AV-D4RL%2520benchmarks.%2520Furthermore%252C%2520we%2520show%2520that%2520our%2520method%2520significantly%2520boosts%250Adata-efficient%2520offline%2520RL%2520across%2520various%2520data%2520qualities%2520and%2520data%2520distributions%250Atrough%2520D4RL%2520and%2520ExoRL%2520benchmarks.%2520Notably%252C%2520our%2520method%2520adapted%2520with%2520only%252010%2525%2520of%250Athe%2520dataset%2520outperforms%2520standard%2520algorithms%2520even%2520with%2520full%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretraining%20a%20Shared%20Q-Network%20for%20Data-Efficient%20Offline%20Reinforcement%0A%20%20Learning&entry.906535625=Jongchan%20Park%20and%20Mingyu%20Park%20and%20Donghwan%20Lee&entry.1292438233=%20%20Offline%20reinforcement%20learning%20%28RL%29%20aims%20to%20learn%20a%20policy%20from%20a%20static%0Adataset%20without%20further%20interactions%20with%20the%20environment.%20Collecting%0Asufficiently%20large%20datasets%20for%20offline%20RL%20is%20exhausting%20since%20this%20data%0Acollection%20requires%20colossus%20interactions%20with%20environments%20and%20becomes%20tricky%0Awhen%20the%20interaction%20with%20the%20environment%20is%20restricted.%20Hence%2C%20how%20an%20agent%0Alearns%20the%20best%20policy%20with%20a%20minimal%20static%20dataset%20is%20a%20crucial%20issue%20in%0Aoffline%20RL%2C%20similar%20to%20the%20sample%20efficiency%20problem%20in%20online%20RL.%20In%20this%0Apaper%2C%20we%20propose%20a%20simple%20yet%20effective%20plug-and-play%20pretraining%20method%20to%0Ainitialize%20a%20feature%20of%20a%20Q-network%20to%20enhance%20data%20efficiency%20in%20offline%20RL.%0ASpecifically%2C%20we%20introduce%20a%20shared%20Q-network%20structure%20that%20outputs%0Apredictions%20of%20the%20next%20state%20and%20Q-value.%20We%20pretrain%20the%20shared%20Q-network%0Athrough%20a%20supervised%20regression%20task%20that%20predicts%20a%20next%20state%20and%20trains%20the%0Ashared%20Q-network%20using%20diverse%20offline%20RL%20methods.%20Through%20extensive%0Aexperiments%2C%20we%20empirically%20demonstrate%20that%20our%20method%20enhances%20the%0Aperformance%20of%20existing%20popular%20offline%20RL%20methods%20on%20the%20D4RL%2C%20Robomimic%20and%0AV-D4RL%20benchmarks.%20Furthermore%2C%20we%20show%20that%20our%20method%20significantly%20boosts%0Adata-efficient%20offline%20RL%20across%20various%20data%20qualities%20and%20data%20distributions%0Atrough%20D4RL%20and%20ExoRL%20benchmarks.%20Notably%2C%20our%20method%20adapted%20with%20only%2010%25%20of%0Athe%20dataset%20outperforms%20standard%20algorithms%20even%20with%20full%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05701v2&entry.124074799=Read"},
{"title": "UniVideo: Unified Understanding, Generation, and Editing for Videos", "author": "Cong Wei and Quande Liu and Zixuan Ye and Qiulin Wang and Xintao Wang and Pengfei Wan and Kun Gai and Wenhu Chen", "abstract": "  Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.\n", "link": "http://arxiv.org/abs/2510.08377v2", "date": "2025-10-21", "relevancy": 2.4126, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6165}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos&body=Title%3A%20UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos%0AAuthor%3A%20Cong%20Wei%20and%20Quande%20Liu%20and%20Zixuan%20Ye%20and%20Qiulin%20Wang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Unified%20multimodal%20models%20have%20shown%20promising%20results%20in%20multimodal%20content%0Ageneration%20and%20editing%20but%20remain%20largely%20limited%20to%20the%20image%20domain.%20In%20this%0Awork%2C%20we%20present%20UniVideo%2C%20a%20versatile%20framework%20that%20extends%20unified%20modeling%0Ato%20the%20video%20domain.%20UniVideo%20adopts%20a%20dual-stream%20design%2C%20combining%20a%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29%20for%20instruction%20understanding%20with%20a%0AMultimodal%20DiT%20%28MMDiT%29%20for%20video%20generation.%20This%20design%20enables%20accurate%0Ainterpretation%20of%20complex%20multimodal%20instructions%20while%20preserving%20visual%0Aconsistency.%20Built%20on%20this%20architecture%2C%20UniVideo%20unifies%20diverse%20video%0Ageneration%20and%20editing%20tasks%20under%20a%20single%20multimodal%20instruction%20paradigm%20and%0Ais%20jointly%20trained%20across%20them.%20Extensive%20experiments%20demonstrate%20that%20UniVideo%0Amatches%20or%20surpasses%20state-of-the-art%20task-specific%20baselines%20in%0Atext/image-to-video%20generation%2C%20in-context%20video%20generation%20and%20in-context%0Avideo%20editing.%20Notably%2C%20the%20unified%20design%20of%20UniVideo%20enables%20two%20forms%20of%0Ageneralization.%20First%2C%20UniVideo%20supports%20task%20composition%2C%20such%20as%20combining%0Aediting%20with%20style%20transfer%2C%20by%20integrating%20multiple%20capabilities%20within%20a%0Asingle%20instruction.%20Second%2C%20even%20without%20explicit%20training%20on%20free-form%20video%0Aediting%2C%20UniVideo%20transfers%20its%20editing%20capability%20from%20large-scale%20image%0Aediting%20data%20to%20this%20setting%2C%20handling%20unseen%20instructions%20such%20as%0Agreen-screening%20characters%20or%20changing%20materials%20within%20a%20video.%20Beyond%20these%0Acore%20capabilities%2C%20UniVideo%20also%20supports%20visual-prompt-based%20video%20generation%2C%0Awhere%20the%20MLLM%20interprets%20visual%20prompts%20and%20guides%20the%20MMDiT%20during%20synthesis.%0ATo%20foster%20future%20research%2C%20we%20will%20release%20our%20model%20and%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniVideo%253A%2520Unified%2520Understanding%252C%2520Generation%252C%2520and%2520Editing%2520for%2520Videos%26entry.906535625%3DCong%2520Wei%2520and%2520Quande%2520Liu%2520and%2520Zixuan%2520Ye%2520and%2520Qiulin%2520Wang%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520models%2520have%2520shown%2520promising%2520results%2520in%2520multimodal%2520content%250Ageneration%2520and%2520editing%2520but%2520remain%2520largely%2520limited%2520to%2520the%2520image%2520domain.%2520In%2520this%250Awork%252C%2520we%2520present%2520UniVideo%252C%2520a%2520versatile%2520framework%2520that%2520extends%2520unified%2520modeling%250Ato%2520the%2520video%2520domain.%2520UniVideo%2520adopts%2520a%2520dual-stream%2520design%252C%2520combining%2520a%250AMultimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520for%2520instruction%2520understanding%2520with%2520a%250AMultimodal%2520DiT%2520%2528MMDiT%2529%2520for%2520video%2520generation.%2520This%2520design%2520enables%2520accurate%250Ainterpretation%2520of%2520complex%2520multimodal%2520instructions%2520while%2520preserving%2520visual%250Aconsistency.%2520Built%2520on%2520this%2520architecture%252C%2520UniVideo%2520unifies%2520diverse%2520video%250Ageneration%2520and%2520editing%2520tasks%2520under%2520a%2520single%2520multimodal%2520instruction%2520paradigm%2520and%250Ais%2520jointly%2520trained%2520across%2520them.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UniVideo%250Amatches%2520or%2520surpasses%2520state-of-the-art%2520task-specific%2520baselines%2520in%250Atext/image-to-video%2520generation%252C%2520in-context%2520video%2520generation%2520and%2520in-context%250Avideo%2520editing.%2520Notably%252C%2520the%2520unified%2520design%2520of%2520UniVideo%2520enables%2520two%2520forms%2520of%250Ageneralization.%2520First%252C%2520UniVideo%2520supports%2520task%2520composition%252C%2520such%2520as%2520combining%250Aediting%2520with%2520style%2520transfer%252C%2520by%2520integrating%2520multiple%2520capabilities%2520within%2520a%250Asingle%2520instruction.%2520Second%252C%2520even%2520without%2520explicit%2520training%2520on%2520free-form%2520video%250Aediting%252C%2520UniVideo%2520transfers%2520its%2520editing%2520capability%2520from%2520large-scale%2520image%250Aediting%2520data%2520to%2520this%2520setting%252C%2520handling%2520unseen%2520instructions%2520such%2520as%250Agreen-screening%2520characters%2520or%2520changing%2520materials%2520within%2520a%2520video.%2520Beyond%2520these%250Acore%2520capabilities%252C%2520UniVideo%2520also%2520supports%2520visual-prompt-based%2520video%2520generation%252C%250Awhere%2520the%2520MLLM%2520interprets%2520visual%2520prompts%2520and%2520guides%2520the%2520MMDiT%2520during%2520synthesis.%250ATo%2520foster%2520future%2520research%252C%2520we%2520will%2520release%2520our%2520model%2520and%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos&entry.906535625=Cong%20Wei%20and%20Quande%20Liu%20and%20Zixuan%20Ye%20and%20Qiulin%20Wang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Wenhu%20Chen&entry.1292438233=%20%20Unified%20multimodal%20models%20have%20shown%20promising%20results%20in%20multimodal%20content%0Ageneration%20and%20editing%20but%20remain%20largely%20limited%20to%20the%20image%20domain.%20In%20this%0Awork%2C%20we%20present%20UniVideo%2C%20a%20versatile%20framework%20that%20extends%20unified%20modeling%0Ato%20the%20video%20domain.%20UniVideo%20adopts%20a%20dual-stream%20design%2C%20combining%20a%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29%20for%20instruction%20understanding%20with%20a%0AMultimodal%20DiT%20%28MMDiT%29%20for%20video%20generation.%20This%20design%20enables%20accurate%0Ainterpretation%20of%20complex%20multimodal%20instructions%20while%20preserving%20visual%0Aconsistency.%20Built%20on%20this%20architecture%2C%20UniVideo%20unifies%20diverse%20video%0Ageneration%20and%20editing%20tasks%20under%20a%20single%20multimodal%20instruction%20paradigm%20and%0Ais%20jointly%20trained%20across%20them.%20Extensive%20experiments%20demonstrate%20that%20UniVideo%0Amatches%20or%20surpasses%20state-of-the-art%20task-specific%20baselines%20in%0Atext/image-to-video%20generation%2C%20in-context%20video%20generation%20and%20in-context%0Avideo%20editing.%20Notably%2C%20the%20unified%20design%20of%20UniVideo%20enables%20two%20forms%20of%0Ageneralization.%20First%2C%20UniVideo%20supports%20task%20composition%2C%20such%20as%20combining%0Aediting%20with%20style%20transfer%2C%20by%20integrating%20multiple%20capabilities%20within%20a%0Asingle%20instruction.%20Second%2C%20even%20without%20explicit%20training%20on%20free-form%20video%0Aediting%2C%20UniVideo%20transfers%20its%20editing%20capability%20from%20large-scale%20image%0Aediting%20data%20to%20this%20setting%2C%20handling%20unseen%20instructions%20such%20as%0Agreen-screening%20characters%20or%20changing%20materials%20within%20a%20video.%20Beyond%20these%0Acore%20capabilities%2C%20UniVideo%20also%20supports%20visual-prompt-based%20video%20generation%2C%0Awhere%20the%20MLLM%20interprets%20visual%20prompts%20and%20guides%20the%20MMDiT%20during%20synthesis.%0ATo%20foster%20future%20research%2C%20we%20will%20release%20our%20model%20and%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08377v2&entry.124074799=Read"},
{"title": "Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial\n  Vehicles", "author": "\u00c0lmos Veres-Vit\u00e0lyos and Genis Castillo Gomez-Raya and Filip Lemic and Daniel Johannes Bugelnig and Bernhard Rinner and Sergi Abadal and Xavier Costa-P\u00e9rez", "abstract": "  Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for\nnavigating indoor and hard-to-reach areas, yet their significant constraints in\npayload and autonomy have largely prevented their use for complex tasks like\nhigh-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we\nintroduce a novel system architecture that enables fully autonomous,\nhigh-fidelity 3D scanning of static objects using UAVs weighing under 100\ngrams. Our core innovation lies in a dual-reconstruction pipeline that creates\na real-time feedback loop between data capture and flight control. A\nnear-real-time (near-RT) process uses Structure from Motion (SfM) to generate\nan instantaneous pointcloud of the object. The system analyzes the model\nquality on the fly and dynamically adapts the UAV's trajectory to intelligently\ncapture new images of poorly covered areas. This ensures comprehensive data\nacquisition. For the final, detailed output, a non-real-time (non-RT) pipeline\nemploys a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR)\napproach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB)\nlocation data to achieve superior accuracy. We implemented and validated this\narchitecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both\nsingle- and multi-UAV configurations, conclusively show that dynamic trajectory\nadaptation consistently improves reconstruction quality over static flight\npaths. This work demonstrates a scalable and autonomous solution that unlocks\nthe potential of miniaturized UAVs for fine-grained 3D reconstruction in\nconstrained environments, a capability previously limited to much larger\nplatforms.\n", "link": "http://arxiv.org/abs/2509.12458v2", "date": "2025-10-21", "relevancy": 2.4053, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6116}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6031}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%203D%20Object%20Reconstruction%20with%20Small-Scale%20Unmanned%20Aerial%0A%20%20Vehicles&body=Title%3A%20Neural%203D%20Object%20Reconstruction%20with%20Small-Scale%20Unmanned%20Aerial%0A%20%20Vehicles%0AAuthor%3A%20%C3%80lmos%20Veres-Vit%C3%A0lyos%20and%20Genis%20Castillo%20Gomez-Raya%20and%20Filip%20Lemic%20and%20Daniel%20Johannes%20Bugelnig%20and%20Bernhard%20Rinner%20and%20Sergi%20Abadal%20and%20Xavier%20Costa-P%C3%A9rez%0AAbstract%3A%20%20%20Small%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20exhibit%20immense%20potential%20for%0Anavigating%20indoor%20and%20hard-to-reach%20areas%2C%20yet%20their%20significant%20constraints%20in%0Apayload%20and%20autonomy%20have%20largely%20prevented%20their%20use%20for%20complex%20tasks%20like%0Ahigh-quality%203-Dimensional%20%283D%29%20reconstruction.%20To%20overcome%20this%20challenge%2C%20we%0Aintroduce%20a%20novel%20system%20architecture%20that%20enables%20fully%20autonomous%2C%0Ahigh-fidelity%203D%20scanning%20of%20static%20objects%20using%20UAVs%20weighing%20under%20100%0Agrams.%20Our%20core%20innovation%20lies%20in%20a%20dual-reconstruction%20pipeline%20that%20creates%0Aa%20real-time%20feedback%20loop%20between%20data%20capture%20and%20flight%20control.%20A%0Anear-real-time%20%28near-RT%29%20process%20uses%20Structure%20from%20Motion%20%28SfM%29%20to%20generate%0Aan%20instantaneous%20pointcloud%20of%20the%20object.%20The%20system%20analyzes%20the%20model%0Aquality%20on%20the%20fly%20and%20dynamically%20adapts%20the%20UAV%27s%20trajectory%20to%20intelligently%0Acapture%20new%20images%20of%20poorly%20covered%20areas.%20This%20ensures%20comprehensive%20data%0Aacquisition.%20For%20the%20final%2C%20detailed%20output%2C%20a%20non-real-time%20%28non-RT%29%20pipeline%0Aemploys%20a%20Neural%20Radiance%20Fields%20%28NeRF%29-based%20Neural%203D%20Reconstruction%20%28N3DR%29%0Aapproach%2C%20fusing%20SfM-derived%20camera%20poses%20with%20precise%20Ultra%20Wide-Band%20%28UWB%29%0Alocation%20data%20to%20achieve%20superior%20accuracy.%20We%20implemented%20and%20validated%20this%0Aarchitecture%20using%20Crazyflie%202.1%20UAVs.%20Our%20experiments%2C%20conducted%20in%20both%0Asingle-%20and%20multi-UAV%20configurations%2C%20conclusively%20show%20that%20dynamic%20trajectory%0Aadaptation%20consistently%20improves%20reconstruction%20quality%20over%20static%20flight%0Apaths.%20This%20work%20demonstrates%20a%20scalable%20and%20autonomous%20solution%20that%20unlocks%0Athe%20potential%20of%20miniaturized%20UAVs%20for%20fine-grained%203D%20reconstruction%20in%0Aconstrained%20environments%2C%20a%20capability%20previously%20limited%20to%20much%20larger%0Aplatforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%25203D%2520Object%2520Reconstruction%2520with%2520Small-Scale%2520Unmanned%2520Aerial%250A%2520%2520Vehicles%26entry.906535625%3D%25C3%2580lmos%2520Veres-Vit%25C3%25A0lyos%2520and%2520Genis%2520Castillo%2520Gomez-Raya%2520and%2520Filip%2520Lemic%2520and%2520Daniel%2520Johannes%2520Bugelnig%2520and%2520Bernhard%2520Rinner%2520and%2520Sergi%2520Abadal%2520and%2520Xavier%2520Costa-P%25C3%25A9rez%26entry.1292438233%3D%2520%2520Small%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520exhibit%2520immense%2520potential%2520for%250Anavigating%2520indoor%2520and%2520hard-to-reach%2520areas%252C%2520yet%2520their%2520significant%2520constraints%2520in%250Apayload%2520and%2520autonomy%2520have%2520largely%2520prevented%2520their%2520use%2520for%2520complex%2520tasks%2520like%250Ahigh-quality%25203-Dimensional%2520%25283D%2529%2520reconstruction.%2520To%2520overcome%2520this%2520challenge%252C%2520we%250Aintroduce%2520a%2520novel%2520system%2520architecture%2520that%2520enables%2520fully%2520autonomous%252C%250Ahigh-fidelity%25203D%2520scanning%2520of%2520static%2520objects%2520using%2520UAVs%2520weighing%2520under%2520100%250Agrams.%2520Our%2520core%2520innovation%2520lies%2520in%2520a%2520dual-reconstruction%2520pipeline%2520that%2520creates%250Aa%2520real-time%2520feedback%2520loop%2520between%2520data%2520capture%2520and%2520flight%2520control.%2520A%250Anear-real-time%2520%2528near-RT%2529%2520process%2520uses%2520Structure%2520from%2520Motion%2520%2528SfM%2529%2520to%2520generate%250Aan%2520instantaneous%2520pointcloud%2520of%2520the%2520object.%2520The%2520system%2520analyzes%2520the%2520model%250Aquality%2520on%2520the%2520fly%2520and%2520dynamically%2520adapts%2520the%2520UAV%2527s%2520trajectory%2520to%2520intelligently%250Acapture%2520new%2520images%2520of%2520poorly%2520covered%2520areas.%2520This%2520ensures%2520comprehensive%2520data%250Aacquisition.%2520For%2520the%2520final%252C%2520detailed%2520output%252C%2520a%2520non-real-time%2520%2528non-RT%2529%2520pipeline%250Aemploys%2520a%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529-based%2520Neural%25203D%2520Reconstruction%2520%2528N3DR%2529%250Aapproach%252C%2520fusing%2520SfM-derived%2520camera%2520poses%2520with%2520precise%2520Ultra%2520Wide-Band%2520%2528UWB%2529%250Alocation%2520data%2520to%2520achieve%2520superior%2520accuracy.%2520We%2520implemented%2520and%2520validated%2520this%250Aarchitecture%2520using%2520Crazyflie%25202.1%2520UAVs.%2520Our%2520experiments%252C%2520conducted%2520in%2520both%250Asingle-%2520and%2520multi-UAV%2520configurations%252C%2520conclusively%2520show%2520that%2520dynamic%2520trajectory%250Aadaptation%2520consistently%2520improves%2520reconstruction%2520quality%2520over%2520static%2520flight%250Apaths.%2520This%2520work%2520demonstrates%2520a%2520scalable%2520and%2520autonomous%2520solution%2520that%2520unlocks%250Athe%2520potential%2520of%2520miniaturized%2520UAVs%2520for%2520fine-grained%25203D%2520reconstruction%2520in%250Aconstrained%2520environments%252C%2520a%2520capability%2520previously%2520limited%2520to%2520much%2520larger%250Aplatforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%203D%20Object%20Reconstruction%20with%20Small-Scale%20Unmanned%20Aerial%0A%20%20Vehicles&entry.906535625=%C3%80lmos%20Veres-Vit%C3%A0lyos%20and%20Genis%20Castillo%20Gomez-Raya%20and%20Filip%20Lemic%20and%20Daniel%20Johannes%20Bugelnig%20and%20Bernhard%20Rinner%20and%20Sergi%20Abadal%20and%20Xavier%20Costa-P%C3%A9rez&entry.1292438233=%20%20Small%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20exhibit%20immense%20potential%20for%0Anavigating%20indoor%20and%20hard-to-reach%20areas%2C%20yet%20their%20significant%20constraints%20in%0Apayload%20and%20autonomy%20have%20largely%20prevented%20their%20use%20for%20complex%20tasks%20like%0Ahigh-quality%203-Dimensional%20%283D%29%20reconstruction.%20To%20overcome%20this%20challenge%2C%20we%0Aintroduce%20a%20novel%20system%20architecture%20that%20enables%20fully%20autonomous%2C%0Ahigh-fidelity%203D%20scanning%20of%20static%20objects%20using%20UAVs%20weighing%20under%20100%0Agrams.%20Our%20core%20innovation%20lies%20in%20a%20dual-reconstruction%20pipeline%20that%20creates%0Aa%20real-time%20feedback%20loop%20between%20data%20capture%20and%20flight%20control.%20A%0Anear-real-time%20%28near-RT%29%20process%20uses%20Structure%20from%20Motion%20%28SfM%29%20to%20generate%0Aan%20instantaneous%20pointcloud%20of%20the%20object.%20The%20system%20analyzes%20the%20model%0Aquality%20on%20the%20fly%20and%20dynamically%20adapts%20the%20UAV%27s%20trajectory%20to%20intelligently%0Acapture%20new%20images%20of%20poorly%20covered%20areas.%20This%20ensures%20comprehensive%20data%0Aacquisition.%20For%20the%20final%2C%20detailed%20output%2C%20a%20non-real-time%20%28non-RT%29%20pipeline%0Aemploys%20a%20Neural%20Radiance%20Fields%20%28NeRF%29-based%20Neural%203D%20Reconstruction%20%28N3DR%29%0Aapproach%2C%20fusing%20SfM-derived%20camera%20poses%20with%20precise%20Ultra%20Wide-Band%20%28UWB%29%0Alocation%20data%20to%20achieve%20superior%20accuracy.%20We%20implemented%20and%20validated%20this%0Aarchitecture%20using%20Crazyflie%202.1%20UAVs.%20Our%20experiments%2C%20conducted%20in%20both%0Asingle-%20and%20multi-UAV%20configurations%2C%20conclusively%20show%20that%20dynamic%20trajectory%0Aadaptation%20consistently%20improves%20reconstruction%20quality%20over%20static%20flight%0Apaths.%20This%20work%20demonstrates%20a%20scalable%20and%20autonomous%20solution%20that%20unlocks%0Athe%20potential%20of%20miniaturized%20UAVs%20for%20fine-grained%203D%20reconstruction%20in%0Aconstrained%20environments%2C%20a%20capability%20previously%20limited%20to%20much%20larger%0Aplatforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12458v2&entry.124074799=Read"},
{"title": "Adapting Language Balance in Code-Switching Speech", "author": "Enes Yavuz Ugan and Ngoc-Quan Pham and Alexander Waibel", "abstract": "  Despite achieving impressive results on standard benchmarks, large\nfoundational models still struggle against code-switching test cases. When data\nscarcity cannot be used as the usual justification for poor performance, the\nreason may lie in the infrequent occurrence of code-switched moments, where the\nembedding of the second language appears subtly. Instead of expecting the\nmodels to learn this infrequency on their own, it might be beneficial to\nprovide the training process with labels. Evaluating model performance on\ncode-switching data requires careful localization of code-switching points\nwhere recognition errors are most consequential, so that the analysis\nemphasizes mistakes occurring at those moments. Building on this observation,\nwe leverage the difference between the embedded and the main language to\nhighlight those code-switching points and thereby emphasize learning at those\nlocations. This simple yet effective differentiable surrogate mitigates context\nbias during generation -- the central challenge in code-switching -- thereby\nimproving the model's robustness. Our experiments with Arabic and\nChinese-English showed that the models are able to predict the switching places\nmore correctly, reflected by the reduced substitution error.\n", "link": "http://arxiv.org/abs/2510.18724v1", "date": "2025-10-21", "relevancy": 2.4024, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Language%20Balance%20in%20Code-Switching%20Speech&body=Title%3A%20Adapting%20Language%20Balance%20in%20Code-Switching%20Speech%0AAuthor%3A%20Enes%20Yavuz%20Ugan%20and%20Ngoc-Quan%20Pham%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20Despite%20achieving%20impressive%20results%20on%20standard%20benchmarks%2C%20large%0Afoundational%20models%20still%20struggle%20against%20code-switching%20test%20cases.%20When%20data%0Ascarcity%20cannot%20be%20used%20as%20the%20usual%20justification%20for%20poor%20performance%2C%20the%0Areason%20may%20lie%20in%20the%20infrequent%20occurrence%20of%20code-switched%20moments%2C%20where%20the%0Aembedding%20of%20the%20second%20language%20appears%20subtly.%20Instead%20of%20expecting%20the%0Amodels%20to%20learn%20this%20infrequency%20on%20their%20own%2C%20it%20might%20be%20beneficial%20to%0Aprovide%20the%20training%20process%20with%20labels.%20Evaluating%20model%20performance%20on%0Acode-switching%20data%20requires%20careful%20localization%20of%20code-switching%20points%0Awhere%20recognition%20errors%20are%20most%20consequential%2C%20so%20that%20the%20analysis%0Aemphasizes%20mistakes%20occurring%20at%20those%20moments.%20Building%20on%20this%20observation%2C%0Awe%20leverage%20the%20difference%20between%20the%20embedded%20and%20the%20main%20language%20to%0Ahighlight%20those%20code-switching%20points%20and%20thereby%20emphasize%20learning%20at%20those%0Alocations.%20This%20simple%20yet%20effective%20differentiable%20surrogate%20mitigates%20context%0Abias%20during%20generation%20--%20the%20central%20challenge%20in%20code-switching%20--%20thereby%0Aimproving%20the%20model%27s%20robustness.%20Our%20experiments%20with%20Arabic%20and%0AChinese-English%20showed%20that%20the%20models%20are%20able%20to%20predict%20the%20switching%20places%0Amore%20correctly%2C%20reflected%20by%20the%20reduced%20substitution%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Language%2520Balance%2520in%2520Code-Switching%2520Speech%26entry.906535625%3DEnes%2520Yavuz%2520Ugan%2520and%2520Ngoc-Quan%2520Pham%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520Despite%2520achieving%2520impressive%2520results%2520on%2520standard%2520benchmarks%252C%2520large%250Afoundational%2520models%2520still%2520struggle%2520against%2520code-switching%2520test%2520cases.%2520When%2520data%250Ascarcity%2520cannot%2520be%2520used%2520as%2520the%2520usual%2520justification%2520for%2520poor%2520performance%252C%2520the%250Areason%2520may%2520lie%2520in%2520the%2520infrequent%2520occurrence%2520of%2520code-switched%2520moments%252C%2520where%2520the%250Aembedding%2520of%2520the%2520second%2520language%2520appears%2520subtly.%2520Instead%2520of%2520expecting%2520the%250Amodels%2520to%2520learn%2520this%2520infrequency%2520on%2520their%2520own%252C%2520it%2520might%2520be%2520beneficial%2520to%250Aprovide%2520the%2520training%2520process%2520with%2520labels.%2520Evaluating%2520model%2520performance%2520on%250Acode-switching%2520data%2520requires%2520careful%2520localization%2520of%2520code-switching%2520points%250Awhere%2520recognition%2520errors%2520are%2520most%2520consequential%252C%2520so%2520that%2520the%2520analysis%250Aemphasizes%2520mistakes%2520occurring%2520at%2520those%2520moments.%2520Building%2520on%2520this%2520observation%252C%250Awe%2520leverage%2520the%2520difference%2520between%2520the%2520embedded%2520and%2520the%2520main%2520language%2520to%250Ahighlight%2520those%2520code-switching%2520points%2520and%2520thereby%2520emphasize%2520learning%2520at%2520those%250Alocations.%2520This%2520simple%2520yet%2520effective%2520differentiable%2520surrogate%2520mitigates%2520context%250Abias%2520during%2520generation%2520--%2520the%2520central%2520challenge%2520in%2520code-switching%2520--%2520thereby%250Aimproving%2520the%2520model%2527s%2520robustness.%2520Our%2520experiments%2520with%2520Arabic%2520and%250AChinese-English%2520showed%2520that%2520the%2520models%2520are%2520able%2520to%2520predict%2520the%2520switching%2520places%250Amore%2520correctly%252C%2520reflected%2520by%2520the%2520reduced%2520substitution%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Language%20Balance%20in%20Code-Switching%20Speech&entry.906535625=Enes%20Yavuz%20Ugan%20and%20Ngoc-Quan%20Pham%20and%20Alexander%20Waibel&entry.1292438233=%20%20Despite%20achieving%20impressive%20results%20on%20standard%20benchmarks%2C%20large%0Afoundational%20models%20still%20struggle%20against%20code-switching%20test%20cases.%20When%20data%0Ascarcity%20cannot%20be%20used%20as%20the%20usual%20justification%20for%20poor%20performance%2C%20the%0Areason%20may%20lie%20in%20the%20infrequent%20occurrence%20of%20code-switched%20moments%2C%20where%20the%0Aembedding%20of%20the%20second%20language%20appears%20subtly.%20Instead%20of%20expecting%20the%0Amodels%20to%20learn%20this%20infrequency%20on%20their%20own%2C%20it%20might%20be%20beneficial%20to%0Aprovide%20the%20training%20process%20with%20labels.%20Evaluating%20model%20performance%20on%0Acode-switching%20data%20requires%20careful%20localization%20of%20code-switching%20points%0Awhere%20recognition%20errors%20are%20most%20consequential%2C%20so%20that%20the%20analysis%0Aemphasizes%20mistakes%20occurring%20at%20those%20moments.%20Building%20on%20this%20observation%2C%0Awe%20leverage%20the%20difference%20between%20the%20embedded%20and%20the%20main%20language%20to%0Ahighlight%20those%20code-switching%20points%20and%20thereby%20emphasize%20learning%20at%20those%0Alocations.%20This%20simple%20yet%20effective%20differentiable%20surrogate%20mitigates%20context%0Abias%20during%20generation%20--%20the%20central%20challenge%20in%20code-switching%20--%20thereby%0Aimproving%20the%20model%27s%20robustness.%20Our%20experiments%20with%20Arabic%20and%0AChinese-English%20showed%20that%20the%20models%20are%20able%20to%20predict%20the%20switching%20places%0Amore%20correctly%2C%20reflected%20by%20the%20reduced%20substitution%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18724v1&entry.124074799=Read"},
{"title": "LightMem: Lightweight and Efficient Memory-Augmented Generation", "author": "Jizhan Fang and Xinle Deng and Haoming Xu and Ziyan Jiang and Yuqi Tang and Ziwen Xu and Shumin Deng and Yunzhi Yao and Mengru Wang and Shuofei Qiao and Huajun Chen and Ningyu Zhang", "abstract": "  Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem.\n", "link": "http://arxiv.org/abs/2510.18866v1", "date": "2025-10-21", "relevancy": 2.3901, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation&body=Title%3A%20LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation%0AAuthor%3A%20Jizhan%20Fang%20and%20Xinle%20Deng%20and%20Haoming%20Xu%20and%20Ziyan%20Jiang%20and%20Yuqi%20Tang%20and%20Ziwen%20Xu%20and%20Shumin%20Deng%20and%20Yunzhi%20Yao%20and%20Mengru%20Wang%20and%20Shuofei%20Qiao%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Despite%20their%20remarkable%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20struggle%0Ato%20effectively%20leverage%20historical%20interaction%20information%20in%20dynamic%20and%0Acomplex%20environments.%20Memory%20systems%20enable%20LLMs%20to%20move%20beyond%20stateless%0Ainteractions%20by%20introducing%20persistent%20information%20storage%2C%20retrieval%2C%20and%0Autilization%20mechanisms.%20However%2C%20existing%20memory%20systems%20often%20introduce%0Asubstantial%20time%20and%20computational%20overhead.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Amemory%20system%20called%20LightMem%2C%20which%20strikes%20a%20balance%20between%20the%20performance%0Aand%20efficiency%20of%20memory%20systems.%20Inspired%20by%20the%20Atkinson-Shiffrin%20model%20of%0Ahuman%20memory%2C%20LightMem%20organizes%20memory%20into%20three%20complementary%20stages.%20First%2C%0Acognition-inspired%20sensory%20memory%20rapidly%20filters%20irrelevant%20information%0Athrough%20lightweight%20compression%20and%20groups%20information%20according%20to%20their%0Atopics.%20Next%2C%20topic-aware%20short-term%20memory%20consolidates%20these%20topic-based%0Agroups%2C%20organizing%20and%20summarizing%20content%20for%20more%20structured%20access.%20Finally%2C%0Along-term%20memory%20with%20sleep-time%20update%20employs%20an%20offline%20procedure%20that%0Adecouples%20consolidation%20from%20online%20inference.%20Experiments%20on%20LongMemEval%20with%0AGPT%20and%20Qwen%20backbones%20show%20that%20LightMem%20outperforms%20strong%20baselines%20in%0Aaccuracy%20%28up%20to%2010.9%25%20gains%29%20while%20reducing%20token%20usage%20by%20up%20to%20117x%2C%20API%0Acalls%20by%20up%20to%20159x%2C%20and%20runtime%20by%20over%2012x.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/LightMem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightMem%253A%2520Lightweight%2520and%2520Efficient%2520Memory-Augmented%2520Generation%26entry.906535625%3DJizhan%2520Fang%2520and%2520Xinle%2520Deng%2520and%2520Haoming%2520Xu%2520and%2520Ziyan%2520Jiang%2520and%2520Yuqi%2520Tang%2520and%2520Ziwen%2520Xu%2520and%2520Shumin%2520Deng%2520and%2520Yunzhi%2520Yao%2520and%2520Mengru%2520Wang%2520and%2520Shuofei%2520Qiao%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520their%2520remarkable%2520capabilities%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggle%250Ato%2520effectively%2520leverage%2520historical%2520interaction%2520information%2520in%2520dynamic%2520and%250Acomplex%2520environments.%2520Memory%2520systems%2520enable%2520LLMs%2520to%2520move%2520beyond%2520stateless%250Ainteractions%2520by%2520introducing%2520persistent%2520information%2520storage%252C%2520retrieval%252C%2520and%250Autilization%2520mechanisms.%2520However%252C%2520existing%2520memory%2520systems%2520often%2520introduce%250Asubstantial%2520time%2520and%2520computational%2520overhead.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520new%250Amemory%2520system%2520called%2520LightMem%252C%2520which%2520strikes%2520a%2520balance%2520between%2520the%2520performance%250Aand%2520efficiency%2520of%2520memory%2520systems.%2520Inspired%2520by%2520the%2520Atkinson-Shiffrin%2520model%2520of%250Ahuman%2520memory%252C%2520LightMem%2520organizes%2520memory%2520into%2520three%2520complementary%2520stages.%2520First%252C%250Acognition-inspired%2520sensory%2520memory%2520rapidly%2520filters%2520irrelevant%2520information%250Athrough%2520lightweight%2520compression%2520and%2520groups%2520information%2520according%2520to%2520their%250Atopics.%2520Next%252C%2520topic-aware%2520short-term%2520memory%2520consolidates%2520these%2520topic-based%250Agroups%252C%2520organizing%2520and%2520summarizing%2520content%2520for%2520more%2520structured%2520access.%2520Finally%252C%250Along-term%2520memory%2520with%2520sleep-time%2520update%2520employs%2520an%2520offline%2520procedure%2520that%250Adecouples%2520consolidation%2520from%2520online%2520inference.%2520Experiments%2520on%2520LongMemEval%2520with%250AGPT%2520and%2520Qwen%2520backbones%2520show%2520that%2520LightMem%2520outperforms%2520strong%2520baselines%2520in%250Aaccuracy%2520%2528up%2520to%252010.9%2525%2520gains%2529%2520while%2520reducing%2520token%2520usage%2520by%2520up%2520to%2520117x%252C%2520API%250Acalls%2520by%2520up%2520to%2520159x%252C%2520and%2520runtime%2520by%2520over%252012x.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zjunlp/LightMem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightMem%3A%20Lightweight%20and%20Efficient%20Memory-Augmented%20Generation&entry.906535625=Jizhan%20Fang%20and%20Xinle%20Deng%20and%20Haoming%20Xu%20and%20Ziyan%20Jiang%20and%20Yuqi%20Tang%20and%20Ziwen%20Xu%20and%20Shumin%20Deng%20and%20Yunzhi%20Yao%20and%20Mengru%20Wang%20and%20Shuofei%20Qiao%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Despite%20their%20remarkable%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20struggle%0Ato%20effectively%20leverage%20historical%20interaction%20information%20in%20dynamic%20and%0Acomplex%20environments.%20Memory%20systems%20enable%20LLMs%20to%20move%20beyond%20stateless%0Ainteractions%20by%20introducing%20persistent%20information%20storage%2C%20retrieval%2C%20and%0Autilization%20mechanisms.%20However%2C%20existing%20memory%20systems%20often%20introduce%0Asubstantial%20time%20and%20computational%20overhead.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Amemory%20system%20called%20LightMem%2C%20which%20strikes%20a%20balance%20between%20the%20performance%0Aand%20efficiency%20of%20memory%20systems.%20Inspired%20by%20the%20Atkinson-Shiffrin%20model%20of%0Ahuman%20memory%2C%20LightMem%20organizes%20memory%20into%20three%20complementary%20stages.%20First%2C%0Acognition-inspired%20sensory%20memory%20rapidly%20filters%20irrelevant%20information%0Athrough%20lightweight%20compression%20and%20groups%20information%20according%20to%20their%0Atopics.%20Next%2C%20topic-aware%20short-term%20memory%20consolidates%20these%20topic-based%0Agroups%2C%20organizing%20and%20summarizing%20content%20for%20more%20structured%20access.%20Finally%2C%0Along-term%20memory%20with%20sleep-time%20update%20employs%20an%20offline%20procedure%20that%0Adecouples%20consolidation%20from%20online%20inference.%20Experiments%20on%20LongMemEval%20with%0AGPT%20and%20Qwen%20backbones%20show%20that%20LightMem%20outperforms%20strong%20baselines%20in%0Aaccuracy%20%28up%20to%2010.9%25%20gains%29%20while%20reducing%20token%20usage%20by%20up%20to%20117x%2C%20API%0Acalls%20by%20up%20to%20159x%2C%20and%20runtime%20by%20over%2012x.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/LightMem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18866v1&entry.124074799=Read"},
{"title": "CAGE: Curvature-Aware Gradient Estimation For Accurate\n  Quantization-Aware Training", "author": "Soroush Tabesh and Mher Safaryan and Dan Alistarh", "abstract": "  Despite significant work on low-bit quantization-aware training (QAT), there\nis still a large accuracy gap between such techniques and native training. To\naddress this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new\nQAT method that augments the straight-through estimator (STE) gradient with a\ncurvature-aware correction designed to counteract the loss increase induced by\nquantization. CAGE is derived from a multi-objective view of QAT that balances\nloss minimization with adherence to quantization constraints, yielding a\nprincipled correction term that depends on local curvature information. On the\ntheoretical side, we introduce the notion of Pareto-optimal solutions for\nquantized optimization, and establish that CAGE yields strong convergence\nguarantees in the smooth non-convex setting. In terms of implementation, our\napproach is optimizer-agnostic, but we provide a highly-efficient\nimplementation that leverages Adam statistics. When pre-training Llama-style\nmodels of up to 800M-parameters, CAGE recovers over 10% of the\nquantization-induced loss increase in the W4A4 regime over outlier-mitigation\nmethods. These results indicate that curvature-aware gradient corrections can\nbridge the remaining performance gap beyond current outlier-handling methods.\n", "link": "http://arxiv.org/abs/2510.18784v1", "date": "2025-10-21", "relevancy": 2.3819, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4864}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4771}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAGE%3A%20Curvature-Aware%20Gradient%20Estimation%20For%20Accurate%0A%20%20Quantization-Aware%20Training&body=Title%3A%20CAGE%3A%20Curvature-Aware%20Gradient%20Estimation%20For%20Accurate%0A%20%20Quantization-Aware%20Training%0AAuthor%3A%20Soroush%20Tabesh%20and%20Mher%20Safaryan%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20Despite%20significant%20work%20on%20low-bit%20quantization-aware%20training%20%28QAT%29%2C%20there%0Ais%20still%20a%20large%20accuracy%20gap%20between%20such%20techniques%20and%20native%20training.%20To%0Aaddress%20this%2C%20we%20introduce%20CAGE%20%28Curvature-Aware%20Gradient%20Estimation%29%2C%20a%20new%0AQAT%20method%20that%20augments%20the%20straight-through%20estimator%20%28STE%29%20gradient%20with%20a%0Acurvature-aware%20correction%20designed%20to%20counteract%20the%20loss%20increase%20induced%20by%0Aquantization.%20CAGE%20is%20derived%20from%20a%20multi-objective%20view%20of%20QAT%20that%20balances%0Aloss%20minimization%20with%20adherence%20to%20quantization%20constraints%2C%20yielding%20a%0Aprincipled%20correction%20term%20that%20depends%20on%20local%20curvature%20information.%20On%20the%0Atheoretical%20side%2C%20we%20introduce%20the%20notion%20of%20Pareto-optimal%20solutions%20for%0Aquantized%20optimization%2C%20and%20establish%20that%20CAGE%20yields%20strong%20convergence%0Aguarantees%20in%20the%20smooth%20non-convex%20setting.%20In%20terms%20of%20implementation%2C%20our%0Aapproach%20is%20optimizer-agnostic%2C%20but%20we%20provide%20a%20highly-efficient%0Aimplementation%20that%20leverages%20Adam%20statistics.%20When%20pre-training%20Llama-style%0Amodels%20of%20up%20to%20800M-parameters%2C%20CAGE%20recovers%20over%2010%25%20of%20the%0Aquantization-induced%20loss%20increase%20in%20the%20W4A4%20regime%20over%20outlier-mitigation%0Amethods.%20These%20results%20indicate%20that%20curvature-aware%20gradient%20corrections%20can%0Abridge%20the%20remaining%20performance%20gap%20beyond%20current%20outlier-handling%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAGE%253A%2520Curvature-Aware%2520Gradient%2520Estimation%2520For%2520Accurate%250A%2520%2520Quantization-Aware%2520Training%26entry.906535625%3DSoroush%2520Tabesh%2520and%2520Mher%2520Safaryan%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520Despite%2520significant%2520work%2520on%2520low-bit%2520quantization-aware%2520training%2520%2528QAT%2529%252C%2520there%250Ais%2520still%2520a%2520large%2520accuracy%2520gap%2520between%2520such%2520techniques%2520and%2520native%2520training.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520CAGE%2520%2528Curvature-Aware%2520Gradient%2520Estimation%2529%252C%2520a%2520new%250AQAT%2520method%2520that%2520augments%2520the%2520straight-through%2520estimator%2520%2528STE%2529%2520gradient%2520with%2520a%250Acurvature-aware%2520correction%2520designed%2520to%2520counteract%2520the%2520loss%2520increase%2520induced%2520by%250Aquantization.%2520CAGE%2520is%2520derived%2520from%2520a%2520multi-objective%2520view%2520of%2520QAT%2520that%2520balances%250Aloss%2520minimization%2520with%2520adherence%2520to%2520quantization%2520constraints%252C%2520yielding%2520a%250Aprincipled%2520correction%2520term%2520that%2520depends%2520on%2520local%2520curvature%2520information.%2520On%2520the%250Atheoretical%2520side%252C%2520we%2520introduce%2520the%2520notion%2520of%2520Pareto-optimal%2520solutions%2520for%250Aquantized%2520optimization%252C%2520and%2520establish%2520that%2520CAGE%2520yields%2520strong%2520convergence%250Aguarantees%2520in%2520the%2520smooth%2520non-convex%2520setting.%2520In%2520terms%2520of%2520implementation%252C%2520our%250Aapproach%2520is%2520optimizer-agnostic%252C%2520but%2520we%2520provide%2520a%2520highly-efficient%250Aimplementation%2520that%2520leverages%2520Adam%2520statistics.%2520When%2520pre-training%2520Llama-style%250Amodels%2520of%2520up%2520to%2520800M-parameters%252C%2520CAGE%2520recovers%2520over%252010%2525%2520of%2520the%250Aquantization-induced%2520loss%2520increase%2520in%2520the%2520W4A4%2520regime%2520over%2520outlier-mitigation%250Amethods.%2520These%2520results%2520indicate%2520that%2520curvature-aware%2520gradient%2520corrections%2520can%250Abridge%2520the%2520remaining%2520performance%2520gap%2520beyond%2520current%2520outlier-handling%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAGE%3A%20Curvature-Aware%20Gradient%20Estimation%20For%20Accurate%0A%20%20Quantization-Aware%20Training&entry.906535625=Soroush%20Tabesh%20and%20Mher%20Safaryan%20and%20Dan%20Alistarh&entry.1292438233=%20%20Despite%20significant%20work%20on%20low-bit%20quantization-aware%20training%20%28QAT%29%2C%20there%0Ais%20still%20a%20large%20accuracy%20gap%20between%20such%20techniques%20and%20native%20training.%20To%0Aaddress%20this%2C%20we%20introduce%20CAGE%20%28Curvature-Aware%20Gradient%20Estimation%29%2C%20a%20new%0AQAT%20method%20that%20augments%20the%20straight-through%20estimator%20%28STE%29%20gradient%20with%20a%0Acurvature-aware%20correction%20designed%20to%20counteract%20the%20loss%20increase%20induced%20by%0Aquantization.%20CAGE%20is%20derived%20from%20a%20multi-objective%20view%20of%20QAT%20that%20balances%0Aloss%20minimization%20with%20adherence%20to%20quantization%20constraints%2C%20yielding%20a%0Aprincipled%20correction%20term%20that%20depends%20on%20local%20curvature%20information.%20On%20the%0Atheoretical%20side%2C%20we%20introduce%20the%20notion%20of%20Pareto-optimal%20solutions%20for%0Aquantized%20optimization%2C%20and%20establish%20that%20CAGE%20yields%20strong%20convergence%0Aguarantees%20in%20the%20smooth%20non-convex%20setting.%20In%20terms%20of%20implementation%2C%20our%0Aapproach%20is%20optimizer-agnostic%2C%20but%20we%20provide%20a%20highly-efficient%0Aimplementation%20that%20leverages%20Adam%20statistics.%20When%20pre-training%20Llama-style%0Amodels%20of%20up%20to%20800M-parameters%2C%20CAGE%20recovers%20over%2010%25%20of%20the%0Aquantization-induced%20loss%20increase%20in%20the%20W4A4%20regime%20over%20outlier-mitigation%0Amethods.%20These%20results%20indicate%20that%20curvature-aware%20gradient%20corrections%20can%0Abridge%20the%20remaining%20performance%20gap%20beyond%20current%20outlier-handling%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18784v1&entry.124074799=Read"},
{"title": "VAR: Visual Attention Reasoning via Structured Search and Backtracking", "author": "Wei Cai and Jian Zhao and Yuchen Yuan and Tianle Zhang and Ming Zhu and Haichuan Tang and Chi Zhang and Xuelong Li", "abstract": "  Multimodal Large Language Models (MLLMs), despite their advances, are\nhindered by their high hallucination tendency and heavy reliance on brittle,\nlinear reasoning processes, leading to failures in complex tasks. To address\nthese limitations, we introduce Visual Attention Reasoning (VAR), a novel\nframework that recasts grounded reasoning as a structured search over a\nreasoning trajectory space. VAR decomposes the reasoning process into two key\nstages: traceable evidence grounding and search-based chain-of-thought (CoT)\ngeneration, which incorporates a backtracking mechanism for self-correction.\nThe search is guided by a multi-faceted reward function with semantic and\ngeometric self-verification components, which penalize outputs that are not\nfaithfully grounded in the visual input. We provide a theoretical analysis for\nour search strategy, validating its capability to find the correct solution\nwith high probability. Experimental results show that our 7B model, VAR-7B,\nsets a new state-of-the-art on a comprehensive suite of hallucination and\nsafety benchmarks, significantly outperforming existing open-source models and\ndemonstrating competitive performance against leading proprietary systems.\n", "link": "http://arxiv.org/abs/2510.18619v1", "date": "2025-10-21", "relevancy": 2.3651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAR%3A%20Visual%20Attention%20Reasoning%20via%20Structured%20Search%20and%20Backtracking&body=Title%3A%20VAR%3A%20Visual%20Attention%20Reasoning%20via%20Structured%20Search%20and%20Backtracking%0AAuthor%3A%20Wei%20Cai%20and%20Jian%20Zhao%20and%20Yuchen%20Yuan%20and%20Tianle%20Zhang%20and%20Ming%20Zhu%20and%20Haichuan%20Tang%20and%20Chi%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20despite%20their%20advances%2C%20are%0Ahindered%20by%20their%20high%20hallucination%20tendency%20and%20heavy%20reliance%20on%20brittle%2C%0Alinear%20reasoning%20processes%2C%20leading%20to%20failures%20in%20complex%20tasks.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20Visual%20Attention%20Reasoning%20%28VAR%29%2C%20a%20novel%0Aframework%20that%20recasts%20grounded%20reasoning%20as%20a%20structured%20search%20over%20a%0Areasoning%20trajectory%20space.%20VAR%20decomposes%20the%20reasoning%20process%20into%20two%20key%0Astages%3A%20traceable%20evidence%20grounding%20and%20search-based%20chain-of-thought%20%28CoT%29%0Ageneration%2C%20which%20incorporates%20a%20backtracking%20mechanism%20for%20self-correction.%0AThe%20search%20is%20guided%20by%20a%20multi-faceted%20reward%20function%20with%20semantic%20and%0Ageometric%20self-verification%20components%2C%20which%20penalize%20outputs%20that%20are%20not%0Afaithfully%20grounded%20in%20the%20visual%20input.%20We%20provide%20a%20theoretical%20analysis%20for%0Aour%20search%20strategy%2C%20validating%20its%20capability%20to%20find%20the%20correct%20solution%0Awith%20high%20probability.%20Experimental%20results%20show%20that%20our%207B%20model%2C%20VAR-7B%2C%0Asets%20a%20new%20state-of-the-art%20on%20a%20comprehensive%20suite%20of%20hallucination%20and%0Asafety%20benchmarks%2C%20significantly%20outperforming%20existing%20open-source%20models%20and%0Ademonstrating%20competitive%20performance%20against%20leading%20proprietary%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAR%253A%2520Visual%2520Attention%2520Reasoning%2520via%2520Structured%2520Search%2520and%2520Backtracking%26entry.906535625%3DWei%2520Cai%2520and%2520Jian%2520Zhao%2520and%2520Yuchen%2520Yuan%2520and%2520Tianle%2520Zhang%2520and%2520Ming%2520Zhu%2520and%2520Haichuan%2520Tang%2520and%2520Chi%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520despite%2520their%2520advances%252C%2520are%250Ahindered%2520by%2520their%2520high%2520hallucination%2520tendency%2520and%2520heavy%2520reliance%2520on%2520brittle%252C%250Alinear%2520reasoning%2520processes%252C%2520leading%2520to%2520failures%2520in%2520complex%2520tasks.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520introduce%2520Visual%2520Attention%2520Reasoning%2520%2528VAR%2529%252C%2520a%2520novel%250Aframework%2520that%2520recasts%2520grounded%2520reasoning%2520as%2520a%2520structured%2520search%2520over%2520a%250Areasoning%2520trajectory%2520space.%2520VAR%2520decomposes%2520the%2520reasoning%2520process%2520into%2520two%2520key%250Astages%253A%2520traceable%2520evidence%2520grounding%2520and%2520search-based%2520chain-of-thought%2520%2528CoT%2529%250Ageneration%252C%2520which%2520incorporates%2520a%2520backtracking%2520mechanism%2520for%2520self-correction.%250AThe%2520search%2520is%2520guided%2520by%2520a%2520multi-faceted%2520reward%2520function%2520with%2520semantic%2520and%250Ageometric%2520self-verification%2520components%252C%2520which%2520penalize%2520outputs%2520that%2520are%2520not%250Afaithfully%2520grounded%2520in%2520the%2520visual%2520input.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520for%250Aour%2520search%2520strategy%252C%2520validating%2520its%2520capability%2520to%2520find%2520the%2520correct%2520solution%250Awith%2520high%2520probability.%2520Experimental%2520results%2520show%2520that%2520our%25207B%2520model%252C%2520VAR-7B%252C%250Asets%2520a%2520new%2520state-of-the-art%2520on%2520a%2520comprehensive%2520suite%2520of%2520hallucination%2520and%250Asafety%2520benchmarks%252C%2520significantly%2520outperforming%2520existing%2520open-source%2520models%2520and%250Ademonstrating%2520competitive%2520performance%2520against%2520leading%2520proprietary%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAR%3A%20Visual%20Attention%20Reasoning%20via%20Structured%20Search%20and%20Backtracking&entry.906535625=Wei%20Cai%20and%20Jian%20Zhao%20and%20Yuchen%20Yuan%20and%20Tianle%20Zhang%20and%20Ming%20Zhu%20and%20Haichuan%20Tang%20and%20Chi%20Zhang%20and%20Xuelong%20Li&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20despite%20their%20advances%2C%20are%0Ahindered%20by%20their%20high%20hallucination%20tendency%20and%20heavy%20reliance%20on%20brittle%2C%0Alinear%20reasoning%20processes%2C%20leading%20to%20failures%20in%20complex%20tasks.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20Visual%20Attention%20Reasoning%20%28VAR%29%2C%20a%20novel%0Aframework%20that%20recasts%20grounded%20reasoning%20as%20a%20structured%20search%20over%20a%0Areasoning%20trajectory%20space.%20VAR%20decomposes%20the%20reasoning%20process%20into%20two%20key%0Astages%3A%20traceable%20evidence%20grounding%20and%20search-based%20chain-of-thought%20%28CoT%29%0Ageneration%2C%20which%20incorporates%20a%20backtracking%20mechanism%20for%20self-correction.%0AThe%20search%20is%20guided%20by%20a%20multi-faceted%20reward%20function%20with%20semantic%20and%0Ageometric%20self-verification%20components%2C%20which%20penalize%20outputs%20that%20are%20not%0Afaithfully%20grounded%20in%20the%20visual%20input.%20We%20provide%20a%20theoretical%20analysis%20for%0Aour%20search%20strategy%2C%20validating%20its%20capability%20to%20find%20the%20correct%20solution%0Awith%20high%20probability.%20Experimental%20results%20show%20that%20our%207B%20model%2C%20VAR-7B%2C%0Asets%20a%20new%20state-of-the-art%20on%20a%20comprehensive%20suite%20of%20hallucination%20and%0Asafety%20benchmarks%2C%20significantly%20outperforming%20existing%20open-source%20models%20and%0Ademonstrating%20competitive%20performance%20against%20leading%20proprietary%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18619v1&entry.124074799=Read"},
{"title": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large\n  Language Models", "author": "Sidhant Narula and Javad Rafiei Asl and Mohammad Ghasemigol and Eduardo Blanco and Daniel Takabi", "abstract": "  Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak\nattacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a\nhierarchical semantic network; a feedback-driven Simulator for iterative query\nrefinement; and a Network Traverser for real-time adaptive attack execution.\nHarmNet systematically explores and refines the adversarial space to uncover\nstealthy, high-success attack paths. Experiments across closed-source and\nopen-source LLMs show that HarmNet outperforms state-of-the-art methods,\nachieving higher attack success rates. For example, on Mistral-7B, HarmNet\nachieves a 99.4% attack success rate, 13.9% higher than the best baseline.\nIndex terms: jailbreak attacks; large language models; adversarial framework;\nquery refinement.\n", "link": "http://arxiv.org/abs/2510.18728v1", "date": "2025-10-21", "relevancy": 2.2861, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HarmNet%3A%20A%20Framework%20for%20Adaptive%20Multi-Turn%20Jailbreak%20Attacks%20on%20Large%0A%20%20Language%20Models&body=Title%3A%20HarmNet%3A%20A%20Framework%20for%20Adaptive%20Multi-Turn%20Jailbreak%20Attacks%20on%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Sidhant%20Narula%20and%20Javad%20Rafiei%20Asl%20and%20Mohammad%20Ghasemigol%20and%20Eduardo%20Blanco%20and%20Daniel%20Takabi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20remain%20vulnerable%20to%20multi-turn%20jailbreak%0Aattacks.%20We%20introduce%20HarmNet%2C%20a%20modular%20framework%20comprising%20ThoughtNet%2C%20a%0Ahierarchical%20semantic%20network%3B%20a%20feedback-driven%20Simulator%20for%20iterative%20query%0Arefinement%3B%20and%20a%20Network%20Traverser%20for%20real-time%20adaptive%20attack%20execution.%0AHarmNet%20systematically%20explores%20and%20refines%20the%20adversarial%20space%20to%20uncover%0Astealthy%2C%20high-success%20attack%20paths.%20Experiments%20across%20closed-source%20and%0Aopen-source%20LLMs%20show%20that%20HarmNet%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20higher%20attack%20success%20rates.%20For%20example%2C%20on%20Mistral-7B%2C%20HarmNet%0Aachieves%20a%2099.4%25%20attack%20success%20rate%2C%2013.9%25%20higher%20than%20the%20best%20baseline.%0AIndex%20terms%3A%20jailbreak%20attacks%3B%20large%20language%20models%3B%20adversarial%20framework%3B%0Aquery%20refinement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmNet%253A%2520A%2520Framework%2520for%2520Adaptive%2520Multi-Turn%2520Jailbreak%2520Attacks%2520on%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DSidhant%2520Narula%2520and%2520Javad%2520Rafiei%2520Asl%2520and%2520Mohammad%2520Ghasemigol%2520and%2520Eduardo%2520Blanco%2520and%2520Daniel%2520Takabi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520remain%2520vulnerable%2520to%2520multi-turn%2520jailbreak%250Aattacks.%2520We%2520introduce%2520HarmNet%252C%2520a%2520modular%2520framework%2520comprising%2520ThoughtNet%252C%2520a%250Ahierarchical%2520semantic%2520network%253B%2520a%2520feedback-driven%2520Simulator%2520for%2520iterative%2520query%250Arefinement%253B%2520and%2520a%2520Network%2520Traverser%2520for%2520real-time%2520adaptive%2520attack%2520execution.%250AHarmNet%2520systematically%2520explores%2520and%2520refines%2520the%2520adversarial%2520space%2520to%2520uncover%250Astealthy%252C%2520high-success%2520attack%2520paths.%2520Experiments%2520across%2520closed-source%2520and%250Aopen-source%2520LLMs%2520show%2520that%2520HarmNet%2520outperforms%2520state-of-the-art%2520methods%252C%250Aachieving%2520higher%2520attack%2520success%2520rates.%2520For%2520example%252C%2520on%2520Mistral-7B%252C%2520HarmNet%250Aachieves%2520a%252099.4%2525%2520attack%2520success%2520rate%252C%252013.9%2525%2520higher%2520than%2520the%2520best%2520baseline.%250AIndex%2520terms%253A%2520jailbreak%2520attacks%253B%2520large%2520language%2520models%253B%2520adversarial%2520framework%253B%250Aquery%2520refinement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HarmNet%3A%20A%20Framework%20for%20Adaptive%20Multi-Turn%20Jailbreak%20Attacks%20on%20Large%0A%20%20Language%20Models&entry.906535625=Sidhant%20Narula%20and%20Javad%20Rafiei%20Asl%20and%20Mohammad%20Ghasemigol%20and%20Eduardo%20Blanco%20and%20Daniel%20Takabi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20remain%20vulnerable%20to%20multi-turn%20jailbreak%0Aattacks.%20We%20introduce%20HarmNet%2C%20a%20modular%20framework%20comprising%20ThoughtNet%2C%20a%0Ahierarchical%20semantic%20network%3B%20a%20feedback-driven%20Simulator%20for%20iterative%20query%0Arefinement%3B%20and%20a%20Network%20Traverser%20for%20real-time%20adaptive%20attack%20execution.%0AHarmNet%20systematically%20explores%20and%20refines%20the%20adversarial%20space%20to%20uncover%0Astealthy%2C%20high-success%20attack%20paths.%20Experiments%20across%20closed-source%20and%0Aopen-source%20LLMs%20show%20that%20HarmNet%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20higher%20attack%20success%20rates.%20For%20example%2C%20on%20Mistral-7B%2C%20HarmNet%0Aachieves%20a%2099.4%25%20attack%20success%20rate%2C%2013.9%25%20higher%20than%20the%20best%20baseline.%0AIndex%20terms%3A%20jailbreak%20attacks%3B%20large%20language%20models%3B%20adversarial%20framework%3B%0Aquery%20refinement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18728v1&entry.124074799=Read"},
{"title": "Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance\n  Propagation", "author": "Paulo Yanez Sarmiento and Simon Witzke and Nadja Klein and Bernhard Y. Renard", "abstract": "  Explainability is a key component in many applications involving deep neural\nnetworks (DNNs). However, current explanation methods for DNNs commonly leave\nit to the human observer to distinguish relevant explanations from spurious\nnoise. This is not feasible anymore when going from easily human-accessible\ndata such as images to more complex data such as genome sequences. To\nfacilitate the accessibility of DNN outputs from such complex data and to\nincrease explainability, we present a modification of the widely used\nexplanation method layer-wise relevance propagation. Our approach enforces\nsparsity directly by pruning the relevance propagation for the different\nlayers. Thereby, we achieve sparser relevance attributions for the input\nfeatures as well as for the intermediate layers. As the relevance propagation\nis input-specific, we aim to prune the relevance propagation rather than the\nunderlying model architecture. This allows to prune different neurons for\ndifferent inputs and hence, might be more appropriate to the local nature of\nexplanation methods. To demonstrate the efficacy of our method, we evaluate it\non two types of data: images and genome sequences. We show that our\nmodification indeed leads to noise reduction and concentrates relevance on the\nmost important features compared to the baseline.\n", "link": "http://arxiv.org/abs/2404.14271v2", "date": "2025-10-21", "relevancy": 2.2715, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4555}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.454}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Explanations%20of%20Neural%20Networks%20Using%20Pruned%20Layer-Wise%20Relevance%0A%20%20Propagation&body=Title%3A%20Sparse%20Explanations%20of%20Neural%20Networks%20Using%20Pruned%20Layer-Wise%20Relevance%0A%20%20Propagation%0AAuthor%3A%20Paulo%20Yanez%20Sarmiento%20and%20Simon%20Witzke%20and%20Nadja%20Klein%20and%20Bernhard%20Y.%20Renard%0AAbstract%3A%20%20%20Explainability%20is%20a%20key%20component%20in%20many%20applications%20involving%20deep%20neural%0Anetworks%20%28DNNs%29.%20However%2C%20current%20explanation%20methods%20for%20DNNs%20commonly%20leave%0Ait%20to%20the%20human%20observer%20to%20distinguish%20relevant%20explanations%20from%20spurious%0Anoise.%20This%20is%20not%20feasible%20anymore%20when%20going%20from%20easily%20human-accessible%0Adata%20such%20as%20images%20to%20more%20complex%20data%20such%20as%20genome%20sequences.%20To%0Afacilitate%20the%20accessibility%20of%20DNN%20outputs%20from%20such%20complex%20data%20and%20to%0Aincrease%20explainability%2C%20we%20present%20a%20modification%20of%20the%20widely%20used%0Aexplanation%20method%20layer-wise%20relevance%20propagation.%20Our%20approach%20enforces%0Asparsity%20directly%20by%20pruning%20the%20relevance%20propagation%20for%20the%20different%0Alayers.%20Thereby%2C%20we%20achieve%20sparser%20relevance%20attributions%20for%20the%20input%0Afeatures%20as%20well%20as%20for%20the%20intermediate%20layers.%20As%20the%20relevance%20propagation%0Ais%20input-specific%2C%20we%20aim%20to%20prune%20the%20relevance%20propagation%20rather%20than%20the%0Aunderlying%20model%20architecture.%20This%20allows%20to%20prune%20different%20neurons%20for%0Adifferent%20inputs%20and%20hence%2C%20might%20be%20more%20appropriate%20to%20the%20local%20nature%20of%0Aexplanation%20methods.%20To%20demonstrate%20the%20efficacy%20of%20our%20method%2C%20we%20evaluate%20it%0Aon%20two%20types%20of%20data%3A%20images%20and%20genome%20sequences.%20We%20show%20that%20our%0Amodification%20indeed%20leads%20to%20noise%20reduction%20and%20concentrates%20relevance%20on%20the%0Amost%20important%20features%20compared%20to%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Explanations%2520of%2520Neural%2520Networks%2520Using%2520Pruned%2520Layer-Wise%2520Relevance%250A%2520%2520Propagation%26entry.906535625%3DPaulo%2520Yanez%2520Sarmiento%2520and%2520Simon%2520Witzke%2520and%2520Nadja%2520Klein%2520and%2520Bernhard%2520Y.%2520Renard%26entry.1292438233%3D%2520%2520Explainability%2520is%2520a%2520key%2520component%2520in%2520many%2520applications%2520involving%2520deep%2520neural%250Anetworks%2520%2528DNNs%2529.%2520However%252C%2520current%2520explanation%2520methods%2520for%2520DNNs%2520commonly%2520leave%250Ait%2520to%2520the%2520human%2520observer%2520to%2520distinguish%2520relevant%2520explanations%2520from%2520spurious%250Anoise.%2520This%2520is%2520not%2520feasible%2520anymore%2520when%2520going%2520from%2520easily%2520human-accessible%250Adata%2520such%2520as%2520images%2520to%2520more%2520complex%2520data%2520such%2520as%2520genome%2520sequences.%2520To%250Afacilitate%2520the%2520accessibility%2520of%2520DNN%2520outputs%2520from%2520such%2520complex%2520data%2520and%2520to%250Aincrease%2520explainability%252C%2520we%2520present%2520a%2520modification%2520of%2520the%2520widely%2520used%250Aexplanation%2520method%2520layer-wise%2520relevance%2520propagation.%2520Our%2520approach%2520enforces%250Asparsity%2520directly%2520by%2520pruning%2520the%2520relevance%2520propagation%2520for%2520the%2520different%250Alayers.%2520Thereby%252C%2520we%2520achieve%2520sparser%2520relevance%2520attributions%2520for%2520the%2520input%250Afeatures%2520as%2520well%2520as%2520for%2520the%2520intermediate%2520layers.%2520As%2520the%2520relevance%2520propagation%250Ais%2520input-specific%252C%2520we%2520aim%2520to%2520prune%2520the%2520relevance%2520propagation%2520rather%2520than%2520the%250Aunderlying%2520model%2520architecture.%2520This%2520allows%2520to%2520prune%2520different%2520neurons%2520for%250Adifferent%2520inputs%2520and%2520hence%252C%2520might%2520be%2520more%2520appropriate%2520to%2520the%2520local%2520nature%2520of%250Aexplanation%2520methods.%2520To%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520method%252C%2520we%2520evaluate%2520it%250Aon%2520two%2520types%2520of%2520data%253A%2520images%2520and%2520genome%2520sequences.%2520We%2520show%2520that%2520our%250Amodification%2520indeed%2520leads%2520to%2520noise%2520reduction%2520and%2520concentrates%2520relevance%2520on%2520the%250Amost%2520important%2520features%2520compared%2520to%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Explanations%20of%20Neural%20Networks%20Using%20Pruned%20Layer-Wise%20Relevance%0A%20%20Propagation&entry.906535625=Paulo%20Yanez%20Sarmiento%20and%20Simon%20Witzke%20and%20Nadja%20Klein%20and%20Bernhard%20Y.%20Renard&entry.1292438233=%20%20Explainability%20is%20a%20key%20component%20in%20many%20applications%20involving%20deep%20neural%0Anetworks%20%28DNNs%29.%20However%2C%20current%20explanation%20methods%20for%20DNNs%20commonly%20leave%0Ait%20to%20the%20human%20observer%20to%20distinguish%20relevant%20explanations%20from%20spurious%0Anoise.%20This%20is%20not%20feasible%20anymore%20when%20going%20from%20easily%20human-accessible%0Adata%20such%20as%20images%20to%20more%20complex%20data%20such%20as%20genome%20sequences.%20To%0Afacilitate%20the%20accessibility%20of%20DNN%20outputs%20from%20such%20complex%20data%20and%20to%0Aincrease%20explainability%2C%20we%20present%20a%20modification%20of%20the%20widely%20used%0Aexplanation%20method%20layer-wise%20relevance%20propagation.%20Our%20approach%20enforces%0Asparsity%20directly%20by%20pruning%20the%20relevance%20propagation%20for%20the%20different%0Alayers.%20Thereby%2C%20we%20achieve%20sparser%20relevance%20attributions%20for%20the%20input%0Afeatures%20as%20well%20as%20for%20the%20intermediate%20layers.%20As%20the%20relevance%20propagation%0Ais%20input-specific%2C%20we%20aim%20to%20prune%20the%20relevance%20propagation%20rather%20than%20the%0Aunderlying%20model%20architecture.%20This%20allows%20to%20prune%20different%20neurons%20for%0Adifferent%20inputs%20and%20hence%2C%20might%20be%20more%20appropriate%20to%20the%20local%20nature%20of%0Aexplanation%20methods.%20To%20demonstrate%20the%20efficacy%20of%20our%20method%2C%20we%20evaluate%20it%0Aon%20two%20types%20of%20data%3A%20images%20and%20genome%20sequences.%20We%20show%20that%20our%0Amodification%20indeed%20leads%20to%20noise%20reduction%20and%20concentrates%20relevance%20on%20the%0Amost%20important%20features%20compared%20to%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14271v2&entry.124074799=Read"},
{"title": "CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with\n  Trainable Text Encoder", "author": "Yongmin Lee and Hye Won Chung", "abstract": "  Multimodal dataset distillation aims to synthesize a small set of image-text\npairs that enables efficient training of large-scale vision-language models.\nWhile dataset distillation has shown promise in unimodal tasks, extending it to\nmultimodal contrastive learning presents key challenges: learning cross-modal\nalignment and managing the high computational cost of large encoders. Prior\napproaches address scalability by freezing the text encoder and update only the\nimage encoder and text projection layer. However, we find this severely limits\nsemantic alignment and becomes a bottleneck for performance scaling. We propose\nCovMatch, a scalable dataset distillation framework that aligns the\ncross-covariance of real and synthetic features while regularizing feature\ndistributions within each modality. Unlike prior approaches, CovMatch enables\njoint optimization of both encoders, leading to stronger cross-modal alignment\nand improved performance. Evaluated on Flickr30K and COCO, CovMatch outperforms\nstate-of-the-art multimodal distillation methods and achieves up to 6.8%\nabsolute gains in retrieval accuracy using only 500 synthetic pairs.\n", "link": "http://arxiv.org/abs/2510.18583v1", "date": "2025-10-21", "relevancy": 2.2698, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5819}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5723}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CovMatch%3A%20Cross-Covariance%20Guided%20Multimodal%20Dataset%20Distillation%20with%0A%20%20Trainable%20Text%20Encoder&body=Title%3A%20CovMatch%3A%20Cross-Covariance%20Guided%20Multimodal%20Dataset%20Distillation%20with%0A%20%20Trainable%20Text%20Encoder%0AAuthor%3A%20Yongmin%20Lee%20and%20Hye%20Won%20Chung%0AAbstract%3A%20%20%20Multimodal%20dataset%20distillation%20aims%20to%20synthesize%20a%20small%20set%20of%20image-text%0Apairs%20that%20enables%20efficient%20training%20of%20large-scale%20vision-language%20models.%0AWhile%20dataset%20distillation%20has%20shown%20promise%20in%20unimodal%20tasks%2C%20extending%20it%20to%0Amultimodal%20contrastive%20learning%20presents%20key%20challenges%3A%20learning%20cross-modal%0Aalignment%20and%20managing%20the%20high%20computational%20cost%20of%20large%20encoders.%20Prior%0Aapproaches%20address%20scalability%20by%20freezing%20the%20text%20encoder%20and%20update%20only%20the%0Aimage%20encoder%20and%20text%20projection%20layer.%20However%2C%20we%20find%20this%20severely%20limits%0Asemantic%20alignment%20and%20becomes%20a%20bottleneck%20for%20performance%20scaling.%20We%20propose%0ACovMatch%2C%20a%20scalable%20dataset%20distillation%20framework%20that%20aligns%20the%0Across-covariance%20of%20real%20and%20synthetic%20features%20while%20regularizing%20feature%0Adistributions%20within%20each%20modality.%20Unlike%20prior%20approaches%2C%20CovMatch%20enables%0Ajoint%20optimization%20of%20both%20encoders%2C%20leading%20to%20stronger%20cross-modal%20alignment%0Aand%20improved%20performance.%20Evaluated%20on%20Flickr30K%20and%20COCO%2C%20CovMatch%20outperforms%0Astate-of-the-art%20multimodal%20distillation%20methods%20and%20achieves%20up%20to%206.8%25%0Aabsolute%20gains%20in%20retrieval%20accuracy%20using%20only%20500%20synthetic%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCovMatch%253A%2520Cross-Covariance%2520Guided%2520Multimodal%2520Dataset%2520Distillation%2520with%250A%2520%2520Trainable%2520Text%2520Encoder%26entry.906535625%3DYongmin%2520Lee%2520and%2520Hye%2520Won%2520Chung%26entry.1292438233%3D%2520%2520Multimodal%2520dataset%2520distillation%2520aims%2520to%2520synthesize%2520a%2520small%2520set%2520of%2520image-text%250Apairs%2520that%2520enables%2520efficient%2520training%2520of%2520large-scale%2520vision-language%2520models.%250AWhile%2520dataset%2520distillation%2520has%2520shown%2520promise%2520in%2520unimodal%2520tasks%252C%2520extending%2520it%2520to%250Amultimodal%2520contrastive%2520learning%2520presents%2520key%2520challenges%253A%2520learning%2520cross-modal%250Aalignment%2520and%2520managing%2520the%2520high%2520computational%2520cost%2520of%2520large%2520encoders.%2520Prior%250Aapproaches%2520address%2520scalability%2520by%2520freezing%2520the%2520text%2520encoder%2520and%2520update%2520only%2520the%250Aimage%2520encoder%2520and%2520text%2520projection%2520layer.%2520However%252C%2520we%2520find%2520this%2520severely%2520limits%250Asemantic%2520alignment%2520and%2520becomes%2520a%2520bottleneck%2520for%2520performance%2520scaling.%2520We%2520propose%250ACovMatch%252C%2520a%2520scalable%2520dataset%2520distillation%2520framework%2520that%2520aligns%2520the%250Across-covariance%2520of%2520real%2520and%2520synthetic%2520features%2520while%2520regularizing%2520feature%250Adistributions%2520within%2520each%2520modality.%2520Unlike%2520prior%2520approaches%252C%2520CovMatch%2520enables%250Ajoint%2520optimization%2520of%2520both%2520encoders%252C%2520leading%2520to%2520stronger%2520cross-modal%2520alignment%250Aand%2520improved%2520performance.%2520Evaluated%2520on%2520Flickr30K%2520and%2520COCO%252C%2520CovMatch%2520outperforms%250Astate-of-the-art%2520multimodal%2520distillation%2520methods%2520and%2520achieves%2520up%2520to%25206.8%2525%250Aabsolute%2520gains%2520in%2520retrieval%2520accuracy%2520using%2520only%2520500%2520synthetic%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CovMatch%3A%20Cross-Covariance%20Guided%20Multimodal%20Dataset%20Distillation%20with%0A%20%20Trainable%20Text%20Encoder&entry.906535625=Yongmin%20Lee%20and%20Hye%20Won%20Chung&entry.1292438233=%20%20Multimodal%20dataset%20distillation%20aims%20to%20synthesize%20a%20small%20set%20of%20image-text%0Apairs%20that%20enables%20efficient%20training%20of%20large-scale%20vision-language%20models.%0AWhile%20dataset%20distillation%20has%20shown%20promise%20in%20unimodal%20tasks%2C%20extending%20it%20to%0Amultimodal%20contrastive%20learning%20presents%20key%20challenges%3A%20learning%20cross-modal%0Aalignment%20and%20managing%20the%20high%20computational%20cost%20of%20large%20encoders.%20Prior%0Aapproaches%20address%20scalability%20by%20freezing%20the%20text%20encoder%20and%20update%20only%20the%0Aimage%20encoder%20and%20text%20projection%20layer.%20However%2C%20we%20find%20this%20severely%20limits%0Asemantic%20alignment%20and%20becomes%20a%20bottleneck%20for%20performance%20scaling.%20We%20propose%0ACovMatch%2C%20a%20scalable%20dataset%20distillation%20framework%20that%20aligns%20the%0Across-covariance%20of%20real%20and%20synthetic%20features%20while%20regularizing%20feature%0Adistributions%20within%20each%20modality.%20Unlike%20prior%20approaches%2C%20CovMatch%20enables%0Ajoint%20optimization%20of%20both%20encoders%2C%20leading%20to%20stronger%20cross-modal%20alignment%0Aand%20improved%20performance.%20Evaluated%20on%20Flickr30K%20and%20COCO%2C%20CovMatch%20outperforms%0Astate-of-the-art%20multimodal%20distillation%20methods%20and%20achieves%20up%20to%206.8%25%0Aabsolute%20gains%20in%20retrieval%20accuracy%20using%20only%20500%20synthetic%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18583v1&entry.124074799=Read"},
{"title": "FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning", "author": "Yubin Zheng and Pak-Hei Yeung and Jing Xia and Tianjie Ju and Peng Tang and Weidong Qiu and Jagath C. Rajapakse", "abstract": "  Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models without exposing local data, balancing performance and\nprivacy. However, domain shift and label heterogeneity across clients often\nhinder the generalization of the aggregated global model. Recently, large-scale\nvision-language models like CLIP have shown strong zero-shot classification\ncapabilities, raising the question of how to effectively fine-tune CLIP across\ndomains in a federated setting. In this work, we propose an adaptive federated\nprompt tuning framework, FedDEAP, to enhance CLIP's generalization in\nmulti-domain scenarios. Our method includes the following three key components:\n(1) To mitigate the loss of domain-specific information caused by\nlabel-supervised tuning, we disentangle semantic and domain-specific features\nin images by using semantic and domain transformation networks with unbiased\nmappings; (2) To preserve domain-specific knowledge during global prompt\naggregation, we introduce a dual-prompt design with a global semantic prompt\nand a local domain prompt to balance shared and personalized information; (3)\nTo maximize the inclusion of semantic and domain information from images in the\ngenerated text features, we align textual and visual representations under the\ntwo learned transformations to preserve semantic and domain consistency.\nTheoretical analysis and extensive experiments on four datasets demonstrate the\neffectiveness of our method in enhancing the generalization of CLIP for\nfederated image recognition across multiple domains.\n", "link": "http://arxiv.org/abs/2510.18837v1", "date": "2025-10-21", "relevancy": 2.2685, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6006}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5442}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedDEAP%3A%20Adaptive%20Dual-Prompt%20Tuning%20for%20Multi-Domain%20Federated%20Learning&body=Title%3A%20FedDEAP%3A%20Adaptive%20Dual-Prompt%20Tuning%20for%20Multi-Domain%20Federated%20Learning%0AAuthor%3A%20Yubin%20Zheng%20and%20Pak-Hei%20Yeung%20and%20Jing%20Xia%20and%20Tianjie%20Ju%20and%20Peng%20Tang%20and%20Weidong%20Qiu%20and%20Jagath%20C.%20Rajapakse%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20multiple%20clients%20to%20collaboratively%20train%0Amachine%20learning%20models%20without%20exposing%20local%20data%2C%20balancing%20performance%20and%0Aprivacy.%20However%2C%20domain%20shift%20and%20label%20heterogeneity%20across%20clients%20often%0Ahinder%20the%20generalization%20of%20the%20aggregated%20global%20model.%20Recently%2C%20large-scale%0Avision-language%20models%20like%20CLIP%20have%20shown%20strong%20zero-shot%20classification%0Acapabilities%2C%20raising%20the%20question%20of%20how%20to%20effectively%20fine-tune%20CLIP%20across%0Adomains%20in%20a%20federated%20setting.%20In%20this%20work%2C%20we%20propose%20an%20adaptive%20federated%0Aprompt%20tuning%20framework%2C%20FedDEAP%2C%20to%20enhance%20CLIP%27s%20generalization%20in%0Amulti-domain%20scenarios.%20Our%20method%20includes%20the%20following%20three%20key%20components%3A%0A%281%29%20To%20mitigate%20the%20loss%20of%20domain-specific%20information%20caused%20by%0Alabel-supervised%20tuning%2C%20we%20disentangle%20semantic%20and%20domain-specific%20features%0Ain%20images%20by%20using%20semantic%20and%20domain%20transformation%20networks%20with%20unbiased%0Amappings%3B%20%282%29%20To%20preserve%20domain-specific%20knowledge%20during%20global%20prompt%0Aaggregation%2C%20we%20introduce%20a%20dual-prompt%20design%20with%20a%20global%20semantic%20prompt%0Aand%20a%20local%20domain%20prompt%20to%20balance%20shared%20and%20personalized%20information%3B%20%283%29%0ATo%20maximize%20the%20inclusion%20of%20semantic%20and%20domain%20information%20from%20images%20in%20the%0Agenerated%20text%20features%2C%20we%20align%20textual%20and%20visual%20representations%20under%20the%0Atwo%20learned%20transformations%20to%20preserve%20semantic%20and%20domain%20consistency.%0ATheoretical%20analysis%20and%20extensive%20experiments%20on%20four%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20enhancing%20the%20generalization%20of%20CLIP%20for%0Afederated%20image%20recognition%20across%20multiple%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedDEAP%253A%2520Adaptive%2520Dual-Prompt%2520Tuning%2520for%2520Multi-Domain%2520Federated%2520Learning%26entry.906535625%3DYubin%2520Zheng%2520and%2520Pak-Hei%2520Yeung%2520and%2520Jing%2520Xia%2520and%2520Tianjie%2520Ju%2520and%2520Peng%2520Tang%2520and%2520Weidong%2520Qiu%2520and%2520Jagath%2520C.%2520Rajapakse%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520multiple%2520clients%2520to%2520collaboratively%2520train%250Amachine%2520learning%2520models%2520without%2520exposing%2520local%2520data%252C%2520balancing%2520performance%2520and%250Aprivacy.%2520However%252C%2520domain%2520shift%2520and%2520label%2520heterogeneity%2520across%2520clients%2520often%250Ahinder%2520the%2520generalization%2520of%2520the%2520aggregated%2520global%2520model.%2520Recently%252C%2520large-scale%250Avision-language%2520models%2520like%2520CLIP%2520have%2520shown%2520strong%2520zero-shot%2520classification%250Acapabilities%252C%2520raising%2520the%2520question%2520of%2520how%2520to%2520effectively%2520fine-tune%2520CLIP%2520across%250Adomains%2520in%2520a%2520federated%2520setting.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520adaptive%2520federated%250Aprompt%2520tuning%2520framework%252C%2520FedDEAP%252C%2520to%2520enhance%2520CLIP%2527s%2520generalization%2520in%250Amulti-domain%2520scenarios.%2520Our%2520method%2520includes%2520the%2520following%2520three%2520key%2520components%253A%250A%25281%2529%2520To%2520mitigate%2520the%2520loss%2520of%2520domain-specific%2520information%2520caused%2520by%250Alabel-supervised%2520tuning%252C%2520we%2520disentangle%2520semantic%2520and%2520domain-specific%2520features%250Ain%2520images%2520by%2520using%2520semantic%2520and%2520domain%2520transformation%2520networks%2520with%2520unbiased%250Amappings%253B%2520%25282%2529%2520To%2520preserve%2520domain-specific%2520knowledge%2520during%2520global%2520prompt%250Aaggregation%252C%2520we%2520introduce%2520a%2520dual-prompt%2520design%2520with%2520a%2520global%2520semantic%2520prompt%250Aand%2520a%2520local%2520domain%2520prompt%2520to%2520balance%2520shared%2520and%2520personalized%2520information%253B%2520%25283%2529%250ATo%2520maximize%2520the%2520inclusion%2520of%2520semantic%2520and%2520domain%2520information%2520from%2520images%2520in%2520the%250Agenerated%2520text%2520features%252C%2520we%2520align%2520textual%2520and%2520visual%2520representations%2520under%2520the%250Atwo%2520learned%2520transformations%2520to%2520preserve%2520semantic%2520and%2520domain%2520consistency.%250ATheoretical%2520analysis%2520and%2520extensive%2520experiments%2520on%2520four%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520enhancing%2520the%2520generalization%2520of%2520CLIP%2520for%250Afederated%2520image%2520recognition%2520across%2520multiple%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedDEAP%3A%20Adaptive%20Dual-Prompt%20Tuning%20for%20Multi-Domain%20Federated%20Learning&entry.906535625=Yubin%20Zheng%20and%20Pak-Hei%20Yeung%20and%20Jing%20Xia%20and%20Tianjie%20Ju%20and%20Peng%20Tang%20and%20Weidong%20Qiu%20and%20Jagath%20C.%20Rajapakse&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20multiple%20clients%20to%20collaboratively%20train%0Amachine%20learning%20models%20without%20exposing%20local%20data%2C%20balancing%20performance%20and%0Aprivacy.%20However%2C%20domain%20shift%20and%20label%20heterogeneity%20across%20clients%20often%0Ahinder%20the%20generalization%20of%20the%20aggregated%20global%20model.%20Recently%2C%20large-scale%0Avision-language%20models%20like%20CLIP%20have%20shown%20strong%20zero-shot%20classification%0Acapabilities%2C%20raising%20the%20question%20of%20how%20to%20effectively%20fine-tune%20CLIP%20across%0Adomains%20in%20a%20federated%20setting.%20In%20this%20work%2C%20we%20propose%20an%20adaptive%20federated%0Aprompt%20tuning%20framework%2C%20FedDEAP%2C%20to%20enhance%20CLIP%27s%20generalization%20in%0Amulti-domain%20scenarios.%20Our%20method%20includes%20the%20following%20three%20key%20components%3A%0A%281%29%20To%20mitigate%20the%20loss%20of%20domain-specific%20information%20caused%20by%0Alabel-supervised%20tuning%2C%20we%20disentangle%20semantic%20and%20domain-specific%20features%0Ain%20images%20by%20using%20semantic%20and%20domain%20transformation%20networks%20with%20unbiased%0Amappings%3B%20%282%29%20To%20preserve%20domain-specific%20knowledge%20during%20global%20prompt%0Aaggregation%2C%20we%20introduce%20a%20dual-prompt%20design%20with%20a%20global%20semantic%20prompt%0Aand%20a%20local%20domain%20prompt%20to%20balance%20shared%20and%20personalized%20information%3B%20%283%29%0ATo%20maximize%20the%20inclusion%20of%20semantic%20and%20domain%20information%20from%20images%20in%20the%0Agenerated%20text%20features%2C%20we%20align%20textual%20and%20visual%20representations%20under%20the%0Atwo%20learned%20transformations%20to%20preserve%20semantic%20and%20domain%20consistency.%0ATheoretical%20analysis%20and%20extensive%20experiments%20on%20four%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20enhancing%20the%20generalization%20of%20CLIP%20for%0Afederated%20image%20recognition%20across%20multiple%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18837v1&entry.124074799=Read"},
{"title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction", "author": "Khaled Boughanmi and Kamel Jedidi and Nour Jedidi", "abstract": "  This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.\n", "link": "http://arxiv.org/abs/2510.16551v2", "date": "2025-10-21", "relevancy": 2.2666, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4537}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Reviews%20to%20Actionable%20Insights%3A%20An%20LLM-Based%20Approach%20for%20Attribute%0A%20%20and%20Feature%20Extraction&body=Title%3A%20From%20Reviews%20to%20Actionable%20Insights%3A%20An%20LLM-Based%20Approach%20for%20Attribute%0A%20%20and%20Feature%20Extraction%0AAuthor%3A%20Khaled%20Boughanmi%20and%20Kamel%20Jedidi%20and%20Nour%20Jedidi%0AAbstract%3A%20%20%20This%20research%20proposes%20a%20systematic%2C%20large%20language%20model%20%28LLM%29%20approach%20for%0Aextracting%20product%20and%20service%20attributes%2C%20features%2C%20and%20associated%20sentiments%0Afrom%20customer%20reviews.%20Grounded%20in%20marketing%20theory%2C%20the%20framework%0Adistinguishes%20perceptual%20attributes%20from%20actionable%20features%2C%20producing%0Ainterpretable%20and%20managerially%20actionable%20insights.%20We%20apply%20the%20methodology%20to%0A20%2C000%20Yelp%20reviews%20of%20Starbucks%20stores%20and%20evaluate%20eight%20prompt%20variants%20on%20a%0Arandom%20subset%20of%20reviews.%20Model%20performance%20is%20assessed%20through%20agreement%20with%0Ahuman%20annotations%20and%20predictive%20validity%20for%20customer%20ratings.%20Results%20show%0Ahigh%20consistency%20between%20LLMs%20and%20human%20coders%20and%20strong%20predictive%20validity%2C%0Aconfirming%20the%20reliability%20of%20the%20approach.%20Human%20coders%20required%20a%20median%20of%0Asix%20minutes%20per%20review%2C%20whereas%20the%20LLM%20processed%20each%20in%20two%20seconds%2C%0Adelivering%20comparable%20insights%20at%20a%20scale%20unattainable%20through%20manual%20coding.%0AManagerially%2C%20the%20analysis%20identifies%20attributes%20and%20features%20that%20most%0Astrongly%20influence%20customer%20satisfaction%20and%20their%20associated%20sentiments%2C%0Aenabling%20firms%20to%20pinpoint%20%22joy%20points%2C%22%20address%20%22pain%20points%2C%22%20and%20design%0Atargeted%20interventions.%20We%20demonstrate%20how%20structured%20review%20data%20can%20power%20an%0Aactionable%20marketing%20dashboard%20that%20tracks%20sentiment%20over%20time%20and%20across%0Astores%2C%20benchmarks%20performance%2C%20and%20highlights%20high-leverage%20features%20for%0Aimprovement.%20Simulations%20indicate%20that%20enhancing%20sentiment%20for%20key%20service%0Afeatures%20could%20yield%201-2%25%20average%20revenue%20gains%20per%20store.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.16551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Reviews%2520to%2520Actionable%2520Insights%253A%2520An%2520LLM-Based%2520Approach%2520for%2520Attribute%250A%2520%2520and%2520Feature%2520Extraction%26entry.906535625%3DKhaled%2520Boughanmi%2520and%2520Kamel%2520Jedidi%2520and%2520Nour%2520Jedidi%26entry.1292438233%3D%2520%2520This%2520research%2520proposes%2520a%2520systematic%252C%2520large%2520language%2520model%2520%2528LLM%2529%2520approach%2520for%250Aextracting%2520product%2520and%2520service%2520attributes%252C%2520features%252C%2520and%2520associated%2520sentiments%250Afrom%2520customer%2520reviews.%2520Grounded%2520in%2520marketing%2520theory%252C%2520the%2520framework%250Adistinguishes%2520perceptual%2520attributes%2520from%2520actionable%2520features%252C%2520producing%250Ainterpretable%2520and%2520managerially%2520actionable%2520insights.%2520We%2520apply%2520the%2520methodology%2520to%250A20%252C000%2520Yelp%2520reviews%2520of%2520Starbucks%2520stores%2520and%2520evaluate%2520eight%2520prompt%2520variants%2520on%2520a%250Arandom%2520subset%2520of%2520reviews.%2520Model%2520performance%2520is%2520assessed%2520through%2520agreement%2520with%250Ahuman%2520annotations%2520and%2520predictive%2520validity%2520for%2520customer%2520ratings.%2520Results%2520show%250Ahigh%2520consistency%2520between%2520LLMs%2520and%2520human%2520coders%2520and%2520strong%2520predictive%2520validity%252C%250Aconfirming%2520the%2520reliability%2520of%2520the%2520approach.%2520Human%2520coders%2520required%2520a%2520median%2520of%250Asix%2520minutes%2520per%2520review%252C%2520whereas%2520the%2520LLM%2520processed%2520each%2520in%2520two%2520seconds%252C%250Adelivering%2520comparable%2520insights%2520at%2520a%2520scale%2520unattainable%2520through%2520manual%2520coding.%250AManagerially%252C%2520the%2520analysis%2520identifies%2520attributes%2520and%2520features%2520that%2520most%250Astrongly%2520influence%2520customer%2520satisfaction%2520and%2520their%2520associated%2520sentiments%252C%250Aenabling%2520firms%2520to%2520pinpoint%2520%2522joy%2520points%252C%2522%2520address%2520%2522pain%2520points%252C%2522%2520and%2520design%250Atargeted%2520interventions.%2520We%2520demonstrate%2520how%2520structured%2520review%2520data%2520can%2520power%2520an%250Aactionable%2520marketing%2520dashboard%2520that%2520tracks%2520sentiment%2520over%2520time%2520and%2520across%250Astores%252C%2520benchmarks%2520performance%252C%2520and%2520highlights%2520high-leverage%2520features%2520for%250Aimprovement.%2520Simulations%2520indicate%2520that%2520enhancing%2520sentiment%2520for%2520key%2520service%250Afeatures%2520could%2520yield%25201-2%2525%2520average%2520revenue%2520gains%2520per%2520store.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Reviews%20to%20Actionable%20Insights%3A%20An%20LLM-Based%20Approach%20for%20Attribute%0A%20%20and%20Feature%20Extraction&entry.906535625=Khaled%20Boughanmi%20and%20Kamel%20Jedidi%20and%20Nour%20Jedidi&entry.1292438233=%20%20This%20research%20proposes%20a%20systematic%2C%20large%20language%20model%20%28LLM%29%20approach%20for%0Aextracting%20product%20and%20service%20attributes%2C%20features%2C%20and%20associated%20sentiments%0Afrom%20customer%20reviews.%20Grounded%20in%20marketing%20theory%2C%20the%20framework%0Adistinguishes%20perceptual%20attributes%20from%20actionable%20features%2C%20producing%0Ainterpretable%20and%20managerially%20actionable%20insights.%20We%20apply%20the%20methodology%20to%0A20%2C000%20Yelp%20reviews%20of%20Starbucks%20stores%20and%20evaluate%20eight%20prompt%20variants%20on%20a%0Arandom%20subset%20of%20reviews.%20Model%20performance%20is%20assessed%20through%20agreement%20with%0Ahuman%20annotations%20and%20predictive%20validity%20for%20customer%20ratings.%20Results%20show%0Ahigh%20consistency%20between%20LLMs%20and%20human%20coders%20and%20strong%20predictive%20validity%2C%0Aconfirming%20the%20reliability%20of%20the%20approach.%20Human%20coders%20required%20a%20median%20of%0Asix%20minutes%20per%20review%2C%20whereas%20the%20LLM%20processed%20each%20in%20two%20seconds%2C%0Adelivering%20comparable%20insights%20at%20a%20scale%20unattainable%20through%20manual%20coding.%0AManagerially%2C%20the%20analysis%20identifies%20attributes%20and%20features%20that%20most%0Astrongly%20influence%20customer%20satisfaction%20and%20their%20associated%20sentiments%2C%0Aenabling%20firms%20to%20pinpoint%20%22joy%20points%2C%22%20address%20%22pain%20points%2C%22%20and%20design%0Atargeted%20interventions.%20We%20demonstrate%20how%20structured%20review%20data%20can%20power%20an%0Aactionable%20marketing%20dashboard%20that%20tracks%20sentiment%20over%20time%20and%20across%0Astores%2C%20benchmarks%20performance%2C%20and%20highlights%20high-leverage%20features%20for%0Aimprovement.%20Simulations%20indicate%20that%20enhancing%20sentiment%20for%20key%20service%0Afeatures%20could%20yield%201-2%25%20average%20revenue%20gains%20per%20store.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.16551v2&entry.124074799=Read"},
{"title": "A Compositional Paradigm for Foundation Models: Towards Smarter Robotic\n  Agents", "author": "Luigi Quarantiello and Elia Piccoli and Jack Bell and Malio Li and Giacomo Carf\u00ec and Eric Nuertey Coleman and Gerlando Gramaglia and Lanpei Li and Mauro Madeddu and Irene Testa and Vincenzo Lomonaco", "abstract": "  The birth of Foundation Models brought unprecedented results in a wide range\nof tasks, from language to vision, to robotic control. These models are able to\nprocess huge quantities of data, and can extract and develop rich\nrepresentations, which can be employed across different domains and modalities.\nHowever, they still have issues in adapting to dynamic, real-world scenarios\nwithout retraining the entire model from scratch. In this work, we propose the\napplication of Continual Learning and Compositionality principles to foster the\ndevelopment of more flexible, efficient and smart AI solutions.\n", "link": "http://arxiv.org/abs/2510.18608v1", "date": "2025-10-21", "relevancy": 2.2537, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Compositional%20Paradigm%20for%20Foundation%20Models%3A%20Towards%20Smarter%20Robotic%0A%20%20Agents&body=Title%3A%20A%20Compositional%20Paradigm%20for%20Foundation%20Models%3A%20Towards%20Smarter%20Robotic%0A%20%20Agents%0AAuthor%3A%20Luigi%20Quarantiello%20and%20Elia%20Piccoli%20and%20Jack%20Bell%20and%20Malio%20Li%20and%20Giacomo%20Carf%C3%AC%20and%20Eric%20Nuertey%20Coleman%20and%20Gerlando%20Gramaglia%20and%20Lanpei%20Li%20and%20Mauro%20Madeddu%20and%20Irene%20Testa%20and%20Vincenzo%20Lomonaco%0AAbstract%3A%20%20%20The%20birth%20of%20Foundation%20Models%20brought%20unprecedented%20results%20in%20a%20wide%20range%0Aof%20tasks%2C%20from%20language%20to%20vision%2C%20to%20robotic%20control.%20These%20models%20are%20able%20to%0Aprocess%20huge%20quantities%20of%20data%2C%20and%20can%20extract%20and%20develop%20rich%0Arepresentations%2C%20which%20can%20be%20employed%20across%20different%20domains%20and%20modalities.%0AHowever%2C%20they%20still%20have%20issues%20in%20adapting%20to%20dynamic%2C%20real-world%20scenarios%0Awithout%20retraining%20the%20entire%20model%20from%20scratch.%20In%20this%20work%2C%20we%20propose%20the%0Aapplication%20of%20Continual%20Learning%20and%20Compositionality%20principles%20to%20foster%20the%0Adevelopment%20of%20more%20flexible%2C%20efficient%20and%20smart%20AI%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Compositional%2520Paradigm%2520for%2520Foundation%2520Models%253A%2520Towards%2520Smarter%2520Robotic%250A%2520%2520Agents%26entry.906535625%3DLuigi%2520Quarantiello%2520and%2520Elia%2520Piccoli%2520and%2520Jack%2520Bell%2520and%2520Malio%2520Li%2520and%2520Giacomo%2520Carf%25C3%25AC%2520and%2520Eric%2520Nuertey%2520Coleman%2520and%2520Gerlando%2520Gramaglia%2520and%2520Lanpei%2520Li%2520and%2520Mauro%2520Madeddu%2520and%2520Irene%2520Testa%2520and%2520Vincenzo%2520Lomonaco%26entry.1292438233%3D%2520%2520The%2520birth%2520of%2520Foundation%2520Models%2520brought%2520unprecedented%2520results%2520in%2520a%2520wide%2520range%250Aof%2520tasks%252C%2520from%2520language%2520to%2520vision%252C%2520to%2520robotic%2520control.%2520These%2520models%2520are%2520able%2520to%250Aprocess%2520huge%2520quantities%2520of%2520data%252C%2520and%2520can%2520extract%2520and%2520develop%2520rich%250Arepresentations%252C%2520which%2520can%2520be%2520employed%2520across%2520different%2520domains%2520and%2520modalities.%250AHowever%252C%2520they%2520still%2520have%2520issues%2520in%2520adapting%2520to%2520dynamic%252C%2520real-world%2520scenarios%250Awithout%2520retraining%2520the%2520entire%2520model%2520from%2520scratch.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%250Aapplication%2520of%2520Continual%2520Learning%2520and%2520Compositionality%2520principles%2520to%2520foster%2520the%250Adevelopment%2520of%2520more%2520flexible%252C%2520efficient%2520and%2520smart%2520AI%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Compositional%20Paradigm%20for%20Foundation%20Models%3A%20Towards%20Smarter%20Robotic%0A%20%20Agents&entry.906535625=Luigi%20Quarantiello%20and%20Elia%20Piccoli%20and%20Jack%20Bell%20and%20Malio%20Li%20and%20Giacomo%20Carf%C3%AC%20and%20Eric%20Nuertey%20Coleman%20and%20Gerlando%20Gramaglia%20and%20Lanpei%20Li%20and%20Mauro%20Madeddu%20and%20Irene%20Testa%20and%20Vincenzo%20Lomonaco&entry.1292438233=%20%20The%20birth%20of%20Foundation%20Models%20brought%20unprecedented%20results%20in%20a%20wide%20range%0Aof%20tasks%2C%20from%20language%20to%20vision%2C%20to%20robotic%20control.%20These%20models%20are%20able%20to%0Aprocess%20huge%20quantities%20of%20data%2C%20and%20can%20extract%20and%20develop%20rich%0Arepresentations%2C%20which%20can%20be%20employed%20across%20different%20domains%20and%20modalities.%0AHowever%2C%20they%20still%20have%20issues%20in%20adapting%20to%20dynamic%2C%20real-world%20scenarios%0Awithout%20retraining%20the%20entire%20model%20from%20scratch.%20In%20this%20work%2C%20we%20propose%20the%0Aapplication%20of%20Continual%20Learning%20and%20Compositionality%20principles%20to%20foster%20the%0Adevelopment%20of%20more%20flexible%2C%20efficient%20and%20smart%20AI%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18608v1&entry.124074799=Read"},
{"title": "A representational framework for learning and encoding structurally\n  enriched trajectories in complex agent environments", "author": "Corina Catarau-Cotutiu and Esther Mondragon and Eduardo Alonso", "abstract": "  The ability of artificial intelligence agents to make optimal decisions and\ngeneralise them to different domains and tasks is compromised in complex\nscenarios. One way to address this issue has focused on learning efficient\nrepresentations of the world and on how the actions of agents affect them in\nstate-action transitions. Whereas such representations are procedurally\nefficient, they lack structural richness. To address this problem, we propose\nto enhance the agent's ontology and extend the traditional conceptualisation of\ntrajectories to provide a more nuanced view of task execution. Structurally\nEnriched Trajectories (SETs) extend the encoding of sequences of states and\ntheir transitions by incorporating hierarchical relations between objects,\ninteractions, and affordances. SETs are built as multi-level graphs, providing\na detailed representation of the agent dynamics and a transferable functional\nabstraction of the task. SETs are integrated into an architecture, Structurally\nEnriched Trajectory Learning and Encoding (SETLE), that employs a heterogeneous\ngraph-based memory structure of multi-level relational dependencies essential\nfor generalisation. We demonstrate that SETLE can support downstream tasks,\nenabling agents to recognise task relevant structural patterns across CREATE\nand MiniGrid environments. Finally, we integrate SETLE with reinforcement\nlearning and show measurable improvements in downstream performance, including\nbreakthrough success rates in complex, sparse-reward tasks.\n", "link": "http://arxiv.org/abs/2503.13194v2", "date": "2025-10-21", "relevancy": 2.2537, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20representational%20framework%20for%20learning%20and%20encoding%20structurally%0A%20%20enriched%20trajectories%20in%20complex%20agent%20environments&body=Title%3A%20A%20representational%20framework%20for%20learning%20and%20encoding%20structurally%0A%20%20enriched%20trajectories%20in%20complex%20agent%20environments%0AAuthor%3A%20Corina%20Catarau-Cotutiu%20and%20Esther%20Mondragon%20and%20Eduardo%20Alonso%0AAbstract%3A%20%20%20The%20ability%20of%20artificial%20intelligence%20agents%20to%20make%20optimal%20decisions%20and%0Ageneralise%20them%20to%20different%20domains%20and%20tasks%20is%20compromised%20in%20complex%0Ascenarios.%20One%20way%20to%20address%20this%20issue%20has%20focused%20on%20learning%20efficient%0Arepresentations%20of%20the%20world%20and%20on%20how%20the%20actions%20of%20agents%20affect%20them%20in%0Astate-action%20transitions.%20Whereas%20such%20representations%20are%20procedurally%0Aefficient%2C%20they%20lack%20structural%20richness.%20To%20address%20this%20problem%2C%20we%20propose%0Ato%20enhance%20the%20agent%27s%20ontology%20and%20extend%20the%20traditional%20conceptualisation%20of%0Atrajectories%20to%20provide%20a%20more%20nuanced%20view%20of%20task%20execution.%20Structurally%0AEnriched%20Trajectories%20%28SETs%29%20extend%20the%20encoding%20of%20sequences%20of%20states%20and%0Atheir%20transitions%20by%20incorporating%20hierarchical%20relations%20between%20objects%2C%0Ainteractions%2C%20and%20affordances.%20SETs%20are%20built%20as%20multi-level%20graphs%2C%20providing%0Aa%20detailed%20representation%20of%20the%20agent%20dynamics%20and%20a%20transferable%20functional%0Aabstraction%20of%20the%20task.%20SETs%20are%20integrated%20into%20an%20architecture%2C%20Structurally%0AEnriched%20Trajectory%20Learning%20and%20Encoding%20%28SETLE%29%2C%20that%20employs%20a%20heterogeneous%0Agraph-based%20memory%20structure%20of%20multi-level%20relational%20dependencies%20essential%0Afor%20generalisation.%20We%20demonstrate%20that%20SETLE%20can%20support%20downstream%20tasks%2C%0Aenabling%20agents%20to%20recognise%20task%20relevant%20structural%20patterns%20across%20CREATE%0Aand%20MiniGrid%20environments.%20Finally%2C%20we%20integrate%20SETLE%20with%20reinforcement%0Alearning%20and%20show%20measurable%20improvements%20in%20downstream%20performance%2C%20including%0Abreakthrough%20success%20rates%20in%20complex%2C%20sparse-reward%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13194v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520representational%2520framework%2520for%2520learning%2520and%2520encoding%2520structurally%250A%2520%2520enriched%2520trajectories%2520in%2520complex%2520agent%2520environments%26entry.906535625%3DCorina%2520Catarau-Cotutiu%2520and%2520Esther%2520Mondragon%2520and%2520Eduardo%2520Alonso%26entry.1292438233%3D%2520%2520The%2520ability%2520of%2520artificial%2520intelligence%2520agents%2520to%2520make%2520optimal%2520decisions%2520and%250Ageneralise%2520them%2520to%2520different%2520domains%2520and%2520tasks%2520is%2520compromised%2520in%2520complex%250Ascenarios.%2520One%2520way%2520to%2520address%2520this%2520issue%2520has%2520focused%2520on%2520learning%2520efficient%250Arepresentations%2520of%2520the%2520world%2520and%2520on%2520how%2520the%2520actions%2520of%2520agents%2520affect%2520them%2520in%250Astate-action%2520transitions.%2520Whereas%2520such%2520representations%2520are%2520procedurally%250Aefficient%252C%2520they%2520lack%2520structural%2520richness.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%250Ato%2520enhance%2520the%2520agent%2527s%2520ontology%2520and%2520extend%2520the%2520traditional%2520conceptualisation%2520of%250Atrajectories%2520to%2520provide%2520a%2520more%2520nuanced%2520view%2520of%2520task%2520execution.%2520Structurally%250AEnriched%2520Trajectories%2520%2528SETs%2529%2520extend%2520the%2520encoding%2520of%2520sequences%2520of%2520states%2520and%250Atheir%2520transitions%2520by%2520incorporating%2520hierarchical%2520relations%2520between%2520objects%252C%250Ainteractions%252C%2520and%2520affordances.%2520SETs%2520are%2520built%2520as%2520multi-level%2520graphs%252C%2520providing%250Aa%2520detailed%2520representation%2520of%2520the%2520agent%2520dynamics%2520and%2520a%2520transferable%2520functional%250Aabstraction%2520of%2520the%2520task.%2520SETs%2520are%2520integrated%2520into%2520an%2520architecture%252C%2520Structurally%250AEnriched%2520Trajectory%2520Learning%2520and%2520Encoding%2520%2528SETLE%2529%252C%2520that%2520employs%2520a%2520heterogeneous%250Agraph-based%2520memory%2520structure%2520of%2520multi-level%2520relational%2520dependencies%2520essential%250Afor%2520generalisation.%2520We%2520demonstrate%2520that%2520SETLE%2520can%2520support%2520downstream%2520tasks%252C%250Aenabling%2520agents%2520to%2520recognise%2520task%2520relevant%2520structural%2520patterns%2520across%2520CREATE%250Aand%2520MiniGrid%2520environments.%2520Finally%252C%2520we%2520integrate%2520SETLE%2520with%2520reinforcement%250Alearning%2520and%2520show%2520measurable%2520improvements%2520in%2520downstream%2520performance%252C%2520including%250Abreakthrough%2520success%2520rates%2520in%2520complex%252C%2520sparse-reward%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13194v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20representational%20framework%20for%20learning%20and%20encoding%20structurally%0A%20%20enriched%20trajectories%20in%20complex%20agent%20environments&entry.906535625=Corina%20Catarau-Cotutiu%20and%20Esther%20Mondragon%20and%20Eduardo%20Alonso&entry.1292438233=%20%20The%20ability%20of%20artificial%20intelligence%20agents%20to%20make%20optimal%20decisions%20and%0Ageneralise%20them%20to%20different%20domains%20and%20tasks%20is%20compromised%20in%20complex%0Ascenarios.%20One%20way%20to%20address%20this%20issue%20has%20focused%20on%20learning%20efficient%0Arepresentations%20of%20the%20world%20and%20on%20how%20the%20actions%20of%20agents%20affect%20them%20in%0Astate-action%20transitions.%20Whereas%20such%20representations%20are%20procedurally%0Aefficient%2C%20they%20lack%20structural%20richness.%20To%20address%20this%20problem%2C%20we%20propose%0Ato%20enhance%20the%20agent%27s%20ontology%20and%20extend%20the%20traditional%20conceptualisation%20of%0Atrajectories%20to%20provide%20a%20more%20nuanced%20view%20of%20task%20execution.%20Structurally%0AEnriched%20Trajectories%20%28SETs%29%20extend%20the%20encoding%20of%20sequences%20of%20states%20and%0Atheir%20transitions%20by%20incorporating%20hierarchical%20relations%20between%20objects%2C%0Ainteractions%2C%20and%20affordances.%20SETs%20are%20built%20as%20multi-level%20graphs%2C%20providing%0Aa%20detailed%20representation%20of%20the%20agent%20dynamics%20and%20a%20transferable%20functional%0Aabstraction%20of%20the%20task.%20SETs%20are%20integrated%20into%20an%20architecture%2C%20Structurally%0AEnriched%20Trajectory%20Learning%20and%20Encoding%20%28SETLE%29%2C%20that%20employs%20a%20heterogeneous%0Agraph-based%20memory%20structure%20of%20multi-level%20relational%20dependencies%20essential%0Afor%20generalisation.%20We%20demonstrate%20that%20SETLE%20can%20support%20downstream%20tasks%2C%0Aenabling%20agents%20to%20recognise%20task%20relevant%20structural%20patterns%20across%20CREATE%0Aand%20MiniGrid%20environments.%20Finally%2C%20we%20integrate%20SETLE%20with%20reinforcement%0Alearning%20and%20show%20measurable%20improvements%20in%20downstream%20performance%2C%20including%0Abreakthrough%20success%20rates%20in%20complex%2C%20sparse-reward%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13194v2&entry.124074799=Read"},
{"title": "A Renaissance of Explicit Motion Information Mining from Transformers\n  for Action Recognition", "author": "Peiqin Zhuang and Lei Bai and Yichao Wu and Ding Liang and Luping Zhou and Yali Wang and Wanli Ouyang", "abstract": "  Recently, action recognition has been dominated by transformer-based methods,\nthanks to their spatiotemporal contextual aggregation capacities. However,\ndespite the significant progress achieved on scene-related datasets, they do\nnot perform well on motion-sensitive datasets due to the lack of elaborate\nmotion modeling designs. Meanwhile, we observe that the widely-used cost volume\nin traditional action recognition is highly similar to the affinity matrix\ndefined in self-attention, but equipped with powerful motion modeling\ncapacities. In light of this, we propose to integrate those effective motion\nmodeling properties into the existing transformer in a unified and neat way,\nwith the proposal of the Explicit Motion Information Mining module (EMIM). In\nEMIM, we propose to construct the desirable affinity matrix in a cost volume\nstyle, where the set of key candidate tokens is sampled from the query-based\nneighboring area in the next frame in a sliding-window manner. Then, the\nconstructed affinity matrix is used to aggregate contextual information for\nappearance modeling and is converted into motion features for motion modeling\nas well. We validate the motion modeling capacities of our method on four\nwidely-used datasets, and our method performs better than existing\nstate-of-the-art approaches, especially on motion-sensitive datasets, i.e.,\nSomething-Something V1 & V2.\n", "link": "http://arxiv.org/abs/2510.18705v1", "date": "2025-10-21", "relevancy": 2.2388, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.573}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5659}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Renaissance%20of%20Explicit%20Motion%20Information%20Mining%20from%20Transformers%0A%20%20for%20Action%20Recognition&body=Title%3A%20A%20Renaissance%20of%20Explicit%20Motion%20Information%20Mining%20from%20Transformers%0A%20%20for%20Action%20Recognition%0AAuthor%3A%20Peiqin%20Zhuang%20and%20Lei%20Bai%20and%20Yichao%20Wu%20and%20Ding%20Liang%20and%20Luping%20Zhou%20and%20Yali%20Wang%20and%20Wanli%20Ouyang%0AAbstract%3A%20%20%20Recently%2C%20action%20recognition%20has%20been%20dominated%20by%20transformer-based%20methods%2C%0Athanks%20to%20their%20spatiotemporal%20contextual%20aggregation%20capacities.%20However%2C%0Adespite%20the%20significant%20progress%20achieved%20on%20scene-related%20datasets%2C%20they%20do%0Anot%20perform%20well%20on%20motion-sensitive%20datasets%20due%20to%20the%20lack%20of%20elaborate%0Amotion%20modeling%20designs.%20Meanwhile%2C%20we%20observe%20that%20the%20widely-used%20cost%20volume%0Ain%20traditional%20action%20recognition%20is%20highly%20similar%20to%20the%20affinity%20matrix%0Adefined%20in%20self-attention%2C%20but%20equipped%20with%20powerful%20motion%20modeling%0Acapacities.%20In%20light%20of%20this%2C%20we%20propose%20to%20integrate%20those%20effective%20motion%0Amodeling%20properties%20into%20the%20existing%20transformer%20in%20a%20unified%20and%20neat%20way%2C%0Awith%20the%20proposal%20of%20the%20Explicit%20Motion%20Information%20Mining%20module%20%28EMIM%29.%20In%0AEMIM%2C%20we%20propose%20to%20construct%20the%20desirable%20affinity%20matrix%20in%20a%20cost%20volume%0Astyle%2C%20where%20the%20set%20of%20key%20candidate%20tokens%20is%20sampled%20from%20the%20query-based%0Aneighboring%20area%20in%20the%20next%20frame%20in%20a%20sliding-window%20manner.%20Then%2C%20the%0Aconstructed%20affinity%20matrix%20is%20used%20to%20aggregate%20contextual%20information%20for%0Aappearance%20modeling%20and%20is%20converted%20into%20motion%20features%20for%20motion%20modeling%0Aas%20well.%20We%20validate%20the%20motion%20modeling%20capacities%20of%20our%20method%20on%20four%0Awidely-used%20datasets%2C%20and%20our%20method%20performs%20better%20than%20existing%0Astate-of-the-art%20approaches%2C%20especially%20on%20motion-sensitive%20datasets%2C%20i.e.%2C%0ASomething-Something%20V1%20%26%20V2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Renaissance%2520of%2520Explicit%2520Motion%2520Information%2520Mining%2520from%2520Transformers%250A%2520%2520for%2520Action%2520Recognition%26entry.906535625%3DPeiqin%2520Zhuang%2520and%2520Lei%2520Bai%2520and%2520Yichao%2520Wu%2520and%2520Ding%2520Liang%2520and%2520Luping%2520Zhou%2520and%2520Yali%2520Wang%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3D%2520%2520Recently%252C%2520action%2520recognition%2520has%2520been%2520dominated%2520by%2520transformer-based%2520methods%252C%250Athanks%2520to%2520their%2520spatiotemporal%2520contextual%2520aggregation%2520capacities.%2520However%252C%250Adespite%2520the%2520significant%2520progress%2520achieved%2520on%2520scene-related%2520datasets%252C%2520they%2520do%250Anot%2520perform%2520well%2520on%2520motion-sensitive%2520datasets%2520due%2520to%2520the%2520lack%2520of%2520elaborate%250Amotion%2520modeling%2520designs.%2520Meanwhile%252C%2520we%2520observe%2520that%2520the%2520widely-used%2520cost%2520volume%250Ain%2520traditional%2520action%2520recognition%2520is%2520highly%2520similar%2520to%2520the%2520affinity%2520matrix%250Adefined%2520in%2520self-attention%252C%2520but%2520equipped%2520with%2520powerful%2520motion%2520modeling%250Acapacities.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520to%2520integrate%2520those%2520effective%2520motion%250Amodeling%2520properties%2520into%2520the%2520existing%2520transformer%2520in%2520a%2520unified%2520and%2520neat%2520way%252C%250Awith%2520the%2520proposal%2520of%2520the%2520Explicit%2520Motion%2520Information%2520Mining%2520module%2520%2528EMIM%2529.%2520In%250AEMIM%252C%2520we%2520propose%2520to%2520construct%2520the%2520desirable%2520affinity%2520matrix%2520in%2520a%2520cost%2520volume%250Astyle%252C%2520where%2520the%2520set%2520of%2520key%2520candidate%2520tokens%2520is%2520sampled%2520from%2520the%2520query-based%250Aneighboring%2520area%2520in%2520the%2520next%2520frame%2520in%2520a%2520sliding-window%2520manner.%2520Then%252C%2520the%250Aconstructed%2520affinity%2520matrix%2520is%2520used%2520to%2520aggregate%2520contextual%2520information%2520for%250Aappearance%2520modeling%2520and%2520is%2520converted%2520into%2520motion%2520features%2520for%2520motion%2520modeling%250Aas%2520well.%2520We%2520validate%2520the%2520motion%2520modeling%2520capacities%2520of%2520our%2520method%2520on%2520four%250Awidely-used%2520datasets%252C%2520and%2520our%2520method%2520performs%2520better%2520than%2520existing%250Astate-of-the-art%2520approaches%252C%2520especially%2520on%2520motion-sensitive%2520datasets%252C%2520i.e.%252C%250ASomething-Something%2520V1%2520%2526%2520V2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Renaissance%20of%20Explicit%20Motion%20Information%20Mining%20from%20Transformers%0A%20%20for%20Action%20Recognition&entry.906535625=Peiqin%20Zhuang%20and%20Lei%20Bai%20and%20Yichao%20Wu%20and%20Ding%20Liang%20and%20Luping%20Zhou%20and%20Yali%20Wang%20and%20Wanli%20Ouyang&entry.1292438233=%20%20Recently%2C%20action%20recognition%20has%20been%20dominated%20by%20transformer-based%20methods%2C%0Athanks%20to%20their%20spatiotemporal%20contextual%20aggregation%20capacities.%20However%2C%0Adespite%20the%20significant%20progress%20achieved%20on%20scene-related%20datasets%2C%20they%20do%0Anot%20perform%20well%20on%20motion-sensitive%20datasets%20due%20to%20the%20lack%20of%20elaborate%0Amotion%20modeling%20designs.%20Meanwhile%2C%20we%20observe%20that%20the%20widely-used%20cost%20volume%0Ain%20traditional%20action%20recognition%20is%20highly%20similar%20to%20the%20affinity%20matrix%0Adefined%20in%20self-attention%2C%20but%20equipped%20with%20powerful%20motion%20modeling%0Acapacities.%20In%20light%20of%20this%2C%20we%20propose%20to%20integrate%20those%20effective%20motion%0Amodeling%20properties%20into%20the%20existing%20transformer%20in%20a%20unified%20and%20neat%20way%2C%0Awith%20the%20proposal%20of%20the%20Explicit%20Motion%20Information%20Mining%20module%20%28EMIM%29.%20In%0AEMIM%2C%20we%20propose%20to%20construct%20the%20desirable%20affinity%20matrix%20in%20a%20cost%20volume%0Astyle%2C%20where%20the%20set%20of%20key%20candidate%20tokens%20is%20sampled%20from%20the%20query-based%0Aneighboring%20area%20in%20the%20next%20frame%20in%20a%20sliding-window%20manner.%20Then%2C%20the%0Aconstructed%20affinity%20matrix%20is%20used%20to%20aggregate%20contextual%20information%20for%0Aappearance%20modeling%20and%20is%20converted%20into%20motion%20features%20for%20motion%20modeling%0Aas%20well.%20We%20validate%20the%20motion%20modeling%20capacities%20of%20our%20method%20on%20four%0Awidely-used%20datasets%2C%20and%20our%20method%20performs%20better%20than%20existing%0Astate-of-the-art%20approaches%2C%20especially%20on%20motion-sensitive%20datasets%2C%20i.e.%2C%0ASomething-Something%20V1%20%26%20V2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18705v1&entry.124074799=Read"},
{"title": "High-Fidelity And Complex Test Data Generation For Google SQL Code\n  Generation Services", "author": "Shivasankari Kannan and Yeounoh Chung and Amita Gondi and Tristan Swadell and Fatma Ozcan", "abstract": "  The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically relevant high-fidelity mock data for\ncomplex data structures that includes columns with nested structures that we\nfrequently encounter in Google workloads. We highlight the limitations of\nexisting approaches used in production, particularly their inability to handle\nlarge and complex data structures, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate syntactically correct and semantically\nrelevant high-fidelity test data that adheres to complex structural constraints\nand maintains semantic integrity to the SQL test targets (queries/functions).\nThis approach supports comprehensive testing of complex SQL queries involving\njoins, aggregations, and even deeply nested subqueries, ensuring robust\nevaluation of SQL code generation services, like NL2SQL and SQL Code Assistant.\nOur results demonstrate the practical utility of an LLM (\\textit{gemini}) based\ntest data generation for industrial SQL code generation services where\ngenerating high-fidelity test data is essential due to the frequent\nunavailability and inaccessibility of production datasets for testing.\n", "link": "http://arxiv.org/abs/2504.17203v3", "date": "2025-10-21", "relevancy": 2.2354, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4515}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4492}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Google%20SQL%20Code%0A%20%20Generation%20Services&body=Title%3A%20High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Google%20SQL%20Code%0A%20%20Generation%20Services%0AAuthor%3A%20Shivasankari%20Kannan%20and%20Yeounoh%20Chung%20and%20Amita%20Gondi%20and%20Tristan%20Swadell%20and%20Fatma%20Ozcan%0AAbstract%3A%20%20%20The%20demand%20for%20high-fidelity%20test%20data%20is%20paramount%20in%20industrial%20settings%0Awhere%20access%20to%20production%20data%20is%20largely%20restricted.%20Traditional%20data%0Ageneration%20methods%20often%20fall%20short%2C%20struggling%20with%20low-fidelity%20and%20the%0Aability%20to%20model%20complex%20data%20structures%20and%20semantic%20relationships%20that%20are%0Acritical%20for%20testing%20complex%20SQL%20code%20generation%20services%20like%20Natural%20Language%0Ato%20SQL%20%28NL2SQL%29.%20In%20this%20paper%2C%20we%20address%20the%20critical%20need%20for%20generating%0Asyntactically%20correct%20and%20semantically%20relevant%20high-fidelity%20mock%20data%20for%0Acomplex%20data%20structures%20that%20includes%20columns%20with%20nested%20structures%20that%20we%0Afrequently%20encounter%20in%20Google%20workloads.%20We%20highlight%20the%20limitations%20of%0Aexisting%20approaches%20used%20in%20production%2C%20particularly%20their%20inability%20to%20handle%0Alarge%20and%20complex%20data%20structures%2C%20as%20well%20as%20the%20lack%20of%20semantically%20coherent%0Atest%20data%20that%20lead%20to%20limited%20test%20coverage.%20We%20demonstrate%20that%20by%20leveraging%0ALarge%20Language%20Models%20%28LLMs%29%20and%20incorporating%20strategic%20pre-%20and%0Apost-processing%20steps%2C%20we%20can%20generate%20syntactically%20correct%20and%20semantically%0Arelevant%20high-fidelity%20test%20data%20that%20adheres%20to%20complex%20structural%20constraints%0Aand%20maintains%20semantic%20integrity%20to%20the%20SQL%20test%20targets%20%28queries/functions%29.%0AThis%20approach%20supports%20comprehensive%20testing%20of%20complex%20SQL%20queries%20involving%0Ajoins%2C%20aggregations%2C%20and%20even%20deeply%20nested%20subqueries%2C%20ensuring%20robust%0Aevaluation%20of%20SQL%20code%20generation%20services%2C%20like%20NL2SQL%20and%20SQL%20Code%20Assistant.%0AOur%20results%20demonstrate%20the%20practical%20utility%20of%20an%20LLM%20%28%5Ctextit%7Bgemini%7D%29%20based%0Atest%20data%20generation%20for%20industrial%20SQL%20code%20generation%20services%20where%0Agenerating%20high-fidelity%20test%20data%20is%20essential%20due%20to%20the%20frequent%0Aunavailability%20and%20inaccessibility%20of%20production%20datasets%20for%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17203v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Fidelity%2520And%2520Complex%2520Test%2520Data%2520Generation%2520For%2520Google%2520SQL%2520Code%250A%2520%2520Generation%2520Services%26entry.906535625%3DShivasankari%2520Kannan%2520and%2520Yeounoh%2520Chung%2520and%2520Amita%2520Gondi%2520and%2520Tristan%2520Swadell%2520and%2520Fatma%2520Ozcan%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520high-fidelity%2520test%2520data%2520is%2520paramount%2520in%2520industrial%2520settings%250Awhere%2520access%2520to%2520production%2520data%2520is%2520largely%2520restricted.%2520Traditional%2520data%250Ageneration%2520methods%2520often%2520fall%2520short%252C%2520struggling%2520with%2520low-fidelity%2520and%2520the%250Aability%2520to%2520model%2520complex%2520data%2520structures%2520and%2520semantic%2520relationships%2520that%2520are%250Acritical%2520for%2520testing%2520complex%2520SQL%2520code%2520generation%2520services%2520like%2520Natural%2520Language%250Ato%2520SQL%2520%2528NL2SQL%2529.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520critical%2520need%2520for%2520generating%250Asyntactically%2520correct%2520and%2520semantically%2520relevant%2520high-fidelity%2520mock%2520data%2520for%250Acomplex%2520data%2520structures%2520that%2520includes%2520columns%2520with%2520nested%2520structures%2520that%2520we%250Afrequently%2520encounter%2520in%2520Google%2520workloads.%2520We%2520highlight%2520the%2520limitations%2520of%250Aexisting%2520approaches%2520used%2520in%2520production%252C%2520particularly%2520their%2520inability%2520to%2520handle%250Alarge%2520and%2520complex%2520data%2520structures%252C%2520as%2520well%2520as%2520the%2520lack%2520of%2520semantically%2520coherent%250Atest%2520data%2520that%2520lead%2520to%2520limited%2520test%2520coverage.%2520We%2520demonstrate%2520that%2520by%2520leveraging%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520incorporating%2520strategic%2520pre-%2520and%250Apost-processing%2520steps%252C%2520we%2520can%2520generate%2520syntactically%2520correct%2520and%2520semantically%250Arelevant%2520high-fidelity%2520test%2520data%2520that%2520adheres%2520to%2520complex%2520structural%2520constraints%250Aand%2520maintains%2520semantic%2520integrity%2520to%2520the%2520SQL%2520test%2520targets%2520%2528queries/functions%2529.%250AThis%2520approach%2520supports%2520comprehensive%2520testing%2520of%2520complex%2520SQL%2520queries%2520involving%250Ajoins%252C%2520aggregations%252C%2520and%2520even%2520deeply%2520nested%2520subqueries%252C%2520ensuring%2520robust%250Aevaluation%2520of%2520SQL%2520code%2520generation%2520services%252C%2520like%2520NL2SQL%2520and%2520SQL%2520Code%2520Assistant.%250AOur%2520results%2520demonstrate%2520the%2520practical%2520utility%2520of%2520an%2520LLM%2520%2528%255Ctextit%257Bgemini%257D%2529%2520based%250Atest%2520data%2520generation%2520for%2520industrial%2520SQL%2520code%2520generation%2520services%2520where%250Agenerating%2520high-fidelity%2520test%2520data%2520is%2520essential%2520due%2520to%2520the%2520frequent%250Aunavailability%2520and%2520inaccessibility%2520of%2520production%2520datasets%2520for%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17203v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Google%20SQL%20Code%0A%20%20Generation%20Services&entry.906535625=Shivasankari%20Kannan%20and%20Yeounoh%20Chung%20and%20Amita%20Gondi%20and%20Tristan%20Swadell%20and%20Fatma%20Ozcan&entry.1292438233=%20%20The%20demand%20for%20high-fidelity%20test%20data%20is%20paramount%20in%20industrial%20settings%0Awhere%20access%20to%20production%20data%20is%20largely%20restricted.%20Traditional%20data%0Ageneration%20methods%20often%20fall%20short%2C%20struggling%20with%20low-fidelity%20and%20the%0Aability%20to%20model%20complex%20data%20structures%20and%20semantic%20relationships%20that%20are%0Acritical%20for%20testing%20complex%20SQL%20code%20generation%20services%20like%20Natural%20Language%0Ato%20SQL%20%28NL2SQL%29.%20In%20this%20paper%2C%20we%20address%20the%20critical%20need%20for%20generating%0Asyntactically%20correct%20and%20semantically%20relevant%20high-fidelity%20mock%20data%20for%0Acomplex%20data%20structures%20that%20includes%20columns%20with%20nested%20structures%20that%20we%0Afrequently%20encounter%20in%20Google%20workloads.%20We%20highlight%20the%20limitations%20of%0Aexisting%20approaches%20used%20in%20production%2C%20particularly%20their%20inability%20to%20handle%0Alarge%20and%20complex%20data%20structures%2C%20as%20well%20as%20the%20lack%20of%20semantically%20coherent%0Atest%20data%20that%20lead%20to%20limited%20test%20coverage.%20We%20demonstrate%20that%20by%20leveraging%0ALarge%20Language%20Models%20%28LLMs%29%20and%20incorporating%20strategic%20pre-%20and%0Apost-processing%20steps%2C%20we%20can%20generate%20syntactically%20correct%20and%20semantically%0Arelevant%20high-fidelity%20test%20data%20that%20adheres%20to%20complex%20structural%20constraints%0Aand%20maintains%20semantic%20integrity%20to%20the%20SQL%20test%20targets%20%28queries/functions%29.%0AThis%20approach%20supports%20comprehensive%20testing%20of%20complex%20SQL%20queries%20involving%0Ajoins%2C%20aggregations%2C%20and%20even%20deeply%20nested%20subqueries%2C%20ensuring%20robust%0Aevaluation%20of%20SQL%20code%20generation%20services%2C%20like%20NL2SQL%20and%20SQL%20Code%20Assistant.%0AOur%20results%20demonstrate%20the%20practical%20utility%20of%20an%20LLM%20%28%5Ctextit%7Bgemini%7D%29%20based%0Atest%20data%20generation%20for%20industrial%20SQL%20code%20generation%20services%20where%0Agenerating%20high-fidelity%20test%20data%20is%20essential%20due%20to%20the%20frequent%0Aunavailability%20and%20inaccessibility%20of%20production%20datasets%20for%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17203v3&entry.124074799=Read"},
{"title": "Glyph: Scaling Context Windows via Visual-Text Compression", "author": "Jiale Cheng and Yusen Liu and Xinyu Zhang and Yulin Fei and Wenyi Hong and Ruiliang Lyu and Weihan Wang and Zhe Su and Xiaotao Gu and Xiao Liu and Yushi Bai and Jie Tang and Hongning Wang and Minlie Huang", "abstract": "  Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.\n", "link": "http://arxiv.org/abs/2510.17800v2", "date": "2025-10-21", "relevancy": 2.2066, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression&body=Title%3A%20Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression%0AAuthor%3A%20Jiale%20Cheng%20and%20Yusen%20Liu%20and%20Xinyu%20Zhang%20and%20Yulin%20Fei%20and%20Wenyi%20Hong%20and%20Ruiliang%20Lyu%20and%20Weihan%20Wang%20and%20Zhe%20Su%20and%20Xiaotao%20Gu%20and%20Xiao%20Liu%20and%20Yushi%20Bai%20and%20Jie%20Tang%20and%20Hongning%20Wang%20and%20Minlie%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20increasingly%20rely%20on%20long-context%20modeling%20for%0Atasks%20such%20as%20document%20understanding%2C%20code%20analysis%2C%20and%20multi-step%20reasoning.%0AHowever%2C%20scaling%20context%20windows%20to%20the%20million-token%20level%20brings%20prohibitive%0Acomputational%20and%20memory%20costs%2C%20limiting%20the%20practicality%20of%20long-context%20LLMs.%0AIn%20this%20work%2C%20we%20take%20a%20different%20perspective-visual%20context%20scaling-to%20tackle%0Athis%20challenge.%20Instead%20of%20extending%20token-based%20sequences%2C%20we%20propose%20Glyph%2C%20a%0Aframework%20that%20renders%20long%20texts%20into%20images%20and%20processes%20them%20with%0Avision-language%20models%20%28VLMs%29.%20This%20approach%20substantially%20compresses%20textual%0Ainput%20while%20preserving%20semantic%20information%2C%20and%20we%20further%20design%20an%0ALLM-driven%20genetic%20search%20to%20identify%20optimal%20visual%20rendering%20configurations%0Afor%20balancing%20accuracy%20and%20compression.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20achieves%203-4x%20token%20compression%20while%20maintaining%0Aaccuracy%20comparable%20to%20leading%20LLMs%20such%20as%20Qwen3-8B%20on%20various%20long-context%0Abenchmarks.%20This%20compression%20also%20leads%20to%20around%204x%20faster%20prefilling%20and%0Adecoding%2C%20and%20approximately%202x%20faster%20SFT%20training.%20Furthermore%2C%20under%20extreme%0Acompression%2C%20a%20128K-context%20VLM%20could%20scale%20to%20handle%201M-token-level%20text%0Atasks.%20In%20addition%2C%20the%20rendered%20text%20data%20benefits%20real-world%20multimodal%0Atasks%2C%20such%20as%20document%20understanding.%20Our%20code%20and%20model%20are%20released%20at%0Ahttps%3A//github.com/thu-coai/Glyph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlyph%253A%2520Scaling%2520Context%2520Windows%2520via%2520Visual-Text%2520Compression%26entry.906535625%3DJiale%2520Cheng%2520and%2520Yusen%2520Liu%2520and%2520Xinyu%2520Zhang%2520and%2520Yulin%2520Fei%2520and%2520Wenyi%2520Hong%2520and%2520Ruiliang%2520Lyu%2520and%2520Weihan%2520Wang%2520and%2520Zhe%2520Su%2520and%2520Xiaotao%2520Gu%2520and%2520Xiao%2520Liu%2520and%2520Yushi%2520Bai%2520and%2520Jie%2520Tang%2520and%2520Hongning%2520Wang%2520and%2520Minlie%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520rely%2520on%2520long-context%2520modeling%2520for%250Atasks%2520such%2520as%2520document%2520understanding%252C%2520code%2520analysis%252C%2520and%2520multi-step%2520reasoning.%250AHowever%252C%2520scaling%2520context%2520windows%2520to%2520the%2520million-token%2520level%2520brings%2520prohibitive%250Acomputational%2520and%2520memory%2520costs%252C%2520limiting%2520the%2520practicality%2520of%2520long-context%2520LLMs.%250AIn%2520this%2520work%252C%2520we%2520take%2520a%2520different%2520perspective-visual%2520context%2520scaling-to%2520tackle%250Athis%2520challenge.%2520Instead%2520of%2520extending%2520token-based%2520sequences%252C%2520we%2520propose%2520Glyph%252C%2520a%250Aframework%2520that%2520renders%2520long%2520texts%2520into%2520images%2520and%2520processes%2520them%2520with%250Avision-language%2520models%2520%2528VLMs%2529.%2520This%2520approach%2520substantially%2520compresses%2520textual%250Ainput%2520while%2520preserving%2520semantic%2520information%252C%2520and%2520we%2520further%2520design%2520an%250ALLM-driven%2520genetic%2520search%2520to%2520identify%2520optimal%2520visual%2520rendering%2520configurations%250Afor%2520balancing%2520accuracy%2520and%2520compression.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520achieves%25203-4x%2520token%2520compression%2520while%2520maintaining%250Aaccuracy%2520comparable%2520to%2520leading%2520LLMs%2520such%2520as%2520Qwen3-8B%2520on%2520various%2520long-context%250Abenchmarks.%2520This%2520compression%2520also%2520leads%2520to%2520around%25204x%2520faster%2520prefilling%2520and%250Adecoding%252C%2520and%2520approximately%25202x%2520faster%2520SFT%2520training.%2520Furthermore%252C%2520under%2520extreme%250Acompression%252C%2520a%2520128K-context%2520VLM%2520could%2520scale%2520to%2520handle%25201M-token-level%2520text%250Atasks.%2520In%2520addition%252C%2520the%2520rendered%2520text%2520data%2520benefits%2520real-world%2520multimodal%250Atasks%252C%2520such%2520as%2520document%2520understanding.%2520Our%2520code%2520and%2520model%2520are%2520released%2520at%250Ahttps%253A//github.com/thu-coai/Glyph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression&entry.906535625=Jiale%20Cheng%20and%20Yusen%20Liu%20and%20Xinyu%20Zhang%20and%20Yulin%20Fei%20and%20Wenyi%20Hong%20and%20Ruiliang%20Lyu%20and%20Weihan%20Wang%20and%20Zhe%20Su%20and%20Xiaotao%20Gu%20and%20Xiao%20Liu%20and%20Yushi%20Bai%20and%20Jie%20Tang%20and%20Hongning%20Wang%20and%20Minlie%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20increasingly%20rely%20on%20long-context%20modeling%20for%0Atasks%20such%20as%20document%20understanding%2C%20code%20analysis%2C%20and%20multi-step%20reasoning.%0AHowever%2C%20scaling%20context%20windows%20to%20the%20million-token%20level%20brings%20prohibitive%0Acomputational%20and%20memory%20costs%2C%20limiting%20the%20practicality%20of%20long-context%20LLMs.%0AIn%20this%20work%2C%20we%20take%20a%20different%20perspective-visual%20context%20scaling-to%20tackle%0Athis%20challenge.%20Instead%20of%20extending%20token-based%20sequences%2C%20we%20propose%20Glyph%2C%20a%0Aframework%20that%20renders%20long%20texts%20into%20images%20and%20processes%20them%20with%0Avision-language%20models%20%28VLMs%29.%20This%20approach%20substantially%20compresses%20textual%0Ainput%20while%20preserving%20semantic%20information%2C%20and%20we%20further%20design%20an%0ALLM-driven%20genetic%20search%20to%20identify%20optimal%20visual%20rendering%20configurations%0Afor%20balancing%20accuracy%20and%20compression.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20achieves%203-4x%20token%20compression%20while%20maintaining%0Aaccuracy%20comparable%20to%20leading%20LLMs%20such%20as%20Qwen3-8B%20on%20various%20long-context%0Abenchmarks.%20This%20compression%20also%20leads%20to%20around%204x%20faster%20prefilling%20and%0Adecoding%2C%20and%20approximately%202x%20faster%20SFT%20training.%20Furthermore%2C%20under%20extreme%0Acompression%2C%20a%20128K-context%20VLM%20could%20scale%20to%20handle%201M-token-level%20text%0Atasks.%20In%20addition%2C%20the%20rendered%20text%20data%20benefits%20real-world%20multimodal%0Atasks%2C%20such%20as%20document%20understanding.%20Our%20code%20and%20model%20are%20released%20at%0Ahttps%3A//github.com/thu-coai/Glyph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17800v2&entry.124074799=Read"},
{"title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning", "author": "Li Kang and Xiufeng Song and Heng Zhou and Yiran Qin and Jie Yang and Xiaohong Liu and Philip Torr and Lei Bai and Zhenfei Yin", "abstract": "  Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.\n", "link": "http://arxiv.org/abs/2506.09049v2", "date": "2025-10-21", "relevancy": 2.2059, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIKI-R%3A%20Coordinating%20Embodied%20Multi-Agent%20Cooperation%20via%20Reinforcement%0A%20%20Learning&body=Title%3A%20VIKI-R%3A%20Coordinating%20Embodied%20Multi-Agent%20Cooperation%20via%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Li%20Kang%20and%20Xiufeng%20Song%20and%20Heng%20Zhou%20and%20Yiran%20Qin%20and%20Jie%20Yang%20and%20Xiaohong%20Liu%20and%20Philip%20Torr%20and%20Lei%20Bai%20and%20Zhenfei%20Yin%0AAbstract%3A%20%20%20Coordinating%20multiple%20embodied%20agents%20in%20dynamic%20environments%20remains%20a%20core%0Achallenge%20in%20artificial%20intelligence%2C%20requiring%20both%20perception-driven%0Areasoning%20and%20scalable%20cooperation%20strategies.%20While%20recent%20works%20have%0Aleveraged%20large%20language%20models%20%28LLMs%29%20for%20multi-agent%20planning%2C%20a%20few%20have%0Abegun%20to%20explore%20vision-language%20models%20%28VLMs%29%20for%20visual%20reasoning.%20However%2C%0Athese%20VLM-based%20approaches%20remain%20limited%20in%20their%20support%20for%20diverse%0Aembodiment%20types.%20In%20this%20work%2C%20we%20introduce%20VIKI-Bench%2C%20the%20first%20hierarchical%0Abenchmark%20tailored%20for%20embodied%20multi-agent%20cooperation%2C%20featuring%20three%0Astructured%20levels%3A%20agent%20activation%2C%20task%20planning%2C%20and%20trajectory%20perception.%0AVIKI-Bench%20includes%20diverse%20robot%20embodiments%2C%20multi-view%20visual%20observations%2C%0Aand%20structured%20supervision%20signals%20to%20evaluate%20reasoning%20grounded%20in%20visual%0Ainputs.%20To%20demonstrate%20the%20utility%20of%20VIKI-Bench%2C%20we%20propose%20VIKI-R%2C%20a%0Atwo-stage%20framework%20that%20fine-tunes%20a%20pretrained%20vision-language%20model%20%28VLM%29%0Ausing%20Chain-of-Thought%20annotated%20demonstrations%2C%20followed%20by%20reinforcement%0Alearning%20under%20multi-level%20reward%20signals.%20Our%20extensive%20experiments%20show%20that%0AVIKI-R%20significantly%20outperforms%20baselines%20method%20across%20all%20task%20levels.%0AFurthermore%2C%20we%20show%20that%20reinforcement%20learning%20enables%20the%20emergence%20of%0Acompositional%20cooperation%20patterns%20among%20heterogeneous%20agents.%20Together%2C%0AVIKI-Bench%20and%20VIKI-R%20offer%20a%20unified%20testbed%20and%20method%20for%20advancing%0Amulti-agent%2C%20visual-driven%20cooperation%20in%20embodied%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIKI-R%253A%2520Coordinating%2520Embodied%2520Multi-Agent%2520Cooperation%2520via%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DLi%2520Kang%2520and%2520Xiufeng%2520Song%2520and%2520Heng%2520Zhou%2520and%2520Yiran%2520Qin%2520and%2520Jie%2520Yang%2520and%2520Xiaohong%2520Liu%2520and%2520Philip%2520Torr%2520and%2520Lei%2520Bai%2520and%2520Zhenfei%2520Yin%26entry.1292438233%3D%2520%2520Coordinating%2520multiple%2520embodied%2520agents%2520in%2520dynamic%2520environments%2520remains%2520a%2520core%250Achallenge%2520in%2520artificial%2520intelligence%252C%2520requiring%2520both%2520perception-driven%250Areasoning%2520and%2520scalable%2520cooperation%2520strategies.%2520While%2520recent%2520works%2520have%250Aleveraged%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520multi-agent%2520planning%252C%2520a%2520few%2520have%250Abegun%2520to%2520explore%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520visual%2520reasoning.%2520However%252C%250Athese%2520VLM-based%2520approaches%2520remain%2520limited%2520in%2520their%2520support%2520for%2520diverse%250Aembodiment%2520types.%2520In%2520this%2520work%252C%2520we%2520introduce%2520VIKI-Bench%252C%2520the%2520first%2520hierarchical%250Abenchmark%2520tailored%2520for%2520embodied%2520multi-agent%2520cooperation%252C%2520featuring%2520three%250Astructured%2520levels%253A%2520agent%2520activation%252C%2520task%2520planning%252C%2520and%2520trajectory%2520perception.%250AVIKI-Bench%2520includes%2520diverse%2520robot%2520embodiments%252C%2520multi-view%2520visual%2520observations%252C%250Aand%2520structured%2520supervision%2520signals%2520to%2520evaluate%2520reasoning%2520grounded%2520in%2520visual%250Ainputs.%2520To%2520demonstrate%2520the%2520utility%2520of%2520VIKI-Bench%252C%2520we%2520propose%2520VIKI-R%252C%2520a%250Atwo-stage%2520framework%2520that%2520fine-tunes%2520a%2520pretrained%2520vision-language%2520model%2520%2528VLM%2529%250Ausing%2520Chain-of-Thought%2520annotated%2520demonstrations%252C%2520followed%2520by%2520reinforcement%250Alearning%2520under%2520multi-level%2520reward%2520signals.%2520Our%2520extensive%2520experiments%2520show%2520that%250AVIKI-R%2520significantly%2520outperforms%2520baselines%2520method%2520across%2520all%2520task%2520levels.%250AFurthermore%252C%2520we%2520show%2520that%2520reinforcement%2520learning%2520enables%2520the%2520emergence%2520of%250Acompositional%2520cooperation%2520patterns%2520among%2520heterogeneous%2520agents.%2520Together%252C%250AVIKI-Bench%2520and%2520VIKI-R%2520offer%2520a%2520unified%2520testbed%2520and%2520method%2520for%2520advancing%250Amulti-agent%252C%2520visual-driven%2520cooperation%2520in%2520embodied%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIKI-R%3A%20Coordinating%20Embodied%20Multi-Agent%20Cooperation%20via%20Reinforcement%0A%20%20Learning&entry.906535625=Li%20Kang%20and%20Xiufeng%20Song%20and%20Heng%20Zhou%20and%20Yiran%20Qin%20and%20Jie%20Yang%20and%20Xiaohong%20Liu%20and%20Philip%20Torr%20and%20Lei%20Bai%20and%20Zhenfei%20Yin&entry.1292438233=%20%20Coordinating%20multiple%20embodied%20agents%20in%20dynamic%20environments%20remains%20a%20core%0Achallenge%20in%20artificial%20intelligence%2C%20requiring%20both%20perception-driven%0Areasoning%20and%20scalable%20cooperation%20strategies.%20While%20recent%20works%20have%0Aleveraged%20large%20language%20models%20%28LLMs%29%20for%20multi-agent%20planning%2C%20a%20few%20have%0Abegun%20to%20explore%20vision-language%20models%20%28VLMs%29%20for%20visual%20reasoning.%20However%2C%0Athese%20VLM-based%20approaches%20remain%20limited%20in%20their%20support%20for%20diverse%0Aembodiment%20types.%20In%20this%20work%2C%20we%20introduce%20VIKI-Bench%2C%20the%20first%20hierarchical%0Abenchmark%20tailored%20for%20embodied%20multi-agent%20cooperation%2C%20featuring%20three%0Astructured%20levels%3A%20agent%20activation%2C%20task%20planning%2C%20and%20trajectory%20perception.%0AVIKI-Bench%20includes%20diverse%20robot%20embodiments%2C%20multi-view%20visual%20observations%2C%0Aand%20structured%20supervision%20signals%20to%20evaluate%20reasoning%20grounded%20in%20visual%0Ainputs.%20To%20demonstrate%20the%20utility%20of%20VIKI-Bench%2C%20we%20propose%20VIKI-R%2C%20a%0Atwo-stage%20framework%20that%20fine-tunes%20a%20pretrained%20vision-language%20model%20%28VLM%29%0Ausing%20Chain-of-Thought%20annotated%20demonstrations%2C%20followed%20by%20reinforcement%0Alearning%20under%20multi-level%20reward%20signals.%20Our%20extensive%20experiments%20show%20that%0AVIKI-R%20significantly%20outperforms%20baselines%20method%20across%20all%20task%20levels.%0AFurthermore%2C%20we%20show%20that%20reinforcement%20learning%20enables%20the%20emergence%20of%0Acompositional%20cooperation%20patterns%20among%20heterogeneous%20agents.%20Together%2C%0AVIKI-Bench%20and%20VIKI-R%20offer%20a%20unified%20testbed%20and%20method%20for%20advancing%0Amulti-agent%2C%20visual-driven%20cooperation%20in%20embodied%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09049v2&entry.124074799=Read"},
{"title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  Generation", "author": "Yibin Wang and Zhimin Li and Yuhang Zang and Jiazi Bu and Yujie Zhou and Yi Xin and Junjun He and Chunyu Wang and Qinglin Lu and Cheng Jin and Jiaqi Wang", "abstract": "  Recent progress in text-to-image (T2I) generation underscores the importance\nof reliable benchmarks in evaluating how accurately generated images reflect\nthe semantics of their textual prompt. However, (1) existing benchmarks lack\nthe diversity of prompt scenarios and multilingual support, both essential for\nreal-world applicability; (2) they offer only coarse evaluations across primary\ndimensions, covering a narrow range of sub-dimensions, and fall short in\nfine-grained sub-dimension assessment. To address these limitations, we\nintroduce UniGenBench++, a unified semantic assessment benchmark for T2I\ngeneration. Specifically, it comprises 600 prompts organized hierarchically to\nensure both coverage and efficiency: (1) spans across diverse real-world\nscenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively\nprobes T2I models' semantic consistency over 10 primary and 27 sub evaluation\ncriteria, with each prompt assessing multiple testpoints. To rigorously assess\nmodel robustness to variations in language and prompt length, we provide both\nEnglish and Chinese versions of each prompt in short and long forms. Leveraging\nthe general world knowledge and fine-grained image understanding capabilities\nof a closed-source Multi-modal Large Language Model (MLLM), i.e.,\nGemini-2.5-Pro, an effective pipeline is developed for reliable benchmark\nconstruction and streamlined model assessment. Moreover, to further facilitate\ncommunity use, we train a robust evaluation model that enables offline\nassessment of T2I model outputs. Through comprehensive benchmarking of both\nopen- and closed-sourced T2I models, we systematically reveal their strengths\nand weaknesses across various aspects.\n", "link": "http://arxiv.org/abs/2510.18701v1", "date": "2025-10-21", "relevancy": 2.1996, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5666}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5407}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGenBench%2B%2B%3A%20A%20Unified%20Semantic%20Evaluation%20Benchmark%20for%20Text-to-Image%0A%20%20Generation&body=Title%3A%20UniGenBench%2B%2B%3A%20A%20Unified%20Semantic%20Evaluation%20Benchmark%20for%20Text-to-Image%0A%20%20Generation%0AAuthor%3A%20Yibin%20Wang%20and%20Zhimin%20Li%20and%20Yuhang%20Zang%20and%20Jiazi%20Bu%20and%20Yujie%20Zhou%20and%20Yi%20Xin%20and%20Junjun%20He%20and%20Chunyu%20Wang%20and%20Qinglin%20Lu%20and%20Cheng%20Jin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Recent%20progress%20in%20text-to-image%20%28T2I%29%20generation%20underscores%20the%20importance%0Aof%20reliable%20benchmarks%20in%20evaluating%20how%20accurately%20generated%20images%20reflect%0Athe%20semantics%20of%20their%20textual%20prompt.%20However%2C%20%281%29%20existing%20benchmarks%20lack%0Athe%20diversity%20of%20prompt%20scenarios%20and%20multilingual%20support%2C%20both%20essential%20for%0Areal-world%20applicability%3B%20%282%29%20they%20offer%20only%20coarse%20evaluations%20across%20primary%0Adimensions%2C%20covering%20a%20narrow%20range%20of%20sub-dimensions%2C%20and%20fall%20short%20in%0Afine-grained%20sub-dimension%20assessment.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20UniGenBench%2B%2B%2C%20a%20unified%20semantic%20assessment%20benchmark%20for%20T2I%0Ageneration.%20Specifically%2C%20it%20comprises%20600%20prompts%20organized%20hierarchically%20to%0Aensure%20both%20coverage%20and%20efficiency%3A%20%281%29%20spans%20across%20diverse%20real-world%0Ascenarios%2C%20i.e.%2C%205%20main%20prompt%20themes%20and%2020%20subthemes%3B%20%282%29%20comprehensively%0Aprobes%20T2I%20models%27%20semantic%20consistency%20over%2010%20primary%20and%2027%20sub%20evaluation%0Acriteria%2C%20with%20each%20prompt%20assessing%20multiple%20testpoints.%20To%20rigorously%20assess%0Amodel%20robustness%20to%20variations%20in%20language%20and%20prompt%20length%2C%20we%20provide%20both%0AEnglish%20and%20Chinese%20versions%20of%20each%20prompt%20in%20short%20and%20long%20forms.%20Leveraging%0Athe%20general%20world%20knowledge%20and%20fine-grained%20image%20understanding%20capabilities%0Aof%20a%20closed-source%20Multi-modal%20Large%20Language%20Model%20%28MLLM%29%2C%20i.e.%2C%0AGemini-2.5-Pro%2C%20an%20effective%20pipeline%20is%20developed%20for%20reliable%20benchmark%0Aconstruction%20and%20streamlined%20model%20assessment.%20Moreover%2C%20to%20further%20facilitate%0Acommunity%20use%2C%20we%20train%20a%20robust%20evaluation%20model%20that%20enables%20offline%0Aassessment%20of%20T2I%20model%20outputs.%20Through%20comprehensive%20benchmarking%20of%20both%0Aopen-%20and%20closed-sourced%20T2I%20models%2C%20we%20systematically%20reveal%20their%20strengths%0Aand%20weaknesses%20across%20various%20aspects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGenBench%252B%252B%253A%2520A%2520Unified%2520Semantic%2520Evaluation%2520Benchmark%2520for%2520Text-to-Image%250A%2520%2520Generation%26entry.906535625%3DYibin%2520Wang%2520and%2520Zhimin%2520Li%2520and%2520Yuhang%2520Zang%2520and%2520Jiazi%2520Bu%2520and%2520Yujie%2520Zhou%2520and%2520Yi%2520Xin%2520and%2520Junjun%2520He%2520and%2520Chunyu%2520Wang%2520and%2520Qinglin%2520Lu%2520and%2520Cheng%2520Jin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520text-to-image%2520%2528T2I%2529%2520generation%2520underscores%2520the%2520importance%250Aof%2520reliable%2520benchmarks%2520in%2520evaluating%2520how%2520accurately%2520generated%2520images%2520reflect%250Athe%2520semantics%2520of%2520their%2520textual%2520prompt.%2520However%252C%2520%25281%2529%2520existing%2520benchmarks%2520lack%250Athe%2520diversity%2520of%2520prompt%2520scenarios%2520and%2520multilingual%2520support%252C%2520both%2520essential%2520for%250Areal-world%2520applicability%253B%2520%25282%2529%2520they%2520offer%2520only%2520coarse%2520evaluations%2520across%2520primary%250Adimensions%252C%2520covering%2520a%2520narrow%2520range%2520of%2520sub-dimensions%252C%2520and%2520fall%2520short%2520in%250Afine-grained%2520sub-dimension%2520assessment.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aintroduce%2520UniGenBench%252B%252B%252C%2520a%2520unified%2520semantic%2520assessment%2520benchmark%2520for%2520T2I%250Ageneration.%2520Specifically%252C%2520it%2520comprises%2520600%2520prompts%2520organized%2520hierarchically%2520to%250Aensure%2520both%2520coverage%2520and%2520efficiency%253A%2520%25281%2529%2520spans%2520across%2520diverse%2520real-world%250Ascenarios%252C%2520i.e.%252C%25205%2520main%2520prompt%2520themes%2520and%252020%2520subthemes%253B%2520%25282%2529%2520comprehensively%250Aprobes%2520T2I%2520models%2527%2520semantic%2520consistency%2520over%252010%2520primary%2520and%252027%2520sub%2520evaluation%250Acriteria%252C%2520with%2520each%2520prompt%2520assessing%2520multiple%2520testpoints.%2520To%2520rigorously%2520assess%250Amodel%2520robustness%2520to%2520variations%2520in%2520language%2520and%2520prompt%2520length%252C%2520we%2520provide%2520both%250AEnglish%2520and%2520Chinese%2520versions%2520of%2520each%2520prompt%2520in%2520short%2520and%2520long%2520forms.%2520Leveraging%250Athe%2520general%2520world%2520knowledge%2520and%2520fine-grained%2520image%2520understanding%2520capabilities%250Aof%2520a%2520closed-source%2520Multi-modal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%252C%2520i.e.%252C%250AGemini-2.5-Pro%252C%2520an%2520effective%2520pipeline%2520is%2520developed%2520for%2520reliable%2520benchmark%250Aconstruction%2520and%2520streamlined%2520model%2520assessment.%2520Moreover%252C%2520to%2520further%2520facilitate%250Acommunity%2520use%252C%2520we%2520train%2520a%2520robust%2520evaluation%2520model%2520that%2520enables%2520offline%250Aassessment%2520of%2520T2I%2520model%2520outputs.%2520Through%2520comprehensive%2520benchmarking%2520of%2520both%250Aopen-%2520and%2520closed-sourced%2520T2I%2520models%252C%2520we%2520systematically%2520reveal%2520their%2520strengths%250Aand%2520weaknesses%2520across%2520various%2520aspects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGenBench%2B%2B%3A%20A%20Unified%20Semantic%20Evaluation%20Benchmark%20for%20Text-to-Image%0A%20%20Generation&entry.906535625=Yibin%20Wang%20and%20Zhimin%20Li%20and%20Yuhang%20Zang%20and%20Jiazi%20Bu%20and%20Yujie%20Zhou%20and%20Yi%20Xin%20and%20Junjun%20He%20and%20Chunyu%20Wang%20and%20Qinglin%20Lu%20and%20Cheng%20Jin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Recent%20progress%20in%20text-to-image%20%28T2I%29%20generation%20underscores%20the%20importance%0Aof%20reliable%20benchmarks%20in%20evaluating%20how%20accurately%20generated%20images%20reflect%0Athe%20semantics%20of%20their%20textual%20prompt.%20However%2C%20%281%29%20existing%20benchmarks%20lack%0Athe%20diversity%20of%20prompt%20scenarios%20and%20multilingual%20support%2C%20both%20essential%20for%0Areal-world%20applicability%3B%20%282%29%20they%20offer%20only%20coarse%20evaluations%20across%20primary%0Adimensions%2C%20covering%20a%20narrow%20range%20of%20sub-dimensions%2C%20and%20fall%20short%20in%0Afine-grained%20sub-dimension%20assessment.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20UniGenBench%2B%2B%2C%20a%20unified%20semantic%20assessment%20benchmark%20for%20T2I%0Ageneration.%20Specifically%2C%20it%20comprises%20600%20prompts%20organized%20hierarchically%20to%0Aensure%20both%20coverage%20and%20efficiency%3A%20%281%29%20spans%20across%20diverse%20real-world%0Ascenarios%2C%20i.e.%2C%205%20main%20prompt%20themes%20and%2020%20subthemes%3B%20%282%29%20comprehensively%0Aprobes%20T2I%20models%27%20semantic%20consistency%20over%2010%20primary%20and%2027%20sub%20evaluation%0Acriteria%2C%20with%20each%20prompt%20assessing%20multiple%20testpoints.%20To%20rigorously%20assess%0Amodel%20robustness%20to%20variations%20in%20language%20and%20prompt%20length%2C%20we%20provide%20both%0AEnglish%20and%20Chinese%20versions%20of%20each%20prompt%20in%20short%20and%20long%20forms.%20Leveraging%0Athe%20general%20world%20knowledge%20and%20fine-grained%20image%20understanding%20capabilities%0Aof%20a%20closed-source%20Multi-modal%20Large%20Language%20Model%20%28MLLM%29%2C%20i.e.%2C%0AGemini-2.5-Pro%2C%20an%20effective%20pipeline%20is%20developed%20for%20reliable%20benchmark%0Aconstruction%20and%20streamlined%20model%20assessment.%20Moreover%2C%20to%20further%20facilitate%0Acommunity%20use%2C%20we%20train%20a%20robust%20evaluation%20model%20that%20enables%20offline%0Aassessment%20of%20T2I%20model%20outputs.%20Through%20comprehensive%20benchmarking%20of%20both%0Aopen-%20and%20closed-sourced%20T2I%20models%2C%20we%20systematically%20reveal%20their%20strengths%0Aand%20weaknesses%20across%20various%20aspects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18701v1&entry.124074799=Read"},
{"title": "Causally Perturbed Fairness Testing", "author": "Chengwen Du and Tao Chen", "abstract": "  To mitigate unfair and unethical discrimination over sensitive features\n(e.g., gender, age, or race), fairness testing plays an integral role in\nengineering systems that leverage AI models to handle tabular data. A key\nchallenge therein is how to effectively reveal fairness bugs under an\nintractable sample size using perturbation. Much current work has been focusing\non designing the test sample generators, ignoring the valuable knowledge about\ndata characteristics that can help guide the perturbation and hence limiting\ntheir full potential. In this paper, we seek to bridge such a gap by proposing\na generic framework of causally perturbed fairness testing, dubbed CausalFT.\nThrough causal inference, the key idea of CausalFT is to extract the most\ndirectly and causally relevant non-sensitive feature to its sensitive\ncounterpart, which can jointly influence the prediction of the label. Such a\ncausal relationship is then seamlessly injected into the perturbation to guide\na test sample generator. Unlike existing generator-level work, CausalFT serves\nas a higher-level framework that can be paired with diverse base generators.\nExtensive experiments on 1296 cases confirm that CausalFT can considerably\nimprove arbitrary base generators in revealing fairness bugs over 93% of the\ncases with acceptable extra runtime overhead. Compared with a state-of-the-art\napproach that ranks the non-sensitive features solely based on correlation,\nCausalFT performs significantly better on 64% cases while being much more\nefficient. Further, CausalFT can better improve bias resilience in nearly all\ncases.\n", "link": "http://arxiv.org/abs/2510.18719v1", "date": "2025-10-21", "relevancy": 2.1973, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4485}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4447}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causally%20Perturbed%20Fairness%20Testing&body=Title%3A%20Causally%20Perturbed%20Fairness%20Testing%0AAuthor%3A%20Chengwen%20Du%20and%20Tao%20Chen%0AAbstract%3A%20%20%20To%20mitigate%20unfair%20and%20unethical%20discrimination%20over%20sensitive%20features%0A%28e.g.%2C%20gender%2C%20age%2C%20or%20race%29%2C%20fairness%20testing%20plays%20an%20integral%20role%20in%0Aengineering%20systems%20that%20leverage%20AI%20models%20to%20handle%20tabular%20data.%20A%20key%0Achallenge%20therein%20is%20how%20to%20effectively%20reveal%20fairness%20bugs%20under%20an%0Aintractable%20sample%20size%20using%20perturbation.%20Much%20current%20work%20has%20been%20focusing%0Aon%20designing%20the%20test%20sample%20generators%2C%20ignoring%20the%20valuable%20knowledge%20about%0Adata%20characteristics%20that%20can%20help%20guide%20the%20perturbation%20and%20hence%20limiting%0Atheir%20full%20potential.%20In%20this%20paper%2C%20we%20seek%20to%20bridge%20such%20a%20gap%20by%20proposing%0Aa%20generic%20framework%20of%20causally%20perturbed%20fairness%20testing%2C%20dubbed%20CausalFT.%0AThrough%20causal%20inference%2C%20the%20key%20idea%20of%20CausalFT%20is%20to%20extract%20the%20most%0Adirectly%20and%20causally%20relevant%20non-sensitive%20feature%20to%20its%20sensitive%0Acounterpart%2C%20which%20can%20jointly%20influence%20the%20prediction%20of%20the%20label.%20Such%20a%0Acausal%20relationship%20is%20then%20seamlessly%20injected%20into%20the%20perturbation%20to%20guide%0Aa%20test%20sample%20generator.%20Unlike%20existing%20generator-level%20work%2C%20CausalFT%20serves%0Aas%20a%20higher-level%20framework%20that%20can%20be%20paired%20with%20diverse%20base%20generators.%0AExtensive%20experiments%20on%201296%20cases%20confirm%20that%20CausalFT%20can%20considerably%0Aimprove%20arbitrary%20base%20generators%20in%20revealing%20fairness%20bugs%20over%2093%25%20of%20the%0Acases%20with%20acceptable%20extra%20runtime%20overhead.%20Compared%20with%20a%20state-of-the-art%0Aapproach%20that%20ranks%20the%20non-sensitive%20features%20solely%20based%20on%20correlation%2C%0ACausalFT%20performs%20significantly%20better%20on%2064%25%20cases%20while%20being%20much%20more%0Aefficient.%20Further%2C%20CausalFT%20can%20better%20improve%20bias%20resilience%20in%20nearly%20all%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausally%2520Perturbed%2520Fairness%2520Testing%26entry.906535625%3DChengwen%2520Du%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520To%2520mitigate%2520unfair%2520and%2520unethical%2520discrimination%2520over%2520sensitive%2520features%250A%2528e.g.%252C%2520gender%252C%2520age%252C%2520or%2520race%2529%252C%2520fairness%2520testing%2520plays%2520an%2520integral%2520role%2520in%250Aengineering%2520systems%2520that%2520leverage%2520AI%2520models%2520to%2520handle%2520tabular%2520data.%2520A%2520key%250Achallenge%2520therein%2520is%2520how%2520to%2520effectively%2520reveal%2520fairness%2520bugs%2520under%2520an%250Aintractable%2520sample%2520size%2520using%2520perturbation.%2520Much%2520current%2520work%2520has%2520been%2520focusing%250Aon%2520designing%2520the%2520test%2520sample%2520generators%252C%2520ignoring%2520the%2520valuable%2520knowledge%2520about%250Adata%2520characteristics%2520that%2520can%2520help%2520guide%2520the%2520perturbation%2520and%2520hence%2520limiting%250Atheir%2520full%2520potential.%2520In%2520this%2520paper%252C%2520we%2520seek%2520to%2520bridge%2520such%2520a%2520gap%2520by%2520proposing%250Aa%2520generic%2520framework%2520of%2520causally%2520perturbed%2520fairness%2520testing%252C%2520dubbed%2520CausalFT.%250AThrough%2520causal%2520inference%252C%2520the%2520key%2520idea%2520of%2520CausalFT%2520is%2520to%2520extract%2520the%2520most%250Adirectly%2520and%2520causally%2520relevant%2520non-sensitive%2520feature%2520to%2520its%2520sensitive%250Acounterpart%252C%2520which%2520can%2520jointly%2520influence%2520the%2520prediction%2520of%2520the%2520label.%2520Such%2520a%250Acausal%2520relationship%2520is%2520then%2520seamlessly%2520injected%2520into%2520the%2520perturbation%2520to%2520guide%250Aa%2520test%2520sample%2520generator.%2520Unlike%2520existing%2520generator-level%2520work%252C%2520CausalFT%2520serves%250Aas%2520a%2520higher-level%2520framework%2520that%2520can%2520be%2520paired%2520with%2520diverse%2520base%2520generators.%250AExtensive%2520experiments%2520on%25201296%2520cases%2520confirm%2520that%2520CausalFT%2520can%2520considerably%250Aimprove%2520arbitrary%2520base%2520generators%2520in%2520revealing%2520fairness%2520bugs%2520over%252093%2525%2520of%2520the%250Acases%2520with%2520acceptable%2520extra%2520runtime%2520overhead.%2520Compared%2520with%2520a%2520state-of-the-art%250Aapproach%2520that%2520ranks%2520the%2520non-sensitive%2520features%2520solely%2520based%2520on%2520correlation%252C%250ACausalFT%2520performs%2520significantly%2520better%2520on%252064%2525%2520cases%2520while%2520being%2520much%2520more%250Aefficient.%2520Further%252C%2520CausalFT%2520can%2520better%2520improve%2520bias%2520resilience%2520in%2520nearly%2520all%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causally%20Perturbed%20Fairness%20Testing&entry.906535625=Chengwen%20Du%20and%20Tao%20Chen&entry.1292438233=%20%20To%20mitigate%20unfair%20and%20unethical%20discrimination%20over%20sensitive%20features%0A%28e.g.%2C%20gender%2C%20age%2C%20or%20race%29%2C%20fairness%20testing%20plays%20an%20integral%20role%20in%0Aengineering%20systems%20that%20leverage%20AI%20models%20to%20handle%20tabular%20data.%20A%20key%0Achallenge%20therein%20is%20how%20to%20effectively%20reveal%20fairness%20bugs%20under%20an%0Aintractable%20sample%20size%20using%20perturbation.%20Much%20current%20work%20has%20been%20focusing%0Aon%20designing%20the%20test%20sample%20generators%2C%20ignoring%20the%20valuable%20knowledge%20about%0Adata%20characteristics%20that%20can%20help%20guide%20the%20perturbation%20and%20hence%20limiting%0Atheir%20full%20potential.%20In%20this%20paper%2C%20we%20seek%20to%20bridge%20such%20a%20gap%20by%20proposing%0Aa%20generic%20framework%20of%20causally%20perturbed%20fairness%20testing%2C%20dubbed%20CausalFT.%0AThrough%20causal%20inference%2C%20the%20key%20idea%20of%20CausalFT%20is%20to%20extract%20the%20most%0Adirectly%20and%20causally%20relevant%20non-sensitive%20feature%20to%20its%20sensitive%0Acounterpart%2C%20which%20can%20jointly%20influence%20the%20prediction%20of%20the%20label.%20Such%20a%0Acausal%20relationship%20is%20then%20seamlessly%20injected%20into%20the%20perturbation%20to%20guide%0Aa%20test%20sample%20generator.%20Unlike%20existing%20generator-level%20work%2C%20CausalFT%20serves%0Aas%20a%20higher-level%20framework%20that%20can%20be%20paired%20with%20diverse%20base%20generators.%0AExtensive%20experiments%20on%201296%20cases%20confirm%20that%20CausalFT%20can%20considerably%0Aimprove%20arbitrary%20base%20generators%20in%20revealing%20fairness%20bugs%20over%2093%25%20of%20the%0Acases%20with%20acceptable%20extra%20runtime%20overhead.%20Compared%20with%20a%20state-of-the-art%0Aapproach%20that%20ranks%20the%20non-sensitive%20features%20solely%20based%20on%20correlation%2C%0ACausalFT%20performs%20significantly%20better%20on%2064%25%20cases%20while%20being%20much%20more%0Aefficient.%20Further%2C%20CausalFT%20can%20better%20improve%20bias%20resilience%20in%20nearly%20all%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18719v1&entry.124074799=Read"},
{"title": "How Do LLMs Use Their Depth?", "author": "Akshat Gupta and Jay Yeung and Gopala Anumanchipalli and Anna Ivanova", "abstract": "  Growing evidence suggests that large language models do not use their depth\nuniformly, yet we still lack a fine-grained understanding of their layer-wise\nprediction dynamics. In this paper, we trace the intermediate representations\nof several open-weight models during inference and reveal a structured and\nnuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework\nthat explains how LLMs internally structure their computations to make\npredictions. We first show that the top-ranked predictions in early LLM layers\nare composed primarily of high-frequency tokens, which act as statistical\nguesses proposed by the model early on due to the lack of appropriate\ncontextual information. As contextual information develops deeper into the\nmodel, these initial guesses get refined into contextually appropriate tokens.\nEven high-frequency token predictions from early layers get refined >70% of the\ntime, indicating that correct token prediction is not \"one-and-done\". We then\ngo beyond frequency-based prediction to examine the dynamic usage of layer\ndepth across three case studies. (i) Part-of-speech analysis shows that\nfunction words are, on average, the earliest to be predicted correctly. (ii)\nFact recall task analysis shows that, in a multi-token answer, the first token\nrequires more computational depth than the rest. (iii) Multiple-choice task\nanalysis shows that the model identifies the format of the response within the\nfirst half of the layers, but finalizes its response only toward the end.\nTogether, our results provide a detailed view of depth usage in LLMs, shedding\nlight on the layer-by-layer computations that underlie successful predictions\nand providing insights for future works to improve computational efficiency in\ntransformer-based models.\n", "link": "http://arxiv.org/abs/2510.18871v1", "date": "2025-10-21", "relevancy": 2.1957, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Do%20LLMs%20Use%20Their%20Depth%3F&body=Title%3A%20How%20Do%20LLMs%20Use%20Their%20Depth%3F%0AAuthor%3A%20Akshat%20Gupta%20and%20Jay%20Yeung%20and%20Gopala%20Anumanchipalli%20and%20Anna%20Ivanova%0AAbstract%3A%20%20%20Growing%20evidence%20suggests%20that%20large%20language%20models%20do%20not%20use%20their%20depth%0Auniformly%2C%20yet%20we%20still%20lack%20a%20fine-grained%20understanding%20of%20their%20layer-wise%0Aprediction%20dynamics.%20In%20this%20paper%2C%20we%20trace%20the%20intermediate%20representations%0Aof%20several%20open-weight%20models%20during%20inference%20and%20reveal%20a%20structured%20and%0Anuanced%20use%20of%20depth.%20Specifically%2C%20we%20propose%20a%20%22Guess-then-Refine%22%20framework%0Athat%20explains%20how%20LLMs%20internally%20structure%20their%20computations%20to%20make%0Apredictions.%20We%20first%20show%20that%20the%20top-ranked%20predictions%20in%20early%20LLM%20layers%0Aare%20composed%20primarily%20of%20high-frequency%20tokens%2C%20which%20act%20as%20statistical%0Aguesses%20proposed%20by%20the%20model%20early%20on%20due%20to%20the%20lack%20of%20appropriate%0Acontextual%20information.%20As%20contextual%20information%20develops%20deeper%20into%20the%0Amodel%2C%20these%20initial%20guesses%20get%20refined%20into%20contextually%20appropriate%20tokens.%0AEven%20high-frequency%20token%20predictions%20from%20early%20layers%20get%20refined%20%3E70%25%20of%20the%0Atime%2C%20indicating%20that%20correct%20token%20prediction%20is%20not%20%22one-and-done%22.%20We%20then%0Ago%20beyond%20frequency-based%20prediction%20to%20examine%20the%20dynamic%20usage%20of%20layer%0Adepth%20across%20three%20case%20studies.%20%28i%29%20Part-of-speech%20analysis%20shows%20that%0Afunction%20words%20are%2C%20on%20average%2C%20the%20earliest%20to%20be%20predicted%20correctly.%20%28ii%29%0AFact%20recall%20task%20analysis%20shows%20that%2C%20in%20a%20multi-token%20answer%2C%20the%20first%20token%0Arequires%20more%20computational%20depth%20than%20the%20rest.%20%28iii%29%20Multiple-choice%20task%0Aanalysis%20shows%20that%20the%20model%20identifies%20the%20format%20of%20the%20response%20within%20the%0Afirst%20half%20of%20the%20layers%2C%20but%20finalizes%20its%20response%20only%20toward%20the%20end.%0ATogether%2C%20our%20results%20provide%20a%20detailed%20view%20of%20depth%20usage%20in%20LLMs%2C%20shedding%0Alight%20on%20the%20layer-by-layer%20computations%20that%20underlie%20successful%20predictions%0Aand%20providing%20insights%20for%20future%20works%20to%20improve%20computational%20efficiency%20in%0Atransformer-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Do%2520LLMs%2520Use%2520Their%2520Depth%253F%26entry.906535625%3DAkshat%2520Gupta%2520and%2520Jay%2520Yeung%2520and%2520Gopala%2520Anumanchipalli%2520and%2520Anna%2520Ivanova%26entry.1292438233%3D%2520%2520Growing%2520evidence%2520suggests%2520that%2520large%2520language%2520models%2520do%2520not%2520use%2520their%2520depth%250Auniformly%252C%2520yet%2520we%2520still%2520lack%2520a%2520fine-grained%2520understanding%2520of%2520their%2520layer-wise%250Aprediction%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520trace%2520the%2520intermediate%2520representations%250Aof%2520several%2520open-weight%2520models%2520during%2520inference%2520and%2520reveal%2520a%2520structured%2520and%250Anuanced%2520use%2520of%2520depth.%2520Specifically%252C%2520we%2520propose%2520a%2520%2522Guess-then-Refine%2522%2520framework%250Athat%2520explains%2520how%2520LLMs%2520internally%2520structure%2520their%2520computations%2520to%2520make%250Apredictions.%2520We%2520first%2520show%2520that%2520the%2520top-ranked%2520predictions%2520in%2520early%2520LLM%2520layers%250Aare%2520composed%2520primarily%2520of%2520high-frequency%2520tokens%252C%2520which%2520act%2520as%2520statistical%250Aguesses%2520proposed%2520by%2520the%2520model%2520early%2520on%2520due%2520to%2520the%2520lack%2520of%2520appropriate%250Acontextual%2520information.%2520As%2520contextual%2520information%2520develops%2520deeper%2520into%2520the%250Amodel%252C%2520these%2520initial%2520guesses%2520get%2520refined%2520into%2520contextually%2520appropriate%2520tokens.%250AEven%2520high-frequency%2520token%2520predictions%2520from%2520early%2520layers%2520get%2520refined%2520%253E70%2525%2520of%2520the%250Atime%252C%2520indicating%2520that%2520correct%2520token%2520prediction%2520is%2520not%2520%2522one-and-done%2522.%2520We%2520then%250Ago%2520beyond%2520frequency-based%2520prediction%2520to%2520examine%2520the%2520dynamic%2520usage%2520of%2520layer%250Adepth%2520across%2520three%2520case%2520studies.%2520%2528i%2529%2520Part-of-speech%2520analysis%2520shows%2520that%250Afunction%2520words%2520are%252C%2520on%2520average%252C%2520the%2520earliest%2520to%2520be%2520predicted%2520correctly.%2520%2528ii%2529%250AFact%2520recall%2520task%2520analysis%2520shows%2520that%252C%2520in%2520a%2520multi-token%2520answer%252C%2520the%2520first%2520token%250Arequires%2520more%2520computational%2520depth%2520than%2520the%2520rest.%2520%2528iii%2529%2520Multiple-choice%2520task%250Aanalysis%2520shows%2520that%2520the%2520model%2520identifies%2520the%2520format%2520of%2520the%2520response%2520within%2520the%250Afirst%2520half%2520of%2520the%2520layers%252C%2520but%2520finalizes%2520its%2520response%2520only%2520toward%2520the%2520end.%250ATogether%252C%2520our%2520results%2520provide%2520a%2520detailed%2520view%2520of%2520depth%2520usage%2520in%2520LLMs%252C%2520shedding%250Alight%2520on%2520the%2520layer-by-layer%2520computations%2520that%2520underlie%2520successful%2520predictions%250Aand%2520providing%2520insights%2520for%2520future%2520works%2520to%2520improve%2520computational%2520efficiency%2520in%250Atransformer-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Do%20LLMs%20Use%20Their%20Depth%3F&entry.906535625=Akshat%20Gupta%20and%20Jay%20Yeung%20and%20Gopala%20Anumanchipalli%20and%20Anna%20Ivanova&entry.1292438233=%20%20Growing%20evidence%20suggests%20that%20large%20language%20models%20do%20not%20use%20their%20depth%0Auniformly%2C%20yet%20we%20still%20lack%20a%20fine-grained%20understanding%20of%20their%20layer-wise%0Aprediction%20dynamics.%20In%20this%20paper%2C%20we%20trace%20the%20intermediate%20representations%0Aof%20several%20open-weight%20models%20during%20inference%20and%20reveal%20a%20structured%20and%0Anuanced%20use%20of%20depth.%20Specifically%2C%20we%20propose%20a%20%22Guess-then-Refine%22%20framework%0Athat%20explains%20how%20LLMs%20internally%20structure%20their%20computations%20to%20make%0Apredictions.%20We%20first%20show%20that%20the%20top-ranked%20predictions%20in%20early%20LLM%20layers%0Aare%20composed%20primarily%20of%20high-frequency%20tokens%2C%20which%20act%20as%20statistical%0Aguesses%20proposed%20by%20the%20model%20early%20on%20due%20to%20the%20lack%20of%20appropriate%0Acontextual%20information.%20As%20contextual%20information%20develops%20deeper%20into%20the%0Amodel%2C%20these%20initial%20guesses%20get%20refined%20into%20contextually%20appropriate%20tokens.%0AEven%20high-frequency%20token%20predictions%20from%20early%20layers%20get%20refined%20%3E70%25%20of%20the%0Atime%2C%20indicating%20that%20correct%20token%20prediction%20is%20not%20%22one-and-done%22.%20We%20then%0Ago%20beyond%20frequency-based%20prediction%20to%20examine%20the%20dynamic%20usage%20of%20layer%0Adepth%20across%20three%20case%20studies.%20%28i%29%20Part-of-speech%20analysis%20shows%20that%0Afunction%20words%20are%2C%20on%20average%2C%20the%20earliest%20to%20be%20predicted%20correctly.%20%28ii%29%0AFact%20recall%20task%20analysis%20shows%20that%2C%20in%20a%20multi-token%20answer%2C%20the%20first%20token%0Arequires%20more%20computational%20depth%20than%20the%20rest.%20%28iii%29%20Multiple-choice%20task%0Aanalysis%20shows%20that%20the%20model%20identifies%20the%20format%20of%20the%20response%20within%20the%0Afirst%20half%20of%20the%20layers%2C%20but%20finalizes%20its%20response%20only%20toward%20the%20end.%0ATogether%2C%20our%20results%20provide%20a%20detailed%20view%20of%20depth%20usage%20in%20LLMs%2C%20shedding%0Alight%20on%20the%20layer-by-layer%20computations%20that%20underlie%20successful%20predictions%0Aand%20providing%20insights%20for%20future%20works%20to%20improve%20computational%20efficiency%20in%0Atransformer-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18871v1&entry.124074799=Read"},
{"title": "Rebellious Student: A Complementary Learning Framework for Background\n  Feature Enhancement in Hyperspectral Anomaly Detection", "author": "Wenping Jin and Yuyang Tang and Li Zhu and Fei Guo", "abstract": "  A recent class of hyperspectral anomaly detection methods that can be trained\nonce on background datasets and then universally deployed -- without per-scene\nretraining or parameter tuning -- has demonstrated remarkable efficiency and\nrobustness. Building upon this paradigm, we focus on the integration of\nspectral and spatial cues and introduce a novel \"Rebellious Student\" framework\nfor complementary feature learning. Unlike conventional teacher-student\nparadigms driven by imitation, our method intentionally trains the spatial\nbranch to diverge from the spectral teacher, thereby learning complementary\nspatial patterns that the teacher fails to capture. A two-stage learning\nstrategy is adopted: (1) a spectral enhancement network is first trained via\nreverse distillation to obtain robust background spectral representations; and\n(2) a spatial network -- the rebellious student -- is subsequently optimized\nusing decorrelation losses that enforce feature orthogonality while maintaining\nreconstruction fidelity to avoid irrelevant noise. Once trained, the framework\nenhances both spectral and spatial background features, enabling parameter-free\nand training-free anomaly detection when paired with conventional detectors.\nExtensive experiments on the HAD100 benchmark show substantial improvements\nover several established baselines with minimal computational overhead,\nconfirming the effectiveness and generality of the proposed complementary\nlearning paradigm. Our code is publicly available at\nhttps://github.com/xjpp2016/FERS.\n", "link": "http://arxiv.org/abs/2510.18781v1", "date": "2025-10-21", "relevancy": 2.184, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5755}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5305}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rebellious%20Student%3A%20A%20Complementary%20Learning%20Framework%20for%20Background%0A%20%20Feature%20Enhancement%20in%20Hyperspectral%20Anomaly%20Detection&body=Title%3A%20Rebellious%20Student%3A%20A%20Complementary%20Learning%20Framework%20for%20Background%0A%20%20Feature%20Enhancement%20in%20Hyperspectral%20Anomaly%20Detection%0AAuthor%3A%20Wenping%20Jin%20and%20Yuyang%20Tang%20and%20Li%20Zhu%20and%20Fei%20Guo%0AAbstract%3A%20%20%20A%20recent%20class%20of%20hyperspectral%20anomaly%20detection%20methods%20that%20can%20be%20trained%0Aonce%20on%20background%20datasets%20and%20then%20universally%20deployed%20--%20without%20per-scene%0Aretraining%20or%20parameter%20tuning%20--%20has%20demonstrated%20remarkable%20efficiency%20and%0Arobustness.%20Building%20upon%20this%20paradigm%2C%20we%20focus%20on%20the%20integration%20of%0Aspectral%20and%20spatial%20cues%20and%20introduce%20a%20novel%20%22Rebellious%20Student%22%20framework%0Afor%20complementary%20feature%20learning.%20Unlike%20conventional%20teacher-student%0Aparadigms%20driven%20by%20imitation%2C%20our%20method%20intentionally%20trains%20the%20spatial%0Abranch%20to%20diverge%20from%20the%20spectral%20teacher%2C%20thereby%20learning%20complementary%0Aspatial%20patterns%20that%20the%20teacher%20fails%20to%20capture.%20A%20two-stage%20learning%0Astrategy%20is%20adopted%3A%20%281%29%20a%20spectral%20enhancement%20network%20is%20first%20trained%20via%0Areverse%20distillation%20to%20obtain%20robust%20background%20spectral%20representations%3B%20and%0A%282%29%20a%20spatial%20network%20--%20the%20rebellious%20student%20--%20is%20subsequently%20optimized%0Ausing%20decorrelation%20losses%20that%20enforce%20feature%20orthogonality%20while%20maintaining%0Areconstruction%20fidelity%20to%20avoid%20irrelevant%20noise.%20Once%20trained%2C%20the%20framework%0Aenhances%20both%20spectral%20and%20spatial%20background%20features%2C%20enabling%20parameter-free%0Aand%20training-free%20anomaly%20detection%20when%20paired%20with%20conventional%20detectors.%0AExtensive%20experiments%20on%20the%20HAD100%20benchmark%20show%20substantial%20improvements%0Aover%20several%20established%20baselines%20with%20minimal%20computational%20overhead%2C%0Aconfirming%20the%20effectiveness%20and%20generality%20of%20the%20proposed%20complementary%0Alearning%20paradigm.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/xjpp2016/FERS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRebellious%2520Student%253A%2520A%2520Complementary%2520Learning%2520Framework%2520for%2520Background%250A%2520%2520Feature%2520Enhancement%2520in%2520Hyperspectral%2520Anomaly%2520Detection%26entry.906535625%3DWenping%2520Jin%2520and%2520Yuyang%2520Tang%2520and%2520Li%2520Zhu%2520and%2520Fei%2520Guo%26entry.1292438233%3D%2520%2520A%2520recent%2520class%2520of%2520hyperspectral%2520anomaly%2520detection%2520methods%2520that%2520can%2520be%2520trained%250Aonce%2520on%2520background%2520datasets%2520and%2520then%2520universally%2520deployed%2520--%2520without%2520per-scene%250Aretraining%2520or%2520parameter%2520tuning%2520--%2520has%2520demonstrated%2520remarkable%2520efficiency%2520and%250Arobustness.%2520Building%2520upon%2520this%2520paradigm%252C%2520we%2520focus%2520on%2520the%2520integration%2520of%250Aspectral%2520and%2520spatial%2520cues%2520and%2520introduce%2520a%2520novel%2520%2522Rebellious%2520Student%2522%2520framework%250Afor%2520complementary%2520feature%2520learning.%2520Unlike%2520conventional%2520teacher-student%250Aparadigms%2520driven%2520by%2520imitation%252C%2520our%2520method%2520intentionally%2520trains%2520the%2520spatial%250Abranch%2520to%2520diverge%2520from%2520the%2520spectral%2520teacher%252C%2520thereby%2520learning%2520complementary%250Aspatial%2520patterns%2520that%2520the%2520teacher%2520fails%2520to%2520capture.%2520A%2520two-stage%2520learning%250Astrategy%2520is%2520adopted%253A%2520%25281%2529%2520a%2520spectral%2520enhancement%2520network%2520is%2520first%2520trained%2520via%250Areverse%2520distillation%2520to%2520obtain%2520robust%2520background%2520spectral%2520representations%253B%2520and%250A%25282%2529%2520a%2520spatial%2520network%2520--%2520the%2520rebellious%2520student%2520--%2520is%2520subsequently%2520optimized%250Ausing%2520decorrelation%2520losses%2520that%2520enforce%2520feature%2520orthogonality%2520while%2520maintaining%250Areconstruction%2520fidelity%2520to%2520avoid%2520irrelevant%2520noise.%2520Once%2520trained%252C%2520the%2520framework%250Aenhances%2520both%2520spectral%2520and%2520spatial%2520background%2520features%252C%2520enabling%2520parameter-free%250Aand%2520training-free%2520anomaly%2520detection%2520when%2520paired%2520with%2520conventional%2520detectors.%250AExtensive%2520experiments%2520on%2520the%2520HAD100%2520benchmark%2520show%2520substantial%2520improvements%250Aover%2520several%2520established%2520baselines%2520with%2520minimal%2520computational%2520overhead%252C%250Aconfirming%2520the%2520effectiveness%2520and%2520generality%2520of%2520the%2520proposed%2520complementary%250Alearning%2520paradigm.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/xjpp2016/FERS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rebellious%20Student%3A%20A%20Complementary%20Learning%20Framework%20for%20Background%0A%20%20Feature%20Enhancement%20in%20Hyperspectral%20Anomaly%20Detection&entry.906535625=Wenping%20Jin%20and%20Yuyang%20Tang%20and%20Li%20Zhu%20and%20Fei%20Guo&entry.1292438233=%20%20A%20recent%20class%20of%20hyperspectral%20anomaly%20detection%20methods%20that%20can%20be%20trained%0Aonce%20on%20background%20datasets%20and%20then%20universally%20deployed%20--%20without%20per-scene%0Aretraining%20or%20parameter%20tuning%20--%20has%20demonstrated%20remarkable%20efficiency%20and%0Arobustness.%20Building%20upon%20this%20paradigm%2C%20we%20focus%20on%20the%20integration%20of%0Aspectral%20and%20spatial%20cues%20and%20introduce%20a%20novel%20%22Rebellious%20Student%22%20framework%0Afor%20complementary%20feature%20learning.%20Unlike%20conventional%20teacher-student%0Aparadigms%20driven%20by%20imitation%2C%20our%20method%20intentionally%20trains%20the%20spatial%0Abranch%20to%20diverge%20from%20the%20spectral%20teacher%2C%20thereby%20learning%20complementary%0Aspatial%20patterns%20that%20the%20teacher%20fails%20to%20capture.%20A%20two-stage%20learning%0Astrategy%20is%20adopted%3A%20%281%29%20a%20spectral%20enhancement%20network%20is%20first%20trained%20via%0Areverse%20distillation%20to%20obtain%20robust%20background%20spectral%20representations%3B%20and%0A%282%29%20a%20spatial%20network%20--%20the%20rebellious%20student%20--%20is%20subsequently%20optimized%0Ausing%20decorrelation%20losses%20that%20enforce%20feature%20orthogonality%20while%20maintaining%0Areconstruction%20fidelity%20to%20avoid%20irrelevant%20noise.%20Once%20trained%2C%20the%20framework%0Aenhances%20both%20spectral%20and%20spatial%20background%20features%2C%20enabling%20parameter-free%0Aand%20training-free%20anomaly%20detection%20when%20paired%20with%20conventional%20detectors.%0AExtensive%20experiments%20on%20the%20HAD100%20benchmark%20show%20substantial%20improvements%0Aover%20several%20established%20baselines%20with%20minimal%20computational%20overhead%2C%0Aconfirming%20the%20effectiveness%20and%20generality%20of%20the%20proposed%20complementary%0Alearning%20paradigm.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/xjpp2016/FERS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18781v1&entry.124074799=Read"},
{"title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation", "author": "Fr\u00e9d\u00e9ric LIN and Biruk Abere Ambaw and Adrian Popescu and Hejer Ammar and Romaric Audigier and Herv\u00e9 Le Borgne", "abstract": "  AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation.\n", "link": "http://arxiv.org/abs/2510.17626v2", "date": "2025-10-21", "relevancy": 2.1763, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5341}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaMiT%3A%20A%20Time-Aware%20Car%20Model%20Dataset%20for%20Classification%20and%20Generation&body=Title%3A%20CaMiT%3A%20A%20Time-Aware%20Car%20Model%20Dataset%20for%20Classification%20and%20Generation%0AAuthor%3A%20Fr%C3%A9d%C3%A9ric%20LIN%20and%20Biruk%20Abere%20Ambaw%20and%20Adrian%20Popescu%20and%20Hejer%20Ammar%20and%20Romaric%20Audigier%20and%20Herv%C3%A9%20Le%20Borgne%0AAbstract%3A%20%20%20AI%20systems%20must%20adapt%20to%20evolving%20visual%20environments%2C%20especially%20in%20domains%0Awhere%20object%20appearances%20change%20over%20time.%20We%20introduce%20Car%20Models%20in%20Time%0A%28CaMiT%29%2C%20a%20fine-grained%20dataset%20capturing%20the%20temporal%20evolution%20of%20car%20models%2C%0Aa%20representative%20class%20of%20technological%20artifacts.%20CaMiT%20includes%20787K%20labeled%0Asamples%20of%20190%20car%20models%20%282007-2023%29%20and%205.1M%20unlabeled%20samples%20%282005-2023%29%2C%0Asupporting%20both%20supervised%20and%20self-supervised%20learning.%20Static%20pretraining%20on%0Ain-domain%20data%20achieves%20competitive%20performance%20with%20large-scale%20generalist%0Amodels%20while%20being%20more%20resource-efficient%2C%20yet%20accuracy%20declines%20when%20models%0Aare%20tested%20across%20years.%20To%20address%20this%2C%20we%20propose%20a%20time-incremental%0Aclassification%20setting%2C%20a%20realistic%20continual%20learning%20scenario%20with%20emerging%2C%0Aevolving%2C%20and%20disappearing%20classes.%20We%20evaluate%20two%20strategies%3A%0Atime-incremental%20pretraining%2C%20which%20updates%20the%20backbone%2C%20and%20time-incremental%0Aclassifier%20learning%2C%20which%20updates%20only%20the%20final%20layer%2C%20both%20improving%0Atemporal%20robustness.%20Finally%2C%20we%20explore%20time-aware%20image%20generation%20that%0Aleverages%20temporal%20metadata%20during%20training%2C%20yielding%20more%20realistic%20outputs.%0ACaMiT%20offers%20a%20rich%20benchmark%20for%20studying%20temporal%20adaptation%20in%20fine-grained%0Avisual%20recognition%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaMiT%253A%2520A%2520Time-Aware%2520Car%2520Model%2520Dataset%2520for%2520Classification%2520and%2520Generation%26entry.906535625%3DFr%25C3%25A9d%25C3%25A9ric%2520LIN%2520and%2520Biruk%2520Abere%2520Ambaw%2520and%2520Adrian%2520Popescu%2520and%2520Hejer%2520Ammar%2520and%2520Romaric%2520Audigier%2520and%2520Herv%25C3%25A9%2520Le%2520Borgne%26entry.1292438233%3D%2520%2520AI%2520systems%2520must%2520adapt%2520to%2520evolving%2520visual%2520environments%252C%2520especially%2520in%2520domains%250Awhere%2520object%2520appearances%2520change%2520over%2520time.%2520We%2520introduce%2520Car%2520Models%2520in%2520Time%250A%2528CaMiT%2529%252C%2520a%2520fine-grained%2520dataset%2520capturing%2520the%2520temporal%2520evolution%2520of%2520car%2520models%252C%250Aa%2520representative%2520class%2520of%2520technological%2520artifacts.%2520CaMiT%2520includes%2520787K%2520labeled%250Asamples%2520of%2520190%2520car%2520models%2520%25282007-2023%2529%2520and%25205.1M%2520unlabeled%2520samples%2520%25282005-2023%2529%252C%250Asupporting%2520both%2520supervised%2520and%2520self-supervised%2520learning.%2520Static%2520pretraining%2520on%250Ain-domain%2520data%2520achieves%2520competitive%2520performance%2520with%2520large-scale%2520generalist%250Amodels%2520while%2520being%2520more%2520resource-efficient%252C%2520yet%2520accuracy%2520declines%2520when%2520models%250Aare%2520tested%2520across%2520years.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520time-incremental%250Aclassification%2520setting%252C%2520a%2520realistic%2520continual%2520learning%2520scenario%2520with%2520emerging%252C%250Aevolving%252C%2520and%2520disappearing%2520classes.%2520We%2520evaluate%2520two%2520strategies%253A%250Atime-incremental%2520pretraining%252C%2520which%2520updates%2520the%2520backbone%252C%2520and%2520time-incremental%250Aclassifier%2520learning%252C%2520which%2520updates%2520only%2520the%2520final%2520layer%252C%2520both%2520improving%250Atemporal%2520robustness.%2520Finally%252C%2520we%2520explore%2520time-aware%2520image%2520generation%2520that%250Aleverages%2520temporal%2520metadata%2520during%2520training%252C%2520yielding%2520more%2520realistic%2520outputs.%250ACaMiT%2520offers%2520a%2520rich%2520benchmark%2520for%2520studying%2520temporal%2520adaptation%2520in%2520fine-grained%250Avisual%2520recognition%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaMiT%3A%20A%20Time-Aware%20Car%20Model%20Dataset%20for%20Classification%20and%20Generation&entry.906535625=Fr%C3%A9d%C3%A9ric%20LIN%20and%20Biruk%20Abere%20Ambaw%20and%20Adrian%20Popescu%20and%20Hejer%20Ammar%20and%20Romaric%20Audigier%20and%20Herv%C3%A9%20Le%20Borgne&entry.1292438233=%20%20AI%20systems%20must%20adapt%20to%20evolving%20visual%20environments%2C%20especially%20in%20domains%0Awhere%20object%20appearances%20change%20over%20time.%20We%20introduce%20Car%20Models%20in%20Time%0A%28CaMiT%29%2C%20a%20fine-grained%20dataset%20capturing%20the%20temporal%20evolution%20of%20car%20models%2C%0Aa%20representative%20class%20of%20technological%20artifacts.%20CaMiT%20includes%20787K%20labeled%0Asamples%20of%20190%20car%20models%20%282007-2023%29%20and%205.1M%20unlabeled%20samples%20%282005-2023%29%2C%0Asupporting%20both%20supervised%20and%20self-supervised%20learning.%20Static%20pretraining%20on%0Ain-domain%20data%20achieves%20competitive%20performance%20with%20large-scale%20generalist%0Amodels%20while%20being%20more%20resource-efficient%2C%20yet%20accuracy%20declines%20when%20models%0Aare%20tested%20across%20years.%20To%20address%20this%2C%20we%20propose%20a%20time-incremental%0Aclassification%20setting%2C%20a%20realistic%20continual%20learning%20scenario%20with%20emerging%2C%0Aevolving%2C%20and%20disappearing%20classes.%20We%20evaluate%20two%20strategies%3A%0Atime-incremental%20pretraining%2C%20which%20updates%20the%20backbone%2C%20and%20time-incremental%0Aclassifier%20learning%2C%20which%20updates%20only%20the%20final%20layer%2C%20both%20improving%0Atemporal%20robustness.%20Finally%2C%20we%20explore%20time-aware%20image%20generation%20that%0Aleverages%20temporal%20metadata%20during%20training%2C%20yielding%20more%20realistic%20outputs.%0ACaMiT%20offers%20a%20rich%20benchmark%20for%20studying%20temporal%20adaptation%20in%20fine-grained%0Avisual%20recognition%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17626v2&entry.124074799=Read"},
{"title": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision", "author": "Hongliang Lu and Yuhang Wen and Pengyu Cheng and Ruijin Ding and Haotian Xu and Jiaqi Guo and Chutian Wang and Haonan Chen and Xiaoxi Jiang and Guanjun Jiang", "abstract": "  Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.\n", "link": "http://arxiv.org/abs/2510.18821v1", "date": "2025-10-21", "relevancy": 2.1758, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5724}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.565}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search%20Self-play%3A%20Pushing%20the%20Frontier%20of%20Agent%20Capability%20without%0A%20%20Supervision&body=Title%3A%20Search%20Self-play%3A%20Pushing%20the%20Frontier%20of%20Agent%20Capability%20without%0A%20%20Supervision%0AAuthor%3A%20Hongliang%20Lu%20and%20Yuhang%20Wen%20and%20Pengyu%20Cheng%20and%20Ruijin%20Ding%20and%20Haotian%20Xu%20and%20Jiaqi%20Guo%20and%20Chutian%20Wang%20and%20Haonan%20Chen%20and%20Xiaoxi%20Jiang%20and%20Guanjun%20Jiang%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20become%20the%0Amainstream%20technique%20for%20training%20LLM%20agents.%20However%2C%20RLVR%20highly%20depends%20on%0Awell-crafted%20task%20queries%20and%20corresponding%20ground-truth%20answers%20to%20provide%0Aaccurate%20rewards%2C%20which%20requires%20massive%20human%20efforts%20and%20hinders%20the%20RL%0Ascaling%20processes%2C%20especially%20under%20agentic%20scenarios.%20Although%20a%20few%20recent%0Aworks%20explore%20task%20synthesis%20methods%2C%20the%20difficulty%20of%20generated%20agentic%20tasks%0Acan%20hardly%20be%20controlled%20to%20provide%20effective%20RL%20training%20advantages.%20To%0Aachieve%20agentic%20RLVR%20with%20higher%20scalability%2C%20we%20explore%20self-play%20training%20for%0Adeep%20search%20agents%2C%20in%20which%20the%20learning%20LLM%20utilizes%20multi-turn%20search%20engine%0Acalling%20and%20acts%20simultaneously%20as%20both%20a%20task%20proposer%20and%20a%20problem%20solver.%0AThe%20task%20proposer%20aims%20to%20generate%20deep%20search%20queries%20with%20well-defined%0Aground-truth%20answers%20and%20increasing%20task%20difficulty.%20The%20problem%20solver%20tries%0Ato%20handle%20the%20generated%20search%20queries%20and%20output%20the%20correct%20answer%0Apredictions.%20To%20ensure%20that%20each%20generated%20search%20query%20has%20accurate%20ground%0Atruth%2C%20we%20collect%20all%20the%20searching%20results%20from%20the%20proposer%27s%20trajectory%20as%0Aexternal%20knowledge%2C%20then%20conduct%20retrieval-augmentation%20generation%20%28RAG%29%20to%0Atest%20whether%20the%20proposed%20query%20can%20be%20correctly%20answered%20with%20all%20necessary%0Asearch%20documents%20provided.%20In%20this%20search%20self-play%20%28SSP%29%20game%2C%20the%20proposer%0Aand%20the%20solver%20co-evolve%20their%20agent%20capabilities%20through%20both%20competition%20and%0Acooperation.%20With%20substantial%20experimental%20results%2C%20we%20find%20that%20SSP%20can%0Asignificantly%20improve%20search%20agents%27%20performance%20uniformly%20on%20various%0Abenchmarks%20without%20any%20supervision%20under%20both%20from-scratch%20and%20continuous%20RL%0Atraining%20setups.%20The%20code%20is%20at%20https%3A//github.com/Alibaba-Quark/SSP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch%2520Self-play%253A%2520Pushing%2520the%2520Frontier%2520of%2520Agent%2520Capability%2520without%250A%2520%2520Supervision%26entry.906535625%3DHongliang%2520Lu%2520and%2520Yuhang%2520Wen%2520and%2520Pengyu%2520Cheng%2520and%2520Ruijin%2520Ding%2520and%2520Haotian%2520Xu%2520and%2520Jiaqi%2520Guo%2520and%2520Chutian%2520Wang%2520and%2520Haonan%2520Chen%2520and%2520Xiaoxi%2520Jiang%2520and%2520Guanjun%2520Jiang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520has%2520become%2520the%250Amainstream%2520technique%2520for%2520training%2520LLM%2520agents.%2520However%252C%2520RLVR%2520highly%2520depends%2520on%250Awell-crafted%2520task%2520queries%2520and%2520corresponding%2520ground-truth%2520answers%2520to%2520provide%250Aaccurate%2520rewards%252C%2520which%2520requires%2520massive%2520human%2520efforts%2520and%2520hinders%2520the%2520RL%250Ascaling%2520processes%252C%2520especially%2520under%2520agentic%2520scenarios.%2520Although%2520a%2520few%2520recent%250Aworks%2520explore%2520task%2520synthesis%2520methods%252C%2520the%2520difficulty%2520of%2520generated%2520agentic%2520tasks%250Acan%2520hardly%2520be%2520controlled%2520to%2520provide%2520effective%2520RL%2520training%2520advantages.%2520To%250Aachieve%2520agentic%2520RLVR%2520with%2520higher%2520scalability%252C%2520we%2520explore%2520self-play%2520training%2520for%250Adeep%2520search%2520agents%252C%2520in%2520which%2520the%2520learning%2520LLM%2520utilizes%2520multi-turn%2520search%2520engine%250Acalling%2520and%2520acts%2520simultaneously%2520as%2520both%2520a%2520task%2520proposer%2520and%2520a%2520problem%2520solver.%250AThe%2520task%2520proposer%2520aims%2520to%2520generate%2520deep%2520search%2520queries%2520with%2520well-defined%250Aground-truth%2520answers%2520and%2520increasing%2520task%2520difficulty.%2520The%2520problem%2520solver%2520tries%250Ato%2520handle%2520the%2520generated%2520search%2520queries%2520and%2520output%2520the%2520correct%2520answer%250Apredictions.%2520To%2520ensure%2520that%2520each%2520generated%2520search%2520query%2520has%2520accurate%2520ground%250Atruth%252C%2520we%2520collect%2520all%2520the%2520searching%2520results%2520from%2520the%2520proposer%2527s%2520trajectory%2520as%250Aexternal%2520knowledge%252C%2520then%2520conduct%2520retrieval-augmentation%2520generation%2520%2528RAG%2529%2520to%250Atest%2520whether%2520the%2520proposed%2520query%2520can%2520be%2520correctly%2520answered%2520with%2520all%2520necessary%250Asearch%2520documents%2520provided.%2520In%2520this%2520search%2520self-play%2520%2528SSP%2529%2520game%252C%2520the%2520proposer%250Aand%2520the%2520solver%2520co-evolve%2520their%2520agent%2520capabilities%2520through%2520both%2520competition%2520and%250Acooperation.%2520With%2520substantial%2520experimental%2520results%252C%2520we%2520find%2520that%2520SSP%2520can%250Asignificantly%2520improve%2520search%2520agents%2527%2520performance%2520uniformly%2520on%2520various%250Abenchmarks%2520without%2520any%2520supervision%2520under%2520both%2520from-scratch%2520and%2520continuous%2520RL%250Atraining%2520setups.%2520The%2520code%2520is%2520at%2520https%253A//github.com/Alibaba-Quark/SSP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search%20Self-play%3A%20Pushing%20the%20Frontier%20of%20Agent%20Capability%20without%0A%20%20Supervision&entry.906535625=Hongliang%20Lu%20and%20Yuhang%20Wen%20and%20Pengyu%20Cheng%20and%20Ruijin%20Ding%20and%20Haotian%20Xu%20and%20Jiaqi%20Guo%20and%20Chutian%20Wang%20and%20Haonan%20Chen%20and%20Xiaoxi%20Jiang%20and%20Guanjun%20Jiang&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20become%20the%0Amainstream%20technique%20for%20training%20LLM%20agents.%20However%2C%20RLVR%20highly%20depends%20on%0Awell-crafted%20task%20queries%20and%20corresponding%20ground-truth%20answers%20to%20provide%0Aaccurate%20rewards%2C%20which%20requires%20massive%20human%20efforts%20and%20hinders%20the%20RL%0Ascaling%20processes%2C%20especially%20under%20agentic%20scenarios.%20Although%20a%20few%20recent%0Aworks%20explore%20task%20synthesis%20methods%2C%20the%20difficulty%20of%20generated%20agentic%20tasks%0Acan%20hardly%20be%20controlled%20to%20provide%20effective%20RL%20training%20advantages.%20To%0Aachieve%20agentic%20RLVR%20with%20higher%20scalability%2C%20we%20explore%20self-play%20training%20for%0Adeep%20search%20agents%2C%20in%20which%20the%20learning%20LLM%20utilizes%20multi-turn%20search%20engine%0Acalling%20and%20acts%20simultaneously%20as%20both%20a%20task%20proposer%20and%20a%20problem%20solver.%0AThe%20task%20proposer%20aims%20to%20generate%20deep%20search%20queries%20with%20well-defined%0Aground-truth%20answers%20and%20increasing%20task%20difficulty.%20The%20problem%20solver%20tries%0Ato%20handle%20the%20generated%20search%20queries%20and%20output%20the%20correct%20answer%0Apredictions.%20To%20ensure%20that%20each%20generated%20search%20query%20has%20accurate%20ground%0Atruth%2C%20we%20collect%20all%20the%20searching%20results%20from%20the%20proposer%27s%20trajectory%20as%0Aexternal%20knowledge%2C%20then%20conduct%20retrieval-augmentation%20generation%20%28RAG%29%20to%0Atest%20whether%20the%20proposed%20query%20can%20be%20correctly%20answered%20with%20all%20necessary%0Asearch%20documents%20provided.%20In%20this%20search%20self-play%20%28SSP%29%20game%2C%20the%20proposer%0Aand%20the%20solver%20co-evolve%20their%20agent%20capabilities%20through%20both%20competition%20and%0Acooperation.%20With%20substantial%20experimental%20results%2C%20we%20find%20that%20SSP%20can%0Asignificantly%20improve%20search%20agents%27%20performance%20uniformly%20on%20various%0Abenchmarks%20without%20any%20supervision%20under%20both%20from-scratch%20and%20continuous%20RL%0Atraining%20setups.%20The%20code%20is%20at%20https%3A//github.com/Alibaba-Quark/SSP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18821v1&entry.124074799=Read"},
{"title": "Increasing the Utility of Synthetic Images through Chamfer Guidance", "author": "Nicola Dall'Asen and Xiaofeng Zhang and Reyhane Askari Hemmat and Melissa Hall and Jakob Verbeek and Adriana Romero-Soriano and Michal Drozdzal", "abstract": "  Conditional image generative models hold considerable promise to produce\ninfinite amounts of synthetic training data. Yet, recent progress in generation\nquality has come at the expense of generation diversity, limiting the utility\nof these models as a source of synthetic training data. Although guidance-based\napproaches have been introduced to improve the utility of generated data by\nfocusing on quality or diversity, the (implicit or explicit) utility functions\noftentimes disregard the potential distribution shift between synthetic and\nreal data. In this work, we introduce Chamfer Guidance: a training-free\nguidance approach which leverages a handful of real exemplar images to\ncharacterize the quality and diversity of synthetic data. We show that by\nleveraging the proposed Chamfer Guidance, we can boost the diversity of the\ngenerations w.r.t. a dataset of real images while maintaining or improving the\ngeneration quality on ImageNet-1k and standard geo-diversity benchmarks. Our\napproach achieves state-of-the-art few-shot performance with as little as 2\nexemplar real images, obtaining 96.4% in terms of precision, and 86.4% in terms\nof distributional coverage, which increase to 97.5% and 92.7%, respectively,\nwhen using 32 real images. We showcase the benefits of the Chamfer Guidance\ngeneration by training downstream image classifiers on synthetic data,\nachieving accuracy boost of up to 15% for in-distribution over the baselines,\nand up to 16% in out-of-distribution. Furthermore, our approach does not\nrequire using the unconditional model, and thus obtains a 31% reduction in\nFLOPs w.r.t. classifier-free-guidance-based approaches at sampling time.\n", "link": "http://arxiv.org/abs/2508.10631v2", "date": "2025-10-21", "relevancy": 2.1664, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5797}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5389}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance&body=Title%3A%20Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance%0AAuthor%3A%20Nicola%20Dall%27Asen%20and%20Xiaofeng%20Zhang%20and%20Reyhane%20Askari%20Hemmat%20and%20Melissa%20Hall%20and%20Jakob%20Verbeek%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal%0AAbstract%3A%20%20%20Conditional%20image%20generative%20models%20hold%20considerable%20promise%20to%20produce%0Ainfinite%20amounts%20of%20synthetic%20training%20data.%20Yet%2C%20recent%20progress%20in%20generation%0Aquality%20has%20come%20at%20the%20expense%20of%20generation%20diversity%2C%20limiting%20the%20utility%0Aof%20these%20models%20as%20a%20source%20of%20synthetic%20training%20data.%20Although%20guidance-based%0Aapproaches%20have%20been%20introduced%20to%20improve%20the%20utility%20of%20generated%20data%20by%0Afocusing%20on%20quality%20or%20diversity%2C%20the%20%28implicit%20or%20explicit%29%20utility%20functions%0Aoftentimes%20disregard%20the%20potential%20distribution%20shift%20between%20synthetic%20and%0Areal%20data.%20In%20this%20work%2C%20we%20introduce%20Chamfer%20Guidance%3A%20a%20training-free%0Aguidance%20approach%20which%20leverages%20a%20handful%20of%20real%20exemplar%20images%20to%0Acharacterize%20the%20quality%20and%20diversity%20of%20synthetic%20data.%20We%20show%20that%20by%0Aleveraging%20the%20proposed%20Chamfer%20Guidance%2C%20we%20can%20boost%20the%20diversity%20of%20the%0Agenerations%20w.r.t.%20a%20dataset%20of%20real%20images%20while%20maintaining%20or%20improving%20the%0Ageneration%20quality%20on%20ImageNet-1k%20and%20standard%20geo-diversity%20benchmarks.%20Our%0Aapproach%20achieves%20state-of-the-art%20few-shot%20performance%20with%20as%20little%20as%202%0Aexemplar%20real%20images%2C%20obtaining%2096.4%25%20in%20terms%20of%20precision%2C%20and%2086.4%25%20in%20terms%0Aof%20distributional%20coverage%2C%20which%20increase%20to%2097.5%25%20and%2092.7%25%2C%20respectively%2C%0Awhen%20using%2032%20real%20images.%20We%20showcase%20the%20benefits%20of%20the%20Chamfer%20Guidance%0Ageneration%20by%20training%20downstream%20image%20classifiers%20on%20synthetic%20data%2C%0Aachieving%20accuracy%20boost%20of%20up%20to%2015%25%20for%20in-distribution%20over%20the%20baselines%2C%0Aand%20up%20to%2016%25%20in%20out-of-distribution.%20Furthermore%2C%20our%20approach%20does%20not%0Arequire%20using%20the%20unconditional%20model%2C%20and%20thus%20obtains%20a%2031%25%20reduction%20in%0AFLOPs%20w.r.t.%20classifier-free-guidance-based%20approaches%20at%20sampling%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncreasing%2520the%2520Utility%2520of%2520Synthetic%2520Images%2520through%2520Chamfer%2520Guidance%26entry.906535625%3DNicola%2520Dall%2527Asen%2520and%2520Xiaofeng%2520Zhang%2520and%2520Reyhane%2520Askari%2520Hemmat%2520and%2520Melissa%2520Hall%2520and%2520Jakob%2520Verbeek%2520and%2520Adriana%2520Romero-Soriano%2520and%2520Michal%2520Drozdzal%26entry.1292438233%3D%2520%2520Conditional%2520image%2520generative%2520models%2520hold%2520considerable%2520promise%2520to%2520produce%250Ainfinite%2520amounts%2520of%2520synthetic%2520training%2520data.%2520Yet%252C%2520recent%2520progress%2520in%2520generation%250Aquality%2520has%2520come%2520at%2520the%2520expense%2520of%2520generation%2520diversity%252C%2520limiting%2520the%2520utility%250Aof%2520these%2520models%2520as%2520a%2520source%2520of%2520synthetic%2520training%2520data.%2520Although%2520guidance-based%250Aapproaches%2520have%2520been%2520introduced%2520to%2520improve%2520the%2520utility%2520of%2520generated%2520data%2520by%250Afocusing%2520on%2520quality%2520or%2520diversity%252C%2520the%2520%2528implicit%2520or%2520explicit%2529%2520utility%2520functions%250Aoftentimes%2520disregard%2520the%2520potential%2520distribution%2520shift%2520between%2520synthetic%2520and%250Areal%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Chamfer%2520Guidance%253A%2520a%2520training-free%250Aguidance%2520approach%2520which%2520leverages%2520a%2520handful%2520of%2520real%2520exemplar%2520images%2520to%250Acharacterize%2520the%2520quality%2520and%2520diversity%2520of%2520synthetic%2520data.%2520We%2520show%2520that%2520by%250Aleveraging%2520the%2520proposed%2520Chamfer%2520Guidance%252C%2520we%2520can%2520boost%2520the%2520diversity%2520of%2520the%250Agenerations%2520w.r.t.%2520a%2520dataset%2520of%2520real%2520images%2520while%2520maintaining%2520or%2520improving%2520the%250Ageneration%2520quality%2520on%2520ImageNet-1k%2520and%2520standard%2520geo-diversity%2520benchmarks.%2520Our%250Aapproach%2520achieves%2520state-of-the-art%2520few-shot%2520performance%2520with%2520as%2520little%2520as%25202%250Aexemplar%2520real%2520images%252C%2520obtaining%252096.4%2525%2520in%2520terms%2520of%2520precision%252C%2520and%252086.4%2525%2520in%2520terms%250Aof%2520distributional%2520coverage%252C%2520which%2520increase%2520to%252097.5%2525%2520and%252092.7%2525%252C%2520respectively%252C%250Awhen%2520using%252032%2520real%2520images.%2520We%2520showcase%2520the%2520benefits%2520of%2520the%2520Chamfer%2520Guidance%250Ageneration%2520by%2520training%2520downstream%2520image%2520classifiers%2520on%2520synthetic%2520data%252C%250Aachieving%2520accuracy%2520boost%2520of%2520up%2520to%252015%2525%2520for%2520in-distribution%2520over%2520the%2520baselines%252C%250Aand%2520up%2520to%252016%2525%2520in%2520out-of-distribution.%2520Furthermore%252C%2520our%2520approach%2520does%2520not%250Arequire%2520using%2520the%2520unconditional%2520model%252C%2520and%2520thus%2520obtains%2520a%252031%2525%2520reduction%2520in%250AFLOPs%2520w.r.t.%2520classifier-free-guidance-based%2520approaches%2520at%2520sampling%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Increasing%20the%20Utility%20of%20Synthetic%20Images%20through%20Chamfer%20Guidance&entry.906535625=Nicola%20Dall%27Asen%20and%20Xiaofeng%20Zhang%20and%20Reyhane%20Askari%20Hemmat%20and%20Melissa%20Hall%20and%20Jakob%20Verbeek%20and%20Adriana%20Romero-Soriano%20and%20Michal%20Drozdzal&entry.1292438233=%20%20Conditional%20image%20generative%20models%20hold%20considerable%20promise%20to%20produce%0Ainfinite%20amounts%20of%20synthetic%20training%20data.%20Yet%2C%20recent%20progress%20in%20generation%0Aquality%20has%20come%20at%20the%20expense%20of%20generation%20diversity%2C%20limiting%20the%20utility%0Aof%20these%20models%20as%20a%20source%20of%20synthetic%20training%20data.%20Although%20guidance-based%0Aapproaches%20have%20been%20introduced%20to%20improve%20the%20utility%20of%20generated%20data%20by%0Afocusing%20on%20quality%20or%20diversity%2C%20the%20%28implicit%20or%20explicit%29%20utility%20functions%0Aoftentimes%20disregard%20the%20potential%20distribution%20shift%20between%20synthetic%20and%0Areal%20data.%20In%20this%20work%2C%20we%20introduce%20Chamfer%20Guidance%3A%20a%20training-free%0Aguidance%20approach%20which%20leverages%20a%20handful%20of%20real%20exemplar%20images%20to%0Acharacterize%20the%20quality%20and%20diversity%20of%20synthetic%20data.%20We%20show%20that%20by%0Aleveraging%20the%20proposed%20Chamfer%20Guidance%2C%20we%20can%20boost%20the%20diversity%20of%20the%0Agenerations%20w.r.t.%20a%20dataset%20of%20real%20images%20while%20maintaining%20or%20improving%20the%0Ageneration%20quality%20on%20ImageNet-1k%20and%20standard%20geo-diversity%20benchmarks.%20Our%0Aapproach%20achieves%20state-of-the-art%20few-shot%20performance%20with%20as%20little%20as%202%0Aexemplar%20real%20images%2C%20obtaining%2096.4%25%20in%20terms%20of%20precision%2C%20and%2086.4%25%20in%20terms%0Aof%20distributional%20coverage%2C%20which%20increase%20to%2097.5%25%20and%2092.7%25%2C%20respectively%2C%0Awhen%20using%2032%20real%20images.%20We%20showcase%20the%20benefits%20of%20the%20Chamfer%20Guidance%0Ageneration%20by%20training%20downstream%20image%20classifiers%20on%20synthetic%20data%2C%0Aachieving%20accuracy%20boost%20of%20up%20to%2015%25%20for%20in-distribution%20over%20the%20baselines%2C%0Aand%20up%20to%2016%25%20in%20out-of-distribution.%20Furthermore%2C%20our%20approach%20does%20not%0Arequire%20using%20the%20unconditional%20model%2C%20and%20thus%20obtains%20a%2031%25%20reduction%20in%0AFLOPs%20w.r.t.%20classifier-free-guidance-based%20approaches%20at%20sampling%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10631v2&entry.124074799=Read"},
{"title": "Counterfactual reasoning: an analysis of in-context emergence", "author": "Moritz Miller and Bernhard Sch\u00f6lkopf and Siyuan Guo", "abstract": "  Large-scale neural language models exhibit remarkable performance in\nin-context learning: the ability to learn and reason about the input context on\nthe fly. This work studies in-context counterfactual reasoning in language\nmodels, that is, the ability to predict consequences of a hypothetical\nscenario. We focus on a well-defined, synthetic linear regression task that\nrequires noise abduction. Accurate prediction is based on (1) inferring an\nunobserved latent concept and (2) copying contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning. Further, we enhance existing identifiability results and reduce\ncounterfactual reasoning for a broad class of functions to a transformation on\nin-context observations. In Transformers, we find that self-attention, model\ndepth and pre-training data diversity drive performance. Moreover, we provide\nmechanistic evidence that the latent concept is linearly represented in the\nresidual stream and we introduce designated \\textit{noise abduction heads}\ncentral to performing counterfactual reasoning. Lastly, our findings extend to\ncounterfactual reasoning under SDE dynamics and reflect that Transformers can\nperform noise abduction on sequential data, providing preliminary evidence on\nthe potential for counterfactual story generation. Our code is available under\nhttps://github.com/mrtzmllr/iccr.\n", "link": "http://arxiv.org/abs/2506.05188v2", "date": "2025-10-21", "relevancy": 2.1648, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20reasoning%3A%20an%20analysis%20of%20in-context%20emergence&body=Title%3A%20Counterfactual%20reasoning%3A%20an%20analysis%20of%20in-context%20emergence%0AAuthor%3A%20Moritz%20Miller%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Siyuan%20Guo%0AAbstract%3A%20%20%20Large-scale%20neural%20language%20models%20exhibit%20remarkable%20performance%20in%0Ain-context%20learning%3A%20the%20ability%20to%20learn%20and%20reason%20about%20the%20input%20context%20on%0Athe%20fly.%20This%20work%20studies%20in-context%20counterfactual%20reasoning%20in%20language%0Amodels%2C%20that%20is%2C%20the%20ability%20to%20predict%20consequences%20of%20a%20hypothetical%0Ascenario.%20We%20focus%20on%20a%20well-defined%2C%20synthetic%20linear%20regression%20task%20that%0Arequires%20noise%20abduction.%20Accurate%20prediction%20is%20based%20on%20%281%29%20inferring%20an%0Aunobserved%20latent%20concept%20and%20%282%29%20copying%20contextual%20noise%20from%20factual%0Aobservations.%20We%20show%20that%20language%20models%20are%20capable%20of%20counterfactual%0Areasoning.%20Further%2C%20we%20enhance%20existing%20identifiability%20results%20and%20reduce%0Acounterfactual%20reasoning%20for%20a%20broad%20class%20of%20functions%20to%20a%20transformation%20on%0Ain-context%20observations.%20In%20Transformers%2C%20we%20find%20that%20self-attention%2C%20model%0Adepth%20and%20pre-training%20data%20diversity%20drive%20performance.%20Moreover%2C%20we%20provide%0Amechanistic%20evidence%20that%20the%20latent%20concept%20is%20linearly%20represented%20in%20the%0Aresidual%20stream%20and%20we%20introduce%20designated%20%5Ctextit%7Bnoise%20abduction%20heads%7D%0Acentral%20to%20performing%20counterfactual%20reasoning.%20Lastly%2C%20our%20findings%20extend%20to%0Acounterfactual%20reasoning%20under%20SDE%20dynamics%20and%20reflect%20that%20Transformers%20can%0Aperform%20noise%20abduction%20on%20sequential%20data%2C%20providing%20preliminary%20evidence%20on%0Athe%20potential%20for%20counterfactual%20story%20generation.%20Our%20code%20is%20available%20under%0Ahttps%3A//github.com/mrtzmllr/iccr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520reasoning%253A%2520an%2520analysis%2520of%2520in-context%2520emergence%26entry.906535625%3DMoritz%2520Miller%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Siyuan%2520Guo%26entry.1292438233%3D%2520%2520Large-scale%2520neural%2520language%2520models%2520exhibit%2520remarkable%2520performance%2520in%250Ain-context%2520learning%253A%2520the%2520ability%2520to%2520learn%2520and%2520reason%2520about%2520the%2520input%2520context%2520on%250Athe%2520fly.%2520This%2520work%2520studies%2520in-context%2520counterfactual%2520reasoning%2520in%2520language%250Amodels%252C%2520that%2520is%252C%2520the%2520ability%2520to%2520predict%2520consequences%2520of%2520a%2520hypothetical%250Ascenario.%2520We%2520focus%2520on%2520a%2520well-defined%252C%2520synthetic%2520linear%2520regression%2520task%2520that%250Arequires%2520noise%2520abduction.%2520Accurate%2520prediction%2520is%2520based%2520on%2520%25281%2529%2520inferring%2520an%250Aunobserved%2520latent%2520concept%2520and%2520%25282%2529%2520copying%2520contextual%2520noise%2520from%2520factual%250Aobservations.%2520We%2520show%2520that%2520language%2520models%2520are%2520capable%2520of%2520counterfactual%250Areasoning.%2520Further%252C%2520we%2520enhance%2520existing%2520identifiability%2520results%2520and%2520reduce%250Acounterfactual%2520reasoning%2520for%2520a%2520broad%2520class%2520of%2520functions%2520to%2520a%2520transformation%2520on%250Ain-context%2520observations.%2520In%2520Transformers%252C%2520we%2520find%2520that%2520self-attention%252C%2520model%250Adepth%2520and%2520pre-training%2520data%2520diversity%2520drive%2520performance.%2520Moreover%252C%2520we%2520provide%250Amechanistic%2520evidence%2520that%2520the%2520latent%2520concept%2520is%2520linearly%2520represented%2520in%2520the%250Aresidual%2520stream%2520and%2520we%2520introduce%2520designated%2520%255Ctextit%257Bnoise%2520abduction%2520heads%257D%250Acentral%2520to%2520performing%2520counterfactual%2520reasoning.%2520Lastly%252C%2520our%2520findings%2520extend%2520to%250Acounterfactual%2520reasoning%2520under%2520SDE%2520dynamics%2520and%2520reflect%2520that%2520Transformers%2520can%250Aperform%2520noise%2520abduction%2520on%2520sequential%2520data%252C%2520providing%2520preliminary%2520evidence%2520on%250Athe%2520potential%2520for%2520counterfactual%2520story%2520generation.%2520Our%2520code%2520is%2520available%2520under%250Ahttps%253A//github.com/mrtzmllr/iccr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20reasoning%3A%20an%20analysis%20of%20in-context%20emergence&entry.906535625=Moritz%20Miller%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Siyuan%20Guo&entry.1292438233=%20%20Large-scale%20neural%20language%20models%20exhibit%20remarkable%20performance%20in%0Ain-context%20learning%3A%20the%20ability%20to%20learn%20and%20reason%20about%20the%20input%20context%20on%0Athe%20fly.%20This%20work%20studies%20in-context%20counterfactual%20reasoning%20in%20language%0Amodels%2C%20that%20is%2C%20the%20ability%20to%20predict%20consequences%20of%20a%20hypothetical%0Ascenario.%20We%20focus%20on%20a%20well-defined%2C%20synthetic%20linear%20regression%20task%20that%0Arequires%20noise%20abduction.%20Accurate%20prediction%20is%20based%20on%20%281%29%20inferring%20an%0Aunobserved%20latent%20concept%20and%20%282%29%20copying%20contextual%20noise%20from%20factual%0Aobservations.%20We%20show%20that%20language%20models%20are%20capable%20of%20counterfactual%0Areasoning.%20Further%2C%20we%20enhance%20existing%20identifiability%20results%20and%20reduce%0Acounterfactual%20reasoning%20for%20a%20broad%20class%20of%20functions%20to%20a%20transformation%20on%0Ain-context%20observations.%20In%20Transformers%2C%20we%20find%20that%20self-attention%2C%20model%0Adepth%20and%20pre-training%20data%20diversity%20drive%20performance.%20Moreover%2C%20we%20provide%0Amechanistic%20evidence%20that%20the%20latent%20concept%20is%20linearly%20represented%20in%20the%0Aresidual%20stream%20and%20we%20introduce%20designated%20%5Ctextit%7Bnoise%20abduction%20heads%7D%0Acentral%20to%20performing%20counterfactual%20reasoning.%20Lastly%2C%20our%20findings%20extend%20to%0Acounterfactual%20reasoning%20under%20SDE%20dynamics%20and%20reflect%20that%20Transformers%20can%0Aperform%20noise%20abduction%20on%20sequential%20data%2C%20providing%20preliminary%20evidence%20on%0Athe%20potential%20for%20counterfactual%20story%20generation.%20Our%20code%20is%20available%20under%0Ahttps%3A//github.com/mrtzmllr/iccr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05188v2&entry.124074799=Read"},
{"title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization", "author": "Yuanli Wu and Long Zhang and Yue Du and Bin Li", "abstract": "  With video exploding across social media, surveillance, and education,\ncompressing long footage into concise yet faithful surrogates is crucial.\nSupervised methods learn frame/shot importance from dense labels and excel\nin-domain, but are costly and brittle across datasets; unsupervised methods\navoid labels but often miss high-level semantics and narrative cues. Recent\nzero-shot pipelines use LLMs for training-free summarization, yet remain\nsensitive to handcrafted prompts and dataset-specific normalization.We propose\na rubric-guided, pseudo-labeled prompting framework. A small subset of human\nannotations is converted into high-confidence pseudo labels and aggregated into\nstructured, dataset-adaptive scoring rubrics for interpretable scene\nevaluation. At inference, boundary scenes (first/last) are scored from their\nown descriptions, while intermediate scenes include brief summaries of adjacent\nsegments to assess progression and redundancy, enabling the LLM to balance\nlocal salience with global coherence without parameter tuning.Across three\nbenchmarks, our method is consistently effective. On SumMe and TVSum it\nachieves F1 of 57.58 and 63.05, surpassing a zero-shot baseline (56.73, 62.21)\nby +0.85 and +0.84 and approaching supervised performance. On the query-focused\nQFVS benchmark it attains 53.79 F1, beating 53.42 by +0.37 and remaining stable\nacross validation videos. These results show that rubric-guided pseudo\nlabeling, coupled with contextual prompting, stabilizes LLM-based scoring and\nyields a general, interpretable zero-shot paradigm for both generic and\nquery-focused video summarization.\n", "link": "http://arxiv.org/abs/2510.17501v2", "date": "2025-10-21", "relevancy": 2.1633, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization&body=Title%3A%20Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization%0AAuthor%3A%20Yuanli%20Wu%20and%20Long%20Zhang%20and%20Yue%20Du%20and%20Bin%20Li%0AAbstract%3A%20%20%20With%20video%20exploding%20across%20social%20media%2C%20surveillance%2C%20and%20education%2C%0Acompressing%20long%20footage%20into%20concise%20yet%20faithful%20surrogates%20is%20crucial.%0ASupervised%20methods%20learn%20frame/shot%20importance%20from%20dense%20labels%20and%20excel%0Ain-domain%2C%20but%20are%20costly%20and%20brittle%20across%20datasets%3B%20unsupervised%20methods%0Aavoid%20labels%20but%20often%20miss%20high-level%20semantics%20and%20narrative%20cues.%20Recent%0Azero-shot%20pipelines%20use%20LLMs%20for%20training-free%20summarization%2C%20yet%20remain%0Asensitive%20to%20handcrafted%20prompts%20and%20dataset-specific%20normalization.We%20propose%0Aa%20rubric-guided%2C%20pseudo-labeled%20prompting%20framework.%20A%20small%20subset%20of%20human%0Aannotations%20is%20converted%20into%20high-confidence%20pseudo%20labels%20and%20aggregated%20into%0Astructured%2C%20dataset-adaptive%20scoring%20rubrics%20for%20interpretable%20scene%0Aevaluation.%20At%20inference%2C%20boundary%20scenes%20%28first/last%29%20are%20scored%20from%20their%0Aown%20descriptions%2C%20while%20intermediate%20scenes%20include%20brief%20summaries%20of%20adjacent%0Asegments%20to%20assess%20progression%20and%20redundancy%2C%20enabling%20the%20LLM%20to%20balance%0Alocal%20salience%20with%20global%20coherence%20without%20parameter%20tuning.Across%20three%0Abenchmarks%2C%20our%20method%20is%20consistently%20effective.%20On%20SumMe%20and%20TVSum%20it%0Aachieves%20F1%20of%2057.58%20and%2063.05%2C%20surpassing%20a%20zero-shot%20baseline%20%2856.73%2C%2062.21%29%0Aby%20%2B0.85%20and%20%2B0.84%20and%20approaching%20supervised%20performance.%20On%20the%20query-focused%0AQFVS%20benchmark%20it%20attains%2053.79%20F1%2C%20beating%2053.42%20by%20%2B0.37%20and%20remaining%20stable%0Aacross%20validation%20videos.%20These%20results%20show%20that%20rubric-guided%20pseudo%0Alabeling%2C%20coupled%20with%20contextual%20prompting%2C%20stabilizes%20LLM-based%20scoring%20and%0Ayields%20a%20general%2C%20interpretable%20zero-shot%20paradigm%20for%20both%20generic%20and%0Aquery-focused%20video%20summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Pseudo-Label%2520Scoring%2520for%2520Zero-Shot%2520Video%2520Summarization%26entry.906535625%3DYuanli%2520Wu%2520and%2520Long%2520Zhang%2520and%2520Yue%2520Du%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520With%2520video%2520exploding%2520across%2520social%2520media%252C%2520surveillance%252C%2520and%2520education%252C%250Acompressing%2520long%2520footage%2520into%2520concise%2520yet%2520faithful%2520surrogates%2520is%2520crucial.%250ASupervised%2520methods%2520learn%2520frame/shot%2520importance%2520from%2520dense%2520labels%2520and%2520excel%250Ain-domain%252C%2520but%2520are%2520costly%2520and%2520brittle%2520across%2520datasets%253B%2520unsupervised%2520methods%250Aavoid%2520labels%2520but%2520often%2520miss%2520high-level%2520semantics%2520and%2520narrative%2520cues.%2520Recent%250Azero-shot%2520pipelines%2520use%2520LLMs%2520for%2520training-free%2520summarization%252C%2520yet%2520remain%250Asensitive%2520to%2520handcrafted%2520prompts%2520and%2520dataset-specific%2520normalization.We%2520propose%250Aa%2520rubric-guided%252C%2520pseudo-labeled%2520prompting%2520framework.%2520A%2520small%2520subset%2520of%2520human%250Aannotations%2520is%2520converted%2520into%2520high-confidence%2520pseudo%2520labels%2520and%2520aggregated%2520into%250Astructured%252C%2520dataset-adaptive%2520scoring%2520rubrics%2520for%2520interpretable%2520scene%250Aevaluation.%2520At%2520inference%252C%2520boundary%2520scenes%2520%2528first/last%2529%2520are%2520scored%2520from%2520their%250Aown%2520descriptions%252C%2520while%2520intermediate%2520scenes%2520include%2520brief%2520summaries%2520of%2520adjacent%250Asegments%2520to%2520assess%2520progression%2520and%2520redundancy%252C%2520enabling%2520the%2520LLM%2520to%2520balance%250Alocal%2520salience%2520with%2520global%2520coherence%2520without%2520parameter%2520tuning.Across%2520three%250Abenchmarks%252C%2520our%2520method%2520is%2520consistently%2520effective.%2520On%2520SumMe%2520and%2520TVSum%2520it%250Aachieves%2520F1%2520of%252057.58%2520and%252063.05%252C%2520surpassing%2520a%2520zero-shot%2520baseline%2520%252856.73%252C%252062.21%2529%250Aby%2520%252B0.85%2520and%2520%252B0.84%2520and%2520approaching%2520supervised%2520performance.%2520On%2520the%2520query-focused%250AQFVS%2520benchmark%2520it%2520attains%252053.79%2520F1%252C%2520beating%252053.42%2520by%2520%252B0.37%2520and%2520remaining%2520stable%250Aacross%2520validation%2520videos.%2520These%2520results%2520show%2520that%2520rubric-guided%2520pseudo%250Alabeling%252C%2520coupled%2520with%2520contextual%2520prompting%252C%2520stabilizes%2520LLM-based%2520scoring%2520and%250Ayields%2520a%2520general%252C%2520interpretable%2520zero-shot%2520paradigm%2520for%2520both%2520generic%2520and%250Aquery-focused%2520video%2520summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Pseudo-Label%20Scoring%20for%20Zero-Shot%20Video%20Summarization&entry.906535625=Yuanli%20Wu%20and%20Long%20Zhang%20and%20Yue%20Du%20and%20Bin%20Li&entry.1292438233=%20%20With%20video%20exploding%20across%20social%20media%2C%20surveillance%2C%20and%20education%2C%0Acompressing%20long%20footage%20into%20concise%20yet%20faithful%20surrogates%20is%20crucial.%0ASupervised%20methods%20learn%20frame/shot%20importance%20from%20dense%20labels%20and%20excel%0Ain-domain%2C%20but%20are%20costly%20and%20brittle%20across%20datasets%3B%20unsupervised%20methods%0Aavoid%20labels%20but%20often%20miss%20high-level%20semantics%20and%20narrative%20cues.%20Recent%0Azero-shot%20pipelines%20use%20LLMs%20for%20training-free%20summarization%2C%20yet%20remain%0Asensitive%20to%20handcrafted%20prompts%20and%20dataset-specific%20normalization.We%20propose%0Aa%20rubric-guided%2C%20pseudo-labeled%20prompting%20framework.%20A%20small%20subset%20of%20human%0Aannotations%20is%20converted%20into%20high-confidence%20pseudo%20labels%20and%20aggregated%20into%0Astructured%2C%20dataset-adaptive%20scoring%20rubrics%20for%20interpretable%20scene%0Aevaluation.%20At%20inference%2C%20boundary%20scenes%20%28first/last%29%20are%20scored%20from%20their%0Aown%20descriptions%2C%20while%20intermediate%20scenes%20include%20brief%20summaries%20of%20adjacent%0Asegments%20to%20assess%20progression%20and%20redundancy%2C%20enabling%20the%20LLM%20to%20balance%0Alocal%20salience%20with%20global%20coherence%20without%20parameter%20tuning.Across%20three%0Abenchmarks%2C%20our%20method%20is%20consistently%20effective.%20On%20SumMe%20and%20TVSum%20it%0Aachieves%20F1%20of%2057.58%20and%2063.05%2C%20surpassing%20a%20zero-shot%20baseline%20%2856.73%2C%2062.21%29%0Aby%20%2B0.85%20and%20%2B0.84%20and%20approaching%20supervised%20performance.%20On%20the%20query-focused%0AQFVS%20benchmark%20it%20attains%2053.79%20F1%2C%20beating%2053.42%20by%20%2B0.37%20and%20remaining%20stable%0Aacross%20validation%20videos.%20These%20results%20show%20that%20rubric-guided%20pseudo%0Alabeling%2C%20coupled%20with%20contextual%20prompting%2C%20stabilizes%20LLM-based%20scoring%20and%0Ayields%20a%20general%2C%20interpretable%20zero-shot%20paradigm%20for%20both%20generic%20and%0Aquery-focused%20video%20summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17501v2&entry.124074799=Read"},
{"title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning\n  with Prediction Augmentation", "author": "Muqun Hu and Wenxi Chen and Wenjing Li and Falak Mandali and Zijian He and Renhong Zhang and Praveen Krisna and Katherine Christian and Leo Benaharon and Dizhi Ma and Karthik Ramani and Yan Gu", "abstract": "  Humanoid table tennis (TT) demands rapid perception, proactive whole-body\nmotion, and agile footwork under strict timing -- capabilities that remain\ndifficult for unified controllers. We propose a reinforcement learning\nframework that maps ball-position observations directly to whole-body joint\ncommands for both arm striking and leg locomotion, strengthened by predictive\nsignals and dense, physics-guided rewards. A lightweight learned predictor, fed\nwith recent ball positions, estimates future ball states and augments the\npolicy's observations for proactive decision-making. During training, a\nphysics-based predictor supplies precise future states to construct dense,\ninformative rewards that lead to effective exploration. The resulting policy\nattains strong performance across varied serve ranges (hit rate $\\geq$ 96% and\nsuccess rate $\\geq$ 92%) in simulations. Ablation studies confirm that both the\nlearned predictor and the predictive reward design are critical for end-to-end\nlearning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute\njoints, the policy produces coordinated lateral and forward-backward footwork\nwith accurate, fast returns, suggesting a practical path toward versatile,\ncompetitive humanoid TT.\n", "link": "http://arxiv.org/abs/2509.21690v2", "date": "2025-10-21", "relevancy": 2.1617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5803}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5424}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Versatile%20Humanoid%20Table%20Tennis%3A%20Unified%20Reinforcement%20Learning%0A%20%20with%20Prediction%20Augmentation&body=Title%3A%20Towards%20Versatile%20Humanoid%20Table%20Tennis%3A%20Unified%20Reinforcement%20Learning%0A%20%20with%20Prediction%20Augmentation%0AAuthor%3A%20Muqun%20Hu%20and%20Wenxi%20Chen%20and%20Wenjing%20Li%20and%20Falak%20Mandali%20and%20Zijian%20He%20and%20Renhong%20Zhang%20and%20Praveen%20Krisna%20and%20Katherine%20Christian%20and%20Leo%20Benaharon%20and%20Dizhi%20Ma%20and%20Karthik%20Ramani%20and%20Yan%20Gu%0AAbstract%3A%20%20%20Humanoid%20table%20tennis%20%28TT%29%20demands%20rapid%20perception%2C%20proactive%20whole-body%0Amotion%2C%20and%20agile%20footwork%20under%20strict%20timing%20--%20capabilities%20that%20remain%0Adifficult%20for%20unified%20controllers.%20We%20propose%20a%20reinforcement%20learning%0Aframework%20that%20maps%20ball-position%20observations%20directly%20to%20whole-body%20joint%0Acommands%20for%20both%20arm%20striking%20and%20leg%20locomotion%2C%20strengthened%20by%20predictive%0Asignals%20and%20dense%2C%20physics-guided%20rewards.%20A%20lightweight%20learned%20predictor%2C%20fed%0Awith%20recent%20ball%20positions%2C%20estimates%20future%20ball%20states%20and%20augments%20the%0Apolicy%27s%20observations%20for%20proactive%20decision-making.%20During%20training%2C%20a%0Aphysics-based%20predictor%20supplies%20precise%20future%20states%20to%20construct%20dense%2C%0Ainformative%20rewards%20that%20lead%20to%20effective%20exploration.%20The%20resulting%20policy%0Aattains%20strong%20performance%20across%20varied%20serve%20ranges%20%28hit%20rate%20%24%5Cgeq%24%2096%25%20and%0Asuccess%20rate%20%24%5Cgeq%24%2092%25%29%20in%20simulations.%20Ablation%20studies%20confirm%20that%20both%20the%0Alearned%20predictor%20and%20the%20predictive%20reward%20design%20are%20critical%20for%20end-to-end%0Alearning.%20Deployed%20zero-shot%20on%20a%20physical%20Booster%20T1%20humanoid%20with%2023%20revolute%0Ajoints%2C%20the%20policy%20produces%20coordinated%20lateral%20and%20forward-backward%20footwork%0Awith%20accurate%2C%20fast%20returns%2C%20suggesting%20a%20practical%20path%20toward%20versatile%2C%0Acompetitive%20humanoid%20TT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Versatile%2520Humanoid%2520Table%2520Tennis%253A%2520Unified%2520Reinforcement%2520Learning%250A%2520%2520with%2520Prediction%2520Augmentation%26entry.906535625%3DMuqun%2520Hu%2520and%2520Wenxi%2520Chen%2520and%2520Wenjing%2520Li%2520and%2520Falak%2520Mandali%2520and%2520Zijian%2520He%2520and%2520Renhong%2520Zhang%2520and%2520Praveen%2520Krisna%2520and%2520Katherine%2520Christian%2520and%2520Leo%2520Benaharon%2520and%2520Dizhi%2520Ma%2520and%2520Karthik%2520Ramani%2520and%2520Yan%2520Gu%26entry.1292438233%3D%2520%2520Humanoid%2520table%2520tennis%2520%2528TT%2529%2520demands%2520rapid%2520perception%252C%2520proactive%2520whole-body%250Amotion%252C%2520and%2520agile%2520footwork%2520under%2520strict%2520timing%2520--%2520capabilities%2520that%2520remain%250Adifficult%2520for%2520unified%2520controllers.%2520We%2520propose%2520a%2520reinforcement%2520learning%250Aframework%2520that%2520maps%2520ball-position%2520observations%2520directly%2520to%2520whole-body%2520joint%250Acommands%2520for%2520both%2520arm%2520striking%2520and%2520leg%2520locomotion%252C%2520strengthened%2520by%2520predictive%250Asignals%2520and%2520dense%252C%2520physics-guided%2520rewards.%2520A%2520lightweight%2520learned%2520predictor%252C%2520fed%250Awith%2520recent%2520ball%2520positions%252C%2520estimates%2520future%2520ball%2520states%2520and%2520augments%2520the%250Apolicy%2527s%2520observations%2520for%2520proactive%2520decision-making.%2520During%2520training%252C%2520a%250Aphysics-based%2520predictor%2520supplies%2520precise%2520future%2520states%2520to%2520construct%2520dense%252C%250Ainformative%2520rewards%2520that%2520lead%2520to%2520effective%2520exploration.%2520The%2520resulting%2520policy%250Aattains%2520strong%2520performance%2520across%2520varied%2520serve%2520ranges%2520%2528hit%2520rate%2520%2524%255Cgeq%2524%252096%2525%2520and%250Asuccess%2520rate%2520%2524%255Cgeq%2524%252092%2525%2529%2520in%2520simulations.%2520Ablation%2520studies%2520confirm%2520that%2520both%2520the%250Alearned%2520predictor%2520and%2520the%2520predictive%2520reward%2520design%2520are%2520critical%2520for%2520end-to-end%250Alearning.%2520Deployed%2520zero-shot%2520on%2520a%2520physical%2520Booster%2520T1%2520humanoid%2520with%252023%2520revolute%250Ajoints%252C%2520the%2520policy%2520produces%2520coordinated%2520lateral%2520and%2520forward-backward%2520footwork%250Awith%2520accurate%252C%2520fast%2520returns%252C%2520suggesting%2520a%2520practical%2520path%2520toward%2520versatile%252C%250Acompetitive%2520humanoid%2520TT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Versatile%20Humanoid%20Table%20Tennis%3A%20Unified%20Reinforcement%20Learning%0A%20%20with%20Prediction%20Augmentation&entry.906535625=Muqun%20Hu%20and%20Wenxi%20Chen%20and%20Wenjing%20Li%20and%20Falak%20Mandali%20and%20Zijian%20He%20and%20Renhong%20Zhang%20and%20Praveen%20Krisna%20and%20Katherine%20Christian%20and%20Leo%20Benaharon%20and%20Dizhi%20Ma%20and%20Karthik%20Ramani%20and%20Yan%20Gu&entry.1292438233=%20%20Humanoid%20table%20tennis%20%28TT%29%20demands%20rapid%20perception%2C%20proactive%20whole-body%0Amotion%2C%20and%20agile%20footwork%20under%20strict%20timing%20--%20capabilities%20that%20remain%0Adifficult%20for%20unified%20controllers.%20We%20propose%20a%20reinforcement%20learning%0Aframework%20that%20maps%20ball-position%20observations%20directly%20to%20whole-body%20joint%0Acommands%20for%20both%20arm%20striking%20and%20leg%20locomotion%2C%20strengthened%20by%20predictive%0Asignals%20and%20dense%2C%20physics-guided%20rewards.%20A%20lightweight%20learned%20predictor%2C%20fed%0Awith%20recent%20ball%20positions%2C%20estimates%20future%20ball%20states%20and%20augments%20the%0Apolicy%27s%20observations%20for%20proactive%20decision-making.%20During%20training%2C%20a%0Aphysics-based%20predictor%20supplies%20precise%20future%20states%20to%20construct%20dense%2C%0Ainformative%20rewards%20that%20lead%20to%20effective%20exploration.%20The%20resulting%20policy%0Aattains%20strong%20performance%20across%20varied%20serve%20ranges%20%28hit%20rate%20%24%5Cgeq%24%2096%25%20and%0Asuccess%20rate%20%24%5Cgeq%24%2092%25%29%20in%20simulations.%20Ablation%20studies%20confirm%20that%20both%20the%0Alearned%20predictor%20and%20the%20predictive%20reward%20design%20are%20critical%20for%20end-to-end%0Alearning.%20Deployed%20zero-shot%20on%20a%20physical%20Booster%20T1%20humanoid%20with%2023%20revolute%0Ajoints%2C%20the%20policy%20produces%20coordinated%20lateral%20and%20forward-backward%20footwork%0Awith%20accurate%2C%20fast%20returns%2C%20suggesting%20a%20practical%20path%20toward%20versatile%2C%0Acompetitive%20humanoid%20TT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21690v2&entry.124074799=Read"},
{"title": "HeFS: Helper-Enhanced Feature Selection via Pareto-Optimized Genetic\n  Search", "author": "Yusi Fan and Tian Wang and Zhiying Yan and Chang Liu and Qiong Zhou and Qi Lu and Zhehao Guo and Ziqi Deng and Wenyu Zhu and Ruochi Zhang and Fengfeng Zhou", "abstract": "  Feature selection is a combinatorial optimization problem that is NP-hard.\nConventional approaches often employ heuristic or greedy strategies, which are\nprone to premature convergence and may fail to capture subtle yet informative\nfeatures. This limitation becomes especially critical in high-dimensional\ndatasets, where complex and interdependent feature relationships prevail. We\nintroduce the HeFS (Helper-Enhanced Feature Selection) framework to refine\nfeature subsets produced by existing algorithms. HeFS systematically searches\nthe residual feature space to identify a Helper Set - features that complement\nthe original subset and improve classification performance. The approach\nemploys a biased initialization scheme and a ratio-guided mutation mechanism\nwithin a genetic algorithm, coupled with Pareto-based multi-objective\noptimization to jointly maximize predictive accuracy and feature\ncomplementarity. Experiments on 18 benchmark datasets demonstrate that HeFS\nconsistently identifies overlooked yet informative features and achieves\nsuperior performance over state-of-the-art methods, including in challenging\ndomains such as gastric cancer classification, drug toxicity prediction, and\ncomputer science applications. The code and datasets are available at\nhttps://healthinformaticslab.org/supp/.\n", "link": "http://arxiv.org/abs/2510.18575v1", "date": "2025-10-21", "relevancy": 2.1515, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4421}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4301}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeFS%3A%20Helper-Enhanced%20Feature%20Selection%20via%20Pareto-Optimized%20Genetic%0A%20%20Search&body=Title%3A%20HeFS%3A%20Helper-Enhanced%20Feature%20Selection%20via%20Pareto-Optimized%20Genetic%0A%20%20Search%0AAuthor%3A%20Yusi%20Fan%20and%20Tian%20Wang%20and%20Zhiying%20Yan%20and%20Chang%20Liu%20and%20Qiong%20Zhou%20and%20Qi%20Lu%20and%20Zhehao%20Guo%20and%20Ziqi%20Deng%20and%20Wenyu%20Zhu%20and%20Ruochi%20Zhang%20and%20Fengfeng%20Zhou%0AAbstract%3A%20%20%20Feature%20selection%20is%20a%20combinatorial%20optimization%20problem%20that%20is%20NP-hard.%0AConventional%20approaches%20often%20employ%20heuristic%20or%20greedy%20strategies%2C%20which%20are%0Aprone%20to%20premature%20convergence%20and%20may%20fail%20to%20capture%20subtle%20yet%20informative%0Afeatures.%20This%20limitation%20becomes%20especially%20critical%20in%20high-dimensional%0Adatasets%2C%20where%20complex%20and%20interdependent%20feature%20relationships%20prevail.%20We%0Aintroduce%20the%20HeFS%20%28Helper-Enhanced%20Feature%20Selection%29%20framework%20to%20refine%0Afeature%20subsets%20produced%20by%20existing%20algorithms.%20HeFS%20systematically%20searches%0Athe%20residual%20feature%20space%20to%20identify%20a%20Helper%20Set%20-%20features%20that%20complement%0Athe%20original%20subset%20and%20improve%20classification%20performance.%20The%20approach%0Aemploys%20a%20biased%20initialization%20scheme%20and%20a%20ratio-guided%20mutation%20mechanism%0Awithin%20a%20genetic%20algorithm%2C%20coupled%20with%20Pareto-based%20multi-objective%0Aoptimization%20to%20jointly%20maximize%20predictive%20accuracy%20and%20feature%0Acomplementarity.%20Experiments%20on%2018%20benchmark%20datasets%20demonstrate%20that%20HeFS%0Aconsistently%20identifies%20overlooked%20yet%20informative%20features%20and%20achieves%0Asuperior%20performance%20over%20state-of-the-art%20methods%2C%20including%20in%20challenging%0Adomains%20such%20as%20gastric%20cancer%20classification%2C%20drug%20toxicity%20prediction%2C%20and%0Acomputer%20science%20applications.%20The%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//healthinformaticslab.org/supp/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeFS%253A%2520Helper-Enhanced%2520Feature%2520Selection%2520via%2520Pareto-Optimized%2520Genetic%250A%2520%2520Search%26entry.906535625%3DYusi%2520Fan%2520and%2520Tian%2520Wang%2520and%2520Zhiying%2520Yan%2520and%2520Chang%2520Liu%2520and%2520Qiong%2520Zhou%2520and%2520Qi%2520Lu%2520and%2520Zhehao%2520Guo%2520and%2520Ziqi%2520Deng%2520and%2520Wenyu%2520Zhu%2520and%2520Ruochi%2520Zhang%2520and%2520Fengfeng%2520Zhou%26entry.1292438233%3D%2520%2520Feature%2520selection%2520is%2520a%2520combinatorial%2520optimization%2520problem%2520that%2520is%2520NP-hard.%250AConventional%2520approaches%2520often%2520employ%2520heuristic%2520or%2520greedy%2520strategies%252C%2520which%2520are%250Aprone%2520to%2520premature%2520convergence%2520and%2520may%2520fail%2520to%2520capture%2520subtle%2520yet%2520informative%250Afeatures.%2520This%2520limitation%2520becomes%2520especially%2520critical%2520in%2520high-dimensional%250Adatasets%252C%2520where%2520complex%2520and%2520interdependent%2520feature%2520relationships%2520prevail.%2520We%250Aintroduce%2520the%2520HeFS%2520%2528Helper-Enhanced%2520Feature%2520Selection%2529%2520framework%2520to%2520refine%250Afeature%2520subsets%2520produced%2520by%2520existing%2520algorithms.%2520HeFS%2520systematically%2520searches%250Athe%2520residual%2520feature%2520space%2520to%2520identify%2520a%2520Helper%2520Set%2520-%2520features%2520that%2520complement%250Athe%2520original%2520subset%2520and%2520improve%2520classification%2520performance.%2520The%2520approach%250Aemploys%2520a%2520biased%2520initialization%2520scheme%2520and%2520a%2520ratio-guided%2520mutation%2520mechanism%250Awithin%2520a%2520genetic%2520algorithm%252C%2520coupled%2520with%2520Pareto-based%2520multi-objective%250Aoptimization%2520to%2520jointly%2520maximize%2520predictive%2520accuracy%2520and%2520feature%250Acomplementarity.%2520Experiments%2520on%252018%2520benchmark%2520datasets%2520demonstrate%2520that%2520HeFS%250Aconsistently%2520identifies%2520overlooked%2520yet%2520informative%2520features%2520and%2520achieves%250Asuperior%2520performance%2520over%2520state-of-the-art%2520methods%252C%2520including%2520in%2520challenging%250Adomains%2520such%2520as%2520gastric%2520cancer%2520classification%252C%2520drug%2520toxicity%2520prediction%252C%2520and%250Acomputer%2520science%2520applications.%2520The%2520code%2520and%2520datasets%2520are%2520available%2520at%250Ahttps%253A//healthinformaticslab.org/supp/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeFS%3A%20Helper-Enhanced%20Feature%20Selection%20via%20Pareto-Optimized%20Genetic%0A%20%20Search&entry.906535625=Yusi%20Fan%20and%20Tian%20Wang%20and%20Zhiying%20Yan%20and%20Chang%20Liu%20and%20Qiong%20Zhou%20and%20Qi%20Lu%20and%20Zhehao%20Guo%20and%20Ziqi%20Deng%20and%20Wenyu%20Zhu%20and%20Ruochi%20Zhang%20and%20Fengfeng%20Zhou&entry.1292438233=%20%20Feature%20selection%20is%20a%20combinatorial%20optimization%20problem%20that%20is%20NP-hard.%0AConventional%20approaches%20often%20employ%20heuristic%20or%20greedy%20strategies%2C%20which%20are%0Aprone%20to%20premature%20convergence%20and%20may%20fail%20to%20capture%20subtle%20yet%20informative%0Afeatures.%20This%20limitation%20becomes%20especially%20critical%20in%20high-dimensional%0Adatasets%2C%20where%20complex%20and%20interdependent%20feature%20relationships%20prevail.%20We%0Aintroduce%20the%20HeFS%20%28Helper-Enhanced%20Feature%20Selection%29%20framework%20to%20refine%0Afeature%20subsets%20produced%20by%20existing%20algorithms.%20HeFS%20systematically%20searches%0Athe%20residual%20feature%20space%20to%20identify%20a%20Helper%20Set%20-%20features%20that%20complement%0Athe%20original%20subset%20and%20improve%20classification%20performance.%20The%20approach%0Aemploys%20a%20biased%20initialization%20scheme%20and%20a%20ratio-guided%20mutation%20mechanism%0Awithin%20a%20genetic%20algorithm%2C%20coupled%20with%20Pareto-based%20multi-objective%0Aoptimization%20to%20jointly%20maximize%20predictive%20accuracy%20and%20feature%0Acomplementarity.%20Experiments%20on%2018%20benchmark%20datasets%20demonstrate%20that%20HeFS%0Aconsistently%20identifies%20overlooked%20yet%20informative%20features%20and%20achieves%0Asuperior%20performance%20over%20state-of-the-art%20methods%2C%20including%20in%20challenging%0Adomains%20such%20as%20gastric%20cancer%20classification%2C%20drug%20toxicity%20prediction%2C%20and%0Acomputer%20science%20applications.%20The%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//healthinformaticslab.org/supp/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18575v1&entry.124074799=Read"},
{"title": "Adapting Medical Vision Foundation Models for Volumetric Medical Image\n  Segmentation via Active Learning and Selective Semi-supervised Fine-tuning", "author": "Jin Yang and Daniel S. Marcus and Aristeidis Sotiras", "abstract": "  Medical Vision Foundation Models (Med-VFMs) have superior capabilities of\ninterpreting medical images due to the knowledge learned from self-supervised\npre-training with extensive unannotated images. To improve their performance on\nadaptive downstream evaluations, especially segmentation, a few samples from\ntarget domains are selected randomly for fine-tuning them. However, there lacks\nworks to explore the way of adapting Med-VFMs to achieve the optimal\nperformance on target domains efficiently. Thus, it is highly demanded to\ndesign an efficient way of fine-tuning Med-VFMs by selecting informative\nsamples to maximize their adaptation performance on target domains. To achieve\nthis, we propose an Active Source-Free Domain Adaptation (ASFDA) method to\nefficiently adapt Med-VFMs to target domains for volumetric medical image\nsegmentation. This ASFDA employs a novel Active Learning (AL) method to select\nthe most informative samples from target domains for fine-tuning Med-VFMs\nwithout the access to source pre-training samples, thus maximizing their\nperformance with the minimal selection budget. In this AL method, we design an\nActive Test Time Sample Query strategy to select samples from the target\ndomains via two query metrics, including Diversified Knowledge Divergence (DKD)\nand Anatomical Segmentation Difficulty (ASD). DKD is designed to measure the\nsource-target knowledge gap and intra-domain diversity. It utilizes the\nknowledge of pre-training to guide the querying of source-dissimilar and\nsemantic-diverse samples from the target domains. ASD is designed to evaluate\nthe difficulty in segmentation of anatomical structures by measuring predictive\nentropy from foreground regions adaptively. Additionally, our ASFDA method\nemploys a Selective Semi-supervised Fine-tuning to improve the performance and\nefficiency of fine-tuning by identifying samples with high reliability from\nunqueried ones.\n", "link": "http://arxiv.org/abs/2509.10784v2", "date": "2025-10-21", "relevancy": 2.1497, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Medical%20Vision%20Foundation%20Models%20for%20Volumetric%20Medical%20Image%0A%20%20Segmentation%20via%20Active%20Learning%20and%20Selective%20Semi-supervised%20Fine-tuning&body=Title%3A%20Adapting%20Medical%20Vision%20Foundation%20Models%20for%20Volumetric%20Medical%20Image%0A%20%20Segmentation%20via%20Active%20Learning%20and%20Selective%20Semi-supervised%20Fine-tuning%0AAuthor%3A%20Jin%20Yang%20and%20Daniel%20S.%20Marcus%20and%20Aristeidis%20Sotiras%0AAbstract%3A%20%20%20Medical%20Vision%20Foundation%20Models%20%28Med-VFMs%29%20have%20superior%20capabilities%20of%0Ainterpreting%20medical%20images%20due%20to%20the%20knowledge%20learned%20from%20self-supervised%0Apre-training%20with%20extensive%20unannotated%20images.%20To%20improve%20their%20performance%20on%0Aadaptive%20downstream%20evaluations%2C%20especially%20segmentation%2C%20a%20few%20samples%20from%0Atarget%20domains%20are%20selected%20randomly%20for%20fine-tuning%20them.%20However%2C%20there%20lacks%0Aworks%20to%20explore%20the%20way%20of%20adapting%20Med-VFMs%20to%20achieve%20the%20optimal%0Aperformance%20on%20target%20domains%20efficiently.%20Thus%2C%20it%20is%20highly%20demanded%20to%0Adesign%20an%20efficient%20way%20of%20fine-tuning%20Med-VFMs%20by%20selecting%20informative%0Asamples%20to%20maximize%20their%20adaptation%20performance%20on%20target%20domains.%20To%20achieve%0Athis%2C%20we%20propose%20an%20Active%20Source-Free%20Domain%20Adaptation%20%28ASFDA%29%20method%20to%0Aefficiently%20adapt%20Med-VFMs%20to%20target%20domains%20for%20volumetric%20medical%20image%0Asegmentation.%20This%20ASFDA%20employs%20a%20novel%20Active%20Learning%20%28AL%29%20method%20to%20select%0Athe%20most%20informative%20samples%20from%20target%20domains%20for%20fine-tuning%20Med-VFMs%0Awithout%20the%20access%20to%20source%20pre-training%20samples%2C%20thus%20maximizing%20their%0Aperformance%20with%20the%20minimal%20selection%20budget.%20In%20this%20AL%20method%2C%20we%20design%20an%0AActive%20Test%20Time%20Sample%20Query%20strategy%20to%20select%20samples%20from%20the%20target%0Adomains%20via%20two%20query%20metrics%2C%20including%20Diversified%20Knowledge%20Divergence%20%28DKD%29%0Aand%20Anatomical%20Segmentation%20Difficulty%20%28ASD%29.%20DKD%20is%20designed%20to%20measure%20the%0Asource-target%20knowledge%20gap%20and%20intra-domain%20diversity.%20It%20utilizes%20the%0Aknowledge%20of%20pre-training%20to%20guide%20the%20querying%20of%20source-dissimilar%20and%0Asemantic-diverse%20samples%20from%20the%20target%20domains.%20ASD%20is%20designed%20to%20evaluate%0Athe%20difficulty%20in%20segmentation%20of%20anatomical%20structures%20by%20measuring%20predictive%0Aentropy%20from%20foreground%20regions%20adaptively.%20Additionally%2C%20our%20ASFDA%20method%0Aemploys%20a%20Selective%20Semi-supervised%20Fine-tuning%20to%20improve%20the%20performance%20and%0Aefficiency%20of%20fine-tuning%20by%20identifying%20samples%20with%20high%20reliability%20from%0Aunqueried%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Medical%2520Vision%2520Foundation%2520Models%2520for%2520Volumetric%2520Medical%2520Image%250A%2520%2520Segmentation%2520via%2520Active%2520Learning%2520and%2520Selective%2520Semi-supervised%2520Fine-tuning%26entry.906535625%3DJin%2520Yang%2520and%2520Daniel%2520S.%2520Marcus%2520and%2520Aristeidis%2520Sotiras%26entry.1292438233%3D%2520%2520Medical%2520Vision%2520Foundation%2520Models%2520%2528Med-VFMs%2529%2520have%2520superior%2520capabilities%2520of%250Ainterpreting%2520medical%2520images%2520due%2520to%2520the%2520knowledge%2520learned%2520from%2520self-supervised%250Apre-training%2520with%2520extensive%2520unannotated%2520images.%2520To%2520improve%2520their%2520performance%2520on%250Aadaptive%2520downstream%2520evaluations%252C%2520especially%2520segmentation%252C%2520a%2520few%2520samples%2520from%250Atarget%2520domains%2520are%2520selected%2520randomly%2520for%2520fine-tuning%2520them.%2520However%252C%2520there%2520lacks%250Aworks%2520to%2520explore%2520the%2520way%2520of%2520adapting%2520Med-VFMs%2520to%2520achieve%2520the%2520optimal%250Aperformance%2520on%2520target%2520domains%2520efficiently.%2520Thus%252C%2520it%2520is%2520highly%2520demanded%2520to%250Adesign%2520an%2520efficient%2520way%2520of%2520fine-tuning%2520Med-VFMs%2520by%2520selecting%2520informative%250Asamples%2520to%2520maximize%2520their%2520adaptation%2520performance%2520on%2520target%2520domains.%2520To%2520achieve%250Athis%252C%2520we%2520propose%2520an%2520Active%2520Source-Free%2520Domain%2520Adaptation%2520%2528ASFDA%2529%2520method%2520to%250Aefficiently%2520adapt%2520Med-VFMs%2520to%2520target%2520domains%2520for%2520volumetric%2520medical%2520image%250Asegmentation.%2520This%2520ASFDA%2520employs%2520a%2520novel%2520Active%2520Learning%2520%2528AL%2529%2520method%2520to%2520select%250Athe%2520most%2520informative%2520samples%2520from%2520target%2520domains%2520for%2520fine-tuning%2520Med-VFMs%250Awithout%2520the%2520access%2520to%2520source%2520pre-training%2520samples%252C%2520thus%2520maximizing%2520their%250Aperformance%2520with%2520the%2520minimal%2520selection%2520budget.%2520In%2520this%2520AL%2520method%252C%2520we%2520design%2520an%250AActive%2520Test%2520Time%2520Sample%2520Query%2520strategy%2520to%2520select%2520samples%2520from%2520the%2520target%250Adomains%2520via%2520two%2520query%2520metrics%252C%2520including%2520Diversified%2520Knowledge%2520Divergence%2520%2528DKD%2529%250Aand%2520Anatomical%2520Segmentation%2520Difficulty%2520%2528ASD%2529.%2520DKD%2520is%2520designed%2520to%2520measure%2520the%250Asource-target%2520knowledge%2520gap%2520and%2520intra-domain%2520diversity.%2520It%2520utilizes%2520the%250Aknowledge%2520of%2520pre-training%2520to%2520guide%2520the%2520querying%2520of%2520source-dissimilar%2520and%250Asemantic-diverse%2520samples%2520from%2520the%2520target%2520domains.%2520ASD%2520is%2520designed%2520to%2520evaluate%250Athe%2520difficulty%2520in%2520segmentation%2520of%2520anatomical%2520structures%2520by%2520measuring%2520predictive%250Aentropy%2520from%2520foreground%2520regions%2520adaptively.%2520Additionally%252C%2520our%2520ASFDA%2520method%250Aemploys%2520a%2520Selective%2520Semi-supervised%2520Fine-tuning%2520to%2520improve%2520the%2520performance%2520and%250Aefficiency%2520of%2520fine-tuning%2520by%2520identifying%2520samples%2520with%2520high%2520reliability%2520from%250Aunqueried%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Medical%20Vision%20Foundation%20Models%20for%20Volumetric%20Medical%20Image%0A%20%20Segmentation%20via%20Active%20Learning%20and%20Selective%20Semi-supervised%20Fine-tuning&entry.906535625=Jin%20Yang%20and%20Daniel%20S.%20Marcus%20and%20Aristeidis%20Sotiras&entry.1292438233=%20%20Medical%20Vision%20Foundation%20Models%20%28Med-VFMs%29%20have%20superior%20capabilities%20of%0Ainterpreting%20medical%20images%20due%20to%20the%20knowledge%20learned%20from%20self-supervised%0Apre-training%20with%20extensive%20unannotated%20images.%20To%20improve%20their%20performance%20on%0Aadaptive%20downstream%20evaluations%2C%20especially%20segmentation%2C%20a%20few%20samples%20from%0Atarget%20domains%20are%20selected%20randomly%20for%20fine-tuning%20them.%20However%2C%20there%20lacks%0Aworks%20to%20explore%20the%20way%20of%20adapting%20Med-VFMs%20to%20achieve%20the%20optimal%0Aperformance%20on%20target%20domains%20efficiently.%20Thus%2C%20it%20is%20highly%20demanded%20to%0Adesign%20an%20efficient%20way%20of%20fine-tuning%20Med-VFMs%20by%20selecting%20informative%0Asamples%20to%20maximize%20their%20adaptation%20performance%20on%20target%20domains.%20To%20achieve%0Athis%2C%20we%20propose%20an%20Active%20Source-Free%20Domain%20Adaptation%20%28ASFDA%29%20method%20to%0Aefficiently%20adapt%20Med-VFMs%20to%20target%20domains%20for%20volumetric%20medical%20image%0Asegmentation.%20This%20ASFDA%20employs%20a%20novel%20Active%20Learning%20%28AL%29%20method%20to%20select%0Athe%20most%20informative%20samples%20from%20target%20domains%20for%20fine-tuning%20Med-VFMs%0Awithout%20the%20access%20to%20source%20pre-training%20samples%2C%20thus%20maximizing%20their%0Aperformance%20with%20the%20minimal%20selection%20budget.%20In%20this%20AL%20method%2C%20we%20design%20an%0AActive%20Test%20Time%20Sample%20Query%20strategy%20to%20select%20samples%20from%20the%20target%0Adomains%20via%20two%20query%20metrics%2C%20including%20Diversified%20Knowledge%20Divergence%20%28DKD%29%0Aand%20Anatomical%20Segmentation%20Difficulty%20%28ASD%29.%20DKD%20is%20designed%20to%20measure%20the%0Asource-target%20knowledge%20gap%20and%20intra-domain%20diversity.%20It%20utilizes%20the%0Aknowledge%20of%20pre-training%20to%20guide%20the%20querying%20of%20source-dissimilar%20and%0Asemantic-diverse%20samples%20from%20the%20target%20domains.%20ASD%20is%20designed%20to%20evaluate%0Athe%20difficulty%20in%20segmentation%20of%20anatomical%20structures%20by%20measuring%20predictive%0Aentropy%20from%20foreground%20regions%20adaptively.%20Additionally%2C%20our%20ASFDA%20method%0Aemploys%20a%20Selective%20Semi-supervised%20Fine-tuning%20to%20improve%20the%20performance%20and%0Aefficiency%20of%20fine-tuning%20by%20identifying%20samples%20with%20high%20reliability%20from%0Aunqueried%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10784v2&entry.124074799=Read"},
{"title": "Learning Task-Agnostic Representations through Multi-Teacher\n  Distillation", "author": "Philippe Formont and Maxime Darrin and Banafsheh Karimian and Jackie CK Cheung and Eric Granger and Ismail Ben Ayed and Mohammadhadi Shateri and Pablo Piantanida", "abstract": "  Casting complex inputs into tractable representations is a critical step\nacross various fields. Diverse embedding models emerge from differences in\narchitectures, loss functions, input modalities and datasets, each capturing\nunique aspects of the input. Multi-teacher distillation leverages this\ndiversity to enrich representations but often remains tailored to specific\ntasks. In this paper, we introduce a task-agnostic framework based on a\n``majority vote\" objective function. We demonstrate that this function is\nbounded by the mutual information between student and teachers' embeddings,\nleading to a task-agnostic distillation loss that eliminates dependence on\ntask-specific labels or prior knowledge. Our evaluations across text, vision\nmodels, and molecular modeling show that our method effectively leverages\nteacher diversity, resulting in representations enabling better performance for\na wide range of downstream tasks such as classification, clustering, or\nregression. Additionally, we train and release state-of-the-art embedding\nmodels, enhancing downstream performance in various modalities.\n", "link": "http://arxiv.org/abs/2510.18680v1", "date": "2025-10-21", "relevancy": 2.1494, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5693}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5212}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Task-Agnostic%20Representations%20through%20Multi-Teacher%0A%20%20Distillation&body=Title%3A%20Learning%20Task-Agnostic%20Representations%20through%20Multi-Teacher%0A%20%20Distillation%0AAuthor%3A%20Philippe%20Formont%20and%20Maxime%20Darrin%20and%20Banafsheh%20Karimian%20and%20Jackie%20CK%20Cheung%20and%20Eric%20Granger%20and%20Ismail%20Ben%20Ayed%20and%20Mohammadhadi%20Shateri%20and%20Pablo%20Piantanida%0AAbstract%3A%20%20%20Casting%20complex%20inputs%20into%20tractable%20representations%20is%20a%20critical%20step%0Aacross%20various%20fields.%20Diverse%20embedding%20models%20emerge%20from%20differences%20in%0Aarchitectures%2C%20loss%20functions%2C%20input%20modalities%20and%20datasets%2C%20each%20capturing%0Aunique%20aspects%20of%20the%20input.%20Multi-teacher%20distillation%20leverages%20this%0Adiversity%20to%20enrich%20representations%20but%20often%20remains%20tailored%20to%20specific%0Atasks.%20In%20this%20paper%2C%20we%20introduce%20a%20task-agnostic%20framework%20based%20on%20a%0A%60%60majority%20vote%22%20objective%20function.%20We%20demonstrate%20that%20this%20function%20is%0Abounded%20by%20the%20mutual%20information%20between%20student%20and%20teachers%27%20embeddings%2C%0Aleading%20to%20a%20task-agnostic%20distillation%20loss%20that%20eliminates%20dependence%20on%0Atask-specific%20labels%20or%20prior%20knowledge.%20Our%20evaluations%20across%20text%2C%20vision%0Amodels%2C%20and%20molecular%20modeling%20show%20that%20our%20method%20effectively%20leverages%0Ateacher%20diversity%2C%20resulting%20in%20representations%20enabling%20better%20performance%20for%0Aa%20wide%20range%20of%20downstream%20tasks%20such%20as%20classification%2C%20clustering%2C%20or%0Aregression.%20Additionally%2C%20we%20train%20and%20release%20state-of-the-art%20embedding%0Amodels%2C%20enhancing%20downstream%20performance%20in%20various%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Task-Agnostic%2520Representations%2520through%2520Multi-Teacher%250A%2520%2520Distillation%26entry.906535625%3DPhilippe%2520Formont%2520and%2520Maxime%2520Darrin%2520and%2520Banafsheh%2520Karimian%2520and%2520Jackie%2520CK%2520Cheung%2520and%2520Eric%2520Granger%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Mohammadhadi%2520Shateri%2520and%2520Pablo%2520Piantanida%26entry.1292438233%3D%2520%2520Casting%2520complex%2520inputs%2520into%2520tractable%2520representations%2520is%2520a%2520critical%2520step%250Aacross%2520various%2520fields.%2520Diverse%2520embedding%2520models%2520emerge%2520from%2520differences%2520in%250Aarchitectures%252C%2520loss%2520functions%252C%2520input%2520modalities%2520and%2520datasets%252C%2520each%2520capturing%250Aunique%2520aspects%2520of%2520the%2520input.%2520Multi-teacher%2520distillation%2520leverages%2520this%250Adiversity%2520to%2520enrich%2520representations%2520but%2520often%2520remains%2520tailored%2520to%2520specific%250Atasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520task-agnostic%2520framework%2520based%2520on%2520a%250A%2560%2560majority%2520vote%2522%2520objective%2520function.%2520We%2520demonstrate%2520that%2520this%2520function%2520is%250Abounded%2520by%2520the%2520mutual%2520information%2520between%2520student%2520and%2520teachers%2527%2520embeddings%252C%250Aleading%2520to%2520a%2520task-agnostic%2520distillation%2520loss%2520that%2520eliminates%2520dependence%2520on%250Atask-specific%2520labels%2520or%2520prior%2520knowledge.%2520Our%2520evaluations%2520across%2520text%252C%2520vision%250Amodels%252C%2520and%2520molecular%2520modeling%2520show%2520that%2520our%2520method%2520effectively%2520leverages%250Ateacher%2520diversity%252C%2520resulting%2520in%2520representations%2520enabling%2520better%2520performance%2520for%250Aa%2520wide%2520range%2520of%2520downstream%2520tasks%2520such%2520as%2520classification%252C%2520clustering%252C%2520or%250Aregression.%2520Additionally%252C%2520we%2520train%2520and%2520release%2520state-of-the-art%2520embedding%250Amodels%252C%2520enhancing%2520downstream%2520performance%2520in%2520various%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Task-Agnostic%20Representations%20through%20Multi-Teacher%0A%20%20Distillation&entry.906535625=Philippe%20Formont%20and%20Maxime%20Darrin%20and%20Banafsheh%20Karimian%20and%20Jackie%20CK%20Cheung%20and%20Eric%20Granger%20and%20Ismail%20Ben%20Ayed%20and%20Mohammadhadi%20Shateri%20and%20Pablo%20Piantanida&entry.1292438233=%20%20Casting%20complex%20inputs%20into%20tractable%20representations%20is%20a%20critical%20step%0Aacross%20various%20fields.%20Diverse%20embedding%20models%20emerge%20from%20differences%20in%0Aarchitectures%2C%20loss%20functions%2C%20input%20modalities%20and%20datasets%2C%20each%20capturing%0Aunique%20aspects%20of%20the%20input.%20Multi-teacher%20distillation%20leverages%20this%0Adiversity%20to%20enrich%20representations%20but%20often%20remains%20tailored%20to%20specific%0Atasks.%20In%20this%20paper%2C%20we%20introduce%20a%20task-agnostic%20framework%20based%20on%20a%0A%60%60majority%20vote%22%20objective%20function.%20We%20demonstrate%20that%20this%20function%20is%0Abounded%20by%20the%20mutual%20information%20between%20student%20and%20teachers%27%20embeddings%2C%0Aleading%20to%20a%20task-agnostic%20distillation%20loss%20that%20eliminates%20dependence%20on%0Atask-specific%20labels%20or%20prior%20knowledge.%20Our%20evaluations%20across%20text%2C%20vision%0Amodels%2C%20and%20molecular%20modeling%20show%20that%20our%20method%20effectively%20leverages%0Ateacher%20diversity%2C%20resulting%20in%20representations%20enabling%20better%20performance%20for%0Aa%20wide%20range%20of%20downstream%20tasks%20such%20as%20classification%2C%20clustering%2C%20or%0Aregression.%20Additionally%2C%20we%20train%20and%20release%20state-of-the-art%20embedding%0Amodels%2C%20enhancing%20downstream%20performance%20in%20various%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18680v1&entry.124074799=Read"},
{"title": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of\n  Coreference Resolution in LLMs", "author": "Amber Shore and Russell Scheinberg and Ameeta Agrawal and So Young Lee", "abstract": "  Large Language Models (LLMs) are intended to reflect human linguistic\ncompetencies. But humans have access to a broad and embodied context, which is\nkey in detecting and resolving linguistic ambiguities, even in isolated text\nspans. A foundational case of semantic ambiguity is found in the task of\ncoreference resolution: how is a pronoun related to an earlier person mention?\nThis capability is implicit in nearly every downstream task, and the presence\nof ambiguity at this level can alter performance significantly. We show that\nLLMs can achieve good performance with minimal prompting in both coreference\ndisambiguation and the detection of ambiguity in coreference, however, they\ncannot do both at the same time. We present the CORRECT-DETECT trade-off:\nthough models have both capabilities and deploy them implicitly, successful\nperformance balancing these two abilities remains elusive.\n", "link": "http://arxiv.org/abs/2509.14456v2", "date": "2025-10-21", "relevancy": 2.137, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correct-Detect%3A%20Balancing%20Performance%20and%20Ambiguity%20Through%20the%20Lens%20of%0A%20%20Coreference%20Resolution%20in%20LLMs&body=Title%3A%20Correct-Detect%3A%20Balancing%20Performance%20and%20Ambiguity%20Through%20the%20Lens%20of%0A%20%20Coreference%20Resolution%20in%20LLMs%0AAuthor%3A%20Amber%20Shore%20and%20Russell%20Scheinberg%20and%20Ameeta%20Agrawal%20and%20So%20Young%20Lee%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20intended%20to%20reflect%20human%20linguistic%0Acompetencies.%20But%20humans%20have%20access%20to%20a%20broad%20and%20embodied%20context%2C%20which%20is%0Akey%20in%20detecting%20and%20resolving%20linguistic%20ambiguities%2C%20even%20in%20isolated%20text%0Aspans.%20A%20foundational%20case%20of%20semantic%20ambiguity%20is%20found%20in%20the%20task%20of%0Acoreference%20resolution%3A%20how%20is%20a%20pronoun%20related%20to%20an%20earlier%20person%20mention%3F%0AThis%20capability%20is%20implicit%20in%20nearly%20every%20downstream%20task%2C%20and%20the%20presence%0Aof%20ambiguity%20at%20this%20level%20can%20alter%20performance%20significantly.%20We%20show%20that%0ALLMs%20can%20achieve%20good%20performance%20with%20minimal%20prompting%20in%20both%20coreference%0Adisambiguation%20and%20the%20detection%20of%20ambiguity%20in%20coreference%2C%20however%2C%20they%0Acannot%20do%20both%20at%20the%20same%20time.%20We%20present%20the%20CORRECT-DETECT%20trade-off%3A%0Athough%20models%20have%20both%20capabilities%20and%20deploy%20them%20implicitly%2C%20successful%0Aperformance%20balancing%20these%20two%20abilities%20remains%20elusive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrect-Detect%253A%2520Balancing%2520Performance%2520and%2520Ambiguity%2520Through%2520the%2520Lens%2520of%250A%2520%2520Coreference%2520Resolution%2520in%2520LLMs%26entry.906535625%3DAmber%2520Shore%2520and%2520Russell%2520Scheinberg%2520and%2520Ameeta%2520Agrawal%2520and%2520So%2520Young%2520Lee%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520intended%2520to%2520reflect%2520human%2520linguistic%250Acompetencies.%2520But%2520humans%2520have%2520access%2520to%2520a%2520broad%2520and%2520embodied%2520context%252C%2520which%2520is%250Akey%2520in%2520detecting%2520and%2520resolving%2520linguistic%2520ambiguities%252C%2520even%2520in%2520isolated%2520text%250Aspans.%2520A%2520foundational%2520case%2520of%2520semantic%2520ambiguity%2520is%2520found%2520in%2520the%2520task%2520of%250Acoreference%2520resolution%253A%2520how%2520is%2520a%2520pronoun%2520related%2520to%2520an%2520earlier%2520person%2520mention%253F%250AThis%2520capability%2520is%2520implicit%2520in%2520nearly%2520every%2520downstream%2520task%252C%2520and%2520the%2520presence%250Aof%2520ambiguity%2520at%2520this%2520level%2520can%2520alter%2520performance%2520significantly.%2520We%2520show%2520that%250ALLMs%2520can%2520achieve%2520good%2520performance%2520with%2520minimal%2520prompting%2520in%2520both%2520coreference%250Adisambiguation%2520and%2520the%2520detection%2520of%2520ambiguity%2520in%2520coreference%252C%2520however%252C%2520they%250Acannot%2520do%2520both%2520at%2520the%2520same%2520time.%2520We%2520present%2520the%2520CORRECT-DETECT%2520trade-off%253A%250Athough%2520models%2520have%2520both%2520capabilities%2520and%2520deploy%2520them%2520implicitly%252C%2520successful%250Aperformance%2520balancing%2520these%2520two%2520abilities%2520remains%2520elusive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correct-Detect%3A%20Balancing%20Performance%20and%20Ambiguity%20Through%20the%20Lens%20of%0A%20%20Coreference%20Resolution%20in%20LLMs&entry.906535625=Amber%20Shore%20and%20Russell%20Scheinberg%20and%20Ameeta%20Agrawal%20and%20So%20Young%20Lee&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20intended%20to%20reflect%20human%20linguistic%0Acompetencies.%20But%20humans%20have%20access%20to%20a%20broad%20and%20embodied%20context%2C%20which%20is%0Akey%20in%20detecting%20and%20resolving%20linguistic%20ambiguities%2C%20even%20in%20isolated%20text%0Aspans.%20A%20foundational%20case%20of%20semantic%20ambiguity%20is%20found%20in%20the%20task%20of%0Acoreference%20resolution%3A%20how%20is%20a%20pronoun%20related%20to%20an%20earlier%20person%20mention%3F%0AThis%20capability%20is%20implicit%20in%20nearly%20every%20downstream%20task%2C%20and%20the%20presence%0Aof%20ambiguity%20at%20this%20level%20can%20alter%20performance%20significantly.%20We%20show%20that%0ALLMs%20can%20achieve%20good%20performance%20with%20minimal%20prompting%20in%20both%20coreference%0Adisambiguation%20and%20the%20detection%20of%20ambiguity%20in%20coreference%2C%20however%2C%20they%0Acannot%20do%20both%20at%20the%20same%20time.%20We%20present%20the%20CORRECT-DETECT%20trade-off%3A%0Athough%20models%20have%20both%20capabilities%20and%20deploy%20them%20implicitly%2C%20successful%0Aperformance%20balancing%20these%20two%20abilities%20remains%20elusive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14456v2&entry.124074799=Read"},
{"title": "\u03b5-Seg: Sparsely Supervised Semantic Segmentation of Microscopy\n  Data", "author": "Sheida Rahnamai Kordasiabi and Damian Dalle Nogare and Florian Jug", "abstract": "  Semantic segmentation of electron microscopy (EM) images of biological\nsamples remains a challenge in the life sciences. EM data captures details of\nbiological structures, sometimes with such complexity that even human observers\ncan find it overwhelming. We introduce {\\epsilon}-Seg, a method based on\nhierarchical variational autoencoders (HVAEs), employing center-region masking,\nsparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior,\nand clustering-free label prediction. Center-region masking and the inpainting\nloss encourage the model to learn robust and representative embeddings to\ndistinguish the desired classes, even if training labels are sparse (0.05% of\nthe total image data or less). For optimal performance, we employ CL and a GMM\nprior to shape the latent space of the HVAE such that encoded input patches\ntend to cluster wrt. the semantic classes we wish to distinguish. Finally,\ninstead of clustering latent embeddings for semantic segmentation, we propose a\nMLP semantic segmentation head to directly predict class labels from latent\nembeddings. We show empirical results of {\\epsilon}-Seg and baseline methods on\n2 dense EM datasets of biological tissues and demonstrate the applicability of\nour method also on fluorescence microscopy data. Our results show that\n{\\epsilon}-Seg is capable of achieving competitive sparsely-supervised\nsegmentation results on complex biological image data, even if only limited\namounts of training labels are available.\n", "link": "http://arxiv.org/abs/2510.18637v1", "date": "2025-10-21", "relevancy": 2.1345, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5577}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%CE%B5-Seg%3A%20Sparsely%20Supervised%20Semantic%20Segmentation%20of%20Microscopy%0A%20%20Data&body=Title%3A%20%CE%B5-Seg%3A%20Sparsely%20Supervised%20Semantic%20Segmentation%20of%20Microscopy%0A%20%20Data%0AAuthor%3A%20Sheida%20Rahnamai%20Kordasiabi%20and%20Damian%20Dalle%20Nogare%20and%20Florian%20Jug%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20electron%20microscopy%20%28EM%29%20images%20of%20biological%0Asamples%20remains%20a%20challenge%20in%20the%20life%20sciences.%20EM%20data%20captures%20details%20of%0Abiological%20structures%2C%20sometimes%20with%20such%20complexity%20that%20even%20human%20observers%0Acan%20find%20it%20overwhelming.%20We%20introduce%20%7B%5Cepsilon%7D-Seg%2C%20a%20method%20based%20on%0Ahierarchical%20variational%20autoencoders%20%28HVAEs%29%2C%20employing%20center-region%20masking%2C%0Asparse%20label%20contrastive%20learning%20%28CL%29%2C%20a%20Gaussian%20mixture%20model%20%28GMM%29%20prior%2C%0Aand%20clustering-free%20label%20prediction.%20Center-region%20masking%20and%20the%20inpainting%0Aloss%20encourage%20the%20model%20to%20learn%20robust%20and%20representative%20embeddings%20to%0Adistinguish%20the%20desired%20classes%2C%20even%20if%20training%20labels%20are%20sparse%20%280.05%25%20of%0Athe%20total%20image%20data%20or%20less%29.%20For%20optimal%20performance%2C%20we%20employ%20CL%20and%20a%20GMM%0Aprior%20to%20shape%20the%20latent%20space%20of%20the%20HVAE%20such%20that%20encoded%20input%20patches%0Atend%20to%20cluster%20wrt.%20the%20semantic%20classes%20we%20wish%20to%20distinguish.%20Finally%2C%0Ainstead%20of%20clustering%20latent%20embeddings%20for%20semantic%20segmentation%2C%20we%20propose%20a%0AMLP%20semantic%20segmentation%20head%20to%20directly%20predict%20class%20labels%20from%20latent%0Aembeddings.%20We%20show%20empirical%20results%20of%20%7B%5Cepsilon%7D-Seg%20and%20baseline%20methods%20on%0A2%20dense%20EM%20datasets%20of%20biological%20tissues%20and%20demonstrate%20the%20applicability%20of%0Aour%20method%20also%20on%20fluorescence%20microscopy%20data.%20Our%20results%20show%20that%0A%7B%5Cepsilon%7D-Seg%20is%20capable%20of%20achieving%20competitive%20sparsely-supervised%0Asegmentation%20results%20on%20complex%20biological%20image%20data%2C%20even%20if%20only%20limited%0Aamounts%20of%20training%20labels%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%25CE%25B5-Seg%253A%2520Sparsely%2520Supervised%2520Semantic%2520Segmentation%2520of%2520Microscopy%250A%2520%2520Data%26entry.906535625%3DSheida%2520Rahnamai%2520Kordasiabi%2520and%2520Damian%2520Dalle%2520Nogare%2520and%2520Florian%2520Jug%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520electron%2520microscopy%2520%2528EM%2529%2520images%2520of%2520biological%250Asamples%2520remains%2520a%2520challenge%2520in%2520the%2520life%2520sciences.%2520EM%2520data%2520captures%2520details%2520of%250Abiological%2520structures%252C%2520sometimes%2520with%2520such%2520complexity%2520that%2520even%2520human%2520observers%250Acan%2520find%2520it%2520overwhelming.%2520We%2520introduce%2520%257B%255Cepsilon%257D-Seg%252C%2520a%2520method%2520based%2520on%250Ahierarchical%2520variational%2520autoencoders%2520%2528HVAEs%2529%252C%2520employing%2520center-region%2520masking%252C%250Asparse%2520label%2520contrastive%2520learning%2520%2528CL%2529%252C%2520a%2520Gaussian%2520mixture%2520model%2520%2528GMM%2529%2520prior%252C%250Aand%2520clustering-free%2520label%2520prediction.%2520Center-region%2520masking%2520and%2520the%2520inpainting%250Aloss%2520encourage%2520the%2520model%2520to%2520learn%2520robust%2520and%2520representative%2520embeddings%2520to%250Adistinguish%2520the%2520desired%2520classes%252C%2520even%2520if%2520training%2520labels%2520are%2520sparse%2520%25280.05%2525%2520of%250Athe%2520total%2520image%2520data%2520or%2520less%2529.%2520For%2520optimal%2520performance%252C%2520we%2520employ%2520CL%2520and%2520a%2520GMM%250Aprior%2520to%2520shape%2520the%2520latent%2520space%2520of%2520the%2520HVAE%2520such%2520that%2520encoded%2520input%2520patches%250Atend%2520to%2520cluster%2520wrt.%2520the%2520semantic%2520classes%2520we%2520wish%2520to%2520distinguish.%2520Finally%252C%250Ainstead%2520of%2520clustering%2520latent%2520embeddings%2520for%2520semantic%2520segmentation%252C%2520we%2520propose%2520a%250AMLP%2520semantic%2520segmentation%2520head%2520to%2520directly%2520predict%2520class%2520labels%2520from%2520latent%250Aembeddings.%2520We%2520show%2520empirical%2520results%2520of%2520%257B%255Cepsilon%257D-Seg%2520and%2520baseline%2520methods%2520on%250A2%2520dense%2520EM%2520datasets%2520of%2520biological%2520tissues%2520and%2520demonstrate%2520the%2520applicability%2520of%250Aour%2520method%2520also%2520on%2520fluorescence%2520microscopy%2520data.%2520Our%2520results%2520show%2520that%250A%257B%255Cepsilon%257D-Seg%2520is%2520capable%2520of%2520achieving%2520competitive%2520sparsely-supervised%250Asegmentation%2520results%2520on%2520complex%2520biological%2520image%2520data%252C%2520even%2520if%2520only%2520limited%250Aamounts%2520of%2520training%2520labels%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%CE%B5-Seg%3A%20Sparsely%20Supervised%20Semantic%20Segmentation%20of%20Microscopy%0A%20%20Data&entry.906535625=Sheida%20Rahnamai%20Kordasiabi%20and%20Damian%20Dalle%20Nogare%20and%20Florian%20Jug&entry.1292438233=%20%20Semantic%20segmentation%20of%20electron%20microscopy%20%28EM%29%20images%20of%20biological%0Asamples%20remains%20a%20challenge%20in%20the%20life%20sciences.%20EM%20data%20captures%20details%20of%0Abiological%20structures%2C%20sometimes%20with%20such%20complexity%20that%20even%20human%20observers%0Acan%20find%20it%20overwhelming.%20We%20introduce%20%7B%5Cepsilon%7D-Seg%2C%20a%20method%20based%20on%0Ahierarchical%20variational%20autoencoders%20%28HVAEs%29%2C%20employing%20center-region%20masking%2C%0Asparse%20label%20contrastive%20learning%20%28CL%29%2C%20a%20Gaussian%20mixture%20model%20%28GMM%29%20prior%2C%0Aand%20clustering-free%20label%20prediction.%20Center-region%20masking%20and%20the%20inpainting%0Aloss%20encourage%20the%20model%20to%20learn%20robust%20and%20representative%20embeddings%20to%0Adistinguish%20the%20desired%20classes%2C%20even%20if%20training%20labels%20are%20sparse%20%280.05%25%20of%0Athe%20total%20image%20data%20or%20less%29.%20For%20optimal%20performance%2C%20we%20employ%20CL%20and%20a%20GMM%0Aprior%20to%20shape%20the%20latent%20space%20of%20the%20HVAE%20such%20that%20encoded%20input%20patches%0Atend%20to%20cluster%20wrt.%20the%20semantic%20classes%20we%20wish%20to%20distinguish.%20Finally%2C%0Ainstead%20of%20clustering%20latent%20embeddings%20for%20semantic%20segmentation%2C%20we%20propose%20a%0AMLP%20semantic%20segmentation%20head%20to%20directly%20predict%20class%20labels%20from%20latent%0Aembeddings.%20We%20show%20empirical%20results%20of%20%7B%5Cepsilon%7D-Seg%20and%20baseline%20methods%20on%0A2%20dense%20EM%20datasets%20of%20biological%20tissues%20and%20demonstrate%20the%20applicability%20of%0Aour%20method%20also%20on%20fluorescence%20microscopy%20data.%20Our%20results%20show%20that%0A%7B%5Cepsilon%7D-Seg%20is%20capable%20of%20achieving%20competitive%20sparsely-supervised%0Asegmentation%20results%20on%20complex%20biological%20image%20data%2C%20even%20if%20only%20limited%0Aamounts%20of%20training%20labels%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18637v1&entry.124074799=Read"},
{"title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via\n  Sequential Problem Solving", "author": "Shihan Dou and Ming Zhang and Chenhao Huang and Jiayi Chen and Feng Chen and Shichun Liu and Yan Liu and Chenxiao Liu and Cheng Zhong and Zongzhang Zhang and Tao Gui and Chao Xin and Chengzhi Wei and Lin Yan and Yonghui Wu and Qi Zhang and Xuanjing Huang", "abstract": "  We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.\n", "link": "http://arxiv.org/abs/2506.02672v3", "date": "2025-10-21", "relevancy": 2.1333, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvaLearn%3A%20Quantifying%20the%20Learning%20Capability%20and%20Efficiency%20of%20LLMs%20via%0A%20%20Sequential%20Problem%20Solving&body=Title%3A%20EvaLearn%3A%20Quantifying%20the%20Learning%20Capability%20and%20Efficiency%20of%20LLMs%20via%0A%20%20Sequential%20Problem%20Solving%0AAuthor%3A%20Shihan%20Dou%20and%20Ming%20Zhang%20and%20Chenhao%20Huang%20and%20Jiayi%20Chen%20and%20Feng%20Chen%20and%20Shichun%20Liu%20and%20Yan%20Liu%20and%20Chenxiao%20Liu%20and%20Cheng%20Zhong%20and%20Zongzhang%20Zhang%20and%20Tao%20Gui%20and%20Chao%20Xin%20and%20Chengzhi%20Wei%20and%20Lin%20Yan%20and%20Yonghui%20Wu%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20We%20introduce%20EvaLearn%2C%20a%20pioneering%20benchmark%20designed%20to%20evaluate%20large%0Alanguage%20models%20%28LLMs%29%20on%20their%20learning%20capability%20and%20efficiency%20in%0Achallenging%20tasks%2C%20a%20critical%2C%20yet%20underexplored%20aspect%20of%20model%20potential.%0AEvaLearn%20contains%20648%20challenging%20problems%20across%20six%20task%20types%2C%20grouped%20into%0A182%20sequences%2C%20each%20sequence%20dedicated%20to%20one%20task%20type.%20Diverging%20from%20most%0Aexisting%20benchmarks%20that%20evaluate%20models%20in%20parallel%2C%20EvaLearn%20requires%20models%0Ato%20solve%20problems%20sequentially%2C%20allowing%20them%20to%20leverage%20the%20experience%20gained%0Afrom%20previous%20solutions.%20EvaLearn%20provides%20five%20comprehensive%20automated%20metrics%0Ato%20evaluate%20models%20and%20quantify%20their%20learning%20capability%20and%20efficiency.%20We%0Aextensively%20benchmark%20nine%20frontier%20models%20and%20observe%20varied%20performance%0Aprofiles%3A%20some%20models%2C%20such%20as%20Claude-3.7-sonnet%2C%20start%20with%20moderate%20initial%0Aperformance%20but%20exhibit%20strong%20learning%20ability%2C%20while%20some%20models%20struggle%20to%0Abenefit%20from%20experience%20and%20may%20even%20show%20negative%20transfer.%20Moreover%2C%20we%0Ainvestigate%20model%20performance%20under%20two%20learning%20settings%20and%20find%20that%0Ainstance-level%20rubrics%20and%20teacher-model%20feedback%20further%20facilitate%20model%0Alearning.%20Importantly%2C%20we%20observe%20that%20current%20LLMs%20with%20stronger%20static%0Aabilities%20do%20not%20show%20a%20clear%20advantage%20in%20learning%20capability%20across%20all%0Atasks%2C%20highlighting%20that%20EvaLearn%20evaluates%20a%20new%20dimension%20of%20model%0Aperformance.%20We%20hope%20EvaLearn%20provides%20a%20novel%20evaluation%20perspective%20for%0Aassessing%20LLM%20potential%20and%20understanding%20the%20gap%20between%20models%20and%20human%0Acapabilities%2C%20promoting%20the%20development%20of%20deeper%20and%20more%20dynamic%20evaluation%0Aapproaches.%20All%20datasets%2C%20the%20automatic%20evaluation%20framework%2C%20and%20the%20results%0Astudied%20in%20this%20paper%20are%20available%20at%20the%20GitHub%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02672v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaLearn%253A%2520Quantifying%2520the%2520Learning%2520Capability%2520and%2520Efficiency%2520of%2520LLMs%2520via%250A%2520%2520Sequential%2520Problem%2520Solving%26entry.906535625%3DShihan%2520Dou%2520and%2520Ming%2520Zhang%2520and%2520Chenhao%2520Huang%2520and%2520Jiayi%2520Chen%2520and%2520Feng%2520Chen%2520and%2520Shichun%2520Liu%2520and%2520Yan%2520Liu%2520and%2520Chenxiao%2520Liu%2520and%2520Cheng%2520Zhong%2520and%2520Zongzhang%2520Zhang%2520and%2520Tao%2520Gui%2520and%2520Chao%2520Xin%2520and%2520Chengzhi%2520Wei%2520and%2520Lin%2520Yan%2520and%2520Yonghui%2520Wu%2520and%2520Qi%2520Zhang%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520We%2520introduce%2520EvaLearn%252C%2520a%2520pioneering%2520benchmark%2520designed%2520to%2520evaluate%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520on%2520their%2520learning%2520capability%2520and%2520efficiency%2520in%250Achallenging%2520tasks%252C%2520a%2520critical%252C%2520yet%2520underexplored%2520aspect%2520of%2520model%2520potential.%250AEvaLearn%2520contains%2520648%2520challenging%2520problems%2520across%2520six%2520task%2520types%252C%2520grouped%2520into%250A182%2520sequences%252C%2520each%2520sequence%2520dedicated%2520to%2520one%2520task%2520type.%2520Diverging%2520from%2520most%250Aexisting%2520benchmarks%2520that%2520evaluate%2520models%2520in%2520parallel%252C%2520EvaLearn%2520requires%2520models%250Ato%2520solve%2520problems%2520sequentially%252C%2520allowing%2520them%2520to%2520leverage%2520the%2520experience%2520gained%250Afrom%2520previous%2520solutions.%2520EvaLearn%2520provides%2520five%2520comprehensive%2520automated%2520metrics%250Ato%2520evaluate%2520models%2520and%2520quantify%2520their%2520learning%2520capability%2520and%2520efficiency.%2520We%250Aextensively%2520benchmark%2520nine%2520frontier%2520models%2520and%2520observe%2520varied%2520performance%250Aprofiles%253A%2520some%2520models%252C%2520such%2520as%2520Claude-3.7-sonnet%252C%2520start%2520with%2520moderate%2520initial%250Aperformance%2520but%2520exhibit%2520strong%2520learning%2520ability%252C%2520while%2520some%2520models%2520struggle%2520to%250Abenefit%2520from%2520experience%2520and%2520may%2520even%2520show%2520negative%2520transfer.%2520Moreover%252C%2520we%250Ainvestigate%2520model%2520performance%2520under%2520two%2520learning%2520settings%2520and%2520find%2520that%250Ainstance-level%2520rubrics%2520and%2520teacher-model%2520feedback%2520further%2520facilitate%2520model%250Alearning.%2520Importantly%252C%2520we%2520observe%2520that%2520current%2520LLMs%2520with%2520stronger%2520static%250Aabilities%2520do%2520not%2520show%2520a%2520clear%2520advantage%2520in%2520learning%2520capability%2520across%2520all%250Atasks%252C%2520highlighting%2520that%2520EvaLearn%2520evaluates%2520a%2520new%2520dimension%2520of%2520model%250Aperformance.%2520We%2520hope%2520EvaLearn%2520provides%2520a%2520novel%2520evaluation%2520perspective%2520for%250Aassessing%2520LLM%2520potential%2520and%2520understanding%2520the%2520gap%2520between%2520models%2520and%2520human%250Acapabilities%252C%2520promoting%2520the%2520development%2520of%2520deeper%2520and%2520more%2520dynamic%2520evaluation%250Aapproaches.%2520All%2520datasets%252C%2520the%2520automatic%2520evaluation%2520framework%252C%2520and%2520the%2520results%250Astudied%2520in%2520this%2520paper%2520are%2520available%2520at%2520the%2520GitHub%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02672v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvaLearn%3A%20Quantifying%20the%20Learning%20Capability%20and%20Efficiency%20of%20LLMs%20via%0A%20%20Sequential%20Problem%20Solving&entry.906535625=Shihan%20Dou%20and%20Ming%20Zhang%20and%20Chenhao%20Huang%20and%20Jiayi%20Chen%20and%20Feng%20Chen%20and%20Shichun%20Liu%20and%20Yan%20Liu%20and%20Chenxiao%20Liu%20and%20Cheng%20Zhong%20and%20Zongzhang%20Zhang%20and%20Tao%20Gui%20and%20Chao%20Xin%20and%20Chengzhi%20Wei%20and%20Lin%20Yan%20and%20Yonghui%20Wu%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang&entry.1292438233=%20%20We%20introduce%20EvaLearn%2C%20a%20pioneering%20benchmark%20designed%20to%20evaluate%20large%0Alanguage%20models%20%28LLMs%29%20on%20their%20learning%20capability%20and%20efficiency%20in%0Achallenging%20tasks%2C%20a%20critical%2C%20yet%20underexplored%20aspect%20of%20model%20potential.%0AEvaLearn%20contains%20648%20challenging%20problems%20across%20six%20task%20types%2C%20grouped%20into%0A182%20sequences%2C%20each%20sequence%20dedicated%20to%20one%20task%20type.%20Diverging%20from%20most%0Aexisting%20benchmarks%20that%20evaluate%20models%20in%20parallel%2C%20EvaLearn%20requires%20models%0Ato%20solve%20problems%20sequentially%2C%20allowing%20them%20to%20leverage%20the%20experience%20gained%0Afrom%20previous%20solutions.%20EvaLearn%20provides%20five%20comprehensive%20automated%20metrics%0Ato%20evaluate%20models%20and%20quantify%20their%20learning%20capability%20and%20efficiency.%20We%0Aextensively%20benchmark%20nine%20frontier%20models%20and%20observe%20varied%20performance%0Aprofiles%3A%20some%20models%2C%20such%20as%20Claude-3.7-sonnet%2C%20start%20with%20moderate%20initial%0Aperformance%20but%20exhibit%20strong%20learning%20ability%2C%20while%20some%20models%20struggle%20to%0Abenefit%20from%20experience%20and%20may%20even%20show%20negative%20transfer.%20Moreover%2C%20we%0Ainvestigate%20model%20performance%20under%20two%20learning%20settings%20and%20find%20that%0Ainstance-level%20rubrics%20and%20teacher-model%20feedback%20further%20facilitate%20model%0Alearning.%20Importantly%2C%20we%20observe%20that%20current%20LLMs%20with%20stronger%20static%0Aabilities%20do%20not%20show%20a%20clear%20advantage%20in%20learning%20capability%20across%20all%0Atasks%2C%20highlighting%20that%20EvaLearn%20evaluates%20a%20new%20dimension%20of%20model%0Aperformance.%20We%20hope%20EvaLearn%20provides%20a%20novel%20evaluation%20perspective%20for%0Aassessing%20LLM%20potential%20and%20understanding%20the%20gap%20between%20models%20and%20human%0Acapabilities%2C%20promoting%20the%20development%20of%20deeper%20and%20more%20dynamic%20evaluation%0Aapproaches.%20All%20datasets%2C%20the%20automatic%20evaluation%20framework%2C%20and%20the%20results%0Astudied%20in%20this%20paper%20are%20available%20at%20the%20GitHub%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02672v3&entry.124074799=Read"},
{"title": "Sharing the Load: Distributed Model-Predictive Control for Precise\n  Multi-Rover Cargo Transport", "author": "Alexander Krawciw and Sven Lilge and Luka Antonyshyn and Timothy D. Barfoot", "abstract": "  For autonomous cargo transportation, teams of mobile robots can provide more\noperational flexibility than a single large robot. In these scenarios,\nprecision in both inter-vehicle distance and path tracking is key. With this\nmotivation, we develop a distributed model-predictive controller (MPC) for\nmulti-vehicle cargo operations that builds on the precise path-tracking of\nlidar teach and repeat. To carry cargo, a following vehicle must maintain a\nEuclidean distance offset from a lead vehicle regardless of the path curvature.\nOur approach uses a shared map to localize the robots relative to each other\nwithout GNSS or direct observations. We compare our approach to a centralized\nMPC and a baseline approach that directly measures the inter-vehicle distance.\nThe distributed MPC shows equivalent nominal performance to the more complex\ncentralized MPC. Using a direct measurement of the relative distance between\nthe leader and follower shows improved tracking performance in close-range\nscenarios but struggles with long-range offsets. The operational flexibility\nprovided by distributing the computation makes it well suited for real\ndeployments. We evaluate four types of convoyed path trackers with over 10 km\nof driving in a coupled convoy. With convoys of two and three rovers, the\nproposed distributed MPC method works in real-time to allow map-based convoying\nto maintain maximum spacing within 20 cm of the target in various conditions.\n", "link": "http://arxiv.org/abs/2510.18766v1", "date": "2025-10-21", "relevancy": 2.1293, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5436}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharing%20the%20Load%3A%20Distributed%20Model-Predictive%20Control%20for%20Precise%0A%20%20Multi-Rover%20Cargo%20Transport&body=Title%3A%20Sharing%20the%20Load%3A%20Distributed%20Model-Predictive%20Control%20for%20Precise%0A%20%20Multi-Rover%20Cargo%20Transport%0AAuthor%3A%20Alexander%20Krawciw%20and%20Sven%20Lilge%20and%20Luka%20Antonyshyn%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20For%20autonomous%20cargo%20transportation%2C%20teams%20of%20mobile%20robots%20can%20provide%20more%0Aoperational%20flexibility%20than%20a%20single%20large%20robot.%20In%20these%20scenarios%2C%0Aprecision%20in%20both%20inter-vehicle%20distance%20and%20path%20tracking%20is%20key.%20With%20this%0Amotivation%2C%20we%20develop%20a%20distributed%20model-predictive%20controller%20%28MPC%29%20for%0Amulti-vehicle%20cargo%20operations%20that%20builds%20on%20the%20precise%20path-tracking%20of%0Alidar%20teach%20and%20repeat.%20To%20carry%20cargo%2C%20a%20following%20vehicle%20must%20maintain%20a%0AEuclidean%20distance%20offset%20from%20a%20lead%20vehicle%20regardless%20of%20the%20path%20curvature.%0AOur%20approach%20uses%20a%20shared%20map%20to%20localize%20the%20robots%20relative%20to%20each%20other%0Awithout%20GNSS%20or%20direct%20observations.%20We%20compare%20our%20approach%20to%20a%20centralized%0AMPC%20and%20a%20baseline%20approach%20that%20directly%20measures%20the%20inter-vehicle%20distance.%0AThe%20distributed%20MPC%20shows%20equivalent%20nominal%20performance%20to%20the%20more%20complex%0Acentralized%20MPC.%20Using%20a%20direct%20measurement%20of%20the%20relative%20distance%20between%0Athe%20leader%20and%20follower%20shows%20improved%20tracking%20performance%20in%20close-range%0Ascenarios%20but%20struggles%20with%20long-range%20offsets.%20The%20operational%20flexibility%0Aprovided%20by%20distributing%20the%20computation%20makes%20it%20well%20suited%20for%20real%0Adeployments.%20We%20evaluate%20four%20types%20of%20convoyed%20path%20trackers%20with%20over%2010%20km%0Aof%20driving%20in%20a%20coupled%20convoy.%20With%20convoys%20of%20two%20and%20three%20rovers%2C%20the%0Aproposed%20distributed%20MPC%20method%20works%20in%20real-time%20to%20allow%20map-based%20convoying%0Ato%20maintain%20maximum%20spacing%20within%2020%20cm%20of%20the%20target%20in%20various%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharing%2520the%2520Load%253A%2520Distributed%2520Model-Predictive%2520Control%2520for%2520Precise%250A%2520%2520Multi-Rover%2520Cargo%2520Transport%26entry.906535625%3DAlexander%2520Krawciw%2520and%2520Sven%2520Lilge%2520and%2520Luka%2520Antonyshyn%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520For%2520autonomous%2520cargo%2520transportation%252C%2520teams%2520of%2520mobile%2520robots%2520can%2520provide%2520more%250Aoperational%2520flexibility%2520than%2520a%2520single%2520large%2520robot.%2520In%2520these%2520scenarios%252C%250Aprecision%2520in%2520both%2520inter-vehicle%2520distance%2520and%2520path%2520tracking%2520is%2520key.%2520With%2520this%250Amotivation%252C%2520we%2520develop%2520a%2520distributed%2520model-predictive%2520controller%2520%2528MPC%2529%2520for%250Amulti-vehicle%2520cargo%2520operations%2520that%2520builds%2520on%2520the%2520precise%2520path-tracking%2520of%250Alidar%2520teach%2520and%2520repeat.%2520To%2520carry%2520cargo%252C%2520a%2520following%2520vehicle%2520must%2520maintain%2520a%250AEuclidean%2520distance%2520offset%2520from%2520a%2520lead%2520vehicle%2520regardless%2520of%2520the%2520path%2520curvature.%250AOur%2520approach%2520uses%2520a%2520shared%2520map%2520to%2520localize%2520the%2520robots%2520relative%2520to%2520each%2520other%250Awithout%2520GNSS%2520or%2520direct%2520observations.%2520We%2520compare%2520our%2520approach%2520to%2520a%2520centralized%250AMPC%2520and%2520a%2520baseline%2520approach%2520that%2520directly%2520measures%2520the%2520inter-vehicle%2520distance.%250AThe%2520distributed%2520MPC%2520shows%2520equivalent%2520nominal%2520performance%2520to%2520the%2520more%2520complex%250Acentralized%2520MPC.%2520Using%2520a%2520direct%2520measurement%2520of%2520the%2520relative%2520distance%2520between%250Athe%2520leader%2520and%2520follower%2520shows%2520improved%2520tracking%2520performance%2520in%2520close-range%250Ascenarios%2520but%2520struggles%2520with%2520long-range%2520offsets.%2520The%2520operational%2520flexibility%250Aprovided%2520by%2520distributing%2520the%2520computation%2520makes%2520it%2520well%2520suited%2520for%2520real%250Adeployments.%2520We%2520evaluate%2520four%2520types%2520of%2520convoyed%2520path%2520trackers%2520with%2520over%252010%2520km%250Aof%2520driving%2520in%2520a%2520coupled%2520convoy.%2520With%2520convoys%2520of%2520two%2520and%2520three%2520rovers%252C%2520the%250Aproposed%2520distributed%2520MPC%2520method%2520works%2520in%2520real-time%2520to%2520allow%2520map-based%2520convoying%250Ato%2520maintain%2520maximum%2520spacing%2520within%252020%2520cm%2520of%2520the%2520target%2520in%2520various%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharing%20the%20Load%3A%20Distributed%20Model-Predictive%20Control%20for%20Precise%0A%20%20Multi-Rover%20Cargo%20Transport&entry.906535625=Alexander%20Krawciw%20and%20Sven%20Lilge%20and%20Luka%20Antonyshyn%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20For%20autonomous%20cargo%20transportation%2C%20teams%20of%20mobile%20robots%20can%20provide%20more%0Aoperational%20flexibility%20than%20a%20single%20large%20robot.%20In%20these%20scenarios%2C%0Aprecision%20in%20both%20inter-vehicle%20distance%20and%20path%20tracking%20is%20key.%20With%20this%0Amotivation%2C%20we%20develop%20a%20distributed%20model-predictive%20controller%20%28MPC%29%20for%0Amulti-vehicle%20cargo%20operations%20that%20builds%20on%20the%20precise%20path-tracking%20of%0Alidar%20teach%20and%20repeat.%20To%20carry%20cargo%2C%20a%20following%20vehicle%20must%20maintain%20a%0AEuclidean%20distance%20offset%20from%20a%20lead%20vehicle%20regardless%20of%20the%20path%20curvature.%0AOur%20approach%20uses%20a%20shared%20map%20to%20localize%20the%20robots%20relative%20to%20each%20other%0Awithout%20GNSS%20or%20direct%20observations.%20We%20compare%20our%20approach%20to%20a%20centralized%0AMPC%20and%20a%20baseline%20approach%20that%20directly%20measures%20the%20inter-vehicle%20distance.%0AThe%20distributed%20MPC%20shows%20equivalent%20nominal%20performance%20to%20the%20more%20complex%0Acentralized%20MPC.%20Using%20a%20direct%20measurement%20of%20the%20relative%20distance%20between%0Athe%20leader%20and%20follower%20shows%20improved%20tracking%20performance%20in%20close-range%0Ascenarios%20but%20struggles%20with%20long-range%20offsets.%20The%20operational%20flexibility%0Aprovided%20by%20distributing%20the%20computation%20makes%20it%20well%20suited%20for%20real%0Adeployments.%20We%20evaluate%20four%20types%20of%20convoyed%20path%20trackers%20with%20over%2010%20km%0Aof%20driving%20in%20a%20coupled%20convoy.%20With%20convoys%20of%20two%20and%20three%20rovers%2C%20the%0Aproposed%20distributed%20MPC%20method%20works%20in%20real-time%20to%20allow%20map-based%20convoying%0Ato%20maintain%20maximum%20spacing%20within%2020%20cm%20of%20the%20target%20in%20various%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18766v1&entry.124074799=Read"},
{"title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth\n  and Depth?", "author": "Yi Lu and Jianing Wang and Linsen Guo and Wei He and Hongyin Tang and Tao Gui and Xuanjing Huang and Xuezhi Cao and Wei Wang and Xunliang Cai", "abstract": "  Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought\n(CoT). However, existing benchmarks mainly focus on immediate, single-horizon\ntasks, failing to adequately evaluate models' ability to understand and respond\nto complex, long-horizon scenarios. To address this incomplete evaluation of\nLarge Reasoning Models (LRMs), we propose R-HORIZON, a method designed to\nstimulate long-horizon reasoning behaviors in LRMs through query composition.\nBased on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising\ncomplex multi-step reasoning tasks with interdependent problems that span long\nreasoning horizons. Through comprehensive evaluation of LRMs using the\nR-HORIZON benchmark, we find that even the most advanced LRMs suffer\nsignificant performance degradation. Our analysis reveals that LRMs exhibit\nlimited effective reasoning length and struggle to allocate thinking budget\nacross multiple problems appropriately. Recognizing these limitations, we use\nR-HORIZON to construct long-horizon reasoning data for reinforcement learning\nwith verified rewards (RLVR). Compared to training with single-horizon data,\nRLVR with R-HORIZON not only substantially improves performance on the\nmulti-horizon reasoning tasks, but also promotes accuracy on standard reasoning\ntasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as\na scalable, controllable, and low-cost paradigm for enhancing and evaluating\nthe long-horizon reasoning capabilities of LRMs.\n", "link": "http://arxiv.org/abs/2510.08189v2", "date": "2025-10-21", "relevancy": 2.1272, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R-Horizon%3A%20How%20Far%20Can%20Your%20Large%20Reasoning%20Model%20Really%20Go%20in%20Breadth%0A%20%20and%20Depth%3F&body=Title%3A%20R-Horizon%3A%20How%20Far%20Can%20Your%20Large%20Reasoning%20Model%20Really%20Go%20in%20Breadth%0A%20%20and%20Depth%3F%0AAuthor%3A%20Yi%20Lu%20and%20Jianing%20Wang%20and%20Linsen%20Guo%20and%20Wei%20He%20and%20Hongyin%20Tang%20and%20Tao%20Gui%20and%20Xuanjing%20Huang%20and%20Xuezhi%20Cao%20and%20Wei%20Wang%20and%20Xunliang%20Cai%0AAbstract%3A%20%20%20Recent%20trends%20in%20test-time%20scaling%20for%20reasoning%20models%20%28e.g.%2C%20OpenAI%20o1%2C%0ADeepSeek-R1%29%20have%20led%20to%20remarkable%20improvements%20through%20long%20Chain-of-Thought%0A%28CoT%29.%20However%2C%20existing%20benchmarks%20mainly%20focus%20on%20immediate%2C%20single-horizon%0Atasks%2C%20failing%20to%20adequately%20evaluate%20models%27%20ability%20to%20understand%20and%20respond%0Ato%20complex%2C%20long-horizon%20scenarios.%20To%20address%20this%20incomplete%20evaluation%20of%0ALarge%20Reasoning%20Models%20%28LRMs%29%2C%20we%20propose%20R-HORIZON%2C%20a%20method%20designed%20to%0Astimulate%20long-horizon%20reasoning%20behaviors%20in%20LRMs%20through%20query%20composition.%0ABased%20on%20R-HORIZON%2C%20we%20construct%20a%20long-horizon%20reasoning%20benchmark%2C%20comprising%0Acomplex%20multi-step%20reasoning%20tasks%20with%20interdependent%20problems%20that%20span%20long%0Areasoning%20horizons.%20Through%20comprehensive%20evaluation%20of%20LRMs%20using%20the%0AR-HORIZON%20benchmark%2C%20we%20find%20that%20even%20the%20most%20advanced%20LRMs%20suffer%0Asignificant%20performance%20degradation.%20Our%20analysis%20reveals%20that%20LRMs%20exhibit%0Alimited%20effective%20reasoning%20length%20and%20struggle%20to%20allocate%20thinking%20budget%0Aacross%20multiple%20problems%20appropriately.%20Recognizing%20these%20limitations%2C%20we%20use%0AR-HORIZON%20to%20construct%20long-horizon%20reasoning%20data%20for%20reinforcement%20learning%0Awith%20verified%20rewards%20%28RLVR%29.%20Compared%20to%20training%20with%20single-horizon%20data%2C%0ARLVR%20with%20R-HORIZON%20not%20only%20substantially%20improves%20performance%20on%20the%0Amulti-horizon%20reasoning%20tasks%2C%20but%20also%20promotes%20accuracy%20on%20standard%20reasoning%0Atasks%2C%20with%20an%20increase%20of%207.5%20on%20AIME2024.%20These%20results%20position%20R-HORIZON%20as%0Aa%20scalable%2C%20controllable%2C%20and%20low-cost%20paradigm%20for%20enhancing%20and%20evaluating%0Athe%20long-horizon%20reasoning%20capabilities%20of%20LRMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR-Horizon%253A%2520How%2520Far%2520Can%2520Your%2520Large%2520Reasoning%2520Model%2520Really%2520Go%2520in%2520Breadth%250A%2520%2520and%2520Depth%253F%26entry.906535625%3DYi%2520Lu%2520and%2520Jianing%2520Wang%2520and%2520Linsen%2520Guo%2520and%2520Wei%2520He%2520and%2520Hongyin%2520Tang%2520and%2520Tao%2520Gui%2520and%2520Xuanjing%2520Huang%2520and%2520Xuezhi%2520Cao%2520and%2520Wei%2520Wang%2520and%2520Xunliang%2520Cai%26entry.1292438233%3D%2520%2520Recent%2520trends%2520in%2520test-time%2520scaling%2520for%2520reasoning%2520models%2520%2528e.g.%252C%2520OpenAI%2520o1%252C%250ADeepSeek-R1%2529%2520have%2520led%2520to%2520remarkable%2520improvements%2520through%2520long%2520Chain-of-Thought%250A%2528CoT%2529.%2520However%252C%2520existing%2520benchmarks%2520mainly%2520focus%2520on%2520immediate%252C%2520single-horizon%250Atasks%252C%2520failing%2520to%2520adequately%2520evaluate%2520models%2527%2520ability%2520to%2520understand%2520and%2520respond%250Ato%2520complex%252C%2520long-horizon%2520scenarios.%2520To%2520address%2520this%2520incomplete%2520evaluation%2520of%250ALarge%2520Reasoning%2520Models%2520%2528LRMs%2529%252C%2520we%2520propose%2520R-HORIZON%252C%2520a%2520method%2520designed%2520to%250Astimulate%2520long-horizon%2520reasoning%2520behaviors%2520in%2520LRMs%2520through%2520query%2520composition.%250ABased%2520on%2520R-HORIZON%252C%2520we%2520construct%2520a%2520long-horizon%2520reasoning%2520benchmark%252C%2520comprising%250Acomplex%2520multi-step%2520reasoning%2520tasks%2520with%2520interdependent%2520problems%2520that%2520span%2520long%250Areasoning%2520horizons.%2520Through%2520comprehensive%2520evaluation%2520of%2520LRMs%2520using%2520the%250AR-HORIZON%2520benchmark%252C%2520we%2520find%2520that%2520even%2520the%2520most%2520advanced%2520LRMs%2520suffer%250Asignificant%2520performance%2520degradation.%2520Our%2520analysis%2520reveals%2520that%2520LRMs%2520exhibit%250Alimited%2520effective%2520reasoning%2520length%2520and%2520struggle%2520to%2520allocate%2520thinking%2520budget%250Aacross%2520multiple%2520problems%2520appropriately.%2520Recognizing%2520these%2520limitations%252C%2520we%2520use%250AR-HORIZON%2520to%2520construct%2520long-horizon%2520reasoning%2520data%2520for%2520reinforcement%2520learning%250Awith%2520verified%2520rewards%2520%2528RLVR%2529.%2520Compared%2520to%2520training%2520with%2520single-horizon%2520data%252C%250ARLVR%2520with%2520R-HORIZON%2520not%2520only%2520substantially%2520improves%2520performance%2520on%2520the%250Amulti-horizon%2520reasoning%2520tasks%252C%2520but%2520also%2520promotes%2520accuracy%2520on%2520standard%2520reasoning%250Atasks%252C%2520with%2520an%2520increase%2520of%25207.5%2520on%2520AIME2024.%2520These%2520results%2520position%2520R-HORIZON%2520as%250Aa%2520scalable%252C%2520controllable%252C%2520and%2520low-cost%2520paradigm%2520for%2520enhancing%2520and%2520evaluating%250Athe%2520long-horizon%2520reasoning%2520capabilities%2520of%2520LRMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R-Horizon%3A%20How%20Far%20Can%20Your%20Large%20Reasoning%20Model%20Really%20Go%20in%20Breadth%0A%20%20and%20Depth%3F&entry.906535625=Yi%20Lu%20and%20Jianing%20Wang%20and%20Linsen%20Guo%20and%20Wei%20He%20and%20Hongyin%20Tang%20and%20Tao%20Gui%20and%20Xuanjing%20Huang%20and%20Xuezhi%20Cao%20and%20Wei%20Wang%20and%20Xunliang%20Cai&entry.1292438233=%20%20Recent%20trends%20in%20test-time%20scaling%20for%20reasoning%20models%20%28e.g.%2C%20OpenAI%20o1%2C%0ADeepSeek-R1%29%20have%20led%20to%20remarkable%20improvements%20through%20long%20Chain-of-Thought%0A%28CoT%29.%20However%2C%20existing%20benchmarks%20mainly%20focus%20on%20immediate%2C%20single-horizon%0Atasks%2C%20failing%20to%20adequately%20evaluate%20models%27%20ability%20to%20understand%20and%20respond%0Ato%20complex%2C%20long-horizon%20scenarios.%20To%20address%20this%20incomplete%20evaluation%20of%0ALarge%20Reasoning%20Models%20%28LRMs%29%2C%20we%20propose%20R-HORIZON%2C%20a%20method%20designed%20to%0Astimulate%20long-horizon%20reasoning%20behaviors%20in%20LRMs%20through%20query%20composition.%0ABased%20on%20R-HORIZON%2C%20we%20construct%20a%20long-horizon%20reasoning%20benchmark%2C%20comprising%0Acomplex%20multi-step%20reasoning%20tasks%20with%20interdependent%20problems%20that%20span%20long%0Areasoning%20horizons.%20Through%20comprehensive%20evaluation%20of%20LRMs%20using%20the%0AR-HORIZON%20benchmark%2C%20we%20find%20that%20even%20the%20most%20advanced%20LRMs%20suffer%0Asignificant%20performance%20degradation.%20Our%20analysis%20reveals%20that%20LRMs%20exhibit%0Alimited%20effective%20reasoning%20length%20and%20struggle%20to%20allocate%20thinking%20budget%0Aacross%20multiple%20problems%20appropriately.%20Recognizing%20these%20limitations%2C%20we%20use%0AR-HORIZON%20to%20construct%20long-horizon%20reasoning%20data%20for%20reinforcement%20learning%0Awith%20verified%20rewards%20%28RLVR%29.%20Compared%20to%20training%20with%20single-horizon%20data%2C%0ARLVR%20with%20R-HORIZON%20not%20only%20substantially%20improves%20performance%20on%20the%0Amulti-horizon%20reasoning%20tasks%2C%20but%20also%20promotes%20accuracy%20on%20standard%20reasoning%0Atasks%2C%20with%20an%20increase%20of%207.5%20on%20AIME2024.%20These%20results%20position%20R-HORIZON%20as%0Aa%20scalable%2C%20controllable%2C%20and%20low-cost%20paradigm%20for%20enhancing%20and%20evaluating%0Athe%20long-horizon%20reasoning%20capabilities%20of%20LRMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08189v2&entry.124074799=Read"},
{"title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over\n  Time Series?", "author": "Zewen Liu and Juntong Ni and Xianfeng Tang and Max S. Y. Lau and Wenpeng Yin and Wei Jin", "abstract": "  Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.\n", "link": "http://arxiv.org/abs/2508.03963v3", "date": "2025-10-21", "relevancy": 2.1252, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Adequately%20Perform%20Symbolic%20Reasoning%20Over%0A%20%20Time%20Series%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Adequately%20Perform%20Symbolic%20Reasoning%20Over%0A%20%20Time%20Series%3F%0AAuthor%3A%20Zewen%20Liu%20and%20Juntong%20Ni%20and%20Xianfeng%20Tang%20and%20Max%20S.%20Y.%20Lau%20and%20Wenpeng%20Yin%20and%20Wei%20Jin%0AAbstract%3A%20%20%20Uncovering%20hidden%20symbolic%20laws%20from%20time%20series%20data%2C%20as%20an%20aspiration%0Adating%20back%20to%20Kepler%27s%20discovery%20of%20planetary%20motion%2C%20remains%20a%20core%20challenge%0Ain%20scientific%20discovery%20and%20artificial%20intelligence.%20While%20Large%20Language%0AModels%20show%20promise%20in%20structured%20reasoning%20tasks%2C%20their%20ability%20to%20infer%0Ainterpretable%2C%20context-aligned%20symbolic%20structures%20from%20time%20series%20data%20is%0Astill%20underexplored.%20To%20systematically%20evaluate%20this%20capability%2C%20we%20introduce%0ASymbolBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20assess%20symbolic%20reasoning%0Aover%20real-world%20time%20series%20across%20three%20tasks%3A%20multivariate%20symbolic%0Aregression%2C%20Boolean%20network%20inference%2C%20and%20causal%20discovery.%20Unlike%20prior%0Aefforts%20limited%20to%20simple%20algebraic%20equations%2C%20SymbolBench%20spans%20a%20diverse%20set%0Aof%20symbolic%20forms%20with%20varying%20complexity.%20We%20further%20propose%20a%20unified%0Aframework%20that%20integrates%20LLMs%20with%20genetic%20programming%20to%20form%20a%20closed-loop%0Asymbolic%20reasoning%20system%2C%20where%20LLMs%20act%20both%20as%20predictors%20and%20evaluators.%0AOur%20empirical%20results%20reveal%20key%20strengths%20and%20limitations%20of%20current%20models%2C%0Ahighlighting%20the%20importance%20of%20combining%20domain%20knowledge%2C%20context%20alignment%2C%0Aand%20reasoning%20structure%20to%20improve%20LLMs%20in%20automated%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03963v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Adequately%2520Perform%2520Symbolic%2520Reasoning%2520Over%250A%2520%2520Time%2520Series%253F%26entry.906535625%3DZewen%2520Liu%2520and%2520Juntong%2520Ni%2520and%2520Xianfeng%2520Tang%2520and%2520Max%2520S.%2520Y.%2520Lau%2520and%2520Wenpeng%2520Yin%2520and%2520Wei%2520Jin%26entry.1292438233%3D%2520%2520Uncovering%2520hidden%2520symbolic%2520laws%2520from%2520time%2520series%2520data%252C%2520as%2520an%2520aspiration%250Adating%2520back%2520to%2520Kepler%2527s%2520discovery%2520of%2520planetary%2520motion%252C%2520remains%2520a%2520core%2520challenge%250Ain%2520scientific%2520discovery%2520and%2520artificial%2520intelligence.%2520While%2520Large%2520Language%250AModels%2520show%2520promise%2520in%2520structured%2520reasoning%2520tasks%252C%2520their%2520ability%2520to%2520infer%250Ainterpretable%252C%2520context-aligned%2520symbolic%2520structures%2520from%2520time%2520series%2520data%2520is%250Astill%2520underexplored.%2520To%2520systematically%2520evaluate%2520this%2520capability%252C%2520we%2520introduce%250ASymbolBench%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520assess%2520symbolic%2520reasoning%250Aover%2520real-world%2520time%2520series%2520across%2520three%2520tasks%253A%2520multivariate%2520symbolic%250Aregression%252C%2520Boolean%2520network%2520inference%252C%2520and%2520causal%2520discovery.%2520Unlike%2520prior%250Aefforts%2520limited%2520to%2520simple%2520algebraic%2520equations%252C%2520SymbolBench%2520spans%2520a%2520diverse%2520set%250Aof%2520symbolic%2520forms%2520with%2520varying%2520complexity.%2520We%2520further%2520propose%2520a%2520unified%250Aframework%2520that%2520integrates%2520LLMs%2520with%2520genetic%2520programming%2520to%2520form%2520a%2520closed-loop%250Asymbolic%2520reasoning%2520system%252C%2520where%2520LLMs%2520act%2520both%2520as%2520predictors%2520and%2520evaluators.%250AOur%2520empirical%2520results%2520reveal%2520key%2520strengths%2520and%2520limitations%2520of%2520current%2520models%252C%250Ahighlighting%2520the%2520importance%2520of%2520combining%2520domain%2520knowledge%252C%2520context%2520alignment%252C%250Aand%2520reasoning%2520structure%2520to%2520improve%2520LLMs%2520in%2520automated%2520scientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03963v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Adequately%20Perform%20Symbolic%20Reasoning%20Over%0A%20%20Time%20Series%3F&entry.906535625=Zewen%20Liu%20and%20Juntong%20Ni%20and%20Xianfeng%20Tang%20and%20Max%20S.%20Y.%20Lau%20and%20Wenpeng%20Yin%20and%20Wei%20Jin&entry.1292438233=%20%20Uncovering%20hidden%20symbolic%20laws%20from%20time%20series%20data%2C%20as%20an%20aspiration%0Adating%20back%20to%20Kepler%27s%20discovery%20of%20planetary%20motion%2C%20remains%20a%20core%20challenge%0Ain%20scientific%20discovery%20and%20artificial%20intelligence.%20While%20Large%20Language%0AModels%20show%20promise%20in%20structured%20reasoning%20tasks%2C%20their%20ability%20to%20infer%0Ainterpretable%2C%20context-aligned%20symbolic%20structures%20from%20time%20series%20data%20is%0Astill%20underexplored.%20To%20systematically%20evaluate%20this%20capability%2C%20we%20introduce%0ASymbolBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20assess%20symbolic%20reasoning%0Aover%20real-world%20time%20series%20across%20three%20tasks%3A%20multivariate%20symbolic%0Aregression%2C%20Boolean%20network%20inference%2C%20and%20causal%20discovery.%20Unlike%20prior%0Aefforts%20limited%20to%20simple%20algebraic%20equations%2C%20SymbolBench%20spans%20a%20diverse%20set%0Aof%20symbolic%20forms%20with%20varying%20complexity.%20We%20further%20propose%20a%20unified%0Aframework%20that%20integrates%20LLMs%20with%20genetic%20programming%20to%20form%20a%20closed-loop%0Asymbolic%20reasoning%20system%2C%20where%20LLMs%20act%20both%20as%20predictors%20and%20evaluators.%0AOur%20empirical%20results%20reveal%20key%20strengths%20and%20limitations%20of%20current%20models%2C%0Ahighlighting%20the%20importance%20of%20combining%20domain%20knowledge%2C%20context%20alignment%2C%0Aand%20reasoning%20structure%20to%20improve%20LLMs%20in%20automated%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03963v3&entry.124074799=Read"},
{"title": "SAM 2++: Tracking Anything at Any Granularity", "author": "Jiaming Zhang and Cheng Liang and Yichun Yang and Chenkai Zeng and Yutao Cui and Xinwen Zhang and Xin Zhou and Kai Ma and Gangshan Wu and Limin Wang", "abstract": "  Video tracking aims at finding the specific target in subsequent frames given\nits initial state. Due to the varying granularity of target states across\ndifferent tasks, most existing trackers are tailored to a single task and\nheavily rely on custom-designed modules within the individual task, which\nlimits their generalization and leads to redundancy in both model design and\nparameters. To unify video tracking tasks, we present SAM 2++, a unified model\ntowards tracking at any granularity, including masks, boxes, and points. First,\nto extend target granularity, we design task-specific prompts to encode various\ntask inputs into general prompt embeddings, and a unified decoder to unify\ndiverse task results into a unified form pre-output. Next, to satisfy memory\nmatching, the core operation of tracking, we introduce a task-adaptive memory\nmechanism that unifies memory across different granularities. Finally, we\nintroduce a customized data engine to support tracking training at any\ngranularity, producing a large and diverse video tracking dataset with rich\nannotations at three granularities, termed Tracking-Any-Granularity, which\nrepresents a comprehensive resource for training and benchmarking on unified\ntracking. Comprehensive experiments on multiple benchmarks confirm that SAM 2++\nsets a new state of the art across diverse tracking tasks at different\ngranularities, establishing a unified and robust tracking framework.\n", "link": "http://arxiv.org/abs/2510.18822v1", "date": "2025-10-21", "relevancy": 2.1247, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5349}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5322}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM%202%2B%2B%3A%20Tracking%20Anything%20at%20Any%20Granularity&body=Title%3A%20SAM%202%2B%2B%3A%20Tracking%20Anything%20at%20Any%20Granularity%0AAuthor%3A%20Jiaming%20Zhang%20and%20Cheng%20Liang%20and%20Yichun%20Yang%20and%20Chenkai%20Zeng%20and%20Yutao%20Cui%20and%20Xinwen%20Zhang%20and%20Xin%20Zhou%20and%20Kai%20Ma%20and%20Gangshan%20Wu%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Video%20tracking%20aims%20at%20finding%20the%20specific%20target%20in%20subsequent%20frames%20given%0Aits%20initial%20state.%20Due%20to%20the%20varying%20granularity%20of%20target%20states%20across%0Adifferent%20tasks%2C%20most%20existing%20trackers%20are%20tailored%20to%20a%20single%20task%20and%0Aheavily%20rely%20on%20custom-designed%20modules%20within%20the%20individual%20task%2C%20which%0Alimits%20their%20generalization%20and%20leads%20to%20redundancy%20in%20both%20model%20design%20and%0Aparameters.%20To%20unify%20video%20tracking%20tasks%2C%20we%20present%20SAM%202%2B%2B%2C%20a%20unified%20model%0Atowards%20tracking%20at%20any%20granularity%2C%20including%20masks%2C%20boxes%2C%20and%20points.%20First%2C%0Ato%20extend%20target%20granularity%2C%20we%20design%20task-specific%20prompts%20to%20encode%20various%0Atask%20inputs%20into%20general%20prompt%20embeddings%2C%20and%20a%20unified%20decoder%20to%20unify%0Adiverse%20task%20results%20into%20a%20unified%20form%20pre-output.%20Next%2C%20to%20satisfy%20memory%0Amatching%2C%20the%20core%20operation%20of%20tracking%2C%20we%20introduce%20a%20task-adaptive%20memory%0Amechanism%20that%20unifies%20memory%20across%20different%20granularities.%20Finally%2C%20we%0Aintroduce%20a%20customized%20data%20engine%20to%20support%20tracking%20training%20at%20any%0Agranularity%2C%20producing%20a%20large%20and%20diverse%20video%20tracking%20dataset%20with%20rich%0Aannotations%20at%20three%20granularities%2C%20termed%20Tracking-Any-Granularity%2C%20which%0Arepresents%20a%20comprehensive%20resource%20for%20training%20and%20benchmarking%20on%20unified%0Atracking.%20Comprehensive%20experiments%20on%20multiple%20benchmarks%20confirm%20that%20SAM%202%2B%2B%0Asets%20a%20new%20state%20of%20the%20art%20across%20diverse%20tracking%20tasks%20at%20different%0Agranularities%2C%20establishing%20a%20unified%20and%20robust%20tracking%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM%25202%252B%252B%253A%2520Tracking%2520Anything%2520at%2520Any%2520Granularity%26entry.906535625%3DJiaming%2520Zhang%2520and%2520Cheng%2520Liang%2520and%2520Yichun%2520Yang%2520and%2520Chenkai%2520Zeng%2520and%2520Yutao%2520Cui%2520and%2520Xinwen%2520Zhang%2520and%2520Xin%2520Zhou%2520and%2520Kai%2520Ma%2520and%2520Gangshan%2520Wu%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Video%2520tracking%2520aims%2520at%2520finding%2520the%2520specific%2520target%2520in%2520subsequent%2520frames%2520given%250Aits%2520initial%2520state.%2520Due%2520to%2520the%2520varying%2520granularity%2520of%2520target%2520states%2520across%250Adifferent%2520tasks%252C%2520most%2520existing%2520trackers%2520are%2520tailored%2520to%2520a%2520single%2520task%2520and%250Aheavily%2520rely%2520on%2520custom-designed%2520modules%2520within%2520the%2520individual%2520task%252C%2520which%250Alimits%2520their%2520generalization%2520and%2520leads%2520to%2520redundancy%2520in%2520both%2520model%2520design%2520and%250Aparameters.%2520To%2520unify%2520video%2520tracking%2520tasks%252C%2520we%2520present%2520SAM%25202%252B%252B%252C%2520a%2520unified%2520model%250Atowards%2520tracking%2520at%2520any%2520granularity%252C%2520including%2520masks%252C%2520boxes%252C%2520and%2520points.%2520First%252C%250Ato%2520extend%2520target%2520granularity%252C%2520we%2520design%2520task-specific%2520prompts%2520to%2520encode%2520various%250Atask%2520inputs%2520into%2520general%2520prompt%2520embeddings%252C%2520and%2520a%2520unified%2520decoder%2520to%2520unify%250Adiverse%2520task%2520results%2520into%2520a%2520unified%2520form%2520pre-output.%2520Next%252C%2520to%2520satisfy%2520memory%250Amatching%252C%2520the%2520core%2520operation%2520of%2520tracking%252C%2520we%2520introduce%2520a%2520task-adaptive%2520memory%250Amechanism%2520that%2520unifies%2520memory%2520across%2520different%2520granularities.%2520Finally%252C%2520we%250Aintroduce%2520a%2520customized%2520data%2520engine%2520to%2520support%2520tracking%2520training%2520at%2520any%250Agranularity%252C%2520producing%2520a%2520large%2520and%2520diverse%2520video%2520tracking%2520dataset%2520with%2520rich%250Aannotations%2520at%2520three%2520granularities%252C%2520termed%2520Tracking-Any-Granularity%252C%2520which%250Arepresents%2520a%2520comprehensive%2520resource%2520for%2520training%2520and%2520benchmarking%2520on%2520unified%250Atracking.%2520Comprehensive%2520experiments%2520on%2520multiple%2520benchmarks%2520confirm%2520that%2520SAM%25202%252B%252B%250Asets%2520a%2520new%2520state%2520of%2520the%2520art%2520across%2520diverse%2520tracking%2520tasks%2520at%2520different%250Agranularities%252C%2520establishing%2520a%2520unified%2520and%2520robust%2520tracking%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM%202%2B%2B%3A%20Tracking%20Anything%20at%20Any%20Granularity&entry.906535625=Jiaming%20Zhang%20and%20Cheng%20Liang%20and%20Yichun%20Yang%20and%20Chenkai%20Zeng%20and%20Yutao%20Cui%20and%20Xinwen%20Zhang%20and%20Xin%20Zhou%20and%20Kai%20Ma%20and%20Gangshan%20Wu%20and%20Limin%20Wang&entry.1292438233=%20%20Video%20tracking%20aims%20at%20finding%20the%20specific%20target%20in%20subsequent%20frames%20given%0Aits%20initial%20state.%20Due%20to%20the%20varying%20granularity%20of%20target%20states%20across%0Adifferent%20tasks%2C%20most%20existing%20trackers%20are%20tailored%20to%20a%20single%20task%20and%0Aheavily%20rely%20on%20custom-designed%20modules%20within%20the%20individual%20task%2C%20which%0Alimits%20their%20generalization%20and%20leads%20to%20redundancy%20in%20both%20model%20design%20and%0Aparameters.%20To%20unify%20video%20tracking%20tasks%2C%20we%20present%20SAM%202%2B%2B%2C%20a%20unified%20model%0Atowards%20tracking%20at%20any%20granularity%2C%20including%20masks%2C%20boxes%2C%20and%20points.%20First%2C%0Ato%20extend%20target%20granularity%2C%20we%20design%20task-specific%20prompts%20to%20encode%20various%0Atask%20inputs%20into%20general%20prompt%20embeddings%2C%20and%20a%20unified%20decoder%20to%20unify%0Adiverse%20task%20results%20into%20a%20unified%20form%20pre-output.%20Next%2C%20to%20satisfy%20memory%0Amatching%2C%20the%20core%20operation%20of%20tracking%2C%20we%20introduce%20a%20task-adaptive%20memory%0Amechanism%20that%20unifies%20memory%20across%20different%20granularities.%20Finally%2C%20we%0Aintroduce%20a%20customized%20data%20engine%20to%20support%20tracking%20training%20at%20any%0Agranularity%2C%20producing%20a%20large%20and%20diverse%20video%20tracking%20dataset%20with%20rich%0Aannotations%20at%20three%20granularities%2C%20termed%20Tracking-Any-Granularity%2C%20which%0Arepresents%20a%20comprehensive%20resource%20for%20training%20and%20benchmarking%20on%20unified%0Atracking.%20Comprehensive%20experiments%20on%20multiple%20benchmarks%20confirm%20that%20SAM%202%2B%2B%0Asets%20a%20new%20state%20of%20the%20art%20across%20diverse%20tracking%20tasks%20at%20different%0Agranularities%2C%20establishing%20a%20unified%20and%20robust%20tracking%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18822v1&entry.124074799=Read"},
{"title": "Learning Confidence Bounds for Classification with Imbalanced Data", "author": "Matt Clifford and Jonathan Erskine and Alexander Hepburn and Ra\u00fal Santos-Rodr\u00edguez and Dario Garcia-Garcia", "abstract": "  Class imbalance poses a significant challenge in classification tasks, where\ntraditional approaches often lead to biased models and unreliable predictions.\nUndersampling and oversampling techniques have been commonly employed to\naddress this issue, yet they suffer from inherent limitations stemming from\ntheir simplistic approach such as loss of information and additional biases\nrespectively. In this paper, we propose a novel framework that leverages\nlearning theory and concentration inequalities to overcome the shortcomings of\ntraditional solutions. We focus on understanding the uncertainty in a\nclass-dependent manner, as captured by confidence bounds that we directly embed\ninto the learning process. By incorporating class-dependent estimates, our\nmethod can effectively adapt to the varying degrees of imbalance across\ndifferent classes, resulting in more robust and reliable classification\noutcomes. We empirically show how our framework provides a promising direction\nfor handling imbalanced data in classification tasks, offering practitioners a\nvaluable tool for building more accurate and trustworthy models.\n", "link": "http://arxiv.org/abs/2407.11878v3", "date": "2025-10-21", "relevancy": 2.1091, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5559}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5202}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Confidence%20Bounds%20for%20Classification%20with%20Imbalanced%20Data&body=Title%3A%20Learning%20Confidence%20Bounds%20for%20Classification%20with%20Imbalanced%20Data%0AAuthor%3A%20Matt%20Clifford%20and%20Jonathan%20Erskine%20and%20Alexander%20Hepburn%20and%20Ra%C3%BAl%20Santos-Rodr%C3%ADguez%20and%20Dario%20Garcia-Garcia%0AAbstract%3A%20%20%20Class%20imbalance%20poses%20a%20significant%20challenge%20in%20classification%20tasks%2C%20where%0Atraditional%20approaches%20often%20lead%20to%20biased%20models%20and%20unreliable%20predictions.%0AUndersampling%20and%20oversampling%20techniques%20have%20been%20commonly%20employed%20to%0Aaddress%20this%20issue%2C%20yet%20they%20suffer%20from%20inherent%20limitations%20stemming%20from%0Atheir%20simplistic%20approach%20such%20as%20loss%20of%20information%20and%20additional%20biases%0Arespectively.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%20leverages%0Alearning%20theory%20and%20concentration%20inequalities%20to%20overcome%20the%20shortcomings%20of%0Atraditional%20solutions.%20We%20focus%20on%20understanding%20the%20uncertainty%20in%20a%0Aclass-dependent%20manner%2C%20as%20captured%20by%20confidence%20bounds%20that%20we%20directly%20embed%0Ainto%20the%20learning%20process.%20By%20incorporating%20class-dependent%20estimates%2C%20our%0Amethod%20can%20effectively%20adapt%20to%20the%20varying%20degrees%20of%20imbalance%20across%0Adifferent%20classes%2C%20resulting%20in%20more%20robust%20and%20reliable%20classification%0Aoutcomes.%20We%20empirically%20show%20how%20our%20framework%20provides%20a%20promising%20direction%0Afor%20handling%20imbalanced%20data%20in%20classification%20tasks%2C%20offering%20practitioners%20a%0Avaluable%20tool%20for%20building%20more%20accurate%20and%20trustworthy%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11878v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Confidence%2520Bounds%2520for%2520Classification%2520with%2520Imbalanced%2520Data%26entry.906535625%3DMatt%2520Clifford%2520and%2520Jonathan%2520Erskine%2520and%2520Alexander%2520Hepburn%2520and%2520Ra%25C3%25BAl%2520Santos-Rodr%25C3%25ADguez%2520and%2520Dario%2520Garcia-Garcia%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520poses%2520a%2520significant%2520challenge%2520in%2520classification%2520tasks%252C%2520where%250Atraditional%2520approaches%2520often%2520lead%2520to%2520biased%2520models%2520and%2520unreliable%2520predictions.%250AUndersampling%2520and%2520oversampling%2520techniques%2520have%2520been%2520commonly%2520employed%2520to%250Aaddress%2520this%2520issue%252C%2520yet%2520they%2520suffer%2520from%2520inherent%2520limitations%2520stemming%2520from%250Atheir%2520simplistic%2520approach%2520such%2520as%2520loss%2520of%2520information%2520and%2520additional%2520biases%250Arespectively.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520leverages%250Alearning%2520theory%2520and%2520concentration%2520inequalities%2520to%2520overcome%2520the%2520shortcomings%2520of%250Atraditional%2520solutions.%2520We%2520focus%2520on%2520understanding%2520the%2520uncertainty%2520in%2520a%250Aclass-dependent%2520manner%252C%2520as%2520captured%2520by%2520confidence%2520bounds%2520that%2520we%2520directly%2520embed%250Ainto%2520the%2520learning%2520process.%2520By%2520incorporating%2520class-dependent%2520estimates%252C%2520our%250Amethod%2520can%2520effectively%2520adapt%2520to%2520the%2520varying%2520degrees%2520of%2520imbalance%2520across%250Adifferent%2520classes%252C%2520resulting%2520in%2520more%2520robust%2520and%2520reliable%2520classification%250Aoutcomes.%2520We%2520empirically%2520show%2520how%2520our%2520framework%2520provides%2520a%2520promising%2520direction%250Afor%2520handling%2520imbalanced%2520data%2520in%2520classification%2520tasks%252C%2520offering%2520practitioners%2520a%250Avaluable%2520tool%2520for%2520building%2520more%2520accurate%2520and%2520trustworthy%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11878v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Confidence%20Bounds%20for%20Classification%20with%20Imbalanced%20Data&entry.906535625=Matt%20Clifford%20and%20Jonathan%20Erskine%20and%20Alexander%20Hepburn%20and%20Ra%C3%BAl%20Santos-Rodr%C3%ADguez%20and%20Dario%20Garcia-Garcia&entry.1292438233=%20%20Class%20imbalance%20poses%20a%20significant%20challenge%20in%20classification%20tasks%2C%20where%0Atraditional%20approaches%20often%20lead%20to%20biased%20models%20and%20unreliable%20predictions.%0AUndersampling%20and%20oversampling%20techniques%20have%20been%20commonly%20employed%20to%0Aaddress%20this%20issue%2C%20yet%20they%20suffer%20from%20inherent%20limitations%20stemming%20from%0Atheir%20simplistic%20approach%20such%20as%20loss%20of%20information%20and%20additional%20biases%0Arespectively.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%20leverages%0Alearning%20theory%20and%20concentration%20inequalities%20to%20overcome%20the%20shortcomings%20of%0Atraditional%20solutions.%20We%20focus%20on%20understanding%20the%20uncertainty%20in%20a%0Aclass-dependent%20manner%2C%20as%20captured%20by%20confidence%20bounds%20that%20we%20directly%20embed%0Ainto%20the%20learning%20process.%20By%20incorporating%20class-dependent%20estimates%2C%20our%0Amethod%20can%20effectively%20adapt%20to%20the%20varying%20degrees%20of%20imbalance%20across%0Adifferent%20classes%2C%20resulting%20in%20more%20robust%20and%20reliable%20classification%0Aoutcomes.%20We%20empirically%20show%20how%20our%20framework%20provides%20a%20promising%20direction%0Afor%20handling%20imbalanced%20data%20in%20classification%20tasks%2C%20offering%20practitioners%20a%0Avaluable%20tool%20for%20building%20more%20accurate%20and%20trustworthy%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11878v3&entry.124074799=Read"},
{"title": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long\n  Context Training", "author": "Wenxuan Li and Chengruidong Zhang and Huiqiang Jiang and Yucheng Li and Yuqing Yang and Lili Qiu", "abstract": "  The adoption of long context windows has become a standard feature in Large\nLanguage Models (LLMs), as extended contexts significantly enhance their\ncapacity for complex reasoning and broaden their applicability across diverse\nscenarios. Dynamic sparse attention is a promising approach for reducing the\ncomputational cost of long-context. However, efficiently training LLMs with\ndynamic sparse attention on ultra-long contexts-especially in distributed\nsettings-remains a significant challenge, due in large part to worker- and\nstep-level imbalance. This paper introduces MTraining, a novel distributed\nmethodology leveraging dynamic sparse attention to enable efficient training\nfor LLMs with ultra-long contexts. Specifically, MTraining integrates three key\ncomponents: a dynamic sparse training pattern, balanced sparse ring attention,\nand hierarchical sparse ring attention. These components are designed to\nsynergistically address the computational imbalance and communication overheads\ninherent in dynamic sparse attention mechanisms during the training of models\nwith extensive context lengths. We demonstrate the efficacy of MTraining by\ntraining Qwen2.5-3B, successfully expanding its context window from 32K to 512K\ntokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite\nof downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A\nHaystack, reveal that MTraining achieves up to a 6x higher training throughput\nwhile preserving model accuracy. Our code is available at\nhttps://github.com/microsoft/MInference/tree/main/MTraining.\n", "link": "http://arxiv.org/abs/2510.18830v1", "date": "2025-10-21", "relevancy": 2.092, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5388}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5154}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTraining%3A%20Distributed%20Dynamic%20Sparse%20Attention%20for%20Efficient%20Ultra-Long%0A%20%20Context%20Training&body=Title%3A%20MTraining%3A%20Distributed%20Dynamic%20Sparse%20Attention%20for%20Efficient%20Ultra-Long%0A%20%20Context%20Training%0AAuthor%3A%20Wenxuan%20Li%20and%20Chengruidong%20Zhang%20and%20Huiqiang%20Jiang%20and%20Yucheng%20Li%20and%20Yuqing%20Yang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20The%20adoption%20of%20long%20context%20windows%20has%20become%20a%20standard%20feature%20in%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20as%20extended%20contexts%20significantly%20enhance%20their%0Acapacity%20for%20complex%20reasoning%20and%20broaden%20their%20applicability%20across%20diverse%0Ascenarios.%20Dynamic%20sparse%20attention%20is%20a%20promising%20approach%20for%20reducing%20the%0Acomputational%20cost%20of%20long-context.%20However%2C%20efficiently%20training%20LLMs%20with%0Adynamic%20sparse%20attention%20on%20ultra-long%20contexts-especially%20in%20distributed%0Asettings-remains%20a%20significant%20challenge%2C%20due%20in%20large%20part%20to%20worker-%20and%0Astep-level%20imbalance.%20This%20paper%20introduces%20MTraining%2C%20a%20novel%20distributed%0Amethodology%20leveraging%20dynamic%20sparse%20attention%20to%20enable%20efficient%20training%0Afor%20LLMs%20with%20ultra-long%20contexts.%20Specifically%2C%20MTraining%20integrates%20three%20key%0Acomponents%3A%20a%20dynamic%20sparse%20training%20pattern%2C%20balanced%20sparse%20ring%20attention%2C%0Aand%20hierarchical%20sparse%20ring%20attention.%20These%20components%20are%20designed%20to%0Asynergistically%20address%20the%20computational%20imbalance%20and%20communication%20overheads%0Ainherent%20in%20dynamic%20sparse%20attention%20mechanisms%20during%20the%20training%20of%20models%0Awith%20extensive%20context%20lengths.%20We%20demonstrate%20the%20efficacy%20of%20MTraining%20by%0Atraining%20Qwen2.5-3B%2C%20successfully%20expanding%20its%20context%20window%20from%2032K%20to%20512K%0Atokens%20on%20a%20cluster%20of%2032%20A100%20GPUs.%20Our%20evaluations%20on%20a%20comprehensive%20suite%0Aof%20downstream%20tasks%2C%20including%20RULER%2C%20PG-19%2C%20InfiniteBench%2C%20and%20Needle%20In%20A%0AHaystack%2C%20reveal%20that%20MTraining%20achieves%20up%20to%20a%206x%20higher%20training%20throughput%0Awhile%20preserving%20model%20accuracy.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/microsoft/MInference/tree/main/MTraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTraining%253A%2520Distributed%2520Dynamic%2520Sparse%2520Attention%2520for%2520Efficient%2520Ultra-Long%250A%2520%2520Context%2520Training%26entry.906535625%3DWenxuan%2520Li%2520and%2520Chengruidong%2520Zhang%2520and%2520Huiqiang%2520Jiang%2520and%2520Yucheng%2520Li%2520and%2520Yuqing%2520Yang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520The%2520adoption%2520of%2520long%2520context%2520windows%2520has%2520become%2520a%2520standard%2520feature%2520in%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520as%2520extended%2520contexts%2520significantly%2520enhance%2520their%250Acapacity%2520for%2520complex%2520reasoning%2520and%2520broaden%2520their%2520applicability%2520across%2520diverse%250Ascenarios.%2520Dynamic%2520sparse%2520attention%2520is%2520a%2520promising%2520approach%2520for%2520reducing%2520the%250Acomputational%2520cost%2520of%2520long-context.%2520However%252C%2520efficiently%2520training%2520LLMs%2520with%250Adynamic%2520sparse%2520attention%2520on%2520ultra-long%2520contexts-especially%2520in%2520distributed%250Asettings-remains%2520a%2520significant%2520challenge%252C%2520due%2520in%2520large%2520part%2520to%2520worker-%2520and%250Astep-level%2520imbalance.%2520This%2520paper%2520introduces%2520MTraining%252C%2520a%2520novel%2520distributed%250Amethodology%2520leveraging%2520dynamic%2520sparse%2520attention%2520to%2520enable%2520efficient%2520training%250Afor%2520LLMs%2520with%2520ultra-long%2520contexts.%2520Specifically%252C%2520MTraining%2520integrates%2520three%2520key%250Acomponents%253A%2520a%2520dynamic%2520sparse%2520training%2520pattern%252C%2520balanced%2520sparse%2520ring%2520attention%252C%250Aand%2520hierarchical%2520sparse%2520ring%2520attention.%2520These%2520components%2520are%2520designed%2520to%250Asynergistically%2520address%2520the%2520computational%2520imbalance%2520and%2520communication%2520overheads%250Ainherent%2520in%2520dynamic%2520sparse%2520attention%2520mechanisms%2520during%2520the%2520training%2520of%2520models%250Awith%2520extensive%2520context%2520lengths.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520MTraining%2520by%250Atraining%2520Qwen2.5-3B%252C%2520successfully%2520expanding%2520its%2520context%2520window%2520from%252032K%2520to%2520512K%250Atokens%2520on%2520a%2520cluster%2520of%252032%2520A100%2520GPUs.%2520Our%2520evaluations%2520on%2520a%2520comprehensive%2520suite%250Aof%2520downstream%2520tasks%252C%2520including%2520RULER%252C%2520PG-19%252C%2520InfiniteBench%252C%2520and%2520Needle%2520In%2520A%250AHaystack%252C%2520reveal%2520that%2520MTraining%2520achieves%2520up%2520to%2520a%25206x%2520higher%2520training%2520throughput%250Awhile%2520preserving%2520model%2520accuracy.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/microsoft/MInference/tree/main/MTraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTraining%3A%20Distributed%20Dynamic%20Sparse%20Attention%20for%20Efficient%20Ultra-Long%0A%20%20Context%20Training&entry.906535625=Wenxuan%20Li%20and%20Chengruidong%20Zhang%20and%20Huiqiang%20Jiang%20and%20Yucheng%20Li%20and%20Yuqing%20Yang%20and%20Lili%20Qiu&entry.1292438233=%20%20The%20adoption%20of%20long%20context%20windows%20has%20become%20a%20standard%20feature%20in%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20as%20extended%20contexts%20significantly%20enhance%20their%0Acapacity%20for%20complex%20reasoning%20and%20broaden%20their%20applicability%20across%20diverse%0Ascenarios.%20Dynamic%20sparse%20attention%20is%20a%20promising%20approach%20for%20reducing%20the%0Acomputational%20cost%20of%20long-context.%20However%2C%20efficiently%20training%20LLMs%20with%0Adynamic%20sparse%20attention%20on%20ultra-long%20contexts-especially%20in%20distributed%0Asettings-remains%20a%20significant%20challenge%2C%20due%20in%20large%20part%20to%20worker-%20and%0Astep-level%20imbalance.%20This%20paper%20introduces%20MTraining%2C%20a%20novel%20distributed%0Amethodology%20leveraging%20dynamic%20sparse%20attention%20to%20enable%20efficient%20training%0Afor%20LLMs%20with%20ultra-long%20contexts.%20Specifically%2C%20MTraining%20integrates%20three%20key%0Acomponents%3A%20a%20dynamic%20sparse%20training%20pattern%2C%20balanced%20sparse%20ring%20attention%2C%0Aand%20hierarchical%20sparse%20ring%20attention.%20These%20components%20are%20designed%20to%0Asynergistically%20address%20the%20computational%20imbalance%20and%20communication%20overheads%0Ainherent%20in%20dynamic%20sparse%20attention%20mechanisms%20during%20the%20training%20of%20models%0Awith%20extensive%20context%20lengths.%20We%20demonstrate%20the%20efficacy%20of%20MTraining%20by%0Atraining%20Qwen2.5-3B%2C%20successfully%20expanding%20its%20context%20window%20from%2032K%20to%20512K%0Atokens%20on%20a%20cluster%20of%2032%20A100%20GPUs.%20Our%20evaluations%20on%20a%20comprehensive%20suite%0Aof%20downstream%20tasks%2C%20including%20RULER%2C%20PG-19%2C%20InfiniteBench%2C%20and%20Needle%20In%20A%0AHaystack%2C%20reveal%20that%20MTraining%20achieves%20up%20to%20a%206x%20higher%20training%20throughput%0Awhile%20preserving%20model%20accuracy.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/microsoft/MInference/tree/main/MTraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18830v1&entry.124074799=Read"},
{"title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study", "author": "Qi Li and Junpan Wu and Xiang Liu and Yuxin Wang and Zeyu Li and Zhenheng Tang and Yuhan Chen and Shaohuai Shi and Xiaowen Chu", "abstract": "  The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.\n", "link": "http://arxiv.org/abs/2510.18672v1", "date": "2025-10-21", "relevancy": 2.0716, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Language%20Model%20Inference%20Serving%20Unveiled%3A%20An%20Empirical%20Study&body=Title%3A%20Reasoning%20Language%20Model%20Inference%20Serving%20Unveiled%3A%20An%20Empirical%20Study%0AAuthor%3A%20Qi%20Li%20and%20Junpan%20Wu%20and%20Xiang%20Liu%20and%20Yuxin%20Wang%20and%20Zeyu%20Li%20and%20Zhenheng%20Tang%20and%20Yuhan%20Chen%20and%20Shaohuai%20Shi%20and%20Xiaowen%20Chu%0AAbstract%3A%20%20%20The%20reasoning%20large%20language%20model%20%28RLLM%29%20has%20been%20proven%20competitive%20in%0Asolving%20complex%20reasoning%20tasks%20such%20as%20mathematics%2C%20coding%2C%20compared%20to%0Ageneral%20LLM.%20However%2C%20the%20serving%20performance%20and%20behavior%20of%20RLLM%20remains%0Aunexplored%2C%20which%20may%20undermine%20the%20deployment%20and%20utilization%20of%20RLLM%20in%0Areal-world%20scenario.%20To%20close%20this%20gap%2C%20in%20this%20paper%2C%20we%20conduct%20a%0Acomprehensive%20study%20of%20RLLM%20service.%20We%20first%20perform%20a%20pilot%20study%20on%0Acomparing%20the%20serving%20performance%20between%20RLLM%20and%20traditional%20LLM%20and%20reveal%0Athat%20there%20are%20several%20distinct%20differences%20regarding%20serving%20behavior%3A%20%281%29%0Asignificant%20memory%20usage%20and%20fluctuations%3B%20%282%29%20straggler%20requests%3B%20%283%29%20adaptive%0Arunning%20time%3B%20%284%29%20domain%20preference.%20Then%20we%20further%20investigate%20whether%0Aexisting%20inference%20optimization%20techniques%20are%20valid%20for%20RLLM.%20Our%20main%0Atakeaways%20are%20that%20model%20quantization%20methods%20and%20speculative%20decoding%20can%0Aimprove%20service%20system%20efficiency%20with%20small%20compromise%20to%20RLLM%20accuracy%2C%20while%0Aprefix%20caching%2C%20KV%20cache%20quantization%20may%20even%20degrade%20accuracy%20or%20serving%0Aperformance%20for%20small%20RLLM.%20Lastly%2C%20we%20conduct%20evaluation%20under%20real%20world%0Aworkload%20modeled%20by%20Gamma%20distribution%20to%20verify%20our%20findings.%20Empirical%0Aresults%20of%20real%20world%20workload%20evaluation%20across%20different%20dataset%20are%20aligned%0Awith%20our%20main%20findings%20regarding%20RLLM%20serving.%20We%20hope%20our%20work%20can%20provide%20the%0Aresearch%20community%20and%20industry%20with%20insights%20to%20advance%20RLLM%20inference%0Aserving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Language%2520Model%2520Inference%2520Serving%2520Unveiled%253A%2520An%2520Empirical%2520Study%26entry.906535625%3DQi%2520Li%2520and%2520Junpan%2520Wu%2520and%2520Xiang%2520Liu%2520and%2520Yuxin%2520Wang%2520and%2520Zeyu%2520Li%2520and%2520Zhenheng%2520Tang%2520and%2520Yuhan%2520Chen%2520and%2520Shaohuai%2520Shi%2520and%2520Xiaowen%2520Chu%26entry.1292438233%3D%2520%2520The%2520reasoning%2520large%2520language%2520model%2520%2528RLLM%2529%2520has%2520been%2520proven%2520competitive%2520in%250Asolving%2520complex%2520reasoning%2520tasks%2520such%2520as%2520mathematics%252C%2520coding%252C%2520compared%2520to%250Ageneral%2520LLM.%2520However%252C%2520the%2520serving%2520performance%2520and%2520behavior%2520of%2520RLLM%2520remains%250Aunexplored%252C%2520which%2520may%2520undermine%2520the%2520deployment%2520and%2520utilization%2520of%2520RLLM%2520in%250Areal-world%2520scenario.%2520To%2520close%2520this%2520gap%252C%2520in%2520this%2520paper%252C%2520we%2520conduct%2520a%250Acomprehensive%2520study%2520of%2520RLLM%2520service.%2520We%2520first%2520perform%2520a%2520pilot%2520study%2520on%250Acomparing%2520the%2520serving%2520performance%2520between%2520RLLM%2520and%2520traditional%2520LLM%2520and%2520reveal%250Athat%2520there%2520are%2520several%2520distinct%2520differences%2520regarding%2520serving%2520behavior%253A%2520%25281%2529%250Asignificant%2520memory%2520usage%2520and%2520fluctuations%253B%2520%25282%2529%2520straggler%2520requests%253B%2520%25283%2529%2520adaptive%250Arunning%2520time%253B%2520%25284%2529%2520domain%2520preference.%2520Then%2520we%2520further%2520investigate%2520whether%250Aexisting%2520inference%2520optimization%2520techniques%2520are%2520valid%2520for%2520RLLM.%2520Our%2520main%250Atakeaways%2520are%2520that%2520model%2520quantization%2520methods%2520and%2520speculative%2520decoding%2520can%250Aimprove%2520service%2520system%2520efficiency%2520with%2520small%2520compromise%2520to%2520RLLM%2520accuracy%252C%2520while%250Aprefix%2520caching%252C%2520KV%2520cache%2520quantization%2520may%2520even%2520degrade%2520accuracy%2520or%2520serving%250Aperformance%2520for%2520small%2520RLLM.%2520Lastly%252C%2520we%2520conduct%2520evaluation%2520under%2520real%2520world%250Aworkload%2520modeled%2520by%2520Gamma%2520distribution%2520to%2520verify%2520our%2520findings.%2520Empirical%250Aresults%2520of%2520real%2520world%2520workload%2520evaluation%2520across%2520different%2520dataset%2520are%2520aligned%250Awith%2520our%2520main%2520findings%2520regarding%2520RLLM%2520serving.%2520We%2520hope%2520our%2520work%2520can%2520provide%2520the%250Aresearch%2520community%2520and%2520industry%2520with%2520insights%2520to%2520advance%2520RLLM%2520inference%250Aserving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Language%20Model%20Inference%20Serving%20Unveiled%3A%20An%20Empirical%20Study&entry.906535625=Qi%20Li%20and%20Junpan%20Wu%20and%20Xiang%20Liu%20and%20Yuxin%20Wang%20and%20Zeyu%20Li%20and%20Zhenheng%20Tang%20and%20Yuhan%20Chen%20and%20Shaohuai%20Shi%20and%20Xiaowen%20Chu&entry.1292438233=%20%20The%20reasoning%20large%20language%20model%20%28RLLM%29%20has%20been%20proven%20competitive%20in%0Asolving%20complex%20reasoning%20tasks%20such%20as%20mathematics%2C%20coding%2C%20compared%20to%0Ageneral%20LLM.%20However%2C%20the%20serving%20performance%20and%20behavior%20of%20RLLM%20remains%0Aunexplored%2C%20which%20may%20undermine%20the%20deployment%20and%20utilization%20of%20RLLM%20in%0Areal-world%20scenario.%20To%20close%20this%20gap%2C%20in%20this%20paper%2C%20we%20conduct%20a%0Acomprehensive%20study%20of%20RLLM%20service.%20We%20first%20perform%20a%20pilot%20study%20on%0Acomparing%20the%20serving%20performance%20between%20RLLM%20and%20traditional%20LLM%20and%20reveal%0Athat%20there%20are%20several%20distinct%20differences%20regarding%20serving%20behavior%3A%20%281%29%0Asignificant%20memory%20usage%20and%20fluctuations%3B%20%282%29%20straggler%20requests%3B%20%283%29%20adaptive%0Arunning%20time%3B%20%284%29%20domain%20preference.%20Then%20we%20further%20investigate%20whether%0Aexisting%20inference%20optimization%20techniques%20are%20valid%20for%20RLLM.%20Our%20main%0Atakeaways%20are%20that%20model%20quantization%20methods%20and%20speculative%20decoding%20can%0Aimprove%20service%20system%20efficiency%20with%20small%20compromise%20to%20RLLM%20accuracy%2C%20while%0Aprefix%20caching%2C%20KV%20cache%20quantization%20may%20even%20degrade%20accuracy%20or%20serving%0Aperformance%20for%20small%20RLLM.%20Lastly%2C%20we%20conduct%20evaluation%20under%20real%20world%0Aworkload%20modeled%20by%20Gamma%20distribution%20to%20verify%20our%20findings.%20Empirical%0Aresults%20of%20real%20world%20workload%20evaluation%20across%20different%20dataset%20are%20aligned%0Awith%20our%20main%20findings%20regarding%20RLLM%20serving.%20We%20hope%20our%20work%20can%20provide%20the%0Aresearch%20community%20and%20industry%20with%20insights%20to%20advance%20RLLM%20inference%0Aserving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18672v1&entry.124074799=Read"},
{"title": "Improving Diffusion-based Inverse Algorithms under Few-Step Constraint\n  via Learnable Linear Extrapolation", "author": "Jiawei Zhang and Ziyuan Liu and Leon Yan and Gen Li and Yuantao Gu", "abstract": "  Diffusion-based inverse algorithms have shown remarkable performance across\nvarious inverse problems, yet their reliance on numerous denoising steps incurs\nhigh computational costs. While recent developments of fast diffusion ODE\nsolvers offer effective acceleration for diffusion sampling without\nobservations, their application in inverse problems remains limited due to the\nheterogeneous formulations of inverse algorithms and their prevalent use of\napproximations and heuristics, which often introduce significant errors that\nundermine the reliability of analytical solvers. In this work, we begin with an\nanalysis of ODE solvers for inverse problems that reveals a linear combination\nstructure of approximations for the inverse trajectory. Building on this\ninsight, we propose a canonical form that unifies a broad class of\ndiffusion-based inverse algorithms and facilitates the design of more\ngeneralizable solvers. Inspired by the linear subspace search strategy, we\npropose Learnable Linear Extrapolation (LLE), a lightweight approach that\nuniversally enhances the performance of any diffusion-based inverse algorithm\nconforming to our canonical form. LLE optimizes the combination coefficients to\nrefine current predictions using previous estimates, alleviating the\nsensitivity of analytical solvers for inverse algorithms. Extensive experiments\ndemonstrate consistent improvements of the proposed LLE method across multiple\nalgorithms and tasks, indicating its potential for more efficient solutions and\nboosted performance of diffusion-based inverse algorithms with limited steps.\nCodes for reproducing our experiments are available at\nhttps://github.com/weigerzan/LLE_inverse_problem.\n", "link": "http://arxiv.org/abs/2503.10103v3", "date": "2025-10-21", "relevancy": 2.0691, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5773}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5185}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Diffusion-based%20Inverse%20Algorithms%20under%20Few-Step%20Constraint%0A%20%20via%20Learnable%20Linear%20Extrapolation&body=Title%3A%20Improving%20Diffusion-based%20Inverse%20Algorithms%20under%20Few-Step%20Constraint%0A%20%20via%20Learnable%20Linear%20Extrapolation%0AAuthor%3A%20Jiawei%20Zhang%20and%20Ziyuan%20Liu%20and%20Leon%20Yan%20and%20Gen%20Li%20and%20Yuantao%20Gu%0AAbstract%3A%20%20%20Diffusion-based%20inverse%20algorithms%20have%20shown%20remarkable%20performance%20across%0Avarious%20inverse%20problems%2C%20yet%20their%20reliance%20on%20numerous%20denoising%20steps%20incurs%0Ahigh%20computational%20costs.%20While%20recent%20developments%20of%20fast%20diffusion%20ODE%0Asolvers%20offer%20effective%20acceleration%20for%20diffusion%20sampling%20without%0Aobservations%2C%20their%20application%20in%20inverse%20problems%20remains%20limited%20due%20to%20the%0Aheterogeneous%20formulations%20of%20inverse%20algorithms%20and%20their%20prevalent%20use%20of%0Aapproximations%20and%20heuristics%2C%20which%20often%20introduce%20significant%20errors%20that%0Aundermine%20the%20reliability%20of%20analytical%20solvers.%20In%20this%20work%2C%20we%20begin%20with%20an%0Aanalysis%20of%20ODE%20solvers%20for%20inverse%20problems%20that%20reveals%20a%20linear%20combination%0Astructure%20of%20approximations%20for%20the%20inverse%20trajectory.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20a%20canonical%20form%20that%20unifies%20a%20broad%20class%20of%0Adiffusion-based%20inverse%20algorithms%20and%20facilitates%20the%20design%20of%20more%0Ageneralizable%20solvers.%20Inspired%20by%20the%20linear%20subspace%20search%20strategy%2C%20we%0Apropose%20Learnable%20Linear%20Extrapolation%20%28LLE%29%2C%20a%20lightweight%20approach%20that%0Auniversally%20enhances%20the%20performance%20of%20any%20diffusion-based%20inverse%20algorithm%0Aconforming%20to%20our%20canonical%20form.%20LLE%20optimizes%20the%20combination%20coefficients%20to%0Arefine%20current%20predictions%20using%20previous%20estimates%2C%20alleviating%20the%0Asensitivity%20of%20analytical%20solvers%20for%20inverse%20algorithms.%20Extensive%20experiments%0Ademonstrate%20consistent%20improvements%20of%20the%20proposed%20LLE%20method%20across%20multiple%0Aalgorithms%20and%20tasks%2C%20indicating%20its%20potential%20for%20more%20efficient%20solutions%20and%0Aboosted%20performance%20of%20diffusion-based%20inverse%20algorithms%20with%20limited%20steps.%0ACodes%20for%20reproducing%20our%20experiments%20are%20available%20at%0Ahttps%3A//github.com/weigerzan/LLE_inverse_problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10103v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Diffusion-based%2520Inverse%2520Algorithms%2520under%2520Few-Step%2520Constraint%250A%2520%2520via%2520Learnable%2520Linear%2520Extrapolation%26entry.906535625%3DJiawei%2520Zhang%2520and%2520Ziyuan%2520Liu%2520and%2520Leon%2520Yan%2520and%2520Gen%2520Li%2520and%2520Yuantao%2520Gu%26entry.1292438233%3D%2520%2520Diffusion-based%2520inverse%2520algorithms%2520have%2520shown%2520remarkable%2520performance%2520across%250Avarious%2520inverse%2520problems%252C%2520yet%2520their%2520reliance%2520on%2520numerous%2520denoising%2520steps%2520incurs%250Ahigh%2520computational%2520costs.%2520While%2520recent%2520developments%2520of%2520fast%2520diffusion%2520ODE%250Asolvers%2520offer%2520effective%2520acceleration%2520for%2520diffusion%2520sampling%2520without%250Aobservations%252C%2520their%2520application%2520in%2520inverse%2520problems%2520remains%2520limited%2520due%2520to%2520the%250Aheterogeneous%2520formulations%2520of%2520inverse%2520algorithms%2520and%2520their%2520prevalent%2520use%2520of%250Aapproximations%2520and%2520heuristics%252C%2520which%2520often%2520introduce%2520significant%2520errors%2520that%250Aundermine%2520the%2520reliability%2520of%2520analytical%2520solvers.%2520In%2520this%2520work%252C%2520we%2520begin%2520with%2520an%250Aanalysis%2520of%2520ODE%2520solvers%2520for%2520inverse%2520problems%2520that%2520reveals%2520a%2520linear%2520combination%250Astructure%2520of%2520approximations%2520for%2520the%2520inverse%2520trajectory.%2520Building%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520a%2520canonical%2520form%2520that%2520unifies%2520a%2520broad%2520class%2520of%250Adiffusion-based%2520inverse%2520algorithms%2520and%2520facilitates%2520the%2520design%2520of%2520more%250Ageneralizable%2520solvers.%2520Inspired%2520by%2520the%2520linear%2520subspace%2520search%2520strategy%252C%2520we%250Apropose%2520Learnable%2520Linear%2520Extrapolation%2520%2528LLE%2529%252C%2520a%2520lightweight%2520approach%2520that%250Auniversally%2520enhances%2520the%2520performance%2520of%2520any%2520diffusion-based%2520inverse%2520algorithm%250Aconforming%2520to%2520our%2520canonical%2520form.%2520LLE%2520optimizes%2520the%2520combination%2520coefficients%2520to%250Arefine%2520current%2520predictions%2520using%2520previous%2520estimates%252C%2520alleviating%2520the%250Asensitivity%2520of%2520analytical%2520solvers%2520for%2520inverse%2520algorithms.%2520Extensive%2520experiments%250Ademonstrate%2520consistent%2520improvements%2520of%2520the%2520proposed%2520LLE%2520method%2520across%2520multiple%250Aalgorithms%2520and%2520tasks%252C%2520indicating%2520its%2520potential%2520for%2520more%2520efficient%2520solutions%2520and%250Aboosted%2520performance%2520of%2520diffusion-based%2520inverse%2520algorithms%2520with%2520limited%2520steps.%250ACodes%2520for%2520reproducing%2520our%2520experiments%2520are%2520available%2520at%250Ahttps%253A//github.com/weigerzan/LLE_inverse_problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10103v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Diffusion-based%20Inverse%20Algorithms%20under%20Few-Step%20Constraint%0A%20%20via%20Learnable%20Linear%20Extrapolation&entry.906535625=Jiawei%20Zhang%20and%20Ziyuan%20Liu%20and%20Leon%20Yan%20and%20Gen%20Li%20and%20Yuantao%20Gu&entry.1292438233=%20%20Diffusion-based%20inverse%20algorithms%20have%20shown%20remarkable%20performance%20across%0Avarious%20inverse%20problems%2C%20yet%20their%20reliance%20on%20numerous%20denoising%20steps%20incurs%0Ahigh%20computational%20costs.%20While%20recent%20developments%20of%20fast%20diffusion%20ODE%0Asolvers%20offer%20effective%20acceleration%20for%20diffusion%20sampling%20without%0Aobservations%2C%20their%20application%20in%20inverse%20problems%20remains%20limited%20due%20to%20the%0Aheterogeneous%20formulations%20of%20inverse%20algorithms%20and%20their%20prevalent%20use%20of%0Aapproximations%20and%20heuristics%2C%20which%20often%20introduce%20significant%20errors%20that%0Aundermine%20the%20reliability%20of%20analytical%20solvers.%20In%20this%20work%2C%20we%20begin%20with%20an%0Aanalysis%20of%20ODE%20solvers%20for%20inverse%20problems%20that%20reveals%20a%20linear%20combination%0Astructure%20of%20approximations%20for%20the%20inverse%20trajectory.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20a%20canonical%20form%20that%20unifies%20a%20broad%20class%20of%0Adiffusion-based%20inverse%20algorithms%20and%20facilitates%20the%20design%20of%20more%0Ageneralizable%20solvers.%20Inspired%20by%20the%20linear%20subspace%20search%20strategy%2C%20we%0Apropose%20Learnable%20Linear%20Extrapolation%20%28LLE%29%2C%20a%20lightweight%20approach%20that%0Auniversally%20enhances%20the%20performance%20of%20any%20diffusion-based%20inverse%20algorithm%0Aconforming%20to%20our%20canonical%20form.%20LLE%20optimizes%20the%20combination%20coefficients%20to%0Arefine%20current%20predictions%20using%20previous%20estimates%2C%20alleviating%20the%0Asensitivity%20of%20analytical%20solvers%20for%20inverse%20algorithms.%20Extensive%20experiments%0Ademonstrate%20consistent%20improvements%20of%20the%20proposed%20LLE%20method%20across%20multiple%0Aalgorithms%20and%20tasks%2C%20indicating%20its%20potential%20for%20more%20efficient%20solutions%20and%0Aboosted%20performance%20of%20diffusion-based%20inverse%20algorithms%20with%20limited%20steps.%0ACodes%20for%20reproducing%20our%20experiments%20are%20available%20at%0Ahttps%3A//github.com/weigerzan/LLE_inverse_problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10103v3&entry.124074799=Read"},
{"title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process\n  Modeling and Understanding", "author": "Yue Jiang and Jichu Li and Yang Liu and Dingkang Yang and Feng Zhou and Quyu Kong", "abstract": "  We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nProject page: https://github.com/FRENKIE-CHIANG/DanmakuTPPBench\n", "link": "http://arxiv.org/abs/2505.18411v2", "date": "2025-10-21", "relevancy": 2.0656, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5357}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5223}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DanmakuTPPBench%3A%20A%20Multi-modal%20Benchmark%20for%20Temporal%20Point%20Process%0A%20%20Modeling%20and%20Understanding&body=Title%3A%20DanmakuTPPBench%3A%20A%20Multi-modal%20Benchmark%20for%20Temporal%20Point%20Process%0A%20%20Modeling%20and%20Understanding%0AAuthor%3A%20Yue%20Jiang%20and%20Jichu%20Li%20and%20Yang%20Liu%20and%20Dingkang%20Yang%20and%20Feng%20Zhou%20and%20Quyu%20Kong%0AAbstract%3A%20%20%20We%20introduce%20DanmakuTPPBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20advance%0Amulti-modal%20Temporal%20Point%20Process%20%28TPP%29%20modeling%20in%20the%20era%20of%20Large%20Language%0AModels%20%28LLMs%29.%20While%20TPPs%20have%20been%20widely%20studied%20for%20modeling%20temporal%20event%0Asequences%2C%20existing%20datasets%20are%20predominantly%20unimodal%2C%20hindering%20progress%20in%0Amodels%20that%20require%20joint%20reasoning%20over%20temporal%2C%20textual%2C%20and%20visual%0Ainformation.%20To%20address%20this%20gap%2C%20DanmakuTPPBench%20comprises%20two%20complementary%0Acomponents%3A%20%281%29%20DanmakuTPP-Events%2C%20a%20novel%20dataset%20derived%20from%20the%20Bilibili%0Avideo%20platform%2C%20where%20user-generated%20bullet%20comments%20%28Danmaku%29%20naturally%20form%0Amulti-modal%20events%20annotated%20with%20precise%20timestamps%2C%20rich%20textual%20content%2C%20and%0Acorresponding%20video%20frames%3B%20%282%29%20DanmakuTPP-QA%2C%20a%20challenging%20question-answering%0Adataset%20constructed%20via%20a%20novel%20multi-agent%20pipeline%20powered%20by%0Astate-of-the-art%20LLMs%20and%20multi-modal%20LLMs%20%28MLLMs%29%2C%20targeting%20complex%0Atemporal-textual-visual%20reasoning.%20We%20conduct%20extensive%20evaluations%20using%20both%0Aclassical%20TPP%20models%20and%20recent%20MLLMs%2C%20revealing%20significant%20performance%20gaps%0Aand%20limitations%20in%20current%20methods%27%20ability%20to%20model%20multi-modal%20event%0Adynamics.%20Our%20benchmark%20establishes%20strong%20baselines%20and%20calls%20for%20further%0Aintegration%20of%20TPP%20modeling%20into%20the%20multi-modal%20language%20modeling%20landscape.%0AProject%20page%3A%20https%3A//github.com/FRENKIE-CHIANG/DanmakuTPPBench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDanmakuTPPBench%253A%2520A%2520Multi-modal%2520Benchmark%2520for%2520Temporal%2520Point%2520Process%250A%2520%2520Modeling%2520and%2520Understanding%26entry.906535625%3DYue%2520Jiang%2520and%2520Jichu%2520Li%2520and%2520Yang%2520Liu%2520and%2520Dingkang%2520Yang%2520and%2520Feng%2520Zhou%2520and%2520Quyu%2520Kong%26entry.1292438233%3D%2520%2520We%2520introduce%2520DanmakuTPPBench%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520advance%250Amulti-modal%2520Temporal%2520Point%2520Process%2520%2528TPP%2529%2520modeling%2520in%2520the%2520era%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529.%2520While%2520TPPs%2520have%2520been%2520widely%2520studied%2520for%2520modeling%2520temporal%2520event%250Asequences%252C%2520existing%2520datasets%2520are%2520predominantly%2520unimodal%252C%2520hindering%2520progress%2520in%250Amodels%2520that%2520require%2520joint%2520reasoning%2520over%2520temporal%252C%2520textual%252C%2520and%2520visual%250Ainformation.%2520To%2520address%2520this%2520gap%252C%2520DanmakuTPPBench%2520comprises%2520two%2520complementary%250Acomponents%253A%2520%25281%2529%2520DanmakuTPP-Events%252C%2520a%2520novel%2520dataset%2520derived%2520from%2520the%2520Bilibili%250Avideo%2520platform%252C%2520where%2520user-generated%2520bullet%2520comments%2520%2528Danmaku%2529%2520naturally%2520form%250Amulti-modal%2520events%2520annotated%2520with%2520precise%2520timestamps%252C%2520rich%2520textual%2520content%252C%2520and%250Acorresponding%2520video%2520frames%253B%2520%25282%2529%2520DanmakuTPP-QA%252C%2520a%2520challenging%2520question-answering%250Adataset%2520constructed%2520via%2520a%2520novel%2520multi-agent%2520pipeline%2520powered%2520by%250Astate-of-the-art%2520LLMs%2520and%2520multi-modal%2520LLMs%2520%2528MLLMs%2529%252C%2520targeting%2520complex%250Atemporal-textual-visual%2520reasoning.%2520We%2520conduct%2520extensive%2520evaluations%2520using%2520both%250Aclassical%2520TPP%2520models%2520and%2520recent%2520MLLMs%252C%2520revealing%2520significant%2520performance%2520gaps%250Aand%2520limitations%2520in%2520current%2520methods%2527%2520ability%2520to%2520model%2520multi-modal%2520event%250Adynamics.%2520Our%2520benchmark%2520establishes%2520strong%2520baselines%2520and%2520calls%2520for%2520further%250Aintegration%2520of%2520TPP%2520modeling%2520into%2520the%2520multi-modal%2520language%2520modeling%2520landscape.%250AProject%2520page%253A%2520https%253A//github.com/FRENKIE-CHIANG/DanmakuTPPBench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanmakuTPPBench%3A%20A%20Multi-modal%20Benchmark%20for%20Temporal%20Point%20Process%0A%20%20Modeling%20and%20Understanding&entry.906535625=Yue%20Jiang%20and%20Jichu%20Li%20and%20Yang%20Liu%20and%20Dingkang%20Yang%20and%20Feng%20Zhou%20and%20Quyu%20Kong&entry.1292438233=%20%20We%20introduce%20DanmakuTPPBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20advance%0Amulti-modal%20Temporal%20Point%20Process%20%28TPP%29%20modeling%20in%20the%20era%20of%20Large%20Language%0AModels%20%28LLMs%29.%20While%20TPPs%20have%20been%20widely%20studied%20for%20modeling%20temporal%20event%0Asequences%2C%20existing%20datasets%20are%20predominantly%20unimodal%2C%20hindering%20progress%20in%0Amodels%20that%20require%20joint%20reasoning%20over%20temporal%2C%20textual%2C%20and%20visual%0Ainformation.%20To%20address%20this%20gap%2C%20DanmakuTPPBench%20comprises%20two%20complementary%0Acomponents%3A%20%281%29%20DanmakuTPP-Events%2C%20a%20novel%20dataset%20derived%20from%20the%20Bilibili%0Avideo%20platform%2C%20where%20user-generated%20bullet%20comments%20%28Danmaku%29%20naturally%20form%0Amulti-modal%20events%20annotated%20with%20precise%20timestamps%2C%20rich%20textual%20content%2C%20and%0Acorresponding%20video%20frames%3B%20%282%29%20DanmakuTPP-QA%2C%20a%20challenging%20question-answering%0Adataset%20constructed%20via%20a%20novel%20multi-agent%20pipeline%20powered%20by%0Astate-of-the-art%20LLMs%20and%20multi-modal%20LLMs%20%28MLLMs%29%2C%20targeting%20complex%0Atemporal-textual-visual%20reasoning.%20We%20conduct%20extensive%20evaluations%20using%20both%0Aclassical%20TPP%20models%20and%20recent%20MLLMs%2C%20revealing%20significant%20performance%20gaps%0Aand%20limitations%20in%20current%20methods%27%20ability%20to%20model%20multi-modal%20event%0Adynamics.%20Our%20benchmark%20establishes%20strong%20baselines%20and%20calls%20for%20further%0Aintegration%20of%20TPP%20modeling%20into%20the%20multi-modal%20language%20modeling%20landscape.%0AProject%20page%3A%20https%3A//github.com/FRENKIE-CHIANG/DanmakuTPPBench%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18411v2&entry.124074799=Read"},
{"title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via\n  Critic-Guided Search", "author": "Zijian Wu and Suozhi Huang and Zhejian Zhou and Huaiyuan Ying and Zheng Yuan and Wenwei Zhang and Dahua Lin and Kai Chen", "abstract": "  Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. A\nprevalent proof method involves the LLM prover iteratively constructing the\nproof tactic by tactic, typically following a best-first search scheme.\nHowever, this method often ignores the critical preference information inside\nthe existing tactic trajectories, hindering the search for deeper proofs. We\npropose an intuitive yet effective method, which utilizes a critic model to\ncapture the preference information and to guide the search of the prover model\nat runtime. Given the prover-critic framework, a large-scale expert iteration\nwith more than 20,000 CPU days is then applied to further fine-tune the prover\nand the critic. The trained InternLM2.5-StepProver critic significantly boosts\nthe performance of the prover model (59.4% to 65.9%). We also analyze the\nimpact of the critic on various aspects of the theorem proving process during\nexpert iteration, providing insights into its effectiveness. We open-source our\nmodels and searched proofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.\n", "link": "http://arxiv.org/abs/2410.15700v2", "date": "2025-10-21", "relevancy": 2.0604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5245}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternLM2.5-StepProver%3A%20Advancing%20Automated%20Theorem%20Proving%20via%0A%20%20Critic-Guided%20Search&body=Title%3A%20InternLM2.5-StepProver%3A%20Advancing%20Automated%20Theorem%20Proving%20via%0A%20%20Critic-Guided%20Search%0AAuthor%3A%20Zijian%20Wu%20and%20Suozhi%20Huang%20and%20Zhejian%20Zhou%20and%20Huaiyuan%20Ying%20and%20Zheng%20Yuan%20and%20Wenwei%20Zhang%20and%20Dahua%20Lin%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20in%20mathematical%0Atheorem%20proving%2C%20particularly%20when%20utilizing%20formal%20languages%20such%20as%20LEAN.%20A%0Aprevalent%20proof%20method%20involves%20the%20LLM%20prover%20iteratively%20constructing%20the%0Aproof%20tactic%20by%20tactic%2C%20typically%20following%20a%20best-first%20search%20scheme.%0AHowever%2C%20this%20method%20often%20ignores%20the%20critical%20preference%20information%20inside%0Athe%20existing%20tactic%20trajectories%2C%20hindering%20the%20search%20for%20deeper%20proofs.%20We%0Apropose%20an%20intuitive%20yet%20effective%20method%2C%20which%20utilizes%20a%20critic%20model%20to%0Acapture%20the%20preference%20information%20and%20to%20guide%20the%20search%20of%20the%20prover%20model%0Aat%20runtime.%20Given%20the%20prover-critic%20framework%2C%20a%20large-scale%20expert%20iteration%0Awith%20more%20than%2020%2C000%20CPU%20days%20is%20then%20applied%20to%20further%20fine-tune%20the%20prover%0Aand%20the%20critic.%20The%20trained%20InternLM2.5-StepProver%20critic%20significantly%20boosts%0Athe%20performance%20of%20the%20prover%20model%20%2859.4%25%20to%2065.9%25%29.%20We%20also%20analyze%20the%0Aimpact%20of%20the%20critic%20on%20various%20aspects%20of%20the%20theorem%20proving%20process%20during%0Aexpert%20iteration%2C%20providing%20insights%20into%20its%20effectiveness.%20We%20open-source%20our%0Amodels%20and%20searched%20proofs%20at%20https%3A//github.com/InternLM/InternLM-Math%20and%0Ahttps%3A//huggingface.co/datasets/internlm/Lean-Workbook.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternLM2.5-StepProver%253A%2520Advancing%2520Automated%2520Theorem%2520Proving%2520via%250A%2520%2520Critic-Guided%2520Search%26entry.906535625%3DZijian%2520Wu%2520and%2520Suozhi%2520Huang%2520and%2520Zhejian%2520Zhou%2520and%2520Huaiyuan%2520Ying%2520and%2520Zheng%2520Yuan%2520and%2520Wenwei%2520Zhang%2520and%2520Dahua%2520Lin%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520in%2520mathematical%250Atheorem%2520proving%252C%2520particularly%2520when%2520utilizing%2520formal%2520languages%2520such%2520as%2520LEAN.%2520A%250Aprevalent%2520proof%2520method%2520involves%2520the%2520LLM%2520prover%2520iteratively%2520constructing%2520the%250Aproof%2520tactic%2520by%2520tactic%252C%2520typically%2520following%2520a%2520best-first%2520search%2520scheme.%250AHowever%252C%2520this%2520method%2520often%2520ignores%2520the%2520critical%2520preference%2520information%2520inside%250Athe%2520existing%2520tactic%2520trajectories%252C%2520hindering%2520the%2520search%2520for%2520deeper%2520proofs.%2520We%250Apropose%2520an%2520intuitive%2520yet%2520effective%2520method%252C%2520which%2520utilizes%2520a%2520critic%2520model%2520to%250Acapture%2520the%2520preference%2520information%2520and%2520to%2520guide%2520the%2520search%2520of%2520the%2520prover%2520model%250Aat%2520runtime.%2520Given%2520the%2520prover-critic%2520framework%252C%2520a%2520large-scale%2520expert%2520iteration%250Awith%2520more%2520than%252020%252C000%2520CPU%2520days%2520is%2520then%2520applied%2520to%2520further%2520fine-tune%2520the%2520prover%250Aand%2520the%2520critic.%2520The%2520trained%2520InternLM2.5-StepProver%2520critic%2520significantly%2520boosts%250Athe%2520performance%2520of%2520the%2520prover%2520model%2520%252859.4%2525%2520to%252065.9%2525%2529.%2520We%2520also%2520analyze%2520the%250Aimpact%2520of%2520the%2520critic%2520on%2520various%2520aspects%2520of%2520the%2520theorem%2520proving%2520process%2520during%250Aexpert%2520iteration%252C%2520providing%2520insights%2520into%2520its%2520effectiveness.%2520We%2520open-source%2520our%250Amodels%2520and%2520searched%2520proofs%2520at%2520https%253A//github.com/InternLM/InternLM-Math%2520and%250Ahttps%253A//huggingface.co/datasets/internlm/Lean-Workbook.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternLM2.5-StepProver%3A%20Advancing%20Automated%20Theorem%20Proving%20via%0A%20%20Critic-Guided%20Search&entry.906535625=Zijian%20Wu%20and%20Suozhi%20Huang%20and%20Zhejian%20Zhou%20and%20Huaiyuan%20Ying%20and%20Zheng%20Yuan%20and%20Wenwei%20Zhang%20and%20Dahua%20Lin%20and%20Kai%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20in%20mathematical%0Atheorem%20proving%2C%20particularly%20when%20utilizing%20formal%20languages%20such%20as%20LEAN.%20A%0Aprevalent%20proof%20method%20involves%20the%20LLM%20prover%20iteratively%20constructing%20the%0Aproof%20tactic%20by%20tactic%2C%20typically%20following%20a%20best-first%20search%20scheme.%0AHowever%2C%20this%20method%20often%20ignores%20the%20critical%20preference%20information%20inside%0Athe%20existing%20tactic%20trajectories%2C%20hindering%20the%20search%20for%20deeper%20proofs.%20We%0Apropose%20an%20intuitive%20yet%20effective%20method%2C%20which%20utilizes%20a%20critic%20model%20to%0Acapture%20the%20preference%20information%20and%20to%20guide%20the%20search%20of%20the%20prover%20model%0Aat%20runtime.%20Given%20the%20prover-critic%20framework%2C%20a%20large-scale%20expert%20iteration%0Awith%20more%20than%2020%2C000%20CPU%20days%20is%20then%20applied%20to%20further%20fine-tune%20the%20prover%0Aand%20the%20critic.%20The%20trained%20InternLM2.5-StepProver%20critic%20significantly%20boosts%0Athe%20performance%20of%20the%20prover%20model%20%2859.4%25%20to%2065.9%25%29.%20We%20also%20analyze%20the%0Aimpact%20of%20the%20critic%20on%20various%20aspects%20of%20the%20theorem%20proving%20process%20during%0Aexpert%20iteration%2C%20providing%20insights%20into%20its%20effectiveness.%20We%20open-source%20our%0Amodels%20and%20searched%20proofs%20at%20https%3A//github.com/InternLM/InternLM-Math%20and%0Ahttps%3A//huggingface.co/datasets/internlm/Lean-Workbook.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15700v2&entry.124074799=Read"},
{"title": "LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and\n  RNA-binding Proteins using Protein Language Models", "author": "Nimisha Ghosh and Dheeran Sankaran and Rahul Balakrishnan Adhi and Sharath S and Amrut Anand", "abstract": "  Identifying DNA- (DBPs) and RNA-binding proteins (RBPs) is crucial for the\nunderstanding of cell function, molecular interactions as well as regulatory\nfunctions. Owing to their high similarity, most of the existing approaches face\nchallenges in differentiating between DBPs and RBPs leading to high\ncross-prediction errors. Moreover, identifying proteins which bind to both DNA\nand RNA (DRBPs) is also quite a challenging task. In this regard, we propose a\nnovel framework viz. LAMP-PRo which is based on pre-trained protein language\nmodel (PLM), attention mechanisms and multi-label learning to mitigate these\nissues. First, pre-trained PLM such ESM-2 is used for embedding the protein\nsequences followed by convolutional neural network (CNN). Subsequently\nmulti-head self-attention mechanism is applied for the contextual information\nwhile label-aware attention is used to compute class-specific representations\nby attending to the sequence in a way that is tailored to each label (DBP, RBP\nand non-NABP) in a multi-label setup. We have also included a novel cross-label\nattention mechanism to explicitly capture dependencies between DNA- and\nRNA-binding proteins, enabling more accurate prediction of DRBP. Finally, a\nlinear layer followed by a sigmoid function are used for the final prediction.\nExtensive experiments are carried out to compare LAMP-PRo with the existing\nmethods wherein the proposed model shows consistent competent performance.\nFurthermore, we also provide visualization to showcase model interpretability,\nhighlighting which parts of the sequence are most relevant for a predicted\nlabel. The original datasets are available at http://bliulab.net/iDRBP\\_MMC and\nthe codes are available at https://github.com/NimishaGhosh/LAMP-PRo.\n", "link": "http://arxiv.org/abs/2509.24262v2", "date": "2025-10-21", "relevancy": 2.0389, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5145}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAMP-PRo%3A%20Label-aware%20Attention%20for%20Multi-label%20Prediction%20of%20DNA-%20and%0A%20%20RNA-binding%20Proteins%20using%20Protein%20Language%20Models&body=Title%3A%20LAMP-PRo%3A%20Label-aware%20Attention%20for%20Multi-label%20Prediction%20of%20DNA-%20and%0A%20%20RNA-binding%20Proteins%20using%20Protein%20Language%20Models%0AAuthor%3A%20Nimisha%20Ghosh%20and%20Dheeran%20Sankaran%20and%20Rahul%20Balakrishnan%20Adhi%20and%20Sharath%20S%20and%20Amrut%20Anand%0AAbstract%3A%20%20%20Identifying%20DNA-%20%28DBPs%29%20and%20RNA-binding%20proteins%20%28RBPs%29%20is%20crucial%20for%20the%0Aunderstanding%20of%20cell%20function%2C%20molecular%20interactions%20as%20well%20as%20regulatory%0Afunctions.%20Owing%20to%20their%20high%20similarity%2C%20most%20of%20the%20existing%20approaches%20face%0Achallenges%20in%20differentiating%20between%20DBPs%20and%20RBPs%20leading%20to%20high%0Across-prediction%20errors.%20Moreover%2C%20identifying%20proteins%20which%20bind%20to%20both%20DNA%0Aand%20RNA%20%28DRBPs%29%20is%20also%20quite%20a%20challenging%20task.%20In%20this%20regard%2C%20we%20propose%20a%0Anovel%20framework%20viz.%20LAMP-PRo%20which%20is%20based%20on%20pre-trained%20protein%20language%0Amodel%20%28PLM%29%2C%20attention%20mechanisms%20and%20multi-label%20learning%20to%20mitigate%20these%0Aissues.%20First%2C%20pre-trained%20PLM%20such%20ESM-2%20is%20used%20for%20embedding%20the%20protein%0Asequences%20followed%20by%20convolutional%20neural%20network%20%28CNN%29.%20Subsequently%0Amulti-head%20self-attention%20mechanism%20is%20applied%20for%20the%20contextual%20information%0Awhile%20label-aware%20attention%20is%20used%20to%20compute%20class-specific%20representations%0Aby%20attending%20to%20the%20sequence%20in%20a%20way%20that%20is%20tailored%20to%20each%20label%20%28DBP%2C%20RBP%0Aand%20non-NABP%29%20in%20a%20multi-label%20setup.%20We%20have%20also%20included%20a%20novel%20cross-label%0Aattention%20mechanism%20to%20explicitly%20capture%20dependencies%20between%20DNA-%20and%0ARNA-binding%20proteins%2C%20enabling%20more%20accurate%20prediction%20of%20DRBP.%20Finally%2C%20a%0Alinear%20layer%20followed%20by%20a%20sigmoid%20function%20are%20used%20for%20the%20final%20prediction.%0AExtensive%20experiments%20are%20carried%20out%20to%20compare%20LAMP-PRo%20with%20the%20existing%0Amethods%20wherein%20the%20proposed%20model%20shows%20consistent%20competent%20performance.%0AFurthermore%2C%20we%20also%20provide%20visualization%20to%20showcase%20model%20interpretability%2C%0Ahighlighting%20which%20parts%20of%20the%20sequence%20are%20most%20relevant%20for%20a%20predicted%0Alabel.%20The%20original%20datasets%20are%20available%20at%20http%3A//bliulab.net/iDRBP%5C_MMC%20and%0Athe%20codes%20are%20available%20at%20https%3A//github.com/NimishaGhosh/LAMP-PRo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAMP-PRo%253A%2520Label-aware%2520Attention%2520for%2520Multi-label%2520Prediction%2520of%2520DNA-%2520and%250A%2520%2520RNA-binding%2520Proteins%2520using%2520Protein%2520Language%2520Models%26entry.906535625%3DNimisha%2520Ghosh%2520and%2520Dheeran%2520Sankaran%2520and%2520Rahul%2520Balakrishnan%2520Adhi%2520and%2520Sharath%2520S%2520and%2520Amrut%2520Anand%26entry.1292438233%3D%2520%2520Identifying%2520DNA-%2520%2528DBPs%2529%2520and%2520RNA-binding%2520proteins%2520%2528RBPs%2529%2520is%2520crucial%2520for%2520the%250Aunderstanding%2520of%2520cell%2520function%252C%2520molecular%2520interactions%2520as%2520well%2520as%2520regulatory%250Afunctions.%2520Owing%2520to%2520their%2520high%2520similarity%252C%2520most%2520of%2520the%2520existing%2520approaches%2520face%250Achallenges%2520in%2520differentiating%2520between%2520DBPs%2520and%2520RBPs%2520leading%2520to%2520high%250Across-prediction%2520errors.%2520Moreover%252C%2520identifying%2520proteins%2520which%2520bind%2520to%2520both%2520DNA%250Aand%2520RNA%2520%2528DRBPs%2529%2520is%2520also%2520quite%2520a%2520challenging%2520task.%2520In%2520this%2520regard%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520viz.%2520LAMP-PRo%2520which%2520is%2520based%2520on%2520pre-trained%2520protein%2520language%250Amodel%2520%2528PLM%2529%252C%2520attention%2520mechanisms%2520and%2520multi-label%2520learning%2520to%2520mitigate%2520these%250Aissues.%2520First%252C%2520pre-trained%2520PLM%2520such%2520ESM-2%2520is%2520used%2520for%2520embedding%2520the%2520protein%250Asequences%2520followed%2520by%2520convolutional%2520neural%2520network%2520%2528CNN%2529.%2520Subsequently%250Amulti-head%2520self-attention%2520mechanism%2520is%2520applied%2520for%2520the%2520contextual%2520information%250Awhile%2520label-aware%2520attention%2520is%2520used%2520to%2520compute%2520class-specific%2520representations%250Aby%2520attending%2520to%2520the%2520sequence%2520in%2520a%2520way%2520that%2520is%2520tailored%2520to%2520each%2520label%2520%2528DBP%252C%2520RBP%250Aand%2520non-NABP%2529%2520in%2520a%2520multi-label%2520setup.%2520We%2520have%2520also%2520included%2520a%2520novel%2520cross-label%250Aattention%2520mechanism%2520to%2520explicitly%2520capture%2520dependencies%2520between%2520DNA-%2520and%250ARNA-binding%2520proteins%252C%2520enabling%2520more%2520accurate%2520prediction%2520of%2520DRBP.%2520Finally%252C%2520a%250Alinear%2520layer%2520followed%2520by%2520a%2520sigmoid%2520function%2520are%2520used%2520for%2520the%2520final%2520prediction.%250AExtensive%2520experiments%2520are%2520carried%2520out%2520to%2520compare%2520LAMP-PRo%2520with%2520the%2520existing%250Amethods%2520wherein%2520the%2520proposed%2520model%2520shows%2520consistent%2520competent%2520performance.%250AFurthermore%252C%2520we%2520also%2520provide%2520visualization%2520to%2520showcase%2520model%2520interpretability%252C%250Ahighlighting%2520which%2520parts%2520of%2520the%2520sequence%2520are%2520most%2520relevant%2520for%2520a%2520predicted%250Alabel.%2520The%2520original%2520datasets%2520are%2520available%2520at%2520http%253A//bliulab.net/iDRBP%255C_MMC%2520and%250Athe%2520codes%2520are%2520available%2520at%2520https%253A//github.com/NimishaGhosh/LAMP-PRo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAMP-PRo%3A%20Label-aware%20Attention%20for%20Multi-label%20Prediction%20of%20DNA-%20and%0A%20%20RNA-binding%20Proteins%20using%20Protein%20Language%20Models&entry.906535625=Nimisha%20Ghosh%20and%20Dheeran%20Sankaran%20and%20Rahul%20Balakrishnan%20Adhi%20and%20Sharath%20S%20and%20Amrut%20Anand&entry.1292438233=%20%20Identifying%20DNA-%20%28DBPs%29%20and%20RNA-binding%20proteins%20%28RBPs%29%20is%20crucial%20for%20the%0Aunderstanding%20of%20cell%20function%2C%20molecular%20interactions%20as%20well%20as%20regulatory%0Afunctions.%20Owing%20to%20their%20high%20similarity%2C%20most%20of%20the%20existing%20approaches%20face%0Achallenges%20in%20differentiating%20between%20DBPs%20and%20RBPs%20leading%20to%20high%0Across-prediction%20errors.%20Moreover%2C%20identifying%20proteins%20which%20bind%20to%20both%20DNA%0Aand%20RNA%20%28DRBPs%29%20is%20also%20quite%20a%20challenging%20task.%20In%20this%20regard%2C%20we%20propose%20a%0Anovel%20framework%20viz.%20LAMP-PRo%20which%20is%20based%20on%20pre-trained%20protein%20language%0Amodel%20%28PLM%29%2C%20attention%20mechanisms%20and%20multi-label%20learning%20to%20mitigate%20these%0Aissues.%20First%2C%20pre-trained%20PLM%20such%20ESM-2%20is%20used%20for%20embedding%20the%20protein%0Asequences%20followed%20by%20convolutional%20neural%20network%20%28CNN%29.%20Subsequently%0Amulti-head%20self-attention%20mechanism%20is%20applied%20for%20the%20contextual%20information%0Awhile%20label-aware%20attention%20is%20used%20to%20compute%20class-specific%20representations%0Aby%20attending%20to%20the%20sequence%20in%20a%20way%20that%20is%20tailored%20to%20each%20label%20%28DBP%2C%20RBP%0Aand%20non-NABP%29%20in%20a%20multi-label%20setup.%20We%20have%20also%20included%20a%20novel%20cross-label%0Aattention%20mechanism%20to%20explicitly%20capture%20dependencies%20between%20DNA-%20and%0ARNA-binding%20proteins%2C%20enabling%20more%20accurate%20prediction%20of%20DRBP.%20Finally%2C%20a%0Alinear%20layer%20followed%20by%20a%20sigmoid%20function%20are%20used%20for%20the%20final%20prediction.%0AExtensive%20experiments%20are%20carried%20out%20to%20compare%20LAMP-PRo%20with%20the%20existing%0Amethods%20wherein%20the%20proposed%20model%20shows%20consistent%20competent%20performance.%0AFurthermore%2C%20we%20also%20provide%20visualization%20to%20showcase%20model%20interpretability%2C%0Ahighlighting%20which%20parts%20of%20the%20sequence%20are%20most%20relevant%20for%20a%20predicted%0Alabel.%20The%20original%20datasets%20are%20available%20at%20http%3A//bliulab.net/iDRBP%5C_MMC%20and%0Athe%20codes%20are%20available%20at%20https%3A//github.com/NimishaGhosh/LAMP-PRo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24262v2&entry.124074799=Read"},
{"title": "LENS: Large Pre-trained Transformer for Exploring Financial Time Series\n  Regularities", "author": "Yuanjian Xu and Anxian Liu and Jianing Hao and Zhenzhuo Li and Shichang Meng and Guang Zhang", "abstract": "  Modeling large-scale time series has gained significant attention in recent\nyears. However, its direct application in finance remains challenging due to\nsubstantial differences in data characteristics across domains. Specifically,\nfinancial systems feature inherent stochasticity and low signal-to-noise\nratios, rendering traditional methods and pre-training approaches ineffective.\nThis underscores the urgent need for a foundation model tailored to financial\ntime series. To bridge this gap, we propose \\textbf{LENS}, a pre-trained model\nfor this domain. \\textbf{LENS} effectively captures the complexity of financial\nstochastic systems through a carefully crafted model architecture and mitigates\nnoise during pre-training by using an invertible embedding module. We provide a\nrigorous theoretical explanation of the model's effectiveness and validate its\nperformance through extensive experiments. Pre-trained on a dataset comprising\n100 billion financial observations, \\textbf{LENS} achieves exceptional results\nacross a wide range of critical downstream tasks. Moreover, our work offers\npractical insights into developing pre-trained time series models in high-noise\nenvironments, paving the way for further advancements in this pivotal research\ndomain.\n", "link": "http://arxiv.org/abs/2408.10111v3", "date": "2025-10-21", "relevancy": 2.0374, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5253}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LENS%3A%20Large%20Pre-trained%20Transformer%20for%20Exploring%20Financial%20Time%20Series%0A%20%20Regularities&body=Title%3A%20LENS%3A%20Large%20Pre-trained%20Transformer%20for%20Exploring%20Financial%20Time%20Series%0A%20%20Regularities%0AAuthor%3A%20Yuanjian%20Xu%20and%20Anxian%20Liu%20and%20Jianing%20Hao%20and%20Zhenzhuo%20Li%20and%20Shichang%20Meng%20and%20Guang%20Zhang%0AAbstract%3A%20%20%20Modeling%20large-scale%20time%20series%20has%20gained%20significant%20attention%20in%20recent%0Ayears.%20However%2C%20its%20direct%20application%20in%20finance%20remains%20challenging%20due%20to%0Asubstantial%20differences%20in%20data%20characteristics%20across%20domains.%20Specifically%2C%0Afinancial%20systems%20feature%20inherent%20stochasticity%20and%20low%20signal-to-noise%0Aratios%2C%20rendering%20traditional%20methods%20and%20pre-training%20approaches%20ineffective.%0AThis%20underscores%20the%20urgent%20need%20for%20a%20foundation%20model%20tailored%20to%20financial%0Atime%20series.%20To%20bridge%20this%20gap%2C%20we%20propose%20%5Ctextbf%7BLENS%7D%2C%20a%20pre-trained%20model%0Afor%20this%20domain.%20%5Ctextbf%7BLENS%7D%20effectively%20captures%20the%20complexity%20of%20financial%0Astochastic%20systems%20through%20a%20carefully%20crafted%20model%20architecture%20and%20mitigates%0Anoise%20during%20pre-training%20by%20using%20an%20invertible%20embedding%20module.%20We%20provide%20a%0Arigorous%20theoretical%20explanation%20of%20the%20model%27s%20effectiveness%20and%20validate%20its%0Aperformance%20through%20extensive%20experiments.%20Pre-trained%20on%20a%20dataset%20comprising%0A100%20billion%20financial%20observations%2C%20%5Ctextbf%7BLENS%7D%20achieves%20exceptional%20results%0Aacross%20a%20wide%20range%20of%20critical%20downstream%20tasks.%20Moreover%2C%20our%20work%20offers%0Apractical%20insights%20into%20developing%20pre-trained%20time%20series%20models%20in%20high-noise%0Aenvironments%2C%20paving%20the%20way%20for%20further%20advancements%20in%20this%20pivotal%20research%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10111v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLENS%253A%2520Large%2520Pre-trained%2520Transformer%2520for%2520Exploring%2520Financial%2520Time%2520Series%250A%2520%2520Regularities%26entry.906535625%3DYuanjian%2520Xu%2520and%2520Anxian%2520Liu%2520and%2520Jianing%2520Hao%2520and%2520Zhenzhuo%2520Li%2520and%2520Shichang%2520Meng%2520and%2520Guang%2520Zhang%26entry.1292438233%3D%2520%2520Modeling%2520large-scale%2520time%2520series%2520has%2520gained%2520significant%2520attention%2520in%2520recent%250Ayears.%2520However%252C%2520its%2520direct%2520application%2520in%2520finance%2520remains%2520challenging%2520due%2520to%250Asubstantial%2520differences%2520in%2520data%2520characteristics%2520across%2520domains.%2520Specifically%252C%250Afinancial%2520systems%2520feature%2520inherent%2520stochasticity%2520and%2520low%2520signal-to-noise%250Aratios%252C%2520rendering%2520traditional%2520methods%2520and%2520pre-training%2520approaches%2520ineffective.%250AThis%2520underscores%2520the%2520urgent%2520need%2520for%2520a%2520foundation%2520model%2520tailored%2520to%2520financial%250Atime%2520series.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520%255Ctextbf%257BLENS%257D%252C%2520a%2520pre-trained%2520model%250Afor%2520this%2520domain.%2520%255Ctextbf%257BLENS%257D%2520effectively%2520captures%2520the%2520complexity%2520of%2520financial%250Astochastic%2520systems%2520through%2520a%2520carefully%2520crafted%2520model%2520architecture%2520and%2520mitigates%250Anoise%2520during%2520pre-training%2520by%2520using%2520an%2520invertible%2520embedding%2520module.%2520We%2520provide%2520a%250Arigorous%2520theoretical%2520explanation%2520of%2520the%2520model%2527s%2520effectiveness%2520and%2520validate%2520its%250Aperformance%2520through%2520extensive%2520experiments.%2520Pre-trained%2520on%2520a%2520dataset%2520comprising%250A100%2520billion%2520financial%2520observations%252C%2520%255Ctextbf%257BLENS%257D%2520achieves%2520exceptional%2520results%250Aacross%2520a%2520wide%2520range%2520of%2520critical%2520downstream%2520tasks.%2520Moreover%252C%2520our%2520work%2520offers%250Apractical%2520insights%2520into%2520developing%2520pre-trained%2520time%2520series%2520models%2520in%2520high-noise%250Aenvironments%252C%2520paving%2520the%2520way%2520for%2520further%2520advancements%2520in%2520this%2520pivotal%2520research%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10111v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LENS%3A%20Large%20Pre-trained%20Transformer%20for%20Exploring%20Financial%20Time%20Series%0A%20%20Regularities&entry.906535625=Yuanjian%20Xu%20and%20Anxian%20Liu%20and%20Jianing%20Hao%20and%20Zhenzhuo%20Li%20and%20Shichang%20Meng%20and%20Guang%20Zhang&entry.1292438233=%20%20Modeling%20large-scale%20time%20series%20has%20gained%20significant%20attention%20in%20recent%0Ayears.%20However%2C%20its%20direct%20application%20in%20finance%20remains%20challenging%20due%20to%0Asubstantial%20differences%20in%20data%20characteristics%20across%20domains.%20Specifically%2C%0Afinancial%20systems%20feature%20inherent%20stochasticity%20and%20low%20signal-to-noise%0Aratios%2C%20rendering%20traditional%20methods%20and%20pre-training%20approaches%20ineffective.%0AThis%20underscores%20the%20urgent%20need%20for%20a%20foundation%20model%20tailored%20to%20financial%0Atime%20series.%20To%20bridge%20this%20gap%2C%20we%20propose%20%5Ctextbf%7BLENS%7D%2C%20a%20pre-trained%20model%0Afor%20this%20domain.%20%5Ctextbf%7BLENS%7D%20effectively%20captures%20the%20complexity%20of%20financial%0Astochastic%20systems%20through%20a%20carefully%20crafted%20model%20architecture%20and%20mitigates%0Anoise%20during%20pre-training%20by%20using%20an%20invertible%20embedding%20module.%20We%20provide%20a%0Arigorous%20theoretical%20explanation%20of%20the%20model%27s%20effectiveness%20and%20validate%20its%0Aperformance%20through%20extensive%20experiments.%20Pre-trained%20on%20a%20dataset%20comprising%0A100%20billion%20financial%20observations%2C%20%5Ctextbf%7BLENS%7D%20achieves%20exceptional%20results%0Aacross%20a%20wide%20range%20of%20critical%20downstream%20tasks.%20Moreover%2C%20our%20work%20offers%0Apractical%20insights%20into%20developing%20pre-trained%20time%20series%20models%20in%20high-noise%0Aenvironments%2C%20paving%20the%20way%20for%20further%20advancements%20in%20this%20pivotal%20research%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10111v3&entry.124074799=Read"},
{"title": "A unified framework for establishing the universal approximation of\n  transformer-type architectures", "author": "Jingpu Cheng and Ting Lin and Zuowei Shen and Qianxiao Li", "abstract": "  We investigate the universal approximation property (UAP) of transformer-type\narchitectures, providing a unified theoretical framework that extends prior\nresults on residual networks to models incorporating attention mechanisms. Our\nwork identifies token distinguishability as a fundamental requirement for UAP\nand introduces a general sufficient condition that applies to a broad class of\narchitectures. Leveraging an analyticity assumption on the attention layer, we\ncan significantly simplify the verification of this condition, providing a\nnon-constructive approach in establishing UAP for such architectures. We\ndemonstrate the applicability of our framework by proving UAP for transformers\nwith various attention mechanisms, including kernel-based and sparse attention\nmechanisms. The corollaries of our results either generalize prior works or\nestablish UAP for architectures not previously covered. Furthermore, our\nframework offers a principled foundation for designing novel transformer\narchitectures with inherent UAP guarantees, including those with specific\nfunctional symmetries. We propose examples to illustrate these insights.\n", "link": "http://arxiv.org/abs/2506.23551v2", "date": "2025-10-21", "relevancy": 2.0362, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5413}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5231}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20unified%20framework%20for%20establishing%20the%20universal%20approximation%20of%0A%20%20transformer-type%20architectures&body=Title%3A%20A%20unified%20framework%20for%20establishing%20the%20universal%20approximation%20of%0A%20%20transformer-type%20architectures%0AAuthor%3A%20Jingpu%20Cheng%20and%20Ting%20Lin%20and%20Zuowei%20Shen%20and%20Qianxiao%20Li%0AAbstract%3A%20%20%20We%20investigate%20the%20universal%20approximation%20property%20%28UAP%29%20of%20transformer-type%0Aarchitectures%2C%20providing%20a%20unified%20theoretical%20framework%20that%20extends%20prior%0Aresults%20on%20residual%20networks%20to%20models%20incorporating%20attention%20mechanisms.%20Our%0Awork%20identifies%20token%20distinguishability%20as%20a%20fundamental%20requirement%20for%20UAP%0Aand%20introduces%20a%20general%20sufficient%20condition%20that%20applies%20to%20a%20broad%20class%20of%0Aarchitectures.%20Leveraging%20an%20analyticity%20assumption%20on%20the%20attention%20layer%2C%20we%0Acan%20significantly%20simplify%20the%20verification%20of%20this%20condition%2C%20providing%20a%0Anon-constructive%20approach%20in%20establishing%20UAP%20for%20such%20architectures.%20We%0Ademonstrate%20the%20applicability%20of%20our%20framework%20by%20proving%20UAP%20for%20transformers%0Awith%20various%20attention%20mechanisms%2C%20including%20kernel-based%20and%20sparse%20attention%0Amechanisms.%20The%20corollaries%20of%20our%20results%20either%20generalize%20prior%20works%20or%0Aestablish%20UAP%20for%20architectures%20not%20previously%20covered.%20Furthermore%2C%20our%0Aframework%20offers%20a%20principled%20foundation%20for%20designing%20novel%20transformer%0Aarchitectures%20with%20inherent%20UAP%20guarantees%2C%20including%20those%20with%20specific%0Afunctional%20symmetries.%20We%20propose%20examples%20to%20illustrate%20these%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520unified%2520framework%2520for%2520establishing%2520the%2520universal%2520approximation%2520of%250A%2520%2520transformer-type%2520architectures%26entry.906535625%3DJingpu%2520Cheng%2520and%2520Ting%2520Lin%2520and%2520Zuowei%2520Shen%2520and%2520Qianxiao%2520Li%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520universal%2520approximation%2520property%2520%2528UAP%2529%2520of%2520transformer-type%250Aarchitectures%252C%2520providing%2520a%2520unified%2520theoretical%2520framework%2520that%2520extends%2520prior%250Aresults%2520on%2520residual%2520networks%2520to%2520models%2520incorporating%2520attention%2520mechanisms.%2520Our%250Awork%2520identifies%2520token%2520distinguishability%2520as%2520a%2520fundamental%2520requirement%2520for%2520UAP%250Aand%2520introduces%2520a%2520general%2520sufficient%2520condition%2520that%2520applies%2520to%2520a%2520broad%2520class%2520of%250Aarchitectures.%2520Leveraging%2520an%2520analyticity%2520assumption%2520on%2520the%2520attention%2520layer%252C%2520we%250Acan%2520significantly%2520simplify%2520the%2520verification%2520of%2520this%2520condition%252C%2520providing%2520a%250Anon-constructive%2520approach%2520in%2520establishing%2520UAP%2520for%2520such%2520architectures.%2520We%250Ademonstrate%2520the%2520applicability%2520of%2520our%2520framework%2520by%2520proving%2520UAP%2520for%2520transformers%250Awith%2520various%2520attention%2520mechanisms%252C%2520including%2520kernel-based%2520and%2520sparse%2520attention%250Amechanisms.%2520The%2520corollaries%2520of%2520our%2520results%2520either%2520generalize%2520prior%2520works%2520or%250Aestablish%2520UAP%2520for%2520architectures%2520not%2520previously%2520covered.%2520Furthermore%252C%2520our%250Aframework%2520offers%2520a%2520principled%2520foundation%2520for%2520designing%2520novel%2520transformer%250Aarchitectures%2520with%2520inherent%2520UAP%2520guarantees%252C%2520including%2520those%2520with%2520specific%250Afunctional%2520symmetries.%2520We%2520propose%2520examples%2520to%2520illustrate%2520these%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20unified%20framework%20for%20establishing%20the%20universal%20approximation%20of%0A%20%20transformer-type%20architectures&entry.906535625=Jingpu%20Cheng%20and%20Ting%20Lin%20and%20Zuowei%20Shen%20and%20Qianxiao%20Li&entry.1292438233=%20%20We%20investigate%20the%20universal%20approximation%20property%20%28UAP%29%20of%20transformer-type%0Aarchitectures%2C%20providing%20a%20unified%20theoretical%20framework%20that%20extends%20prior%0Aresults%20on%20residual%20networks%20to%20models%20incorporating%20attention%20mechanisms.%20Our%0Awork%20identifies%20token%20distinguishability%20as%20a%20fundamental%20requirement%20for%20UAP%0Aand%20introduces%20a%20general%20sufficient%20condition%20that%20applies%20to%20a%20broad%20class%20of%0Aarchitectures.%20Leveraging%20an%20analyticity%20assumption%20on%20the%20attention%20layer%2C%20we%0Acan%20significantly%20simplify%20the%20verification%20of%20this%20condition%2C%20providing%20a%0Anon-constructive%20approach%20in%20establishing%20UAP%20for%20such%20architectures.%20We%0Ademonstrate%20the%20applicability%20of%20our%20framework%20by%20proving%20UAP%20for%20transformers%0Awith%20various%20attention%20mechanisms%2C%20including%20kernel-based%20and%20sparse%20attention%0Amechanisms.%20The%20corollaries%20of%20our%20results%20either%20generalize%20prior%20works%20or%0Aestablish%20UAP%20for%20architectures%20not%20previously%20covered.%20Furthermore%2C%20our%0Aframework%20offers%20a%20principled%20foundation%20for%20designing%20novel%20transformer%0Aarchitectures%20with%20inherent%20UAP%20guarantees%2C%20including%20those%20with%20specific%0Afunctional%20symmetries.%20We%20propose%20examples%20to%20illustrate%20these%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23551v2&entry.124074799=Read"},
{"title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate\n  Human Behaviors", "author": "Tiancheng Hu and Joachim Baumann and Lorenzo Lupo and Nigel Collier and Dirk Hovy and Paul R\u00f6ttger", "abstract": "  Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.\n", "link": "http://arxiv.org/abs/2510.17516v2", "date": "2025-10-21", "relevancy": 2.0337, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimBench%3A%20Benchmarking%20the%20Ability%20of%20Large%20Language%20Models%20to%20Simulate%0A%20%20Human%20Behaviors&body=Title%3A%20SimBench%3A%20Benchmarking%20the%20Ability%20of%20Large%20Language%20Models%20to%20Simulate%0A%20%20Human%20Behaviors%0AAuthor%3A%20Tiancheng%20Hu%20and%20Joachim%20Baumann%20and%20Lorenzo%20Lupo%20and%20Nigel%20Collier%20and%20Dirk%20Hovy%20and%20Paul%20R%C3%B6ttger%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20simulations%20of%20human%20behavior%20have%20the%20potential%0Ato%20revolutionize%20the%20social%20and%20behavioral%20sciences%2C%20if%20and%20only%20if%20they%0Afaithfully%20reflect%20real%20human%20behaviors.%20Current%20evaluations%20are%20fragmented%2C%0Abased%20on%20bespoke%20tasks%20and%20metrics%2C%20creating%20a%20patchwork%20of%20incomparable%0Aresults.%20To%20address%20this%2C%20we%20introduce%20SimBench%2C%20the%20first%20large-scale%2C%0Astandardized%20benchmark%20for%20a%20robust%2C%20reproducible%20science%20of%20LLM%20simulation.%20By%0Aunifying%2020%20diverse%20datasets%20covering%20tasks%20from%20moral%20decision-making%20to%0Aeconomic%20choice%20across%20a%20large%20global%20participant%20pool%2C%20SimBench%20provides%20the%0Anecessary%20foundation%20to%20ask%20fundamental%20questions%20about%20when%2C%20how%2C%20and%20why%20LLM%0Asimulations%20succeed%20or%20fail.%20We%20show%20that%2C%20while%20even%20the%20best%20LLMs%20today%20have%0Alimited%20simulation%20ability%20%28score%3A%2040.80/100%29%2C%20performance%20scales%20log-linearly%0Awith%20model%20size.%20Simulation%20performance%20is%20not%20improved%20by%20increased%0Ainference-time%20compute.%20We%20demonstrate%20an%20alignment-simulation%20trade-off%3A%0Ainstruction-tuning%20improves%20performance%20on%20low-entropy%20%28consensus%29%20questions%0Abut%20degrades%20it%20on%20high-entropy%20%28diverse%29%20ones.%20Models%20particularly%20struggle%0Awhen%20simulating%20specific%20demographic%20groups.%20Finally%2C%20we%20demonstrate%20that%0Asimulation%20ability%20correlates%20most%20strongly%20with%20deep%2C%20knowledge-intensive%0Areasoning%20%28MMLU-Pro%2C%20r%3D0.939%29.%20By%20making%20progress%20measurable%2C%20we%20aim%20to%0Aaccelerate%20the%20development%20of%20more%20faithful%20LLM%20simulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimBench%253A%2520Benchmarking%2520the%2520Ability%2520of%2520Large%2520Language%2520Models%2520to%2520Simulate%250A%2520%2520Human%2520Behaviors%26entry.906535625%3DTiancheng%2520Hu%2520and%2520Joachim%2520Baumann%2520and%2520Lorenzo%2520Lupo%2520and%2520Nigel%2520Collier%2520and%2520Dirk%2520Hovy%2520and%2520Paul%2520R%25C3%25B6ttger%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520simulations%2520of%2520human%2520behavior%2520have%2520the%2520potential%250Ato%2520revolutionize%2520the%2520social%2520and%2520behavioral%2520sciences%252C%2520if%2520and%2520only%2520if%2520they%250Afaithfully%2520reflect%2520real%2520human%2520behaviors.%2520Current%2520evaluations%2520are%2520fragmented%252C%250Abased%2520on%2520bespoke%2520tasks%2520and%2520metrics%252C%2520creating%2520a%2520patchwork%2520of%2520incomparable%250Aresults.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SimBench%252C%2520the%2520first%2520large-scale%252C%250Astandardized%2520benchmark%2520for%2520a%2520robust%252C%2520reproducible%2520science%2520of%2520LLM%2520simulation.%2520By%250Aunifying%252020%2520diverse%2520datasets%2520covering%2520tasks%2520from%2520moral%2520decision-making%2520to%250Aeconomic%2520choice%2520across%2520a%2520large%2520global%2520participant%2520pool%252C%2520SimBench%2520provides%2520the%250Anecessary%2520foundation%2520to%2520ask%2520fundamental%2520questions%2520about%2520when%252C%2520how%252C%2520and%2520why%2520LLM%250Asimulations%2520succeed%2520or%2520fail.%2520We%2520show%2520that%252C%2520while%2520even%2520the%2520best%2520LLMs%2520today%2520have%250Alimited%2520simulation%2520ability%2520%2528score%253A%252040.80/100%2529%252C%2520performance%2520scales%2520log-linearly%250Awith%2520model%2520size.%2520Simulation%2520performance%2520is%2520not%2520improved%2520by%2520increased%250Ainference-time%2520compute.%2520We%2520demonstrate%2520an%2520alignment-simulation%2520trade-off%253A%250Ainstruction-tuning%2520improves%2520performance%2520on%2520low-entropy%2520%2528consensus%2529%2520questions%250Abut%2520degrades%2520it%2520on%2520high-entropy%2520%2528diverse%2529%2520ones.%2520Models%2520particularly%2520struggle%250Awhen%2520simulating%2520specific%2520demographic%2520groups.%2520Finally%252C%2520we%2520demonstrate%2520that%250Asimulation%2520ability%2520correlates%2520most%2520strongly%2520with%2520deep%252C%2520knowledge-intensive%250Areasoning%2520%2528MMLU-Pro%252C%2520r%253D0.939%2529.%2520By%2520making%2520progress%2520measurable%252C%2520we%2520aim%2520to%250Aaccelerate%2520the%2520development%2520of%2520more%2520faithful%2520LLM%2520simulators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimBench%3A%20Benchmarking%20the%20Ability%20of%20Large%20Language%20Models%20to%20Simulate%0A%20%20Human%20Behaviors&entry.906535625=Tiancheng%20Hu%20and%20Joachim%20Baumann%20and%20Lorenzo%20Lupo%20and%20Nigel%20Collier%20and%20Dirk%20Hovy%20and%20Paul%20R%C3%B6ttger&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20simulations%20of%20human%20behavior%20have%20the%20potential%0Ato%20revolutionize%20the%20social%20and%20behavioral%20sciences%2C%20if%20and%20only%20if%20they%0Afaithfully%20reflect%20real%20human%20behaviors.%20Current%20evaluations%20are%20fragmented%2C%0Abased%20on%20bespoke%20tasks%20and%20metrics%2C%20creating%20a%20patchwork%20of%20incomparable%0Aresults.%20To%20address%20this%2C%20we%20introduce%20SimBench%2C%20the%20first%20large-scale%2C%0Astandardized%20benchmark%20for%20a%20robust%2C%20reproducible%20science%20of%20LLM%20simulation.%20By%0Aunifying%2020%20diverse%20datasets%20covering%20tasks%20from%20moral%20decision-making%20to%0Aeconomic%20choice%20across%20a%20large%20global%20participant%20pool%2C%20SimBench%20provides%20the%0Anecessary%20foundation%20to%20ask%20fundamental%20questions%20about%20when%2C%20how%2C%20and%20why%20LLM%0Asimulations%20succeed%20or%20fail.%20We%20show%20that%2C%20while%20even%20the%20best%20LLMs%20today%20have%0Alimited%20simulation%20ability%20%28score%3A%2040.80/100%29%2C%20performance%20scales%20log-linearly%0Awith%20model%20size.%20Simulation%20performance%20is%20not%20improved%20by%20increased%0Ainference-time%20compute.%20We%20demonstrate%20an%20alignment-simulation%20trade-off%3A%0Ainstruction-tuning%20improves%20performance%20on%20low-entropy%20%28consensus%29%20questions%0Abut%20degrades%20it%20on%20high-entropy%20%28diverse%29%20ones.%20Models%20particularly%20struggle%0Awhen%20simulating%20specific%20demographic%20groups.%20Finally%2C%20we%20demonstrate%20that%0Asimulation%20ability%20correlates%20most%20strongly%20with%20deep%2C%20knowledge-intensive%0Areasoning%20%28MMLU-Pro%2C%20r%3D0.939%29.%20By%20making%20progress%20measurable%2C%20we%20aim%20to%0Aaccelerate%20the%20development%20of%20more%20faithful%20LLM%20simulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17516v2&entry.124074799=Read"},
{"title": "A Geometric Approach to Steerable Convolutions", "author": "Soumyabrata Kundu and Risi Kondor", "abstract": "  In contrast to the somewhat abstract, group theoretical approach adopted by\nmany papers, our work provides a new and more intuitive derivation of steerable\nconvolutional neural networks in $d$ dimensions. This derivation is based on\ngeometric arguments and fundamental principles of pattern matching. We offer an\nintuitive explanation for the appearance of the Clebsch--Gordan decomposition\nand spherical harmonic basis functions. Furthermore, we suggest a novel way to\nconstruct steerable convolution layers using interpolation kernels that improve\nupon existing implementation, and offer greater robustness to noisy data.\n", "link": "http://arxiv.org/abs/2510.18813v1", "date": "2025-10-21", "relevancy": 2.033, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5339}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.512}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Geometric%20Approach%20to%20Steerable%20Convolutions&body=Title%3A%20A%20Geometric%20Approach%20to%20Steerable%20Convolutions%0AAuthor%3A%20Soumyabrata%20Kundu%20and%20Risi%20Kondor%0AAbstract%3A%20%20%20In%20contrast%20to%20the%20somewhat%20abstract%2C%20group%20theoretical%20approach%20adopted%20by%0Amany%20papers%2C%20our%20work%20provides%20a%20new%20and%20more%20intuitive%20derivation%20of%20steerable%0Aconvolutional%20neural%20networks%20in%20%24d%24%20dimensions.%20This%20derivation%20is%20based%20on%0Ageometric%20arguments%20and%20fundamental%20principles%20of%20pattern%20matching.%20We%20offer%20an%0Aintuitive%20explanation%20for%20the%20appearance%20of%20the%20Clebsch--Gordan%20decomposition%0Aand%20spherical%20harmonic%20basis%20functions.%20Furthermore%2C%20we%20suggest%20a%20novel%20way%20to%0Aconstruct%20steerable%20convolution%20layers%20using%20interpolation%20kernels%20that%20improve%0Aupon%20existing%20implementation%2C%20and%20offer%20greater%20robustness%20to%20noisy%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Geometric%2520Approach%2520to%2520Steerable%2520Convolutions%26entry.906535625%3DSoumyabrata%2520Kundu%2520and%2520Risi%2520Kondor%26entry.1292438233%3D%2520%2520In%2520contrast%2520to%2520the%2520somewhat%2520abstract%252C%2520group%2520theoretical%2520approach%2520adopted%2520by%250Amany%2520papers%252C%2520our%2520work%2520provides%2520a%2520new%2520and%2520more%2520intuitive%2520derivation%2520of%2520steerable%250Aconvolutional%2520neural%2520networks%2520in%2520%2524d%2524%2520dimensions.%2520This%2520derivation%2520is%2520based%2520on%250Ageometric%2520arguments%2520and%2520fundamental%2520principles%2520of%2520pattern%2520matching.%2520We%2520offer%2520an%250Aintuitive%2520explanation%2520for%2520the%2520appearance%2520of%2520the%2520Clebsch--Gordan%2520decomposition%250Aand%2520spherical%2520harmonic%2520basis%2520functions.%2520Furthermore%252C%2520we%2520suggest%2520a%2520novel%2520way%2520to%250Aconstruct%2520steerable%2520convolution%2520layers%2520using%2520interpolation%2520kernels%2520that%2520improve%250Aupon%2520existing%2520implementation%252C%2520and%2520offer%2520greater%2520robustness%2520to%2520noisy%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Geometric%20Approach%20to%20Steerable%20Convolutions&entry.906535625=Soumyabrata%20Kundu%20and%20Risi%20Kondor&entry.1292438233=%20%20In%20contrast%20to%20the%20somewhat%20abstract%2C%20group%20theoretical%20approach%20adopted%20by%0Amany%20papers%2C%20our%20work%20provides%20a%20new%20and%20more%20intuitive%20derivation%20of%20steerable%0Aconvolutional%20neural%20networks%20in%20%24d%24%20dimensions.%20This%20derivation%20is%20based%20on%0Ageometric%20arguments%20and%20fundamental%20principles%20of%20pattern%20matching.%20We%20offer%20an%0Aintuitive%20explanation%20for%20the%20appearance%20of%20the%20Clebsch--Gordan%20decomposition%0Aand%20spherical%20harmonic%20basis%20functions.%20Furthermore%2C%20we%20suggest%20a%20novel%20way%20to%0Aconstruct%20steerable%20convolution%20layers%20using%20interpolation%20kernels%20that%20improve%0Aupon%20existing%20implementation%2C%20and%20offer%20greater%20robustness%20to%20noisy%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18813v1&entry.124074799=Read"},
{"title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal\n  Inconsistencies", "author": "Lukas Selch and Yufang Hou and M. Jehanzeb Mirza and Sivan Doveh and James Glass and Rogerio Feris and Wei Lin", "abstract": "  Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.\n", "link": "http://arxiv.org/abs/2510.16505v2", "date": "2025-10-21", "relevancy": 2.0328, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRISMM-Bench%3A%20A%20Benchmark%20of%20Peer-Review%20Grounded%20Multimodal%0A%20%20Inconsistencies&body=Title%3A%20PRISMM-Bench%3A%20A%20Benchmark%20of%20Peer-Review%20Grounded%20Multimodal%0A%20%20Inconsistencies%0AAuthor%3A%20Lukas%20Selch%20and%20Yufang%20Hou%20and%20M.%20Jehanzeb%20Mirza%20and%20Sivan%20Doveh%20and%20James%20Glass%20and%20Rogerio%20Feris%20and%20Wei%20Lin%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20are%20increasingly%20applied%20to%20scientific%0Aresearch%2C%20yet%20it%20remains%20unclear%20whether%20they%20can%20reliably%20understand%20and%0Areason%20over%20the%20multimodal%20complexity%20of%20papers.%20A%20central%20challenge%20lies%20in%0Adetecting%20and%20resolving%20inconsistencies%20across%20text%2C%20figures%2C%20tables%2C%20and%0Aequations%2C%20issues%20that%20are%20often%20subtle%2C%20domain-specific%2C%20and%20ultimately%0Aundermine%20clarity%2C%20reproducibility%2C%20and%20trust.%20Existing%20benchmarks%20overlook%0Athis%20issue%2C%20either%20isolating%20single%20modalities%20or%20relying%20on%20synthetic%20errors%0Athat%20fail%20to%20capture%20real-world%20complexity.%20We%20introduce%20PRISMM-Bench%0A%28Peer-Review-sourced%20Inconsistency%20Set%20for%20Multimodal%20Models%29%2C%20the%20first%0Abenchmark%20grounded%20in%20real%20reviewer-flagged%20inconsistencies%20in%20scientific%0Apapers.%20Through%20a%20multi-stage%20pipeline%20of%20review%20mining%2C%20LLM-assisted%20filtering%0Aand%20human%20verification%2C%20we%20curate%20262%20inconsistencies%20from%20242%20papers.%20Based%20on%0Athis%20set%2C%20we%20design%20three%20tasks%2C%20namely%20inconsistency%20identification%2C%20remedy%0Aand%20pair%20matching%2C%20which%20assess%20a%20model%27s%20capacity%20to%20detect%2C%20correct%2C%20and%0Areason%20over%20inconsistencies%20across%20different%20modalities.%20Furthermore%2C%20to%0Aaddress%20the%20notorious%20problem%20of%20choice-only%20shortcuts%20in%20multiple-choice%0Aevaluation%2C%20where%20models%20exploit%20answer%20patterns%20without%20truly%20understanding%0Athe%20question%2C%20we%20further%20introduce%20structured%20JSON-based%20answer%20representations%0Athat%20minimize%20linguistic%20biases%20by%20reducing%20reliance%20on%20superficial%20stylistic%0Acues.%20We%20benchmark%2021%20leading%20LMMs%2C%20including%20large%20open-weight%20models%0A%28GLM-4.5V%20106B%2C%20InternVL3%2078B%29%20and%20proprietary%20models%20%28Gemini%202.5%20Pro%2C%20GPT-5%0Awith%20high%20reasoning%29.%20Results%20reveal%20strikingly%20low%20performance%20%2826.1-54.2%25%29%2C%0Aunderscoring%20the%20challenge%20of%20multimodal%20scientific%20reasoning%20and%20motivating%0Aprogress%20towards%20trustworthy%20scientific%20assistants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.16505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRISMM-Bench%253A%2520A%2520Benchmark%2520of%2520Peer-Review%2520Grounded%2520Multimodal%250A%2520%2520Inconsistencies%26entry.906535625%3DLukas%2520Selch%2520and%2520Yufang%2520Hou%2520and%2520M.%2520Jehanzeb%2520Mirza%2520and%2520Sivan%2520Doveh%2520and%2520James%2520Glass%2520and%2520Rogerio%2520Feris%2520and%2520Wei%2520Lin%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520are%2520increasingly%2520applied%2520to%2520scientific%250Aresearch%252C%2520yet%2520it%2520remains%2520unclear%2520whether%2520they%2520can%2520reliably%2520understand%2520and%250Areason%2520over%2520the%2520multimodal%2520complexity%2520of%2520papers.%2520A%2520central%2520challenge%2520lies%2520in%250Adetecting%2520and%2520resolving%2520inconsistencies%2520across%2520text%252C%2520figures%252C%2520tables%252C%2520and%250Aequations%252C%2520issues%2520that%2520are%2520often%2520subtle%252C%2520domain-specific%252C%2520and%2520ultimately%250Aundermine%2520clarity%252C%2520reproducibility%252C%2520and%2520trust.%2520Existing%2520benchmarks%2520overlook%250Athis%2520issue%252C%2520either%2520isolating%2520single%2520modalities%2520or%2520relying%2520on%2520synthetic%2520errors%250Athat%2520fail%2520to%2520capture%2520real-world%2520complexity.%2520We%2520introduce%2520PRISMM-Bench%250A%2528Peer-Review-sourced%2520Inconsistency%2520Set%2520for%2520Multimodal%2520Models%2529%252C%2520the%2520first%250Abenchmark%2520grounded%2520in%2520real%2520reviewer-flagged%2520inconsistencies%2520in%2520scientific%250Apapers.%2520Through%2520a%2520multi-stage%2520pipeline%2520of%2520review%2520mining%252C%2520LLM-assisted%2520filtering%250Aand%2520human%2520verification%252C%2520we%2520curate%2520262%2520inconsistencies%2520from%2520242%2520papers.%2520Based%2520on%250Athis%2520set%252C%2520we%2520design%2520three%2520tasks%252C%2520namely%2520inconsistency%2520identification%252C%2520remedy%250Aand%2520pair%2520matching%252C%2520which%2520assess%2520a%2520model%2527s%2520capacity%2520to%2520detect%252C%2520correct%252C%2520and%250Areason%2520over%2520inconsistencies%2520across%2520different%2520modalities.%2520Furthermore%252C%2520to%250Aaddress%2520the%2520notorious%2520problem%2520of%2520choice-only%2520shortcuts%2520in%2520multiple-choice%250Aevaluation%252C%2520where%2520models%2520exploit%2520answer%2520patterns%2520without%2520truly%2520understanding%250Athe%2520question%252C%2520we%2520further%2520introduce%2520structured%2520JSON-based%2520answer%2520representations%250Athat%2520minimize%2520linguistic%2520biases%2520by%2520reducing%2520reliance%2520on%2520superficial%2520stylistic%250Acues.%2520We%2520benchmark%252021%2520leading%2520LMMs%252C%2520including%2520large%2520open-weight%2520models%250A%2528GLM-4.5V%2520106B%252C%2520InternVL3%252078B%2529%2520and%2520proprietary%2520models%2520%2528Gemini%25202.5%2520Pro%252C%2520GPT-5%250Awith%2520high%2520reasoning%2529.%2520Results%2520reveal%2520strikingly%2520low%2520performance%2520%252826.1-54.2%2525%2529%252C%250Aunderscoring%2520the%2520challenge%2520of%2520multimodal%2520scientific%2520reasoning%2520and%2520motivating%250Aprogress%2520towards%2520trustworthy%2520scientific%2520assistants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISMM-Bench%3A%20A%20Benchmark%20of%20Peer-Review%20Grounded%20Multimodal%0A%20%20Inconsistencies&entry.906535625=Lukas%20Selch%20and%20Yufang%20Hou%20and%20M.%20Jehanzeb%20Mirza%20and%20Sivan%20Doveh%20and%20James%20Glass%20and%20Rogerio%20Feris%20and%20Wei%20Lin&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20are%20increasingly%20applied%20to%20scientific%0Aresearch%2C%20yet%20it%20remains%20unclear%20whether%20they%20can%20reliably%20understand%20and%0Areason%20over%20the%20multimodal%20complexity%20of%20papers.%20A%20central%20challenge%20lies%20in%0Adetecting%20and%20resolving%20inconsistencies%20across%20text%2C%20figures%2C%20tables%2C%20and%0Aequations%2C%20issues%20that%20are%20often%20subtle%2C%20domain-specific%2C%20and%20ultimately%0Aundermine%20clarity%2C%20reproducibility%2C%20and%20trust.%20Existing%20benchmarks%20overlook%0Athis%20issue%2C%20either%20isolating%20single%20modalities%20or%20relying%20on%20synthetic%20errors%0Athat%20fail%20to%20capture%20real-world%20complexity.%20We%20introduce%20PRISMM-Bench%0A%28Peer-Review-sourced%20Inconsistency%20Set%20for%20Multimodal%20Models%29%2C%20the%20first%0Abenchmark%20grounded%20in%20real%20reviewer-flagged%20inconsistencies%20in%20scientific%0Apapers.%20Through%20a%20multi-stage%20pipeline%20of%20review%20mining%2C%20LLM-assisted%20filtering%0Aand%20human%20verification%2C%20we%20curate%20262%20inconsistencies%20from%20242%20papers.%20Based%20on%0Athis%20set%2C%20we%20design%20three%20tasks%2C%20namely%20inconsistency%20identification%2C%20remedy%0Aand%20pair%20matching%2C%20which%20assess%20a%20model%27s%20capacity%20to%20detect%2C%20correct%2C%20and%0Areason%20over%20inconsistencies%20across%20different%20modalities.%20Furthermore%2C%20to%0Aaddress%20the%20notorious%20problem%20of%20choice-only%20shortcuts%20in%20multiple-choice%0Aevaluation%2C%20where%20models%20exploit%20answer%20patterns%20without%20truly%20understanding%0Athe%20question%2C%20we%20further%20introduce%20structured%20JSON-based%20answer%20representations%0Athat%20minimize%20linguistic%20biases%20by%20reducing%20reliance%20on%20superficial%20stylistic%0Acues.%20We%20benchmark%2021%20leading%20LMMs%2C%20including%20large%20open-weight%20models%0A%28GLM-4.5V%20106B%2C%20InternVL3%2078B%29%20and%20proprietary%20models%20%28Gemini%202.5%20Pro%2C%20GPT-5%0Awith%20high%20reasoning%29.%20Results%20reveal%20strikingly%20low%20performance%20%2826.1-54.2%25%29%2C%0Aunderscoring%20the%20challenge%20of%20multimodal%20scientific%20reasoning%20and%20motivating%0Aprogress%20towards%20trustworthy%20scientific%20assistants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.16505v2&entry.124074799=Read"},
{"title": "How Transformers Learn In-Context Recall Tasks? Optimality, Training\n  Dynamics and Generalization", "author": "Quan Nguyen and Thanh Nguyen-Tang", "abstract": "  We study the approximation capabilities, convergence speeds and\non-convergence behaviors of transformers trained on in-context recall tasks --\nwhich requires to recognize the \\emph{positional} association between a pair of\ntokens from in-context examples. Existing theoretical results only focus on the\nin-context reasoning behavior of transformers after being trained for the\n\\emph{one} gradient descent step. It remains unclear what is the on-convergence\nbehavior of transformers being trained by gradient descent and how fast the\nconvergence rate is. In addition, the generalization of transformers in\none-step in-context reasoning has not been formally investigated. This work\naddresses these gaps. We first show that a class of transformers with either\nlinear, ReLU or softmax attentions, is provably Bayes-optimal for an in-context\nrecall task. When being trained with gradient descent, we show via a\nfinite-sample analysis that the expected loss converges at linear rate to the\nBayes risks. Moreover, we show that the trained transformers exhibit\nout-of-distribution (OOD) generalization, i.e., generalizing to samples outside\nof the population distribution. Our theoretical findings are further supported\nby extensive empirical validations, showing that \\emph{without} proper\nparameterization, models with larger expressive power surprisingly \\emph{fail}\nto generalize OOD after being trained by gradient descent.\n", "link": "http://arxiv.org/abs/2505.15009v3", "date": "2025-10-21", "relevancy": 2.0297, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5615}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Transformers%20Learn%20In-Context%20Recall%20Tasks%3F%20Optimality%2C%20Training%0A%20%20Dynamics%20and%20Generalization&body=Title%3A%20How%20Transformers%20Learn%20In-Context%20Recall%20Tasks%3F%20Optimality%2C%20Training%0A%20%20Dynamics%20and%20Generalization%0AAuthor%3A%20Quan%20Nguyen%20and%20Thanh%20Nguyen-Tang%0AAbstract%3A%20%20%20We%20study%20the%20approximation%20capabilities%2C%20convergence%20speeds%20and%0Aon-convergence%20behaviors%20of%20transformers%20trained%20on%20in-context%20recall%20tasks%20--%0Awhich%20requires%20to%20recognize%20the%20%5Cemph%7Bpositional%7D%20association%20between%20a%20pair%20of%0Atokens%20from%20in-context%20examples.%20Existing%20theoretical%20results%20only%20focus%20on%20the%0Ain-context%20reasoning%20behavior%20of%20transformers%20after%20being%20trained%20for%20the%0A%5Cemph%7Bone%7D%20gradient%20descent%20step.%20It%20remains%20unclear%20what%20is%20the%20on-convergence%0Abehavior%20of%20transformers%20being%20trained%20by%20gradient%20descent%20and%20how%20fast%20the%0Aconvergence%20rate%20is.%20In%20addition%2C%20the%20generalization%20of%20transformers%20in%0Aone-step%20in-context%20reasoning%20has%20not%20been%20formally%20investigated.%20This%20work%0Aaddresses%20these%20gaps.%20We%20first%20show%20that%20a%20class%20of%20transformers%20with%20either%0Alinear%2C%20ReLU%20or%20softmax%20attentions%2C%20is%20provably%20Bayes-optimal%20for%20an%20in-context%0Arecall%20task.%20When%20being%20trained%20with%20gradient%20descent%2C%20we%20show%20via%20a%0Afinite-sample%20analysis%20that%20the%20expected%20loss%20converges%20at%20linear%20rate%20to%20the%0ABayes%20risks.%20Moreover%2C%20we%20show%20that%20the%20trained%20transformers%20exhibit%0Aout-of-distribution%20%28OOD%29%20generalization%2C%20i.e.%2C%20generalizing%20to%20samples%20outside%0Aof%20the%20population%20distribution.%20Our%20theoretical%20findings%20are%20further%20supported%0Aby%20extensive%20empirical%20validations%2C%20showing%20that%20%5Cemph%7Bwithout%7D%20proper%0Aparameterization%2C%20models%20with%20larger%20expressive%20power%20surprisingly%20%5Cemph%7Bfail%7D%0Ato%20generalize%20OOD%20after%20being%20trained%20by%20gradient%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Transformers%2520Learn%2520In-Context%2520Recall%2520Tasks%253F%2520Optimality%252C%2520Training%250A%2520%2520Dynamics%2520and%2520Generalization%26entry.906535625%3DQuan%2520Nguyen%2520and%2520Thanh%2520Nguyen-Tang%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520approximation%2520capabilities%252C%2520convergence%2520speeds%2520and%250Aon-convergence%2520behaviors%2520of%2520transformers%2520trained%2520on%2520in-context%2520recall%2520tasks%2520--%250Awhich%2520requires%2520to%2520recognize%2520the%2520%255Cemph%257Bpositional%257D%2520association%2520between%2520a%2520pair%2520of%250Atokens%2520from%2520in-context%2520examples.%2520Existing%2520theoretical%2520results%2520only%2520focus%2520on%2520the%250Ain-context%2520reasoning%2520behavior%2520of%2520transformers%2520after%2520being%2520trained%2520for%2520the%250A%255Cemph%257Bone%257D%2520gradient%2520descent%2520step.%2520It%2520remains%2520unclear%2520what%2520is%2520the%2520on-convergence%250Abehavior%2520of%2520transformers%2520being%2520trained%2520by%2520gradient%2520descent%2520and%2520how%2520fast%2520the%250Aconvergence%2520rate%2520is.%2520In%2520addition%252C%2520the%2520generalization%2520of%2520transformers%2520in%250Aone-step%2520in-context%2520reasoning%2520has%2520not%2520been%2520formally%2520investigated.%2520This%2520work%250Aaddresses%2520these%2520gaps.%2520We%2520first%2520show%2520that%2520a%2520class%2520of%2520transformers%2520with%2520either%250Alinear%252C%2520ReLU%2520or%2520softmax%2520attentions%252C%2520is%2520provably%2520Bayes-optimal%2520for%2520an%2520in-context%250Arecall%2520task.%2520When%2520being%2520trained%2520with%2520gradient%2520descent%252C%2520we%2520show%2520via%2520a%250Afinite-sample%2520analysis%2520that%2520the%2520expected%2520loss%2520converges%2520at%2520linear%2520rate%2520to%2520the%250ABayes%2520risks.%2520Moreover%252C%2520we%2520show%2520that%2520the%2520trained%2520transformers%2520exhibit%250Aout-of-distribution%2520%2528OOD%2529%2520generalization%252C%2520i.e.%252C%2520generalizing%2520to%2520samples%2520outside%250Aof%2520the%2520population%2520distribution.%2520Our%2520theoretical%2520findings%2520are%2520further%2520supported%250Aby%2520extensive%2520empirical%2520validations%252C%2520showing%2520that%2520%255Cemph%257Bwithout%257D%2520proper%250Aparameterization%252C%2520models%2520with%2520larger%2520expressive%2520power%2520surprisingly%2520%255Cemph%257Bfail%257D%250Ato%2520generalize%2520OOD%2520after%2520being%2520trained%2520by%2520gradient%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Transformers%20Learn%20In-Context%20Recall%20Tasks%3F%20Optimality%2C%20Training%0A%20%20Dynamics%20and%20Generalization&entry.906535625=Quan%20Nguyen%20and%20Thanh%20Nguyen-Tang&entry.1292438233=%20%20We%20study%20the%20approximation%20capabilities%2C%20convergence%20speeds%20and%0Aon-convergence%20behaviors%20of%20transformers%20trained%20on%20in-context%20recall%20tasks%20--%0Awhich%20requires%20to%20recognize%20the%20%5Cemph%7Bpositional%7D%20association%20between%20a%20pair%20of%0Atokens%20from%20in-context%20examples.%20Existing%20theoretical%20results%20only%20focus%20on%20the%0Ain-context%20reasoning%20behavior%20of%20transformers%20after%20being%20trained%20for%20the%0A%5Cemph%7Bone%7D%20gradient%20descent%20step.%20It%20remains%20unclear%20what%20is%20the%20on-convergence%0Abehavior%20of%20transformers%20being%20trained%20by%20gradient%20descent%20and%20how%20fast%20the%0Aconvergence%20rate%20is.%20In%20addition%2C%20the%20generalization%20of%20transformers%20in%0Aone-step%20in-context%20reasoning%20has%20not%20been%20formally%20investigated.%20This%20work%0Aaddresses%20these%20gaps.%20We%20first%20show%20that%20a%20class%20of%20transformers%20with%20either%0Alinear%2C%20ReLU%20or%20softmax%20attentions%2C%20is%20provably%20Bayes-optimal%20for%20an%20in-context%0Arecall%20task.%20When%20being%20trained%20with%20gradient%20descent%2C%20we%20show%20via%20a%0Afinite-sample%20analysis%20that%20the%20expected%20loss%20converges%20at%20linear%20rate%20to%20the%0ABayes%20risks.%20Moreover%2C%20we%20show%20that%20the%20trained%20transformers%20exhibit%0Aout-of-distribution%20%28OOD%29%20generalization%2C%20i.e.%2C%20generalizing%20to%20samples%20outside%0Aof%20the%20population%20distribution.%20Our%20theoretical%20findings%20are%20further%20supported%0Aby%20extensive%20empirical%20validations%2C%20showing%20that%20%5Cemph%7Bwithout%7D%20proper%0Aparameterization%2C%20models%20with%20larger%20expressive%20power%20surprisingly%20%5Cemph%7Bfail%7D%0Ato%20generalize%20OOD%20after%20being%20trained%20by%20gradient%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15009v3&entry.124074799=Read"},
{"title": "Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically\n  Valid Molecular Docking", "author": "Daria Frolova and Talgat Daulbaev and Egor Sevriugov and Sergei A. Nikolenko and Dmitry N. Ivankov and Ivan Oseledets and Marina A. Pak", "abstract": "  Accurate prediction of protein-ligand binding poses is crucial for\nstructure-based drug design, yet existing methods struggle to balance speed,\naccuracy, and physical plausibility. We introduce Matcha, a novel molecular\ndocking pipeline that combines multi-stage flow matching with learned scoring\nand physical validity filtering. Our approach consists of three sequential\nstages applied consecutively to refine docking predictions, each implemented as\na flow matching model operating on appropriate geometric spaces\n($\\mathbb{R}^3$, $\\mathrm{SO}(3)$, and $\\mathrm{SO}(2)$). We enhance the\nprediction quality through a dedicated scoring model and apply unsupervised\nphysical validity filters to eliminate unrealistic poses. Compared to various\napproaches, Matcha demonstrates superior performance on Astex and PDBbind test\nsets in terms of docking success rate and physical plausibility. Moreover, our\nmethod works approximately 25 times faster than modern large-scale co-folding\nmodels. The model weights and inference code to reproduce our results are\navailable at https://github.com/LigandPro/Matcha.\n", "link": "http://arxiv.org/abs/2510.14586v2", "date": "2025-10-21", "relevancy": 2.0234, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5304}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matcha%3A%20Multi-Stage%20Riemannian%20Flow%20Matching%20for%20Accurate%20and%20Physically%0A%20%20Valid%20Molecular%20Docking&body=Title%3A%20Matcha%3A%20Multi-Stage%20Riemannian%20Flow%20Matching%20for%20Accurate%20and%20Physically%0A%20%20Valid%20Molecular%20Docking%0AAuthor%3A%20Daria%20Frolova%20and%20Talgat%20Daulbaev%20and%20Egor%20Sevriugov%20and%20Sergei%20A.%20Nikolenko%20and%20Dmitry%20N.%20Ivankov%20and%20Ivan%20Oseledets%20and%20Marina%20A.%20Pak%0AAbstract%3A%20%20%20Accurate%20prediction%20of%20protein-ligand%20binding%20poses%20is%20crucial%20for%0Astructure-based%20drug%20design%2C%20yet%20existing%20methods%20struggle%20to%20balance%20speed%2C%0Aaccuracy%2C%20and%20physical%20plausibility.%20We%20introduce%20Matcha%2C%20a%20novel%20molecular%0Adocking%20pipeline%20that%20combines%20multi-stage%20flow%20matching%20with%20learned%20scoring%0Aand%20physical%20validity%20filtering.%20Our%20approach%20consists%20of%20three%20sequential%0Astages%20applied%20consecutively%20to%20refine%20docking%20predictions%2C%20each%20implemented%20as%0Aa%20flow%20matching%20model%20operating%20on%20appropriate%20geometric%20spaces%0A%28%24%5Cmathbb%7BR%7D%5E3%24%2C%20%24%5Cmathrm%7BSO%7D%283%29%24%2C%20and%20%24%5Cmathrm%7BSO%7D%282%29%24%29.%20We%20enhance%20the%0Aprediction%20quality%20through%20a%20dedicated%20scoring%20model%20and%20apply%20unsupervised%0Aphysical%20validity%20filters%20to%20eliminate%20unrealistic%20poses.%20Compared%20to%20various%0Aapproaches%2C%20Matcha%20demonstrates%20superior%20performance%20on%20Astex%20and%20PDBbind%20test%0Asets%20in%20terms%20of%20docking%20success%20rate%20and%20physical%20plausibility.%20Moreover%2C%20our%0Amethod%20works%20approximately%2025%20times%20faster%20than%20modern%20large-scale%20co-folding%0Amodels.%20The%20model%20weights%20and%20inference%20code%20to%20reproduce%20our%20results%20are%0Aavailable%20at%20https%3A//github.com/LigandPro/Matcha.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatcha%253A%2520Multi-Stage%2520Riemannian%2520Flow%2520Matching%2520for%2520Accurate%2520and%2520Physically%250A%2520%2520Valid%2520Molecular%2520Docking%26entry.906535625%3DDaria%2520Frolova%2520and%2520Talgat%2520Daulbaev%2520and%2520Egor%2520Sevriugov%2520and%2520Sergei%2520A.%2520Nikolenko%2520and%2520Dmitry%2520N.%2520Ivankov%2520and%2520Ivan%2520Oseledets%2520and%2520Marina%2520A.%2520Pak%26entry.1292438233%3D%2520%2520Accurate%2520prediction%2520of%2520protein-ligand%2520binding%2520poses%2520is%2520crucial%2520for%250Astructure-based%2520drug%2520design%252C%2520yet%2520existing%2520methods%2520struggle%2520to%2520balance%2520speed%252C%250Aaccuracy%252C%2520and%2520physical%2520plausibility.%2520We%2520introduce%2520Matcha%252C%2520a%2520novel%2520molecular%250Adocking%2520pipeline%2520that%2520combines%2520multi-stage%2520flow%2520matching%2520with%2520learned%2520scoring%250Aand%2520physical%2520validity%2520filtering.%2520Our%2520approach%2520consists%2520of%2520three%2520sequential%250Astages%2520applied%2520consecutively%2520to%2520refine%2520docking%2520predictions%252C%2520each%2520implemented%2520as%250Aa%2520flow%2520matching%2520model%2520operating%2520on%2520appropriate%2520geometric%2520spaces%250A%2528%2524%255Cmathbb%257BR%257D%255E3%2524%252C%2520%2524%255Cmathrm%257BSO%257D%25283%2529%2524%252C%2520and%2520%2524%255Cmathrm%257BSO%257D%25282%2529%2524%2529.%2520We%2520enhance%2520the%250Aprediction%2520quality%2520through%2520a%2520dedicated%2520scoring%2520model%2520and%2520apply%2520unsupervised%250Aphysical%2520validity%2520filters%2520to%2520eliminate%2520unrealistic%2520poses.%2520Compared%2520to%2520various%250Aapproaches%252C%2520Matcha%2520demonstrates%2520superior%2520performance%2520on%2520Astex%2520and%2520PDBbind%2520test%250Asets%2520in%2520terms%2520of%2520docking%2520success%2520rate%2520and%2520physical%2520plausibility.%2520Moreover%252C%2520our%250Amethod%2520works%2520approximately%252025%2520times%2520faster%2520than%2520modern%2520large-scale%2520co-folding%250Amodels.%2520The%2520model%2520weights%2520and%2520inference%2520code%2520to%2520reproduce%2520our%2520results%2520are%250Aavailable%2520at%2520https%253A//github.com/LigandPro/Matcha.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matcha%3A%20Multi-Stage%20Riemannian%20Flow%20Matching%20for%20Accurate%20and%20Physically%0A%20%20Valid%20Molecular%20Docking&entry.906535625=Daria%20Frolova%20and%20Talgat%20Daulbaev%20and%20Egor%20Sevriugov%20and%20Sergei%20A.%20Nikolenko%20and%20Dmitry%20N.%20Ivankov%20and%20Ivan%20Oseledets%20and%20Marina%20A.%20Pak&entry.1292438233=%20%20Accurate%20prediction%20of%20protein-ligand%20binding%20poses%20is%20crucial%20for%0Astructure-based%20drug%20design%2C%20yet%20existing%20methods%20struggle%20to%20balance%20speed%2C%0Aaccuracy%2C%20and%20physical%20plausibility.%20We%20introduce%20Matcha%2C%20a%20novel%20molecular%0Adocking%20pipeline%20that%20combines%20multi-stage%20flow%20matching%20with%20learned%20scoring%0Aand%20physical%20validity%20filtering.%20Our%20approach%20consists%20of%20three%20sequential%0Astages%20applied%20consecutively%20to%20refine%20docking%20predictions%2C%20each%20implemented%20as%0Aa%20flow%20matching%20model%20operating%20on%20appropriate%20geometric%20spaces%0A%28%24%5Cmathbb%7BR%7D%5E3%24%2C%20%24%5Cmathrm%7BSO%7D%283%29%24%2C%20and%20%24%5Cmathrm%7BSO%7D%282%29%24%29.%20We%20enhance%20the%0Aprediction%20quality%20through%20a%20dedicated%20scoring%20model%20and%20apply%20unsupervised%0Aphysical%20validity%20filters%20to%20eliminate%20unrealistic%20poses.%20Compared%20to%20various%0Aapproaches%2C%20Matcha%20demonstrates%20superior%20performance%20on%20Astex%20and%20PDBbind%20test%0Asets%20in%20terms%20of%20docking%20success%20rate%20and%20physical%20plausibility.%20Moreover%2C%20our%0Amethod%20works%20approximately%2025%20times%20faster%20than%20modern%20large-scale%20co-folding%0Amodels.%20The%20model%20weights%20and%20inference%20code%20to%20reproduce%20our%20results%20are%0Aavailable%20at%20https%3A//github.com/LigandPro/Matcha.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14586v2&entry.124074799=Read"},
{"title": "Image augmentation with invertible networks in interactive satellite\n  image change detection", "author": "Hichem Sahbi", "abstract": "  This paper devises a novel interactive satellite image change detection\nalgorithm based on active learning. Our framework employs an iterative process\nthat leverages a question-and-answer model. This model queries the oracle\n(user) about the labels of a small subset of images (dubbed as display), and\nbased on the oracle's responses, change detection model is dynamically updated.\nThe main contribution of our framework resides in a novel invertible network\nthat allows augmenting displays, by mapping them from highly nonlinear input\nspaces to latent ones, where augmentation transformations become linear and\nmore tractable. The resulting augmented data are afterwards mapped back to the\ninput space, and used to retrain more effective change detection criteria in\nthe subsequent iterations of active learning. Experimental results demonstrate\nsuperior performance of our proposed method compared to the related work.\n", "link": "http://arxiv.org/abs/2510.18660v1", "date": "2025-10-21", "relevancy": 2.0191, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5203}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5051}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20augmentation%20with%20invertible%20networks%20in%20interactive%20satellite%0A%20%20image%20change%20detection&body=Title%3A%20Image%20augmentation%20with%20invertible%20networks%20in%20interactive%20satellite%0A%20%20image%20change%20detection%0AAuthor%3A%20Hichem%20Sahbi%0AAbstract%3A%20%20%20This%20paper%20devises%20a%20novel%20interactive%20satellite%20image%20change%20detection%0Aalgorithm%20based%20on%20active%20learning.%20Our%20framework%20employs%20an%20iterative%20process%0Athat%20leverages%20a%20question-and-answer%20model.%20This%20model%20queries%20the%20oracle%0A%28user%29%20about%20the%20labels%20of%20a%20small%20subset%20of%20images%20%28dubbed%20as%20display%29%2C%20and%0Abased%20on%20the%20oracle%27s%20responses%2C%20change%20detection%20model%20is%20dynamically%20updated.%0AThe%20main%20contribution%20of%20our%20framework%20resides%20in%20a%20novel%20invertible%20network%0Athat%20allows%20augmenting%20displays%2C%20by%20mapping%20them%20from%20highly%20nonlinear%20input%0Aspaces%20to%20latent%20ones%2C%20where%20augmentation%20transformations%20become%20linear%20and%0Amore%20tractable.%20The%20resulting%20augmented%20data%20are%20afterwards%20mapped%20back%20to%20the%0Ainput%20space%2C%20and%20used%20to%20retrain%20more%20effective%20change%20detection%20criteria%20in%0Athe%20subsequent%20iterations%20of%20active%20learning.%20Experimental%20results%20demonstrate%0Asuperior%20performance%20of%20our%20proposed%20method%20compared%20to%20the%20related%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520augmentation%2520with%2520invertible%2520networks%2520in%2520interactive%2520satellite%250A%2520%2520image%2520change%2520detection%26entry.906535625%3DHichem%2520Sahbi%26entry.1292438233%3D%2520%2520This%2520paper%2520devises%2520a%2520novel%2520interactive%2520satellite%2520image%2520change%2520detection%250Aalgorithm%2520based%2520on%2520active%2520learning.%2520Our%2520framework%2520employs%2520an%2520iterative%2520process%250Athat%2520leverages%2520a%2520question-and-answer%2520model.%2520This%2520model%2520queries%2520the%2520oracle%250A%2528user%2529%2520about%2520the%2520labels%2520of%2520a%2520small%2520subset%2520of%2520images%2520%2528dubbed%2520as%2520display%2529%252C%2520and%250Abased%2520on%2520the%2520oracle%2527s%2520responses%252C%2520change%2520detection%2520model%2520is%2520dynamically%2520updated.%250AThe%2520main%2520contribution%2520of%2520our%2520framework%2520resides%2520in%2520a%2520novel%2520invertible%2520network%250Athat%2520allows%2520augmenting%2520displays%252C%2520by%2520mapping%2520them%2520from%2520highly%2520nonlinear%2520input%250Aspaces%2520to%2520latent%2520ones%252C%2520where%2520augmentation%2520transformations%2520become%2520linear%2520and%250Amore%2520tractable.%2520The%2520resulting%2520augmented%2520data%2520are%2520afterwards%2520mapped%2520back%2520to%2520the%250Ainput%2520space%252C%2520and%2520used%2520to%2520retrain%2520more%2520effective%2520change%2520detection%2520criteria%2520in%250Athe%2520subsequent%2520iterations%2520of%2520active%2520learning.%2520Experimental%2520results%2520demonstrate%250Asuperior%2520performance%2520of%2520our%2520proposed%2520method%2520compared%2520to%2520the%2520related%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20augmentation%20with%20invertible%20networks%20in%20interactive%20satellite%0A%20%20image%20change%20detection&entry.906535625=Hichem%20Sahbi&entry.1292438233=%20%20This%20paper%20devises%20a%20novel%20interactive%20satellite%20image%20change%20detection%0Aalgorithm%20based%20on%20active%20learning.%20Our%20framework%20employs%20an%20iterative%20process%0Athat%20leverages%20a%20question-and-answer%20model.%20This%20model%20queries%20the%20oracle%0A%28user%29%20about%20the%20labels%20of%20a%20small%20subset%20of%20images%20%28dubbed%20as%20display%29%2C%20and%0Abased%20on%20the%20oracle%27s%20responses%2C%20change%20detection%20model%20is%20dynamically%20updated.%0AThe%20main%20contribution%20of%20our%20framework%20resides%20in%20a%20novel%20invertible%20network%0Athat%20allows%20augmenting%20displays%2C%20by%20mapping%20them%20from%20highly%20nonlinear%20input%0Aspaces%20to%20latent%20ones%2C%20where%20augmentation%20transformations%20become%20linear%20and%0Amore%20tractable.%20The%20resulting%20augmented%20data%20are%20afterwards%20mapped%20back%20to%20the%0Ainput%20space%2C%20and%20used%20to%20retrain%20more%20effective%20change%20detection%20criteria%20in%0Athe%20subsequent%20iterations%20of%20active%20learning.%20Experimental%20results%20demonstrate%0Asuperior%20performance%20of%20our%20proposed%20method%20compared%20to%20the%20related%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18660v1&entry.124074799=Read"},
{"title": "Decoding Funded Research: Comparative Analysis of Topic Models and\n  Uncovering the Effect of Gender and Geographic Location", "author": "Shirin Tavakoli Kafiabad and Andrea Schiffauerova and Ashkan Ebadi", "abstract": "  Optimizing national scientific investment requires a clear understanding of\nevolving research trends and the demographic and geographical forces shaping\nthem, particularly in light of commitments to equity, diversity, and inclusion.\nThis study addresses this need by analyzing 18 years (2005-2022) of research\nproposals funded by the Natural Sciences and Engineering Research Council of\nCanada (NSERC). We conducted a comprehensive comparative evaluation of three\ntopic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic\nModelling (STM), and BERTopic. We also introduced a novel algorithm, named\nCOFFEE, designed to enable robust covariate effect estimation for BERTopic.\nThis advancement addresses a significant gap, as BERTopic lacks a native\nfunction for covariate analysis, unlike the probabilistic STM. Our findings\nhighlight that while all models effectively delineate core scientific domains,\nBERTopic outperformed by consistently identifying more granular, coherent, and\nemergent themes, such as the rapid expansion of artificial intelligence.\nAdditionally, the covariate analysis, powered by COFFEE, confirmed distinct\nprovincial research specializations and revealed consistent gender-based\nthematic patterns across various scientific disciplines. These insights offer a\nrobust empirical foundation for funding organizations to formulate more\nequitable and impactful funding strategies, thereby enhancing the effectiveness\nof the scientific ecosystem.\n", "link": "http://arxiv.org/abs/2510.18803v1", "date": "2025-10-21", "relevancy": 2.0171, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Funded%20Research%3A%20Comparative%20Analysis%20of%20Topic%20Models%20and%0A%20%20Uncovering%20the%20Effect%20of%20Gender%20and%20Geographic%20Location&body=Title%3A%20Decoding%20Funded%20Research%3A%20Comparative%20Analysis%20of%20Topic%20Models%20and%0A%20%20Uncovering%20the%20Effect%20of%20Gender%20and%20Geographic%20Location%0AAuthor%3A%20Shirin%20Tavakoli%20Kafiabad%20and%20Andrea%20Schiffauerova%20and%20Ashkan%20Ebadi%0AAbstract%3A%20%20%20Optimizing%20national%20scientific%20investment%20requires%20a%20clear%20understanding%20of%0Aevolving%20research%20trends%20and%20the%20demographic%20and%20geographical%20forces%20shaping%0Athem%2C%20particularly%20in%20light%20of%20commitments%20to%20equity%2C%20diversity%2C%20and%20inclusion.%0AThis%20study%20addresses%20this%20need%20by%20analyzing%2018%20years%20%282005-2022%29%20of%20research%0Aproposals%20funded%20by%20the%20Natural%20Sciences%20and%20Engineering%20Research%20Council%20of%0ACanada%20%28NSERC%29.%20We%20conducted%20a%20comprehensive%20comparative%20evaluation%20of%20three%0Atopic%20modelling%20approaches%3A%20Latent%20Dirichlet%20Allocation%20%28LDA%29%2C%20Structural%20Topic%0AModelling%20%28STM%29%2C%20and%20BERTopic.%20We%20also%20introduced%20a%20novel%20algorithm%2C%20named%0ACOFFEE%2C%20designed%20to%20enable%20robust%20covariate%20effect%20estimation%20for%20BERTopic.%0AThis%20advancement%20addresses%20a%20significant%20gap%2C%20as%20BERTopic%20lacks%20a%20native%0Afunction%20for%20covariate%20analysis%2C%20unlike%20the%20probabilistic%20STM.%20Our%20findings%0Ahighlight%20that%20while%20all%20models%20effectively%20delineate%20core%20scientific%20domains%2C%0ABERTopic%20outperformed%20by%20consistently%20identifying%20more%20granular%2C%20coherent%2C%20and%0Aemergent%20themes%2C%20such%20as%20the%20rapid%20expansion%20of%20artificial%20intelligence.%0AAdditionally%2C%20the%20covariate%20analysis%2C%20powered%20by%20COFFEE%2C%20confirmed%20distinct%0Aprovincial%20research%20specializations%20and%20revealed%20consistent%20gender-based%0Athematic%20patterns%20across%20various%20scientific%20disciplines.%20These%20insights%20offer%20a%0Arobust%20empirical%20foundation%20for%20funding%20organizations%20to%20formulate%20more%0Aequitable%20and%20impactful%20funding%20strategies%2C%20thereby%20enhancing%20the%20effectiveness%0Aof%20the%20scientific%20ecosystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Funded%2520Research%253A%2520Comparative%2520Analysis%2520of%2520Topic%2520Models%2520and%250A%2520%2520Uncovering%2520the%2520Effect%2520of%2520Gender%2520and%2520Geographic%2520Location%26entry.906535625%3DShirin%2520Tavakoli%2520Kafiabad%2520and%2520Andrea%2520Schiffauerova%2520and%2520Ashkan%2520Ebadi%26entry.1292438233%3D%2520%2520Optimizing%2520national%2520scientific%2520investment%2520requires%2520a%2520clear%2520understanding%2520of%250Aevolving%2520research%2520trends%2520and%2520the%2520demographic%2520and%2520geographical%2520forces%2520shaping%250Athem%252C%2520particularly%2520in%2520light%2520of%2520commitments%2520to%2520equity%252C%2520diversity%252C%2520and%2520inclusion.%250AThis%2520study%2520addresses%2520this%2520need%2520by%2520analyzing%252018%2520years%2520%25282005-2022%2529%2520of%2520research%250Aproposals%2520funded%2520by%2520the%2520Natural%2520Sciences%2520and%2520Engineering%2520Research%2520Council%2520of%250ACanada%2520%2528NSERC%2529.%2520We%2520conducted%2520a%2520comprehensive%2520comparative%2520evaluation%2520of%2520three%250Atopic%2520modelling%2520approaches%253A%2520Latent%2520Dirichlet%2520Allocation%2520%2528LDA%2529%252C%2520Structural%2520Topic%250AModelling%2520%2528STM%2529%252C%2520and%2520BERTopic.%2520We%2520also%2520introduced%2520a%2520novel%2520algorithm%252C%2520named%250ACOFFEE%252C%2520designed%2520to%2520enable%2520robust%2520covariate%2520effect%2520estimation%2520for%2520BERTopic.%250AThis%2520advancement%2520addresses%2520a%2520significant%2520gap%252C%2520as%2520BERTopic%2520lacks%2520a%2520native%250Afunction%2520for%2520covariate%2520analysis%252C%2520unlike%2520the%2520probabilistic%2520STM.%2520Our%2520findings%250Ahighlight%2520that%2520while%2520all%2520models%2520effectively%2520delineate%2520core%2520scientific%2520domains%252C%250ABERTopic%2520outperformed%2520by%2520consistently%2520identifying%2520more%2520granular%252C%2520coherent%252C%2520and%250Aemergent%2520themes%252C%2520such%2520as%2520the%2520rapid%2520expansion%2520of%2520artificial%2520intelligence.%250AAdditionally%252C%2520the%2520covariate%2520analysis%252C%2520powered%2520by%2520COFFEE%252C%2520confirmed%2520distinct%250Aprovincial%2520research%2520specializations%2520and%2520revealed%2520consistent%2520gender-based%250Athematic%2520patterns%2520across%2520various%2520scientific%2520disciplines.%2520These%2520insights%2520offer%2520a%250Arobust%2520empirical%2520foundation%2520for%2520funding%2520organizations%2520to%2520formulate%2520more%250Aequitable%2520and%2520impactful%2520funding%2520strategies%252C%2520thereby%2520enhancing%2520the%2520effectiveness%250Aof%2520the%2520scientific%2520ecosystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Funded%20Research%3A%20Comparative%20Analysis%20of%20Topic%20Models%20and%0A%20%20Uncovering%20the%20Effect%20of%20Gender%20and%20Geographic%20Location&entry.906535625=Shirin%20Tavakoli%20Kafiabad%20and%20Andrea%20Schiffauerova%20and%20Ashkan%20Ebadi&entry.1292438233=%20%20Optimizing%20national%20scientific%20investment%20requires%20a%20clear%20understanding%20of%0Aevolving%20research%20trends%20and%20the%20demographic%20and%20geographical%20forces%20shaping%0Athem%2C%20particularly%20in%20light%20of%20commitments%20to%20equity%2C%20diversity%2C%20and%20inclusion.%0AThis%20study%20addresses%20this%20need%20by%20analyzing%2018%20years%20%282005-2022%29%20of%20research%0Aproposals%20funded%20by%20the%20Natural%20Sciences%20and%20Engineering%20Research%20Council%20of%0ACanada%20%28NSERC%29.%20We%20conducted%20a%20comprehensive%20comparative%20evaluation%20of%20three%0Atopic%20modelling%20approaches%3A%20Latent%20Dirichlet%20Allocation%20%28LDA%29%2C%20Structural%20Topic%0AModelling%20%28STM%29%2C%20and%20BERTopic.%20We%20also%20introduced%20a%20novel%20algorithm%2C%20named%0ACOFFEE%2C%20designed%20to%20enable%20robust%20covariate%20effect%20estimation%20for%20BERTopic.%0AThis%20advancement%20addresses%20a%20significant%20gap%2C%20as%20BERTopic%20lacks%20a%20native%0Afunction%20for%20covariate%20analysis%2C%20unlike%20the%20probabilistic%20STM.%20Our%20findings%0Ahighlight%20that%20while%20all%20models%20effectively%20delineate%20core%20scientific%20domains%2C%0ABERTopic%20outperformed%20by%20consistently%20identifying%20more%20granular%2C%20coherent%2C%20and%0Aemergent%20themes%2C%20such%20as%20the%20rapid%20expansion%20of%20artificial%20intelligence.%0AAdditionally%2C%20the%20covariate%20analysis%2C%20powered%20by%20COFFEE%2C%20confirmed%20distinct%0Aprovincial%20research%20specializations%20and%20revealed%20consistent%20gender-based%0Athematic%20patterns%20across%20various%20scientific%20disciplines.%20These%20insights%20offer%20a%0Arobust%20empirical%20foundation%20for%20funding%20organizations%20to%20formulate%20more%0Aequitable%20and%20impactful%20funding%20strategies%2C%20thereby%20enhancing%20the%20effectiveness%0Aof%20the%20scientific%20ecosystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18803v1&entry.124074799=Read"},
{"title": "Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries", "author": "Marius Dragoi and Ioana Pintilie and Florin Gogianu and Florin Brad", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm to improve Large Language Models on reasoning tasks such as\ncoding, math or logic. To assess the reasoning boundary (the fraction of\nproblems a model can solve) researchers often report Pass@k at large sampling\nbudgets. Recent results reveal a crossover phenomenon: while RLVR models\noutperform the base model at small k values, the base model usually outperforms\nthem when sampling a very large number of completions. This has been\ninterpreted as evidence that base models have a larger reasoning boundary. We\nargue that on tasks with discrete answer spaces, such as math with numeric\noutputs, Pass@k at large k reflects the increasingly higher chance of success\nin the limit of the number of trials rather than genuine reasoning, and can\ntherefore be misleading. We propose Cover@tau, which measures the fraction of\nproblems that a model can solve for which at least a tau proportion of\ncompletions are correct. Unlike Pass@k, Cover@tau captures reasoning under an\nexplicit reliability threshold: models that rely on random guessing degrade\nrapidly as tau increases. We evaluate several RLVR models using Cover@tau-based\nmetrics and illustrate how the relative rankings of popular algorithms change\ncompared to Pass@1, offering a different perspective on reasoning boundaries.\n", "link": "http://arxiv.org/abs/2510.08325v2", "date": "2025-10-21", "relevancy": 2.0128, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5072}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5072}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Pass%40k%3A%20Breadth-Depth%20Metrics%20for%20Reasoning%20Boundaries&body=Title%3A%20Beyond%20Pass%40k%3A%20Breadth-Depth%20Metrics%20for%20Reasoning%20Boundaries%0AAuthor%3A%20Marius%20Dragoi%20and%20Ioana%20Pintilie%20and%20Florin%20Gogianu%20and%20Florin%20Brad%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20to%20improve%20Large%20Language%20Models%20on%20reasoning%20tasks%20such%20as%0Acoding%2C%20math%20or%20logic.%20To%20assess%20the%20reasoning%20boundary%20%28the%20fraction%20of%0Aproblems%20a%20model%20can%20solve%29%20researchers%20often%20report%20Pass%40k%20at%20large%20sampling%0Abudgets.%20Recent%20results%20reveal%20a%20crossover%20phenomenon%3A%20while%20RLVR%20models%0Aoutperform%20the%20base%20model%20at%20small%20k%20values%2C%20the%20base%20model%20usually%20outperforms%0Athem%20when%20sampling%20a%20very%20large%20number%20of%20completions.%20This%20has%20been%0Ainterpreted%20as%20evidence%20that%20base%20models%20have%20a%20larger%20reasoning%20boundary.%20We%0Aargue%20that%20on%20tasks%20with%20discrete%20answer%20spaces%2C%20such%20as%20math%20with%20numeric%0Aoutputs%2C%20Pass%40k%20at%20large%20k%20reflects%20the%20increasingly%20higher%20chance%20of%20success%0Ain%20the%20limit%20of%20the%20number%20of%20trials%20rather%20than%20genuine%20reasoning%2C%20and%20can%0Atherefore%20be%20misleading.%20We%20propose%20Cover%40tau%2C%20which%20measures%20the%20fraction%20of%0Aproblems%20that%20a%20model%20can%20solve%20for%20which%20at%20least%20a%20tau%20proportion%20of%0Acompletions%20are%20correct.%20Unlike%20Pass%40k%2C%20Cover%40tau%20captures%20reasoning%20under%20an%0Aexplicit%20reliability%20threshold%3A%20models%20that%20rely%20on%20random%20guessing%20degrade%0Arapidly%20as%20tau%20increases.%20We%20evaluate%20several%20RLVR%20models%20using%20Cover%40tau-based%0Ametrics%20and%20illustrate%20how%20the%20relative%20rankings%20of%20popular%20algorithms%20change%0Acompared%20to%20Pass%401%2C%20offering%20a%20different%20perspective%20on%20reasoning%20boundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Pass%2540k%253A%2520Breadth-Depth%2520Metrics%2520for%2520Reasoning%2520Boundaries%26entry.906535625%3DMarius%2520Dragoi%2520and%2520Ioana%2520Pintilie%2520and%2520Florin%2520Gogianu%2520and%2520Florin%2520Brad%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%250Apowerful%2520paradigm%2520to%2520improve%2520Large%2520Language%2520Models%2520on%2520reasoning%2520tasks%2520such%2520as%250Acoding%252C%2520math%2520or%2520logic.%2520To%2520assess%2520the%2520reasoning%2520boundary%2520%2528the%2520fraction%2520of%250Aproblems%2520a%2520model%2520can%2520solve%2529%2520researchers%2520often%2520report%2520Pass%2540k%2520at%2520large%2520sampling%250Abudgets.%2520Recent%2520results%2520reveal%2520a%2520crossover%2520phenomenon%253A%2520while%2520RLVR%2520models%250Aoutperform%2520the%2520base%2520model%2520at%2520small%2520k%2520values%252C%2520the%2520base%2520model%2520usually%2520outperforms%250Athem%2520when%2520sampling%2520a%2520very%2520large%2520number%2520of%2520completions.%2520This%2520has%2520been%250Ainterpreted%2520as%2520evidence%2520that%2520base%2520models%2520have%2520a%2520larger%2520reasoning%2520boundary.%2520We%250Aargue%2520that%2520on%2520tasks%2520with%2520discrete%2520answer%2520spaces%252C%2520such%2520as%2520math%2520with%2520numeric%250Aoutputs%252C%2520Pass%2540k%2520at%2520large%2520k%2520reflects%2520the%2520increasingly%2520higher%2520chance%2520of%2520success%250Ain%2520the%2520limit%2520of%2520the%2520number%2520of%2520trials%2520rather%2520than%2520genuine%2520reasoning%252C%2520and%2520can%250Atherefore%2520be%2520misleading.%2520We%2520propose%2520Cover%2540tau%252C%2520which%2520measures%2520the%2520fraction%2520of%250Aproblems%2520that%2520a%2520model%2520can%2520solve%2520for%2520which%2520at%2520least%2520a%2520tau%2520proportion%2520of%250Acompletions%2520are%2520correct.%2520Unlike%2520Pass%2540k%252C%2520Cover%2540tau%2520captures%2520reasoning%2520under%2520an%250Aexplicit%2520reliability%2520threshold%253A%2520models%2520that%2520rely%2520on%2520random%2520guessing%2520degrade%250Arapidly%2520as%2520tau%2520increases.%2520We%2520evaluate%2520several%2520RLVR%2520models%2520using%2520Cover%2540tau-based%250Ametrics%2520and%2520illustrate%2520how%2520the%2520relative%2520rankings%2520of%2520popular%2520algorithms%2520change%250Acompared%2520to%2520Pass%25401%252C%2520offering%2520a%2520different%2520perspective%2520on%2520reasoning%2520boundaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Pass%40k%3A%20Breadth-Depth%20Metrics%20for%20Reasoning%20Boundaries&entry.906535625=Marius%20Dragoi%20and%20Ioana%20Pintilie%20and%20Florin%20Gogianu%20and%20Florin%20Brad&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20to%20improve%20Large%20Language%20Models%20on%20reasoning%20tasks%20such%20as%0Acoding%2C%20math%20or%20logic.%20To%20assess%20the%20reasoning%20boundary%20%28the%20fraction%20of%0Aproblems%20a%20model%20can%20solve%29%20researchers%20often%20report%20Pass%40k%20at%20large%20sampling%0Abudgets.%20Recent%20results%20reveal%20a%20crossover%20phenomenon%3A%20while%20RLVR%20models%0Aoutperform%20the%20base%20model%20at%20small%20k%20values%2C%20the%20base%20model%20usually%20outperforms%0Athem%20when%20sampling%20a%20very%20large%20number%20of%20completions.%20This%20has%20been%0Ainterpreted%20as%20evidence%20that%20base%20models%20have%20a%20larger%20reasoning%20boundary.%20We%0Aargue%20that%20on%20tasks%20with%20discrete%20answer%20spaces%2C%20such%20as%20math%20with%20numeric%0Aoutputs%2C%20Pass%40k%20at%20large%20k%20reflects%20the%20increasingly%20higher%20chance%20of%20success%0Ain%20the%20limit%20of%20the%20number%20of%20trials%20rather%20than%20genuine%20reasoning%2C%20and%20can%0Atherefore%20be%20misleading.%20We%20propose%20Cover%40tau%2C%20which%20measures%20the%20fraction%20of%0Aproblems%20that%20a%20model%20can%20solve%20for%20which%20at%20least%20a%20tau%20proportion%20of%0Acompletions%20are%20correct.%20Unlike%20Pass%40k%2C%20Cover%40tau%20captures%20reasoning%20under%20an%0Aexplicit%20reliability%20threshold%3A%20models%20that%20rely%20on%20random%20guessing%20degrade%0Arapidly%20as%20tau%20increases.%20We%20evaluate%20several%20RLVR%20models%20using%20Cover%40tau-based%0Ametrics%20and%20illustrate%20how%20the%20relative%20rankings%20of%20popular%20algorithms%20change%0Acompared%20to%20Pass%401%2C%20offering%20a%20different%20perspective%20on%20reasoning%20boundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08325v2&entry.124074799=Read"},
{"title": "FedMeld: A Model-dispersal Federated Learning Framework for Space-ground\n  Integrated Networks", "author": "Qian Chen and Xianhao Chen and Kaibin Huang", "abstract": "  To bridge the digital divide, the space-ground integrated networks (SGINs),\nwhich will be a key component of the six-generation (6G) mobile networks, are\nexpected to deliver artificial intelligence (AI) services to every corner of\nthe world. One mission of SGINs is to support federated learning (FL) at a\nglobal scale. However, existing space-ground integrated FL frameworks involve\nground stations or costly inter-satellite links, entailing excessive training\nlatency and communication costs. To overcome these limitations, we propose an\ninfrastructure-free federated learning framework based on a model dispersal\n(FedMeld) strategy, which exploits periodic movement patterns and\nstore-carry-forward capabilities of satellites to enable parameter mixing\nacross large-scale geographical regions. We theoretically show that FedMeld\nleads to global model convergence and quantify the effects of round interval\nand mixing ratio between adjacent areas on its learning performance. Based on\nthe theoretical results, we formulate a joint optimization problem to design\nthe staleness control and mixing ratio (SC-MR) for minimizing the training\nloss. By decomposing the problem into sequential SC and MR subproblems without\ncompromising the optimality, we derive the round interval solution in a closed\nform and the mixing ratio in a semi-closed form to achieve the optimal\nlatency-accuracy tradeoff. Experiments using various datasets demonstrate that\nFedMeld achieves superior model accuracy while significantly reducing\ncommunication costs as compared with traditional FL schemes for SGINs.\n", "link": "http://arxiv.org/abs/2412.17231v2", "date": "2025-10-21", "relevancy": 1.9968, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5144}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.497}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMeld%3A%20A%20Model-dispersal%20Federated%20Learning%20Framework%20for%20Space-ground%0A%20%20Integrated%20Networks&body=Title%3A%20FedMeld%3A%20A%20Model-dispersal%20Federated%20Learning%20Framework%20for%20Space-ground%0A%20%20Integrated%20Networks%0AAuthor%3A%20Qian%20Chen%20and%20Xianhao%20Chen%20and%20Kaibin%20Huang%0AAbstract%3A%20%20%20To%20bridge%20the%20digital%20divide%2C%20the%20space-ground%20integrated%20networks%20%28SGINs%29%2C%0Awhich%20will%20be%20a%20key%20component%20of%20the%20six-generation%20%286G%29%20mobile%20networks%2C%20are%0Aexpected%20to%20deliver%20artificial%20intelligence%20%28AI%29%20services%20to%20every%20corner%20of%0Athe%20world.%20One%20mission%20of%20SGINs%20is%20to%20support%20federated%20learning%20%28FL%29%20at%20a%0Aglobal%20scale.%20However%2C%20existing%20space-ground%20integrated%20FL%20frameworks%20involve%0Aground%20stations%20or%20costly%20inter-satellite%20links%2C%20entailing%20excessive%20training%0Alatency%20and%20communication%20costs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%0Ainfrastructure-free%20federated%20learning%20framework%20based%20on%20a%20model%20dispersal%0A%28FedMeld%29%20strategy%2C%20which%20exploits%20periodic%20movement%20patterns%20and%0Astore-carry-forward%20capabilities%20of%20satellites%20to%20enable%20parameter%20mixing%0Aacross%20large-scale%20geographical%20regions.%20We%20theoretically%20show%20that%20FedMeld%0Aleads%20to%20global%20model%20convergence%20and%20quantify%20the%20effects%20of%20round%20interval%0Aand%20mixing%20ratio%20between%20adjacent%20areas%20on%20its%20learning%20performance.%20Based%20on%0Athe%20theoretical%20results%2C%20we%20formulate%20a%20joint%20optimization%20problem%20to%20design%0Athe%20staleness%20control%20and%20mixing%20ratio%20%28SC-MR%29%20for%20minimizing%20the%20training%0Aloss.%20By%20decomposing%20the%20problem%20into%20sequential%20SC%20and%20MR%20subproblems%20without%0Acompromising%20the%20optimality%2C%20we%20derive%20the%20round%20interval%20solution%20in%20a%20closed%0Aform%20and%20the%20mixing%20ratio%20in%20a%20semi-closed%20form%20to%20achieve%20the%20optimal%0Alatency-accuracy%20tradeoff.%20Experiments%20using%20various%20datasets%20demonstrate%20that%0AFedMeld%20achieves%20superior%20model%20accuracy%20while%20significantly%20reducing%0Acommunication%20costs%20as%20compared%20with%20traditional%20FL%20schemes%20for%20SGINs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMeld%253A%2520A%2520Model-dispersal%2520Federated%2520Learning%2520Framework%2520for%2520Space-ground%250A%2520%2520Integrated%2520Networks%26entry.906535625%3DQian%2520Chen%2520and%2520Xianhao%2520Chen%2520and%2520Kaibin%2520Huang%26entry.1292438233%3D%2520%2520To%2520bridge%2520the%2520digital%2520divide%252C%2520the%2520space-ground%2520integrated%2520networks%2520%2528SGINs%2529%252C%250Awhich%2520will%2520be%2520a%2520key%2520component%2520of%2520the%2520six-generation%2520%25286G%2529%2520mobile%2520networks%252C%2520are%250Aexpected%2520to%2520deliver%2520artificial%2520intelligence%2520%2528AI%2529%2520services%2520to%2520every%2520corner%2520of%250Athe%2520world.%2520One%2520mission%2520of%2520SGINs%2520is%2520to%2520support%2520federated%2520learning%2520%2528FL%2529%2520at%2520a%250Aglobal%2520scale.%2520However%252C%2520existing%2520space-ground%2520integrated%2520FL%2520frameworks%2520involve%250Aground%2520stations%2520or%2520costly%2520inter-satellite%2520links%252C%2520entailing%2520excessive%2520training%250Alatency%2520and%2520communication%2520costs.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520an%250Ainfrastructure-free%2520federated%2520learning%2520framework%2520based%2520on%2520a%2520model%2520dispersal%250A%2528FedMeld%2529%2520strategy%252C%2520which%2520exploits%2520periodic%2520movement%2520patterns%2520and%250Astore-carry-forward%2520capabilities%2520of%2520satellites%2520to%2520enable%2520parameter%2520mixing%250Aacross%2520large-scale%2520geographical%2520regions.%2520We%2520theoretically%2520show%2520that%2520FedMeld%250Aleads%2520to%2520global%2520model%2520convergence%2520and%2520quantify%2520the%2520effects%2520of%2520round%2520interval%250Aand%2520mixing%2520ratio%2520between%2520adjacent%2520areas%2520on%2520its%2520learning%2520performance.%2520Based%2520on%250Athe%2520theoretical%2520results%252C%2520we%2520formulate%2520a%2520joint%2520optimization%2520problem%2520to%2520design%250Athe%2520staleness%2520control%2520and%2520mixing%2520ratio%2520%2528SC-MR%2529%2520for%2520minimizing%2520the%2520training%250Aloss.%2520By%2520decomposing%2520the%2520problem%2520into%2520sequential%2520SC%2520and%2520MR%2520subproblems%2520without%250Acompromising%2520the%2520optimality%252C%2520we%2520derive%2520the%2520round%2520interval%2520solution%2520in%2520a%2520closed%250Aform%2520and%2520the%2520mixing%2520ratio%2520in%2520a%2520semi-closed%2520form%2520to%2520achieve%2520the%2520optimal%250Alatency-accuracy%2520tradeoff.%2520Experiments%2520using%2520various%2520datasets%2520demonstrate%2520that%250AFedMeld%2520achieves%2520superior%2520model%2520accuracy%2520while%2520significantly%2520reducing%250Acommunication%2520costs%2520as%2520compared%2520with%2520traditional%2520FL%2520schemes%2520for%2520SGINs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMeld%3A%20A%20Model-dispersal%20Federated%20Learning%20Framework%20for%20Space-ground%0A%20%20Integrated%20Networks&entry.906535625=Qian%20Chen%20and%20Xianhao%20Chen%20and%20Kaibin%20Huang&entry.1292438233=%20%20To%20bridge%20the%20digital%20divide%2C%20the%20space-ground%20integrated%20networks%20%28SGINs%29%2C%0Awhich%20will%20be%20a%20key%20component%20of%20the%20six-generation%20%286G%29%20mobile%20networks%2C%20are%0Aexpected%20to%20deliver%20artificial%20intelligence%20%28AI%29%20services%20to%20every%20corner%20of%0Athe%20world.%20One%20mission%20of%20SGINs%20is%20to%20support%20federated%20learning%20%28FL%29%20at%20a%0Aglobal%20scale.%20However%2C%20existing%20space-ground%20integrated%20FL%20frameworks%20involve%0Aground%20stations%20or%20costly%20inter-satellite%20links%2C%20entailing%20excessive%20training%0Alatency%20and%20communication%20costs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%0Ainfrastructure-free%20federated%20learning%20framework%20based%20on%20a%20model%20dispersal%0A%28FedMeld%29%20strategy%2C%20which%20exploits%20periodic%20movement%20patterns%20and%0Astore-carry-forward%20capabilities%20of%20satellites%20to%20enable%20parameter%20mixing%0Aacross%20large-scale%20geographical%20regions.%20We%20theoretically%20show%20that%20FedMeld%0Aleads%20to%20global%20model%20convergence%20and%20quantify%20the%20effects%20of%20round%20interval%0Aand%20mixing%20ratio%20between%20adjacent%20areas%20on%20its%20learning%20performance.%20Based%20on%0Athe%20theoretical%20results%2C%20we%20formulate%20a%20joint%20optimization%20problem%20to%20design%0Athe%20staleness%20control%20and%20mixing%20ratio%20%28SC-MR%29%20for%20minimizing%20the%20training%0Aloss.%20By%20decomposing%20the%20problem%20into%20sequential%20SC%20and%20MR%20subproblems%20without%0Acompromising%20the%20optimality%2C%20we%20derive%20the%20round%20interval%20solution%20in%20a%20closed%0Aform%20and%20the%20mixing%20ratio%20in%20a%20semi-closed%20form%20to%20achieve%20the%20optimal%0Alatency-accuracy%20tradeoff.%20Experiments%20using%20various%20datasets%20demonstrate%20that%0AFedMeld%20achieves%20superior%20model%20accuracy%20while%20significantly%20reducing%0Acommunication%20costs%20as%20compared%20with%20traditional%20FL%20schemes%20for%20SGINs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17231v2&entry.124074799=Read"},
{"title": "A Survey of Process Reward Models: From Outcome Signals to Process\n  Supervisions for Large Language Models", "author": "Congming Zheng and Jiachen Zhu and Zhuoying Ou and Yuxiang Chen and Kangning Zhang and Rong Shan and Zeyu Zheng and Mengyue Yang and Jianghao Lin and Yong Yu and Weinan Zhang", "abstract": "  Although Large Language Models (LLMs) exhibit advanced reasoning ability,\nconventional alignment remains largely dominated by outcome reward models\n(ORMs) that judge only final answers. Process Reward Models(PRMs) address this\ngap by evaluating and guiding reasoning at the step or trajectory level. This\nsurvey provides a systematic overview of PRMs through the full loop: how to\ngenerate process data, build PRMs, and use PRMs for test-time scaling and\nreinforcement learning. We summarize applications across math, code, text,\nmultimodal reasoning, robotics, and agents, and review emerging benchmarks. Our\ngoal is to clarify design spaces, reveal open challenges, and guide future\nresearch toward fine-grained, robust reasoning alignment.\n", "link": "http://arxiv.org/abs/2510.08049v2", "date": "2025-10-21", "relevancy": 1.9961, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Process%20Reward%20Models%3A%20From%20Outcome%20Signals%20to%20Process%0A%20%20Supervisions%20for%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20of%20Process%20Reward%20Models%3A%20From%20Outcome%20Signals%20to%20Process%0A%20%20Supervisions%20for%20Large%20Language%20Models%0AAuthor%3A%20Congming%20Zheng%20and%20Jiachen%20Zhu%20and%20Zhuoying%20Ou%20and%20Yuxiang%20Chen%20and%20Kangning%20Zhang%20and%20Rong%20Shan%20and%20Zeyu%20Zheng%20and%20Mengyue%20Yang%20and%20Jianghao%20Lin%20and%20Yong%20Yu%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20Although%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20advanced%20reasoning%20ability%2C%0Aconventional%20alignment%20remains%20largely%20dominated%20by%20outcome%20reward%20models%0A%28ORMs%29%20that%20judge%20only%20final%20answers.%20Process%20Reward%20Models%28PRMs%29%20address%20this%0Agap%20by%20evaluating%20and%20guiding%20reasoning%20at%20the%20step%20or%20trajectory%20level.%20This%0Asurvey%20provides%20a%20systematic%20overview%20of%20PRMs%20through%20the%20full%20loop%3A%20how%20to%0Agenerate%20process%20data%2C%20build%20PRMs%2C%20and%20use%20PRMs%20for%20test-time%20scaling%20and%0Areinforcement%20learning.%20We%20summarize%20applications%20across%20math%2C%20code%2C%20text%2C%0Amultimodal%20reasoning%2C%20robotics%2C%20and%20agents%2C%20and%20review%20emerging%20benchmarks.%20Our%0Agoal%20is%20to%20clarify%20design%20spaces%2C%20reveal%20open%20challenges%2C%20and%20guide%20future%0Aresearch%20toward%20fine-grained%2C%20robust%20reasoning%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Process%2520Reward%2520Models%253A%2520From%2520Outcome%2520Signals%2520to%2520Process%250A%2520%2520Supervisions%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DCongming%2520Zheng%2520and%2520Jiachen%2520Zhu%2520and%2520Zhuoying%2520Ou%2520and%2520Yuxiang%2520Chen%2520and%2520Kangning%2520Zhang%2520and%2520Rong%2520Shan%2520and%2520Zeyu%2520Zheng%2520and%2520Mengyue%2520Yang%2520and%2520Jianghao%2520Lin%2520and%2520Yong%2520Yu%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520Although%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520advanced%2520reasoning%2520ability%252C%250Aconventional%2520alignment%2520remains%2520largely%2520dominated%2520by%2520outcome%2520reward%2520models%250A%2528ORMs%2529%2520that%2520judge%2520only%2520final%2520answers.%2520Process%2520Reward%2520Models%2528PRMs%2529%2520address%2520this%250Agap%2520by%2520evaluating%2520and%2520guiding%2520reasoning%2520at%2520the%2520step%2520or%2520trajectory%2520level.%2520This%250Asurvey%2520provides%2520a%2520systematic%2520overview%2520of%2520PRMs%2520through%2520the%2520full%2520loop%253A%2520how%2520to%250Agenerate%2520process%2520data%252C%2520build%2520PRMs%252C%2520and%2520use%2520PRMs%2520for%2520test-time%2520scaling%2520and%250Areinforcement%2520learning.%2520We%2520summarize%2520applications%2520across%2520math%252C%2520code%252C%2520text%252C%250Amultimodal%2520reasoning%252C%2520robotics%252C%2520and%2520agents%252C%2520and%2520review%2520emerging%2520benchmarks.%2520Our%250Agoal%2520is%2520to%2520clarify%2520design%2520spaces%252C%2520reveal%2520open%2520challenges%252C%2520and%2520guide%2520future%250Aresearch%2520toward%2520fine-grained%252C%2520robust%2520reasoning%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Process%20Reward%20Models%3A%20From%20Outcome%20Signals%20to%20Process%0A%20%20Supervisions%20for%20Large%20Language%20Models&entry.906535625=Congming%20Zheng%20and%20Jiachen%20Zhu%20and%20Zhuoying%20Ou%20and%20Yuxiang%20Chen%20and%20Kangning%20Zhang%20and%20Rong%20Shan%20and%20Zeyu%20Zheng%20and%20Mengyue%20Yang%20and%20Jianghao%20Lin%20and%20Yong%20Yu%20and%20Weinan%20Zhang&entry.1292438233=%20%20Although%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20advanced%20reasoning%20ability%2C%0Aconventional%20alignment%20remains%20largely%20dominated%20by%20outcome%20reward%20models%0A%28ORMs%29%20that%20judge%20only%20final%20answers.%20Process%20Reward%20Models%28PRMs%29%20address%20this%0Agap%20by%20evaluating%20and%20guiding%20reasoning%20at%20the%20step%20or%20trajectory%20level.%20This%0Asurvey%20provides%20a%20systematic%20overview%20of%20PRMs%20through%20the%20full%20loop%3A%20how%20to%0Agenerate%20process%20data%2C%20build%20PRMs%2C%20and%20use%20PRMs%20for%20test-time%20scaling%20and%0Areinforcement%20learning.%20We%20summarize%20applications%20across%20math%2C%20code%2C%20text%2C%0Amultimodal%20reasoning%2C%20robotics%2C%20and%20agents%2C%20and%20review%20emerging%20benchmarks.%20Our%0Agoal%20is%20to%20clarify%20design%20spaces%2C%20reveal%20open%20challenges%2C%20and%20guide%20future%0Aresearch%20toward%20fine-grained%2C%20robust%20reasoning%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08049v2&entry.124074799=Read"},
{"title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based\n  Structures", "author": "Mihir Gupte and Paolo Giusto and Ramesh S", "abstract": "  Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.\n", "link": "http://arxiv.org/abs/2510.10806v2", "date": "2025-10-21", "relevancy": 1.9849, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4974}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Implicit%20Knowledge%20Enough%20for%20LLMs%3F%20A%20RAG%20Approach%20for%20Tree-based%0A%20%20Structures&body=Title%3A%20Is%20Implicit%20Knowledge%20Enough%20for%20LLMs%3F%20A%20RAG%20Approach%20for%20Tree-based%0A%20%20Structures%0AAuthor%3A%20Mihir%20Gupte%20and%20Paolo%20Giusto%20and%20Ramesh%20S%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20adept%20at%20generating%20responses%20based%20on%0Ainformation%20within%20their%20context.%20While%20this%20ability%20is%20useful%20for%20interacting%0Awith%20structured%20data%20like%20code%20files%2C%20another%20popular%20method%2C%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20retrieves%20relevant%20documents%20to%20augment%0Athe%20model%27s%20in-context%20learning.%20However%2C%20it%20is%20not%20well-explored%20how%20to%20best%0Arepresent%20this%20retrieved%20knowledge%20for%20generating%20responses%20on%20structured%20data%2C%0Aparticularly%20hierarchical%20structures%20like%20trees.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20bottom-up%20method%20to%20linearize%20knowledge%20from%20tree-like%20structures%20%28like%20a%0AGitHub%20repository%29%20by%20generating%20implicit%2C%20aggregated%20summaries%20at%20each%0Ahierarchical%20level.%20This%20approach%20enables%20the%20knowledge%20to%20be%20stored%20in%20a%0Aknowledge%20base%20and%20used%20directly%20with%20RAG.%20We%20then%20compare%20our%20method%20to%20using%0ARAG%20on%20raw%2C%20unstructured%20code%2C%20evaluating%20the%20accuracy%20and%20quality%20of%20the%0Agenerated%20responses.%20Our%20results%20show%20that%20while%20response%20quality%20is%20comparable%0Aacross%20both%20methods%2C%20our%20approach%20generates%20over%2068%25%20fewer%20documents%20in%20the%0Aretriever%2C%20a%20significant%20gain%20in%20efficiency.%20This%20finding%20suggests%20that%0Aleveraging%20implicit%2C%20linearized%20knowledge%20may%20be%20a%20highly%20effective%20and%0Ascalable%20strategy%20for%20handling%20complex%2C%20hierarchical%20data%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.10806v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Implicit%2520Knowledge%2520Enough%2520for%2520LLMs%253F%2520A%2520RAG%2520Approach%2520for%2520Tree-based%250A%2520%2520Structures%26entry.906535625%3DMihir%2520Gupte%2520and%2520Paolo%2520Giusto%2520and%2520Ramesh%2520S%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520adept%2520at%2520generating%2520responses%2520based%2520on%250Ainformation%2520within%2520their%2520context.%2520While%2520this%2520ability%2520is%2520useful%2520for%2520interacting%250Awith%2520structured%2520data%2520like%2520code%2520files%252C%2520another%2520popular%2520method%252C%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%252C%2520retrieves%2520relevant%2520documents%2520to%2520augment%250Athe%2520model%2527s%2520in-context%2520learning.%2520However%252C%2520it%2520is%2520not%2520well-explored%2520how%2520to%2520best%250Arepresent%2520this%2520retrieved%2520knowledge%2520for%2520generating%2520responses%2520on%2520structured%2520data%252C%250Aparticularly%2520hierarchical%2520structures%2520like%2520trees.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520bottom-up%2520method%2520to%2520linearize%2520knowledge%2520from%2520tree-like%2520structures%2520%2528like%2520a%250AGitHub%2520repository%2529%2520by%2520generating%2520implicit%252C%2520aggregated%2520summaries%2520at%2520each%250Ahierarchical%2520level.%2520This%2520approach%2520enables%2520the%2520knowledge%2520to%2520be%2520stored%2520in%2520a%250Aknowledge%2520base%2520and%2520used%2520directly%2520with%2520RAG.%2520We%2520then%2520compare%2520our%2520method%2520to%2520using%250ARAG%2520on%2520raw%252C%2520unstructured%2520code%252C%2520evaluating%2520the%2520accuracy%2520and%2520quality%2520of%2520the%250Agenerated%2520responses.%2520Our%2520results%2520show%2520that%2520while%2520response%2520quality%2520is%2520comparable%250Aacross%2520both%2520methods%252C%2520our%2520approach%2520generates%2520over%252068%2525%2520fewer%2520documents%2520in%2520the%250Aretriever%252C%2520a%2520significant%2520gain%2520in%2520efficiency.%2520This%2520finding%2520suggests%2520that%250Aleveraging%2520implicit%252C%2520linearized%2520knowledge%2520may%2520be%2520a%2520highly%2520effective%2520and%250Ascalable%2520strategy%2520for%2520handling%2520complex%252C%2520hierarchical%2520data%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10806v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Implicit%20Knowledge%20Enough%20for%20LLMs%3F%20A%20RAG%20Approach%20for%20Tree-based%0A%20%20Structures&entry.906535625=Mihir%20Gupte%20and%20Paolo%20Giusto%20and%20Ramesh%20S&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20adept%20at%20generating%20responses%20based%20on%0Ainformation%20within%20their%20context.%20While%20this%20ability%20is%20useful%20for%20interacting%0Awith%20structured%20data%20like%20code%20files%2C%20another%20popular%20method%2C%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20retrieves%20relevant%20documents%20to%20augment%0Athe%20model%27s%20in-context%20learning.%20However%2C%20it%20is%20not%20well-explored%20how%20to%20best%0Arepresent%20this%20retrieved%20knowledge%20for%20generating%20responses%20on%20structured%20data%2C%0Aparticularly%20hierarchical%20structures%20like%20trees.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20bottom-up%20method%20to%20linearize%20knowledge%20from%20tree-like%20structures%20%28like%20a%0AGitHub%20repository%29%20by%20generating%20implicit%2C%20aggregated%20summaries%20at%20each%0Ahierarchical%20level.%20This%20approach%20enables%20the%20knowledge%20to%20be%20stored%20in%20a%0Aknowledge%20base%20and%20used%20directly%20with%20RAG.%20We%20then%20compare%20our%20method%20to%20using%0ARAG%20on%20raw%2C%20unstructured%20code%2C%20evaluating%20the%20accuracy%20and%20quality%20of%20the%0Agenerated%20responses.%20Our%20results%20show%20that%20while%20response%20quality%20is%20comparable%0Aacross%20both%20methods%2C%20our%20approach%20generates%20over%2068%25%20fewer%20documents%20in%20the%0Aretriever%2C%20a%20significant%20gain%20in%20efficiency.%20This%20finding%20suggests%20that%0Aleveraging%20implicit%2C%20linearized%20knowledge%20may%20be%20a%20highly%20effective%20and%0Ascalable%20strategy%20for%20handling%20complex%2C%20hierarchical%20data%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.10806v2&entry.124074799=Read"},
{"title": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode\n  Accelerator with Table-Lookup Matmul on Edge FPGAs", "author": "Ye Qiao and Zhiheng Chen and Yifan Zhang and Yian Wang and Sitao Huang", "abstract": "  With the emergence of wearable devices and other embedded systems, deploying\nlarge language models (LLMs) on edge platforms has become an urgent need.\nHowever, this is challenging because of their high computational and memory\ndemands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)\ncompress weights to as low as 1.58~bits with minimal accuracy loss, edge\ndeployment is still constrained by limited on-chip resources, power budgets,\nand the often-neglected long latency of the prefill stage. We present\n\\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for\nlow-power edge FPGAs that fully supports both prefill and autoregressive\ndecoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates\nseveral novel techniques, including (1) a table-lookup-based ternary matrix\nmultiplication (TLMM) engine utilizing grouped activations and online\nprecomputation for low resource utilization and high throughput; (2) a\nfine-grained analytic URAM-based weight buffer management scheme for efficient\nloading and compute engine access; (3) a streaming dataflow architecture that\nfuses floating-point element-wise operations with linear computations to hide\nlatency; (4) a reversed-reordered prefill stage attention with fused attention\noperations for high memory efficiency; and (5) a resource-efficient specialized\ndecoding stage attention. Under a 5~W power budget, TeLLMe delivers up to\n25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for\n64--128 token prompts, marking a significant energy-efficiency advancement in\nLLM inference on edge FPGAs.\n", "link": "http://arxiv.org/abs/2510.15926v2", "date": "2025-10-21", "relevancy": 1.98, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5078}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeLLMe%20v2%3A%20An%20Efficient%20End-to-End%20Ternary%20LLM%20Prefill%20and%20Decode%0A%20%20Accelerator%20with%20Table-Lookup%20Matmul%20on%20Edge%20FPGAs&body=Title%3A%20TeLLMe%20v2%3A%20An%20Efficient%20End-to-End%20Ternary%20LLM%20Prefill%20and%20Decode%0A%20%20Accelerator%20with%20Table-Lookup%20Matmul%20on%20Edge%20FPGAs%0AAuthor%3A%20Ye%20Qiao%20and%20Zhiheng%20Chen%20and%20Yifan%20Zhang%20and%20Yian%20Wang%20and%20Sitao%20Huang%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20wearable%20devices%20and%20other%20embedded%20systems%2C%20deploying%0Alarge%20language%20models%20%28LLMs%29%20on%20edge%20platforms%20has%20become%20an%20urgent%20need.%0AHowever%2C%20this%20is%20challenging%20because%20of%20their%20high%20computational%20and%20memory%0Ademands.%20Although%20recent%20low-bit%20quantization%20methods%20%28e.g.%2C%20BitNet%2C%20DeepSeek%29%0Acompress%20weights%20to%20as%20low%20as%201.58~bits%20with%20minimal%20accuracy%20loss%2C%20edge%0Adeployment%20is%20still%20constrained%20by%20limited%20on-chip%20resources%2C%20power%20budgets%2C%0Aand%20the%20often-neglected%20long%20latency%20of%20the%20prefill%20stage.%20We%20present%0A%5Ctextbf%7BTeLLMe%7D%2C%20the%20first%20table-lookup-based%20ternary%20LLM%20accelerator%20for%0Alow-power%20edge%20FPGAs%20that%20fully%20supports%20both%20prefill%20and%20autoregressive%0Adecoding%20using%201.58-bit%20weights%20and%208-bit%20activations.%20TeLLMe%20incorporates%0Aseveral%20novel%20techniques%2C%20including%20%281%29%20a%20table-lookup-based%20ternary%20matrix%0Amultiplication%20%28TLMM%29%20engine%20utilizing%20grouped%20activations%20and%20online%0Aprecomputation%20for%20low%20resource%20utilization%20and%20high%20throughput%3B%20%282%29%20a%0Afine-grained%20analytic%20URAM-based%20weight%20buffer%20management%20scheme%20for%20efficient%0Aloading%20and%20compute%20engine%20access%3B%20%283%29%20a%20streaming%20dataflow%20architecture%20that%0Afuses%20floating-point%20element-wise%20operations%20with%20linear%20computations%20to%20hide%0Alatency%3B%20%284%29%20a%20reversed-reordered%20prefill%20stage%20attention%20with%20fused%20attention%0Aoperations%20for%20high%20memory%20efficiency%3B%20and%20%285%29%20a%20resource-efficient%20specialized%0Adecoding%20stage%20attention.%20Under%20a%205~W%20power%20budget%2C%20TeLLMe%20delivers%20up%20to%0A25~tokens/s%20decoding%20throughput%20and%200.45--0.96~s%20time-to-first-token%20%28TTFT%29%20for%0A64--128%20token%20prompts%2C%20marking%20a%20significant%20energy-efficiency%20advancement%20in%0ALLM%20inference%20on%20edge%20FPGAs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeLLMe%2520v2%253A%2520An%2520Efficient%2520End-to-End%2520Ternary%2520LLM%2520Prefill%2520and%2520Decode%250A%2520%2520Accelerator%2520with%2520Table-Lookup%2520Matmul%2520on%2520Edge%2520FPGAs%26entry.906535625%3DYe%2520Qiao%2520and%2520Zhiheng%2520Chen%2520and%2520Yifan%2520Zhang%2520and%2520Yian%2520Wang%2520and%2520Sitao%2520Huang%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520wearable%2520devices%2520and%2520other%2520embedded%2520systems%252C%2520deploying%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520on%2520edge%2520platforms%2520has%2520become%2520an%2520urgent%2520need.%250AHowever%252C%2520this%2520is%2520challenging%2520because%2520of%2520their%2520high%2520computational%2520and%2520memory%250Ademands.%2520Although%2520recent%2520low-bit%2520quantization%2520methods%2520%2528e.g.%252C%2520BitNet%252C%2520DeepSeek%2529%250Acompress%2520weights%2520to%2520as%2520low%2520as%25201.58~bits%2520with%2520minimal%2520accuracy%2520loss%252C%2520edge%250Adeployment%2520is%2520still%2520constrained%2520by%2520limited%2520on-chip%2520resources%252C%2520power%2520budgets%252C%250Aand%2520the%2520often-neglected%2520long%2520latency%2520of%2520the%2520prefill%2520stage.%2520We%2520present%250A%255Ctextbf%257BTeLLMe%257D%252C%2520the%2520first%2520table-lookup-based%2520ternary%2520LLM%2520accelerator%2520for%250Alow-power%2520edge%2520FPGAs%2520that%2520fully%2520supports%2520both%2520prefill%2520and%2520autoregressive%250Adecoding%2520using%25201.58-bit%2520weights%2520and%25208-bit%2520activations.%2520TeLLMe%2520incorporates%250Aseveral%2520novel%2520techniques%252C%2520including%2520%25281%2529%2520a%2520table-lookup-based%2520ternary%2520matrix%250Amultiplication%2520%2528TLMM%2529%2520engine%2520utilizing%2520grouped%2520activations%2520and%2520online%250Aprecomputation%2520for%2520low%2520resource%2520utilization%2520and%2520high%2520throughput%253B%2520%25282%2529%2520a%250Afine-grained%2520analytic%2520URAM-based%2520weight%2520buffer%2520management%2520scheme%2520for%2520efficient%250Aloading%2520and%2520compute%2520engine%2520access%253B%2520%25283%2529%2520a%2520streaming%2520dataflow%2520architecture%2520that%250Afuses%2520floating-point%2520element-wise%2520operations%2520with%2520linear%2520computations%2520to%2520hide%250Alatency%253B%2520%25284%2529%2520a%2520reversed-reordered%2520prefill%2520stage%2520attention%2520with%2520fused%2520attention%250Aoperations%2520for%2520high%2520memory%2520efficiency%253B%2520and%2520%25285%2529%2520a%2520resource-efficient%2520specialized%250Adecoding%2520stage%2520attention.%2520Under%2520a%25205~W%2520power%2520budget%252C%2520TeLLMe%2520delivers%2520up%2520to%250A25~tokens/s%2520decoding%2520throughput%2520and%25200.45--0.96~s%2520time-to-first-token%2520%2528TTFT%2529%2520for%250A64--128%2520token%2520prompts%252C%2520marking%2520a%2520significant%2520energy-efficiency%2520advancement%2520in%250ALLM%2520inference%2520on%2520edge%2520FPGAs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeLLMe%20v2%3A%20An%20Efficient%20End-to-End%20Ternary%20LLM%20Prefill%20and%20Decode%0A%20%20Accelerator%20with%20Table-Lookup%20Matmul%20on%20Edge%20FPGAs&entry.906535625=Ye%20Qiao%20and%20Zhiheng%20Chen%20and%20Yifan%20Zhang%20and%20Yian%20Wang%20and%20Sitao%20Huang&entry.1292438233=%20%20With%20the%20emergence%20of%20wearable%20devices%20and%20other%20embedded%20systems%2C%20deploying%0Alarge%20language%20models%20%28LLMs%29%20on%20edge%20platforms%20has%20become%20an%20urgent%20need.%0AHowever%2C%20this%20is%20challenging%20because%20of%20their%20high%20computational%20and%20memory%0Ademands.%20Although%20recent%20low-bit%20quantization%20methods%20%28e.g.%2C%20BitNet%2C%20DeepSeek%29%0Acompress%20weights%20to%20as%20low%20as%201.58~bits%20with%20minimal%20accuracy%20loss%2C%20edge%0Adeployment%20is%20still%20constrained%20by%20limited%20on-chip%20resources%2C%20power%20budgets%2C%0Aand%20the%20often-neglected%20long%20latency%20of%20the%20prefill%20stage.%20We%20present%0A%5Ctextbf%7BTeLLMe%7D%2C%20the%20first%20table-lookup-based%20ternary%20LLM%20accelerator%20for%0Alow-power%20edge%20FPGAs%20that%20fully%20supports%20both%20prefill%20and%20autoregressive%0Adecoding%20using%201.58-bit%20weights%20and%208-bit%20activations.%20TeLLMe%20incorporates%0Aseveral%20novel%20techniques%2C%20including%20%281%29%20a%20table-lookup-based%20ternary%20matrix%0Amultiplication%20%28TLMM%29%20engine%20utilizing%20grouped%20activations%20and%20online%0Aprecomputation%20for%20low%20resource%20utilization%20and%20high%20throughput%3B%20%282%29%20a%0Afine-grained%20analytic%20URAM-based%20weight%20buffer%20management%20scheme%20for%20efficient%0Aloading%20and%20compute%20engine%20access%3B%20%283%29%20a%20streaming%20dataflow%20architecture%20that%0Afuses%20floating-point%20element-wise%20operations%20with%20linear%20computations%20to%20hide%0Alatency%3B%20%284%29%20a%20reversed-reordered%20prefill%20stage%20attention%20with%20fused%20attention%0Aoperations%20for%20high%20memory%20efficiency%3B%20and%20%285%29%20a%20resource-efficient%20specialized%0Adecoding%20stage%20attention.%20Under%20a%205~W%20power%20budget%2C%20TeLLMe%20delivers%20up%20to%0A25~tokens/s%20decoding%20throughput%20and%200.45--0.96~s%20time-to-first-token%20%28TTFT%29%20for%0A64--128%20token%20prompts%2C%20marking%20a%20significant%20energy-efficiency%20advancement%20in%0ALLM%20inference%20on%20edge%20FPGAs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15926v2&entry.124074799=Read"},
{"title": "A Unified Perspective on Optimization in Machine Learning and\n  Neuroscience: From Gradient Descent to Neural Adaptation", "author": "Jes\u00fas Garc\u00eda Fern\u00e1ndez and Nasir Ahmad and Marcel van Gerven", "abstract": "  Iterative optimization is central to modern artificial intelligence (AI) and\nprovides a crucial framework for understanding adaptive systems. This review\nprovides a unified perspective on this subject, bridging classic theory with\nneural network training and biological learning. Although gradient-based\nmethods, powered by the efficient but biologically implausible backpropagation\n(BP), dominate machine learning, their computational demands can hinder\nscalability in high-dimensional settings. In contrast, derivative-free or\nzeroth-order (ZO) optimization feature computationally lighter approaches that\nrely only on function evaluations and randomness. While generally less sample\nefficient, recent breakthroughs demonstrate that modern ZO methods can\neffectively approximate gradients and achieve performance competitive with BP\nin neural network models. This ZO paradigm is also particularly relevant for\nbiology. Its core principles of random exploration (probing) and\nfeedback-guided adaptation (reinforcing) parallel key mechanisms of biological\nlearning, offering a mathematically principled perspective on how the brain\nlearns. In this review, we begin by categorizing optimization approaches based\non the order of derivative information they utilize, ranging from first-,\nsecond-, and higher-order gradient-based to ZO methods. We then explore how\nthese methods are adapted to the unique challenges of neural network training\nand the resulting learning dynamics. Finally, we build upon these insights to\nview biological learning through an optimization lens, arguing that a ZO\nparadigm leverages the brain's intrinsic noise as a computational resource.\nThis framework not only illuminates our understanding of natural intelligence\nbut also holds vast implications for neuromorphic hardware, helping us design\nfast and energy-efficient AI systems that exploit intrinsic hardware noise.\n", "link": "http://arxiv.org/abs/2510.18812v1", "date": "2025-10-21", "relevancy": 1.9685, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Perspective%20on%20Optimization%20in%20Machine%20Learning%20and%0A%20%20Neuroscience%3A%20From%20Gradient%20Descent%20to%20Neural%20Adaptation&body=Title%3A%20A%20Unified%20Perspective%20on%20Optimization%20in%20Machine%20Learning%20and%0A%20%20Neuroscience%3A%20From%20Gradient%20Descent%20to%20Neural%20Adaptation%0AAuthor%3A%20Jes%C3%BAs%20Garc%C3%ADa%20Fern%C3%A1ndez%20and%20Nasir%20Ahmad%20and%20Marcel%20van%20Gerven%0AAbstract%3A%20%20%20Iterative%20optimization%20is%20central%20to%20modern%20artificial%20intelligence%20%28AI%29%20and%0Aprovides%20a%20crucial%20framework%20for%20understanding%20adaptive%20systems.%20This%20review%0Aprovides%20a%20unified%20perspective%20on%20this%20subject%2C%20bridging%20classic%20theory%20with%0Aneural%20network%20training%20and%20biological%20learning.%20Although%20gradient-based%0Amethods%2C%20powered%20by%20the%20efficient%20but%20biologically%20implausible%20backpropagation%0A%28BP%29%2C%20dominate%20machine%20learning%2C%20their%20computational%20demands%20can%20hinder%0Ascalability%20in%20high-dimensional%20settings.%20In%20contrast%2C%20derivative-free%20or%0Azeroth-order%20%28ZO%29%20optimization%20feature%20computationally%20lighter%20approaches%20that%0Arely%20only%20on%20function%20evaluations%20and%20randomness.%20While%20generally%20less%20sample%0Aefficient%2C%20recent%20breakthroughs%20demonstrate%20that%20modern%20ZO%20methods%20can%0Aeffectively%20approximate%20gradients%20and%20achieve%20performance%20competitive%20with%20BP%0Ain%20neural%20network%20models.%20This%20ZO%20paradigm%20is%20also%20particularly%20relevant%20for%0Abiology.%20Its%20core%20principles%20of%20random%20exploration%20%28probing%29%20and%0Afeedback-guided%20adaptation%20%28reinforcing%29%20parallel%20key%20mechanisms%20of%20biological%0Alearning%2C%20offering%20a%20mathematically%20principled%20perspective%20on%20how%20the%20brain%0Alearns.%20In%20this%20review%2C%20we%20begin%20by%20categorizing%20optimization%20approaches%20based%0Aon%20the%20order%20of%20derivative%20information%20they%20utilize%2C%20ranging%20from%20first-%2C%0Asecond-%2C%20and%20higher-order%20gradient-based%20to%20ZO%20methods.%20We%20then%20explore%20how%0Athese%20methods%20are%20adapted%20to%20the%20unique%20challenges%20of%20neural%20network%20training%0Aand%20the%20resulting%20learning%20dynamics.%20Finally%2C%20we%20build%20upon%20these%20insights%20to%0Aview%20biological%20learning%20through%20an%20optimization%20lens%2C%20arguing%20that%20a%20ZO%0Aparadigm%20leverages%20the%20brain%27s%20intrinsic%20noise%20as%20a%20computational%20resource.%0AThis%20framework%20not%20only%20illuminates%20our%20understanding%20of%20natural%20intelligence%0Abut%20also%20holds%20vast%20implications%20for%20neuromorphic%20hardware%2C%20helping%20us%20design%0Afast%20and%20energy-efficient%20AI%20systems%20that%20exploit%20intrinsic%20hardware%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Perspective%2520on%2520Optimization%2520in%2520Machine%2520Learning%2520and%250A%2520%2520Neuroscience%253A%2520From%2520Gradient%2520Descent%2520to%2520Neural%2520Adaptation%26entry.906535625%3DJes%25C3%25BAs%2520Garc%25C3%25ADa%2520Fern%25C3%25A1ndez%2520and%2520Nasir%2520Ahmad%2520and%2520Marcel%2520van%2520Gerven%26entry.1292438233%3D%2520%2520Iterative%2520optimization%2520is%2520central%2520to%2520modern%2520artificial%2520intelligence%2520%2528AI%2529%2520and%250Aprovides%2520a%2520crucial%2520framework%2520for%2520understanding%2520adaptive%2520systems.%2520This%2520review%250Aprovides%2520a%2520unified%2520perspective%2520on%2520this%2520subject%252C%2520bridging%2520classic%2520theory%2520with%250Aneural%2520network%2520training%2520and%2520biological%2520learning.%2520Although%2520gradient-based%250Amethods%252C%2520powered%2520by%2520the%2520efficient%2520but%2520biologically%2520implausible%2520backpropagation%250A%2528BP%2529%252C%2520dominate%2520machine%2520learning%252C%2520their%2520computational%2520demands%2520can%2520hinder%250Ascalability%2520in%2520high-dimensional%2520settings.%2520In%2520contrast%252C%2520derivative-free%2520or%250Azeroth-order%2520%2528ZO%2529%2520optimization%2520feature%2520computationally%2520lighter%2520approaches%2520that%250Arely%2520only%2520on%2520function%2520evaluations%2520and%2520randomness.%2520While%2520generally%2520less%2520sample%250Aefficient%252C%2520recent%2520breakthroughs%2520demonstrate%2520that%2520modern%2520ZO%2520methods%2520can%250Aeffectively%2520approximate%2520gradients%2520and%2520achieve%2520performance%2520competitive%2520with%2520BP%250Ain%2520neural%2520network%2520models.%2520This%2520ZO%2520paradigm%2520is%2520also%2520particularly%2520relevant%2520for%250Abiology.%2520Its%2520core%2520principles%2520of%2520random%2520exploration%2520%2528probing%2529%2520and%250Afeedback-guided%2520adaptation%2520%2528reinforcing%2529%2520parallel%2520key%2520mechanisms%2520of%2520biological%250Alearning%252C%2520offering%2520a%2520mathematically%2520principled%2520perspective%2520on%2520how%2520the%2520brain%250Alearns.%2520In%2520this%2520review%252C%2520we%2520begin%2520by%2520categorizing%2520optimization%2520approaches%2520based%250Aon%2520the%2520order%2520of%2520derivative%2520information%2520they%2520utilize%252C%2520ranging%2520from%2520first-%252C%250Asecond-%252C%2520and%2520higher-order%2520gradient-based%2520to%2520ZO%2520methods.%2520We%2520then%2520explore%2520how%250Athese%2520methods%2520are%2520adapted%2520to%2520the%2520unique%2520challenges%2520of%2520neural%2520network%2520training%250Aand%2520the%2520resulting%2520learning%2520dynamics.%2520Finally%252C%2520we%2520build%2520upon%2520these%2520insights%2520to%250Aview%2520biological%2520learning%2520through%2520an%2520optimization%2520lens%252C%2520arguing%2520that%2520a%2520ZO%250Aparadigm%2520leverages%2520the%2520brain%2527s%2520intrinsic%2520noise%2520as%2520a%2520computational%2520resource.%250AThis%2520framework%2520not%2520only%2520illuminates%2520our%2520understanding%2520of%2520natural%2520intelligence%250Abut%2520also%2520holds%2520vast%2520implications%2520for%2520neuromorphic%2520hardware%252C%2520helping%2520us%2520design%250Afast%2520and%2520energy-efficient%2520AI%2520systems%2520that%2520exploit%2520intrinsic%2520hardware%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Perspective%20on%20Optimization%20in%20Machine%20Learning%20and%0A%20%20Neuroscience%3A%20From%20Gradient%20Descent%20to%20Neural%20Adaptation&entry.906535625=Jes%C3%BAs%20Garc%C3%ADa%20Fern%C3%A1ndez%20and%20Nasir%20Ahmad%20and%20Marcel%20van%20Gerven&entry.1292438233=%20%20Iterative%20optimization%20is%20central%20to%20modern%20artificial%20intelligence%20%28AI%29%20and%0Aprovides%20a%20crucial%20framework%20for%20understanding%20adaptive%20systems.%20This%20review%0Aprovides%20a%20unified%20perspective%20on%20this%20subject%2C%20bridging%20classic%20theory%20with%0Aneural%20network%20training%20and%20biological%20learning.%20Although%20gradient-based%0Amethods%2C%20powered%20by%20the%20efficient%20but%20biologically%20implausible%20backpropagation%0A%28BP%29%2C%20dominate%20machine%20learning%2C%20their%20computational%20demands%20can%20hinder%0Ascalability%20in%20high-dimensional%20settings.%20In%20contrast%2C%20derivative-free%20or%0Azeroth-order%20%28ZO%29%20optimization%20feature%20computationally%20lighter%20approaches%20that%0Arely%20only%20on%20function%20evaluations%20and%20randomness.%20While%20generally%20less%20sample%0Aefficient%2C%20recent%20breakthroughs%20demonstrate%20that%20modern%20ZO%20methods%20can%0Aeffectively%20approximate%20gradients%20and%20achieve%20performance%20competitive%20with%20BP%0Ain%20neural%20network%20models.%20This%20ZO%20paradigm%20is%20also%20particularly%20relevant%20for%0Abiology.%20Its%20core%20principles%20of%20random%20exploration%20%28probing%29%20and%0Afeedback-guided%20adaptation%20%28reinforcing%29%20parallel%20key%20mechanisms%20of%20biological%0Alearning%2C%20offering%20a%20mathematically%20principled%20perspective%20on%20how%20the%20brain%0Alearns.%20In%20this%20review%2C%20we%20begin%20by%20categorizing%20optimization%20approaches%20based%0Aon%20the%20order%20of%20derivative%20information%20they%20utilize%2C%20ranging%20from%20first-%2C%0Asecond-%2C%20and%20higher-order%20gradient-based%20to%20ZO%20methods.%20We%20then%20explore%20how%0Athese%20methods%20are%20adapted%20to%20the%20unique%20challenges%20of%20neural%20network%20training%0Aand%20the%20resulting%20learning%20dynamics.%20Finally%2C%20we%20build%20upon%20these%20insights%20to%0Aview%20biological%20learning%20through%20an%20optimization%20lens%2C%20arguing%20that%20a%20ZO%0Aparadigm%20leverages%20the%20brain%27s%20intrinsic%20noise%20as%20a%20computational%20resource.%0AThis%20framework%20not%20only%20illuminates%20our%20understanding%20of%20natural%20intelligence%0Abut%20also%20holds%20vast%20implications%20for%20neuromorphic%20hardware%2C%20helping%20us%20design%0Afast%20and%20energy-efficient%20AI%20systems%20that%20exploit%20intrinsic%20hardware%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18812v1&entry.124074799=Read"},
{"title": "Improving the Generation and Evaluation of Synthetic Data for Downstream\n  Medical Causal Inference", "author": "Harry Amad and Zhaozhi Qian and Dennis Frauen and Julianna Piskorz and Stefan Feuerriegel and Mihaela van der Schaar", "abstract": "  Causal inference is essential for developing and evaluating medical\ninterventions, yet real-world medical datasets are often difficult to access\ndue to regulatory barriers. This makes synthetic data a potentially valuable\nasset that enables these medical analyses, along with the development of new\ninference methods themselves. Generative models can produce synthetic data that\nclosely approximate real data distributions, yet existing methods do not\nconsider the unique challenges that downstream causal inference tasks, and\nspecifically those focused on treatments, pose. We establish a set of\ndesiderata that synthetic data containing treatments should satisfy to maximise\ndownstream utility: preservation of (i) the covariate distribution, (ii) the\ntreatment assignment mechanism, and (iii) the outcome generation mechanism.\nBased on these desiderata, we propose a set of evaluation metrics to assess\nsuch synthetic data. Finally, we present STEAM: a novel method for generating\nSynthetic data for Treatment Effect Analysis in Medicine that mimics the\ndata-generating process of data containing treatments and optimises for our\ndesiderata. We empirically demonstrate that STEAM achieves state-of-the-art\nperformance across our metrics as compared to existing generative models,\nparticularly as the complexity of the true data-generating process increases.\n", "link": "http://arxiv.org/abs/2510.18768v1", "date": "2025-10-21", "relevancy": 1.9655, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5214}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4718}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Generation%20and%20Evaluation%20of%20Synthetic%20Data%20for%20Downstream%0A%20%20Medical%20Causal%20Inference&body=Title%3A%20Improving%20the%20Generation%20and%20Evaluation%20of%20Synthetic%20Data%20for%20Downstream%0A%20%20Medical%20Causal%20Inference%0AAuthor%3A%20Harry%20Amad%20and%20Zhaozhi%20Qian%20and%20Dennis%20Frauen%20and%20Julianna%20Piskorz%20and%20Stefan%20Feuerriegel%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20%20%20Causal%20inference%20is%20essential%20for%20developing%20and%20evaluating%20medical%0Ainterventions%2C%20yet%20real-world%20medical%20datasets%20are%20often%20difficult%20to%20access%0Adue%20to%20regulatory%20barriers.%20This%20makes%20synthetic%20data%20a%20potentially%20valuable%0Aasset%20that%20enables%20these%20medical%20analyses%2C%20along%20with%20the%20development%20of%20new%0Ainference%20methods%20themselves.%20Generative%20models%20can%20produce%20synthetic%20data%20that%0Aclosely%20approximate%20real%20data%20distributions%2C%20yet%20existing%20methods%20do%20not%0Aconsider%20the%20unique%20challenges%20that%20downstream%20causal%20inference%20tasks%2C%20and%0Aspecifically%20those%20focused%20on%20treatments%2C%20pose.%20We%20establish%20a%20set%20of%0Adesiderata%20that%20synthetic%20data%20containing%20treatments%20should%20satisfy%20to%20maximise%0Adownstream%20utility%3A%20preservation%20of%20%28i%29%20the%20covariate%20distribution%2C%20%28ii%29%20the%0Atreatment%20assignment%20mechanism%2C%20and%20%28iii%29%20the%20outcome%20generation%20mechanism.%0ABased%20on%20these%20desiderata%2C%20we%20propose%20a%20set%20of%20evaluation%20metrics%20to%20assess%0Asuch%20synthetic%20data.%20Finally%2C%20we%20present%20STEAM%3A%20a%20novel%20method%20for%20generating%0ASynthetic%20data%20for%20Treatment%20Effect%20Analysis%20in%20Medicine%20that%20mimics%20the%0Adata-generating%20process%20of%20data%20containing%20treatments%20and%20optimises%20for%20our%0Adesiderata.%20We%20empirically%20demonstrate%20that%20STEAM%20achieves%20state-of-the-art%0Aperformance%20across%20our%20metrics%20as%20compared%20to%20existing%20generative%20models%2C%0Aparticularly%20as%20the%20complexity%20of%20the%20true%20data-generating%20process%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Generation%2520and%2520Evaluation%2520of%2520Synthetic%2520Data%2520for%2520Downstream%250A%2520%2520Medical%2520Causal%2520Inference%26entry.906535625%3DHarry%2520Amad%2520and%2520Zhaozhi%2520Qian%2520and%2520Dennis%2520Frauen%2520and%2520Julianna%2520Piskorz%2520and%2520Stefan%2520Feuerriegel%2520and%2520Mihaela%2520van%2520der%2520Schaar%26entry.1292438233%3D%2520%2520Causal%2520inference%2520is%2520essential%2520for%2520developing%2520and%2520evaluating%2520medical%250Ainterventions%252C%2520yet%2520real-world%2520medical%2520datasets%2520are%2520often%2520difficult%2520to%2520access%250Adue%2520to%2520regulatory%2520barriers.%2520This%2520makes%2520synthetic%2520data%2520a%2520potentially%2520valuable%250Aasset%2520that%2520enables%2520these%2520medical%2520analyses%252C%2520along%2520with%2520the%2520development%2520of%2520new%250Ainference%2520methods%2520themselves.%2520Generative%2520models%2520can%2520produce%2520synthetic%2520data%2520that%250Aclosely%2520approximate%2520real%2520data%2520distributions%252C%2520yet%2520existing%2520methods%2520do%2520not%250Aconsider%2520the%2520unique%2520challenges%2520that%2520downstream%2520causal%2520inference%2520tasks%252C%2520and%250Aspecifically%2520those%2520focused%2520on%2520treatments%252C%2520pose.%2520We%2520establish%2520a%2520set%2520of%250Adesiderata%2520that%2520synthetic%2520data%2520containing%2520treatments%2520should%2520satisfy%2520to%2520maximise%250Adownstream%2520utility%253A%2520preservation%2520of%2520%2528i%2529%2520the%2520covariate%2520distribution%252C%2520%2528ii%2529%2520the%250Atreatment%2520assignment%2520mechanism%252C%2520and%2520%2528iii%2529%2520the%2520outcome%2520generation%2520mechanism.%250ABased%2520on%2520these%2520desiderata%252C%2520we%2520propose%2520a%2520set%2520of%2520evaluation%2520metrics%2520to%2520assess%250Asuch%2520synthetic%2520data.%2520Finally%252C%2520we%2520present%2520STEAM%253A%2520a%2520novel%2520method%2520for%2520generating%250ASynthetic%2520data%2520for%2520Treatment%2520Effect%2520Analysis%2520in%2520Medicine%2520that%2520mimics%2520the%250Adata-generating%2520process%2520of%2520data%2520containing%2520treatments%2520and%2520optimises%2520for%2520our%250Adesiderata.%2520We%2520empirically%2520demonstrate%2520that%2520STEAM%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520our%2520metrics%2520as%2520compared%2520to%2520existing%2520generative%2520models%252C%250Aparticularly%2520as%2520the%2520complexity%2520of%2520the%2520true%2520data-generating%2520process%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Generation%20and%20Evaluation%20of%20Synthetic%20Data%20for%20Downstream%0A%20%20Medical%20Causal%20Inference&entry.906535625=Harry%20Amad%20and%20Zhaozhi%20Qian%20and%20Dennis%20Frauen%20and%20Julianna%20Piskorz%20and%20Stefan%20Feuerriegel%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=%20%20Causal%20inference%20is%20essential%20for%20developing%20and%20evaluating%20medical%0Ainterventions%2C%20yet%20real-world%20medical%20datasets%20are%20often%20difficult%20to%20access%0Adue%20to%20regulatory%20barriers.%20This%20makes%20synthetic%20data%20a%20potentially%20valuable%0Aasset%20that%20enables%20these%20medical%20analyses%2C%20along%20with%20the%20development%20of%20new%0Ainference%20methods%20themselves.%20Generative%20models%20can%20produce%20synthetic%20data%20that%0Aclosely%20approximate%20real%20data%20distributions%2C%20yet%20existing%20methods%20do%20not%0Aconsider%20the%20unique%20challenges%20that%20downstream%20causal%20inference%20tasks%2C%20and%0Aspecifically%20those%20focused%20on%20treatments%2C%20pose.%20We%20establish%20a%20set%20of%0Adesiderata%20that%20synthetic%20data%20containing%20treatments%20should%20satisfy%20to%20maximise%0Adownstream%20utility%3A%20preservation%20of%20%28i%29%20the%20covariate%20distribution%2C%20%28ii%29%20the%0Atreatment%20assignment%20mechanism%2C%20and%20%28iii%29%20the%20outcome%20generation%20mechanism.%0ABased%20on%20these%20desiderata%2C%20we%20propose%20a%20set%20of%20evaluation%20metrics%20to%20assess%0Asuch%20synthetic%20data.%20Finally%2C%20we%20present%20STEAM%3A%20a%20novel%20method%20for%20generating%0ASynthetic%20data%20for%20Treatment%20Effect%20Analysis%20in%20Medicine%20that%20mimics%20the%0Adata-generating%20process%20of%20data%20containing%20treatments%20and%20optimises%20for%20our%0Adesiderata.%20We%20empirically%20demonstrate%20that%20STEAM%20achieves%20state-of-the-art%0Aperformance%20across%20our%20metrics%20as%20compared%20to%20existing%20generative%20models%2C%0Aparticularly%20as%20the%20complexity%20of%20the%20true%20data-generating%20process%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18768v1&entry.124074799=Read"},
{"title": "Channel-Aware Vector Quantization for Robust Semantic Communication on\n  Discrete Channels", "author": "Zian Meng and Qiang Li and Wenqian Tang and Mingdie Yan and Xiaohu Ge", "abstract": "  Deep learning-based semantic communication has largely relied on analog or\nsemi-digital transmission, which limits compatibility with modern digital\ncommunication infrastructures. Recent studies have employed vector quantization\n(VQ) to enable discrete semantic transmission, yet existing methods neglect\nchannel state information during codebook optimization, leading to suboptimal\nrobustness. To bridge this gap, we propose a channel-aware vector quantization\n(CAVQ) algorithm within a joint source-channel coding (JSCC) framework, termed\nVQJSCC, established on a discrete memoryless channel. In this framework,\nsemantic features are discretized and directly mapped to modulation\nconstellation symbols, while CAVQ integrates channel transition probabilities\ninto the quantization process, aligning easily confused symbols with\nsemantically similar codewords. A multi-codebook alignment mechanism is further\nintroduced to handle mismatches between codebook order and modulation order by\ndecomposing the transmission stream into multiple independently optimized\nsubchannels. Experimental results demonstrate that VQJSCC effectively mitigates\nthe digital cliff effect, achieves superior reconstruction quality across\nvarious modulation schemes, and outperforms state-of-the-art digital semantic\ncommunication baselines in both robustness and efficiency.\n", "link": "http://arxiv.org/abs/2510.18604v1", "date": "2025-10-21", "relevancy": 1.9619, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5011}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4936}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Channel-Aware%20Vector%20Quantization%20for%20Robust%20Semantic%20Communication%20on%0A%20%20Discrete%20Channels&body=Title%3A%20Channel-Aware%20Vector%20Quantization%20for%20Robust%20Semantic%20Communication%20on%0A%20%20Discrete%20Channels%0AAuthor%3A%20Zian%20Meng%20and%20Qiang%20Li%20and%20Wenqian%20Tang%20and%20Mingdie%20Yan%20and%20Xiaohu%20Ge%0AAbstract%3A%20%20%20Deep%20learning-based%20semantic%20communication%20has%20largely%20relied%20on%20analog%20or%0Asemi-digital%20transmission%2C%20which%20limits%20compatibility%20with%20modern%20digital%0Acommunication%20infrastructures.%20Recent%20studies%20have%20employed%20vector%20quantization%0A%28VQ%29%20to%20enable%20discrete%20semantic%20transmission%2C%20yet%20existing%20methods%20neglect%0Achannel%20state%20information%20during%20codebook%20optimization%2C%20leading%20to%20suboptimal%0Arobustness.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20channel-aware%20vector%20quantization%0A%28CAVQ%29%20algorithm%20within%20a%20joint%20source-channel%20coding%20%28JSCC%29%20framework%2C%20termed%0AVQJSCC%2C%20established%20on%20a%20discrete%20memoryless%20channel.%20In%20this%20framework%2C%0Asemantic%20features%20are%20discretized%20and%20directly%20mapped%20to%20modulation%0Aconstellation%20symbols%2C%20while%20CAVQ%20integrates%20channel%20transition%20probabilities%0Ainto%20the%20quantization%20process%2C%20aligning%20easily%20confused%20symbols%20with%0Asemantically%20similar%20codewords.%20A%20multi-codebook%20alignment%20mechanism%20is%20further%0Aintroduced%20to%20handle%20mismatches%20between%20codebook%20order%20and%20modulation%20order%20by%0Adecomposing%20the%20transmission%20stream%20into%20multiple%20independently%20optimized%0Asubchannels.%20Experimental%20results%20demonstrate%20that%20VQJSCC%20effectively%20mitigates%0Athe%20digital%20cliff%20effect%2C%20achieves%20superior%20reconstruction%20quality%20across%0Avarious%20modulation%20schemes%2C%20and%20outperforms%20state-of-the-art%20digital%20semantic%0Acommunication%20baselines%20in%20both%20robustness%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChannel-Aware%2520Vector%2520Quantization%2520for%2520Robust%2520Semantic%2520Communication%2520on%250A%2520%2520Discrete%2520Channels%26entry.906535625%3DZian%2520Meng%2520and%2520Qiang%2520Li%2520and%2520Wenqian%2520Tang%2520and%2520Mingdie%2520Yan%2520and%2520Xiaohu%2520Ge%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520semantic%2520communication%2520has%2520largely%2520relied%2520on%2520analog%2520or%250Asemi-digital%2520transmission%252C%2520which%2520limits%2520compatibility%2520with%2520modern%2520digital%250Acommunication%2520infrastructures.%2520Recent%2520studies%2520have%2520employed%2520vector%2520quantization%250A%2528VQ%2529%2520to%2520enable%2520discrete%2520semantic%2520transmission%252C%2520yet%2520existing%2520methods%2520neglect%250Achannel%2520state%2520information%2520during%2520codebook%2520optimization%252C%2520leading%2520to%2520suboptimal%250Arobustness.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520channel-aware%2520vector%2520quantization%250A%2528CAVQ%2529%2520algorithm%2520within%2520a%2520joint%2520source-channel%2520coding%2520%2528JSCC%2529%2520framework%252C%2520termed%250AVQJSCC%252C%2520established%2520on%2520a%2520discrete%2520memoryless%2520channel.%2520In%2520this%2520framework%252C%250Asemantic%2520features%2520are%2520discretized%2520and%2520directly%2520mapped%2520to%2520modulation%250Aconstellation%2520symbols%252C%2520while%2520CAVQ%2520integrates%2520channel%2520transition%2520probabilities%250Ainto%2520the%2520quantization%2520process%252C%2520aligning%2520easily%2520confused%2520symbols%2520with%250Asemantically%2520similar%2520codewords.%2520A%2520multi-codebook%2520alignment%2520mechanism%2520is%2520further%250Aintroduced%2520to%2520handle%2520mismatches%2520between%2520codebook%2520order%2520and%2520modulation%2520order%2520by%250Adecomposing%2520the%2520transmission%2520stream%2520into%2520multiple%2520independently%2520optimized%250Asubchannels.%2520Experimental%2520results%2520demonstrate%2520that%2520VQJSCC%2520effectively%2520mitigates%250Athe%2520digital%2520cliff%2520effect%252C%2520achieves%2520superior%2520reconstruction%2520quality%2520across%250Avarious%2520modulation%2520schemes%252C%2520and%2520outperforms%2520state-of-the-art%2520digital%2520semantic%250Acommunication%2520baselines%2520in%2520both%2520robustness%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Channel-Aware%20Vector%20Quantization%20for%20Robust%20Semantic%20Communication%20on%0A%20%20Discrete%20Channels&entry.906535625=Zian%20Meng%20and%20Qiang%20Li%20and%20Wenqian%20Tang%20and%20Mingdie%20Yan%20and%20Xiaohu%20Ge&entry.1292438233=%20%20Deep%20learning-based%20semantic%20communication%20has%20largely%20relied%20on%20analog%20or%0Asemi-digital%20transmission%2C%20which%20limits%20compatibility%20with%20modern%20digital%0Acommunication%20infrastructures.%20Recent%20studies%20have%20employed%20vector%20quantization%0A%28VQ%29%20to%20enable%20discrete%20semantic%20transmission%2C%20yet%20existing%20methods%20neglect%0Achannel%20state%20information%20during%20codebook%20optimization%2C%20leading%20to%20suboptimal%0Arobustness.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20channel-aware%20vector%20quantization%0A%28CAVQ%29%20algorithm%20within%20a%20joint%20source-channel%20coding%20%28JSCC%29%20framework%2C%20termed%0AVQJSCC%2C%20established%20on%20a%20discrete%20memoryless%20channel.%20In%20this%20framework%2C%0Asemantic%20features%20are%20discretized%20and%20directly%20mapped%20to%20modulation%0Aconstellation%20symbols%2C%20while%20CAVQ%20integrates%20channel%20transition%20probabilities%0Ainto%20the%20quantization%20process%2C%20aligning%20easily%20confused%20symbols%20with%0Asemantically%20similar%20codewords.%20A%20multi-codebook%20alignment%20mechanism%20is%20further%0Aintroduced%20to%20handle%20mismatches%20between%20codebook%20order%20and%20modulation%20order%20by%0Adecomposing%20the%20transmission%20stream%20into%20multiple%20independently%20optimized%0Asubchannels.%20Experimental%20results%20demonstrate%20that%20VQJSCC%20effectively%20mitigates%0Athe%20digital%20cliff%20effect%2C%20achieves%20superior%20reconstruction%20quality%20across%0Avarious%20modulation%20schemes%2C%20and%20outperforms%20state-of-the-art%20digital%20semantic%0Acommunication%20baselines%20in%20both%20robustness%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18604v1&entry.124074799=Read"},
{"title": "When LRP Diverges from Leave-One-Out in Transformers", "author": "Weiqiu You and Siqi Zeng and Yao-Hung Hubert Tsai and Makoto Yamada and Han Zhao", "abstract": "  Leave-One-Out (LOO) provides an intuitive measure of feature importance but\nis computationally prohibitive. While Layer-Wise Relevance Propagation (LRP)\noffers a potentially efficient alternative, its axiomatic soundness in modern\nTransformers remains largely under-examined. In this work, we first show that\nthe bilinear propagation rules used in recent advances of AttnLRP violate the\nimplementation invariance axiom. We prove this analytically and confirm it\nempirically in linear attention layers. Second, we also revisit CP-LRP as a\ndiagnostic baseline and find that bypassing relevance propagation through the\nsoftmax layer -- backpropagating relevance only through the value matrices --\nsignificantly improves alignment with LOO, particularly in middle-to-late\nTransformer layers. Overall, our results suggest that (i) bilinear\nfactorization sensitivity and (ii) softmax propagation error potentially\njointly undermine LRP's ability to approximate LOO in Transformers.\n", "link": "http://arxiv.org/abs/2510.18810v1", "date": "2025-10-21", "relevancy": 1.8858, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4789}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20LRP%20Diverges%20from%20Leave-One-Out%20in%20Transformers&body=Title%3A%20When%20LRP%20Diverges%20from%20Leave-One-Out%20in%20Transformers%0AAuthor%3A%20Weiqiu%20You%20and%20Siqi%20Zeng%20and%20Yao-Hung%20Hubert%20Tsai%20and%20Makoto%20Yamada%20and%20Han%20Zhao%0AAbstract%3A%20%20%20Leave-One-Out%20%28LOO%29%20provides%20an%20intuitive%20measure%20of%20feature%20importance%20but%0Ais%20computationally%20prohibitive.%20While%20Layer-Wise%20Relevance%20Propagation%20%28LRP%29%0Aoffers%20a%20potentially%20efficient%20alternative%2C%20its%20axiomatic%20soundness%20in%20modern%0ATransformers%20remains%20largely%20under-examined.%20In%20this%20work%2C%20we%20first%20show%20that%0Athe%20bilinear%20propagation%20rules%20used%20in%20recent%20advances%20of%20AttnLRP%20violate%20the%0Aimplementation%20invariance%20axiom.%20We%20prove%20this%20analytically%20and%20confirm%20it%0Aempirically%20in%20linear%20attention%20layers.%20Second%2C%20we%20also%20revisit%20CP-LRP%20as%20a%0Adiagnostic%20baseline%20and%20find%20that%20bypassing%20relevance%20propagation%20through%20the%0Asoftmax%20layer%20--%20backpropagating%20relevance%20only%20through%20the%20value%20matrices%20--%0Asignificantly%20improves%20alignment%20with%20LOO%2C%20particularly%20in%20middle-to-late%0ATransformer%20layers.%20Overall%2C%20our%20results%20suggest%20that%20%28i%29%20bilinear%0Afactorization%20sensitivity%20and%20%28ii%29%20softmax%20propagation%20error%20potentially%0Ajointly%20undermine%20LRP%27s%20ability%20to%20approximate%20LOO%20in%20Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520LRP%2520Diverges%2520from%2520Leave-One-Out%2520in%2520Transformers%26entry.906535625%3DWeiqiu%2520You%2520and%2520Siqi%2520Zeng%2520and%2520Yao-Hung%2520Hubert%2520Tsai%2520and%2520Makoto%2520Yamada%2520and%2520Han%2520Zhao%26entry.1292438233%3D%2520%2520Leave-One-Out%2520%2528LOO%2529%2520provides%2520an%2520intuitive%2520measure%2520of%2520feature%2520importance%2520but%250Ais%2520computationally%2520prohibitive.%2520While%2520Layer-Wise%2520Relevance%2520Propagation%2520%2528LRP%2529%250Aoffers%2520a%2520potentially%2520efficient%2520alternative%252C%2520its%2520axiomatic%2520soundness%2520in%2520modern%250ATransformers%2520remains%2520largely%2520under-examined.%2520In%2520this%2520work%252C%2520we%2520first%2520show%2520that%250Athe%2520bilinear%2520propagation%2520rules%2520used%2520in%2520recent%2520advances%2520of%2520AttnLRP%2520violate%2520the%250Aimplementation%2520invariance%2520axiom.%2520We%2520prove%2520this%2520analytically%2520and%2520confirm%2520it%250Aempirically%2520in%2520linear%2520attention%2520layers.%2520Second%252C%2520we%2520also%2520revisit%2520CP-LRP%2520as%2520a%250Adiagnostic%2520baseline%2520and%2520find%2520that%2520bypassing%2520relevance%2520propagation%2520through%2520the%250Asoftmax%2520layer%2520--%2520backpropagating%2520relevance%2520only%2520through%2520the%2520value%2520matrices%2520--%250Asignificantly%2520improves%2520alignment%2520with%2520LOO%252C%2520particularly%2520in%2520middle-to-late%250ATransformer%2520layers.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520%2528i%2529%2520bilinear%250Afactorization%2520sensitivity%2520and%2520%2528ii%2529%2520softmax%2520propagation%2520error%2520potentially%250Ajointly%2520undermine%2520LRP%2527s%2520ability%2520to%2520approximate%2520LOO%2520in%2520Transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20LRP%20Diverges%20from%20Leave-One-Out%20in%20Transformers&entry.906535625=Weiqiu%20You%20and%20Siqi%20Zeng%20and%20Yao-Hung%20Hubert%20Tsai%20and%20Makoto%20Yamada%20and%20Han%20Zhao&entry.1292438233=%20%20Leave-One-Out%20%28LOO%29%20provides%20an%20intuitive%20measure%20of%20feature%20importance%20but%0Ais%20computationally%20prohibitive.%20While%20Layer-Wise%20Relevance%20Propagation%20%28LRP%29%0Aoffers%20a%20potentially%20efficient%20alternative%2C%20its%20axiomatic%20soundness%20in%20modern%0ATransformers%20remains%20largely%20under-examined.%20In%20this%20work%2C%20we%20first%20show%20that%0Athe%20bilinear%20propagation%20rules%20used%20in%20recent%20advances%20of%20AttnLRP%20violate%20the%0Aimplementation%20invariance%20axiom.%20We%20prove%20this%20analytically%20and%20confirm%20it%0Aempirically%20in%20linear%20attention%20layers.%20Second%2C%20we%20also%20revisit%20CP-LRP%20as%20a%0Adiagnostic%20baseline%20and%20find%20that%20bypassing%20relevance%20propagation%20through%20the%0Asoftmax%20layer%20--%20backpropagating%20relevance%20only%20through%20the%20value%20matrices%20--%0Asignificantly%20improves%20alignment%20with%20LOO%2C%20particularly%20in%20middle-to-late%0ATransformer%20layers.%20Overall%2C%20our%20results%20suggest%20that%20%28i%29%20bilinear%0Afactorization%20sensitivity%20and%20%28ii%29%20softmax%20propagation%20error%20potentially%0Ajointly%20undermine%20LRP%27s%20ability%20to%20approximate%20LOO%20in%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18810v1&entry.124074799=Read"},
{"title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and\n  Segmentation", "author": "Patterson Hsieh and Jerry Yeh and Mao-Chi He and Wen-Han Hsieh and Elvis Hsieh", "abstract": "  Climate change is intensifying the occurrence of harmful algal bloom (HAB),\nparticularly cyanobacteria, which threaten aquatic ecosystems and human health\nthrough oxygen depletion, toxin release, and disruption of marine biodiversity.\nTraditional monitoring approaches, such as manual water sampling, remain\nlabor-intensive and limited in spatial and temporal coverage. Recent advances\nin vision-language models (VLMs) for remote sensing have shown potential for\nscalable AI-driven solutions, yet challenges remain in reasoning over imagery\nand quantifying bloom severity. In this work, we introduce ALGae Observation\nand Segmentation (ALGOS), a segmentation-and-reasoning system for HAB\nmonitoring that combines remote sensing image understanding with severity\nestimation. Our approach integrates GeoSAM-assisted human evaluation for\nhigh-quality segmentation mask curation and fine-tunes vision language model on\nseverity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)\nfrom NASA. Experiments demonstrate that ALGOS achieves robust performance on\nboth segmentation and severity-level estimation, paving the way toward\npractical and automated cyanobacterial monitoring systems.\n", "link": "http://arxiv.org/abs/2510.18751v1", "date": "2025-10-21", "relevancy": 1.583, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5362}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5211}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seg%20the%20HAB%3A%20Language-Guided%20Geospatial%20Algae%20Bloom%20Reasoning%20and%0A%20%20Segmentation&body=Title%3A%20Seg%20the%20HAB%3A%20Language-Guided%20Geospatial%20Algae%20Bloom%20Reasoning%20and%0A%20%20Segmentation%0AAuthor%3A%20Patterson%20Hsieh%20and%20Jerry%20Yeh%20and%20Mao-Chi%20He%20and%20Wen-Han%20Hsieh%20and%20Elvis%20Hsieh%0AAbstract%3A%20%20%20Climate%20change%20is%20intensifying%20the%20occurrence%20of%20harmful%20algal%20bloom%20%28HAB%29%2C%0Aparticularly%20cyanobacteria%2C%20which%20threaten%20aquatic%20ecosystems%20and%20human%20health%0Athrough%20oxygen%20depletion%2C%20toxin%20release%2C%20and%20disruption%20of%20marine%20biodiversity.%0ATraditional%20monitoring%20approaches%2C%20such%20as%20manual%20water%20sampling%2C%20remain%0Alabor-intensive%20and%20limited%20in%20spatial%20and%20temporal%20coverage.%20Recent%20advances%0Ain%20vision-language%20models%20%28VLMs%29%20for%20remote%20sensing%20have%20shown%20potential%20for%0Ascalable%20AI-driven%20solutions%2C%20yet%20challenges%20remain%20in%20reasoning%20over%20imagery%0Aand%20quantifying%20bloom%20severity.%20In%20this%20work%2C%20we%20introduce%20ALGae%20Observation%0Aand%20Segmentation%20%28ALGOS%29%2C%20a%20segmentation-and-reasoning%20system%20for%20HAB%0Amonitoring%20that%20combines%20remote%20sensing%20image%20understanding%20with%20severity%0Aestimation.%20Our%20approach%20integrates%20GeoSAM-assisted%20human%20evaluation%20for%0Ahigh-quality%20segmentation%20mask%20curation%20and%20fine-tunes%20vision%20language%20model%20on%0Aseverity%20prediction%20using%20the%20Cyanobacteria%20Aggregated%20Manual%20Labels%20%28CAML%29%0Afrom%20NASA.%20Experiments%20demonstrate%20that%20ALGOS%20achieves%20robust%20performance%20on%0Aboth%20segmentation%20and%20severity-level%20estimation%2C%20paving%20the%20way%20toward%0Apractical%20and%20automated%20cyanobacterial%20monitoring%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeg%2520the%2520HAB%253A%2520Language-Guided%2520Geospatial%2520Algae%2520Bloom%2520Reasoning%2520and%250A%2520%2520Segmentation%26entry.906535625%3DPatterson%2520Hsieh%2520and%2520Jerry%2520Yeh%2520and%2520Mao-Chi%2520He%2520and%2520Wen-Han%2520Hsieh%2520and%2520Elvis%2520Hsieh%26entry.1292438233%3D%2520%2520Climate%2520change%2520is%2520intensifying%2520the%2520occurrence%2520of%2520harmful%2520algal%2520bloom%2520%2528HAB%2529%252C%250Aparticularly%2520cyanobacteria%252C%2520which%2520threaten%2520aquatic%2520ecosystems%2520and%2520human%2520health%250Athrough%2520oxygen%2520depletion%252C%2520toxin%2520release%252C%2520and%2520disruption%2520of%2520marine%2520biodiversity.%250ATraditional%2520monitoring%2520approaches%252C%2520such%2520as%2520manual%2520water%2520sampling%252C%2520remain%250Alabor-intensive%2520and%2520limited%2520in%2520spatial%2520and%2520temporal%2520coverage.%2520Recent%2520advances%250Ain%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520remote%2520sensing%2520have%2520shown%2520potential%2520for%250Ascalable%2520AI-driven%2520solutions%252C%2520yet%2520challenges%2520remain%2520in%2520reasoning%2520over%2520imagery%250Aand%2520quantifying%2520bloom%2520severity.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ALGae%2520Observation%250Aand%2520Segmentation%2520%2528ALGOS%2529%252C%2520a%2520segmentation-and-reasoning%2520system%2520for%2520HAB%250Amonitoring%2520that%2520combines%2520remote%2520sensing%2520image%2520understanding%2520with%2520severity%250Aestimation.%2520Our%2520approach%2520integrates%2520GeoSAM-assisted%2520human%2520evaluation%2520for%250Ahigh-quality%2520segmentation%2520mask%2520curation%2520and%2520fine-tunes%2520vision%2520language%2520model%2520on%250Aseverity%2520prediction%2520using%2520the%2520Cyanobacteria%2520Aggregated%2520Manual%2520Labels%2520%2528CAML%2529%250Afrom%2520NASA.%2520Experiments%2520demonstrate%2520that%2520ALGOS%2520achieves%2520robust%2520performance%2520on%250Aboth%2520segmentation%2520and%2520severity-level%2520estimation%252C%2520paving%2520the%2520way%2520toward%250Apractical%2520and%2520automated%2520cyanobacterial%2520monitoring%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seg%20the%20HAB%3A%20Language-Guided%20Geospatial%20Algae%20Bloom%20Reasoning%20and%0A%20%20Segmentation&entry.906535625=Patterson%20Hsieh%20and%20Jerry%20Yeh%20and%20Mao-Chi%20He%20and%20Wen-Han%20Hsieh%20and%20Elvis%20Hsieh&entry.1292438233=%20%20Climate%20change%20is%20intensifying%20the%20occurrence%20of%20harmful%20algal%20bloom%20%28HAB%29%2C%0Aparticularly%20cyanobacteria%2C%20which%20threaten%20aquatic%20ecosystems%20and%20human%20health%0Athrough%20oxygen%20depletion%2C%20toxin%20release%2C%20and%20disruption%20of%20marine%20biodiversity.%0ATraditional%20monitoring%20approaches%2C%20such%20as%20manual%20water%20sampling%2C%20remain%0Alabor-intensive%20and%20limited%20in%20spatial%20and%20temporal%20coverage.%20Recent%20advances%0Ain%20vision-language%20models%20%28VLMs%29%20for%20remote%20sensing%20have%20shown%20potential%20for%0Ascalable%20AI-driven%20solutions%2C%20yet%20challenges%20remain%20in%20reasoning%20over%20imagery%0Aand%20quantifying%20bloom%20severity.%20In%20this%20work%2C%20we%20introduce%20ALGae%20Observation%0Aand%20Segmentation%20%28ALGOS%29%2C%20a%20segmentation-and-reasoning%20system%20for%20HAB%0Amonitoring%20that%20combines%20remote%20sensing%20image%20understanding%20with%20severity%0Aestimation.%20Our%20approach%20integrates%20GeoSAM-assisted%20human%20evaluation%20for%0Ahigh-quality%20segmentation%20mask%20curation%20and%20fine-tunes%20vision%20language%20model%20on%0Aseverity%20prediction%20using%20the%20Cyanobacteria%20Aggregated%20Manual%20Labels%20%28CAML%29%0Afrom%20NASA.%20Experiments%20demonstrate%20that%20ALGOS%20achieves%20robust%20performance%20on%0Aboth%20segmentation%20and%20severity-level%20estimation%2C%20paving%20the%20way%20toward%0Apractical%20and%20automated%20cyanobacterial%20monitoring%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18751v1&entry.124074799=Read"},
{"title": "Exploring Membership Inference Vulnerabilities in Clinical Large\n  Language Models", "author": "Alexander Nemecek and Zebin Yun and Zahra Rahmani and Yaniv Harel and Vipin Chaudhary and Mahmood Sharif and Erman Ayday", "abstract": "  As large language models (LLMs) become progressively more embedded in\nclinical decision-support, documentation, and patient-information systems,\nensuring their privacy and trustworthiness has emerged as an imperative\nchallenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic\nhealth record (EHR) data improves domain alignment but also raises the risk of\nexposing patient information through model behaviors. In this work-in-progress,\nwe present an exploratory empirical study on membership inference\nvulnerabilities in clinical LLMs, focusing on whether adversaries can infer if\nspecific patient records were used during model training. Using a\nstate-of-the-art clinical question-answering model, Llemr, we evaluate both\ncanonical loss-based attacks and a domain-motivated paraphrasing-based\nperturbation strategy that more realistically reflects clinical adversarial\nconditions. Our preliminary findings reveal limited but measurable membership\nleakage, suggesting that current clinical LLMs provide partial resistance yet\nremain susceptible to subtle privacy risks that could undermine trust in\nclinical AI adoption. These results motivate continued development of\ncontext-aware, domain-specific privacy evaluations and defenses such as\ndifferential privacy fine-tuning and paraphrase-aware training, to strengthen\nthe security and trustworthiness of healthcare AI systems.\n", "link": "http://arxiv.org/abs/2510.18674v1", "date": "2025-10-21", "relevancy": 1.9564, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Membership%20Inference%20Vulnerabilities%20in%20Clinical%20Large%0A%20%20Language%20Models&body=Title%3A%20Exploring%20Membership%20Inference%20Vulnerabilities%20in%20Clinical%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Alexander%20Nemecek%20and%20Zebin%20Yun%20and%20Zahra%20Rahmani%20and%20Yaniv%20Harel%20and%20Vipin%20Chaudhary%20and%20Mahmood%20Sharif%20and%20Erman%20Ayday%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20become%20progressively%20more%20embedded%20in%0Aclinical%20decision-support%2C%20documentation%2C%20and%20patient-information%20systems%2C%0Aensuring%20their%20privacy%20and%20trustworthiness%20has%20emerged%20as%20an%20imperative%0Achallenge%20for%20the%20healthcare%20sector.%20Fine-tuning%20LLMs%20on%20sensitive%20electronic%0Ahealth%20record%20%28EHR%29%20data%20improves%20domain%20alignment%20but%20also%20raises%20the%20risk%20of%0Aexposing%20patient%20information%20through%20model%20behaviors.%20In%20this%20work-in-progress%2C%0Awe%20present%20an%20exploratory%20empirical%20study%20on%20membership%20inference%0Avulnerabilities%20in%20clinical%20LLMs%2C%20focusing%20on%20whether%20adversaries%20can%20infer%20if%0Aspecific%20patient%20records%20were%20used%20during%20model%20training.%20Using%20a%0Astate-of-the-art%20clinical%20question-answering%20model%2C%20Llemr%2C%20we%20evaluate%20both%0Acanonical%20loss-based%20attacks%20and%20a%20domain-motivated%20paraphrasing-based%0Aperturbation%20strategy%20that%20more%20realistically%20reflects%20clinical%20adversarial%0Aconditions.%20Our%20preliminary%20findings%20reveal%20limited%20but%20measurable%20membership%0Aleakage%2C%20suggesting%20that%20current%20clinical%20LLMs%20provide%20partial%20resistance%20yet%0Aremain%20susceptible%20to%20subtle%20privacy%20risks%20that%20could%20undermine%20trust%20in%0Aclinical%20AI%20adoption.%20These%20results%20motivate%20continued%20development%20of%0Acontext-aware%2C%20domain-specific%20privacy%20evaluations%20and%20defenses%20such%20as%0Adifferential%20privacy%20fine-tuning%20and%20paraphrase-aware%20training%2C%20to%20strengthen%0Athe%20security%20and%20trustworthiness%20of%20healthcare%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Membership%2520Inference%2520Vulnerabilities%2520in%2520Clinical%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DAlexander%2520Nemecek%2520and%2520Zebin%2520Yun%2520and%2520Zahra%2520Rahmani%2520and%2520Yaniv%2520Harel%2520and%2520Vipin%2520Chaudhary%2520and%2520Mahmood%2520Sharif%2520and%2520Erman%2520Ayday%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%2520progressively%2520more%2520embedded%2520in%250Aclinical%2520decision-support%252C%2520documentation%252C%2520and%2520patient-information%2520systems%252C%250Aensuring%2520their%2520privacy%2520and%2520trustworthiness%2520has%2520emerged%2520as%2520an%2520imperative%250Achallenge%2520for%2520the%2520healthcare%2520sector.%2520Fine-tuning%2520LLMs%2520on%2520sensitive%2520electronic%250Ahealth%2520record%2520%2528EHR%2529%2520data%2520improves%2520domain%2520alignment%2520but%2520also%2520raises%2520the%2520risk%2520of%250Aexposing%2520patient%2520information%2520through%2520model%2520behaviors.%2520In%2520this%2520work-in-progress%252C%250Awe%2520present%2520an%2520exploratory%2520empirical%2520study%2520on%2520membership%2520inference%250Avulnerabilities%2520in%2520clinical%2520LLMs%252C%2520focusing%2520on%2520whether%2520adversaries%2520can%2520infer%2520if%250Aspecific%2520patient%2520records%2520were%2520used%2520during%2520model%2520training.%2520Using%2520a%250Astate-of-the-art%2520clinical%2520question-answering%2520model%252C%2520Llemr%252C%2520we%2520evaluate%2520both%250Acanonical%2520loss-based%2520attacks%2520and%2520a%2520domain-motivated%2520paraphrasing-based%250Aperturbation%2520strategy%2520that%2520more%2520realistically%2520reflects%2520clinical%2520adversarial%250Aconditions.%2520Our%2520preliminary%2520findings%2520reveal%2520limited%2520but%2520measurable%2520membership%250Aleakage%252C%2520suggesting%2520that%2520current%2520clinical%2520LLMs%2520provide%2520partial%2520resistance%2520yet%250Aremain%2520susceptible%2520to%2520subtle%2520privacy%2520risks%2520that%2520could%2520undermine%2520trust%2520in%250Aclinical%2520AI%2520adoption.%2520These%2520results%2520motivate%2520continued%2520development%2520of%250Acontext-aware%252C%2520domain-specific%2520privacy%2520evaluations%2520and%2520defenses%2520such%2520as%250Adifferential%2520privacy%2520fine-tuning%2520and%2520paraphrase-aware%2520training%252C%2520to%2520strengthen%250Athe%2520security%2520and%2520trustworthiness%2520of%2520healthcare%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Membership%20Inference%20Vulnerabilities%20in%20Clinical%20Large%0A%20%20Language%20Models&entry.906535625=Alexander%20Nemecek%20and%20Zebin%20Yun%20and%20Zahra%20Rahmani%20and%20Yaniv%20Harel%20and%20Vipin%20Chaudhary%20and%20Mahmood%20Sharif%20and%20Erman%20Ayday&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20become%20progressively%20more%20embedded%20in%0Aclinical%20decision-support%2C%20documentation%2C%20and%20patient-information%20systems%2C%0Aensuring%20their%20privacy%20and%20trustworthiness%20has%20emerged%20as%20an%20imperative%0Achallenge%20for%20the%20healthcare%20sector.%20Fine-tuning%20LLMs%20on%20sensitive%20electronic%0Ahealth%20record%20%28EHR%29%20data%20improves%20domain%20alignment%20but%20also%20raises%20the%20risk%20of%0Aexposing%20patient%20information%20through%20model%20behaviors.%20In%20this%20work-in-progress%2C%0Awe%20present%20an%20exploratory%20empirical%20study%20on%20membership%20inference%0Avulnerabilities%20in%20clinical%20LLMs%2C%20focusing%20on%20whether%20adversaries%20can%20infer%20if%0Aspecific%20patient%20records%20were%20used%20during%20model%20training.%20Using%20a%0Astate-of-the-art%20clinical%20question-answering%20model%2C%20Llemr%2C%20we%20evaluate%20both%0Acanonical%20loss-based%20attacks%20and%20a%20domain-motivated%20paraphrasing-based%0Aperturbation%20strategy%20that%20more%20realistically%20reflects%20clinical%20adversarial%0Aconditions.%20Our%20preliminary%20findings%20reveal%20limited%20but%20measurable%20membership%0Aleakage%2C%20suggesting%20that%20current%20clinical%20LLMs%20provide%20partial%20resistance%20yet%0Aremain%20susceptible%20to%20subtle%20privacy%20risks%20that%20could%20undermine%20trust%20in%0Aclinical%20AI%20adoption.%20These%20results%20motivate%20continued%20development%20of%0Acontext-aware%2C%20domain-specific%20privacy%20evaluations%20and%20defenses%20such%20as%0Adifferential%20privacy%20fine-tuning%20and%20paraphrase-aware%20training%2C%20to%20strengthen%0Athe%20security%20and%20trustworthiness%20of%20healthcare%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18674v1&entry.124074799=Read"},
{"title": "Least Restrictive Hyperplane Control Barrier Functions", "author": "Mattias Trende and Petter \u00d6gren", "abstract": "  Control Barrier Functions (CBFs) can provide provable safety guarantees for\ndynamic systems. However, finding a valid CBF for a system of interest is often\nnon-trivial, especially if the shape of the unsafe region is complex and the\nCBFs are of higher order. A common solution to this problem is to make a\nconservative approximation of the unsafe region in the form of a\nline/hyperplane, and use the corresponding conservative Hyperplane-CBF when\ndeciding on safe control actions. In this letter, we note that conservative\nconstraints are only a problem if they prevent us from doing what we want.\nThus, instead of first choosing a CBF and then choosing a safe control with\nrespect to the CBF, we optimize over a combination of CBFs and safe controls to\nget as close as possible to our desired control, while still having the safety\nguarantee provided by the CBF. We call the corresponding CBF the least\nrestrictive Hyperplane-CBF. Finally, we also provide a way of creating a smooth\nparameterization of the CBF-family for the optimization, and illustrate the\napproach on a double integrator dynamical system with acceleration constraints,\nmoving through a group of arbitrarily shaped static and moving obstacles.\n", "link": "http://arxiv.org/abs/2510.18643v1", "date": "2025-10-21", "relevancy": 1.7482, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4449}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Least%20Restrictive%20Hyperplane%20Control%20Barrier%20Functions&body=Title%3A%20Least%20Restrictive%20Hyperplane%20Control%20Barrier%20Functions%0AAuthor%3A%20Mattias%20Trende%20and%20Petter%20%C3%96gren%0AAbstract%3A%20%20%20Control%20Barrier%20Functions%20%28CBFs%29%20can%20provide%20provable%20safety%20guarantees%20for%0Adynamic%20systems.%20However%2C%20finding%20a%20valid%20CBF%20for%20a%20system%20of%20interest%20is%20often%0Anon-trivial%2C%20especially%20if%20the%20shape%20of%20the%20unsafe%20region%20is%20complex%20and%20the%0ACBFs%20are%20of%20higher%20order.%20A%20common%20solution%20to%20this%20problem%20is%20to%20make%20a%0Aconservative%20approximation%20of%20the%20unsafe%20region%20in%20the%20form%20of%20a%0Aline/hyperplane%2C%20and%20use%20the%20corresponding%20conservative%20Hyperplane-CBF%20when%0Adeciding%20on%20safe%20control%20actions.%20In%20this%20letter%2C%20we%20note%20that%20conservative%0Aconstraints%20are%20only%20a%20problem%20if%20they%20prevent%20us%20from%20doing%20what%20we%20want.%0AThus%2C%20instead%20of%20first%20choosing%20a%20CBF%20and%20then%20choosing%20a%20safe%20control%20with%0Arespect%20to%20the%20CBF%2C%20we%20optimize%20over%20a%20combination%20of%20CBFs%20and%20safe%20controls%20to%0Aget%20as%20close%20as%20possible%20to%20our%20desired%20control%2C%20while%20still%20having%20the%20safety%0Aguarantee%20provided%20by%20the%20CBF.%20We%20call%20the%20corresponding%20CBF%20the%20least%0Arestrictive%20Hyperplane-CBF.%20Finally%2C%20we%20also%20provide%20a%20way%20of%20creating%20a%20smooth%0Aparameterization%20of%20the%20CBF-family%20for%20the%20optimization%2C%20and%20illustrate%20the%0Aapproach%20on%20a%20double%20integrator%20dynamical%20system%20with%20acceleration%20constraints%2C%0Amoving%20through%20a%20group%20of%20arbitrarily%20shaped%20static%20and%20moving%20obstacles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeast%2520Restrictive%2520Hyperplane%2520Control%2520Barrier%2520Functions%26entry.906535625%3DMattias%2520Trende%2520and%2520Petter%2520%25C3%2596gren%26entry.1292438233%3D%2520%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529%2520can%2520provide%2520provable%2520safety%2520guarantees%2520for%250Adynamic%2520systems.%2520However%252C%2520finding%2520a%2520valid%2520CBF%2520for%2520a%2520system%2520of%2520interest%2520is%2520often%250Anon-trivial%252C%2520especially%2520if%2520the%2520shape%2520of%2520the%2520unsafe%2520region%2520is%2520complex%2520and%2520the%250ACBFs%2520are%2520of%2520higher%2520order.%2520A%2520common%2520solution%2520to%2520this%2520problem%2520is%2520to%2520make%2520a%250Aconservative%2520approximation%2520of%2520the%2520unsafe%2520region%2520in%2520the%2520form%2520of%2520a%250Aline/hyperplane%252C%2520and%2520use%2520the%2520corresponding%2520conservative%2520Hyperplane-CBF%2520when%250Adeciding%2520on%2520safe%2520control%2520actions.%2520In%2520this%2520letter%252C%2520we%2520note%2520that%2520conservative%250Aconstraints%2520are%2520only%2520a%2520problem%2520if%2520they%2520prevent%2520us%2520from%2520doing%2520what%2520we%2520want.%250AThus%252C%2520instead%2520of%2520first%2520choosing%2520a%2520CBF%2520and%2520then%2520choosing%2520a%2520safe%2520control%2520with%250Arespect%2520to%2520the%2520CBF%252C%2520we%2520optimize%2520over%2520a%2520combination%2520of%2520CBFs%2520and%2520safe%2520controls%2520to%250Aget%2520as%2520close%2520as%2520possible%2520to%2520our%2520desired%2520control%252C%2520while%2520still%2520having%2520the%2520safety%250Aguarantee%2520provided%2520by%2520the%2520CBF.%2520We%2520call%2520the%2520corresponding%2520CBF%2520the%2520least%250Arestrictive%2520Hyperplane-CBF.%2520Finally%252C%2520we%2520also%2520provide%2520a%2520way%2520of%2520creating%2520a%2520smooth%250Aparameterization%2520of%2520the%2520CBF-family%2520for%2520the%2520optimization%252C%2520and%2520illustrate%2520the%250Aapproach%2520on%2520a%2520double%2520integrator%2520dynamical%2520system%2520with%2520acceleration%2520constraints%252C%250Amoving%2520through%2520a%2520group%2520of%2520arbitrarily%2520shaped%2520static%2520and%2520moving%2520obstacles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Least%20Restrictive%20Hyperplane%20Control%20Barrier%20Functions&entry.906535625=Mattias%20Trende%20and%20Petter%20%C3%96gren&entry.1292438233=%20%20Control%20Barrier%20Functions%20%28CBFs%29%20can%20provide%20provable%20safety%20guarantees%20for%0Adynamic%20systems.%20However%2C%20finding%20a%20valid%20CBF%20for%20a%20system%20of%20interest%20is%20often%0Anon-trivial%2C%20especially%20if%20the%20shape%20of%20the%20unsafe%20region%20is%20complex%20and%20the%0ACBFs%20are%20of%20higher%20order.%20A%20common%20solution%20to%20this%20problem%20is%20to%20make%20a%0Aconservative%20approximation%20of%20the%20unsafe%20region%20in%20the%20form%20of%20a%0Aline/hyperplane%2C%20and%20use%20the%20corresponding%20conservative%20Hyperplane-CBF%20when%0Adeciding%20on%20safe%20control%20actions.%20In%20this%20letter%2C%20we%20note%20that%20conservative%0Aconstraints%20are%20only%20a%20problem%20if%20they%20prevent%20us%20from%20doing%20what%20we%20want.%0AThus%2C%20instead%20of%20first%20choosing%20a%20CBF%20and%20then%20choosing%20a%20safe%20control%20with%0Arespect%20to%20the%20CBF%2C%20we%20optimize%20over%20a%20combination%20of%20CBFs%20and%20safe%20controls%20to%0Aget%20as%20close%20as%20possible%20to%20our%20desired%20control%2C%20while%20still%20having%20the%20safety%0Aguarantee%20provided%20by%20the%20CBF.%20We%20call%20the%20corresponding%20CBF%20the%20least%0Arestrictive%20Hyperplane-CBF.%20Finally%2C%20we%20also%20provide%20a%20way%20of%20creating%20a%20smooth%0Aparameterization%20of%20the%20CBF-family%20for%20the%20optimization%2C%20and%20illustrate%20the%0Aapproach%20on%20a%20double%20integrator%20dynamical%20system%20with%20acceleration%20constraints%2C%0Amoving%20through%20a%20group%20of%20arbitrarily%20shaped%20static%20and%20moving%20obstacles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18643v1&entry.124074799=Read"},
{"title": "Regression is all you need for medical image translation", "author": "Sebastian Rassmann and David K\u00fcgler and Christian Ewert and Martin Reuter", "abstract": "  While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved impressive results in natural image synthesis, their core strengths -\ncreativity and realism - can be detrimental in medical applications, where\naccuracy and fidelity are paramount. These models instead risk introducing\nhallucinations and replication of unwanted acquisition noise. Here, we propose\nYODA (You Only Denoise once - or Average), a 2.5D diffusion-based framework for\nmedical image translation (MIT). Consistent with DM theory, we find that\nconventional diffusion sampling stochastically replicates noise. To mitigate\nthis, we draw and average multiple samples, akin to physical signal averaging.\nAs this effectively approximates the DM's expected value, we term this\nExpectation-Approximation (ExpA) sampling. We additionally propose regression\nsampling YODA, which retains the initial DM prediction and omits iterative\nrefinement to produce noise-free images in a single step. Across five diverse\nmulti-modal datasets - including multi-contrast brain MRI and pelvic MRI-CT -\nwe demonstrate that regression sampling is not only substantially more\nefficient but also matches or exceeds image quality of full diffusion sampling\neven with ExpA. Our results reveal that iterative refinement solely enhances\nperceptual realism without benefiting information translation, which we confirm\nin relevant downstream tasks. YODA outperforms eight state-of-the-art DMs and\nGANs and challenges the presumed superiority of DMs and GANs over\ncomputationally cheap regression models for high-quality MIT. Furthermore, we\nshow that YODA-translated images are interchangeable with, or even superior to,\nphysical acquisitions for several medical applications.\n", "link": "http://arxiv.org/abs/2505.02048v3", "date": "2025-10-21", "relevancy": 1.7276, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5904}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5841}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regression%20is%20all%20you%20need%20for%20medical%20image%20translation&body=Title%3A%20Regression%20is%20all%20you%20need%20for%20medical%20image%20translation%0AAuthor%3A%20Sebastian%20Rassmann%20and%20David%20K%C3%BCgler%20and%20Christian%20Ewert%20and%20Martin%20Reuter%0AAbstract%3A%20%20%20While%20Generative%20Adversarial%20Nets%20%28GANs%29%20and%20Diffusion%20Models%20%28DMs%29%20have%0Aachieved%20impressive%20results%20in%20natural%20image%20synthesis%2C%20their%20core%20strengths%20-%0Acreativity%20and%20realism%20-%20can%20be%20detrimental%20in%20medical%20applications%2C%20where%0Aaccuracy%20and%20fidelity%20are%20paramount.%20These%20models%20instead%20risk%20introducing%0Ahallucinations%20and%20replication%20of%20unwanted%20acquisition%20noise.%20Here%2C%20we%20propose%0AYODA%20%28You%20Only%20Denoise%20once%20-%20or%20Average%29%2C%20a%202.5D%20diffusion-based%20framework%20for%0Amedical%20image%20translation%20%28MIT%29.%20Consistent%20with%20DM%20theory%2C%20we%20find%20that%0Aconventional%20diffusion%20sampling%20stochastically%20replicates%20noise.%20To%20mitigate%0Athis%2C%20we%20draw%20and%20average%20multiple%20samples%2C%20akin%20to%20physical%20signal%20averaging.%0AAs%20this%20effectively%20approximates%20the%20DM%27s%20expected%20value%2C%20we%20term%20this%0AExpectation-Approximation%20%28ExpA%29%20sampling.%20We%20additionally%20propose%20regression%0Asampling%20YODA%2C%20which%20retains%20the%20initial%20DM%20prediction%20and%20omits%20iterative%0Arefinement%20to%20produce%20noise-free%20images%20in%20a%20single%20step.%20Across%20five%20diverse%0Amulti-modal%20datasets%20-%20including%20multi-contrast%20brain%20MRI%20and%20pelvic%20MRI-CT%20-%0Awe%20demonstrate%20that%20regression%20sampling%20is%20not%20only%20substantially%20more%0Aefficient%20but%20also%20matches%20or%20exceeds%20image%20quality%20of%20full%20diffusion%20sampling%0Aeven%20with%20ExpA.%20Our%20results%20reveal%20that%20iterative%20refinement%20solely%20enhances%0Aperceptual%20realism%20without%20benefiting%20information%20translation%2C%20which%20we%20confirm%0Ain%20relevant%20downstream%20tasks.%20YODA%20outperforms%20eight%20state-of-the-art%20DMs%20and%0AGANs%20and%20challenges%20the%20presumed%20superiority%20of%20DMs%20and%20GANs%20over%0Acomputationally%20cheap%20regression%20models%20for%20high-quality%20MIT.%20Furthermore%2C%20we%0Ashow%20that%20YODA-translated%20images%20are%20interchangeable%20with%2C%20or%20even%20superior%20to%2C%0Aphysical%20acquisitions%20for%20several%20medical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02048v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegression%2520is%2520all%2520you%2520need%2520for%2520medical%2520image%2520translation%26entry.906535625%3DSebastian%2520Rassmann%2520and%2520David%2520K%25C3%25BCgler%2520and%2520Christian%2520Ewert%2520and%2520Martin%2520Reuter%26entry.1292438233%3D%2520%2520While%2520Generative%2520Adversarial%2520Nets%2520%2528GANs%2529%2520and%2520Diffusion%2520Models%2520%2528DMs%2529%2520have%250Aachieved%2520impressive%2520results%2520in%2520natural%2520image%2520synthesis%252C%2520their%2520core%2520strengths%2520-%250Acreativity%2520and%2520realism%2520-%2520can%2520be%2520detrimental%2520in%2520medical%2520applications%252C%2520where%250Aaccuracy%2520and%2520fidelity%2520are%2520paramount.%2520These%2520models%2520instead%2520risk%2520introducing%250Ahallucinations%2520and%2520replication%2520of%2520unwanted%2520acquisition%2520noise.%2520Here%252C%2520we%2520propose%250AYODA%2520%2528You%2520Only%2520Denoise%2520once%2520-%2520or%2520Average%2529%252C%2520a%25202.5D%2520diffusion-based%2520framework%2520for%250Amedical%2520image%2520translation%2520%2528MIT%2529.%2520Consistent%2520with%2520DM%2520theory%252C%2520we%2520find%2520that%250Aconventional%2520diffusion%2520sampling%2520stochastically%2520replicates%2520noise.%2520To%2520mitigate%250Athis%252C%2520we%2520draw%2520and%2520average%2520multiple%2520samples%252C%2520akin%2520to%2520physical%2520signal%2520averaging.%250AAs%2520this%2520effectively%2520approximates%2520the%2520DM%2527s%2520expected%2520value%252C%2520we%2520term%2520this%250AExpectation-Approximation%2520%2528ExpA%2529%2520sampling.%2520We%2520additionally%2520propose%2520regression%250Asampling%2520YODA%252C%2520which%2520retains%2520the%2520initial%2520DM%2520prediction%2520and%2520omits%2520iterative%250Arefinement%2520to%2520produce%2520noise-free%2520images%2520in%2520a%2520single%2520step.%2520Across%2520five%2520diverse%250Amulti-modal%2520datasets%2520-%2520including%2520multi-contrast%2520brain%2520MRI%2520and%2520pelvic%2520MRI-CT%2520-%250Awe%2520demonstrate%2520that%2520regression%2520sampling%2520is%2520not%2520only%2520substantially%2520more%250Aefficient%2520but%2520also%2520matches%2520or%2520exceeds%2520image%2520quality%2520of%2520full%2520diffusion%2520sampling%250Aeven%2520with%2520ExpA.%2520Our%2520results%2520reveal%2520that%2520iterative%2520refinement%2520solely%2520enhances%250Aperceptual%2520realism%2520without%2520benefiting%2520information%2520translation%252C%2520which%2520we%2520confirm%250Ain%2520relevant%2520downstream%2520tasks.%2520YODA%2520outperforms%2520eight%2520state-of-the-art%2520DMs%2520and%250AGANs%2520and%2520challenges%2520the%2520presumed%2520superiority%2520of%2520DMs%2520and%2520GANs%2520over%250Acomputationally%2520cheap%2520regression%2520models%2520for%2520high-quality%2520MIT.%2520Furthermore%252C%2520we%250Ashow%2520that%2520YODA-translated%2520images%2520are%2520interchangeable%2520with%252C%2520or%2520even%2520superior%2520to%252C%250Aphysical%2520acquisitions%2520for%2520several%2520medical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02048v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regression%20is%20all%20you%20need%20for%20medical%20image%20translation&entry.906535625=Sebastian%20Rassmann%20and%20David%20K%C3%BCgler%20and%20Christian%20Ewert%20and%20Martin%20Reuter&entry.1292438233=%20%20While%20Generative%20Adversarial%20Nets%20%28GANs%29%20and%20Diffusion%20Models%20%28DMs%29%20have%0Aachieved%20impressive%20results%20in%20natural%20image%20synthesis%2C%20their%20core%20strengths%20-%0Acreativity%20and%20realism%20-%20can%20be%20detrimental%20in%20medical%20applications%2C%20where%0Aaccuracy%20and%20fidelity%20are%20paramount.%20These%20models%20instead%20risk%20introducing%0Ahallucinations%20and%20replication%20of%20unwanted%20acquisition%20noise.%20Here%2C%20we%20propose%0AYODA%20%28You%20Only%20Denoise%20once%20-%20or%20Average%29%2C%20a%202.5D%20diffusion-based%20framework%20for%0Amedical%20image%20translation%20%28MIT%29.%20Consistent%20with%20DM%20theory%2C%20we%20find%20that%0Aconventional%20diffusion%20sampling%20stochastically%20replicates%20noise.%20To%20mitigate%0Athis%2C%20we%20draw%20and%20average%20multiple%20samples%2C%20akin%20to%20physical%20signal%20averaging.%0AAs%20this%20effectively%20approximates%20the%20DM%27s%20expected%20value%2C%20we%20term%20this%0AExpectation-Approximation%20%28ExpA%29%20sampling.%20We%20additionally%20propose%20regression%0Asampling%20YODA%2C%20which%20retains%20the%20initial%20DM%20prediction%20and%20omits%20iterative%0Arefinement%20to%20produce%20noise-free%20images%20in%20a%20single%20step.%20Across%20five%20diverse%0Amulti-modal%20datasets%20-%20including%20multi-contrast%20brain%20MRI%20and%20pelvic%20MRI-CT%20-%0Awe%20demonstrate%20that%20regression%20sampling%20is%20not%20only%20substantially%20more%0Aefficient%20but%20also%20matches%20or%20exceeds%20image%20quality%20of%20full%20diffusion%20sampling%0Aeven%20with%20ExpA.%20Our%20results%20reveal%20that%20iterative%20refinement%20solely%20enhances%0Aperceptual%20realism%20without%20benefiting%20information%20translation%2C%20which%20we%20confirm%0Ain%20relevant%20downstream%20tasks.%20YODA%20outperforms%20eight%20state-of-the-art%20DMs%20and%0AGANs%20and%20challenges%20the%20presumed%20superiority%20of%20DMs%20and%20GANs%20over%0Acomputationally%20cheap%20regression%20models%20for%20high-quality%20MIT.%20Furthermore%2C%20we%0Ashow%20that%20YODA-translated%20images%20are%20interchangeable%20with%2C%20or%20even%20superior%20to%2C%0Aphysical%20acquisitions%20for%20several%20medical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02048v3&entry.124074799=Read"},
{"title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires\n  Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations", "author": "Jinkun Chen and Sher Badshah and Xuemin Yu and Sijia Han", "abstract": "  What if artificial agents could not just communicate, but also evolve, adapt,\nand reshape their worlds in ways we cannot fully predict? With llm now powering\nmulti-agent systems and social simulations, we are witnessing new possibilities\nfor modeling open-ended, ever-changing environments. Yet, most current\nsimulations remain constrained within static sandboxes, characterized by\npredefined tasks, limited dynamics, and rigid evaluation criteria. These\nlimitations prevent them from capturing the complexity of real-world societies.\nIn this paper, we argue that static, task-specific benchmarks are fundamentally\ninadequate and must be rethought. We critically review emerging architectures\nthat blend llm with multi-agent dynamics, highlight key hurdles such as\nbalancing stability and diversity, evaluating unexpected behaviors, and scaling\nto greater complexity, and introduce a fresh taxonomy for this rapidly evolving\nfield. Finally, we present a research roadmap centered on open-endedness,\ncontinuous co-evolution, and the development of resilient, socially aligned AI\necosystems. We call on the community to move beyond static paradigms and help\nshape the next generation of adaptive, socially-aware multi-agent simulations.\n", "link": "http://arxiv.org/abs/2510.13982v3", "date": "2025-10-21", "relevancy": 1.5147, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Static%20Sandboxes%20Are%20Inadequate%3A%20Modeling%20Societal%20Complexity%20Requires%0A%20%20Open-Ended%20Co-Evolution%20in%20LLM-Based%20Multi-Agent%20Simulations&body=Title%3A%20Static%20Sandboxes%20Are%20Inadequate%3A%20Modeling%20Societal%20Complexity%20Requires%0A%20%20Open-Ended%20Co-Evolution%20in%20LLM-Based%20Multi-Agent%20Simulations%0AAuthor%3A%20Jinkun%20Chen%20and%20Sher%20Badshah%20and%20Xuemin%20Yu%20and%20Sijia%20Han%0AAbstract%3A%20%20%20What%20if%20artificial%20agents%20could%20not%20just%20communicate%2C%20but%20also%20evolve%2C%20adapt%2C%0Aand%20reshape%20their%20worlds%20in%20ways%20we%20cannot%20fully%20predict%3F%20With%20llm%20now%20powering%0Amulti-agent%20systems%20and%20social%20simulations%2C%20we%20are%20witnessing%20new%20possibilities%0Afor%20modeling%20open-ended%2C%20ever-changing%20environments.%20Yet%2C%20most%20current%0Asimulations%20remain%20constrained%20within%20static%20sandboxes%2C%20characterized%20by%0Apredefined%20tasks%2C%20limited%20dynamics%2C%20and%20rigid%20evaluation%20criteria.%20These%0Alimitations%20prevent%20them%20from%20capturing%20the%20complexity%20of%20real-world%20societies.%0AIn%20this%20paper%2C%20we%20argue%20that%20static%2C%20task-specific%20benchmarks%20are%20fundamentally%0Ainadequate%20and%20must%20be%20rethought.%20We%20critically%20review%20emerging%20architectures%0Athat%20blend%20llm%20with%20multi-agent%20dynamics%2C%20highlight%20key%20hurdles%20such%20as%0Abalancing%20stability%20and%20diversity%2C%20evaluating%20unexpected%20behaviors%2C%20and%20scaling%0Ato%20greater%20complexity%2C%20and%20introduce%20a%20fresh%20taxonomy%20for%20this%20rapidly%20evolving%0Afield.%20Finally%2C%20we%20present%20a%20research%20roadmap%20centered%20on%20open-endedness%2C%0Acontinuous%20co-evolution%2C%20and%20the%20development%20of%20resilient%2C%20socially%20aligned%20AI%0Aecosystems.%20We%20call%20on%20the%20community%20to%20move%20beyond%20static%20paradigms%20and%20help%0Ashape%20the%20next%20generation%20of%20adaptive%2C%20socially-aware%20multi-agent%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13982v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatic%2520Sandboxes%2520Are%2520Inadequate%253A%2520Modeling%2520Societal%2520Complexity%2520Requires%250A%2520%2520Open-Ended%2520Co-Evolution%2520in%2520LLM-Based%2520Multi-Agent%2520Simulations%26entry.906535625%3DJinkun%2520Chen%2520and%2520Sher%2520Badshah%2520and%2520Xuemin%2520Yu%2520and%2520Sijia%2520Han%26entry.1292438233%3D%2520%2520What%2520if%2520artificial%2520agents%2520could%2520not%2520just%2520communicate%252C%2520but%2520also%2520evolve%252C%2520adapt%252C%250Aand%2520reshape%2520their%2520worlds%2520in%2520ways%2520we%2520cannot%2520fully%2520predict%253F%2520With%2520llm%2520now%2520powering%250Amulti-agent%2520systems%2520and%2520social%2520simulations%252C%2520we%2520are%2520witnessing%2520new%2520possibilities%250Afor%2520modeling%2520open-ended%252C%2520ever-changing%2520environments.%2520Yet%252C%2520most%2520current%250Asimulations%2520remain%2520constrained%2520within%2520static%2520sandboxes%252C%2520characterized%2520by%250Apredefined%2520tasks%252C%2520limited%2520dynamics%252C%2520and%2520rigid%2520evaluation%2520criteria.%2520These%250Alimitations%2520prevent%2520them%2520from%2520capturing%2520the%2520complexity%2520of%2520real-world%2520societies.%250AIn%2520this%2520paper%252C%2520we%2520argue%2520that%2520static%252C%2520task-specific%2520benchmarks%2520are%2520fundamentally%250Ainadequate%2520and%2520must%2520be%2520rethought.%2520We%2520critically%2520review%2520emerging%2520architectures%250Athat%2520blend%2520llm%2520with%2520multi-agent%2520dynamics%252C%2520highlight%2520key%2520hurdles%2520such%2520as%250Abalancing%2520stability%2520and%2520diversity%252C%2520evaluating%2520unexpected%2520behaviors%252C%2520and%2520scaling%250Ato%2520greater%2520complexity%252C%2520and%2520introduce%2520a%2520fresh%2520taxonomy%2520for%2520this%2520rapidly%2520evolving%250Afield.%2520Finally%252C%2520we%2520present%2520a%2520research%2520roadmap%2520centered%2520on%2520open-endedness%252C%250Acontinuous%2520co-evolution%252C%2520and%2520the%2520development%2520of%2520resilient%252C%2520socially%2520aligned%2520AI%250Aecosystems.%2520We%2520call%2520on%2520the%2520community%2520to%2520move%2520beyond%2520static%2520paradigms%2520and%2520help%250Ashape%2520the%2520next%2520generation%2520of%2520adaptive%252C%2520socially-aware%2520multi-agent%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13982v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Static%20Sandboxes%20Are%20Inadequate%3A%20Modeling%20Societal%20Complexity%20Requires%0A%20%20Open-Ended%20Co-Evolution%20in%20LLM-Based%20Multi-Agent%20Simulations&entry.906535625=Jinkun%20Chen%20and%20Sher%20Badshah%20and%20Xuemin%20Yu%20and%20Sijia%20Han&entry.1292438233=%20%20What%20if%20artificial%20agents%20could%20not%20just%20communicate%2C%20but%20also%20evolve%2C%20adapt%2C%0Aand%20reshape%20their%20worlds%20in%20ways%20we%20cannot%20fully%20predict%3F%20With%20llm%20now%20powering%0Amulti-agent%20systems%20and%20social%20simulations%2C%20we%20are%20witnessing%20new%20possibilities%0Afor%20modeling%20open-ended%2C%20ever-changing%20environments.%20Yet%2C%20most%20current%0Asimulations%20remain%20constrained%20within%20static%20sandboxes%2C%20characterized%20by%0Apredefined%20tasks%2C%20limited%20dynamics%2C%20and%20rigid%20evaluation%20criteria.%20These%0Alimitations%20prevent%20them%20from%20capturing%20the%20complexity%20of%20real-world%20societies.%0AIn%20this%20paper%2C%20we%20argue%20that%20static%2C%20task-specific%20benchmarks%20are%20fundamentally%0Ainadequate%20and%20must%20be%20rethought.%20We%20critically%20review%20emerging%20architectures%0Athat%20blend%20llm%20with%20multi-agent%20dynamics%2C%20highlight%20key%20hurdles%20such%20as%0Abalancing%20stability%20and%20diversity%2C%20evaluating%20unexpected%20behaviors%2C%20and%20scaling%0Ato%20greater%20complexity%2C%20and%20introduce%20a%20fresh%20taxonomy%20for%20this%20rapidly%20evolving%0Afield.%20Finally%2C%20we%20present%20a%20research%20roadmap%20centered%20on%20open-endedness%2C%0Acontinuous%20co-evolution%2C%20and%20the%20development%20of%20resilient%2C%20socially%20aligned%20AI%0Aecosystems.%20We%20call%20on%20the%20community%20to%20move%20beyond%20static%20paradigms%20and%20help%0Ashape%20the%20next%20generation%20of%20adaptive%2C%20socially-aware%20multi-agent%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13982v3&entry.124074799=Read"},
{"title": "Inverse Q-Learning Done Right: Offline Imitation Learning in\n  $Q^\u03c0$-Realizable MDPs", "author": "Antoine Moulin and Gergely Neu and Luca Viano", "abstract": "  We study the problem of offline imitation learning in Markov decision\nprocesses (MDPs), where the goal is to learn a well-performing policy given a\ndataset of state-action pairs generated by an expert policy. Complementing a\nrecent line of work on this topic that assumes the expert belongs to a\ntractable class of known policies, we approach this problem from a new angle\nand leverage a different type of structural assumption about the environment.\nSpecifically, for the class of linear $Q^\\pi$-realizable MDPs, we introduce a\nnew algorithm called saddle-point offline imitation learning (\\SPOIL), which is\nguaranteed to match the performance of any expert up to an additive error\n$\\varepsilon$ with access to $\\mathcal{O}(\\varepsilon^{-2})$ samples. Moreover,\nwe extend this result to possibly non-linear $Q^\\pi$-realizable MDPs at the\ncost of a worse sample complexity of order $\\mathcal{O}(\\varepsilon^{-4})$.\nFinally, our analysis suggests a new loss function for training critic networks\nfrom expert data in deep imitation learning. Empirical evaluations on standard\nbenchmarks demonstrate that the neural net implementation of \\SPOIL is superior\nto behavior cloning and competitive with state-of-the-art algorithms.\n", "link": "http://arxiv.org/abs/2505.19946v3", "date": "2025-10-21", "relevancy": 1.8699, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4679}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs&body=Title%3A%20Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs%0AAuthor%3A%20Antoine%20Moulin%20and%20Gergely%20Neu%20and%20Luca%20Viano%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20offline%20imitation%20learning%20in%20Markov%20decision%0Aprocesses%20%28MDPs%29%2C%20where%20the%20goal%20is%20to%20learn%20a%20well-performing%20policy%20given%20a%0Adataset%20of%20state-action%20pairs%20generated%20by%20an%20expert%20policy.%20Complementing%20a%0Arecent%20line%20of%20work%20on%20this%20topic%20that%20assumes%20the%20expert%20belongs%20to%20a%0Atractable%20class%20of%20known%20policies%2C%20we%20approach%20this%20problem%20from%20a%20new%20angle%0Aand%20leverage%20a%20different%20type%20of%20structural%20assumption%20about%20the%20environment.%0ASpecifically%2C%20for%20the%20class%20of%20linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%2C%20we%20introduce%20a%0Anew%20algorithm%20called%20saddle-point%20offline%20imitation%20learning%20%28%5CSPOIL%29%2C%20which%20is%0Aguaranteed%20to%20match%20the%20performance%20of%20any%20expert%20up%20to%20an%20additive%20error%0A%24%5Cvarepsilon%24%20with%20access%20to%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-2%7D%29%24%20samples.%20Moreover%2C%0Awe%20extend%20this%20result%20to%20possibly%20non-linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%20at%20the%0Acost%20of%20a%20worse%20sample%20complexity%20of%20order%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-4%7D%29%24.%0AFinally%2C%20our%20analysis%20suggests%20a%20new%20loss%20function%20for%20training%20critic%20networks%0Afrom%20expert%20data%20in%20deep%20imitation%20learning.%20Empirical%20evaluations%20on%20standard%0Abenchmarks%20demonstrate%20that%20the%20neural%20net%20implementation%20of%20%5CSPOIL%20is%20superior%0Ato%20behavior%20cloning%20and%20competitive%20with%20state-of-the-art%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19946v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Q-Learning%2520Done%2520Right%253A%2520Offline%2520Imitation%2520Learning%2520in%250A%2520%2520%2524Q%255E%25CF%2580%2524-Realizable%2520MDPs%26entry.906535625%3DAntoine%2520Moulin%2520and%2520Gergely%2520Neu%2520and%2520Luca%2520Viano%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520offline%2520imitation%2520learning%2520in%2520Markov%2520decision%250Aprocesses%2520%2528MDPs%2529%252C%2520where%2520the%2520goal%2520is%2520to%2520learn%2520a%2520well-performing%2520policy%2520given%2520a%250Adataset%2520of%2520state-action%2520pairs%2520generated%2520by%2520an%2520expert%2520policy.%2520Complementing%2520a%250Arecent%2520line%2520of%2520work%2520on%2520this%2520topic%2520that%2520assumes%2520the%2520expert%2520belongs%2520to%2520a%250Atractable%2520class%2520of%2520known%2520policies%252C%2520we%2520approach%2520this%2520problem%2520from%2520a%2520new%2520angle%250Aand%2520leverage%2520a%2520different%2520type%2520of%2520structural%2520assumption%2520about%2520the%2520environment.%250ASpecifically%252C%2520for%2520the%2520class%2520of%2520linear%2520%2524Q%255E%255Cpi%2524-realizable%2520MDPs%252C%2520we%2520introduce%2520a%250Anew%2520algorithm%2520called%2520saddle-point%2520offline%2520imitation%2520learning%2520%2528%255CSPOIL%2529%252C%2520which%2520is%250Aguaranteed%2520to%2520match%2520the%2520performance%2520of%2520any%2520expert%2520up%2520to%2520an%2520additive%2520error%250A%2524%255Cvarepsilon%2524%2520with%2520access%2520to%2520%2524%255Cmathcal%257BO%257D%2528%255Cvarepsilon%255E%257B-2%257D%2529%2524%2520samples.%2520Moreover%252C%250Awe%2520extend%2520this%2520result%2520to%2520possibly%2520non-linear%2520%2524Q%255E%255Cpi%2524-realizable%2520MDPs%2520at%2520the%250Acost%2520of%2520a%2520worse%2520sample%2520complexity%2520of%2520order%2520%2524%255Cmathcal%257BO%257D%2528%255Cvarepsilon%255E%257B-4%257D%2529%2524.%250AFinally%252C%2520our%2520analysis%2520suggests%2520a%2520new%2520loss%2520function%2520for%2520training%2520critic%2520networks%250Afrom%2520expert%2520data%2520in%2520deep%2520imitation%2520learning.%2520Empirical%2520evaluations%2520on%2520standard%250Abenchmarks%2520demonstrate%2520that%2520the%2520neural%2520net%2520implementation%2520of%2520%255CSPOIL%2520is%2520superior%250Ato%2520behavior%2520cloning%2520and%2520competitive%2520with%2520state-of-the-art%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19946v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Q-Learning%20Done%20Right%3A%20Offline%20Imitation%20Learning%20in%0A%20%20%24Q%5E%CF%80%24-Realizable%20MDPs&entry.906535625=Antoine%20Moulin%20and%20Gergely%20Neu%20and%20Luca%20Viano&entry.1292438233=%20%20We%20study%20the%20problem%20of%20offline%20imitation%20learning%20in%20Markov%20decision%0Aprocesses%20%28MDPs%29%2C%20where%20the%20goal%20is%20to%20learn%20a%20well-performing%20policy%20given%20a%0Adataset%20of%20state-action%20pairs%20generated%20by%20an%20expert%20policy.%20Complementing%20a%0Arecent%20line%20of%20work%20on%20this%20topic%20that%20assumes%20the%20expert%20belongs%20to%20a%0Atractable%20class%20of%20known%20policies%2C%20we%20approach%20this%20problem%20from%20a%20new%20angle%0Aand%20leverage%20a%20different%20type%20of%20structural%20assumption%20about%20the%20environment.%0ASpecifically%2C%20for%20the%20class%20of%20linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%2C%20we%20introduce%20a%0Anew%20algorithm%20called%20saddle-point%20offline%20imitation%20learning%20%28%5CSPOIL%29%2C%20which%20is%0Aguaranteed%20to%20match%20the%20performance%20of%20any%20expert%20up%20to%20an%20additive%20error%0A%24%5Cvarepsilon%24%20with%20access%20to%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-2%7D%29%24%20samples.%20Moreover%2C%0Awe%20extend%20this%20result%20to%20possibly%20non-linear%20%24Q%5E%5Cpi%24-realizable%20MDPs%20at%20the%0Acost%20of%20a%20worse%20sample%20complexity%20of%20order%20%24%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-4%7D%29%24.%0AFinally%2C%20our%20analysis%20suggests%20a%20new%20loss%20function%20for%20training%20critic%20networks%0Afrom%20expert%20data%20in%20deep%20imitation%20learning.%20Empirical%20evaluations%20on%20standard%0Abenchmarks%20demonstrate%20that%20the%20neural%20net%20implementation%20of%20%5CSPOIL%20is%20superior%0Ato%20behavior%20cloning%20and%20competitive%20with%20state-of-the-art%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19946v3&entry.124074799=Read"},
{"title": "Spike-timing-dependent Hebbian learning as noisy gradient descent", "author": "Niklas Dexheimer and Sascha Gaudlitz and Johannes Schmidt-Hieber", "abstract": "  Hebbian learning is a key principle underlying learning in biological neural\nnetworks. We relate a Hebbian spike-timing-dependent plasticity rule to noisy\ngradient descent with respect to a non-convex loss function on the probability\nsimplex. Despite the constant injection of noise and the non-convexity of the\nunderlying optimization problem, one can rigorously prove that the considered\nHebbian learning dynamic identifies the presynaptic neuron with the highest\nactivity and that the convergence is exponentially fast in the number of\niterations. This is non-standard and surprising as typically noisy gradient\ndescent with fixed noise level only converges to a stationary regime where the\nnoise causes the dynamic to fluctuate around a minimiser.\n", "link": "http://arxiv.org/abs/2505.10272v2", "date": "2025-10-21", "relevancy": 1.9148, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5298}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4435}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spike-timing-dependent%20Hebbian%20learning%20as%20noisy%20gradient%20descent&body=Title%3A%20Spike-timing-dependent%20Hebbian%20learning%20as%20noisy%20gradient%20descent%0AAuthor%3A%20Niklas%20Dexheimer%20and%20Sascha%20Gaudlitz%20and%20Johannes%20Schmidt-Hieber%0AAbstract%3A%20%20%20Hebbian%20learning%20is%20a%20key%20principle%20underlying%20learning%20in%20biological%20neural%0Anetworks.%20We%20relate%20a%20Hebbian%20spike-timing-dependent%20plasticity%20rule%20to%20noisy%0Agradient%20descent%20with%20respect%20to%20a%20non-convex%20loss%20function%20on%20the%20probability%0Asimplex.%20Despite%20the%20constant%20injection%20of%20noise%20and%20the%20non-convexity%20of%20the%0Aunderlying%20optimization%20problem%2C%20one%20can%20rigorously%20prove%20that%20the%20considered%0AHebbian%20learning%20dynamic%20identifies%20the%20presynaptic%20neuron%20with%20the%20highest%0Aactivity%20and%20that%20the%20convergence%20is%20exponentially%20fast%20in%20the%20number%20of%0Aiterations.%20This%20is%20non-standard%20and%20surprising%20as%20typically%20noisy%20gradient%0Adescent%20with%20fixed%20noise%20level%20only%20converges%20to%20a%20stationary%20regime%20where%20the%0Anoise%20causes%20the%20dynamic%20to%20fluctuate%20around%20a%20minimiser.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpike-timing-dependent%2520Hebbian%2520learning%2520as%2520noisy%2520gradient%2520descent%26entry.906535625%3DNiklas%2520Dexheimer%2520and%2520Sascha%2520Gaudlitz%2520and%2520Johannes%2520Schmidt-Hieber%26entry.1292438233%3D%2520%2520Hebbian%2520learning%2520is%2520a%2520key%2520principle%2520underlying%2520learning%2520in%2520biological%2520neural%250Anetworks.%2520We%2520relate%2520a%2520Hebbian%2520spike-timing-dependent%2520plasticity%2520rule%2520to%2520noisy%250Agradient%2520descent%2520with%2520respect%2520to%2520a%2520non-convex%2520loss%2520function%2520on%2520the%2520probability%250Asimplex.%2520Despite%2520the%2520constant%2520injection%2520of%2520noise%2520and%2520the%2520non-convexity%2520of%2520the%250Aunderlying%2520optimization%2520problem%252C%2520one%2520can%2520rigorously%2520prove%2520that%2520the%2520considered%250AHebbian%2520learning%2520dynamic%2520identifies%2520the%2520presynaptic%2520neuron%2520with%2520the%2520highest%250Aactivity%2520and%2520that%2520the%2520convergence%2520is%2520exponentially%2520fast%2520in%2520the%2520number%2520of%250Aiterations.%2520This%2520is%2520non-standard%2520and%2520surprising%2520as%2520typically%2520noisy%2520gradient%250Adescent%2520with%2520fixed%2520noise%2520level%2520only%2520converges%2520to%2520a%2520stationary%2520regime%2520where%2520the%250Anoise%2520causes%2520the%2520dynamic%2520to%2520fluctuate%2520around%2520a%2520minimiser.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spike-timing-dependent%20Hebbian%20learning%20as%20noisy%20gradient%20descent&entry.906535625=Niklas%20Dexheimer%20and%20Sascha%20Gaudlitz%20and%20Johannes%20Schmidt-Hieber&entry.1292438233=%20%20Hebbian%20learning%20is%20a%20key%20principle%20underlying%20learning%20in%20biological%20neural%0Anetworks.%20We%20relate%20a%20Hebbian%20spike-timing-dependent%20plasticity%20rule%20to%20noisy%0Agradient%20descent%20with%20respect%20to%20a%20non-convex%20loss%20function%20on%20the%20probability%0Asimplex.%20Despite%20the%20constant%20injection%20of%20noise%20and%20the%20non-convexity%20of%20the%0Aunderlying%20optimization%20problem%2C%20one%20can%20rigorously%20prove%20that%20the%20considered%0AHebbian%20learning%20dynamic%20identifies%20the%20presynaptic%20neuron%20with%20the%20highest%0Aactivity%20and%20that%20the%20convergence%20is%20exponentially%20fast%20in%20the%20number%20of%0Aiterations.%20This%20is%20non-standard%20and%20surprising%20as%20typically%20noisy%20gradient%0Adescent%20with%20fixed%20noise%20level%20only%20converges%20to%20a%20stationary%20regime%20where%20the%0Anoise%20causes%20the%20dynamic%20to%20fluctuate%20around%20a%20minimiser.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10272v2&entry.124074799=Read"},
{"title": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural\n  Networks Compression", "author": "Baptiste Bauvin and Lo\u00efc Baret and Ola Ahmad", "abstract": "  Neural network compression has gained increasing attention in recent years,\nparticularly in computer vision applications, where the need for model\nreduction is crucial for overcoming deployment constraints. Pruning is a widely\nused technique that prompts sparsity in model structures, e.g. weights,\nneurons, and layers, reducing size and inference costs. Structured pruning is\nespecially important as it allows for the removal of entire structures, which\nfurther accelerates inference time and reduces memory overhead. However, it can\nbe computationally expensive, requiring iterative retraining and optimization.\nTo overcome this problem, recent methods considered one-shot setting, which\napplies pruning directly at post-training. Unfortunately, they often lead to a\nconsiderable drop in performance. In this paper, we focus on this issue by\nproposing a novel one-shot pruning framework that relies on explainable deep\nlearning. First, we introduce a causal-aware pruning approach that leverages\ncause-effect relations between model predictions and structures in a\nprogressive pruning process. It allows us to efficiently reduce the size of the\nnetwork, ensuring that the removed structures do not deter the performance of\nthe model. Then, through experiments conducted on convolution neural network\nand vision transformer baselines, pre-trained on classification tasks, we\ndemonstrate that our method consistently achieves substantial reductions in\nmodel size, with minimal impact on performance, and without the need for\nfine-tuning. Overall, our approach outperforms its counterparts, offering the\nbest trade-off. Our code is available on GitHub.\n", "link": "http://arxiv.org/abs/2510.18636v1", "date": "2025-10-21", "relevancy": 1.9583, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4945}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4897}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-SWAP%3A%20Explainability-Aware%20Structured%20Pruning%20for%20Efficient%20Neural%0A%20%20Networks%20Compression&body=Title%3A%20C-SWAP%3A%20Explainability-Aware%20Structured%20Pruning%20for%20Efficient%20Neural%0A%20%20Networks%20Compression%0AAuthor%3A%20Baptiste%20Bauvin%20and%20Lo%C3%AFc%20Baret%20and%20Ola%20Ahmad%0AAbstract%3A%20%20%20Neural%20network%20compression%20has%20gained%20increasing%20attention%20in%20recent%20years%2C%0Aparticularly%20in%20computer%20vision%20applications%2C%20where%20the%20need%20for%20model%0Areduction%20is%20crucial%20for%20overcoming%20deployment%20constraints.%20Pruning%20is%20a%20widely%0Aused%20technique%20that%20prompts%20sparsity%20in%20model%20structures%2C%20e.g.%20weights%2C%0Aneurons%2C%20and%20layers%2C%20reducing%20size%20and%20inference%20costs.%20Structured%20pruning%20is%0Aespecially%20important%20as%20it%20allows%20for%20the%20removal%20of%20entire%20structures%2C%20which%0Afurther%20accelerates%20inference%20time%20and%20reduces%20memory%20overhead.%20However%2C%20it%20can%0Abe%20computationally%20expensive%2C%20requiring%20iterative%20retraining%20and%20optimization.%0ATo%20overcome%20this%20problem%2C%20recent%20methods%20considered%20one-shot%20setting%2C%20which%0Aapplies%20pruning%20directly%20at%20post-training.%20Unfortunately%2C%20they%20often%20lead%20to%20a%0Aconsiderable%20drop%20in%20performance.%20In%20this%20paper%2C%20we%20focus%20on%20this%20issue%20by%0Aproposing%20a%20novel%20one-shot%20pruning%20framework%20that%20relies%20on%20explainable%20deep%0Alearning.%20First%2C%20we%20introduce%20a%20causal-aware%20pruning%20approach%20that%20leverages%0Acause-effect%20relations%20between%20model%20predictions%20and%20structures%20in%20a%0Aprogressive%20pruning%20process.%20It%20allows%20us%20to%20efficiently%20reduce%20the%20size%20of%20the%0Anetwork%2C%20ensuring%20that%20the%20removed%20structures%20do%20not%20deter%20the%20performance%20of%0Athe%20model.%20Then%2C%20through%20experiments%20conducted%20on%20convolution%20neural%20network%0Aand%20vision%20transformer%20baselines%2C%20pre-trained%20on%20classification%20tasks%2C%20we%0Ademonstrate%20that%20our%20method%20consistently%20achieves%20substantial%20reductions%20in%0Amodel%20size%2C%20with%20minimal%20impact%20on%20performance%2C%20and%20without%20the%20need%20for%0Afine-tuning.%20Overall%2C%20our%20approach%20outperforms%20its%20counterparts%2C%20offering%20the%0Abest%20trade-off.%20Our%20code%20is%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.18636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-SWAP%253A%2520Explainability-Aware%2520Structured%2520Pruning%2520for%2520Efficient%2520Neural%250A%2520%2520Networks%2520Compression%26entry.906535625%3DBaptiste%2520Bauvin%2520and%2520Lo%25C3%25AFc%2520Baret%2520and%2520Ola%2520Ahmad%26entry.1292438233%3D%2520%2520Neural%2520network%2520compression%2520has%2520gained%2520increasing%2520attention%2520in%2520recent%2520years%252C%250Aparticularly%2520in%2520computer%2520vision%2520applications%252C%2520where%2520the%2520need%2520for%2520model%250Areduction%2520is%2520crucial%2520for%2520overcoming%2520deployment%2520constraints.%2520Pruning%2520is%2520a%2520widely%250Aused%2520technique%2520that%2520prompts%2520sparsity%2520in%2520model%2520structures%252C%2520e.g.%2520weights%252C%250Aneurons%252C%2520and%2520layers%252C%2520reducing%2520size%2520and%2520inference%2520costs.%2520Structured%2520pruning%2520is%250Aespecially%2520important%2520as%2520it%2520allows%2520for%2520the%2520removal%2520of%2520entire%2520structures%252C%2520which%250Afurther%2520accelerates%2520inference%2520time%2520and%2520reduces%2520memory%2520overhead.%2520However%252C%2520it%2520can%250Abe%2520computationally%2520expensive%252C%2520requiring%2520iterative%2520retraining%2520and%2520optimization.%250ATo%2520overcome%2520this%2520problem%252C%2520recent%2520methods%2520considered%2520one-shot%2520setting%252C%2520which%250Aapplies%2520pruning%2520directly%2520at%2520post-training.%2520Unfortunately%252C%2520they%2520often%2520lead%2520to%2520a%250Aconsiderable%2520drop%2520in%2520performance.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520this%2520issue%2520by%250Aproposing%2520a%2520novel%2520one-shot%2520pruning%2520framework%2520that%2520relies%2520on%2520explainable%2520deep%250Alearning.%2520First%252C%2520we%2520introduce%2520a%2520causal-aware%2520pruning%2520approach%2520that%2520leverages%250Acause-effect%2520relations%2520between%2520model%2520predictions%2520and%2520structures%2520in%2520a%250Aprogressive%2520pruning%2520process.%2520It%2520allows%2520us%2520to%2520efficiently%2520reduce%2520the%2520size%2520of%2520the%250Anetwork%252C%2520ensuring%2520that%2520the%2520removed%2520structures%2520do%2520not%2520deter%2520the%2520performance%2520of%250Athe%2520model.%2520Then%252C%2520through%2520experiments%2520conducted%2520on%2520convolution%2520neural%2520network%250Aand%2520vision%2520transformer%2520baselines%252C%2520pre-trained%2520on%2520classification%2520tasks%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520consistently%2520achieves%2520substantial%2520reductions%2520in%250Amodel%2520size%252C%2520with%2520minimal%2520impact%2520on%2520performance%252C%2520and%2520without%2520the%2520need%2520for%250Afine-tuning.%2520Overall%252C%2520our%2520approach%2520outperforms%2520its%2520counterparts%252C%2520offering%2520the%250Abest%2520trade-off.%2520Our%2520code%2520is%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-SWAP%3A%20Explainability-Aware%20Structured%20Pruning%20for%20Efficient%20Neural%0A%20%20Networks%20Compression&entry.906535625=Baptiste%20Bauvin%20and%20Lo%C3%AFc%20Baret%20and%20Ola%20Ahmad&entry.1292438233=%20%20Neural%20network%20compression%20has%20gained%20increasing%20attention%20in%20recent%20years%2C%0Aparticularly%20in%20computer%20vision%20applications%2C%20where%20the%20need%20for%20model%0Areduction%20is%20crucial%20for%20overcoming%20deployment%20constraints.%20Pruning%20is%20a%20widely%0Aused%20technique%20that%20prompts%20sparsity%20in%20model%20structures%2C%20e.g.%20weights%2C%0Aneurons%2C%20and%20layers%2C%20reducing%20size%20and%20inference%20costs.%20Structured%20pruning%20is%0Aespecially%20important%20as%20it%20allows%20for%20the%20removal%20of%20entire%20structures%2C%20which%0Afurther%20accelerates%20inference%20time%20and%20reduces%20memory%20overhead.%20However%2C%20it%20can%0Abe%20computationally%20expensive%2C%20requiring%20iterative%20retraining%20and%20optimization.%0ATo%20overcome%20this%20problem%2C%20recent%20methods%20considered%20one-shot%20setting%2C%20which%0Aapplies%20pruning%20directly%20at%20post-training.%20Unfortunately%2C%20they%20often%20lead%20to%20a%0Aconsiderable%20drop%20in%20performance.%20In%20this%20paper%2C%20we%20focus%20on%20this%20issue%20by%0Aproposing%20a%20novel%20one-shot%20pruning%20framework%20that%20relies%20on%20explainable%20deep%0Alearning.%20First%2C%20we%20introduce%20a%20causal-aware%20pruning%20approach%20that%20leverages%0Acause-effect%20relations%20between%20model%20predictions%20and%20structures%20in%20a%0Aprogressive%20pruning%20process.%20It%20allows%20us%20to%20efficiently%20reduce%20the%20size%20of%20the%0Anetwork%2C%20ensuring%20that%20the%20removed%20structures%20do%20not%20deter%20the%20performance%20of%0Athe%20model.%20Then%2C%20through%20experiments%20conducted%20on%20convolution%20neural%20network%0Aand%20vision%20transformer%20baselines%2C%20pre-trained%20on%20classification%20tasks%2C%20we%0Ademonstrate%20that%20our%20method%20consistently%20achieves%20substantial%20reductions%20in%0Amodel%20size%2C%20with%20minimal%20impact%20on%20performance%2C%20and%20without%20the%20need%20for%0Afine-tuning.%20Overall%2C%20our%20approach%20outperforms%20its%20counterparts%2C%20offering%20the%0Abest%20trade-off.%20Our%20code%20is%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.18636v1&entry.124074799=Read"},
{"title": "PowerChain: A Verifiable Agentic AI System for Automating Distribution\n  Grid Analyses", "author": "Emmanuel O. Badmus and Peng Sang and Dimitrios Stamoulis and Amritanshu Pandey", "abstract": "  Rapid electrification and decarbonization are increasing the complexity of\ndistribution grid (DG) operation and planning, necessitating advanced\ncomputational analyses to ensure reliability and resilience. These analyses\ndepend on disparate workflows comprising complex models, function calls, and\ndata pipelines that require substantial expert knowledge and remain difficult\nto automate. Workforce and budget constraints further limit utilities' ability\nto apply such analyses at scale. To address this gap, we build an agentic\nsystem PowerChain, which is capable of autonomously performing complex grid\nanalyses. Existing agentic AI systems are typically developed in a bottom-up\nmanner with customized context for predefined analysis tasks; therefore, they\ndo not generalize to tasks that the agent has never seen. In comparison, to\ngeneralize to unseen DG analysis tasks, PowerChain dynamically generates\nstructured context by leveraging supervisory signals from self-contained power\nsystems tools (e.g., GridLAB-D) and an optimized set of expert-annotated and\nverified reasoning trajectories. For complex DG tasks defined in natural\nlanguage, empirical results on real utility data demonstrate that PowerChain\nachieves up to a 144/% improvement in performance over baselines.\n", "link": "http://arxiv.org/abs/2508.17094v3", "date": "2025-10-21", "relevancy": 1.8912, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5052}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4852}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PowerChain%3A%20A%20Verifiable%20Agentic%20AI%20System%20for%20Automating%20Distribution%0A%20%20Grid%20Analyses&body=Title%3A%20PowerChain%3A%20A%20Verifiable%20Agentic%20AI%20System%20for%20Automating%20Distribution%0A%20%20Grid%20Analyses%0AAuthor%3A%20Emmanuel%20O.%20Badmus%20and%20Peng%20Sang%20and%20Dimitrios%20Stamoulis%20and%20Amritanshu%20Pandey%0AAbstract%3A%20%20%20Rapid%20electrification%20and%20decarbonization%20are%20increasing%20the%20complexity%20of%0Adistribution%20grid%20%28DG%29%20operation%20and%20planning%2C%20necessitating%20advanced%0Acomputational%20analyses%20to%20ensure%20reliability%20and%20resilience.%20These%20analyses%0Adepend%20on%20disparate%20workflows%20comprising%20complex%20models%2C%20function%20calls%2C%20and%0Adata%20pipelines%20that%20require%20substantial%20expert%20knowledge%20and%20remain%20difficult%0Ato%20automate.%20Workforce%20and%20budget%20constraints%20further%20limit%20utilities%27%20ability%0Ato%20apply%20such%20analyses%20at%20scale.%20To%20address%20this%20gap%2C%20we%20build%20an%20agentic%0Asystem%20PowerChain%2C%20which%20is%20capable%20of%20autonomously%20performing%20complex%20grid%0Aanalyses.%20Existing%20agentic%20AI%20systems%20are%20typically%20developed%20in%20a%20bottom-up%0Amanner%20with%20customized%20context%20for%20predefined%20analysis%20tasks%3B%20therefore%2C%20they%0Ado%20not%20generalize%20to%20tasks%20that%20the%20agent%20has%20never%20seen.%20In%20comparison%2C%20to%0Ageneralize%20to%20unseen%20DG%20analysis%20tasks%2C%20PowerChain%20dynamically%20generates%0Astructured%20context%20by%20leveraging%20supervisory%20signals%20from%20self-contained%20power%0Asystems%20tools%20%28e.g.%2C%20GridLAB-D%29%20and%20an%20optimized%20set%20of%20expert-annotated%20and%0Averified%20reasoning%20trajectories.%20For%20complex%20DG%20tasks%20defined%20in%20natural%0Alanguage%2C%20empirical%20results%20on%20real%20utility%20data%20demonstrate%20that%20PowerChain%0Aachieves%20up%20to%20a%20144/%25%20improvement%20in%20performance%20over%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17094v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPowerChain%253A%2520A%2520Verifiable%2520Agentic%2520AI%2520System%2520for%2520Automating%2520Distribution%250A%2520%2520Grid%2520Analyses%26entry.906535625%3DEmmanuel%2520O.%2520Badmus%2520and%2520Peng%2520Sang%2520and%2520Dimitrios%2520Stamoulis%2520and%2520Amritanshu%2520Pandey%26entry.1292438233%3D%2520%2520Rapid%2520electrification%2520and%2520decarbonization%2520are%2520increasing%2520the%2520complexity%2520of%250Adistribution%2520grid%2520%2528DG%2529%2520operation%2520and%2520planning%252C%2520necessitating%2520advanced%250Acomputational%2520analyses%2520to%2520ensure%2520reliability%2520and%2520resilience.%2520These%2520analyses%250Adepend%2520on%2520disparate%2520workflows%2520comprising%2520complex%2520models%252C%2520function%2520calls%252C%2520and%250Adata%2520pipelines%2520that%2520require%2520substantial%2520expert%2520knowledge%2520and%2520remain%2520difficult%250Ato%2520automate.%2520Workforce%2520and%2520budget%2520constraints%2520further%2520limit%2520utilities%2527%2520ability%250Ato%2520apply%2520such%2520analyses%2520at%2520scale.%2520To%2520address%2520this%2520gap%252C%2520we%2520build%2520an%2520agentic%250Asystem%2520PowerChain%252C%2520which%2520is%2520capable%2520of%2520autonomously%2520performing%2520complex%2520grid%250Aanalyses.%2520Existing%2520agentic%2520AI%2520systems%2520are%2520typically%2520developed%2520in%2520a%2520bottom-up%250Amanner%2520with%2520customized%2520context%2520for%2520predefined%2520analysis%2520tasks%253B%2520therefore%252C%2520they%250Ado%2520not%2520generalize%2520to%2520tasks%2520that%2520the%2520agent%2520has%2520never%2520seen.%2520In%2520comparison%252C%2520to%250Ageneralize%2520to%2520unseen%2520DG%2520analysis%2520tasks%252C%2520PowerChain%2520dynamically%2520generates%250Astructured%2520context%2520by%2520leveraging%2520supervisory%2520signals%2520from%2520self-contained%2520power%250Asystems%2520tools%2520%2528e.g.%252C%2520GridLAB-D%2529%2520and%2520an%2520optimized%2520set%2520of%2520expert-annotated%2520and%250Averified%2520reasoning%2520trajectories.%2520For%2520complex%2520DG%2520tasks%2520defined%2520in%2520natural%250Alanguage%252C%2520empirical%2520results%2520on%2520real%2520utility%2520data%2520demonstrate%2520that%2520PowerChain%250Aachieves%2520up%2520to%2520a%2520144/%2525%2520improvement%2520in%2520performance%2520over%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17094v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PowerChain%3A%20A%20Verifiable%20Agentic%20AI%20System%20for%20Automating%20Distribution%0A%20%20Grid%20Analyses&entry.906535625=Emmanuel%20O.%20Badmus%20and%20Peng%20Sang%20and%20Dimitrios%20Stamoulis%20and%20Amritanshu%20Pandey&entry.1292438233=%20%20Rapid%20electrification%20and%20decarbonization%20are%20increasing%20the%20complexity%20of%0Adistribution%20grid%20%28DG%29%20operation%20and%20planning%2C%20necessitating%20advanced%0Acomputational%20analyses%20to%20ensure%20reliability%20and%20resilience.%20These%20analyses%0Adepend%20on%20disparate%20workflows%20comprising%20complex%20models%2C%20function%20calls%2C%20and%0Adata%20pipelines%20that%20require%20substantial%20expert%20knowledge%20and%20remain%20difficult%0Ato%20automate.%20Workforce%20and%20budget%20constraints%20further%20limit%20utilities%27%20ability%0Ato%20apply%20such%20analyses%20at%20scale.%20To%20address%20this%20gap%2C%20we%20build%20an%20agentic%0Asystem%20PowerChain%2C%20which%20is%20capable%20of%20autonomously%20performing%20complex%20grid%0Aanalyses.%20Existing%20agentic%20AI%20systems%20are%20typically%20developed%20in%20a%20bottom-up%0Amanner%20with%20customized%20context%20for%20predefined%20analysis%20tasks%3B%20therefore%2C%20they%0Ado%20not%20generalize%20to%20tasks%20that%20the%20agent%20has%20never%20seen.%20In%20comparison%2C%20to%0Ageneralize%20to%20unseen%20DG%20analysis%20tasks%2C%20PowerChain%20dynamically%20generates%0Astructured%20context%20by%20leveraging%20supervisory%20signals%20from%20self-contained%20power%0Asystems%20tools%20%28e.g.%2C%20GridLAB-D%29%20and%20an%20optimized%20set%20of%20expert-annotated%20and%0Averified%20reasoning%20trajectories.%20For%20complex%20DG%20tasks%20defined%20in%20natural%0Alanguage%2C%20empirical%20results%20on%20real%20utility%20data%20demonstrate%20that%20PowerChain%0Aachieves%20up%20to%20a%20144/%25%20improvement%20in%20performance%20over%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17094v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


