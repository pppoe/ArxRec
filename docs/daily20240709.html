<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240708.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from\n  Multi-View Stereo", "author": "Tianqi Liu and Guangcong Wang and Shoukang Hu and Liao Shen and Xinyi Ye and Yuhang Zang and Zhiguo Cao and Wei Li and Ziwei Liu", "abstract": "  We present MVSGaussian, a new generalizable 3D Gaussian representation\napproach derived from Multi-View Stereo (MVS) that can efficiently reconstruct\nunseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware\nGaussian representations and decode them into Gaussian parameters. 2) To\nfurther enhance performance, we propose a hybrid Gaussian rendering that\nintegrates an efficient volume rendering design for novel view synthesis. 3) To\nsupport fast fine-tuning for specific scenes, we introduce a multi-view\ngeometric consistent aggregation strategy to effectively aggregate the point\nclouds generated by the generalizable model, serving as the initialization for\nper-scene optimization. Compared with previous generalizable NeRF-based\nmethods, which typically require minutes of fine-tuning and seconds of\nrendering per image, MVSGaussian achieves real-time rendering with better\nsynthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian\nachieves better view synthesis with less training computational cost. Extensive\nexperiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples\ndatasets validate that MVSGaussian attains state-of-the-art performance with\nconvincing generalizability, real-time rendering speed, and fast per-scene\noptimization.\n", "link": "http://arxiv.org/abs/2405.12218v2", "date": "2024-07-08", "relevancy": 3.3985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7315}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.699}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVSGaussian%3A%20Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%0A%20%20Multi-View%20Stereo&body=Title%3A%20MVSGaussian%3A%20Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%0A%20%20Multi-View%20Stereo%0AAuthor%3A%20Tianqi%20Liu%20and%20Guangcong%20Wang%20and%20Shoukang%20Hu%20and%20Liao%20Shen%20and%20Xinyi%20Ye%20and%20Yuhang%20Zang%20and%20Zhiguo%20Cao%20and%20Wei%20Li%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20We%20present%20MVSGaussian%2C%20a%20new%20generalizable%203D%20Gaussian%20representation%0Aapproach%20derived%20from%20Multi-View%20Stereo%20%28MVS%29%20that%20can%20efficiently%20reconstruct%0Aunseen%20scenes.%20Specifically%2C%201%29%20we%20leverage%20MVS%20to%20encode%20geometry-aware%0AGaussian%20representations%20and%20decode%20them%20into%20Gaussian%20parameters.%202%29%20To%0Afurther%20enhance%20performance%2C%20we%20propose%20a%20hybrid%20Gaussian%20rendering%20that%0Aintegrates%20an%20efficient%20volume%20rendering%20design%20for%20novel%20view%20synthesis.%203%29%20To%0Asupport%20fast%20fine-tuning%20for%20specific%20scenes%2C%20we%20introduce%20a%20multi-view%0Ageometric%20consistent%20aggregation%20strategy%20to%20effectively%20aggregate%20the%20point%0Aclouds%20generated%20by%20the%20generalizable%20model%2C%20serving%20as%20the%20initialization%20for%0Aper-scene%20optimization.%20Compared%20with%20previous%20generalizable%20NeRF-based%0Amethods%2C%20which%20typically%20require%20minutes%20of%20fine-tuning%20and%20seconds%20of%0Arendering%20per%20image%2C%20MVSGaussian%20achieves%20real-time%20rendering%20with%20better%0Asynthesis%20quality%20for%20each%20scene.%20Compared%20with%20the%20vanilla%203D-GS%2C%20MVSGaussian%0Aachieves%20better%20view%20synthesis%20with%20less%20training%20computational%20cost.%20Extensive%0Aexperiments%20on%20DTU%2C%20Real%20Forward-facing%2C%20NeRF%20Synthetic%2C%20and%20Tanks%20and%20Temples%0Adatasets%20validate%20that%20MVSGaussian%20attains%20state-of-the-art%20performance%20with%0Aconvincing%20generalizability%2C%20real-time%20rendering%20speed%2C%20and%20fast%20per-scene%0Aoptimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12218v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVSGaussian%253A%2520Fast%2520Generalizable%2520Gaussian%2520Splatting%2520Reconstruction%2520from%250A%2520%2520Multi-View%2520Stereo%26entry.906535625%3DTianqi%2520Liu%2520and%2520Guangcong%2520Wang%2520and%2520Shoukang%2520Hu%2520and%2520Liao%2520Shen%2520and%2520Xinyi%2520Ye%2520and%2520Yuhang%2520Zang%2520and%2520Zhiguo%2520Cao%2520and%2520Wei%2520Li%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520MVSGaussian%252C%2520a%2520new%2520generalizable%25203D%2520Gaussian%2520representation%250Aapproach%2520derived%2520from%2520Multi-View%2520Stereo%2520%2528MVS%2529%2520that%2520can%2520efficiently%2520reconstruct%250Aunseen%2520scenes.%2520Specifically%252C%25201%2529%2520we%2520leverage%2520MVS%2520to%2520encode%2520geometry-aware%250AGaussian%2520representations%2520and%2520decode%2520them%2520into%2520Gaussian%2520parameters.%25202%2529%2520To%250Afurther%2520enhance%2520performance%252C%2520we%2520propose%2520a%2520hybrid%2520Gaussian%2520rendering%2520that%250Aintegrates%2520an%2520efficient%2520volume%2520rendering%2520design%2520for%2520novel%2520view%2520synthesis.%25203%2529%2520To%250Asupport%2520fast%2520fine-tuning%2520for%2520specific%2520scenes%252C%2520we%2520introduce%2520a%2520multi-view%250Ageometric%2520consistent%2520aggregation%2520strategy%2520to%2520effectively%2520aggregate%2520the%2520point%250Aclouds%2520generated%2520by%2520the%2520generalizable%2520model%252C%2520serving%2520as%2520the%2520initialization%2520for%250Aper-scene%2520optimization.%2520Compared%2520with%2520previous%2520generalizable%2520NeRF-based%250Amethods%252C%2520which%2520typically%2520require%2520minutes%2520of%2520fine-tuning%2520and%2520seconds%2520of%250Arendering%2520per%2520image%252C%2520MVSGaussian%2520achieves%2520real-time%2520rendering%2520with%2520better%250Asynthesis%2520quality%2520for%2520each%2520scene.%2520Compared%2520with%2520the%2520vanilla%25203D-GS%252C%2520MVSGaussian%250Aachieves%2520better%2520view%2520synthesis%2520with%2520less%2520training%2520computational%2520cost.%2520Extensive%250Aexperiments%2520on%2520DTU%252C%2520Real%2520Forward-facing%252C%2520NeRF%2520Synthetic%252C%2520and%2520Tanks%2520and%2520Temples%250Adatasets%2520validate%2520that%2520MVSGaussian%2520attains%2520state-of-the-art%2520performance%2520with%250Aconvincing%2520generalizability%252C%2520real-time%2520rendering%2520speed%252C%2520and%2520fast%2520per-scene%250Aoptimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12218v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVSGaussian%3A%20Fast%20Generalizable%20Gaussian%20Splatting%20Reconstruction%20from%0A%20%20Multi-View%20Stereo&entry.906535625=Tianqi%20Liu%20and%20Guangcong%20Wang%20and%20Shoukang%20Hu%20and%20Liao%20Shen%20and%20Xinyi%20Ye%20and%20Yuhang%20Zang%20and%20Zhiguo%20Cao%20and%20Wei%20Li%20and%20Ziwei%20Liu&entry.1292438233=%20%20We%20present%20MVSGaussian%2C%20a%20new%20generalizable%203D%20Gaussian%20representation%0Aapproach%20derived%20from%20Multi-View%20Stereo%20%28MVS%29%20that%20can%20efficiently%20reconstruct%0Aunseen%20scenes.%20Specifically%2C%201%29%20we%20leverage%20MVS%20to%20encode%20geometry-aware%0AGaussian%20representations%20and%20decode%20them%20into%20Gaussian%20parameters.%202%29%20To%0Afurther%20enhance%20performance%2C%20we%20propose%20a%20hybrid%20Gaussian%20rendering%20that%0Aintegrates%20an%20efficient%20volume%20rendering%20design%20for%20novel%20view%20synthesis.%203%29%20To%0Asupport%20fast%20fine-tuning%20for%20specific%20scenes%2C%20we%20introduce%20a%20multi-view%0Ageometric%20consistent%20aggregation%20strategy%20to%20effectively%20aggregate%20the%20point%0Aclouds%20generated%20by%20the%20generalizable%20model%2C%20serving%20as%20the%20initialization%20for%0Aper-scene%20optimization.%20Compared%20with%20previous%20generalizable%20NeRF-based%0Amethods%2C%20which%20typically%20require%20minutes%20of%20fine-tuning%20and%20seconds%20of%0Arendering%20per%20image%2C%20MVSGaussian%20achieves%20real-time%20rendering%20with%20better%0Asynthesis%20quality%20for%20each%20scene.%20Compared%20with%20the%20vanilla%203D-GS%2C%20MVSGaussian%0Aachieves%20better%20view%20synthesis%20with%20less%20training%20computational%20cost.%20Extensive%0Aexperiments%20on%20DTU%2C%20Real%20Forward-facing%2C%20NeRF%20Synthetic%2C%20and%20Tanks%20and%20Temples%0Adatasets%20validate%20that%20MVSGaussian%20attains%20state-of-the-art%20performance%20with%0Aconvincing%20generalizability%2C%20real-time%20rendering%20speed%2C%20and%20fast%20per-scene%0Aoptimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12218v2&entry.124074799=Read"},
{"title": "Gaussian Grouping: Segment and Edit Anything in 3D Scenes", "author": "Mingqiao Ye and Martin Danelljan and Fisher Yu and Lei Ke", "abstract": "  The recent Gaussian Splatting achieves high-quality and real-time novel-view\nsynthesis of the 3D scenes. However, it is solely concentrated on the\nappearance and geometry modeling, while lacking in fine-grained object-level\nscene understanding. To address this issue, we propose Gaussian Grouping, which\nextends Gaussian Splatting to jointly reconstruct and segment anything in\nopen-world 3D scenes. We augment each Gaussian with a compact Identity\nEncoding, allowing the Gaussians to be grouped according to their object\ninstance or stuff membership in the 3D scene. Instead of resorting to expensive\n3D labels, we supervise the Identity Encodings during the differentiable\nrendering by leveraging the 2D mask predictions by Segment Anything Model\n(SAM), along with introduced 3D spatial consistency regularization. Compared to\nthe implicit NeRF representation, we show that the discrete and grouped 3D\nGaussians can reconstruct, segment and edit anything in 3D with high visual\nquality, fine granularity and efficiency. Based on Gaussian Grouping, we\nfurther propose a local Gaussian Editing scheme, which shows efficacy in\nversatile scene editing applications, including 3D object removal, inpainting,\ncolorization, style transfer and scene recomposition. Our code and models are\nat https://github.com/lkeab/gaussian-grouping.\n", "link": "http://arxiv.org/abs/2312.00732v2", "date": "2024-07-08", "relevancy": 3.3168, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7066}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6657}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Grouping%3A%20Segment%20and%20Edit%20Anything%20in%203D%20Scenes&body=Title%3A%20Gaussian%20Grouping%3A%20Segment%20and%20Edit%20Anything%20in%203D%20Scenes%0AAuthor%3A%20Mingqiao%20Ye%20and%20Martin%20Danelljan%20and%20Fisher%20Yu%20and%20Lei%20Ke%0AAbstract%3A%20%20%20The%20recent%20Gaussian%20Splatting%20achieves%20high-quality%20and%20real-time%20novel-view%0Asynthesis%20of%20the%203D%20scenes.%20However%2C%20it%20is%20solely%20concentrated%20on%20the%0Aappearance%20and%20geometry%20modeling%2C%20while%20lacking%20in%20fine-grained%20object-level%0Ascene%20understanding.%20To%20address%20this%20issue%2C%20we%20propose%20Gaussian%20Grouping%2C%20which%0Aextends%20Gaussian%20Splatting%20to%20jointly%20reconstruct%20and%20segment%20anything%20in%0Aopen-world%203D%20scenes.%20We%20augment%20each%20Gaussian%20with%20a%20compact%20Identity%0AEncoding%2C%20allowing%20the%20Gaussians%20to%20be%20grouped%20according%20to%20their%20object%0Ainstance%20or%20stuff%20membership%20in%20the%203D%20scene.%20Instead%20of%20resorting%20to%20expensive%0A3D%20labels%2C%20we%20supervise%20the%20Identity%20Encodings%20during%20the%20differentiable%0Arendering%20by%20leveraging%20the%202D%20mask%20predictions%20by%20Segment%20Anything%20Model%0A%28SAM%29%2C%20along%20with%20introduced%203D%20spatial%20consistency%20regularization.%20Compared%20to%0Athe%20implicit%20NeRF%20representation%2C%20we%20show%20that%20the%20discrete%20and%20grouped%203D%0AGaussians%20can%20reconstruct%2C%20segment%20and%20edit%20anything%20in%203D%20with%20high%20visual%0Aquality%2C%20fine%20granularity%20and%20efficiency.%20Based%20on%20Gaussian%20Grouping%2C%20we%0Afurther%20propose%20a%20local%20Gaussian%20Editing%20scheme%2C%20which%20shows%20efficacy%20in%0Aversatile%20scene%20editing%20applications%2C%20including%203D%20object%20removal%2C%20inpainting%2C%0Acolorization%2C%20style%20transfer%20and%20scene%20recomposition.%20Our%20code%20and%20models%20are%0Aat%20https%3A//github.com/lkeab/gaussian-grouping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Grouping%253A%2520Segment%2520and%2520Edit%2520Anything%2520in%25203D%2520Scenes%26entry.906535625%3DMingqiao%2520Ye%2520and%2520Martin%2520Danelljan%2520and%2520Fisher%2520Yu%2520and%2520Lei%2520Ke%26entry.1292438233%3D%2520%2520The%2520recent%2520Gaussian%2520Splatting%2520achieves%2520high-quality%2520and%2520real-time%2520novel-view%250Asynthesis%2520of%2520the%25203D%2520scenes.%2520However%252C%2520it%2520is%2520solely%2520concentrated%2520on%2520the%250Aappearance%2520and%2520geometry%2520modeling%252C%2520while%2520lacking%2520in%2520fine-grained%2520object-level%250Ascene%2520understanding.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Gaussian%2520Grouping%252C%2520which%250Aextends%2520Gaussian%2520Splatting%2520to%2520jointly%2520reconstruct%2520and%2520segment%2520anything%2520in%250Aopen-world%25203D%2520scenes.%2520We%2520augment%2520each%2520Gaussian%2520with%2520a%2520compact%2520Identity%250AEncoding%252C%2520allowing%2520the%2520Gaussians%2520to%2520be%2520grouped%2520according%2520to%2520their%2520object%250Ainstance%2520or%2520stuff%2520membership%2520in%2520the%25203D%2520scene.%2520Instead%2520of%2520resorting%2520to%2520expensive%250A3D%2520labels%252C%2520we%2520supervise%2520the%2520Identity%2520Encodings%2520during%2520the%2520differentiable%250Arendering%2520by%2520leveraging%2520the%25202D%2520mask%2520predictions%2520by%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%252C%2520along%2520with%2520introduced%25203D%2520spatial%2520consistency%2520regularization.%2520Compared%2520to%250Athe%2520implicit%2520NeRF%2520representation%252C%2520we%2520show%2520that%2520the%2520discrete%2520and%2520grouped%25203D%250AGaussians%2520can%2520reconstruct%252C%2520segment%2520and%2520edit%2520anything%2520in%25203D%2520with%2520high%2520visual%250Aquality%252C%2520fine%2520granularity%2520and%2520efficiency.%2520Based%2520on%2520Gaussian%2520Grouping%252C%2520we%250Afurther%2520propose%2520a%2520local%2520Gaussian%2520Editing%2520scheme%252C%2520which%2520shows%2520efficacy%2520in%250Aversatile%2520scene%2520editing%2520applications%252C%2520including%25203D%2520object%2520removal%252C%2520inpainting%252C%250Acolorization%252C%2520style%2520transfer%2520and%2520scene%2520recomposition.%2520Our%2520code%2520and%2520models%2520are%250Aat%2520https%253A//github.com/lkeab/gaussian-grouping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Grouping%3A%20Segment%20and%20Edit%20Anything%20in%203D%20Scenes&entry.906535625=Mingqiao%20Ye%20and%20Martin%20Danelljan%20and%20Fisher%20Yu%20and%20Lei%20Ke&entry.1292438233=%20%20The%20recent%20Gaussian%20Splatting%20achieves%20high-quality%20and%20real-time%20novel-view%0Asynthesis%20of%20the%203D%20scenes.%20However%2C%20it%20is%20solely%20concentrated%20on%20the%0Aappearance%20and%20geometry%20modeling%2C%20while%20lacking%20in%20fine-grained%20object-level%0Ascene%20understanding.%20To%20address%20this%20issue%2C%20we%20propose%20Gaussian%20Grouping%2C%20which%0Aextends%20Gaussian%20Splatting%20to%20jointly%20reconstruct%20and%20segment%20anything%20in%0Aopen-world%203D%20scenes.%20We%20augment%20each%20Gaussian%20with%20a%20compact%20Identity%0AEncoding%2C%20allowing%20the%20Gaussians%20to%20be%20grouped%20according%20to%20their%20object%0Ainstance%20or%20stuff%20membership%20in%20the%203D%20scene.%20Instead%20of%20resorting%20to%20expensive%0A3D%20labels%2C%20we%20supervise%20the%20Identity%20Encodings%20during%20the%20differentiable%0Arendering%20by%20leveraging%20the%202D%20mask%20predictions%20by%20Segment%20Anything%20Model%0A%28SAM%29%2C%20along%20with%20introduced%203D%20spatial%20consistency%20regularization.%20Compared%20to%0Athe%20implicit%20NeRF%20representation%2C%20we%20show%20that%20the%20discrete%20and%20grouped%203D%0AGaussians%20can%20reconstruct%2C%20segment%20and%20edit%20anything%20in%203D%20with%20high%20visual%0Aquality%2C%20fine%20granularity%20and%20efficiency.%20Based%20on%20Gaussian%20Grouping%2C%20we%0Afurther%20propose%20a%20local%20Gaussian%20Editing%20scheme%2C%20which%20shows%20efficacy%20in%0Aversatile%20scene%20editing%20applications%2C%20including%203D%20object%20removal%2C%20inpainting%2C%0Acolorization%2C%20style%20transfer%20and%20scene%20recomposition.%20Our%20code%20and%20models%20are%0Aat%20https%3A//github.com/lkeab/gaussian-grouping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00732v2&entry.124074799=Read"},
{"title": "Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side\n  Images", "author": "Zhangyang Qi and Yunhan Yang and Mengchen Zhang and Long Xing and Xiaoyang Wu and Tong Wu and Dahua Lin and Xihui Liu and Jiaqi Wang and Hengshuang Zhao", "abstract": "  Recent advances in 3D AIGC have shown promise in directly creating 3D objects\nfrom text and images, offering significant cost savings in animation and\nproduct design. However, detailed edit and customization of 3D assets remains a\nlong-standing challenge. Specifically, 3D Generation methods lack the ability\nto follow finely detailed instructions as precisely as their 2D image creation\ncounterparts. Imagine you can get a toy through 3D AIGC but with undesired\naccessories and dressing. To tackle this challenge, we propose a novel pipeline\ncalled Tailor3D, which swiftly creates customized 3D assets from editable\ndual-side images. We aim to emulate a tailor's ability to locally change\nobjects or perform overall style transfer. Unlike creating 3D assets from\nmultiple views, using dual-side images eliminates conflicts on overlapping\nareas that occur when editing individual views. Specifically, it begins by\nediting the front view, then generates the back view of the object through\nmulti-view diffusion. Afterward, it proceeds to edit the back views. Finally, a\nDual-sided LRM is proposed to seamlessly stitch together the front and back 3D\nfeatures, akin to a tailor sewing together the front and back of a garment. The\nDual-sided LRM rectifies imperfect consistencies between the front and back\nviews, enhancing editing capabilities and reducing memory burdens while\nseamlessly integrating them into a unified 3D representation with the LoRA\nTriplane Transformer. Experimental results demonstrate Tailor3D's effectiveness\nacross various 3D generation and editing tasks, including 3D generative fill\nand style transfer. It provides a user-friendly, efficient solution for editing\n3D assets, with each editing step taking only seconds to complete.\n", "link": "http://arxiv.org/abs/2407.06191v1", "date": "2024-07-08", "relevancy": 3.2403, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6565}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6565}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailor3D%3A%20Customized%203D%20Assets%20Editing%20and%20Generation%20with%20Dual-Side%0A%20%20Images&body=Title%3A%20Tailor3D%3A%20Customized%203D%20Assets%20Editing%20and%20Generation%20with%20Dual-Side%0A%20%20Images%0AAuthor%3A%20Zhangyang%20Qi%20and%20Yunhan%20Yang%20and%20Mengchen%20Zhang%20and%20Long%20Xing%20and%20Xiaoyang%20Wu%20and%20Tong%20Wu%20and%20Dahua%20Lin%20and%20Xihui%20Liu%20and%20Jiaqi%20Wang%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20AIGC%20have%20shown%20promise%20in%20directly%20creating%203D%20objects%0Afrom%20text%20and%20images%2C%20offering%20significant%20cost%20savings%20in%20animation%20and%0Aproduct%20design.%20However%2C%20detailed%20edit%20and%20customization%20of%203D%20assets%20remains%20a%0Along-standing%20challenge.%20Specifically%2C%203D%20Generation%20methods%20lack%20the%20ability%0Ato%20follow%20finely%20detailed%20instructions%20as%20precisely%20as%20their%202D%20image%20creation%0Acounterparts.%20Imagine%20you%20can%20get%20a%20toy%20through%203D%20AIGC%20but%20with%20undesired%0Aaccessories%20and%20dressing.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20novel%20pipeline%0Acalled%20Tailor3D%2C%20which%20swiftly%20creates%20customized%203D%20assets%20from%20editable%0Adual-side%20images.%20We%20aim%20to%20emulate%20a%20tailor%27s%20ability%20to%20locally%20change%0Aobjects%20or%20perform%20overall%20style%20transfer.%20Unlike%20creating%203D%20assets%20from%0Amultiple%20views%2C%20using%20dual-side%20images%20eliminates%20conflicts%20on%20overlapping%0Aareas%20that%20occur%20when%20editing%20individual%20views.%20Specifically%2C%20it%20begins%20by%0Aediting%20the%20front%20view%2C%20then%20generates%20the%20back%20view%20of%20the%20object%20through%0Amulti-view%20diffusion.%20Afterward%2C%20it%20proceeds%20to%20edit%20the%20back%20views.%20Finally%2C%20a%0ADual-sided%20LRM%20is%20proposed%20to%20seamlessly%20stitch%20together%20the%20front%20and%20back%203D%0Afeatures%2C%20akin%20to%20a%20tailor%20sewing%20together%20the%20front%20and%20back%20of%20a%20garment.%20The%0ADual-sided%20LRM%20rectifies%20imperfect%20consistencies%20between%20the%20front%20and%20back%0Aviews%2C%20enhancing%20editing%20capabilities%20and%20reducing%20memory%20burdens%20while%0Aseamlessly%20integrating%20them%20into%20a%20unified%203D%20representation%20with%20the%20LoRA%0ATriplane%20Transformer.%20Experimental%20results%20demonstrate%20Tailor3D%27s%20effectiveness%0Aacross%20various%203D%20generation%20and%20editing%20tasks%2C%20including%203D%20generative%20fill%0Aand%20style%20transfer.%20It%20provides%20a%20user-friendly%2C%20efficient%20solution%20for%20editing%0A3D%20assets%2C%20with%20each%20editing%20step%20taking%20only%20seconds%20to%20complete.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailor3D%253A%2520Customized%25203D%2520Assets%2520Editing%2520and%2520Generation%2520with%2520Dual-Side%250A%2520%2520Images%26entry.906535625%3DZhangyang%2520Qi%2520and%2520Yunhan%2520Yang%2520and%2520Mengchen%2520Zhang%2520and%2520Long%2520Xing%2520and%2520Xiaoyang%2520Wu%2520and%2520Tong%2520Wu%2520and%2520Dahua%2520Lin%2520and%2520Xihui%2520Liu%2520and%2520Jiaqi%2520Wang%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520AIGC%2520have%2520shown%2520promise%2520in%2520directly%2520creating%25203D%2520objects%250Afrom%2520text%2520and%2520images%252C%2520offering%2520significant%2520cost%2520savings%2520in%2520animation%2520and%250Aproduct%2520design.%2520However%252C%2520detailed%2520edit%2520and%2520customization%2520of%25203D%2520assets%2520remains%2520a%250Along-standing%2520challenge.%2520Specifically%252C%25203D%2520Generation%2520methods%2520lack%2520the%2520ability%250Ato%2520follow%2520finely%2520detailed%2520instructions%2520as%2520precisely%2520as%2520their%25202D%2520image%2520creation%250Acounterparts.%2520Imagine%2520you%2520can%2520get%2520a%2520toy%2520through%25203D%2520AIGC%2520but%2520with%2520undesired%250Aaccessories%2520and%2520dressing.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520pipeline%250Acalled%2520Tailor3D%252C%2520which%2520swiftly%2520creates%2520customized%25203D%2520assets%2520from%2520editable%250Adual-side%2520images.%2520We%2520aim%2520to%2520emulate%2520a%2520tailor%2527s%2520ability%2520to%2520locally%2520change%250Aobjects%2520or%2520perform%2520overall%2520style%2520transfer.%2520Unlike%2520creating%25203D%2520assets%2520from%250Amultiple%2520views%252C%2520using%2520dual-side%2520images%2520eliminates%2520conflicts%2520on%2520overlapping%250Aareas%2520that%2520occur%2520when%2520editing%2520individual%2520views.%2520Specifically%252C%2520it%2520begins%2520by%250Aediting%2520the%2520front%2520view%252C%2520then%2520generates%2520the%2520back%2520view%2520of%2520the%2520object%2520through%250Amulti-view%2520diffusion.%2520Afterward%252C%2520it%2520proceeds%2520to%2520edit%2520the%2520back%2520views.%2520Finally%252C%2520a%250ADual-sided%2520LRM%2520is%2520proposed%2520to%2520seamlessly%2520stitch%2520together%2520the%2520front%2520and%2520back%25203D%250Afeatures%252C%2520akin%2520to%2520a%2520tailor%2520sewing%2520together%2520the%2520front%2520and%2520back%2520of%2520a%2520garment.%2520The%250ADual-sided%2520LRM%2520rectifies%2520imperfect%2520consistencies%2520between%2520the%2520front%2520and%2520back%250Aviews%252C%2520enhancing%2520editing%2520capabilities%2520and%2520reducing%2520memory%2520burdens%2520while%250Aseamlessly%2520integrating%2520them%2520into%2520a%2520unified%25203D%2520representation%2520with%2520the%2520LoRA%250ATriplane%2520Transformer.%2520Experimental%2520results%2520demonstrate%2520Tailor3D%2527s%2520effectiveness%250Aacross%2520various%25203D%2520generation%2520and%2520editing%2520tasks%252C%2520including%25203D%2520generative%2520fill%250Aand%2520style%2520transfer.%2520It%2520provides%2520a%2520user-friendly%252C%2520efficient%2520solution%2520for%2520editing%250A3D%2520assets%252C%2520with%2520each%2520editing%2520step%2520taking%2520only%2520seconds%2520to%2520complete.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailor3D%3A%20Customized%203D%20Assets%20Editing%20and%20Generation%20with%20Dual-Side%0A%20%20Images&entry.906535625=Zhangyang%20Qi%20and%20Yunhan%20Yang%20and%20Mengchen%20Zhang%20and%20Long%20Xing%20and%20Xiaoyang%20Wu%20and%20Tong%20Wu%20and%20Dahua%20Lin%20and%20Xihui%20Liu%20and%20Jiaqi%20Wang%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Recent%20advances%20in%203D%20AIGC%20have%20shown%20promise%20in%20directly%20creating%203D%20objects%0Afrom%20text%20and%20images%2C%20offering%20significant%20cost%20savings%20in%20animation%20and%0Aproduct%20design.%20However%2C%20detailed%20edit%20and%20customization%20of%203D%20assets%20remains%20a%0Along-standing%20challenge.%20Specifically%2C%203D%20Generation%20methods%20lack%20the%20ability%0Ato%20follow%20finely%20detailed%20instructions%20as%20precisely%20as%20their%202D%20image%20creation%0Acounterparts.%20Imagine%20you%20can%20get%20a%20toy%20through%203D%20AIGC%20but%20with%20undesired%0Aaccessories%20and%20dressing.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20novel%20pipeline%0Acalled%20Tailor3D%2C%20which%20swiftly%20creates%20customized%203D%20assets%20from%20editable%0Adual-side%20images.%20We%20aim%20to%20emulate%20a%20tailor%27s%20ability%20to%20locally%20change%0Aobjects%20or%20perform%20overall%20style%20transfer.%20Unlike%20creating%203D%20assets%20from%0Amultiple%20views%2C%20using%20dual-side%20images%20eliminates%20conflicts%20on%20overlapping%0Aareas%20that%20occur%20when%20editing%20individual%20views.%20Specifically%2C%20it%20begins%20by%0Aediting%20the%20front%20view%2C%20then%20generates%20the%20back%20view%20of%20the%20object%20through%0Amulti-view%20diffusion.%20Afterward%2C%20it%20proceeds%20to%20edit%20the%20back%20views.%20Finally%2C%20a%0ADual-sided%20LRM%20is%20proposed%20to%20seamlessly%20stitch%20together%20the%20front%20and%20back%203D%0Afeatures%2C%20akin%20to%20a%20tailor%20sewing%20together%20the%20front%20and%20back%20of%20a%20garment.%20The%0ADual-sided%20LRM%20rectifies%20imperfect%20consistencies%20between%20the%20front%20and%20back%0Aviews%2C%20enhancing%20editing%20capabilities%20and%20reducing%20memory%20burdens%20while%0Aseamlessly%20integrating%20them%20into%20a%20unified%203D%20representation%20with%20the%20LoRA%0ATriplane%20Transformer.%20Experimental%20results%20demonstrate%20Tailor3D%27s%20effectiveness%0Aacross%20various%203D%20generation%20and%20editing%20tasks%2C%20including%203D%20generative%20fill%0Aand%20style%20transfer.%20It%20provides%20a%20user-friendly%2C%20efficient%20solution%20for%20editing%0A3D%20assets%2C%20with%20each%20editing%20step%20taking%20only%20seconds%20to%20complete.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06191v1&entry.124074799=Read"},
{"title": "Bringing Masked Autoencoders Explicit Contrastive Properties for Point\n  Cloud Self-Supervised Learning", "author": "Bin Ren and Guofeng Mei and Danda Pani Paudel and Weijie Wang and Yawei Li and Mengyuan Liu and Rita Cucchiara and Luc Van Gool and Nicu Sebe", "abstract": "  Contrastive learning (CL) for Vision Transformers (ViTs) in image domains has\nachieved performance comparable to CL for traditional convolutional backbones.\nHowever, in 3D point cloud pretraining with ViTs, masked autoencoder (MAE)\nmodeling remains dominant. This raises the question: Can we take the best of\nboth worlds? To answer this question, we first empirically validate that\nintegrating MAE-based point cloud pre-training with the standard contrastive\nlearning paradigm, even with meticulous design, can lead to a decrease in\nperformance. To address this limitation, we reintroduce CL into the MAE-based\npoint cloud pre-training paradigm by leveraging the inherent contrastive\nproperties of MAE. Specifically, rather than relying on extensive data\naugmentation as commonly used in the image domain, we randomly mask the input\ntokens twice to generate contrastive input pairs. Subsequently, a\nweight-sharing encoder and two identically structured decoders are utilized to\nperform masked token reconstruction. Additionally, we propose that for an input\ntoken masked by both masks simultaneously, the reconstructed features should be\nas similar as possible. This naturally establishes an explicit contrastive\nconstraint within the generative MAE-based pre-training paradigm, resulting in\nour proposed method, Point-CMAE. Consequently, Point-CMAE effectively enhances\nthe representation quality and transfer performance compared to its MAE\ncounterpart. Experimental evaluations across various downstream applications,\nincluding classification, part segmentation, and few-shot learning, demonstrate\nthe efficacy of our framework in surpassing state-of-the-art techniques under\nstandard ViTs and single-modal settings. The source code and trained models are\navailable at: https://github.com/Amazingren/Point-CMAE.\n", "link": "http://arxiv.org/abs/2407.05862v1", "date": "2024-07-08", "relevancy": 2.9679, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5725}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bringing%20Masked%20Autoencoders%20Explicit%20Contrastive%20Properties%20for%20Point%0A%20%20Cloud%20Self-Supervised%20Learning&body=Title%3A%20Bringing%20Masked%20Autoencoders%20Explicit%20Contrastive%20Properties%20for%20Point%0A%20%20Cloud%20Self-Supervised%20Learning%0AAuthor%3A%20Bin%20Ren%20and%20Guofeng%20Mei%20and%20Danda%20Pani%20Paudel%20and%20Weijie%20Wang%20and%20Yawei%20Li%20and%20Mengyuan%20Liu%20and%20Rita%20Cucchiara%20and%20Luc%20Van%20Gool%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Contrastive%20learning%20%28CL%29%20for%20Vision%20Transformers%20%28ViTs%29%20in%20image%20domains%20has%0Aachieved%20performance%20comparable%20to%20CL%20for%20traditional%20convolutional%20backbones.%0AHowever%2C%20in%203D%20point%20cloud%20pretraining%20with%20ViTs%2C%20masked%20autoencoder%20%28MAE%29%0Amodeling%20remains%20dominant.%20This%20raises%20the%20question%3A%20Can%20we%20take%20the%20best%20of%0Aboth%20worlds%3F%20To%20answer%20this%20question%2C%20we%20first%20empirically%20validate%20that%0Aintegrating%20MAE-based%20point%20cloud%20pre-training%20with%20the%20standard%20contrastive%0Alearning%20paradigm%2C%20even%20with%20meticulous%20design%2C%20can%20lead%20to%20a%20decrease%20in%0Aperformance.%20To%20address%20this%20limitation%2C%20we%20reintroduce%20CL%20into%20the%20MAE-based%0Apoint%20cloud%20pre-training%20paradigm%20by%20leveraging%20the%20inherent%20contrastive%0Aproperties%20of%20MAE.%20Specifically%2C%20rather%20than%20relying%20on%20extensive%20data%0Aaugmentation%20as%20commonly%20used%20in%20the%20image%20domain%2C%20we%20randomly%20mask%20the%20input%0Atokens%20twice%20to%20generate%20contrastive%20input%20pairs.%20Subsequently%2C%20a%0Aweight-sharing%20encoder%20and%20two%20identically%20structured%20decoders%20are%20utilized%20to%0Aperform%20masked%20token%20reconstruction.%20Additionally%2C%20we%20propose%20that%20for%20an%20input%0Atoken%20masked%20by%20both%20masks%20simultaneously%2C%20the%20reconstructed%20features%20should%20be%0Aas%20similar%20as%20possible.%20This%20naturally%20establishes%20an%20explicit%20contrastive%0Aconstraint%20within%20the%20generative%20MAE-based%20pre-training%20paradigm%2C%20resulting%20in%0Aour%20proposed%20method%2C%20Point-CMAE.%20Consequently%2C%20Point-CMAE%20effectively%20enhances%0Athe%20representation%20quality%20and%20transfer%20performance%20compared%20to%20its%20MAE%0Acounterpart.%20Experimental%20evaluations%20across%20various%20downstream%20applications%2C%0Aincluding%20classification%2C%20part%20segmentation%2C%20and%20few-shot%20learning%2C%20demonstrate%0Athe%20efficacy%20of%20our%20framework%20in%20surpassing%20state-of-the-art%20techniques%20under%0Astandard%20ViTs%20and%20single-modal%20settings.%20The%20source%20code%20and%20trained%20models%20are%0Aavailable%20at%3A%20https%3A//github.com/Amazingren/Point-CMAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBringing%2520Masked%2520Autoencoders%2520Explicit%2520Contrastive%2520Properties%2520for%2520Point%250A%2520%2520Cloud%2520Self-Supervised%2520Learning%26entry.906535625%3DBin%2520Ren%2520and%2520Guofeng%2520Mei%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Weijie%2520Wang%2520and%2520Yawei%2520Li%2520and%2520Mengyuan%2520Liu%2520and%2520Rita%2520Cucchiara%2520and%2520Luc%2520Van%2520Gool%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520%2528CL%2529%2520for%2520Vision%2520Transformers%2520%2528ViTs%2529%2520in%2520image%2520domains%2520has%250Aachieved%2520performance%2520comparable%2520to%2520CL%2520for%2520traditional%2520convolutional%2520backbones.%250AHowever%252C%2520in%25203D%2520point%2520cloud%2520pretraining%2520with%2520ViTs%252C%2520masked%2520autoencoder%2520%2528MAE%2529%250Amodeling%2520remains%2520dominant.%2520This%2520raises%2520the%2520question%253A%2520Can%2520we%2520take%2520the%2520best%2520of%250Aboth%2520worlds%253F%2520To%2520answer%2520this%2520question%252C%2520we%2520first%2520empirically%2520validate%2520that%250Aintegrating%2520MAE-based%2520point%2520cloud%2520pre-training%2520with%2520the%2520standard%2520contrastive%250Alearning%2520paradigm%252C%2520even%2520with%2520meticulous%2520design%252C%2520can%2520lead%2520to%2520a%2520decrease%2520in%250Aperformance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520reintroduce%2520CL%2520into%2520the%2520MAE-based%250Apoint%2520cloud%2520pre-training%2520paradigm%2520by%2520leveraging%2520the%2520inherent%2520contrastive%250Aproperties%2520of%2520MAE.%2520Specifically%252C%2520rather%2520than%2520relying%2520on%2520extensive%2520data%250Aaugmentation%2520as%2520commonly%2520used%2520in%2520the%2520image%2520domain%252C%2520we%2520randomly%2520mask%2520the%2520input%250Atokens%2520twice%2520to%2520generate%2520contrastive%2520input%2520pairs.%2520Subsequently%252C%2520a%250Aweight-sharing%2520encoder%2520and%2520two%2520identically%2520structured%2520decoders%2520are%2520utilized%2520to%250Aperform%2520masked%2520token%2520reconstruction.%2520Additionally%252C%2520we%2520propose%2520that%2520for%2520an%2520input%250Atoken%2520masked%2520by%2520both%2520masks%2520simultaneously%252C%2520the%2520reconstructed%2520features%2520should%2520be%250Aas%2520similar%2520as%2520possible.%2520This%2520naturally%2520establishes%2520an%2520explicit%2520contrastive%250Aconstraint%2520within%2520the%2520generative%2520MAE-based%2520pre-training%2520paradigm%252C%2520resulting%2520in%250Aour%2520proposed%2520method%252C%2520Point-CMAE.%2520Consequently%252C%2520Point-CMAE%2520effectively%2520enhances%250Athe%2520representation%2520quality%2520and%2520transfer%2520performance%2520compared%2520to%2520its%2520MAE%250Acounterpart.%2520Experimental%2520evaluations%2520across%2520various%2520downstream%2520applications%252C%250Aincluding%2520classification%252C%2520part%2520segmentation%252C%2520and%2520few-shot%2520learning%252C%2520demonstrate%250Athe%2520efficacy%2520of%2520our%2520framework%2520in%2520surpassing%2520state-of-the-art%2520techniques%2520under%250Astandard%2520ViTs%2520and%2520single-modal%2520settings.%2520The%2520source%2520code%2520and%2520trained%2520models%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/Amazingren/Point-CMAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bringing%20Masked%20Autoencoders%20Explicit%20Contrastive%20Properties%20for%20Point%0A%20%20Cloud%20Self-Supervised%20Learning&entry.906535625=Bin%20Ren%20and%20Guofeng%20Mei%20and%20Danda%20Pani%20Paudel%20and%20Weijie%20Wang%20and%20Yawei%20Li%20and%20Mengyuan%20Liu%20and%20Rita%20Cucchiara%20and%20Luc%20Van%20Gool%20and%20Nicu%20Sebe&entry.1292438233=%20%20Contrastive%20learning%20%28CL%29%20for%20Vision%20Transformers%20%28ViTs%29%20in%20image%20domains%20has%0Aachieved%20performance%20comparable%20to%20CL%20for%20traditional%20convolutional%20backbones.%0AHowever%2C%20in%203D%20point%20cloud%20pretraining%20with%20ViTs%2C%20masked%20autoencoder%20%28MAE%29%0Amodeling%20remains%20dominant.%20This%20raises%20the%20question%3A%20Can%20we%20take%20the%20best%20of%0Aboth%20worlds%3F%20To%20answer%20this%20question%2C%20we%20first%20empirically%20validate%20that%0Aintegrating%20MAE-based%20point%20cloud%20pre-training%20with%20the%20standard%20contrastive%0Alearning%20paradigm%2C%20even%20with%20meticulous%20design%2C%20can%20lead%20to%20a%20decrease%20in%0Aperformance.%20To%20address%20this%20limitation%2C%20we%20reintroduce%20CL%20into%20the%20MAE-based%0Apoint%20cloud%20pre-training%20paradigm%20by%20leveraging%20the%20inherent%20contrastive%0Aproperties%20of%20MAE.%20Specifically%2C%20rather%20than%20relying%20on%20extensive%20data%0Aaugmentation%20as%20commonly%20used%20in%20the%20image%20domain%2C%20we%20randomly%20mask%20the%20input%0Atokens%20twice%20to%20generate%20contrastive%20input%20pairs.%20Subsequently%2C%20a%0Aweight-sharing%20encoder%20and%20two%20identically%20structured%20decoders%20are%20utilized%20to%0Aperform%20masked%20token%20reconstruction.%20Additionally%2C%20we%20propose%20that%20for%20an%20input%0Atoken%20masked%20by%20both%20masks%20simultaneously%2C%20the%20reconstructed%20features%20should%20be%0Aas%20similar%20as%20possible.%20This%20naturally%20establishes%20an%20explicit%20contrastive%0Aconstraint%20within%20the%20generative%20MAE-based%20pre-training%20paradigm%2C%20resulting%20in%0Aour%20proposed%20method%2C%20Point-CMAE.%20Consequently%2C%20Point-CMAE%20effectively%20enhances%0Athe%20representation%20quality%20and%20transfer%20performance%20compared%20to%20its%20MAE%0Acounterpart.%20Experimental%20evaluations%20across%20various%20downstream%20applications%2C%0Aincluding%20classification%2C%20part%20segmentation%2C%20and%20few-shot%20learning%2C%20demonstrate%0Athe%20efficacy%20of%20our%20framework%20in%20surpassing%20state-of-the-art%20techniques%20under%0Astandard%20ViTs%20and%20single-modal%20settings.%20The%20source%20code%20and%20trained%20models%20are%0Aavailable%20at%3A%20https%3A//github.com/Amazingren/Point-CMAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05862v1&entry.124074799=Read"},
{"title": "TAPVid-3D: A Benchmark for Tracking Any Point in 3D", "author": "Skanda Koppula and Ignacio Rocco and Yi Yang and Joe Heyward and Jo\u00e3o Carreira and Andrew Zisserman and Gabriel Brostow and Carl Doersch", "abstract": "  We introduce a new benchmark, TAPVid-3D, for evaluating the task of\nlong-range Tracking Any Point in 3D (TAP-3D). While point tracking in two\ndimensions (TAP) has many benchmarks measuring performance on real-world\nvideos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To\nthis end, leveraging existing footage, we build a new benchmark for 3D point\ntracking featuring 4,000+ real-world videos, composed of three different data\nsources spanning a variety of object types, motion patterns, and indoor and\noutdoor environments. To measure performance on the TAP-3D task, we formulate a\ncollection of metrics that extend the Jaccard-based metric used in TAP to\nhandle the complexities of ambiguous depth scales across models, occlusions,\nand multi-track spatio-temporal smoothness. We manually verify a large sample\nof trajectories to ensure correct video annotations, and assess the current\nstate of the TAP-3D task by constructing competitive baselines using existing\ntracking models. We anticipate this benchmark will serve as a guidepost to\nimprove our ability to understand precise 3D motion and surface deformation\nfrom monocular video. Code for dataset download, generation, and model\nevaluation is available at https://tapvid3d.github.io\n", "link": "http://arxiv.org/abs/2407.05921v1", "date": "2024-07-08", "relevancy": 2.9532, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6041}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAPVid-3D%3A%20A%20Benchmark%20for%20Tracking%20Any%20Point%20in%203D&body=Title%3A%20TAPVid-3D%3A%20A%20Benchmark%20for%20Tracking%20Any%20Point%20in%203D%0AAuthor%3A%20Skanda%20Koppula%20and%20Ignacio%20Rocco%20and%20Yi%20Yang%20and%20Joe%20Heyward%20and%20Jo%C3%A3o%20Carreira%20and%20Andrew%20Zisserman%20and%20Gabriel%20Brostow%20and%20Carl%20Doersch%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20benchmark%2C%20TAPVid-3D%2C%20for%20evaluating%20the%20task%20of%0Along-range%20Tracking%20Any%20Point%20in%203D%20%28TAP-3D%29.%20While%20point%20tracking%20in%20two%0Adimensions%20%28TAP%29%20has%20many%20benchmarks%20measuring%20performance%20on%20real-world%0Avideos%2C%20such%20as%20TAPVid-DAVIS%2C%20three-dimensional%20point%20tracking%20has%20none.%20To%0Athis%20end%2C%20leveraging%20existing%20footage%2C%20we%20build%20a%20new%20benchmark%20for%203D%20point%0Atracking%20featuring%204%2C000%2B%20real-world%20videos%2C%20composed%20of%20three%20different%20data%0Asources%20spanning%20a%20variety%20of%20object%20types%2C%20motion%20patterns%2C%20and%20indoor%20and%0Aoutdoor%20environments.%20To%20measure%20performance%20on%20the%20TAP-3D%20task%2C%20we%20formulate%20a%0Acollection%20of%20metrics%20that%20extend%20the%20Jaccard-based%20metric%20used%20in%20TAP%20to%0Ahandle%20the%20complexities%20of%20ambiguous%20depth%20scales%20across%20models%2C%20occlusions%2C%0Aand%20multi-track%20spatio-temporal%20smoothness.%20We%20manually%20verify%20a%20large%20sample%0Aof%20trajectories%20to%20ensure%20correct%20video%20annotations%2C%20and%20assess%20the%20current%0Astate%20of%20the%20TAP-3D%20task%20by%20constructing%20competitive%20baselines%20using%20existing%0Atracking%20models.%20We%20anticipate%20this%20benchmark%20will%20serve%20as%20a%20guidepost%20to%0Aimprove%20our%20ability%20to%20understand%20precise%203D%20motion%20and%20surface%20deformation%0Afrom%20monocular%20video.%20Code%20for%20dataset%20download%2C%20generation%2C%20and%20model%0Aevaluation%20is%20available%20at%20https%3A//tapvid3d.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAPVid-3D%253A%2520A%2520Benchmark%2520for%2520Tracking%2520Any%2520Point%2520in%25203D%26entry.906535625%3DSkanda%2520Koppula%2520and%2520Ignacio%2520Rocco%2520and%2520Yi%2520Yang%2520and%2520Joe%2520Heyward%2520and%2520Jo%25C3%25A3o%2520Carreira%2520and%2520Andrew%2520Zisserman%2520and%2520Gabriel%2520Brostow%2520and%2520Carl%2520Doersch%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520benchmark%252C%2520TAPVid-3D%252C%2520for%2520evaluating%2520the%2520task%2520of%250Along-range%2520Tracking%2520Any%2520Point%2520in%25203D%2520%2528TAP-3D%2529.%2520While%2520point%2520tracking%2520in%2520two%250Adimensions%2520%2528TAP%2529%2520has%2520many%2520benchmarks%2520measuring%2520performance%2520on%2520real-world%250Avideos%252C%2520such%2520as%2520TAPVid-DAVIS%252C%2520three-dimensional%2520point%2520tracking%2520has%2520none.%2520To%250Athis%2520end%252C%2520leveraging%2520existing%2520footage%252C%2520we%2520build%2520a%2520new%2520benchmark%2520for%25203D%2520point%250Atracking%2520featuring%25204%252C000%252B%2520real-world%2520videos%252C%2520composed%2520of%2520three%2520different%2520data%250Asources%2520spanning%2520a%2520variety%2520of%2520object%2520types%252C%2520motion%2520patterns%252C%2520and%2520indoor%2520and%250Aoutdoor%2520environments.%2520To%2520measure%2520performance%2520on%2520the%2520TAP-3D%2520task%252C%2520we%2520formulate%2520a%250Acollection%2520of%2520metrics%2520that%2520extend%2520the%2520Jaccard-based%2520metric%2520used%2520in%2520TAP%2520to%250Ahandle%2520the%2520complexities%2520of%2520ambiguous%2520depth%2520scales%2520across%2520models%252C%2520occlusions%252C%250Aand%2520multi-track%2520spatio-temporal%2520smoothness.%2520We%2520manually%2520verify%2520a%2520large%2520sample%250Aof%2520trajectories%2520to%2520ensure%2520correct%2520video%2520annotations%252C%2520and%2520assess%2520the%2520current%250Astate%2520of%2520the%2520TAP-3D%2520task%2520by%2520constructing%2520competitive%2520baselines%2520using%2520existing%250Atracking%2520models.%2520We%2520anticipate%2520this%2520benchmark%2520will%2520serve%2520as%2520a%2520guidepost%2520to%250Aimprove%2520our%2520ability%2520to%2520understand%2520precise%25203D%2520motion%2520and%2520surface%2520deformation%250Afrom%2520monocular%2520video.%2520Code%2520for%2520dataset%2520download%252C%2520generation%252C%2520and%2520model%250Aevaluation%2520is%2520available%2520at%2520https%253A//tapvid3d.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAPVid-3D%3A%20A%20Benchmark%20for%20Tracking%20Any%20Point%20in%203D&entry.906535625=Skanda%20Koppula%20and%20Ignacio%20Rocco%20and%20Yi%20Yang%20and%20Joe%20Heyward%20and%20Jo%C3%A3o%20Carreira%20and%20Andrew%20Zisserman%20and%20Gabriel%20Brostow%20and%20Carl%20Doersch&entry.1292438233=%20%20We%20introduce%20a%20new%20benchmark%2C%20TAPVid-3D%2C%20for%20evaluating%20the%20task%20of%0Along-range%20Tracking%20Any%20Point%20in%203D%20%28TAP-3D%29.%20While%20point%20tracking%20in%20two%0Adimensions%20%28TAP%29%20has%20many%20benchmarks%20measuring%20performance%20on%20real-world%0Avideos%2C%20such%20as%20TAPVid-DAVIS%2C%20three-dimensional%20point%20tracking%20has%20none.%20To%0Athis%20end%2C%20leveraging%20existing%20footage%2C%20we%20build%20a%20new%20benchmark%20for%203D%20point%0Atracking%20featuring%204%2C000%2B%20real-world%20videos%2C%20composed%20of%20three%20different%20data%0Asources%20spanning%20a%20variety%20of%20object%20types%2C%20motion%20patterns%2C%20and%20indoor%20and%0Aoutdoor%20environments.%20To%20measure%20performance%20on%20the%20TAP-3D%20task%2C%20we%20formulate%20a%0Acollection%20of%20metrics%20that%20extend%20the%20Jaccard-based%20metric%20used%20in%20TAP%20to%0Ahandle%20the%20complexities%20of%20ambiguous%20depth%20scales%20across%20models%2C%20occlusions%2C%0Aand%20multi-track%20spatio-temporal%20smoothness.%20We%20manually%20verify%20a%20large%20sample%0Aof%20trajectories%20to%20ensure%20correct%20video%20annotations%2C%20and%20assess%20the%20current%0Astate%20of%20the%20TAP-3D%20task%20by%20constructing%20competitive%20baselines%20using%20existing%0Atracking%20models.%20We%20anticipate%20this%20benchmark%20will%20serve%20as%20a%20guidepost%20to%0Aimprove%20our%20ability%20to%20understand%20precise%203D%20motion%20and%20surface%20deformation%0Afrom%20monocular%20video.%20Code%20for%20dataset%20download%2C%20generation%2C%20and%20model%0Aevaluation%20is%20available%20at%20https%3A//tapvid3d.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05921v1&entry.124074799=Read"},
{"title": "TARGO: Benchmarking Target-driven Object Grasping under Occlusions", "author": "Yan Xia and Ran Ding and Ziyuan Qin and Guanqi Zhan and Kaichen Zhou and Long Yang and Hao Dong and Daniel Cremers", "abstract": "  Recent advances in predicting 6D grasp poses from a single depth image have\nled to promising performance in robotic grasping. However, previous grasping\nmodels face challenges in cluttered environments where nearby objects impact\nthe target object's grasp. In this paper, we first establish a new benchmark\ndataset for TARget-driven Grasping under Occlusions, named TARGO. We make the\nfollowing contributions: 1) We are the first to study the occlusion level of\ngrasping. 2) We set up an evaluation benchmark consisting of large-scale\nsynthetic data and part of real-world data, and we evaluated five grasp models\nand found that even the current SOTA model suffers when the occlusion level\nincreases, leaving grasping under occlusion still a challenge. 3) We also\ngenerate a large-scale training dataset via a scalable pipeline, which can be\nused to boost the performance of grasping under occlusion and generalized to\nthe real world. 4) We further propose a transformer-based grasping model\ninvolving a shape completion module, termed TARGO-Net, which performs most\nrobustly as occlusion increases. Our benchmark dataset can be found at\nhttps://TARGO-benchmark.github.io/.\n", "link": "http://arxiv.org/abs/2407.06168v1", "date": "2024-07-08", "relevancy": 2.9127, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.636}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5576}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TARGO%3A%20Benchmarking%20Target-driven%20Object%20Grasping%20under%20Occlusions&body=Title%3A%20TARGO%3A%20Benchmarking%20Target-driven%20Object%20Grasping%20under%20Occlusions%0AAuthor%3A%20Yan%20Xia%20and%20Ran%20Ding%20and%20Ziyuan%20Qin%20and%20Guanqi%20Zhan%20and%20Kaichen%20Zhou%20and%20Long%20Yang%20and%20Hao%20Dong%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Recent%20advances%20in%20predicting%206D%20grasp%20poses%20from%20a%20single%20depth%20image%20have%0Aled%20to%20promising%20performance%20in%20robotic%20grasping.%20However%2C%20previous%20grasping%0Amodels%20face%20challenges%20in%20cluttered%20environments%20where%20nearby%20objects%20impact%0Athe%20target%20object%27s%20grasp.%20In%20this%20paper%2C%20we%20first%20establish%20a%20new%20benchmark%0Adataset%20for%20TARget-driven%20Grasping%20under%20Occlusions%2C%20named%20TARGO.%20We%20make%20the%0Afollowing%20contributions%3A%201%29%20We%20are%20the%20first%20to%20study%20the%20occlusion%20level%20of%0Agrasping.%202%29%20We%20set%20up%20an%20evaluation%20benchmark%20consisting%20of%20large-scale%0Asynthetic%20data%20and%20part%20of%20real-world%20data%2C%20and%20we%20evaluated%20five%20grasp%20models%0Aand%20found%20that%20even%20the%20current%20SOTA%20model%20suffers%20when%20the%20occlusion%20level%0Aincreases%2C%20leaving%20grasping%20under%20occlusion%20still%20a%20challenge.%203%29%20We%20also%0Agenerate%20a%20large-scale%20training%20dataset%20via%20a%20scalable%20pipeline%2C%20which%20can%20be%0Aused%20to%20boost%20the%20performance%20of%20grasping%20under%20occlusion%20and%20generalized%20to%0Athe%20real%20world.%204%29%20We%20further%20propose%20a%20transformer-based%20grasping%20model%0Ainvolving%20a%20shape%20completion%20module%2C%20termed%20TARGO-Net%2C%20which%20performs%20most%0Arobustly%20as%20occlusion%20increases.%20Our%20benchmark%20dataset%20can%20be%20found%20at%0Ahttps%3A//TARGO-benchmark.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTARGO%253A%2520Benchmarking%2520Target-driven%2520Object%2520Grasping%2520under%2520Occlusions%26entry.906535625%3DYan%2520Xia%2520and%2520Ran%2520Ding%2520and%2520Ziyuan%2520Qin%2520and%2520Guanqi%2520Zhan%2520and%2520Kaichen%2520Zhou%2520and%2520Long%2520Yang%2520and%2520Hao%2520Dong%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520predicting%25206D%2520grasp%2520poses%2520from%2520a%2520single%2520depth%2520image%2520have%250Aled%2520to%2520promising%2520performance%2520in%2520robotic%2520grasping.%2520However%252C%2520previous%2520grasping%250Amodels%2520face%2520challenges%2520in%2520cluttered%2520environments%2520where%2520nearby%2520objects%2520impact%250Athe%2520target%2520object%2527s%2520grasp.%2520In%2520this%2520paper%252C%2520we%2520first%2520establish%2520a%2520new%2520benchmark%250Adataset%2520for%2520TARget-driven%2520Grasping%2520under%2520Occlusions%252C%2520named%2520TARGO.%2520We%2520make%2520the%250Afollowing%2520contributions%253A%25201%2529%2520We%2520are%2520the%2520first%2520to%2520study%2520the%2520occlusion%2520level%2520of%250Agrasping.%25202%2529%2520We%2520set%2520up%2520an%2520evaluation%2520benchmark%2520consisting%2520of%2520large-scale%250Asynthetic%2520data%2520and%2520part%2520of%2520real-world%2520data%252C%2520and%2520we%2520evaluated%2520five%2520grasp%2520models%250Aand%2520found%2520that%2520even%2520the%2520current%2520SOTA%2520model%2520suffers%2520when%2520the%2520occlusion%2520level%250Aincreases%252C%2520leaving%2520grasping%2520under%2520occlusion%2520still%2520a%2520challenge.%25203%2529%2520We%2520also%250Agenerate%2520a%2520large-scale%2520training%2520dataset%2520via%2520a%2520scalable%2520pipeline%252C%2520which%2520can%2520be%250Aused%2520to%2520boost%2520the%2520performance%2520of%2520grasping%2520under%2520occlusion%2520and%2520generalized%2520to%250Athe%2520real%2520world.%25204%2529%2520We%2520further%2520propose%2520a%2520transformer-based%2520grasping%2520model%250Ainvolving%2520a%2520shape%2520completion%2520module%252C%2520termed%2520TARGO-Net%252C%2520which%2520performs%2520most%250Arobustly%2520as%2520occlusion%2520increases.%2520Our%2520benchmark%2520dataset%2520can%2520be%2520found%2520at%250Ahttps%253A//TARGO-benchmark.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TARGO%3A%20Benchmarking%20Target-driven%20Object%20Grasping%20under%20Occlusions&entry.906535625=Yan%20Xia%20and%20Ran%20Ding%20and%20Ziyuan%20Qin%20and%20Guanqi%20Zhan%20and%20Kaichen%20Zhou%20and%20Long%20Yang%20and%20Hao%20Dong%20and%20Daniel%20Cremers&entry.1292438233=%20%20Recent%20advances%20in%20predicting%206D%20grasp%20poses%20from%20a%20single%20depth%20image%20have%0Aled%20to%20promising%20performance%20in%20robotic%20grasping.%20However%2C%20previous%20grasping%0Amodels%20face%20challenges%20in%20cluttered%20environments%20where%20nearby%20objects%20impact%0Athe%20target%20object%27s%20grasp.%20In%20this%20paper%2C%20we%20first%20establish%20a%20new%20benchmark%0Adataset%20for%20TARget-driven%20Grasping%20under%20Occlusions%2C%20named%20TARGO.%20We%20make%20the%0Afollowing%20contributions%3A%201%29%20We%20are%20the%20first%20to%20study%20the%20occlusion%20level%20of%0Agrasping.%202%29%20We%20set%20up%20an%20evaluation%20benchmark%20consisting%20of%20large-scale%0Asynthetic%20data%20and%20part%20of%20real-world%20data%2C%20and%20we%20evaluated%20five%20grasp%20models%0Aand%20found%20that%20even%20the%20current%20SOTA%20model%20suffers%20when%20the%20occlusion%20level%0Aincreases%2C%20leaving%20grasping%20under%20occlusion%20still%20a%20challenge.%203%29%20We%20also%0Agenerate%20a%20large-scale%20training%20dataset%20via%20a%20scalable%20pipeline%2C%20which%20can%20be%0Aused%20to%20boost%20the%20performance%20of%20grasping%20under%20occlusion%20and%20generalized%20to%0Athe%20real%20world.%204%29%20We%20further%20propose%20a%20transformer-based%20grasping%20model%0Ainvolving%20a%20shape%20completion%20module%2C%20termed%20TARGO-Net%2C%20which%20performs%20most%0Arobustly%20as%20occlusion%20increases.%20Our%20benchmark%20dataset%20can%20be%20found%20at%0Ahttps%3A//TARGO-benchmark.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06168v1&entry.124074799=Read"},
{"title": "Unsupervised View-Invariant Human Posture Representation", "author": "Faegheh Sardari and Bj\u00f6rn Ommer and Majid Mirmehdi", "abstract": "  Most recent view-invariant action recognition and performance assessment\napproaches rely on a large amount of annotated 3D skeleton data to extract\nview-invariant features. However, acquiring 3D skeleton data can be cumbersome,\nif not impractical, in in-the-wild scenarios. To overcome this problem, we\npresent a novel unsupervised approach that learns to extract view-invariant 3D\nhuman pose representation from a 2D image without using 3D joint data. Our\nmodel is trained by exploiting the intrinsic view-invariant properties of human\npose between simultaneous frames from different viewpoints and their\nequivariant properties between augmented frames from the same viewpoint. We\nevaluate the learned view-invariant pose representations for two downstream\ntasks. We perform comparative experiments that show improvements on the\nstate-of-the-art unsupervised cross-view action classification accuracy on NTU\nRGB+D by a significant margin, on both RGB and depth images. We also show the\nefficiency of transferring the learned representations from NTU RGB+D to obtain\nthe first ever unsupervised cross-view and cross-subject rank correlation\nresults on the multi-view human movement quality dataset, QMAR, and marginally\nimprove on the-state-of-the-art supervised results for this dataset. We also\ncarry out ablation studies to examine the contributions of the different\ncomponents of our proposed network.\n", "link": "http://arxiv.org/abs/2109.08730v2", "date": "2024-07-08", "relevancy": 2.8682, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.577}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5761}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20View-Invariant%20Human%20Posture%20Representation&body=Title%3A%20Unsupervised%20View-Invariant%20Human%20Posture%20Representation%0AAuthor%3A%20Faegheh%20Sardari%20and%20Bj%C3%B6rn%20Ommer%20and%20Majid%20Mirmehdi%0AAbstract%3A%20%20%20Most%20recent%20view-invariant%20action%20recognition%20and%20performance%20assessment%0Aapproaches%20rely%20on%20a%20large%20amount%20of%20annotated%203D%20skeleton%20data%20to%20extract%0Aview-invariant%20features.%20However%2C%20acquiring%203D%20skeleton%20data%20can%20be%20cumbersome%2C%0Aif%20not%20impractical%2C%20in%20in-the-wild%20scenarios.%20To%20overcome%20this%20problem%2C%20we%0Apresent%20a%20novel%20unsupervised%20approach%20that%20learns%20to%20extract%20view-invariant%203D%0Ahuman%20pose%20representation%20from%20a%202D%20image%20without%20using%203D%20joint%20data.%20Our%0Amodel%20is%20trained%20by%20exploiting%20the%20intrinsic%20view-invariant%20properties%20of%20human%0Apose%20between%20simultaneous%20frames%20from%20different%20viewpoints%20and%20their%0Aequivariant%20properties%20between%20augmented%20frames%20from%20the%20same%20viewpoint.%20We%0Aevaluate%20the%20learned%20view-invariant%20pose%20representations%20for%20two%20downstream%0Atasks.%20We%20perform%20comparative%20experiments%20that%20show%20improvements%20on%20the%0Astate-of-the-art%20unsupervised%20cross-view%20action%20classification%20accuracy%20on%20NTU%0ARGB%2BD%20by%20a%20significant%20margin%2C%20on%20both%20RGB%20and%20depth%20images.%20We%20also%20show%20the%0Aefficiency%20of%20transferring%20the%20learned%20representations%20from%20NTU%20RGB%2BD%20to%20obtain%0Athe%20first%20ever%20unsupervised%20cross-view%20and%20cross-subject%20rank%20correlation%0Aresults%20on%20the%20multi-view%20human%20movement%20quality%20dataset%2C%20QMAR%2C%20and%20marginally%0Aimprove%20on%20the-state-of-the-art%20supervised%20results%20for%20this%20dataset.%20We%20also%0Acarry%20out%20ablation%20studies%20to%20examine%20the%20contributions%20of%20the%20different%0Acomponents%20of%20our%20proposed%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2109.08730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520View-Invariant%2520Human%2520Posture%2520Representation%26entry.906535625%3DFaegheh%2520Sardari%2520and%2520Bj%25C3%25B6rn%2520Ommer%2520and%2520Majid%2520Mirmehdi%26entry.1292438233%3D%2520%2520Most%2520recent%2520view-invariant%2520action%2520recognition%2520and%2520performance%2520assessment%250Aapproaches%2520rely%2520on%2520a%2520large%2520amount%2520of%2520annotated%25203D%2520skeleton%2520data%2520to%2520extract%250Aview-invariant%2520features.%2520However%252C%2520acquiring%25203D%2520skeleton%2520data%2520can%2520be%2520cumbersome%252C%250Aif%2520not%2520impractical%252C%2520in%2520in-the-wild%2520scenarios.%2520To%2520overcome%2520this%2520problem%252C%2520we%250Apresent%2520a%2520novel%2520unsupervised%2520approach%2520that%2520learns%2520to%2520extract%2520view-invariant%25203D%250Ahuman%2520pose%2520representation%2520from%2520a%25202D%2520image%2520without%2520using%25203D%2520joint%2520data.%2520Our%250Amodel%2520is%2520trained%2520by%2520exploiting%2520the%2520intrinsic%2520view-invariant%2520properties%2520of%2520human%250Apose%2520between%2520simultaneous%2520frames%2520from%2520different%2520viewpoints%2520and%2520their%250Aequivariant%2520properties%2520between%2520augmented%2520frames%2520from%2520the%2520same%2520viewpoint.%2520We%250Aevaluate%2520the%2520learned%2520view-invariant%2520pose%2520representations%2520for%2520two%2520downstream%250Atasks.%2520We%2520perform%2520comparative%2520experiments%2520that%2520show%2520improvements%2520on%2520the%250Astate-of-the-art%2520unsupervised%2520cross-view%2520action%2520classification%2520accuracy%2520on%2520NTU%250ARGB%252BD%2520by%2520a%2520significant%2520margin%252C%2520on%2520both%2520RGB%2520and%2520depth%2520images.%2520We%2520also%2520show%2520the%250Aefficiency%2520of%2520transferring%2520the%2520learned%2520representations%2520from%2520NTU%2520RGB%252BD%2520to%2520obtain%250Athe%2520first%2520ever%2520unsupervised%2520cross-view%2520and%2520cross-subject%2520rank%2520correlation%250Aresults%2520on%2520the%2520multi-view%2520human%2520movement%2520quality%2520dataset%252C%2520QMAR%252C%2520and%2520marginally%250Aimprove%2520on%2520the-state-of-the-art%2520supervised%2520results%2520for%2520this%2520dataset.%2520We%2520also%250Acarry%2520out%2520ablation%2520studies%2520to%2520examine%2520the%2520contributions%2520of%2520the%2520different%250Acomponents%2520of%2520our%2520proposed%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2109.08730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20View-Invariant%20Human%20Posture%20Representation&entry.906535625=Faegheh%20Sardari%20and%20Bj%C3%B6rn%20Ommer%20and%20Majid%20Mirmehdi&entry.1292438233=%20%20Most%20recent%20view-invariant%20action%20recognition%20and%20performance%20assessment%0Aapproaches%20rely%20on%20a%20large%20amount%20of%20annotated%203D%20skeleton%20data%20to%20extract%0Aview-invariant%20features.%20However%2C%20acquiring%203D%20skeleton%20data%20can%20be%20cumbersome%2C%0Aif%20not%20impractical%2C%20in%20in-the-wild%20scenarios.%20To%20overcome%20this%20problem%2C%20we%0Apresent%20a%20novel%20unsupervised%20approach%20that%20learns%20to%20extract%20view-invariant%203D%0Ahuman%20pose%20representation%20from%20a%202D%20image%20without%20using%203D%20joint%20data.%20Our%0Amodel%20is%20trained%20by%20exploiting%20the%20intrinsic%20view-invariant%20properties%20of%20human%0Apose%20between%20simultaneous%20frames%20from%20different%20viewpoints%20and%20their%0Aequivariant%20properties%20between%20augmented%20frames%20from%20the%20same%20viewpoint.%20We%0Aevaluate%20the%20learned%20view-invariant%20pose%20representations%20for%20two%20downstream%0Atasks.%20We%20perform%20comparative%20experiments%20that%20show%20improvements%20on%20the%0Astate-of-the-art%20unsupervised%20cross-view%20action%20classification%20accuracy%20on%20NTU%0ARGB%2BD%20by%20a%20significant%20margin%2C%20on%20both%20RGB%20and%20depth%20images.%20We%20also%20show%20the%0Aefficiency%20of%20transferring%20the%20learned%20representations%20from%20NTU%20RGB%2BD%20to%20obtain%0Athe%20first%20ever%20unsupervised%20cross-view%20and%20cross-subject%20rank%20correlation%0Aresults%20on%20the%20multi-view%20human%20movement%20quality%20dataset%2C%20QMAR%2C%20and%20marginally%0Aimprove%20on%20the-state-of-the-art%20supervised%20results%20for%20this%20dataset.%20We%20also%0Acarry%20out%20ablation%20studies%20to%20examine%20the%20contributions%20of%20the%20different%0Acomponents%20of%20our%20proposed%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.08730v2&entry.124074799=Read"},
{"title": "Leveraging Transformers for Weakly Supervised Object Localization in\n  Unconstrained Videos", "author": "Shakeeb Murtaza and Marco Pedersoli and Aydin Sarraf and Eric Granger", "abstract": "  Weakly-Supervised Video Object Localization (WSVOL) involves localizing an\nobject in videos using only video-level labels, also referred to as tags.\nState-of-the-art WSVOL methods like Temporal CAM (TCAM) rely on class\nactivation mapping (CAM) and typically require a pre-trained CNN classifier.\nHowever, their localization accuracy is affected by their tendency to minimize\nthe mutual information between different instances of a class and exploit\ntemporal information during training for downstream tasks, e.g., detection and\ntracking. In the absence of bounding box annotation, it is challenging to\nexploit precise information about objects from temporal cues because the model\nstruggles to locate objects over time. To address these issues, a novel method\ncalled transformer based CAM for videos (TrCAM-V), is proposed for WSVOL. It\nconsists of a DeiT backbone with two heads for classification and localization.\nThe classification head is trained using standard classification loss (CL),\nwhile the localization head is trained using pseudo-labels that are extracted\nusing a pre-trained CLIP model. From these pseudo-labels, the high and low\nactivation values are considered to be foreground and background regions,\nrespectively. Our TrCAM-V method allows training a localization network by\nsampling pseudo-pixels on the fly from these regions. Additionally, a\nconditional random field (CRF) loss is employed to align the object boundaries\nwith the foreground map. During inference, the model can process individual\nframes for real-time localization applications. Extensive experiments on\nchallenging YouTube-Objects unconstrained video datasets show that our TrCAM-V\nmethod achieves new state-of-the-art performance in terms of classification and\nlocalization accuracy.\n", "link": "http://arxiv.org/abs/2407.06018v1", "date": "2024-07-08", "relevancy": 2.8572, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5777}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Transformers%20for%20Weakly%20Supervised%20Object%20Localization%20in%0A%20%20Unconstrained%20Videos&body=Title%3A%20Leveraging%20Transformers%20for%20Weakly%20Supervised%20Object%20Localization%20in%0A%20%20Unconstrained%20Videos%0AAuthor%3A%20Shakeeb%20Murtaza%20and%20Marco%20Pedersoli%20and%20Aydin%20Sarraf%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Weakly-Supervised%20Video%20Object%20Localization%20%28WSVOL%29%20involves%20localizing%20an%0Aobject%20in%20videos%20using%20only%20video-level%20labels%2C%20also%20referred%20to%20as%20tags.%0AState-of-the-art%20WSVOL%20methods%20like%20Temporal%20CAM%20%28TCAM%29%20rely%20on%20class%0Aactivation%20mapping%20%28CAM%29%20and%20typically%20require%20a%20pre-trained%20CNN%20classifier.%0AHowever%2C%20their%20localization%20accuracy%20is%20affected%20by%20their%20tendency%20to%20minimize%0Athe%20mutual%20information%20between%20different%20instances%20of%20a%20class%20and%20exploit%0Atemporal%20information%20during%20training%20for%20downstream%20tasks%2C%20e.g.%2C%20detection%20and%0Atracking.%20In%20the%20absence%20of%20bounding%20box%20annotation%2C%20it%20is%20challenging%20to%0Aexploit%20precise%20information%20about%20objects%20from%20temporal%20cues%20because%20the%20model%0Astruggles%20to%20locate%20objects%20over%20time.%20To%20address%20these%20issues%2C%20a%20novel%20method%0Acalled%20transformer%20based%20CAM%20for%20videos%20%28TrCAM-V%29%2C%20is%20proposed%20for%20WSVOL.%20It%0Aconsists%20of%20a%20DeiT%20backbone%20with%20two%20heads%20for%20classification%20and%20localization.%0AThe%20classification%20head%20is%20trained%20using%20standard%20classification%20loss%20%28CL%29%2C%0Awhile%20the%20localization%20head%20is%20trained%20using%20pseudo-labels%20that%20are%20extracted%0Ausing%20a%20pre-trained%20CLIP%20model.%20From%20these%20pseudo-labels%2C%20the%20high%20and%20low%0Aactivation%20values%20are%20considered%20to%20be%20foreground%20and%20background%20regions%2C%0Arespectively.%20Our%20TrCAM-V%20method%20allows%20training%20a%20localization%20network%20by%0Asampling%20pseudo-pixels%20on%20the%20fly%20from%20these%20regions.%20Additionally%2C%20a%0Aconditional%20random%20field%20%28CRF%29%20loss%20is%20employed%20to%20align%20the%20object%20boundaries%0Awith%20the%20foreground%20map.%20During%20inference%2C%20the%20model%20can%20process%20individual%0Aframes%20for%20real-time%20localization%20applications.%20Extensive%20experiments%20on%0Achallenging%20YouTube-Objects%20unconstrained%20video%20datasets%20show%20that%20our%20TrCAM-V%0Amethod%20achieves%20new%20state-of-the-art%20performance%20in%20terms%20of%20classification%20and%0Alocalization%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Transformers%2520for%2520Weakly%2520Supervised%2520Object%2520Localization%2520in%250A%2520%2520Unconstrained%2520Videos%26entry.906535625%3DShakeeb%2520Murtaza%2520and%2520Marco%2520Pedersoli%2520and%2520Aydin%2520Sarraf%2520and%2520Eric%2520Granger%26entry.1292438233%3D%2520%2520Weakly-Supervised%2520Video%2520Object%2520Localization%2520%2528WSVOL%2529%2520involves%2520localizing%2520an%250Aobject%2520in%2520videos%2520using%2520only%2520video-level%2520labels%252C%2520also%2520referred%2520to%2520as%2520tags.%250AState-of-the-art%2520WSVOL%2520methods%2520like%2520Temporal%2520CAM%2520%2528TCAM%2529%2520rely%2520on%2520class%250Aactivation%2520mapping%2520%2528CAM%2529%2520and%2520typically%2520require%2520a%2520pre-trained%2520CNN%2520classifier.%250AHowever%252C%2520their%2520localization%2520accuracy%2520is%2520affected%2520by%2520their%2520tendency%2520to%2520minimize%250Athe%2520mutual%2520information%2520between%2520different%2520instances%2520of%2520a%2520class%2520and%2520exploit%250Atemporal%2520information%2520during%2520training%2520for%2520downstream%2520tasks%252C%2520e.g.%252C%2520detection%2520and%250Atracking.%2520In%2520the%2520absence%2520of%2520bounding%2520box%2520annotation%252C%2520it%2520is%2520challenging%2520to%250Aexploit%2520precise%2520information%2520about%2520objects%2520from%2520temporal%2520cues%2520because%2520the%2520model%250Astruggles%2520to%2520locate%2520objects%2520over%2520time.%2520To%2520address%2520these%2520issues%252C%2520a%2520novel%2520method%250Acalled%2520transformer%2520based%2520CAM%2520for%2520videos%2520%2528TrCAM-V%2529%252C%2520is%2520proposed%2520for%2520WSVOL.%2520It%250Aconsists%2520of%2520a%2520DeiT%2520backbone%2520with%2520two%2520heads%2520for%2520classification%2520and%2520localization.%250AThe%2520classification%2520head%2520is%2520trained%2520using%2520standard%2520classification%2520loss%2520%2528CL%2529%252C%250Awhile%2520the%2520localization%2520head%2520is%2520trained%2520using%2520pseudo-labels%2520that%2520are%2520extracted%250Ausing%2520a%2520pre-trained%2520CLIP%2520model.%2520From%2520these%2520pseudo-labels%252C%2520the%2520high%2520and%2520low%250Aactivation%2520values%2520are%2520considered%2520to%2520be%2520foreground%2520and%2520background%2520regions%252C%250Arespectively.%2520Our%2520TrCAM-V%2520method%2520allows%2520training%2520a%2520localization%2520network%2520by%250Asampling%2520pseudo-pixels%2520on%2520the%2520fly%2520from%2520these%2520regions.%2520Additionally%252C%2520a%250Aconditional%2520random%2520field%2520%2528CRF%2529%2520loss%2520is%2520employed%2520to%2520align%2520the%2520object%2520boundaries%250Awith%2520the%2520foreground%2520map.%2520During%2520inference%252C%2520the%2520model%2520can%2520process%2520individual%250Aframes%2520for%2520real-time%2520localization%2520applications.%2520Extensive%2520experiments%2520on%250Achallenging%2520YouTube-Objects%2520unconstrained%2520video%2520datasets%2520show%2520that%2520our%2520TrCAM-V%250Amethod%2520achieves%2520new%2520state-of-the-art%2520performance%2520in%2520terms%2520of%2520classification%2520and%250Alocalization%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Transformers%20for%20Weakly%20Supervised%20Object%20Localization%20in%0A%20%20Unconstrained%20Videos&entry.906535625=Shakeeb%20Murtaza%20and%20Marco%20Pedersoli%20and%20Aydin%20Sarraf%20and%20Eric%20Granger&entry.1292438233=%20%20Weakly-Supervised%20Video%20Object%20Localization%20%28WSVOL%29%20involves%20localizing%20an%0Aobject%20in%20videos%20using%20only%20video-level%20labels%2C%20also%20referred%20to%20as%20tags.%0AState-of-the-art%20WSVOL%20methods%20like%20Temporal%20CAM%20%28TCAM%29%20rely%20on%20class%0Aactivation%20mapping%20%28CAM%29%20and%20typically%20require%20a%20pre-trained%20CNN%20classifier.%0AHowever%2C%20their%20localization%20accuracy%20is%20affected%20by%20their%20tendency%20to%20minimize%0Athe%20mutual%20information%20between%20different%20instances%20of%20a%20class%20and%20exploit%0Atemporal%20information%20during%20training%20for%20downstream%20tasks%2C%20e.g.%2C%20detection%20and%0Atracking.%20In%20the%20absence%20of%20bounding%20box%20annotation%2C%20it%20is%20challenging%20to%0Aexploit%20precise%20information%20about%20objects%20from%20temporal%20cues%20because%20the%20model%0Astruggles%20to%20locate%20objects%20over%20time.%20To%20address%20these%20issues%2C%20a%20novel%20method%0Acalled%20transformer%20based%20CAM%20for%20videos%20%28TrCAM-V%29%2C%20is%20proposed%20for%20WSVOL.%20It%0Aconsists%20of%20a%20DeiT%20backbone%20with%20two%20heads%20for%20classification%20and%20localization.%0AThe%20classification%20head%20is%20trained%20using%20standard%20classification%20loss%20%28CL%29%2C%0Awhile%20the%20localization%20head%20is%20trained%20using%20pseudo-labels%20that%20are%20extracted%0Ausing%20a%20pre-trained%20CLIP%20model.%20From%20these%20pseudo-labels%2C%20the%20high%20and%20low%0Aactivation%20values%20are%20considered%20to%20be%20foreground%20and%20background%20regions%2C%0Arespectively.%20Our%20TrCAM-V%20method%20allows%20training%20a%20localization%20network%20by%0Asampling%20pseudo-pixels%20on%20the%20fly%20from%20these%20regions.%20Additionally%2C%20a%0Aconditional%20random%20field%20%28CRF%29%20loss%20is%20employed%20to%20align%20the%20object%20boundaries%0Awith%20the%20foreground%20map.%20During%20inference%2C%20the%20model%20can%20process%20individual%0Aframes%20for%20real-time%20localization%20applications.%20Extensive%20experiments%20on%0Achallenging%20YouTube-Objects%20unconstrained%20video%20datasets%20show%20that%20our%20TrCAM-V%0Amethod%20achieves%20new%20state-of-the-art%20performance%20in%20terms%20of%20classification%20and%0Alocalization%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06018v1&entry.124074799=Read"},
{"title": "CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation", "author": "Xinying Guo and Mingyuan Zhang and Haozhe Xie and Chenyang Gu and Ziwei Liu", "abstract": "  Crowd Motion Generation is essential in entertainment industries such as\nanimation and games as well as in strategic fields like urban simulation and\nplanning. This new task requires an intricate integration of control and\ngeneration to realistically synthesize crowd dynamics under specific spatial\nand semantic constraints, whose challenges are yet to be fully explored. On the\none hand, existing human motion generation models typically focus on individual\nbehaviors, neglecting the complexities of collective behaviors. On the other\nhand, recent methods for multi-person motion generation depend heavily on\npre-defined scenarios and are limited to a fixed, small number of inter-person\ninteractions, thus hampering their practicality. To overcome these challenges,\nwe introduce CrowdMoGen, a zero-shot text-driven framework that harnesses the\npower of Large Language Model (LLM) to incorporate the collective intelligence\ninto the motion generation framework as guidance, thereby enabling\ngeneralizable planning and generation of crowd motions without paired training\ndata. Our framework consists of two key components: 1) Crowd Scene Planner that\nlearns to coordinate motions and dynamics according to specific scene contexts\nor introduced perturbations, and 2) Collective Motion Generator that\nefficiently synthesizes the required collective motions based on the holistic\nplans. Extensive quantitative and qualitative experiments have validated the\neffectiveness of our framework, which not only fills a critical gap by\nproviding scalable and generalizable solutions for Crowd Motion Generation task\nbut also achieves high levels of realism and flexibility.\n", "link": "http://arxiv.org/abs/2407.06188v1", "date": "2024-07-08", "relevancy": 2.8544, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6162}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.551}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrowdMoGen%3A%20Zero-Shot%20Text-Driven%20Collective%20Motion%20Generation&body=Title%3A%20CrowdMoGen%3A%20Zero-Shot%20Text-Driven%20Collective%20Motion%20Generation%0AAuthor%3A%20Xinying%20Guo%20and%20Mingyuan%20Zhang%20and%20Haozhe%20Xie%20and%20Chenyang%20Gu%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Crowd%20Motion%20Generation%20is%20essential%20in%20entertainment%20industries%20such%20as%0Aanimation%20and%20games%20as%20well%20as%20in%20strategic%20fields%20like%20urban%20simulation%20and%0Aplanning.%20This%20new%20task%20requires%20an%20intricate%20integration%20of%20control%20and%0Ageneration%20to%20realistically%20synthesize%20crowd%20dynamics%20under%20specific%20spatial%0Aand%20semantic%20constraints%2C%20whose%20challenges%20are%20yet%20to%20be%20fully%20explored.%20On%20the%0Aone%20hand%2C%20existing%20human%20motion%20generation%20models%20typically%20focus%20on%20individual%0Abehaviors%2C%20neglecting%20the%20complexities%20of%20collective%20behaviors.%20On%20the%20other%0Ahand%2C%20recent%20methods%20for%20multi-person%20motion%20generation%20depend%20heavily%20on%0Apre-defined%20scenarios%20and%20are%20limited%20to%20a%20fixed%2C%20small%20number%20of%20inter-person%0Ainteractions%2C%20thus%20hampering%20their%20practicality.%20To%20overcome%20these%20challenges%2C%0Awe%20introduce%20CrowdMoGen%2C%20a%20zero-shot%20text-driven%20framework%20that%20harnesses%20the%0Apower%20of%20Large%20Language%20Model%20%28LLM%29%20to%20incorporate%20the%20collective%20intelligence%0Ainto%20the%20motion%20generation%20framework%20as%20guidance%2C%20thereby%20enabling%0Ageneralizable%20planning%20and%20generation%20of%20crowd%20motions%20without%20paired%20training%0Adata.%20Our%20framework%20consists%20of%20two%20key%20components%3A%201%29%20Crowd%20Scene%20Planner%20that%0Alearns%20to%20coordinate%20motions%20and%20dynamics%20according%20to%20specific%20scene%20contexts%0Aor%20introduced%20perturbations%2C%20and%202%29%20Collective%20Motion%20Generator%20that%0Aefficiently%20synthesizes%20the%20required%20collective%20motions%20based%20on%20the%20holistic%0Aplans.%20Extensive%20quantitative%20and%20qualitative%20experiments%20have%20validated%20the%0Aeffectiveness%20of%20our%20framework%2C%20which%20not%20only%20fills%20a%20critical%20gap%20by%0Aproviding%20scalable%20and%20generalizable%20solutions%20for%20Crowd%20Motion%20Generation%20task%0Abut%20also%20achieves%20high%20levels%20of%20realism%20and%20flexibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrowdMoGen%253A%2520Zero-Shot%2520Text-Driven%2520Collective%2520Motion%2520Generation%26entry.906535625%3DXinying%2520Guo%2520and%2520Mingyuan%2520Zhang%2520and%2520Haozhe%2520Xie%2520and%2520Chenyang%2520Gu%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Crowd%2520Motion%2520Generation%2520is%2520essential%2520in%2520entertainment%2520industries%2520such%2520as%250Aanimation%2520and%2520games%2520as%2520well%2520as%2520in%2520strategic%2520fields%2520like%2520urban%2520simulation%2520and%250Aplanning.%2520This%2520new%2520task%2520requires%2520an%2520intricate%2520integration%2520of%2520control%2520and%250Ageneration%2520to%2520realistically%2520synthesize%2520crowd%2520dynamics%2520under%2520specific%2520spatial%250Aand%2520semantic%2520constraints%252C%2520whose%2520challenges%2520are%2520yet%2520to%2520be%2520fully%2520explored.%2520On%2520the%250Aone%2520hand%252C%2520existing%2520human%2520motion%2520generation%2520models%2520typically%2520focus%2520on%2520individual%250Abehaviors%252C%2520neglecting%2520the%2520complexities%2520of%2520collective%2520behaviors.%2520On%2520the%2520other%250Ahand%252C%2520recent%2520methods%2520for%2520multi-person%2520motion%2520generation%2520depend%2520heavily%2520on%250Apre-defined%2520scenarios%2520and%2520are%2520limited%2520to%2520a%2520fixed%252C%2520small%2520number%2520of%2520inter-person%250Ainteractions%252C%2520thus%2520hampering%2520their%2520practicality.%2520To%2520overcome%2520these%2520challenges%252C%250Awe%2520introduce%2520CrowdMoGen%252C%2520a%2520zero-shot%2520text-driven%2520framework%2520that%2520harnesses%2520the%250Apower%2520of%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520incorporate%2520the%2520collective%2520intelligence%250Ainto%2520the%2520motion%2520generation%2520framework%2520as%2520guidance%252C%2520thereby%2520enabling%250Ageneralizable%2520planning%2520and%2520generation%2520of%2520crowd%2520motions%2520without%2520paired%2520training%250Adata.%2520Our%2520framework%2520consists%2520of%2520two%2520key%2520components%253A%25201%2529%2520Crowd%2520Scene%2520Planner%2520that%250Alearns%2520to%2520coordinate%2520motions%2520and%2520dynamics%2520according%2520to%2520specific%2520scene%2520contexts%250Aor%2520introduced%2520perturbations%252C%2520and%25202%2529%2520Collective%2520Motion%2520Generator%2520that%250Aefficiently%2520synthesizes%2520the%2520required%2520collective%2520motions%2520based%2520on%2520the%2520holistic%250Aplans.%2520Extensive%2520quantitative%2520and%2520qualitative%2520experiments%2520have%2520validated%2520the%250Aeffectiveness%2520of%2520our%2520framework%252C%2520which%2520not%2520only%2520fills%2520a%2520critical%2520gap%2520by%250Aproviding%2520scalable%2520and%2520generalizable%2520solutions%2520for%2520Crowd%2520Motion%2520Generation%2520task%250Abut%2520also%2520achieves%2520high%2520levels%2520of%2520realism%2520and%2520flexibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrowdMoGen%3A%20Zero-Shot%20Text-Driven%20Collective%20Motion%20Generation&entry.906535625=Xinying%20Guo%20and%20Mingyuan%20Zhang%20and%20Haozhe%20Xie%20and%20Chenyang%20Gu%20and%20Ziwei%20Liu&entry.1292438233=%20%20Crowd%20Motion%20Generation%20is%20essential%20in%20entertainment%20industries%20such%20as%0Aanimation%20and%20games%20as%20well%20as%20in%20strategic%20fields%20like%20urban%20simulation%20and%0Aplanning.%20This%20new%20task%20requires%20an%20intricate%20integration%20of%20control%20and%0Ageneration%20to%20realistically%20synthesize%20crowd%20dynamics%20under%20specific%20spatial%0Aand%20semantic%20constraints%2C%20whose%20challenges%20are%20yet%20to%20be%20fully%20explored.%20On%20the%0Aone%20hand%2C%20existing%20human%20motion%20generation%20models%20typically%20focus%20on%20individual%0Abehaviors%2C%20neglecting%20the%20complexities%20of%20collective%20behaviors.%20On%20the%20other%0Ahand%2C%20recent%20methods%20for%20multi-person%20motion%20generation%20depend%20heavily%20on%0Apre-defined%20scenarios%20and%20are%20limited%20to%20a%20fixed%2C%20small%20number%20of%20inter-person%0Ainteractions%2C%20thus%20hampering%20their%20practicality.%20To%20overcome%20these%20challenges%2C%0Awe%20introduce%20CrowdMoGen%2C%20a%20zero-shot%20text-driven%20framework%20that%20harnesses%20the%0Apower%20of%20Large%20Language%20Model%20%28LLM%29%20to%20incorporate%20the%20collective%20intelligence%0Ainto%20the%20motion%20generation%20framework%20as%20guidance%2C%20thereby%20enabling%0Ageneralizable%20planning%20and%20generation%20of%20crowd%20motions%20without%20paired%20training%0Adata.%20Our%20framework%20consists%20of%20two%20key%20components%3A%201%29%20Crowd%20Scene%20Planner%20that%0Alearns%20to%20coordinate%20motions%20and%20dynamics%20according%20to%20specific%20scene%20contexts%0Aor%20introduced%20perturbations%2C%20and%202%29%20Collective%20Motion%20Generator%20that%0Aefficiently%20synthesizes%20the%20required%20collective%20motions%20based%20on%20the%20holistic%0Aplans.%20Extensive%20quantitative%20and%20qualitative%20experiments%20have%20validated%20the%0Aeffectiveness%20of%20our%20framework%2C%20which%20not%20only%20fills%20a%20critical%20gap%20by%0Aproviding%20scalable%20and%20generalizable%20solutions%20for%20Crowd%20Motion%20Generation%20task%0Abut%20also%20achieves%20high%20levels%20of%20realism%20and%20flexibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06188v1&entry.124074799=Read"},
{"title": "MICP-L: Mesh-based ICP for Robot Localization using Hardware-Accelerated\n  Ray Casting", "author": "Alexander Mock and Sebastian P\u00fctz and Thomas Wiemann and Joachim Hertzberg", "abstract": "  Triangle mesh maps are a versatile 3D environment representation for robots\nto navigate in challenging indoor and outdoor environments exhibiting tunnels,\nhills and varying slopes. To make use of these mesh maps, methods are needed to\naccurately localize robots in such maps to perform essential tasks like path\nplanning and navigation. We present Mesh ICP Localization (MICP-L), a novel and\ncomputationally efficient method for registering one or more range sensors to a\ntriangle mesh map to continuously localize a robot in 6D, even in GPS-denied\nenvironments. We accelerate the computation of ray casting correspondences\n(RCC) between range sensors and mesh maps by supporting different parallel\ncomputing devices like multicore CPUs, GPUs and the latest NVIDIA RTX hardware.\nBy additionally transforming the covariance computation into a reduction\noperation, we can optimize the initial guessed poses in parallel on CPUs or\nGPUs, making our implementation applicable in real-time on many architectures.\nWe demonstrate the robustness of our localization approach with datasets from\nagricultural, aerial, and automotive domains.\n", "link": "http://arxiv.org/abs/2210.13904v4", "date": "2024-07-08", "relevancy": 2.8374, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6014}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5525}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MICP-L%3A%20Mesh-based%20ICP%20for%20Robot%20Localization%20using%20Hardware-Accelerated%0A%20%20Ray%20Casting&body=Title%3A%20MICP-L%3A%20Mesh-based%20ICP%20for%20Robot%20Localization%20using%20Hardware-Accelerated%0A%20%20Ray%20Casting%0AAuthor%3A%20Alexander%20Mock%20and%20Sebastian%20P%C3%BCtz%20and%20Thomas%20Wiemann%20and%20Joachim%20Hertzberg%0AAbstract%3A%20%20%20Triangle%20mesh%20maps%20are%20a%20versatile%203D%20environment%20representation%20for%20robots%0Ato%20navigate%20in%20challenging%20indoor%20and%20outdoor%20environments%20exhibiting%20tunnels%2C%0Ahills%20and%20varying%20slopes.%20To%20make%20use%20of%20these%20mesh%20maps%2C%20methods%20are%20needed%20to%0Aaccurately%20localize%20robots%20in%20such%20maps%20to%20perform%20essential%20tasks%20like%20path%0Aplanning%20and%20navigation.%20We%20present%20Mesh%20ICP%20Localization%20%28MICP-L%29%2C%20a%20novel%20and%0Acomputationally%20efficient%20method%20for%20registering%20one%20or%20more%20range%20sensors%20to%20a%0Atriangle%20mesh%20map%20to%20continuously%20localize%20a%20robot%20in%206D%2C%20even%20in%20GPS-denied%0Aenvironments.%20We%20accelerate%20the%20computation%20of%20ray%20casting%20correspondences%0A%28RCC%29%20between%20range%20sensors%20and%20mesh%20maps%20by%20supporting%20different%20parallel%0Acomputing%20devices%20like%20multicore%20CPUs%2C%20GPUs%20and%20the%20latest%20NVIDIA%20RTX%20hardware.%0ABy%20additionally%20transforming%20the%20covariance%20computation%20into%20a%20reduction%0Aoperation%2C%20we%20can%20optimize%20the%20initial%20guessed%20poses%20in%20parallel%20on%20CPUs%20or%0AGPUs%2C%20making%20our%20implementation%20applicable%20in%20real-time%20on%20many%20architectures.%0AWe%20demonstrate%20the%20robustness%20of%20our%20localization%20approach%20with%20datasets%20from%0Aagricultural%2C%20aerial%2C%20and%20automotive%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.13904v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMICP-L%253A%2520Mesh-based%2520ICP%2520for%2520Robot%2520Localization%2520using%2520Hardware-Accelerated%250A%2520%2520Ray%2520Casting%26entry.906535625%3DAlexander%2520Mock%2520and%2520Sebastian%2520P%25C3%25BCtz%2520and%2520Thomas%2520Wiemann%2520and%2520Joachim%2520Hertzberg%26entry.1292438233%3D%2520%2520Triangle%2520mesh%2520maps%2520are%2520a%2520versatile%25203D%2520environment%2520representation%2520for%2520robots%250Ato%2520navigate%2520in%2520challenging%2520indoor%2520and%2520outdoor%2520environments%2520exhibiting%2520tunnels%252C%250Ahills%2520and%2520varying%2520slopes.%2520To%2520make%2520use%2520of%2520these%2520mesh%2520maps%252C%2520methods%2520are%2520needed%2520to%250Aaccurately%2520localize%2520robots%2520in%2520such%2520maps%2520to%2520perform%2520essential%2520tasks%2520like%2520path%250Aplanning%2520and%2520navigation.%2520We%2520present%2520Mesh%2520ICP%2520Localization%2520%2528MICP-L%2529%252C%2520a%2520novel%2520and%250Acomputationally%2520efficient%2520method%2520for%2520registering%2520one%2520or%2520more%2520range%2520sensors%2520to%2520a%250Atriangle%2520mesh%2520map%2520to%2520continuously%2520localize%2520a%2520robot%2520in%25206D%252C%2520even%2520in%2520GPS-denied%250Aenvironments.%2520We%2520accelerate%2520the%2520computation%2520of%2520ray%2520casting%2520correspondences%250A%2528RCC%2529%2520between%2520range%2520sensors%2520and%2520mesh%2520maps%2520by%2520supporting%2520different%2520parallel%250Acomputing%2520devices%2520like%2520multicore%2520CPUs%252C%2520GPUs%2520and%2520the%2520latest%2520NVIDIA%2520RTX%2520hardware.%250ABy%2520additionally%2520transforming%2520the%2520covariance%2520computation%2520into%2520a%2520reduction%250Aoperation%252C%2520we%2520can%2520optimize%2520the%2520initial%2520guessed%2520poses%2520in%2520parallel%2520on%2520CPUs%2520or%250AGPUs%252C%2520making%2520our%2520implementation%2520applicable%2520in%2520real-time%2520on%2520many%2520architectures.%250AWe%2520demonstrate%2520the%2520robustness%2520of%2520our%2520localization%2520approach%2520with%2520datasets%2520from%250Aagricultural%252C%2520aerial%252C%2520and%2520automotive%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.13904v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MICP-L%3A%20Mesh-based%20ICP%20for%20Robot%20Localization%20using%20Hardware-Accelerated%0A%20%20Ray%20Casting&entry.906535625=Alexander%20Mock%20and%20Sebastian%20P%C3%BCtz%20and%20Thomas%20Wiemann%20and%20Joachim%20Hertzberg&entry.1292438233=%20%20Triangle%20mesh%20maps%20are%20a%20versatile%203D%20environment%20representation%20for%20robots%0Ato%20navigate%20in%20challenging%20indoor%20and%20outdoor%20environments%20exhibiting%20tunnels%2C%0Ahills%20and%20varying%20slopes.%20To%20make%20use%20of%20these%20mesh%20maps%2C%20methods%20are%20needed%20to%0Aaccurately%20localize%20robots%20in%20such%20maps%20to%20perform%20essential%20tasks%20like%20path%0Aplanning%20and%20navigation.%20We%20present%20Mesh%20ICP%20Localization%20%28MICP-L%29%2C%20a%20novel%20and%0Acomputationally%20efficient%20method%20for%20registering%20one%20or%20more%20range%20sensors%20to%20a%0Atriangle%20mesh%20map%20to%20continuously%20localize%20a%20robot%20in%206D%2C%20even%20in%20GPS-denied%0Aenvironments.%20We%20accelerate%20the%20computation%20of%20ray%20casting%20correspondences%0A%28RCC%29%20between%20range%20sensors%20and%20mesh%20maps%20by%20supporting%20different%20parallel%0Acomputing%20devices%20like%20multicore%20CPUs%2C%20GPUs%20and%20the%20latest%20NVIDIA%20RTX%20hardware.%0ABy%20additionally%20transforming%20the%20covariance%20computation%20into%20a%20reduction%0Aoperation%2C%20we%20can%20optimize%20the%20initial%20guessed%20poses%20in%20parallel%20on%20CPUs%20or%0AGPUs%2C%20making%20our%20implementation%20applicable%20in%20real-time%20on%20many%20architectures.%0AWe%20demonstrate%20the%20robustness%20of%20our%20localization%20approach%20with%20datasets%20from%0Aagricultural%2C%20aerial%2C%20and%20automotive%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.13904v4&entry.124074799=Read"},
{"title": "Learning Embeddings with Centroid Triplet Loss for Object Identification\n  in Robotic Grasping", "author": "Anas Gouda and Max Schwarz and Christopher Reining and Sven Behnke and Alice Kirchheim", "abstract": "  Foundation models are a strong trend in deep learning and computer vision.\nThese models serve as a base for applications as they require minor or no\nfurther fine-tuning by developers to integrate into their applications.\nFoundation models for zero-shot object segmentation such as Segment Anything\n(SAM) output segmentation masks from images without any further object\ninformation. When they are followed in a pipeline by an object identification\nmodel, they can perform object detection without training. Here, we focus on\ntraining such an object identification model. A crucial practical aspect for an\nobject identification model is to be flexible in input size. As object\nidentification is an image retrieval problem, a suitable method should handle\nmulti-query multi-gallery situations without constraining the number of input\nimages (e.g. by having fixed-size aggregation layers). The key solution to\ntrain such a model is the centroid triplet loss (CTL), which aggregates image\nfeatures to their centroids. CTL yields high accuracy, avoids misleading\ntraining signals and keeps the model input size flexible. In our experiments,\nwe establish a new state of the art on the ArmBench object identification task,\nwhich shows general applicability of our model. We furthermore demonstrate an\nintegrated unseen object detection pipeline on the challenging HOPE dataset,\nwhich requires fine-grained detection. There, our pipeline matches and\nsurpasses related methods which have been trained on dataset-specific data.\n", "link": "http://arxiv.org/abs/2404.06277v2", "date": "2024-07-08", "relevancy": 2.8032, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5731}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5635}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Embeddings%20with%20Centroid%20Triplet%20Loss%20for%20Object%20Identification%0A%20%20in%20Robotic%20Grasping&body=Title%3A%20Learning%20Embeddings%20with%20Centroid%20Triplet%20Loss%20for%20Object%20Identification%0A%20%20in%20Robotic%20Grasping%0AAuthor%3A%20Anas%20Gouda%20and%20Max%20Schwarz%20and%20Christopher%20Reining%20and%20Sven%20Behnke%20and%20Alice%20Kirchheim%0AAbstract%3A%20%20%20Foundation%20models%20are%20a%20strong%20trend%20in%20deep%20learning%20and%20computer%20vision.%0AThese%20models%20serve%20as%20a%20base%20for%20applications%20as%20they%20require%20minor%20or%20no%0Afurther%20fine-tuning%20by%20developers%20to%20integrate%20into%20their%20applications.%0AFoundation%20models%20for%20zero-shot%20object%20segmentation%20such%20as%20Segment%20Anything%0A%28SAM%29%20output%20segmentation%20masks%20from%20images%20without%20any%20further%20object%0Ainformation.%20When%20they%20are%20followed%20in%20a%20pipeline%20by%20an%20object%20identification%0Amodel%2C%20they%20can%20perform%20object%20detection%20without%20training.%20Here%2C%20we%20focus%20on%0Atraining%20such%20an%20object%20identification%20model.%20A%20crucial%20practical%20aspect%20for%20an%0Aobject%20identification%20model%20is%20to%20be%20flexible%20in%20input%20size.%20As%20object%0Aidentification%20is%20an%20image%20retrieval%20problem%2C%20a%20suitable%20method%20should%20handle%0Amulti-query%20multi-gallery%20situations%20without%20constraining%20the%20number%20of%20input%0Aimages%20%28e.g.%20by%20having%20fixed-size%20aggregation%20layers%29.%20The%20key%20solution%20to%0Atrain%20such%20a%20model%20is%20the%20centroid%20triplet%20loss%20%28CTL%29%2C%20which%20aggregates%20image%0Afeatures%20to%20their%20centroids.%20CTL%20yields%20high%20accuracy%2C%20avoids%20misleading%0Atraining%20signals%20and%20keeps%20the%20model%20input%20size%20flexible.%20In%20our%20experiments%2C%0Awe%20establish%20a%20new%20state%20of%20the%20art%20on%20the%20ArmBench%20object%20identification%20task%2C%0Awhich%20shows%20general%20applicability%20of%20our%20model.%20We%20furthermore%20demonstrate%20an%0Aintegrated%20unseen%20object%20detection%20pipeline%20on%20the%20challenging%20HOPE%20dataset%2C%0Awhich%20requires%20fine-grained%20detection.%20There%2C%20our%20pipeline%20matches%20and%0Asurpasses%20related%20methods%20which%20have%20been%20trained%20on%20dataset-specific%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Embeddings%2520with%2520Centroid%2520Triplet%2520Loss%2520for%2520Object%2520Identification%250A%2520%2520in%2520Robotic%2520Grasping%26entry.906535625%3DAnas%2520Gouda%2520and%2520Max%2520Schwarz%2520and%2520Christopher%2520Reining%2520and%2520Sven%2520Behnke%2520and%2520Alice%2520Kirchheim%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520a%2520strong%2520trend%2520in%2520deep%2520learning%2520and%2520computer%2520vision.%250AThese%2520models%2520serve%2520as%2520a%2520base%2520for%2520applications%2520as%2520they%2520require%2520minor%2520or%2520no%250Afurther%2520fine-tuning%2520by%2520developers%2520to%2520integrate%2520into%2520their%2520applications.%250AFoundation%2520models%2520for%2520zero-shot%2520object%2520segmentation%2520such%2520as%2520Segment%2520Anything%250A%2528SAM%2529%2520output%2520segmentation%2520masks%2520from%2520images%2520without%2520any%2520further%2520object%250Ainformation.%2520When%2520they%2520are%2520followed%2520in%2520a%2520pipeline%2520by%2520an%2520object%2520identification%250Amodel%252C%2520they%2520can%2520perform%2520object%2520detection%2520without%2520training.%2520Here%252C%2520we%2520focus%2520on%250Atraining%2520such%2520an%2520object%2520identification%2520model.%2520A%2520crucial%2520practical%2520aspect%2520for%2520an%250Aobject%2520identification%2520model%2520is%2520to%2520be%2520flexible%2520in%2520input%2520size.%2520As%2520object%250Aidentification%2520is%2520an%2520image%2520retrieval%2520problem%252C%2520a%2520suitable%2520method%2520should%2520handle%250Amulti-query%2520multi-gallery%2520situations%2520without%2520constraining%2520the%2520number%2520of%2520input%250Aimages%2520%2528e.g.%2520by%2520having%2520fixed-size%2520aggregation%2520layers%2529.%2520The%2520key%2520solution%2520to%250Atrain%2520such%2520a%2520model%2520is%2520the%2520centroid%2520triplet%2520loss%2520%2528CTL%2529%252C%2520which%2520aggregates%2520image%250Afeatures%2520to%2520their%2520centroids.%2520CTL%2520yields%2520high%2520accuracy%252C%2520avoids%2520misleading%250Atraining%2520signals%2520and%2520keeps%2520the%2520model%2520input%2520size%2520flexible.%2520In%2520our%2520experiments%252C%250Awe%2520establish%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520the%2520ArmBench%2520object%2520identification%2520task%252C%250Awhich%2520shows%2520general%2520applicability%2520of%2520our%2520model.%2520We%2520furthermore%2520demonstrate%2520an%250Aintegrated%2520unseen%2520object%2520detection%2520pipeline%2520on%2520the%2520challenging%2520HOPE%2520dataset%252C%250Awhich%2520requires%2520fine-grained%2520detection.%2520There%252C%2520our%2520pipeline%2520matches%2520and%250Asurpasses%2520related%2520methods%2520which%2520have%2520been%2520trained%2520on%2520dataset-specific%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Embeddings%20with%20Centroid%20Triplet%20Loss%20for%20Object%20Identification%0A%20%20in%20Robotic%20Grasping&entry.906535625=Anas%20Gouda%20and%20Max%20Schwarz%20and%20Christopher%20Reining%20and%20Sven%20Behnke%20and%20Alice%20Kirchheim&entry.1292438233=%20%20Foundation%20models%20are%20a%20strong%20trend%20in%20deep%20learning%20and%20computer%20vision.%0AThese%20models%20serve%20as%20a%20base%20for%20applications%20as%20they%20require%20minor%20or%20no%0Afurther%20fine-tuning%20by%20developers%20to%20integrate%20into%20their%20applications.%0AFoundation%20models%20for%20zero-shot%20object%20segmentation%20such%20as%20Segment%20Anything%0A%28SAM%29%20output%20segmentation%20masks%20from%20images%20without%20any%20further%20object%0Ainformation.%20When%20they%20are%20followed%20in%20a%20pipeline%20by%20an%20object%20identification%0Amodel%2C%20they%20can%20perform%20object%20detection%20without%20training.%20Here%2C%20we%20focus%20on%0Atraining%20such%20an%20object%20identification%20model.%20A%20crucial%20practical%20aspect%20for%20an%0Aobject%20identification%20model%20is%20to%20be%20flexible%20in%20input%20size.%20As%20object%0Aidentification%20is%20an%20image%20retrieval%20problem%2C%20a%20suitable%20method%20should%20handle%0Amulti-query%20multi-gallery%20situations%20without%20constraining%20the%20number%20of%20input%0Aimages%20%28e.g.%20by%20having%20fixed-size%20aggregation%20layers%29.%20The%20key%20solution%20to%0Atrain%20such%20a%20model%20is%20the%20centroid%20triplet%20loss%20%28CTL%29%2C%20which%20aggregates%20image%0Afeatures%20to%20their%20centroids.%20CTL%20yields%20high%20accuracy%2C%20avoids%20misleading%0Atraining%20signals%20and%20keeps%20the%20model%20input%20size%20flexible.%20In%20our%20experiments%2C%0Awe%20establish%20a%20new%20state%20of%20the%20art%20on%20the%20ArmBench%20object%20identification%20task%2C%0Awhich%20shows%20general%20applicability%20of%20our%20model.%20We%20furthermore%20demonstrate%20an%0Aintegrated%20unseen%20object%20detection%20pipeline%20on%20the%20challenging%20HOPE%20dataset%2C%0Awhich%20requires%20fine-grained%20detection.%20There%2C%20our%20pipeline%20matches%20and%0Asurpasses%20related%20methods%20which%20have%20been%20trained%20on%20dataset-specific%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06277v2&entry.124074799=Read"},
{"title": "3D Vessel Graph Generation Using Denoising Diffusion", "author": "Chinmay Prabhakar and Suprosanna Shit and Fabio Musio and Kaiyuan Yang and Tamaz Amiranashvili and Johannes C. Paetzold and Hongwei Bran Li and Bjoern Menze", "abstract": "  Blood vessel networks, represented as 3D graphs, help predict disease\nbiomarkers, simulate blood flow, and aid in synthetic image generation,\nrelevant in both clinical and pre-clinical settings. However, generating\nrealistic vessel graphs that correspond to an anatomy of interest is\nchallenging. Previous methods aimed at generating vessel trees mostly in an\nautoregressive style and could not be applied to vessel graphs with cycles such\nas capillaries or specific anatomical structures such as the Circle of Willis.\nAddressing this gap, we introduce the first application of \\textit{denoising\ndiffusion models} in 3D vessel graph generation. Our contributions include a\nnovel, two-stage generation method that sequentially denoises node coordinates\nand edges. We experiment with two real-world vessel datasets, consisting of\nmicroscopic capillaries and major cerebral vessels, and demonstrate the\ngeneralizability of our method for producing diverse, novel, and anatomically\nplausible vessel graphs.\n", "link": "http://arxiv.org/abs/2407.05842v1", "date": "2024-07-08", "relevancy": 2.7743, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5588}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5588}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Vessel%20Graph%20Generation%20Using%20Denoising%20Diffusion&body=Title%3A%203D%20Vessel%20Graph%20Generation%20Using%20Denoising%20Diffusion%0AAuthor%3A%20Chinmay%20Prabhakar%20and%20Suprosanna%20Shit%20and%20Fabio%20Musio%20and%20Kaiyuan%20Yang%20and%20Tamaz%20Amiranashvili%20and%20Johannes%20C.%20Paetzold%20and%20Hongwei%20Bran%20Li%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20Blood%20vessel%20networks%2C%20represented%20as%203D%20graphs%2C%20help%20predict%20disease%0Abiomarkers%2C%20simulate%20blood%20flow%2C%20and%20aid%20in%20synthetic%20image%20generation%2C%0Arelevant%20in%20both%20clinical%20and%20pre-clinical%20settings.%20However%2C%20generating%0Arealistic%20vessel%20graphs%20that%20correspond%20to%20an%20anatomy%20of%20interest%20is%0Achallenging.%20Previous%20methods%20aimed%20at%20generating%20vessel%20trees%20mostly%20in%20an%0Aautoregressive%20style%20and%20could%20not%20be%20applied%20to%20vessel%20graphs%20with%20cycles%20such%0Aas%20capillaries%20or%20specific%20anatomical%20structures%20such%20as%20the%20Circle%20of%20Willis.%0AAddressing%20this%20gap%2C%20we%20introduce%20the%20first%20application%20of%20%5Ctextit%7Bdenoising%0Adiffusion%20models%7D%20in%203D%20vessel%20graph%20generation.%20Our%20contributions%20include%20a%0Anovel%2C%20two-stage%20generation%20method%20that%20sequentially%20denoises%20node%20coordinates%0Aand%20edges.%20We%20experiment%20with%20two%20real-world%20vessel%20datasets%2C%20consisting%20of%0Amicroscopic%20capillaries%20and%20major%20cerebral%20vessels%2C%20and%20demonstrate%20the%0Ageneralizability%20of%20our%20method%20for%20producing%20diverse%2C%20novel%2C%20and%20anatomically%0Aplausible%20vessel%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Vessel%2520Graph%2520Generation%2520Using%2520Denoising%2520Diffusion%26entry.906535625%3DChinmay%2520Prabhakar%2520and%2520Suprosanna%2520Shit%2520and%2520Fabio%2520Musio%2520and%2520Kaiyuan%2520Yang%2520and%2520Tamaz%2520Amiranashvili%2520and%2520Johannes%2520C.%2520Paetzold%2520and%2520Hongwei%2520Bran%2520Li%2520and%2520Bjoern%2520Menze%26entry.1292438233%3D%2520%2520Blood%2520vessel%2520networks%252C%2520represented%2520as%25203D%2520graphs%252C%2520help%2520predict%2520disease%250Abiomarkers%252C%2520simulate%2520blood%2520flow%252C%2520and%2520aid%2520in%2520synthetic%2520image%2520generation%252C%250Arelevant%2520in%2520both%2520clinical%2520and%2520pre-clinical%2520settings.%2520However%252C%2520generating%250Arealistic%2520vessel%2520graphs%2520that%2520correspond%2520to%2520an%2520anatomy%2520of%2520interest%2520is%250Achallenging.%2520Previous%2520methods%2520aimed%2520at%2520generating%2520vessel%2520trees%2520mostly%2520in%2520an%250Aautoregressive%2520style%2520and%2520could%2520not%2520be%2520applied%2520to%2520vessel%2520graphs%2520with%2520cycles%2520such%250Aas%2520capillaries%2520or%2520specific%2520anatomical%2520structures%2520such%2520as%2520the%2520Circle%2520of%2520Willis.%250AAddressing%2520this%2520gap%252C%2520we%2520introduce%2520the%2520first%2520application%2520of%2520%255Ctextit%257Bdenoising%250Adiffusion%2520models%257D%2520in%25203D%2520vessel%2520graph%2520generation.%2520Our%2520contributions%2520include%2520a%250Anovel%252C%2520two-stage%2520generation%2520method%2520that%2520sequentially%2520denoises%2520node%2520coordinates%250Aand%2520edges.%2520We%2520experiment%2520with%2520two%2520real-world%2520vessel%2520datasets%252C%2520consisting%2520of%250Amicroscopic%2520capillaries%2520and%2520major%2520cerebral%2520vessels%252C%2520and%2520demonstrate%2520the%250Ageneralizability%2520of%2520our%2520method%2520for%2520producing%2520diverse%252C%2520novel%252C%2520and%2520anatomically%250Aplausible%2520vessel%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Vessel%20Graph%20Generation%20Using%20Denoising%20Diffusion&entry.906535625=Chinmay%20Prabhakar%20and%20Suprosanna%20Shit%20and%20Fabio%20Musio%20and%20Kaiyuan%20Yang%20and%20Tamaz%20Amiranashvili%20and%20Johannes%20C.%20Paetzold%20and%20Hongwei%20Bran%20Li%20and%20Bjoern%20Menze&entry.1292438233=%20%20Blood%20vessel%20networks%2C%20represented%20as%203D%20graphs%2C%20help%20predict%20disease%0Abiomarkers%2C%20simulate%20blood%20flow%2C%20and%20aid%20in%20synthetic%20image%20generation%2C%0Arelevant%20in%20both%20clinical%20and%20pre-clinical%20settings.%20However%2C%20generating%0Arealistic%20vessel%20graphs%20that%20correspond%20to%20an%20anatomy%20of%20interest%20is%0Achallenging.%20Previous%20methods%20aimed%20at%20generating%20vessel%20trees%20mostly%20in%20an%0Aautoregressive%20style%20and%20could%20not%20be%20applied%20to%20vessel%20graphs%20with%20cycles%20such%0Aas%20capillaries%20or%20specific%20anatomical%20structures%20such%20as%20the%20Circle%20of%20Willis.%0AAddressing%20this%20gap%2C%20we%20introduce%20the%20first%20application%20of%20%5Ctextit%7Bdenoising%0Adiffusion%20models%7D%20in%203D%20vessel%20graph%20generation.%20Our%20contributions%20include%20a%0Anovel%2C%20two-stage%20generation%20method%20that%20sequentially%20denoises%20node%20coordinates%0Aand%20edges.%20We%20experiment%20with%20two%20real-world%20vessel%20datasets%2C%20consisting%20of%0Amicroscopic%20capillaries%20and%20major%20cerebral%20vessels%2C%20and%20demonstrate%20the%0Ageneralizability%20of%20our%20method%20for%20producing%20diverse%2C%20novel%2C%20and%20anatomically%0Aplausible%20vessel%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05842v1&entry.124074799=Read"},
{"title": "PanDORA: Casual HDR Radiance Acquisition for Indoor Scenes", "author": "Mohammad Reza Karimi Dastjerdi and Fr\u00e9d\u00e9ric Fortier-Chouinard and Yannick Hold-Geoffroy and Marc H\u00e9bert and Claude Demers and Nima Kalantari and Jean-Fran\u00e7ois Lalonde", "abstract": "  Most novel view synthesis methods such as NeRF are unable to capture the true\nhigh dynamic range (HDR) radiance of scenes since they are typically trained on\nphotos captured with standard low dynamic range (LDR) cameras. While the\ntraditional exposure bracketing approach which captures several images at\ndifferent exposures has recently been adapted to the multi-view case, we find\nsuch methods to fall short of capturing the full dynamic range of indoor\nscenes, which includes very bright light sources. In this paper, we present\nPanDORA: a PANoramic Dual-Observer Radiance Acquisition system for the casual\ncapture of indoor scenes in high dynamic range. Our proposed system comprises\ntwo 360{\\deg} cameras rigidly attached to a portable tripod. The cameras\nsimultaneously acquire two 360{\\deg} videos: one at a regular exposure and the\nother at a very fast exposure, allowing a user to simply wave the apparatus\ncasually around the scene in a matter of minutes. The resulting images are fed\nto a NeRF-based algorithm that reconstructs the scene's full high dynamic\nrange. Compared to HDR baselines from previous work, our approach reconstructs\nthe full HDR radiance of indoor scenes without sacrificing the visual quality\nwhile retaining the ease of capture from recent NeRF-like approaches.\n", "link": "http://arxiv.org/abs/2407.06150v1", "date": "2024-07-08", "relevancy": 2.7392, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5522}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5522}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanDORA%3A%20Casual%20HDR%20Radiance%20Acquisition%20for%20Indoor%20Scenes&body=Title%3A%20PanDORA%3A%20Casual%20HDR%20Radiance%20Acquisition%20for%20Indoor%20Scenes%0AAuthor%3A%20Mohammad%20Reza%20Karimi%20Dastjerdi%20and%20Fr%C3%A9d%C3%A9ric%20Fortier-Chouinard%20and%20Yannick%20Hold-Geoffroy%20and%20Marc%20H%C3%A9bert%20and%20Claude%20Demers%20and%20Nima%20Kalantari%20and%20Jean-Fran%C3%A7ois%20Lalonde%0AAbstract%3A%20%20%20Most%20novel%20view%20synthesis%20methods%20such%20as%20NeRF%20are%20unable%20to%20capture%20the%20true%0Ahigh%20dynamic%20range%20%28HDR%29%20radiance%20of%20scenes%20since%20they%20are%20typically%20trained%20on%0Aphotos%20captured%20with%20standard%20low%20dynamic%20range%20%28LDR%29%20cameras.%20While%20the%0Atraditional%20exposure%20bracketing%20approach%20which%20captures%20several%20images%20at%0Adifferent%20exposures%20has%20recently%20been%20adapted%20to%20the%20multi-view%20case%2C%20we%20find%0Asuch%20methods%20to%20fall%20short%20of%20capturing%20the%20full%20dynamic%20range%20of%20indoor%0Ascenes%2C%20which%20includes%20very%20bright%20light%20sources.%20In%20this%20paper%2C%20we%20present%0APanDORA%3A%20a%20PANoramic%20Dual-Observer%20Radiance%20Acquisition%20system%20for%20the%20casual%0Acapture%20of%20indoor%20scenes%20in%20high%20dynamic%20range.%20Our%20proposed%20system%20comprises%0Atwo%20360%7B%5Cdeg%7D%20cameras%20rigidly%20attached%20to%20a%20portable%20tripod.%20The%20cameras%0Asimultaneously%20acquire%20two%20360%7B%5Cdeg%7D%20videos%3A%20one%20at%20a%20regular%20exposure%20and%20the%0Aother%20at%20a%20very%20fast%20exposure%2C%20allowing%20a%20user%20to%20simply%20wave%20the%20apparatus%0Acasually%20around%20the%20scene%20in%20a%20matter%20of%20minutes.%20The%20resulting%20images%20are%20fed%0Ato%20a%20NeRF-based%20algorithm%20that%20reconstructs%20the%20scene%27s%20full%20high%20dynamic%0Arange.%20Compared%20to%20HDR%20baselines%20from%20previous%20work%2C%20our%20approach%20reconstructs%0Athe%20full%20HDR%20radiance%20of%20indoor%20scenes%20without%20sacrificing%20the%20visual%20quality%0Awhile%20retaining%20the%20ease%20of%20capture%20from%20recent%20NeRF-like%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanDORA%253A%2520Casual%2520HDR%2520Radiance%2520Acquisition%2520for%2520Indoor%2520Scenes%26entry.906535625%3DMohammad%2520Reza%2520Karimi%2520Dastjerdi%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Fortier-Chouinard%2520and%2520Yannick%2520Hold-Geoffroy%2520and%2520Marc%2520H%25C3%25A9bert%2520and%2520Claude%2520Demers%2520and%2520Nima%2520Kalantari%2520and%2520Jean-Fran%25C3%25A7ois%2520Lalonde%26entry.1292438233%3D%2520%2520Most%2520novel%2520view%2520synthesis%2520methods%2520such%2520as%2520NeRF%2520are%2520unable%2520to%2520capture%2520the%2520true%250Ahigh%2520dynamic%2520range%2520%2528HDR%2529%2520radiance%2520of%2520scenes%2520since%2520they%2520are%2520typically%2520trained%2520on%250Aphotos%2520captured%2520with%2520standard%2520low%2520dynamic%2520range%2520%2528LDR%2529%2520cameras.%2520While%2520the%250Atraditional%2520exposure%2520bracketing%2520approach%2520which%2520captures%2520several%2520images%2520at%250Adifferent%2520exposures%2520has%2520recently%2520been%2520adapted%2520to%2520the%2520multi-view%2520case%252C%2520we%2520find%250Asuch%2520methods%2520to%2520fall%2520short%2520of%2520capturing%2520the%2520full%2520dynamic%2520range%2520of%2520indoor%250Ascenes%252C%2520which%2520includes%2520very%2520bright%2520light%2520sources.%2520In%2520this%2520paper%252C%2520we%2520present%250APanDORA%253A%2520a%2520PANoramic%2520Dual-Observer%2520Radiance%2520Acquisition%2520system%2520for%2520the%2520casual%250Acapture%2520of%2520indoor%2520scenes%2520in%2520high%2520dynamic%2520range.%2520Our%2520proposed%2520system%2520comprises%250Atwo%2520360%257B%255Cdeg%257D%2520cameras%2520rigidly%2520attached%2520to%2520a%2520portable%2520tripod.%2520The%2520cameras%250Asimultaneously%2520acquire%2520two%2520360%257B%255Cdeg%257D%2520videos%253A%2520one%2520at%2520a%2520regular%2520exposure%2520and%2520the%250Aother%2520at%2520a%2520very%2520fast%2520exposure%252C%2520allowing%2520a%2520user%2520to%2520simply%2520wave%2520the%2520apparatus%250Acasually%2520around%2520the%2520scene%2520in%2520a%2520matter%2520of%2520minutes.%2520The%2520resulting%2520images%2520are%2520fed%250Ato%2520a%2520NeRF-based%2520algorithm%2520that%2520reconstructs%2520the%2520scene%2527s%2520full%2520high%2520dynamic%250Arange.%2520Compared%2520to%2520HDR%2520baselines%2520from%2520previous%2520work%252C%2520our%2520approach%2520reconstructs%250Athe%2520full%2520HDR%2520radiance%2520of%2520indoor%2520scenes%2520without%2520sacrificing%2520the%2520visual%2520quality%250Awhile%2520retaining%2520the%2520ease%2520of%2520capture%2520from%2520recent%2520NeRF-like%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanDORA%3A%20Casual%20HDR%20Radiance%20Acquisition%20for%20Indoor%20Scenes&entry.906535625=Mohammad%20Reza%20Karimi%20Dastjerdi%20and%20Fr%C3%A9d%C3%A9ric%20Fortier-Chouinard%20and%20Yannick%20Hold-Geoffroy%20and%20Marc%20H%C3%A9bert%20and%20Claude%20Demers%20and%20Nima%20Kalantari%20and%20Jean-Fran%C3%A7ois%20Lalonde&entry.1292438233=%20%20Most%20novel%20view%20synthesis%20methods%20such%20as%20NeRF%20are%20unable%20to%20capture%20the%20true%0Ahigh%20dynamic%20range%20%28HDR%29%20radiance%20of%20scenes%20since%20they%20are%20typically%20trained%20on%0Aphotos%20captured%20with%20standard%20low%20dynamic%20range%20%28LDR%29%20cameras.%20While%20the%0Atraditional%20exposure%20bracketing%20approach%20which%20captures%20several%20images%20at%0Adifferent%20exposures%20has%20recently%20been%20adapted%20to%20the%20multi-view%20case%2C%20we%20find%0Asuch%20methods%20to%20fall%20short%20of%20capturing%20the%20full%20dynamic%20range%20of%20indoor%0Ascenes%2C%20which%20includes%20very%20bright%20light%20sources.%20In%20this%20paper%2C%20we%20present%0APanDORA%3A%20a%20PANoramic%20Dual-Observer%20Radiance%20Acquisition%20system%20for%20the%20casual%0Acapture%20of%20indoor%20scenes%20in%20high%20dynamic%20range.%20Our%20proposed%20system%20comprises%0Atwo%20360%7B%5Cdeg%7D%20cameras%20rigidly%20attached%20to%20a%20portable%20tripod.%20The%20cameras%0Asimultaneously%20acquire%20two%20360%7B%5Cdeg%7D%20videos%3A%20one%20at%20a%20regular%20exposure%20and%20the%0Aother%20at%20a%20very%20fast%20exposure%2C%20allowing%20a%20user%20to%20simply%20wave%20the%20apparatus%0Acasually%20around%20the%20scene%20in%20a%20matter%20of%20minutes.%20The%20resulting%20images%20are%20fed%0Ato%20a%20NeRF-based%20algorithm%20that%20reconstructs%20the%20scene%27s%20full%20high%20dynamic%0Arange.%20Compared%20to%20HDR%20baselines%20from%20previous%20work%2C%20our%20approach%20reconstructs%0Athe%20full%20HDR%20radiance%20of%20indoor%20scenes%20without%20sacrificing%20the%20visual%20quality%0Awhile%20retaining%20the%20ease%20of%20capture%20from%20recent%20NeRF-like%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06150v1&entry.124074799=Read"},
{"title": "Cross-domain Few-shot In-context Learning for Enhancing Traffic Sign\n  Recognition", "author": "Yaozong Gan and Guang Li and Ren Togo and Keisuke Maeda and Takahiro Ogawa and Miki Haseyama", "abstract": "  Recent multimodal large language models (MLLM) such as GPT-4o and GPT-4v have\nshown great potential in autonomous driving. In this paper, we propose a\ncross-domain few-shot in-context learning method based on the MLLM for\nenhancing traffic sign recognition (TSR). We first construct a traffic sign\ndetection network based on Vision Transformer Adapter and an extraction module\nto extract traffic signs from the original road images. To reduce the\ndependence on training data and improve the performance stability of\ncross-country TSR, we introduce a cross-domain few-shot in-context learning\nmethod based on the MLLM. To enhance MLLM's fine-grained recognition ability of\ntraffic signs, the proposed method generates corresponding description texts\nusing template traffic signs. These description texts contain key information\nabout the shape, color, and composition of traffic signs, which can stimulate\nthe ability of MLLM to perceive fine-grained traffic sign categories. By using\nthe description texts, our method reduces the cross-domain differences between\ntemplate and real traffic signs. Our approach requires only simple and uniform\ntextual indications, without the need for large-scale traffic sign images and\nlabels. We perform comprehensive evaluations on the German traffic sign\nrecognition benchmark dataset, the Belgium traffic sign dataset, and two\nreal-world datasets taken from Japan. The experimental results show that our\nmethod significantly enhances the TSR performance.\n", "link": "http://arxiv.org/abs/2407.05814v1", "date": "2024-07-08", "relevancy": 2.7065, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5753}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5372}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-domain%20Few-shot%20In-context%20Learning%20for%20Enhancing%20Traffic%20Sign%0A%20%20Recognition&body=Title%3A%20Cross-domain%20Few-shot%20In-context%20Learning%20for%20Enhancing%20Traffic%20Sign%0A%20%20Recognition%0AAuthor%3A%20Yaozong%20Gan%20and%20Guang%20Li%20and%20Ren%20Togo%20and%20Keisuke%20Maeda%20and%20Takahiro%20Ogawa%20and%20Miki%20Haseyama%0AAbstract%3A%20%20%20Recent%20multimodal%20large%20language%20models%20%28MLLM%29%20such%20as%20GPT-4o%20and%20GPT-4v%20have%0Ashown%20great%20potential%20in%20autonomous%20driving.%20In%20this%20paper%2C%20we%20propose%20a%0Across-domain%20few-shot%20in-context%20learning%20method%20based%20on%20the%20MLLM%20for%0Aenhancing%20traffic%20sign%20recognition%20%28TSR%29.%20We%20first%20construct%20a%20traffic%20sign%0Adetection%20network%20based%20on%20Vision%20Transformer%20Adapter%20and%20an%20extraction%20module%0Ato%20extract%20traffic%20signs%20from%20the%20original%20road%20images.%20To%20reduce%20the%0Adependence%20on%20training%20data%20and%20improve%20the%20performance%20stability%20of%0Across-country%20TSR%2C%20we%20introduce%20a%20cross-domain%20few-shot%20in-context%20learning%0Amethod%20based%20on%20the%20MLLM.%20To%20enhance%20MLLM%27s%20fine-grained%20recognition%20ability%20of%0Atraffic%20signs%2C%20the%20proposed%20method%20generates%20corresponding%20description%20texts%0Ausing%20template%20traffic%20signs.%20These%20description%20texts%20contain%20key%20information%0Aabout%20the%20shape%2C%20color%2C%20and%20composition%20of%20traffic%20signs%2C%20which%20can%20stimulate%0Athe%20ability%20of%20MLLM%20to%20perceive%20fine-grained%20traffic%20sign%20categories.%20By%20using%0Athe%20description%20texts%2C%20our%20method%20reduces%20the%20cross-domain%20differences%20between%0Atemplate%20and%20real%20traffic%20signs.%20Our%20approach%20requires%20only%20simple%20and%20uniform%0Atextual%20indications%2C%20without%20the%20need%20for%20large-scale%20traffic%20sign%20images%20and%0Alabels.%20We%20perform%20comprehensive%20evaluations%20on%20the%20German%20traffic%20sign%0Arecognition%20benchmark%20dataset%2C%20the%20Belgium%20traffic%20sign%20dataset%2C%20and%20two%0Areal-world%20datasets%20taken%20from%20Japan.%20The%20experimental%20results%20show%20that%20our%0Amethod%20significantly%20enhances%20the%20TSR%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-domain%2520Few-shot%2520In-context%2520Learning%2520for%2520Enhancing%2520Traffic%2520Sign%250A%2520%2520Recognition%26entry.906535625%3DYaozong%2520Gan%2520and%2520Guang%2520Li%2520and%2520Ren%2520Togo%2520and%2520Keisuke%2520Maeda%2520and%2520Takahiro%2520Ogawa%2520and%2520Miki%2520Haseyama%26entry.1292438233%3D%2520%2520Recent%2520multimodal%2520large%2520language%2520models%2520%2528MLLM%2529%2520such%2520as%2520GPT-4o%2520and%2520GPT-4v%2520have%250Ashown%2520great%2520potential%2520in%2520autonomous%2520driving.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Across-domain%2520few-shot%2520in-context%2520learning%2520method%2520based%2520on%2520the%2520MLLM%2520for%250Aenhancing%2520traffic%2520sign%2520recognition%2520%2528TSR%2529.%2520We%2520first%2520construct%2520a%2520traffic%2520sign%250Adetection%2520network%2520based%2520on%2520Vision%2520Transformer%2520Adapter%2520and%2520an%2520extraction%2520module%250Ato%2520extract%2520traffic%2520signs%2520from%2520the%2520original%2520road%2520images.%2520To%2520reduce%2520the%250Adependence%2520on%2520training%2520data%2520and%2520improve%2520the%2520performance%2520stability%2520of%250Across-country%2520TSR%252C%2520we%2520introduce%2520a%2520cross-domain%2520few-shot%2520in-context%2520learning%250Amethod%2520based%2520on%2520the%2520MLLM.%2520To%2520enhance%2520MLLM%2527s%2520fine-grained%2520recognition%2520ability%2520of%250Atraffic%2520signs%252C%2520the%2520proposed%2520method%2520generates%2520corresponding%2520description%2520texts%250Ausing%2520template%2520traffic%2520signs.%2520These%2520description%2520texts%2520contain%2520key%2520information%250Aabout%2520the%2520shape%252C%2520color%252C%2520and%2520composition%2520of%2520traffic%2520signs%252C%2520which%2520can%2520stimulate%250Athe%2520ability%2520of%2520MLLM%2520to%2520perceive%2520fine-grained%2520traffic%2520sign%2520categories.%2520By%2520using%250Athe%2520description%2520texts%252C%2520our%2520method%2520reduces%2520the%2520cross-domain%2520differences%2520between%250Atemplate%2520and%2520real%2520traffic%2520signs.%2520Our%2520approach%2520requires%2520only%2520simple%2520and%2520uniform%250Atextual%2520indications%252C%2520without%2520the%2520need%2520for%2520large-scale%2520traffic%2520sign%2520images%2520and%250Alabels.%2520We%2520perform%2520comprehensive%2520evaluations%2520on%2520the%2520German%2520traffic%2520sign%250Arecognition%2520benchmark%2520dataset%252C%2520the%2520Belgium%2520traffic%2520sign%2520dataset%252C%2520and%2520two%250Areal-world%2520datasets%2520taken%2520from%2520Japan.%2520The%2520experimental%2520results%2520show%2520that%2520our%250Amethod%2520significantly%2520enhances%2520the%2520TSR%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-domain%20Few-shot%20In-context%20Learning%20for%20Enhancing%20Traffic%20Sign%0A%20%20Recognition&entry.906535625=Yaozong%20Gan%20and%20Guang%20Li%20and%20Ren%20Togo%20and%20Keisuke%20Maeda%20and%20Takahiro%20Ogawa%20and%20Miki%20Haseyama&entry.1292438233=%20%20Recent%20multimodal%20large%20language%20models%20%28MLLM%29%20such%20as%20GPT-4o%20and%20GPT-4v%20have%0Ashown%20great%20potential%20in%20autonomous%20driving.%20In%20this%20paper%2C%20we%20propose%20a%0Across-domain%20few-shot%20in-context%20learning%20method%20based%20on%20the%20MLLM%20for%0Aenhancing%20traffic%20sign%20recognition%20%28TSR%29.%20We%20first%20construct%20a%20traffic%20sign%0Adetection%20network%20based%20on%20Vision%20Transformer%20Adapter%20and%20an%20extraction%20module%0Ato%20extract%20traffic%20signs%20from%20the%20original%20road%20images.%20To%20reduce%20the%0Adependence%20on%20training%20data%20and%20improve%20the%20performance%20stability%20of%0Across-country%20TSR%2C%20we%20introduce%20a%20cross-domain%20few-shot%20in-context%20learning%0Amethod%20based%20on%20the%20MLLM.%20To%20enhance%20MLLM%27s%20fine-grained%20recognition%20ability%20of%0Atraffic%20signs%2C%20the%20proposed%20method%20generates%20corresponding%20description%20texts%0Ausing%20template%20traffic%20signs.%20These%20description%20texts%20contain%20key%20information%0Aabout%20the%20shape%2C%20color%2C%20and%20composition%20of%20traffic%20signs%2C%20which%20can%20stimulate%0Athe%20ability%20of%20MLLM%20to%20perceive%20fine-grained%20traffic%20sign%20categories.%20By%20using%0Athe%20description%20texts%2C%20our%20method%20reduces%20the%20cross-domain%20differences%20between%0Atemplate%20and%20real%20traffic%20signs.%20Our%20approach%20requires%20only%20simple%20and%20uniform%0Atextual%20indications%2C%20without%20the%20need%20for%20large-scale%20traffic%20sign%20images%20and%0Alabels.%20We%20perform%20comprehensive%20evaluations%20on%20the%20German%20traffic%20sign%0Arecognition%20benchmark%20dataset%2C%20the%20Belgium%20traffic%20sign%20dataset%2C%20and%20two%0Areal-world%20datasets%20taken%20from%20Japan.%20The%20experimental%20results%20show%20that%20our%0Amethod%20significantly%20enhances%20the%20TSR%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05814v1&entry.124074799=Read"},
{"title": "PartCraft: Crafting Creative Objects by Parts", "author": "Kam Woh Ng and Xiatian Zhu and Yi-Zhe Song and Tao Xiang", "abstract": "  This paper propels creative control in generative visual AI by allowing users\nto \"select\". Departing from traditional text or sketch-based methods, we for\nthe first time allow users to choose visual concepts by parts for their\ncreative endeavors. The outcome is fine-grained generation that precisely\ncaptures selected visual concepts, ensuring a holistically faithful and\nplausible result. To achieve this, we first parse objects into parts through\nunsupervised feature clustering. Then, we encode parts into text tokens and\nintroduce an entropy-based normalized attention loss that operates on them.\nThis loss design enables our model to learn generic prior topology knowledge\nabout object's part composition, and further generalize to novel part\ncompositions to ensure the generation looks holistically faithful. Lastly, we\nemploy a bottleneck encoder to project the part tokens. This not only enhances\nfidelity but also accelerates learning, by leveraging shared knowledge and\nfacilitating information exchange among instances. Visual results in the paper\nand supplementary material showcase the compelling power of PartCraft in\ncrafting highly customized, innovative creations, exemplified by the \"charming\"\nand creative birds. Code is released at https://github.com/kamwoh/partcraft.\n", "link": "http://arxiv.org/abs/2407.04604v2", "date": "2024-07-08", "relevancy": 2.7021, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5599}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.548}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartCraft%3A%20Crafting%20Creative%20Objects%20by%20Parts&body=Title%3A%20PartCraft%3A%20Crafting%20Creative%20Objects%20by%20Parts%0AAuthor%3A%20Kam%20Woh%20Ng%20and%20Xiatian%20Zhu%20and%20Yi-Zhe%20Song%20and%20Tao%20Xiang%0AAbstract%3A%20%20%20This%20paper%20propels%20creative%20control%20in%20generative%20visual%20AI%20by%20allowing%20users%0Ato%20%22select%22.%20Departing%20from%20traditional%20text%20or%20sketch-based%20methods%2C%20we%20for%0Athe%20first%20time%20allow%20users%20to%20choose%20visual%20concepts%20by%20parts%20for%20their%0Acreative%20endeavors.%20The%20outcome%20is%20fine-grained%20generation%20that%20precisely%0Acaptures%20selected%20visual%20concepts%2C%20ensuring%20a%20holistically%20faithful%20and%0Aplausible%20result.%20To%20achieve%20this%2C%20we%20first%20parse%20objects%20into%20parts%20through%0Aunsupervised%20feature%20clustering.%20Then%2C%20we%20encode%20parts%20into%20text%20tokens%20and%0Aintroduce%20an%20entropy-based%20normalized%20attention%20loss%20that%20operates%20on%20them.%0AThis%20loss%20design%20enables%20our%20model%20to%20learn%20generic%20prior%20topology%20knowledge%0Aabout%20object%27s%20part%20composition%2C%20and%20further%20generalize%20to%20novel%20part%0Acompositions%20to%20ensure%20the%20generation%20looks%20holistically%20faithful.%20Lastly%2C%20we%0Aemploy%20a%20bottleneck%20encoder%20to%20project%20the%20part%20tokens.%20This%20not%20only%20enhances%0Afidelity%20but%20also%20accelerates%20learning%2C%20by%20leveraging%20shared%20knowledge%20and%0Afacilitating%20information%20exchange%20among%20instances.%20Visual%20results%20in%20the%20paper%0Aand%20supplementary%20material%20showcase%20the%20compelling%20power%20of%20PartCraft%20in%0Acrafting%20highly%20customized%2C%20innovative%20creations%2C%20exemplified%20by%20the%20%22charming%22%0Aand%20creative%20birds.%20Code%20is%20released%20at%20https%3A//github.com/kamwoh/partcraft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04604v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartCraft%253A%2520Crafting%2520Creative%2520Objects%2520by%2520Parts%26entry.906535625%3DKam%2520Woh%2520Ng%2520and%2520Xiatian%2520Zhu%2520and%2520Yi-Zhe%2520Song%2520and%2520Tao%2520Xiang%26entry.1292438233%3D%2520%2520This%2520paper%2520propels%2520creative%2520control%2520in%2520generative%2520visual%2520AI%2520by%2520allowing%2520users%250Ato%2520%2522select%2522.%2520Departing%2520from%2520traditional%2520text%2520or%2520sketch-based%2520methods%252C%2520we%2520for%250Athe%2520first%2520time%2520allow%2520users%2520to%2520choose%2520visual%2520concepts%2520by%2520parts%2520for%2520their%250Acreative%2520endeavors.%2520The%2520outcome%2520is%2520fine-grained%2520generation%2520that%2520precisely%250Acaptures%2520selected%2520visual%2520concepts%252C%2520ensuring%2520a%2520holistically%2520faithful%2520and%250Aplausible%2520result.%2520To%2520achieve%2520this%252C%2520we%2520first%2520parse%2520objects%2520into%2520parts%2520through%250Aunsupervised%2520feature%2520clustering.%2520Then%252C%2520we%2520encode%2520parts%2520into%2520text%2520tokens%2520and%250Aintroduce%2520an%2520entropy-based%2520normalized%2520attention%2520loss%2520that%2520operates%2520on%2520them.%250AThis%2520loss%2520design%2520enables%2520our%2520model%2520to%2520learn%2520generic%2520prior%2520topology%2520knowledge%250Aabout%2520object%2527s%2520part%2520composition%252C%2520and%2520further%2520generalize%2520to%2520novel%2520part%250Acompositions%2520to%2520ensure%2520the%2520generation%2520looks%2520holistically%2520faithful.%2520Lastly%252C%2520we%250Aemploy%2520a%2520bottleneck%2520encoder%2520to%2520project%2520the%2520part%2520tokens.%2520This%2520not%2520only%2520enhances%250Afidelity%2520but%2520also%2520accelerates%2520learning%252C%2520by%2520leveraging%2520shared%2520knowledge%2520and%250Afacilitating%2520information%2520exchange%2520among%2520instances.%2520Visual%2520results%2520in%2520the%2520paper%250Aand%2520supplementary%2520material%2520showcase%2520the%2520compelling%2520power%2520of%2520PartCraft%2520in%250Acrafting%2520highly%2520customized%252C%2520innovative%2520creations%252C%2520exemplified%2520by%2520the%2520%2522charming%2522%250Aand%2520creative%2520birds.%2520Code%2520is%2520released%2520at%2520https%253A//github.com/kamwoh/partcraft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04604v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartCraft%3A%20Crafting%20Creative%20Objects%20by%20Parts&entry.906535625=Kam%20Woh%20Ng%20and%20Xiatian%20Zhu%20and%20Yi-Zhe%20Song%20and%20Tao%20Xiang&entry.1292438233=%20%20This%20paper%20propels%20creative%20control%20in%20generative%20visual%20AI%20by%20allowing%20users%0Ato%20%22select%22.%20Departing%20from%20traditional%20text%20or%20sketch-based%20methods%2C%20we%20for%0Athe%20first%20time%20allow%20users%20to%20choose%20visual%20concepts%20by%20parts%20for%20their%0Acreative%20endeavors.%20The%20outcome%20is%20fine-grained%20generation%20that%20precisely%0Acaptures%20selected%20visual%20concepts%2C%20ensuring%20a%20holistically%20faithful%20and%0Aplausible%20result.%20To%20achieve%20this%2C%20we%20first%20parse%20objects%20into%20parts%20through%0Aunsupervised%20feature%20clustering.%20Then%2C%20we%20encode%20parts%20into%20text%20tokens%20and%0Aintroduce%20an%20entropy-based%20normalized%20attention%20loss%20that%20operates%20on%20them.%0AThis%20loss%20design%20enables%20our%20model%20to%20learn%20generic%20prior%20topology%20knowledge%0Aabout%20object%27s%20part%20composition%2C%20and%20further%20generalize%20to%20novel%20part%0Acompositions%20to%20ensure%20the%20generation%20looks%20holistically%20faithful.%20Lastly%2C%20we%0Aemploy%20a%20bottleneck%20encoder%20to%20project%20the%20part%20tokens.%20This%20not%20only%20enhances%0Afidelity%20but%20also%20accelerates%20learning%2C%20by%20leveraging%20shared%20knowledge%20and%0Afacilitating%20information%20exchange%20among%20instances.%20Visual%20results%20in%20the%20paper%0Aand%20supplementary%20material%20showcase%20the%20compelling%20power%20of%20PartCraft%20in%0Acrafting%20highly%20customized%2C%20innovative%20creations%2C%20exemplified%20by%20the%20%22charming%22%0Aand%20creative%20birds.%20Code%20is%20released%20at%20https%3A//github.com/kamwoh/partcraft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04604v2&entry.124074799=Read"},
{"title": "Pseudo-triplet Guided Few-shot Composed Image Retrieval", "author": "Bohan Hou and Haoqiang Lin and Haokun Wen and Meng Liu and Xuemeng Song", "abstract": "  Composed Image Retrieval (CIR) is a challenging task that aims to retrieve\nthe target image based on a multimodal query, i.e., a reference image and its\ncorresponding modification text. While previous supervised or zero-shot\nlearning paradigms all fail to strike a good trade-off between time-consuming\nannotation cost and retrieval performance, recent researchers introduced the\ntask of few-shot CIR (FS-CIR) and proposed a textual inversion-based network\nbased on pretrained CLIP model to realize it. Despite its promising\nperformance, the approach suffers from two key limitations: insufficient\nmultimodal query composition training and indiscriminative training triplet\nselection. To address these two limitations, in this work, we propose a novel\ntwo-stage pseudo triplet guided few-shot CIR scheme, dubbed PTG-FSCIR. In the\nfirst stage, we employ a masked training strategy and advanced image caption\ngenerator to construct pseudo triplets from pure image data to enable the model\nto acquire primary knowledge related to multimodal query composition. In the\nsecond stage, based on active learning, we design a pseudo modification\ntext-based query-target distance metric to evaluate the challenging score for\neach unlabeled sample. Meanwhile, we propose a robust top range-based random\nsampling strategy according to the 3-$\\sigma$ rule in statistics, to sample the\nchallenging samples for fine-tuning the pretrained model. Notably, our scheme\nis plug-and-play and compatible with any existing supervised CIR models. We\ntested our scheme across three backbones on three public datasets (i.e.,\nFashionIQ, CIRR, and Birds-to-Words), achieving maximum improvements of 26.4%,\n25.5% and 21.6% respectively, demonstrating our scheme's effectiveness.\n", "link": "http://arxiv.org/abs/2407.06001v1", "date": "2024-07-08", "relevancy": 2.6827, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5625}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5311}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-triplet%20Guided%20Few-shot%20Composed%20Image%20Retrieval&body=Title%3A%20Pseudo-triplet%20Guided%20Few-shot%20Composed%20Image%20Retrieval%0AAuthor%3A%20Bohan%20Hou%20and%20Haoqiang%20Lin%20and%20Haokun%20Wen%20and%20Meng%20Liu%20and%20Xuemeng%20Song%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20challenging%20task%20that%20aims%20to%20retrieve%0Athe%20target%20image%20based%20on%20a%20multimodal%20query%2C%20i.e.%2C%20a%20reference%20image%20and%20its%0Acorresponding%20modification%20text.%20While%20previous%20supervised%20or%20zero-shot%0Alearning%20paradigms%20all%20fail%20to%20strike%20a%20good%20trade-off%20between%20time-consuming%0Aannotation%20cost%20and%20retrieval%20performance%2C%20recent%20researchers%20introduced%20the%0Atask%20of%20few-shot%20CIR%20%28FS-CIR%29%20and%20proposed%20a%20textual%20inversion-based%20network%0Abased%20on%20pretrained%20CLIP%20model%20to%20realize%20it.%20Despite%20its%20promising%0Aperformance%2C%20the%20approach%20suffers%20from%20two%20key%20limitations%3A%20insufficient%0Amultimodal%20query%20composition%20training%20and%20indiscriminative%20training%20triplet%0Aselection.%20To%20address%20these%20two%20limitations%2C%20in%20this%20work%2C%20we%20propose%20a%20novel%0Atwo-stage%20pseudo%20triplet%20guided%20few-shot%20CIR%20scheme%2C%20dubbed%20PTG-FSCIR.%20In%20the%0Afirst%20stage%2C%20we%20employ%20a%20masked%20training%20strategy%20and%20advanced%20image%20caption%0Agenerator%20to%20construct%20pseudo%20triplets%20from%20pure%20image%20data%20to%20enable%20the%20model%0Ato%20acquire%20primary%20knowledge%20related%20to%20multimodal%20query%20composition.%20In%20the%0Asecond%20stage%2C%20based%20on%20active%20learning%2C%20we%20design%20a%20pseudo%20modification%0Atext-based%20query-target%20distance%20metric%20to%20evaluate%20the%20challenging%20score%20for%0Aeach%20unlabeled%20sample.%20Meanwhile%2C%20we%20propose%20a%20robust%20top%20range-based%20random%0Asampling%20strategy%20according%20to%20the%203-%24%5Csigma%24%20rule%20in%20statistics%2C%20to%20sample%20the%0Achallenging%20samples%20for%20fine-tuning%20the%20pretrained%20model.%20Notably%2C%20our%20scheme%0Ais%20plug-and-play%20and%20compatible%20with%20any%20existing%20supervised%20CIR%20models.%20We%0Atested%20our%20scheme%20across%20three%20backbones%20on%20three%20public%20datasets%20%28i.e.%2C%0AFashionIQ%2C%20CIRR%2C%20and%20Birds-to-Words%29%2C%20achieving%20maximum%20improvements%20of%2026.4%25%2C%0A25.5%25%20and%2021.6%25%20respectively%2C%20demonstrating%20our%20scheme%27s%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-triplet%2520Guided%2520Few-shot%2520Composed%2520Image%2520Retrieval%26entry.906535625%3DBohan%2520Hou%2520and%2520Haoqiang%2520Lin%2520and%2520Haokun%2520Wen%2520and%2520Meng%2520Liu%2520and%2520Xuemeng%2520Song%26entry.1292438233%3D%2520%2520Composed%2520Image%2520Retrieval%2520%2528CIR%2529%2520is%2520a%2520challenging%2520task%2520that%2520aims%2520to%2520retrieve%250Athe%2520target%2520image%2520based%2520on%2520a%2520multimodal%2520query%252C%2520i.e.%252C%2520a%2520reference%2520image%2520and%2520its%250Acorresponding%2520modification%2520text.%2520While%2520previous%2520supervised%2520or%2520zero-shot%250Alearning%2520paradigms%2520all%2520fail%2520to%2520strike%2520a%2520good%2520trade-off%2520between%2520time-consuming%250Aannotation%2520cost%2520and%2520retrieval%2520performance%252C%2520recent%2520researchers%2520introduced%2520the%250Atask%2520of%2520few-shot%2520CIR%2520%2528FS-CIR%2529%2520and%2520proposed%2520a%2520textual%2520inversion-based%2520network%250Abased%2520on%2520pretrained%2520CLIP%2520model%2520to%2520realize%2520it.%2520Despite%2520its%2520promising%250Aperformance%252C%2520the%2520approach%2520suffers%2520from%2520two%2520key%2520limitations%253A%2520insufficient%250Amultimodal%2520query%2520composition%2520training%2520and%2520indiscriminative%2520training%2520triplet%250Aselection.%2520To%2520address%2520these%2520two%2520limitations%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Atwo-stage%2520pseudo%2520triplet%2520guided%2520few-shot%2520CIR%2520scheme%252C%2520dubbed%2520PTG-FSCIR.%2520In%2520the%250Afirst%2520stage%252C%2520we%2520employ%2520a%2520masked%2520training%2520strategy%2520and%2520advanced%2520image%2520caption%250Agenerator%2520to%2520construct%2520pseudo%2520triplets%2520from%2520pure%2520image%2520data%2520to%2520enable%2520the%2520model%250Ato%2520acquire%2520primary%2520knowledge%2520related%2520to%2520multimodal%2520query%2520composition.%2520In%2520the%250Asecond%2520stage%252C%2520based%2520on%2520active%2520learning%252C%2520we%2520design%2520a%2520pseudo%2520modification%250Atext-based%2520query-target%2520distance%2520metric%2520to%2520evaluate%2520the%2520challenging%2520score%2520for%250Aeach%2520unlabeled%2520sample.%2520Meanwhile%252C%2520we%2520propose%2520a%2520robust%2520top%2520range-based%2520random%250Asampling%2520strategy%2520according%2520to%2520the%25203-%2524%255Csigma%2524%2520rule%2520in%2520statistics%252C%2520to%2520sample%2520the%250Achallenging%2520samples%2520for%2520fine-tuning%2520the%2520pretrained%2520model.%2520Notably%252C%2520our%2520scheme%250Ais%2520plug-and-play%2520and%2520compatible%2520with%2520any%2520existing%2520supervised%2520CIR%2520models.%2520We%250Atested%2520our%2520scheme%2520across%2520three%2520backbones%2520on%2520three%2520public%2520datasets%2520%2528i.e.%252C%250AFashionIQ%252C%2520CIRR%252C%2520and%2520Birds-to-Words%2529%252C%2520achieving%2520maximum%2520improvements%2520of%252026.4%2525%252C%250A25.5%2525%2520and%252021.6%2525%2520respectively%252C%2520demonstrating%2520our%2520scheme%2527s%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-triplet%20Guided%20Few-shot%20Composed%20Image%20Retrieval&entry.906535625=Bohan%20Hou%20and%20Haoqiang%20Lin%20and%20Haokun%20Wen%20and%20Meng%20Liu%20and%20Xuemeng%20Song&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20challenging%20task%20that%20aims%20to%20retrieve%0Athe%20target%20image%20based%20on%20a%20multimodal%20query%2C%20i.e.%2C%20a%20reference%20image%20and%20its%0Acorresponding%20modification%20text.%20While%20previous%20supervised%20or%20zero-shot%0Alearning%20paradigms%20all%20fail%20to%20strike%20a%20good%20trade-off%20between%20time-consuming%0Aannotation%20cost%20and%20retrieval%20performance%2C%20recent%20researchers%20introduced%20the%0Atask%20of%20few-shot%20CIR%20%28FS-CIR%29%20and%20proposed%20a%20textual%20inversion-based%20network%0Abased%20on%20pretrained%20CLIP%20model%20to%20realize%20it.%20Despite%20its%20promising%0Aperformance%2C%20the%20approach%20suffers%20from%20two%20key%20limitations%3A%20insufficient%0Amultimodal%20query%20composition%20training%20and%20indiscriminative%20training%20triplet%0Aselection.%20To%20address%20these%20two%20limitations%2C%20in%20this%20work%2C%20we%20propose%20a%20novel%0Atwo-stage%20pseudo%20triplet%20guided%20few-shot%20CIR%20scheme%2C%20dubbed%20PTG-FSCIR.%20In%20the%0Afirst%20stage%2C%20we%20employ%20a%20masked%20training%20strategy%20and%20advanced%20image%20caption%0Agenerator%20to%20construct%20pseudo%20triplets%20from%20pure%20image%20data%20to%20enable%20the%20model%0Ato%20acquire%20primary%20knowledge%20related%20to%20multimodal%20query%20composition.%20In%20the%0Asecond%20stage%2C%20based%20on%20active%20learning%2C%20we%20design%20a%20pseudo%20modification%0Atext-based%20query-target%20distance%20metric%20to%20evaluate%20the%20challenging%20score%20for%0Aeach%20unlabeled%20sample.%20Meanwhile%2C%20we%20propose%20a%20robust%20top%20range-based%20random%0Asampling%20strategy%20according%20to%20the%203-%24%5Csigma%24%20rule%20in%20statistics%2C%20to%20sample%20the%0Achallenging%20samples%20for%20fine-tuning%20the%20pretrained%20model.%20Notably%2C%20our%20scheme%0Ais%20plug-and-play%20and%20compatible%20with%20any%20existing%20supervised%20CIR%20models.%20We%0Atested%20our%20scheme%20across%20three%20backbones%20on%20three%20public%20datasets%20%28i.e.%2C%0AFashionIQ%2C%20CIRR%2C%20and%20Birds-to-Words%29%2C%20achieving%20maximum%20improvements%20of%2026.4%25%2C%0A25.5%25%20and%2021.6%25%20respectively%2C%20demonstrating%20our%20scheme%27s%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06001v1&entry.124074799=Read"},
{"title": "Test-time adaptation for geospatial point cloud semantic segmentation\n  with distinct domain shifts", "author": "Puzuo Wang and Wei Yao and Jie Shao and Zhiyi He", "abstract": "  Domain adaptation (DA) techniques help deep learning models generalize across\ndata shifts for point cloud semantic segmentation (PCSS). Test-time adaptation\n(TTA) allows direct adaptation of a pre-trained model to unlabeled data during\ninference stage without access to source data or additional training, avoiding\nprivacy issues and large computational resources. We address TTA for geospatial\nPCSS by introducing three domain shift paradigms: photogrammetric to airborne\nLiDAR, airborne to mobile LiDAR, and synthetic to mobile laser scanning. We\npropose a TTA method that progressively updates batch normalization (BN)\nstatistics with each testing batch. Additionally, a self-supervised learning\nmodule optimizes learnable BN affine parameters. Information maximization and\nreliability-constrained pseudo-labeling improve prediction confidence and\nsupply supervisory signals. Experimental results show our method improves\nclassification accuracy by up to 20\\% mIoU, outperforming other methods. For\nphotogrammetric (SensatUrban) to airborne (Hessigheim 3D) adaptation at the\ninference stage, our method achieves 59.46\\% mIoU and 85.97\\% OA without\nretraining or fine-turning.\n", "link": "http://arxiv.org/abs/2407.06043v1", "date": "2024-07-08", "relevancy": 2.6738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5167}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20adaptation%20for%20geospatial%20point%20cloud%20semantic%20segmentation%0A%20%20with%20distinct%20domain%20shifts&body=Title%3A%20Test-time%20adaptation%20for%20geospatial%20point%20cloud%20semantic%20segmentation%0A%20%20with%20distinct%20domain%20shifts%0AAuthor%3A%20Puzuo%20Wang%20and%20Wei%20Yao%20and%20Jie%20Shao%20and%20Zhiyi%20He%0AAbstract%3A%20%20%20Domain%20adaptation%20%28DA%29%20techniques%20help%20deep%20learning%20models%20generalize%20across%0Adata%20shifts%20for%20point%20cloud%20semantic%20segmentation%20%28PCSS%29.%20Test-time%20adaptation%0A%28TTA%29%20allows%20direct%20adaptation%20of%20a%20pre-trained%20model%20to%20unlabeled%20data%20during%0Ainference%20stage%20without%20access%20to%20source%20data%20or%20additional%20training%2C%20avoiding%0Aprivacy%20issues%20and%20large%20computational%20resources.%20We%20address%20TTA%20for%20geospatial%0APCSS%20by%20introducing%20three%20domain%20shift%20paradigms%3A%20photogrammetric%20to%20airborne%0ALiDAR%2C%20airborne%20to%20mobile%20LiDAR%2C%20and%20synthetic%20to%20mobile%20laser%20scanning.%20We%0Apropose%20a%20TTA%20method%20that%20progressively%20updates%20batch%20normalization%20%28BN%29%0Astatistics%20with%20each%20testing%20batch.%20Additionally%2C%20a%20self-supervised%20learning%0Amodule%20optimizes%20learnable%20BN%20affine%20parameters.%20Information%20maximization%20and%0Areliability-constrained%20pseudo-labeling%20improve%20prediction%20confidence%20and%0Asupply%20supervisory%20signals.%20Experimental%20results%20show%20our%20method%20improves%0Aclassification%20accuracy%20by%20up%20to%2020%5C%25%20mIoU%2C%20outperforming%20other%20methods.%20For%0Aphotogrammetric%20%28SensatUrban%29%20to%20airborne%20%28Hessigheim%203D%29%20adaptation%20at%20the%0Ainference%20stage%2C%20our%20method%20achieves%2059.46%5C%25%20mIoU%20and%2085.97%5C%25%20OA%20without%0Aretraining%20or%20fine-turning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520adaptation%2520for%2520geospatial%2520point%2520cloud%2520semantic%2520segmentation%250A%2520%2520with%2520distinct%2520domain%2520shifts%26entry.906535625%3DPuzuo%2520Wang%2520and%2520Wei%2520Yao%2520and%2520Jie%2520Shao%2520and%2520Zhiyi%2520He%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520%2528DA%2529%2520techniques%2520help%2520deep%2520learning%2520models%2520generalize%2520across%250Adata%2520shifts%2520for%2520point%2520cloud%2520semantic%2520segmentation%2520%2528PCSS%2529.%2520Test-time%2520adaptation%250A%2528TTA%2529%2520allows%2520direct%2520adaptation%2520of%2520a%2520pre-trained%2520model%2520to%2520unlabeled%2520data%2520during%250Ainference%2520stage%2520without%2520access%2520to%2520source%2520data%2520or%2520additional%2520training%252C%2520avoiding%250Aprivacy%2520issues%2520and%2520large%2520computational%2520resources.%2520We%2520address%2520TTA%2520for%2520geospatial%250APCSS%2520by%2520introducing%2520three%2520domain%2520shift%2520paradigms%253A%2520photogrammetric%2520to%2520airborne%250ALiDAR%252C%2520airborne%2520to%2520mobile%2520LiDAR%252C%2520and%2520synthetic%2520to%2520mobile%2520laser%2520scanning.%2520We%250Apropose%2520a%2520TTA%2520method%2520that%2520progressively%2520updates%2520batch%2520normalization%2520%2528BN%2529%250Astatistics%2520with%2520each%2520testing%2520batch.%2520Additionally%252C%2520a%2520self-supervised%2520learning%250Amodule%2520optimizes%2520learnable%2520BN%2520affine%2520parameters.%2520Information%2520maximization%2520and%250Areliability-constrained%2520pseudo-labeling%2520improve%2520prediction%2520confidence%2520and%250Asupply%2520supervisory%2520signals.%2520Experimental%2520results%2520show%2520our%2520method%2520improves%250Aclassification%2520accuracy%2520by%2520up%2520to%252020%255C%2525%2520mIoU%252C%2520outperforming%2520other%2520methods.%2520For%250Aphotogrammetric%2520%2528SensatUrban%2529%2520to%2520airborne%2520%2528Hessigheim%25203D%2529%2520adaptation%2520at%2520the%250Ainference%2520stage%252C%2520our%2520method%2520achieves%252059.46%255C%2525%2520mIoU%2520and%252085.97%255C%2525%2520OA%2520without%250Aretraining%2520or%2520fine-turning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20adaptation%20for%20geospatial%20point%20cloud%20semantic%20segmentation%0A%20%20with%20distinct%20domain%20shifts&entry.906535625=Puzuo%20Wang%20and%20Wei%20Yao%20and%20Jie%20Shao%20and%20Zhiyi%20He&entry.1292438233=%20%20Domain%20adaptation%20%28DA%29%20techniques%20help%20deep%20learning%20models%20generalize%20across%0Adata%20shifts%20for%20point%20cloud%20semantic%20segmentation%20%28PCSS%29.%20Test-time%20adaptation%0A%28TTA%29%20allows%20direct%20adaptation%20of%20a%20pre-trained%20model%20to%20unlabeled%20data%20during%0Ainference%20stage%20without%20access%20to%20source%20data%20or%20additional%20training%2C%20avoiding%0Aprivacy%20issues%20and%20large%20computational%20resources.%20We%20address%20TTA%20for%20geospatial%0APCSS%20by%20introducing%20three%20domain%20shift%20paradigms%3A%20photogrammetric%20to%20airborne%0ALiDAR%2C%20airborne%20to%20mobile%20LiDAR%2C%20and%20synthetic%20to%20mobile%20laser%20scanning.%20We%0Apropose%20a%20TTA%20method%20that%20progressively%20updates%20batch%20normalization%20%28BN%29%0Astatistics%20with%20each%20testing%20batch.%20Additionally%2C%20a%20self-supervised%20learning%0Amodule%20optimizes%20learnable%20BN%20affine%20parameters.%20Information%20maximization%20and%0Areliability-constrained%20pseudo-labeling%20improve%20prediction%20confidence%20and%0Asupply%20supervisory%20signals.%20Experimental%20results%20show%20our%20method%20improves%0Aclassification%20accuracy%20by%20up%20to%2020%5C%25%20mIoU%2C%20outperforming%20other%20methods.%20For%0Aphotogrammetric%20%28SensatUrban%29%20to%20airborne%20%28Hessigheim%203D%29%20adaptation%20at%20the%0Ainference%20stage%2C%20our%20method%20achieves%2059.46%5C%25%20mIoU%20and%2085.97%5C%25%20OA%20without%0Aretraining%20or%20fine-turning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06043v1&entry.124074799=Read"},
{"title": "Holistically-Nested Structure-Aware Graph Neural Network for Road\n  Extraction", "author": "Tinghuai Wang and Guangming Wang and Kuan Eeik Tan", "abstract": "  Convolutional neural networks (CNN) have made significant advances in\ndetecting roads from satellite images. However, existing CNN approaches are\ngenerally repurposed semantic segmentation architectures and suffer from the\npoor delineation of long and curved regions. Lack of overall road topology and\nstructure information further deteriorates their performance on challenging\nremote sensing images. This paper presents a novel multi-task graph neural\nnetwork (GNN) which simultaneously detects both road regions and road borders;\nthe inter-play between these two tasks unlocks superior performance from two\nperspectives: (1) the hierarchically detected road borders enable the network\nto capture and encode holistic road structure to enhance road connectivity (2)\nidentifying the intrinsic correlation of semantic landcover regions mitigates\nthe difficulty in recognizing roads cluttered by regions with similar\nappearance. Experiments on challenging dataset demonstrate that the proposed\narchitecture can improve the road border delineation and road extraction\naccuracy compared with the existing methods.\n", "link": "http://arxiv.org/abs/2407.02639v2", "date": "2024-07-08", "relevancy": 2.6712, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5634}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5201}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holistically-Nested%20Structure-Aware%20Graph%20Neural%20Network%20for%20Road%0A%20%20Extraction&body=Title%3A%20Holistically-Nested%20Structure-Aware%20Graph%20Neural%20Network%20for%20Road%0A%20%20Extraction%0AAuthor%3A%20Tinghuai%20Wang%20and%20Guangming%20Wang%20and%20Kuan%20Eeik%20Tan%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNN%29%20have%20made%20significant%20advances%20in%0Adetecting%20roads%20from%20satellite%20images.%20However%2C%20existing%20CNN%20approaches%20are%0Agenerally%20repurposed%20semantic%20segmentation%20architectures%20and%20suffer%20from%20the%0Apoor%20delineation%20of%20long%20and%20curved%20regions.%20Lack%20of%20overall%20road%20topology%20and%0Astructure%20information%20further%20deteriorates%20their%20performance%20on%20challenging%0Aremote%20sensing%20images.%20This%20paper%20presents%20a%20novel%20multi-task%20graph%20neural%0Anetwork%20%28GNN%29%20which%20simultaneously%20detects%20both%20road%20regions%20and%20road%20borders%3B%0Athe%20inter-play%20between%20these%20two%20tasks%20unlocks%20superior%20performance%20from%20two%0Aperspectives%3A%20%281%29%20the%20hierarchically%20detected%20road%20borders%20enable%20the%20network%0Ato%20capture%20and%20encode%20holistic%20road%20structure%20to%20enhance%20road%20connectivity%20%282%29%0Aidentifying%20the%20intrinsic%20correlation%20of%20semantic%20landcover%20regions%20mitigates%0Athe%20difficulty%20in%20recognizing%20roads%20cluttered%20by%20regions%20with%20similar%0Aappearance.%20Experiments%20on%20challenging%20dataset%20demonstrate%20that%20the%20proposed%0Aarchitecture%20can%20improve%20the%20road%20border%20delineation%20and%20road%20extraction%0Aaccuracy%20compared%20with%20the%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHolistically-Nested%2520Structure-Aware%2520Graph%2520Neural%2520Network%2520for%2520Road%250A%2520%2520Extraction%26entry.906535625%3DTinghuai%2520Wang%2520and%2520Guangming%2520Wang%2520and%2520Kuan%2520Eeik%2520Tan%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNN%2529%2520have%2520made%2520significant%2520advances%2520in%250Adetecting%2520roads%2520from%2520satellite%2520images.%2520However%252C%2520existing%2520CNN%2520approaches%2520are%250Agenerally%2520repurposed%2520semantic%2520segmentation%2520architectures%2520and%2520suffer%2520from%2520the%250Apoor%2520delineation%2520of%2520long%2520and%2520curved%2520regions.%2520Lack%2520of%2520overall%2520road%2520topology%2520and%250Astructure%2520information%2520further%2520deteriorates%2520their%2520performance%2520on%2520challenging%250Aremote%2520sensing%2520images.%2520This%2520paper%2520presents%2520a%2520novel%2520multi-task%2520graph%2520neural%250Anetwork%2520%2528GNN%2529%2520which%2520simultaneously%2520detects%2520both%2520road%2520regions%2520and%2520road%2520borders%253B%250Athe%2520inter-play%2520between%2520these%2520two%2520tasks%2520unlocks%2520superior%2520performance%2520from%2520two%250Aperspectives%253A%2520%25281%2529%2520the%2520hierarchically%2520detected%2520road%2520borders%2520enable%2520the%2520network%250Ato%2520capture%2520and%2520encode%2520holistic%2520road%2520structure%2520to%2520enhance%2520road%2520connectivity%2520%25282%2529%250Aidentifying%2520the%2520intrinsic%2520correlation%2520of%2520semantic%2520landcover%2520regions%2520mitigates%250Athe%2520difficulty%2520in%2520recognizing%2520roads%2520cluttered%2520by%2520regions%2520with%2520similar%250Aappearance.%2520Experiments%2520on%2520challenging%2520dataset%2520demonstrate%2520that%2520the%2520proposed%250Aarchitecture%2520can%2520improve%2520the%2520road%2520border%2520delineation%2520and%2520road%2520extraction%250Aaccuracy%2520compared%2520with%2520the%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holistically-Nested%20Structure-Aware%20Graph%20Neural%20Network%20for%20Road%0A%20%20Extraction&entry.906535625=Tinghuai%20Wang%20and%20Guangming%20Wang%20and%20Kuan%20Eeik%20Tan&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNN%29%20have%20made%20significant%20advances%20in%0Adetecting%20roads%20from%20satellite%20images.%20However%2C%20existing%20CNN%20approaches%20are%0Agenerally%20repurposed%20semantic%20segmentation%20architectures%20and%20suffer%20from%20the%0Apoor%20delineation%20of%20long%20and%20curved%20regions.%20Lack%20of%20overall%20road%20topology%20and%0Astructure%20information%20further%20deteriorates%20their%20performance%20on%20challenging%0Aremote%20sensing%20images.%20This%20paper%20presents%20a%20novel%20multi-task%20graph%20neural%0Anetwork%20%28GNN%29%20which%20simultaneously%20detects%20both%20road%20regions%20and%20road%20borders%3B%0Athe%20inter-play%20between%20these%20two%20tasks%20unlocks%20superior%20performance%20from%20two%0Aperspectives%3A%20%281%29%20the%20hierarchically%20detected%20road%20borders%20enable%20the%20network%0Ato%20capture%20and%20encode%20holistic%20road%20structure%20to%20enhance%20road%20connectivity%20%282%29%0Aidentifying%20the%20intrinsic%20correlation%20of%20semantic%20landcover%20regions%20mitigates%0Athe%20difficulty%20in%20recognizing%20roads%20cluttered%20by%20regions%20with%20similar%0Aappearance.%20Experiments%20on%20challenging%20dataset%20demonstrate%20that%20the%20proposed%0Aarchitecture%20can%20improve%20the%20road%20border%20delineation%20and%20road%20extraction%0Aaccuracy%20compared%20with%20the%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02639v2&entry.124074799=Read"},
{"title": "LaFAM: Unsupervised Feature Attribution with Label-free Activation Maps", "author": "Aray Karjauv and Sahin Albayrak", "abstract": "  Convolutional Neural Networks (CNNs) are known for their ability to learn\nhierarchical structures, naturally developing detectors for objects, and\nsemantic concepts within their deeper layers. Activation maps (AMs) reveal\nthese saliency regions, which are crucial for many Explainable AI (XAI)\nmethods. However, the direct exploitation of raw AMs in CNNs for feature\nattribution remains underexplored in literature. This work revises Class\nActivation Map (CAM) methods by introducing the Label-free Activation Map\n(LaFAM), a streamlined approach utilizing raw AMs for feature attribution\nwithout reliance on labels. LaFAM presents an efficient alternative to\nconventional CAM methods, demonstrating particular effectiveness in saliency\nmap generation for self-supervised learning while maintaining applicability in\nsupervised learning scenarios.\n", "link": "http://arxiv.org/abs/2407.06059v1", "date": "2024-07-08", "relevancy": 2.6706, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5534}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5264}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaFAM%3A%20Unsupervised%20Feature%20Attribution%20with%20Label-free%20Activation%20Maps&body=Title%3A%20LaFAM%3A%20Unsupervised%20Feature%20Attribution%20with%20Label-free%20Activation%20Maps%0AAuthor%3A%20Aray%20Karjauv%20and%20Sahin%20Albayrak%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20known%20for%20their%20ability%20to%20learn%0Ahierarchical%20structures%2C%20naturally%20developing%20detectors%20for%20objects%2C%20and%0Asemantic%20concepts%20within%20their%20deeper%20layers.%20Activation%20maps%20%28AMs%29%20reveal%0Athese%20saliency%20regions%2C%20which%20are%20crucial%20for%20many%20Explainable%20AI%20%28XAI%29%0Amethods.%20However%2C%20the%20direct%20exploitation%20of%20raw%20AMs%20in%20CNNs%20for%20feature%0Aattribution%20remains%20underexplored%20in%20literature.%20This%20work%20revises%20Class%0AActivation%20Map%20%28CAM%29%20methods%20by%20introducing%20the%20Label-free%20Activation%20Map%0A%28LaFAM%29%2C%20a%20streamlined%20approach%20utilizing%20raw%20AMs%20for%20feature%20attribution%0Awithout%20reliance%20on%20labels.%20LaFAM%20presents%20an%20efficient%20alternative%20to%0Aconventional%20CAM%20methods%2C%20demonstrating%20particular%20effectiveness%20in%20saliency%0Amap%20generation%20for%20self-supervised%20learning%20while%20maintaining%20applicability%20in%0Asupervised%20learning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaFAM%253A%2520Unsupervised%2520Feature%2520Attribution%2520with%2520Label-free%2520Activation%2520Maps%26entry.906535625%3DAray%2520Karjauv%2520and%2520Sahin%2520Albayrak%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520are%2520known%2520for%2520their%2520ability%2520to%2520learn%250Ahierarchical%2520structures%252C%2520naturally%2520developing%2520detectors%2520for%2520objects%252C%2520and%250Asemantic%2520concepts%2520within%2520their%2520deeper%2520layers.%2520Activation%2520maps%2520%2528AMs%2529%2520reveal%250Athese%2520saliency%2520regions%252C%2520which%2520are%2520crucial%2520for%2520many%2520Explainable%2520AI%2520%2528XAI%2529%250Amethods.%2520However%252C%2520the%2520direct%2520exploitation%2520of%2520raw%2520AMs%2520in%2520CNNs%2520for%2520feature%250Aattribution%2520remains%2520underexplored%2520in%2520literature.%2520This%2520work%2520revises%2520Class%250AActivation%2520Map%2520%2528CAM%2529%2520methods%2520by%2520introducing%2520the%2520Label-free%2520Activation%2520Map%250A%2528LaFAM%2529%252C%2520a%2520streamlined%2520approach%2520utilizing%2520raw%2520AMs%2520for%2520feature%2520attribution%250Awithout%2520reliance%2520on%2520labels.%2520LaFAM%2520presents%2520an%2520efficient%2520alternative%2520to%250Aconventional%2520CAM%2520methods%252C%2520demonstrating%2520particular%2520effectiveness%2520in%2520saliency%250Amap%2520generation%2520for%2520self-supervised%2520learning%2520while%2520maintaining%2520applicability%2520in%250Asupervised%2520learning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaFAM%3A%20Unsupervised%20Feature%20Attribution%20with%20Label-free%20Activation%20Maps&entry.906535625=Aray%20Karjauv%20and%20Sahin%20Albayrak&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20known%20for%20their%20ability%20to%20learn%0Ahierarchical%20structures%2C%20naturally%20developing%20detectors%20for%20objects%2C%20and%0Asemantic%20concepts%20within%20their%20deeper%20layers.%20Activation%20maps%20%28AMs%29%20reveal%0Athese%20saliency%20regions%2C%20which%20are%20crucial%20for%20many%20Explainable%20AI%20%28XAI%29%0Amethods.%20However%2C%20the%20direct%20exploitation%20of%20raw%20AMs%20in%20CNNs%20for%20feature%0Aattribution%20remains%20underexplored%20in%20literature.%20This%20work%20revises%20Class%0AActivation%20Map%20%28CAM%29%20methods%20by%20introducing%20the%20Label-free%20Activation%20Map%0A%28LaFAM%29%2C%20a%20streamlined%20approach%20utilizing%20raw%20AMs%20for%20feature%20attribution%0Awithout%20reliance%20on%20labels.%20LaFAM%20presents%20an%20efficient%20alternative%20to%0Aconventional%20CAM%20methods%2C%20demonstrating%20particular%20effectiveness%20in%20saliency%0Amap%20generation%20for%20self-supervised%20learning%20while%20maintaining%20applicability%20in%0Asupervised%20learning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06059v1&entry.124074799=Read"},
{"title": "An efficient method to automate tooth identification and 3D bounding box\n  extraction from Cone Beam CT Images", "author": "Ignacio Garrido Botella and Ignacio Arranz \u00c1gueda and Juan Carlos Armenteros Carmona and Oleg Vorontsov and Fernando Bay\u00f3n Robledo and Adri\u00e1n Alonso Barriuso", "abstract": "  Accurate identification, localization, and segregation of teeth from Cone\nBeam Computed Tomography (CBCT) images are essential for analyzing dental\npathologies. Modeling an individual tooth can be challenging and intricate to\naccomplish, especially when fillings and other restorations introduce\nartifacts. This paper proposes a method for automatically detecting,\nidentifying, and extracting teeth from CBCT images. Our approach involves\ndividing the three-dimensional images into axial slices for image detection.\nTeeth are pinpointed and labeled using a single-stage object detector.\nSubsequently, bounding boxes are delineated and identified to create\nthree-dimensional representations of each tooth. The proposed solution has been\nsuccessfully integrated into the dental analysis tool Dentomo.\n", "link": "http://arxiv.org/abs/2407.05892v1", "date": "2024-07-08", "relevancy": 2.6187, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5384}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5384}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20efficient%20method%20to%20automate%20tooth%20identification%20and%203D%20bounding%20box%0A%20%20extraction%20from%20Cone%20Beam%20CT%20Images&body=Title%3A%20An%20efficient%20method%20to%20automate%20tooth%20identification%20and%203D%20bounding%20box%0A%20%20extraction%20from%20Cone%20Beam%20CT%20Images%0AAuthor%3A%20Ignacio%20Garrido%20Botella%20and%20Ignacio%20Arranz%20%C3%81gueda%20and%20Juan%20Carlos%20Armenteros%20Carmona%20and%20Oleg%20Vorontsov%20and%20Fernando%20Bay%C3%B3n%20Robledo%20and%20Adri%C3%A1n%20Alonso%20Barriuso%0AAbstract%3A%20%20%20Accurate%20identification%2C%20localization%2C%20and%20segregation%20of%20teeth%20from%20Cone%0ABeam%20Computed%20Tomography%20%28CBCT%29%20images%20are%20essential%20for%20analyzing%20dental%0Apathologies.%20Modeling%20an%20individual%20tooth%20can%20be%20challenging%20and%20intricate%20to%0Aaccomplish%2C%20especially%20when%20fillings%20and%20other%20restorations%20introduce%0Aartifacts.%20This%20paper%20proposes%20a%20method%20for%20automatically%20detecting%2C%0Aidentifying%2C%20and%20extracting%20teeth%20from%20CBCT%20images.%20Our%20approach%20involves%0Adividing%20the%20three-dimensional%20images%20into%20axial%20slices%20for%20image%20detection.%0ATeeth%20are%20pinpointed%20and%20labeled%20using%20a%20single-stage%20object%20detector.%0ASubsequently%2C%20bounding%20boxes%20are%20delineated%20and%20identified%20to%20create%0Athree-dimensional%20representations%20of%20each%20tooth.%20The%20proposed%20solution%20has%20been%0Asuccessfully%20integrated%20into%20the%20dental%20analysis%20tool%20Dentomo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520efficient%2520method%2520to%2520automate%2520tooth%2520identification%2520and%25203D%2520bounding%2520box%250A%2520%2520extraction%2520from%2520Cone%2520Beam%2520CT%2520Images%26entry.906535625%3DIgnacio%2520Garrido%2520Botella%2520and%2520Ignacio%2520Arranz%2520%25C3%2581gueda%2520and%2520Juan%2520Carlos%2520Armenteros%2520Carmona%2520and%2520Oleg%2520Vorontsov%2520and%2520Fernando%2520Bay%25C3%25B3n%2520Robledo%2520and%2520Adri%25C3%25A1n%2520Alonso%2520Barriuso%26entry.1292438233%3D%2520%2520Accurate%2520identification%252C%2520localization%252C%2520and%2520segregation%2520of%2520teeth%2520from%2520Cone%250ABeam%2520Computed%2520Tomography%2520%2528CBCT%2529%2520images%2520are%2520essential%2520for%2520analyzing%2520dental%250Apathologies.%2520Modeling%2520an%2520individual%2520tooth%2520can%2520be%2520challenging%2520and%2520intricate%2520to%250Aaccomplish%252C%2520especially%2520when%2520fillings%2520and%2520other%2520restorations%2520introduce%250Aartifacts.%2520This%2520paper%2520proposes%2520a%2520method%2520for%2520automatically%2520detecting%252C%250Aidentifying%252C%2520and%2520extracting%2520teeth%2520from%2520CBCT%2520images.%2520Our%2520approach%2520involves%250Adividing%2520the%2520three-dimensional%2520images%2520into%2520axial%2520slices%2520for%2520image%2520detection.%250ATeeth%2520are%2520pinpointed%2520and%2520labeled%2520using%2520a%2520single-stage%2520object%2520detector.%250ASubsequently%252C%2520bounding%2520boxes%2520are%2520delineated%2520and%2520identified%2520to%2520create%250Athree-dimensional%2520representations%2520of%2520each%2520tooth.%2520The%2520proposed%2520solution%2520has%2520been%250Asuccessfully%2520integrated%2520into%2520the%2520dental%2520analysis%2520tool%2520Dentomo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20efficient%20method%20to%20automate%20tooth%20identification%20and%203D%20bounding%20box%0A%20%20extraction%20from%20Cone%20Beam%20CT%20Images&entry.906535625=Ignacio%20Garrido%20Botella%20and%20Ignacio%20Arranz%20%C3%81gueda%20and%20Juan%20Carlos%20Armenteros%20Carmona%20and%20Oleg%20Vorontsov%20and%20Fernando%20Bay%C3%B3n%20Robledo%20and%20Adri%C3%A1n%20Alonso%20Barriuso&entry.1292438233=%20%20Accurate%20identification%2C%20localization%2C%20and%20segregation%20of%20teeth%20from%20Cone%0ABeam%20Computed%20Tomography%20%28CBCT%29%20images%20are%20essential%20for%20analyzing%20dental%0Apathologies.%20Modeling%20an%20individual%20tooth%20can%20be%20challenging%20and%20intricate%20to%0Aaccomplish%2C%20especially%20when%20fillings%20and%20other%20restorations%20introduce%0Aartifacts.%20This%20paper%20proposes%20a%20method%20for%20automatically%20detecting%2C%0Aidentifying%2C%20and%20extracting%20teeth%20from%20CBCT%20images.%20Our%20approach%20involves%0Adividing%20the%20three-dimensional%20images%20into%20axial%20slices%20for%20image%20detection.%0ATeeth%20are%20pinpointed%20and%20labeled%20using%20a%20single-stage%20object%20detector.%0ASubsequently%2C%20bounding%20boxes%20are%20delineated%20and%20identified%20to%20create%0Athree-dimensional%20representations%20of%20each%20tooth.%20The%20proposed%20solution%20has%20been%0Asuccessfully%20integrated%20into%20the%20dental%20analysis%20tool%20Dentomo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05892v1&entry.124074799=Read"},
{"title": "Towards A Comprehensive Visual Saliency Explanation Framework for\n  AI-based Face Recognition Systems", "author": "Yuhang Lu and Zewei Xu and Touradj Ebrahimi", "abstract": "  Over recent years, deep convolutional neural networks have significantly\nadvanced the field of face recognition techniques for both verification and\nidentification purposes. Despite the impressive accuracy, these neural networks\nare often criticized for lacking explainability. There is a growing demand for\nunderstanding the decision-making process of AI-based face recognition systems.\nSome studies have investigated the use of visual saliency maps as explanations,\nbut they have predominantly focused on the specific face verification case. The\ndiscussion on more general face recognition scenarios and the corresponding\nevaluation methodology for these explanations have long been absent in current\nresearch. Therefore, this manuscript conceives a comprehensive explanation\nframework for face recognition tasks. Firstly, an exhaustive definition of\nvisual saliency map-based explanations for AI-based face recognition systems is\nprovided, taking into account the two most common recognition situations\nindividually, i.e., face verification and identification. Secondly, a new\nmodel-agnostic explanation method named CorrRISE is proposed to produce\nsaliency maps, which reveal both the similar and dissimilar regions between any\ngiven face images. Subsequently, the explanation framework conceives a new\nevaluation methodology that offers quantitative measurement and comparison of\nthe performance of general visual saliency explanation methods in face\nrecognition. Consequently, extensive experiments are carried out on multiple\nverification and identification scenarios. The results showcase that CorrRISE\ngenerates insightful saliency maps and demonstrates superior performance,\nparticularly in similarity maps in comparison with the state-of-the-art\nexplanation approaches.\n", "link": "http://arxiv.org/abs/2407.05983v1", "date": "2024-07-08", "relevancy": 2.6145, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5571}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5191}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20A%20Comprehensive%20Visual%20Saliency%20Explanation%20Framework%20for%0A%20%20AI-based%20Face%20Recognition%20Systems&body=Title%3A%20Towards%20A%20Comprehensive%20Visual%20Saliency%20Explanation%20Framework%20for%0A%20%20AI-based%20Face%20Recognition%20Systems%0AAuthor%3A%20Yuhang%20Lu%20and%20Zewei%20Xu%20and%20Touradj%20Ebrahimi%0AAbstract%3A%20%20%20Over%20recent%20years%2C%20deep%20convolutional%20neural%20networks%20have%20significantly%0Aadvanced%20the%20field%20of%20face%20recognition%20techniques%20for%20both%20verification%20and%0Aidentification%20purposes.%20Despite%20the%20impressive%20accuracy%2C%20these%20neural%20networks%0Aare%20often%20criticized%20for%20lacking%20explainability.%20There%20is%20a%20growing%20demand%20for%0Aunderstanding%20the%20decision-making%20process%20of%20AI-based%20face%20recognition%20systems.%0ASome%20studies%20have%20investigated%20the%20use%20of%20visual%20saliency%20maps%20as%20explanations%2C%0Abut%20they%20have%20predominantly%20focused%20on%20the%20specific%20face%20verification%20case.%20The%0Adiscussion%20on%20more%20general%20face%20recognition%20scenarios%20and%20the%20corresponding%0Aevaluation%20methodology%20for%20these%20explanations%20have%20long%20been%20absent%20in%20current%0Aresearch.%20Therefore%2C%20this%20manuscript%20conceives%20a%20comprehensive%20explanation%0Aframework%20for%20face%20recognition%20tasks.%20Firstly%2C%20an%20exhaustive%20definition%20of%0Avisual%20saliency%20map-based%20explanations%20for%20AI-based%20face%20recognition%20systems%20is%0Aprovided%2C%20taking%20into%20account%20the%20two%20most%20common%20recognition%20situations%0Aindividually%2C%20i.e.%2C%20face%20verification%20and%20identification.%20Secondly%2C%20a%20new%0Amodel-agnostic%20explanation%20method%20named%20CorrRISE%20is%20proposed%20to%20produce%0Asaliency%20maps%2C%20which%20reveal%20both%20the%20similar%20and%20dissimilar%20regions%20between%20any%0Agiven%20face%20images.%20Subsequently%2C%20the%20explanation%20framework%20conceives%20a%20new%0Aevaluation%20methodology%20that%20offers%20quantitative%20measurement%20and%20comparison%20of%0Athe%20performance%20of%20general%20visual%20saliency%20explanation%20methods%20in%20face%0Arecognition.%20Consequently%2C%20extensive%20experiments%20are%20carried%20out%20on%20multiple%0Averification%20and%20identification%20scenarios.%20The%20results%20showcase%20that%20CorrRISE%0Agenerates%20insightful%20saliency%20maps%20and%20demonstrates%20superior%20performance%2C%0Aparticularly%20in%20similarity%20maps%20in%20comparison%20with%20the%20state-of-the-art%0Aexplanation%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520A%2520Comprehensive%2520Visual%2520Saliency%2520Explanation%2520Framework%2520for%250A%2520%2520AI-based%2520Face%2520Recognition%2520Systems%26entry.906535625%3DYuhang%2520Lu%2520and%2520Zewei%2520Xu%2520and%2520Touradj%2520Ebrahimi%26entry.1292438233%3D%2520%2520Over%2520recent%2520years%252C%2520deep%2520convolutional%2520neural%2520networks%2520have%2520significantly%250Aadvanced%2520the%2520field%2520of%2520face%2520recognition%2520techniques%2520for%2520both%2520verification%2520and%250Aidentification%2520purposes.%2520Despite%2520the%2520impressive%2520accuracy%252C%2520these%2520neural%2520networks%250Aare%2520often%2520criticized%2520for%2520lacking%2520explainability.%2520There%2520is%2520a%2520growing%2520demand%2520for%250Aunderstanding%2520the%2520decision-making%2520process%2520of%2520AI-based%2520face%2520recognition%2520systems.%250ASome%2520studies%2520have%2520investigated%2520the%2520use%2520of%2520visual%2520saliency%2520maps%2520as%2520explanations%252C%250Abut%2520they%2520have%2520predominantly%2520focused%2520on%2520the%2520specific%2520face%2520verification%2520case.%2520The%250Adiscussion%2520on%2520more%2520general%2520face%2520recognition%2520scenarios%2520and%2520the%2520corresponding%250Aevaluation%2520methodology%2520for%2520these%2520explanations%2520have%2520long%2520been%2520absent%2520in%2520current%250Aresearch.%2520Therefore%252C%2520this%2520manuscript%2520conceives%2520a%2520comprehensive%2520explanation%250Aframework%2520for%2520face%2520recognition%2520tasks.%2520Firstly%252C%2520an%2520exhaustive%2520definition%2520of%250Avisual%2520saliency%2520map-based%2520explanations%2520for%2520AI-based%2520face%2520recognition%2520systems%2520is%250Aprovided%252C%2520taking%2520into%2520account%2520the%2520two%2520most%2520common%2520recognition%2520situations%250Aindividually%252C%2520i.e.%252C%2520face%2520verification%2520and%2520identification.%2520Secondly%252C%2520a%2520new%250Amodel-agnostic%2520explanation%2520method%2520named%2520CorrRISE%2520is%2520proposed%2520to%2520produce%250Asaliency%2520maps%252C%2520which%2520reveal%2520both%2520the%2520similar%2520and%2520dissimilar%2520regions%2520between%2520any%250Agiven%2520face%2520images.%2520Subsequently%252C%2520the%2520explanation%2520framework%2520conceives%2520a%2520new%250Aevaluation%2520methodology%2520that%2520offers%2520quantitative%2520measurement%2520and%2520comparison%2520of%250Athe%2520performance%2520of%2520general%2520visual%2520saliency%2520explanation%2520methods%2520in%2520face%250Arecognition.%2520Consequently%252C%2520extensive%2520experiments%2520are%2520carried%2520out%2520on%2520multiple%250Averification%2520and%2520identification%2520scenarios.%2520The%2520results%2520showcase%2520that%2520CorrRISE%250Agenerates%2520insightful%2520saliency%2520maps%2520and%2520demonstrates%2520superior%2520performance%252C%250Aparticularly%2520in%2520similarity%2520maps%2520in%2520comparison%2520with%2520the%2520state-of-the-art%250Aexplanation%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20A%20Comprehensive%20Visual%20Saliency%20Explanation%20Framework%20for%0A%20%20AI-based%20Face%20Recognition%20Systems&entry.906535625=Yuhang%20Lu%20and%20Zewei%20Xu%20and%20Touradj%20Ebrahimi&entry.1292438233=%20%20Over%20recent%20years%2C%20deep%20convolutional%20neural%20networks%20have%20significantly%0Aadvanced%20the%20field%20of%20face%20recognition%20techniques%20for%20both%20verification%20and%0Aidentification%20purposes.%20Despite%20the%20impressive%20accuracy%2C%20these%20neural%20networks%0Aare%20often%20criticized%20for%20lacking%20explainability.%20There%20is%20a%20growing%20demand%20for%0Aunderstanding%20the%20decision-making%20process%20of%20AI-based%20face%20recognition%20systems.%0ASome%20studies%20have%20investigated%20the%20use%20of%20visual%20saliency%20maps%20as%20explanations%2C%0Abut%20they%20have%20predominantly%20focused%20on%20the%20specific%20face%20verification%20case.%20The%0Adiscussion%20on%20more%20general%20face%20recognition%20scenarios%20and%20the%20corresponding%0Aevaluation%20methodology%20for%20these%20explanations%20have%20long%20been%20absent%20in%20current%0Aresearch.%20Therefore%2C%20this%20manuscript%20conceives%20a%20comprehensive%20explanation%0Aframework%20for%20face%20recognition%20tasks.%20Firstly%2C%20an%20exhaustive%20definition%20of%0Avisual%20saliency%20map-based%20explanations%20for%20AI-based%20face%20recognition%20systems%20is%0Aprovided%2C%20taking%20into%20account%20the%20two%20most%20common%20recognition%20situations%0Aindividually%2C%20i.e.%2C%20face%20verification%20and%20identification.%20Secondly%2C%20a%20new%0Amodel-agnostic%20explanation%20method%20named%20CorrRISE%20is%20proposed%20to%20produce%0Asaliency%20maps%2C%20which%20reveal%20both%20the%20similar%20and%20dissimilar%20regions%20between%20any%0Agiven%20face%20images.%20Subsequently%2C%20the%20explanation%20framework%20conceives%20a%20new%0Aevaluation%20methodology%20that%20offers%20quantitative%20measurement%20and%20comparison%20of%0Athe%20performance%20of%20general%20visual%20saliency%20explanation%20methods%20in%20face%0Arecognition.%20Consequently%2C%20extensive%20experiments%20are%20carried%20out%20on%20multiple%0Averification%20and%20identification%20scenarios.%20The%20results%20showcase%20that%20CorrRISE%0Agenerates%20insightful%20saliency%20maps%20and%20demonstrates%20superior%20performance%2C%0Aparticularly%20in%20similarity%20maps%20in%20comparison%20with%20the%20state-of-the-art%0Aexplanation%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05983v1&entry.124074799=Read"},
{"title": "C2C: Component-to-Composition Learning for Zero-Shot Compositional\n  Action Recognition", "author": "Rongchang Li and Zhenhua Feng and Tianyang Xu and Linze Li and Xiao-Jun Wu and Muhammad Awais and Sara Atito and Josef Kittler", "abstract": "  Compositional actions consist of dynamic (verbs) and static (objects)\nconcepts. Humans can easily recognize unseen compositions using the learned\nconcepts. For machines, solving such a problem requires a model to recognize\nunseen actions composed of previously observed verbs and objects, thus\nrequiring, so-called, compositional generalization ability. To facilitate this\nresearch, we propose a novel Zero-Shot Compositional Action Recognition\n(ZS-CAR) task. For evaluating the task, we construct a new benchmark,\nSomething-composition (Sth-com), based on the widely used Something-Something\nV2 dataset. We also propose a novel Component-to-Composition (C2C) learning\nmethod to solve the new ZS-CAR task. C2C includes an independent component\nlearning module and a composition inference module. Last, we devise an enhanced\ntraining strategy to address the challenges of component variation between seen\nand unseen compositions and to handle the subtle balance between learning seen\nand unseen actions. The experimental results demonstrate that the proposed\nframework significantly surpasses the existing compositional generalization\nmethods and sets a new state-of-the-art. The new Sth-com benchmark and code are\navailable at https://github.com/RongchangLi/ZSCAR_C2C.\n", "link": "http://arxiv.org/abs/2407.06113v1", "date": "2024-07-08", "relevancy": 2.6112, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5326}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5209}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C2C%3A%20Component-to-Composition%20Learning%20for%20Zero-Shot%20Compositional%0A%20%20Action%20Recognition&body=Title%3A%20C2C%3A%20Component-to-Composition%20Learning%20for%20Zero-Shot%20Compositional%0A%20%20Action%20Recognition%0AAuthor%3A%20Rongchang%20Li%20and%20Zhenhua%20Feng%20and%20Tianyang%20Xu%20and%20Linze%20Li%20and%20Xiao-Jun%20Wu%20and%20Muhammad%20Awais%20and%20Sara%20Atito%20and%20Josef%20Kittler%0AAbstract%3A%20%20%20Compositional%20actions%20consist%20of%20dynamic%20%28verbs%29%20and%20static%20%28objects%29%0Aconcepts.%20Humans%20can%20easily%20recognize%20unseen%20compositions%20using%20the%20learned%0Aconcepts.%20For%20machines%2C%20solving%20such%20a%20problem%20requires%20a%20model%20to%20recognize%0Aunseen%20actions%20composed%20of%20previously%20observed%20verbs%20and%20objects%2C%20thus%0Arequiring%2C%20so-called%2C%20compositional%20generalization%20ability.%20To%20facilitate%20this%0Aresearch%2C%20we%20propose%20a%20novel%20Zero-Shot%20Compositional%20Action%20Recognition%0A%28ZS-CAR%29%20task.%20For%20evaluating%20the%20task%2C%20we%20construct%20a%20new%20benchmark%2C%0ASomething-composition%20%28Sth-com%29%2C%20based%20on%20the%20widely%20used%20Something-Something%0AV2%20dataset.%20We%20also%20propose%20a%20novel%20Component-to-Composition%20%28C2C%29%20learning%0Amethod%20to%20solve%20the%20new%20ZS-CAR%20task.%20C2C%20includes%20an%20independent%20component%0Alearning%20module%20and%20a%20composition%20inference%20module.%20Last%2C%20we%20devise%20an%20enhanced%0Atraining%20strategy%20to%20address%20the%20challenges%20of%20component%20variation%20between%20seen%0Aand%20unseen%20compositions%20and%20to%20handle%20the%20subtle%20balance%20between%20learning%20seen%0Aand%20unseen%20actions.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%0Aframework%20significantly%20surpasses%20the%20existing%20compositional%20generalization%0Amethods%20and%20sets%20a%20new%20state-of-the-art.%20The%20new%20Sth-com%20benchmark%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/RongchangLi/ZSCAR_C2C.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC2C%253A%2520Component-to-Composition%2520Learning%2520for%2520Zero-Shot%2520Compositional%250A%2520%2520Action%2520Recognition%26entry.906535625%3DRongchang%2520Li%2520and%2520Zhenhua%2520Feng%2520and%2520Tianyang%2520Xu%2520and%2520Linze%2520Li%2520and%2520Xiao-Jun%2520Wu%2520and%2520Muhammad%2520Awais%2520and%2520Sara%2520Atito%2520and%2520Josef%2520Kittler%26entry.1292438233%3D%2520%2520Compositional%2520actions%2520consist%2520of%2520dynamic%2520%2528verbs%2529%2520and%2520static%2520%2528objects%2529%250Aconcepts.%2520Humans%2520can%2520easily%2520recognize%2520unseen%2520compositions%2520using%2520the%2520learned%250Aconcepts.%2520For%2520machines%252C%2520solving%2520such%2520a%2520problem%2520requires%2520a%2520model%2520to%2520recognize%250Aunseen%2520actions%2520composed%2520of%2520previously%2520observed%2520verbs%2520and%2520objects%252C%2520thus%250Arequiring%252C%2520so-called%252C%2520compositional%2520generalization%2520ability.%2520To%2520facilitate%2520this%250Aresearch%252C%2520we%2520propose%2520a%2520novel%2520Zero-Shot%2520Compositional%2520Action%2520Recognition%250A%2528ZS-CAR%2529%2520task.%2520For%2520evaluating%2520the%2520task%252C%2520we%2520construct%2520a%2520new%2520benchmark%252C%250ASomething-composition%2520%2528Sth-com%2529%252C%2520based%2520on%2520the%2520widely%2520used%2520Something-Something%250AV2%2520dataset.%2520We%2520also%2520propose%2520a%2520novel%2520Component-to-Composition%2520%2528C2C%2529%2520learning%250Amethod%2520to%2520solve%2520the%2520new%2520ZS-CAR%2520task.%2520C2C%2520includes%2520an%2520independent%2520component%250Alearning%2520module%2520and%2520a%2520composition%2520inference%2520module.%2520Last%252C%2520we%2520devise%2520an%2520enhanced%250Atraining%2520strategy%2520to%2520address%2520the%2520challenges%2520of%2520component%2520variation%2520between%2520seen%250Aand%2520unseen%2520compositions%2520and%2520to%2520handle%2520the%2520subtle%2520balance%2520between%2520learning%2520seen%250Aand%2520unseen%2520actions.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%250Aframework%2520significantly%2520surpasses%2520the%2520existing%2520compositional%2520generalization%250Amethods%2520and%2520sets%2520a%2520new%2520state-of-the-art.%2520The%2520new%2520Sth-com%2520benchmark%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//github.com/RongchangLi/ZSCAR_C2C.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C2C%3A%20Component-to-Composition%20Learning%20for%20Zero-Shot%20Compositional%0A%20%20Action%20Recognition&entry.906535625=Rongchang%20Li%20and%20Zhenhua%20Feng%20and%20Tianyang%20Xu%20and%20Linze%20Li%20and%20Xiao-Jun%20Wu%20and%20Muhammad%20Awais%20and%20Sara%20Atito%20and%20Josef%20Kittler&entry.1292438233=%20%20Compositional%20actions%20consist%20of%20dynamic%20%28verbs%29%20and%20static%20%28objects%29%0Aconcepts.%20Humans%20can%20easily%20recognize%20unseen%20compositions%20using%20the%20learned%0Aconcepts.%20For%20machines%2C%20solving%20such%20a%20problem%20requires%20a%20model%20to%20recognize%0Aunseen%20actions%20composed%20of%20previously%20observed%20verbs%20and%20objects%2C%20thus%0Arequiring%2C%20so-called%2C%20compositional%20generalization%20ability.%20To%20facilitate%20this%0Aresearch%2C%20we%20propose%20a%20novel%20Zero-Shot%20Compositional%20Action%20Recognition%0A%28ZS-CAR%29%20task.%20For%20evaluating%20the%20task%2C%20we%20construct%20a%20new%20benchmark%2C%0ASomething-composition%20%28Sth-com%29%2C%20based%20on%20the%20widely%20used%20Something-Something%0AV2%20dataset.%20We%20also%20propose%20a%20novel%20Component-to-Composition%20%28C2C%29%20learning%0Amethod%20to%20solve%20the%20new%20ZS-CAR%20task.%20C2C%20includes%20an%20independent%20component%0Alearning%20module%20and%20a%20composition%20inference%20module.%20Last%2C%20we%20devise%20an%20enhanced%0Atraining%20strategy%20to%20address%20the%20challenges%20of%20component%20variation%20between%20seen%0Aand%20unseen%20compositions%20and%20to%20handle%20the%20subtle%20balance%20between%20learning%20seen%0Aand%20unseen%20actions.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%0Aframework%20significantly%20surpasses%20the%20existing%20compositional%20generalization%0Amethods%20and%20sets%20a%20new%20state-of-the-art.%20The%20new%20Sth-com%20benchmark%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/RongchangLi/ZSCAR_C2C.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06113v1&entry.124074799=Read"},
{"title": "Self-Prior Guided Mamba-UNet Networks for Medical Image Super-Resolution", "author": "Zexin Ji and Beiji Zou and Xiaoyan Kui and Pierre Vera and Su Ruan", "abstract": "  In this paper, we propose a self-prior guided Mamba-UNet network\n(SMamba-UNet) for medical image super-resolution. Existing methods are\nprimarily based on convolutional neural networks (CNNs) or Transformers.\nCNNs-based methods fail to capture long-range dependencies, while\nTransformer-based approaches face heavy calculation challenges due to their\nquadratic computational complexity. Recently, State Space Models (SSMs)\nespecially Mamba have emerged, capable of modeling long-range dependencies with\nlinear computational complexity. Inspired by Mamba, our approach aims to learn\nthe self-prior multi-scale contextual features under Mamba-UNet networks, which\nmay help to super-resolve low-resolution medical images in an efficient way.\nSpecifically, we obtain self-priors by perturbing the brightness inpainting of\nthe input image during network training, which can learn detailed texture and\nbrightness information that is beneficial for super-resolution. Furthermore, we\ncombine Mamba with Unet network to mine global features at different levels. We\nalso design an improved 2D-Selective-Scan (ISS2D) module to divide image\nfeatures into different directional sequences to learn long-range dependencies\nin multiple directions, and adaptively fuse sequence information to enhance\nsuper-resolved feature representation. Both qualitative and quantitative\nexperimental results demonstrate that our approach outperforms current\nstate-of-the-art methods on two public medical datasets: the IXI and fastMRI.\n", "link": "http://arxiv.org/abs/2407.05993v1", "date": "2024-07-08", "relevancy": 2.6086, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5224}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5221}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Prior%20Guided%20Mamba-UNet%20Networks%20for%20Medical%20Image%20Super-Resolution&body=Title%3A%20Self-Prior%20Guided%20Mamba-UNet%20Networks%20for%20Medical%20Image%20Super-Resolution%0AAuthor%3A%20Zexin%20Ji%20and%20Beiji%20Zou%20and%20Xiaoyan%20Kui%20and%20Pierre%20Vera%20and%20Su%20Ruan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20self-prior%20guided%20Mamba-UNet%20network%0A%28SMamba-UNet%29%20for%20medical%20image%20super-resolution.%20Existing%20methods%20are%0Aprimarily%20based%20on%20convolutional%20neural%20networks%20%28CNNs%29%20or%20Transformers.%0ACNNs-based%20methods%20fail%20to%20capture%20long-range%20dependencies%2C%20while%0ATransformer-based%20approaches%20face%20heavy%20calculation%20challenges%20due%20to%20their%0Aquadratic%20computational%20complexity.%20Recently%2C%20State%20Space%20Models%20%28SSMs%29%0Aespecially%20Mamba%20have%20emerged%2C%20capable%20of%20modeling%20long-range%20dependencies%20with%0Alinear%20computational%20complexity.%20Inspired%20by%20Mamba%2C%20our%20approach%20aims%20to%20learn%0Athe%20self-prior%20multi-scale%20contextual%20features%20under%20Mamba-UNet%20networks%2C%20which%0Amay%20help%20to%20super-resolve%20low-resolution%20medical%20images%20in%20an%20efficient%20way.%0ASpecifically%2C%20we%20obtain%20self-priors%20by%20perturbing%20the%20brightness%20inpainting%20of%0Athe%20input%20image%20during%20network%20training%2C%20which%20can%20learn%20detailed%20texture%20and%0Abrightness%20information%20that%20is%20beneficial%20for%20super-resolution.%20Furthermore%2C%20we%0Acombine%20Mamba%20with%20Unet%20network%20to%20mine%20global%20features%20at%20different%20levels.%20We%0Aalso%20design%20an%20improved%202D-Selective-Scan%20%28ISS2D%29%20module%20to%20divide%20image%0Afeatures%20into%20different%20directional%20sequences%20to%20learn%20long-range%20dependencies%0Ain%20multiple%20directions%2C%20and%20adaptively%20fuse%20sequence%20information%20to%20enhance%0Asuper-resolved%20feature%20representation.%20Both%20qualitative%20and%20quantitative%0Aexperimental%20results%20demonstrate%20that%20our%20approach%20outperforms%20current%0Astate-of-the-art%20methods%20on%20two%20public%20medical%20datasets%3A%20the%20IXI%20and%20fastMRI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Prior%2520Guided%2520Mamba-UNet%2520Networks%2520for%2520Medical%2520Image%2520Super-Resolution%26entry.906535625%3DZexin%2520Ji%2520and%2520Beiji%2520Zou%2520and%2520Xiaoyan%2520Kui%2520and%2520Pierre%2520Vera%2520and%2520Su%2520Ruan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520self-prior%2520guided%2520Mamba-UNet%2520network%250A%2528SMamba-UNet%2529%2520for%2520medical%2520image%2520super-resolution.%2520Existing%2520methods%2520are%250Aprimarily%2520based%2520on%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520or%2520Transformers.%250ACNNs-based%2520methods%2520fail%2520to%2520capture%2520long-range%2520dependencies%252C%2520while%250ATransformer-based%2520approaches%2520face%2520heavy%2520calculation%2520challenges%2520due%2520to%2520their%250Aquadratic%2520computational%2520complexity.%2520Recently%252C%2520State%2520Space%2520Models%2520%2528SSMs%2529%250Aespecially%2520Mamba%2520have%2520emerged%252C%2520capable%2520of%2520modeling%2520long-range%2520dependencies%2520with%250Alinear%2520computational%2520complexity.%2520Inspired%2520by%2520Mamba%252C%2520our%2520approach%2520aims%2520to%2520learn%250Athe%2520self-prior%2520multi-scale%2520contextual%2520features%2520under%2520Mamba-UNet%2520networks%252C%2520which%250Amay%2520help%2520to%2520super-resolve%2520low-resolution%2520medical%2520images%2520in%2520an%2520efficient%2520way.%250ASpecifically%252C%2520we%2520obtain%2520self-priors%2520by%2520perturbing%2520the%2520brightness%2520inpainting%2520of%250Athe%2520input%2520image%2520during%2520network%2520training%252C%2520which%2520can%2520learn%2520detailed%2520texture%2520and%250Abrightness%2520information%2520that%2520is%2520beneficial%2520for%2520super-resolution.%2520Furthermore%252C%2520we%250Acombine%2520Mamba%2520with%2520Unet%2520network%2520to%2520mine%2520global%2520features%2520at%2520different%2520levels.%2520We%250Aalso%2520design%2520an%2520improved%25202D-Selective-Scan%2520%2528ISS2D%2529%2520module%2520to%2520divide%2520image%250Afeatures%2520into%2520different%2520directional%2520sequences%2520to%2520learn%2520long-range%2520dependencies%250Ain%2520multiple%2520directions%252C%2520and%2520adaptively%2520fuse%2520sequence%2520information%2520to%2520enhance%250Asuper-resolved%2520feature%2520representation.%2520Both%2520qualitative%2520and%2520quantitative%250Aexperimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520current%250Astate-of-the-art%2520methods%2520on%2520two%2520public%2520medical%2520datasets%253A%2520the%2520IXI%2520and%2520fastMRI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Prior%20Guided%20Mamba-UNet%20Networks%20for%20Medical%20Image%20Super-Resolution&entry.906535625=Zexin%20Ji%20and%20Beiji%20Zou%20and%20Xiaoyan%20Kui%20and%20Pierre%20Vera%20and%20Su%20Ruan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20self-prior%20guided%20Mamba-UNet%20network%0A%28SMamba-UNet%29%20for%20medical%20image%20super-resolution.%20Existing%20methods%20are%0Aprimarily%20based%20on%20convolutional%20neural%20networks%20%28CNNs%29%20or%20Transformers.%0ACNNs-based%20methods%20fail%20to%20capture%20long-range%20dependencies%2C%20while%0ATransformer-based%20approaches%20face%20heavy%20calculation%20challenges%20due%20to%20their%0Aquadratic%20computational%20complexity.%20Recently%2C%20State%20Space%20Models%20%28SSMs%29%0Aespecially%20Mamba%20have%20emerged%2C%20capable%20of%20modeling%20long-range%20dependencies%20with%0Alinear%20computational%20complexity.%20Inspired%20by%20Mamba%2C%20our%20approach%20aims%20to%20learn%0Athe%20self-prior%20multi-scale%20contextual%20features%20under%20Mamba-UNet%20networks%2C%20which%0Amay%20help%20to%20super-resolve%20low-resolution%20medical%20images%20in%20an%20efficient%20way.%0ASpecifically%2C%20we%20obtain%20self-priors%20by%20perturbing%20the%20brightness%20inpainting%20of%0Athe%20input%20image%20during%20network%20training%2C%20which%20can%20learn%20detailed%20texture%20and%0Abrightness%20information%20that%20is%20beneficial%20for%20super-resolution.%20Furthermore%2C%20we%0Acombine%20Mamba%20with%20Unet%20network%20to%20mine%20global%20features%20at%20different%20levels.%20We%0Aalso%20design%20an%20improved%202D-Selective-Scan%20%28ISS2D%29%20module%20to%20divide%20image%0Afeatures%20into%20different%20directional%20sequences%20to%20learn%20long-range%20dependencies%0Ain%20multiple%20directions%2C%20and%20adaptively%20fuse%20sequence%20information%20to%20enhance%0Asuper-resolved%20feature%20representation.%20Both%20qualitative%20and%20quantitative%0Aexperimental%20results%20demonstrate%20that%20our%20approach%20outperforms%20current%0Astate-of-the-art%20methods%20on%20two%20public%20medical%20datasets%3A%20the%20IXI%20and%20fastMRI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05993v1&entry.124074799=Read"},
{"title": "Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for\n  Few-Shot Class-Incremental Learning", "author": "Xiaojie Li and Yibo Yang and Jianlong Wu and Bernard Ghanem and Liqiang Nie and Min Zhang", "abstract": "  Few-shot class-incremental learning (FSCIL) confronts the challenge of\nintegrating new classes into a model with minimal training samples while\npreserving the knowledge of previously learned classes. Traditional methods\nwidely adopt static adaptation relying on a fixed parameter space to learn from\ndata that arrive sequentially, prone to overfitting to the current session.\nExisting dynamic strategies require the expansion of the parameter space\ncontinually, leading to increased complexity. To address these challenges, we\nintegrate the recently proposed selective state space model (SSM) into FSCIL.\nConcretely, we propose a dual selective SSM projector that dynamically adjusts\nthe projection parameters based on the intermediate features for dynamic\nadaptation. The dual design enables the model to maintain the robust features\nof base classes, while adaptively learning distinctive feature shifts for novel\nclasses. Additionally, we develop a class-sensitive selective scan mechanism to\nguide dynamic adaptation. It minimizes the disruption to base-class\nrepresentations caused by training on novel data, and meanwhile, forces the\nselective scan to perform in distinct patterns between base and novel classes.\nExperiments on miniImageNet, CUB-200, and CIFAR-100 demonstrate that our\nframework outperforms the existing state-of-the-art methods. The code is\navailable at https://github.com/xiaojieli0903/Mamba-FSCIL.\n", "link": "http://arxiv.org/abs/2407.06136v1", "date": "2024-07-08", "relevancy": 2.6073, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5205}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba-FSCIL%3A%20Dynamic%20Adaptation%20with%20Selective%20State%20Space%20Model%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning&body=Title%3A%20Mamba-FSCIL%3A%20Dynamic%20Adaptation%20with%20Selective%20State%20Space%20Model%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning%0AAuthor%3A%20Xiaojie%20Li%20and%20Yibo%20Yang%20and%20Jianlong%20Wu%20and%20Bernard%20Ghanem%20and%20Liqiang%20Nie%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20confronts%20the%20challenge%20of%0Aintegrating%20new%20classes%20into%20a%20model%20with%20minimal%20training%20samples%20while%0Apreserving%20the%20knowledge%20of%20previously%20learned%20classes.%20Traditional%20methods%0Awidely%20adopt%20static%20adaptation%20relying%20on%20a%20fixed%20parameter%20space%20to%20learn%20from%0Adata%20that%20arrive%20sequentially%2C%20prone%20to%20overfitting%20to%20the%20current%20session.%0AExisting%20dynamic%20strategies%20require%20the%20expansion%20of%20the%20parameter%20space%0Acontinually%2C%20leading%20to%20increased%20complexity.%20To%20address%20these%20challenges%2C%20we%0Aintegrate%20the%20recently%20proposed%20selective%20state%20space%20model%20%28SSM%29%20into%20FSCIL.%0AConcretely%2C%20we%20propose%20a%20dual%20selective%20SSM%20projector%20that%20dynamically%20adjusts%0Athe%20projection%20parameters%20based%20on%20the%20intermediate%20features%20for%20dynamic%0Aadaptation.%20The%20dual%20design%20enables%20the%20model%20to%20maintain%20the%20robust%20features%0Aof%20base%20classes%2C%20while%20adaptively%20learning%20distinctive%20feature%20shifts%20for%20novel%0Aclasses.%20Additionally%2C%20we%20develop%20a%20class-sensitive%20selective%20scan%20mechanism%20to%0Aguide%20dynamic%20adaptation.%20It%20minimizes%20the%20disruption%20to%20base-class%0Arepresentations%20caused%20by%20training%20on%20novel%20data%2C%20and%20meanwhile%2C%20forces%20the%0Aselective%20scan%20to%20perform%20in%20distinct%20patterns%20between%20base%20and%20novel%20classes.%0AExperiments%20on%20miniImageNet%2C%20CUB-200%2C%20and%20CIFAR-100%20demonstrate%20that%20our%0Aframework%20outperforms%20the%20existing%20state-of-the-art%20methods.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/xiaojieli0903/Mamba-FSCIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba-FSCIL%253A%2520Dynamic%2520Adaptation%2520with%2520Selective%2520State%2520Space%2520Model%2520for%250A%2520%2520Few-Shot%2520Class-Incremental%2520Learning%26entry.906535625%3DXiaojie%2520Li%2520and%2520Yibo%2520Yang%2520and%2520Jianlong%2520Wu%2520and%2520Bernard%2520Ghanem%2520and%2520Liqiang%2520Nie%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Few-shot%2520class-incremental%2520learning%2520%2528FSCIL%2529%2520confronts%2520the%2520challenge%2520of%250Aintegrating%2520new%2520classes%2520into%2520a%2520model%2520with%2520minimal%2520training%2520samples%2520while%250Apreserving%2520the%2520knowledge%2520of%2520previously%2520learned%2520classes.%2520Traditional%2520methods%250Awidely%2520adopt%2520static%2520adaptation%2520relying%2520on%2520a%2520fixed%2520parameter%2520space%2520to%2520learn%2520from%250Adata%2520that%2520arrive%2520sequentially%252C%2520prone%2520to%2520overfitting%2520to%2520the%2520current%2520session.%250AExisting%2520dynamic%2520strategies%2520require%2520the%2520expansion%2520of%2520the%2520parameter%2520space%250Acontinually%252C%2520leading%2520to%2520increased%2520complexity.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintegrate%2520the%2520recently%2520proposed%2520selective%2520state%2520space%2520model%2520%2528SSM%2529%2520into%2520FSCIL.%250AConcretely%252C%2520we%2520propose%2520a%2520dual%2520selective%2520SSM%2520projector%2520that%2520dynamically%2520adjusts%250Athe%2520projection%2520parameters%2520based%2520on%2520the%2520intermediate%2520features%2520for%2520dynamic%250Aadaptation.%2520The%2520dual%2520design%2520enables%2520the%2520model%2520to%2520maintain%2520the%2520robust%2520features%250Aof%2520base%2520classes%252C%2520while%2520adaptively%2520learning%2520distinctive%2520feature%2520shifts%2520for%2520novel%250Aclasses.%2520Additionally%252C%2520we%2520develop%2520a%2520class-sensitive%2520selective%2520scan%2520mechanism%2520to%250Aguide%2520dynamic%2520adaptation.%2520It%2520minimizes%2520the%2520disruption%2520to%2520base-class%250Arepresentations%2520caused%2520by%2520training%2520on%2520novel%2520data%252C%2520and%2520meanwhile%252C%2520forces%2520the%250Aselective%2520scan%2520to%2520perform%2520in%2520distinct%2520patterns%2520between%2520base%2520and%2520novel%2520classes.%250AExperiments%2520on%2520miniImageNet%252C%2520CUB-200%252C%2520and%2520CIFAR-100%2520demonstrate%2520that%2520our%250Aframework%2520outperforms%2520the%2520existing%2520state-of-the-art%2520methods.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/xiaojieli0903/Mamba-FSCIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba-FSCIL%3A%20Dynamic%20Adaptation%20with%20Selective%20State%20Space%20Model%20for%0A%20%20Few-Shot%20Class-Incremental%20Learning&entry.906535625=Xiaojie%20Li%20and%20Yibo%20Yang%20and%20Jianlong%20Wu%20and%20Bernard%20Ghanem%20and%20Liqiang%20Nie%20and%20Min%20Zhang&entry.1292438233=%20%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20confronts%20the%20challenge%20of%0Aintegrating%20new%20classes%20into%20a%20model%20with%20minimal%20training%20samples%20while%0Apreserving%20the%20knowledge%20of%20previously%20learned%20classes.%20Traditional%20methods%0Awidely%20adopt%20static%20adaptation%20relying%20on%20a%20fixed%20parameter%20space%20to%20learn%20from%0Adata%20that%20arrive%20sequentially%2C%20prone%20to%20overfitting%20to%20the%20current%20session.%0AExisting%20dynamic%20strategies%20require%20the%20expansion%20of%20the%20parameter%20space%0Acontinually%2C%20leading%20to%20increased%20complexity.%20To%20address%20these%20challenges%2C%20we%0Aintegrate%20the%20recently%20proposed%20selective%20state%20space%20model%20%28SSM%29%20into%20FSCIL.%0AConcretely%2C%20we%20propose%20a%20dual%20selective%20SSM%20projector%20that%20dynamically%20adjusts%0Athe%20projection%20parameters%20based%20on%20the%20intermediate%20features%20for%20dynamic%0Aadaptation.%20The%20dual%20design%20enables%20the%20model%20to%20maintain%20the%20robust%20features%0Aof%20base%20classes%2C%20while%20adaptively%20learning%20distinctive%20feature%20shifts%20for%20novel%0Aclasses.%20Additionally%2C%20we%20develop%20a%20class-sensitive%20selective%20scan%20mechanism%20to%0Aguide%20dynamic%20adaptation.%20It%20minimizes%20the%20disruption%20to%20base-class%0Arepresentations%20caused%20by%20training%20on%20novel%20data%2C%20and%20meanwhile%2C%20forces%20the%0Aselective%20scan%20to%20perform%20in%20distinct%20patterns%20between%20base%20and%20novel%20classes.%0AExperiments%20on%20miniImageNet%2C%20CUB-200%2C%20and%20CIFAR-100%20demonstrate%20that%20our%0Aframework%20outperforms%20the%20existing%20state-of-the-art%20methods.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/xiaojieli0903/Mamba-FSCIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06136v1&entry.124074799=Read"},
{"title": "Learning Dynamics from Multicellular Graphs with Deep Neural Networks", "author": "Haiqian Yang and Florian Meyer and Shaoxun Huang and Liu Yang and Cristiana Lungu and Monilola A. Olayioye and Markus J. Buehler and Ming Guo", "abstract": "  Multicellular self-assembly into functional structures is a dynamic process\nthat is critical in the development and diseases, including embryo development,\norgan formation, tumor invasion, and others. Being able to infer collective\ncell migratory dynamics from their static configuration is valuable for both\nunderstanding and predicting these complex processes. However, the\nidentification of structural features that can indicate multicellular motion\nhas been difficult, and existing metrics largely rely on physical instincts.\nHere we show that using a graph neural network (GNN), the motion of\nmulticellular collectives can be inferred from a static snapshot of cell\npositions, in both experimental and synthetic datasets.\n", "link": "http://arxiv.org/abs/2401.12196v2", "date": "2024-07-08", "relevancy": 2.6021, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5435}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5125}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamics%20from%20Multicellular%20Graphs%20with%20Deep%20Neural%20Networks&body=Title%3A%20Learning%20Dynamics%20from%20Multicellular%20Graphs%20with%20Deep%20Neural%20Networks%0AAuthor%3A%20Haiqian%20Yang%20and%20Florian%20Meyer%20and%20Shaoxun%20Huang%20and%20Liu%20Yang%20and%20Cristiana%20Lungu%20and%20Monilola%20A.%20Olayioye%20and%20Markus%20J.%20Buehler%20and%20Ming%20Guo%0AAbstract%3A%20%20%20Multicellular%20self-assembly%20into%20functional%20structures%20is%20a%20dynamic%20process%0Athat%20is%20critical%20in%20the%20development%20and%20diseases%2C%20including%20embryo%20development%2C%0Aorgan%20formation%2C%20tumor%20invasion%2C%20and%20others.%20Being%20able%20to%20infer%20collective%0Acell%20migratory%20dynamics%20from%20their%20static%20configuration%20is%20valuable%20for%20both%0Aunderstanding%20and%20predicting%20these%20complex%20processes.%20However%2C%20the%0Aidentification%20of%20structural%20features%20that%20can%20indicate%20multicellular%20motion%0Ahas%20been%20difficult%2C%20and%20existing%20metrics%20largely%20rely%20on%20physical%20instincts.%0AHere%20we%20show%20that%20using%20a%20graph%20neural%20network%20%28GNN%29%2C%20the%20motion%20of%0Amulticellular%20collectives%20can%20be%20inferred%20from%20a%20static%20snapshot%20of%20cell%0Apositions%2C%20in%20both%20experimental%20and%20synthetic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamics%2520from%2520Multicellular%2520Graphs%2520with%2520Deep%2520Neural%2520Networks%26entry.906535625%3DHaiqian%2520Yang%2520and%2520Florian%2520Meyer%2520and%2520Shaoxun%2520Huang%2520and%2520Liu%2520Yang%2520and%2520Cristiana%2520Lungu%2520and%2520Monilola%2520A.%2520Olayioye%2520and%2520Markus%2520J.%2520Buehler%2520and%2520Ming%2520Guo%26entry.1292438233%3D%2520%2520Multicellular%2520self-assembly%2520into%2520functional%2520structures%2520is%2520a%2520dynamic%2520process%250Athat%2520is%2520critical%2520in%2520the%2520development%2520and%2520diseases%252C%2520including%2520embryo%2520development%252C%250Aorgan%2520formation%252C%2520tumor%2520invasion%252C%2520and%2520others.%2520Being%2520able%2520to%2520infer%2520collective%250Acell%2520migratory%2520dynamics%2520from%2520their%2520static%2520configuration%2520is%2520valuable%2520for%2520both%250Aunderstanding%2520and%2520predicting%2520these%2520complex%2520processes.%2520However%252C%2520the%250Aidentification%2520of%2520structural%2520features%2520that%2520can%2520indicate%2520multicellular%2520motion%250Ahas%2520been%2520difficult%252C%2520and%2520existing%2520metrics%2520largely%2520rely%2520on%2520physical%2520instincts.%250AHere%2520we%2520show%2520that%2520using%2520a%2520graph%2520neural%2520network%2520%2528GNN%2529%252C%2520the%2520motion%2520of%250Amulticellular%2520collectives%2520can%2520be%2520inferred%2520from%2520a%2520static%2520snapshot%2520of%2520cell%250Apositions%252C%2520in%2520both%2520experimental%2520and%2520synthetic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamics%20from%20Multicellular%20Graphs%20with%20Deep%20Neural%20Networks&entry.906535625=Haiqian%20Yang%20and%20Florian%20Meyer%20and%20Shaoxun%20Huang%20and%20Liu%20Yang%20and%20Cristiana%20Lungu%20and%20Monilola%20A.%20Olayioye%20and%20Markus%20J.%20Buehler%20and%20Ming%20Guo&entry.1292438233=%20%20Multicellular%20self-assembly%20into%20functional%20structures%20is%20a%20dynamic%20process%0Athat%20is%20critical%20in%20the%20development%20and%20diseases%2C%20including%20embryo%20development%2C%0Aorgan%20formation%2C%20tumor%20invasion%2C%20and%20others.%20Being%20able%20to%20infer%20collective%0Acell%20migratory%20dynamics%20from%20their%20static%20configuration%20is%20valuable%20for%20both%0Aunderstanding%20and%20predicting%20these%20complex%20processes.%20However%2C%20the%0Aidentification%20of%20structural%20features%20that%20can%20indicate%20multicellular%20motion%0Ahas%20been%20difficult%2C%20and%20existing%20metrics%20largely%20rely%20on%20physical%20instincts.%0AHere%20we%20show%20that%20using%20a%20graph%20neural%20network%20%28GNN%29%2C%20the%20motion%20of%0Amulticellular%20collectives%20can%20be%20inferred%20from%20a%20static%20snapshot%20of%20cell%0Apositions%2C%20in%20both%20experimental%20and%20synthetic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12196v2&entry.124074799=Read"},
{"title": "LPGD: A General Framework for Backpropagation through Embedded\n  Optimization Layers", "author": "Anselm Paulus and Georg Martius and V\u00edt Musil", "abstract": "  Embedding parameterized optimization problems as layers into machine learning\narchitectures serves as a powerful inductive bias. Training such architectures\nwith stochastic gradient descent requires care, as degenerate derivatives of\nthe embedded optimization problem often render the gradients uninformative. We\npropose Lagrangian Proximal Gradient Descent (LPGD) a flexible framework for\ntraining architectures with embedded optimization layers that seamlessly\nintegrates into automatic differentiation libraries. LPGD efficiently computes\nmeaningful replacements of the degenerate optimization layer derivatives by\nre-running the forward solver oracle on a perturbed input. LPGD captures\nvarious previously proposed methods as special cases, while fostering deep\nlinks to traditional optimization methods. We theoretically analyze our method\nand demonstrate on historical and synthetic data that LPGD converges faster\nthan gradient descent even in a differentiable setup.\n", "link": "http://arxiv.org/abs/2407.05920v1", "date": "2024-07-08", "relevancy": 2.5924, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5541}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5025}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LPGD%3A%20A%20General%20Framework%20for%20Backpropagation%20through%20Embedded%0A%20%20Optimization%20Layers&body=Title%3A%20LPGD%3A%20A%20General%20Framework%20for%20Backpropagation%20through%20Embedded%0A%20%20Optimization%20Layers%0AAuthor%3A%20Anselm%20Paulus%20and%20Georg%20Martius%20and%20V%C3%ADt%20Musil%0AAbstract%3A%20%20%20Embedding%20parameterized%20optimization%20problems%20as%20layers%20into%20machine%20learning%0Aarchitectures%20serves%20as%20a%20powerful%20inductive%20bias.%20Training%20such%20architectures%0Awith%20stochastic%20gradient%20descent%20requires%20care%2C%20as%20degenerate%20derivatives%20of%0Athe%20embedded%20optimization%20problem%20often%20render%20the%20gradients%20uninformative.%20We%0Apropose%20Lagrangian%20Proximal%20Gradient%20Descent%20%28LPGD%29%20a%20flexible%20framework%20for%0Atraining%20architectures%20with%20embedded%20optimization%20layers%20that%20seamlessly%0Aintegrates%20into%20automatic%20differentiation%20libraries.%20LPGD%20efficiently%20computes%0Ameaningful%20replacements%20of%20the%20degenerate%20optimization%20layer%20derivatives%20by%0Are-running%20the%20forward%20solver%20oracle%20on%20a%20perturbed%20input.%20LPGD%20captures%0Avarious%20previously%20proposed%20methods%20as%20special%20cases%2C%20while%20fostering%20deep%0Alinks%20to%20traditional%20optimization%20methods.%20We%20theoretically%20analyze%20our%20method%0Aand%20demonstrate%20on%20historical%20and%20synthetic%20data%20that%20LPGD%20converges%20faster%0Athan%20gradient%20descent%20even%20in%20a%20differentiable%20setup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLPGD%253A%2520A%2520General%2520Framework%2520for%2520Backpropagation%2520through%2520Embedded%250A%2520%2520Optimization%2520Layers%26entry.906535625%3DAnselm%2520Paulus%2520and%2520Georg%2520Martius%2520and%2520V%25C3%25ADt%2520Musil%26entry.1292438233%3D%2520%2520Embedding%2520parameterized%2520optimization%2520problems%2520as%2520layers%2520into%2520machine%2520learning%250Aarchitectures%2520serves%2520as%2520a%2520powerful%2520inductive%2520bias.%2520Training%2520such%2520architectures%250Awith%2520stochastic%2520gradient%2520descent%2520requires%2520care%252C%2520as%2520degenerate%2520derivatives%2520of%250Athe%2520embedded%2520optimization%2520problem%2520often%2520render%2520the%2520gradients%2520uninformative.%2520We%250Apropose%2520Lagrangian%2520Proximal%2520Gradient%2520Descent%2520%2528LPGD%2529%2520a%2520flexible%2520framework%2520for%250Atraining%2520architectures%2520with%2520embedded%2520optimization%2520layers%2520that%2520seamlessly%250Aintegrates%2520into%2520automatic%2520differentiation%2520libraries.%2520LPGD%2520efficiently%2520computes%250Ameaningful%2520replacements%2520of%2520the%2520degenerate%2520optimization%2520layer%2520derivatives%2520by%250Are-running%2520the%2520forward%2520solver%2520oracle%2520on%2520a%2520perturbed%2520input.%2520LPGD%2520captures%250Avarious%2520previously%2520proposed%2520methods%2520as%2520special%2520cases%252C%2520while%2520fostering%2520deep%250Alinks%2520to%2520traditional%2520optimization%2520methods.%2520We%2520theoretically%2520analyze%2520our%2520method%250Aand%2520demonstrate%2520on%2520historical%2520and%2520synthetic%2520data%2520that%2520LPGD%2520converges%2520faster%250Athan%2520gradient%2520descent%2520even%2520in%2520a%2520differentiable%2520setup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPGD%3A%20A%20General%20Framework%20for%20Backpropagation%20through%20Embedded%0A%20%20Optimization%20Layers&entry.906535625=Anselm%20Paulus%20and%20Georg%20Martius%20and%20V%C3%ADt%20Musil&entry.1292438233=%20%20Embedding%20parameterized%20optimization%20problems%20as%20layers%20into%20machine%20learning%0Aarchitectures%20serves%20as%20a%20powerful%20inductive%20bias.%20Training%20such%20architectures%0Awith%20stochastic%20gradient%20descent%20requires%20care%2C%20as%20degenerate%20derivatives%20of%0Athe%20embedded%20optimization%20problem%20often%20render%20the%20gradients%20uninformative.%20We%0Apropose%20Lagrangian%20Proximal%20Gradient%20Descent%20%28LPGD%29%20a%20flexible%20framework%20for%0Atraining%20architectures%20with%20embedded%20optimization%20layers%20that%20seamlessly%0Aintegrates%20into%20automatic%20differentiation%20libraries.%20LPGD%20efficiently%20computes%0Ameaningful%20replacements%20of%20the%20degenerate%20optimization%20layer%20derivatives%20by%0Are-running%20the%20forward%20solver%20oracle%20on%20a%20perturbed%20input.%20LPGD%20captures%0Avarious%20previously%20proposed%20methods%20as%20special%20cases%2C%20while%20fostering%20deep%0Alinks%20to%20traditional%20optimization%20methods.%20We%20theoretically%20analyze%20our%20method%0Aand%20demonstrate%20on%20historical%20and%20synthetic%20data%20that%20LPGD%20converges%20faster%0Athan%20gradient%20descent%20even%20in%20a%20differentiable%20setup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05920v1&entry.124074799=Read"},
{"title": "Deciphering the Role of Representation Disentanglement: Investigating\n  Compositional Generalization in CLIP Models", "author": "Reza Abbasi and Mohammad Hossein Rohban and Mahdieh Soleymani Baghshah", "abstract": "  CLIP models have recently shown to exhibit Out of Distribution (OoD)\ngeneralization capabilities. However, Compositional Out of Distribution (C-OoD)\ngeneralization, which is a crucial aspect of a model's ability to understand\nunseen compositions of known concepts, is relatively unexplored for the CLIP\nmodels. Our goal is to address this problem and identify the factors that\ncontribute to the C-OoD in CLIPs. We noted that previous studies regarding\ncompositional understanding of CLIPs frequently fail to ensure that test\nsamples are genuinely novel relative to the CLIP training data. To this end, we\ncarefully synthesized a large and diverse dataset in the single object setting,\ncomprising attributes for objects that are highly unlikely to be encountered in\nthe combined training datasets of various CLIP models. This dataset enables an\nauthentic evaluation of C-OoD generalization. Our observations reveal varying\nlevels of C-OoD generalization across different CLIP models. We propose that\nthe disentanglement of CLIP representations serves as a critical indicator in\nthis context. By utilizing our synthesized datasets and other existing\ndatasets, we assess various disentanglement metrics of text and image\nrepresentations. Our study reveals that the disentanglement of image and text\nrepresentations, particularly with respect to their compositional elements,\nplays a crucial role in improving the generalization of CLIP models in\nout-of-distribution settings. This finding suggests promising opportunities for\nadvancing out-of-distribution generalization in CLIPs.\n", "link": "http://arxiv.org/abs/2407.05897v1", "date": "2024-07-08", "relevancy": 2.5849, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5235}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deciphering%20the%20Role%20of%20Representation%20Disentanglement%3A%20Investigating%0A%20%20Compositional%20Generalization%20in%20CLIP%20Models&body=Title%3A%20Deciphering%20the%20Role%20of%20Representation%20Disentanglement%3A%20Investigating%0A%20%20Compositional%20Generalization%20in%20CLIP%20Models%0AAuthor%3A%20Reza%20Abbasi%20and%20Mohammad%20Hossein%20Rohban%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20CLIP%20models%20have%20recently%20shown%20to%20exhibit%20Out%20of%20Distribution%20%28OoD%29%0Ageneralization%20capabilities.%20However%2C%20Compositional%20Out%20of%20Distribution%20%28C-OoD%29%0Ageneralization%2C%20which%20is%20a%20crucial%20aspect%20of%20a%20model%27s%20ability%20to%20understand%0Aunseen%20compositions%20of%20known%20concepts%2C%20is%20relatively%20unexplored%20for%20the%20CLIP%0Amodels.%20Our%20goal%20is%20to%20address%20this%20problem%20and%20identify%20the%20factors%20that%0Acontribute%20to%20the%20C-OoD%20in%20CLIPs.%20We%20noted%20that%20previous%20studies%20regarding%0Acompositional%20understanding%20of%20CLIPs%20frequently%20fail%20to%20ensure%20that%20test%0Asamples%20are%20genuinely%20novel%20relative%20to%20the%20CLIP%20training%20data.%20To%20this%20end%2C%20we%0Acarefully%20synthesized%20a%20large%20and%20diverse%20dataset%20in%20the%20single%20object%20setting%2C%0Acomprising%20attributes%20for%20objects%20that%20are%20highly%20unlikely%20to%20be%20encountered%20in%0Athe%20combined%20training%20datasets%20of%20various%20CLIP%20models.%20This%20dataset%20enables%20an%0Aauthentic%20evaluation%20of%20C-OoD%20generalization.%20Our%20observations%20reveal%20varying%0Alevels%20of%20C-OoD%20generalization%20across%20different%20CLIP%20models.%20We%20propose%20that%0Athe%20disentanglement%20of%20CLIP%20representations%20serves%20as%20a%20critical%20indicator%20in%0Athis%20context.%20By%20utilizing%20our%20synthesized%20datasets%20and%20other%20existing%0Adatasets%2C%20we%20assess%20various%20disentanglement%20metrics%20of%20text%20and%20image%0Arepresentations.%20Our%20study%20reveals%20that%20the%20disentanglement%20of%20image%20and%20text%0Arepresentations%2C%20particularly%20with%20respect%20to%20their%20compositional%20elements%2C%0Aplays%20a%20crucial%20role%20in%20improving%20the%20generalization%20of%20CLIP%20models%20in%0Aout-of-distribution%20settings.%20This%20finding%20suggests%20promising%20opportunities%20for%0Aadvancing%20out-of-distribution%20generalization%20in%20CLIPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeciphering%2520the%2520Role%2520of%2520Representation%2520Disentanglement%253A%2520Investigating%250A%2520%2520Compositional%2520Generalization%2520in%2520CLIP%2520Models%26entry.906535625%3DReza%2520Abbasi%2520and%2520Mohammad%2520Hossein%2520Rohban%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520CLIP%2520models%2520have%2520recently%2520shown%2520to%2520exhibit%2520Out%2520of%2520Distribution%2520%2528OoD%2529%250Ageneralization%2520capabilities.%2520However%252C%2520Compositional%2520Out%2520of%2520Distribution%2520%2528C-OoD%2529%250Ageneralization%252C%2520which%2520is%2520a%2520crucial%2520aspect%2520of%2520a%2520model%2527s%2520ability%2520to%2520understand%250Aunseen%2520compositions%2520of%2520known%2520concepts%252C%2520is%2520relatively%2520unexplored%2520for%2520the%2520CLIP%250Amodels.%2520Our%2520goal%2520is%2520to%2520address%2520this%2520problem%2520and%2520identify%2520the%2520factors%2520that%250Acontribute%2520to%2520the%2520C-OoD%2520in%2520CLIPs.%2520We%2520noted%2520that%2520previous%2520studies%2520regarding%250Acompositional%2520understanding%2520of%2520CLIPs%2520frequently%2520fail%2520to%2520ensure%2520that%2520test%250Asamples%2520are%2520genuinely%2520novel%2520relative%2520to%2520the%2520CLIP%2520training%2520data.%2520To%2520this%2520end%252C%2520we%250Acarefully%2520synthesized%2520a%2520large%2520and%2520diverse%2520dataset%2520in%2520the%2520single%2520object%2520setting%252C%250Acomprising%2520attributes%2520for%2520objects%2520that%2520are%2520highly%2520unlikely%2520to%2520be%2520encountered%2520in%250Athe%2520combined%2520training%2520datasets%2520of%2520various%2520CLIP%2520models.%2520This%2520dataset%2520enables%2520an%250Aauthentic%2520evaluation%2520of%2520C-OoD%2520generalization.%2520Our%2520observations%2520reveal%2520varying%250Alevels%2520of%2520C-OoD%2520generalization%2520across%2520different%2520CLIP%2520models.%2520We%2520propose%2520that%250Athe%2520disentanglement%2520of%2520CLIP%2520representations%2520serves%2520as%2520a%2520critical%2520indicator%2520in%250Athis%2520context.%2520By%2520utilizing%2520our%2520synthesized%2520datasets%2520and%2520other%2520existing%250Adatasets%252C%2520we%2520assess%2520various%2520disentanglement%2520metrics%2520of%2520text%2520and%2520image%250Arepresentations.%2520Our%2520study%2520reveals%2520that%2520the%2520disentanglement%2520of%2520image%2520and%2520text%250Arepresentations%252C%2520particularly%2520with%2520respect%2520to%2520their%2520compositional%2520elements%252C%250Aplays%2520a%2520crucial%2520role%2520in%2520improving%2520the%2520generalization%2520of%2520CLIP%2520models%2520in%250Aout-of-distribution%2520settings.%2520This%2520finding%2520suggests%2520promising%2520opportunities%2520for%250Aadvancing%2520out-of-distribution%2520generalization%2520in%2520CLIPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20the%20Role%20of%20Representation%20Disentanglement%3A%20Investigating%0A%20%20Compositional%20Generalization%20in%20CLIP%20Models&entry.906535625=Reza%20Abbasi%20and%20Mohammad%20Hossein%20Rohban%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20CLIP%20models%20have%20recently%20shown%20to%20exhibit%20Out%20of%20Distribution%20%28OoD%29%0Ageneralization%20capabilities.%20However%2C%20Compositional%20Out%20of%20Distribution%20%28C-OoD%29%0Ageneralization%2C%20which%20is%20a%20crucial%20aspect%20of%20a%20model%27s%20ability%20to%20understand%0Aunseen%20compositions%20of%20known%20concepts%2C%20is%20relatively%20unexplored%20for%20the%20CLIP%0Amodels.%20Our%20goal%20is%20to%20address%20this%20problem%20and%20identify%20the%20factors%20that%0Acontribute%20to%20the%20C-OoD%20in%20CLIPs.%20We%20noted%20that%20previous%20studies%20regarding%0Acompositional%20understanding%20of%20CLIPs%20frequently%20fail%20to%20ensure%20that%20test%0Asamples%20are%20genuinely%20novel%20relative%20to%20the%20CLIP%20training%20data.%20To%20this%20end%2C%20we%0Acarefully%20synthesized%20a%20large%20and%20diverse%20dataset%20in%20the%20single%20object%20setting%2C%0Acomprising%20attributes%20for%20objects%20that%20are%20highly%20unlikely%20to%20be%20encountered%20in%0Athe%20combined%20training%20datasets%20of%20various%20CLIP%20models.%20This%20dataset%20enables%20an%0Aauthentic%20evaluation%20of%20C-OoD%20generalization.%20Our%20observations%20reveal%20varying%0Alevels%20of%20C-OoD%20generalization%20across%20different%20CLIP%20models.%20We%20propose%20that%0Athe%20disentanglement%20of%20CLIP%20representations%20serves%20as%20a%20critical%20indicator%20in%0Athis%20context.%20By%20utilizing%20our%20synthesized%20datasets%20and%20other%20existing%0Adatasets%2C%20we%20assess%20various%20disentanglement%20metrics%20of%20text%20and%20image%0Arepresentations.%20Our%20study%20reveals%20that%20the%20disentanglement%20of%20image%20and%20text%0Arepresentations%2C%20particularly%20with%20respect%20to%20their%20compositional%20elements%2C%0Aplays%20a%20crucial%20role%20in%20improving%20the%20generalization%20of%20CLIP%20models%20in%0Aout-of-distribution%20settings.%20This%20finding%20suggests%20promising%20opportunities%20for%0Aadvancing%20out-of-distribution%20generalization%20in%20CLIPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05897v1&entry.124074799=Read"},
{"title": "VSViG: Real-time Video-based Seizure Detection via Skeleton-based\n  Spatiotemporal ViG", "author": "Yankun Xu and Junzhe Wang and Yun-Hsuan Chen and Jie Yang and Wenjie Ming and Shuang Wang and Mohamad Sawan", "abstract": "  An accurate and efficient epileptic seizure onset detection can significantly\nbenefit patients. Traditional diagnostic methods, primarily relying on\nelectroencephalograms (EEGs), often result in cumbersome and non-portable\nsolutions, making continuous patient monitoring challenging. The video-based\nseizure detection system is expected to free patients from the constraints of\nscalp or implanted EEG devices and enable remote monitoring in residential\nsettings. Previous video-based methods neither enable all-day monitoring nor\nprovide short detection latency due to insufficient resources and ineffective\npatient action recognition techniques. Additionally, skeleton-based action\nrecognition approaches remain limitations in identifying subtle seizure-related\nactions. To address these challenges, we propose a novel Video-based Seizure\ndetection model via a skeleton-based spatiotemporal Vision Graph neural network\n(VSViG) for its efficient, accurate and timely purpose in real-time scenarios.\nOur experimental results indicate VSViG outperforms previous state-of-the-art\naction recognition models on our collected patients' video data with higher\naccuracy (5.9% error), lower FLOPs (0.4G), and smaller model size (1.4M).\nFurthermore, by integrating a decision-making rule that combines output\nprobabilities and an accumulative function, we achieve a 5.1 s detection\nlatency after EEG onset, a 13.1 s detection advance before clinical onset, and\na zero false detection rate. The project homepage is available at:\nhttps://github.com/xuyankun/VSViG/\n", "link": "http://arxiv.org/abs/2311.14775v2", "date": "2024-07-08", "relevancy": 2.572, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5359}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5071}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VSViG%3A%20Real-time%20Video-based%20Seizure%20Detection%20via%20Skeleton-based%0A%20%20Spatiotemporal%20ViG&body=Title%3A%20VSViG%3A%20Real-time%20Video-based%20Seizure%20Detection%20via%20Skeleton-based%0A%20%20Spatiotemporal%20ViG%0AAuthor%3A%20Yankun%20Xu%20and%20Junzhe%20Wang%20and%20Yun-Hsuan%20Chen%20and%20Jie%20Yang%20and%20Wenjie%20Ming%20and%20Shuang%20Wang%20and%20Mohamad%20Sawan%0AAbstract%3A%20%20%20An%20accurate%20and%20efficient%20epileptic%20seizure%20onset%20detection%20can%20significantly%0Abenefit%20patients.%20Traditional%20diagnostic%20methods%2C%20primarily%20relying%20on%0Aelectroencephalograms%20%28EEGs%29%2C%20often%20result%20in%20cumbersome%20and%20non-portable%0Asolutions%2C%20making%20continuous%20patient%20monitoring%20challenging.%20The%20video-based%0Aseizure%20detection%20system%20is%20expected%20to%20free%20patients%20from%20the%20constraints%20of%0Ascalp%20or%20implanted%20EEG%20devices%20and%20enable%20remote%20monitoring%20in%20residential%0Asettings.%20Previous%20video-based%20methods%20neither%20enable%20all-day%20monitoring%20nor%0Aprovide%20short%20detection%20latency%20due%20to%20insufficient%20resources%20and%20ineffective%0Apatient%20action%20recognition%20techniques.%20Additionally%2C%20skeleton-based%20action%0Arecognition%20approaches%20remain%20limitations%20in%20identifying%20subtle%20seizure-related%0Aactions.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20Video-based%20Seizure%0Adetection%20model%20via%20a%20skeleton-based%20spatiotemporal%20Vision%20Graph%20neural%20network%0A%28VSViG%29%20for%20its%20efficient%2C%20accurate%20and%20timely%20purpose%20in%20real-time%20scenarios.%0AOur%20experimental%20results%20indicate%20VSViG%20outperforms%20previous%20state-of-the-art%0Aaction%20recognition%20models%20on%20our%20collected%20patients%27%20video%20data%20with%20higher%0Aaccuracy%20%285.9%25%20error%29%2C%20lower%20FLOPs%20%280.4G%29%2C%20and%20smaller%20model%20size%20%281.4M%29.%0AFurthermore%2C%20by%20integrating%20a%20decision-making%20rule%20that%20combines%20output%0Aprobabilities%20and%20an%20accumulative%20function%2C%20we%20achieve%20a%205.1%20s%20detection%0Alatency%20after%20EEG%20onset%2C%20a%2013.1%20s%20detection%20advance%20before%20clinical%20onset%2C%20and%0Aa%20zero%20false%20detection%20rate.%20The%20project%20homepage%20is%20available%20at%3A%0Ahttps%3A//github.com/xuyankun/VSViG/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVSViG%253A%2520Real-time%2520Video-based%2520Seizure%2520Detection%2520via%2520Skeleton-based%250A%2520%2520Spatiotemporal%2520ViG%26entry.906535625%3DYankun%2520Xu%2520and%2520Junzhe%2520Wang%2520and%2520Yun-Hsuan%2520Chen%2520and%2520Jie%2520Yang%2520and%2520Wenjie%2520Ming%2520and%2520Shuang%2520Wang%2520and%2520Mohamad%2520Sawan%26entry.1292438233%3D%2520%2520An%2520accurate%2520and%2520efficient%2520epileptic%2520seizure%2520onset%2520detection%2520can%2520significantly%250Abenefit%2520patients.%2520Traditional%2520diagnostic%2520methods%252C%2520primarily%2520relying%2520on%250Aelectroencephalograms%2520%2528EEGs%2529%252C%2520often%2520result%2520in%2520cumbersome%2520and%2520non-portable%250Asolutions%252C%2520making%2520continuous%2520patient%2520monitoring%2520challenging.%2520The%2520video-based%250Aseizure%2520detection%2520system%2520is%2520expected%2520to%2520free%2520patients%2520from%2520the%2520constraints%2520of%250Ascalp%2520or%2520implanted%2520EEG%2520devices%2520and%2520enable%2520remote%2520monitoring%2520in%2520residential%250Asettings.%2520Previous%2520video-based%2520methods%2520neither%2520enable%2520all-day%2520monitoring%2520nor%250Aprovide%2520short%2520detection%2520latency%2520due%2520to%2520insufficient%2520resources%2520and%2520ineffective%250Apatient%2520action%2520recognition%2520techniques.%2520Additionally%252C%2520skeleton-based%2520action%250Arecognition%2520approaches%2520remain%2520limitations%2520in%2520identifying%2520subtle%2520seizure-related%250Aactions.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520Video-based%2520Seizure%250Adetection%2520model%2520via%2520a%2520skeleton-based%2520spatiotemporal%2520Vision%2520Graph%2520neural%2520network%250A%2528VSViG%2529%2520for%2520its%2520efficient%252C%2520accurate%2520and%2520timely%2520purpose%2520in%2520real-time%2520scenarios.%250AOur%2520experimental%2520results%2520indicate%2520VSViG%2520outperforms%2520previous%2520state-of-the-art%250Aaction%2520recognition%2520models%2520on%2520our%2520collected%2520patients%2527%2520video%2520data%2520with%2520higher%250Aaccuracy%2520%25285.9%2525%2520error%2529%252C%2520lower%2520FLOPs%2520%25280.4G%2529%252C%2520and%2520smaller%2520model%2520size%2520%25281.4M%2529.%250AFurthermore%252C%2520by%2520integrating%2520a%2520decision-making%2520rule%2520that%2520combines%2520output%250Aprobabilities%2520and%2520an%2520accumulative%2520function%252C%2520we%2520achieve%2520a%25205.1%2520s%2520detection%250Alatency%2520after%2520EEG%2520onset%252C%2520a%252013.1%2520s%2520detection%2520advance%2520before%2520clinical%2520onset%252C%2520and%250Aa%2520zero%2520false%2520detection%2520rate.%2520The%2520project%2520homepage%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/xuyankun/VSViG/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VSViG%3A%20Real-time%20Video-based%20Seizure%20Detection%20via%20Skeleton-based%0A%20%20Spatiotemporal%20ViG&entry.906535625=Yankun%20Xu%20and%20Junzhe%20Wang%20and%20Yun-Hsuan%20Chen%20and%20Jie%20Yang%20and%20Wenjie%20Ming%20and%20Shuang%20Wang%20and%20Mohamad%20Sawan&entry.1292438233=%20%20An%20accurate%20and%20efficient%20epileptic%20seizure%20onset%20detection%20can%20significantly%0Abenefit%20patients.%20Traditional%20diagnostic%20methods%2C%20primarily%20relying%20on%0Aelectroencephalograms%20%28EEGs%29%2C%20often%20result%20in%20cumbersome%20and%20non-portable%0Asolutions%2C%20making%20continuous%20patient%20monitoring%20challenging.%20The%20video-based%0Aseizure%20detection%20system%20is%20expected%20to%20free%20patients%20from%20the%20constraints%20of%0Ascalp%20or%20implanted%20EEG%20devices%20and%20enable%20remote%20monitoring%20in%20residential%0Asettings.%20Previous%20video-based%20methods%20neither%20enable%20all-day%20monitoring%20nor%0Aprovide%20short%20detection%20latency%20due%20to%20insufficient%20resources%20and%20ineffective%0Apatient%20action%20recognition%20techniques.%20Additionally%2C%20skeleton-based%20action%0Arecognition%20approaches%20remain%20limitations%20in%20identifying%20subtle%20seizure-related%0Aactions.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20Video-based%20Seizure%0Adetection%20model%20via%20a%20skeleton-based%20spatiotemporal%20Vision%20Graph%20neural%20network%0A%28VSViG%29%20for%20its%20efficient%2C%20accurate%20and%20timely%20purpose%20in%20real-time%20scenarios.%0AOur%20experimental%20results%20indicate%20VSViG%20outperforms%20previous%20state-of-the-art%0Aaction%20recognition%20models%20on%20our%20collected%20patients%27%20video%20data%20with%20higher%0Aaccuracy%20%285.9%25%20error%29%2C%20lower%20FLOPs%20%280.4G%29%2C%20and%20smaller%20model%20size%20%281.4M%29.%0AFurthermore%2C%20by%20integrating%20a%20decision-making%20rule%20that%20combines%20output%0Aprobabilities%20and%20an%20accumulative%20function%2C%20we%20achieve%20a%205.1%20s%20detection%0Alatency%20after%20EEG%20onset%2C%20a%2013.1%20s%20detection%20advance%20before%20clinical%20onset%2C%20and%0Aa%20zero%20false%20detection%20rate.%20The%20project%20homepage%20is%20available%20at%3A%0Ahttps%3A//github.com/xuyankun/VSViG/%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14775v2&entry.124074799=Read"},
{"title": "Learning With Generalised Card Representations for \"Magic: The\n  Gathering\"", "author": "Timo Bertram and Johannes F\u00fcrnkranz and Martin M\u00fcller", "abstract": "  A defining feature of collectable card games is the deck building process\nprior to actual gameplay, in which players form their decks according to some\nrestrictions. Learning to build decks is difficult for players and models alike\ndue to the large card variety and highly complex semantics, as well as\nrequiring meaningful card and deck representations when aiming to utilise AI.\nIn addition, regular releases of new card sets lead to unforeseeable\nfluctuations in the available card pool, thus affecting possible deck\nconfigurations and requiring continuous updates. Previous Game AI approaches to\nbuilding decks have often been limited to fixed sets of possible cards, which\ngreatly limits their utility in practice. In this work, we explore possible\ncard representations that generalise to unseen cards, thus greatly extending\nthe real-world utility of AI-based deck building for the game \"Magic: The\nGathering\".We study such representations based on numerical, nominal, and\ntext-based features of cards, card images, and meta information about card\nusage from third-party services. Our results show that while the particular\nchoice of generalised input representation has little effect on learning to\npredict human card selections among known cards, the performance on new, unseen\ncards can be greatly improved. Our generalised model is able to predict 55\\% of\nhuman choices on completely unseen cards, thus showing a deep understanding of\ncard quality and strategy.\n", "link": "http://arxiv.org/abs/2407.05879v1", "date": "2024-07-08", "relevancy": 2.5108, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5092}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5007}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20With%20Generalised%20Card%20Representations%20for%20%22Magic%3A%20The%0A%20%20Gathering%22&body=Title%3A%20Learning%20With%20Generalised%20Card%20Representations%20for%20%22Magic%3A%20The%0A%20%20Gathering%22%0AAuthor%3A%20Timo%20Bertram%20and%20Johannes%20F%C3%BCrnkranz%20and%20Martin%20M%C3%BCller%0AAbstract%3A%20%20%20A%20defining%20feature%20of%20collectable%20card%20games%20is%20the%20deck%20building%20process%0Aprior%20to%20actual%20gameplay%2C%20in%20which%20players%20form%20their%20decks%20according%20to%20some%0Arestrictions.%20Learning%20to%20build%20decks%20is%20difficult%20for%20players%20and%20models%20alike%0Adue%20to%20the%20large%20card%20variety%20and%20highly%20complex%20semantics%2C%20as%20well%20as%0Arequiring%20meaningful%20card%20and%20deck%20representations%20when%20aiming%20to%20utilise%20AI.%0AIn%20addition%2C%20regular%20releases%20of%20new%20card%20sets%20lead%20to%20unforeseeable%0Afluctuations%20in%20the%20available%20card%20pool%2C%20thus%20affecting%20possible%20deck%0Aconfigurations%20and%20requiring%20continuous%20updates.%20Previous%20Game%20AI%20approaches%20to%0Abuilding%20decks%20have%20often%20been%20limited%20to%20fixed%20sets%20of%20possible%20cards%2C%20which%0Agreatly%20limits%20their%20utility%20in%20practice.%20In%20this%20work%2C%20we%20explore%20possible%0Acard%20representations%20that%20generalise%20to%20unseen%20cards%2C%20thus%20greatly%20extending%0Athe%20real-world%20utility%20of%20AI-based%20deck%20building%20for%20the%20game%20%22Magic%3A%20The%0AGathering%22.We%20study%20such%20representations%20based%20on%20numerical%2C%20nominal%2C%20and%0Atext-based%20features%20of%20cards%2C%20card%20images%2C%20and%20meta%20information%20about%20card%0Ausage%20from%20third-party%20services.%20Our%20results%20show%20that%20while%20the%20particular%0Achoice%20of%20generalised%20input%20representation%20has%20little%20effect%20on%20learning%20to%0Apredict%20human%20card%20selections%20among%20known%20cards%2C%20the%20performance%20on%20new%2C%20unseen%0Acards%20can%20be%20greatly%20improved.%20Our%20generalised%20model%20is%20able%20to%20predict%2055%5C%25%20of%0Ahuman%20choices%20on%20completely%20unseen%20cards%2C%20thus%20showing%20a%20deep%20understanding%20of%0Acard%20quality%20and%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520With%2520Generalised%2520Card%2520Representations%2520for%2520%2522Magic%253A%2520The%250A%2520%2520Gathering%2522%26entry.906535625%3DTimo%2520Bertram%2520and%2520Johannes%2520F%25C3%25BCrnkranz%2520and%2520Martin%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520A%2520defining%2520feature%2520of%2520collectable%2520card%2520games%2520is%2520the%2520deck%2520building%2520process%250Aprior%2520to%2520actual%2520gameplay%252C%2520in%2520which%2520players%2520form%2520their%2520decks%2520according%2520to%2520some%250Arestrictions.%2520Learning%2520to%2520build%2520decks%2520is%2520difficult%2520for%2520players%2520and%2520models%2520alike%250Adue%2520to%2520the%2520large%2520card%2520variety%2520and%2520highly%2520complex%2520semantics%252C%2520as%2520well%2520as%250Arequiring%2520meaningful%2520card%2520and%2520deck%2520representations%2520when%2520aiming%2520to%2520utilise%2520AI.%250AIn%2520addition%252C%2520regular%2520releases%2520of%2520new%2520card%2520sets%2520lead%2520to%2520unforeseeable%250Afluctuations%2520in%2520the%2520available%2520card%2520pool%252C%2520thus%2520affecting%2520possible%2520deck%250Aconfigurations%2520and%2520requiring%2520continuous%2520updates.%2520Previous%2520Game%2520AI%2520approaches%2520to%250Abuilding%2520decks%2520have%2520often%2520been%2520limited%2520to%2520fixed%2520sets%2520of%2520possible%2520cards%252C%2520which%250Agreatly%2520limits%2520their%2520utility%2520in%2520practice.%2520In%2520this%2520work%252C%2520we%2520explore%2520possible%250Acard%2520representations%2520that%2520generalise%2520to%2520unseen%2520cards%252C%2520thus%2520greatly%2520extending%250Athe%2520real-world%2520utility%2520of%2520AI-based%2520deck%2520building%2520for%2520the%2520game%2520%2522Magic%253A%2520The%250AGathering%2522.We%2520study%2520such%2520representations%2520based%2520on%2520numerical%252C%2520nominal%252C%2520and%250Atext-based%2520features%2520of%2520cards%252C%2520card%2520images%252C%2520and%2520meta%2520information%2520about%2520card%250Ausage%2520from%2520third-party%2520services.%2520Our%2520results%2520show%2520that%2520while%2520the%2520particular%250Achoice%2520of%2520generalised%2520input%2520representation%2520has%2520little%2520effect%2520on%2520learning%2520to%250Apredict%2520human%2520card%2520selections%2520among%2520known%2520cards%252C%2520the%2520performance%2520on%2520new%252C%2520unseen%250Acards%2520can%2520be%2520greatly%2520improved.%2520Our%2520generalised%2520model%2520is%2520able%2520to%2520predict%252055%255C%2525%2520of%250Ahuman%2520choices%2520on%2520completely%2520unseen%2520cards%252C%2520thus%2520showing%2520a%2520deep%2520understanding%2520of%250Acard%2520quality%2520and%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20With%20Generalised%20Card%20Representations%20for%20%22Magic%3A%20The%0A%20%20Gathering%22&entry.906535625=Timo%20Bertram%20and%20Johannes%20F%C3%BCrnkranz%20and%20Martin%20M%C3%BCller&entry.1292438233=%20%20A%20defining%20feature%20of%20collectable%20card%20games%20is%20the%20deck%20building%20process%0Aprior%20to%20actual%20gameplay%2C%20in%20which%20players%20form%20their%20decks%20according%20to%20some%0Arestrictions.%20Learning%20to%20build%20decks%20is%20difficult%20for%20players%20and%20models%20alike%0Adue%20to%20the%20large%20card%20variety%20and%20highly%20complex%20semantics%2C%20as%20well%20as%0Arequiring%20meaningful%20card%20and%20deck%20representations%20when%20aiming%20to%20utilise%20AI.%0AIn%20addition%2C%20regular%20releases%20of%20new%20card%20sets%20lead%20to%20unforeseeable%0Afluctuations%20in%20the%20available%20card%20pool%2C%20thus%20affecting%20possible%20deck%0Aconfigurations%20and%20requiring%20continuous%20updates.%20Previous%20Game%20AI%20approaches%20to%0Abuilding%20decks%20have%20often%20been%20limited%20to%20fixed%20sets%20of%20possible%20cards%2C%20which%0Agreatly%20limits%20their%20utility%20in%20practice.%20In%20this%20work%2C%20we%20explore%20possible%0Acard%20representations%20that%20generalise%20to%20unseen%20cards%2C%20thus%20greatly%20extending%0Athe%20real-world%20utility%20of%20AI-based%20deck%20building%20for%20the%20game%20%22Magic%3A%20The%0AGathering%22.We%20study%20such%20representations%20based%20on%20numerical%2C%20nominal%2C%20and%0Atext-based%20features%20of%20cards%2C%20card%20images%2C%20and%20meta%20information%20about%20card%0Ausage%20from%20third-party%20services.%20Our%20results%20show%20that%20while%20the%20particular%0Achoice%20of%20generalised%20input%20representation%20has%20little%20effect%20on%20learning%20to%0Apredict%20human%20card%20selections%20among%20known%20cards%2C%20the%20performance%20on%20new%2C%20unseen%0Acards%20can%20be%20greatly%20improved.%20Our%20generalised%20model%20is%20able%20to%20predict%2055%5C%25%20of%0Ahuman%20choices%20on%20completely%20unseen%20cards%2C%20thus%20showing%20a%20deep%20understanding%20of%0Acard%20quality%20and%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05879v1&entry.124074799=Read"},
{"title": "GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting", "author": "Xinjie Zhang and Xingtong Ge and Tongda Xu and Dailan He and Yan Wang and Hongwei Qin and Guo Lu and Jing Geng and Jun Zhang", "abstract": "  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 2000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding. Code is available at\nhttps://github.com/Xinjie-Q/GaussianImage.\n", "link": "http://arxiv.org/abs/2403.08551v4", "date": "2024-07-08", "relevancy": 2.5062, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6665}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6083}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianImage%3A%201000%20FPS%20Image%20Representation%20and%20Compression%20by%202D%0A%20%20Gaussian%20Splatting&body=Title%3A%20GaussianImage%3A%201000%20FPS%20Image%20Representation%20and%20Compression%20by%202D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Xinjie%20Zhang%20and%20Xingtong%20Ge%20and%20Tongda%20Xu%20and%20Dailan%20He%20and%20Yan%20Wang%20and%20Hongwei%20Qin%20and%20Guo%20Lu%20and%20Jing%20Geng%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Implicit%20neural%20representations%20%28INRs%29%20recently%20achieved%20great%20success%20in%0Aimage%20representation%20and%20compression%2C%20offering%20high%20visual%20quality%20and%20fast%0Arendering%20speeds%20with%2010-1000%20FPS%2C%20assuming%20sufficient%20GPU%20resources%20are%0Aavailable.%20However%2C%20this%20requirement%20often%20hinders%20their%20use%20on%20low-end%20devices%0Awith%20limited%20memory.%20In%20response%2C%20we%20propose%20a%20groundbreaking%20paradigm%20of%20image%0Arepresentation%20and%20compression%20by%202D%20Gaussian%20Splatting%2C%20named%20GaussianImage.%0AWe%20first%20introduce%202D%20Gaussian%20to%20represent%20the%20image%2C%20where%20each%20Gaussian%20has%0A8%20parameters%20including%20position%2C%20covariance%20and%20color.%20Subsequently%2C%20we%20unveil%0Aa%20novel%20rendering%20algorithm%20based%20on%20accumulated%20summation.%20Remarkably%2C%20our%0Amethod%20with%20a%20minimum%20of%203%24%5Ctimes%24%20lower%20GPU%20memory%20usage%20and%205%24%5Ctimes%24%20faster%0Afitting%20time%20not%20only%20rivals%20INRs%20%28e.g.%2C%20WIRE%2C%20I-NGP%29%20in%20representation%0Aperformance%2C%20but%20also%20delivers%20a%20faster%20rendering%20speed%20of%201500-2000%20FPS%0Aregardless%20of%20parameter%20size.%20Furthermore%2C%20we%20integrate%20existing%20vector%0Aquantization%20technique%20to%20build%20an%20image%20codec.%20Experimental%20results%0Ademonstrate%20that%20our%20codec%20attains%20rate-distortion%20performance%20comparable%20to%0Acompression-based%20INRs%20such%20as%20COIN%20and%20COIN%2B%2B%2C%20while%20facilitating%20decoding%0Aspeeds%20of%20approximately%202000%20FPS.%20Additionally%2C%20preliminary%20proof%20of%20concept%0Ashows%20that%20our%20codec%20surpasses%20COIN%20and%20COIN%2B%2B%20in%20performance%20when%20using%0Apartial%20bits-back%20coding.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Xinjie-Q/GaussianImage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08551v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianImage%253A%25201000%2520FPS%2520Image%2520Representation%2520and%2520Compression%2520by%25202D%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DXinjie%2520Zhang%2520and%2520Xingtong%2520Ge%2520and%2520Tongda%2520Xu%2520and%2520Dailan%2520He%2520and%2520Yan%2520Wang%2520and%2520Hongwei%2520Qin%2520and%2520Guo%2520Lu%2520and%2520Jing%2520Geng%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representations%2520%2528INRs%2529%2520recently%2520achieved%2520great%2520success%2520in%250Aimage%2520representation%2520and%2520compression%252C%2520offering%2520high%2520visual%2520quality%2520and%2520fast%250Arendering%2520speeds%2520with%252010-1000%2520FPS%252C%2520assuming%2520sufficient%2520GPU%2520resources%2520are%250Aavailable.%2520However%252C%2520this%2520requirement%2520often%2520hinders%2520their%2520use%2520on%2520low-end%2520devices%250Awith%2520limited%2520memory.%2520In%2520response%252C%2520we%2520propose%2520a%2520groundbreaking%2520paradigm%2520of%2520image%250Arepresentation%2520and%2520compression%2520by%25202D%2520Gaussian%2520Splatting%252C%2520named%2520GaussianImage.%250AWe%2520first%2520introduce%25202D%2520Gaussian%2520to%2520represent%2520the%2520image%252C%2520where%2520each%2520Gaussian%2520has%250A8%2520parameters%2520including%2520position%252C%2520covariance%2520and%2520color.%2520Subsequently%252C%2520we%2520unveil%250Aa%2520novel%2520rendering%2520algorithm%2520based%2520on%2520accumulated%2520summation.%2520Remarkably%252C%2520our%250Amethod%2520with%2520a%2520minimum%2520of%25203%2524%255Ctimes%2524%2520lower%2520GPU%2520memory%2520usage%2520and%25205%2524%255Ctimes%2524%2520faster%250Afitting%2520time%2520not%2520only%2520rivals%2520INRs%2520%2528e.g.%252C%2520WIRE%252C%2520I-NGP%2529%2520in%2520representation%250Aperformance%252C%2520but%2520also%2520delivers%2520a%2520faster%2520rendering%2520speed%2520of%25201500-2000%2520FPS%250Aregardless%2520of%2520parameter%2520size.%2520Furthermore%252C%2520we%2520integrate%2520existing%2520vector%250Aquantization%2520technique%2520to%2520build%2520an%2520image%2520codec.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520codec%2520attains%2520rate-distortion%2520performance%2520comparable%2520to%250Acompression-based%2520INRs%2520such%2520as%2520COIN%2520and%2520COIN%252B%252B%252C%2520while%2520facilitating%2520decoding%250Aspeeds%2520of%2520approximately%25202000%2520FPS.%2520Additionally%252C%2520preliminary%2520proof%2520of%2520concept%250Ashows%2520that%2520our%2520codec%2520surpasses%2520COIN%2520and%2520COIN%252B%252B%2520in%2520performance%2520when%2520using%250Apartial%2520bits-back%2520coding.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Xinjie-Q/GaussianImage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08551v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianImage%3A%201000%20FPS%20Image%20Representation%20and%20Compression%20by%202D%0A%20%20Gaussian%20Splatting&entry.906535625=Xinjie%20Zhang%20and%20Xingtong%20Ge%20and%20Tongda%20Xu%20and%20Dailan%20He%20and%20Yan%20Wang%20and%20Hongwei%20Qin%20and%20Guo%20Lu%20and%20Jing%20Geng%20and%20Jun%20Zhang&entry.1292438233=%20%20Implicit%20neural%20representations%20%28INRs%29%20recently%20achieved%20great%20success%20in%0Aimage%20representation%20and%20compression%2C%20offering%20high%20visual%20quality%20and%20fast%0Arendering%20speeds%20with%2010-1000%20FPS%2C%20assuming%20sufficient%20GPU%20resources%20are%0Aavailable.%20However%2C%20this%20requirement%20often%20hinders%20their%20use%20on%20low-end%20devices%0Awith%20limited%20memory.%20In%20response%2C%20we%20propose%20a%20groundbreaking%20paradigm%20of%20image%0Arepresentation%20and%20compression%20by%202D%20Gaussian%20Splatting%2C%20named%20GaussianImage.%0AWe%20first%20introduce%202D%20Gaussian%20to%20represent%20the%20image%2C%20where%20each%20Gaussian%20has%0A8%20parameters%20including%20position%2C%20covariance%20and%20color.%20Subsequently%2C%20we%20unveil%0Aa%20novel%20rendering%20algorithm%20based%20on%20accumulated%20summation.%20Remarkably%2C%20our%0Amethod%20with%20a%20minimum%20of%203%24%5Ctimes%24%20lower%20GPU%20memory%20usage%20and%205%24%5Ctimes%24%20faster%0Afitting%20time%20not%20only%20rivals%20INRs%20%28e.g.%2C%20WIRE%2C%20I-NGP%29%20in%20representation%0Aperformance%2C%20but%20also%20delivers%20a%20faster%20rendering%20speed%20of%201500-2000%20FPS%0Aregardless%20of%20parameter%20size.%20Furthermore%2C%20we%20integrate%20existing%20vector%0Aquantization%20technique%20to%20build%20an%20image%20codec.%20Experimental%20results%0Ademonstrate%20that%20our%20codec%20attains%20rate-distortion%20performance%20comparable%20to%0Acompression-based%20INRs%20such%20as%20COIN%20and%20COIN%2B%2B%2C%20while%20facilitating%20decoding%0Aspeeds%20of%20approximately%202000%20FPS.%20Additionally%2C%20preliminary%20proof%20of%20concept%0Ashows%20that%20our%20codec%20surpasses%20COIN%20and%20COIN%2B%2B%20in%20performance%20when%20using%0Apartial%20bits-back%20coding.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Xinjie-Q/GaussianImage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08551v4&entry.124074799=Read"},
{"title": "HyperMAML: Few-Shot Adaptation of Deep Models with Hypernetworks", "author": "M. Przewi\u0119\u017alikowski and P. Przybysz and J. Tabor and M. Zi\u0119ba and P. Spurek", "abstract": "  The aim of Few-Shot learning methods is to train models which can easily\nadapt to previously unseen tasks, based on small amounts of data. One of the\nmost popular and elegant Few-Shot learning approaches is Model-Agnostic\nMeta-Learning (MAML). The main idea behind this method is to learn the general\nweights of the meta-model, which are further adapted to specific problems in a\nsmall number of gradient steps. However, the model's main limitation lies in\nthe fact that the update procedure is realized by gradient-based optimisation.\nIn consequence, MAML cannot always modify weights to the essential level in one\nor even a few gradient iterations. On the other hand, using many gradient steps\nresults in a complex and time-consuming optimization procedure, which is hard\nto train in practice, and may lead to overfitting. In this paper, we propose\nHyperMAML, a novel generalization of MAML, where the training of the update\nprocedure is also part of the model. Namely, in HyperMAML, instead of updating\nthe weights with gradient descent, we use for this purpose a trainable\nHypernetwork. Consequently, in this framework, the model can generate\nsignificant updates whose range is not limited to a fixed number of gradient\nsteps. Experiments show that HyperMAML consistently outperforms MAML and\nperforms comparably to other state-of-the-art techniques in a number of\nstandard Few-Shot learning benchmarks.\n", "link": "http://arxiv.org/abs/2205.15745v3", "date": "2024-07-08", "relevancy": 2.4941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5121}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4983}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperMAML%3A%20Few-Shot%20Adaptation%20of%20Deep%20Models%20with%20Hypernetworks&body=Title%3A%20HyperMAML%3A%20Few-Shot%20Adaptation%20of%20Deep%20Models%20with%20Hypernetworks%0AAuthor%3A%20M.%20Przewi%C4%99%C5%BAlikowski%20and%20P.%20Przybysz%20and%20J.%20Tabor%20and%20M.%20Zi%C4%99ba%20and%20P.%20Spurek%0AAbstract%3A%20%20%20The%20aim%20of%20Few-Shot%20learning%20methods%20is%20to%20train%20models%20which%20can%20easily%0Aadapt%20to%20previously%20unseen%20tasks%2C%20based%20on%20small%20amounts%20of%20data.%20One%20of%20the%0Amost%20popular%20and%20elegant%20Few-Shot%20learning%20approaches%20is%20Model-Agnostic%0AMeta-Learning%20%28MAML%29.%20The%20main%20idea%20behind%20this%20method%20is%20to%20learn%20the%20general%0Aweights%20of%20the%20meta-model%2C%20which%20are%20further%20adapted%20to%20specific%20problems%20in%20a%0Asmall%20number%20of%20gradient%20steps.%20However%2C%20the%20model%27s%20main%20limitation%20lies%20in%0Athe%20fact%20that%20the%20update%20procedure%20is%20realized%20by%20gradient-based%20optimisation.%0AIn%20consequence%2C%20MAML%20cannot%20always%20modify%20weights%20to%20the%20essential%20level%20in%20one%0Aor%20even%20a%20few%20gradient%20iterations.%20On%20the%20other%20hand%2C%20using%20many%20gradient%20steps%0Aresults%20in%20a%20complex%20and%20time-consuming%20optimization%20procedure%2C%20which%20is%20hard%0Ato%20train%20in%20practice%2C%20and%20may%20lead%20to%20overfitting.%20In%20this%20paper%2C%20we%20propose%0AHyperMAML%2C%20a%20novel%20generalization%20of%20MAML%2C%20where%20the%20training%20of%20the%20update%0Aprocedure%20is%20also%20part%20of%20the%20model.%20Namely%2C%20in%20HyperMAML%2C%20instead%20of%20updating%0Athe%20weights%20with%20gradient%20descent%2C%20we%20use%20for%20this%20purpose%20a%20trainable%0AHypernetwork.%20Consequently%2C%20in%20this%20framework%2C%20the%20model%20can%20generate%0Asignificant%20updates%20whose%20range%20is%20not%20limited%20to%20a%20fixed%20number%20of%20gradient%0Asteps.%20Experiments%20show%20that%20HyperMAML%20consistently%20outperforms%20MAML%20and%0Aperforms%20comparably%20to%20other%20state-of-the-art%20techniques%20in%20a%20number%20of%0Astandard%20Few-Shot%20learning%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.15745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperMAML%253A%2520Few-Shot%2520Adaptation%2520of%2520Deep%2520Models%2520with%2520Hypernetworks%26entry.906535625%3DM.%2520Przewi%25C4%2599%25C5%25BAlikowski%2520and%2520P.%2520Przybysz%2520and%2520J.%2520Tabor%2520and%2520M.%2520Zi%25C4%2599ba%2520and%2520P.%2520Spurek%26entry.1292438233%3D%2520%2520The%2520aim%2520of%2520Few-Shot%2520learning%2520methods%2520is%2520to%2520train%2520models%2520which%2520can%2520easily%250Aadapt%2520to%2520previously%2520unseen%2520tasks%252C%2520based%2520on%2520small%2520amounts%2520of%2520data.%2520One%2520of%2520the%250Amost%2520popular%2520and%2520elegant%2520Few-Shot%2520learning%2520approaches%2520is%2520Model-Agnostic%250AMeta-Learning%2520%2528MAML%2529.%2520The%2520main%2520idea%2520behind%2520this%2520method%2520is%2520to%2520learn%2520the%2520general%250Aweights%2520of%2520the%2520meta-model%252C%2520which%2520are%2520further%2520adapted%2520to%2520specific%2520problems%2520in%2520a%250Asmall%2520number%2520of%2520gradient%2520steps.%2520However%252C%2520the%2520model%2527s%2520main%2520limitation%2520lies%2520in%250Athe%2520fact%2520that%2520the%2520update%2520procedure%2520is%2520realized%2520by%2520gradient-based%2520optimisation.%250AIn%2520consequence%252C%2520MAML%2520cannot%2520always%2520modify%2520weights%2520to%2520the%2520essential%2520level%2520in%2520one%250Aor%2520even%2520a%2520few%2520gradient%2520iterations.%2520On%2520the%2520other%2520hand%252C%2520using%2520many%2520gradient%2520steps%250Aresults%2520in%2520a%2520complex%2520and%2520time-consuming%2520optimization%2520procedure%252C%2520which%2520is%2520hard%250Ato%2520train%2520in%2520practice%252C%2520and%2520may%2520lead%2520to%2520overfitting.%2520In%2520this%2520paper%252C%2520we%2520propose%250AHyperMAML%252C%2520a%2520novel%2520generalization%2520of%2520MAML%252C%2520where%2520the%2520training%2520of%2520the%2520update%250Aprocedure%2520is%2520also%2520part%2520of%2520the%2520model.%2520Namely%252C%2520in%2520HyperMAML%252C%2520instead%2520of%2520updating%250Athe%2520weights%2520with%2520gradient%2520descent%252C%2520we%2520use%2520for%2520this%2520purpose%2520a%2520trainable%250AHypernetwork.%2520Consequently%252C%2520in%2520this%2520framework%252C%2520the%2520model%2520can%2520generate%250Asignificant%2520updates%2520whose%2520range%2520is%2520not%2520limited%2520to%2520a%2520fixed%2520number%2520of%2520gradient%250Asteps.%2520Experiments%2520show%2520that%2520HyperMAML%2520consistently%2520outperforms%2520MAML%2520and%250Aperforms%2520comparably%2520to%2520other%2520state-of-the-art%2520techniques%2520in%2520a%2520number%2520of%250Astandard%2520Few-Shot%2520learning%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.15745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperMAML%3A%20Few-Shot%20Adaptation%20of%20Deep%20Models%20with%20Hypernetworks&entry.906535625=M.%20Przewi%C4%99%C5%BAlikowski%20and%20P.%20Przybysz%20and%20J.%20Tabor%20and%20M.%20Zi%C4%99ba%20and%20P.%20Spurek&entry.1292438233=%20%20The%20aim%20of%20Few-Shot%20learning%20methods%20is%20to%20train%20models%20which%20can%20easily%0Aadapt%20to%20previously%20unseen%20tasks%2C%20based%20on%20small%20amounts%20of%20data.%20One%20of%20the%0Amost%20popular%20and%20elegant%20Few-Shot%20learning%20approaches%20is%20Model-Agnostic%0AMeta-Learning%20%28MAML%29.%20The%20main%20idea%20behind%20this%20method%20is%20to%20learn%20the%20general%0Aweights%20of%20the%20meta-model%2C%20which%20are%20further%20adapted%20to%20specific%20problems%20in%20a%0Asmall%20number%20of%20gradient%20steps.%20However%2C%20the%20model%27s%20main%20limitation%20lies%20in%0Athe%20fact%20that%20the%20update%20procedure%20is%20realized%20by%20gradient-based%20optimisation.%0AIn%20consequence%2C%20MAML%20cannot%20always%20modify%20weights%20to%20the%20essential%20level%20in%20one%0Aor%20even%20a%20few%20gradient%20iterations.%20On%20the%20other%20hand%2C%20using%20many%20gradient%20steps%0Aresults%20in%20a%20complex%20and%20time-consuming%20optimization%20procedure%2C%20which%20is%20hard%0Ato%20train%20in%20practice%2C%20and%20may%20lead%20to%20overfitting.%20In%20this%20paper%2C%20we%20propose%0AHyperMAML%2C%20a%20novel%20generalization%20of%20MAML%2C%20where%20the%20training%20of%20the%20update%0Aprocedure%20is%20also%20part%20of%20the%20model.%20Namely%2C%20in%20HyperMAML%2C%20instead%20of%20updating%0Athe%20weights%20with%20gradient%20descent%2C%20we%20use%20for%20this%20purpose%20a%20trainable%0AHypernetwork.%20Consequently%2C%20in%20this%20framework%2C%20the%20model%20can%20generate%0Asignificant%20updates%20whose%20range%20is%20not%20limited%20to%20a%20fixed%20number%20of%20gradient%0Asteps.%20Experiments%20show%20that%20HyperMAML%20consistently%20outperforms%20MAML%20and%0Aperforms%20comparably%20to%20other%20state-of-the-art%20techniques%20in%20a%20number%20of%0Astandard%20Few-Shot%20learning%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.15745v3&entry.124074799=Read"},
{"title": "Anatomy-guided Pathology Segmentation", "author": "Alexander Jaus and Constantin Seibold and Simon Rei\u00df and Lukas Heine and Anton Schily and Moon Kim and Fin Hendrik Bahnsen and Ken Herrmann and Rainer Stiefelhagen and Jens Kleesiek", "abstract": "  Pathological structures in medical images are typically deviations from the\nexpected anatomy of a patient. While clinicians consider this interplay between\nanatomy and pathology, recent deep learning algorithms specialize in\nrecognizing either one of the two, rarely considering the patient's body from\nsuch a joint perspective. In this paper, we develop a generalist segmentation\nmodel that combines anatomical and pathological information, aiming to enhance\nthe segmentation accuracy of pathological features. Our Anatomy-Pathology\nExchange (APEx) training utilizes a query-based segmentation transformer which\ndecodes a joint feature space into query-representations for human anatomy and\ninterleaves them via a mixing strategy into the pathology-decoder for\nanatomy-informed pathology predictions. In doing so, we are able to report the\nbest results across the board on FDG-PET-CT and Chest X-Ray pathology\nsegmentation tasks with a margin of up to 3.3% as compared to strong baseline\nmethods. Code and models will be publicly available at\ngithub.com/alexanderjaus/APEx.\n", "link": "http://arxiv.org/abs/2407.05844v1", "date": "2024-07-08", "relevancy": 2.4888, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.488}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anatomy-guided%20Pathology%20Segmentation&body=Title%3A%20Anatomy-guided%20Pathology%20Segmentation%0AAuthor%3A%20Alexander%20Jaus%20and%20Constantin%20Seibold%20and%20Simon%20Rei%C3%9F%20and%20Lukas%20Heine%20and%20Anton%20Schily%20and%20Moon%20Kim%20and%20Fin%20Hendrik%20Bahnsen%20and%20Ken%20Herrmann%20and%20Rainer%20Stiefelhagen%20and%20Jens%20Kleesiek%0AAbstract%3A%20%20%20Pathological%20structures%20in%20medical%20images%20are%20typically%20deviations%20from%20the%0Aexpected%20anatomy%20of%20a%20patient.%20While%20clinicians%20consider%20this%20interplay%20between%0Aanatomy%20and%20pathology%2C%20recent%20deep%20learning%20algorithms%20specialize%20in%0Arecognizing%20either%20one%20of%20the%20two%2C%20rarely%20considering%20the%20patient%27s%20body%20from%0Asuch%20a%20joint%20perspective.%20In%20this%20paper%2C%20we%20develop%20a%20generalist%20segmentation%0Amodel%20that%20combines%20anatomical%20and%20pathological%20information%2C%20aiming%20to%20enhance%0Athe%20segmentation%20accuracy%20of%20pathological%20features.%20Our%20Anatomy-Pathology%0AExchange%20%28APEx%29%20training%20utilizes%20a%20query-based%20segmentation%20transformer%20which%0Adecodes%20a%20joint%20feature%20space%20into%20query-representations%20for%20human%20anatomy%20and%0Ainterleaves%20them%20via%20a%20mixing%20strategy%20into%20the%20pathology-decoder%20for%0Aanatomy-informed%20pathology%20predictions.%20In%20doing%20so%2C%20we%20are%20able%20to%20report%20the%0Abest%20results%20across%20the%20board%20on%20FDG-PET-CT%20and%20Chest%20X-Ray%20pathology%0Asegmentation%20tasks%20with%20a%20margin%20of%20up%20to%203.3%25%20as%20compared%20to%20strong%20baseline%0Amethods.%20Code%20and%20models%20will%20be%20publicly%20available%20at%0Agithub.com/alexanderjaus/APEx.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatomy-guided%2520Pathology%2520Segmentation%26entry.906535625%3DAlexander%2520Jaus%2520and%2520Constantin%2520Seibold%2520and%2520Simon%2520Rei%25C3%259F%2520and%2520Lukas%2520Heine%2520and%2520Anton%2520Schily%2520and%2520Moon%2520Kim%2520and%2520Fin%2520Hendrik%2520Bahnsen%2520and%2520Ken%2520Herrmann%2520and%2520Rainer%2520Stiefelhagen%2520and%2520Jens%2520Kleesiek%26entry.1292438233%3D%2520%2520Pathological%2520structures%2520in%2520medical%2520images%2520are%2520typically%2520deviations%2520from%2520the%250Aexpected%2520anatomy%2520of%2520a%2520patient.%2520While%2520clinicians%2520consider%2520this%2520interplay%2520between%250Aanatomy%2520and%2520pathology%252C%2520recent%2520deep%2520learning%2520algorithms%2520specialize%2520in%250Arecognizing%2520either%2520one%2520of%2520the%2520two%252C%2520rarely%2520considering%2520the%2520patient%2527s%2520body%2520from%250Asuch%2520a%2520joint%2520perspective.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520generalist%2520segmentation%250Amodel%2520that%2520combines%2520anatomical%2520and%2520pathological%2520information%252C%2520aiming%2520to%2520enhance%250Athe%2520segmentation%2520accuracy%2520of%2520pathological%2520features.%2520Our%2520Anatomy-Pathology%250AExchange%2520%2528APEx%2529%2520training%2520utilizes%2520a%2520query-based%2520segmentation%2520transformer%2520which%250Adecodes%2520a%2520joint%2520feature%2520space%2520into%2520query-representations%2520for%2520human%2520anatomy%2520and%250Ainterleaves%2520them%2520via%2520a%2520mixing%2520strategy%2520into%2520the%2520pathology-decoder%2520for%250Aanatomy-informed%2520pathology%2520predictions.%2520In%2520doing%2520so%252C%2520we%2520are%2520able%2520to%2520report%2520the%250Abest%2520results%2520across%2520the%2520board%2520on%2520FDG-PET-CT%2520and%2520Chest%2520X-Ray%2520pathology%250Asegmentation%2520tasks%2520with%2520a%2520margin%2520of%2520up%2520to%25203.3%2525%2520as%2520compared%2520to%2520strong%2520baseline%250Amethods.%2520Code%2520and%2520models%2520will%2520be%2520publicly%2520available%2520at%250Agithub.com/alexanderjaus/APEx.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomy-guided%20Pathology%20Segmentation&entry.906535625=Alexander%20Jaus%20and%20Constantin%20Seibold%20and%20Simon%20Rei%C3%9F%20and%20Lukas%20Heine%20and%20Anton%20Schily%20and%20Moon%20Kim%20and%20Fin%20Hendrik%20Bahnsen%20and%20Ken%20Herrmann%20and%20Rainer%20Stiefelhagen%20and%20Jens%20Kleesiek&entry.1292438233=%20%20Pathological%20structures%20in%20medical%20images%20are%20typically%20deviations%20from%20the%0Aexpected%20anatomy%20of%20a%20patient.%20While%20clinicians%20consider%20this%20interplay%20between%0Aanatomy%20and%20pathology%2C%20recent%20deep%20learning%20algorithms%20specialize%20in%0Arecognizing%20either%20one%20of%20the%20two%2C%20rarely%20considering%20the%20patient%27s%20body%20from%0Asuch%20a%20joint%20perspective.%20In%20this%20paper%2C%20we%20develop%20a%20generalist%20segmentation%0Amodel%20that%20combines%20anatomical%20and%20pathological%20information%2C%20aiming%20to%20enhance%0Athe%20segmentation%20accuracy%20of%20pathological%20features.%20Our%20Anatomy-Pathology%0AExchange%20%28APEx%29%20training%20utilizes%20a%20query-based%20segmentation%20transformer%20which%0Adecodes%20a%20joint%20feature%20space%20into%20query-representations%20for%20human%20anatomy%20and%0Ainterleaves%20them%20via%20a%20mixing%20strategy%20into%20the%20pathology-decoder%20for%0Aanatomy-informed%20pathology%20predictions.%20In%20doing%20so%2C%20we%20are%20able%20to%20report%20the%0Abest%20results%20across%20the%20board%20on%20FDG-PET-CT%20and%20Chest%20X-Ray%20pathology%0Asegmentation%20tasks%20with%20a%20margin%20of%20up%20to%203.3%25%20as%20compared%20to%20strong%20baseline%0Amethods.%20Code%20and%20models%20will%20be%20publicly%20available%20at%0Agithub.com/alexanderjaus/APEx.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05844v1&entry.124074799=Read"},
{"title": "PerlDiff: Controllable Street View Synthesis Using Perspective-Layout\n  Diffusion Models", "author": "Jinhua Zhang and Hualian Sheng and Sijia Cai and Bing Deng and Qiao Liang and Wen Li and Ying Fu and Jieping Ye and Shuhang Gu", "abstract": "  Controllable generation is considered a potentially vital approach to address\nthe challenge of annotating 3D data, and the precision of such controllable\ngeneration becomes particularly imperative in the context of data production\nfor autonomous driving. Existing methods focus on the integration of diverse\ngenerative information into controlling inputs, utilizing frameworks such as\nGLIGEN or ControlNet, to produce commendable outcomes in controllable\ngeneration. However, such approaches intrinsically restrict generation\nperformance to the learning capacities of predefined network architectures. In\nthis paper, we explore the integration of controlling information and introduce\nPerlDiff (Perspective-Layout Diffusion Models), a method for effective street\nview image generation that fully leverages perspective 3D geometric\ninformation. Our PerlDiff employs 3D geometric priors to guide the generation\nof street view images with precise object-level control within the network\nlearning process, resulting in a more robust and controllable output. Moreover,\nit demonstrates superior controllability compared to alternative layout control\nmethods. Empirical results justify that our PerlDiff markedly enhances the\nprecision of generation on the NuScenes and KITTI datasets. Our codes and\nmodels are publicly available at https://github.com/LabShuHangGU/PerlDiff.\n", "link": "http://arxiv.org/abs/2407.06109v1", "date": "2024-07-08", "relevancy": 2.4671, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6259}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6259}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerlDiff%3A%20Controllable%20Street%20View%20Synthesis%20Using%20Perspective-Layout%0A%20%20Diffusion%20Models&body=Title%3A%20PerlDiff%3A%20Controllable%20Street%20View%20Synthesis%20Using%20Perspective-Layout%0A%20%20Diffusion%20Models%0AAuthor%3A%20Jinhua%20Zhang%20and%20Hualian%20Sheng%20and%20Sijia%20Cai%20and%20Bing%20Deng%20and%20Qiao%20Liang%20and%20Wen%20Li%20and%20Ying%20Fu%20and%20Jieping%20Ye%20and%20Shuhang%20Gu%0AAbstract%3A%20%20%20Controllable%20generation%20is%20considered%20a%20potentially%20vital%20approach%20to%20address%0Athe%20challenge%20of%20annotating%203D%20data%2C%20and%20the%20precision%20of%20such%20controllable%0Ageneration%20becomes%20particularly%20imperative%20in%20the%20context%20of%20data%20production%0Afor%20autonomous%20driving.%20Existing%20methods%20focus%20on%20the%20integration%20of%20diverse%0Agenerative%20information%20into%20controlling%20inputs%2C%20utilizing%20frameworks%20such%20as%0AGLIGEN%20or%20ControlNet%2C%20to%20produce%20commendable%20outcomes%20in%20controllable%0Ageneration.%20However%2C%20such%20approaches%20intrinsically%20restrict%20generation%0Aperformance%20to%20the%20learning%20capacities%20of%20predefined%20network%20architectures.%20In%0Athis%20paper%2C%20we%20explore%20the%20integration%20of%20controlling%20information%20and%20introduce%0APerlDiff%20%28Perspective-Layout%20Diffusion%20Models%29%2C%20a%20method%20for%20effective%20street%0Aview%20image%20generation%20that%20fully%20leverages%20perspective%203D%20geometric%0Ainformation.%20Our%20PerlDiff%20employs%203D%20geometric%20priors%20to%20guide%20the%20generation%0Aof%20street%20view%20images%20with%20precise%20object-level%20control%20within%20the%20network%0Alearning%20process%2C%20resulting%20in%20a%20more%20robust%20and%20controllable%20output.%20Moreover%2C%0Ait%20demonstrates%20superior%20controllability%20compared%20to%20alternative%20layout%20control%0Amethods.%20Empirical%20results%20justify%20that%20our%20PerlDiff%20markedly%20enhances%20the%0Aprecision%20of%20generation%20on%20the%20NuScenes%20and%20KITTI%20datasets.%20Our%20codes%20and%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/LabShuHangGU/PerlDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerlDiff%253A%2520Controllable%2520Street%2520View%2520Synthesis%2520Using%2520Perspective-Layout%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DJinhua%2520Zhang%2520and%2520Hualian%2520Sheng%2520and%2520Sijia%2520Cai%2520and%2520Bing%2520Deng%2520and%2520Qiao%2520Liang%2520and%2520Wen%2520Li%2520and%2520Ying%2520Fu%2520and%2520Jieping%2520Ye%2520and%2520Shuhang%2520Gu%26entry.1292438233%3D%2520%2520Controllable%2520generation%2520is%2520considered%2520a%2520potentially%2520vital%2520approach%2520to%2520address%250Athe%2520challenge%2520of%2520annotating%25203D%2520data%252C%2520and%2520the%2520precision%2520of%2520such%2520controllable%250Ageneration%2520becomes%2520particularly%2520imperative%2520in%2520the%2520context%2520of%2520data%2520production%250Afor%2520autonomous%2520driving.%2520Existing%2520methods%2520focus%2520on%2520the%2520integration%2520of%2520diverse%250Agenerative%2520information%2520into%2520controlling%2520inputs%252C%2520utilizing%2520frameworks%2520such%2520as%250AGLIGEN%2520or%2520ControlNet%252C%2520to%2520produce%2520commendable%2520outcomes%2520in%2520controllable%250Ageneration.%2520However%252C%2520such%2520approaches%2520intrinsically%2520restrict%2520generation%250Aperformance%2520to%2520the%2520learning%2520capacities%2520of%2520predefined%2520network%2520architectures.%2520In%250Athis%2520paper%252C%2520we%2520explore%2520the%2520integration%2520of%2520controlling%2520information%2520and%2520introduce%250APerlDiff%2520%2528Perspective-Layout%2520Diffusion%2520Models%2529%252C%2520a%2520method%2520for%2520effective%2520street%250Aview%2520image%2520generation%2520that%2520fully%2520leverages%2520perspective%25203D%2520geometric%250Ainformation.%2520Our%2520PerlDiff%2520employs%25203D%2520geometric%2520priors%2520to%2520guide%2520the%2520generation%250Aof%2520street%2520view%2520images%2520with%2520precise%2520object-level%2520control%2520within%2520the%2520network%250Alearning%2520process%252C%2520resulting%2520in%2520a%2520more%2520robust%2520and%2520controllable%2520output.%2520Moreover%252C%250Ait%2520demonstrates%2520superior%2520controllability%2520compared%2520to%2520alternative%2520layout%2520control%250Amethods.%2520Empirical%2520results%2520justify%2520that%2520our%2520PerlDiff%2520markedly%2520enhances%2520the%250Aprecision%2520of%2520generation%2520on%2520the%2520NuScenes%2520and%2520KITTI%2520datasets.%2520Our%2520codes%2520and%250Amodels%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/LabShuHangGU/PerlDiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerlDiff%3A%20Controllable%20Street%20View%20Synthesis%20Using%20Perspective-Layout%0A%20%20Diffusion%20Models&entry.906535625=Jinhua%20Zhang%20and%20Hualian%20Sheng%20and%20Sijia%20Cai%20and%20Bing%20Deng%20and%20Qiao%20Liang%20and%20Wen%20Li%20and%20Ying%20Fu%20and%20Jieping%20Ye%20and%20Shuhang%20Gu&entry.1292438233=%20%20Controllable%20generation%20is%20considered%20a%20potentially%20vital%20approach%20to%20address%0Athe%20challenge%20of%20annotating%203D%20data%2C%20and%20the%20precision%20of%20such%20controllable%0Ageneration%20becomes%20particularly%20imperative%20in%20the%20context%20of%20data%20production%0Afor%20autonomous%20driving.%20Existing%20methods%20focus%20on%20the%20integration%20of%20diverse%0Agenerative%20information%20into%20controlling%20inputs%2C%20utilizing%20frameworks%20such%20as%0AGLIGEN%20or%20ControlNet%2C%20to%20produce%20commendable%20outcomes%20in%20controllable%0Ageneration.%20However%2C%20such%20approaches%20intrinsically%20restrict%20generation%0Aperformance%20to%20the%20learning%20capacities%20of%20predefined%20network%20architectures.%20In%0Athis%20paper%2C%20we%20explore%20the%20integration%20of%20controlling%20information%20and%20introduce%0APerlDiff%20%28Perspective-Layout%20Diffusion%20Models%29%2C%20a%20method%20for%20effective%20street%0Aview%20image%20generation%20that%20fully%20leverages%20perspective%203D%20geometric%0Ainformation.%20Our%20PerlDiff%20employs%203D%20geometric%20priors%20to%20guide%20the%20generation%0Aof%20street%20view%20images%20with%20precise%20object-level%20control%20within%20the%20network%0Alearning%20process%2C%20resulting%20in%20a%20more%20robust%20and%20controllable%20output.%20Moreover%2C%0Ait%20demonstrates%20superior%20controllability%20compared%20to%20alternative%20layout%20control%0Amethods.%20Empirical%20results%20justify%20that%20our%20PerlDiff%20markedly%20enhances%20the%0Aprecision%20of%20generation%20on%20the%20NuScenes%20and%20KITTI%20datasets.%20Our%20codes%20and%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/LabShuHangGU/PerlDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06109v1&entry.124074799=Read"},
{"title": "Spectral Graph Reasoning Network for Hyperspectral Image Classification", "author": "Huiling Wang", "abstract": "  Convolutional neural networks (CNNs) have achieved remarkable performance in\nhyperspectral image (HSI) classification over the last few years. Despite the\nprogress that has been made, rich and informative spectral information of HSI\nhas been largely underutilized by existing methods which employ convolutional\nkernels with limited size of receptive field in the spectral domain. To address\nthis issue, we propose a spectral graph reasoning network (SGR) learning\nframework comprising two crucial modules: 1) a spectral decoupling module which\nunpacks and casts multiple spectral embeddings into a unified graph whose node\ncorresponds to an individual spectral feature channel in the embedding space;\nthe graph performs interpretable reasoning to aggregate and align spectral\ninformation to guide learning spectral-specific graph embeddings at multiple\ncontextual levels 2) a spectral ensembling module explores the interactions and\ninterdependencies across graph embedding hierarchy via a novel recurrent graph\npropagation mechanism. Experiments on two HSI datasets demonstrate that the\nproposed architecture can significantly improve the classification accuracy\ncompared with the existing methods with a sizable margin.\n", "link": "http://arxiv.org/abs/2407.02647v2", "date": "2024-07-08", "relevancy": 2.4429, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5113}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4897}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Graph%20Reasoning%20Network%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20Spectral%20Graph%20Reasoning%20Network%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Huiling%20Wang%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20achieved%20remarkable%20performance%20in%0Ahyperspectral%20image%20%28HSI%29%20classification%20over%20the%20last%20few%20years.%20Despite%20the%0Aprogress%20that%20has%20been%20made%2C%20rich%20and%20informative%20spectral%20information%20of%20HSI%0Ahas%20been%20largely%20underutilized%20by%20existing%20methods%20which%20employ%20convolutional%0Akernels%20with%20limited%20size%20of%20receptive%20field%20in%20the%20spectral%20domain.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20spectral%20graph%20reasoning%20network%20%28SGR%29%20learning%0Aframework%20comprising%20two%20crucial%20modules%3A%201%29%20a%20spectral%20decoupling%20module%20which%0Aunpacks%20and%20casts%20multiple%20spectral%20embeddings%20into%20a%20unified%20graph%20whose%20node%0Acorresponds%20to%20an%20individual%20spectral%20feature%20channel%20in%20the%20embedding%20space%3B%0Athe%20graph%20performs%20interpretable%20reasoning%20to%20aggregate%20and%20align%20spectral%0Ainformation%20to%20guide%20learning%20spectral-specific%20graph%20embeddings%20at%20multiple%0Acontextual%20levels%202%29%20a%20spectral%20ensembling%20module%20explores%20the%20interactions%20and%0Ainterdependencies%20across%20graph%20embedding%20hierarchy%20via%20a%20novel%20recurrent%20graph%0Apropagation%20mechanism.%20Experiments%20on%20two%20HSI%20datasets%20demonstrate%20that%20the%0Aproposed%20architecture%20can%20significantly%20improve%20the%20classification%20accuracy%0Acompared%20with%20the%20existing%20methods%20with%20a%20sizable%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Graph%2520Reasoning%2520Network%2520for%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DHuiling%2520Wang%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520achieved%2520remarkable%2520performance%2520in%250Ahyperspectral%2520image%2520%2528HSI%2529%2520classification%2520over%2520the%2520last%2520few%2520years.%2520Despite%2520the%250Aprogress%2520that%2520has%2520been%2520made%252C%2520rich%2520and%2520informative%2520spectral%2520information%2520of%2520HSI%250Ahas%2520been%2520largely%2520underutilized%2520by%2520existing%2520methods%2520which%2520employ%2520convolutional%250Akernels%2520with%2520limited%2520size%2520of%2520receptive%2520field%2520in%2520the%2520spectral%2520domain.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520a%2520spectral%2520graph%2520reasoning%2520network%2520%2528SGR%2529%2520learning%250Aframework%2520comprising%2520two%2520crucial%2520modules%253A%25201%2529%2520a%2520spectral%2520decoupling%2520module%2520which%250Aunpacks%2520and%2520casts%2520multiple%2520spectral%2520embeddings%2520into%2520a%2520unified%2520graph%2520whose%2520node%250Acorresponds%2520to%2520an%2520individual%2520spectral%2520feature%2520channel%2520in%2520the%2520embedding%2520space%253B%250Athe%2520graph%2520performs%2520interpretable%2520reasoning%2520to%2520aggregate%2520and%2520align%2520spectral%250Ainformation%2520to%2520guide%2520learning%2520spectral-specific%2520graph%2520embeddings%2520at%2520multiple%250Acontextual%2520levels%25202%2529%2520a%2520spectral%2520ensembling%2520module%2520explores%2520the%2520interactions%2520and%250Ainterdependencies%2520across%2520graph%2520embedding%2520hierarchy%2520via%2520a%2520novel%2520recurrent%2520graph%250Apropagation%2520mechanism.%2520Experiments%2520on%2520two%2520HSI%2520datasets%2520demonstrate%2520that%2520the%250Aproposed%2520architecture%2520can%2520significantly%2520improve%2520the%2520classification%2520accuracy%250Acompared%2520with%2520the%2520existing%2520methods%2520with%2520a%2520sizable%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Graph%20Reasoning%20Network%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Huiling%20Wang&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20achieved%20remarkable%20performance%20in%0Ahyperspectral%20image%20%28HSI%29%20classification%20over%20the%20last%20few%20years.%20Despite%20the%0Aprogress%20that%20has%20been%20made%2C%20rich%20and%20informative%20spectral%20information%20of%20HSI%0Ahas%20been%20largely%20underutilized%20by%20existing%20methods%20which%20employ%20convolutional%0Akernels%20with%20limited%20size%20of%20receptive%20field%20in%20the%20spectral%20domain.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20spectral%20graph%20reasoning%20network%20%28SGR%29%20learning%0Aframework%20comprising%20two%20crucial%20modules%3A%201%29%20a%20spectral%20decoupling%20module%20which%0Aunpacks%20and%20casts%20multiple%20spectral%20embeddings%20into%20a%20unified%20graph%20whose%20node%0Acorresponds%20to%20an%20individual%20spectral%20feature%20channel%20in%20the%20embedding%20space%3B%0Athe%20graph%20performs%20interpretable%20reasoning%20to%20aggregate%20and%20align%20spectral%0Ainformation%20to%20guide%20learning%20spectral-specific%20graph%20embeddings%20at%20multiple%0Acontextual%20levels%202%29%20a%20spectral%20ensembling%20module%20explores%20the%20interactions%20and%0Ainterdependencies%20across%20graph%20embedding%20hierarchy%20via%20a%20novel%20recurrent%20graph%0Apropagation%20mechanism.%20Experiments%20on%20two%20HSI%20datasets%20demonstrate%20that%20the%0Aproposed%20architecture%20can%20significantly%20improve%20the%20classification%20accuracy%0Acompared%20with%20the%20existing%20methods%20with%20a%20sizable%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02647v2&entry.124074799=Read"},
{"title": "Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN\n  Pipeline applied on PSMA PET/CT Scans", "author": "Stefan P. Hein and Manuel Schultheiss and Andrei Gafita and Raphael Zaum and Farid Yagubbayli and Robert Tauber and Isabel Rauscher and Matthias Eiber and Franz Pfeiffer and Wolfgang A. Weber", "abstract": "  Assessing tumor response to systemic therapies is one of the main\napplications of PET/CT. Routinely, only a small subset of index lesions out of\nmultiple lesions is analyzed. However, this operator dependent selection may\nbias the results due to possible significant inter-metastatic heterogeneity of\nresponse to therapy. Automated, AI based approaches for lesion tracking hold\npromise in enabling the analysis of many more lesions and thus providing a\nbetter assessment of tumor response. This work introduces a Siamese CNN\napproach for lesion tracking between PET/CT scans. Our approach is applied on\nthe laborious task of tracking a high number of bone lesions in full-body\nbaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles\nof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer\npatients. Data preparation includes lesion segmentation and affine\nregistration. Our algorithm extracts suitable lesion patches and forwards them\ninto a Siamese CNN trained to classify the lesion patch pairs as corresponding\nor non-corresponding lesions. Experiments have been performed with different\ninput patch types and a Siamese network in 2D and 3D. The CNN model\nsuccessfully learned to classify lesion assignments, reaching a lesion tracking\naccuracy of 83 % in its best configuration with an AUC = 0.91. For remaining\nlesions the pipeline accomplished a re-identification rate of 89 %. We proved\nthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT\nscans. Future clinical studies are necessary if this improves the prediction of\nthe outcome of therapies.\n", "link": "http://arxiv.org/abs/2406.09327v3", "date": "2024-07-08", "relevancy": 2.4409, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4972}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4854}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20AI%20Lesion%20Tracking%20in%20PET/CT%20Imaging%3A%20A%20Siamese-based%20CNN%0A%20%20Pipeline%20applied%20on%20PSMA%20PET/CT%20Scans&body=Title%3A%20Towards%20AI%20Lesion%20Tracking%20in%20PET/CT%20Imaging%3A%20A%20Siamese-based%20CNN%0A%20%20Pipeline%20applied%20on%20PSMA%20PET/CT%20Scans%0AAuthor%3A%20Stefan%20P.%20Hein%20and%20Manuel%20Schultheiss%20and%20Andrei%20Gafita%20and%20Raphael%20Zaum%20and%20Farid%20Yagubbayli%20and%20Robert%20Tauber%20and%20Isabel%20Rauscher%20and%20Matthias%20Eiber%20and%20Franz%20Pfeiffer%20and%20Wolfgang%20A.%20Weber%0AAbstract%3A%20%20%20Assessing%20tumor%20response%20to%20systemic%20therapies%20is%20one%20of%20the%20main%0Aapplications%20of%20PET/CT.%20Routinely%2C%20only%20a%20small%20subset%20of%20index%20lesions%20out%20of%0Amultiple%20lesions%20is%20analyzed.%20However%2C%20this%20operator%20dependent%20selection%20may%0Abias%20the%20results%20due%20to%20possible%20significant%20inter-metastatic%20heterogeneity%20of%0Aresponse%20to%20therapy.%20Automated%2C%20AI%20based%20approaches%20for%20lesion%20tracking%20hold%0Apromise%20in%20enabling%20the%20analysis%20of%20many%20more%20lesions%20and%20thus%20providing%20a%0Abetter%20assessment%20of%20tumor%20response.%20This%20work%20introduces%20a%20Siamese%20CNN%0Aapproach%20for%20lesion%20tracking%20between%20PET/CT%20scans.%20Our%20approach%20is%20applied%20on%0Athe%20laborious%20task%20of%20tracking%20a%20high%20number%20of%20bone%20lesions%20in%20full-body%0Abaseline%20and%20follow-up%20%5B68Ga%5DGa-%20or%20%5B18F%5DF-PSMA%20PET/CT%20scans%20after%20two%20cycles%0Aof%20%5B177Lu%5DLu-PSMA%20therapy%20of%20metastatic%20castration%20resistant%20prostate%20cancer%0Apatients.%20Data%20preparation%20includes%20lesion%20segmentation%20and%20affine%0Aregistration.%20Our%20algorithm%20extracts%20suitable%20lesion%20patches%20and%20forwards%20them%0Ainto%20a%20Siamese%20CNN%20trained%20to%20classify%20the%20lesion%20patch%20pairs%20as%20corresponding%0Aor%20non-corresponding%20lesions.%20Experiments%20have%20been%20performed%20with%20different%0Ainput%20patch%20types%20and%20a%20Siamese%20network%20in%202D%20and%203D.%20The%20CNN%20model%0Asuccessfully%20learned%20to%20classify%20lesion%20assignments%2C%20reaching%20a%20lesion%20tracking%0Aaccuracy%20of%2083%20%25%20in%20its%20best%20configuration%20with%20an%20AUC%20%3D%200.91.%20For%20remaining%0Alesions%20the%20pipeline%20accomplished%20a%20re-identification%20rate%20of%2089%20%25.%20We%20proved%0Athat%20a%20CNN%20may%20facilitate%20the%20tracking%20of%20multiple%20lesions%20in%20PSMA%20PET/CT%0Ascans.%20Future%20clinical%20studies%20are%20necessary%20if%20this%20improves%20the%20prediction%20of%0Athe%20outcome%20of%20therapies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09327v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520AI%2520Lesion%2520Tracking%2520in%2520PET/CT%2520Imaging%253A%2520A%2520Siamese-based%2520CNN%250A%2520%2520Pipeline%2520applied%2520on%2520PSMA%2520PET/CT%2520Scans%26entry.906535625%3DStefan%2520P.%2520Hein%2520and%2520Manuel%2520Schultheiss%2520and%2520Andrei%2520Gafita%2520and%2520Raphael%2520Zaum%2520and%2520Farid%2520Yagubbayli%2520and%2520Robert%2520Tauber%2520and%2520Isabel%2520Rauscher%2520and%2520Matthias%2520Eiber%2520and%2520Franz%2520Pfeiffer%2520and%2520Wolfgang%2520A.%2520Weber%26entry.1292438233%3D%2520%2520Assessing%2520tumor%2520response%2520to%2520systemic%2520therapies%2520is%2520one%2520of%2520the%2520main%250Aapplications%2520of%2520PET/CT.%2520Routinely%252C%2520only%2520a%2520small%2520subset%2520of%2520index%2520lesions%2520out%2520of%250Amultiple%2520lesions%2520is%2520analyzed.%2520However%252C%2520this%2520operator%2520dependent%2520selection%2520may%250Abias%2520the%2520results%2520due%2520to%2520possible%2520significant%2520inter-metastatic%2520heterogeneity%2520of%250Aresponse%2520to%2520therapy.%2520Automated%252C%2520AI%2520based%2520approaches%2520for%2520lesion%2520tracking%2520hold%250Apromise%2520in%2520enabling%2520the%2520analysis%2520of%2520many%2520more%2520lesions%2520and%2520thus%2520providing%2520a%250Abetter%2520assessment%2520of%2520tumor%2520response.%2520This%2520work%2520introduces%2520a%2520Siamese%2520CNN%250Aapproach%2520for%2520lesion%2520tracking%2520between%2520PET/CT%2520scans.%2520Our%2520approach%2520is%2520applied%2520on%250Athe%2520laborious%2520task%2520of%2520tracking%2520a%2520high%2520number%2520of%2520bone%2520lesions%2520in%2520full-body%250Abaseline%2520and%2520follow-up%2520%255B68Ga%255DGa-%2520or%2520%255B18F%255DF-PSMA%2520PET/CT%2520scans%2520after%2520two%2520cycles%250Aof%2520%255B177Lu%255DLu-PSMA%2520therapy%2520of%2520metastatic%2520castration%2520resistant%2520prostate%2520cancer%250Apatients.%2520Data%2520preparation%2520includes%2520lesion%2520segmentation%2520and%2520affine%250Aregistration.%2520Our%2520algorithm%2520extracts%2520suitable%2520lesion%2520patches%2520and%2520forwards%2520them%250Ainto%2520a%2520Siamese%2520CNN%2520trained%2520to%2520classify%2520the%2520lesion%2520patch%2520pairs%2520as%2520corresponding%250Aor%2520non-corresponding%2520lesions.%2520Experiments%2520have%2520been%2520performed%2520with%2520different%250Ainput%2520patch%2520types%2520and%2520a%2520Siamese%2520network%2520in%25202D%2520and%25203D.%2520The%2520CNN%2520model%250Asuccessfully%2520learned%2520to%2520classify%2520lesion%2520assignments%252C%2520reaching%2520a%2520lesion%2520tracking%250Aaccuracy%2520of%252083%2520%2525%2520in%2520its%2520best%2520configuration%2520with%2520an%2520AUC%2520%253D%25200.91.%2520For%2520remaining%250Alesions%2520the%2520pipeline%2520accomplished%2520a%2520re-identification%2520rate%2520of%252089%2520%2525.%2520We%2520proved%250Athat%2520a%2520CNN%2520may%2520facilitate%2520the%2520tracking%2520of%2520multiple%2520lesions%2520in%2520PSMA%2520PET/CT%250Ascans.%2520Future%2520clinical%2520studies%2520are%2520necessary%2520if%2520this%2520improves%2520the%2520prediction%2520of%250Athe%2520outcome%2520of%2520therapies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09327v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20AI%20Lesion%20Tracking%20in%20PET/CT%20Imaging%3A%20A%20Siamese-based%20CNN%0A%20%20Pipeline%20applied%20on%20PSMA%20PET/CT%20Scans&entry.906535625=Stefan%20P.%20Hein%20and%20Manuel%20Schultheiss%20and%20Andrei%20Gafita%20and%20Raphael%20Zaum%20and%20Farid%20Yagubbayli%20and%20Robert%20Tauber%20and%20Isabel%20Rauscher%20and%20Matthias%20Eiber%20and%20Franz%20Pfeiffer%20and%20Wolfgang%20A.%20Weber&entry.1292438233=%20%20Assessing%20tumor%20response%20to%20systemic%20therapies%20is%20one%20of%20the%20main%0Aapplications%20of%20PET/CT.%20Routinely%2C%20only%20a%20small%20subset%20of%20index%20lesions%20out%20of%0Amultiple%20lesions%20is%20analyzed.%20However%2C%20this%20operator%20dependent%20selection%20may%0Abias%20the%20results%20due%20to%20possible%20significant%20inter-metastatic%20heterogeneity%20of%0Aresponse%20to%20therapy.%20Automated%2C%20AI%20based%20approaches%20for%20lesion%20tracking%20hold%0Apromise%20in%20enabling%20the%20analysis%20of%20many%20more%20lesions%20and%20thus%20providing%20a%0Abetter%20assessment%20of%20tumor%20response.%20This%20work%20introduces%20a%20Siamese%20CNN%0Aapproach%20for%20lesion%20tracking%20between%20PET/CT%20scans.%20Our%20approach%20is%20applied%20on%0Athe%20laborious%20task%20of%20tracking%20a%20high%20number%20of%20bone%20lesions%20in%20full-body%0Abaseline%20and%20follow-up%20%5B68Ga%5DGa-%20or%20%5B18F%5DF-PSMA%20PET/CT%20scans%20after%20two%20cycles%0Aof%20%5B177Lu%5DLu-PSMA%20therapy%20of%20metastatic%20castration%20resistant%20prostate%20cancer%0Apatients.%20Data%20preparation%20includes%20lesion%20segmentation%20and%20affine%0Aregistration.%20Our%20algorithm%20extracts%20suitable%20lesion%20patches%20and%20forwards%20them%0Ainto%20a%20Siamese%20CNN%20trained%20to%20classify%20the%20lesion%20patch%20pairs%20as%20corresponding%0Aor%20non-corresponding%20lesions.%20Experiments%20have%20been%20performed%20with%20different%0Ainput%20patch%20types%20and%20a%20Siamese%20network%20in%202D%20and%203D.%20The%20CNN%20model%0Asuccessfully%20learned%20to%20classify%20lesion%20assignments%2C%20reaching%20a%20lesion%20tracking%0Aaccuracy%20of%2083%20%25%20in%20its%20best%20configuration%20with%20an%20AUC%20%3D%200.91.%20For%20remaining%0Alesions%20the%20pipeline%20accomplished%20a%20re-identification%20rate%20of%2089%20%25.%20We%20proved%0Athat%20a%20CNN%20may%20facilitate%20the%20tracking%20of%20multiple%20lesions%20in%20PSMA%20PET/CT%0Ascans.%20Future%20clinical%20studies%20are%20necessary%20if%20this%20improves%20the%20prediction%20of%0Athe%20outcome%20of%20therapies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09327v3&entry.124074799=Read"},
{"title": "Sketchy Moment Matching: Toward Fast and Provable Data Selection for\n  Finetuning", "author": "Yijun Dong and Hoang Phan and Xiang Pan and Qi Lei", "abstract": "  We revisit data selection in a modern context of finetuning from a\nfundamental perspective. Extending the classical wisdom of variance\nminimization in low dimensions to high-dimensional finetuning, our\ngeneralization analysis unveils the importance of additionally reducing bias\ninduced by low-rank approximation. Inspired by the variance-bias tradeoff in\nhigh dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a\nscalable data selection scheme with two stages. (i) First, the bias is\ncontrolled using gradient sketching that explores the finetuning parameter\nspace for an informative low-dimensional subspace $\\mathcal{S}$; (ii) then the\nvariance is reduced over $\\mathcal{S}$ via moment matching between the original\nand selected datasets. Theoretically, we show that gradient sketching is fast\nand provably accurate: selecting $n$ samples by reducing variance over\n$\\mathcal{S}$ preserves the fast-rate generalization $O(\\dim(\\mathcal{S})/n)$,\nindependent of the parameter dimension. Empirically, we concretize the\nvariance-bias balance via synthetic experiments and demonstrate the\neffectiveness of SkMM for finetuning in real vision tasks.\n", "link": "http://arxiv.org/abs/2407.06120v1", "date": "2024-07-08", "relevancy": 2.4276, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4898}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4866}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketchy%20Moment%20Matching%3A%20Toward%20Fast%20and%20Provable%20Data%20Selection%20for%0A%20%20Finetuning&body=Title%3A%20Sketchy%20Moment%20Matching%3A%20Toward%20Fast%20and%20Provable%20Data%20Selection%20for%0A%20%20Finetuning%0AAuthor%3A%20Yijun%20Dong%20and%20Hoang%20Phan%20and%20Xiang%20Pan%20and%20Qi%20Lei%0AAbstract%3A%20%20%20We%20revisit%20data%20selection%20in%20a%20modern%20context%20of%20finetuning%20from%20a%0Afundamental%20perspective.%20Extending%20the%20classical%20wisdom%20of%20variance%0Aminimization%20in%20low%20dimensions%20to%20high-dimensional%20finetuning%2C%20our%0Ageneralization%20analysis%20unveils%20the%20importance%20of%20additionally%20reducing%20bias%0Ainduced%20by%20low-rank%20approximation.%20Inspired%20by%20the%20variance-bias%20tradeoff%20in%0Ahigh%20dimensions%20from%20the%20theory%2C%20we%20introduce%20Sketchy%20Moment%20Matching%20%28SkMM%29%2C%20a%0Ascalable%20data%20selection%20scheme%20with%20two%20stages.%20%28i%29%20First%2C%20the%20bias%20is%0Acontrolled%20using%20gradient%20sketching%20that%20explores%20the%20finetuning%20parameter%0Aspace%20for%20an%20informative%20low-dimensional%20subspace%20%24%5Cmathcal%7BS%7D%24%3B%20%28ii%29%20then%20the%0Avariance%20is%20reduced%20over%20%24%5Cmathcal%7BS%7D%24%20via%20moment%20matching%20between%20the%20original%0Aand%20selected%20datasets.%20Theoretically%2C%20we%20show%20that%20gradient%20sketching%20is%20fast%0Aand%20provably%20accurate%3A%20selecting%20%24n%24%20samples%20by%20reducing%20variance%20over%0A%24%5Cmathcal%7BS%7D%24%20preserves%20the%20fast-rate%20generalization%20%24O%28%5Cdim%28%5Cmathcal%7BS%7D%29/n%29%24%2C%0Aindependent%20of%20the%20parameter%20dimension.%20Empirically%2C%20we%20concretize%20the%0Avariance-bias%20balance%20via%20synthetic%20experiments%20and%20demonstrate%20the%0Aeffectiveness%20of%20SkMM%20for%20finetuning%20in%20real%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketchy%2520Moment%2520Matching%253A%2520Toward%2520Fast%2520and%2520Provable%2520Data%2520Selection%2520for%250A%2520%2520Finetuning%26entry.906535625%3DYijun%2520Dong%2520and%2520Hoang%2520Phan%2520and%2520Xiang%2520Pan%2520and%2520Qi%2520Lei%26entry.1292438233%3D%2520%2520We%2520revisit%2520data%2520selection%2520in%2520a%2520modern%2520context%2520of%2520finetuning%2520from%2520a%250Afundamental%2520perspective.%2520Extending%2520the%2520classical%2520wisdom%2520of%2520variance%250Aminimization%2520in%2520low%2520dimensions%2520to%2520high-dimensional%2520finetuning%252C%2520our%250Ageneralization%2520analysis%2520unveils%2520the%2520importance%2520of%2520additionally%2520reducing%2520bias%250Ainduced%2520by%2520low-rank%2520approximation.%2520Inspired%2520by%2520the%2520variance-bias%2520tradeoff%2520in%250Ahigh%2520dimensions%2520from%2520the%2520theory%252C%2520we%2520introduce%2520Sketchy%2520Moment%2520Matching%2520%2528SkMM%2529%252C%2520a%250Ascalable%2520data%2520selection%2520scheme%2520with%2520two%2520stages.%2520%2528i%2529%2520First%252C%2520the%2520bias%2520is%250Acontrolled%2520using%2520gradient%2520sketching%2520that%2520explores%2520the%2520finetuning%2520parameter%250Aspace%2520for%2520an%2520informative%2520low-dimensional%2520subspace%2520%2524%255Cmathcal%257BS%257D%2524%253B%2520%2528ii%2529%2520then%2520the%250Avariance%2520is%2520reduced%2520over%2520%2524%255Cmathcal%257BS%257D%2524%2520via%2520moment%2520matching%2520between%2520the%2520original%250Aand%2520selected%2520datasets.%2520Theoretically%252C%2520we%2520show%2520that%2520gradient%2520sketching%2520is%2520fast%250Aand%2520provably%2520accurate%253A%2520selecting%2520%2524n%2524%2520samples%2520by%2520reducing%2520variance%2520over%250A%2524%255Cmathcal%257BS%257D%2524%2520preserves%2520the%2520fast-rate%2520generalization%2520%2524O%2528%255Cdim%2528%255Cmathcal%257BS%257D%2529/n%2529%2524%252C%250Aindependent%2520of%2520the%2520parameter%2520dimension.%2520Empirically%252C%2520we%2520concretize%2520the%250Avariance-bias%2520balance%2520via%2520synthetic%2520experiments%2520and%2520demonstrate%2520the%250Aeffectiveness%2520of%2520SkMM%2520for%2520finetuning%2520in%2520real%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketchy%20Moment%20Matching%3A%20Toward%20Fast%20and%20Provable%20Data%20Selection%20for%0A%20%20Finetuning&entry.906535625=Yijun%20Dong%20and%20Hoang%20Phan%20and%20Xiang%20Pan%20and%20Qi%20Lei&entry.1292438233=%20%20We%20revisit%20data%20selection%20in%20a%20modern%20context%20of%20finetuning%20from%20a%0Afundamental%20perspective.%20Extending%20the%20classical%20wisdom%20of%20variance%0Aminimization%20in%20low%20dimensions%20to%20high-dimensional%20finetuning%2C%20our%0Ageneralization%20analysis%20unveils%20the%20importance%20of%20additionally%20reducing%20bias%0Ainduced%20by%20low-rank%20approximation.%20Inspired%20by%20the%20variance-bias%20tradeoff%20in%0Ahigh%20dimensions%20from%20the%20theory%2C%20we%20introduce%20Sketchy%20Moment%20Matching%20%28SkMM%29%2C%20a%0Ascalable%20data%20selection%20scheme%20with%20two%20stages.%20%28i%29%20First%2C%20the%20bias%20is%0Acontrolled%20using%20gradient%20sketching%20that%20explores%20the%20finetuning%20parameter%0Aspace%20for%20an%20informative%20low-dimensional%20subspace%20%24%5Cmathcal%7BS%7D%24%3B%20%28ii%29%20then%20the%0Avariance%20is%20reduced%20over%20%24%5Cmathcal%7BS%7D%24%20via%20moment%20matching%20between%20the%20original%0Aand%20selected%20datasets.%20Theoretically%2C%20we%20show%20that%20gradient%20sketching%20is%20fast%0Aand%20provably%20accurate%3A%20selecting%20%24n%24%20samples%20by%20reducing%20variance%20over%0A%24%5Cmathcal%7BS%7D%24%20preserves%20the%20fast-rate%20generalization%20%24O%28%5Cdim%28%5Cmathcal%7BS%7D%29/n%29%24%2C%0Aindependent%20of%20the%20parameter%20dimension.%20Empirically%2C%20we%20concretize%20the%0Avariance-bias%20balance%20via%20synthetic%20experiments%20and%20demonstrate%20the%0Aeffectiveness%20of%20SkMM%20for%20finetuning%20in%20real%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06120v1&entry.124074799=Read"},
{"title": "IPNET:Influential Prototypical Networks for Few Shot Learning", "author": "Ranjana Roy Chowdhury and Deepti R. Bathula", "abstract": "  Prototypical network (PN) is a simple yet effective few shot learning\nstrategy. It is a metric-based meta-learning technique where classification is\nperformed by computing Euclidean distances to prototypical representations of\neach class. Conventional PN attributes equal importance to all samples and\ngenerates prototypes by simply averaging the support sample embeddings\nbelonging to each class. In this work, we propose a novel version of PN that\nattributes weights to support samples corresponding to their influence on the\nsupport sample distribution. Influence weights of samples are calculated based\non maximum mean discrepancy (MMD) between the mean embeddings of sample\ndistributions including and excluding the sample. Further, the influence factor\nof a sample is measured using MMD based on the shift in the distribution in the\nabsence of that sample.\n", "link": "http://arxiv.org/abs/2208.09345v2", "date": "2024-07-08", "relevancy": 2.4229, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4904}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.485}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPNET%3AInfluential%20Prototypical%20Networks%20for%20Few%20Shot%20Learning&body=Title%3A%20IPNET%3AInfluential%20Prototypical%20Networks%20for%20Few%20Shot%20Learning%0AAuthor%3A%20Ranjana%20Roy%20Chowdhury%20and%20Deepti%20R.%20Bathula%0AAbstract%3A%20%20%20Prototypical%20network%20%28PN%29%20is%20a%20simple%20yet%20effective%20few%20shot%20learning%0Astrategy.%20It%20is%20a%20metric-based%20meta-learning%20technique%20where%20classification%20is%0Aperformed%20by%20computing%20Euclidean%20distances%20to%20prototypical%20representations%20of%0Aeach%20class.%20Conventional%20PN%20attributes%20equal%20importance%20to%20all%20samples%20and%0Agenerates%20prototypes%20by%20simply%20averaging%20the%20support%20sample%20embeddings%0Abelonging%20to%20each%20class.%20In%20this%20work%2C%20we%20propose%20a%20novel%20version%20of%20PN%20that%0Aattributes%20weights%20to%20support%20samples%20corresponding%20to%20their%20influence%20on%20the%0Asupport%20sample%20distribution.%20Influence%20weights%20of%20samples%20are%20calculated%20based%0Aon%20maximum%20mean%20discrepancy%20%28MMD%29%20between%20the%20mean%20embeddings%20of%20sample%0Adistributions%20including%20and%20excluding%20the%20sample.%20Further%2C%20the%20influence%20factor%0Aof%20a%20sample%20is%20measured%20using%20MMD%20based%20on%20the%20shift%20in%20the%20distribution%20in%20the%0Aabsence%20of%20that%20sample.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.09345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPNET%253AInfluential%2520Prototypical%2520Networks%2520for%2520Few%2520Shot%2520Learning%26entry.906535625%3DRanjana%2520Roy%2520Chowdhury%2520and%2520Deepti%2520R.%2520Bathula%26entry.1292438233%3D%2520%2520Prototypical%2520network%2520%2528PN%2529%2520is%2520a%2520simple%2520yet%2520effective%2520few%2520shot%2520learning%250Astrategy.%2520It%2520is%2520a%2520metric-based%2520meta-learning%2520technique%2520where%2520classification%2520is%250Aperformed%2520by%2520computing%2520Euclidean%2520distances%2520to%2520prototypical%2520representations%2520of%250Aeach%2520class.%2520Conventional%2520PN%2520attributes%2520equal%2520importance%2520to%2520all%2520samples%2520and%250Agenerates%2520prototypes%2520by%2520simply%2520averaging%2520the%2520support%2520sample%2520embeddings%250Abelonging%2520to%2520each%2520class.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520version%2520of%2520PN%2520that%250Aattributes%2520weights%2520to%2520support%2520samples%2520corresponding%2520to%2520their%2520influence%2520on%2520the%250Asupport%2520sample%2520distribution.%2520Influence%2520weights%2520of%2520samples%2520are%2520calculated%2520based%250Aon%2520maximum%2520mean%2520discrepancy%2520%2528MMD%2529%2520between%2520the%2520mean%2520embeddings%2520of%2520sample%250Adistributions%2520including%2520and%2520excluding%2520the%2520sample.%2520Further%252C%2520the%2520influence%2520factor%250Aof%2520a%2520sample%2520is%2520measured%2520using%2520MMD%2520based%2520on%2520the%2520shift%2520in%2520the%2520distribution%2520in%2520the%250Aabsence%2520of%2520that%2520sample.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.09345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPNET%3AInfluential%20Prototypical%20Networks%20for%20Few%20Shot%20Learning&entry.906535625=Ranjana%20Roy%20Chowdhury%20and%20Deepti%20R.%20Bathula&entry.1292438233=%20%20Prototypical%20network%20%28PN%29%20is%20a%20simple%20yet%20effective%20few%20shot%20learning%0Astrategy.%20It%20is%20a%20metric-based%20meta-learning%20technique%20where%20classification%20is%0Aperformed%20by%20computing%20Euclidean%20distances%20to%20prototypical%20representations%20of%0Aeach%20class.%20Conventional%20PN%20attributes%20equal%20importance%20to%20all%20samples%20and%0Agenerates%20prototypes%20by%20simply%20averaging%20the%20support%20sample%20embeddings%0Abelonging%20to%20each%20class.%20In%20this%20work%2C%20we%20propose%20a%20novel%20version%20of%20PN%20that%0Aattributes%20weights%20to%20support%20samples%20corresponding%20to%20their%20influence%20on%20the%0Asupport%20sample%20distribution.%20Influence%20weights%20of%20samples%20are%20calculated%20based%0Aon%20maximum%20mean%20discrepancy%20%28MMD%29%20between%20the%20mean%20embeddings%20of%20sample%0Adistributions%20including%20and%20excluding%20the%20sample.%20Further%2C%20the%20influence%20factor%0Aof%20a%20sample%20is%20measured%20using%20MMD%20based%20on%20the%20shift%20in%20the%20distribution%20in%20the%0Aabsence%20of%20that%20sample.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.09345v2&entry.124074799=Read"},
{"title": "FGA: Fourier-Guided Attention Network for Crowd Count Estimation", "author": "Yashwardhan Chaudhuri and Ankit Kumar and Arun Balaji Buduru and Adel Alshamrani", "abstract": "  Crowd counting is gaining societal relevance, particularly in domains of\nUrban Planning, Crowd Management, and Public Safety. This paper introduces\nFourier-guided attention (FGA), a novel attention mechanism for crowd count\nestimation designed to address the inefficient full-scale global pattern\ncapture in existing works on convolution-based attention networks. FGA\nefficiently captures multi-scale information, including full-scale global\npatterns, by utilizing Fast-Fourier Transformations (FFT) along with spatial\nattention for global features and convolutions with channel-wise attention for\nsemi-global and local features. The architecture of FGA involves a dual-path\napproach: (1) a path for processing full-scale global features through FFT,\nallowing for efficient extraction of information in the frequency domain, and\n(2) a path for processing remaining feature maps for semi-global and local\nfeatures using traditional convolutions and channel-wise attention. This\ndual-path architecture enables FGA to seamlessly integrate frequency and\nspatial information, enhancing its ability to capture diverse crowd patterns.\nWe apply FGA in the last layers of two popular crowd-counting works, CSRNet and\nCANNet, to evaluate the module's performance on benchmark datasets such as\nShanghaiTech-A, ShanghaiTech-B, UCF-CC-50, and JHU++ crowd. The experiments\ndemonstrate a notable improvement across all datasets based on\nMean-Squared-Error (MSE) and Mean-Absolute-Error (MAE) metrics, showing\ncomparable performance to recent state-of-the-art methods. Additionally, we\nillustrate the interpretability using qualitative analysis, leveraging Grad-CAM\nheatmaps, to show the effectiveness of FGA in capturing crowd patterns.\n", "link": "http://arxiv.org/abs/2407.06110v1", "date": "2024-07-08", "relevancy": 2.4217, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4938}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4911}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FGA%3A%20Fourier-Guided%20Attention%20Network%20for%20Crowd%20Count%20Estimation&body=Title%3A%20FGA%3A%20Fourier-Guided%20Attention%20Network%20for%20Crowd%20Count%20Estimation%0AAuthor%3A%20Yashwardhan%20Chaudhuri%20and%20Ankit%20Kumar%20and%20Arun%20Balaji%20Buduru%20and%20Adel%20Alshamrani%0AAbstract%3A%20%20%20Crowd%20counting%20is%20gaining%20societal%20relevance%2C%20particularly%20in%20domains%20of%0AUrban%20Planning%2C%20Crowd%20Management%2C%20and%20Public%20Safety.%20This%20paper%20introduces%0AFourier-guided%20attention%20%28FGA%29%2C%20a%20novel%20attention%20mechanism%20for%20crowd%20count%0Aestimation%20designed%20to%20address%20the%20inefficient%20full-scale%20global%20pattern%0Acapture%20in%20existing%20works%20on%20convolution-based%20attention%20networks.%20FGA%0Aefficiently%20captures%20multi-scale%20information%2C%20including%20full-scale%20global%0Apatterns%2C%20by%20utilizing%20Fast-Fourier%20Transformations%20%28FFT%29%20along%20with%20spatial%0Aattention%20for%20global%20features%20and%20convolutions%20with%20channel-wise%20attention%20for%0Asemi-global%20and%20local%20features.%20The%20architecture%20of%20FGA%20involves%20a%20dual-path%0Aapproach%3A%20%281%29%20a%20path%20for%20processing%20full-scale%20global%20features%20through%20FFT%2C%0Aallowing%20for%20efficient%20extraction%20of%20information%20in%20the%20frequency%20domain%2C%20and%0A%282%29%20a%20path%20for%20processing%20remaining%20feature%20maps%20for%20semi-global%20and%20local%0Afeatures%20using%20traditional%20convolutions%20and%20channel-wise%20attention.%20This%0Adual-path%20architecture%20enables%20FGA%20to%20seamlessly%20integrate%20frequency%20and%0Aspatial%20information%2C%20enhancing%20its%20ability%20to%20capture%20diverse%20crowd%20patterns.%0AWe%20apply%20FGA%20in%20the%20last%20layers%20of%20two%20popular%20crowd-counting%20works%2C%20CSRNet%20and%0ACANNet%2C%20to%20evaluate%20the%20module%27s%20performance%20on%20benchmark%20datasets%20such%20as%0AShanghaiTech-A%2C%20ShanghaiTech-B%2C%20UCF-CC-50%2C%20and%20JHU%2B%2B%20crowd.%20The%20experiments%0Ademonstrate%20a%20notable%20improvement%20across%20all%20datasets%20based%20on%0AMean-Squared-Error%20%28MSE%29%20and%20Mean-Absolute-Error%20%28MAE%29%20metrics%2C%20showing%0Acomparable%20performance%20to%20recent%20state-of-the-art%20methods.%20Additionally%2C%20we%0Aillustrate%20the%20interpretability%20using%20qualitative%20analysis%2C%20leveraging%20Grad-CAM%0Aheatmaps%2C%20to%20show%20the%20effectiveness%20of%20FGA%20in%20capturing%20crowd%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFGA%253A%2520Fourier-Guided%2520Attention%2520Network%2520for%2520Crowd%2520Count%2520Estimation%26entry.906535625%3DYashwardhan%2520Chaudhuri%2520and%2520Ankit%2520Kumar%2520and%2520Arun%2520Balaji%2520Buduru%2520and%2520Adel%2520Alshamrani%26entry.1292438233%3D%2520%2520Crowd%2520counting%2520is%2520gaining%2520societal%2520relevance%252C%2520particularly%2520in%2520domains%2520of%250AUrban%2520Planning%252C%2520Crowd%2520Management%252C%2520and%2520Public%2520Safety.%2520This%2520paper%2520introduces%250AFourier-guided%2520attention%2520%2528FGA%2529%252C%2520a%2520novel%2520attention%2520mechanism%2520for%2520crowd%2520count%250Aestimation%2520designed%2520to%2520address%2520the%2520inefficient%2520full-scale%2520global%2520pattern%250Acapture%2520in%2520existing%2520works%2520on%2520convolution-based%2520attention%2520networks.%2520FGA%250Aefficiently%2520captures%2520multi-scale%2520information%252C%2520including%2520full-scale%2520global%250Apatterns%252C%2520by%2520utilizing%2520Fast-Fourier%2520Transformations%2520%2528FFT%2529%2520along%2520with%2520spatial%250Aattention%2520for%2520global%2520features%2520and%2520convolutions%2520with%2520channel-wise%2520attention%2520for%250Asemi-global%2520and%2520local%2520features.%2520The%2520architecture%2520of%2520FGA%2520involves%2520a%2520dual-path%250Aapproach%253A%2520%25281%2529%2520a%2520path%2520for%2520processing%2520full-scale%2520global%2520features%2520through%2520FFT%252C%250Aallowing%2520for%2520efficient%2520extraction%2520of%2520information%2520in%2520the%2520frequency%2520domain%252C%2520and%250A%25282%2529%2520a%2520path%2520for%2520processing%2520remaining%2520feature%2520maps%2520for%2520semi-global%2520and%2520local%250Afeatures%2520using%2520traditional%2520convolutions%2520and%2520channel-wise%2520attention.%2520This%250Adual-path%2520architecture%2520enables%2520FGA%2520to%2520seamlessly%2520integrate%2520frequency%2520and%250Aspatial%2520information%252C%2520enhancing%2520its%2520ability%2520to%2520capture%2520diverse%2520crowd%2520patterns.%250AWe%2520apply%2520FGA%2520in%2520the%2520last%2520layers%2520of%2520two%2520popular%2520crowd-counting%2520works%252C%2520CSRNet%2520and%250ACANNet%252C%2520to%2520evaluate%2520the%2520module%2527s%2520performance%2520on%2520benchmark%2520datasets%2520such%2520as%250AShanghaiTech-A%252C%2520ShanghaiTech-B%252C%2520UCF-CC-50%252C%2520and%2520JHU%252B%252B%2520crowd.%2520The%2520experiments%250Ademonstrate%2520a%2520notable%2520improvement%2520across%2520all%2520datasets%2520based%2520on%250AMean-Squared-Error%2520%2528MSE%2529%2520and%2520Mean-Absolute-Error%2520%2528MAE%2529%2520metrics%252C%2520showing%250Acomparable%2520performance%2520to%2520recent%2520state-of-the-art%2520methods.%2520Additionally%252C%2520we%250Aillustrate%2520the%2520interpretability%2520using%2520qualitative%2520analysis%252C%2520leveraging%2520Grad-CAM%250Aheatmaps%252C%2520to%2520show%2520the%2520effectiveness%2520of%2520FGA%2520in%2520capturing%2520crowd%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FGA%3A%20Fourier-Guided%20Attention%20Network%20for%20Crowd%20Count%20Estimation&entry.906535625=Yashwardhan%20Chaudhuri%20and%20Ankit%20Kumar%20and%20Arun%20Balaji%20Buduru%20and%20Adel%20Alshamrani&entry.1292438233=%20%20Crowd%20counting%20is%20gaining%20societal%20relevance%2C%20particularly%20in%20domains%20of%0AUrban%20Planning%2C%20Crowd%20Management%2C%20and%20Public%20Safety.%20This%20paper%20introduces%0AFourier-guided%20attention%20%28FGA%29%2C%20a%20novel%20attention%20mechanism%20for%20crowd%20count%0Aestimation%20designed%20to%20address%20the%20inefficient%20full-scale%20global%20pattern%0Acapture%20in%20existing%20works%20on%20convolution-based%20attention%20networks.%20FGA%0Aefficiently%20captures%20multi-scale%20information%2C%20including%20full-scale%20global%0Apatterns%2C%20by%20utilizing%20Fast-Fourier%20Transformations%20%28FFT%29%20along%20with%20spatial%0Aattention%20for%20global%20features%20and%20convolutions%20with%20channel-wise%20attention%20for%0Asemi-global%20and%20local%20features.%20The%20architecture%20of%20FGA%20involves%20a%20dual-path%0Aapproach%3A%20%281%29%20a%20path%20for%20processing%20full-scale%20global%20features%20through%20FFT%2C%0Aallowing%20for%20efficient%20extraction%20of%20information%20in%20the%20frequency%20domain%2C%20and%0A%282%29%20a%20path%20for%20processing%20remaining%20feature%20maps%20for%20semi-global%20and%20local%0Afeatures%20using%20traditional%20convolutions%20and%20channel-wise%20attention.%20This%0Adual-path%20architecture%20enables%20FGA%20to%20seamlessly%20integrate%20frequency%20and%0Aspatial%20information%2C%20enhancing%20its%20ability%20to%20capture%20diverse%20crowd%20patterns.%0AWe%20apply%20FGA%20in%20the%20last%20layers%20of%20two%20popular%20crowd-counting%20works%2C%20CSRNet%20and%0ACANNet%2C%20to%20evaluate%20the%20module%27s%20performance%20on%20benchmark%20datasets%20such%20as%0AShanghaiTech-A%2C%20ShanghaiTech-B%2C%20UCF-CC-50%2C%20and%20JHU%2B%2B%20crowd.%20The%20experiments%0Ademonstrate%20a%20notable%20improvement%20across%20all%20datasets%20based%20on%0AMean-Squared-Error%20%28MSE%29%20and%20Mean-Absolute-Error%20%28MAE%29%20metrics%2C%20showing%0Acomparable%20performance%20to%20recent%20state-of-the-art%20methods.%20Additionally%2C%20we%0Aillustrate%20the%20interpretability%20using%20qualitative%20analysis%2C%20leveraging%20Grad-CAM%0Aheatmaps%2C%20to%20show%20the%20effectiveness%20of%20FGA%20in%20capturing%20crowd%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06110v1&entry.124074799=Read"},
{"title": "Transfer Learning with Self-Supervised Vision Transformers for Snake\n  Identification", "author": "Anthony Miyaguchi and Murilo Gustineli and Austin Fischer and Ryan Lundqvist", "abstract": "  We present our approach for the SnakeCLEF 2024 competition to predict snake\nspecies from images. We explore and use Meta's DINOv2 vision transformer model\nfor feature extraction to tackle species' high variability and visual\nsimilarity in a dataset of 182,261 images. We perform exploratory analysis on\nembeddings to understand their structure, and train a linear classifier on the\nembeddings to predict species. Despite achieving a score of 39.69, our results\nshow promise for DINOv2 embeddings in snake identification. All code for this\nproject is available at https://github.com/dsgt-kaggle-clef/snakeclef-2024.\n", "link": "http://arxiv.org/abs/2407.06178v1", "date": "2024-07-08", "relevancy": 2.3729, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4825}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4717}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20with%20Self-Supervised%20Vision%20Transformers%20for%20Snake%0A%20%20Identification&body=Title%3A%20Transfer%20Learning%20with%20Self-Supervised%20Vision%20Transformers%20for%20Snake%0A%20%20Identification%0AAuthor%3A%20Anthony%20Miyaguchi%20and%20Murilo%20Gustineli%20and%20Austin%20Fischer%20and%20Ryan%20Lundqvist%0AAbstract%3A%20%20%20We%20present%20our%20approach%20for%20the%20SnakeCLEF%202024%20competition%20to%20predict%20snake%0Aspecies%20from%20images.%20We%20explore%20and%20use%20Meta%27s%20DINOv2%20vision%20transformer%20model%0Afor%20feature%20extraction%20to%20tackle%20species%27%20high%20variability%20and%20visual%0Asimilarity%20in%20a%20dataset%20of%20182%2C261%20images.%20We%20perform%20exploratory%20analysis%20on%0Aembeddings%20to%20understand%20their%20structure%2C%20and%20train%20a%20linear%20classifier%20on%20the%0Aembeddings%20to%20predict%20species.%20Despite%20achieving%20a%20score%20of%2039.69%2C%20our%20results%0Ashow%20promise%20for%20DINOv2%20embeddings%20in%20snake%20identification.%20All%20code%20for%20this%0Aproject%20is%20available%20at%20https%3A//github.com/dsgt-kaggle-clef/snakeclef-2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520with%2520Self-Supervised%2520Vision%2520Transformers%2520for%2520Snake%250A%2520%2520Identification%26entry.906535625%3DAnthony%2520Miyaguchi%2520and%2520Murilo%2520Gustineli%2520and%2520Austin%2520Fischer%2520and%2520Ryan%2520Lundqvist%26entry.1292438233%3D%2520%2520We%2520present%2520our%2520approach%2520for%2520the%2520SnakeCLEF%25202024%2520competition%2520to%2520predict%2520snake%250Aspecies%2520from%2520images.%2520We%2520explore%2520and%2520use%2520Meta%2527s%2520DINOv2%2520vision%2520transformer%2520model%250Afor%2520feature%2520extraction%2520to%2520tackle%2520species%2527%2520high%2520variability%2520and%2520visual%250Asimilarity%2520in%2520a%2520dataset%2520of%2520182%252C261%2520images.%2520We%2520perform%2520exploratory%2520analysis%2520on%250Aembeddings%2520to%2520understand%2520their%2520structure%252C%2520and%2520train%2520a%2520linear%2520classifier%2520on%2520the%250Aembeddings%2520to%2520predict%2520species.%2520Despite%2520achieving%2520a%2520score%2520of%252039.69%252C%2520our%2520results%250Ashow%2520promise%2520for%2520DINOv2%2520embeddings%2520in%2520snake%2520identification.%2520All%2520code%2520for%2520this%250Aproject%2520is%2520available%2520at%2520https%253A//github.com/dsgt-kaggle-clef/snakeclef-2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20with%20Self-Supervised%20Vision%20Transformers%20for%20Snake%0A%20%20Identification&entry.906535625=Anthony%20Miyaguchi%20and%20Murilo%20Gustineli%20and%20Austin%20Fischer%20and%20Ryan%20Lundqvist&entry.1292438233=%20%20We%20present%20our%20approach%20for%20the%20SnakeCLEF%202024%20competition%20to%20predict%20snake%0Aspecies%20from%20images.%20We%20explore%20and%20use%20Meta%27s%20DINOv2%20vision%20transformer%20model%0Afor%20feature%20extraction%20to%20tackle%20species%27%20high%20variability%20and%20visual%0Asimilarity%20in%20a%20dataset%20of%20182%2C261%20images.%20We%20perform%20exploratory%20analysis%20on%0Aembeddings%20to%20understand%20their%20structure%2C%20and%20train%20a%20linear%20classifier%20on%20the%0Aembeddings%20to%20predict%20species.%20Despite%20achieving%20a%20score%20of%2039.69%2C%20our%20results%0Ashow%20promise%20for%20DINOv2%20embeddings%20in%20snake%20identification.%20All%20code%20for%20this%0Aproject%20is%20available%20at%20https%3A//github.com/dsgt-kaggle-clef/snakeclef-2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06178v1&entry.124074799=Read"},
{"title": "On the Topology Awareness and Generalization Performance of Graph Neural\n  Networks", "author": "Junwei Su and Chuan Wu", "abstract": "  Many computer vision and machine learning problems are modelled as learning\ntasks on graphs where graph neural networks GNNs have emerged as a dominant\ntool for learning representations of graph structured data A key feature of\nGNNs is their use of graph structures as input enabling them to exploit the\ngraphs inherent topological properties known as the topology awareness of GNNs\nDespite the empirical successes of GNNs the influence of topology awareness on\ngeneralization performance remains unexplored, particularly for node level\ntasks that diverge from the assumption of data being independent and\nidentically distributed IID The precise definition and characterization of the\ntopology awareness of GNNs especially concerning different topological features\nare still unclear This paper introduces a comprehensive framework to\ncharacterize the topology awareness of GNNs across any topological feature\nUsing this framework we investigate the effects of topology awareness on GNN\ngeneralization performance Contrary to the prevailing belief that enhancing the\ntopology awareness of GNNs is always advantageous our analysis reveals a\ncritical insight improving the topology awareness of GNNs may inadvertently\nlead to unfair generalization across structural groups which might not be\ndesired in some scenarios Additionally we conduct a case study using the\nintrinsic graph metric the shortest path distance on various benchmark datasets\nThe empirical results of this case study confirm our theoretical insights\nMoreover we demonstrate the practical applicability of our framework by using\nit to tackle the cold start problem in graph active learning\n", "link": "http://arxiv.org/abs/2403.04482v2", "date": "2024-07-08", "relevancy": 2.3676, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4828}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4726}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Topology%20Awareness%20and%20Generalization%20Performance%20of%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20On%20the%20Topology%20Awareness%20and%20Generalization%20Performance%20of%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Junwei%20Su%20and%20Chuan%20Wu%0AAbstract%3A%20%20%20Many%20computer%20vision%20and%20machine%20learning%20problems%20are%20modelled%20as%20learning%0Atasks%20on%20graphs%20where%20graph%20neural%20networks%20GNNs%20have%20emerged%20as%20a%20dominant%0Atool%20for%20learning%20representations%20of%20graph%20structured%20data%20A%20key%20feature%20of%0AGNNs%20is%20their%20use%20of%20graph%20structures%20as%20input%20enabling%20them%20to%20exploit%20the%0Agraphs%20inherent%20topological%20properties%20known%20as%20the%20topology%20awareness%20of%20GNNs%0ADespite%20the%20empirical%20successes%20of%20GNNs%20the%20influence%20of%20topology%20awareness%20on%0Ageneralization%20performance%20remains%20unexplored%2C%20particularly%20for%20node%20level%0Atasks%20that%20diverge%20from%20the%20assumption%20of%20data%20being%20independent%20and%0Aidentically%20distributed%20IID%20The%20precise%20definition%20and%20characterization%20of%20the%0Atopology%20awareness%20of%20GNNs%20especially%20concerning%20different%20topological%20features%0Aare%20still%20unclear%20This%20paper%20introduces%20a%20comprehensive%20framework%20to%0Acharacterize%20the%20topology%20awareness%20of%20GNNs%20across%20any%20topological%20feature%0AUsing%20this%20framework%20we%20investigate%20the%20effects%20of%20topology%20awareness%20on%20GNN%0Ageneralization%20performance%20Contrary%20to%20the%20prevailing%20belief%20that%20enhancing%20the%0Atopology%20awareness%20of%20GNNs%20is%20always%20advantageous%20our%20analysis%20reveals%20a%0Acritical%20insight%20improving%20the%20topology%20awareness%20of%20GNNs%20may%20inadvertently%0Alead%20to%20unfair%20generalization%20across%20structural%20groups%20which%20might%20not%20be%0Adesired%20in%20some%20scenarios%20Additionally%20we%20conduct%20a%20case%20study%20using%20the%0Aintrinsic%20graph%20metric%20the%20shortest%20path%20distance%20on%20various%20benchmark%20datasets%0AThe%20empirical%20results%20of%20this%20case%20study%20confirm%20our%20theoretical%20insights%0AMoreover%20we%20demonstrate%20the%20practical%20applicability%20of%20our%20framework%20by%20using%0Ait%20to%20tackle%20the%20cold%20start%20problem%20in%20graph%20active%20learning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Topology%2520Awareness%2520and%2520Generalization%2520Performance%2520of%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DJunwei%2520Su%2520and%2520Chuan%2520Wu%26entry.1292438233%3D%2520%2520Many%2520computer%2520vision%2520and%2520machine%2520learning%2520problems%2520are%2520modelled%2520as%2520learning%250Atasks%2520on%2520graphs%2520where%2520graph%2520neural%2520networks%2520GNNs%2520have%2520emerged%2520as%2520a%2520dominant%250Atool%2520for%2520learning%2520representations%2520of%2520graph%2520structured%2520data%2520A%2520key%2520feature%2520of%250AGNNs%2520is%2520their%2520use%2520of%2520graph%2520structures%2520as%2520input%2520enabling%2520them%2520to%2520exploit%2520the%250Agraphs%2520inherent%2520topological%2520properties%2520known%2520as%2520the%2520topology%2520awareness%2520of%2520GNNs%250ADespite%2520the%2520empirical%2520successes%2520of%2520GNNs%2520the%2520influence%2520of%2520topology%2520awareness%2520on%250Ageneralization%2520performance%2520remains%2520unexplored%252C%2520particularly%2520for%2520node%2520level%250Atasks%2520that%2520diverge%2520from%2520the%2520assumption%2520of%2520data%2520being%2520independent%2520and%250Aidentically%2520distributed%2520IID%2520The%2520precise%2520definition%2520and%2520characterization%2520of%2520the%250Atopology%2520awareness%2520of%2520GNNs%2520especially%2520concerning%2520different%2520topological%2520features%250Aare%2520still%2520unclear%2520This%2520paper%2520introduces%2520a%2520comprehensive%2520framework%2520to%250Acharacterize%2520the%2520topology%2520awareness%2520of%2520GNNs%2520across%2520any%2520topological%2520feature%250AUsing%2520this%2520framework%2520we%2520investigate%2520the%2520effects%2520of%2520topology%2520awareness%2520on%2520GNN%250Ageneralization%2520performance%2520Contrary%2520to%2520the%2520prevailing%2520belief%2520that%2520enhancing%2520the%250Atopology%2520awareness%2520of%2520GNNs%2520is%2520always%2520advantageous%2520our%2520analysis%2520reveals%2520a%250Acritical%2520insight%2520improving%2520the%2520topology%2520awareness%2520of%2520GNNs%2520may%2520inadvertently%250Alead%2520to%2520unfair%2520generalization%2520across%2520structural%2520groups%2520which%2520might%2520not%2520be%250Adesired%2520in%2520some%2520scenarios%2520Additionally%2520we%2520conduct%2520a%2520case%2520study%2520using%2520the%250Aintrinsic%2520graph%2520metric%2520the%2520shortest%2520path%2520distance%2520on%2520various%2520benchmark%2520datasets%250AThe%2520empirical%2520results%2520of%2520this%2520case%2520study%2520confirm%2520our%2520theoretical%2520insights%250AMoreover%2520we%2520demonstrate%2520the%2520practical%2520applicability%2520of%2520our%2520framework%2520by%2520using%250Ait%2520to%2520tackle%2520the%2520cold%2520start%2520problem%2520in%2520graph%2520active%2520learning%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Topology%20Awareness%20and%20Generalization%20Performance%20of%20Graph%20Neural%0A%20%20Networks&entry.906535625=Junwei%20Su%20and%20Chuan%20Wu&entry.1292438233=%20%20Many%20computer%20vision%20and%20machine%20learning%20problems%20are%20modelled%20as%20learning%0Atasks%20on%20graphs%20where%20graph%20neural%20networks%20GNNs%20have%20emerged%20as%20a%20dominant%0Atool%20for%20learning%20representations%20of%20graph%20structured%20data%20A%20key%20feature%20of%0AGNNs%20is%20their%20use%20of%20graph%20structures%20as%20input%20enabling%20them%20to%20exploit%20the%0Agraphs%20inherent%20topological%20properties%20known%20as%20the%20topology%20awareness%20of%20GNNs%0ADespite%20the%20empirical%20successes%20of%20GNNs%20the%20influence%20of%20topology%20awareness%20on%0Ageneralization%20performance%20remains%20unexplored%2C%20particularly%20for%20node%20level%0Atasks%20that%20diverge%20from%20the%20assumption%20of%20data%20being%20independent%20and%0Aidentically%20distributed%20IID%20The%20precise%20definition%20and%20characterization%20of%20the%0Atopology%20awareness%20of%20GNNs%20especially%20concerning%20different%20topological%20features%0Aare%20still%20unclear%20This%20paper%20introduces%20a%20comprehensive%20framework%20to%0Acharacterize%20the%20topology%20awareness%20of%20GNNs%20across%20any%20topological%20feature%0AUsing%20this%20framework%20we%20investigate%20the%20effects%20of%20topology%20awareness%20on%20GNN%0Ageneralization%20performance%20Contrary%20to%20the%20prevailing%20belief%20that%20enhancing%20the%0Atopology%20awareness%20of%20GNNs%20is%20always%20advantageous%20our%20analysis%20reveals%20a%0Acritical%20insight%20improving%20the%20topology%20awareness%20of%20GNNs%20may%20inadvertently%0Alead%20to%20unfair%20generalization%20across%20structural%20groups%20which%20might%20not%20be%0Adesired%20in%20some%20scenarios%20Additionally%20we%20conduct%20a%20case%20study%20using%20the%0Aintrinsic%20graph%20metric%20the%20shortest%20path%20distance%20on%20various%20benchmark%20datasets%0AThe%20empirical%20results%20of%20this%20case%20study%20confirm%20our%20theoretical%20insights%0AMoreover%20we%20demonstrate%20the%20practical%20applicability%20of%20our%20framework%20by%20using%0Ait%20to%20tackle%20the%20cold%20start%20problem%20in%20graph%20active%20learning%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04482v2&entry.124074799=Read"},
{"title": "Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding\n  (Survey)", "author": "Subba Reddy Oota and Zijiao Chen and Manish Gupta and Raju S. Bapi and Gael Jobard and Frederic Alexandre and Xavier Hinaut", "abstract": "  Can we obtain insights about the brain using AI models? How is the\ninformation in deep learning models related to brain recordings? Can we improve\nAI models with the help of brain recordings? Such questions can be tackled by\nstudying brain recordings like functional magnetic resonance imaging (fMRI). As\na first step, the neuroscience community has contributed several large\ncognitive neuroscience datasets related to passive reading/listening/viewing of\nconcept words, narratives, pictures, and movies. Encoding and decoding models\nusing these datasets have also been proposed in the past two decades. These\nmodels serve as additional tools for basic cognitive science and neuroscience\nresearch. Encoding models aim at generating fMRI brain representations given a\nstimulus automatically. They have several practical applications in evaluating\nand diagnosing neurological conditions and thus may also help design therapies\nfor brain damage. Decoding models solve the inverse problem of reconstructing\nthe stimuli given the fMRI. They are useful for designing brain-machine or\nbrain-computer interfaces. Inspired by the effectiveness of deep learning\nmodels for natural language processing, computer vision, and speech, several\nneural encoding and decoding models have been recently proposed. In this\nsurvey, we will first discuss popular representations of language, vision and\nspeech stimuli, and present a summary of neuroscience datasets. Further, we\nwill review popular deep learning based encoding and decoding architectures and\nnote their benefits and limitations. Finally, we will conclude with a summary\nand discussion about future trends. Given the large amount of recently\npublished work in the computational cognitive neuroscience (CCN) community, we\nbelieve that this survey enables an entry point for DNN researchers to\ndiversify into CCN research.\n", "link": "http://arxiv.org/abs/2307.10246v2", "date": "2024-07-08", "relevancy": 2.3463, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.479}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4675}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Neural%20Networks%20and%20Brain%20Alignment%3A%20Brain%20Encoding%20and%20Decoding%0A%20%20%28Survey%29&body=Title%3A%20Deep%20Neural%20Networks%20and%20Brain%20Alignment%3A%20Brain%20Encoding%20and%20Decoding%0A%20%20%28Survey%29%0AAuthor%3A%20Subba%20Reddy%20Oota%20and%20Zijiao%20Chen%20and%20Manish%20Gupta%20and%20Raju%20S.%20Bapi%20and%20Gael%20Jobard%20and%20Frederic%20Alexandre%20and%20Xavier%20Hinaut%0AAbstract%3A%20%20%20Can%20we%20obtain%20insights%20about%20the%20brain%20using%20AI%20models%3F%20How%20is%20the%0Ainformation%20in%20deep%20learning%20models%20related%20to%20brain%20recordings%3F%20Can%20we%20improve%0AAI%20models%20with%20the%20help%20of%20brain%20recordings%3F%20Such%20questions%20can%20be%20tackled%20by%0Astudying%20brain%20recordings%20like%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29.%20As%0Aa%20first%20step%2C%20the%20neuroscience%20community%20has%20contributed%20several%20large%0Acognitive%20neuroscience%20datasets%20related%20to%20passive%20reading/listening/viewing%20of%0Aconcept%20words%2C%20narratives%2C%20pictures%2C%20and%20movies.%20Encoding%20and%20decoding%20models%0Ausing%20these%20datasets%20have%20also%20been%20proposed%20in%20the%20past%20two%20decades.%20These%0Amodels%20serve%20as%20additional%20tools%20for%20basic%20cognitive%20science%20and%20neuroscience%0Aresearch.%20Encoding%20models%20aim%20at%20generating%20fMRI%20brain%20representations%20given%20a%0Astimulus%20automatically.%20They%20have%20several%20practical%20applications%20in%20evaluating%0Aand%20diagnosing%20neurological%20conditions%20and%20thus%20may%20also%20help%20design%20therapies%0Afor%20brain%20damage.%20Decoding%20models%20solve%20the%20inverse%20problem%20of%20reconstructing%0Athe%20stimuli%20given%20the%20fMRI.%20They%20are%20useful%20for%20designing%20brain-machine%20or%0Abrain-computer%20interfaces.%20Inspired%20by%20the%20effectiveness%20of%20deep%20learning%0Amodels%20for%20natural%20language%20processing%2C%20computer%20vision%2C%20and%20speech%2C%20several%0Aneural%20encoding%20and%20decoding%20models%20have%20been%20recently%20proposed.%20In%20this%0Asurvey%2C%20we%20will%20first%20discuss%20popular%20representations%20of%20language%2C%20vision%20and%0Aspeech%20stimuli%2C%20and%20present%20a%20summary%20of%20neuroscience%20datasets.%20Further%2C%20we%0Awill%20review%20popular%20deep%20learning%20based%20encoding%20and%20decoding%20architectures%20and%0Anote%20their%20benefits%20and%20limitations.%20Finally%2C%20we%20will%20conclude%20with%20a%20summary%0Aand%20discussion%20about%20future%20trends.%20Given%20the%20large%20amount%20of%20recently%0Apublished%20work%20in%20the%20computational%20cognitive%20neuroscience%20%28CCN%29%20community%2C%20we%0Abelieve%20that%20this%20survey%20enables%20an%20entry%20point%20for%20DNN%20researchers%20to%0Adiversify%20into%20CCN%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.10246v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Neural%2520Networks%2520and%2520Brain%2520Alignment%253A%2520Brain%2520Encoding%2520and%2520Decoding%250A%2520%2520%2528Survey%2529%26entry.906535625%3DSubba%2520Reddy%2520Oota%2520and%2520Zijiao%2520Chen%2520and%2520Manish%2520Gupta%2520and%2520Raju%2520S.%2520Bapi%2520and%2520Gael%2520Jobard%2520and%2520Frederic%2520Alexandre%2520and%2520Xavier%2520Hinaut%26entry.1292438233%3D%2520%2520Can%2520we%2520obtain%2520insights%2520about%2520the%2520brain%2520using%2520AI%2520models%253F%2520How%2520is%2520the%250Ainformation%2520in%2520deep%2520learning%2520models%2520related%2520to%2520brain%2520recordings%253F%2520Can%2520we%2520improve%250AAI%2520models%2520with%2520the%2520help%2520of%2520brain%2520recordings%253F%2520Such%2520questions%2520can%2520be%2520tackled%2520by%250Astudying%2520brain%2520recordings%2520like%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529.%2520As%250Aa%2520first%2520step%252C%2520the%2520neuroscience%2520community%2520has%2520contributed%2520several%2520large%250Acognitive%2520neuroscience%2520datasets%2520related%2520to%2520passive%2520reading/listening/viewing%2520of%250Aconcept%2520words%252C%2520narratives%252C%2520pictures%252C%2520and%2520movies.%2520Encoding%2520and%2520decoding%2520models%250Ausing%2520these%2520datasets%2520have%2520also%2520been%2520proposed%2520in%2520the%2520past%2520two%2520decades.%2520These%250Amodels%2520serve%2520as%2520additional%2520tools%2520for%2520basic%2520cognitive%2520science%2520and%2520neuroscience%250Aresearch.%2520Encoding%2520models%2520aim%2520at%2520generating%2520fMRI%2520brain%2520representations%2520given%2520a%250Astimulus%2520automatically.%2520They%2520have%2520several%2520practical%2520applications%2520in%2520evaluating%250Aand%2520diagnosing%2520neurological%2520conditions%2520and%2520thus%2520may%2520also%2520help%2520design%2520therapies%250Afor%2520brain%2520damage.%2520Decoding%2520models%2520solve%2520the%2520inverse%2520problem%2520of%2520reconstructing%250Athe%2520stimuli%2520given%2520the%2520fMRI.%2520They%2520are%2520useful%2520for%2520designing%2520brain-machine%2520or%250Abrain-computer%2520interfaces.%2520Inspired%2520by%2520the%2520effectiveness%2520of%2520deep%2520learning%250Amodels%2520for%2520natural%2520language%2520processing%252C%2520computer%2520vision%252C%2520and%2520speech%252C%2520several%250Aneural%2520encoding%2520and%2520decoding%2520models%2520have%2520been%2520recently%2520proposed.%2520In%2520this%250Asurvey%252C%2520we%2520will%2520first%2520discuss%2520popular%2520representations%2520of%2520language%252C%2520vision%2520and%250Aspeech%2520stimuli%252C%2520and%2520present%2520a%2520summary%2520of%2520neuroscience%2520datasets.%2520Further%252C%2520we%250Awill%2520review%2520popular%2520deep%2520learning%2520based%2520encoding%2520and%2520decoding%2520architectures%2520and%250Anote%2520their%2520benefits%2520and%2520limitations.%2520Finally%252C%2520we%2520will%2520conclude%2520with%2520a%2520summary%250Aand%2520discussion%2520about%2520future%2520trends.%2520Given%2520the%2520large%2520amount%2520of%2520recently%250Apublished%2520work%2520in%2520the%2520computational%2520cognitive%2520neuroscience%2520%2528CCN%2529%2520community%252C%2520we%250Abelieve%2520that%2520this%2520survey%2520enables%2520an%2520entry%2520point%2520for%2520DNN%2520researchers%2520to%250Adiversify%2520into%2520CCN%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.10246v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Neural%20Networks%20and%20Brain%20Alignment%3A%20Brain%20Encoding%20and%20Decoding%0A%20%20%28Survey%29&entry.906535625=Subba%20Reddy%20Oota%20and%20Zijiao%20Chen%20and%20Manish%20Gupta%20and%20Raju%20S.%20Bapi%20and%20Gael%20Jobard%20and%20Frederic%20Alexandre%20and%20Xavier%20Hinaut&entry.1292438233=%20%20Can%20we%20obtain%20insights%20about%20the%20brain%20using%20AI%20models%3F%20How%20is%20the%0Ainformation%20in%20deep%20learning%20models%20related%20to%20brain%20recordings%3F%20Can%20we%20improve%0AAI%20models%20with%20the%20help%20of%20brain%20recordings%3F%20Such%20questions%20can%20be%20tackled%20by%0Astudying%20brain%20recordings%20like%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29.%20As%0Aa%20first%20step%2C%20the%20neuroscience%20community%20has%20contributed%20several%20large%0Acognitive%20neuroscience%20datasets%20related%20to%20passive%20reading/listening/viewing%20of%0Aconcept%20words%2C%20narratives%2C%20pictures%2C%20and%20movies.%20Encoding%20and%20decoding%20models%0Ausing%20these%20datasets%20have%20also%20been%20proposed%20in%20the%20past%20two%20decades.%20These%0Amodels%20serve%20as%20additional%20tools%20for%20basic%20cognitive%20science%20and%20neuroscience%0Aresearch.%20Encoding%20models%20aim%20at%20generating%20fMRI%20brain%20representations%20given%20a%0Astimulus%20automatically.%20They%20have%20several%20practical%20applications%20in%20evaluating%0Aand%20diagnosing%20neurological%20conditions%20and%20thus%20may%20also%20help%20design%20therapies%0Afor%20brain%20damage.%20Decoding%20models%20solve%20the%20inverse%20problem%20of%20reconstructing%0Athe%20stimuli%20given%20the%20fMRI.%20They%20are%20useful%20for%20designing%20brain-machine%20or%0Abrain-computer%20interfaces.%20Inspired%20by%20the%20effectiveness%20of%20deep%20learning%0Amodels%20for%20natural%20language%20processing%2C%20computer%20vision%2C%20and%20speech%2C%20several%0Aneural%20encoding%20and%20decoding%20models%20have%20been%20recently%20proposed.%20In%20this%0Asurvey%2C%20we%20will%20first%20discuss%20popular%20representations%20of%20language%2C%20vision%20and%0Aspeech%20stimuli%2C%20and%20present%20a%20summary%20of%20neuroscience%20datasets.%20Further%2C%20we%0Awill%20review%20popular%20deep%20learning%20based%20encoding%20and%20decoding%20architectures%20and%0Anote%20their%20benefits%20and%20limitations.%20Finally%2C%20we%20will%20conclude%20with%20a%20summary%0Aand%20discussion%20about%20future%20trends.%20Given%20the%20large%20amount%20of%20recently%0Apublished%20work%20in%20the%20computational%20cognitive%20neuroscience%20%28CCN%29%20community%2C%20we%0Abelieve%20that%20this%20survey%20enables%20an%20entry%20point%20for%20DNN%20researchers%20to%0Adiversify%20into%20CCN%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.10246v2&entry.124074799=Read"},
{"title": "4D Contrastive Superflows are Dense 3D Representation Learners", "author": "Xiang Xu and Lingdong Kong and Hui Shuai and Wenwei Zhang and Liang Pan and Kai Chen and Ziwei Liu and Qingshan Liu", "abstract": "  In the realm of autonomous driving, accurate 3D perception is the foundation.\nHowever, developing such models relies on extensive human annotations -- a\nprocess that is both costly and labor-intensive. To address this challenge from\na data representation learning perspective, we introduce SuperFlow, a novel\nframework designed to harness consecutive LiDAR-camera pairs for establishing\nspatiotemporal pretraining objectives. SuperFlow stands out by integrating two\nkey designs: 1) a dense-to-sparse consistency regularization, which promotes\ninsensitivity to point cloud density variations during feature learning, and 2)\na flow-based contrastive learning module, carefully crafted to extract\nmeaningful temporal cues from readily available sensor calibrations. To further\nboost learning efficiency, we incorporate a plug-and-play view consistency\nmodule that enhances the alignment of the knowledge distilled from camera\nviews. Extensive comparative and ablation studies across 11 heterogeneous LiDAR\ndatasets validate our effectiveness and superiority. Additionally, we observe\nseveral interesting emerging properties by scaling up the 2D and 3D backbones\nduring pretraining, shedding light on the future research of 3D foundation\nmodels for LiDAR-based perception.\n", "link": "http://arxiv.org/abs/2407.06190v1", "date": "2024-07-08", "relevancy": 2.3341, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5934}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5878}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D%20Contrastive%20Superflows%20are%20Dense%203D%20Representation%20Learners&body=Title%3A%204D%20Contrastive%20Superflows%20are%20Dense%203D%20Representation%20Learners%0AAuthor%3A%20Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Hui%20Shuai%20and%20Wenwei%20Zhang%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Ziwei%20Liu%20and%20Qingshan%20Liu%0AAbstract%3A%20%20%20In%20the%20realm%20of%20autonomous%20driving%2C%20accurate%203D%20perception%20is%20the%20foundation.%0AHowever%2C%20developing%20such%20models%20relies%20on%20extensive%20human%20annotations%20--%20a%0Aprocess%20that%20is%20both%20costly%20and%20labor-intensive.%20To%20address%20this%20challenge%20from%0Aa%20data%20representation%20learning%20perspective%2C%20we%20introduce%20SuperFlow%2C%20a%20novel%0Aframework%20designed%20to%20harness%20consecutive%20LiDAR-camera%20pairs%20for%20establishing%0Aspatiotemporal%20pretraining%20objectives.%20SuperFlow%20stands%20out%20by%20integrating%20two%0Akey%20designs%3A%201%29%20a%20dense-to-sparse%20consistency%20regularization%2C%20which%20promotes%0Ainsensitivity%20to%20point%20cloud%20density%20variations%20during%20feature%20learning%2C%20and%202%29%0Aa%20flow-based%20contrastive%20learning%20module%2C%20carefully%20crafted%20to%20extract%0Ameaningful%20temporal%20cues%20from%20readily%20available%20sensor%20calibrations.%20To%20further%0Aboost%20learning%20efficiency%2C%20we%20incorporate%20a%20plug-and-play%20view%20consistency%0Amodule%20that%20enhances%20the%20alignment%20of%20the%20knowledge%20distilled%20from%20camera%0Aviews.%20Extensive%20comparative%20and%20ablation%20studies%20across%2011%20heterogeneous%20LiDAR%0Adatasets%20validate%20our%20effectiveness%20and%20superiority.%20Additionally%2C%20we%20observe%0Aseveral%20interesting%20emerging%20properties%20by%20scaling%20up%20the%202D%20and%203D%20backbones%0Aduring%20pretraining%2C%20shedding%20light%20on%20the%20future%20research%20of%203D%20foundation%0Amodels%20for%20LiDAR-based%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D%2520Contrastive%2520Superflows%2520are%2520Dense%25203D%2520Representation%2520Learners%26entry.906535625%3DXiang%2520Xu%2520and%2520Lingdong%2520Kong%2520and%2520Hui%2520Shuai%2520and%2520Wenwei%2520Zhang%2520and%2520Liang%2520Pan%2520and%2520Kai%2520Chen%2520and%2520Ziwei%2520Liu%2520and%2520Qingshan%2520Liu%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520autonomous%2520driving%252C%2520accurate%25203D%2520perception%2520is%2520the%2520foundation.%250AHowever%252C%2520developing%2520such%2520models%2520relies%2520on%2520extensive%2520human%2520annotations%2520--%2520a%250Aprocess%2520that%2520is%2520both%2520costly%2520and%2520labor-intensive.%2520To%2520address%2520this%2520challenge%2520from%250Aa%2520data%2520representation%2520learning%2520perspective%252C%2520we%2520introduce%2520SuperFlow%252C%2520a%2520novel%250Aframework%2520designed%2520to%2520harness%2520consecutive%2520LiDAR-camera%2520pairs%2520for%2520establishing%250Aspatiotemporal%2520pretraining%2520objectives.%2520SuperFlow%2520stands%2520out%2520by%2520integrating%2520two%250Akey%2520designs%253A%25201%2529%2520a%2520dense-to-sparse%2520consistency%2520regularization%252C%2520which%2520promotes%250Ainsensitivity%2520to%2520point%2520cloud%2520density%2520variations%2520during%2520feature%2520learning%252C%2520and%25202%2529%250Aa%2520flow-based%2520contrastive%2520learning%2520module%252C%2520carefully%2520crafted%2520to%2520extract%250Ameaningful%2520temporal%2520cues%2520from%2520readily%2520available%2520sensor%2520calibrations.%2520To%2520further%250Aboost%2520learning%2520efficiency%252C%2520we%2520incorporate%2520a%2520plug-and-play%2520view%2520consistency%250Amodule%2520that%2520enhances%2520the%2520alignment%2520of%2520the%2520knowledge%2520distilled%2520from%2520camera%250Aviews.%2520Extensive%2520comparative%2520and%2520ablation%2520studies%2520across%252011%2520heterogeneous%2520LiDAR%250Adatasets%2520validate%2520our%2520effectiveness%2520and%2520superiority.%2520Additionally%252C%2520we%2520observe%250Aseveral%2520interesting%2520emerging%2520properties%2520by%2520scaling%2520up%2520the%25202D%2520and%25203D%2520backbones%250Aduring%2520pretraining%252C%2520shedding%2520light%2520on%2520the%2520future%2520research%2520of%25203D%2520foundation%250Amodels%2520for%2520LiDAR-based%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Contrastive%20Superflows%20are%20Dense%203D%20Representation%20Learners&entry.906535625=Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Hui%20Shuai%20and%20Wenwei%20Zhang%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Ziwei%20Liu%20and%20Qingshan%20Liu&entry.1292438233=%20%20In%20the%20realm%20of%20autonomous%20driving%2C%20accurate%203D%20perception%20is%20the%20foundation.%0AHowever%2C%20developing%20such%20models%20relies%20on%20extensive%20human%20annotations%20--%20a%0Aprocess%20that%20is%20both%20costly%20and%20labor-intensive.%20To%20address%20this%20challenge%20from%0Aa%20data%20representation%20learning%20perspective%2C%20we%20introduce%20SuperFlow%2C%20a%20novel%0Aframework%20designed%20to%20harness%20consecutive%20LiDAR-camera%20pairs%20for%20establishing%0Aspatiotemporal%20pretraining%20objectives.%20SuperFlow%20stands%20out%20by%20integrating%20two%0Akey%20designs%3A%201%29%20a%20dense-to-sparse%20consistency%20regularization%2C%20which%20promotes%0Ainsensitivity%20to%20point%20cloud%20density%20variations%20during%20feature%20learning%2C%20and%202%29%0Aa%20flow-based%20contrastive%20learning%20module%2C%20carefully%20crafted%20to%20extract%0Ameaningful%20temporal%20cues%20from%20readily%20available%20sensor%20calibrations.%20To%20further%0Aboost%20learning%20efficiency%2C%20we%20incorporate%20a%20plug-and-play%20view%20consistency%0Amodule%20that%20enhances%20the%20alignment%20of%20the%20knowledge%20distilled%20from%20camera%0Aviews.%20Extensive%20comparative%20and%20ablation%20studies%20across%2011%20heterogeneous%20LiDAR%0Adatasets%20validate%20our%20effectiveness%20and%20superiority.%20Additionally%2C%20we%20observe%0Aseveral%20interesting%20emerging%20properties%20by%20scaling%20up%20the%202D%20and%203D%20backbones%0Aduring%20pretraining%2C%20shedding%20light%20on%20the%20future%20research%20of%203D%20foundation%0Amodels%20for%20LiDAR-based%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06190v1&entry.124074799=Read"},
{"title": "Autonomous Mobile Robot Navigation: Tracking problem", "author": "Salem Ameen and Husan F. Vokhidov", "abstract": "  This paper presents a study on autonomous robot navigation, focusing on three\nkey behaviors: Odometry, Target Tracking, and Obstacle Avoidance. Each behavior\nis described in detail, along with experimental setups for simulated and\nreal-world environments. Odometry utilizes wheel encoder data for precise\nnavigation along predefined paths, validated through experiments with a Pioneer\nrobot. Target Tracking employs vision-based techniques for pursuing designated\ntargets while avoiding obstacles, demonstrated on the same platform. Obstacle\nAvoidance utilizes ultrasonic sensors to navigate cluttered environments\nsafely, validated in both simulated and real-world scenarios. Additionally, the\npaper extends the project to include an Elegoo robot car, leveraging its\nfeatures for enhanced experimentation. Through advanced algorithms and\nexperimental validations, this study provides insights into developing robust\nnavigation systems for autonomous robots.\n", "link": "http://arxiv.org/abs/2407.06118v1", "date": "2024-07-08", "relevancy": 2.3191, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6494}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5334}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Mobile%20Robot%20Navigation%3A%20Tracking%20problem&body=Title%3A%20Autonomous%20Mobile%20Robot%20Navigation%3A%20Tracking%20problem%0AAuthor%3A%20Salem%20Ameen%20and%20Husan%20F.%20Vokhidov%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20study%20on%20autonomous%20robot%20navigation%2C%20focusing%20on%20three%0Akey%20behaviors%3A%20Odometry%2C%20Target%20Tracking%2C%20and%20Obstacle%20Avoidance.%20Each%20behavior%0Ais%20described%20in%20detail%2C%20along%20with%20experimental%20setups%20for%20simulated%20and%0Areal-world%20environments.%20Odometry%20utilizes%20wheel%20encoder%20data%20for%20precise%0Anavigation%20along%20predefined%20paths%2C%20validated%20through%20experiments%20with%20a%20Pioneer%0Arobot.%20Target%20Tracking%20employs%20vision-based%20techniques%20for%20pursuing%20designated%0Atargets%20while%20avoiding%20obstacles%2C%20demonstrated%20on%20the%20same%20platform.%20Obstacle%0AAvoidance%20utilizes%20ultrasonic%20sensors%20to%20navigate%20cluttered%20environments%0Asafely%2C%20validated%20in%20both%20simulated%20and%20real-world%20scenarios.%20Additionally%2C%20the%0Apaper%20extends%20the%20project%20to%20include%20an%20Elegoo%20robot%20car%2C%20leveraging%20its%0Afeatures%20for%20enhanced%20experimentation.%20Through%20advanced%20algorithms%20and%0Aexperimental%20validations%2C%20this%20study%20provides%20insights%20into%20developing%20robust%0Anavigation%20systems%20for%20autonomous%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Mobile%2520Robot%2520Navigation%253A%2520Tracking%2520problem%26entry.906535625%3DSalem%2520Ameen%2520and%2520Husan%2520F.%2520Vokhidov%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520study%2520on%2520autonomous%2520robot%2520navigation%252C%2520focusing%2520on%2520three%250Akey%2520behaviors%253A%2520Odometry%252C%2520Target%2520Tracking%252C%2520and%2520Obstacle%2520Avoidance.%2520Each%2520behavior%250Ais%2520described%2520in%2520detail%252C%2520along%2520with%2520experimental%2520setups%2520for%2520simulated%2520and%250Areal-world%2520environments.%2520Odometry%2520utilizes%2520wheel%2520encoder%2520data%2520for%2520precise%250Anavigation%2520along%2520predefined%2520paths%252C%2520validated%2520through%2520experiments%2520with%2520a%2520Pioneer%250Arobot.%2520Target%2520Tracking%2520employs%2520vision-based%2520techniques%2520for%2520pursuing%2520designated%250Atargets%2520while%2520avoiding%2520obstacles%252C%2520demonstrated%2520on%2520the%2520same%2520platform.%2520Obstacle%250AAvoidance%2520utilizes%2520ultrasonic%2520sensors%2520to%2520navigate%2520cluttered%2520environments%250Asafely%252C%2520validated%2520in%2520both%2520simulated%2520and%2520real-world%2520scenarios.%2520Additionally%252C%2520the%250Apaper%2520extends%2520the%2520project%2520to%2520include%2520an%2520Elegoo%2520robot%2520car%252C%2520leveraging%2520its%250Afeatures%2520for%2520enhanced%2520experimentation.%2520Through%2520advanced%2520algorithms%2520and%250Aexperimental%2520validations%252C%2520this%2520study%2520provides%2520insights%2520into%2520developing%2520robust%250Anavigation%2520systems%2520for%2520autonomous%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Mobile%20Robot%20Navigation%3A%20Tracking%20problem&entry.906535625=Salem%20Ameen%20and%20Husan%20F.%20Vokhidov&entry.1292438233=%20%20This%20paper%20presents%20a%20study%20on%20autonomous%20robot%20navigation%2C%20focusing%20on%20three%0Akey%20behaviors%3A%20Odometry%2C%20Target%20Tracking%2C%20and%20Obstacle%20Avoidance.%20Each%20behavior%0Ais%20described%20in%20detail%2C%20along%20with%20experimental%20setups%20for%20simulated%20and%0Areal-world%20environments.%20Odometry%20utilizes%20wheel%20encoder%20data%20for%20precise%0Anavigation%20along%20predefined%20paths%2C%20validated%20through%20experiments%20with%20a%20Pioneer%0Arobot.%20Target%20Tracking%20employs%20vision-based%20techniques%20for%20pursuing%20designated%0Atargets%20while%20avoiding%20obstacles%2C%20demonstrated%20on%20the%20same%20platform.%20Obstacle%0AAvoidance%20utilizes%20ultrasonic%20sensors%20to%20navigate%20cluttered%20environments%0Asafely%2C%20validated%20in%20both%20simulated%20and%20real-world%20scenarios.%20Additionally%2C%20the%0Apaper%20extends%20the%20project%20to%20include%20an%20Elegoo%20robot%20car%2C%20leveraging%20its%0Afeatures%20for%20enhanced%20experimentation.%20Through%20advanced%20algorithms%20and%0Aexperimental%20validations%2C%20this%20study%20provides%20insights%20into%20developing%20robust%0Anavigation%20systems%20for%20autonomous%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06118v1&entry.124074799=Read"},
{"title": "Multi-clue Consistency Learning to Bridge Gaps Between General and\n  Oriented Object in Semi-supervised Detection", "author": "Chenxu Wang and Chunyan Xu and Ziqi Gu and Zhen Cui", "abstract": "  While existing semi-supervised object detection (SSOD) methods perform well\nin general scenes, they encounter challenges in handling oriented objects in\naerial images. We experimentally find three gaps between general and oriented\nobject detection in semi-supervised learning: 1) Sampling inconsistency: the\ncommon center sampling is not suitable for oriented objects with larger aspect\nratios when selecting positive labels from labeled data. 2) Assignment\ninconsistency: balancing the precision and localization quality of oriented\npseudo-boxes poses greater challenges which introduces more noise when\nselecting positive labels from unlabeled data. 3) Confidence inconsistency:\nthere exists more mismatch between the predicted classification and\nlocalization qualities when considering oriented objects, affecting the\nselection of pseudo-labels. Therefore, we propose a Multi-clue Consistency\nLearning (MCL) framework to bridge gaps between general and oriented objects in\nsemi-supervised detection. Specifically, considering various shapes of rotated\nobjects, the Gaussian Center Assignment is specially designed to select the\npixel-level positive labels from labeled data. We then introduce the\nScale-aware Label Assignment to select pixel-level pseudo-labels instead of\nunreliable pseudo-boxes, which is a divide-and-rule strategy suited for objects\nwith various scales. The Consistent Confidence Soft Label is adopted to further\nboost the detector by maintaining the alignment of the predicted results.\nComprehensive experiments on DOTA-v1.5 and DOTA-v1.0 benchmarks demonstrate\nthat our proposed MCL can achieve state-of-the-art performance in the\nsemi-supervised oriented object detection task.\n", "link": "http://arxiv.org/abs/2407.05909v1", "date": "2024-07-08", "relevancy": 2.3184, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5915}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5722}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-clue%20Consistency%20Learning%20to%20Bridge%20Gaps%20Between%20General%20and%0A%20%20Oriented%20Object%20in%20Semi-supervised%20Detection&body=Title%3A%20Multi-clue%20Consistency%20Learning%20to%20Bridge%20Gaps%20Between%20General%20and%0A%20%20Oriented%20Object%20in%20Semi-supervised%20Detection%0AAuthor%3A%20Chenxu%20Wang%20and%20Chunyan%20Xu%20and%20Ziqi%20Gu%20and%20Zhen%20Cui%0AAbstract%3A%20%20%20While%20existing%20semi-supervised%20object%20detection%20%28SSOD%29%20methods%20perform%20well%0Ain%20general%20scenes%2C%20they%20encounter%20challenges%20in%20handling%20oriented%20objects%20in%0Aaerial%20images.%20We%20experimentally%20find%20three%20gaps%20between%20general%20and%20oriented%0Aobject%20detection%20in%20semi-supervised%20learning%3A%201%29%20Sampling%20inconsistency%3A%20the%0Acommon%20center%20sampling%20is%20not%20suitable%20for%20oriented%20objects%20with%20larger%20aspect%0Aratios%20when%20selecting%20positive%20labels%20from%20labeled%20data.%202%29%20Assignment%0Ainconsistency%3A%20balancing%20the%20precision%20and%20localization%20quality%20of%20oriented%0Apseudo-boxes%20poses%20greater%20challenges%20which%20introduces%20more%20noise%20when%0Aselecting%20positive%20labels%20from%20unlabeled%20data.%203%29%20Confidence%20inconsistency%3A%0Athere%20exists%20more%20mismatch%20between%20the%20predicted%20classification%20and%0Alocalization%20qualities%20when%20considering%20oriented%20objects%2C%20affecting%20the%0Aselection%20of%20pseudo-labels.%20Therefore%2C%20we%20propose%20a%20Multi-clue%20Consistency%0ALearning%20%28MCL%29%20framework%20to%20bridge%20gaps%20between%20general%20and%20oriented%20objects%20in%0Asemi-supervised%20detection.%20Specifically%2C%20considering%20various%20shapes%20of%20rotated%0Aobjects%2C%20the%20Gaussian%20Center%20Assignment%20is%20specially%20designed%20to%20select%20the%0Apixel-level%20positive%20labels%20from%20labeled%20data.%20We%20then%20introduce%20the%0AScale-aware%20Label%20Assignment%20to%20select%20pixel-level%20pseudo-labels%20instead%20of%0Aunreliable%20pseudo-boxes%2C%20which%20is%20a%20divide-and-rule%20strategy%20suited%20for%20objects%0Awith%20various%20scales.%20The%20Consistent%20Confidence%20Soft%20Label%20is%20adopted%20to%20further%0Aboost%20the%20detector%20by%20maintaining%20the%20alignment%20of%20the%20predicted%20results.%0AComprehensive%20experiments%20on%20DOTA-v1.5%20and%20DOTA-v1.0%20benchmarks%20demonstrate%0Athat%20our%20proposed%20MCL%20can%20achieve%20state-of-the-art%20performance%20in%20the%0Asemi-supervised%20oriented%20object%20detection%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-clue%2520Consistency%2520Learning%2520to%2520Bridge%2520Gaps%2520Between%2520General%2520and%250A%2520%2520Oriented%2520Object%2520in%2520Semi-supervised%2520Detection%26entry.906535625%3DChenxu%2520Wang%2520and%2520Chunyan%2520Xu%2520and%2520Ziqi%2520Gu%2520and%2520Zhen%2520Cui%26entry.1292438233%3D%2520%2520While%2520existing%2520semi-supervised%2520object%2520detection%2520%2528SSOD%2529%2520methods%2520perform%2520well%250Ain%2520general%2520scenes%252C%2520they%2520encounter%2520challenges%2520in%2520handling%2520oriented%2520objects%2520in%250Aaerial%2520images.%2520We%2520experimentally%2520find%2520three%2520gaps%2520between%2520general%2520and%2520oriented%250Aobject%2520detection%2520in%2520semi-supervised%2520learning%253A%25201%2529%2520Sampling%2520inconsistency%253A%2520the%250Acommon%2520center%2520sampling%2520is%2520not%2520suitable%2520for%2520oriented%2520objects%2520with%2520larger%2520aspect%250Aratios%2520when%2520selecting%2520positive%2520labels%2520from%2520labeled%2520data.%25202%2529%2520Assignment%250Ainconsistency%253A%2520balancing%2520the%2520precision%2520and%2520localization%2520quality%2520of%2520oriented%250Apseudo-boxes%2520poses%2520greater%2520challenges%2520which%2520introduces%2520more%2520noise%2520when%250Aselecting%2520positive%2520labels%2520from%2520unlabeled%2520data.%25203%2529%2520Confidence%2520inconsistency%253A%250Athere%2520exists%2520more%2520mismatch%2520between%2520the%2520predicted%2520classification%2520and%250Alocalization%2520qualities%2520when%2520considering%2520oriented%2520objects%252C%2520affecting%2520the%250Aselection%2520of%2520pseudo-labels.%2520Therefore%252C%2520we%2520propose%2520a%2520Multi-clue%2520Consistency%250ALearning%2520%2528MCL%2529%2520framework%2520to%2520bridge%2520gaps%2520between%2520general%2520and%2520oriented%2520objects%2520in%250Asemi-supervised%2520detection.%2520Specifically%252C%2520considering%2520various%2520shapes%2520of%2520rotated%250Aobjects%252C%2520the%2520Gaussian%2520Center%2520Assignment%2520is%2520specially%2520designed%2520to%2520select%2520the%250Apixel-level%2520positive%2520labels%2520from%2520labeled%2520data.%2520We%2520then%2520introduce%2520the%250AScale-aware%2520Label%2520Assignment%2520to%2520select%2520pixel-level%2520pseudo-labels%2520instead%2520of%250Aunreliable%2520pseudo-boxes%252C%2520which%2520is%2520a%2520divide-and-rule%2520strategy%2520suited%2520for%2520objects%250Awith%2520various%2520scales.%2520The%2520Consistent%2520Confidence%2520Soft%2520Label%2520is%2520adopted%2520to%2520further%250Aboost%2520the%2520detector%2520by%2520maintaining%2520the%2520alignment%2520of%2520the%2520predicted%2520results.%250AComprehensive%2520experiments%2520on%2520DOTA-v1.5%2520and%2520DOTA-v1.0%2520benchmarks%2520demonstrate%250Athat%2520our%2520proposed%2520MCL%2520can%2520achieve%2520state-of-the-art%2520performance%2520in%2520the%250Asemi-supervised%2520oriented%2520object%2520detection%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-clue%20Consistency%20Learning%20to%20Bridge%20Gaps%20Between%20General%20and%0A%20%20Oriented%20Object%20in%20Semi-supervised%20Detection&entry.906535625=Chenxu%20Wang%20and%20Chunyan%20Xu%20and%20Ziqi%20Gu%20and%20Zhen%20Cui&entry.1292438233=%20%20While%20existing%20semi-supervised%20object%20detection%20%28SSOD%29%20methods%20perform%20well%0Ain%20general%20scenes%2C%20they%20encounter%20challenges%20in%20handling%20oriented%20objects%20in%0Aaerial%20images.%20We%20experimentally%20find%20three%20gaps%20between%20general%20and%20oriented%0Aobject%20detection%20in%20semi-supervised%20learning%3A%201%29%20Sampling%20inconsistency%3A%20the%0Acommon%20center%20sampling%20is%20not%20suitable%20for%20oriented%20objects%20with%20larger%20aspect%0Aratios%20when%20selecting%20positive%20labels%20from%20labeled%20data.%202%29%20Assignment%0Ainconsistency%3A%20balancing%20the%20precision%20and%20localization%20quality%20of%20oriented%0Apseudo-boxes%20poses%20greater%20challenges%20which%20introduces%20more%20noise%20when%0Aselecting%20positive%20labels%20from%20unlabeled%20data.%203%29%20Confidence%20inconsistency%3A%0Athere%20exists%20more%20mismatch%20between%20the%20predicted%20classification%20and%0Alocalization%20qualities%20when%20considering%20oriented%20objects%2C%20affecting%20the%0Aselection%20of%20pseudo-labels.%20Therefore%2C%20we%20propose%20a%20Multi-clue%20Consistency%0ALearning%20%28MCL%29%20framework%20to%20bridge%20gaps%20between%20general%20and%20oriented%20objects%20in%0Asemi-supervised%20detection.%20Specifically%2C%20considering%20various%20shapes%20of%20rotated%0Aobjects%2C%20the%20Gaussian%20Center%20Assignment%20is%20specially%20designed%20to%20select%20the%0Apixel-level%20positive%20labels%20from%20labeled%20data.%20We%20then%20introduce%20the%0AScale-aware%20Label%20Assignment%20to%20select%20pixel-level%20pseudo-labels%20instead%20of%0Aunreliable%20pseudo-boxes%2C%20which%20is%20a%20divide-and-rule%20strategy%20suited%20for%20objects%0Awith%20various%20scales.%20The%20Consistent%20Confidence%20Soft%20Label%20is%20adopted%20to%20further%0Aboost%20the%20detector%20by%20maintaining%20the%20alignment%20of%20the%20predicted%20results.%0AComprehensive%20experiments%20on%20DOTA-v1.5%20and%20DOTA-v1.0%20benchmarks%20demonstrate%0Athat%20our%20proposed%20MCL%20can%20achieve%20state-of-the-art%20performance%20in%20the%0Asemi-supervised%20oriented%20object%20detection%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05909v1&entry.124074799=Read"},
{"title": "Object-Oriented Material Classification and 3D Clustering for Improved\n  Semantic Perception and Mapping in Mobile Robots", "author": "Siva Krishna Ravipati and Ehsan Latif and Ramviyas Parasuraman and Suchendra M. Bhandarkar", "abstract": "  Classification of different object surface material types can play a\nsignificant role in the decision-making algorithms for mobile robots and\nautonomous vehicles. RGB-based scene-level semantic segmentation has been\nwell-addressed in the literature. However, improving material recognition using\nthe depth modality and its integration with SLAM algorithms for 3D semantic\nmapping could unlock new potential benefits in the robotics perception\npipeline. To this end, we propose a complementarity-aware deep learning\napproach for RGB-D-based material classification built on top of an\nobject-oriented pipeline. The approach further integrates the ORB-SLAM2 method\nfor 3D scene mapping with multiscale clustering of the detected material\nsemantics in the point cloud map generated by the visual SLAM algorithm.\nExtensive experimental results with existing public datasets and newly\ncontributed real-world robot datasets demonstrate a significant improvement in\nmaterial classification and 3D clustering accuracy compared to state-of-the-art\napproaches for 3D semantic scene mapping.\n", "link": "http://arxiv.org/abs/2407.06077v1", "date": "2024-07-08", "relevancy": 2.3116, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.662}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Oriented%20Material%20Classification%20and%203D%20Clustering%20for%20Improved%0A%20%20Semantic%20Perception%20and%20Mapping%20in%20Mobile%20Robots&body=Title%3A%20Object-Oriented%20Material%20Classification%20and%203D%20Clustering%20for%20Improved%0A%20%20Semantic%20Perception%20and%20Mapping%20in%20Mobile%20Robots%0AAuthor%3A%20Siva%20Krishna%20Ravipati%20and%20Ehsan%20Latif%20and%20Ramviyas%20Parasuraman%20and%20Suchendra%20M.%20Bhandarkar%0AAbstract%3A%20%20%20Classification%20of%20different%20object%20surface%20material%20types%20can%20play%20a%0Asignificant%20role%20in%20the%20decision-making%20algorithms%20for%20mobile%20robots%20and%0Aautonomous%20vehicles.%20RGB-based%20scene-level%20semantic%20segmentation%20has%20been%0Awell-addressed%20in%20the%20literature.%20However%2C%20improving%20material%20recognition%20using%0Athe%20depth%20modality%20and%20its%20integration%20with%20SLAM%20algorithms%20for%203D%20semantic%0Amapping%20could%20unlock%20new%20potential%20benefits%20in%20the%20robotics%20perception%0Apipeline.%20To%20this%20end%2C%20we%20propose%20a%20complementarity-aware%20deep%20learning%0Aapproach%20for%20RGB-D-based%20material%20classification%20built%20on%20top%20of%20an%0Aobject-oriented%20pipeline.%20The%20approach%20further%20integrates%20the%20ORB-SLAM2%20method%0Afor%203D%20scene%20mapping%20with%20multiscale%20clustering%20of%20the%20detected%20material%0Asemantics%20in%20the%20point%20cloud%20map%20generated%20by%20the%20visual%20SLAM%20algorithm.%0AExtensive%20experimental%20results%20with%20existing%20public%20datasets%20and%20newly%0Acontributed%20real-world%20robot%20datasets%20demonstrate%20a%20significant%20improvement%20in%0Amaterial%20classification%20and%203D%20clustering%20accuracy%20compared%20to%20state-of-the-art%0Aapproaches%20for%203D%20semantic%20scene%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Oriented%2520Material%2520Classification%2520and%25203D%2520Clustering%2520for%2520Improved%250A%2520%2520Semantic%2520Perception%2520and%2520Mapping%2520in%2520Mobile%2520Robots%26entry.906535625%3DSiva%2520Krishna%2520Ravipati%2520and%2520Ehsan%2520Latif%2520and%2520Ramviyas%2520Parasuraman%2520and%2520Suchendra%2520M.%2520Bhandarkar%26entry.1292438233%3D%2520%2520Classification%2520of%2520different%2520object%2520surface%2520material%2520types%2520can%2520play%2520a%250Asignificant%2520role%2520in%2520the%2520decision-making%2520algorithms%2520for%2520mobile%2520robots%2520and%250Aautonomous%2520vehicles.%2520RGB-based%2520scene-level%2520semantic%2520segmentation%2520has%2520been%250Awell-addressed%2520in%2520the%2520literature.%2520However%252C%2520improving%2520material%2520recognition%2520using%250Athe%2520depth%2520modality%2520and%2520its%2520integration%2520with%2520SLAM%2520algorithms%2520for%25203D%2520semantic%250Amapping%2520could%2520unlock%2520new%2520potential%2520benefits%2520in%2520the%2520robotics%2520perception%250Apipeline.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520complementarity-aware%2520deep%2520learning%250Aapproach%2520for%2520RGB-D-based%2520material%2520classification%2520built%2520on%2520top%2520of%2520an%250Aobject-oriented%2520pipeline.%2520The%2520approach%2520further%2520integrates%2520the%2520ORB-SLAM2%2520method%250Afor%25203D%2520scene%2520mapping%2520with%2520multiscale%2520clustering%2520of%2520the%2520detected%2520material%250Asemantics%2520in%2520the%2520point%2520cloud%2520map%2520generated%2520by%2520the%2520visual%2520SLAM%2520algorithm.%250AExtensive%2520experimental%2520results%2520with%2520existing%2520public%2520datasets%2520and%2520newly%250Acontributed%2520real-world%2520robot%2520datasets%2520demonstrate%2520a%2520significant%2520improvement%2520in%250Amaterial%2520classification%2520and%25203D%2520clustering%2520accuracy%2520compared%2520to%2520state-of-the-art%250Aapproaches%2520for%25203D%2520semantic%2520scene%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Oriented%20Material%20Classification%20and%203D%20Clustering%20for%20Improved%0A%20%20Semantic%20Perception%20and%20Mapping%20in%20Mobile%20Robots&entry.906535625=Siva%20Krishna%20Ravipati%20and%20Ehsan%20Latif%20and%20Ramviyas%20Parasuraman%20and%20Suchendra%20M.%20Bhandarkar&entry.1292438233=%20%20Classification%20of%20different%20object%20surface%20material%20types%20can%20play%20a%0Asignificant%20role%20in%20the%20decision-making%20algorithms%20for%20mobile%20robots%20and%0Aautonomous%20vehicles.%20RGB-based%20scene-level%20semantic%20segmentation%20has%20been%0Awell-addressed%20in%20the%20literature.%20However%2C%20improving%20material%20recognition%20using%0Athe%20depth%20modality%20and%20its%20integration%20with%20SLAM%20algorithms%20for%203D%20semantic%0Amapping%20could%20unlock%20new%20potential%20benefits%20in%20the%20robotics%20perception%0Apipeline.%20To%20this%20end%2C%20we%20propose%20a%20complementarity-aware%20deep%20learning%0Aapproach%20for%20RGB-D-based%20material%20classification%20built%20on%20top%20of%20an%0Aobject-oriented%20pipeline.%20The%20approach%20further%20integrates%20the%20ORB-SLAM2%20method%0Afor%203D%20scene%20mapping%20with%20multiscale%20clustering%20of%20the%20detected%20material%0Asemantics%20in%20the%20point%20cloud%20map%20generated%20by%20the%20visual%20SLAM%20algorithm.%0AExtensive%20experimental%20results%20with%20existing%20public%20datasets%20and%20newly%0Acontributed%20real-world%20robot%20datasets%20demonstrate%20a%20significant%20improvement%20in%0Amaterial%20classification%20and%203D%20clustering%20accuracy%20compared%20to%20state-of-the-art%0Aapproaches%20for%203D%20semantic%20scene%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06077v1&entry.124074799=Read"},
{"title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models", "author": "Yibo Miao and Yifan Zhu and Yinpeng Dong and Lijia Yu and Jun Zhu and Xiao-Shan Gao", "abstract": "  The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset using LLMs and jailbreaking prompt attacks. Based on our evaluation\nresults, we draw several important findings, including: 1) no single model\nexcels in all aspects, with different models showing various strengths; 2) the\ncorrelation between GPT-4 assessments and manual reviews is generally high; 3)\nthere is a trade-off between the usability and safety of text-to-video\ngenerative models. This indicates that as the field of video generation rapidly\nadvances, safety risks are set to surge, highlighting the urgency of\nprioritizing video safety. We hope that T2VSafetyBench can provide insights for\nbetter understanding the safety of video generation in the era of generative\nAI.\n", "link": "http://arxiv.org/abs/2407.05965v1", "date": "2024-07-08", "relevancy": 2.3106, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5936}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5705}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2VSafetyBench%3A%20Evaluating%20the%20Safety%20of%20Text-to-Video%20Generative%20Models&body=Title%3A%20T2VSafetyBench%3A%20Evaluating%20the%20Safety%20of%20Text-to-Video%20Generative%20Models%0AAuthor%3A%20Yibo%20Miao%20and%20Yifan%20Zhu%20and%20Yinpeng%20Dong%20and%20Lijia%20Yu%20and%20Jun%20Zhu%20and%20Xiao-Shan%20Gao%0AAbstract%3A%20%20%20The%20recent%20development%20of%20Sora%20leads%20to%20a%20new%20era%20in%20text-to-video%20%28T2V%29%0Ageneration.%20Along%20with%20this%20comes%20the%20rising%20concern%20about%20its%20security%20risks.%0AThe%20generated%20videos%20may%20contain%20illegal%20or%20unethical%20content%2C%20and%20there%20is%20a%0Alack%20of%20comprehensive%20quantitative%20understanding%20of%20their%20safety%2C%20posing%20a%0Achallenge%20to%20their%20reliability%20and%20practical%20deployment.%20Previous%20evaluations%0Aprimarily%20focus%20on%20the%20quality%20of%20video%20generation.%20While%20some%20evaluations%20of%0Atext-to-image%20models%20have%20considered%20safety%2C%20they%20cover%20fewer%20aspects%20and%20do%0Anot%20address%20the%20unique%20temporal%20risk%20inherent%20in%20video%20generation.%20To%20bridge%0Athis%20research%20gap%2C%20we%20introduce%20T2VSafetyBench%2C%20a%20new%20benchmark%20designed%20for%0Aconducting%20safety-critical%20assessments%20of%20text-to-video%20models.%20We%20define%2012%0Acritical%20aspects%20of%20video%20generation%20safety%20and%20construct%20a%20malicious%20prompt%0Adataset%20using%20LLMs%20and%20jailbreaking%20prompt%20attacks.%20Based%20on%20our%20evaluation%0Aresults%2C%20we%20draw%20several%20important%20findings%2C%20including%3A%201%29%20no%20single%20model%0Aexcels%20in%20all%20aspects%2C%20with%20different%20models%20showing%20various%20strengths%3B%202%29%20the%0Acorrelation%20between%20GPT-4%20assessments%20and%20manual%20reviews%20is%20generally%20high%3B%203%29%0Athere%20is%20a%20trade-off%20between%20the%20usability%20and%20safety%20of%20text-to-video%0Agenerative%20models.%20This%20indicates%20that%20as%20the%20field%20of%20video%20generation%20rapidly%0Aadvances%2C%20safety%20risks%20are%20set%20to%20surge%2C%20highlighting%20the%20urgency%20of%0Aprioritizing%20video%20safety.%20We%20hope%20that%20T2VSafetyBench%20can%20provide%20insights%20for%0Abetter%20understanding%20the%20safety%20of%20video%20generation%20in%20the%20era%20of%20generative%0AAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2VSafetyBench%253A%2520Evaluating%2520the%2520Safety%2520of%2520Text-to-Video%2520Generative%2520Models%26entry.906535625%3DYibo%2520Miao%2520and%2520Yifan%2520Zhu%2520and%2520Yinpeng%2520Dong%2520and%2520Lijia%2520Yu%2520and%2520Jun%2520Zhu%2520and%2520Xiao-Shan%2520Gao%26entry.1292438233%3D%2520%2520The%2520recent%2520development%2520of%2520Sora%2520leads%2520to%2520a%2520new%2520era%2520in%2520text-to-video%2520%2528T2V%2529%250Ageneration.%2520Along%2520with%2520this%2520comes%2520the%2520rising%2520concern%2520about%2520its%2520security%2520risks.%250AThe%2520generated%2520videos%2520may%2520contain%2520illegal%2520or%2520unethical%2520content%252C%2520and%2520there%2520is%2520a%250Alack%2520of%2520comprehensive%2520quantitative%2520understanding%2520of%2520their%2520safety%252C%2520posing%2520a%250Achallenge%2520to%2520their%2520reliability%2520and%2520practical%2520deployment.%2520Previous%2520evaluations%250Aprimarily%2520focus%2520on%2520the%2520quality%2520of%2520video%2520generation.%2520While%2520some%2520evaluations%2520of%250Atext-to-image%2520models%2520have%2520considered%2520safety%252C%2520they%2520cover%2520fewer%2520aspects%2520and%2520do%250Anot%2520address%2520the%2520unique%2520temporal%2520risk%2520inherent%2520in%2520video%2520generation.%2520To%2520bridge%250Athis%2520research%2520gap%252C%2520we%2520introduce%2520T2VSafetyBench%252C%2520a%2520new%2520benchmark%2520designed%2520for%250Aconducting%2520safety-critical%2520assessments%2520of%2520text-to-video%2520models.%2520We%2520define%252012%250Acritical%2520aspects%2520of%2520video%2520generation%2520safety%2520and%2520construct%2520a%2520malicious%2520prompt%250Adataset%2520using%2520LLMs%2520and%2520jailbreaking%2520prompt%2520attacks.%2520Based%2520on%2520our%2520evaluation%250Aresults%252C%2520we%2520draw%2520several%2520important%2520findings%252C%2520including%253A%25201%2529%2520no%2520single%2520model%250Aexcels%2520in%2520all%2520aspects%252C%2520with%2520different%2520models%2520showing%2520various%2520strengths%253B%25202%2529%2520the%250Acorrelation%2520between%2520GPT-4%2520assessments%2520and%2520manual%2520reviews%2520is%2520generally%2520high%253B%25203%2529%250Athere%2520is%2520a%2520trade-off%2520between%2520the%2520usability%2520and%2520safety%2520of%2520text-to-video%250Agenerative%2520models.%2520This%2520indicates%2520that%2520as%2520the%2520field%2520of%2520video%2520generation%2520rapidly%250Aadvances%252C%2520safety%2520risks%2520are%2520set%2520to%2520surge%252C%2520highlighting%2520the%2520urgency%2520of%250Aprioritizing%2520video%2520safety.%2520We%2520hope%2520that%2520T2VSafetyBench%2520can%2520provide%2520insights%2520for%250Abetter%2520understanding%2520the%2520safety%2520of%2520video%2520generation%2520in%2520the%2520era%2520of%2520generative%250AAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2VSafetyBench%3A%20Evaluating%20the%20Safety%20of%20Text-to-Video%20Generative%20Models&entry.906535625=Yibo%20Miao%20and%20Yifan%20Zhu%20and%20Yinpeng%20Dong%20and%20Lijia%20Yu%20and%20Jun%20Zhu%20and%20Xiao-Shan%20Gao&entry.1292438233=%20%20The%20recent%20development%20of%20Sora%20leads%20to%20a%20new%20era%20in%20text-to-video%20%28T2V%29%0Ageneration.%20Along%20with%20this%20comes%20the%20rising%20concern%20about%20its%20security%20risks.%0AThe%20generated%20videos%20may%20contain%20illegal%20or%20unethical%20content%2C%20and%20there%20is%20a%0Alack%20of%20comprehensive%20quantitative%20understanding%20of%20their%20safety%2C%20posing%20a%0Achallenge%20to%20their%20reliability%20and%20practical%20deployment.%20Previous%20evaluations%0Aprimarily%20focus%20on%20the%20quality%20of%20video%20generation.%20While%20some%20evaluations%20of%0Atext-to-image%20models%20have%20considered%20safety%2C%20they%20cover%20fewer%20aspects%20and%20do%0Anot%20address%20the%20unique%20temporal%20risk%20inherent%20in%20video%20generation.%20To%20bridge%0Athis%20research%20gap%2C%20we%20introduce%20T2VSafetyBench%2C%20a%20new%20benchmark%20designed%20for%0Aconducting%20safety-critical%20assessments%20of%20text-to-video%20models.%20We%20define%2012%0Acritical%20aspects%20of%20video%20generation%20safety%20and%20construct%20a%20malicious%20prompt%0Adataset%20using%20LLMs%20and%20jailbreaking%20prompt%20attacks.%20Based%20on%20our%20evaluation%0Aresults%2C%20we%20draw%20several%20important%20findings%2C%20including%3A%201%29%20no%20single%20model%0Aexcels%20in%20all%20aspects%2C%20with%20different%20models%20showing%20various%20strengths%3B%202%29%20the%0Acorrelation%20between%20GPT-4%20assessments%20and%20manual%20reviews%20is%20generally%20high%3B%203%29%0Athere%20is%20a%20trade-off%20between%20the%20usability%20and%20safety%20of%20text-to-video%0Agenerative%20models.%20This%20indicates%20that%20as%20the%20field%20of%20video%20generation%20rapidly%0Aadvances%2C%20safety%20risks%20are%20set%20to%20surge%2C%20highlighting%20the%20urgency%20of%0Aprioritizing%20video%20safety.%20We%20hope%20that%20T2VSafetyBench%20can%20provide%20insights%20for%0Abetter%20understanding%20the%20safety%20of%20video%20generation%20in%20the%20era%20of%20generative%0AAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05965v1&entry.124074799=Read"},
{"title": "Co-RaL: Complementary Radar-Leg Odometry with 4-DoF Optimization and\n  Rolling Contact", "author": "Sangwoo Jung and Wooseong Yang and Ayoung Kim", "abstract": "  Robust and accurate localization in challenging environments is becoming\ncrucial for SLAM. In this paper, we propose a unique sensor configuration for\nprecise and robust odometry by integrating chip radar and a legged robot.\nSpecifically, we introduce a tightly coupled radar-leg odometry algorithm for\ncomplementary drift correction. Adopting the 4-DoF optimization and decoupled\nRANSAC to mmWave chip radar significantly enhances radar odometry beyond the\nexisting method, especially z-directional even when using a single radar. For\nthe leg odometry, we employ rolling contact modeling-aided forward kinematics,\naccommodating scenarios with the potential possibility of contact drift and\nradar failure. We evaluate our method by comparing it with other chip radar\nodometry algorithms using real-world datasets with diverse environments while\nthe datasets will be released for the robotics community.\nhttps://github.com/SangwooJung98/Co-RaL-Dataset\n", "link": "http://arxiv.org/abs/2407.05820v1", "date": "2024-07-08", "relevancy": 2.2984, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5779}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5775}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-RaL%3A%20Complementary%20Radar-Leg%20Odometry%20with%204-DoF%20Optimization%20and%0A%20%20Rolling%20Contact&body=Title%3A%20Co-RaL%3A%20Complementary%20Radar-Leg%20Odometry%20with%204-DoF%20Optimization%20and%0A%20%20Rolling%20Contact%0AAuthor%3A%20Sangwoo%20Jung%20and%20Wooseong%20Yang%20and%20Ayoung%20Kim%0AAbstract%3A%20%20%20Robust%20and%20accurate%20localization%20in%20challenging%20environments%20is%20becoming%0Acrucial%20for%20SLAM.%20In%20this%20paper%2C%20we%20propose%20a%20unique%20sensor%20configuration%20for%0Aprecise%20and%20robust%20odometry%20by%20integrating%20chip%20radar%20and%20a%20legged%20robot.%0ASpecifically%2C%20we%20introduce%20a%20tightly%20coupled%20radar-leg%20odometry%20algorithm%20for%0Acomplementary%20drift%20correction.%20Adopting%20the%204-DoF%20optimization%20and%20decoupled%0ARANSAC%20to%20mmWave%20chip%20radar%20significantly%20enhances%20radar%20odometry%20beyond%20the%0Aexisting%20method%2C%20especially%20z-directional%20even%20when%20using%20a%20single%20radar.%20For%0Athe%20leg%20odometry%2C%20we%20employ%20rolling%20contact%20modeling-aided%20forward%20kinematics%2C%0Aaccommodating%20scenarios%20with%20the%20potential%20possibility%20of%20contact%20drift%20and%0Aradar%20failure.%20We%20evaluate%20our%20method%20by%20comparing%20it%20with%20other%20chip%20radar%0Aodometry%20algorithms%20using%20real-world%20datasets%20with%20diverse%20environments%20while%0Athe%20datasets%20will%20be%20released%20for%20the%20robotics%20community.%0Ahttps%3A//github.com/SangwooJung98/Co-RaL-Dataset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-RaL%253A%2520Complementary%2520Radar-Leg%2520Odometry%2520with%25204-DoF%2520Optimization%2520and%250A%2520%2520Rolling%2520Contact%26entry.906535625%3DSangwoo%2520Jung%2520and%2520Wooseong%2520Yang%2520and%2520Ayoung%2520Kim%26entry.1292438233%3D%2520%2520Robust%2520and%2520accurate%2520localization%2520in%2520challenging%2520environments%2520is%2520becoming%250Acrucial%2520for%2520SLAM.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520unique%2520sensor%2520configuration%2520for%250Aprecise%2520and%2520robust%2520odometry%2520by%2520integrating%2520chip%2520radar%2520and%2520a%2520legged%2520robot.%250ASpecifically%252C%2520we%2520introduce%2520a%2520tightly%2520coupled%2520radar-leg%2520odometry%2520algorithm%2520for%250Acomplementary%2520drift%2520correction.%2520Adopting%2520the%25204-DoF%2520optimization%2520and%2520decoupled%250ARANSAC%2520to%2520mmWave%2520chip%2520radar%2520significantly%2520enhances%2520radar%2520odometry%2520beyond%2520the%250Aexisting%2520method%252C%2520especially%2520z-directional%2520even%2520when%2520using%2520a%2520single%2520radar.%2520For%250Athe%2520leg%2520odometry%252C%2520we%2520employ%2520rolling%2520contact%2520modeling-aided%2520forward%2520kinematics%252C%250Aaccommodating%2520scenarios%2520with%2520the%2520potential%2520possibility%2520of%2520contact%2520drift%2520and%250Aradar%2520failure.%2520We%2520evaluate%2520our%2520method%2520by%2520comparing%2520it%2520with%2520other%2520chip%2520radar%250Aodometry%2520algorithms%2520using%2520real-world%2520datasets%2520with%2520diverse%2520environments%2520while%250Athe%2520datasets%2520will%2520be%2520released%2520for%2520the%2520robotics%2520community.%250Ahttps%253A//github.com/SangwooJung98/Co-RaL-Dataset%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-RaL%3A%20Complementary%20Radar-Leg%20Odometry%20with%204-DoF%20Optimization%20and%0A%20%20Rolling%20Contact&entry.906535625=Sangwoo%20Jung%20and%20Wooseong%20Yang%20and%20Ayoung%20Kim&entry.1292438233=%20%20Robust%20and%20accurate%20localization%20in%20challenging%20environments%20is%20becoming%0Acrucial%20for%20SLAM.%20In%20this%20paper%2C%20we%20propose%20a%20unique%20sensor%20configuration%20for%0Aprecise%20and%20robust%20odometry%20by%20integrating%20chip%20radar%20and%20a%20legged%20robot.%0ASpecifically%2C%20we%20introduce%20a%20tightly%20coupled%20radar-leg%20odometry%20algorithm%20for%0Acomplementary%20drift%20correction.%20Adopting%20the%204-DoF%20optimization%20and%20decoupled%0ARANSAC%20to%20mmWave%20chip%20radar%20significantly%20enhances%20radar%20odometry%20beyond%20the%0Aexisting%20method%2C%20especially%20z-directional%20even%20when%20using%20a%20single%20radar.%20For%0Athe%20leg%20odometry%2C%20we%20employ%20rolling%20contact%20modeling-aided%20forward%20kinematics%2C%0Aaccommodating%20scenarios%20with%20the%20potential%20possibility%20of%20contact%20drift%20and%0Aradar%20failure.%20We%20evaluate%20our%20method%20by%20comparing%20it%20with%20other%20chip%20radar%0Aodometry%20algorithms%20using%20real-world%20datasets%20with%20diverse%20environments%20while%0Athe%20datasets%20will%20be%20released%20for%20the%20robotics%20community.%0Ahttps%3A//github.com/SangwooJung98/Co-RaL-Dataset%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05820v1&entry.124074799=Read"},
{"title": "Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping", "author": "Alex Costanzino and Pierluigi Zama Ramirez and Giuseppe Lisanti and Luigi Di Stefano", "abstract": "  The paper explores the industrial multimodal Anomaly Detection (AD) task,\nwhich exploits point clouds and RGB images to localize anomalies. We introduce\na novel light and fast framework that learns to map features from one modality\nto the other on nominal samples. At test time, anomalies are detected by\npinpointing inconsistencies between observed and mapped features. Extensive\nexperiments show that our approach achieves state-of-the-art detection and\nsegmentation performance in both the standard and few-shot settings on the\nMVTec 3D-AD dataset while achieving faster inference and occupying less memory\nthan previous multimodal AD methods. Moreover, we propose a layer-pruning\ntechnique to improve memory and time efficiency with a marginal sacrifice in\nperformance.\n", "link": "http://arxiv.org/abs/2312.04521v2", "date": "2024-07-08", "relevancy": 2.2977, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5795}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5736}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Industrial%20Anomaly%20Detection%20by%20Crossmodal%20Feature%20Mapping&body=Title%3A%20Multimodal%20Industrial%20Anomaly%20Detection%20by%20Crossmodal%20Feature%20Mapping%0AAuthor%3A%20Alex%20Costanzino%20and%20Pierluigi%20Zama%20Ramirez%20and%20Giuseppe%20Lisanti%20and%20Luigi%20Di%20Stefano%0AAbstract%3A%20%20%20The%20paper%20explores%20the%20industrial%20multimodal%20Anomaly%20Detection%20%28AD%29%20task%2C%0Awhich%20exploits%20point%20clouds%20and%20RGB%20images%20to%20localize%20anomalies.%20We%20introduce%0Aa%20novel%20light%20and%20fast%20framework%20that%20learns%20to%20map%20features%20from%20one%20modality%0Ato%20the%20other%20on%20nominal%20samples.%20At%20test%20time%2C%20anomalies%20are%20detected%20by%0Apinpointing%20inconsistencies%20between%20observed%20and%20mapped%20features.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20achieves%20state-of-the-art%20detection%20and%0Asegmentation%20performance%20in%20both%20the%20standard%20and%20few-shot%20settings%20on%20the%0AMVTec%203D-AD%20dataset%20while%20achieving%20faster%20inference%20and%20occupying%20less%20memory%0Athan%20previous%20multimodal%20AD%20methods.%20Moreover%2C%20we%20propose%20a%20layer-pruning%0Atechnique%20to%20improve%20memory%20and%20time%20efficiency%20with%20a%20marginal%20sacrifice%20in%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Industrial%2520Anomaly%2520Detection%2520by%2520Crossmodal%2520Feature%2520Mapping%26entry.906535625%3DAlex%2520Costanzino%2520and%2520Pierluigi%2520Zama%2520Ramirez%2520and%2520Giuseppe%2520Lisanti%2520and%2520Luigi%2520Di%2520Stefano%26entry.1292438233%3D%2520%2520The%2520paper%2520explores%2520the%2520industrial%2520multimodal%2520Anomaly%2520Detection%2520%2528AD%2529%2520task%252C%250Awhich%2520exploits%2520point%2520clouds%2520and%2520RGB%2520images%2520to%2520localize%2520anomalies.%2520We%2520introduce%250Aa%2520novel%2520light%2520and%2520fast%2520framework%2520that%2520learns%2520to%2520map%2520features%2520from%2520one%2520modality%250Ato%2520the%2520other%2520on%2520nominal%2520samples.%2520At%2520test%2520time%252C%2520anomalies%2520are%2520detected%2520by%250Apinpointing%2520inconsistencies%2520between%2520observed%2520and%2520mapped%2520features.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520detection%2520and%250Asegmentation%2520performance%2520in%2520both%2520the%2520standard%2520and%2520few-shot%2520settings%2520on%2520the%250AMVTec%25203D-AD%2520dataset%2520while%2520achieving%2520faster%2520inference%2520and%2520occupying%2520less%2520memory%250Athan%2520previous%2520multimodal%2520AD%2520methods.%2520Moreover%252C%2520we%2520propose%2520a%2520layer-pruning%250Atechnique%2520to%2520improve%2520memory%2520and%2520time%2520efficiency%2520with%2520a%2520marginal%2520sacrifice%2520in%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Industrial%20Anomaly%20Detection%20by%20Crossmodal%20Feature%20Mapping&entry.906535625=Alex%20Costanzino%20and%20Pierluigi%20Zama%20Ramirez%20and%20Giuseppe%20Lisanti%20and%20Luigi%20Di%20Stefano&entry.1292438233=%20%20The%20paper%20explores%20the%20industrial%20multimodal%20Anomaly%20Detection%20%28AD%29%20task%2C%0Awhich%20exploits%20point%20clouds%20and%20RGB%20images%20to%20localize%20anomalies.%20We%20introduce%0Aa%20novel%20light%20and%20fast%20framework%20that%20learns%20to%20map%20features%20from%20one%20modality%0Ato%20the%20other%20on%20nominal%20samples.%20At%20test%20time%2C%20anomalies%20are%20detected%20by%0Apinpointing%20inconsistencies%20between%20observed%20and%20mapped%20features.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20achieves%20state-of-the-art%20detection%20and%0Asegmentation%20performance%20in%20both%20the%20standard%20and%20few-shot%20settings%20on%20the%0AMVTec%203D-AD%20dataset%20while%20achieving%20faster%20inference%20and%20occupying%20less%20memory%0Athan%20previous%20multimodal%20AD%20methods.%20Moreover%2C%20we%20propose%20a%20layer-pruning%0Atechnique%20to%20improve%20memory%20and%20time%20efficiency%20with%20a%20marginal%20sacrifice%20in%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04521v2&entry.124074799=Read"},
{"title": "Fault Detection for agents on power grid topology optimization: A\n  Comprehensive analysis", "author": "Malte Lehna and Mohamed Hassouna and Dmitry Degtyar and Sven Tomforde and Christoph Scholz", "abstract": "  The topology optimization of transmission networks using Deep Reinforcement\nLearning (DRL) has increasingly come into focus. Various researchers have\nproposed different DRL agents, which are often benchmarked on the Grid2Op\nenvironment from the Learning to Run a Power Network (L2RPN) challenges. The\nenvironments have many advantages with their realistic chronics and underlying\npower flow backends. However, the interpretation of agent survival or failure\nis not always clear, as there are a variety of potential causes. In this work,\nwe focus on the failures of the power grid to identify patterns and detect them\na priori. We collect the failed chronics of three different agents on the WCCI\n2022 L2RPN environment, totaling about 40k data points. By clustering, we are\nable to detect five distinct clusters, identifying different failure types.\nFurther, we propose a multi-class prediction approach to detect failures\nbeforehand and evaluate five different models. Here, the Light\nGradient-Boosting Machine (LightGBM) shows the best performance, with an\naccuracy of 86%. It also correctly identifies in 91% of the time failure and\nsurvival observations. Finally, we provide a detailed feature importance\nanalysis that identifies critical features and regions in the grid.\n", "link": "http://arxiv.org/abs/2406.16426v2", "date": "2024-07-08", "relevancy": 2.2847, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4835}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4458}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis&body=Title%3A%20Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis%0AAuthor%3A%20Malte%20Lehna%20and%20Mohamed%20Hassouna%20and%20Dmitry%20Degtyar%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20The%20topology%20optimization%20of%20transmission%20networks%20using%20Deep%20Reinforcement%0ALearning%20%28DRL%29%20has%20increasingly%20come%20into%20focus.%20Various%20researchers%20have%0Aproposed%20different%20DRL%20agents%2C%20which%20are%20often%20benchmarked%20on%20the%20Grid2Op%0Aenvironment%20from%20the%20Learning%20to%20Run%20a%20Power%20Network%20%28L2RPN%29%20challenges.%20The%0Aenvironments%20have%20many%20advantages%20with%20their%20realistic%20chronics%20and%20underlying%0Apower%20flow%20backends.%20However%2C%20the%20interpretation%20of%20agent%20survival%20or%20failure%0Ais%20not%20always%20clear%2C%20as%20there%20are%20a%20variety%20of%20potential%20causes.%20In%20this%20work%2C%0Awe%20focus%20on%20the%20failures%20of%20the%20power%20grid%20to%20identify%20patterns%20and%20detect%20them%0Aa%20priori.%20We%20collect%20the%20failed%20chronics%20of%20three%20different%20agents%20on%20the%20WCCI%0A2022%20L2RPN%20environment%2C%20totaling%20about%2040k%20data%20points.%20By%20clustering%2C%20we%20are%0Aable%20to%20detect%20five%20distinct%20clusters%2C%20identifying%20different%20failure%20types.%0AFurther%2C%20we%20propose%20a%20multi-class%20prediction%20approach%20to%20detect%20failures%0Abeforehand%20and%20evaluate%20five%20different%20models.%20Here%2C%20the%20Light%0AGradient-Boosting%20Machine%20%28LightGBM%29%20shows%20the%20best%20performance%2C%20with%20an%0Aaccuracy%20of%2086%25.%20It%20also%20correctly%20identifies%20in%2091%25%20of%20the%20time%20failure%20and%0Asurvival%20observations.%20Finally%2C%20we%20provide%20a%20detailed%20feature%20importance%0Aanalysis%20that%20identifies%20critical%20features%20and%20regions%20in%20the%20grid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFault%2520Detection%2520for%2520agents%2520on%2520power%2520grid%2520topology%2520optimization%253A%2520A%250A%2520%2520Comprehensive%2520analysis%26entry.906535625%3DMalte%2520Lehna%2520and%2520Mohamed%2520Hassouna%2520and%2520Dmitry%2520Degtyar%2520and%2520Sven%2520Tomforde%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520The%2520topology%2520optimization%2520of%2520transmission%2520networks%2520using%2520Deep%2520Reinforcement%250ALearning%2520%2528DRL%2529%2520has%2520increasingly%2520come%2520into%2520focus.%2520Various%2520researchers%2520have%250Aproposed%2520different%2520DRL%2520agents%252C%2520which%2520are%2520often%2520benchmarked%2520on%2520the%2520Grid2Op%250Aenvironment%2520from%2520the%2520Learning%2520to%2520Run%2520a%2520Power%2520Network%2520%2528L2RPN%2529%2520challenges.%2520The%250Aenvironments%2520have%2520many%2520advantages%2520with%2520their%2520realistic%2520chronics%2520and%2520underlying%250Apower%2520flow%2520backends.%2520However%252C%2520the%2520interpretation%2520of%2520agent%2520survival%2520or%2520failure%250Ais%2520not%2520always%2520clear%252C%2520as%2520there%2520are%2520a%2520variety%2520of%2520potential%2520causes.%2520In%2520this%2520work%252C%250Awe%2520focus%2520on%2520the%2520failures%2520of%2520the%2520power%2520grid%2520to%2520identify%2520patterns%2520and%2520detect%2520them%250Aa%2520priori.%2520We%2520collect%2520the%2520failed%2520chronics%2520of%2520three%2520different%2520agents%2520on%2520the%2520WCCI%250A2022%2520L2RPN%2520environment%252C%2520totaling%2520about%252040k%2520data%2520points.%2520By%2520clustering%252C%2520we%2520are%250Aable%2520to%2520detect%2520five%2520distinct%2520clusters%252C%2520identifying%2520different%2520failure%2520types.%250AFurther%252C%2520we%2520propose%2520a%2520multi-class%2520prediction%2520approach%2520to%2520detect%2520failures%250Abeforehand%2520and%2520evaluate%2520five%2520different%2520models.%2520Here%252C%2520the%2520Light%250AGradient-Boosting%2520Machine%2520%2528LightGBM%2529%2520shows%2520the%2520best%2520performance%252C%2520with%2520an%250Aaccuracy%2520of%252086%2525.%2520It%2520also%2520correctly%2520identifies%2520in%252091%2525%2520of%2520the%2520time%2520failure%2520and%250Asurvival%2520observations.%2520Finally%252C%2520we%2520provide%2520a%2520detailed%2520feature%2520importance%250Aanalysis%2520that%2520identifies%2520critical%2520features%2520and%2520regions%2520in%2520the%2520grid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis&entry.906535625=Malte%20Lehna%20and%20Mohamed%20Hassouna%20and%20Dmitry%20Degtyar%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz&entry.1292438233=%20%20The%20topology%20optimization%20of%20transmission%20networks%20using%20Deep%20Reinforcement%0ALearning%20%28DRL%29%20has%20increasingly%20come%20into%20focus.%20Various%20researchers%20have%0Aproposed%20different%20DRL%20agents%2C%20which%20are%20often%20benchmarked%20on%20the%20Grid2Op%0Aenvironment%20from%20the%20Learning%20to%20Run%20a%20Power%20Network%20%28L2RPN%29%20challenges.%20The%0Aenvironments%20have%20many%20advantages%20with%20their%20realistic%20chronics%20and%20underlying%0Apower%20flow%20backends.%20However%2C%20the%20interpretation%20of%20agent%20survival%20or%20failure%0Ais%20not%20always%20clear%2C%20as%20there%20are%20a%20variety%20of%20potential%20causes.%20In%20this%20work%2C%0Awe%20focus%20on%20the%20failures%20of%20the%20power%20grid%20to%20identify%20patterns%20and%20detect%20them%0Aa%20priori.%20We%20collect%20the%20failed%20chronics%20of%20three%20different%20agents%20on%20the%20WCCI%0A2022%20L2RPN%20environment%2C%20totaling%20about%2040k%20data%20points.%20By%20clustering%2C%20we%20are%0Aable%20to%20detect%20five%20distinct%20clusters%2C%20identifying%20different%20failure%20types.%0AFurther%2C%20we%20propose%20a%20multi-class%20prediction%20approach%20to%20detect%20failures%0Abeforehand%20and%20evaluate%20five%20different%20models.%20Here%2C%20the%20Light%0AGradient-Boosting%20Machine%20%28LightGBM%29%20shows%20the%20best%20performance%2C%20with%20an%0Aaccuracy%20of%2086%25.%20It%20also%20correctly%20identifies%20in%2091%25%20of%20the%20time%20failure%20and%0Asurvival%20observations.%20Finally%2C%20we%20provide%20a%20detailed%20feature%20importance%0Aanalysis%20that%20identifies%20critical%20features%20and%20regions%20in%20the%20grid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16426v2&entry.124074799=Read"},
{"title": "Leveraging Representations from Intermediate Encoder-blocks for\n  Synthetic Image Detection", "author": "Christos Koutlis and Symeon Papadopoulos", "abstract": "  The recently developed and publicly available synthetic image generation\nmethods and services make it possible to create extremely realistic imagery on\ndemand, raising great risks for the integrity and safety of online information.\nState-of-the-art Synthetic Image Detection (SID) research has led to strong\nevidence on the advantages of feature extraction from foundation models.\nHowever, such extracted features mostly encapsulate high-level visual semantics\ninstead of fine-grained details, which are more important for the SID task. On\nthe contrary, shallow layers encode low-level visual information. In this work,\nwe leverage the image representations extracted by intermediate Transformer\nblocks of CLIP's image-encoder via a lightweight network that maps them to a\nlearnable forgery-aware vector space capable of generalizing exceptionally\nwell. We also employ a trainable module to incorporate the importance of each\nTransformer block to the final prediction. Our method is compared against the\nstate-of-the-art by evaluating it on 20 test datasets and exhibits an average\n+10.6% absolute performance improvement. Notably, the best performing models\nrequire just a single epoch for training (~8 minutes). Code available at\nhttps://github.com/mever-team/rine.\n", "link": "http://arxiv.org/abs/2402.19091v2", "date": "2024-07-08", "relevancy": 2.2838, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5831}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5751}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Representations%20from%20Intermediate%20Encoder-blocks%20for%0A%20%20Synthetic%20Image%20Detection&body=Title%3A%20Leveraging%20Representations%20from%20Intermediate%20Encoder-blocks%20for%0A%20%20Synthetic%20Image%20Detection%0AAuthor%3A%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos%0AAbstract%3A%20%20%20The%20recently%20developed%20and%20publicly%20available%20synthetic%20image%20generation%0Amethods%20and%20services%20make%20it%20possible%20to%20create%20extremely%20realistic%20imagery%20on%0Ademand%2C%20raising%20great%20risks%20for%20the%20integrity%20and%20safety%20of%20online%20information.%0AState-of-the-art%20Synthetic%20Image%20Detection%20%28SID%29%20research%20has%20led%20to%20strong%0Aevidence%20on%20the%20advantages%20of%20feature%20extraction%20from%20foundation%20models.%0AHowever%2C%20such%20extracted%20features%20mostly%20encapsulate%20high-level%20visual%20semantics%0Ainstead%20of%20fine-grained%20details%2C%20which%20are%20more%20important%20for%20the%20SID%20task.%20On%0Athe%20contrary%2C%20shallow%20layers%20encode%20low-level%20visual%20information.%20In%20this%20work%2C%0Awe%20leverage%20the%20image%20representations%20extracted%20by%20intermediate%20Transformer%0Ablocks%20of%20CLIP%27s%20image-encoder%20via%20a%20lightweight%20network%20that%20maps%20them%20to%20a%0Alearnable%20forgery-aware%20vector%20space%20capable%20of%20generalizing%20exceptionally%0Awell.%20We%20also%20employ%20a%20trainable%20module%20to%20incorporate%20the%20importance%20of%20each%0ATransformer%20block%20to%20the%20final%20prediction.%20Our%20method%20is%20compared%20against%20the%0Astate-of-the-art%20by%20evaluating%20it%20on%2020%20test%20datasets%20and%20exhibits%20an%20average%0A%2B10.6%25%20absolute%20performance%20improvement.%20Notably%2C%20the%20best%20performing%20models%0Arequire%20just%20a%20single%20epoch%20for%20training%20%28~8%20minutes%29.%20Code%20available%20at%0Ahttps%3A//github.com/mever-team/rine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19091v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Representations%2520from%2520Intermediate%2520Encoder-blocks%2520for%250A%2520%2520Synthetic%2520Image%2520Detection%26entry.906535625%3DChristos%2520Koutlis%2520and%2520Symeon%2520Papadopoulos%26entry.1292438233%3D%2520%2520The%2520recently%2520developed%2520and%2520publicly%2520available%2520synthetic%2520image%2520generation%250Amethods%2520and%2520services%2520make%2520it%2520possible%2520to%2520create%2520extremely%2520realistic%2520imagery%2520on%250Ademand%252C%2520raising%2520great%2520risks%2520for%2520the%2520integrity%2520and%2520safety%2520of%2520online%2520information.%250AState-of-the-art%2520Synthetic%2520Image%2520Detection%2520%2528SID%2529%2520research%2520has%2520led%2520to%2520strong%250Aevidence%2520on%2520the%2520advantages%2520of%2520feature%2520extraction%2520from%2520foundation%2520models.%250AHowever%252C%2520such%2520extracted%2520features%2520mostly%2520encapsulate%2520high-level%2520visual%2520semantics%250Ainstead%2520of%2520fine-grained%2520details%252C%2520which%2520are%2520more%2520important%2520for%2520the%2520SID%2520task.%2520On%250Athe%2520contrary%252C%2520shallow%2520layers%2520encode%2520low-level%2520visual%2520information.%2520In%2520this%2520work%252C%250Awe%2520leverage%2520the%2520image%2520representations%2520extracted%2520by%2520intermediate%2520Transformer%250Ablocks%2520of%2520CLIP%2527s%2520image-encoder%2520via%2520a%2520lightweight%2520network%2520that%2520maps%2520them%2520to%2520a%250Alearnable%2520forgery-aware%2520vector%2520space%2520capable%2520of%2520generalizing%2520exceptionally%250Awell.%2520We%2520also%2520employ%2520a%2520trainable%2520module%2520to%2520incorporate%2520the%2520importance%2520of%2520each%250ATransformer%2520block%2520to%2520the%2520final%2520prediction.%2520Our%2520method%2520is%2520compared%2520against%2520the%250Astate-of-the-art%2520by%2520evaluating%2520it%2520on%252020%2520test%2520datasets%2520and%2520exhibits%2520an%2520average%250A%252B10.6%2525%2520absolute%2520performance%2520improvement.%2520Notably%252C%2520the%2520best%2520performing%2520models%250Arequire%2520just%2520a%2520single%2520epoch%2520for%2520training%2520%2528~8%2520minutes%2529.%2520Code%2520available%2520at%250Ahttps%253A//github.com/mever-team/rine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19091v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Representations%20from%20Intermediate%20Encoder-blocks%20for%0A%20%20Synthetic%20Image%20Detection&entry.906535625=Christos%20Koutlis%20and%20Symeon%20Papadopoulos&entry.1292438233=%20%20The%20recently%20developed%20and%20publicly%20available%20synthetic%20image%20generation%0Amethods%20and%20services%20make%20it%20possible%20to%20create%20extremely%20realistic%20imagery%20on%0Ademand%2C%20raising%20great%20risks%20for%20the%20integrity%20and%20safety%20of%20online%20information.%0AState-of-the-art%20Synthetic%20Image%20Detection%20%28SID%29%20research%20has%20led%20to%20strong%0Aevidence%20on%20the%20advantages%20of%20feature%20extraction%20from%20foundation%20models.%0AHowever%2C%20such%20extracted%20features%20mostly%20encapsulate%20high-level%20visual%20semantics%0Ainstead%20of%20fine-grained%20details%2C%20which%20are%20more%20important%20for%20the%20SID%20task.%20On%0Athe%20contrary%2C%20shallow%20layers%20encode%20low-level%20visual%20information.%20In%20this%20work%2C%0Awe%20leverage%20the%20image%20representations%20extracted%20by%20intermediate%20Transformer%0Ablocks%20of%20CLIP%27s%20image-encoder%20via%20a%20lightweight%20network%20that%20maps%20them%20to%20a%0Alearnable%20forgery-aware%20vector%20space%20capable%20of%20generalizing%20exceptionally%0Awell.%20We%20also%20employ%20a%20trainable%20module%20to%20incorporate%20the%20importance%20of%20each%0ATransformer%20block%20to%20the%20final%20prediction.%20Our%20method%20is%20compared%20against%20the%0Astate-of-the-art%20by%20evaluating%20it%20on%2020%20test%20datasets%20and%20exhibits%20an%20average%0A%2B10.6%25%20absolute%20performance%20improvement.%20Notably%2C%20the%20best%20performing%20models%0Arequire%20just%20a%20single%20epoch%20for%20training%20%28~8%20minutes%29.%20Code%20available%20at%0Ahttps%3A//github.com/mever-team/rine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19091v2&entry.124074799=Read"},
{"title": "Efficiently Training Neural Networks for Imperfect Information Games by\n  Sampling Information Sets", "author": "Timo Bertram and Johannes F\u00fcrnkranz and Martin M\u00fcller", "abstract": "  In imperfect information games, the evaluation of a game state not only\ndepends on the observable world but also relies on hidden parts of the\nenvironment. As accessing the obstructed information trivialises state\nevaluations, one approach to tackle such problems is to estimate the value of\nthe imperfect state as a combination of all states in the information set,\ni.e., all possible states that are consistent with the current imperfect\ninformation. In this work, the goal is to learn a function that maps from the\nimperfect game information state to its expected value. However, constructing a\nperfect training set, i.e. an enumeration of the whole information set for\nnumerous imperfect states, is often infeasible. To compute the expected values\nfor an imperfect information game like \\textit{Reconnaissance Blind Chess}, one\nwould need to evaluate thousands of chess positions just to obtain the training\ntarget for a single state. Still, the expected value of a state can already be\napproximated with appropriate accuracy from a much smaller set of evaluations.\nThus, in this paper, we empirically investigate how a budget of perfect\ninformation game evaluations should be distributed among training samples to\nmaximise the return. Our results show that sampling a small number of states,\nin our experiments roughly 3, for a larger number of separate positions is\npreferable over repeatedly sampling a smaller quantity of states. Thus, we find\nthat in our case, the quantity of different samples seems to be more important\nthan higher target quality.\n", "link": "http://arxiv.org/abs/2407.05876v1", "date": "2024-07-08", "relevancy": 2.2736, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4633}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4547}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Training%20Neural%20Networks%20for%20Imperfect%20Information%20Games%20by%0A%20%20Sampling%20Information%20Sets&body=Title%3A%20Efficiently%20Training%20Neural%20Networks%20for%20Imperfect%20Information%20Games%20by%0A%20%20Sampling%20Information%20Sets%0AAuthor%3A%20Timo%20Bertram%20and%20Johannes%20F%C3%BCrnkranz%20and%20Martin%20M%C3%BCller%0AAbstract%3A%20%20%20In%20imperfect%20information%20games%2C%20the%20evaluation%20of%20a%20game%20state%20not%20only%0Adepends%20on%20the%20observable%20world%20but%20also%20relies%20on%20hidden%20parts%20of%20the%0Aenvironment.%20As%20accessing%20the%20obstructed%20information%20trivialises%20state%0Aevaluations%2C%20one%20approach%20to%20tackle%20such%20problems%20is%20to%20estimate%20the%20value%20of%0Athe%20imperfect%20state%20as%20a%20combination%20of%20all%20states%20in%20the%20information%20set%2C%0Ai.e.%2C%20all%20possible%20states%20that%20are%20consistent%20with%20the%20current%20imperfect%0Ainformation.%20In%20this%20work%2C%20the%20goal%20is%20to%20learn%20a%20function%20that%20maps%20from%20the%0Aimperfect%20game%20information%20state%20to%20its%20expected%20value.%20However%2C%20constructing%20a%0Aperfect%20training%20set%2C%20i.e.%20an%20enumeration%20of%20the%20whole%20information%20set%20for%0Anumerous%20imperfect%20states%2C%20is%20often%20infeasible.%20To%20compute%20the%20expected%20values%0Afor%20an%20imperfect%20information%20game%20like%20%5Ctextit%7BReconnaissance%20Blind%20Chess%7D%2C%20one%0Awould%20need%20to%20evaluate%20thousands%20of%20chess%20positions%20just%20to%20obtain%20the%20training%0Atarget%20for%20a%20single%20state.%20Still%2C%20the%20expected%20value%20of%20a%20state%20can%20already%20be%0Aapproximated%20with%20appropriate%20accuracy%20from%20a%20much%20smaller%20set%20of%20evaluations.%0AThus%2C%20in%20this%20paper%2C%20we%20empirically%20investigate%20how%20a%20budget%20of%20perfect%0Ainformation%20game%20evaluations%20should%20be%20distributed%20among%20training%20samples%20to%0Amaximise%20the%20return.%20Our%20results%20show%20that%20sampling%20a%20small%20number%20of%20states%2C%0Ain%20our%20experiments%20roughly%203%2C%20for%20a%20larger%20number%20of%20separate%20positions%20is%0Apreferable%20over%20repeatedly%20sampling%20a%20smaller%20quantity%20of%20states.%20Thus%2C%20we%20find%0Athat%20in%20our%20case%2C%20the%20quantity%20of%20different%20samples%20seems%20to%20be%20more%20important%0Athan%20higher%20target%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Training%2520Neural%2520Networks%2520for%2520Imperfect%2520Information%2520Games%2520by%250A%2520%2520Sampling%2520Information%2520Sets%26entry.906535625%3DTimo%2520Bertram%2520and%2520Johannes%2520F%25C3%25BCrnkranz%2520and%2520Martin%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520In%2520imperfect%2520information%2520games%252C%2520the%2520evaluation%2520of%2520a%2520game%2520state%2520not%2520only%250Adepends%2520on%2520the%2520observable%2520world%2520but%2520also%2520relies%2520on%2520hidden%2520parts%2520of%2520the%250Aenvironment.%2520As%2520accessing%2520the%2520obstructed%2520information%2520trivialises%2520state%250Aevaluations%252C%2520one%2520approach%2520to%2520tackle%2520such%2520problems%2520is%2520to%2520estimate%2520the%2520value%2520of%250Athe%2520imperfect%2520state%2520as%2520a%2520combination%2520of%2520all%2520states%2520in%2520the%2520information%2520set%252C%250Ai.e.%252C%2520all%2520possible%2520states%2520that%2520are%2520consistent%2520with%2520the%2520current%2520imperfect%250Ainformation.%2520In%2520this%2520work%252C%2520the%2520goal%2520is%2520to%2520learn%2520a%2520function%2520that%2520maps%2520from%2520the%250Aimperfect%2520game%2520information%2520state%2520to%2520its%2520expected%2520value.%2520However%252C%2520constructing%2520a%250Aperfect%2520training%2520set%252C%2520i.e.%2520an%2520enumeration%2520of%2520the%2520whole%2520information%2520set%2520for%250Anumerous%2520imperfect%2520states%252C%2520is%2520often%2520infeasible.%2520To%2520compute%2520the%2520expected%2520values%250Afor%2520an%2520imperfect%2520information%2520game%2520like%2520%255Ctextit%257BReconnaissance%2520Blind%2520Chess%257D%252C%2520one%250Awould%2520need%2520to%2520evaluate%2520thousands%2520of%2520chess%2520positions%2520just%2520to%2520obtain%2520the%2520training%250Atarget%2520for%2520a%2520single%2520state.%2520Still%252C%2520the%2520expected%2520value%2520of%2520a%2520state%2520can%2520already%2520be%250Aapproximated%2520with%2520appropriate%2520accuracy%2520from%2520a%2520much%2520smaller%2520set%2520of%2520evaluations.%250AThus%252C%2520in%2520this%2520paper%252C%2520we%2520empirically%2520investigate%2520how%2520a%2520budget%2520of%2520perfect%250Ainformation%2520game%2520evaluations%2520should%2520be%2520distributed%2520among%2520training%2520samples%2520to%250Amaximise%2520the%2520return.%2520Our%2520results%2520show%2520that%2520sampling%2520a%2520small%2520number%2520of%2520states%252C%250Ain%2520our%2520experiments%2520roughly%25203%252C%2520for%2520a%2520larger%2520number%2520of%2520separate%2520positions%2520is%250Apreferable%2520over%2520repeatedly%2520sampling%2520a%2520smaller%2520quantity%2520of%2520states.%2520Thus%252C%2520we%2520find%250Athat%2520in%2520our%2520case%252C%2520the%2520quantity%2520of%2520different%2520samples%2520seems%2520to%2520be%2520more%2520important%250Athan%2520higher%2520target%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Training%20Neural%20Networks%20for%20Imperfect%20Information%20Games%20by%0A%20%20Sampling%20Information%20Sets&entry.906535625=Timo%20Bertram%20and%20Johannes%20F%C3%BCrnkranz%20and%20Martin%20M%C3%BCller&entry.1292438233=%20%20In%20imperfect%20information%20games%2C%20the%20evaluation%20of%20a%20game%20state%20not%20only%0Adepends%20on%20the%20observable%20world%20but%20also%20relies%20on%20hidden%20parts%20of%20the%0Aenvironment.%20As%20accessing%20the%20obstructed%20information%20trivialises%20state%0Aevaluations%2C%20one%20approach%20to%20tackle%20such%20problems%20is%20to%20estimate%20the%20value%20of%0Athe%20imperfect%20state%20as%20a%20combination%20of%20all%20states%20in%20the%20information%20set%2C%0Ai.e.%2C%20all%20possible%20states%20that%20are%20consistent%20with%20the%20current%20imperfect%0Ainformation.%20In%20this%20work%2C%20the%20goal%20is%20to%20learn%20a%20function%20that%20maps%20from%20the%0Aimperfect%20game%20information%20state%20to%20its%20expected%20value.%20However%2C%20constructing%20a%0Aperfect%20training%20set%2C%20i.e.%20an%20enumeration%20of%20the%20whole%20information%20set%20for%0Anumerous%20imperfect%20states%2C%20is%20often%20infeasible.%20To%20compute%20the%20expected%20values%0Afor%20an%20imperfect%20information%20game%20like%20%5Ctextit%7BReconnaissance%20Blind%20Chess%7D%2C%20one%0Awould%20need%20to%20evaluate%20thousands%20of%20chess%20positions%20just%20to%20obtain%20the%20training%0Atarget%20for%20a%20single%20state.%20Still%2C%20the%20expected%20value%20of%20a%20state%20can%20already%20be%0Aapproximated%20with%20appropriate%20accuracy%20from%20a%20much%20smaller%20set%20of%20evaluations.%0AThus%2C%20in%20this%20paper%2C%20we%20empirically%20investigate%20how%20a%20budget%20of%20perfect%0Ainformation%20game%20evaluations%20should%20be%20distributed%20among%20training%20samples%20to%0Amaximise%20the%20return.%20Our%20results%20show%20that%20sampling%20a%20small%20number%20of%20states%2C%0Ain%20our%20experiments%20roughly%203%2C%20for%20a%20larger%20number%20of%20separate%20positions%20is%0Apreferable%20over%20repeatedly%20sampling%20a%20smaller%20quantity%20of%20states.%20Thus%2C%20we%20find%0Athat%20in%20our%20case%2C%20the%20quantity%20of%20different%20samples%20seems%20to%20be%20more%20important%0Athan%20higher%20target%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05876v1&entry.124074799=Read"},
{"title": "Masked Image Modeling as a Framework for Self-Supervised Learning across\n  Eye Movements", "author": "Robin Weiler and Matthias Brucklacher and Cyriel M. A. Pennartz and Sander M. Boht\u00e9", "abstract": "  To make sense of their surroundings, intelligent systems must transform\ncomplex sensory inputs to structured codes that are reduced to task-relevant\ninformation such as object category. Biological agents achieve this in a\nlargely autonomous manner, presumably via self-supervised learning. Whereas\nprevious attempts to model the underlying mechanisms were largely\ndiscriminative in nature, there is ample evidence that the brain employs a\ngenerative model of the world. Here, we propose that eye movements, in\ncombination with the focused nature of primate vision, constitute a generative,\nself-supervised task of predicting and revealing visual information. We\nconstruct a proof-of-principle model starting from the framework of masked\nimage modeling (MIM), a common approach in deep representation learning. To do\nso, we analyze how core components of MIM such as masking technique and data\naugmentation influence the formation of category-specific representations. This\nallows us not only to better understand the principles behind MIM, but to then\nreassemble a MIM more in line with the focused nature of biological perception.\nWe find that MIM disentangles neurons in latent space without explicit\nregularization, a property that has been suggested to structure visual\nrepresentations in primates. Together with previous findings of invariance\nlearning, this highlights an interesting connection of MIM to latent\nregularization approaches for self-supervised learning. The source code is\navailable under https://github.com/RobinWeiler/FocusMIM\n", "link": "http://arxiv.org/abs/2404.08526v2", "date": "2024-07-08", "relevancy": 2.2711, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5734}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5647}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Image%20Modeling%20as%20a%20Framework%20for%20Self-Supervised%20Learning%20across%0A%20%20Eye%20Movements&body=Title%3A%20Masked%20Image%20Modeling%20as%20a%20Framework%20for%20Self-Supervised%20Learning%20across%0A%20%20Eye%20Movements%0AAuthor%3A%20Robin%20Weiler%20and%20Matthias%20Brucklacher%20and%20Cyriel%20M.%20A.%20Pennartz%20and%20Sander%20M.%20Boht%C3%A9%0AAbstract%3A%20%20%20To%20make%20sense%20of%20their%20surroundings%2C%20intelligent%20systems%20must%20transform%0Acomplex%20sensory%20inputs%20to%20structured%20codes%20that%20are%20reduced%20to%20task-relevant%0Ainformation%20such%20as%20object%20category.%20Biological%20agents%20achieve%20this%20in%20a%0Alargely%20autonomous%20manner%2C%20presumably%20via%20self-supervised%20learning.%20Whereas%0Aprevious%20attempts%20to%20model%20the%20underlying%20mechanisms%20were%20largely%0Adiscriminative%20in%20nature%2C%20there%20is%20ample%20evidence%20that%20the%20brain%20employs%20a%0Agenerative%20model%20of%20the%20world.%20Here%2C%20we%20propose%20that%20eye%20movements%2C%20in%0Acombination%20with%20the%20focused%20nature%20of%20primate%20vision%2C%20constitute%20a%20generative%2C%0Aself-supervised%20task%20of%20predicting%20and%20revealing%20visual%20information.%20We%0Aconstruct%20a%20proof-of-principle%20model%20starting%20from%20the%20framework%20of%20masked%0Aimage%20modeling%20%28MIM%29%2C%20a%20common%20approach%20in%20deep%20representation%20learning.%20To%20do%0Aso%2C%20we%20analyze%20how%20core%20components%20of%20MIM%20such%20as%20masking%20technique%20and%20data%0Aaugmentation%20influence%20the%20formation%20of%20category-specific%20representations.%20This%0Aallows%20us%20not%20only%20to%20better%20understand%20the%20principles%20behind%20MIM%2C%20but%20to%20then%0Areassemble%20a%20MIM%20more%20in%20line%20with%20the%20focused%20nature%20of%20biological%20perception.%0AWe%20find%20that%20MIM%20disentangles%20neurons%20in%20latent%20space%20without%20explicit%0Aregularization%2C%20a%20property%20that%20has%20been%20suggested%20to%20structure%20visual%0Arepresentations%20in%20primates.%20Together%20with%20previous%20findings%20of%20invariance%0Alearning%2C%20this%20highlights%20an%20interesting%20connection%20of%20MIM%20to%20latent%0Aregularization%20approaches%20for%20self-supervised%20learning.%20The%20source%20code%20is%0Aavailable%20under%20https%3A//github.com/RobinWeiler/FocusMIM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Image%2520Modeling%2520as%2520a%2520Framework%2520for%2520Self-Supervised%2520Learning%2520across%250A%2520%2520Eye%2520Movements%26entry.906535625%3DRobin%2520Weiler%2520and%2520Matthias%2520Brucklacher%2520and%2520Cyriel%2520M.%2520A.%2520Pennartz%2520and%2520Sander%2520M.%2520Boht%25C3%25A9%26entry.1292438233%3D%2520%2520To%2520make%2520sense%2520of%2520their%2520surroundings%252C%2520intelligent%2520systems%2520must%2520transform%250Acomplex%2520sensory%2520inputs%2520to%2520structured%2520codes%2520that%2520are%2520reduced%2520to%2520task-relevant%250Ainformation%2520such%2520as%2520object%2520category.%2520Biological%2520agents%2520achieve%2520this%2520in%2520a%250Alargely%2520autonomous%2520manner%252C%2520presumably%2520via%2520self-supervised%2520learning.%2520Whereas%250Aprevious%2520attempts%2520to%2520model%2520the%2520underlying%2520mechanisms%2520were%2520largely%250Adiscriminative%2520in%2520nature%252C%2520there%2520is%2520ample%2520evidence%2520that%2520the%2520brain%2520employs%2520a%250Agenerative%2520model%2520of%2520the%2520world.%2520Here%252C%2520we%2520propose%2520that%2520eye%2520movements%252C%2520in%250Acombination%2520with%2520the%2520focused%2520nature%2520of%2520primate%2520vision%252C%2520constitute%2520a%2520generative%252C%250Aself-supervised%2520task%2520of%2520predicting%2520and%2520revealing%2520visual%2520information.%2520We%250Aconstruct%2520a%2520proof-of-principle%2520model%2520starting%2520from%2520the%2520framework%2520of%2520masked%250Aimage%2520modeling%2520%2528MIM%2529%252C%2520a%2520common%2520approach%2520in%2520deep%2520representation%2520learning.%2520To%2520do%250Aso%252C%2520we%2520analyze%2520how%2520core%2520components%2520of%2520MIM%2520such%2520as%2520masking%2520technique%2520and%2520data%250Aaugmentation%2520influence%2520the%2520formation%2520of%2520category-specific%2520representations.%2520This%250Aallows%2520us%2520not%2520only%2520to%2520better%2520understand%2520the%2520principles%2520behind%2520MIM%252C%2520but%2520to%2520then%250Areassemble%2520a%2520MIM%2520more%2520in%2520line%2520with%2520the%2520focused%2520nature%2520of%2520biological%2520perception.%250AWe%2520find%2520that%2520MIM%2520disentangles%2520neurons%2520in%2520latent%2520space%2520without%2520explicit%250Aregularization%252C%2520a%2520property%2520that%2520has%2520been%2520suggested%2520to%2520structure%2520visual%250Arepresentations%2520in%2520primates.%2520Together%2520with%2520previous%2520findings%2520of%2520invariance%250Alearning%252C%2520this%2520highlights%2520an%2520interesting%2520connection%2520of%2520MIM%2520to%2520latent%250Aregularization%2520approaches%2520for%2520self-supervised%2520learning.%2520The%2520source%2520code%2520is%250Aavailable%2520under%2520https%253A//github.com/RobinWeiler/FocusMIM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Image%20Modeling%20as%20a%20Framework%20for%20Self-Supervised%20Learning%20across%0A%20%20Eye%20Movements&entry.906535625=Robin%20Weiler%20and%20Matthias%20Brucklacher%20and%20Cyriel%20M.%20A.%20Pennartz%20and%20Sander%20M.%20Boht%C3%A9&entry.1292438233=%20%20To%20make%20sense%20of%20their%20surroundings%2C%20intelligent%20systems%20must%20transform%0Acomplex%20sensory%20inputs%20to%20structured%20codes%20that%20are%20reduced%20to%20task-relevant%0Ainformation%20such%20as%20object%20category.%20Biological%20agents%20achieve%20this%20in%20a%0Alargely%20autonomous%20manner%2C%20presumably%20via%20self-supervised%20learning.%20Whereas%0Aprevious%20attempts%20to%20model%20the%20underlying%20mechanisms%20were%20largely%0Adiscriminative%20in%20nature%2C%20there%20is%20ample%20evidence%20that%20the%20brain%20employs%20a%0Agenerative%20model%20of%20the%20world.%20Here%2C%20we%20propose%20that%20eye%20movements%2C%20in%0Acombination%20with%20the%20focused%20nature%20of%20primate%20vision%2C%20constitute%20a%20generative%2C%0Aself-supervised%20task%20of%20predicting%20and%20revealing%20visual%20information.%20We%0Aconstruct%20a%20proof-of-principle%20model%20starting%20from%20the%20framework%20of%20masked%0Aimage%20modeling%20%28MIM%29%2C%20a%20common%20approach%20in%20deep%20representation%20learning.%20To%20do%0Aso%2C%20we%20analyze%20how%20core%20components%20of%20MIM%20such%20as%20masking%20technique%20and%20data%0Aaugmentation%20influence%20the%20formation%20of%20category-specific%20representations.%20This%0Aallows%20us%20not%20only%20to%20better%20understand%20the%20principles%20behind%20MIM%2C%20but%20to%20then%0Areassemble%20a%20MIM%20more%20in%20line%20with%20the%20focused%20nature%20of%20biological%20perception.%0AWe%20find%20that%20MIM%20disentangles%20neurons%20in%20latent%20space%20without%20explicit%0Aregularization%2C%20a%20property%20that%20has%20been%20suggested%20to%20structure%20visual%0Arepresentations%20in%20primates.%20Together%20with%20previous%20findings%20of%20invariance%0Alearning%2C%20this%20highlights%20an%20interesting%20connection%20of%20MIM%20to%20latent%0Aregularization%20approaches%20for%20self-supervised%20learning.%20The%20source%20code%20is%0Aavailable%20under%20https%3A//github.com/RobinWeiler/FocusMIM%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08526v2&entry.124074799=Read"},
{"title": "The Multilingual Alignment Prism: Aligning Global and Local Preferences\n  to Reduce Harm", "author": " Aakanksha and Arash Ahmadian and Beyza Ermis and Seraphina Goldfarb-Tarrant and Julia Kreutzer and Marzieh Fadaee and Sara Hooker", "abstract": "  A key concern with the concept of \"alignment\" is the implicit question of\n\"alignment to what?\". AI systems are increasingly used across the world, yet\nsafety alignment is often focused on homogeneous monolingual settings.\nAdditionally, preference training and safety measures often overfit to harms\ncommon in Western-centric datasets. Here, we explore the viability of different\nalignment approaches when balancing dual objectives: addressing and optimizing\nfor a non-homogeneous set of languages and cultural preferences while\nminimizing both global and local harms. We collect the first set of human\nannotated red-teaming prompts in different languages distinguishing between\nglobal and local harm, which serve as a laboratory for understanding the\nreliability of alignment techniques when faced with preference distributions\nthat are non-stationary across geographies and languages. While this setting is\nseldom covered by the literature to date, which primarily centers on English\nharm mitigation, it captures real-world interactions with AI systems around the\nworld. We establish a new precedent for state-of-the-art alignment techniques\nacross 6 languages with minimal degradation in general performance. Our work\nprovides important insights into cross-lingual transfer and novel optimization\napproaches to safeguard AI systems designed to serve global populations.\n", "link": "http://arxiv.org/abs/2406.18682v2", "date": "2024-07-08", "relevancy": 2.2644, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4595}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4551}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Multilingual%20Alignment%20Prism%3A%20Aligning%20Global%20and%20Local%20Preferences%0A%20%20to%20Reduce%20Harm&body=Title%3A%20The%20Multilingual%20Alignment%20Prism%3A%20Aligning%20Global%20and%20Local%20Preferences%0A%20%20to%20Reduce%20Harm%0AAuthor%3A%20%20Aakanksha%20and%20Arash%20Ahmadian%20and%20Beyza%20Ermis%20and%20Seraphina%20Goldfarb-Tarrant%20and%20Julia%20Kreutzer%20and%20Marzieh%20Fadaee%20and%20Sara%20Hooker%0AAbstract%3A%20%20%20A%20key%20concern%20with%20the%20concept%20of%20%22alignment%22%20is%20the%20implicit%20question%20of%0A%22alignment%20to%20what%3F%22.%20AI%20systems%20are%20increasingly%20used%20across%20the%20world%2C%20yet%0Asafety%20alignment%20is%20often%20focused%20on%20homogeneous%20monolingual%20settings.%0AAdditionally%2C%20preference%20training%20and%20safety%20measures%20often%20overfit%20to%20harms%0Acommon%20in%20Western-centric%20datasets.%20Here%2C%20we%20explore%20the%20viability%20of%20different%0Aalignment%20approaches%20when%20balancing%20dual%20objectives%3A%20addressing%20and%20optimizing%0Afor%20a%20non-homogeneous%20set%20of%20languages%20and%20cultural%20preferences%20while%0Aminimizing%20both%20global%20and%20local%20harms.%20We%20collect%20the%20first%20set%20of%20human%0Aannotated%20red-teaming%20prompts%20in%20different%20languages%20distinguishing%20between%0Aglobal%20and%20local%20harm%2C%20which%20serve%20as%20a%20laboratory%20for%20understanding%20the%0Areliability%20of%20alignment%20techniques%20when%20faced%20with%20preference%20distributions%0Athat%20are%20non-stationary%20across%20geographies%20and%20languages.%20While%20this%20setting%20is%0Aseldom%20covered%20by%20the%20literature%20to%20date%2C%20which%20primarily%20centers%20on%20English%0Aharm%20mitigation%2C%20it%20captures%20real-world%20interactions%20with%20AI%20systems%20around%20the%0Aworld.%20We%20establish%20a%20new%20precedent%20for%20state-of-the-art%20alignment%20techniques%0Aacross%206%20languages%20with%20minimal%20degradation%20in%20general%20performance.%20Our%20work%0Aprovides%20important%20insights%20into%20cross-lingual%20transfer%20and%20novel%20optimization%0Aapproaches%20to%20safeguard%20AI%20systems%20designed%20to%20serve%20global%20populations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Multilingual%2520Alignment%2520Prism%253A%2520Aligning%2520Global%2520and%2520Local%2520Preferences%250A%2520%2520to%2520Reduce%2520Harm%26entry.906535625%3D%2520Aakanksha%2520and%2520Arash%2520Ahmadian%2520and%2520Beyza%2520Ermis%2520and%2520Seraphina%2520Goldfarb-Tarrant%2520and%2520Julia%2520Kreutzer%2520and%2520Marzieh%2520Fadaee%2520and%2520Sara%2520Hooker%26entry.1292438233%3D%2520%2520A%2520key%2520concern%2520with%2520the%2520concept%2520of%2520%2522alignment%2522%2520is%2520the%2520implicit%2520question%2520of%250A%2522alignment%2520to%2520what%253F%2522.%2520AI%2520systems%2520are%2520increasingly%2520used%2520across%2520the%2520world%252C%2520yet%250Asafety%2520alignment%2520is%2520often%2520focused%2520on%2520homogeneous%2520monolingual%2520settings.%250AAdditionally%252C%2520preference%2520training%2520and%2520safety%2520measures%2520often%2520overfit%2520to%2520harms%250Acommon%2520in%2520Western-centric%2520datasets.%2520Here%252C%2520we%2520explore%2520the%2520viability%2520of%2520different%250Aalignment%2520approaches%2520when%2520balancing%2520dual%2520objectives%253A%2520addressing%2520and%2520optimizing%250Afor%2520a%2520non-homogeneous%2520set%2520of%2520languages%2520and%2520cultural%2520preferences%2520while%250Aminimizing%2520both%2520global%2520and%2520local%2520harms.%2520We%2520collect%2520the%2520first%2520set%2520of%2520human%250Aannotated%2520red-teaming%2520prompts%2520in%2520different%2520languages%2520distinguishing%2520between%250Aglobal%2520and%2520local%2520harm%252C%2520which%2520serve%2520as%2520a%2520laboratory%2520for%2520understanding%2520the%250Areliability%2520of%2520alignment%2520techniques%2520when%2520faced%2520with%2520preference%2520distributions%250Athat%2520are%2520non-stationary%2520across%2520geographies%2520and%2520languages.%2520While%2520this%2520setting%2520is%250Aseldom%2520covered%2520by%2520the%2520literature%2520to%2520date%252C%2520which%2520primarily%2520centers%2520on%2520English%250Aharm%2520mitigation%252C%2520it%2520captures%2520real-world%2520interactions%2520with%2520AI%2520systems%2520around%2520the%250Aworld.%2520We%2520establish%2520a%2520new%2520precedent%2520for%2520state-of-the-art%2520alignment%2520techniques%250Aacross%25206%2520languages%2520with%2520minimal%2520degradation%2520in%2520general%2520performance.%2520Our%2520work%250Aprovides%2520important%2520insights%2520into%2520cross-lingual%2520transfer%2520and%2520novel%2520optimization%250Aapproaches%2520to%2520safeguard%2520AI%2520systems%2520designed%2520to%2520serve%2520global%2520populations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Multilingual%20Alignment%20Prism%3A%20Aligning%20Global%20and%20Local%20Preferences%0A%20%20to%20Reduce%20Harm&entry.906535625=%20Aakanksha%20and%20Arash%20Ahmadian%20and%20Beyza%20Ermis%20and%20Seraphina%20Goldfarb-Tarrant%20and%20Julia%20Kreutzer%20and%20Marzieh%20Fadaee%20and%20Sara%20Hooker&entry.1292438233=%20%20A%20key%20concern%20with%20the%20concept%20of%20%22alignment%22%20is%20the%20implicit%20question%20of%0A%22alignment%20to%20what%3F%22.%20AI%20systems%20are%20increasingly%20used%20across%20the%20world%2C%20yet%0Asafety%20alignment%20is%20often%20focused%20on%20homogeneous%20monolingual%20settings.%0AAdditionally%2C%20preference%20training%20and%20safety%20measures%20often%20overfit%20to%20harms%0Acommon%20in%20Western-centric%20datasets.%20Here%2C%20we%20explore%20the%20viability%20of%20different%0Aalignment%20approaches%20when%20balancing%20dual%20objectives%3A%20addressing%20and%20optimizing%0Afor%20a%20non-homogeneous%20set%20of%20languages%20and%20cultural%20preferences%20while%0Aminimizing%20both%20global%20and%20local%20harms.%20We%20collect%20the%20first%20set%20of%20human%0Aannotated%20red-teaming%20prompts%20in%20different%20languages%20distinguishing%20between%0Aglobal%20and%20local%20harm%2C%20which%20serve%20as%20a%20laboratory%20for%20understanding%20the%0Areliability%20of%20alignment%20techniques%20when%20faced%20with%20preference%20distributions%0Athat%20are%20non-stationary%20across%20geographies%20and%20languages.%20While%20this%20setting%20is%0Aseldom%20covered%20by%20the%20literature%20to%20date%2C%20which%20primarily%20centers%20on%20English%0Aharm%20mitigation%2C%20it%20captures%20real-world%20interactions%20with%20AI%20systems%20around%20the%0Aworld.%20We%20establish%20a%20new%20precedent%20for%20state-of-the-art%20alignment%20techniques%0Aacross%206%20languages%20with%20minimal%20degradation%20in%20general%20performance.%20Our%20work%0Aprovides%20important%20insights%20into%20cross-lingual%20transfer%20and%20novel%20optimization%0Aapproaches%20to%20safeguard%20AI%20systems%20designed%20to%20serve%20global%20populations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18682v2&entry.124074799=Read"},
{"title": "Mask as Supervision: Leveraging Unified Mask Information for\n  Unsupervised 3D Pose Estimation", "author": "Yuchen Yang and Yu Qiao and Xiao Sun", "abstract": "  Automatic estimation of 3D human pose from monocular RGB images is a\nchallenging and unsolved problem in computer vision. In a supervised manner,\napproaches heavily rely on laborious annotations and present hampered\ngeneralization ability due to the limited diversity of 3D pose datasets. To\naddress these challenges, we propose a unified framework that leverages mask as\nsupervision for unsupervised 3D pose estimation. With general unsupervised\nsegmentation algorithms, the proposed model employs skeleton and physique\nrepresentations that exploit accurate pose information from coarse to fine.\nCompared with previous unsupervised approaches, we organize the human skeleton\nin a fully unsupervised way which enables the processing of annotation-free\ndata and provides ready-to-use estimation results. Comprehensive experiments\ndemonstrate our state-of-the-art pose estimation performance on Human3.6M and\nMPI-INF-3DHP datasets. Further experiments on in-the-wild datasets also\nillustrate the capability to access more data to boost our model. Code will be\navailable at https://github.com/Charrrrrlie/Mask-as-Supervision.\n", "link": "http://arxiv.org/abs/2312.07051v2", "date": "2024-07-08", "relevancy": 2.2558, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5835}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5711}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask%20as%20Supervision%3A%20Leveraging%20Unified%20Mask%20Information%20for%0A%20%20Unsupervised%203D%20Pose%20Estimation&body=Title%3A%20Mask%20as%20Supervision%3A%20Leveraging%20Unified%20Mask%20Information%20for%0A%20%20Unsupervised%203D%20Pose%20Estimation%0AAuthor%3A%20Yuchen%20Yang%20and%20Yu%20Qiao%20and%20Xiao%20Sun%0AAbstract%3A%20%20%20Automatic%20estimation%20of%203D%20human%20pose%20from%20monocular%20RGB%20images%20is%20a%0Achallenging%20and%20unsolved%20problem%20in%20computer%20vision.%20In%20a%20supervised%20manner%2C%0Aapproaches%20heavily%20rely%20on%20laborious%20annotations%20and%20present%20hampered%0Ageneralization%20ability%20due%20to%20the%20limited%20diversity%20of%203D%20pose%20datasets.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20unified%20framework%20that%20leverages%20mask%20as%0Asupervision%20for%20unsupervised%203D%20pose%20estimation.%20With%20general%20unsupervised%0Asegmentation%20algorithms%2C%20the%20proposed%20model%20employs%20skeleton%20and%20physique%0Arepresentations%20that%20exploit%20accurate%20pose%20information%20from%20coarse%20to%20fine.%0ACompared%20with%20previous%20unsupervised%20approaches%2C%20we%20organize%20the%20human%20skeleton%0Ain%20a%20fully%20unsupervised%20way%20which%20enables%20the%20processing%20of%20annotation-free%0Adata%20and%20provides%20ready-to-use%20estimation%20results.%20Comprehensive%20experiments%0Ademonstrate%20our%20state-of-the-art%20pose%20estimation%20performance%20on%20Human3.6M%20and%0AMPI-INF-3DHP%20datasets.%20Further%20experiments%20on%20in-the-wild%20datasets%20also%0Aillustrate%20the%20capability%20to%20access%20more%20data%20to%20boost%20our%20model.%20Code%20will%20be%0Aavailable%20at%20https%3A//github.com/Charrrrrlie/Mask-as-Supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07051v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask%2520as%2520Supervision%253A%2520Leveraging%2520Unified%2520Mask%2520Information%2520for%250A%2520%2520Unsupervised%25203D%2520Pose%2520Estimation%26entry.906535625%3DYuchen%2520Yang%2520and%2520Yu%2520Qiao%2520and%2520Xiao%2520Sun%26entry.1292438233%3D%2520%2520Automatic%2520estimation%2520of%25203D%2520human%2520pose%2520from%2520monocular%2520RGB%2520images%2520is%2520a%250Achallenging%2520and%2520unsolved%2520problem%2520in%2520computer%2520vision.%2520In%2520a%2520supervised%2520manner%252C%250Aapproaches%2520heavily%2520rely%2520on%2520laborious%2520annotations%2520and%2520present%2520hampered%250Ageneralization%2520ability%2520due%2520to%2520the%2520limited%2520diversity%2520of%25203D%2520pose%2520datasets.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520unified%2520framework%2520that%2520leverages%2520mask%2520as%250Asupervision%2520for%2520unsupervised%25203D%2520pose%2520estimation.%2520With%2520general%2520unsupervised%250Asegmentation%2520algorithms%252C%2520the%2520proposed%2520model%2520employs%2520skeleton%2520and%2520physique%250Arepresentations%2520that%2520exploit%2520accurate%2520pose%2520information%2520from%2520coarse%2520to%2520fine.%250ACompared%2520with%2520previous%2520unsupervised%2520approaches%252C%2520we%2520organize%2520the%2520human%2520skeleton%250Ain%2520a%2520fully%2520unsupervised%2520way%2520which%2520enables%2520the%2520processing%2520of%2520annotation-free%250Adata%2520and%2520provides%2520ready-to-use%2520estimation%2520results.%2520Comprehensive%2520experiments%250Ademonstrate%2520our%2520state-of-the-art%2520pose%2520estimation%2520performance%2520on%2520Human3.6M%2520and%250AMPI-INF-3DHP%2520datasets.%2520Further%2520experiments%2520on%2520in-the-wild%2520datasets%2520also%250Aillustrate%2520the%2520capability%2520to%2520access%2520more%2520data%2520to%2520boost%2520our%2520model.%2520Code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/Charrrrrlie/Mask-as-Supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07051v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask%20as%20Supervision%3A%20Leveraging%20Unified%20Mask%20Information%20for%0A%20%20Unsupervised%203D%20Pose%20Estimation&entry.906535625=Yuchen%20Yang%20and%20Yu%20Qiao%20and%20Xiao%20Sun&entry.1292438233=%20%20Automatic%20estimation%20of%203D%20human%20pose%20from%20monocular%20RGB%20images%20is%20a%0Achallenging%20and%20unsolved%20problem%20in%20computer%20vision.%20In%20a%20supervised%20manner%2C%0Aapproaches%20heavily%20rely%20on%20laborious%20annotations%20and%20present%20hampered%0Ageneralization%20ability%20due%20to%20the%20limited%20diversity%20of%203D%20pose%20datasets.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20unified%20framework%20that%20leverages%20mask%20as%0Asupervision%20for%20unsupervised%203D%20pose%20estimation.%20With%20general%20unsupervised%0Asegmentation%20algorithms%2C%20the%20proposed%20model%20employs%20skeleton%20and%20physique%0Arepresentations%20that%20exploit%20accurate%20pose%20information%20from%20coarse%20to%20fine.%0ACompared%20with%20previous%20unsupervised%20approaches%2C%20we%20organize%20the%20human%20skeleton%0Ain%20a%20fully%20unsupervised%20way%20which%20enables%20the%20processing%20of%20annotation-free%0Adata%20and%20provides%20ready-to-use%20estimation%20results.%20Comprehensive%20experiments%0Ademonstrate%20our%20state-of-the-art%20pose%20estimation%20performance%20on%20Human3.6M%20and%0AMPI-INF-3DHP%20datasets.%20Further%20experiments%20on%20in-the-wild%20datasets%20also%0Aillustrate%20the%20capability%20to%20access%20more%20data%20to%20boost%20our%20model.%20Code%20will%20be%0Aavailable%20at%20https%3A//github.com/Charrrrrlie/Mask-as-Supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07051v2&entry.124074799=Read"},
{"title": "Looking for Tiny Defects via Forward-Backward Feature Transfer", "author": "Alex Costanzino and Pierluigi Zama Ramirez and Giuseppe Lisanti and Luigi Di Stefano", "abstract": "  Motivated by efficiency requirements, most anomaly detection and segmentation\n(AD&S) methods focus on processing low-resolution images, e.g., $224\\times 224$\npixels, obtained by downsampling the original input images. In this setting,\ndownsampling is typically applied also to the provided ground-truth defect\nmasks. Yet, as numerous industrial applications demand identification of both\nlarge and tiny defects, the above-described protocol may fall short in\nproviding a realistic picture of the actual performance attainable by current\nmethods. Hence, in this work, we introduce a novel benchmark that evaluates\nmethods on the original, high-resolution image and ground-truth masks, focusing\non segmentation performance as a function of the size of anomalies. Our\nbenchmark includes a metric that captures robustness with respect to defect\nsize, i.e., the ability of a method to preserve good localization from large\nanomalies to tiny ones. Furthermore, we introduce an AD&S approach based on a\nnovel Teacher-Student paradigm which relies on two shallow MLPs (the Students)\nthat learn to transfer patch features across the layers of a frozen vision\ntransformer (the Teacher). By means of our benchmark, we evaluate our proposal\nand other recent AD&S methods on high-resolution inputs containing large and\ntiny defects. Our proposal features the highest robustness to defect size, runs\nat the fastest speed, yields state-of-the-art performance on the MVTec AD\ndataset and state-of-the-art segmentation performance on the VisA dataset.\n", "link": "http://arxiv.org/abs/2407.04092v2", "date": "2024-07-08", "relevancy": 2.2333, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5878}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5639}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20for%20Tiny%20Defects%20via%20Forward-Backward%20Feature%20Transfer&body=Title%3A%20Looking%20for%20Tiny%20Defects%20via%20Forward-Backward%20Feature%20Transfer%0AAuthor%3A%20Alex%20Costanzino%20and%20Pierluigi%20Zama%20Ramirez%20and%20Giuseppe%20Lisanti%20and%20Luigi%20Di%20Stefano%0AAbstract%3A%20%20%20Motivated%20by%20efficiency%20requirements%2C%20most%20anomaly%20detection%20and%20segmentation%0A%28AD%26S%29%20methods%20focus%20on%20processing%20low-resolution%20images%2C%20e.g.%2C%20%24224%5Ctimes%20224%24%0Apixels%2C%20obtained%20by%20downsampling%20the%20original%20input%20images.%20In%20this%20setting%2C%0Adownsampling%20is%20typically%20applied%20also%20to%20the%20provided%20ground-truth%20defect%0Amasks.%20Yet%2C%20as%20numerous%20industrial%20applications%20demand%20identification%20of%20both%0Alarge%20and%20tiny%20defects%2C%20the%20above-described%20protocol%20may%20fall%20short%20in%0Aproviding%20a%20realistic%20picture%20of%20the%20actual%20performance%20attainable%20by%20current%0Amethods.%20Hence%2C%20in%20this%20work%2C%20we%20introduce%20a%20novel%20benchmark%20that%20evaluates%0Amethods%20on%20the%20original%2C%20high-resolution%20image%20and%20ground-truth%20masks%2C%20focusing%0Aon%20segmentation%20performance%20as%20a%20function%20of%20the%20size%20of%20anomalies.%20Our%0Abenchmark%20includes%20a%20metric%20that%20captures%20robustness%20with%20respect%20to%20defect%0Asize%2C%20i.e.%2C%20the%20ability%20of%20a%20method%20to%20preserve%20good%20localization%20from%20large%0Aanomalies%20to%20tiny%20ones.%20Furthermore%2C%20we%20introduce%20an%20AD%26S%20approach%20based%20on%20a%0Anovel%20Teacher-Student%20paradigm%20which%20relies%20on%20two%20shallow%20MLPs%20%28the%20Students%29%0Athat%20learn%20to%20transfer%20patch%20features%20across%20the%20layers%20of%20a%20frozen%20vision%0Atransformer%20%28the%20Teacher%29.%20By%20means%20of%20our%20benchmark%2C%20we%20evaluate%20our%20proposal%0Aand%20other%20recent%20AD%26S%20methods%20on%20high-resolution%20inputs%20containing%20large%20and%0Atiny%20defects.%20Our%20proposal%20features%20the%20highest%20robustness%20to%20defect%20size%2C%20runs%0Aat%20the%20fastest%20speed%2C%20yields%20state-of-the-art%20performance%20on%20the%20MVTec%20AD%0Adataset%20and%20state-of-the-art%20segmentation%20performance%20on%20the%20VisA%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520for%2520Tiny%2520Defects%2520via%2520Forward-Backward%2520Feature%2520Transfer%26entry.906535625%3DAlex%2520Costanzino%2520and%2520Pierluigi%2520Zama%2520Ramirez%2520and%2520Giuseppe%2520Lisanti%2520and%2520Luigi%2520Di%2520Stefano%26entry.1292438233%3D%2520%2520Motivated%2520by%2520efficiency%2520requirements%252C%2520most%2520anomaly%2520detection%2520and%2520segmentation%250A%2528AD%2526S%2529%2520methods%2520focus%2520on%2520processing%2520low-resolution%2520images%252C%2520e.g.%252C%2520%2524224%255Ctimes%2520224%2524%250Apixels%252C%2520obtained%2520by%2520downsampling%2520the%2520original%2520input%2520images.%2520In%2520this%2520setting%252C%250Adownsampling%2520is%2520typically%2520applied%2520also%2520to%2520the%2520provided%2520ground-truth%2520defect%250Amasks.%2520Yet%252C%2520as%2520numerous%2520industrial%2520applications%2520demand%2520identification%2520of%2520both%250Alarge%2520and%2520tiny%2520defects%252C%2520the%2520above-described%2520protocol%2520may%2520fall%2520short%2520in%250Aproviding%2520a%2520realistic%2520picture%2520of%2520the%2520actual%2520performance%2520attainable%2520by%2520current%250Amethods.%2520Hence%252C%2520in%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%2520that%2520evaluates%250Amethods%2520on%2520the%2520original%252C%2520high-resolution%2520image%2520and%2520ground-truth%2520masks%252C%2520focusing%250Aon%2520segmentation%2520performance%2520as%2520a%2520function%2520of%2520the%2520size%2520of%2520anomalies.%2520Our%250Abenchmark%2520includes%2520a%2520metric%2520that%2520captures%2520robustness%2520with%2520respect%2520to%2520defect%250Asize%252C%2520i.e.%252C%2520the%2520ability%2520of%2520a%2520method%2520to%2520preserve%2520good%2520localization%2520from%2520large%250Aanomalies%2520to%2520tiny%2520ones.%2520Furthermore%252C%2520we%2520introduce%2520an%2520AD%2526S%2520approach%2520based%2520on%2520a%250Anovel%2520Teacher-Student%2520paradigm%2520which%2520relies%2520on%2520two%2520shallow%2520MLPs%2520%2528the%2520Students%2529%250Athat%2520learn%2520to%2520transfer%2520patch%2520features%2520across%2520the%2520layers%2520of%2520a%2520frozen%2520vision%250Atransformer%2520%2528the%2520Teacher%2529.%2520By%2520means%2520of%2520our%2520benchmark%252C%2520we%2520evaluate%2520our%2520proposal%250Aand%2520other%2520recent%2520AD%2526S%2520methods%2520on%2520high-resolution%2520inputs%2520containing%2520large%2520and%250Atiny%2520defects.%2520Our%2520proposal%2520features%2520the%2520highest%2520robustness%2520to%2520defect%2520size%252C%2520runs%250Aat%2520the%2520fastest%2520speed%252C%2520yields%2520state-of-the-art%2520performance%2520on%2520the%2520MVTec%2520AD%250Adataset%2520and%2520state-of-the-art%2520segmentation%2520performance%2520on%2520the%2520VisA%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20for%20Tiny%20Defects%20via%20Forward-Backward%20Feature%20Transfer&entry.906535625=Alex%20Costanzino%20and%20Pierluigi%20Zama%20Ramirez%20and%20Giuseppe%20Lisanti%20and%20Luigi%20Di%20Stefano&entry.1292438233=%20%20Motivated%20by%20efficiency%20requirements%2C%20most%20anomaly%20detection%20and%20segmentation%0A%28AD%26S%29%20methods%20focus%20on%20processing%20low-resolution%20images%2C%20e.g.%2C%20%24224%5Ctimes%20224%24%0Apixels%2C%20obtained%20by%20downsampling%20the%20original%20input%20images.%20In%20this%20setting%2C%0Adownsampling%20is%20typically%20applied%20also%20to%20the%20provided%20ground-truth%20defect%0Amasks.%20Yet%2C%20as%20numerous%20industrial%20applications%20demand%20identification%20of%20both%0Alarge%20and%20tiny%20defects%2C%20the%20above-described%20protocol%20may%20fall%20short%20in%0Aproviding%20a%20realistic%20picture%20of%20the%20actual%20performance%20attainable%20by%20current%0Amethods.%20Hence%2C%20in%20this%20work%2C%20we%20introduce%20a%20novel%20benchmark%20that%20evaluates%0Amethods%20on%20the%20original%2C%20high-resolution%20image%20and%20ground-truth%20masks%2C%20focusing%0Aon%20segmentation%20performance%20as%20a%20function%20of%20the%20size%20of%20anomalies.%20Our%0Abenchmark%20includes%20a%20metric%20that%20captures%20robustness%20with%20respect%20to%20defect%0Asize%2C%20i.e.%2C%20the%20ability%20of%20a%20method%20to%20preserve%20good%20localization%20from%20large%0Aanomalies%20to%20tiny%20ones.%20Furthermore%2C%20we%20introduce%20an%20AD%26S%20approach%20based%20on%20a%0Anovel%20Teacher-Student%20paradigm%20which%20relies%20on%20two%20shallow%20MLPs%20%28the%20Students%29%0Athat%20learn%20to%20transfer%20patch%20features%20across%20the%20layers%20of%20a%20frozen%20vision%0Atransformer%20%28the%20Teacher%29.%20By%20means%20of%20our%20benchmark%2C%20we%20evaluate%20our%20proposal%0Aand%20other%20recent%20AD%26S%20methods%20on%20high-resolution%20inputs%20containing%20large%20and%0Atiny%20defects.%20Our%20proposal%20features%20the%20highest%20robustness%20to%20defect%20size%2C%20runs%0Aat%20the%20fastest%20speed%2C%20yields%20state-of-the-art%20performance%20on%20the%20MVTec%20AD%0Adataset%20and%20state-of-the-art%20segmentation%20performance%20on%20the%20VisA%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04092v2&entry.124074799=Read"},
{"title": "SLAM for Visually Impaired People: a Survey", "author": "Marziyeh Bamdad and Davide Scaramuzza and Alireza Darvishy", "abstract": "  In recent decades, several assistive technologies have been developed to\nimprove the ability of blind and visually impaired individuals to navigate\nindependently and safely. At the same time, simultaneous localization and\nmapping (SLAM) techniques have become sufficiently robust and efficient to be\nadopted in developing these assistive technologies. We present the first\nsystematic literature review of 54 recent studies on SLAM-based solutions for\nblind and visually impaired people, focusing on literature published from 2017\nonward. This review explores various localization and mapping techniques\nemployed in this context. We systematically identified and categorized diverse\nSLAM approaches and analyzed their localization and mapping techniques, sensor\ntypes, computing resources, and machine-learning methods. We discuss the\nadvantages and limitations of these techniques for blind and visually impaired\nnavigation. Moreover, we examine the major challenges described across studies,\nincluding practical considerations that affect usability and adoption. Our\nanalysis also evaluates the effectiveness of these SLAM-based solutions in\nreal-world scenarios and user satisfaction, providing insights into their\npractical impact on BVI mobility. The insights derived from this review\nidentify critical gaps and opportunities for future research activities,\nparticularly in addressing the challenges presented by dynamic and complex\nenvironments. We explain how SLAM technology offers the potential to improve\nthe ability of visually impaired individuals to navigate effectively. Finally,\nwe present future opportunities and challenges in this domain.\n", "link": "http://arxiv.org/abs/2212.04745v5", "date": "2024-07-08", "relevancy": 2.23, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5816}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAM%20for%20Visually%20Impaired%20People%3A%20a%20Survey&body=Title%3A%20SLAM%20for%20Visually%20Impaired%20People%3A%20a%20Survey%0AAuthor%3A%20Marziyeh%20Bamdad%20and%20Davide%20Scaramuzza%20and%20Alireza%20Darvishy%0AAbstract%3A%20%20%20In%20recent%20decades%2C%20several%20assistive%20technologies%20have%20been%20developed%20to%0Aimprove%20the%20ability%20of%20blind%20and%20visually%20impaired%20individuals%20to%20navigate%0Aindependently%20and%20safely.%20At%20the%20same%20time%2C%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20techniques%20have%20become%20sufficiently%20robust%20and%20efficient%20to%20be%0Aadopted%20in%20developing%20these%20assistive%20technologies.%20We%20present%20the%20first%0Asystematic%20literature%20review%20of%2054%20recent%20studies%20on%20SLAM-based%20solutions%20for%0Ablind%20and%20visually%20impaired%20people%2C%20focusing%20on%20literature%20published%20from%202017%0Aonward.%20This%20review%20explores%20various%20localization%20and%20mapping%20techniques%0Aemployed%20in%20this%20context.%20We%20systematically%20identified%20and%20categorized%20diverse%0ASLAM%20approaches%20and%20analyzed%20their%20localization%20and%20mapping%20techniques%2C%20sensor%0Atypes%2C%20computing%20resources%2C%20and%20machine-learning%20methods.%20We%20discuss%20the%0Aadvantages%20and%20limitations%20of%20these%20techniques%20for%20blind%20and%20visually%20impaired%0Anavigation.%20Moreover%2C%20we%20examine%20the%20major%20challenges%20described%20across%20studies%2C%0Aincluding%20practical%20considerations%20that%20affect%20usability%20and%20adoption.%20Our%0Aanalysis%20also%20evaluates%20the%20effectiveness%20of%20these%20SLAM-based%20solutions%20in%0Areal-world%20scenarios%20and%20user%20satisfaction%2C%20providing%20insights%20into%20their%0Apractical%20impact%20on%20BVI%20mobility.%20The%20insights%20derived%20from%20this%20review%0Aidentify%20critical%20gaps%20and%20opportunities%20for%20future%20research%20activities%2C%0Aparticularly%20in%20addressing%20the%20challenges%20presented%20by%20dynamic%20and%20complex%0Aenvironments.%20We%20explain%20how%20SLAM%20technology%20offers%20the%20potential%20to%20improve%0Athe%20ability%20of%20visually%20impaired%20individuals%20to%20navigate%20effectively.%20Finally%2C%0Awe%20present%20future%20opportunities%20and%20challenges%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.04745v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAM%2520for%2520Visually%2520Impaired%2520People%253A%2520a%2520Survey%26entry.906535625%3DMarziyeh%2520Bamdad%2520and%2520Davide%2520Scaramuzza%2520and%2520Alireza%2520Darvishy%26entry.1292438233%3D%2520%2520In%2520recent%2520decades%252C%2520several%2520assistive%2520technologies%2520have%2520been%2520developed%2520to%250Aimprove%2520the%2520ability%2520of%2520blind%2520and%2520visually%2520impaired%2520individuals%2520to%2520navigate%250Aindependently%2520and%2520safely.%2520At%2520the%2520same%2520time%252C%2520simultaneous%2520localization%2520and%250Amapping%2520%2528SLAM%2529%2520techniques%2520have%2520become%2520sufficiently%2520robust%2520and%2520efficient%2520to%2520be%250Aadopted%2520in%2520developing%2520these%2520assistive%2520technologies.%2520We%2520present%2520the%2520first%250Asystematic%2520literature%2520review%2520of%252054%2520recent%2520studies%2520on%2520SLAM-based%2520solutions%2520for%250Ablind%2520and%2520visually%2520impaired%2520people%252C%2520focusing%2520on%2520literature%2520published%2520from%25202017%250Aonward.%2520This%2520review%2520explores%2520various%2520localization%2520and%2520mapping%2520techniques%250Aemployed%2520in%2520this%2520context.%2520We%2520systematically%2520identified%2520and%2520categorized%2520diverse%250ASLAM%2520approaches%2520and%2520analyzed%2520their%2520localization%2520and%2520mapping%2520techniques%252C%2520sensor%250Atypes%252C%2520computing%2520resources%252C%2520and%2520machine-learning%2520methods.%2520We%2520discuss%2520the%250Aadvantages%2520and%2520limitations%2520of%2520these%2520techniques%2520for%2520blind%2520and%2520visually%2520impaired%250Anavigation.%2520Moreover%252C%2520we%2520examine%2520the%2520major%2520challenges%2520described%2520across%2520studies%252C%250Aincluding%2520practical%2520considerations%2520that%2520affect%2520usability%2520and%2520adoption.%2520Our%250Aanalysis%2520also%2520evaluates%2520the%2520effectiveness%2520of%2520these%2520SLAM-based%2520solutions%2520in%250Areal-world%2520scenarios%2520and%2520user%2520satisfaction%252C%2520providing%2520insights%2520into%2520their%250Apractical%2520impact%2520on%2520BVI%2520mobility.%2520The%2520insights%2520derived%2520from%2520this%2520review%250Aidentify%2520critical%2520gaps%2520and%2520opportunities%2520for%2520future%2520research%2520activities%252C%250Aparticularly%2520in%2520addressing%2520the%2520challenges%2520presented%2520by%2520dynamic%2520and%2520complex%250Aenvironments.%2520We%2520explain%2520how%2520SLAM%2520technology%2520offers%2520the%2520potential%2520to%2520improve%250Athe%2520ability%2520of%2520visually%2520impaired%2520individuals%2520to%2520navigate%2520effectively.%2520Finally%252C%250Awe%2520present%2520future%2520opportunities%2520and%2520challenges%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.04745v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAM%20for%20Visually%20Impaired%20People%3A%20a%20Survey&entry.906535625=Marziyeh%20Bamdad%20and%20Davide%20Scaramuzza%20and%20Alireza%20Darvishy&entry.1292438233=%20%20In%20recent%20decades%2C%20several%20assistive%20technologies%20have%20been%20developed%20to%0Aimprove%20the%20ability%20of%20blind%20and%20visually%20impaired%20individuals%20to%20navigate%0Aindependently%20and%20safely.%20At%20the%20same%20time%2C%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20techniques%20have%20become%20sufficiently%20robust%20and%20efficient%20to%20be%0Aadopted%20in%20developing%20these%20assistive%20technologies.%20We%20present%20the%20first%0Asystematic%20literature%20review%20of%2054%20recent%20studies%20on%20SLAM-based%20solutions%20for%0Ablind%20and%20visually%20impaired%20people%2C%20focusing%20on%20literature%20published%20from%202017%0Aonward.%20This%20review%20explores%20various%20localization%20and%20mapping%20techniques%0Aemployed%20in%20this%20context.%20We%20systematically%20identified%20and%20categorized%20diverse%0ASLAM%20approaches%20and%20analyzed%20their%20localization%20and%20mapping%20techniques%2C%20sensor%0Atypes%2C%20computing%20resources%2C%20and%20machine-learning%20methods.%20We%20discuss%20the%0Aadvantages%20and%20limitations%20of%20these%20techniques%20for%20blind%20and%20visually%20impaired%0Anavigation.%20Moreover%2C%20we%20examine%20the%20major%20challenges%20described%20across%20studies%2C%0Aincluding%20practical%20considerations%20that%20affect%20usability%20and%20adoption.%20Our%0Aanalysis%20also%20evaluates%20the%20effectiveness%20of%20these%20SLAM-based%20solutions%20in%0Areal-world%20scenarios%20and%20user%20satisfaction%2C%20providing%20insights%20into%20their%0Apractical%20impact%20on%20BVI%20mobility.%20The%20insights%20derived%20from%20this%20review%0Aidentify%20critical%20gaps%20and%20opportunities%20for%20future%20research%20activities%2C%0Aparticularly%20in%20addressing%20the%20challenges%20presented%20by%20dynamic%20and%20complex%0Aenvironments.%20We%20explain%20how%20SLAM%20technology%20offers%20the%20potential%20to%20improve%0Athe%20ability%20of%20visually%20impaired%20individuals%20to%20navigate%20effectively.%20Finally%2C%0Awe%20present%20future%20opportunities%20and%20challenges%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.04745v5&entry.124074799=Read"},
{"title": "ANOLE: An Open, Autoregressive, Native Large Multimodal Models for\n  Interleaved Image-Text Generation", "author": "Ethan Chern and Jiadi Su and Yan Ma and Pengfei Liu", "abstract": "  Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.\n", "link": "http://arxiv.org/abs/2407.06135v1", "date": "2024-07-08", "relevancy": 2.2175, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5467}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANOLE%3A%20An%20Open%2C%20Autoregressive%2C%20Native%20Large%20Multimodal%20Models%20for%0A%20%20Interleaved%20Image-Text%20Generation&body=Title%3A%20ANOLE%3A%20An%20Open%2C%20Autoregressive%2C%20Native%20Large%20Multimodal%20Models%20for%0A%20%20Interleaved%20Image-Text%20Generation%0AAuthor%3A%20Ethan%20Chern%20and%20Jiadi%20Su%20and%20Yan%20Ma%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20Previous%20open-source%20large%20multimodal%20models%20%28LMMs%29%20have%20faced%20several%0Alimitations%3A%20%281%29%20they%20often%20lack%20native%20integration%2C%20requiring%20adapters%20to%0Aalign%20visual%20representations%20with%20pre-trained%20large%20language%20models%20%28LLMs%29%3B%20%282%29%0Amany%20are%20restricted%20to%20single-modal%20generation%3B%20%283%29%20while%20some%20support%0Amultimodal%20generation%2C%20they%20rely%20on%20separate%20diffusion%20models%20for%20visual%0Amodeling%20and%20generation.%20To%20mitigate%20these%20limitations%2C%20we%20present%20Anole%2C%20an%0Aopen%2C%20autoregressive%2C%20native%20large%20multimodal%20model%20for%20interleaved%20image-text%0Ageneration.%20We%20build%20Anole%20from%20Meta%20AI%27s%20Chameleon%2C%20adopting%20an%20innovative%0Afine-tuning%20strategy%20that%20is%20both%20data-efficient%20and%20parameter-efficient.%20Anole%0Ademonstrates%20high-quality%2C%20coherent%20multimodal%20generation%20capabilities.%20We%20have%0Aopen-sourced%20our%20model%2C%20training%20framework%2C%20and%20instruction%20tuning%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANOLE%253A%2520An%2520Open%252C%2520Autoregressive%252C%2520Native%2520Large%2520Multimodal%2520Models%2520for%250A%2520%2520Interleaved%2520Image-Text%2520Generation%26entry.906535625%3DEthan%2520Chern%2520and%2520Jiadi%2520Su%2520and%2520Yan%2520Ma%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520Previous%2520open-source%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520faced%2520several%250Alimitations%253A%2520%25281%2529%2520they%2520often%2520lack%2520native%2520integration%252C%2520requiring%2520adapters%2520to%250Aalign%2520visual%2520representations%2520with%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%253B%2520%25282%2529%250Amany%2520are%2520restricted%2520to%2520single-modal%2520generation%253B%2520%25283%2529%2520while%2520some%2520support%250Amultimodal%2520generation%252C%2520they%2520rely%2520on%2520separate%2520diffusion%2520models%2520for%2520visual%250Amodeling%2520and%2520generation.%2520To%2520mitigate%2520these%2520limitations%252C%2520we%2520present%2520Anole%252C%2520an%250Aopen%252C%2520autoregressive%252C%2520native%2520large%2520multimodal%2520model%2520for%2520interleaved%2520image-text%250Ageneration.%2520We%2520build%2520Anole%2520from%2520Meta%2520AI%2527s%2520Chameleon%252C%2520adopting%2520an%2520innovative%250Afine-tuning%2520strategy%2520that%2520is%2520both%2520data-efficient%2520and%2520parameter-efficient.%2520Anole%250Ademonstrates%2520high-quality%252C%2520coherent%2520multimodal%2520generation%2520capabilities.%2520We%2520have%250Aopen-sourced%2520our%2520model%252C%2520training%2520framework%252C%2520and%2520instruction%2520tuning%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANOLE%3A%20An%20Open%2C%20Autoregressive%2C%20Native%20Large%20Multimodal%20Models%20for%0A%20%20Interleaved%20Image-Text%20Generation&entry.906535625=Ethan%20Chern%20and%20Jiadi%20Su%20and%20Yan%20Ma%20and%20Pengfei%20Liu&entry.1292438233=%20%20Previous%20open-source%20large%20multimodal%20models%20%28LMMs%29%20have%20faced%20several%0Alimitations%3A%20%281%29%20they%20often%20lack%20native%20integration%2C%20requiring%20adapters%20to%0Aalign%20visual%20representations%20with%20pre-trained%20large%20language%20models%20%28LLMs%29%3B%20%282%29%0Amany%20are%20restricted%20to%20single-modal%20generation%3B%20%283%29%20while%20some%20support%0Amultimodal%20generation%2C%20they%20rely%20on%20separate%20diffusion%20models%20for%20visual%0Amodeling%20and%20generation.%20To%20mitigate%20these%20limitations%2C%20we%20present%20Anole%2C%20an%0Aopen%2C%20autoregressive%2C%20native%20large%20multimodal%20model%20for%20interleaved%20image-text%0Ageneration.%20We%20build%20Anole%20from%20Meta%20AI%27s%20Chameleon%2C%20adopting%20an%20innovative%0Afine-tuning%20strategy%20that%20is%20both%20data-efficient%20and%20parameter-efficient.%20Anole%0Ademonstrates%20high-quality%2C%20coherent%20multimodal%20generation%20capabilities.%20We%20have%0Aopen-sourced%20our%20model%2C%20training%20framework%2C%20and%20instruction%20tuning%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06135v1&entry.124074799=Read"},
{"title": "Safety-Critical Control with Uncertainty Quantification using Adaptive\n  Conformal Prediction", "author": "Hao Zhou and Yanze Zhang and Wenhao Luo", "abstract": "  Safety assurance is critical in the planning and control of robotic systems.\nFor robots operating in the real world, the safety-critical design often needs\nto explicitly address uncertainties and the pre-computed guarantees often rely\non the assumption of the particular distribution of the uncertainty. However,\nit is difficult to characterize the actual uncertainty distribution beforehand\nand thus the established safety guarantee may be violated due to possible\ndistribution mismatch. In this paper, we propose a novel safe control framework\nthat provides a high-probability safety guarantee for stochastic dynamical\nsystems following unknown distributions of motion noise. Specifically, this\nframework adopts adaptive conformal prediction to dynamically quantify the\nprediction uncertainty from online observations and combines that with the\nprobabilistic extension of the control barrier functions (CBFs) to characterize\nthe uncertainty-aware control constraints. By integrating the constraints in\nthe model predictive control scheme, it allows robots to adaptively capture the\ntrue prediction uncertainty online in a distribution-free setting and enjoys\nformally provable high-probability safety assurance. Simulation results on\nmulti-robot systems with stochastic single-integrator dynamics and unicycle\ndynamics are provided to demonstrate the effectiveness of our framework.\n", "link": "http://arxiv.org/abs/2407.03569v2", "date": "2024-07-08", "relevancy": 2.2123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6173}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.559}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety-Critical%20Control%20with%20Uncertainty%20Quantification%20using%20Adaptive%0A%20%20Conformal%20Prediction&body=Title%3A%20Safety-Critical%20Control%20with%20Uncertainty%20Quantification%20using%20Adaptive%0A%20%20Conformal%20Prediction%0AAuthor%3A%20Hao%20Zhou%20and%20Yanze%20Zhang%20and%20Wenhao%20Luo%0AAbstract%3A%20%20%20Safety%20assurance%20is%20critical%20in%20the%20planning%20and%20control%20of%20robotic%20systems.%0AFor%20robots%20operating%20in%20the%20real%20world%2C%20the%20safety-critical%20design%20often%20needs%0Ato%20explicitly%20address%20uncertainties%20and%20the%20pre-computed%20guarantees%20often%20rely%0Aon%20the%20assumption%20of%20the%20particular%20distribution%20of%20the%20uncertainty.%20However%2C%0Ait%20is%20difficult%20to%20characterize%20the%20actual%20uncertainty%20distribution%20beforehand%0Aand%20thus%20the%20established%20safety%20guarantee%20may%20be%20violated%20due%20to%20possible%0Adistribution%20mismatch.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20safe%20control%20framework%0Athat%20provides%20a%20high-probability%20safety%20guarantee%20for%20stochastic%20dynamical%0Asystems%20following%20unknown%20distributions%20of%20motion%20noise.%20Specifically%2C%20this%0Aframework%20adopts%20adaptive%20conformal%20prediction%20to%20dynamically%20quantify%20the%0Aprediction%20uncertainty%20from%20online%20observations%20and%20combines%20that%20with%20the%0Aprobabilistic%20extension%20of%20the%20control%20barrier%20functions%20%28CBFs%29%20to%20characterize%0Athe%20uncertainty-aware%20control%20constraints.%20By%20integrating%20the%20constraints%20in%0Athe%20model%20predictive%20control%20scheme%2C%20it%20allows%20robots%20to%20adaptively%20capture%20the%0Atrue%20prediction%20uncertainty%20online%20in%20a%20distribution-free%20setting%20and%20enjoys%0Aformally%20provable%20high-probability%20safety%20assurance.%20Simulation%20results%20on%0Amulti-robot%20systems%20with%20stochastic%20single-integrator%20dynamics%20and%20unicycle%0Adynamics%20are%20provided%20to%20demonstrate%20the%20effectiveness%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety-Critical%2520Control%2520with%2520Uncertainty%2520Quantification%2520using%2520Adaptive%250A%2520%2520Conformal%2520Prediction%26entry.906535625%3DHao%2520Zhou%2520and%2520Yanze%2520Zhang%2520and%2520Wenhao%2520Luo%26entry.1292438233%3D%2520%2520Safety%2520assurance%2520is%2520critical%2520in%2520the%2520planning%2520and%2520control%2520of%2520robotic%2520systems.%250AFor%2520robots%2520operating%2520in%2520the%2520real%2520world%252C%2520the%2520safety-critical%2520design%2520often%2520needs%250Ato%2520explicitly%2520address%2520uncertainties%2520and%2520the%2520pre-computed%2520guarantees%2520often%2520rely%250Aon%2520the%2520assumption%2520of%2520the%2520particular%2520distribution%2520of%2520the%2520uncertainty.%2520However%252C%250Ait%2520is%2520difficult%2520to%2520characterize%2520the%2520actual%2520uncertainty%2520distribution%2520beforehand%250Aand%2520thus%2520the%2520established%2520safety%2520guarantee%2520may%2520be%2520violated%2520due%2520to%2520possible%250Adistribution%2520mismatch.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520safe%2520control%2520framework%250Athat%2520provides%2520a%2520high-probability%2520safety%2520guarantee%2520for%2520stochastic%2520dynamical%250Asystems%2520following%2520unknown%2520distributions%2520of%2520motion%2520noise.%2520Specifically%252C%2520this%250Aframework%2520adopts%2520adaptive%2520conformal%2520prediction%2520to%2520dynamically%2520quantify%2520the%250Aprediction%2520uncertainty%2520from%2520online%2520observations%2520and%2520combines%2520that%2520with%2520the%250Aprobabilistic%2520extension%2520of%2520the%2520control%2520barrier%2520functions%2520%2528CBFs%2529%2520to%2520characterize%250Athe%2520uncertainty-aware%2520control%2520constraints.%2520By%2520integrating%2520the%2520constraints%2520in%250Athe%2520model%2520predictive%2520control%2520scheme%252C%2520it%2520allows%2520robots%2520to%2520adaptively%2520capture%2520the%250Atrue%2520prediction%2520uncertainty%2520online%2520in%2520a%2520distribution-free%2520setting%2520and%2520enjoys%250Aformally%2520provable%2520high-probability%2520safety%2520assurance.%2520Simulation%2520results%2520on%250Amulti-robot%2520systems%2520with%2520stochastic%2520single-integrator%2520dynamics%2520and%2520unicycle%250Adynamics%2520are%2520provided%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety-Critical%20Control%20with%20Uncertainty%20Quantification%20using%20Adaptive%0A%20%20Conformal%20Prediction&entry.906535625=Hao%20Zhou%20and%20Yanze%20Zhang%20and%20Wenhao%20Luo&entry.1292438233=%20%20Safety%20assurance%20is%20critical%20in%20the%20planning%20and%20control%20of%20robotic%20systems.%0AFor%20robots%20operating%20in%20the%20real%20world%2C%20the%20safety-critical%20design%20often%20needs%0Ato%20explicitly%20address%20uncertainties%20and%20the%20pre-computed%20guarantees%20often%20rely%0Aon%20the%20assumption%20of%20the%20particular%20distribution%20of%20the%20uncertainty.%20However%2C%0Ait%20is%20difficult%20to%20characterize%20the%20actual%20uncertainty%20distribution%20beforehand%0Aand%20thus%20the%20established%20safety%20guarantee%20may%20be%20violated%20due%20to%20possible%0Adistribution%20mismatch.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20safe%20control%20framework%0Athat%20provides%20a%20high-probability%20safety%20guarantee%20for%20stochastic%20dynamical%0Asystems%20following%20unknown%20distributions%20of%20motion%20noise.%20Specifically%2C%20this%0Aframework%20adopts%20adaptive%20conformal%20prediction%20to%20dynamically%20quantify%20the%0Aprediction%20uncertainty%20from%20online%20observations%20and%20combines%20that%20with%20the%0Aprobabilistic%20extension%20of%20the%20control%20barrier%20functions%20%28CBFs%29%20to%20characterize%0Athe%20uncertainty-aware%20control%20constraints.%20By%20integrating%20the%20constraints%20in%0Athe%20model%20predictive%20control%20scheme%2C%20it%20allows%20robots%20to%20adaptively%20capture%20the%0Atrue%20prediction%20uncertainty%20online%20in%20a%20distribution-free%20setting%20and%20enjoys%0Aformally%20provable%20high-probability%20safety%20assurance.%20Simulation%20results%20on%0Amulti-robot%20systems%20with%20stochastic%20single-integrator%20dynamics%20and%20unicycle%0Adynamics%20are%20provided%20to%20demonstrate%20the%20effectiveness%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03569v2&entry.124074799=Read"},
{"title": "Improving Adversarial Transferability of Vision-Language Pre-training\n  Models through Collaborative Multimodal Interaction", "author": "Jiyuan Fu and Zhaoyu Chen and Kaixun Jiang and Haijing Guo and Jiafeng Wang and Shuyong Gao and Wenqiang Zhang", "abstract": "  Despite the substantial advancements in Vision-Language Pre-training (VLP)\nmodels, their susceptibility to adversarial attacks poses a significant\nchallenge. Existing work rarely studies the transferability of attacks on VLP\nmodels, resulting in a substantial performance gap from white-box attacks. We\nobserve that prior work overlooks the interaction mechanisms between\nmodalities, which plays a crucial role in understanding the intricacies of VLP\nmodels. In response, we propose a novel attack, called Collaborative Multimodal\nInteraction Attack (CMI-Attack), leveraging modality interaction through\nembedding guidance and interaction enhancement. Specifically, attacking text at\nthe embedding level while preserving semantics, as well as utilizing\ninteraction image gradients to enhance constraints on perturbations of texts\nand images. Significantly, in the image-text retrieval task on Flickr30K\ndataset, CMI-Attack raises the transfer success rates from ALBEF to TCL,\n$\\text{CLIP}_{\\text{ViT}}$ and $\\text{CLIP}_{\\text{CNN}}$ by 8.11%-16.75% over\nstate-of-the-art methods. Moreover, CMI-Attack also demonstrates superior\nperformance in cross-task generalization scenarios. Our work addresses the\nunderexplored realm of transfer attacks on VLP models, shedding light on the\nimportance of modality interaction for enhanced adversarial robustness.\n", "link": "http://arxiv.org/abs/2403.10883v2", "date": "2024-07-08", "relevancy": 2.1991, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6079}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5244}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Adversarial%20Transferability%20of%20Vision-Language%20Pre-training%0A%20%20Models%20through%20Collaborative%20Multimodal%20Interaction&body=Title%3A%20Improving%20Adversarial%20Transferability%20of%20Vision-Language%20Pre-training%0A%20%20Models%20through%20Collaborative%20Multimodal%20Interaction%0AAuthor%3A%20Jiyuan%20Fu%20and%20Zhaoyu%20Chen%20and%20Kaixun%20Jiang%20and%20Haijing%20Guo%20and%20Jiafeng%20Wang%20and%20Shuyong%20Gao%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20Despite%20the%20substantial%20advancements%20in%20Vision-Language%20Pre-training%20%28VLP%29%0Amodels%2C%20their%20susceptibility%20to%20adversarial%20attacks%20poses%20a%20significant%0Achallenge.%20Existing%20work%20rarely%20studies%20the%20transferability%20of%20attacks%20on%20VLP%0Amodels%2C%20resulting%20in%20a%20substantial%20performance%20gap%20from%20white-box%20attacks.%20We%0Aobserve%20that%20prior%20work%20overlooks%20the%20interaction%20mechanisms%20between%0Amodalities%2C%20which%20plays%20a%20crucial%20role%20in%20understanding%20the%20intricacies%20of%20VLP%0Amodels.%20In%20response%2C%20we%20propose%20a%20novel%20attack%2C%20called%20Collaborative%20Multimodal%0AInteraction%20Attack%20%28CMI-Attack%29%2C%20leveraging%20modality%20interaction%20through%0Aembedding%20guidance%20and%20interaction%20enhancement.%20Specifically%2C%20attacking%20text%20at%0Athe%20embedding%20level%20while%20preserving%20semantics%2C%20as%20well%20as%20utilizing%0Ainteraction%20image%20gradients%20to%20enhance%20constraints%20on%20perturbations%20of%20texts%0Aand%20images.%20Significantly%2C%20in%20the%20image-text%20retrieval%20task%20on%20Flickr30K%0Adataset%2C%20CMI-Attack%20raises%20the%20transfer%20success%20rates%20from%20ALBEF%20to%20TCL%2C%0A%24%5Ctext%7BCLIP%7D_%7B%5Ctext%7BViT%7D%7D%24%20and%20%24%5Ctext%7BCLIP%7D_%7B%5Ctext%7BCNN%7D%7D%24%20by%208.11%25-16.75%25%20over%0Astate-of-the-art%20methods.%20Moreover%2C%20CMI-Attack%20also%20demonstrates%20superior%0Aperformance%20in%20cross-task%20generalization%20scenarios.%20Our%20work%20addresses%20the%0Aunderexplored%20realm%20of%20transfer%20attacks%20on%20VLP%20models%2C%20shedding%20light%20on%20the%0Aimportance%20of%20modality%20interaction%20for%20enhanced%20adversarial%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Adversarial%2520Transferability%2520of%2520Vision-Language%2520Pre-training%250A%2520%2520Models%2520through%2520Collaborative%2520Multimodal%2520Interaction%26entry.906535625%3DJiyuan%2520Fu%2520and%2520Zhaoyu%2520Chen%2520and%2520Kaixun%2520Jiang%2520and%2520Haijing%2520Guo%2520and%2520Jiafeng%2520Wang%2520and%2520Shuyong%2520Gao%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520the%2520substantial%2520advancements%2520in%2520Vision-Language%2520Pre-training%2520%2528VLP%2529%250Amodels%252C%2520their%2520susceptibility%2520to%2520adversarial%2520attacks%2520poses%2520a%2520significant%250Achallenge.%2520Existing%2520work%2520rarely%2520studies%2520the%2520transferability%2520of%2520attacks%2520on%2520VLP%250Amodels%252C%2520resulting%2520in%2520a%2520substantial%2520performance%2520gap%2520from%2520white-box%2520attacks.%2520We%250Aobserve%2520that%2520prior%2520work%2520overlooks%2520the%2520interaction%2520mechanisms%2520between%250Amodalities%252C%2520which%2520plays%2520a%2520crucial%2520role%2520in%2520understanding%2520the%2520intricacies%2520of%2520VLP%250Amodels.%2520In%2520response%252C%2520we%2520propose%2520a%2520novel%2520attack%252C%2520called%2520Collaborative%2520Multimodal%250AInteraction%2520Attack%2520%2528CMI-Attack%2529%252C%2520leveraging%2520modality%2520interaction%2520through%250Aembedding%2520guidance%2520and%2520interaction%2520enhancement.%2520Specifically%252C%2520attacking%2520text%2520at%250Athe%2520embedding%2520level%2520while%2520preserving%2520semantics%252C%2520as%2520well%2520as%2520utilizing%250Ainteraction%2520image%2520gradients%2520to%2520enhance%2520constraints%2520on%2520perturbations%2520of%2520texts%250Aand%2520images.%2520Significantly%252C%2520in%2520the%2520image-text%2520retrieval%2520task%2520on%2520Flickr30K%250Adataset%252C%2520CMI-Attack%2520raises%2520the%2520transfer%2520success%2520rates%2520from%2520ALBEF%2520to%2520TCL%252C%250A%2524%255Ctext%257BCLIP%257D_%257B%255Ctext%257BViT%257D%257D%2524%2520and%2520%2524%255Ctext%257BCLIP%257D_%257B%255Ctext%257BCNN%257D%257D%2524%2520by%25208.11%2525-16.75%2525%2520over%250Astate-of-the-art%2520methods.%2520Moreover%252C%2520CMI-Attack%2520also%2520demonstrates%2520superior%250Aperformance%2520in%2520cross-task%2520generalization%2520scenarios.%2520Our%2520work%2520addresses%2520the%250Aunderexplored%2520realm%2520of%2520transfer%2520attacks%2520on%2520VLP%2520models%252C%2520shedding%2520light%2520on%2520the%250Aimportance%2520of%2520modality%2520interaction%2520for%2520enhanced%2520adversarial%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Adversarial%20Transferability%20of%20Vision-Language%20Pre-training%0A%20%20Models%20through%20Collaborative%20Multimodal%20Interaction&entry.906535625=Jiyuan%20Fu%20and%20Zhaoyu%20Chen%20and%20Kaixun%20Jiang%20and%20Haijing%20Guo%20and%20Jiafeng%20Wang%20and%20Shuyong%20Gao%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Despite%20the%20substantial%20advancements%20in%20Vision-Language%20Pre-training%20%28VLP%29%0Amodels%2C%20their%20susceptibility%20to%20adversarial%20attacks%20poses%20a%20significant%0Achallenge.%20Existing%20work%20rarely%20studies%20the%20transferability%20of%20attacks%20on%20VLP%0Amodels%2C%20resulting%20in%20a%20substantial%20performance%20gap%20from%20white-box%20attacks.%20We%0Aobserve%20that%20prior%20work%20overlooks%20the%20interaction%20mechanisms%20between%0Amodalities%2C%20which%20plays%20a%20crucial%20role%20in%20understanding%20the%20intricacies%20of%20VLP%0Amodels.%20In%20response%2C%20we%20propose%20a%20novel%20attack%2C%20called%20Collaborative%20Multimodal%0AInteraction%20Attack%20%28CMI-Attack%29%2C%20leveraging%20modality%20interaction%20through%0Aembedding%20guidance%20and%20interaction%20enhancement.%20Specifically%2C%20attacking%20text%20at%0Athe%20embedding%20level%20while%20preserving%20semantics%2C%20as%20well%20as%20utilizing%0Ainteraction%20image%20gradients%20to%20enhance%20constraints%20on%20perturbations%20of%20texts%0Aand%20images.%20Significantly%2C%20in%20the%20image-text%20retrieval%20task%20on%20Flickr30K%0Adataset%2C%20CMI-Attack%20raises%20the%20transfer%20success%20rates%20from%20ALBEF%20to%20TCL%2C%0A%24%5Ctext%7BCLIP%7D_%7B%5Ctext%7BViT%7D%7D%24%20and%20%24%5Ctext%7BCLIP%7D_%7B%5Ctext%7BCNN%7D%7D%24%20by%208.11%25-16.75%25%20over%0Astate-of-the-art%20methods.%20Moreover%2C%20CMI-Attack%20also%20demonstrates%20superior%0Aperformance%20in%20cross-task%20generalization%20scenarios.%20Our%20work%20addresses%20the%0Aunderexplored%20realm%20of%20transfer%20attacks%20on%20VLP%20models%2C%20shedding%20light%20on%20the%0Aimportance%20of%20modality%20interaction%20for%20enhanced%20adversarial%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10883v2&entry.124074799=Read"},
{"title": "Tiny Models are the Computational Saver for Large Models", "author": "Qingyuan Wang and Barry Cardiff and Antoine Frapp\u00e9 and Benoit Larras and Deepu John", "abstract": "  This paper introduces TinySaver, an early-exit-like dynamic model compression\napproach which employs tiny models to substitute large models adaptively.\nDistinct from traditional compression techniques, dynamic methods like\nTinySaver can leverage the difficulty differences to allow certain inputs to\ncomplete their inference processes early, thereby conserving computational\nresources. Most existing early exit designs are implemented by attaching\nadditional network branches to the model's backbone. Our study, however,\nreveals that completely independent tiny models can replace a substantial\nportion of the larger models' job with minimal impact on performance. Employing\nthem as the first exit can remarkably enhance computational efficiency. By\nsearching and employing the most appropriate tiny model as the computational\nsaver for a given large model, the proposed approaches work as a novel and\ngeneric method to model compression. This finding will help the research\ncommunity in exploring new compression methods to address the escalating\ncomputational demands posed by rapidly evolving AI models. Our evaluation of\nthis approach in ImageNet-1k classification demonstrates its potential to\nreduce the number of compute operations by up to 90\\%, with only negligible\nlosses in performance, across various modern vision models.\n", "link": "http://arxiv.org/abs/2403.17726v2", "date": "2024-07-08", "relevancy": 2.1814, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5681}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiny%20Models%20are%20the%20Computational%20Saver%20for%20Large%20Models&body=Title%3A%20Tiny%20Models%20are%20the%20Computational%20Saver%20for%20Large%20Models%0AAuthor%3A%20Qingyuan%20Wang%20and%20Barry%20Cardiff%20and%20Antoine%20Frapp%C3%A9%20and%20Benoit%20Larras%20and%20Deepu%20John%0AAbstract%3A%20%20%20This%20paper%20introduces%20TinySaver%2C%20an%20early-exit-like%20dynamic%20model%20compression%0Aapproach%20which%20employs%20tiny%20models%20to%20substitute%20large%20models%20adaptively.%0ADistinct%20from%20traditional%20compression%20techniques%2C%20dynamic%20methods%20like%0ATinySaver%20can%20leverage%20the%20difficulty%20differences%20to%20allow%20certain%20inputs%20to%0Acomplete%20their%20inference%20processes%20early%2C%20thereby%20conserving%20computational%0Aresources.%20Most%20existing%20early%20exit%20designs%20are%20implemented%20by%20attaching%0Aadditional%20network%20branches%20to%20the%20model%27s%20backbone.%20Our%20study%2C%20however%2C%0Areveals%20that%20completely%20independent%20tiny%20models%20can%20replace%20a%20substantial%0Aportion%20of%20the%20larger%20models%27%20job%20with%20minimal%20impact%20on%20performance.%20Employing%0Athem%20as%20the%20first%20exit%20can%20remarkably%20enhance%20computational%20efficiency.%20By%0Asearching%20and%20employing%20the%20most%20appropriate%20tiny%20model%20as%20the%20computational%0Asaver%20for%20a%20given%20large%20model%2C%20the%20proposed%20approaches%20work%20as%20a%20novel%20and%0Ageneric%20method%20to%20model%20compression.%20This%20finding%20will%20help%20the%20research%0Acommunity%20in%20exploring%20new%20compression%20methods%20to%20address%20the%20escalating%0Acomputational%20demands%20posed%20by%20rapidly%20evolving%20AI%20models.%20Our%20evaluation%20of%0Athis%20approach%20in%20ImageNet-1k%20classification%20demonstrates%20its%20potential%20to%0Areduce%20the%20number%20of%20compute%20operations%20by%20up%20to%2090%5C%25%2C%20with%20only%20negligible%0Alosses%20in%20performance%2C%20across%20various%20modern%20vision%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17726v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiny%2520Models%2520are%2520the%2520Computational%2520Saver%2520for%2520Large%2520Models%26entry.906535625%3DQingyuan%2520Wang%2520and%2520Barry%2520Cardiff%2520and%2520Antoine%2520Frapp%25C3%25A9%2520and%2520Benoit%2520Larras%2520and%2520Deepu%2520John%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520TinySaver%252C%2520an%2520early-exit-like%2520dynamic%2520model%2520compression%250Aapproach%2520which%2520employs%2520tiny%2520models%2520to%2520substitute%2520large%2520models%2520adaptively.%250ADistinct%2520from%2520traditional%2520compression%2520techniques%252C%2520dynamic%2520methods%2520like%250ATinySaver%2520can%2520leverage%2520the%2520difficulty%2520differences%2520to%2520allow%2520certain%2520inputs%2520to%250Acomplete%2520their%2520inference%2520processes%2520early%252C%2520thereby%2520conserving%2520computational%250Aresources.%2520Most%2520existing%2520early%2520exit%2520designs%2520are%2520implemented%2520by%2520attaching%250Aadditional%2520network%2520branches%2520to%2520the%2520model%2527s%2520backbone.%2520Our%2520study%252C%2520however%252C%250Areveals%2520that%2520completely%2520independent%2520tiny%2520models%2520can%2520replace%2520a%2520substantial%250Aportion%2520of%2520the%2520larger%2520models%2527%2520job%2520with%2520minimal%2520impact%2520on%2520performance.%2520Employing%250Athem%2520as%2520the%2520first%2520exit%2520can%2520remarkably%2520enhance%2520computational%2520efficiency.%2520By%250Asearching%2520and%2520employing%2520the%2520most%2520appropriate%2520tiny%2520model%2520as%2520the%2520computational%250Asaver%2520for%2520a%2520given%2520large%2520model%252C%2520the%2520proposed%2520approaches%2520work%2520as%2520a%2520novel%2520and%250Ageneric%2520method%2520to%2520model%2520compression.%2520This%2520finding%2520will%2520help%2520the%2520research%250Acommunity%2520in%2520exploring%2520new%2520compression%2520methods%2520to%2520address%2520the%2520escalating%250Acomputational%2520demands%2520posed%2520by%2520rapidly%2520evolving%2520AI%2520models.%2520Our%2520evaluation%2520of%250Athis%2520approach%2520in%2520ImageNet-1k%2520classification%2520demonstrates%2520its%2520potential%2520to%250Areduce%2520the%2520number%2520of%2520compute%2520operations%2520by%2520up%2520to%252090%255C%2525%252C%2520with%2520only%2520negligible%250Alosses%2520in%2520performance%252C%2520across%2520various%2520modern%2520vision%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17726v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiny%20Models%20are%20the%20Computational%20Saver%20for%20Large%20Models&entry.906535625=Qingyuan%20Wang%20and%20Barry%20Cardiff%20and%20Antoine%20Frapp%C3%A9%20and%20Benoit%20Larras%20and%20Deepu%20John&entry.1292438233=%20%20This%20paper%20introduces%20TinySaver%2C%20an%20early-exit-like%20dynamic%20model%20compression%0Aapproach%20which%20employs%20tiny%20models%20to%20substitute%20large%20models%20adaptively.%0ADistinct%20from%20traditional%20compression%20techniques%2C%20dynamic%20methods%20like%0ATinySaver%20can%20leverage%20the%20difficulty%20differences%20to%20allow%20certain%20inputs%20to%0Acomplete%20their%20inference%20processes%20early%2C%20thereby%20conserving%20computational%0Aresources.%20Most%20existing%20early%20exit%20designs%20are%20implemented%20by%20attaching%0Aadditional%20network%20branches%20to%20the%20model%27s%20backbone.%20Our%20study%2C%20however%2C%0Areveals%20that%20completely%20independent%20tiny%20models%20can%20replace%20a%20substantial%0Aportion%20of%20the%20larger%20models%27%20job%20with%20minimal%20impact%20on%20performance.%20Employing%0Athem%20as%20the%20first%20exit%20can%20remarkably%20enhance%20computational%20efficiency.%20By%0Asearching%20and%20employing%20the%20most%20appropriate%20tiny%20model%20as%20the%20computational%0Asaver%20for%20a%20given%20large%20model%2C%20the%20proposed%20approaches%20work%20as%20a%20novel%20and%0Ageneric%20method%20to%20model%20compression.%20This%20finding%20will%20help%20the%20research%0Acommunity%20in%20exploring%20new%20compression%20methods%20to%20address%20the%20escalating%0Acomputational%20demands%20posed%20by%20rapidly%20evolving%20AI%20models.%20Our%20evaluation%20of%0Athis%20approach%20in%20ImageNet-1k%20classification%20demonstrates%20its%20potential%20to%0Areduce%20the%20number%20of%20compute%20operations%20by%20up%20to%2090%5C%25%2C%20with%20only%20negligible%0Alosses%20in%20performance%2C%20across%20various%20modern%20vision%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17726v2&entry.124074799=Read"},
{"title": "D\u03b5pS: Delayed \u03b5-Shrinking for Faster Once-For-All\n  Training", "author": "Aditya Annavajjala and Alind Khare and Animesh Agrawal and Igor Fedorov and Hugo Latapie and Myungjin Lee and Alexey Tumanov", "abstract": "  CNNs are increasingly deployed across different hardware, dynamic\nenvironments, and low-power embedded devices. This has led to the design and\ntraining of CNN architectures with the goal of maximizing accuracy subject to\nsuch variable deployment constraints. As the number of deployment scenarios\ngrows, there is a need to find scalable solutions to design and train\nspecialized CNNs. Once-for-all training has emerged as a scalable approach that\njointly co-trains many models (subnets) at once with a constant training cost\nand finds specialized CNNs later. The scalability is achieved by training the\nfull model and simultaneously reducing it to smaller subnets that share model\nweights (weight-shared shrinking). However, existing once-for-all training\napproaches incur huge training costs reaching 1200 GPU hours. We argue this is\nbecause they either start the process of shrinking the full model too early or\ntoo late. Hence, we propose Delayed $\\epsilon$-Shrinking (D$\\epsilon$pS) that\nstarts the process of shrinking the full model when it is partially trained\n(~50%) which leads to training cost improvement and better in-place knowledge\ndistillation to smaller models. The proposed approach also consists of novel\nheuristics that dynamically adjust subnet learning rates incrementally (E),\nleading to improved weight-shared knowledge distillation from larger to smaller\nsubnets as well. As a result, DEpS outperforms state-of-the-art once-for-all\ntraining techniques across different datasets including CIFAR10/100,\nImageNet-100, and ImageNet-1k on accuracy and cost. It achieves 1.83% higher\nImageNet-1k top1 accuracy or the same accuracy with 1.3x reduction in FLOPs and\n2.5x drop in training cost (GPU*hrs)\n", "link": "http://arxiv.org/abs/2407.06167v1", "date": "2024-07-08", "relevancy": 2.1782, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6057}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5437}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D%CE%B5pS%3A%20Delayed%20%CE%B5-Shrinking%20for%20Faster%20Once-For-All%0A%20%20Training&body=Title%3A%20D%CE%B5pS%3A%20Delayed%20%CE%B5-Shrinking%20for%20Faster%20Once-For-All%0A%20%20Training%0AAuthor%3A%20Aditya%20Annavajjala%20and%20Alind%20Khare%20and%20Animesh%20Agrawal%20and%20Igor%20Fedorov%20and%20Hugo%20Latapie%20and%20Myungjin%20Lee%20and%20Alexey%20Tumanov%0AAbstract%3A%20%20%20CNNs%20are%20increasingly%20deployed%20across%20different%20hardware%2C%20dynamic%0Aenvironments%2C%20and%20low-power%20embedded%20devices.%20This%20has%20led%20to%20the%20design%20and%0Atraining%20of%20CNN%20architectures%20with%20the%20goal%20of%20maximizing%20accuracy%20subject%20to%0Asuch%20variable%20deployment%20constraints.%20As%20the%20number%20of%20deployment%20scenarios%0Agrows%2C%20there%20is%20a%20need%20to%20find%20scalable%20solutions%20to%20design%20and%20train%0Aspecialized%20CNNs.%20Once-for-all%20training%20has%20emerged%20as%20a%20scalable%20approach%20that%0Ajointly%20co-trains%20many%20models%20%28subnets%29%20at%20once%20with%20a%20constant%20training%20cost%0Aand%20finds%20specialized%20CNNs%20later.%20The%20scalability%20is%20achieved%20by%20training%20the%0Afull%20model%20and%20simultaneously%20reducing%20it%20to%20smaller%20subnets%20that%20share%20model%0Aweights%20%28weight-shared%20shrinking%29.%20However%2C%20existing%20once-for-all%20training%0Aapproaches%20incur%20huge%20training%20costs%20reaching%201200%20GPU%20hours.%20We%20argue%20this%20is%0Abecause%20they%20either%20start%20the%20process%20of%20shrinking%20the%20full%20model%20too%20early%20or%0Atoo%20late.%20Hence%2C%20we%20propose%20Delayed%20%24%5Cepsilon%24-Shrinking%20%28D%24%5Cepsilon%24pS%29%20that%0Astarts%20the%20process%20of%20shrinking%20the%20full%20model%20when%20it%20is%20partially%20trained%0A%28~50%25%29%20which%20leads%20to%20training%20cost%20improvement%20and%20better%20in-place%20knowledge%0Adistillation%20to%20smaller%20models.%20The%20proposed%20approach%20also%20consists%20of%20novel%0Aheuristics%20that%20dynamically%20adjust%20subnet%20learning%20rates%20incrementally%20%28E%29%2C%0Aleading%20to%20improved%20weight-shared%20knowledge%20distillation%20from%20larger%20to%20smaller%0Asubnets%20as%20well.%20As%20a%20result%2C%20DEpS%20outperforms%20state-of-the-art%20once-for-all%0Atraining%20techniques%20across%20different%20datasets%20including%20CIFAR10/100%2C%0AImageNet-100%2C%20and%20ImageNet-1k%20on%20accuracy%20and%20cost.%20It%20achieves%201.83%25%20higher%0AImageNet-1k%20top1%20accuracy%20or%20the%20same%20accuracy%20with%201.3x%20reduction%20in%20FLOPs%20and%0A2.5x%20drop%20in%20training%20cost%20%28GPU%2Ahrs%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD%25CE%25B5pS%253A%2520Delayed%2520%25CE%25B5-Shrinking%2520for%2520Faster%2520Once-For-All%250A%2520%2520Training%26entry.906535625%3DAditya%2520Annavajjala%2520and%2520Alind%2520Khare%2520and%2520Animesh%2520Agrawal%2520and%2520Igor%2520Fedorov%2520and%2520Hugo%2520Latapie%2520and%2520Myungjin%2520Lee%2520and%2520Alexey%2520Tumanov%26entry.1292438233%3D%2520%2520CNNs%2520are%2520increasingly%2520deployed%2520across%2520different%2520hardware%252C%2520dynamic%250Aenvironments%252C%2520and%2520low-power%2520embedded%2520devices.%2520This%2520has%2520led%2520to%2520the%2520design%2520and%250Atraining%2520of%2520CNN%2520architectures%2520with%2520the%2520goal%2520of%2520maximizing%2520accuracy%2520subject%2520to%250Asuch%2520variable%2520deployment%2520constraints.%2520As%2520the%2520number%2520of%2520deployment%2520scenarios%250Agrows%252C%2520there%2520is%2520a%2520need%2520to%2520find%2520scalable%2520solutions%2520to%2520design%2520and%2520train%250Aspecialized%2520CNNs.%2520Once-for-all%2520training%2520has%2520emerged%2520as%2520a%2520scalable%2520approach%2520that%250Ajointly%2520co-trains%2520many%2520models%2520%2528subnets%2529%2520at%2520once%2520with%2520a%2520constant%2520training%2520cost%250Aand%2520finds%2520specialized%2520CNNs%2520later.%2520The%2520scalability%2520is%2520achieved%2520by%2520training%2520the%250Afull%2520model%2520and%2520simultaneously%2520reducing%2520it%2520to%2520smaller%2520subnets%2520that%2520share%2520model%250Aweights%2520%2528weight-shared%2520shrinking%2529.%2520However%252C%2520existing%2520once-for-all%2520training%250Aapproaches%2520incur%2520huge%2520training%2520costs%2520reaching%25201200%2520GPU%2520hours.%2520We%2520argue%2520this%2520is%250Abecause%2520they%2520either%2520start%2520the%2520process%2520of%2520shrinking%2520the%2520full%2520model%2520too%2520early%2520or%250Atoo%2520late.%2520Hence%252C%2520we%2520propose%2520Delayed%2520%2524%255Cepsilon%2524-Shrinking%2520%2528D%2524%255Cepsilon%2524pS%2529%2520that%250Astarts%2520the%2520process%2520of%2520shrinking%2520the%2520full%2520model%2520when%2520it%2520is%2520partially%2520trained%250A%2528~50%2525%2529%2520which%2520leads%2520to%2520training%2520cost%2520improvement%2520and%2520better%2520in-place%2520knowledge%250Adistillation%2520to%2520smaller%2520models.%2520The%2520proposed%2520approach%2520also%2520consists%2520of%2520novel%250Aheuristics%2520that%2520dynamically%2520adjust%2520subnet%2520learning%2520rates%2520incrementally%2520%2528E%2529%252C%250Aleading%2520to%2520improved%2520weight-shared%2520knowledge%2520distillation%2520from%2520larger%2520to%2520smaller%250Asubnets%2520as%2520well.%2520As%2520a%2520result%252C%2520DEpS%2520outperforms%2520state-of-the-art%2520once-for-all%250Atraining%2520techniques%2520across%2520different%2520datasets%2520including%2520CIFAR10/100%252C%250AImageNet-100%252C%2520and%2520ImageNet-1k%2520on%2520accuracy%2520and%2520cost.%2520It%2520achieves%25201.83%2525%2520higher%250AImageNet-1k%2520top1%2520accuracy%2520or%2520the%2520same%2520accuracy%2520with%25201.3x%2520reduction%2520in%2520FLOPs%2520and%250A2.5x%2520drop%2520in%2520training%2520cost%2520%2528GPU%252Ahrs%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D%CE%B5pS%3A%20Delayed%20%CE%B5-Shrinking%20for%20Faster%20Once-For-All%0A%20%20Training&entry.906535625=Aditya%20Annavajjala%20and%20Alind%20Khare%20and%20Animesh%20Agrawal%20and%20Igor%20Fedorov%20and%20Hugo%20Latapie%20and%20Myungjin%20Lee%20and%20Alexey%20Tumanov&entry.1292438233=%20%20CNNs%20are%20increasingly%20deployed%20across%20different%20hardware%2C%20dynamic%0Aenvironments%2C%20and%20low-power%20embedded%20devices.%20This%20has%20led%20to%20the%20design%20and%0Atraining%20of%20CNN%20architectures%20with%20the%20goal%20of%20maximizing%20accuracy%20subject%20to%0Asuch%20variable%20deployment%20constraints.%20As%20the%20number%20of%20deployment%20scenarios%0Agrows%2C%20there%20is%20a%20need%20to%20find%20scalable%20solutions%20to%20design%20and%20train%0Aspecialized%20CNNs.%20Once-for-all%20training%20has%20emerged%20as%20a%20scalable%20approach%20that%0Ajointly%20co-trains%20many%20models%20%28subnets%29%20at%20once%20with%20a%20constant%20training%20cost%0Aand%20finds%20specialized%20CNNs%20later.%20The%20scalability%20is%20achieved%20by%20training%20the%0Afull%20model%20and%20simultaneously%20reducing%20it%20to%20smaller%20subnets%20that%20share%20model%0Aweights%20%28weight-shared%20shrinking%29.%20However%2C%20existing%20once-for-all%20training%0Aapproaches%20incur%20huge%20training%20costs%20reaching%201200%20GPU%20hours.%20We%20argue%20this%20is%0Abecause%20they%20either%20start%20the%20process%20of%20shrinking%20the%20full%20model%20too%20early%20or%0Atoo%20late.%20Hence%2C%20we%20propose%20Delayed%20%24%5Cepsilon%24-Shrinking%20%28D%24%5Cepsilon%24pS%29%20that%0Astarts%20the%20process%20of%20shrinking%20the%20full%20model%20when%20it%20is%20partially%20trained%0A%28~50%25%29%20which%20leads%20to%20training%20cost%20improvement%20and%20better%20in-place%20knowledge%0Adistillation%20to%20smaller%20models.%20The%20proposed%20approach%20also%20consists%20of%20novel%0Aheuristics%20that%20dynamically%20adjust%20subnet%20learning%20rates%20incrementally%20%28E%29%2C%0Aleading%20to%20improved%20weight-shared%20knowledge%20distillation%20from%20larger%20to%20smaller%0Asubnets%20as%20well.%20As%20a%20result%2C%20DEpS%20outperforms%20state-of-the-art%20once-for-all%0Atraining%20techniques%20across%20different%20datasets%20including%20CIFAR10/100%2C%0AImageNet-100%2C%20and%20ImageNet-1k%20on%20accuracy%20and%20cost.%20It%20achieves%201.83%25%20higher%0AImageNet-1k%20top1%20accuracy%20or%20the%20same%20accuracy%20with%201.3x%20reduction%20in%20FLOPs%20and%0A2.5x%20drop%20in%20training%20cost%20%28GPU%2Ahrs%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06167v1&entry.124074799=Read"},
{"title": "Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in\n  Cross-domain 3D Object Detection", "author": "Ruixiao Zhang and Yihong Wu and Juheon Lee and Adam Prugel-Bennett and Xiaohao Cai", "abstract": "  The performance of domain adaptation technologies has not yet reached an\nideal level in the current 3D object detection field for autonomous driving,\nwhich is mainly due to significant differences in the size of vehicles, as well\nas the environments they operate in when applied across domains. These factors\ntogether hinder the effective transfer and application of knowledge learned\nfrom specific datasets. Since the existing evaluation metrics are initially\ndesigned for evaluation on a single domain by calculating the 2D or 3D overlap\nbetween the prediction and ground-truth bounding boxes, they often suffer from\nthe overfitting problem caused by the size differences among datasets. This\nraises a fundamental question related to the evaluation of the 3D object\ndetection models' cross-domain performance: Do we really need models to\nmaintain excellent performance in their original 3D bounding boxes after being\napplied across domains? From a practical application perspective, one of our\nmain focuses is actually on preventing collisions between vehicles and other\nobstacles, especially in cross-domain scenarios where correctly predicting the\nsize of vehicles is much more difficult. In other words, as long as a model can\naccurately identify the closest surfaces to the ego vehicle, it is sufficient\nto effectively avoid obstacles. In this paper, we propose two metrics to\nmeasure 3D object detection models' ability of detecting the closer surfaces to\nthe sensor on the ego vehicle, which can be used to evaluate their cross-domain\nperformance more comprehensively and reasonably. Furthermore, we propose a\nrefinement head, named EdgeHead, to guide models to focus more on the learnable\ncloser surfaces, which can greatly improve the cross-domain performance of\nexisting models not only under our new metrics, but even also under the\noriginal BEV/3D metrics.\n", "link": "http://arxiv.org/abs/2407.04061v2", "date": "2024-07-08", "relevancy": 2.173, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detect%20Closer%20Surfaces%20that%20can%20be%20Seen%3A%20New%20Modeling%20and%20Evaluation%20in%0A%20%20Cross-domain%203D%20Object%20Detection&body=Title%3A%20Detect%20Closer%20Surfaces%20that%20can%20be%20Seen%3A%20New%20Modeling%20and%20Evaluation%20in%0A%20%20Cross-domain%203D%20Object%20Detection%0AAuthor%3A%20Ruixiao%20Zhang%20and%20Yihong%20Wu%20and%20Juheon%20Lee%20and%20Adam%20Prugel-Bennett%20and%20Xiaohao%20Cai%0AAbstract%3A%20%20%20The%20performance%20of%20domain%20adaptation%20technologies%20has%20not%20yet%20reached%20an%0Aideal%20level%20in%20the%20current%203D%20object%20detection%20field%20for%20autonomous%20driving%2C%0Awhich%20is%20mainly%20due%20to%20significant%20differences%20in%20the%20size%20of%20vehicles%2C%20as%20well%0Aas%20the%20environments%20they%20operate%20in%20when%20applied%20across%20domains.%20These%20factors%0Atogether%20hinder%20the%20effective%20transfer%20and%20application%20of%20knowledge%20learned%0Afrom%20specific%20datasets.%20Since%20the%20existing%20evaluation%20metrics%20are%20initially%0Adesigned%20for%20evaluation%20on%20a%20single%20domain%20by%20calculating%20the%202D%20or%203D%20overlap%0Abetween%20the%20prediction%20and%20ground-truth%20bounding%20boxes%2C%20they%20often%20suffer%20from%0Athe%20overfitting%20problem%20caused%20by%20the%20size%20differences%20among%20datasets.%20This%0Araises%20a%20fundamental%20question%20related%20to%20the%20evaluation%20of%20the%203D%20object%0Adetection%20models%27%20cross-domain%20performance%3A%20Do%20we%20really%20need%20models%20to%0Amaintain%20excellent%20performance%20in%20their%20original%203D%20bounding%20boxes%20after%20being%0Aapplied%20across%20domains%3F%20From%20a%20practical%20application%20perspective%2C%20one%20of%20our%0Amain%20focuses%20is%20actually%20on%20preventing%20collisions%20between%20vehicles%20and%20other%0Aobstacles%2C%20especially%20in%20cross-domain%20scenarios%20where%20correctly%20predicting%20the%0Asize%20of%20vehicles%20is%20much%20more%20difficult.%20In%20other%20words%2C%20as%20long%20as%20a%20model%20can%0Aaccurately%20identify%20the%20closest%20surfaces%20to%20the%20ego%20vehicle%2C%20it%20is%20sufficient%0Ato%20effectively%20avoid%20obstacles.%20In%20this%20paper%2C%20we%20propose%20two%20metrics%20to%0Ameasure%203D%20object%20detection%20models%27%20ability%20of%20detecting%20the%20closer%20surfaces%20to%0Athe%20sensor%20on%20the%20ego%20vehicle%2C%20which%20can%20be%20used%20to%20evaluate%20their%20cross-domain%0Aperformance%20more%20comprehensively%20and%20reasonably.%20Furthermore%2C%20we%20propose%20a%0Arefinement%20head%2C%20named%20EdgeHead%2C%20to%20guide%20models%20to%20focus%20more%20on%20the%20learnable%0Acloser%20surfaces%2C%20which%20can%20greatly%20improve%20the%20cross-domain%20performance%20of%0Aexisting%20models%20not%20only%20under%20our%20new%20metrics%2C%20but%20even%20also%20under%20the%0Aoriginal%20BEV/3D%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetect%2520Closer%2520Surfaces%2520that%2520can%2520be%2520Seen%253A%2520New%2520Modeling%2520and%2520Evaluation%2520in%250A%2520%2520Cross-domain%25203D%2520Object%2520Detection%26entry.906535625%3DRuixiao%2520Zhang%2520and%2520Yihong%2520Wu%2520and%2520Juheon%2520Lee%2520and%2520Adam%2520Prugel-Bennett%2520and%2520Xiaohao%2520Cai%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520domain%2520adaptation%2520technologies%2520has%2520not%2520yet%2520reached%2520an%250Aideal%2520level%2520in%2520the%2520current%25203D%2520object%2520detection%2520field%2520for%2520autonomous%2520driving%252C%250Awhich%2520is%2520mainly%2520due%2520to%2520significant%2520differences%2520in%2520the%2520size%2520of%2520vehicles%252C%2520as%2520well%250Aas%2520the%2520environments%2520they%2520operate%2520in%2520when%2520applied%2520across%2520domains.%2520These%2520factors%250Atogether%2520hinder%2520the%2520effective%2520transfer%2520and%2520application%2520of%2520knowledge%2520learned%250Afrom%2520specific%2520datasets.%2520Since%2520the%2520existing%2520evaluation%2520metrics%2520are%2520initially%250Adesigned%2520for%2520evaluation%2520on%2520a%2520single%2520domain%2520by%2520calculating%2520the%25202D%2520or%25203D%2520overlap%250Abetween%2520the%2520prediction%2520and%2520ground-truth%2520bounding%2520boxes%252C%2520they%2520often%2520suffer%2520from%250Athe%2520overfitting%2520problem%2520caused%2520by%2520the%2520size%2520differences%2520among%2520datasets.%2520This%250Araises%2520a%2520fundamental%2520question%2520related%2520to%2520the%2520evaluation%2520of%2520the%25203D%2520object%250Adetection%2520models%2527%2520cross-domain%2520performance%253A%2520Do%2520we%2520really%2520need%2520models%2520to%250Amaintain%2520excellent%2520performance%2520in%2520their%2520original%25203D%2520bounding%2520boxes%2520after%2520being%250Aapplied%2520across%2520domains%253F%2520From%2520a%2520practical%2520application%2520perspective%252C%2520one%2520of%2520our%250Amain%2520focuses%2520is%2520actually%2520on%2520preventing%2520collisions%2520between%2520vehicles%2520and%2520other%250Aobstacles%252C%2520especially%2520in%2520cross-domain%2520scenarios%2520where%2520correctly%2520predicting%2520the%250Asize%2520of%2520vehicles%2520is%2520much%2520more%2520difficult.%2520In%2520other%2520words%252C%2520as%2520long%2520as%2520a%2520model%2520can%250Aaccurately%2520identify%2520the%2520closest%2520surfaces%2520to%2520the%2520ego%2520vehicle%252C%2520it%2520is%2520sufficient%250Ato%2520effectively%2520avoid%2520obstacles.%2520In%2520this%2520paper%252C%2520we%2520propose%2520two%2520metrics%2520to%250Ameasure%25203D%2520object%2520detection%2520models%2527%2520ability%2520of%2520detecting%2520the%2520closer%2520surfaces%2520to%250Athe%2520sensor%2520on%2520the%2520ego%2520vehicle%252C%2520which%2520can%2520be%2520used%2520to%2520evaluate%2520their%2520cross-domain%250Aperformance%2520more%2520comprehensively%2520and%2520reasonably.%2520Furthermore%252C%2520we%2520propose%2520a%250Arefinement%2520head%252C%2520named%2520EdgeHead%252C%2520to%2520guide%2520models%2520to%2520focus%2520more%2520on%2520the%2520learnable%250Acloser%2520surfaces%252C%2520which%2520can%2520greatly%2520improve%2520the%2520cross-domain%2520performance%2520of%250Aexisting%2520models%2520not%2520only%2520under%2520our%2520new%2520metrics%252C%2520but%2520even%2520also%2520under%2520the%250Aoriginal%2520BEV/3D%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detect%20Closer%20Surfaces%20that%20can%20be%20Seen%3A%20New%20Modeling%20and%20Evaluation%20in%0A%20%20Cross-domain%203D%20Object%20Detection&entry.906535625=Ruixiao%20Zhang%20and%20Yihong%20Wu%20and%20Juheon%20Lee%20and%20Adam%20Prugel-Bennett%20and%20Xiaohao%20Cai&entry.1292438233=%20%20The%20performance%20of%20domain%20adaptation%20technologies%20has%20not%20yet%20reached%20an%0Aideal%20level%20in%20the%20current%203D%20object%20detection%20field%20for%20autonomous%20driving%2C%0Awhich%20is%20mainly%20due%20to%20significant%20differences%20in%20the%20size%20of%20vehicles%2C%20as%20well%0Aas%20the%20environments%20they%20operate%20in%20when%20applied%20across%20domains.%20These%20factors%0Atogether%20hinder%20the%20effective%20transfer%20and%20application%20of%20knowledge%20learned%0Afrom%20specific%20datasets.%20Since%20the%20existing%20evaluation%20metrics%20are%20initially%0Adesigned%20for%20evaluation%20on%20a%20single%20domain%20by%20calculating%20the%202D%20or%203D%20overlap%0Abetween%20the%20prediction%20and%20ground-truth%20bounding%20boxes%2C%20they%20often%20suffer%20from%0Athe%20overfitting%20problem%20caused%20by%20the%20size%20differences%20among%20datasets.%20This%0Araises%20a%20fundamental%20question%20related%20to%20the%20evaluation%20of%20the%203D%20object%0Adetection%20models%27%20cross-domain%20performance%3A%20Do%20we%20really%20need%20models%20to%0Amaintain%20excellent%20performance%20in%20their%20original%203D%20bounding%20boxes%20after%20being%0Aapplied%20across%20domains%3F%20From%20a%20practical%20application%20perspective%2C%20one%20of%20our%0Amain%20focuses%20is%20actually%20on%20preventing%20collisions%20between%20vehicles%20and%20other%0Aobstacles%2C%20especially%20in%20cross-domain%20scenarios%20where%20correctly%20predicting%20the%0Asize%20of%20vehicles%20is%20much%20more%20difficult.%20In%20other%20words%2C%20as%20long%20as%20a%20model%20can%0Aaccurately%20identify%20the%20closest%20surfaces%20to%20the%20ego%20vehicle%2C%20it%20is%20sufficient%0Ato%20effectively%20avoid%20obstacles.%20In%20this%20paper%2C%20we%20propose%20two%20metrics%20to%0Ameasure%203D%20object%20detection%20models%27%20ability%20of%20detecting%20the%20closer%20surfaces%20to%0Athe%20sensor%20on%20the%20ego%20vehicle%2C%20which%20can%20be%20used%20to%20evaluate%20their%20cross-domain%0Aperformance%20more%20comprehensively%20and%20reasonably.%20Furthermore%2C%20we%20propose%20a%0Arefinement%20head%2C%20named%20EdgeHead%2C%20to%20guide%20models%20to%20focus%20more%20on%20the%20learnable%0Acloser%20surfaces%2C%20which%20can%20greatly%20improve%20the%20cross-domain%20performance%20of%0Aexisting%20models%20not%20only%20under%20our%20new%20metrics%2C%20but%20even%20also%20under%20the%0Aoriginal%20BEV/3D%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04061v2&entry.124074799=Read"},
{"title": "Occlusion-Free Image Based Visual Servoing using Probabilistic Control\n  Barrier Certificates", "author": "Yanze Zhang and Yupeng Yang and Wenhao Luo", "abstract": "  Image-based visual servoing (IBVS) is a widely-used approach in robotics that\nemploys visual information to guide robots towards desired positions. However,\nocclusions in this approach can lead to visual servoing failure and degrade the\ncontrol performance due to the obstructed vision feature points that are\nessential for providing visual feedback. In this paper, we propose a Control\nBarrier Function (CBF) based controller that enables occlusion-free IBVS tasks\nby automatically adjusting the robot's configuration to keep the feature points\nin the field of view and away from obstacles. In particular, to account for\nmeasurement noise of the feature points, we develop the Probabilistic Control\nBarrier Certificates (PrCBC) using control barrier functions that encode the\nchance-constrained occlusion avoidance constraints under uncertainty into\ndeterministic admissible control space for the robot, from which the resulting\nconfiguration of robot ensures that the feature points stay occlusion free from\nobstacles with a satisfying predefined probability. By integrating such\nconstraints with a Model Predictive Control (MPC) framework, the sequence of\noptimized control inputs can be derived to achieve the primary IBVS task while\nenforcing the occlusion avoidance during robot movements. Simulation results\nare provided to validate the performance of our proposed method.\n", "link": "http://arxiv.org/abs/2309.03476v2", "date": "2024-07-08", "relevancy": 2.1665, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5775}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5347}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occlusion-Free%20Image%20Based%20Visual%20Servoing%20using%20Probabilistic%20Control%0A%20%20Barrier%20Certificates&body=Title%3A%20Occlusion-Free%20Image%20Based%20Visual%20Servoing%20using%20Probabilistic%20Control%0A%20%20Barrier%20Certificates%0AAuthor%3A%20Yanze%20Zhang%20and%20Yupeng%20Yang%20and%20Wenhao%20Luo%0AAbstract%3A%20%20%20Image-based%20visual%20servoing%20%28IBVS%29%20is%20a%20widely-used%20approach%20in%20robotics%20that%0Aemploys%20visual%20information%20to%20guide%20robots%20towards%20desired%20positions.%20However%2C%0Aocclusions%20in%20this%20approach%20can%20lead%20to%20visual%20servoing%20failure%20and%20degrade%20the%0Acontrol%20performance%20due%20to%20the%20obstructed%20vision%20feature%20points%20that%20are%0Aessential%20for%20providing%20visual%20feedback.%20In%20this%20paper%2C%20we%20propose%20a%20Control%0ABarrier%20Function%20%28CBF%29%20based%20controller%20that%20enables%20occlusion-free%20IBVS%20tasks%0Aby%20automatically%20adjusting%20the%20robot%27s%20configuration%20to%20keep%20the%20feature%20points%0Ain%20the%20field%20of%20view%20and%20away%20from%20obstacles.%20In%20particular%2C%20to%20account%20for%0Ameasurement%20noise%20of%20the%20feature%20points%2C%20we%20develop%20the%20Probabilistic%20Control%0ABarrier%20Certificates%20%28PrCBC%29%20using%20control%20barrier%20functions%20that%20encode%20the%0Achance-constrained%20occlusion%20avoidance%20constraints%20under%20uncertainty%20into%0Adeterministic%20admissible%20control%20space%20for%20the%20robot%2C%20from%20which%20the%20resulting%0Aconfiguration%20of%20robot%20ensures%20that%20the%20feature%20points%20stay%20occlusion%20free%20from%0Aobstacles%20with%20a%20satisfying%20predefined%20probability.%20By%20integrating%20such%0Aconstraints%20with%20a%20Model%20Predictive%20Control%20%28MPC%29%20framework%2C%20the%20sequence%20of%0Aoptimized%20control%20inputs%20can%20be%20derived%20to%20achieve%20the%20primary%20IBVS%20task%20while%0Aenforcing%20the%20occlusion%20avoidance%20during%20robot%20movements.%20Simulation%20results%0Aare%20provided%20to%20validate%20the%20performance%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcclusion-Free%2520Image%2520Based%2520Visual%2520Servoing%2520using%2520Probabilistic%2520Control%250A%2520%2520Barrier%2520Certificates%26entry.906535625%3DYanze%2520Zhang%2520and%2520Yupeng%2520Yang%2520and%2520Wenhao%2520Luo%26entry.1292438233%3D%2520%2520Image-based%2520visual%2520servoing%2520%2528IBVS%2529%2520is%2520a%2520widely-used%2520approach%2520in%2520robotics%2520that%250Aemploys%2520visual%2520information%2520to%2520guide%2520robots%2520towards%2520desired%2520positions.%2520However%252C%250Aocclusions%2520in%2520this%2520approach%2520can%2520lead%2520to%2520visual%2520servoing%2520failure%2520and%2520degrade%2520the%250Acontrol%2520performance%2520due%2520to%2520the%2520obstructed%2520vision%2520feature%2520points%2520that%2520are%250Aessential%2520for%2520providing%2520visual%2520feedback.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Control%250ABarrier%2520Function%2520%2528CBF%2529%2520based%2520controller%2520that%2520enables%2520occlusion-free%2520IBVS%2520tasks%250Aby%2520automatically%2520adjusting%2520the%2520robot%2527s%2520configuration%2520to%2520keep%2520the%2520feature%2520points%250Ain%2520the%2520field%2520of%2520view%2520and%2520away%2520from%2520obstacles.%2520In%2520particular%252C%2520to%2520account%2520for%250Ameasurement%2520noise%2520of%2520the%2520feature%2520points%252C%2520we%2520develop%2520the%2520Probabilistic%2520Control%250ABarrier%2520Certificates%2520%2528PrCBC%2529%2520using%2520control%2520barrier%2520functions%2520that%2520encode%2520the%250Achance-constrained%2520occlusion%2520avoidance%2520constraints%2520under%2520uncertainty%2520into%250Adeterministic%2520admissible%2520control%2520space%2520for%2520the%2520robot%252C%2520from%2520which%2520the%2520resulting%250Aconfiguration%2520of%2520robot%2520ensures%2520that%2520the%2520feature%2520points%2520stay%2520occlusion%2520free%2520from%250Aobstacles%2520with%2520a%2520satisfying%2520predefined%2520probability.%2520By%2520integrating%2520such%250Aconstraints%2520with%2520a%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520framework%252C%2520the%2520sequence%2520of%250Aoptimized%2520control%2520inputs%2520can%2520be%2520derived%2520to%2520achieve%2520the%2520primary%2520IBVS%2520task%2520while%250Aenforcing%2520the%2520occlusion%2520avoidance%2520during%2520robot%2520movements.%2520Simulation%2520results%250Aare%2520provided%2520to%2520validate%2520the%2520performance%2520of%2520our%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.03476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occlusion-Free%20Image%20Based%20Visual%20Servoing%20using%20Probabilistic%20Control%0A%20%20Barrier%20Certificates&entry.906535625=Yanze%20Zhang%20and%20Yupeng%20Yang%20and%20Wenhao%20Luo&entry.1292438233=%20%20Image-based%20visual%20servoing%20%28IBVS%29%20is%20a%20widely-used%20approach%20in%20robotics%20that%0Aemploys%20visual%20information%20to%20guide%20robots%20towards%20desired%20positions.%20However%2C%0Aocclusions%20in%20this%20approach%20can%20lead%20to%20visual%20servoing%20failure%20and%20degrade%20the%0Acontrol%20performance%20due%20to%20the%20obstructed%20vision%20feature%20points%20that%20are%0Aessential%20for%20providing%20visual%20feedback.%20In%20this%20paper%2C%20we%20propose%20a%20Control%0ABarrier%20Function%20%28CBF%29%20based%20controller%20that%20enables%20occlusion-free%20IBVS%20tasks%0Aby%20automatically%20adjusting%20the%20robot%27s%20configuration%20to%20keep%20the%20feature%20points%0Ain%20the%20field%20of%20view%20and%20away%20from%20obstacles.%20In%20particular%2C%20to%20account%20for%0Ameasurement%20noise%20of%20the%20feature%20points%2C%20we%20develop%20the%20Probabilistic%20Control%0ABarrier%20Certificates%20%28PrCBC%29%20using%20control%20barrier%20functions%20that%20encode%20the%0Achance-constrained%20occlusion%20avoidance%20constraints%20under%20uncertainty%20into%0Adeterministic%20admissible%20control%20space%20for%20the%20robot%2C%20from%20which%20the%20resulting%0Aconfiguration%20of%20robot%20ensures%20that%20the%20feature%20points%20stay%20occlusion%20free%20from%0Aobstacles%20with%20a%20satisfying%20predefined%20probability.%20By%20integrating%20such%0Aconstraints%20with%20a%20Model%20Predictive%20Control%20%28MPC%29%20framework%2C%20the%20sequence%20of%0Aoptimized%20control%20inputs%20can%20be%20derived%20to%20achieve%20the%20primary%20IBVS%20task%20while%0Aenforcing%20the%20occlusion%20avoidance%20during%20robot%20movements.%20Simulation%20results%0Aare%20provided%20to%20validate%20the%20performance%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03476v2&entry.124074799=Read"},
{"title": "Grid Cell-Inspired Fragmentation and Recall for Efficient Map Building", "author": "Jaedong Hwang and Zhang-Wei Hong and Eric Chen and Akhilan Boopathy and Pulkit Agrawal and Ila Fiete", "abstract": "  Animals and robots navigate through environments by building and refining\nmaps of space. These maps enable functions including navigation back to home,\nplanning, search and foraging. Here, we use observations from neuroscience,\nspecifically the observed fragmentation of grid cell map in compartmentalized\nspaces, to propose and apply the concept of Fragmentation-and-Recall (FARMap)\nin the mapping of large spaces. Agents solve the mapping problem by building\nlocal maps via a surprisal-based clustering of space, which they use to set\nsubgoals for spatial exploration. Agents build and use a local map to predict\ntheir observations; high surprisal leads to a \"fragmentation event\" that\ntruncates the local map. At these events, the recent local map is placed into\nlong-term memory (LTM) and a different local map is initialized. If\nobservations at a fracture point match observations in one of the stored local\nmaps, that map is recalled (and thus reused) from LTM. The fragmentation points\ninduce a natural online clustering of the larger space, forming a set of\nintrinsic potential subgoals that are stored in LTM as a topological graph.\nAgents choose their next subgoal from the set of near and far potential\nsubgoals from within the current local map or LTM, respectively. Thus, local\nmaps guide exploration locally, while LTM promotes global exploration. We\ndemonstrate that FARMap replicates the fragmentation points observed in animal\nstudies. We evaluate FARMap on complex procedurally-generated spatial\nenvironments and realistic simulations to demonstrate that this mapping\nstrategy much more rapidly covers the environment (number of agent steps and\nwall clock time) and is more efficient in active memory usage, without loss of\nperformance. https://jd730.github.io/projects/FARMap/\n", "link": "http://arxiv.org/abs/2307.05793v3", "date": "2024-07-08", "relevancy": 2.1601, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5628}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5599}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grid%20Cell-Inspired%20Fragmentation%20and%20Recall%20for%20Efficient%20Map%20Building&body=Title%3A%20Grid%20Cell-Inspired%20Fragmentation%20and%20Recall%20for%20Efficient%20Map%20Building%0AAuthor%3A%20Jaedong%20Hwang%20and%20Zhang-Wei%20Hong%20and%20Eric%20Chen%20and%20Akhilan%20Boopathy%20and%20Pulkit%20Agrawal%20and%20Ila%20Fiete%0AAbstract%3A%20%20%20Animals%20and%20robots%20navigate%20through%20environments%20by%20building%20and%20refining%0Amaps%20of%20space.%20These%20maps%20enable%20functions%20including%20navigation%20back%20to%20home%2C%0Aplanning%2C%20search%20and%20foraging.%20Here%2C%20we%20use%20observations%20from%20neuroscience%2C%0Aspecifically%20the%20observed%20fragmentation%20of%20grid%20cell%20map%20in%20compartmentalized%0Aspaces%2C%20to%20propose%20and%20apply%20the%20concept%20of%20Fragmentation-and-Recall%20%28FARMap%29%0Ain%20the%20mapping%20of%20large%20spaces.%20Agents%20solve%20the%20mapping%20problem%20by%20building%0Alocal%20maps%20via%20a%20surprisal-based%20clustering%20of%20space%2C%20which%20they%20use%20to%20set%0Asubgoals%20for%20spatial%20exploration.%20Agents%20build%20and%20use%20a%20local%20map%20to%20predict%0Atheir%20observations%3B%20high%20surprisal%20leads%20to%20a%20%22fragmentation%20event%22%20that%0Atruncates%20the%20local%20map.%20At%20these%20events%2C%20the%20recent%20local%20map%20is%20placed%20into%0Along-term%20memory%20%28LTM%29%20and%20a%20different%20local%20map%20is%20initialized.%20If%0Aobservations%20at%20a%20fracture%20point%20match%20observations%20in%20one%20of%20the%20stored%20local%0Amaps%2C%20that%20map%20is%20recalled%20%28and%20thus%20reused%29%20from%20LTM.%20The%20fragmentation%20points%0Ainduce%20a%20natural%20online%20clustering%20of%20the%20larger%20space%2C%20forming%20a%20set%20of%0Aintrinsic%20potential%20subgoals%20that%20are%20stored%20in%20LTM%20as%20a%20topological%20graph.%0AAgents%20choose%20their%20next%20subgoal%20from%20the%20set%20of%20near%20and%20far%20potential%0Asubgoals%20from%20within%20the%20current%20local%20map%20or%20LTM%2C%20respectively.%20Thus%2C%20local%0Amaps%20guide%20exploration%20locally%2C%20while%20LTM%20promotes%20global%20exploration.%20We%0Ademonstrate%20that%20FARMap%20replicates%20the%20fragmentation%20points%20observed%20in%20animal%0Astudies.%20We%20evaluate%20FARMap%20on%20complex%20procedurally-generated%20spatial%0Aenvironments%20and%20realistic%20simulations%20to%20demonstrate%20that%20this%20mapping%0Astrategy%20much%20more%20rapidly%20covers%20the%20environment%20%28number%20of%20agent%20steps%20and%0Awall%20clock%20time%29%20and%20is%20more%20efficient%20in%20active%20memory%20usage%2C%20without%20loss%20of%0Aperformance.%20https%3A//jd730.github.io/projects/FARMap/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.05793v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrid%2520Cell-Inspired%2520Fragmentation%2520and%2520Recall%2520for%2520Efficient%2520Map%2520Building%26entry.906535625%3DJaedong%2520Hwang%2520and%2520Zhang-Wei%2520Hong%2520and%2520Eric%2520Chen%2520and%2520Akhilan%2520Boopathy%2520and%2520Pulkit%2520Agrawal%2520and%2520Ila%2520Fiete%26entry.1292438233%3D%2520%2520Animals%2520and%2520robots%2520navigate%2520through%2520environments%2520by%2520building%2520and%2520refining%250Amaps%2520of%2520space.%2520These%2520maps%2520enable%2520functions%2520including%2520navigation%2520back%2520to%2520home%252C%250Aplanning%252C%2520search%2520and%2520foraging.%2520Here%252C%2520we%2520use%2520observations%2520from%2520neuroscience%252C%250Aspecifically%2520the%2520observed%2520fragmentation%2520of%2520grid%2520cell%2520map%2520in%2520compartmentalized%250Aspaces%252C%2520to%2520propose%2520and%2520apply%2520the%2520concept%2520of%2520Fragmentation-and-Recall%2520%2528FARMap%2529%250Ain%2520the%2520mapping%2520of%2520large%2520spaces.%2520Agents%2520solve%2520the%2520mapping%2520problem%2520by%2520building%250Alocal%2520maps%2520via%2520a%2520surprisal-based%2520clustering%2520of%2520space%252C%2520which%2520they%2520use%2520to%2520set%250Asubgoals%2520for%2520spatial%2520exploration.%2520Agents%2520build%2520and%2520use%2520a%2520local%2520map%2520to%2520predict%250Atheir%2520observations%253B%2520high%2520surprisal%2520leads%2520to%2520a%2520%2522fragmentation%2520event%2522%2520that%250Atruncates%2520the%2520local%2520map.%2520At%2520these%2520events%252C%2520the%2520recent%2520local%2520map%2520is%2520placed%2520into%250Along-term%2520memory%2520%2528LTM%2529%2520and%2520a%2520different%2520local%2520map%2520is%2520initialized.%2520If%250Aobservations%2520at%2520a%2520fracture%2520point%2520match%2520observations%2520in%2520one%2520of%2520the%2520stored%2520local%250Amaps%252C%2520that%2520map%2520is%2520recalled%2520%2528and%2520thus%2520reused%2529%2520from%2520LTM.%2520The%2520fragmentation%2520points%250Ainduce%2520a%2520natural%2520online%2520clustering%2520of%2520the%2520larger%2520space%252C%2520forming%2520a%2520set%2520of%250Aintrinsic%2520potential%2520subgoals%2520that%2520are%2520stored%2520in%2520LTM%2520as%2520a%2520topological%2520graph.%250AAgents%2520choose%2520their%2520next%2520subgoal%2520from%2520the%2520set%2520of%2520near%2520and%2520far%2520potential%250Asubgoals%2520from%2520within%2520the%2520current%2520local%2520map%2520or%2520LTM%252C%2520respectively.%2520Thus%252C%2520local%250Amaps%2520guide%2520exploration%2520locally%252C%2520while%2520LTM%2520promotes%2520global%2520exploration.%2520We%250Ademonstrate%2520that%2520FARMap%2520replicates%2520the%2520fragmentation%2520points%2520observed%2520in%2520animal%250Astudies.%2520We%2520evaluate%2520FARMap%2520on%2520complex%2520procedurally-generated%2520spatial%250Aenvironments%2520and%2520realistic%2520simulations%2520to%2520demonstrate%2520that%2520this%2520mapping%250Astrategy%2520much%2520more%2520rapidly%2520covers%2520the%2520environment%2520%2528number%2520of%2520agent%2520steps%2520and%250Awall%2520clock%2520time%2529%2520and%2520is%2520more%2520efficient%2520in%2520active%2520memory%2520usage%252C%2520without%2520loss%2520of%250Aperformance.%2520https%253A//jd730.github.io/projects/FARMap/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.05793v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grid%20Cell-Inspired%20Fragmentation%20and%20Recall%20for%20Efficient%20Map%20Building&entry.906535625=Jaedong%20Hwang%20and%20Zhang-Wei%20Hong%20and%20Eric%20Chen%20and%20Akhilan%20Boopathy%20and%20Pulkit%20Agrawal%20and%20Ila%20Fiete&entry.1292438233=%20%20Animals%20and%20robots%20navigate%20through%20environments%20by%20building%20and%20refining%0Amaps%20of%20space.%20These%20maps%20enable%20functions%20including%20navigation%20back%20to%20home%2C%0Aplanning%2C%20search%20and%20foraging.%20Here%2C%20we%20use%20observations%20from%20neuroscience%2C%0Aspecifically%20the%20observed%20fragmentation%20of%20grid%20cell%20map%20in%20compartmentalized%0Aspaces%2C%20to%20propose%20and%20apply%20the%20concept%20of%20Fragmentation-and-Recall%20%28FARMap%29%0Ain%20the%20mapping%20of%20large%20spaces.%20Agents%20solve%20the%20mapping%20problem%20by%20building%0Alocal%20maps%20via%20a%20surprisal-based%20clustering%20of%20space%2C%20which%20they%20use%20to%20set%0Asubgoals%20for%20spatial%20exploration.%20Agents%20build%20and%20use%20a%20local%20map%20to%20predict%0Atheir%20observations%3B%20high%20surprisal%20leads%20to%20a%20%22fragmentation%20event%22%20that%0Atruncates%20the%20local%20map.%20At%20these%20events%2C%20the%20recent%20local%20map%20is%20placed%20into%0Along-term%20memory%20%28LTM%29%20and%20a%20different%20local%20map%20is%20initialized.%20If%0Aobservations%20at%20a%20fracture%20point%20match%20observations%20in%20one%20of%20the%20stored%20local%0Amaps%2C%20that%20map%20is%20recalled%20%28and%20thus%20reused%29%20from%20LTM.%20The%20fragmentation%20points%0Ainduce%20a%20natural%20online%20clustering%20of%20the%20larger%20space%2C%20forming%20a%20set%20of%0Aintrinsic%20potential%20subgoals%20that%20are%20stored%20in%20LTM%20as%20a%20topological%20graph.%0AAgents%20choose%20their%20next%20subgoal%20from%20the%20set%20of%20near%20and%20far%20potential%0Asubgoals%20from%20within%20the%20current%20local%20map%20or%20LTM%2C%20respectively.%20Thus%2C%20local%0Amaps%20guide%20exploration%20locally%2C%20while%20LTM%20promotes%20global%20exploration.%20We%0Ademonstrate%20that%20FARMap%20replicates%20the%20fragmentation%20points%20observed%20in%20animal%0Astudies.%20We%20evaluate%20FARMap%20on%20complex%20procedurally-generated%20spatial%0Aenvironments%20and%20realistic%20simulations%20to%20demonstrate%20that%20this%20mapping%0Astrategy%20much%20more%20rapidly%20covers%20the%20environment%20%28number%20of%20agent%20steps%20and%0Awall%20clock%20time%29%20and%20is%20more%20efficient%20in%20active%20memory%20usage%2C%20without%20loss%20of%0Aperformance.%20https%3A//jd730.github.io/projects/FARMap/%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.05793v3&entry.124074799=Read"},
{"title": "Advancing Automated Deception Detection: A Multimodal Approach to\n  Feature Extraction and Analysis", "author": "Mohamed Bahaa and Mena Hany and Ehab E. Zakaria", "abstract": "  With the exponential increase in video content, the need for accurate\ndeception detection in human-centric video analysis has become paramount. This\nresearch focuses on the extraction and combination of various features to\nenhance the accuracy of deception detection models. By systematically\nextracting features from visual, audio, and text data, and experimenting with\ndifferent combinations, we developed a robust model that achieved an impressive\n99% accuracy. Our methodology emphasizes the significance of feature\nengineering in deception detection, providing a clear and interpretable\nframework. We trained various machine learning models, including LSTM, BiLSTM,\nand pre-trained CNNs, using both single and multi-modal approaches. The results\ndemonstrated that combining multiple modalities significantly enhances\ndetection performance compared to single modality training. This study\nhighlights the potential of strategic feature extraction and combination in\ndeveloping reliable and transparent automated deception detection systems in\nvideo analysis, paving the way for more advanced and accurate detection\nmethodologies in future research.\n", "link": "http://arxiv.org/abs/2407.06005v1", "date": "2024-07-08", "relevancy": 2.1554, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Automated%20Deception%20Detection%3A%20A%20Multimodal%20Approach%20to%0A%20%20Feature%20Extraction%20and%20Analysis&body=Title%3A%20Advancing%20Automated%20Deception%20Detection%3A%20A%20Multimodal%20Approach%20to%0A%20%20Feature%20Extraction%20and%20Analysis%0AAuthor%3A%20Mohamed%20Bahaa%20and%20Mena%20Hany%20and%20Ehab%20E.%20Zakaria%0AAbstract%3A%20%20%20With%20the%20exponential%20increase%20in%20video%20content%2C%20the%20need%20for%20accurate%0Adeception%20detection%20in%20human-centric%20video%20analysis%20has%20become%20paramount.%20This%0Aresearch%20focuses%20on%20the%20extraction%20and%20combination%20of%20various%20features%20to%0Aenhance%20the%20accuracy%20of%20deception%20detection%20models.%20By%20systematically%0Aextracting%20features%20from%20visual%2C%20audio%2C%20and%20text%20data%2C%20and%20experimenting%20with%0Adifferent%20combinations%2C%20we%20developed%20a%20robust%20model%20that%20achieved%20an%20impressive%0A99%25%20accuracy.%20Our%20methodology%20emphasizes%20the%20significance%20of%20feature%0Aengineering%20in%20deception%20detection%2C%20providing%20a%20clear%20and%20interpretable%0Aframework.%20We%20trained%20various%20machine%20learning%20models%2C%20including%20LSTM%2C%20BiLSTM%2C%0Aand%20pre-trained%20CNNs%2C%20using%20both%20single%20and%20multi-modal%20approaches.%20The%20results%0Ademonstrated%20that%20combining%20multiple%20modalities%20significantly%20enhances%0Adetection%20performance%20compared%20to%20single%20modality%20training.%20This%20study%0Ahighlights%20the%20potential%20of%20strategic%20feature%20extraction%20and%20combination%20in%0Adeveloping%20reliable%20and%20transparent%20automated%20deception%20detection%20systems%20in%0Avideo%20analysis%2C%20paving%20the%20way%20for%20more%20advanced%20and%20accurate%20detection%0Amethodologies%20in%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Automated%2520Deception%2520Detection%253A%2520A%2520Multimodal%2520Approach%2520to%250A%2520%2520Feature%2520Extraction%2520and%2520Analysis%26entry.906535625%3DMohamed%2520Bahaa%2520and%2520Mena%2520Hany%2520and%2520Ehab%2520E.%2520Zakaria%26entry.1292438233%3D%2520%2520With%2520the%2520exponential%2520increase%2520in%2520video%2520content%252C%2520the%2520need%2520for%2520accurate%250Adeception%2520detection%2520in%2520human-centric%2520video%2520analysis%2520has%2520become%2520paramount.%2520This%250Aresearch%2520focuses%2520on%2520the%2520extraction%2520and%2520combination%2520of%2520various%2520features%2520to%250Aenhance%2520the%2520accuracy%2520of%2520deception%2520detection%2520models.%2520By%2520systematically%250Aextracting%2520features%2520from%2520visual%252C%2520audio%252C%2520and%2520text%2520data%252C%2520and%2520experimenting%2520with%250Adifferent%2520combinations%252C%2520we%2520developed%2520a%2520robust%2520model%2520that%2520achieved%2520an%2520impressive%250A99%2525%2520accuracy.%2520Our%2520methodology%2520emphasizes%2520the%2520significance%2520of%2520feature%250Aengineering%2520in%2520deception%2520detection%252C%2520providing%2520a%2520clear%2520and%2520interpretable%250Aframework.%2520We%2520trained%2520various%2520machine%2520learning%2520models%252C%2520including%2520LSTM%252C%2520BiLSTM%252C%250Aand%2520pre-trained%2520CNNs%252C%2520using%2520both%2520single%2520and%2520multi-modal%2520approaches.%2520The%2520results%250Ademonstrated%2520that%2520combining%2520multiple%2520modalities%2520significantly%2520enhances%250Adetection%2520performance%2520compared%2520to%2520single%2520modality%2520training.%2520This%2520study%250Ahighlights%2520the%2520potential%2520of%2520strategic%2520feature%2520extraction%2520and%2520combination%2520in%250Adeveloping%2520reliable%2520and%2520transparent%2520automated%2520deception%2520detection%2520systems%2520in%250Avideo%2520analysis%252C%2520paving%2520the%2520way%2520for%2520more%2520advanced%2520and%2520accurate%2520detection%250Amethodologies%2520in%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Automated%20Deception%20Detection%3A%20A%20Multimodal%20Approach%20to%0A%20%20Feature%20Extraction%20and%20Analysis&entry.906535625=Mohamed%20Bahaa%20and%20Mena%20Hany%20and%20Ehab%20E.%20Zakaria&entry.1292438233=%20%20With%20the%20exponential%20increase%20in%20video%20content%2C%20the%20need%20for%20accurate%0Adeception%20detection%20in%20human-centric%20video%20analysis%20has%20become%20paramount.%20This%0Aresearch%20focuses%20on%20the%20extraction%20and%20combination%20of%20various%20features%20to%0Aenhance%20the%20accuracy%20of%20deception%20detection%20models.%20By%20systematically%0Aextracting%20features%20from%20visual%2C%20audio%2C%20and%20text%20data%2C%20and%20experimenting%20with%0Adifferent%20combinations%2C%20we%20developed%20a%20robust%20model%20that%20achieved%20an%20impressive%0A99%25%20accuracy.%20Our%20methodology%20emphasizes%20the%20significance%20of%20feature%0Aengineering%20in%20deception%20detection%2C%20providing%20a%20clear%20and%20interpretable%0Aframework.%20We%20trained%20various%20machine%20learning%20models%2C%20including%20LSTM%2C%20BiLSTM%2C%0Aand%20pre-trained%20CNNs%2C%20using%20both%20single%20and%20multi-modal%20approaches.%20The%20results%0Ademonstrated%20that%20combining%20multiple%20modalities%20significantly%20enhances%0Adetection%20performance%20compared%20to%20single%20modality%20training.%20This%20study%0Ahighlights%20the%20potential%20of%20strategic%20feature%20extraction%20and%20combination%20in%0Adeveloping%20reliable%20and%20transparent%20automated%20deception%20detection%20systems%20in%0Avideo%20analysis%2C%20paving%20the%20way%20for%20more%20advanced%20and%20accurate%20detection%0Amethodologies%20in%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06005v1&entry.124074799=Read"},
{"title": "A Universal Growth Rate for Learning with Smooth Surrogate Losses", "author": "Anqi Mao and Mehryar Mohri and Yutao Zhong", "abstract": "  This paper presents a comprehensive analysis of the growth rate of\n$H$-consistency bounds (and excess error bounds) for various surrogate losses\nused in classification. We prove a square-root growth rate near zero for smooth\nmargin-based surrogate losses in binary classification, providing both upper\nand lower bounds under mild assumptions. This result also translates to excess\nerror bounds. Our lower bound requires weaker conditions than those in previous\nwork for excess error bounds, and our upper bound is entirely novel. Moreover,\nwe extend this analysis to multi-class classification with a series of novel\nresults, demonstrating a universal square-root growth rate for smooth comp-sum\nand constrained losses, covering common choices for training neural networks in\nmulti-class classification. Given this universal rate, we turn to the question\nof choosing among different surrogate losses. We first examine how\n$H$-consistency bounds vary across surrogates based on the number of classes.\nNext, ignoring constants and focusing on behavior near zero, we identify\nminimizability gaps as the key differentiating factor in these bounds. Thus, we\nthoroughly analyze these gaps, to guide surrogate loss selection, covering:\ncomparisons across different comp-sum losses, conditions where gaps become\nzero, and general conditions leading to small gaps. Additionally, we\ndemonstrate the key role of minimizability gaps in comparing excess error\nbounds and $H$-consistency bounds.\n", "link": "http://arxiv.org/abs/2405.05968v2", "date": "2024-07-08", "relevancy": 2.1505, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4442}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4256}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20Growth%20Rate%20for%20Learning%20with%20Smooth%20Surrogate%20Losses&body=Title%3A%20A%20Universal%20Growth%20Rate%20for%20Learning%20with%20Smooth%20Surrogate%20Losses%0AAuthor%3A%20Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20analysis%20of%20the%20growth%20rate%20of%0A%24H%24-consistency%20bounds%20%28and%20excess%20error%20bounds%29%20for%20various%20surrogate%20losses%0Aused%20in%20classification.%20We%20prove%20a%20square-root%20growth%20rate%20near%20zero%20for%20smooth%0Amargin-based%20surrogate%20losses%20in%20binary%20classification%2C%20providing%20both%20upper%0Aand%20lower%20bounds%20under%20mild%20assumptions.%20This%20result%20also%20translates%20to%20excess%0Aerror%20bounds.%20Our%20lower%20bound%20requires%20weaker%20conditions%20than%20those%20in%20previous%0Awork%20for%20excess%20error%20bounds%2C%20and%20our%20upper%20bound%20is%20entirely%20novel.%20Moreover%2C%0Awe%20extend%20this%20analysis%20to%20multi-class%20classification%20with%20a%20series%20of%20novel%0Aresults%2C%20demonstrating%20a%20universal%20square-root%20growth%20rate%20for%20smooth%20comp-sum%0Aand%20constrained%20losses%2C%20covering%20common%20choices%20for%20training%20neural%20networks%20in%0Amulti-class%20classification.%20Given%20this%20universal%20rate%2C%20we%20turn%20to%20the%20question%0Aof%20choosing%20among%20different%20surrogate%20losses.%20We%20first%20examine%20how%0A%24H%24-consistency%20bounds%20vary%20across%20surrogates%20based%20on%20the%20number%20of%20classes.%0ANext%2C%20ignoring%20constants%20and%20focusing%20on%20behavior%20near%20zero%2C%20we%20identify%0Aminimizability%20gaps%20as%20the%20key%20differentiating%20factor%20in%20these%20bounds.%20Thus%2C%20we%0Athoroughly%20analyze%20these%20gaps%2C%20to%20guide%20surrogate%20loss%20selection%2C%20covering%3A%0Acomparisons%20across%20different%20comp-sum%20losses%2C%20conditions%20where%20gaps%20become%0Azero%2C%20and%20general%20conditions%20leading%20to%20small%20gaps.%20Additionally%2C%20we%0Ademonstrate%20the%20key%20role%20of%20minimizability%20gaps%20in%20comparing%20excess%20error%0Abounds%20and%20%24H%24-consistency%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Universal%2520Growth%2520Rate%2520for%2520Learning%2520with%2520Smooth%2520Surrogate%2520Losses%26entry.906535625%3DAnqi%2520Mao%2520and%2520Mehryar%2520Mohri%2520and%2520Yutao%2520Zhong%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520analysis%2520of%2520the%2520growth%2520rate%2520of%250A%2524H%2524-consistency%2520bounds%2520%2528and%2520excess%2520error%2520bounds%2529%2520for%2520various%2520surrogate%2520losses%250Aused%2520in%2520classification.%2520We%2520prove%2520a%2520square-root%2520growth%2520rate%2520near%2520zero%2520for%2520smooth%250Amargin-based%2520surrogate%2520losses%2520in%2520binary%2520classification%252C%2520providing%2520both%2520upper%250Aand%2520lower%2520bounds%2520under%2520mild%2520assumptions.%2520This%2520result%2520also%2520translates%2520to%2520excess%250Aerror%2520bounds.%2520Our%2520lower%2520bound%2520requires%2520weaker%2520conditions%2520than%2520those%2520in%2520previous%250Awork%2520for%2520excess%2520error%2520bounds%252C%2520and%2520our%2520upper%2520bound%2520is%2520entirely%2520novel.%2520Moreover%252C%250Awe%2520extend%2520this%2520analysis%2520to%2520multi-class%2520classification%2520with%2520a%2520series%2520of%2520novel%250Aresults%252C%2520demonstrating%2520a%2520universal%2520square-root%2520growth%2520rate%2520for%2520smooth%2520comp-sum%250Aand%2520constrained%2520losses%252C%2520covering%2520common%2520choices%2520for%2520training%2520neural%2520networks%2520in%250Amulti-class%2520classification.%2520Given%2520this%2520universal%2520rate%252C%2520we%2520turn%2520to%2520the%2520question%250Aof%2520choosing%2520among%2520different%2520surrogate%2520losses.%2520We%2520first%2520examine%2520how%250A%2524H%2524-consistency%2520bounds%2520vary%2520across%2520surrogates%2520based%2520on%2520the%2520number%2520of%2520classes.%250ANext%252C%2520ignoring%2520constants%2520and%2520focusing%2520on%2520behavior%2520near%2520zero%252C%2520we%2520identify%250Aminimizability%2520gaps%2520as%2520the%2520key%2520differentiating%2520factor%2520in%2520these%2520bounds.%2520Thus%252C%2520we%250Athoroughly%2520analyze%2520these%2520gaps%252C%2520to%2520guide%2520surrogate%2520loss%2520selection%252C%2520covering%253A%250Acomparisons%2520across%2520different%2520comp-sum%2520losses%252C%2520conditions%2520where%2520gaps%2520become%250Azero%252C%2520and%2520general%2520conditions%2520leading%2520to%2520small%2520gaps.%2520Additionally%252C%2520we%250Ademonstrate%2520the%2520key%2520role%2520of%2520minimizability%2520gaps%2520in%2520comparing%2520excess%2520error%250Abounds%2520and%2520%2524H%2524-consistency%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20Growth%20Rate%20for%20Learning%20with%20Smooth%20Surrogate%20Losses&entry.906535625=Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20analysis%20of%20the%20growth%20rate%20of%0A%24H%24-consistency%20bounds%20%28and%20excess%20error%20bounds%29%20for%20various%20surrogate%20losses%0Aused%20in%20classification.%20We%20prove%20a%20square-root%20growth%20rate%20near%20zero%20for%20smooth%0Amargin-based%20surrogate%20losses%20in%20binary%20classification%2C%20providing%20both%20upper%0Aand%20lower%20bounds%20under%20mild%20assumptions.%20This%20result%20also%20translates%20to%20excess%0Aerror%20bounds.%20Our%20lower%20bound%20requires%20weaker%20conditions%20than%20those%20in%20previous%0Awork%20for%20excess%20error%20bounds%2C%20and%20our%20upper%20bound%20is%20entirely%20novel.%20Moreover%2C%0Awe%20extend%20this%20analysis%20to%20multi-class%20classification%20with%20a%20series%20of%20novel%0Aresults%2C%20demonstrating%20a%20universal%20square-root%20growth%20rate%20for%20smooth%20comp-sum%0Aand%20constrained%20losses%2C%20covering%20common%20choices%20for%20training%20neural%20networks%20in%0Amulti-class%20classification.%20Given%20this%20universal%20rate%2C%20we%20turn%20to%20the%20question%0Aof%20choosing%20among%20different%20surrogate%20losses.%20We%20first%20examine%20how%0A%24H%24-consistency%20bounds%20vary%20across%20surrogates%20based%20on%20the%20number%20of%20classes.%0ANext%2C%20ignoring%20constants%20and%20focusing%20on%20behavior%20near%20zero%2C%20we%20identify%0Aminimizability%20gaps%20as%20the%20key%20differentiating%20factor%20in%20these%20bounds.%20Thus%2C%20we%0Athoroughly%20analyze%20these%20gaps%2C%20to%20guide%20surrogate%20loss%20selection%2C%20covering%3A%0Acomparisons%20across%20different%20comp-sum%20losses%2C%20conditions%20where%20gaps%20become%0Azero%2C%20and%20general%20conditions%20leading%20to%20small%20gaps.%20Additionally%2C%20we%0Ademonstrate%20the%20key%20role%20of%20minimizability%20gaps%20in%20comparing%20excess%20error%0Abounds%20and%20%24H%24-consistency%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05968v2&entry.124074799=Read"},
{"title": "The Tug-of-War Between Deepfake Generation and Detection", "author": "Hannah Lee and Changyeon Lee and Kevin Farhat and Lin Qiu and Steve Geluso and Aerin Kim and Oren Etzioni", "abstract": "  Multimodal generative models are rapidly evolving, leading to a surge in the\ngeneration of realistic video and audio that offers exciting possibilities but\nalso serious risks. Deepfake videos, which can convincingly impersonate\nindividuals, have particularly garnered attention due to their potential misuse\nin spreading misinformation and creating fraudulent content. This survey paper\nexamines the dual landscape of deepfake video generation and detection,\nemphasizing the need for effective countermeasures against potential abuses. We\nprovide a comprehensive overview of current deepfake generation techniques,\nincluding face swapping, reenactment, and audio-driven animation, which\nleverage cutting-edge technologies like generative adversarial networks and\ndiffusion models to produce highly realistic fake videos. Additionally, we\nanalyze various detection approaches designed to differentiate authentic from\naltered videos, from detecting visual artifacts to deploying advanced\nalgorithms that pinpoint inconsistencies across video and audio signals.\n  The effectiveness of these detection methods heavily relies on the diversity\nand quality of datasets used for training and evaluation. We discuss the\nevolution of deepfake datasets, highlighting the importance of robust, diverse,\nand frequently updated collections to enhance the detection accuracy and\ngeneralizability. As deepfakes become increasingly indistinguishable from\nauthentic content, developing advanced detection techniques that can keep pace\nwith generation technologies is crucial. We advocate for a proactive approach\nin the \"tug-of-war\" between deepfake creators and detectors, emphasizing the\nneed for continuous research collaboration, standardization of evaluation\nmetrics, and the creation of comprehensive benchmarks.\n", "link": "http://arxiv.org/abs/2407.06174v1", "date": "2024-07-08", "relevancy": 2.1501, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5519}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5318}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Tug-of-War%20Between%20Deepfake%20Generation%20and%20Detection&body=Title%3A%20The%20Tug-of-War%20Between%20Deepfake%20Generation%20and%20Detection%0AAuthor%3A%20Hannah%20Lee%20and%20Changyeon%20Lee%20and%20Kevin%20Farhat%20and%20Lin%20Qiu%20and%20Steve%20Geluso%20and%20Aerin%20Kim%20and%20Oren%20Etzioni%0AAbstract%3A%20%20%20Multimodal%20generative%20models%20are%20rapidly%20evolving%2C%20leading%20to%20a%20surge%20in%20the%0Ageneration%20of%20realistic%20video%20and%20audio%20that%20offers%20exciting%20possibilities%20but%0Aalso%20serious%20risks.%20Deepfake%20videos%2C%20which%20can%20convincingly%20impersonate%0Aindividuals%2C%20have%20particularly%20garnered%20attention%20due%20to%20their%20potential%20misuse%0Ain%20spreading%20misinformation%20and%20creating%20fraudulent%20content.%20This%20survey%20paper%0Aexamines%20the%20dual%20landscape%20of%20deepfake%20video%20generation%20and%20detection%2C%0Aemphasizing%20the%20need%20for%20effective%20countermeasures%20against%20potential%20abuses.%20We%0Aprovide%20a%20comprehensive%20overview%20of%20current%20deepfake%20generation%20techniques%2C%0Aincluding%20face%20swapping%2C%20reenactment%2C%20and%20audio-driven%20animation%2C%20which%0Aleverage%20cutting-edge%20technologies%20like%20generative%20adversarial%20networks%20and%0Adiffusion%20models%20to%20produce%20highly%20realistic%20fake%20videos.%20Additionally%2C%20we%0Aanalyze%20various%20detection%20approaches%20designed%20to%20differentiate%20authentic%20from%0Aaltered%20videos%2C%20from%20detecting%20visual%20artifacts%20to%20deploying%20advanced%0Aalgorithms%20that%20pinpoint%20inconsistencies%20across%20video%20and%20audio%20signals.%0A%20%20The%20effectiveness%20of%20these%20detection%20methods%20heavily%20relies%20on%20the%20diversity%0Aand%20quality%20of%20datasets%20used%20for%20training%20and%20evaluation.%20We%20discuss%20the%0Aevolution%20of%20deepfake%20datasets%2C%20highlighting%20the%20importance%20of%20robust%2C%20diverse%2C%0Aand%20frequently%20updated%20collections%20to%20enhance%20the%20detection%20accuracy%20and%0Ageneralizability.%20As%20deepfakes%20become%20increasingly%20indistinguishable%20from%0Aauthentic%20content%2C%20developing%20advanced%20detection%20techniques%20that%20can%20keep%20pace%0Awith%20generation%20technologies%20is%20crucial.%20We%20advocate%20for%20a%20proactive%20approach%0Ain%20the%20%22tug-of-war%22%20between%20deepfake%20creators%20and%20detectors%2C%20emphasizing%20the%0Aneed%20for%20continuous%20research%20collaboration%2C%20standardization%20of%20evaluation%0Ametrics%2C%20and%20the%20creation%20of%20comprehensive%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Tug-of-War%2520Between%2520Deepfake%2520Generation%2520and%2520Detection%26entry.906535625%3DHannah%2520Lee%2520and%2520Changyeon%2520Lee%2520and%2520Kevin%2520Farhat%2520and%2520Lin%2520Qiu%2520and%2520Steve%2520Geluso%2520and%2520Aerin%2520Kim%2520and%2520Oren%2520Etzioni%26entry.1292438233%3D%2520%2520Multimodal%2520generative%2520models%2520are%2520rapidly%2520evolving%252C%2520leading%2520to%2520a%2520surge%2520in%2520the%250Ageneration%2520of%2520realistic%2520video%2520and%2520audio%2520that%2520offers%2520exciting%2520possibilities%2520but%250Aalso%2520serious%2520risks.%2520Deepfake%2520videos%252C%2520which%2520can%2520convincingly%2520impersonate%250Aindividuals%252C%2520have%2520particularly%2520garnered%2520attention%2520due%2520to%2520their%2520potential%2520misuse%250Ain%2520spreading%2520misinformation%2520and%2520creating%2520fraudulent%2520content.%2520This%2520survey%2520paper%250Aexamines%2520the%2520dual%2520landscape%2520of%2520deepfake%2520video%2520generation%2520and%2520detection%252C%250Aemphasizing%2520the%2520need%2520for%2520effective%2520countermeasures%2520against%2520potential%2520abuses.%2520We%250Aprovide%2520a%2520comprehensive%2520overview%2520of%2520current%2520deepfake%2520generation%2520techniques%252C%250Aincluding%2520face%2520swapping%252C%2520reenactment%252C%2520and%2520audio-driven%2520animation%252C%2520which%250Aleverage%2520cutting-edge%2520technologies%2520like%2520generative%2520adversarial%2520networks%2520and%250Adiffusion%2520models%2520to%2520produce%2520highly%2520realistic%2520fake%2520videos.%2520Additionally%252C%2520we%250Aanalyze%2520various%2520detection%2520approaches%2520designed%2520to%2520differentiate%2520authentic%2520from%250Aaltered%2520videos%252C%2520from%2520detecting%2520visual%2520artifacts%2520to%2520deploying%2520advanced%250Aalgorithms%2520that%2520pinpoint%2520inconsistencies%2520across%2520video%2520and%2520audio%2520signals.%250A%2520%2520The%2520effectiveness%2520of%2520these%2520detection%2520methods%2520heavily%2520relies%2520on%2520the%2520diversity%250Aand%2520quality%2520of%2520datasets%2520used%2520for%2520training%2520and%2520evaluation.%2520We%2520discuss%2520the%250Aevolution%2520of%2520deepfake%2520datasets%252C%2520highlighting%2520the%2520importance%2520of%2520robust%252C%2520diverse%252C%250Aand%2520frequently%2520updated%2520collections%2520to%2520enhance%2520the%2520detection%2520accuracy%2520and%250Ageneralizability.%2520As%2520deepfakes%2520become%2520increasingly%2520indistinguishable%2520from%250Aauthentic%2520content%252C%2520developing%2520advanced%2520detection%2520techniques%2520that%2520can%2520keep%2520pace%250Awith%2520generation%2520technologies%2520is%2520crucial.%2520We%2520advocate%2520for%2520a%2520proactive%2520approach%250Ain%2520the%2520%2522tug-of-war%2522%2520between%2520deepfake%2520creators%2520and%2520detectors%252C%2520emphasizing%2520the%250Aneed%2520for%2520continuous%2520research%2520collaboration%252C%2520standardization%2520of%2520evaluation%250Ametrics%252C%2520and%2520the%2520creation%2520of%2520comprehensive%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Tug-of-War%20Between%20Deepfake%20Generation%20and%20Detection&entry.906535625=Hannah%20Lee%20and%20Changyeon%20Lee%20and%20Kevin%20Farhat%20and%20Lin%20Qiu%20and%20Steve%20Geluso%20and%20Aerin%20Kim%20and%20Oren%20Etzioni&entry.1292438233=%20%20Multimodal%20generative%20models%20are%20rapidly%20evolving%2C%20leading%20to%20a%20surge%20in%20the%0Ageneration%20of%20realistic%20video%20and%20audio%20that%20offers%20exciting%20possibilities%20but%0Aalso%20serious%20risks.%20Deepfake%20videos%2C%20which%20can%20convincingly%20impersonate%0Aindividuals%2C%20have%20particularly%20garnered%20attention%20due%20to%20their%20potential%20misuse%0Ain%20spreading%20misinformation%20and%20creating%20fraudulent%20content.%20This%20survey%20paper%0Aexamines%20the%20dual%20landscape%20of%20deepfake%20video%20generation%20and%20detection%2C%0Aemphasizing%20the%20need%20for%20effective%20countermeasures%20against%20potential%20abuses.%20We%0Aprovide%20a%20comprehensive%20overview%20of%20current%20deepfake%20generation%20techniques%2C%0Aincluding%20face%20swapping%2C%20reenactment%2C%20and%20audio-driven%20animation%2C%20which%0Aleverage%20cutting-edge%20technologies%20like%20generative%20adversarial%20networks%20and%0Adiffusion%20models%20to%20produce%20highly%20realistic%20fake%20videos.%20Additionally%2C%20we%0Aanalyze%20various%20detection%20approaches%20designed%20to%20differentiate%20authentic%20from%0Aaltered%20videos%2C%20from%20detecting%20visual%20artifacts%20to%20deploying%20advanced%0Aalgorithms%20that%20pinpoint%20inconsistencies%20across%20video%20and%20audio%20signals.%0A%20%20The%20effectiveness%20of%20these%20detection%20methods%20heavily%20relies%20on%20the%20diversity%0Aand%20quality%20of%20datasets%20used%20for%20training%20and%20evaluation.%20We%20discuss%20the%0Aevolution%20of%20deepfake%20datasets%2C%20highlighting%20the%20importance%20of%20robust%2C%20diverse%2C%0Aand%20frequently%20updated%20collections%20to%20enhance%20the%20detection%20accuracy%20and%0Ageneralizability.%20As%20deepfakes%20become%20increasingly%20indistinguishable%20from%0Aauthentic%20content%2C%20developing%20advanced%20detection%20techniques%20that%20can%20keep%20pace%0Awith%20generation%20technologies%20is%20crucial.%20We%20advocate%20for%20a%20proactive%20approach%0Ain%20the%20%22tug-of-war%22%20between%20deepfake%20creators%20and%20detectors%2C%20emphasizing%20the%0Aneed%20for%20continuous%20research%20collaboration%2C%20standardization%20of%20evaluation%0Ametrics%2C%20and%20the%20creation%20of%20comprehensive%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06174v1&entry.124074799=Read"},
{"title": "An explainable three dimension framework to uncover learning patterns: A\n  unified look in variable sulci recognition", "author": "Michail Mamalakis and Heloise de Vareilles and Atheer AI-Manea and Samantha C. Mitchell and Ingrid Arartz and Lynn Egeland Morch-Johnsen and Jane Garrison and Jon Simons and Pietro Lio and John Suckling and Graham Murray", "abstract": "  The significant features identified in a representative subset of the dataset\nduring the learning process of an artificial intelligence model are referred to\nas a 'global' explanation. Three-dimensional (3D) global explanations are\ncrucial in neuroimaging where a complex representational space demands more\nthan basic two-dimensional interpretations. Curently, studies in the literature\nlack accurate, low-complexity, and 3D global explanations in neuroimaging and\nbeyond. To fill this gap, we develop a novel explainable artificial\nintelligence (XAI) 3D-Framework that provides robust, faithful, and\nlow-complexity global explanations. We evaluated our framework on various 3D\ndeep learning networks trained, validated, and tested on a well-annotated\ncohort of 596 MRI images. The focus of detection was on the presence or absence\nof the paracingulate sulcus, a highly variable feature of brain topology\nassociated with symptoms of psychosis. Our proposed 3D-Framework outperformed\ntraditional XAI methods in terms of faithfulness for global explanations. As a\nresult, these explanations uncovered new patterns that not only enhance the\ncredibility and reliability of the training process but also reveal the broader\ndevelopmental landscape of the human cortex. Our XAI 3D-Framework proposes for\nthe first time, a way to utilize global explanations to discover the context in\nwhich detection of specific features are embedded, opening our understanding of\nnormative brain development and atypical trajectories that can lead to the\nemergence of mental illness.\n", "link": "http://arxiv.org/abs/2309.00903v4", "date": "2024-07-08", "relevancy": 2.1473, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5439}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20explainable%20three%20dimension%20framework%20to%20uncover%20learning%20patterns%3A%20A%0A%20%20unified%20look%20in%20variable%20sulci%20recognition&body=Title%3A%20An%20explainable%20three%20dimension%20framework%20to%20uncover%20learning%20patterns%3A%20A%0A%20%20unified%20look%20in%20variable%20sulci%20recognition%0AAuthor%3A%20Michail%20Mamalakis%20and%20Heloise%20de%20Vareilles%20and%20Atheer%20AI-Manea%20and%20Samantha%20C.%20Mitchell%20and%20Ingrid%20Arartz%20and%20Lynn%20Egeland%20Morch-Johnsen%20and%20Jane%20Garrison%20and%20Jon%20Simons%20and%20Pietro%20Lio%20and%20John%20Suckling%20and%20Graham%20Murray%0AAbstract%3A%20%20%20The%20significant%20features%20identified%20in%20a%20representative%20subset%20of%20the%20dataset%0Aduring%20the%20learning%20process%20of%20an%20artificial%20intelligence%20model%20are%20referred%20to%0Aas%20a%20%27global%27%20explanation.%20Three-dimensional%20%283D%29%20global%20explanations%20are%0Acrucial%20in%20neuroimaging%20where%20a%20complex%20representational%20space%20demands%20more%0Athan%20basic%20two-dimensional%20interpretations.%20Curently%2C%20studies%20in%20the%20literature%0Alack%20accurate%2C%20low-complexity%2C%20and%203D%20global%20explanations%20in%20neuroimaging%20and%0Abeyond.%20To%20fill%20this%20gap%2C%20we%20develop%20a%20novel%20explainable%20artificial%0Aintelligence%20%28XAI%29%203D-Framework%20that%20provides%20robust%2C%20faithful%2C%20and%0Alow-complexity%20global%20explanations.%20We%20evaluated%20our%20framework%20on%20various%203D%0Adeep%20learning%20networks%20trained%2C%20validated%2C%20and%20tested%20on%20a%20well-annotated%0Acohort%20of%20596%20MRI%20images.%20The%20focus%20of%20detection%20was%20on%20the%20presence%20or%20absence%0Aof%20the%20paracingulate%20sulcus%2C%20a%20highly%20variable%20feature%20of%20brain%20topology%0Aassociated%20with%20symptoms%20of%20psychosis.%20Our%20proposed%203D-Framework%20outperformed%0Atraditional%20XAI%20methods%20in%20terms%20of%20faithfulness%20for%20global%20explanations.%20As%20a%0Aresult%2C%20these%20explanations%20uncovered%20new%20patterns%20that%20not%20only%20enhance%20the%0Acredibility%20and%20reliability%20of%20the%20training%20process%20but%20also%20reveal%20the%20broader%0Adevelopmental%20landscape%20of%20the%20human%20cortex.%20Our%20XAI%203D-Framework%20proposes%20for%0Athe%20first%20time%2C%20a%20way%20to%20utilize%20global%20explanations%20to%20discover%20the%20context%20in%0Awhich%20detection%20of%20specific%20features%20are%20embedded%2C%20opening%20our%20understanding%20of%0Anormative%20brain%20development%20and%20atypical%20trajectories%20that%20can%20lead%20to%20the%0Aemergence%20of%20mental%20illness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00903v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520explainable%2520three%2520dimension%2520framework%2520to%2520uncover%2520learning%2520patterns%253A%2520A%250A%2520%2520unified%2520look%2520in%2520variable%2520sulci%2520recognition%26entry.906535625%3DMichail%2520Mamalakis%2520and%2520Heloise%2520de%2520Vareilles%2520and%2520Atheer%2520AI-Manea%2520and%2520Samantha%2520C.%2520Mitchell%2520and%2520Ingrid%2520Arartz%2520and%2520Lynn%2520Egeland%2520Morch-Johnsen%2520and%2520Jane%2520Garrison%2520and%2520Jon%2520Simons%2520and%2520Pietro%2520Lio%2520and%2520John%2520Suckling%2520and%2520Graham%2520Murray%26entry.1292438233%3D%2520%2520The%2520significant%2520features%2520identified%2520in%2520a%2520representative%2520subset%2520of%2520the%2520dataset%250Aduring%2520the%2520learning%2520process%2520of%2520an%2520artificial%2520intelligence%2520model%2520are%2520referred%2520to%250Aas%2520a%2520%2527global%2527%2520explanation.%2520Three-dimensional%2520%25283D%2529%2520global%2520explanations%2520are%250Acrucial%2520in%2520neuroimaging%2520where%2520a%2520complex%2520representational%2520space%2520demands%2520more%250Athan%2520basic%2520two-dimensional%2520interpretations.%2520Curently%252C%2520studies%2520in%2520the%2520literature%250Alack%2520accurate%252C%2520low-complexity%252C%2520and%25203D%2520global%2520explanations%2520in%2520neuroimaging%2520and%250Abeyond.%2520To%2520fill%2520this%2520gap%252C%2520we%2520develop%2520a%2520novel%2520explainable%2520artificial%250Aintelligence%2520%2528XAI%2529%25203D-Framework%2520that%2520provides%2520robust%252C%2520faithful%252C%2520and%250Alow-complexity%2520global%2520explanations.%2520We%2520evaluated%2520our%2520framework%2520on%2520various%25203D%250Adeep%2520learning%2520networks%2520trained%252C%2520validated%252C%2520and%2520tested%2520on%2520a%2520well-annotated%250Acohort%2520of%2520596%2520MRI%2520images.%2520The%2520focus%2520of%2520detection%2520was%2520on%2520the%2520presence%2520or%2520absence%250Aof%2520the%2520paracingulate%2520sulcus%252C%2520a%2520highly%2520variable%2520feature%2520of%2520brain%2520topology%250Aassociated%2520with%2520symptoms%2520of%2520psychosis.%2520Our%2520proposed%25203D-Framework%2520outperformed%250Atraditional%2520XAI%2520methods%2520in%2520terms%2520of%2520faithfulness%2520for%2520global%2520explanations.%2520As%2520a%250Aresult%252C%2520these%2520explanations%2520uncovered%2520new%2520patterns%2520that%2520not%2520only%2520enhance%2520the%250Acredibility%2520and%2520reliability%2520of%2520the%2520training%2520process%2520but%2520also%2520reveal%2520the%2520broader%250Adevelopmental%2520landscape%2520of%2520the%2520human%2520cortex.%2520Our%2520XAI%25203D-Framework%2520proposes%2520for%250Athe%2520first%2520time%252C%2520a%2520way%2520to%2520utilize%2520global%2520explanations%2520to%2520discover%2520the%2520context%2520in%250Awhich%2520detection%2520of%2520specific%2520features%2520are%2520embedded%252C%2520opening%2520our%2520understanding%2520of%250Anormative%2520brain%2520development%2520and%2520atypical%2520trajectories%2520that%2520can%2520lead%2520to%2520the%250Aemergence%2520of%2520mental%2520illness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00903v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20explainable%20three%20dimension%20framework%20to%20uncover%20learning%20patterns%3A%20A%0A%20%20unified%20look%20in%20variable%20sulci%20recognition&entry.906535625=Michail%20Mamalakis%20and%20Heloise%20de%20Vareilles%20and%20Atheer%20AI-Manea%20and%20Samantha%20C.%20Mitchell%20and%20Ingrid%20Arartz%20and%20Lynn%20Egeland%20Morch-Johnsen%20and%20Jane%20Garrison%20and%20Jon%20Simons%20and%20Pietro%20Lio%20and%20John%20Suckling%20and%20Graham%20Murray&entry.1292438233=%20%20The%20significant%20features%20identified%20in%20a%20representative%20subset%20of%20the%20dataset%0Aduring%20the%20learning%20process%20of%20an%20artificial%20intelligence%20model%20are%20referred%20to%0Aas%20a%20%27global%27%20explanation.%20Three-dimensional%20%283D%29%20global%20explanations%20are%0Acrucial%20in%20neuroimaging%20where%20a%20complex%20representational%20space%20demands%20more%0Athan%20basic%20two-dimensional%20interpretations.%20Curently%2C%20studies%20in%20the%20literature%0Alack%20accurate%2C%20low-complexity%2C%20and%203D%20global%20explanations%20in%20neuroimaging%20and%0Abeyond.%20To%20fill%20this%20gap%2C%20we%20develop%20a%20novel%20explainable%20artificial%0Aintelligence%20%28XAI%29%203D-Framework%20that%20provides%20robust%2C%20faithful%2C%20and%0Alow-complexity%20global%20explanations.%20We%20evaluated%20our%20framework%20on%20various%203D%0Adeep%20learning%20networks%20trained%2C%20validated%2C%20and%20tested%20on%20a%20well-annotated%0Acohort%20of%20596%20MRI%20images.%20The%20focus%20of%20detection%20was%20on%20the%20presence%20or%20absence%0Aof%20the%20paracingulate%20sulcus%2C%20a%20highly%20variable%20feature%20of%20brain%20topology%0Aassociated%20with%20symptoms%20of%20psychosis.%20Our%20proposed%203D-Framework%20outperformed%0Atraditional%20XAI%20methods%20in%20terms%20of%20faithfulness%20for%20global%20explanations.%20As%20a%0Aresult%2C%20these%20explanations%20uncovered%20new%20patterns%20that%20not%20only%20enhance%20the%0Acredibility%20and%20reliability%20of%20the%20training%20process%20but%20also%20reveal%20the%20broader%0Adevelopmental%20landscape%20of%20the%20human%20cortex.%20Our%20XAI%203D-Framework%20proposes%20for%0Athe%20first%20time%2C%20a%20way%20to%20utilize%20global%20explanations%20to%20discover%20the%20context%20in%0Awhich%20detection%20of%20specific%20features%20are%20embedded%2C%20opening%20our%20understanding%20of%0Anormative%20brain%20development%20and%20atypical%20trajectories%20that%20can%20lead%20to%20the%0Aemergence%20of%20mental%20illness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00903v4&entry.124074799=Read"},
{"title": "Video-STaR: Self-Training Enables Video Instruction Tuning with Any\n  Supervision", "author": "Orr Zohar and Xiaohan Wang and Yonatan Bitton and Idan Szpektor and Serena Yeung-Levy", "abstract": "  The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.\n", "link": "http://arxiv.org/abs/2407.06189v1", "date": "2024-07-08", "relevancy": 2.1465, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5461}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5362}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-STaR%3A%20Self-Training%20Enables%20Video%20Instruction%20Tuning%20with%20Any%0A%20%20Supervision&body=Title%3A%20Video-STaR%3A%20Self-Training%20Enables%20Video%20Instruction%20Tuning%20with%20Any%0A%20%20Supervision%0AAuthor%3A%20Orr%20Zohar%20and%20Xiaohan%20Wang%20and%20Yonatan%20Bitton%20and%20Idan%20Szpektor%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20%20%20The%20performance%20of%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20is%20dependent%20on%20the%0Asize%20and%20quality%20of%20their%20training%20datasets.%20Existing%20video%20instruction%20tuning%0Adatasets%20lack%20diversity%20as%20they%20are%20derived%20by%20prompting%20large%20language%20models%0Awith%20video%20captions%20to%20generate%20question-answer%20pairs%2C%20and%20are%20therefore%20mostly%0Adescriptive.%20Meanwhile%2C%20many%20labeled%20video%20datasets%20with%20diverse%20labels%20and%0Asupervision%20exist%20-%20however%2C%20we%20find%20that%20their%20integration%20into%20LVLMs%20is%0Anon-trivial.%20Herein%2C%20we%20present%20Video%20Self-Training%20with%20augmented%20Reasoning%0A%28Video-STaR%29%2C%20the%20first%20video%20self-training%20approach.%20Video-STaR%20allows%20the%0Autilization%20of%20any%20labeled%20video%20dataset%20for%20video%20instruction%20tuning.%20In%0AVideo-STaR%2C%20an%20LVLM%20cycles%20between%20instruction%20generation%20and%20finetuning%2C%20which%0Awe%20show%20%28I%29%20improves%20general%20video%20understanding%20and%20%28II%29%20adapts%20LVLMs%20to%20novel%0Adownstream%20tasks%20with%20existing%20supervision.%20During%20generation%2C%20an%20LVLM%20is%0Aprompted%20to%20propose%20an%20answer.%20The%20answers%20are%20then%20filtered%20only%20to%20those%20that%0Acontain%20the%20original%20video%20labels%2C%20and%20the%20LVLM%20is%20then%20re-trained%20on%20the%0Agenerated%20dataset.%20By%20only%20training%20on%20generated%20answers%20that%20contain%20the%0Acorrect%20video%20labels%2C%20Video-STaR%20utilizes%20these%20existing%20video%20labels%20as%20weak%0Asupervision%20for%20video%20instruction%20tuning.%20Our%20results%20demonstrate%20that%0AVideo-STaR-enhanced%20LVLMs%20exhibit%20improved%20performance%20in%20%28I%29%20general%20video%20QA%2C%0Awhere%20TempCompass%20performance%20improved%20by%2010%25%2C%20and%20%28II%29%20on%20downstream%20tasks%2C%0Awhere%20Video-STaR%20improved%20Kinetics700-QA%20accuracy%20by%2020%25%20and%20action%20quality%0Aassessment%20on%20FineDiving%20by%2015%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-STaR%253A%2520Self-Training%2520Enables%2520Video%2520Instruction%2520Tuning%2520with%2520Any%250A%2520%2520Supervision%26entry.906535625%3DOrr%2520Zohar%2520and%2520Xiaohan%2520Wang%2520and%2520Yonatan%2520Bitton%2520and%2520Idan%2520Szpektor%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520is%2520dependent%2520on%2520the%250Asize%2520and%2520quality%2520of%2520their%2520training%2520datasets.%2520Existing%2520video%2520instruction%2520tuning%250Adatasets%2520lack%2520diversity%2520as%2520they%2520are%2520derived%2520by%2520prompting%2520large%2520language%2520models%250Awith%2520video%2520captions%2520to%2520generate%2520question-answer%2520pairs%252C%2520and%2520are%2520therefore%2520mostly%250Adescriptive.%2520Meanwhile%252C%2520many%2520labeled%2520video%2520datasets%2520with%2520diverse%2520labels%2520and%250Asupervision%2520exist%2520-%2520however%252C%2520we%2520find%2520that%2520their%2520integration%2520into%2520LVLMs%2520is%250Anon-trivial.%2520Herein%252C%2520we%2520present%2520Video%2520Self-Training%2520with%2520augmented%2520Reasoning%250A%2528Video-STaR%2529%252C%2520the%2520first%2520video%2520self-training%2520approach.%2520Video-STaR%2520allows%2520the%250Autilization%2520of%2520any%2520labeled%2520video%2520dataset%2520for%2520video%2520instruction%2520tuning.%2520In%250AVideo-STaR%252C%2520an%2520LVLM%2520cycles%2520between%2520instruction%2520generation%2520and%2520finetuning%252C%2520which%250Awe%2520show%2520%2528I%2529%2520improves%2520general%2520video%2520understanding%2520and%2520%2528II%2529%2520adapts%2520LVLMs%2520to%2520novel%250Adownstream%2520tasks%2520with%2520existing%2520supervision.%2520During%2520generation%252C%2520an%2520LVLM%2520is%250Aprompted%2520to%2520propose%2520an%2520answer.%2520The%2520answers%2520are%2520then%2520filtered%2520only%2520to%2520those%2520that%250Acontain%2520the%2520original%2520video%2520labels%252C%2520and%2520the%2520LVLM%2520is%2520then%2520re-trained%2520on%2520the%250Agenerated%2520dataset.%2520By%2520only%2520training%2520on%2520generated%2520answers%2520that%2520contain%2520the%250Acorrect%2520video%2520labels%252C%2520Video-STaR%2520utilizes%2520these%2520existing%2520video%2520labels%2520as%2520weak%250Asupervision%2520for%2520video%2520instruction%2520tuning.%2520Our%2520results%2520demonstrate%2520that%250AVideo-STaR-enhanced%2520LVLMs%2520exhibit%2520improved%2520performance%2520in%2520%2528I%2529%2520general%2520video%2520QA%252C%250Awhere%2520TempCompass%2520performance%2520improved%2520by%252010%2525%252C%2520and%2520%2528II%2529%2520on%2520downstream%2520tasks%252C%250Awhere%2520Video-STaR%2520improved%2520Kinetics700-QA%2520accuracy%2520by%252020%2525%2520and%2520action%2520quality%250Aassessment%2520on%2520FineDiving%2520by%252015%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-STaR%3A%20Self-Training%20Enables%20Video%20Instruction%20Tuning%20with%20Any%0A%20%20Supervision&entry.906535625=Orr%20Zohar%20and%20Xiaohan%20Wang%20and%20Yonatan%20Bitton%20and%20Idan%20Szpektor%20and%20Serena%20Yeung-Levy&entry.1292438233=%20%20The%20performance%20of%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20is%20dependent%20on%20the%0Asize%20and%20quality%20of%20their%20training%20datasets.%20Existing%20video%20instruction%20tuning%0Adatasets%20lack%20diversity%20as%20they%20are%20derived%20by%20prompting%20large%20language%20models%0Awith%20video%20captions%20to%20generate%20question-answer%20pairs%2C%20and%20are%20therefore%20mostly%0Adescriptive.%20Meanwhile%2C%20many%20labeled%20video%20datasets%20with%20diverse%20labels%20and%0Asupervision%20exist%20-%20however%2C%20we%20find%20that%20their%20integration%20into%20LVLMs%20is%0Anon-trivial.%20Herein%2C%20we%20present%20Video%20Self-Training%20with%20augmented%20Reasoning%0A%28Video-STaR%29%2C%20the%20first%20video%20self-training%20approach.%20Video-STaR%20allows%20the%0Autilization%20of%20any%20labeled%20video%20dataset%20for%20video%20instruction%20tuning.%20In%0AVideo-STaR%2C%20an%20LVLM%20cycles%20between%20instruction%20generation%20and%20finetuning%2C%20which%0Awe%20show%20%28I%29%20improves%20general%20video%20understanding%20and%20%28II%29%20adapts%20LVLMs%20to%20novel%0Adownstream%20tasks%20with%20existing%20supervision.%20During%20generation%2C%20an%20LVLM%20is%0Aprompted%20to%20propose%20an%20answer.%20The%20answers%20are%20then%20filtered%20only%20to%20those%20that%0Acontain%20the%20original%20video%20labels%2C%20and%20the%20LVLM%20is%20then%20re-trained%20on%20the%0Agenerated%20dataset.%20By%20only%20training%20on%20generated%20answers%20that%20contain%20the%0Acorrect%20video%20labels%2C%20Video-STaR%20utilizes%20these%20existing%20video%20labels%20as%20weak%0Asupervision%20for%20video%20instruction%20tuning.%20Our%20results%20demonstrate%20that%0AVideo-STaR-enhanced%20LVLMs%20exhibit%20improved%20performance%20in%20%28I%29%20general%20video%20QA%2C%0Awhere%20TempCompass%20performance%20improved%20by%2010%25%2C%20and%20%28II%29%20on%20downstream%20tasks%2C%0Awhere%20Video-STaR%20improved%20Kinetics700-QA%20accuracy%20by%2020%25%20and%20action%20quality%0Aassessment%20on%20FineDiving%20by%2015%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06189v1&entry.124074799=Read"},
{"title": "STMR: Spiral Transformer for Hand Mesh Reconstruction", "author": "Huilong Xie and Wenwei Song and Wenxiong Kang and Yihong Lin", "abstract": "  Recent advancements in both transformer-based methods and spiral neighbor\nsampling techniques have greatly enhanced hand mesh reconstruction.\nTransformers excel in capturing complex vertex relationships, and spiral\nneighbor sampling is vital for utilizing topological structures. This paper\ningeniously integrates spiral sampling into the Transformer architecture,\nenhancing its ability to leverage mesh topology for superior performance in\nhand mesh reconstruction, resulting in substantial accuracy boosts. STMR\nemploys a single image encoder for model efficiency. To augment its information\nextraction capability, we design the multi-scale pose feature extraction\n(MSPFE) module, which facilitates the extraction of rich pose features,\nultimately enhancing the model's performance. Moreover, the proposed predefined\npose-to-vertex lifting (PPVL) method improves vertex feature representation,\nfurther boosting reconstruction performance. Extensive experiments on the\nFreiHAND dataset demonstrate the state-of-the-art performance and unparalleled\ninference speed of STMR compared with similar backbone methods, showcasing its\nefficiency and effectiveness. The code is available at\nhttps://github.com/SmallXieGithub/STMR.\n", "link": "http://arxiv.org/abs/2407.05967v1", "date": "2024-07-08", "relevancy": 2.1407, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5565}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5351}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STMR%3A%20Spiral%20Transformer%20for%20Hand%20Mesh%20Reconstruction&body=Title%3A%20STMR%3A%20Spiral%20Transformer%20for%20Hand%20Mesh%20Reconstruction%0AAuthor%3A%20Huilong%20Xie%20and%20Wenwei%20Song%20and%20Wenxiong%20Kang%20and%20Yihong%20Lin%0AAbstract%3A%20%20%20Recent%20advancements%20in%20both%20transformer-based%20methods%20and%20spiral%20neighbor%0Asampling%20techniques%20have%20greatly%20enhanced%20hand%20mesh%20reconstruction.%0ATransformers%20excel%20in%20capturing%20complex%20vertex%20relationships%2C%20and%20spiral%0Aneighbor%20sampling%20is%20vital%20for%20utilizing%20topological%20structures.%20This%20paper%0Aingeniously%20integrates%20spiral%20sampling%20into%20the%20Transformer%20architecture%2C%0Aenhancing%20its%20ability%20to%20leverage%20mesh%20topology%20for%20superior%20performance%20in%0Ahand%20mesh%20reconstruction%2C%20resulting%20in%20substantial%20accuracy%20boosts.%20STMR%0Aemploys%20a%20single%20image%20encoder%20for%20model%20efficiency.%20To%20augment%20its%20information%0Aextraction%20capability%2C%20we%20design%20the%20multi-scale%20pose%20feature%20extraction%0A%28MSPFE%29%20module%2C%20which%20facilitates%20the%20extraction%20of%20rich%20pose%20features%2C%0Aultimately%20enhancing%20the%20model%27s%20performance.%20Moreover%2C%20the%20proposed%20predefined%0Apose-to-vertex%20lifting%20%28PPVL%29%20method%20improves%20vertex%20feature%20representation%2C%0Afurther%20boosting%20reconstruction%20performance.%20Extensive%20experiments%20on%20the%0AFreiHAND%20dataset%20demonstrate%20the%20state-of-the-art%20performance%20and%20unparalleled%0Ainference%20speed%20of%20STMR%20compared%20with%20similar%20backbone%20methods%2C%20showcasing%20its%0Aefficiency%20and%20effectiveness.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/SmallXieGithub/STMR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTMR%253A%2520Spiral%2520Transformer%2520for%2520Hand%2520Mesh%2520Reconstruction%26entry.906535625%3DHuilong%2520Xie%2520and%2520Wenwei%2520Song%2520and%2520Wenxiong%2520Kang%2520and%2520Yihong%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520both%2520transformer-based%2520methods%2520and%2520spiral%2520neighbor%250Asampling%2520techniques%2520have%2520greatly%2520enhanced%2520hand%2520mesh%2520reconstruction.%250ATransformers%2520excel%2520in%2520capturing%2520complex%2520vertex%2520relationships%252C%2520and%2520spiral%250Aneighbor%2520sampling%2520is%2520vital%2520for%2520utilizing%2520topological%2520structures.%2520This%2520paper%250Aingeniously%2520integrates%2520spiral%2520sampling%2520into%2520the%2520Transformer%2520architecture%252C%250Aenhancing%2520its%2520ability%2520to%2520leverage%2520mesh%2520topology%2520for%2520superior%2520performance%2520in%250Ahand%2520mesh%2520reconstruction%252C%2520resulting%2520in%2520substantial%2520accuracy%2520boosts.%2520STMR%250Aemploys%2520a%2520single%2520image%2520encoder%2520for%2520model%2520efficiency.%2520To%2520augment%2520its%2520information%250Aextraction%2520capability%252C%2520we%2520design%2520the%2520multi-scale%2520pose%2520feature%2520extraction%250A%2528MSPFE%2529%2520module%252C%2520which%2520facilitates%2520the%2520extraction%2520of%2520rich%2520pose%2520features%252C%250Aultimately%2520enhancing%2520the%2520model%2527s%2520performance.%2520Moreover%252C%2520the%2520proposed%2520predefined%250Apose-to-vertex%2520lifting%2520%2528PPVL%2529%2520method%2520improves%2520vertex%2520feature%2520representation%252C%250Afurther%2520boosting%2520reconstruction%2520performance.%2520Extensive%2520experiments%2520on%2520the%250AFreiHAND%2520dataset%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520and%2520unparalleled%250Ainference%2520speed%2520of%2520STMR%2520compared%2520with%2520similar%2520backbone%2520methods%252C%2520showcasing%2520its%250Aefficiency%2520and%2520effectiveness.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/SmallXieGithub/STMR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STMR%3A%20Spiral%20Transformer%20for%20Hand%20Mesh%20Reconstruction&entry.906535625=Huilong%20Xie%20and%20Wenwei%20Song%20and%20Wenxiong%20Kang%20and%20Yihong%20Lin&entry.1292438233=%20%20Recent%20advancements%20in%20both%20transformer-based%20methods%20and%20spiral%20neighbor%0Asampling%20techniques%20have%20greatly%20enhanced%20hand%20mesh%20reconstruction.%0ATransformers%20excel%20in%20capturing%20complex%20vertex%20relationships%2C%20and%20spiral%0Aneighbor%20sampling%20is%20vital%20for%20utilizing%20topological%20structures.%20This%20paper%0Aingeniously%20integrates%20spiral%20sampling%20into%20the%20Transformer%20architecture%2C%0Aenhancing%20its%20ability%20to%20leverage%20mesh%20topology%20for%20superior%20performance%20in%0Ahand%20mesh%20reconstruction%2C%20resulting%20in%20substantial%20accuracy%20boosts.%20STMR%0Aemploys%20a%20single%20image%20encoder%20for%20model%20efficiency.%20To%20augment%20its%20information%0Aextraction%20capability%2C%20we%20design%20the%20multi-scale%20pose%20feature%20extraction%0A%28MSPFE%29%20module%2C%20which%20facilitates%20the%20extraction%20of%20rich%20pose%20features%2C%0Aultimately%20enhancing%20the%20model%27s%20performance.%20Moreover%2C%20the%20proposed%20predefined%0Apose-to-vertex%20lifting%20%28PPVL%29%20method%20improves%20vertex%20feature%20representation%2C%0Afurther%20boosting%20reconstruction%20performance.%20Extensive%20experiments%20on%20the%0AFreiHAND%20dataset%20demonstrate%20the%20state-of-the-art%20performance%20and%20unparalleled%0Ainference%20speed%20of%20STMR%20compared%20with%20similar%20backbone%20methods%2C%20showcasing%20its%0Aefficiency%20and%20effectiveness.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/SmallXieGithub/STMR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05967v1&entry.124074799=Read"},
{"title": "MMIS: Multimodal Dataset for Interior Scene Visual Generation and\n  Recognition", "author": "Hozaifa Kassab and Ahmed Mahmoud and Mohamed Bahaa and Ammar Mohamed and Ali Hamdi", "abstract": "  We introduce MMIS, a novel dataset designed to advance MultiModal Interior\nScene generation and recognition. MMIS consists of nearly 160,000 images. Each\nimage within the dataset is accompanied by its corresponding textual\ndescription and an audio recording of that description, providing rich and\ndiverse sources of information for scene generation and recognition. MMIS\nencompasses a wide range of interior spaces, capturing various styles, layouts,\nand furnishings. To construct this dataset, we employed careful processes\ninvolving the collection of images, the generation of textual descriptions, and\ncorresponding speech annotations. The presented dataset contributes to research\nin multi-modal representation learning tasks such as image generation,\nretrieval, captioning, and classification.\n", "link": "http://arxiv.org/abs/2407.05980v1", "date": "2024-07-08", "relevancy": 2.1172, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5422}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5208}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMIS%3A%20Multimodal%20Dataset%20for%20Interior%20Scene%20Visual%20Generation%20and%0A%20%20Recognition&body=Title%3A%20MMIS%3A%20Multimodal%20Dataset%20for%20Interior%20Scene%20Visual%20Generation%20and%0A%20%20Recognition%0AAuthor%3A%20Hozaifa%20Kassab%20and%20Ahmed%20Mahmoud%20and%20Mohamed%20Bahaa%20and%20Ammar%20Mohamed%20and%20Ali%20Hamdi%0AAbstract%3A%20%20%20We%20introduce%20MMIS%2C%20a%20novel%20dataset%20designed%20to%20advance%20MultiModal%20Interior%0AScene%20generation%20and%20recognition.%20MMIS%20consists%20of%20nearly%20160%2C000%20images.%20Each%0Aimage%20within%20the%20dataset%20is%20accompanied%20by%20its%20corresponding%20textual%0Adescription%20and%20an%20audio%20recording%20of%20that%20description%2C%20providing%20rich%20and%0Adiverse%20sources%20of%20information%20for%20scene%20generation%20and%20recognition.%20MMIS%0Aencompasses%20a%20wide%20range%20of%20interior%20spaces%2C%20capturing%20various%20styles%2C%20layouts%2C%0Aand%20furnishings.%20To%20construct%20this%20dataset%2C%20we%20employed%20careful%20processes%0Ainvolving%20the%20collection%20of%20images%2C%20the%20generation%20of%20textual%20descriptions%2C%20and%0Acorresponding%20speech%20annotations.%20The%20presented%20dataset%20contributes%20to%20research%0Ain%20multi-modal%20representation%20learning%20tasks%20such%20as%20image%20generation%2C%0Aretrieval%2C%20captioning%2C%20and%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMIS%253A%2520Multimodal%2520Dataset%2520for%2520Interior%2520Scene%2520Visual%2520Generation%2520and%250A%2520%2520Recognition%26entry.906535625%3DHozaifa%2520Kassab%2520and%2520Ahmed%2520Mahmoud%2520and%2520Mohamed%2520Bahaa%2520and%2520Ammar%2520Mohamed%2520and%2520Ali%2520Hamdi%26entry.1292438233%3D%2520%2520We%2520introduce%2520MMIS%252C%2520a%2520novel%2520dataset%2520designed%2520to%2520advance%2520MultiModal%2520Interior%250AScene%2520generation%2520and%2520recognition.%2520MMIS%2520consists%2520of%2520nearly%2520160%252C000%2520images.%2520Each%250Aimage%2520within%2520the%2520dataset%2520is%2520accompanied%2520by%2520its%2520corresponding%2520textual%250Adescription%2520and%2520an%2520audio%2520recording%2520of%2520that%2520description%252C%2520providing%2520rich%2520and%250Adiverse%2520sources%2520of%2520information%2520for%2520scene%2520generation%2520and%2520recognition.%2520MMIS%250Aencompasses%2520a%2520wide%2520range%2520of%2520interior%2520spaces%252C%2520capturing%2520various%2520styles%252C%2520layouts%252C%250Aand%2520furnishings.%2520To%2520construct%2520this%2520dataset%252C%2520we%2520employed%2520careful%2520processes%250Ainvolving%2520the%2520collection%2520of%2520images%252C%2520the%2520generation%2520of%2520textual%2520descriptions%252C%2520and%250Acorresponding%2520speech%2520annotations.%2520The%2520presented%2520dataset%2520contributes%2520to%2520research%250Ain%2520multi-modal%2520representation%2520learning%2520tasks%2520such%2520as%2520image%2520generation%252C%250Aretrieval%252C%2520captioning%252C%2520and%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMIS%3A%20Multimodal%20Dataset%20for%20Interior%20Scene%20Visual%20Generation%20and%0A%20%20Recognition&entry.906535625=Hozaifa%20Kassab%20and%20Ahmed%20Mahmoud%20and%20Mohamed%20Bahaa%20and%20Ammar%20Mohamed%20and%20Ali%20Hamdi&entry.1292438233=%20%20We%20introduce%20MMIS%2C%20a%20novel%20dataset%20designed%20to%20advance%20MultiModal%20Interior%0AScene%20generation%20and%20recognition.%20MMIS%20consists%20of%20nearly%20160%2C000%20images.%20Each%0Aimage%20within%20the%20dataset%20is%20accompanied%20by%20its%20corresponding%20textual%0Adescription%20and%20an%20audio%20recording%20of%20that%20description%2C%20providing%20rich%20and%0Adiverse%20sources%20of%20information%20for%20scene%20generation%20and%20recognition.%20MMIS%0Aencompasses%20a%20wide%20range%20of%20interior%20spaces%2C%20capturing%20various%20styles%2C%20layouts%2C%0Aand%20furnishings.%20To%20construct%20this%20dataset%2C%20we%20employed%20careful%20processes%0Ainvolving%20the%20collection%20of%20images%2C%20the%20generation%20of%20textual%20descriptions%2C%20and%0Acorresponding%20speech%20annotations.%20The%20presented%20dataset%20contributes%20to%20research%0Ain%20multi-modal%20representation%20learning%20tasks%20such%20as%20image%20generation%2C%0Aretrieval%2C%20captioning%2C%20and%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05980v1&entry.124074799=Read"},
{"title": "A Computer Vision Approach to Estimate the Localized Sea State", "author": "Aleksandar Vorkapic and Miran Pobar and Marina Ivasic-Kos", "abstract": "  This research presents a novel application of computer vision (CV) and deep\nlearning methods for real-time sea state recognition, aiming to contribute to\nimproving the operational safety and energy efficiency of seagoing vessels, key\nfactors in meeting the legislative carbon reduction targets. Our work focuses\non utilizing sea images in operational envelopes captured by a single\nstationary camera mounted on the ship bridge. The collected images are used to\ntrain a deep learning model to automatically recognize the state of the sea\nbased on the Beaufort scale. To recognize the sea state, we used 4\nstate-of-the-art deep neural networks with different characteristics that\nproved useful in various computer vision tasks: Resnet-101, NASNet,\nMobileNet_v2, and Transformer ViT-b32. Furthermore, we have defined a unique\nlarge-scale dataset, collected over a broad range of sea conditions from an\nocean-going vessel prepared for machine learning. We used the transfer learning\napproach to fine-tune the models on our dataset. The obtained results\ndemonstrate the potential for this approach to complement traditional methods,\nparticularly where in-situ measurements are unfeasible or interpolated weather\nbuoy data is insufficiently accurate. This study sets the groundwork for\nfurther development of sea state classification models to address recognized\ngaps in maritime research and enable safer and more efficient maritime\noperations.\n", "link": "http://arxiv.org/abs/2407.03755v2", "date": "2024-07-08", "relevancy": 2.1127, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5638}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Computer%20Vision%20Approach%20to%20Estimate%20the%20Localized%20Sea%20State&body=Title%3A%20A%20Computer%20Vision%20Approach%20to%20Estimate%20the%20Localized%20Sea%20State%0AAuthor%3A%20Aleksandar%20Vorkapic%20and%20Miran%20Pobar%20and%20Marina%20Ivasic-Kos%0AAbstract%3A%20%20%20This%20research%20presents%20a%20novel%20application%20of%20computer%20vision%20%28CV%29%20and%20deep%0Alearning%20methods%20for%20real-time%20sea%20state%20recognition%2C%20aiming%20to%20contribute%20to%0Aimproving%20the%20operational%20safety%20and%20energy%20efficiency%20of%20seagoing%20vessels%2C%20key%0Afactors%20in%20meeting%20the%20legislative%20carbon%20reduction%20targets.%20Our%20work%20focuses%0Aon%20utilizing%20sea%20images%20in%20operational%20envelopes%20captured%20by%20a%20single%0Astationary%20camera%20mounted%20on%20the%20ship%20bridge.%20The%20collected%20images%20are%20used%20to%0Atrain%20a%20deep%20learning%20model%20to%20automatically%20recognize%20the%20state%20of%20the%20sea%0Abased%20on%20the%20Beaufort%20scale.%20To%20recognize%20the%20sea%20state%2C%20we%20used%204%0Astate-of-the-art%20deep%20neural%20networks%20with%20different%20characteristics%20that%0Aproved%20useful%20in%20various%20computer%20vision%20tasks%3A%20Resnet-101%2C%20NASNet%2C%0AMobileNet_v2%2C%20and%20Transformer%20ViT-b32.%20Furthermore%2C%20we%20have%20defined%20a%20unique%0Alarge-scale%20dataset%2C%20collected%20over%20a%20broad%20range%20of%20sea%20conditions%20from%20an%0Aocean-going%20vessel%20prepared%20for%20machine%20learning.%20We%20used%20the%20transfer%20learning%0Aapproach%20to%20fine-tune%20the%20models%20on%20our%20dataset.%20The%20obtained%20results%0Ademonstrate%20the%20potential%20for%20this%20approach%20to%20complement%20traditional%20methods%2C%0Aparticularly%20where%20in-situ%20measurements%20are%20unfeasible%20or%20interpolated%20weather%0Abuoy%20data%20is%20insufficiently%20accurate.%20This%20study%20sets%20the%20groundwork%20for%0Afurther%20development%20of%20sea%20state%20classification%20models%20to%20address%20recognized%0Agaps%20in%20maritime%20research%20and%20enable%20safer%20and%20more%20efficient%20maritime%0Aoperations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Computer%2520Vision%2520Approach%2520to%2520Estimate%2520the%2520Localized%2520Sea%2520State%26entry.906535625%3DAleksandar%2520Vorkapic%2520and%2520Miran%2520Pobar%2520and%2520Marina%2520Ivasic-Kos%26entry.1292438233%3D%2520%2520This%2520research%2520presents%2520a%2520novel%2520application%2520of%2520computer%2520vision%2520%2528CV%2529%2520and%2520deep%250Alearning%2520methods%2520for%2520real-time%2520sea%2520state%2520recognition%252C%2520aiming%2520to%2520contribute%2520to%250Aimproving%2520the%2520operational%2520safety%2520and%2520energy%2520efficiency%2520of%2520seagoing%2520vessels%252C%2520key%250Afactors%2520in%2520meeting%2520the%2520legislative%2520carbon%2520reduction%2520targets.%2520Our%2520work%2520focuses%250Aon%2520utilizing%2520sea%2520images%2520in%2520operational%2520envelopes%2520captured%2520by%2520a%2520single%250Astationary%2520camera%2520mounted%2520on%2520the%2520ship%2520bridge.%2520The%2520collected%2520images%2520are%2520used%2520to%250Atrain%2520a%2520deep%2520learning%2520model%2520to%2520automatically%2520recognize%2520the%2520state%2520of%2520the%2520sea%250Abased%2520on%2520the%2520Beaufort%2520scale.%2520To%2520recognize%2520the%2520sea%2520state%252C%2520we%2520used%25204%250Astate-of-the-art%2520deep%2520neural%2520networks%2520with%2520different%2520characteristics%2520that%250Aproved%2520useful%2520in%2520various%2520computer%2520vision%2520tasks%253A%2520Resnet-101%252C%2520NASNet%252C%250AMobileNet_v2%252C%2520and%2520Transformer%2520ViT-b32.%2520Furthermore%252C%2520we%2520have%2520defined%2520a%2520unique%250Alarge-scale%2520dataset%252C%2520collected%2520over%2520a%2520broad%2520range%2520of%2520sea%2520conditions%2520from%2520an%250Aocean-going%2520vessel%2520prepared%2520for%2520machine%2520learning.%2520We%2520used%2520the%2520transfer%2520learning%250Aapproach%2520to%2520fine-tune%2520the%2520models%2520on%2520our%2520dataset.%2520The%2520obtained%2520results%250Ademonstrate%2520the%2520potential%2520for%2520this%2520approach%2520to%2520complement%2520traditional%2520methods%252C%250Aparticularly%2520where%2520in-situ%2520measurements%2520are%2520unfeasible%2520or%2520interpolated%2520weather%250Abuoy%2520data%2520is%2520insufficiently%2520accurate.%2520This%2520study%2520sets%2520the%2520groundwork%2520for%250Afurther%2520development%2520of%2520sea%2520state%2520classification%2520models%2520to%2520address%2520recognized%250Agaps%2520in%2520maritime%2520research%2520and%2520enable%2520safer%2520and%2520more%2520efficient%2520maritime%250Aoperations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Computer%20Vision%20Approach%20to%20Estimate%20the%20Localized%20Sea%20State&entry.906535625=Aleksandar%20Vorkapic%20and%20Miran%20Pobar%20and%20Marina%20Ivasic-Kos&entry.1292438233=%20%20This%20research%20presents%20a%20novel%20application%20of%20computer%20vision%20%28CV%29%20and%20deep%0Alearning%20methods%20for%20real-time%20sea%20state%20recognition%2C%20aiming%20to%20contribute%20to%0Aimproving%20the%20operational%20safety%20and%20energy%20efficiency%20of%20seagoing%20vessels%2C%20key%0Afactors%20in%20meeting%20the%20legislative%20carbon%20reduction%20targets.%20Our%20work%20focuses%0Aon%20utilizing%20sea%20images%20in%20operational%20envelopes%20captured%20by%20a%20single%0Astationary%20camera%20mounted%20on%20the%20ship%20bridge.%20The%20collected%20images%20are%20used%20to%0Atrain%20a%20deep%20learning%20model%20to%20automatically%20recognize%20the%20state%20of%20the%20sea%0Abased%20on%20the%20Beaufort%20scale.%20To%20recognize%20the%20sea%20state%2C%20we%20used%204%0Astate-of-the-art%20deep%20neural%20networks%20with%20different%20characteristics%20that%0Aproved%20useful%20in%20various%20computer%20vision%20tasks%3A%20Resnet-101%2C%20NASNet%2C%0AMobileNet_v2%2C%20and%20Transformer%20ViT-b32.%20Furthermore%2C%20we%20have%20defined%20a%20unique%0Alarge-scale%20dataset%2C%20collected%20over%20a%20broad%20range%20of%20sea%20conditions%20from%20an%0Aocean-going%20vessel%20prepared%20for%20machine%20learning.%20We%20used%20the%20transfer%20learning%0Aapproach%20to%20fine-tune%20the%20models%20on%20our%20dataset.%20The%20obtained%20results%0Ademonstrate%20the%20potential%20for%20this%20approach%20to%20complement%20traditional%20methods%2C%0Aparticularly%20where%20in-situ%20measurements%20are%20unfeasible%20or%20interpolated%20weather%0Abuoy%20data%20is%20insufficiently%20accurate.%20This%20study%20sets%20the%20groundwork%20for%0Afurther%20development%20of%20sea%20state%20classification%20models%20to%20address%20recognized%0Agaps%20in%20maritime%20research%20and%20enable%20safer%20and%20more%20efficient%20maritime%0Aoperations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03755v2&entry.124074799=Read"},
{"title": "PDiscoFormer: Relaxing Part Discovery Constraints with Vision\n  Transformers", "author": "Ananthu Aniraj and Cassio F. Dantas and Dino Ienco and Diego Marcos", "abstract": "  Computer vision methods that explicitly detect object parts and reason on\nthem are a step towards inherently interpretable models. Existing approaches\nthat perform part discovery driven by a fine-grained classification task make\nvery restrictive assumptions on the geometric properties of the discovered\nparts; they should be small and compact. Although this prior is useful in some\ncases, in this paper we show that pre-trained transformer-based vision models,\nsuch as self-supervised DINOv2 ViT, enable the relaxation of these constraints.\nIn particular, we find that a total variation (TV) prior, which allows for\nmultiple connected components of any size, substantially outperforms previous\nwork. We test our approach on three fine-grained classification benchmarks:\nCUB, PartImageNet and Oxford Flowers, and compare our results to previously\npublished methods as well as a re-implementation of the state-of-the-art method\nPDiscoNet with a transformer-based backbone. We consistently obtain substantial\nimprovements across the board, both on part discovery metrics and the\ndownstream classification task, showing that the strong inductive biases in\nself-supervised ViT models require to rethink the geometric priors that can be\nused for unsupervised part discovery.\n", "link": "http://arxiv.org/abs/2407.04538v2", "date": "2024-07-08", "relevancy": 2.1105, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5385}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5265}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PDiscoFormer%3A%20Relaxing%20Part%20Discovery%20Constraints%20with%20Vision%0A%20%20Transformers&body=Title%3A%20PDiscoFormer%3A%20Relaxing%20Part%20Discovery%20Constraints%20with%20Vision%0A%20%20Transformers%0AAuthor%3A%20Ananthu%20Aniraj%20and%20Cassio%20F.%20Dantas%20and%20Dino%20Ienco%20and%20Diego%20Marcos%0AAbstract%3A%20%20%20Computer%20vision%20methods%20that%20explicitly%20detect%20object%20parts%20and%20reason%20on%0Athem%20are%20a%20step%20towards%20inherently%20interpretable%20models.%20Existing%20approaches%0Athat%20perform%20part%20discovery%20driven%20by%20a%20fine-grained%20classification%20task%20make%0Avery%20restrictive%20assumptions%20on%20the%20geometric%20properties%20of%20the%20discovered%0Aparts%3B%20they%20should%20be%20small%20and%20compact.%20Although%20this%20prior%20is%20useful%20in%20some%0Acases%2C%20in%20this%20paper%20we%20show%20that%20pre-trained%20transformer-based%20vision%20models%2C%0Asuch%20as%20self-supervised%20DINOv2%20ViT%2C%20enable%20the%20relaxation%20of%20these%20constraints.%0AIn%20particular%2C%20we%20find%20that%20a%20total%20variation%20%28TV%29%20prior%2C%20which%20allows%20for%0Amultiple%20connected%20components%20of%20any%20size%2C%20substantially%20outperforms%20previous%0Awork.%20We%20test%20our%20approach%20on%20three%20fine-grained%20classification%20benchmarks%3A%0ACUB%2C%20PartImageNet%20and%20Oxford%20Flowers%2C%20and%20compare%20our%20results%20to%20previously%0Apublished%20methods%20as%20well%20as%20a%20re-implementation%20of%20the%20state-of-the-art%20method%0APDiscoNet%20with%20a%20transformer-based%20backbone.%20We%20consistently%20obtain%20substantial%0Aimprovements%20across%20the%20board%2C%20both%20on%20part%20discovery%20metrics%20and%20the%0Adownstream%20classification%20task%2C%20showing%20that%20the%20strong%20inductive%20biases%20in%0Aself-supervised%20ViT%20models%20require%20to%20rethink%20the%20geometric%20priors%20that%20can%20be%0Aused%20for%20unsupervised%20part%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04538v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPDiscoFormer%253A%2520Relaxing%2520Part%2520Discovery%2520Constraints%2520with%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DAnanthu%2520Aniraj%2520and%2520Cassio%2520F.%2520Dantas%2520and%2520Dino%2520Ienco%2520and%2520Diego%2520Marcos%26entry.1292438233%3D%2520%2520Computer%2520vision%2520methods%2520that%2520explicitly%2520detect%2520object%2520parts%2520and%2520reason%2520on%250Athem%2520are%2520a%2520step%2520towards%2520inherently%2520interpretable%2520models.%2520Existing%2520approaches%250Athat%2520perform%2520part%2520discovery%2520driven%2520by%2520a%2520fine-grained%2520classification%2520task%2520make%250Avery%2520restrictive%2520assumptions%2520on%2520the%2520geometric%2520properties%2520of%2520the%2520discovered%250Aparts%253B%2520they%2520should%2520be%2520small%2520and%2520compact.%2520Although%2520this%2520prior%2520is%2520useful%2520in%2520some%250Acases%252C%2520in%2520this%2520paper%2520we%2520show%2520that%2520pre-trained%2520transformer-based%2520vision%2520models%252C%250Asuch%2520as%2520self-supervised%2520DINOv2%2520ViT%252C%2520enable%2520the%2520relaxation%2520of%2520these%2520constraints.%250AIn%2520particular%252C%2520we%2520find%2520that%2520a%2520total%2520variation%2520%2528TV%2529%2520prior%252C%2520which%2520allows%2520for%250Amultiple%2520connected%2520components%2520of%2520any%2520size%252C%2520substantially%2520outperforms%2520previous%250Awork.%2520We%2520test%2520our%2520approach%2520on%2520three%2520fine-grained%2520classification%2520benchmarks%253A%250ACUB%252C%2520PartImageNet%2520and%2520Oxford%2520Flowers%252C%2520and%2520compare%2520our%2520results%2520to%2520previously%250Apublished%2520methods%2520as%2520well%2520as%2520a%2520re-implementation%2520of%2520the%2520state-of-the-art%2520method%250APDiscoNet%2520with%2520a%2520transformer-based%2520backbone.%2520We%2520consistently%2520obtain%2520substantial%250Aimprovements%2520across%2520the%2520board%252C%2520both%2520on%2520part%2520discovery%2520metrics%2520and%2520the%250Adownstream%2520classification%2520task%252C%2520showing%2520that%2520the%2520strong%2520inductive%2520biases%2520in%250Aself-supervised%2520ViT%2520models%2520require%2520to%2520rethink%2520the%2520geometric%2520priors%2520that%2520can%2520be%250Aused%2520for%2520unsupervised%2520part%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04538v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDiscoFormer%3A%20Relaxing%20Part%20Discovery%20Constraints%20with%20Vision%0A%20%20Transformers&entry.906535625=Ananthu%20Aniraj%20and%20Cassio%20F.%20Dantas%20and%20Dino%20Ienco%20and%20Diego%20Marcos&entry.1292438233=%20%20Computer%20vision%20methods%20that%20explicitly%20detect%20object%20parts%20and%20reason%20on%0Athem%20are%20a%20step%20towards%20inherently%20interpretable%20models.%20Existing%20approaches%0Athat%20perform%20part%20discovery%20driven%20by%20a%20fine-grained%20classification%20task%20make%0Avery%20restrictive%20assumptions%20on%20the%20geometric%20properties%20of%20the%20discovered%0Aparts%3B%20they%20should%20be%20small%20and%20compact.%20Although%20this%20prior%20is%20useful%20in%20some%0Acases%2C%20in%20this%20paper%20we%20show%20that%20pre-trained%20transformer-based%20vision%20models%2C%0Asuch%20as%20self-supervised%20DINOv2%20ViT%2C%20enable%20the%20relaxation%20of%20these%20constraints.%0AIn%20particular%2C%20we%20find%20that%20a%20total%20variation%20%28TV%29%20prior%2C%20which%20allows%20for%0Amultiple%20connected%20components%20of%20any%20size%2C%20substantially%20outperforms%20previous%0Awork.%20We%20test%20our%20approach%20on%20three%20fine-grained%20classification%20benchmarks%3A%0ACUB%2C%20PartImageNet%20and%20Oxford%20Flowers%2C%20and%20compare%20our%20results%20to%20previously%0Apublished%20methods%20as%20well%20as%20a%20re-implementation%20of%20the%20state-of-the-art%20method%0APDiscoNet%20with%20a%20transformer-based%20backbone.%20We%20consistently%20obtain%20substantial%0Aimprovements%20across%20the%20board%2C%20both%20on%20part%20discovery%20metrics%20and%20the%0Adownstream%20classification%20task%2C%20showing%20that%20the%20strong%20inductive%20biases%20in%0Aself-supervised%20ViT%20models%20require%20to%20rethink%20the%20geometric%20priors%20that%20can%20be%0Aused%20for%20unsupervised%20part%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04538v2&entry.124074799=Read"},
{"title": "Graph-Boosted Attentive Network for Semantic Body Parsing", "author": "Tinghuai Wang and Huiling Wang", "abstract": "  Human body parsing remains a challenging problem in natural scenes due to\nmulti-instance and inter-part semantic confusions as well as occlusions. This\npaper proposes a novel approach to decomposing multiple human bodies into\nsemantic part regions in unconstrained environments. Specifically we propose a\nconvolutional neural network (CNN) architecture which comprises of novel\nsemantic and contour attention mechanisms across feature hierarchy to resolve\nthe semantic ambiguities and boundary localization issues related to semantic\nbody parsing. We further propose to encode estimated pose as higher-level\ncontextual information which is combined with local semantic cues in a novel\ngraphical model in a principled manner. In this proposed model, the lower-level\nsemantic cues can be recursively updated by propagating higher-level contextual\ninformation from estimated pose and vice versa across the graph, so as to\nalleviate erroneous pose information and pixel level predictions. We further\npropose an optimization technique to efficiently derive the solutions. Our\nproposed method achieves the state-of-art results on the challenging Pascal\nPerson-Part dataset.\n", "link": "http://arxiv.org/abs/2407.05924v1", "date": "2024-07-08", "relevancy": 2.0979, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5648}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5234}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Boosted%20Attentive%20Network%20for%20Semantic%20Body%20Parsing&body=Title%3A%20Graph-Boosted%20Attentive%20Network%20for%20Semantic%20Body%20Parsing%0AAuthor%3A%20Tinghuai%20Wang%20and%20Huiling%20Wang%0AAbstract%3A%20%20%20Human%20body%20parsing%20remains%20a%20challenging%20problem%20in%20natural%20scenes%20due%20to%0Amulti-instance%20and%20inter-part%20semantic%20confusions%20as%20well%20as%20occlusions.%20This%0Apaper%20proposes%20a%20novel%20approach%20to%20decomposing%20multiple%20human%20bodies%20into%0Asemantic%20part%20regions%20in%20unconstrained%20environments.%20Specifically%20we%20propose%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20architecture%20which%20comprises%20of%20novel%0Asemantic%20and%20contour%20attention%20mechanisms%20across%20feature%20hierarchy%20to%20resolve%0Athe%20semantic%20ambiguities%20and%20boundary%20localization%20issues%20related%20to%20semantic%0Abody%20parsing.%20We%20further%20propose%20to%20encode%20estimated%20pose%20as%20higher-level%0Acontextual%20information%20which%20is%20combined%20with%20local%20semantic%20cues%20in%20a%20novel%0Agraphical%20model%20in%20a%20principled%20manner.%20In%20this%20proposed%20model%2C%20the%20lower-level%0Asemantic%20cues%20can%20be%20recursively%20updated%20by%20propagating%20higher-level%20contextual%0Ainformation%20from%20estimated%20pose%20and%20vice%20versa%20across%20the%20graph%2C%20so%20as%20to%0Aalleviate%20erroneous%20pose%20information%20and%20pixel%20level%20predictions.%20We%20further%0Apropose%20an%20optimization%20technique%20to%20efficiently%20derive%20the%20solutions.%20Our%0Aproposed%20method%20achieves%20the%20state-of-art%20results%20on%20the%20challenging%20Pascal%0APerson-Part%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Boosted%2520Attentive%2520Network%2520for%2520Semantic%2520Body%2520Parsing%26entry.906535625%3DTinghuai%2520Wang%2520and%2520Huiling%2520Wang%26entry.1292438233%3D%2520%2520Human%2520body%2520parsing%2520remains%2520a%2520challenging%2520problem%2520in%2520natural%2520scenes%2520due%2520to%250Amulti-instance%2520and%2520inter-part%2520semantic%2520confusions%2520as%2520well%2520as%2520occlusions.%2520This%250Apaper%2520proposes%2520a%2520novel%2520approach%2520to%2520decomposing%2520multiple%2520human%2520bodies%2520into%250Asemantic%2520part%2520regions%2520in%2520unconstrained%2520environments.%2520Specifically%2520we%2520propose%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520architecture%2520which%2520comprises%2520of%2520novel%250Asemantic%2520and%2520contour%2520attention%2520mechanisms%2520across%2520feature%2520hierarchy%2520to%2520resolve%250Athe%2520semantic%2520ambiguities%2520and%2520boundary%2520localization%2520issues%2520related%2520to%2520semantic%250Abody%2520parsing.%2520We%2520further%2520propose%2520to%2520encode%2520estimated%2520pose%2520as%2520higher-level%250Acontextual%2520information%2520which%2520is%2520combined%2520with%2520local%2520semantic%2520cues%2520in%2520a%2520novel%250Agraphical%2520model%2520in%2520a%2520principled%2520manner.%2520In%2520this%2520proposed%2520model%252C%2520the%2520lower-level%250Asemantic%2520cues%2520can%2520be%2520recursively%2520updated%2520by%2520propagating%2520higher-level%2520contextual%250Ainformation%2520from%2520estimated%2520pose%2520and%2520vice%2520versa%2520across%2520the%2520graph%252C%2520so%2520as%2520to%250Aalleviate%2520erroneous%2520pose%2520information%2520and%2520pixel%2520level%2520predictions.%2520We%2520further%250Apropose%2520an%2520optimization%2520technique%2520to%2520efficiently%2520derive%2520the%2520solutions.%2520Our%250Aproposed%2520method%2520achieves%2520the%2520state-of-art%2520results%2520on%2520the%2520challenging%2520Pascal%250APerson-Part%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Boosted%20Attentive%20Network%20for%20Semantic%20Body%20Parsing&entry.906535625=Tinghuai%20Wang%20and%20Huiling%20Wang&entry.1292438233=%20%20Human%20body%20parsing%20remains%20a%20challenging%20problem%20in%20natural%20scenes%20due%20to%0Amulti-instance%20and%20inter-part%20semantic%20confusions%20as%20well%20as%20occlusions.%20This%0Apaper%20proposes%20a%20novel%20approach%20to%20decomposing%20multiple%20human%20bodies%20into%0Asemantic%20part%20regions%20in%20unconstrained%20environments.%20Specifically%20we%20propose%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20architecture%20which%20comprises%20of%20novel%0Asemantic%20and%20contour%20attention%20mechanisms%20across%20feature%20hierarchy%20to%20resolve%0Athe%20semantic%20ambiguities%20and%20boundary%20localization%20issues%20related%20to%20semantic%0Abody%20parsing.%20We%20further%20propose%20to%20encode%20estimated%20pose%20as%20higher-level%0Acontextual%20information%20which%20is%20combined%20with%20local%20semantic%20cues%20in%20a%20novel%0Agraphical%20model%20in%20a%20principled%20manner.%20In%20this%20proposed%20model%2C%20the%20lower-level%0Asemantic%20cues%20can%20be%20recursively%20updated%20by%20propagating%20higher-level%20contextual%0Ainformation%20from%20estimated%20pose%20and%20vice%20versa%20across%20the%20graph%2C%20so%20as%20to%0Aalleviate%20erroneous%20pose%20information%20and%20pixel%20level%20predictions.%20We%20further%0Apropose%20an%20optimization%20technique%20to%20efficiently%20derive%20the%20solutions.%20Our%0Aproposed%20method%20achieves%20the%20state-of-art%20results%20on%20the%20challenging%20Pascal%0APerson-Part%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05924v1&entry.124074799=Read"},
{"title": "Active Label Refinement for Robust Training of Imbalanced Medical Image\n  Classification Tasks in the Presence of High Label Noise", "author": "Bidur Khanal and Tianhong Dai and Binod Bhattarai and Cristian Linte", "abstract": "  The robustness of supervised deep learning-based medical image classification\nis significantly undermined by label noise. Although several methods have been\nproposed to enhance classification performance in the presence of noisy labels,\nthey face some challenges: 1) a struggle with class-imbalanced datasets,\nleading to the frequent overlooking of minority classes as noisy samples; 2) a\nsingular focus on maximizing performance using noisy datasets, without\nincorporating experts-in-the-loop for actively cleaning the noisy labels. To\nmitigate these challenges, we propose a two-phase approach that combines\nLearning with Noisy Labels (LNL) and active learning. This approach not only\nimproves the robustness of medical image classification in the presence of\nnoisy labels, but also iteratively improves the quality of the dataset by\nrelabeling the important incorrect labels, under a limited annotation budget.\nFurthermore, we introduce a novel Variance of Gradients approach in LNL phase,\nwhich complements the loss-based sample selection by also sampling\nunder-represented samples. Using two imbalanced noisy medical classification\ndatasets, we demonstrate that that our proposed technique is superior to its\npredecessors at handling class imbalance by not misidentifying clean samples\nfrom minority classes as mostly noisy samples.\n", "link": "http://arxiv.org/abs/2407.05973v1", "date": "2024-07-08", "relevancy": 2.0973, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5618}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5223}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Label%20Refinement%20for%20Robust%20Training%20of%20Imbalanced%20Medical%20Image%0A%20%20Classification%20Tasks%20in%20the%20Presence%20of%20High%20Label%20Noise&body=Title%3A%20Active%20Label%20Refinement%20for%20Robust%20Training%20of%20Imbalanced%20Medical%20Image%0A%20%20Classification%20Tasks%20in%20the%20Presence%20of%20High%20Label%20Noise%0AAuthor%3A%20Bidur%20Khanal%20and%20Tianhong%20Dai%20and%20Binod%20Bhattarai%20and%20Cristian%20Linte%0AAbstract%3A%20%20%20The%20robustness%20of%20supervised%20deep%20learning-based%20medical%20image%20classification%0Ais%20significantly%20undermined%20by%20label%20noise.%20Although%20several%20methods%20have%20been%0Aproposed%20to%20enhance%20classification%20performance%20in%20the%20presence%20of%20noisy%20labels%2C%0Athey%20face%20some%20challenges%3A%201%29%20a%20struggle%20with%20class-imbalanced%20datasets%2C%0Aleading%20to%20the%20frequent%20overlooking%20of%20minority%20classes%20as%20noisy%20samples%3B%202%29%20a%0Asingular%20focus%20on%20maximizing%20performance%20using%20noisy%20datasets%2C%20without%0Aincorporating%20experts-in-the-loop%20for%20actively%20cleaning%20the%20noisy%20labels.%20To%0Amitigate%20these%20challenges%2C%20we%20propose%20a%20two-phase%20approach%20that%20combines%0ALearning%20with%20Noisy%20Labels%20%28LNL%29%20and%20active%20learning.%20This%20approach%20not%20only%0Aimproves%20the%20robustness%20of%20medical%20image%20classification%20in%20the%20presence%20of%0Anoisy%20labels%2C%20but%20also%20iteratively%20improves%20the%20quality%20of%20the%20dataset%20by%0Arelabeling%20the%20important%20incorrect%20labels%2C%20under%20a%20limited%20annotation%20budget.%0AFurthermore%2C%20we%20introduce%20a%20novel%20Variance%20of%20Gradients%20approach%20in%20LNL%20phase%2C%0Awhich%20complements%20the%20loss-based%20sample%20selection%20by%20also%20sampling%0Aunder-represented%20samples.%20Using%20two%20imbalanced%20noisy%20medical%20classification%0Adatasets%2C%20we%20demonstrate%20that%20that%20our%20proposed%20technique%20is%20superior%20to%20its%0Apredecessors%20at%20handling%20class%20imbalance%20by%20not%20misidentifying%20clean%20samples%0Afrom%20minority%20classes%20as%20mostly%20noisy%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Label%2520Refinement%2520for%2520Robust%2520Training%2520of%2520Imbalanced%2520Medical%2520Image%250A%2520%2520Classification%2520Tasks%2520in%2520the%2520Presence%2520of%2520High%2520Label%2520Noise%26entry.906535625%3DBidur%2520Khanal%2520and%2520Tianhong%2520Dai%2520and%2520Binod%2520Bhattarai%2520and%2520Cristian%2520Linte%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520supervised%2520deep%2520learning-based%2520medical%2520image%2520classification%250Ais%2520significantly%2520undermined%2520by%2520label%2520noise.%2520Although%2520several%2520methods%2520have%2520been%250Aproposed%2520to%2520enhance%2520classification%2520performance%2520in%2520the%2520presence%2520of%2520noisy%2520labels%252C%250Athey%2520face%2520some%2520challenges%253A%25201%2529%2520a%2520struggle%2520with%2520class-imbalanced%2520datasets%252C%250Aleading%2520to%2520the%2520frequent%2520overlooking%2520of%2520minority%2520classes%2520as%2520noisy%2520samples%253B%25202%2529%2520a%250Asingular%2520focus%2520on%2520maximizing%2520performance%2520using%2520noisy%2520datasets%252C%2520without%250Aincorporating%2520experts-in-the-loop%2520for%2520actively%2520cleaning%2520the%2520noisy%2520labels.%2520To%250Amitigate%2520these%2520challenges%252C%2520we%2520propose%2520a%2520two-phase%2520approach%2520that%2520combines%250ALearning%2520with%2520Noisy%2520Labels%2520%2528LNL%2529%2520and%2520active%2520learning.%2520This%2520approach%2520not%2520only%250Aimproves%2520the%2520robustness%2520of%2520medical%2520image%2520classification%2520in%2520the%2520presence%2520of%250Anoisy%2520labels%252C%2520but%2520also%2520iteratively%2520improves%2520the%2520quality%2520of%2520the%2520dataset%2520by%250Arelabeling%2520the%2520important%2520incorrect%2520labels%252C%2520under%2520a%2520limited%2520annotation%2520budget.%250AFurthermore%252C%2520we%2520introduce%2520a%2520novel%2520Variance%2520of%2520Gradients%2520approach%2520in%2520LNL%2520phase%252C%250Awhich%2520complements%2520the%2520loss-based%2520sample%2520selection%2520by%2520also%2520sampling%250Aunder-represented%2520samples.%2520Using%2520two%2520imbalanced%2520noisy%2520medical%2520classification%250Adatasets%252C%2520we%2520demonstrate%2520that%2520that%2520our%2520proposed%2520technique%2520is%2520superior%2520to%2520its%250Apredecessors%2520at%2520handling%2520class%2520imbalance%2520by%2520not%2520misidentifying%2520clean%2520samples%250Afrom%2520minority%2520classes%2520as%2520mostly%2520noisy%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Label%20Refinement%20for%20Robust%20Training%20of%20Imbalanced%20Medical%20Image%0A%20%20Classification%20Tasks%20in%20the%20Presence%20of%20High%20Label%20Noise&entry.906535625=Bidur%20Khanal%20and%20Tianhong%20Dai%20and%20Binod%20Bhattarai%20and%20Cristian%20Linte&entry.1292438233=%20%20The%20robustness%20of%20supervised%20deep%20learning-based%20medical%20image%20classification%0Ais%20significantly%20undermined%20by%20label%20noise.%20Although%20several%20methods%20have%20been%0Aproposed%20to%20enhance%20classification%20performance%20in%20the%20presence%20of%20noisy%20labels%2C%0Athey%20face%20some%20challenges%3A%201%29%20a%20struggle%20with%20class-imbalanced%20datasets%2C%0Aleading%20to%20the%20frequent%20overlooking%20of%20minority%20classes%20as%20noisy%20samples%3B%202%29%20a%0Asingular%20focus%20on%20maximizing%20performance%20using%20noisy%20datasets%2C%20without%0Aincorporating%20experts-in-the-loop%20for%20actively%20cleaning%20the%20noisy%20labels.%20To%0Amitigate%20these%20challenges%2C%20we%20propose%20a%20two-phase%20approach%20that%20combines%0ALearning%20with%20Noisy%20Labels%20%28LNL%29%20and%20active%20learning.%20This%20approach%20not%20only%0Aimproves%20the%20robustness%20of%20medical%20image%20classification%20in%20the%20presence%20of%0Anoisy%20labels%2C%20but%20also%20iteratively%20improves%20the%20quality%20of%20the%20dataset%20by%0Arelabeling%20the%20important%20incorrect%20labels%2C%20under%20a%20limited%20annotation%20budget.%0AFurthermore%2C%20we%20introduce%20a%20novel%20Variance%20of%20Gradients%20approach%20in%20LNL%20phase%2C%0Awhich%20complements%20the%20loss-based%20sample%20selection%20by%20also%20sampling%0Aunder-represented%20samples.%20Using%20two%20imbalanced%20noisy%20medical%20classification%0Adatasets%2C%20we%20demonstrate%20that%20that%20our%20proposed%20technique%20is%20superior%20to%20its%0Apredecessors%20at%20handling%20class%20imbalance%20by%20not%20misidentifying%20clean%20samples%0Afrom%20minority%20classes%20as%20mostly%20noisy%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05973v1&entry.124074799=Read"},
{"title": "Pan-denoising: Guided Hyperspectral Image Denoising via Weighted\n  Represent Coefficient Total Variation", "author": "Shuang Xu and Qiao Ke and Jiangjun Peng and Xiangyong Cao and Zixiang Zhao", "abstract": "  This paper introduces a novel paradigm for hyperspectral image (HSI)\ndenoising, which is termed \\textit{pan-denoising}. In a given scene,\npanchromatic (PAN) images capture similar structures and textures to HSIs but\nwith less noise. This enables the utilization of PAN images to guide the HSI\ndenoising process. Consequently, pan-denoising, which incorporates an\nadditional prior, has the potential to uncover underlying structures and\ndetails beyond the internal information modeling of traditional HSI denoising\nmethods. However, the proper modeling of this additional prior poses a\nsignificant challenge. To alleviate this issue, the paper proposes a novel\nregularization term, Panchromatic Weighted Representation Coefficient Total\nVariation (PWRCTV). It employs the gradient maps of PAN images to automatically\nassign different weights of TV regularization for each pixel, resulting in\nlarger weights for smooth areas and smaller weights for edges. This\nregularization forms the basis of a pan-denoising model, which is solved using\nthe Alternating Direction Method of Multipliers. Extensive experiments on\nsynthetic and real-world datasets demonstrate that PWRCTV outperforms several\nstate-of-the-art methods in terms of metrics and visual quality. Furthermore,\nan HSI classification experiment confirms that PWRCTV, as a preprocessing\nmethod, can enhance the performance of downstream classification tasks. The\ncode and data are available at https://github.com/shuangxu96/PWRCTV.\n", "link": "http://arxiv.org/abs/2407.06064v1", "date": "2024-07-08", "relevancy": 2.092, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5241}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pan-denoising%3A%20Guided%20Hyperspectral%20Image%20Denoising%20via%20Weighted%0A%20%20Represent%20Coefficient%20Total%20Variation&body=Title%3A%20Pan-denoising%3A%20Guided%20Hyperspectral%20Image%20Denoising%20via%20Weighted%0A%20%20Represent%20Coefficient%20Total%20Variation%0AAuthor%3A%20Shuang%20Xu%20and%20Qiao%20Ke%20and%20Jiangjun%20Peng%20and%20Xiangyong%20Cao%20and%20Zixiang%20Zhao%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20paradigm%20for%20hyperspectral%20image%20%28HSI%29%0Adenoising%2C%20which%20is%20termed%20%5Ctextit%7Bpan-denoising%7D.%20In%20a%20given%20scene%2C%0Apanchromatic%20%28PAN%29%20images%20capture%20similar%20structures%20and%20textures%20to%20HSIs%20but%0Awith%20less%20noise.%20This%20enables%20the%20utilization%20of%20PAN%20images%20to%20guide%20the%20HSI%0Adenoising%20process.%20Consequently%2C%20pan-denoising%2C%20which%20incorporates%20an%0Aadditional%20prior%2C%20has%20the%20potential%20to%20uncover%20underlying%20structures%20and%0Adetails%20beyond%20the%20internal%20information%20modeling%20of%20traditional%20HSI%20denoising%0Amethods.%20However%2C%20the%20proper%20modeling%20of%20this%20additional%20prior%20poses%20a%0Asignificant%20challenge.%20To%20alleviate%20this%20issue%2C%20the%20paper%20proposes%20a%20novel%0Aregularization%20term%2C%20Panchromatic%20Weighted%20Representation%20Coefficient%20Total%0AVariation%20%28PWRCTV%29.%20It%20employs%20the%20gradient%20maps%20of%20PAN%20images%20to%20automatically%0Aassign%20different%20weights%20of%20TV%20regularization%20for%20each%20pixel%2C%20resulting%20in%0Alarger%20weights%20for%20smooth%20areas%20and%20smaller%20weights%20for%20edges.%20This%0Aregularization%20forms%20the%20basis%20of%20a%20pan-denoising%20model%2C%20which%20is%20solved%20using%0Athe%20Alternating%20Direction%20Method%20of%20Multipliers.%20Extensive%20experiments%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20that%20PWRCTV%20outperforms%20several%0Astate-of-the-art%20methods%20in%20terms%20of%20metrics%20and%20visual%20quality.%20Furthermore%2C%0Aan%20HSI%20classification%20experiment%20confirms%20that%20PWRCTV%2C%20as%20a%20preprocessing%0Amethod%2C%20can%20enhance%20the%20performance%20of%20downstream%20classification%20tasks.%20The%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/shuangxu96/PWRCTV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPan-denoising%253A%2520Guided%2520Hyperspectral%2520Image%2520Denoising%2520via%2520Weighted%250A%2520%2520Represent%2520Coefficient%2520Total%2520Variation%26entry.906535625%3DShuang%2520Xu%2520and%2520Qiao%2520Ke%2520and%2520Jiangjun%2520Peng%2520and%2520Xiangyong%2520Cao%2520and%2520Zixiang%2520Zhao%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520paradigm%2520for%2520hyperspectral%2520image%2520%2528HSI%2529%250Adenoising%252C%2520which%2520is%2520termed%2520%255Ctextit%257Bpan-denoising%257D.%2520In%2520a%2520given%2520scene%252C%250Apanchromatic%2520%2528PAN%2529%2520images%2520capture%2520similar%2520structures%2520and%2520textures%2520to%2520HSIs%2520but%250Awith%2520less%2520noise.%2520This%2520enables%2520the%2520utilization%2520of%2520PAN%2520images%2520to%2520guide%2520the%2520HSI%250Adenoising%2520process.%2520Consequently%252C%2520pan-denoising%252C%2520which%2520incorporates%2520an%250Aadditional%2520prior%252C%2520has%2520the%2520potential%2520to%2520uncover%2520underlying%2520structures%2520and%250Adetails%2520beyond%2520the%2520internal%2520information%2520modeling%2520of%2520traditional%2520HSI%2520denoising%250Amethods.%2520However%252C%2520the%2520proper%2520modeling%2520of%2520this%2520additional%2520prior%2520poses%2520a%250Asignificant%2520challenge.%2520To%2520alleviate%2520this%2520issue%252C%2520the%2520paper%2520proposes%2520a%2520novel%250Aregularization%2520term%252C%2520Panchromatic%2520Weighted%2520Representation%2520Coefficient%2520Total%250AVariation%2520%2528PWRCTV%2529.%2520It%2520employs%2520the%2520gradient%2520maps%2520of%2520PAN%2520images%2520to%2520automatically%250Aassign%2520different%2520weights%2520of%2520TV%2520regularization%2520for%2520each%2520pixel%252C%2520resulting%2520in%250Alarger%2520weights%2520for%2520smooth%2520areas%2520and%2520smaller%2520weights%2520for%2520edges.%2520This%250Aregularization%2520forms%2520the%2520basis%2520of%2520a%2520pan-denoising%2520model%252C%2520which%2520is%2520solved%2520using%250Athe%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers.%2520Extensive%2520experiments%2520on%250Asynthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520PWRCTV%2520outperforms%2520several%250Astate-of-the-art%2520methods%2520in%2520terms%2520of%2520metrics%2520and%2520visual%2520quality.%2520Furthermore%252C%250Aan%2520HSI%2520classification%2520experiment%2520confirms%2520that%2520PWRCTV%252C%2520as%2520a%2520preprocessing%250Amethod%252C%2520can%2520enhance%2520the%2520performance%2520of%2520downstream%2520classification%2520tasks.%2520The%250Acode%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/shuangxu96/PWRCTV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pan-denoising%3A%20Guided%20Hyperspectral%20Image%20Denoising%20via%20Weighted%0A%20%20Represent%20Coefficient%20Total%20Variation&entry.906535625=Shuang%20Xu%20and%20Qiao%20Ke%20and%20Jiangjun%20Peng%20and%20Xiangyong%20Cao%20and%20Zixiang%20Zhao&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20paradigm%20for%20hyperspectral%20image%20%28HSI%29%0Adenoising%2C%20which%20is%20termed%20%5Ctextit%7Bpan-denoising%7D.%20In%20a%20given%20scene%2C%0Apanchromatic%20%28PAN%29%20images%20capture%20similar%20structures%20and%20textures%20to%20HSIs%20but%0Awith%20less%20noise.%20This%20enables%20the%20utilization%20of%20PAN%20images%20to%20guide%20the%20HSI%0Adenoising%20process.%20Consequently%2C%20pan-denoising%2C%20which%20incorporates%20an%0Aadditional%20prior%2C%20has%20the%20potential%20to%20uncover%20underlying%20structures%20and%0Adetails%20beyond%20the%20internal%20information%20modeling%20of%20traditional%20HSI%20denoising%0Amethods.%20However%2C%20the%20proper%20modeling%20of%20this%20additional%20prior%20poses%20a%0Asignificant%20challenge.%20To%20alleviate%20this%20issue%2C%20the%20paper%20proposes%20a%20novel%0Aregularization%20term%2C%20Panchromatic%20Weighted%20Representation%20Coefficient%20Total%0AVariation%20%28PWRCTV%29.%20It%20employs%20the%20gradient%20maps%20of%20PAN%20images%20to%20automatically%0Aassign%20different%20weights%20of%20TV%20regularization%20for%20each%20pixel%2C%20resulting%20in%0Alarger%20weights%20for%20smooth%20areas%20and%20smaller%20weights%20for%20edges.%20This%0Aregularization%20forms%20the%20basis%20of%20a%20pan-denoising%20model%2C%20which%20is%20solved%20using%0Athe%20Alternating%20Direction%20Method%20of%20Multipliers.%20Extensive%20experiments%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20that%20PWRCTV%20outperforms%20several%0Astate-of-the-art%20methods%20in%20terms%20of%20metrics%20and%20visual%20quality.%20Furthermore%2C%0Aan%20HSI%20classification%20experiment%20confirms%20that%20PWRCTV%2C%20as%20a%20preprocessing%0Amethod%2C%20can%20enhance%20the%20performance%20of%20downstream%20classification%20tasks.%20The%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/shuangxu96/PWRCTV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06064v1&entry.124074799=Read"},
{"title": "Multi-Object Hallucination in Vision-Language Models", "author": "Xuweiyi Chen and Ziqiao Ma and Xuejun Zhang and Sihan Xu and Shengyi Qian and Jianing Yang and David F. Fouhey and Joyce Chai", "abstract": "  Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1) LVLMs suffer more hallucinations when focusing\non multiple objects compared to a single object. (2) The tested object class\ndistribution affects hallucination behaviors, indicating that LVLMs may follow\nshortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced\nby data-specific factors, salience and frequency, and model intrinsic\nbehaviors. We hope to enable LVLMs to recognize and reason about multiple\nobjects that often occur in realistic visual scenes, provide insights, and\nquantify our progress towards mitigating the issues.\n", "link": "http://arxiv.org/abs/2407.06192v1", "date": "2024-07-08", "relevancy": 2.0907, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.525}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Object%20Hallucination%20in%20Vision-Language%20Models&body=Title%3A%20Multi-Object%20Hallucination%20in%20Vision-Language%20Models%0AAuthor%3A%20Xuweiyi%20Chen%20and%20Ziqiao%20Ma%20and%20Xuejun%20Zhang%20and%20Sihan%20Xu%20and%20Shengyi%20Qian%20and%20Jianing%20Yang%20and%20David%20F.%20Fouhey%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Large%20vision%20language%20models%20%28LVLMs%29%20often%20suffer%20from%20object%20hallucination%2C%0Aproducing%20objects%20not%20present%20in%20the%20given%20images.%20While%20current%20benchmarks%20for%0Aobject%20hallucination%20primarily%20concentrate%20on%20the%20presence%20of%20a%20single%20object%0Aclass%20rather%20than%20individual%20entities%2C%20this%20work%20systematically%20investigates%0Amulti-object%20hallucination%2C%20examining%20how%20models%20misperceive%20%28e.g.%2C%20invent%0Anonexistent%20objects%20or%20become%20distracted%29%20when%20tasked%20with%20focusing%20on%20multiple%0Aobjects%20simultaneously.%20We%20introduce%20Recognition-based%20Object%20Probing%0AEvaluation%20%28ROPE%29%2C%20an%20automated%20evaluation%20protocol%20that%20considers%20the%0Adistribution%20of%20object%20classes%20within%20a%20single%20image%20during%20testing%20and%20uses%0Avisual%20referring%20prompts%20to%20eliminate%20ambiguity.%20With%20comprehensive%20empirical%0Astudies%20and%20analysis%20of%20potential%20factors%20leading%20to%20multi-object%0Ahallucination%2C%20we%20found%20that%20%281%29%20LVLMs%20suffer%20more%20hallucinations%20when%20focusing%0Aon%20multiple%20objects%20compared%20to%20a%20single%20object.%20%282%29%20The%20tested%20object%20class%0Adistribution%20affects%20hallucination%20behaviors%2C%20indicating%20that%20LVLMs%20may%20follow%0Ashortcuts%20and%20spurious%20correlations.%283%29%20Hallucinatory%20behaviors%20are%20influenced%0Aby%20data-specific%20factors%2C%20salience%20and%20frequency%2C%20and%20model%20intrinsic%0Abehaviors.%20We%20hope%20to%20enable%20LVLMs%20to%20recognize%20and%20reason%20about%20multiple%0Aobjects%20that%20often%20occur%20in%20realistic%20visual%20scenes%2C%20provide%20insights%2C%20and%0Aquantify%20our%20progress%20towards%20mitigating%20the%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Object%2520Hallucination%2520in%2520Vision-Language%2520Models%26entry.906535625%3DXuweiyi%2520Chen%2520and%2520Ziqiao%2520Ma%2520and%2520Xuejun%2520Zhang%2520and%2520Sihan%2520Xu%2520and%2520Shengyi%2520Qian%2520and%2520Jianing%2520Yang%2520and%2520David%2520F.%2520Fouhey%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520Large%2520vision%2520language%2520models%2520%2528LVLMs%2529%2520often%2520suffer%2520from%2520object%2520hallucination%252C%250Aproducing%2520objects%2520not%2520present%2520in%2520the%2520given%2520images.%2520While%2520current%2520benchmarks%2520for%250Aobject%2520hallucination%2520primarily%2520concentrate%2520on%2520the%2520presence%2520of%2520a%2520single%2520object%250Aclass%2520rather%2520than%2520individual%2520entities%252C%2520this%2520work%2520systematically%2520investigates%250Amulti-object%2520hallucination%252C%2520examining%2520how%2520models%2520misperceive%2520%2528e.g.%252C%2520invent%250Anonexistent%2520objects%2520or%2520become%2520distracted%2529%2520when%2520tasked%2520with%2520focusing%2520on%2520multiple%250Aobjects%2520simultaneously.%2520We%2520introduce%2520Recognition-based%2520Object%2520Probing%250AEvaluation%2520%2528ROPE%2529%252C%2520an%2520automated%2520evaluation%2520protocol%2520that%2520considers%2520the%250Adistribution%2520of%2520object%2520classes%2520within%2520a%2520single%2520image%2520during%2520testing%2520and%2520uses%250Avisual%2520referring%2520prompts%2520to%2520eliminate%2520ambiguity.%2520With%2520comprehensive%2520empirical%250Astudies%2520and%2520analysis%2520of%2520potential%2520factors%2520leading%2520to%2520multi-object%250Ahallucination%252C%2520we%2520found%2520that%2520%25281%2529%2520LVLMs%2520suffer%2520more%2520hallucinations%2520when%2520focusing%250Aon%2520multiple%2520objects%2520compared%2520to%2520a%2520single%2520object.%2520%25282%2529%2520The%2520tested%2520object%2520class%250Adistribution%2520affects%2520hallucination%2520behaviors%252C%2520indicating%2520that%2520LVLMs%2520may%2520follow%250Ashortcuts%2520and%2520spurious%2520correlations.%25283%2529%2520Hallucinatory%2520behaviors%2520are%2520influenced%250Aby%2520data-specific%2520factors%252C%2520salience%2520and%2520frequency%252C%2520and%2520model%2520intrinsic%250Abehaviors.%2520We%2520hope%2520to%2520enable%2520LVLMs%2520to%2520recognize%2520and%2520reason%2520about%2520multiple%250Aobjects%2520that%2520often%2520occur%2520in%2520realistic%2520visual%2520scenes%252C%2520provide%2520insights%252C%2520and%250Aquantify%2520our%2520progress%2520towards%2520mitigating%2520the%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Object%20Hallucination%20in%20Vision-Language%20Models&entry.906535625=Xuweiyi%20Chen%20and%20Ziqiao%20Ma%20and%20Xuejun%20Zhang%20and%20Sihan%20Xu%20and%20Shengyi%20Qian%20and%20Jianing%20Yang%20and%20David%20F.%20Fouhey%20and%20Joyce%20Chai&entry.1292438233=%20%20Large%20vision%20language%20models%20%28LVLMs%29%20often%20suffer%20from%20object%20hallucination%2C%0Aproducing%20objects%20not%20present%20in%20the%20given%20images.%20While%20current%20benchmarks%20for%0Aobject%20hallucination%20primarily%20concentrate%20on%20the%20presence%20of%20a%20single%20object%0Aclass%20rather%20than%20individual%20entities%2C%20this%20work%20systematically%20investigates%0Amulti-object%20hallucination%2C%20examining%20how%20models%20misperceive%20%28e.g.%2C%20invent%0Anonexistent%20objects%20or%20become%20distracted%29%20when%20tasked%20with%20focusing%20on%20multiple%0Aobjects%20simultaneously.%20We%20introduce%20Recognition-based%20Object%20Probing%0AEvaluation%20%28ROPE%29%2C%20an%20automated%20evaluation%20protocol%20that%20considers%20the%0Adistribution%20of%20object%20classes%20within%20a%20single%20image%20during%20testing%20and%20uses%0Avisual%20referring%20prompts%20to%20eliminate%20ambiguity.%20With%20comprehensive%20empirical%0Astudies%20and%20analysis%20of%20potential%20factors%20leading%20to%20multi-object%0Ahallucination%2C%20we%20found%20that%20%281%29%20LVLMs%20suffer%20more%20hallucinations%20when%20focusing%0Aon%20multiple%20objects%20compared%20to%20a%20single%20object.%20%282%29%20The%20tested%20object%20class%0Adistribution%20affects%20hallucination%20behaviors%2C%20indicating%20that%20LVLMs%20may%20follow%0Ashortcuts%20and%20spurious%20correlations.%283%29%20Hallucinatory%20behaviors%20are%20influenced%0Aby%20data-specific%20factors%2C%20salience%20and%20frequency%2C%20and%20model%20intrinsic%0Abehaviors.%20We%20hope%20to%20enable%20LVLMs%20to%20recognize%20and%20reason%20about%20multiple%0Aobjects%20that%20often%20occur%20in%20realistic%20visual%20scenes%2C%20provide%20insights%2C%20and%0Aquantify%20our%20progress%20towards%20mitigating%20the%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06192v1&entry.124074799=Read"},
{"title": "Surprising gender biases in GPT", "author": "Raluca Alexandra Fulgu and Valerio Capraro", "abstract": "  We present seven experiments exploring gender biases in GPT. Initially, GPT\nwas asked to generate demographics of a potential writer of twenty phrases\ncontaining feminine stereotypes and twenty with masculine stereotypes. Results\nshow a strong asymmetry, with stereotypically masculine sentences attributed to\na female more often than vice versa. For example, the sentence \"I love playing\nfotbal! Im practicing with my cosin Michael\" was constantly assigned by ChatGPT\nto a female writer. This phenomenon likely reflects that while initiatives to\nintegrate women in traditionally masculine roles have gained momentum, the\nreverse movement remains relatively underdeveloped. Subsequent experiments\ninvestigate the same issue in high-stakes moral dilemmas. GPT-4 finds it more\nappropriate to abuse a man to prevent a nuclear apocalypse than to abuse a\nwoman. This bias extends to other forms of violence central to the gender\nparity debate (abuse), but not to those less central (torture). Moreover, this\nbias increases in cases of mixed-sex violence for the greater good: GPT-4\nagrees with a woman using violence against a man to prevent a nuclear\napocalypse but disagrees with a man using violence against a woman for the same\npurpose. Finally, these biases are implicit, as they do not emerge when GPT-4\nis directly asked to rank moral violations. These results highlight the\nnecessity of carefully managing inclusivity efforts to prevent unintended\ndiscrimination.\n", "link": "http://arxiv.org/abs/2407.06003v1", "date": "2024-07-08", "relevancy": 2.0892, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4577}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4093}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surprising%20gender%20biases%20in%20GPT&body=Title%3A%20Surprising%20gender%20biases%20in%20GPT%0AAuthor%3A%20Raluca%20Alexandra%20Fulgu%20and%20Valerio%20Capraro%0AAbstract%3A%20%20%20We%20present%20seven%20experiments%20exploring%20gender%20biases%20in%20GPT.%20Initially%2C%20GPT%0Awas%20asked%20to%20generate%20demographics%20of%20a%20potential%20writer%20of%20twenty%20phrases%0Acontaining%20feminine%20stereotypes%20and%20twenty%20with%20masculine%20stereotypes.%20Results%0Ashow%20a%20strong%20asymmetry%2C%20with%20stereotypically%20masculine%20sentences%20attributed%20to%0Aa%20female%20more%20often%20than%20vice%20versa.%20For%20example%2C%20the%20sentence%20%22I%20love%20playing%0Afotbal%21%20Im%20practicing%20with%20my%20cosin%20Michael%22%20was%20constantly%20assigned%20by%20ChatGPT%0Ato%20a%20female%20writer.%20This%20phenomenon%20likely%20reflects%20that%20while%20initiatives%20to%0Aintegrate%20women%20in%20traditionally%20masculine%20roles%20have%20gained%20momentum%2C%20the%0Areverse%20movement%20remains%20relatively%20underdeveloped.%20Subsequent%20experiments%0Ainvestigate%20the%20same%20issue%20in%20high-stakes%20moral%20dilemmas.%20GPT-4%20finds%20it%20more%0Aappropriate%20to%20abuse%20a%20man%20to%20prevent%20a%20nuclear%20apocalypse%20than%20to%20abuse%20a%0Awoman.%20This%20bias%20extends%20to%20other%20forms%20of%20violence%20central%20to%20the%20gender%0Aparity%20debate%20%28abuse%29%2C%20but%20not%20to%20those%20less%20central%20%28torture%29.%20Moreover%2C%20this%0Abias%20increases%20in%20cases%20of%20mixed-sex%20violence%20for%20the%20greater%20good%3A%20GPT-4%0Aagrees%20with%20a%20woman%20using%20violence%20against%20a%20man%20to%20prevent%20a%20nuclear%0Aapocalypse%20but%20disagrees%20with%20a%20man%20using%20violence%20against%20a%20woman%20for%20the%20same%0Apurpose.%20Finally%2C%20these%20biases%20are%20implicit%2C%20as%20they%20do%20not%20emerge%20when%20GPT-4%0Ais%20directly%20asked%20to%20rank%20moral%20violations.%20These%20results%20highlight%20the%0Anecessity%20of%20carefully%20managing%20inclusivity%20efforts%20to%20prevent%20unintended%0Adiscrimination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurprising%2520gender%2520biases%2520in%2520GPT%26entry.906535625%3DRaluca%2520Alexandra%2520Fulgu%2520and%2520Valerio%2520Capraro%26entry.1292438233%3D%2520%2520We%2520present%2520seven%2520experiments%2520exploring%2520gender%2520biases%2520in%2520GPT.%2520Initially%252C%2520GPT%250Awas%2520asked%2520to%2520generate%2520demographics%2520of%2520a%2520potential%2520writer%2520of%2520twenty%2520phrases%250Acontaining%2520feminine%2520stereotypes%2520and%2520twenty%2520with%2520masculine%2520stereotypes.%2520Results%250Ashow%2520a%2520strong%2520asymmetry%252C%2520with%2520stereotypically%2520masculine%2520sentences%2520attributed%2520to%250Aa%2520female%2520more%2520often%2520than%2520vice%2520versa.%2520For%2520example%252C%2520the%2520sentence%2520%2522I%2520love%2520playing%250Afotbal%2521%2520Im%2520practicing%2520with%2520my%2520cosin%2520Michael%2522%2520was%2520constantly%2520assigned%2520by%2520ChatGPT%250Ato%2520a%2520female%2520writer.%2520This%2520phenomenon%2520likely%2520reflects%2520that%2520while%2520initiatives%2520to%250Aintegrate%2520women%2520in%2520traditionally%2520masculine%2520roles%2520have%2520gained%2520momentum%252C%2520the%250Areverse%2520movement%2520remains%2520relatively%2520underdeveloped.%2520Subsequent%2520experiments%250Ainvestigate%2520the%2520same%2520issue%2520in%2520high-stakes%2520moral%2520dilemmas.%2520GPT-4%2520finds%2520it%2520more%250Aappropriate%2520to%2520abuse%2520a%2520man%2520to%2520prevent%2520a%2520nuclear%2520apocalypse%2520than%2520to%2520abuse%2520a%250Awoman.%2520This%2520bias%2520extends%2520to%2520other%2520forms%2520of%2520violence%2520central%2520to%2520the%2520gender%250Aparity%2520debate%2520%2528abuse%2529%252C%2520but%2520not%2520to%2520those%2520less%2520central%2520%2528torture%2529.%2520Moreover%252C%2520this%250Abias%2520increases%2520in%2520cases%2520of%2520mixed-sex%2520violence%2520for%2520the%2520greater%2520good%253A%2520GPT-4%250Aagrees%2520with%2520a%2520woman%2520using%2520violence%2520against%2520a%2520man%2520to%2520prevent%2520a%2520nuclear%250Aapocalypse%2520but%2520disagrees%2520with%2520a%2520man%2520using%2520violence%2520against%2520a%2520woman%2520for%2520the%2520same%250Apurpose.%2520Finally%252C%2520these%2520biases%2520are%2520implicit%252C%2520as%2520they%2520do%2520not%2520emerge%2520when%2520GPT-4%250Ais%2520directly%2520asked%2520to%2520rank%2520moral%2520violations.%2520These%2520results%2520highlight%2520the%250Anecessity%2520of%2520carefully%2520managing%2520inclusivity%2520efforts%2520to%2520prevent%2520unintended%250Adiscrimination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surprising%20gender%20biases%20in%20GPT&entry.906535625=Raluca%20Alexandra%20Fulgu%20and%20Valerio%20Capraro&entry.1292438233=%20%20We%20present%20seven%20experiments%20exploring%20gender%20biases%20in%20GPT.%20Initially%2C%20GPT%0Awas%20asked%20to%20generate%20demographics%20of%20a%20potential%20writer%20of%20twenty%20phrases%0Acontaining%20feminine%20stereotypes%20and%20twenty%20with%20masculine%20stereotypes.%20Results%0Ashow%20a%20strong%20asymmetry%2C%20with%20stereotypically%20masculine%20sentences%20attributed%20to%0Aa%20female%20more%20often%20than%20vice%20versa.%20For%20example%2C%20the%20sentence%20%22I%20love%20playing%0Afotbal%21%20Im%20practicing%20with%20my%20cosin%20Michael%22%20was%20constantly%20assigned%20by%20ChatGPT%0Ato%20a%20female%20writer.%20This%20phenomenon%20likely%20reflects%20that%20while%20initiatives%20to%0Aintegrate%20women%20in%20traditionally%20masculine%20roles%20have%20gained%20momentum%2C%20the%0Areverse%20movement%20remains%20relatively%20underdeveloped.%20Subsequent%20experiments%0Ainvestigate%20the%20same%20issue%20in%20high-stakes%20moral%20dilemmas.%20GPT-4%20finds%20it%20more%0Aappropriate%20to%20abuse%20a%20man%20to%20prevent%20a%20nuclear%20apocalypse%20than%20to%20abuse%20a%0Awoman.%20This%20bias%20extends%20to%20other%20forms%20of%20violence%20central%20to%20the%20gender%0Aparity%20debate%20%28abuse%29%2C%20but%20not%20to%20those%20less%20central%20%28torture%29.%20Moreover%2C%20this%0Abias%20increases%20in%20cases%20of%20mixed-sex%20violence%20for%20the%20greater%20good%3A%20GPT-4%0Aagrees%20with%20a%20woman%20using%20violence%20against%20a%20man%20to%20prevent%20a%20nuclear%0Aapocalypse%20but%20disagrees%20with%20a%20man%20using%20violence%20against%20a%20woman%20for%20the%20same%0Apurpose.%20Finally%2C%20these%20biases%20are%20implicit%2C%20as%20they%20do%20not%20emerge%20when%20GPT-4%0Ais%20directly%20asked%20to%20rank%20moral%20violations.%20These%20results%20highlight%20the%0Anecessity%20of%20carefully%20managing%20inclusivity%20efforts%20to%20prevent%20unintended%0Adiscrimination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06003v1&entry.124074799=Read"},
{"title": "Vision-Language Models under Cultural and Inclusive Considerations", "author": "Antonia Karamolegkou and Phillip Rust and Yong Cao and Ruixiang Cui and Anders S\u00f8gaard and Daniel Hershcovich", "abstract": "  Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available.\n", "link": "http://arxiv.org/abs/2407.06177v1", "date": "2024-07-08", "relevancy": 2.0866, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5571}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5174}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Models%20under%20Cultural%20and%20Inclusive%20Considerations&body=Title%3A%20Vision-Language%20Models%20under%20Cultural%20and%20Inclusive%20Considerations%0AAuthor%3A%20Antonia%20Karamolegkou%20and%20Phillip%20Rust%20and%20Yong%20Cao%20and%20Ruixiang%20Cui%20and%20Anders%20S%C3%B8gaard%20and%20Daniel%20Hershcovich%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28VLMs%29%20can%20assist%20visually%20impaired%20people%20by%0Adescribing%20images%20from%20their%20daily%20lives.%20Current%20evaluation%20datasets%20may%20not%0Areflect%20diverse%20cultural%20user%20backgrounds%20or%20the%20situational%20context%20of%20this%0Ause%20case.%20To%20address%20this%20problem%2C%20we%20create%20a%20survey%20to%20determine%20caption%0Apreferences%20and%20propose%20a%20culture-centric%20evaluation%20benchmark%20by%20filtering%0AVizWiz%2C%20an%20existing%20dataset%20with%20images%20taken%20by%20people%20who%20are%20blind.%20We%20then%0Aevaluate%20several%20VLMs%2C%20investigating%20their%20reliability%20as%20visual%20assistants%20in%0Aa%20culturally%20diverse%20setting.%20While%20our%20results%20for%20state-of-the-art%20models%20are%0Apromising%2C%20we%20identify%20challenges%20such%20as%20hallucination%20and%20misalignment%20of%0Aautomatic%20evaluation%20metrics%20with%20human%20judgment.%20We%20make%20our%20survey%2C%20data%2C%0Acode%2C%20and%20model%20outputs%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Models%2520under%2520Cultural%2520and%2520Inclusive%2520Considerations%26entry.906535625%3DAntonia%2520Karamolegkou%2520and%2520Phillip%2520Rust%2520and%2520Yong%2520Cao%2520and%2520Ruixiang%2520Cui%2520and%2520Anders%2520S%25C3%25B8gaard%2520and%2520Daniel%2520Hershcovich%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528VLMs%2529%2520can%2520assist%2520visually%2520impaired%2520people%2520by%250Adescribing%2520images%2520from%2520their%2520daily%2520lives.%2520Current%2520evaluation%2520datasets%2520may%2520not%250Areflect%2520diverse%2520cultural%2520user%2520backgrounds%2520or%2520the%2520situational%2520context%2520of%2520this%250Ause%2520case.%2520To%2520address%2520this%2520problem%252C%2520we%2520create%2520a%2520survey%2520to%2520determine%2520caption%250Apreferences%2520and%2520propose%2520a%2520culture-centric%2520evaluation%2520benchmark%2520by%2520filtering%250AVizWiz%252C%2520an%2520existing%2520dataset%2520with%2520images%2520taken%2520by%2520people%2520who%2520are%2520blind.%2520We%2520then%250Aevaluate%2520several%2520VLMs%252C%2520investigating%2520their%2520reliability%2520as%2520visual%2520assistants%2520in%250Aa%2520culturally%2520diverse%2520setting.%2520While%2520our%2520results%2520for%2520state-of-the-art%2520models%2520are%250Apromising%252C%2520we%2520identify%2520challenges%2520such%2520as%2520hallucination%2520and%2520misalignment%2520of%250Aautomatic%2520evaluation%2520metrics%2520with%2520human%2520judgment.%2520We%2520make%2520our%2520survey%252C%2520data%252C%250Acode%252C%2520and%2520model%2520outputs%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Models%20under%20Cultural%20and%20Inclusive%20Considerations&entry.906535625=Antonia%20Karamolegkou%20and%20Phillip%20Rust%20and%20Yong%20Cao%20and%20Ruixiang%20Cui%20and%20Anders%20S%C3%B8gaard%20and%20Daniel%20Hershcovich&entry.1292438233=%20%20Large%20vision-language%20models%20%28VLMs%29%20can%20assist%20visually%20impaired%20people%20by%0Adescribing%20images%20from%20their%20daily%20lives.%20Current%20evaluation%20datasets%20may%20not%0Areflect%20diverse%20cultural%20user%20backgrounds%20or%20the%20situational%20context%20of%20this%0Ause%20case.%20To%20address%20this%20problem%2C%20we%20create%20a%20survey%20to%20determine%20caption%0Apreferences%20and%20propose%20a%20culture-centric%20evaluation%20benchmark%20by%20filtering%0AVizWiz%2C%20an%20existing%20dataset%20with%20images%20taken%20by%20people%20who%20are%20blind.%20We%20then%0Aevaluate%20several%20VLMs%2C%20investigating%20their%20reliability%20as%20visual%20assistants%20in%0Aa%20culturally%20diverse%20setting.%20While%20our%20results%20for%20state-of-the-art%20models%20are%0Apromising%2C%20we%20identify%20challenges%20such%20as%20hallucination%20and%20misalignment%20of%0Aautomatic%20evaluation%20metrics%20with%20human%20judgment.%20We%20make%20our%20survey%2C%20data%2C%0Acode%2C%20and%20model%20outputs%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06177v1&entry.124074799=Read"},
{"title": "CAM-Based Methods Can See through Walls", "author": "Magamed Taimeskhanov and Ronan Sicre and Damien Garreau", "abstract": "  CAM-based methods are widely-used post-hoc interpretability method that\nproduce a saliency map to explain the decision of an image classification\nmodel. The saliency map highlights the important areas of the image relevant to\nthe prediction. In this paper, we show that most of these methods can\nincorrectly attribute an important score to parts of the image that the model\ncannot see. We show that this phenomenon occurs both theoretically and\nexperimentally. On the theory side, we analyze the behavior of GradCAM on a\nsimple masked CNN model at initialization. Experimentally, we train a VGG-like\nmodel constrained to not use the lower part of the image and nevertheless\nobserve positive scores in the unseen part of the image. This behavior is\nevaluated quantitatively on two new datasets. We believe that this is\nproblematic, potentially leading to mis-interpretation of the model's behavior.\n", "link": "http://arxiv.org/abs/2404.01964v2", "date": "2024-07-08", "relevancy": 2.0818, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAM-Based%20Methods%20Can%20See%20through%20Walls&body=Title%3A%20CAM-Based%20Methods%20Can%20See%20through%20Walls%0AAuthor%3A%20Magamed%20Taimeskhanov%20and%20Ronan%20Sicre%20and%20Damien%20Garreau%0AAbstract%3A%20%20%20CAM-based%20methods%20are%20widely-used%20post-hoc%20interpretability%20method%20that%0Aproduce%20a%20saliency%20map%20to%20explain%20the%20decision%20of%20an%20image%20classification%0Amodel.%20The%20saliency%20map%20highlights%20the%20important%20areas%20of%20the%20image%20relevant%20to%0Athe%20prediction.%20In%20this%20paper%2C%20we%20show%20that%20most%20of%20these%20methods%20can%0Aincorrectly%20attribute%20an%20important%20score%20to%20parts%20of%20the%20image%20that%20the%20model%0Acannot%20see.%20We%20show%20that%20this%20phenomenon%20occurs%20both%20theoretically%20and%0Aexperimentally.%20On%20the%20theory%20side%2C%20we%20analyze%20the%20behavior%20of%20GradCAM%20on%20a%0Asimple%20masked%20CNN%20model%20at%20initialization.%20Experimentally%2C%20we%20train%20a%20VGG-like%0Amodel%20constrained%20to%20not%20use%20the%20lower%20part%20of%20the%20image%20and%20nevertheless%0Aobserve%20positive%20scores%20in%20the%20unseen%20part%20of%20the%20image.%20This%20behavior%20is%0Aevaluated%20quantitatively%20on%20two%20new%20datasets.%20We%20believe%20that%20this%20is%0Aproblematic%2C%20potentially%20leading%20to%20mis-interpretation%20of%20the%20model%27s%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01964v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAM-Based%2520Methods%2520Can%2520See%2520through%2520Walls%26entry.906535625%3DMagamed%2520Taimeskhanov%2520and%2520Ronan%2520Sicre%2520and%2520Damien%2520Garreau%26entry.1292438233%3D%2520%2520CAM-based%2520methods%2520are%2520widely-used%2520post-hoc%2520interpretability%2520method%2520that%250Aproduce%2520a%2520saliency%2520map%2520to%2520explain%2520the%2520decision%2520of%2520an%2520image%2520classification%250Amodel.%2520The%2520saliency%2520map%2520highlights%2520the%2520important%2520areas%2520of%2520the%2520image%2520relevant%2520to%250Athe%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520most%2520of%2520these%2520methods%2520can%250Aincorrectly%2520attribute%2520an%2520important%2520score%2520to%2520parts%2520of%2520the%2520image%2520that%2520the%2520model%250Acannot%2520see.%2520We%2520show%2520that%2520this%2520phenomenon%2520occurs%2520both%2520theoretically%2520and%250Aexperimentally.%2520On%2520the%2520theory%2520side%252C%2520we%2520analyze%2520the%2520behavior%2520of%2520GradCAM%2520on%2520a%250Asimple%2520masked%2520CNN%2520model%2520at%2520initialization.%2520Experimentally%252C%2520we%2520train%2520a%2520VGG-like%250Amodel%2520constrained%2520to%2520not%2520use%2520the%2520lower%2520part%2520of%2520the%2520image%2520and%2520nevertheless%250Aobserve%2520positive%2520scores%2520in%2520the%2520unseen%2520part%2520of%2520the%2520image.%2520This%2520behavior%2520is%250Aevaluated%2520quantitatively%2520on%2520two%2520new%2520datasets.%2520We%2520believe%2520that%2520this%2520is%250Aproblematic%252C%2520potentially%2520leading%2520to%2520mis-interpretation%2520of%2520the%2520model%2527s%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01964v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAM-Based%20Methods%20Can%20See%20through%20Walls&entry.906535625=Magamed%20Taimeskhanov%20and%20Ronan%20Sicre%20and%20Damien%20Garreau&entry.1292438233=%20%20CAM-based%20methods%20are%20widely-used%20post-hoc%20interpretability%20method%20that%0Aproduce%20a%20saliency%20map%20to%20explain%20the%20decision%20of%20an%20image%20classification%0Amodel.%20The%20saliency%20map%20highlights%20the%20important%20areas%20of%20the%20image%20relevant%20to%0Athe%20prediction.%20In%20this%20paper%2C%20we%20show%20that%20most%20of%20these%20methods%20can%0Aincorrectly%20attribute%20an%20important%20score%20to%20parts%20of%20the%20image%20that%20the%20model%0Acannot%20see.%20We%20show%20that%20this%20phenomenon%20occurs%20both%20theoretically%20and%0Aexperimentally.%20On%20the%20theory%20side%2C%20we%20analyze%20the%20behavior%20of%20GradCAM%20on%20a%0Asimple%20masked%20CNN%20model%20at%20initialization.%20Experimentally%2C%20we%20train%20a%20VGG-like%0Amodel%20constrained%20to%20not%20use%20the%20lower%20part%20of%20the%20image%20and%20nevertheless%0Aobserve%20positive%20scores%20in%20the%20unseen%20part%20of%20the%20image.%20This%20behavior%20is%0Aevaluated%20quantitatively%20on%20two%20new%20datasets.%20We%20believe%20that%20this%20is%0Aproblematic%2C%20potentially%20leading%20to%20mis-interpretation%20of%20the%20model%27s%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01964v2&entry.124074799=Read"},
{"title": "An efficient Deep Spatio-Temporal Context Aware decision Network\n  (DST-CAN) for Predictive Manoeuvre Planning", "author": "Jayabrata Chowdhury and Suresh Sundaram and Nishanth Rao and Narasimhan Sundararajan", "abstract": "  To ensure the safety and efficiency of its maneuvers, an Autonomous Vehicle\n(AV) should anticipate the future intentions of surrounding vehicles using its\nsensor information. If an AV can predict its surrounding vehicles' future\ntrajectories, it can make safe and efficient manoeuvre decisions. In this\npaper, we present such a Deep Spatio-Temporal Context-Aware decision Network\n(DST-CAN) model for predictive manoeuvre planning of AVs. A memory neuron\nnetwork is used to predict future trajectories of its surrounding vehicles. The\ndriving environment's spatio-temporal information (past, present, and predicted\nfuture trajectories) are embedded into a context-aware grid. The proposed\nDST-CAN model employs these context-aware grids as inputs to a convolutional\nneural network to understand the spatial relationships between the vehicles and\ndetermine a safe and efficient manoeuvre decision. The DST-CAN model also uses\ninformation of human driving behavior on a highway. Performance evaluation of\nDST-CAN has been carried out using two publicly available NGSIM US-101 and I-80\ndatasets. Also, rule-based ground truth decisions have been compared with those\ngenerated by DST-CAN. The results clearly show that DST-CAN can make much\nbetter decisions with 3-sec of predicted trajectories of neighboring vehicles\ncompared to currently existing methods that do not use this prediction.\n", "link": "http://arxiv.org/abs/2205.10092v2", "date": "2024-07-08", "relevancy": 2.0731, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5313}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5247}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20efficient%20Deep%20Spatio-Temporal%20Context%20Aware%20decision%20Network%0A%20%20%28DST-CAN%29%20for%20Predictive%20Manoeuvre%20Planning&body=Title%3A%20An%20efficient%20Deep%20Spatio-Temporal%20Context%20Aware%20decision%20Network%0A%20%20%28DST-CAN%29%20for%20Predictive%20Manoeuvre%20Planning%0AAuthor%3A%20Jayabrata%20Chowdhury%20and%20Suresh%20Sundaram%20and%20Nishanth%20Rao%20and%20Narasimhan%20Sundararajan%0AAbstract%3A%20%20%20To%20ensure%20the%20safety%20and%20efficiency%20of%20its%20maneuvers%2C%20an%20Autonomous%20Vehicle%0A%28AV%29%20should%20anticipate%20the%20future%20intentions%20of%20surrounding%20vehicles%20using%20its%0Asensor%20information.%20If%20an%20AV%20can%20predict%20its%20surrounding%20vehicles%27%20future%0Atrajectories%2C%20it%20can%20make%20safe%20and%20efficient%20manoeuvre%20decisions.%20In%20this%0Apaper%2C%20we%20present%20such%20a%20Deep%20Spatio-Temporal%20Context-Aware%20decision%20Network%0A%28DST-CAN%29%20model%20for%20predictive%20manoeuvre%20planning%20of%20AVs.%20A%20memory%20neuron%0Anetwork%20is%20used%20to%20predict%20future%20trajectories%20of%20its%20surrounding%20vehicles.%20The%0Adriving%20environment%27s%20spatio-temporal%20information%20%28past%2C%20present%2C%20and%20predicted%0Afuture%20trajectories%29%20are%20embedded%20into%20a%20context-aware%20grid.%20The%20proposed%0ADST-CAN%20model%20employs%20these%20context-aware%20grids%20as%20inputs%20to%20a%20convolutional%0Aneural%20network%20to%20understand%20the%20spatial%20relationships%20between%20the%20vehicles%20and%0Adetermine%20a%20safe%20and%20efficient%20manoeuvre%20decision.%20The%20DST-CAN%20model%20also%20uses%0Ainformation%20of%20human%20driving%20behavior%20on%20a%20highway.%20Performance%20evaluation%20of%0ADST-CAN%20has%20been%20carried%20out%20using%20two%20publicly%20available%20NGSIM%20US-101%20and%20I-80%0Adatasets.%20Also%2C%20rule-based%20ground%20truth%20decisions%20have%20been%20compared%20with%20those%0Agenerated%20by%20DST-CAN.%20The%20results%20clearly%20show%20that%20DST-CAN%20can%20make%20much%0Abetter%20decisions%20with%203-sec%20of%20predicted%20trajectories%20of%20neighboring%20vehicles%0Acompared%20to%20currently%20existing%20methods%20that%20do%20not%20use%20this%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.10092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520efficient%2520Deep%2520Spatio-Temporal%2520Context%2520Aware%2520decision%2520Network%250A%2520%2520%2528DST-CAN%2529%2520for%2520Predictive%2520Manoeuvre%2520Planning%26entry.906535625%3DJayabrata%2520Chowdhury%2520and%2520Suresh%2520Sundaram%2520and%2520Nishanth%2520Rao%2520and%2520Narasimhan%2520Sundararajan%26entry.1292438233%3D%2520%2520To%2520ensure%2520the%2520safety%2520and%2520efficiency%2520of%2520its%2520maneuvers%252C%2520an%2520Autonomous%2520Vehicle%250A%2528AV%2529%2520should%2520anticipate%2520the%2520future%2520intentions%2520of%2520surrounding%2520vehicles%2520using%2520its%250Asensor%2520information.%2520If%2520an%2520AV%2520can%2520predict%2520its%2520surrounding%2520vehicles%2527%2520future%250Atrajectories%252C%2520it%2520can%2520make%2520safe%2520and%2520efficient%2520manoeuvre%2520decisions.%2520In%2520this%250Apaper%252C%2520we%2520present%2520such%2520a%2520Deep%2520Spatio-Temporal%2520Context-Aware%2520decision%2520Network%250A%2528DST-CAN%2529%2520model%2520for%2520predictive%2520manoeuvre%2520planning%2520of%2520AVs.%2520A%2520memory%2520neuron%250Anetwork%2520is%2520used%2520to%2520predict%2520future%2520trajectories%2520of%2520its%2520surrounding%2520vehicles.%2520The%250Adriving%2520environment%2527s%2520spatio-temporal%2520information%2520%2528past%252C%2520present%252C%2520and%2520predicted%250Afuture%2520trajectories%2529%2520are%2520embedded%2520into%2520a%2520context-aware%2520grid.%2520The%2520proposed%250ADST-CAN%2520model%2520employs%2520these%2520context-aware%2520grids%2520as%2520inputs%2520to%2520a%2520convolutional%250Aneural%2520network%2520to%2520understand%2520the%2520spatial%2520relationships%2520between%2520the%2520vehicles%2520and%250Adetermine%2520a%2520safe%2520and%2520efficient%2520manoeuvre%2520decision.%2520The%2520DST-CAN%2520model%2520also%2520uses%250Ainformation%2520of%2520human%2520driving%2520behavior%2520on%2520a%2520highway.%2520Performance%2520evaluation%2520of%250ADST-CAN%2520has%2520been%2520carried%2520out%2520using%2520two%2520publicly%2520available%2520NGSIM%2520US-101%2520and%2520I-80%250Adatasets.%2520Also%252C%2520rule-based%2520ground%2520truth%2520decisions%2520have%2520been%2520compared%2520with%2520those%250Agenerated%2520by%2520DST-CAN.%2520The%2520results%2520clearly%2520show%2520that%2520DST-CAN%2520can%2520make%2520much%250Abetter%2520decisions%2520with%25203-sec%2520of%2520predicted%2520trajectories%2520of%2520neighboring%2520vehicles%250Acompared%2520to%2520currently%2520existing%2520methods%2520that%2520do%2520not%2520use%2520this%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.10092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20efficient%20Deep%20Spatio-Temporal%20Context%20Aware%20decision%20Network%0A%20%20%28DST-CAN%29%20for%20Predictive%20Manoeuvre%20Planning&entry.906535625=Jayabrata%20Chowdhury%20and%20Suresh%20Sundaram%20and%20Nishanth%20Rao%20and%20Narasimhan%20Sundararajan&entry.1292438233=%20%20To%20ensure%20the%20safety%20and%20efficiency%20of%20its%20maneuvers%2C%20an%20Autonomous%20Vehicle%0A%28AV%29%20should%20anticipate%20the%20future%20intentions%20of%20surrounding%20vehicles%20using%20its%0Asensor%20information.%20If%20an%20AV%20can%20predict%20its%20surrounding%20vehicles%27%20future%0Atrajectories%2C%20it%20can%20make%20safe%20and%20efficient%20manoeuvre%20decisions.%20In%20this%0Apaper%2C%20we%20present%20such%20a%20Deep%20Spatio-Temporal%20Context-Aware%20decision%20Network%0A%28DST-CAN%29%20model%20for%20predictive%20manoeuvre%20planning%20of%20AVs.%20A%20memory%20neuron%0Anetwork%20is%20used%20to%20predict%20future%20trajectories%20of%20its%20surrounding%20vehicles.%20The%0Adriving%20environment%27s%20spatio-temporal%20information%20%28past%2C%20present%2C%20and%20predicted%0Afuture%20trajectories%29%20are%20embedded%20into%20a%20context-aware%20grid.%20The%20proposed%0ADST-CAN%20model%20employs%20these%20context-aware%20grids%20as%20inputs%20to%20a%20convolutional%0Aneural%20network%20to%20understand%20the%20spatial%20relationships%20between%20the%20vehicles%20and%0Adetermine%20a%20safe%20and%20efficient%20manoeuvre%20decision.%20The%20DST-CAN%20model%20also%20uses%0Ainformation%20of%20human%20driving%20behavior%20on%20a%20highway.%20Performance%20evaluation%20of%0ADST-CAN%20has%20been%20carried%20out%20using%20two%20publicly%20available%20NGSIM%20US-101%20and%20I-80%0Adatasets.%20Also%2C%20rule-based%20ground%20truth%20decisions%20have%20been%20compared%20with%20those%0Agenerated%20by%20DST-CAN.%20The%20results%20clearly%20show%20that%20DST-CAN%20can%20make%20much%0Abetter%20decisions%20with%203-sec%20of%20predicted%20trajectories%20of%20neighboring%20vehicles%0Acompared%20to%20currently%20existing%20methods%20that%20do%20not%20use%20this%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.10092v2&entry.124074799=Read"},
{"title": "Deform-Mamba Network for MRI Super-Resolution", "author": "Zexin Ji and Beiji Zou and Xiaoyan Kui and Pierre Vera and Su Ruan", "abstract": "  In this paper, we propose a new architecture, called Deform-Mamba, for MR\nimage super-resolution. Unlike conventional CNN or Transformer-based\nsuper-resolution approaches which encounter challenges related to the local\nrespective field or heavy computational cost, our approach aims to effectively\nexplore the local and global information of images. Specifically, we develop a\nDeform-Mamba encoder which is composed of two branches, modulated deform block\nand vision Mamba block. We also design a multi-view context module in the\nbottleneck layer to explore the multi-view contextual content. Thanks to the\nextracted features of the encoder, which include content-adaptive local and\nefficient global information, the vision Mamba decoder finally generates\nhigh-quality MR images. Moreover, we introduce a contrastive edge loss to\npromote the reconstruction of edge and contrast related content. Quantitative\nand qualitative experimental results indicate that our approach on IXI and\nfastMRI datasets achieves competitive performance.\n", "link": "http://arxiv.org/abs/2407.05969v1", "date": "2024-07-08", "relevancy": 2.0515, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5199}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5154}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deform-Mamba%20Network%20for%20MRI%20Super-Resolution&body=Title%3A%20Deform-Mamba%20Network%20for%20MRI%20Super-Resolution%0AAuthor%3A%20Zexin%20Ji%20and%20Beiji%20Zou%20and%20Xiaoyan%20Kui%20and%20Pierre%20Vera%20and%20Su%20Ruan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20architecture%2C%20called%20Deform-Mamba%2C%20for%20MR%0Aimage%20super-resolution.%20Unlike%20conventional%20CNN%20or%20Transformer-based%0Asuper-resolution%20approaches%20which%20encounter%20challenges%20related%20to%20the%20local%0Arespective%20field%20or%20heavy%20computational%20cost%2C%20our%20approach%20aims%20to%20effectively%0Aexplore%20the%20local%20and%20global%20information%20of%20images.%20Specifically%2C%20we%20develop%20a%0ADeform-Mamba%20encoder%20which%20is%20composed%20of%20two%20branches%2C%20modulated%20deform%20block%0Aand%20vision%20Mamba%20block.%20We%20also%20design%20a%20multi-view%20context%20module%20in%20the%0Abottleneck%20layer%20to%20explore%20the%20multi-view%20contextual%20content.%20Thanks%20to%20the%0Aextracted%20features%20of%20the%20encoder%2C%20which%20include%20content-adaptive%20local%20and%0Aefficient%20global%20information%2C%20the%20vision%20Mamba%20decoder%20finally%20generates%0Ahigh-quality%20MR%20images.%20Moreover%2C%20we%20introduce%20a%20contrastive%20edge%20loss%20to%0Apromote%20the%20reconstruction%20of%20edge%20and%20contrast%20related%20content.%20Quantitative%0Aand%20qualitative%20experimental%20results%20indicate%20that%20our%20approach%20on%20IXI%20and%0AfastMRI%20datasets%20achieves%20competitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeform-Mamba%2520Network%2520for%2520MRI%2520Super-Resolution%26entry.906535625%3DZexin%2520Ji%2520and%2520Beiji%2520Zou%2520and%2520Xiaoyan%2520Kui%2520and%2520Pierre%2520Vera%2520and%2520Su%2520Ruan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520architecture%252C%2520called%2520Deform-Mamba%252C%2520for%2520MR%250Aimage%2520super-resolution.%2520Unlike%2520conventional%2520CNN%2520or%2520Transformer-based%250Asuper-resolution%2520approaches%2520which%2520encounter%2520challenges%2520related%2520to%2520the%2520local%250Arespective%2520field%2520or%2520heavy%2520computational%2520cost%252C%2520our%2520approach%2520aims%2520to%2520effectively%250Aexplore%2520the%2520local%2520and%2520global%2520information%2520of%2520images.%2520Specifically%252C%2520we%2520develop%2520a%250ADeform-Mamba%2520encoder%2520which%2520is%2520composed%2520of%2520two%2520branches%252C%2520modulated%2520deform%2520block%250Aand%2520vision%2520Mamba%2520block.%2520We%2520also%2520design%2520a%2520multi-view%2520context%2520module%2520in%2520the%250Abottleneck%2520layer%2520to%2520explore%2520the%2520multi-view%2520contextual%2520content.%2520Thanks%2520to%2520the%250Aextracted%2520features%2520of%2520the%2520encoder%252C%2520which%2520include%2520content-adaptive%2520local%2520and%250Aefficient%2520global%2520information%252C%2520the%2520vision%2520Mamba%2520decoder%2520finally%2520generates%250Ahigh-quality%2520MR%2520images.%2520Moreover%252C%2520we%2520introduce%2520a%2520contrastive%2520edge%2520loss%2520to%250Apromote%2520the%2520reconstruction%2520of%2520edge%2520and%2520contrast%2520related%2520content.%2520Quantitative%250Aand%2520qualitative%2520experimental%2520results%2520indicate%2520that%2520our%2520approach%2520on%2520IXI%2520and%250AfastMRI%2520datasets%2520achieves%2520competitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deform-Mamba%20Network%20for%20MRI%20Super-Resolution&entry.906535625=Zexin%20Ji%20and%20Beiji%20Zou%20and%20Xiaoyan%20Kui%20and%20Pierre%20Vera%20and%20Su%20Ruan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20architecture%2C%20called%20Deform-Mamba%2C%20for%20MR%0Aimage%20super-resolution.%20Unlike%20conventional%20CNN%20or%20Transformer-based%0Asuper-resolution%20approaches%20which%20encounter%20challenges%20related%20to%20the%20local%0Arespective%20field%20or%20heavy%20computational%20cost%2C%20our%20approach%20aims%20to%20effectively%0Aexplore%20the%20local%20and%20global%20information%20of%20images.%20Specifically%2C%20we%20develop%20a%0ADeform-Mamba%20encoder%20which%20is%20composed%20of%20two%20branches%2C%20modulated%20deform%20block%0Aand%20vision%20Mamba%20block.%20We%20also%20design%20a%20multi-view%20context%20module%20in%20the%0Abottleneck%20layer%20to%20explore%20the%20multi-view%20contextual%20content.%20Thanks%20to%20the%0Aextracted%20features%20of%20the%20encoder%2C%20which%20include%20content-adaptive%20local%20and%0Aefficient%20global%20information%2C%20the%20vision%20Mamba%20decoder%20finally%20generates%0Ahigh-quality%20MR%20images.%20Moreover%2C%20we%20introduce%20a%20contrastive%20edge%20loss%20to%0Apromote%20the%20reconstruction%20of%20edge%20and%20contrast%20related%20content.%20Quantitative%0Aand%20qualitative%20experimental%20results%20indicate%20that%20our%20approach%20on%20IXI%20and%0AfastMRI%20datasets%20achieves%20competitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05969v1&entry.124074799=Read"},
{"title": "Autonomous Drone Racing: A Survey", "author": "Drew Hanover and Antonio Loquercio and Leonard Bauersfeld and Angel Romero and Robert Penicka and Yunlong Song and Giovanni Cioffi and Elia Kaufmann and Davide Scaramuzza", "abstract": "  Over the last decade, the use of autonomous drone systems for surveying,\nsearch and rescue, or last-mile delivery has increased exponentially. With the\nrise of these applications comes the need for highly robust, safety-critical\nalgorithms which can operate drones in complex and uncertain environments.\nAdditionally, flying fast enables drones to cover more ground which in turn\nincreases productivity and further strengthens their use case. One proxy for\ndeveloping algorithms used in high-speed navigation is the task of autonomous\ndrone racing, where researchers program drones to fly through a sequence of\ngates and avoid obstacles as quickly as possible using onboard sensors and\nlimited computational power. Speeds and accelerations exceed over 80 kph and 4\ng respectively, raising significant challenges across perception, planning,\ncontrol, and state estimation. To achieve maximum performance, systems require\nreal-time algorithms that are robust to motion blur, high dynamic range, model\nuncertainties, aerodynamic disturbances, and often unpredictable opponents.\nThis survey covers the progression of autonomous drone racing across\nmodel-based and learning-based approaches. We provide an overview of the field,\nits evolution over the years, and conclude with the biggest challenges and open\nquestions to be faced in the future.\n", "link": "http://arxiv.org/abs/2301.01755v4", "date": "2024-07-08", "relevancy": 2.0454, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5234}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5139}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Drone%20Racing%3A%20A%20Survey&body=Title%3A%20Autonomous%20Drone%20Racing%3A%20A%20Survey%0AAuthor%3A%20Drew%20Hanover%20and%20Antonio%20Loquercio%20and%20Leonard%20Bauersfeld%20and%20Angel%20Romero%20and%20Robert%20Penicka%20and%20Yunlong%20Song%20and%20Giovanni%20Cioffi%20and%20Elia%20Kaufmann%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Over%20the%20last%20decade%2C%20the%20use%20of%20autonomous%20drone%20systems%20for%20surveying%2C%0Asearch%20and%20rescue%2C%20or%20last-mile%20delivery%20has%20increased%20exponentially.%20With%20the%0Arise%20of%20these%20applications%20comes%20the%20need%20for%20highly%20robust%2C%20safety-critical%0Aalgorithms%20which%20can%20operate%20drones%20in%20complex%20and%20uncertain%20environments.%0AAdditionally%2C%20flying%20fast%20enables%20drones%20to%20cover%20more%20ground%20which%20in%20turn%0Aincreases%20productivity%20and%20further%20strengthens%20their%20use%20case.%20One%20proxy%20for%0Adeveloping%20algorithms%20used%20in%20high-speed%20navigation%20is%20the%20task%20of%20autonomous%0Adrone%20racing%2C%20where%20researchers%20program%20drones%20to%20fly%20through%20a%20sequence%20of%0Agates%20and%20avoid%20obstacles%20as%20quickly%20as%20possible%20using%20onboard%20sensors%20and%0Alimited%20computational%20power.%20Speeds%20and%20accelerations%20exceed%20over%2080%20kph%20and%204%0Ag%20respectively%2C%20raising%20significant%20challenges%20across%20perception%2C%20planning%2C%0Acontrol%2C%20and%20state%20estimation.%20To%20achieve%20maximum%20performance%2C%20systems%20require%0Areal-time%20algorithms%20that%20are%20robust%20to%20motion%20blur%2C%20high%20dynamic%20range%2C%20model%0Auncertainties%2C%20aerodynamic%20disturbances%2C%20and%20often%20unpredictable%20opponents.%0AThis%20survey%20covers%20the%20progression%20of%20autonomous%20drone%20racing%20across%0Amodel-based%20and%20learning-based%20approaches.%20We%20provide%20an%20overview%20of%20the%20field%2C%0Aits%20evolution%20over%20the%20years%2C%20and%20conclude%20with%20the%20biggest%20challenges%20and%20open%0Aquestions%20to%20be%20faced%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.01755v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Drone%2520Racing%253A%2520A%2520Survey%26entry.906535625%3DDrew%2520Hanover%2520and%2520Antonio%2520Loquercio%2520and%2520Leonard%2520Bauersfeld%2520and%2520Angel%2520Romero%2520and%2520Robert%2520Penicka%2520and%2520Yunlong%2520Song%2520and%2520Giovanni%2520Cioffi%2520and%2520Elia%2520Kaufmann%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Over%2520the%2520last%2520decade%252C%2520the%2520use%2520of%2520autonomous%2520drone%2520systems%2520for%2520surveying%252C%250Asearch%2520and%2520rescue%252C%2520or%2520last-mile%2520delivery%2520has%2520increased%2520exponentially.%2520With%2520the%250Arise%2520of%2520these%2520applications%2520comes%2520the%2520need%2520for%2520highly%2520robust%252C%2520safety-critical%250Aalgorithms%2520which%2520can%2520operate%2520drones%2520in%2520complex%2520and%2520uncertain%2520environments.%250AAdditionally%252C%2520flying%2520fast%2520enables%2520drones%2520to%2520cover%2520more%2520ground%2520which%2520in%2520turn%250Aincreases%2520productivity%2520and%2520further%2520strengthens%2520their%2520use%2520case.%2520One%2520proxy%2520for%250Adeveloping%2520algorithms%2520used%2520in%2520high-speed%2520navigation%2520is%2520the%2520task%2520of%2520autonomous%250Adrone%2520racing%252C%2520where%2520researchers%2520program%2520drones%2520to%2520fly%2520through%2520a%2520sequence%2520of%250Agates%2520and%2520avoid%2520obstacles%2520as%2520quickly%2520as%2520possible%2520using%2520onboard%2520sensors%2520and%250Alimited%2520computational%2520power.%2520Speeds%2520and%2520accelerations%2520exceed%2520over%252080%2520kph%2520and%25204%250Ag%2520respectively%252C%2520raising%2520significant%2520challenges%2520across%2520perception%252C%2520planning%252C%250Acontrol%252C%2520and%2520state%2520estimation.%2520To%2520achieve%2520maximum%2520performance%252C%2520systems%2520require%250Areal-time%2520algorithms%2520that%2520are%2520robust%2520to%2520motion%2520blur%252C%2520high%2520dynamic%2520range%252C%2520model%250Auncertainties%252C%2520aerodynamic%2520disturbances%252C%2520and%2520often%2520unpredictable%2520opponents.%250AThis%2520survey%2520covers%2520the%2520progression%2520of%2520autonomous%2520drone%2520racing%2520across%250Amodel-based%2520and%2520learning-based%2520approaches.%2520We%2520provide%2520an%2520overview%2520of%2520the%2520field%252C%250Aits%2520evolution%2520over%2520the%2520years%252C%2520and%2520conclude%2520with%2520the%2520biggest%2520challenges%2520and%2520open%250Aquestions%2520to%2520be%2520faced%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.01755v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Drone%20Racing%3A%20A%20Survey&entry.906535625=Drew%20Hanover%20and%20Antonio%20Loquercio%20and%20Leonard%20Bauersfeld%20and%20Angel%20Romero%20and%20Robert%20Penicka%20and%20Yunlong%20Song%20and%20Giovanni%20Cioffi%20and%20Elia%20Kaufmann%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Over%20the%20last%20decade%2C%20the%20use%20of%20autonomous%20drone%20systems%20for%20surveying%2C%0Asearch%20and%20rescue%2C%20or%20last-mile%20delivery%20has%20increased%20exponentially.%20With%20the%0Arise%20of%20these%20applications%20comes%20the%20need%20for%20highly%20robust%2C%20safety-critical%0Aalgorithms%20which%20can%20operate%20drones%20in%20complex%20and%20uncertain%20environments.%0AAdditionally%2C%20flying%20fast%20enables%20drones%20to%20cover%20more%20ground%20which%20in%20turn%0Aincreases%20productivity%20and%20further%20strengthens%20their%20use%20case.%20One%20proxy%20for%0Adeveloping%20algorithms%20used%20in%20high-speed%20navigation%20is%20the%20task%20of%20autonomous%0Adrone%20racing%2C%20where%20researchers%20program%20drones%20to%20fly%20through%20a%20sequence%20of%0Agates%20and%20avoid%20obstacles%20as%20quickly%20as%20possible%20using%20onboard%20sensors%20and%0Alimited%20computational%20power.%20Speeds%20and%20accelerations%20exceed%20over%2080%20kph%20and%204%0Ag%20respectively%2C%20raising%20significant%20challenges%20across%20perception%2C%20planning%2C%0Acontrol%2C%20and%20state%20estimation.%20To%20achieve%20maximum%20performance%2C%20systems%20require%0Areal-time%20algorithms%20that%20are%20robust%20to%20motion%20blur%2C%20high%20dynamic%20range%2C%20model%0Auncertainties%2C%20aerodynamic%20disturbances%2C%20and%20often%20unpredictable%20opponents.%0AThis%20survey%20covers%20the%20progression%20of%20autonomous%20drone%20racing%20across%0Amodel-based%20and%20learning-based%20approaches.%20We%20provide%20an%20overview%20of%20the%20field%2C%0Aits%20evolution%20over%20the%20years%2C%20and%20conclude%20with%20the%20biggest%20challenges%20and%20open%0Aquestions%20to%20be%20faced%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.01755v4&entry.124074799=Read"},
{"title": "Understanding Visual Feature Reliance through the Lens of Complexity", "author": "Thomas Fel and Louis Bethune and Andrew Kyle Lampinen and Thomas Serre and Katherine Hermann", "abstract": "  Recent studies suggest that deep learning models inductive bias towards\nfavoring simpler features may be one of the sources of shortcut learning. Yet,\nthere has been limited focus on understanding the complexity of the myriad\nfeatures that models learn. In this work, we introduce a new metric for\nquantifying feature complexity, based on $\\mathscr{V}$-information and\ncapturing whether a feature requires complex computational transformations to\nbe extracted. Using this $\\mathscr{V}$-information metric, we analyze the\ncomplexities of 10,000 features, represented as directions in the penultimate\nlayer, that were extracted from a standard ImageNet-trained vision model. Our\nstudy addresses four key questions: First, we ask what features look like as a\nfunction of complexity and find a spectrum of simple to complex features\npresent within the model. Second, we ask when features are learned during\ntraining. We find that simpler features dominate early in training, and more\ncomplex features emerge gradually. Third, we investigate where within the\nnetwork simple and complex features flow, and find that simpler features tend\nto bypass the visual hierarchy via residual connections. Fourth, we explore the\nconnection between features complexity and their importance in driving the\nnetworks decision. We find that complex features tend to be less important.\nSurprisingly, important features become accessible at earlier layers during\ntraining, like a sedimentation process, allowing the model to build upon these\nfoundational elements.\n", "link": "http://arxiv.org/abs/2407.06076v1", "date": "2024-07-08", "relevancy": 2.0431, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5276}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5012}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Visual%20Feature%20Reliance%20through%20the%20Lens%20of%20Complexity&body=Title%3A%20Understanding%20Visual%20Feature%20Reliance%20through%20the%20Lens%20of%20Complexity%0AAuthor%3A%20Thomas%20Fel%20and%20Louis%20Bethune%20and%20Andrew%20Kyle%20Lampinen%20and%20Thomas%20Serre%20and%20Katherine%20Hermann%0AAbstract%3A%20%20%20Recent%20studies%20suggest%20that%20deep%20learning%20models%20inductive%20bias%20towards%0Afavoring%20simpler%20features%20may%20be%20one%20of%20the%20sources%20of%20shortcut%20learning.%20Yet%2C%0Athere%20has%20been%20limited%20focus%20on%20understanding%20the%20complexity%20of%20the%20myriad%0Afeatures%20that%20models%20learn.%20In%20this%20work%2C%20we%20introduce%20a%20new%20metric%20for%0Aquantifying%20feature%20complexity%2C%20based%20on%20%24%5Cmathscr%7BV%7D%24-information%20and%0Acapturing%20whether%20a%20feature%20requires%20complex%20computational%20transformations%20to%0Abe%20extracted.%20Using%20this%20%24%5Cmathscr%7BV%7D%24-information%20metric%2C%20we%20analyze%20the%0Acomplexities%20of%2010%2C000%20features%2C%20represented%20as%20directions%20in%20the%20penultimate%0Alayer%2C%20that%20were%20extracted%20from%20a%20standard%20ImageNet-trained%20vision%20model.%20Our%0Astudy%20addresses%20four%20key%20questions%3A%20First%2C%20we%20ask%20what%20features%20look%20like%20as%20a%0Afunction%20of%20complexity%20and%20find%20a%20spectrum%20of%20simple%20to%20complex%20features%0Apresent%20within%20the%20model.%20Second%2C%20we%20ask%20when%20features%20are%20learned%20during%0Atraining.%20We%20find%20that%20simpler%20features%20dominate%20early%20in%20training%2C%20and%20more%0Acomplex%20features%20emerge%20gradually.%20Third%2C%20we%20investigate%20where%20within%20the%0Anetwork%20simple%20and%20complex%20features%20flow%2C%20and%20find%20that%20simpler%20features%20tend%0Ato%20bypass%20the%20visual%20hierarchy%20via%20residual%20connections.%20Fourth%2C%20we%20explore%20the%0Aconnection%20between%20features%20complexity%20and%20their%20importance%20in%20driving%20the%0Anetworks%20decision.%20We%20find%20that%20complex%20features%20tend%20to%20be%20less%20important.%0ASurprisingly%2C%20important%20features%20become%20accessible%20at%20earlier%20layers%20during%0Atraining%2C%20like%20a%20sedimentation%20process%2C%20allowing%20the%20model%20to%20build%20upon%20these%0Afoundational%20elements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Visual%2520Feature%2520Reliance%2520through%2520the%2520Lens%2520of%2520Complexity%26entry.906535625%3DThomas%2520Fel%2520and%2520Louis%2520Bethune%2520and%2520Andrew%2520Kyle%2520Lampinen%2520and%2520Thomas%2520Serre%2520and%2520Katherine%2520Hermann%26entry.1292438233%3D%2520%2520Recent%2520studies%2520suggest%2520that%2520deep%2520learning%2520models%2520inductive%2520bias%2520towards%250Afavoring%2520simpler%2520features%2520may%2520be%2520one%2520of%2520the%2520sources%2520of%2520shortcut%2520learning.%2520Yet%252C%250Athere%2520has%2520been%2520limited%2520focus%2520on%2520understanding%2520the%2520complexity%2520of%2520the%2520myriad%250Afeatures%2520that%2520models%2520learn.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520metric%2520for%250Aquantifying%2520feature%2520complexity%252C%2520based%2520on%2520%2524%255Cmathscr%257BV%257D%2524-information%2520and%250Acapturing%2520whether%2520a%2520feature%2520requires%2520complex%2520computational%2520transformations%2520to%250Abe%2520extracted.%2520Using%2520this%2520%2524%255Cmathscr%257BV%257D%2524-information%2520metric%252C%2520we%2520analyze%2520the%250Acomplexities%2520of%252010%252C000%2520features%252C%2520represented%2520as%2520directions%2520in%2520the%2520penultimate%250Alayer%252C%2520that%2520were%2520extracted%2520from%2520a%2520standard%2520ImageNet-trained%2520vision%2520model.%2520Our%250Astudy%2520addresses%2520four%2520key%2520questions%253A%2520First%252C%2520we%2520ask%2520what%2520features%2520look%2520like%2520as%2520a%250Afunction%2520of%2520complexity%2520and%2520find%2520a%2520spectrum%2520of%2520simple%2520to%2520complex%2520features%250Apresent%2520within%2520the%2520model.%2520Second%252C%2520we%2520ask%2520when%2520features%2520are%2520learned%2520during%250Atraining.%2520We%2520find%2520that%2520simpler%2520features%2520dominate%2520early%2520in%2520training%252C%2520and%2520more%250Acomplex%2520features%2520emerge%2520gradually.%2520Third%252C%2520we%2520investigate%2520where%2520within%2520the%250Anetwork%2520simple%2520and%2520complex%2520features%2520flow%252C%2520and%2520find%2520that%2520simpler%2520features%2520tend%250Ato%2520bypass%2520the%2520visual%2520hierarchy%2520via%2520residual%2520connections.%2520Fourth%252C%2520we%2520explore%2520the%250Aconnection%2520between%2520features%2520complexity%2520and%2520their%2520importance%2520in%2520driving%2520the%250Anetworks%2520decision.%2520We%2520find%2520that%2520complex%2520features%2520tend%2520to%2520be%2520less%2520important.%250ASurprisingly%252C%2520important%2520features%2520become%2520accessible%2520at%2520earlier%2520layers%2520during%250Atraining%252C%2520like%2520a%2520sedimentation%2520process%252C%2520allowing%2520the%2520model%2520to%2520build%2520upon%2520these%250Afoundational%2520elements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Visual%20Feature%20Reliance%20through%20the%20Lens%20of%20Complexity&entry.906535625=Thomas%20Fel%20and%20Louis%20Bethune%20and%20Andrew%20Kyle%20Lampinen%20and%20Thomas%20Serre%20and%20Katherine%20Hermann&entry.1292438233=%20%20Recent%20studies%20suggest%20that%20deep%20learning%20models%20inductive%20bias%20towards%0Afavoring%20simpler%20features%20may%20be%20one%20of%20the%20sources%20of%20shortcut%20learning.%20Yet%2C%0Athere%20has%20been%20limited%20focus%20on%20understanding%20the%20complexity%20of%20the%20myriad%0Afeatures%20that%20models%20learn.%20In%20this%20work%2C%20we%20introduce%20a%20new%20metric%20for%0Aquantifying%20feature%20complexity%2C%20based%20on%20%24%5Cmathscr%7BV%7D%24-information%20and%0Acapturing%20whether%20a%20feature%20requires%20complex%20computational%20transformations%20to%0Abe%20extracted.%20Using%20this%20%24%5Cmathscr%7BV%7D%24-information%20metric%2C%20we%20analyze%20the%0Acomplexities%20of%2010%2C000%20features%2C%20represented%20as%20directions%20in%20the%20penultimate%0Alayer%2C%20that%20were%20extracted%20from%20a%20standard%20ImageNet-trained%20vision%20model.%20Our%0Astudy%20addresses%20four%20key%20questions%3A%20First%2C%20we%20ask%20what%20features%20look%20like%20as%20a%0Afunction%20of%20complexity%20and%20find%20a%20spectrum%20of%20simple%20to%20complex%20features%0Apresent%20within%20the%20model.%20Second%2C%20we%20ask%20when%20features%20are%20learned%20during%0Atraining.%20We%20find%20that%20simpler%20features%20dominate%20early%20in%20training%2C%20and%20more%0Acomplex%20features%20emerge%20gradually.%20Third%2C%20we%20investigate%20where%20within%20the%0Anetwork%20simple%20and%20complex%20features%20flow%2C%20and%20find%20that%20simpler%20features%20tend%0Ato%20bypass%20the%20visual%20hierarchy%20via%20residual%20connections.%20Fourth%2C%20we%20explore%20the%0Aconnection%20between%20features%20complexity%20and%20their%20importance%20in%20driving%20the%0Anetworks%20decision.%20We%20find%20that%20complex%20features%20tend%20to%20be%20less%20important.%0ASurprisingly%2C%20important%20features%20become%20accessible%20at%20earlier%20layers%20during%0Atraining%2C%20like%20a%20sedimentation%20process%2C%20allowing%20the%20model%20to%20build%20upon%20these%0Afoundational%20elements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06076v1&entry.124074799=Read"},
{"title": "MTL-Split: Multi-Task Learning for Edge Devices using Split Computing", "author": "Luigi Capogrosso and Enrico Fraccaroli and Samarjit Chakraborty and Franco Fummi and Marco Cristani", "abstract": "  Split Computing (SC), where a Deep Neural Network (DNN) is intelligently\nsplit with a part of it deployed on an edge device and the rest on a remote\nserver is emerging as a promising approach. It allows the power of DNNs to be\nleveraged for latency-sensitive applications that do not allow the entire DNN\nto be deployed remotely, while not having sufficient computation bandwidth\navailable locally. In many such embedded systems scenarios, such as those in\nthe automotive domain, computational resource constraints also necessitate\nMulti-Task Learning (MTL), where the same DNN is used for multiple inference\ntasks instead of having dedicated DNNs for each task, which would need more\ncomputing bandwidth. However, how to partition such a multi-tasking DNN to be\ndeployed within a SC framework has not been sufficiently studied. This paper\nstudies this problem, and MTL-Split, our novel proposed architecture, shows\nencouraging results on both synthetic and real-world data. The source code is\navailable at https://github.com/intelligolabs/MTL-Split.\n", "link": "http://arxiv.org/abs/2407.05982v1", "date": "2024-07-08", "relevancy": 2.0233, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5269}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5147}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTL-Split%3A%20Multi-Task%20Learning%20for%20Edge%20Devices%20using%20Split%20Computing&body=Title%3A%20MTL-Split%3A%20Multi-Task%20Learning%20for%20Edge%20Devices%20using%20Split%20Computing%0AAuthor%3A%20Luigi%20Capogrosso%20and%20Enrico%20Fraccaroli%20and%20Samarjit%20Chakraborty%20and%20Franco%20Fummi%20and%20Marco%20Cristani%0AAbstract%3A%20%20%20Split%20Computing%20%28SC%29%2C%20where%20a%20Deep%20Neural%20Network%20%28DNN%29%20is%20intelligently%0Asplit%20with%20a%20part%20of%20it%20deployed%20on%20an%20edge%20device%20and%20the%20rest%20on%20a%20remote%0Aserver%20is%20emerging%20as%20a%20promising%20approach.%20It%20allows%20the%20power%20of%20DNNs%20to%20be%0Aleveraged%20for%20latency-sensitive%20applications%20that%20do%20not%20allow%20the%20entire%20DNN%0Ato%20be%20deployed%20remotely%2C%20while%20not%20having%20sufficient%20computation%20bandwidth%0Aavailable%20locally.%20In%20many%20such%20embedded%20systems%20scenarios%2C%20such%20as%20those%20in%0Athe%20automotive%20domain%2C%20computational%20resource%20constraints%20also%20necessitate%0AMulti-Task%20Learning%20%28MTL%29%2C%20where%20the%20same%20DNN%20is%20used%20for%20multiple%20inference%0Atasks%20instead%20of%20having%20dedicated%20DNNs%20for%20each%20task%2C%20which%20would%20need%20more%0Acomputing%20bandwidth.%20However%2C%20how%20to%20partition%20such%20a%20multi-tasking%20DNN%20to%20be%0Adeployed%20within%20a%20SC%20framework%20has%20not%20been%20sufficiently%20studied.%20This%20paper%0Astudies%20this%20problem%2C%20and%20MTL-Split%2C%20our%20novel%20proposed%20architecture%2C%20shows%0Aencouraging%20results%20on%20both%20synthetic%20and%20real-world%20data.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/intelligolabs/MTL-Split.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTL-Split%253A%2520Multi-Task%2520Learning%2520for%2520Edge%2520Devices%2520using%2520Split%2520Computing%26entry.906535625%3DLuigi%2520Capogrosso%2520and%2520Enrico%2520Fraccaroli%2520and%2520Samarjit%2520Chakraborty%2520and%2520Franco%2520Fummi%2520and%2520Marco%2520Cristani%26entry.1292438233%3D%2520%2520Split%2520Computing%2520%2528SC%2529%252C%2520where%2520a%2520Deep%2520Neural%2520Network%2520%2528DNN%2529%2520is%2520intelligently%250Asplit%2520with%2520a%2520part%2520of%2520it%2520deployed%2520on%2520an%2520edge%2520device%2520and%2520the%2520rest%2520on%2520a%2520remote%250Aserver%2520is%2520emerging%2520as%2520a%2520promising%2520approach.%2520It%2520allows%2520the%2520power%2520of%2520DNNs%2520to%2520be%250Aleveraged%2520for%2520latency-sensitive%2520applications%2520that%2520do%2520not%2520allow%2520the%2520entire%2520DNN%250Ato%2520be%2520deployed%2520remotely%252C%2520while%2520not%2520having%2520sufficient%2520computation%2520bandwidth%250Aavailable%2520locally.%2520In%2520many%2520such%2520embedded%2520systems%2520scenarios%252C%2520such%2520as%2520those%2520in%250Athe%2520automotive%2520domain%252C%2520computational%2520resource%2520constraints%2520also%2520necessitate%250AMulti-Task%2520Learning%2520%2528MTL%2529%252C%2520where%2520the%2520same%2520DNN%2520is%2520used%2520for%2520multiple%2520inference%250Atasks%2520instead%2520of%2520having%2520dedicated%2520DNNs%2520for%2520each%2520task%252C%2520which%2520would%2520need%2520more%250Acomputing%2520bandwidth.%2520However%252C%2520how%2520to%2520partition%2520such%2520a%2520multi-tasking%2520DNN%2520to%2520be%250Adeployed%2520within%2520a%2520SC%2520framework%2520has%2520not%2520been%2520sufficiently%2520studied.%2520This%2520paper%250Astudies%2520this%2520problem%252C%2520and%2520MTL-Split%252C%2520our%2520novel%2520proposed%2520architecture%252C%2520shows%250Aencouraging%2520results%2520on%2520both%2520synthetic%2520and%2520real-world%2520data.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/intelligolabs/MTL-Split.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTL-Split%3A%20Multi-Task%20Learning%20for%20Edge%20Devices%20using%20Split%20Computing&entry.906535625=Luigi%20Capogrosso%20and%20Enrico%20Fraccaroli%20and%20Samarjit%20Chakraborty%20and%20Franco%20Fummi%20and%20Marco%20Cristani&entry.1292438233=%20%20Split%20Computing%20%28SC%29%2C%20where%20a%20Deep%20Neural%20Network%20%28DNN%29%20is%20intelligently%0Asplit%20with%20a%20part%20of%20it%20deployed%20on%20an%20edge%20device%20and%20the%20rest%20on%20a%20remote%0Aserver%20is%20emerging%20as%20a%20promising%20approach.%20It%20allows%20the%20power%20of%20DNNs%20to%20be%0Aleveraged%20for%20latency-sensitive%20applications%20that%20do%20not%20allow%20the%20entire%20DNN%0Ato%20be%20deployed%20remotely%2C%20while%20not%20having%20sufficient%20computation%20bandwidth%0Aavailable%20locally.%20In%20many%20such%20embedded%20systems%20scenarios%2C%20such%20as%20those%20in%0Athe%20automotive%20domain%2C%20computational%20resource%20constraints%20also%20necessitate%0AMulti-Task%20Learning%20%28MTL%29%2C%20where%20the%20same%20DNN%20is%20used%20for%20multiple%20inference%0Atasks%20instead%20of%20having%20dedicated%20DNNs%20for%20each%20task%2C%20which%20would%20need%20more%0Acomputing%20bandwidth.%20However%2C%20how%20to%20partition%20such%20a%20multi-tasking%20DNN%20to%20be%0Adeployed%20within%20a%20SC%20framework%20has%20not%20been%20sufficiently%20studied.%20This%20paper%0Astudies%20this%20problem%2C%20and%20MTL-Split%2C%20our%20novel%20proposed%20architecture%2C%20shows%0Aencouraging%20results%20on%20both%20synthetic%20and%20real-world%20data.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/intelligolabs/MTL-Split.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05982v1&entry.124074799=Read"},
{"title": "OpenCIL: Benchmarking Out-of-Distribution Detection in Class-Incremental\n  Learning", "author": "Wenjun Miao and Guansong Pang and Trong-Tung Nguyen and Ruohang Fang and Jin Zheng and Xiao Bai", "abstract": "  Class incremental learning (CIL) aims to learn a model that can not only\nincrementally accommodate new classes, but also maintain the learned knowledge\nof old classes. Out-of-distribution (OOD) detection in CIL is to retain this\nincremental learning ability, while being able to reject unknown samples that\nare drawn from different distributions of the learned classes. This capability\nis crucial to the safety of deploying CIL models in open worlds. However,\ndespite remarkable advancements in the respective CIL and OOD detection, there\nlacks a systematic and large-scale benchmark to assess the capability of\nadvanced CIL models in detecting OOD samples. To fill this gap, in this study\nwe design a comprehensive empirical study to establish such a benchmark, named\n$\\textbf{OpenCIL}$. To this end, we propose two principled frameworks for\nenabling four representative CIL models with 15 diverse OOD detection methods,\nresulting in 60 baseline models for OOD detection in CIL. The empirical\nevaluation is performed on two popular CIL datasets with six commonly-used OOD\ndatasets. One key observation we find through our comprehensive evaluation is\nthat the CIL models can be severely biased towards the OOD samples and newly\nadded classes when they are exposed to open environments. Motivated by this, we\nfurther propose a new baseline for OOD detection in CIL, namely Bi-directional\nEnergy Regularization ($\\textbf{BER}$), which is specially designed to mitigate\nthese two biases in different CIL models by having energy regularization on\nboth old and new classes. Its superior performance is justified in our\nexperiments. All codes and datasets are open-source at\n$https://github.com/mala-lab/OpenCIL$.\n", "link": "http://arxiv.org/abs/2407.06045v1", "date": "2024-07-08", "relevancy": 2.0223, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5115}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5062}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenCIL%3A%20Benchmarking%20Out-of-Distribution%20Detection%20in%20Class-Incremental%0A%20%20Learning&body=Title%3A%20OpenCIL%3A%20Benchmarking%20Out-of-Distribution%20Detection%20in%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Wenjun%20Miao%20and%20Guansong%20Pang%20and%20Trong-Tung%20Nguyen%20and%20Ruohang%20Fang%20and%20Jin%20Zheng%20and%20Xiao%20Bai%0AAbstract%3A%20%20%20Class%20incremental%20learning%20%28CIL%29%20aims%20to%20learn%20a%20model%20that%20can%20not%20only%0Aincrementally%20accommodate%20new%20classes%2C%20but%20also%20maintain%20the%20learned%20knowledge%0Aof%20old%20classes.%20Out-of-distribution%20%28OOD%29%20detection%20in%20CIL%20is%20to%20retain%20this%0Aincremental%20learning%20ability%2C%20while%20being%20able%20to%20reject%20unknown%20samples%20that%0Aare%20drawn%20from%20different%20distributions%20of%20the%20learned%20classes.%20This%20capability%0Ais%20crucial%20to%20the%20safety%20of%20deploying%20CIL%20models%20in%20open%20worlds.%20However%2C%0Adespite%20remarkable%20advancements%20in%20the%20respective%20CIL%20and%20OOD%20detection%2C%20there%0Alacks%20a%20systematic%20and%20large-scale%20benchmark%20to%20assess%20the%20capability%20of%0Aadvanced%20CIL%20models%20in%20detecting%20OOD%20samples.%20To%20fill%20this%20gap%2C%20in%20this%20study%0Awe%20design%20a%20comprehensive%20empirical%20study%20to%20establish%20such%20a%20benchmark%2C%20named%0A%24%5Ctextbf%7BOpenCIL%7D%24.%20To%20this%20end%2C%20we%20propose%20two%20principled%20frameworks%20for%0Aenabling%20four%20representative%20CIL%20models%20with%2015%20diverse%20OOD%20detection%20methods%2C%0Aresulting%20in%2060%20baseline%20models%20for%20OOD%20detection%20in%20CIL.%20The%20empirical%0Aevaluation%20is%20performed%20on%20two%20popular%20CIL%20datasets%20with%20six%20commonly-used%20OOD%0Adatasets.%20One%20key%20observation%20we%20find%20through%20our%20comprehensive%20evaluation%20is%0Athat%20the%20CIL%20models%20can%20be%20severely%20biased%20towards%20the%20OOD%20samples%20and%20newly%0Aadded%20classes%20when%20they%20are%20exposed%20to%20open%20environments.%20Motivated%20by%20this%2C%20we%0Afurther%20propose%20a%20new%20baseline%20for%20OOD%20detection%20in%20CIL%2C%20namely%20Bi-directional%0AEnergy%20Regularization%20%28%24%5Ctextbf%7BBER%7D%24%29%2C%20which%20is%20specially%20designed%20to%20mitigate%0Athese%20two%20biases%20in%20different%20CIL%20models%20by%20having%20energy%20regularization%20on%0Aboth%20old%20and%20new%20classes.%20Its%20superior%20performance%20is%20justified%20in%20our%0Aexperiments.%20All%20codes%20and%20datasets%20are%20open-source%20at%0A%24https%3A//github.com/mala-lab/OpenCIL%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenCIL%253A%2520Benchmarking%2520Out-of-Distribution%2520Detection%2520in%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DWenjun%2520Miao%2520and%2520Guansong%2520Pang%2520and%2520Trong-Tung%2520Nguyen%2520and%2520Ruohang%2520Fang%2520and%2520Jin%2520Zheng%2520and%2520Xiao%2520Bai%26entry.1292438233%3D%2520%2520Class%2520incremental%2520learning%2520%2528CIL%2529%2520aims%2520to%2520learn%2520a%2520model%2520that%2520can%2520not%2520only%250Aincrementally%2520accommodate%2520new%2520classes%252C%2520but%2520also%2520maintain%2520the%2520learned%2520knowledge%250Aof%2520old%2520classes.%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520in%2520CIL%2520is%2520to%2520retain%2520this%250Aincremental%2520learning%2520ability%252C%2520while%2520being%2520able%2520to%2520reject%2520unknown%2520samples%2520that%250Aare%2520drawn%2520from%2520different%2520distributions%2520of%2520the%2520learned%2520classes.%2520This%2520capability%250Ais%2520crucial%2520to%2520the%2520safety%2520of%2520deploying%2520CIL%2520models%2520in%2520open%2520worlds.%2520However%252C%250Adespite%2520remarkable%2520advancements%2520in%2520the%2520respective%2520CIL%2520and%2520OOD%2520detection%252C%2520there%250Alacks%2520a%2520systematic%2520and%2520large-scale%2520benchmark%2520to%2520assess%2520the%2520capability%2520of%250Aadvanced%2520CIL%2520models%2520in%2520detecting%2520OOD%2520samples.%2520To%2520fill%2520this%2520gap%252C%2520in%2520this%2520study%250Awe%2520design%2520a%2520comprehensive%2520empirical%2520study%2520to%2520establish%2520such%2520a%2520benchmark%252C%2520named%250A%2524%255Ctextbf%257BOpenCIL%257D%2524.%2520To%2520this%2520end%252C%2520we%2520propose%2520two%2520principled%2520frameworks%2520for%250Aenabling%2520four%2520representative%2520CIL%2520models%2520with%252015%2520diverse%2520OOD%2520detection%2520methods%252C%250Aresulting%2520in%252060%2520baseline%2520models%2520for%2520OOD%2520detection%2520in%2520CIL.%2520The%2520empirical%250Aevaluation%2520is%2520performed%2520on%2520two%2520popular%2520CIL%2520datasets%2520with%2520six%2520commonly-used%2520OOD%250Adatasets.%2520One%2520key%2520observation%2520we%2520find%2520through%2520our%2520comprehensive%2520evaluation%2520is%250Athat%2520the%2520CIL%2520models%2520can%2520be%2520severely%2520biased%2520towards%2520the%2520OOD%2520samples%2520and%2520newly%250Aadded%2520classes%2520when%2520they%2520are%2520exposed%2520to%2520open%2520environments.%2520Motivated%2520by%2520this%252C%2520we%250Afurther%2520propose%2520a%2520new%2520baseline%2520for%2520OOD%2520detection%2520in%2520CIL%252C%2520namely%2520Bi-directional%250AEnergy%2520Regularization%2520%2528%2524%255Ctextbf%257BBER%257D%2524%2529%252C%2520which%2520is%2520specially%2520designed%2520to%2520mitigate%250Athese%2520two%2520biases%2520in%2520different%2520CIL%2520models%2520by%2520having%2520energy%2520regularization%2520on%250Aboth%2520old%2520and%2520new%2520classes.%2520Its%2520superior%2520performance%2520is%2520justified%2520in%2520our%250Aexperiments.%2520All%2520codes%2520and%2520datasets%2520are%2520open-source%2520at%250A%2524https%253A//github.com/mala-lab/OpenCIL%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenCIL%3A%20Benchmarking%20Out-of-Distribution%20Detection%20in%20Class-Incremental%0A%20%20Learning&entry.906535625=Wenjun%20Miao%20and%20Guansong%20Pang%20and%20Trong-Tung%20Nguyen%20and%20Ruohang%20Fang%20and%20Jin%20Zheng%20and%20Xiao%20Bai&entry.1292438233=%20%20Class%20incremental%20learning%20%28CIL%29%20aims%20to%20learn%20a%20model%20that%20can%20not%20only%0Aincrementally%20accommodate%20new%20classes%2C%20but%20also%20maintain%20the%20learned%20knowledge%0Aof%20old%20classes.%20Out-of-distribution%20%28OOD%29%20detection%20in%20CIL%20is%20to%20retain%20this%0Aincremental%20learning%20ability%2C%20while%20being%20able%20to%20reject%20unknown%20samples%20that%0Aare%20drawn%20from%20different%20distributions%20of%20the%20learned%20classes.%20This%20capability%0Ais%20crucial%20to%20the%20safety%20of%20deploying%20CIL%20models%20in%20open%20worlds.%20However%2C%0Adespite%20remarkable%20advancements%20in%20the%20respective%20CIL%20and%20OOD%20detection%2C%20there%0Alacks%20a%20systematic%20and%20large-scale%20benchmark%20to%20assess%20the%20capability%20of%0Aadvanced%20CIL%20models%20in%20detecting%20OOD%20samples.%20To%20fill%20this%20gap%2C%20in%20this%20study%0Awe%20design%20a%20comprehensive%20empirical%20study%20to%20establish%20such%20a%20benchmark%2C%20named%0A%24%5Ctextbf%7BOpenCIL%7D%24.%20To%20this%20end%2C%20we%20propose%20two%20principled%20frameworks%20for%0Aenabling%20four%20representative%20CIL%20models%20with%2015%20diverse%20OOD%20detection%20methods%2C%0Aresulting%20in%2060%20baseline%20models%20for%20OOD%20detection%20in%20CIL.%20The%20empirical%0Aevaluation%20is%20performed%20on%20two%20popular%20CIL%20datasets%20with%20six%20commonly-used%20OOD%0Adatasets.%20One%20key%20observation%20we%20find%20through%20our%20comprehensive%20evaluation%20is%0Athat%20the%20CIL%20models%20can%20be%20severely%20biased%20towards%20the%20OOD%20samples%20and%20newly%0Aadded%20classes%20when%20they%20are%20exposed%20to%20open%20environments.%20Motivated%20by%20this%2C%20we%0Afurther%20propose%20a%20new%20baseline%20for%20OOD%20detection%20in%20CIL%2C%20namely%20Bi-directional%0AEnergy%20Regularization%20%28%24%5Ctextbf%7BBER%7D%24%29%2C%20which%20is%20specially%20designed%20to%20mitigate%0Athese%20two%20biases%20in%20different%20CIL%20models%20by%20having%20energy%20regularization%20on%0Aboth%20old%20and%20new%20classes.%20Its%20superior%20performance%20is%20justified%20in%20our%0Aexperiments.%20All%20codes%20and%20datasets%20are%20open-source%20at%0A%24https%3A//github.com/mala-lab/OpenCIL%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06045v1&entry.124074799=Read"},
{"title": "RHRSegNet: Relighting High-Resolution Night-Time Semantic Segmentation", "author": "Sarah Elmahdy and Rodaina Hebishy and Ali Hamdi", "abstract": "  Night time semantic segmentation is a crucial task in computer vision,\nfocusing on accurately classifying and segmenting objects in low-light\nconditions. Unlike daytime techniques, which often perform worse in nighttime\nscenes, it is essential for autonomous driving due to insufficient lighting,\nlow illumination, dynamic lighting, shadow effects, and reduced contrast. We\npropose RHRSegNet, implementing a relighting model over a High-Resolution\nNetwork for semantic segmentation. RHRSegNet implements residual convolutional\nfeature learning to handle complex lighting conditions. Our model then feeds\nthe lightened scene feature maps into a high-resolution network for scene\nsegmentation. The network consists of a convolutional producing feature maps\nwith varying resolutions, achieving different levels of resolution through\ndown-sampling and up-sampling. Large nighttime datasets are used for training\nand evaluation, such as NightCity, City-Scape, and Dark-Zurich datasets. Our\nproposed model increases the HRnet segmentation performance by 5% in low-light\nor nighttime images.\n", "link": "http://arxiv.org/abs/2407.06016v1", "date": "2024-07-08", "relevancy": 2.0214, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5332}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4937}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RHRSegNet%3A%20Relighting%20High-Resolution%20Night-Time%20Semantic%20Segmentation&body=Title%3A%20RHRSegNet%3A%20Relighting%20High-Resolution%20Night-Time%20Semantic%20Segmentation%0AAuthor%3A%20Sarah%20Elmahdy%20and%20Rodaina%20Hebishy%20and%20Ali%20Hamdi%0AAbstract%3A%20%20%20Night%20time%20semantic%20segmentation%20is%20a%20crucial%20task%20in%20computer%20vision%2C%0Afocusing%20on%20accurately%20classifying%20and%20segmenting%20objects%20in%20low-light%0Aconditions.%20Unlike%20daytime%20techniques%2C%20which%20often%20perform%20worse%20in%20nighttime%0Ascenes%2C%20it%20is%20essential%20for%20autonomous%20driving%20due%20to%20insufficient%20lighting%2C%0Alow%20illumination%2C%20dynamic%20lighting%2C%20shadow%20effects%2C%20and%20reduced%20contrast.%20We%0Apropose%20RHRSegNet%2C%20implementing%20a%20relighting%20model%20over%20a%20High-Resolution%0ANetwork%20for%20semantic%20segmentation.%20RHRSegNet%20implements%20residual%20convolutional%0Afeature%20learning%20to%20handle%20complex%20lighting%20conditions.%20Our%20model%20then%20feeds%0Athe%20lightened%20scene%20feature%20maps%20into%20a%20high-resolution%20network%20for%20scene%0Asegmentation.%20The%20network%20consists%20of%20a%20convolutional%20producing%20feature%20maps%0Awith%20varying%20resolutions%2C%20achieving%20different%20levels%20of%20resolution%20through%0Adown-sampling%20and%20up-sampling.%20Large%20nighttime%20datasets%20are%20used%20for%20training%0Aand%20evaluation%2C%20such%20as%20NightCity%2C%20City-Scape%2C%20and%20Dark-Zurich%20datasets.%20Our%0Aproposed%20model%20increases%20the%20HRnet%20segmentation%20performance%20by%205%25%20in%20low-light%0Aor%20nighttime%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRHRSegNet%253A%2520Relighting%2520High-Resolution%2520Night-Time%2520Semantic%2520Segmentation%26entry.906535625%3DSarah%2520Elmahdy%2520and%2520Rodaina%2520Hebishy%2520and%2520Ali%2520Hamdi%26entry.1292438233%3D%2520%2520Night%2520time%2520semantic%2520segmentation%2520is%2520a%2520crucial%2520task%2520in%2520computer%2520vision%252C%250Afocusing%2520on%2520accurately%2520classifying%2520and%2520segmenting%2520objects%2520in%2520low-light%250Aconditions.%2520Unlike%2520daytime%2520techniques%252C%2520which%2520often%2520perform%2520worse%2520in%2520nighttime%250Ascenes%252C%2520it%2520is%2520essential%2520for%2520autonomous%2520driving%2520due%2520to%2520insufficient%2520lighting%252C%250Alow%2520illumination%252C%2520dynamic%2520lighting%252C%2520shadow%2520effects%252C%2520and%2520reduced%2520contrast.%2520We%250Apropose%2520RHRSegNet%252C%2520implementing%2520a%2520relighting%2520model%2520over%2520a%2520High-Resolution%250ANetwork%2520for%2520semantic%2520segmentation.%2520RHRSegNet%2520implements%2520residual%2520convolutional%250Afeature%2520learning%2520to%2520handle%2520complex%2520lighting%2520conditions.%2520Our%2520model%2520then%2520feeds%250Athe%2520lightened%2520scene%2520feature%2520maps%2520into%2520a%2520high-resolution%2520network%2520for%2520scene%250Asegmentation.%2520The%2520network%2520consists%2520of%2520a%2520convolutional%2520producing%2520feature%2520maps%250Awith%2520varying%2520resolutions%252C%2520achieving%2520different%2520levels%2520of%2520resolution%2520through%250Adown-sampling%2520and%2520up-sampling.%2520Large%2520nighttime%2520datasets%2520are%2520used%2520for%2520training%250Aand%2520evaluation%252C%2520such%2520as%2520NightCity%252C%2520City-Scape%252C%2520and%2520Dark-Zurich%2520datasets.%2520Our%250Aproposed%2520model%2520increases%2520the%2520HRnet%2520segmentation%2520performance%2520by%25205%2525%2520in%2520low-light%250Aor%2520nighttime%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RHRSegNet%3A%20Relighting%20High-Resolution%20Night-Time%20Semantic%20Segmentation&entry.906535625=Sarah%20Elmahdy%20and%20Rodaina%20Hebishy%20and%20Ali%20Hamdi&entry.1292438233=%20%20Night%20time%20semantic%20segmentation%20is%20a%20crucial%20task%20in%20computer%20vision%2C%0Afocusing%20on%20accurately%20classifying%20and%20segmenting%20objects%20in%20low-light%0Aconditions.%20Unlike%20daytime%20techniques%2C%20which%20often%20perform%20worse%20in%20nighttime%0Ascenes%2C%20it%20is%20essential%20for%20autonomous%20driving%20due%20to%20insufficient%20lighting%2C%0Alow%20illumination%2C%20dynamic%20lighting%2C%20shadow%20effects%2C%20and%20reduced%20contrast.%20We%0Apropose%20RHRSegNet%2C%20implementing%20a%20relighting%20model%20over%20a%20High-Resolution%0ANetwork%20for%20semantic%20segmentation.%20RHRSegNet%20implements%20residual%20convolutional%0Afeature%20learning%20to%20handle%20complex%20lighting%20conditions.%20Our%20model%20then%20feeds%0Athe%20lightened%20scene%20feature%20maps%20into%20a%20high-resolution%20network%20for%20scene%0Asegmentation.%20The%20network%20consists%20of%20a%20convolutional%20producing%20feature%20maps%0Awith%20varying%20resolutions%2C%20achieving%20different%20levels%20of%20resolution%20through%0Adown-sampling%20and%20up-sampling.%20Large%20nighttime%20datasets%20are%20used%20for%20training%0Aand%20evaluation%2C%20such%20as%20NightCity%2C%20City-Scape%2C%20and%20Dark-Zurich%20datasets.%20Our%0Aproposed%20model%20increases%20the%20HRnet%20segmentation%20performance%20by%205%25%20in%20low-light%0Aor%20nighttime%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06016v1&entry.124074799=Read"},
{"title": "Graph Anomaly Detection with Noisy Labels by Reinforcement Learning", "author": "Zhu Wang and Shuang Zhou and Junnan Dong and Chang Yang and Xiao Huang and Shengjie Zhao", "abstract": "  Graph anomaly detection (GAD) has been widely applied in many areas, e.g.,\nfraud detection in finance and robot accounts in social networks. Existing\nmethods are dedicated to identifying the outlier nodes that deviate from normal\nones. While they heavily rely on high-quality annotation, which is hard to\nobtain in real-world scenarios, this could lead to severely degraded\nperformance based on noisy labels. Thus, we are motivated to cut the edges of\nsuspicious nodes to alleviate the impact of noise. However, it remains\ndifficult to precisely identify the nodes with noisy labels. Moreover, it is\nhard to quantitatively evaluate the regret of cutting the edges, which may have\neither positive or negative influences. To this end, we propose a novel\nframework REGAD, i.e., REinforced Graph Anomaly Detector. Specifically, we aim\nto maximize the performance improvement (AUC) of a base detector by cutting\nnoisy edges approximated through the nodes with high-confidence labels. (i) We\ndesign a tailored action and search space to train a policy network to\ncarefully prune edges step by step, where only a few suspicious edges are\nprioritized in each step. (ii) We design a policy-in-the-loop mechanism to\niteratively optimize the policy based on the feedback from base detector. The\noverall performance is evaluated by the cumulative rewards. Extensive\nexperiments are conducted on three datasets under different anomaly ratios. The\nresults indicate the superior performance of our proposed REGAD.\n", "link": "http://arxiv.org/abs/2407.05934v1", "date": "2024-07-08", "relevancy": 2.0061, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5045}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5025}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Anomaly%20Detection%20with%20Noisy%20Labels%20by%20Reinforcement%20Learning&body=Title%3A%20Graph%20Anomaly%20Detection%20with%20Noisy%20Labels%20by%20Reinforcement%20Learning%0AAuthor%3A%20Zhu%20Wang%20and%20Shuang%20Zhou%20and%20Junnan%20Dong%20and%20Chang%20Yang%20and%20Xiao%20Huang%20and%20Shengjie%20Zhao%0AAbstract%3A%20%20%20Graph%20anomaly%20detection%20%28GAD%29%20has%20been%20widely%20applied%20in%20many%20areas%2C%20e.g.%2C%0Afraud%20detection%20in%20finance%20and%20robot%20accounts%20in%20social%20networks.%20Existing%0Amethods%20are%20dedicated%20to%20identifying%20the%20outlier%20nodes%20that%20deviate%20from%20normal%0Aones.%20While%20they%20heavily%20rely%20on%20high-quality%20annotation%2C%20which%20is%20hard%20to%0Aobtain%20in%20real-world%20scenarios%2C%20this%20could%20lead%20to%20severely%20degraded%0Aperformance%20based%20on%20noisy%20labels.%20Thus%2C%20we%20are%20motivated%20to%20cut%20the%20edges%20of%0Asuspicious%20nodes%20to%20alleviate%20the%20impact%20of%20noise.%20However%2C%20it%20remains%0Adifficult%20to%20precisely%20identify%20the%20nodes%20with%20noisy%20labels.%20Moreover%2C%20it%20is%0Ahard%20to%20quantitatively%20evaluate%20the%20regret%20of%20cutting%20the%20edges%2C%20which%20may%20have%0Aeither%20positive%20or%20negative%20influences.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Aframework%20REGAD%2C%20i.e.%2C%20REinforced%20Graph%20Anomaly%20Detector.%20Specifically%2C%20we%20aim%0Ato%20maximize%20the%20performance%20improvement%20%28AUC%29%20of%20a%20base%20detector%20by%20cutting%0Anoisy%20edges%20approximated%20through%20the%20nodes%20with%20high-confidence%20labels.%20%28i%29%20We%0Adesign%20a%20tailored%20action%20and%20search%20space%20to%20train%20a%20policy%20network%20to%0Acarefully%20prune%20edges%20step%20by%20step%2C%20where%20only%20a%20few%20suspicious%20edges%20are%0Aprioritized%20in%20each%20step.%20%28ii%29%20We%20design%20a%20policy-in-the-loop%20mechanism%20to%0Aiteratively%20optimize%20the%20policy%20based%20on%20the%20feedback%20from%20base%20detector.%20The%0Aoverall%20performance%20is%20evaluated%20by%20the%20cumulative%20rewards.%20Extensive%0Aexperiments%20are%20conducted%20on%20three%20datasets%20under%20different%20anomaly%20ratios.%20The%0Aresults%20indicate%20the%20superior%20performance%20of%20our%20proposed%20REGAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Anomaly%2520Detection%2520with%2520Noisy%2520Labels%2520by%2520Reinforcement%2520Learning%26entry.906535625%3DZhu%2520Wang%2520and%2520Shuang%2520Zhou%2520and%2520Junnan%2520Dong%2520and%2520Chang%2520Yang%2520and%2520Xiao%2520Huang%2520and%2520Shengjie%2520Zhao%26entry.1292438233%3D%2520%2520Graph%2520anomaly%2520detection%2520%2528GAD%2529%2520has%2520been%2520widely%2520applied%2520in%2520many%2520areas%252C%2520e.g.%252C%250Afraud%2520detection%2520in%2520finance%2520and%2520robot%2520accounts%2520in%2520social%2520networks.%2520Existing%250Amethods%2520are%2520dedicated%2520to%2520identifying%2520the%2520outlier%2520nodes%2520that%2520deviate%2520from%2520normal%250Aones.%2520While%2520they%2520heavily%2520rely%2520on%2520high-quality%2520annotation%252C%2520which%2520is%2520hard%2520to%250Aobtain%2520in%2520real-world%2520scenarios%252C%2520this%2520could%2520lead%2520to%2520severely%2520degraded%250Aperformance%2520based%2520on%2520noisy%2520labels.%2520Thus%252C%2520we%2520are%2520motivated%2520to%2520cut%2520the%2520edges%2520of%250Asuspicious%2520nodes%2520to%2520alleviate%2520the%2520impact%2520of%2520noise.%2520However%252C%2520it%2520remains%250Adifficult%2520to%2520precisely%2520identify%2520the%2520nodes%2520with%2520noisy%2520labels.%2520Moreover%252C%2520it%2520is%250Ahard%2520to%2520quantitatively%2520evaluate%2520the%2520regret%2520of%2520cutting%2520the%2520edges%252C%2520which%2520may%2520have%250Aeither%2520positive%2520or%2520negative%2520influences.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520REGAD%252C%2520i.e.%252C%2520REinforced%2520Graph%2520Anomaly%2520Detector.%2520Specifically%252C%2520we%2520aim%250Ato%2520maximize%2520the%2520performance%2520improvement%2520%2528AUC%2529%2520of%2520a%2520base%2520detector%2520by%2520cutting%250Anoisy%2520edges%2520approximated%2520through%2520the%2520nodes%2520with%2520high-confidence%2520labels.%2520%2528i%2529%2520We%250Adesign%2520a%2520tailored%2520action%2520and%2520search%2520space%2520to%2520train%2520a%2520policy%2520network%2520to%250Acarefully%2520prune%2520edges%2520step%2520by%2520step%252C%2520where%2520only%2520a%2520few%2520suspicious%2520edges%2520are%250Aprioritized%2520in%2520each%2520step.%2520%2528ii%2529%2520We%2520design%2520a%2520policy-in-the-loop%2520mechanism%2520to%250Aiteratively%2520optimize%2520the%2520policy%2520based%2520on%2520the%2520feedback%2520from%2520base%2520detector.%2520The%250Aoverall%2520performance%2520is%2520evaluated%2520by%2520the%2520cumulative%2520rewards.%2520Extensive%250Aexperiments%2520are%2520conducted%2520on%2520three%2520datasets%2520under%2520different%2520anomaly%2520ratios.%2520The%250Aresults%2520indicate%2520the%2520superior%2520performance%2520of%2520our%2520proposed%2520REGAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Anomaly%20Detection%20with%20Noisy%20Labels%20by%20Reinforcement%20Learning&entry.906535625=Zhu%20Wang%20and%20Shuang%20Zhou%20and%20Junnan%20Dong%20and%20Chang%20Yang%20and%20Xiao%20Huang%20and%20Shengjie%20Zhao&entry.1292438233=%20%20Graph%20anomaly%20detection%20%28GAD%29%20has%20been%20widely%20applied%20in%20many%20areas%2C%20e.g.%2C%0Afraud%20detection%20in%20finance%20and%20robot%20accounts%20in%20social%20networks.%20Existing%0Amethods%20are%20dedicated%20to%20identifying%20the%20outlier%20nodes%20that%20deviate%20from%20normal%0Aones.%20While%20they%20heavily%20rely%20on%20high-quality%20annotation%2C%20which%20is%20hard%20to%0Aobtain%20in%20real-world%20scenarios%2C%20this%20could%20lead%20to%20severely%20degraded%0Aperformance%20based%20on%20noisy%20labels.%20Thus%2C%20we%20are%20motivated%20to%20cut%20the%20edges%20of%0Asuspicious%20nodes%20to%20alleviate%20the%20impact%20of%20noise.%20However%2C%20it%20remains%0Adifficult%20to%20precisely%20identify%20the%20nodes%20with%20noisy%20labels.%20Moreover%2C%20it%20is%0Ahard%20to%20quantitatively%20evaluate%20the%20regret%20of%20cutting%20the%20edges%2C%20which%20may%20have%0Aeither%20positive%20or%20negative%20influences.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Aframework%20REGAD%2C%20i.e.%2C%20REinforced%20Graph%20Anomaly%20Detector.%20Specifically%2C%20we%20aim%0Ato%20maximize%20the%20performance%20improvement%20%28AUC%29%20of%20a%20base%20detector%20by%20cutting%0Anoisy%20edges%20approximated%20through%20the%20nodes%20with%20high-confidence%20labels.%20%28i%29%20We%0Adesign%20a%20tailored%20action%20and%20search%20space%20to%20train%20a%20policy%20network%20to%0Acarefully%20prune%20edges%20step%20by%20step%2C%20where%20only%20a%20few%20suspicious%20edges%20are%0Aprioritized%20in%20each%20step.%20%28ii%29%20We%20design%20a%20policy-in-the-loop%20mechanism%20to%0Aiteratively%20optimize%20the%20policy%20based%20on%20the%20feedback%20from%20base%20detector.%20The%0Aoverall%20performance%20is%20evaluated%20by%20the%20cumulative%20rewards.%20Extensive%0Aexperiments%20are%20conducted%20on%20three%20datasets%20under%20different%20anomaly%20ratios.%20The%0Aresults%20indicate%20the%20superior%20performance%20of%20our%20proposed%20REGAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05934v1&entry.124074799=Read"},
{"title": "Potential Based Diffusion Motion Planning", "author": "Yunhao Luo and Chen Sun and Joshua B. Tenenbaum and Yilun Du", "abstract": "  Effective motion planning in high dimensional spaces is a long-standing open\nproblem in robotics. One class of traditional motion planning algorithms\ncorresponds to potential-based motion planning. An advantage of potential based\nmotion planning is composability -- different motion constraints can be easily\ncombined by adding corresponding potentials. However, constructing motion paths\nfrom potentials requires solving a global optimization across configuration\nspace potential landscape, which is often prone to local minima. We propose a\nnew approach towards learning potential based motion planning, where we train a\nneural network to capture and learn an easily optimizable potentials over\nmotion planning trajectories. We illustrate the effectiveness of such approach,\nsignificantly outperforming both classical and recent learned motion planning\napproaches and avoiding issues with local minima. We further illustrate its\ninherent composability, enabling us to generalize to a multitude of different\nmotion constraints.\n", "link": "http://arxiv.org/abs/2407.06169v1", "date": "2024-07-08", "relevancy": 2.0054, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5316}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4995}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Potential%20Based%20Diffusion%20Motion%20Planning&body=Title%3A%20Potential%20Based%20Diffusion%20Motion%20Planning%0AAuthor%3A%20Yunhao%20Luo%20and%20Chen%20Sun%20and%20Joshua%20B.%20Tenenbaum%20and%20Yilun%20Du%0AAbstract%3A%20%20%20Effective%20motion%20planning%20in%20high%20dimensional%20spaces%20is%20a%20long-standing%20open%0Aproblem%20in%20robotics.%20One%20class%20of%20traditional%20motion%20planning%20algorithms%0Acorresponds%20to%20potential-based%20motion%20planning.%20An%20advantage%20of%20potential%20based%0Amotion%20planning%20is%20composability%20--%20different%20motion%20constraints%20can%20be%20easily%0Acombined%20by%20adding%20corresponding%20potentials.%20However%2C%20constructing%20motion%20paths%0Afrom%20potentials%20requires%20solving%20a%20global%20optimization%20across%20configuration%0Aspace%20potential%20landscape%2C%20which%20is%20often%20prone%20to%20local%20minima.%20We%20propose%20a%0Anew%20approach%20towards%20learning%20potential%20based%20motion%20planning%2C%20where%20we%20train%20a%0Aneural%20network%20to%20capture%20and%20learn%20an%20easily%20optimizable%20potentials%20over%0Amotion%20planning%20trajectories.%20We%20illustrate%20the%20effectiveness%20of%20such%20approach%2C%0Asignificantly%20outperforming%20both%20classical%20and%20recent%20learned%20motion%20planning%0Aapproaches%20and%20avoiding%20issues%20with%20local%20minima.%20We%20further%20illustrate%20its%0Ainherent%20composability%2C%20enabling%20us%20to%20generalize%20to%20a%20multitude%20of%20different%0Amotion%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPotential%2520Based%2520Diffusion%2520Motion%2520Planning%26entry.906535625%3DYunhao%2520Luo%2520and%2520Chen%2520Sun%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Yilun%2520Du%26entry.1292438233%3D%2520%2520Effective%2520motion%2520planning%2520in%2520high%2520dimensional%2520spaces%2520is%2520a%2520long-standing%2520open%250Aproblem%2520in%2520robotics.%2520One%2520class%2520of%2520traditional%2520motion%2520planning%2520algorithms%250Acorresponds%2520to%2520potential-based%2520motion%2520planning.%2520An%2520advantage%2520of%2520potential%2520based%250Amotion%2520planning%2520is%2520composability%2520--%2520different%2520motion%2520constraints%2520can%2520be%2520easily%250Acombined%2520by%2520adding%2520corresponding%2520potentials.%2520However%252C%2520constructing%2520motion%2520paths%250Afrom%2520potentials%2520requires%2520solving%2520a%2520global%2520optimization%2520across%2520configuration%250Aspace%2520potential%2520landscape%252C%2520which%2520is%2520often%2520prone%2520to%2520local%2520minima.%2520We%2520propose%2520a%250Anew%2520approach%2520towards%2520learning%2520potential%2520based%2520motion%2520planning%252C%2520where%2520we%2520train%2520a%250Aneural%2520network%2520to%2520capture%2520and%2520learn%2520an%2520easily%2520optimizable%2520potentials%2520over%250Amotion%2520planning%2520trajectories.%2520We%2520illustrate%2520the%2520effectiveness%2520of%2520such%2520approach%252C%250Asignificantly%2520outperforming%2520both%2520classical%2520and%2520recent%2520learned%2520motion%2520planning%250Aapproaches%2520and%2520avoiding%2520issues%2520with%2520local%2520minima.%2520We%2520further%2520illustrate%2520its%250Ainherent%2520composability%252C%2520enabling%2520us%2520to%2520generalize%2520to%2520a%2520multitude%2520of%2520different%250Amotion%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Potential%20Based%20Diffusion%20Motion%20Planning&entry.906535625=Yunhao%20Luo%20and%20Chen%20Sun%20and%20Joshua%20B.%20Tenenbaum%20and%20Yilun%20Du&entry.1292438233=%20%20Effective%20motion%20planning%20in%20high%20dimensional%20spaces%20is%20a%20long-standing%20open%0Aproblem%20in%20robotics.%20One%20class%20of%20traditional%20motion%20planning%20algorithms%0Acorresponds%20to%20potential-based%20motion%20planning.%20An%20advantage%20of%20potential%20based%0Amotion%20planning%20is%20composability%20--%20different%20motion%20constraints%20can%20be%20easily%0Acombined%20by%20adding%20corresponding%20potentials.%20However%2C%20constructing%20motion%20paths%0Afrom%20potentials%20requires%20solving%20a%20global%20optimization%20across%20configuration%0Aspace%20potential%20landscape%2C%20which%20is%20often%20prone%20to%20local%20minima.%20We%20propose%20a%0Anew%20approach%20towards%20learning%20potential%20based%20motion%20planning%2C%20where%20we%20train%20a%0Aneural%20network%20to%20capture%20and%20learn%20an%20easily%20optimizable%20potentials%20over%0Amotion%20planning%20trajectories.%20We%20illustrate%20the%20effectiveness%20of%20such%20approach%2C%0Asignificantly%20outperforming%20both%20classical%20and%20recent%20learned%20motion%20planning%0Aapproaches%20and%20avoiding%20issues%20with%20local%20minima.%20We%20further%20illustrate%20its%0Ainherent%20composability%2C%20enabling%20us%20to%20generalize%20to%20a%20multitude%20of%20different%0Amotion%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06169v1&entry.124074799=Read"},
{"title": "Link Representation Learning for Probabilistic Travel Time Estimation", "author": "Chen Xu and Qiang Wang and Lijun Sun", "abstract": "  Travel time estimation is a crucial application in navigation apps and web\nmapping services. Current deterministic and probabilistic methods primarily\nfocus on modeling individual trips, assuming independence among trips. However,\nin real-world scenarios, we often observe strong inter-trip correlations due to\nfactors such as weather conditions, traffic management, and road works. In this\npaper, we propose to model trip-level link travel time using a Gaussian\nhierarchical model, which can characterize both inter-trip and intra-trip\ncorrelations. The joint distribution of travel time of multiple trips becomes a\nmultivariate Gaussian parameterized by learnable link representations. To\neffectively use the sparse GPS trajectories, we also propose a data\naugmentation method based on trip sub-sampling, which allows for fine-grained\ngradient backpropagation in learning link representations. During inference, we\nestimate the probability distribution of the travel time of a queried trip\nconditional on the completed trips that are spatiotemporally adjacent. We refer\nto the overall framework as ProbTTE. We evaluate ProbTTE on two real-world GPS\ntrajectory datasets, and the results demonstrate its superior performance\ncompared to state-of-the-art deterministic and probabilistic baselines.\nAdditionally, we find that the learned link representations align well with the\nphysical geometry of the network, making them suitable as input for other\napplications.\n", "link": "http://arxiv.org/abs/2407.05895v1", "date": "2024-07-08", "relevancy": 2.0014, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5199}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5052}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Link%20Representation%20Learning%20for%20Probabilistic%20Travel%20Time%20Estimation&body=Title%3A%20Link%20Representation%20Learning%20for%20Probabilistic%20Travel%20Time%20Estimation%0AAuthor%3A%20Chen%20Xu%20and%20Qiang%20Wang%20and%20Lijun%20Sun%0AAbstract%3A%20%20%20Travel%20time%20estimation%20is%20a%20crucial%20application%20in%20navigation%20apps%20and%20web%0Amapping%20services.%20Current%20deterministic%20and%20probabilistic%20methods%20primarily%0Afocus%20on%20modeling%20individual%20trips%2C%20assuming%20independence%20among%20trips.%20However%2C%0Ain%20real-world%20scenarios%2C%20we%20often%20observe%20strong%20inter-trip%20correlations%20due%20to%0Afactors%20such%20as%20weather%20conditions%2C%20traffic%20management%2C%20and%20road%20works.%20In%20this%0Apaper%2C%20we%20propose%20to%20model%20trip-level%20link%20travel%20time%20using%20a%20Gaussian%0Ahierarchical%20model%2C%20which%20can%20characterize%20both%20inter-trip%20and%20intra-trip%0Acorrelations.%20The%20joint%20distribution%20of%20travel%20time%20of%20multiple%20trips%20becomes%20a%0Amultivariate%20Gaussian%20parameterized%20by%20learnable%20link%20representations.%20To%0Aeffectively%20use%20the%20sparse%20GPS%20trajectories%2C%20we%20also%20propose%20a%20data%0Aaugmentation%20method%20based%20on%20trip%20sub-sampling%2C%20which%20allows%20for%20fine-grained%0Agradient%20backpropagation%20in%20learning%20link%20representations.%20During%20inference%2C%20we%0Aestimate%20the%20probability%20distribution%20of%20the%20travel%20time%20of%20a%20queried%20trip%0Aconditional%20on%20the%20completed%20trips%20that%20are%20spatiotemporally%20adjacent.%20We%20refer%0Ato%20the%20overall%20framework%20as%20ProbTTE.%20We%20evaluate%20ProbTTE%20on%20two%20real-world%20GPS%0Atrajectory%20datasets%2C%20and%20the%20results%20demonstrate%20its%20superior%20performance%0Acompared%20to%20state-of-the-art%20deterministic%20and%20probabilistic%20baselines.%0AAdditionally%2C%20we%20find%20that%20the%20learned%20link%20representations%20align%20well%20with%20the%0Aphysical%20geometry%20of%20the%20network%2C%20making%20them%20suitable%20as%20input%20for%20other%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLink%2520Representation%2520Learning%2520for%2520Probabilistic%2520Travel%2520Time%2520Estimation%26entry.906535625%3DChen%2520Xu%2520and%2520Qiang%2520Wang%2520and%2520Lijun%2520Sun%26entry.1292438233%3D%2520%2520Travel%2520time%2520estimation%2520is%2520a%2520crucial%2520application%2520in%2520navigation%2520apps%2520and%2520web%250Amapping%2520services.%2520Current%2520deterministic%2520and%2520probabilistic%2520methods%2520primarily%250Afocus%2520on%2520modeling%2520individual%2520trips%252C%2520assuming%2520independence%2520among%2520trips.%2520However%252C%250Ain%2520real-world%2520scenarios%252C%2520we%2520often%2520observe%2520strong%2520inter-trip%2520correlations%2520due%2520to%250Afactors%2520such%2520as%2520weather%2520conditions%252C%2520traffic%2520management%252C%2520and%2520road%2520works.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520to%2520model%2520trip-level%2520link%2520travel%2520time%2520using%2520a%2520Gaussian%250Ahierarchical%2520model%252C%2520which%2520can%2520characterize%2520both%2520inter-trip%2520and%2520intra-trip%250Acorrelations.%2520The%2520joint%2520distribution%2520of%2520travel%2520time%2520of%2520multiple%2520trips%2520becomes%2520a%250Amultivariate%2520Gaussian%2520parameterized%2520by%2520learnable%2520link%2520representations.%2520To%250Aeffectively%2520use%2520the%2520sparse%2520GPS%2520trajectories%252C%2520we%2520also%2520propose%2520a%2520data%250Aaugmentation%2520method%2520based%2520on%2520trip%2520sub-sampling%252C%2520which%2520allows%2520for%2520fine-grained%250Agradient%2520backpropagation%2520in%2520learning%2520link%2520representations.%2520During%2520inference%252C%2520we%250Aestimate%2520the%2520probability%2520distribution%2520of%2520the%2520travel%2520time%2520of%2520a%2520queried%2520trip%250Aconditional%2520on%2520the%2520completed%2520trips%2520that%2520are%2520spatiotemporally%2520adjacent.%2520We%2520refer%250Ato%2520the%2520overall%2520framework%2520as%2520ProbTTE.%2520We%2520evaluate%2520ProbTTE%2520on%2520two%2520real-world%2520GPS%250Atrajectory%2520datasets%252C%2520and%2520the%2520results%2520demonstrate%2520its%2520superior%2520performance%250Acompared%2520to%2520state-of-the-art%2520deterministic%2520and%2520probabilistic%2520baselines.%250AAdditionally%252C%2520we%2520find%2520that%2520the%2520learned%2520link%2520representations%2520align%2520well%2520with%2520the%250Aphysical%2520geometry%2520of%2520the%2520network%252C%2520making%2520them%2520suitable%2520as%2520input%2520for%2520other%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Link%20Representation%20Learning%20for%20Probabilistic%20Travel%20Time%20Estimation&entry.906535625=Chen%20Xu%20and%20Qiang%20Wang%20and%20Lijun%20Sun&entry.1292438233=%20%20Travel%20time%20estimation%20is%20a%20crucial%20application%20in%20navigation%20apps%20and%20web%0Amapping%20services.%20Current%20deterministic%20and%20probabilistic%20methods%20primarily%0Afocus%20on%20modeling%20individual%20trips%2C%20assuming%20independence%20among%20trips.%20However%2C%0Ain%20real-world%20scenarios%2C%20we%20often%20observe%20strong%20inter-trip%20correlations%20due%20to%0Afactors%20such%20as%20weather%20conditions%2C%20traffic%20management%2C%20and%20road%20works.%20In%20this%0Apaper%2C%20we%20propose%20to%20model%20trip-level%20link%20travel%20time%20using%20a%20Gaussian%0Ahierarchical%20model%2C%20which%20can%20characterize%20both%20inter-trip%20and%20intra-trip%0Acorrelations.%20The%20joint%20distribution%20of%20travel%20time%20of%20multiple%20trips%20becomes%20a%0Amultivariate%20Gaussian%20parameterized%20by%20learnable%20link%20representations.%20To%0Aeffectively%20use%20the%20sparse%20GPS%20trajectories%2C%20we%20also%20propose%20a%20data%0Aaugmentation%20method%20based%20on%20trip%20sub-sampling%2C%20which%20allows%20for%20fine-grained%0Agradient%20backpropagation%20in%20learning%20link%20representations.%20During%20inference%2C%20we%0Aestimate%20the%20probability%20distribution%20of%20the%20travel%20time%20of%20a%20queried%20trip%0Aconditional%20on%20the%20completed%20trips%20that%20are%20spatiotemporally%20adjacent.%20We%20refer%0Ato%20the%20overall%20framework%20as%20ProbTTE.%20We%20evaluate%20ProbTTE%20on%20two%20real-world%20GPS%0Atrajectory%20datasets%2C%20and%20the%20results%20demonstrate%20its%20superior%20performance%0Acompared%20to%20state-of-the-art%20deterministic%20and%20probabilistic%20baselines.%0AAdditionally%2C%20we%20find%20that%20the%20learned%20link%20representations%20align%20well%20with%20the%0Aphysical%20geometry%20of%20the%20network%2C%20making%20them%20suitable%20as%20input%20for%20other%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05895v1&entry.124074799=Read"},
{"title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable\n  AI Systems", "author": "David \"davidad\" Dalrymple and Joar Skalse and Yoshua Bengio and Stuart Russell and Max Tegmark and Sanjit Seshia and Steve Omohundro and Christian Szegedy and Ben Goldhaber and Nora Ammann and Alessandro Abate and Joe Halpern and Clark Barrett and Ding Zhao and Tan Zhi-Xuan and Jeannette Wing and Joshua Tenenbaum", "abstract": "  Ensuring that AI systems reliably and robustly avoid harmful or dangerous\nbehaviours is a crucial challenge, especially for AI systems with a high degree\nof autonomy and general intelligence, or systems used in safety-critical\ncontexts. In this paper, we will introduce and define a family of approaches to\nAI safety, which we will refer to as guaranteed safe (GS) AI. The core feature\nof these approaches is that they aim to produce AI systems which are equipped\nwith high-assurance quantitative safety guarantees. This is achieved by the\ninterplay of three core components: a world model (which provides a\nmathematical description of how the AI system affects the outside world), a\nsafety specification (which is a mathematical description of what effects are\nacceptable), and a verifier (which provides an auditable proof certificate that\nthe AI satisfies the safety specification relative to the world model). We\noutline a number of approaches for creating each of these three core\ncomponents, describe the main technical challenges, and suggest a number of\npotential solutions to them. We also argue for the necessity of this approach\nto AI safety, and for the inadequacy of the main alternative approaches.\n", "link": "http://arxiv.org/abs/2405.06624v3", "date": "2024-07-08", "relevancy": 0.9153, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4609}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4592}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Guaranteed%20Safe%20AI%3A%20A%20Framework%20for%20Ensuring%20Robust%20and%20Reliable%0A%20%20AI%20Systems&body=Title%3A%20Towards%20Guaranteed%20Safe%20AI%3A%20A%20Framework%20for%20Ensuring%20Robust%20and%20Reliable%0A%20%20AI%20Systems%0AAuthor%3A%20David%20%22davidad%22%20Dalrymple%20and%20Joar%20Skalse%20and%20Yoshua%20Bengio%20and%20Stuart%20Russell%20and%20Max%20Tegmark%20and%20Sanjit%20Seshia%20and%20Steve%20Omohundro%20and%20Christian%20Szegedy%20and%20Ben%20Goldhaber%20and%20Nora%20Ammann%20and%20Alessandro%20Abate%20and%20Joe%20Halpern%20and%20Clark%20Barrett%20and%20Ding%20Zhao%20and%20Tan%20Zhi-Xuan%20and%20Jeannette%20Wing%20and%20Joshua%20Tenenbaum%0AAbstract%3A%20%20%20Ensuring%20that%20AI%20systems%20reliably%20and%20robustly%20avoid%20harmful%20or%20dangerous%0Abehaviours%20is%20a%20crucial%20challenge%2C%20especially%20for%20AI%20systems%20with%20a%20high%20degree%0Aof%20autonomy%20and%20general%20intelligence%2C%20or%20systems%20used%20in%20safety-critical%0Acontexts.%20In%20this%20paper%2C%20we%20will%20introduce%20and%20define%20a%20family%20of%20approaches%20to%0AAI%20safety%2C%20which%20we%20will%20refer%20to%20as%20guaranteed%20safe%20%28GS%29%20AI.%20The%20core%20feature%0Aof%20these%20approaches%20is%20that%20they%20aim%20to%20produce%20AI%20systems%20which%20are%20equipped%0Awith%20high-assurance%20quantitative%20safety%20guarantees.%20This%20is%20achieved%20by%20the%0Ainterplay%20of%20three%20core%20components%3A%20a%20world%20model%20%28which%20provides%20a%0Amathematical%20description%20of%20how%20the%20AI%20system%20affects%20the%20outside%20world%29%2C%20a%0Asafety%20specification%20%28which%20is%20a%20mathematical%20description%20of%20what%20effects%20are%0Aacceptable%29%2C%20and%20a%20verifier%20%28which%20provides%20an%20auditable%20proof%20certificate%20that%0Athe%20AI%20satisfies%20the%20safety%20specification%20relative%20to%20the%20world%20model%29.%20We%0Aoutline%20a%20number%20of%20approaches%20for%20creating%20each%20of%20these%20three%20core%0Acomponents%2C%20describe%20the%20main%20technical%20challenges%2C%20and%20suggest%20a%20number%20of%0Apotential%20solutions%20to%20them.%20We%20also%20argue%20for%20the%20necessity%20of%20this%20approach%0Ato%20AI%20safety%2C%20and%20for%20the%20inadequacy%20of%20the%20main%20alternative%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06624v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Guaranteed%2520Safe%2520AI%253A%2520A%2520Framework%2520for%2520Ensuring%2520Robust%2520and%2520Reliable%250A%2520%2520AI%2520Systems%26entry.906535625%3DDavid%2520%2522davidad%2522%2520Dalrymple%2520and%2520Joar%2520Skalse%2520and%2520Yoshua%2520Bengio%2520and%2520Stuart%2520Russell%2520and%2520Max%2520Tegmark%2520and%2520Sanjit%2520Seshia%2520and%2520Steve%2520Omohundro%2520and%2520Christian%2520Szegedy%2520and%2520Ben%2520Goldhaber%2520and%2520Nora%2520Ammann%2520and%2520Alessandro%2520Abate%2520and%2520Joe%2520Halpern%2520and%2520Clark%2520Barrett%2520and%2520Ding%2520Zhao%2520and%2520Tan%2520Zhi-Xuan%2520and%2520Jeannette%2520Wing%2520and%2520Joshua%2520Tenenbaum%26entry.1292438233%3D%2520%2520Ensuring%2520that%2520AI%2520systems%2520reliably%2520and%2520robustly%2520avoid%2520harmful%2520or%2520dangerous%250Abehaviours%2520is%2520a%2520crucial%2520challenge%252C%2520especially%2520for%2520AI%2520systems%2520with%2520a%2520high%2520degree%250Aof%2520autonomy%2520and%2520general%2520intelligence%252C%2520or%2520systems%2520used%2520in%2520safety-critical%250Acontexts.%2520In%2520this%2520paper%252C%2520we%2520will%2520introduce%2520and%2520define%2520a%2520family%2520of%2520approaches%2520to%250AAI%2520safety%252C%2520which%2520we%2520will%2520refer%2520to%2520as%2520guaranteed%2520safe%2520%2528GS%2529%2520AI.%2520The%2520core%2520feature%250Aof%2520these%2520approaches%2520is%2520that%2520they%2520aim%2520to%2520produce%2520AI%2520systems%2520which%2520are%2520equipped%250Awith%2520high-assurance%2520quantitative%2520safety%2520guarantees.%2520This%2520is%2520achieved%2520by%2520the%250Ainterplay%2520of%2520three%2520core%2520components%253A%2520a%2520world%2520model%2520%2528which%2520provides%2520a%250Amathematical%2520description%2520of%2520how%2520the%2520AI%2520system%2520affects%2520the%2520outside%2520world%2529%252C%2520a%250Asafety%2520specification%2520%2528which%2520is%2520a%2520mathematical%2520description%2520of%2520what%2520effects%2520are%250Aacceptable%2529%252C%2520and%2520a%2520verifier%2520%2528which%2520provides%2520an%2520auditable%2520proof%2520certificate%2520that%250Athe%2520AI%2520satisfies%2520the%2520safety%2520specification%2520relative%2520to%2520the%2520world%2520model%2529.%2520We%250Aoutline%2520a%2520number%2520of%2520approaches%2520for%2520creating%2520each%2520of%2520these%2520three%2520core%250Acomponents%252C%2520describe%2520the%2520main%2520technical%2520challenges%252C%2520and%2520suggest%2520a%2520number%2520of%250Apotential%2520solutions%2520to%2520them.%2520We%2520also%2520argue%2520for%2520the%2520necessity%2520of%2520this%2520approach%250Ato%2520AI%2520safety%252C%2520and%2520for%2520the%2520inadequacy%2520of%2520the%2520main%2520alternative%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06624v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Guaranteed%20Safe%20AI%3A%20A%20Framework%20for%20Ensuring%20Robust%20and%20Reliable%0A%20%20AI%20Systems&entry.906535625=David%20%22davidad%22%20Dalrymple%20and%20Joar%20Skalse%20and%20Yoshua%20Bengio%20and%20Stuart%20Russell%20and%20Max%20Tegmark%20and%20Sanjit%20Seshia%20and%20Steve%20Omohundro%20and%20Christian%20Szegedy%20and%20Ben%20Goldhaber%20and%20Nora%20Ammann%20and%20Alessandro%20Abate%20and%20Joe%20Halpern%20and%20Clark%20Barrett%20and%20Ding%20Zhao%20and%20Tan%20Zhi-Xuan%20and%20Jeannette%20Wing%20and%20Joshua%20Tenenbaum&entry.1292438233=%20%20Ensuring%20that%20AI%20systems%20reliably%20and%20robustly%20avoid%20harmful%20or%20dangerous%0Abehaviours%20is%20a%20crucial%20challenge%2C%20especially%20for%20AI%20systems%20with%20a%20high%20degree%0Aof%20autonomy%20and%20general%20intelligence%2C%20or%20systems%20used%20in%20safety-critical%0Acontexts.%20In%20this%20paper%2C%20we%20will%20introduce%20and%20define%20a%20family%20of%20approaches%20to%0AAI%20safety%2C%20which%20we%20will%20refer%20to%20as%20guaranteed%20safe%20%28GS%29%20AI.%20The%20core%20feature%0Aof%20these%20approaches%20is%20that%20they%20aim%20to%20produce%20AI%20systems%20which%20are%20equipped%0Awith%20high-assurance%20quantitative%20safety%20guarantees.%20This%20is%20achieved%20by%20the%0Ainterplay%20of%20three%20core%20components%3A%20a%20world%20model%20%28which%20provides%20a%0Amathematical%20description%20of%20how%20the%20AI%20system%20affects%20the%20outside%20world%29%2C%20a%0Asafety%20specification%20%28which%20is%20a%20mathematical%20description%20of%20what%20effects%20are%0Aacceptable%29%2C%20and%20a%20verifier%20%28which%20provides%20an%20auditable%20proof%20certificate%20that%0Athe%20AI%20satisfies%20the%20safety%20specification%20relative%20to%20the%20world%20model%29.%20We%0Aoutline%20a%20number%20of%20approaches%20for%20creating%20each%20of%20these%20three%20core%0Acomponents%2C%20describe%20the%20main%20technical%20challenges%2C%20and%20suggest%20a%20number%20of%0Apotential%20solutions%20to%20them.%20We%20also%20argue%20for%20the%20necessity%20of%20this%20approach%0Ato%20AI%20safety%2C%20and%20for%20the%20inadequacy%20of%20the%20main%20alternative%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06624v3&entry.124074799=Read"},
{"title": "Qualitative Event Perception: Leveraging Spatiotemporal Episodic Memory\n  for Learning Combat in a Strategy Game", "author": "Will Hancock and Kenneth D. Forbus", "abstract": "  Event perception refers to people's ability to carve up continuous experience\ninto meaningful discrete events. We speak of finishing our morning coffee,\nmowing the lawn, leaving work, etc. as singular occurrences that are localized\nin time and space. In this work, we analyze how spatiotemporal representations\ncan be used to automatically segment continuous experience into structured\nepisodes, and how these descriptions can be used for analogical learning. These\nrepresentations are based on Hayes' notion of histories and build upon existing\nwork on qualitative episodic memory. Our agent automatically generates event\ndescriptions of military battles in a strategy game and improves its gameplay\nby learning from this experience. Episodes are segmented based on changing\nproperties in the world and we show evidence that they facilitate learning\nbecause they capture event descriptions at a useful spatiotemporal grain size.\nThis is evaluated through our agent's performance in the game. We also show\nempirical evidence that the perception of spatial extent of episodes affects\nboth their temporal duration as well as the number of overall cases generated.\n", "link": "http://arxiv.org/abs/2407.06088v1", "date": "2024-07-08", "relevancy": 1.4495, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5223}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5065}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qualitative%20Event%20Perception%3A%20Leveraging%20Spatiotemporal%20Episodic%20Memory%0A%20%20for%20Learning%20Combat%20in%20a%20Strategy%20Game&body=Title%3A%20Qualitative%20Event%20Perception%3A%20Leveraging%20Spatiotemporal%20Episodic%20Memory%0A%20%20for%20Learning%20Combat%20in%20a%20Strategy%20Game%0AAuthor%3A%20Will%20Hancock%20and%20Kenneth%20D.%20Forbus%0AAbstract%3A%20%20%20Event%20perception%20refers%20to%20people%27s%20ability%20to%20carve%20up%20continuous%20experience%0Ainto%20meaningful%20discrete%20events.%20We%20speak%20of%20finishing%20our%20morning%20coffee%2C%0Amowing%20the%20lawn%2C%20leaving%20work%2C%20etc.%20as%20singular%20occurrences%20that%20are%20localized%0Ain%20time%20and%20space.%20In%20this%20work%2C%20we%20analyze%20how%20spatiotemporal%20representations%0Acan%20be%20used%20to%20automatically%20segment%20continuous%20experience%20into%20structured%0Aepisodes%2C%20and%20how%20these%20descriptions%20can%20be%20used%20for%20analogical%20learning.%20These%0Arepresentations%20are%20based%20on%20Hayes%27%20notion%20of%20histories%20and%20build%20upon%20existing%0Awork%20on%20qualitative%20episodic%20memory.%20Our%20agent%20automatically%20generates%20event%0Adescriptions%20of%20military%20battles%20in%20a%20strategy%20game%20and%20improves%20its%20gameplay%0Aby%20learning%20from%20this%20experience.%20Episodes%20are%20segmented%20based%20on%20changing%0Aproperties%20in%20the%20world%20and%20we%20show%20evidence%20that%20they%20facilitate%20learning%0Abecause%20they%20capture%20event%20descriptions%20at%20a%20useful%20spatiotemporal%20grain%20size.%0AThis%20is%20evaluated%20through%20our%20agent%27s%20performance%20in%20the%20game.%20We%20also%20show%0Aempirical%20evidence%20that%20the%20perception%20of%20spatial%20extent%20of%20episodes%20affects%0Aboth%20their%20temporal%20duration%20as%20well%20as%20the%20number%20of%20overall%20cases%20generated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQualitative%2520Event%2520Perception%253A%2520Leveraging%2520Spatiotemporal%2520Episodic%2520Memory%250A%2520%2520for%2520Learning%2520Combat%2520in%2520a%2520Strategy%2520Game%26entry.906535625%3DWill%2520Hancock%2520and%2520Kenneth%2520D.%2520Forbus%26entry.1292438233%3D%2520%2520Event%2520perception%2520refers%2520to%2520people%2527s%2520ability%2520to%2520carve%2520up%2520continuous%2520experience%250Ainto%2520meaningful%2520discrete%2520events.%2520We%2520speak%2520of%2520finishing%2520our%2520morning%2520coffee%252C%250Amowing%2520the%2520lawn%252C%2520leaving%2520work%252C%2520etc.%2520as%2520singular%2520occurrences%2520that%2520are%2520localized%250Ain%2520time%2520and%2520space.%2520In%2520this%2520work%252C%2520we%2520analyze%2520how%2520spatiotemporal%2520representations%250Acan%2520be%2520used%2520to%2520automatically%2520segment%2520continuous%2520experience%2520into%2520structured%250Aepisodes%252C%2520and%2520how%2520these%2520descriptions%2520can%2520be%2520used%2520for%2520analogical%2520learning.%2520These%250Arepresentations%2520are%2520based%2520on%2520Hayes%2527%2520notion%2520of%2520histories%2520and%2520build%2520upon%2520existing%250Awork%2520on%2520qualitative%2520episodic%2520memory.%2520Our%2520agent%2520automatically%2520generates%2520event%250Adescriptions%2520of%2520military%2520battles%2520in%2520a%2520strategy%2520game%2520and%2520improves%2520its%2520gameplay%250Aby%2520learning%2520from%2520this%2520experience.%2520Episodes%2520are%2520segmented%2520based%2520on%2520changing%250Aproperties%2520in%2520the%2520world%2520and%2520we%2520show%2520evidence%2520that%2520they%2520facilitate%2520learning%250Abecause%2520they%2520capture%2520event%2520descriptions%2520at%2520a%2520useful%2520spatiotemporal%2520grain%2520size.%250AThis%2520is%2520evaluated%2520through%2520our%2520agent%2527s%2520performance%2520in%2520the%2520game.%2520We%2520also%2520show%250Aempirical%2520evidence%2520that%2520the%2520perception%2520of%2520spatial%2520extent%2520of%2520episodes%2520affects%250Aboth%2520their%2520temporal%2520duration%2520as%2520well%2520as%2520the%2520number%2520of%2520overall%2520cases%2520generated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qualitative%20Event%20Perception%3A%20Leveraging%20Spatiotemporal%20Episodic%20Memory%0A%20%20for%20Learning%20Combat%20in%20a%20Strategy%20Game&entry.906535625=Will%20Hancock%20and%20Kenneth%20D.%20Forbus&entry.1292438233=%20%20Event%20perception%20refers%20to%20people%27s%20ability%20to%20carve%20up%20continuous%20experience%0Ainto%20meaningful%20discrete%20events.%20We%20speak%20of%20finishing%20our%20morning%20coffee%2C%0Amowing%20the%20lawn%2C%20leaving%20work%2C%20etc.%20as%20singular%20occurrences%20that%20are%20localized%0Ain%20time%20and%20space.%20In%20this%20work%2C%20we%20analyze%20how%20spatiotemporal%20representations%0Acan%20be%20used%20to%20automatically%20segment%20continuous%20experience%20into%20structured%0Aepisodes%2C%20and%20how%20these%20descriptions%20can%20be%20used%20for%20analogical%20learning.%20These%0Arepresentations%20are%20based%20on%20Hayes%27%20notion%20of%20histories%20and%20build%20upon%20existing%0Awork%20on%20qualitative%20episodic%20memory.%20Our%20agent%20automatically%20generates%20event%0Adescriptions%20of%20military%20battles%20in%20a%20strategy%20game%20and%20improves%20its%20gameplay%0Aby%20learning%20from%20this%20experience.%20Episodes%20are%20segmented%20based%20on%20changing%0Aproperties%20in%20the%20world%20and%20we%20show%20evidence%20that%20they%20facilitate%20learning%0Abecause%20they%20capture%20event%20descriptions%20at%20a%20useful%20spatiotemporal%20grain%20size.%0AThis%20is%20evaluated%20through%20our%20agent%27s%20performance%20in%20the%20game.%20We%20also%20show%0Aempirical%20evidence%20that%20the%20perception%20of%20spatial%20extent%20of%20episodes%20affects%0Aboth%20their%20temporal%20duration%20as%20well%20as%20the%20number%20of%20overall%20cases%20generated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06088v1&entry.124074799=Read"},
{"title": "Stepping on the Edge: Curvature Aware Learning Rate Tuners", "author": "Vincent Roulet and Atish Agarwala and Jean-Bastien Grill and Grzegorz Swirszcz and Mathieu Blondel and Fabian Pedregosa", "abstract": "  Curvature information -- particularly, the largest eigenvalue of the loss\nHessian, known as the sharpness -- often forms the basis for learning rate\ntuners. However, recent work has shown that the curvature information undergoes\ncomplex dynamics during training, going from a phase of increasing sharpness to\neventual stabilization. We analyze the closed-loop feedback effect between\nlearning rate tuning and curvature. We find that classical learning rate tuners\nmay yield greater one-step loss reduction, yet they ultimately underperform in\nthe long term when compared to constant learning rates in the full batch\nregime. These models break the stabilization of the sharpness, which we explain\nusing a simplified model of the joint dynamics of the learning rate and the\ncurvature. To further investigate these effects, we introduce a new learning\nrate tuning method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes\nlong term curvature stabilization over instantaneous progress on the objective.\nIn the full batch regime, CDAT shows behavior akin to prefixed warm-up\nschedules on deep learning objectives, outperforming tuned constant learning\nrates. In the mini batch regime, we observe that stochasticity introduces\nconfounding effects that explain the previous success of some learning rate\ntuners at appropriate batch sizes. Our findings highlight the critical role of\nunderstanding the joint dynamics of the learning rate and curvature, beyond\ngreedy minimization, to diagnose failures and design effective adaptive\nlearning rate tuners.\n", "link": "http://arxiv.org/abs/2407.06183v1", "date": "2024-07-08", "relevancy": 1.9579, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4972}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.494}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stepping%20on%20the%20Edge%3A%20Curvature%20Aware%20Learning%20Rate%20Tuners&body=Title%3A%20Stepping%20on%20the%20Edge%3A%20Curvature%20Aware%20Learning%20Rate%20Tuners%0AAuthor%3A%20Vincent%20Roulet%20and%20Atish%20Agarwala%20and%20Jean-Bastien%20Grill%20and%20Grzegorz%20Swirszcz%20and%20Mathieu%20Blondel%20and%20Fabian%20Pedregosa%0AAbstract%3A%20%20%20Curvature%20information%20--%20particularly%2C%20the%20largest%20eigenvalue%20of%20the%20loss%0AHessian%2C%20known%20as%20the%20sharpness%20--%20often%20forms%20the%20basis%20for%20learning%20rate%0Atuners.%20However%2C%20recent%20work%20has%20shown%20that%20the%20curvature%20information%20undergoes%0Acomplex%20dynamics%20during%20training%2C%20going%20from%20a%20phase%20of%20increasing%20sharpness%20to%0Aeventual%20stabilization.%20We%20analyze%20the%20closed-loop%20feedback%20effect%20between%0Alearning%20rate%20tuning%20and%20curvature.%20We%20find%20that%20classical%20learning%20rate%20tuners%0Amay%20yield%20greater%20one-step%20loss%20reduction%2C%20yet%20they%20ultimately%20underperform%20in%0Athe%20long%20term%20when%20compared%20to%20constant%20learning%20rates%20in%20the%20full%20batch%0Aregime.%20These%20models%20break%20the%20stabilization%20of%20the%20sharpness%2C%20which%20we%20explain%0Ausing%20a%20simplified%20model%20of%20the%20joint%20dynamics%20of%20the%20learning%20rate%20and%20the%0Acurvature.%20To%20further%20investigate%20these%20effects%2C%20we%20introduce%20a%20new%20learning%0Arate%20tuning%20method%2C%20Curvature%20Dynamics%20Aware%20Tuning%20%28CDAT%29%2C%20which%20prioritizes%0Along%20term%20curvature%20stabilization%20over%20instantaneous%20progress%20on%20the%20objective.%0AIn%20the%20full%20batch%20regime%2C%20CDAT%20shows%20behavior%20akin%20to%20prefixed%20warm-up%0Aschedules%20on%20deep%20learning%20objectives%2C%20outperforming%20tuned%20constant%20learning%0Arates.%20In%20the%20mini%20batch%20regime%2C%20we%20observe%20that%20stochasticity%20introduces%0Aconfounding%20effects%20that%20explain%20the%20previous%20success%20of%20some%20learning%20rate%0Atuners%20at%20appropriate%20batch%20sizes.%20Our%20findings%20highlight%20the%20critical%20role%20of%0Aunderstanding%20the%20joint%20dynamics%20of%20the%20learning%20rate%20and%20curvature%2C%20beyond%0Agreedy%20minimization%2C%20to%20diagnose%20failures%20and%20design%20effective%20adaptive%0Alearning%20rate%20tuners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepping%2520on%2520the%2520Edge%253A%2520Curvature%2520Aware%2520Learning%2520Rate%2520Tuners%26entry.906535625%3DVincent%2520Roulet%2520and%2520Atish%2520Agarwala%2520and%2520Jean-Bastien%2520Grill%2520and%2520Grzegorz%2520Swirszcz%2520and%2520Mathieu%2520Blondel%2520and%2520Fabian%2520Pedregosa%26entry.1292438233%3D%2520%2520Curvature%2520information%2520--%2520particularly%252C%2520the%2520largest%2520eigenvalue%2520of%2520the%2520loss%250AHessian%252C%2520known%2520as%2520the%2520sharpness%2520--%2520often%2520forms%2520the%2520basis%2520for%2520learning%2520rate%250Atuners.%2520However%252C%2520recent%2520work%2520has%2520shown%2520that%2520the%2520curvature%2520information%2520undergoes%250Acomplex%2520dynamics%2520during%2520training%252C%2520going%2520from%2520a%2520phase%2520of%2520increasing%2520sharpness%2520to%250Aeventual%2520stabilization.%2520We%2520analyze%2520the%2520closed-loop%2520feedback%2520effect%2520between%250Alearning%2520rate%2520tuning%2520and%2520curvature.%2520We%2520find%2520that%2520classical%2520learning%2520rate%2520tuners%250Amay%2520yield%2520greater%2520one-step%2520loss%2520reduction%252C%2520yet%2520they%2520ultimately%2520underperform%2520in%250Athe%2520long%2520term%2520when%2520compared%2520to%2520constant%2520learning%2520rates%2520in%2520the%2520full%2520batch%250Aregime.%2520These%2520models%2520break%2520the%2520stabilization%2520of%2520the%2520sharpness%252C%2520which%2520we%2520explain%250Ausing%2520a%2520simplified%2520model%2520of%2520the%2520joint%2520dynamics%2520of%2520the%2520learning%2520rate%2520and%2520the%250Acurvature.%2520To%2520further%2520investigate%2520these%2520effects%252C%2520we%2520introduce%2520a%2520new%2520learning%250Arate%2520tuning%2520method%252C%2520Curvature%2520Dynamics%2520Aware%2520Tuning%2520%2528CDAT%2529%252C%2520which%2520prioritizes%250Along%2520term%2520curvature%2520stabilization%2520over%2520instantaneous%2520progress%2520on%2520the%2520objective.%250AIn%2520the%2520full%2520batch%2520regime%252C%2520CDAT%2520shows%2520behavior%2520akin%2520to%2520prefixed%2520warm-up%250Aschedules%2520on%2520deep%2520learning%2520objectives%252C%2520outperforming%2520tuned%2520constant%2520learning%250Arates.%2520In%2520the%2520mini%2520batch%2520regime%252C%2520we%2520observe%2520that%2520stochasticity%2520introduces%250Aconfounding%2520effects%2520that%2520explain%2520the%2520previous%2520success%2520of%2520some%2520learning%2520rate%250Atuners%2520at%2520appropriate%2520batch%2520sizes.%2520Our%2520findings%2520highlight%2520the%2520critical%2520role%2520of%250Aunderstanding%2520the%2520joint%2520dynamics%2520of%2520the%2520learning%2520rate%2520and%2520curvature%252C%2520beyond%250Agreedy%2520minimization%252C%2520to%2520diagnose%2520failures%2520and%2520design%2520effective%2520adaptive%250Alearning%2520rate%2520tuners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stepping%20on%20the%20Edge%3A%20Curvature%20Aware%20Learning%20Rate%20Tuners&entry.906535625=Vincent%20Roulet%20and%20Atish%20Agarwala%20and%20Jean-Bastien%20Grill%20and%20Grzegorz%20Swirszcz%20and%20Mathieu%20Blondel%20and%20Fabian%20Pedregosa&entry.1292438233=%20%20Curvature%20information%20--%20particularly%2C%20the%20largest%20eigenvalue%20of%20the%20loss%0AHessian%2C%20known%20as%20the%20sharpness%20--%20often%20forms%20the%20basis%20for%20learning%20rate%0Atuners.%20However%2C%20recent%20work%20has%20shown%20that%20the%20curvature%20information%20undergoes%0Acomplex%20dynamics%20during%20training%2C%20going%20from%20a%20phase%20of%20increasing%20sharpness%20to%0Aeventual%20stabilization.%20We%20analyze%20the%20closed-loop%20feedback%20effect%20between%0Alearning%20rate%20tuning%20and%20curvature.%20We%20find%20that%20classical%20learning%20rate%20tuners%0Amay%20yield%20greater%20one-step%20loss%20reduction%2C%20yet%20they%20ultimately%20underperform%20in%0Athe%20long%20term%20when%20compared%20to%20constant%20learning%20rates%20in%20the%20full%20batch%0Aregime.%20These%20models%20break%20the%20stabilization%20of%20the%20sharpness%2C%20which%20we%20explain%0Ausing%20a%20simplified%20model%20of%20the%20joint%20dynamics%20of%20the%20learning%20rate%20and%20the%0Acurvature.%20To%20further%20investigate%20these%20effects%2C%20we%20introduce%20a%20new%20learning%0Arate%20tuning%20method%2C%20Curvature%20Dynamics%20Aware%20Tuning%20%28CDAT%29%2C%20which%20prioritizes%0Along%20term%20curvature%20stabilization%20over%20instantaneous%20progress%20on%20the%20objective.%0AIn%20the%20full%20batch%20regime%2C%20CDAT%20shows%20behavior%20akin%20to%20prefixed%20warm-up%0Aschedules%20on%20deep%20learning%20objectives%2C%20outperforming%20tuned%20constant%20learning%0Arates.%20In%20the%20mini%20batch%20regime%2C%20we%20observe%20that%20stochasticity%20introduces%0Aconfounding%20effects%20that%20explain%20the%20previous%20success%20of%20some%20learning%20rate%0Atuners%20at%20appropriate%20batch%20sizes.%20Our%20findings%20highlight%20the%20critical%20role%20of%0Aunderstanding%20the%20joint%20dynamics%20of%20the%20learning%20rate%20and%20curvature%2C%20beyond%0Agreedy%20minimization%2C%20to%20diagnose%20failures%20and%20design%20effective%20adaptive%0Alearning%20rate%20tuners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06183v1&entry.124074799=Read"},
{"title": "Unified theory for joint covariance properties under geometric image\n  transformations for spatio-temporal receptive fields according to the\n  generalized Gaussian derivative model for visual receptive fields", "author": "Tony Lindeberg", "abstract": "  The influence of natural image transformations on receptive field responses\nis crucial for modelling visual operations in computer vision and biological\nvision. In this regard, covariance properties with respect to geometric image\ntransformations in the earliest layers of the visual hierarchy are essential\nfor expressing robust image operations, and for formulating invariant visual\noperations at higher levels.\n  This paper defines and proves a set of joint covariance properties for\nspatio-temporal receptive fields in terms of spatio-temporal derivative\noperators applied to spatio-temporally smoothed image data under compositions\nof spatial scaling transformations, spatial affine transformations, Galilean\ntransformations and temporal scaling transformations. Specifically, the derived\nrelations show how the parameters of the receptive fields need to be\ntransformed, in order to match the output from spatio-temporal receptive fields\nunder composed spatio-temporal image transformations.\n  For this purpose, we also fundamentally extend the notion of scale-normalized\nderivatives to affine-normalized derivatives, that are computed based on\nspatial smoothing with affine Gaussian kernels, and analyze the covariance\nproperties of the resulting affine-normalized derivatives for the affine group\nas well as for important subgroups thereof.\n  We conclude with a geometric analysis, showing how the derived joint\ncovariance properties make it possible to relate or match spatio-temporal\nreceptive field responses, when observing, possibly moving, local surface\npatches from different views, under locally linearized perspective or\nprojective transformations, as well as when observing different instances of\nspatio-temporal events, that may occur either faster or slower between\ndifferent views of similar spatio-temporal events.\n", "link": "http://arxiv.org/abs/2311.10543v6", "date": "2024-07-08", "relevancy": 1.5542, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5262}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.517}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20theory%20for%20joint%20covariance%20properties%20under%20geometric%20image%0A%20%20transformations%20for%20spatio-temporal%20receptive%20fields%20according%20to%20the%0A%20%20generalized%20Gaussian%20derivative%20model%20for%20visual%20receptive%20fields&body=Title%3A%20Unified%20theory%20for%20joint%20covariance%20properties%20under%20geometric%20image%0A%20%20transformations%20for%20spatio-temporal%20receptive%20fields%20according%20to%20the%0A%20%20generalized%20Gaussian%20derivative%20model%20for%20visual%20receptive%20fields%0AAuthor%3A%20Tony%20Lindeberg%0AAbstract%3A%20%20%20The%20influence%20of%20natural%20image%20transformations%20on%20receptive%20field%20responses%0Ais%20crucial%20for%20modelling%20visual%20operations%20in%20computer%20vision%20and%20biological%0Avision.%20In%20this%20regard%2C%20covariance%20properties%20with%20respect%20to%20geometric%20image%0Atransformations%20in%20the%20earliest%20layers%20of%20the%20visual%20hierarchy%20are%20essential%0Afor%20expressing%20robust%20image%20operations%2C%20and%20for%20formulating%20invariant%20visual%0Aoperations%20at%20higher%20levels.%0A%20%20This%20paper%20defines%20and%20proves%20a%20set%20of%20joint%20covariance%20properties%20for%0Aspatio-temporal%20receptive%20fields%20in%20terms%20of%20spatio-temporal%20derivative%0Aoperators%20applied%20to%20spatio-temporally%20smoothed%20image%20data%20under%20compositions%0Aof%20spatial%20scaling%20transformations%2C%20spatial%20affine%20transformations%2C%20Galilean%0Atransformations%20and%20temporal%20scaling%20transformations.%20Specifically%2C%20the%20derived%0Arelations%20show%20how%20the%20parameters%20of%20the%20receptive%20fields%20need%20to%20be%0Atransformed%2C%20in%20order%20to%20match%20the%20output%20from%20spatio-temporal%20receptive%20fields%0Aunder%20composed%20spatio-temporal%20image%20transformations.%0A%20%20For%20this%20purpose%2C%20we%20also%20fundamentally%20extend%20the%20notion%20of%20scale-normalized%0Aderivatives%20to%20affine-normalized%20derivatives%2C%20that%20are%20computed%20based%20on%0Aspatial%20smoothing%20with%20affine%20Gaussian%20kernels%2C%20and%20analyze%20the%20covariance%0Aproperties%20of%20the%20resulting%20affine-normalized%20derivatives%20for%20the%20affine%20group%0Aas%20well%20as%20for%20important%20subgroups%20thereof.%0A%20%20We%20conclude%20with%20a%20geometric%20analysis%2C%20showing%20how%20the%20derived%20joint%0Acovariance%20properties%20make%20it%20possible%20to%20relate%20or%20match%20spatio-temporal%0Areceptive%20field%20responses%2C%20when%20observing%2C%20possibly%20moving%2C%20local%20surface%0Apatches%20from%20different%20views%2C%20under%20locally%20linearized%20perspective%20or%0Aprojective%20transformations%2C%20as%20well%20as%20when%20observing%20different%20instances%20of%0Aspatio-temporal%20events%2C%20that%20may%20occur%20either%20faster%20or%20slower%20between%0Adifferent%20views%20of%20similar%20spatio-temporal%20events.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10543v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520theory%2520for%2520joint%2520covariance%2520properties%2520under%2520geometric%2520image%250A%2520%2520transformations%2520for%2520spatio-temporal%2520receptive%2520fields%2520according%2520to%2520the%250A%2520%2520generalized%2520Gaussian%2520derivative%2520model%2520for%2520visual%2520receptive%2520fields%26entry.906535625%3DTony%2520Lindeberg%26entry.1292438233%3D%2520%2520The%2520influence%2520of%2520natural%2520image%2520transformations%2520on%2520receptive%2520field%2520responses%250Ais%2520crucial%2520for%2520modelling%2520visual%2520operations%2520in%2520computer%2520vision%2520and%2520biological%250Avision.%2520In%2520this%2520regard%252C%2520covariance%2520properties%2520with%2520respect%2520to%2520geometric%2520image%250Atransformations%2520in%2520the%2520earliest%2520layers%2520of%2520the%2520visual%2520hierarchy%2520are%2520essential%250Afor%2520expressing%2520robust%2520image%2520operations%252C%2520and%2520for%2520formulating%2520invariant%2520visual%250Aoperations%2520at%2520higher%2520levels.%250A%2520%2520This%2520paper%2520defines%2520and%2520proves%2520a%2520set%2520of%2520joint%2520covariance%2520properties%2520for%250Aspatio-temporal%2520receptive%2520fields%2520in%2520terms%2520of%2520spatio-temporal%2520derivative%250Aoperators%2520applied%2520to%2520spatio-temporally%2520smoothed%2520image%2520data%2520under%2520compositions%250Aof%2520spatial%2520scaling%2520transformations%252C%2520spatial%2520affine%2520transformations%252C%2520Galilean%250Atransformations%2520and%2520temporal%2520scaling%2520transformations.%2520Specifically%252C%2520the%2520derived%250Arelations%2520show%2520how%2520the%2520parameters%2520of%2520the%2520receptive%2520fields%2520need%2520to%2520be%250Atransformed%252C%2520in%2520order%2520to%2520match%2520the%2520output%2520from%2520spatio-temporal%2520receptive%2520fields%250Aunder%2520composed%2520spatio-temporal%2520image%2520transformations.%250A%2520%2520For%2520this%2520purpose%252C%2520we%2520also%2520fundamentally%2520extend%2520the%2520notion%2520of%2520scale-normalized%250Aderivatives%2520to%2520affine-normalized%2520derivatives%252C%2520that%2520are%2520computed%2520based%2520on%250Aspatial%2520smoothing%2520with%2520affine%2520Gaussian%2520kernels%252C%2520and%2520analyze%2520the%2520covariance%250Aproperties%2520of%2520the%2520resulting%2520affine-normalized%2520derivatives%2520for%2520the%2520affine%2520group%250Aas%2520well%2520as%2520for%2520important%2520subgroups%2520thereof.%250A%2520%2520We%2520conclude%2520with%2520a%2520geometric%2520analysis%252C%2520showing%2520how%2520the%2520derived%2520joint%250Acovariance%2520properties%2520make%2520it%2520possible%2520to%2520relate%2520or%2520match%2520spatio-temporal%250Areceptive%2520field%2520responses%252C%2520when%2520observing%252C%2520possibly%2520moving%252C%2520local%2520surface%250Apatches%2520from%2520different%2520views%252C%2520under%2520locally%2520linearized%2520perspective%2520or%250Aprojective%2520transformations%252C%2520as%2520well%2520as%2520when%2520observing%2520different%2520instances%2520of%250Aspatio-temporal%2520events%252C%2520that%2520may%2520occur%2520either%2520faster%2520or%2520slower%2520between%250Adifferent%2520views%2520of%2520similar%2520spatio-temporal%2520events.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10543v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20theory%20for%20joint%20covariance%20properties%20under%20geometric%20image%0A%20%20transformations%20for%20spatio-temporal%20receptive%20fields%20according%20to%20the%0A%20%20generalized%20Gaussian%20derivative%20model%20for%20visual%20receptive%20fields&entry.906535625=Tony%20Lindeberg&entry.1292438233=%20%20The%20influence%20of%20natural%20image%20transformations%20on%20receptive%20field%20responses%0Ais%20crucial%20for%20modelling%20visual%20operations%20in%20computer%20vision%20and%20biological%0Avision.%20In%20this%20regard%2C%20covariance%20properties%20with%20respect%20to%20geometric%20image%0Atransformations%20in%20the%20earliest%20layers%20of%20the%20visual%20hierarchy%20are%20essential%0Afor%20expressing%20robust%20image%20operations%2C%20and%20for%20formulating%20invariant%20visual%0Aoperations%20at%20higher%20levels.%0A%20%20This%20paper%20defines%20and%20proves%20a%20set%20of%20joint%20covariance%20properties%20for%0Aspatio-temporal%20receptive%20fields%20in%20terms%20of%20spatio-temporal%20derivative%0Aoperators%20applied%20to%20spatio-temporally%20smoothed%20image%20data%20under%20compositions%0Aof%20spatial%20scaling%20transformations%2C%20spatial%20affine%20transformations%2C%20Galilean%0Atransformations%20and%20temporal%20scaling%20transformations.%20Specifically%2C%20the%20derived%0Arelations%20show%20how%20the%20parameters%20of%20the%20receptive%20fields%20need%20to%20be%0Atransformed%2C%20in%20order%20to%20match%20the%20output%20from%20spatio-temporal%20receptive%20fields%0Aunder%20composed%20spatio-temporal%20image%20transformations.%0A%20%20For%20this%20purpose%2C%20we%20also%20fundamentally%20extend%20the%20notion%20of%20scale-normalized%0Aderivatives%20to%20affine-normalized%20derivatives%2C%20that%20are%20computed%20based%20on%0Aspatial%20smoothing%20with%20affine%20Gaussian%20kernels%2C%20and%20analyze%20the%20covariance%0Aproperties%20of%20the%20resulting%20affine-normalized%20derivatives%20for%20the%20affine%20group%0Aas%20well%20as%20for%20important%20subgroups%20thereof.%0A%20%20We%20conclude%20with%20a%20geometric%20analysis%2C%20showing%20how%20the%20derived%20joint%0Acovariance%20properties%20make%20it%20possible%20to%20relate%20or%20match%20spatio-temporal%0Areceptive%20field%20responses%2C%20when%20observing%2C%20possibly%20moving%2C%20local%20surface%0Apatches%20from%20different%20views%2C%20under%20locally%20linearized%20perspective%20or%0Aprojective%20transformations%2C%20as%20well%20as%20when%20observing%20different%20instances%20of%0Aspatio-temporal%20events%2C%20that%20may%20occur%20either%20faster%20or%20slower%20between%0Adifferent%20views%20of%20similar%20spatio-temporal%20events.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10543v6&entry.124074799=Read"},
{"title": "An Empirical Comparison of Vocabulary Expansion and Initialization\n  Approaches for Language Models", "author": "Nandini Mundra and Aditya Nanda Kishore and Raj Dabre and Ratish Puduppully and Anoop Kunchukuttan and Mitesh M. Khapra", "abstract": "  Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods.\n", "link": "http://arxiv.org/abs/2407.05841v1", "date": "2024-07-08", "relevancy": 1.8586, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4921}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4494}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Comparison%20of%20Vocabulary%20Expansion%20and%20Initialization%0A%20%20Approaches%20for%20Language%20Models&body=Title%3A%20An%20Empirical%20Comparison%20of%20Vocabulary%20Expansion%20and%20Initialization%0A%20%20Approaches%20for%20Language%20Models%0AAuthor%3A%20Nandini%20Mundra%20and%20Aditya%20Nanda%20Kishore%20and%20Raj%20Dabre%20and%20Ratish%20Puduppully%20and%20Anoop%20Kunchukuttan%20and%20Mitesh%20M.%20Khapra%0AAbstract%3A%20%20%20Language%20Models%20%28LMs%29%20excel%20in%20natural%20language%20processing%20tasks%20for%20English%0Abut%20show%20reduced%20performance%20in%20most%20other%20languages.%20This%20problem%20is%20commonly%0Atackled%20by%20continually%20pre-training%20and%20fine-tuning%20these%20models%20for%20said%0Alanguages.%20A%20significant%20issue%20in%20this%20process%20is%20the%20limited%20vocabulary%0Acoverage%20in%20the%20original%20model%27s%20tokenizer%2C%20leading%20to%20inadequate%0Arepresentation%20of%20new%20languages%20and%20necessitating%20an%20expansion%20of%20the%0Atokenizer.%20The%20initialization%20of%20the%20embeddings%20corresponding%20to%20new%20vocabulary%0Aitems%20presents%20a%20further%20challenge.%20Current%20strategies%20require%20cross-lingual%0Aembeddings%20and%20lack%20a%20solid%20theoretical%20foundation%20as%20well%20as%20comparisons%20with%0Astrong%20baselines.%20In%20this%20paper%2C%20we%20first%20establish%20theoretically%20that%0Ainitializing%20within%20the%20convex%20hull%20of%20existing%20embeddings%20is%20a%20good%0Ainitialization%2C%20followed%20by%20a%20novel%20but%20simple%20approach%2C%20Constrained%20Word2Vec%0A%28CW2V%29%2C%20which%20does%20not%20require%20cross-lingual%20embeddings.%20Our%20study%20evaluates%0Adifferent%20initialization%20methods%20for%20expanding%20RoBERTa%20and%20LLaMA%202%20across%20four%0Alanguages%20and%20five%20tasks.%20The%20results%20show%20that%20CW2V%20performs%20equally%20well%20or%0Aeven%20better%20than%20more%20advanced%20techniques.%20Additionally%2C%20simpler%20approaches%0Alike%20multivariate%20initialization%20perform%20on%20par%20with%20these%20advanced%20methods%0Aindicating%20that%20efficient%20large-scale%20multilingual%20continued%20pretraining%20can%20be%0Aachieved%20even%20with%20simpler%20initialization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Comparison%2520of%2520Vocabulary%2520Expansion%2520and%2520Initialization%250A%2520%2520Approaches%2520for%2520Language%2520Models%26entry.906535625%3DNandini%2520Mundra%2520and%2520Aditya%2520Nanda%2520Kishore%2520and%2520Raj%2520Dabre%2520and%2520Ratish%2520Puduppully%2520and%2520Anoop%2520Kunchukuttan%2520and%2520Mitesh%2520M.%2520Khapra%26entry.1292438233%3D%2520%2520Language%2520Models%2520%2528LMs%2529%2520excel%2520in%2520natural%2520language%2520processing%2520tasks%2520for%2520English%250Abut%2520show%2520reduced%2520performance%2520in%2520most%2520other%2520languages.%2520This%2520problem%2520is%2520commonly%250Atackled%2520by%2520continually%2520pre-training%2520and%2520fine-tuning%2520these%2520models%2520for%2520said%250Alanguages.%2520A%2520significant%2520issue%2520in%2520this%2520process%2520is%2520the%2520limited%2520vocabulary%250Acoverage%2520in%2520the%2520original%2520model%2527s%2520tokenizer%252C%2520leading%2520to%2520inadequate%250Arepresentation%2520of%2520new%2520languages%2520and%2520necessitating%2520an%2520expansion%2520of%2520the%250Atokenizer.%2520The%2520initialization%2520of%2520the%2520embeddings%2520corresponding%2520to%2520new%2520vocabulary%250Aitems%2520presents%2520a%2520further%2520challenge.%2520Current%2520strategies%2520require%2520cross-lingual%250Aembeddings%2520and%2520lack%2520a%2520solid%2520theoretical%2520foundation%2520as%2520well%2520as%2520comparisons%2520with%250Astrong%2520baselines.%2520In%2520this%2520paper%252C%2520we%2520first%2520establish%2520theoretically%2520that%250Ainitializing%2520within%2520the%2520convex%2520hull%2520of%2520existing%2520embeddings%2520is%2520a%2520good%250Ainitialization%252C%2520followed%2520by%2520a%2520novel%2520but%2520simple%2520approach%252C%2520Constrained%2520Word2Vec%250A%2528CW2V%2529%252C%2520which%2520does%2520not%2520require%2520cross-lingual%2520embeddings.%2520Our%2520study%2520evaluates%250Adifferent%2520initialization%2520methods%2520for%2520expanding%2520RoBERTa%2520and%2520LLaMA%25202%2520across%2520four%250Alanguages%2520and%2520five%2520tasks.%2520The%2520results%2520show%2520that%2520CW2V%2520performs%2520equally%2520well%2520or%250Aeven%2520better%2520than%2520more%2520advanced%2520techniques.%2520Additionally%252C%2520simpler%2520approaches%250Alike%2520multivariate%2520initialization%2520perform%2520on%2520par%2520with%2520these%2520advanced%2520methods%250Aindicating%2520that%2520efficient%2520large-scale%2520multilingual%2520continued%2520pretraining%2520can%2520be%250Aachieved%2520even%2520with%2520simpler%2520initialization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Comparison%20of%20Vocabulary%20Expansion%20and%20Initialization%0A%20%20Approaches%20for%20Language%20Models&entry.906535625=Nandini%20Mundra%20and%20Aditya%20Nanda%20Kishore%20and%20Raj%20Dabre%20and%20Ratish%20Puduppully%20and%20Anoop%20Kunchukuttan%20and%20Mitesh%20M.%20Khapra&entry.1292438233=%20%20Language%20Models%20%28LMs%29%20excel%20in%20natural%20language%20processing%20tasks%20for%20English%0Abut%20show%20reduced%20performance%20in%20most%20other%20languages.%20This%20problem%20is%20commonly%0Atackled%20by%20continually%20pre-training%20and%20fine-tuning%20these%20models%20for%20said%0Alanguages.%20A%20significant%20issue%20in%20this%20process%20is%20the%20limited%20vocabulary%0Acoverage%20in%20the%20original%20model%27s%20tokenizer%2C%20leading%20to%20inadequate%0Arepresentation%20of%20new%20languages%20and%20necessitating%20an%20expansion%20of%20the%0Atokenizer.%20The%20initialization%20of%20the%20embeddings%20corresponding%20to%20new%20vocabulary%0Aitems%20presents%20a%20further%20challenge.%20Current%20strategies%20require%20cross-lingual%0Aembeddings%20and%20lack%20a%20solid%20theoretical%20foundation%20as%20well%20as%20comparisons%20with%0Astrong%20baselines.%20In%20this%20paper%2C%20we%20first%20establish%20theoretically%20that%0Ainitializing%20within%20the%20convex%20hull%20of%20existing%20embeddings%20is%20a%20good%0Ainitialization%2C%20followed%20by%20a%20novel%20but%20simple%20approach%2C%20Constrained%20Word2Vec%0A%28CW2V%29%2C%20which%20does%20not%20require%20cross-lingual%20embeddings.%20Our%20study%20evaluates%0Adifferent%20initialization%20methods%20for%20expanding%20RoBERTa%20and%20LLaMA%202%20across%20four%0Alanguages%20and%20five%20tasks.%20The%20results%20show%20that%20CW2V%20performs%20equally%20well%20or%0Aeven%20better%20than%20more%20advanced%20techniques.%20Additionally%2C%20simpler%20approaches%0Alike%20multivariate%20initialization%20perform%20on%20par%20with%20these%20advanced%20methods%0Aindicating%20that%20efficient%20large-scale%20multilingual%20continued%20pretraining%20can%20be%0Aachieved%20even%20with%20simpler%20initialization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05841v1&entry.124074799=Read"},
{"title": "Exploring Human-LLM Conversations: Mental Models and the Originator of\n  Toxicity", "author": "Johannes Schneider and Arianna Casanova Flores and Anne-Catherine Kranz", "abstract": "  This study explores real-world human interactions with large language models\n(LLMs) in diverse, unconstrained settings in contrast to most prior research\nfocusing on ethically trimmed models like ChatGPT for specific tasks. We aim to\nunderstand the originator of toxicity. Our findings show that although LLMs are\nrightfully accused of providing toxic content, it is mostly demanded or at\nleast provoked by humans who actively seek such content. Our manual analysis of\nhundreds of conversations judged as toxic by APIs commercial vendors, also\nraises questions with respect to current practices of what user requests are\nrefused to answer. Furthermore, we conjecture based on multiple empirical\nindicators that humans exhibit a change of their mental model, switching from\nthe mindset of interacting with a machine more towards interacting with a\nhuman.\n", "link": "http://arxiv.org/abs/2407.05977v1", "date": "2024-07-08", "relevancy": 1.2876, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4336}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4283}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Human-LLM%20Conversations%3A%20Mental%20Models%20and%20the%20Originator%20of%0A%20%20Toxicity&body=Title%3A%20Exploring%20Human-LLM%20Conversations%3A%20Mental%20Models%20and%20the%20Originator%20of%0A%20%20Toxicity%0AAuthor%3A%20Johannes%20Schneider%20and%20Arianna%20Casanova%20Flores%20and%20Anne-Catherine%20Kranz%0AAbstract%3A%20%20%20This%20study%20explores%20real-world%20human%20interactions%20with%20large%20language%20models%0A%28LLMs%29%20in%20diverse%2C%20unconstrained%20settings%20in%20contrast%20to%20most%20prior%20research%0Afocusing%20on%20ethically%20trimmed%20models%20like%20ChatGPT%20for%20specific%20tasks.%20We%20aim%20to%0Aunderstand%20the%20originator%20of%20toxicity.%20Our%20findings%20show%20that%20although%20LLMs%20are%0Arightfully%20accused%20of%20providing%20toxic%20content%2C%20it%20is%20mostly%20demanded%20or%20at%0Aleast%20provoked%20by%20humans%20who%20actively%20seek%20such%20content.%20Our%20manual%20analysis%20of%0Ahundreds%20of%20conversations%20judged%20as%20toxic%20by%20APIs%20commercial%20vendors%2C%20also%0Araises%20questions%20with%20respect%20to%20current%20practices%20of%20what%20user%20requests%20are%0Arefused%20to%20answer.%20Furthermore%2C%20we%20conjecture%20based%20on%20multiple%20empirical%0Aindicators%20that%20humans%20exhibit%20a%20change%20of%20their%20mental%20model%2C%20switching%20from%0Athe%20mindset%20of%20interacting%20with%20a%20machine%20more%20towards%20interacting%20with%20a%0Ahuman.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Human-LLM%2520Conversations%253A%2520Mental%2520Models%2520and%2520the%2520Originator%2520of%250A%2520%2520Toxicity%26entry.906535625%3DJohannes%2520Schneider%2520and%2520Arianna%2520Casanova%2520Flores%2520and%2520Anne-Catherine%2520Kranz%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520real-world%2520human%2520interactions%2520with%2520large%2520language%2520models%250A%2528LLMs%2529%2520in%2520diverse%252C%2520unconstrained%2520settings%2520in%2520contrast%2520to%2520most%2520prior%2520research%250Afocusing%2520on%2520ethically%2520trimmed%2520models%2520like%2520ChatGPT%2520for%2520specific%2520tasks.%2520We%2520aim%2520to%250Aunderstand%2520the%2520originator%2520of%2520toxicity.%2520Our%2520findings%2520show%2520that%2520although%2520LLMs%2520are%250Arightfully%2520accused%2520of%2520providing%2520toxic%2520content%252C%2520it%2520is%2520mostly%2520demanded%2520or%2520at%250Aleast%2520provoked%2520by%2520humans%2520who%2520actively%2520seek%2520such%2520content.%2520Our%2520manual%2520analysis%2520of%250Ahundreds%2520of%2520conversations%2520judged%2520as%2520toxic%2520by%2520APIs%2520commercial%2520vendors%252C%2520also%250Araises%2520questions%2520with%2520respect%2520to%2520current%2520practices%2520of%2520what%2520user%2520requests%2520are%250Arefused%2520to%2520answer.%2520Furthermore%252C%2520we%2520conjecture%2520based%2520on%2520multiple%2520empirical%250Aindicators%2520that%2520humans%2520exhibit%2520a%2520change%2520of%2520their%2520mental%2520model%252C%2520switching%2520from%250Athe%2520mindset%2520of%2520interacting%2520with%2520a%2520machine%2520more%2520towards%2520interacting%2520with%2520a%250Ahuman.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Human-LLM%20Conversations%3A%20Mental%20Models%20and%20the%20Originator%20of%0A%20%20Toxicity&entry.906535625=Johannes%20Schneider%20and%20Arianna%20Casanova%20Flores%20and%20Anne-Catherine%20Kranz&entry.1292438233=%20%20This%20study%20explores%20real-world%20human%20interactions%20with%20large%20language%20models%0A%28LLMs%29%20in%20diverse%2C%20unconstrained%20settings%20in%20contrast%20to%20most%20prior%20research%0Afocusing%20on%20ethically%20trimmed%20models%20like%20ChatGPT%20for%20specific%20tasks.%20We%20aim%20to%0Aunderstand%20the%20originator%20of%20toxicity.%20Our%20findings%20show%20that%20although%20LLMs%20are%0Arightfully%20accused%20of%20providing%20toxic%20content%2C%20it%20is%20mostly%20demanded%20or%20at%0Aleast%20provoked%20by%20humans%20who%20actively%20seek%20such%20content.%20Our%20manual%20analysis%20of%0Ahundreds%20of%20conversations%20judged%20as%20toxic%20by%20APIs%20commercial%20vendors%2C%20also%0Araises%20questions%20with%20respect%20to%20current%20practices%20of%20what%20user%20requests%20are%0Arefused%20to%20answer.%20Furthermore%2C%20we%20conjecture%20based%20on%20multiple%20empirical%0Aindicators%20that%20humans%20exhibit%20a%20change%20of%20their%20mental%20model%2C%20switching%20from%0Athe%20mindset%20of%20interacting%20with%20a%20machine%20more%20towards%20interacting%20with%20a%0Ahuman.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05977v1&entry.124074799=Read"},
{"title": "Research on Autonomous Robots Navigation based on Reinforcement Learning", "author": "Zixiang Wang and Hao Yan and Yining Wang and Zhengjia Xu and Zhuoyue Wang and Zhizhong Wu", "abstract": "  Reinforcement learning continuously optimizes decision-making based on\nreal-time feedback reward signals through continuous interaction with the\nenvironment, demonstrating strong adaptive and self-learning capabilities. In\nrecent years, it has become one of the key methods to achieve autonomous\nnavigation of robots. In this work, an autonomous robot navigation method based\non reinforcement learning is introduced. We use the Deep Q Network (DQN) and\nProximal Policy Optimization (PPO) models to optimize the path planning and\ndecision-making process through the continuous interaction between the robot\nand the environment, and the reward signals with real-time feedback. By\ncombining the Q-value function with the deep neural network, deep Q network can\nhandle high-dimensional state space, so as to realize path planning in complex\nenvironments. Proximal policy optimization is a strategy gradient-based method,\nwhich enables robots to explore and utilize environmental information more\nefficiently by optimizing policy functions. These methods not only improve the\nrobot's navigation ability in the unknown environment, but also enhance its\nadaptive and self-learning capabilities. Through multiple training and\nsimulation experiments, we have verified the effectiveness and robustness of\nthese models in various complex scenarios.\n", "link": "http://arxiv.org/abs/2407.02539v2", "date": "2024-07-08", "relevancy": 1.6071, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5425}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5281}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20Autonomous%20Robots%20Navigation%20based%20on%20Reinforcement%20Learning&body=Title%3A%20Research%20on%20Autonomous%20Robots%20Navigation%20based%20on%20Reinforcement%20Learning%0AAuthor%3A%20Zixiang%20Wang%20and%20Hao%20Yan%20and%20Yining%20Wang%20and%20Zhengjia%20Xu%20and%20Zhuoyue%20Wang%20and%20Zhizhong%20Wu%0AAbstract%3A%20%20%20Reinforcement%20learning%20continuously%20optimizes%20decision-making%20based%20on%0Areal-time%20feedback%20reward%20signals%20through%20continuous%20interaction%20with%20the%0Aenvironment%2C%20demonstrating%20strong%20adaptive%20and%20self-learning%20capabilities.%20In%0Arecent%20years%2C%20it%20has%20become%20one%20of%20the%20key%20methods%20to%20achieve%20autonomous%0Anavigation%20of%20robots.%20In%20this%20work%2C%20an%20autonomous%20robot%20navigation%20method%20based%0Aon%20reinforcement%20learning%20is%20introduced.%20We%20use%20the%20Deep%20Q%20Network%20%28DQN%29%20and%0AProximal%20Policy%20Optimization%20%28PPO%29%20models%20to%20optimize%20the%20path%20planning%20and%0Adecision-making%20process%20through%20the%20continuous%20interaction%20between%20the%20robot%0Aand%20the%20environment%2C%20and%20the%20reward%20signals%20with%20real-time%20feedback.%20By%0Acombining%20the%20Q-value%20function%20with%20the%20deep%20neural%20network%2C%20deep%20Q%20network%20can%0Ahandle%20high-dimensional%20state%20space%2C%20so%20as%20to%20realize%20path%20planning%20in%20complex%0Aenvironments.%20Proximal%20policy%20optimization%20is%20a%20strategy%20gradient-based%20method%2C%0Awhich%20enables%20robots%20to%20explore%20and%20utilize%20environmental%20information%20more%0Aefficiently%20by%20optimizing%20policy%20functions.%20These%20methods%20not%20only%20improve%20the%0Arobot%27s%20navigation%20ability%20in%20the%20unknown%20environment%2C%20but%20also%20enhance%20its%0Aadaptive%20and%20self-learning%20capabilities.%20Through%20multiple%20training%20and%0Asimulation%20experiments%2C%20we%20have%20verified%20the%20effectiveness%20and%20robustness%20of%0Athese%20models%20in%20various%20complex%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520Autonomous%2520Robots%2520Navigation%2520based%2520on%2520Reinforcement%2520Learning%26entry.906535625%3DZixiang%2520Wang%2520and%2520Hao%2520Yan%2520and%2520Yining%2520Wang%2520and%2520Zhengjia%2520Xu%2520and%2520Zhuoyue%2520Wang%2520and%2520Zhizhong%2520Wu%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520continuously%2520optimizes%2520decision-making%2520based%2520on%250Areal-time%2520feedback%2520reward%2520signals%2520through%2520continuous%2520interaction%2520with%2520the%250Aenvironment%252C%2520demonstrating%2520strong%2520adaptive%2520and%2520self-learning%2520capabilities.%2520In%250Arecent%2520years%252C%2520it%2520has%2520become%2520one%2520of%2520the%2520key%2520methods%2520to%2520achieve%2520autonomous%250Anavigation%2520of%2520robots.%2520In%2520this%2520work%252C%2520an%2520autonomous%2520robot%2520navigation%2520method%2520based%250Aon%2520reinforcement%2520learning%2520is%2520introduced.%2520We%2520use%2520the%2520Deep%2520Q%2520Network%2520%2528DQN%2529%2520and%250AProximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520models%2520to%2520optimize%2520the%2520path%2520planning%2520and%250Adecision-making%2520process%2520through%2520the%2520continuous%2520interaction%2520between%2520the%2520robot%250Aand%2520the%2520environment%252C%2520and%2520the%2520reward%2520signals%2520with%2520real-time%2520feedback.%2520By%250Acombining%2520the%2520Q-value%2520function%2520with%2520the%2520deep%2520neural%2520network%252C%2520deep%2520Q%2520network%2520can%250Ahandle%2520high-dimensional%2520state%2520space%252C%2520so%2520as%2520to%2520realize%2520path%2520planning%2520in%2520complex%250Aenvironments.%2520Proximal%2520policy%2520optimization%2520is%2520a%2520strategy%2520gradient-based%2520method%252C%250Awhich%2520enables%2520robots%2520to%2520explore%2520and%2520utilize%2520environmental%2520information%2520more%250Aefficiently%2520by%2520optimizing%2520policy%2520functions.%2520These%2520methods%2520not%2520only%2520improve%2520the%250Arobot%2527s%2520navigation%2520ability%2520in%2520the%2520unknown%2520environment%252C%2520but%2520also%2520enhance%2520its%250Aadaptive%2520and%2520self-learning%2520capabilities.%2520Through%2520multiple%2520training%2520and%250Asimulation%2520experiments%252C%2520we%2520have%2520verified%2520the%2520effectiveness%2520and%2520robustness%2520of%250Athese%2520models%2520in%2520various%2520complex%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20Autonomous%20Robots%20Navigation%20based%20on%20Reinforcement%20Learning&entry.906535625=Zixiang%20Wang%20and%20Hao%20Yan%20and%20Yining%20Wang%20and%20Zhengjia%20Xu%20and%20Zhuoyue%20Wang%20and%20Zhizhong%20Wu&entry.1292438233=%20%20Reinforcement%20learning%20continuously%20optimizes%20decision-making%20based%20on%0Areal-time%20feedback%20reward%20signals%20through%20continuous%20interaction%20with%20the%0Aenvironment%2C%20demonstrating%20strong%20adaptive%20and%20self-learning%20capabilities.%20In%0Arecent%20years%2C%20it%20has%20become%20one%20of%20the%20key%20methods%20to%20achieve%20autonomous%0Anavigation%20of%20robots.%20In%20this%20work%2C%20an%20autonomous%20robot%20navigation%20method%20based%0Aon%20reinforcement%20learning%20is%20introduced.%20We%20use%20the%20Deep%20Q%20Network%20%28DQN%29%20and%0AProximal%20Policy%20Optimization%20%28PPO%29%20models%20to%20optimize%20the%20path%20planning%20and%0Adecision-making%20process%20through%20the%20continuous%20interaction%20between%20the%20robot%0Aand%20the%20environment%2C%20and%20the%20reward%20signals%20with%20real-time%20feedback.%20By%0Acombining%20the%20Q-value%20function%20with%20the%20deep%20neural%20network%2C%20deep%20Q%20network%20can%0Ahandle%20high-dimensional%20state%20space%2C%20so%20as%20to%20realize%20path%20planning%20in%20complex%0Aenvironments.%20Proximal%20policy%20optimization%20is%20a%20strategy%20gradient-based%20method%2C%0Awhich%20enables%20robots%20to%20explore%20and%20utilize%20environmental%20information%20more%0Aefficiently%20by%20optimizing%20policy%20functions.%20These%20methods%20not%20only%20improve%20the%0Arobot%27s%20navigation%20ability%20in%20the%20unknown%20environment%2C%20but%20also%20enhance%20its%0Aadaptive%20and%20self-learning%20capabilities.%20Through%20multiple%20training%20and%0Asimulation%20experiments%2C%20we%20have%20verified%20the%20effectiveness%20and%20robustness%20of%0Athese%20models%20in%20various%20complex%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02539v2&entry.124074799=Read"},
{"title": "The Disagreement Problem in Explainable Machine Learning: A\n  Practitioner's Perspective", "author": "Satyapriya Krishna and Tessa Han and Alex Gu and Steven Wu and Shahin Jabbari and Himabindu Lakkaraju", "abstract": "  As various post hoc explanation methods are increasingly being leveraged to\nexplain complex models in high-stakes settings, it becomes critical to develop\na deeper understanding of if and when the explanations output by these methods\ndisagree with each other, and how such disagreements are resolved in practice.\nHowever, there is little to no research that provides answers to these critical\nquestions. In this work, we introduce and study the disagreement problem in\nexplainable machine learning. More specifically, we formalize the notion of\ndisagreement between explanations, analyze how often such disagreements occur\nin practice, and how practitioners resolve these disagreements. We first\nconduct interviews with data scientists to understand what constitutes\ndisagreement between explanations generated by different methods for the same\nmodel prediction and introduce a novel quantitative framework to formalize this\nunderstanding. We then leverage this framework to carry out a rigorous\nempirical analysis with four real-world datasets, six state-of-the-art post hoc\nexplanation methods, and six different predictive models, to measure the extent\nof disagreement between the explanations generated by various popular\nexplanation methods. In addition, we carry out an online user study with data\nscientists to understand how they resolve the aforementioned disagreements. Our\nresults indicate that (1) state-of-the-art explanation methods often disagree\nin terms of the explanations they output, and (2) machine learning\npractitioners often employ ad hoc heuristics when resolving such disagreements.\nThese findings suggest that practitioners may be relying on misleading\nexplanations when making consequential decisions. They also underscore the\nimportance of developing principled frameworks for effectively evaluating and\ncomparing explanations output by various explanation techniques.\n", "link": "http://arxiv.org/abs/2202.01602v4", "date": "2024-07-08", "relevancy": 1.4328, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5134}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4808}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Disagreement%20Problem%20in%20Explainable%20Machine%20Learning%3A%20A%0A%20%20Practitioner%27s%20Perspective&body=Title%3A%20The%20Disagreement%20Problem%20in%20Explainable%20Machine%20Learning%3A%20A%0A%20%20Practitioner%27s%20Perspective%0AAuthor%3A%20Satyapriya%20Krishna%20and%20Tessa%20Han%20and%20Alex%20Gu%20and%20Steven%20Wu%20and%20Shahin%20Jabbari%20and%20Himabindu%20Lakkaraju%0AAbstract%3A%20%20%20As%20various%20post%20hoc%20explanation%20methods%20are%20increasingly%20being%20leveraged%20to%0Aexplain%20complex%20models%20in%20high-stakes%20settings%2C%20it%20becomes%20critical%20to%20develop%0Aa%20deeper%20understanding%20of%20if%20and%20when%20the%20explanations%20output%20by%20these%20methods%0Adisagree%20with%20each%20other%2C%20and%20how%20such%20disagreements%20are%20resolved%20in%20practice.%0AHowever%2C%20there%20is%20little%20to%20no%20research%20that%20provides%20answers%20to%20these%20critical%0Aquestions.%20In%20this%20work%2C%20we%20introduce%20and%20study%20the%20disagreement%20problem%20in%0Aexplainable%20machine%20learning.%20More%20specifically%2C%20we%20formalize%20the%20notion%20of%0Adisagreement%20between%20explanations%2C%20analyze%20how%20often%20such%20disagreements%20occur%0Ain%20practice%2C%20and%20how%20practitioners%20resolve%20these%20disagreements.%20We%20first%0Aconduct%20interviews%20with%20data%20scientists%20to%20understand%20what%20constitutes%0Adisagreement%20between%20explanations%20generated%20by%20different%20methods%20for%20the%20same%0Amodel%20prediction%20and%20introduce%20a%20novel%20quantitative%20framework%20to%20formalize%20this%0Aunderstanding.%20We%20then%20leverage%20this%20framework%20to%20carry%20out%20a%20rigorous%0Aempirical%20analysis%20with%20four%20real-world%20datasets%2C%20six%20state-of-the-art%20post%20hoc%0Aexplanation%20methods%2C%20and%20six%20different%20predictive%20models%2C%20to%20measure%20the%20extent%0Aof%20disagreement%20between%20the%20explanations%20generated%20by%20various%20popular%0Aexplanation%20methods.%20In%20addition%2C%20we%20carry%20out%20an%20online%20user%20study%20with%20data%0Ascientists%20to%20understand%20how%20they%20resolve%20the%20aforementioned%20disagreements.%20Our%0Aresults%20indicate%20that%20%281%29%20state-of-the-art%20explanation%20methods%20often%20disagree%0Ain%20terms%20of%20the%20explanations%20they%20output%2C%20and%20%282%29%20machine%20learning%0Apractitioners%20often%20employ%20ad%20hoc%20heuristics%20when%20resolving%20such%20disagreements.%0AThese%20findings%20suggest%20that%20practitioners%20may%20be%20relying%20on%20misleading%0Aexplanations%20when%20making%20consequential%20decisions.%20They%20also%20underscore%20the%0Aimportance%20of%20developing%20principled%20frameworks%20for%20effectively%20evaluating%20and%0Acomparing%20explanations%20output%20by%20various%20explanation%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.01602v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Disagreement%2520Problem%2520in%2520Explainable%2520Machine%2520Learning%253A%2520A%250A%2520%2520Practitioner%2527s%2520Perspective%26entry.906535625%3DSatyapriya%2520Krishna%2520and%2520Tessa%2520Han%2520and%2520Alex%2520Gu%2520and%2520Steven%2520Wu%2520and%2520Shahin%2520Jabbari%2520and%2520Himabindu%2520Lakkaraju%26entry.1292438233%3D%2520%2520As%2520various%2520post%2520hoc%2520explanation%2520methods%2520are%2520increasingly%2520being%2520leveraged%2520to%250Aexplain%2520complex%2520models%2520in%2520high-stakes%2520settings%252C%2520it%2520becomes%2520critical%2520to%2520develop%250Aa%2520deeper%2520understanding%2520of%2520if%2520and%2520when%2520the%2520explanations%2520output%2520by%2520these%2520methods%250Adisagree%2520with%2520each%2520other%252C%2520and%2520how%2520such%2520disagreements%2520are%2520resolved%2520in%2520practice.%250AHowever%252C%2520there%2520is%2520little%2520to%2520no%2520research%2520that%2520provides%2520answers%2520to%2520these%2520critical%250Aquestions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520and%2520study%2520the%2520disagreement%2520problem%2520in%250Aexplainable%2520machine%2520learning.%2520More%2520specifically%252C%2520we%2520formalize%2520the%2520notion%2520of%250Adisagreement%2520between%2520explanations%252C%2520analyze%2520how%2520often%2520such%2520disagreements%2520occur%250Ain%2520practice%252C%2520and%2520how%2520practitioners%2520resolve%2520these%2520disagreements.%2520We%2520first%250Aconduct%2520interviews%2520with%2520data%2520scientists%2520to%2520understand%2520what%2520constitutes%250Adisagreement%2520between%2520explanations%2520generated%2520by%2520different%2520methods%2520for%2520the%2520same%250Amodel%2520prediction%2520and%2520introduce%2520a%2520novel%2520quantitative%2520framework%2520to%2520formalize%2520this%250Aunderstanding.%2520We%2520then%2520leverage%2520this%2520framework%2520to%2520carry%2520out%2520a%2520rigorous%250Aempirical%2520analysis%2520with%2520four%2520real-world%2520datasets%252C%2520six%2520state-of-the-art%2520post%2520hoc%250Aexplanation%2520methods%252C%2520and%2520six%2520different%2520predictive%2520models%252C%2520to%2520measure%2520the%2520extent%250Aof%2520disagreement%2520between%2520the%2520explanations%2520generated%2520by%2520various%2520popular%250Aexplanation%2520methods.%2520In%2520addition%252C%2520we%2520carry%2520out%2520an%2520online%2520user%2520study%2520with%2520data%250Ascientists%2520to%2520understand%2520how%2520they%2520resolve%2520the%2520aforementioned%2520disagreements.%2520Our%250Aresults%2520indicate%2520that%2520%25281%2529%2520state-of-the-art%2520explanation%2520methods%2520often%2520disagree%250Ain%2520terms%2520of%2520the%2520explanations%2520they%2520output%252C%2520and%2520%25282%2529%2520machine%2520learning%250Apractitioners%2520often%2520employ%2520ad%2520hoc%2520heuristics%2520when%2520resolving%2520such%2520disagreements.%250AThese%2520findings%2520suggest%2520that%2520practitioners%2520may%2520be%2520relying%2520on%2520misleading%250Aexplanations%2520when%2520making%2520consequential%2520decisions.%2520They%2520also%2520underscore%2520the%250Aimportance%2520of%2520developing%2520principled%2520frameworks%2520for%2520effectively%2520evaluating%2520and%250Acomparing%2520explanations%2520output%2520by%2520various%2520explanation%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.01602v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Disagreement%20Problem%20in%20Explainable%20Machine%20Learning%3A%20A%0A%20%20Practitioner%27s%20Perspective&entry.906535625=Satyapriya%20Krishna%20and%20Tessa%20Han%20and%20Alex%20Gu%20and%20Steven%20Wu%20and%20Shahin%20Jabbari%20and%20Himabindu%20Lakkaraju&entry.1292438233=%20%20As%20various%20post%20hoc%20explanation%20methods%20are%20increasingly%20being%20leveraged%20to%0Aexplain%20complex%20models%20in%20high-stakes%20settings%2C%20it%20becomes%20critical%20to%20develop%0Aa%20deeper%20understanding%20of%20if%20and%20when%20the%20explanations%20output%20by%20these%20methods%0Adisagree%20with%20each%20other%2C%20and%20how%20such%20disagreements%20are%20resolved%20in%20practice.%0AHowever%2C%20there%20is%20little%20to%20no%20research%20that%20provides%20answers%20to%20these%20critical%0Aquestions.%20In%20this%20work%2C%20we%20introduce%20and%20study%20the%20disagreement%20problem%20in%0Aexplainable%20machine%20learning.%20More%20specifically%2C%20we%20formalize%20the%20notion%20of%0Adisagreement%20between%20explanations%2C%20analyze%20how%20often%20such%20disagreements%20occur%0Ain%20practice%2C%20and%20how%20practitioners%20resolve%20these%20disagreements.%20We%20first%0Aconduct%20interviews%20with%20data%20scientists%20to%20understand%20what%20constitutes%0Adisagreement%20between%20explanations%20generated%20by%20different%20methods%20for%20the%20same%0Amodel%20prediction%20and%20introduce%20a%20novel%20quantitative%20framework%20to%20formalize%20this%0Aunderstanding.%20We%20then%20leverage%20this%20framework%20to%20carry%20out%20a%20rigorous%0Aempirical%20analysis%20with%20four%20real-world%20datasets%2C%20six%20state-of-the-art%20post%20hoc%0Aexplanation%20methods%2C%20and%20six%20different%20predictive%20models%2C%20to%20measure%20the%20extent%0Aof%20disagreement%20between%20the%20explanations%20generated%20by%20various%20popular%0Aexplanation%20methods.%20In%20addition%2C%20we%20carry%20out%20an%20online%20user%20study%20with%20data%0Ascientists%20to%20understand%20how%20they%20resolve%20the%20aforementioned%20disagreements.%20Our%0Aresults%20indicate%20that%20%281%29%20state-of-the-art%20explanation%20methods%20often%20disagree%0Ain%20terms%20of%20the%20explanations%20they%20output%2C%20and%20%282%29%20machine%20learning%0Apractitioners%20often%20employ%20ad%20hoc%20heuristics%20when%20resolving%20such%20disagreements.%0AThese%20findings%20suggest%20that%20practitioners%20may%20be%20relying%20on%20misleading%0Aexplanations%20when%20making%20consequential%20decisions.%20They%20also%20underscore%20the%0Aimportance%20of%20developing%20principled%20frameworks%20for%20effectively%20evaluating%20and%0Acomparing%20explanations%20output%20by%20various%20explanation%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.01602v4&entry.124074799=Read"},
{"title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation", "author": "Abhishek Divekar and Greg Durrett", "abstract": "  It is often desirable to distill the capabilities of large language models\n(LLMs) into smaller student models due to compute and memory constraints. One\nway to do this for classification tasks is via dataset synthesis, which can be\naccomplished by generating examples of each label from the LLM. Prior\napproaches to synthesis use few-shot prompting, which relies on the LLM's\nparametric knowledge to generate usable examples. However, this leads to issues\nof repetition, bias towards popular entities, and stylistic differences from\nhuman text. In this work, we propose Synthesize by Retrieval and Refinement\n(SynthesizRR), which uses retrieval augmentation to introduce variety into the\ndataset synthesis process: as retrieved passages vary, the LLM is seeded with\ndifferent content to generate its examples. We empirically study the synthesis\nof six datasets, covering topic classification, sentiment analysis, tone\ndetection, and humor, requiring complex synthesis strategies. We find that\nSynthesizRR greatly improves lexical and semantic diversity, similarity to\nhuman-written text, and distillation performance, when compared to 32-shot\nprompting and four prior approaches. We release our extensive codebase at\nhttps://github.com/amazon-science/synthesizrr\n", "link": "http://arxiv.org/abs/2405.10040v2", "date": "2024-07-08", "relevancy": 1.9746, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5022}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4936}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation&body=Title%3A%20SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation%0AAuthor%3A%20Abhishek%20Divekar%20and%20Greg%20Durrett%0AAbstract%3A%20%20%20It%20is%20often%20desirable%20to%20distill%20the%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20into%20smaller%20student%20models%20due%20to%20compute%20and%20memory%20constraints.%20One%0Away%20to%20do%20this%20for%20classification%20tasks%20is%20via%20dataset%20synthesis%2C%20which%20can%20be%0Aaccomplished%20by%20generating%20examples%20of%20each%20label%20from%20the%20LLM.%20Prior%0Aapproaches%20to%20synthesis%20use%20few-shot%20prompting%2C%20which%20relies%20on%20the%20LLM%27s%0Aparametric%20knowledge%20to%20generate%20usable%20examples.%20However%2C%20this%20leads%20to%20issues%0Aof%20repetition%2C%20bias%20towards%20popular%20entities%2C%20and%20stylistic%20differences%20from%0Ahuman%20text.%20In%20this%20work%2C%20we%20propose%20Synthesize%20by%20Retrieval%20and%20Refinement%0A%28SynthesizRR%29%2C%20which%20uses%20retrieval%20augmentation%20to%20introduce%20variety%20into%20the%0Adataset%20synthesis%20process%3A%20as%20retrieved%20passages%20vary%2C%20the%20LLM%20is%20seeded%20with%0Adifferent%20content%20to%20generate%20its%20examples.%20We%20empirically%20study%20the%20synthesis%0Aof%20six%20datasets%2C%20covering%20topic%20classification%2C%20sentiment%20analysis%2C%20tone%0Adetection%2C%20and%20humor%2C%20requiring%20complex%20synthesis%20strategies.%20We%20find%20that%0ASynthesizRR%20greatly%20improves%20lexical%20and%20semantic%20diversity%2C%20similarity%20to%0Ahuman-written%20text%2C%20and%20distillation%20performance%2C%20when%20compared%20to%2032-shot%0Aprompting%20and%20four%20prior%20approaches.%20We%20release%20our%20extensive%20codebase%20at%0Ahttps%3A//github.com/amazon-science/synthesizrr%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizRR%253A%2520Generating%2520Diverse%2520Datasets%2520with%2520Retrieval%2520Augmentation%26entry.906535625%3DAbhishek%2520Divekar%2520and%2520Greg%2520Durrett%26entry.1292438233%3D%2520%2520It%2520is%2520often%2520desirable%2520to%2520distill%2520the%2520capabilities%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520into%2520smaller%2520student%2520models%2520due%2520to%2520compute%2520and%2520memory%2520constraints.%2520One%250Away%2520to%2520do%2520this%2520for%2520classification%2520tasks%2520is%2520via%2520dataset%2520synthesis%252C%2520which%2520can%2520be%250Aaccomplished%2520by%2520generating%2520examples%2520of%2520each%2520label%2520from%2520the%2520LLM.%2520Prior%250Aapproaches%2520to%2520synthesis%2520use%2520few-shot%2520prompting%252C%2520which%2520relies%2520on%2520the%2520LLM%2527s%250Aparametric%2520knowledge%2520to%2520generate%2520usable%2520examples.%2520However%252C%2520this%2520leads%2520to%2520issues%250Aof%2520repetition%252C%2520bias%2520towards%2520popular%2520entities%252C%2520and%2520stylistic%2520differences%2520from%250Ahuman%2520text.%2520In%2520this%2520work%252C%2520we%2520propose%2520Synthesize%2520by%2520Retrieval%2520and%2520Refinement%250A%2528SynthesizRR%2529%252C%2520which%2520uses%2520retrieval%2520augmentation%2520to%2520introduce%2520variety%2520into%2520the%250Adataset%2520synthesis%2520process%253A%2520as%2520retrieved%2520passages%2520vary%252C%2520the%2520LLM%2520is%2520seeded%2520with%250Adifferent%2520content%2520to%2520generate%2520its%2520examples.%2520We%2520empirically%2520study%2520the%2520synthesis%250Aof%2520six%2520datasets%252C%2520covering%2520topic%2520classification%252C%2520sentiment%2520analysis%252C%2520tone%250Adetection%252C%2520and%2520humor%252C%2520requiring%2520complex%2520synthesis%2520strategies.%2520We%2520find%2520that%250ASynthesizRR%2520greatly%2520improves%2520lexical%2520and%2520semantic%2520diversity%252C%2520similarity%2520to%250Ahuman-written%2520text%252C%2520and%2520distillation%2520performance%252C%2520when%2520compared%2520to%252032-shot%250Aprompting%2520and%2520four%2520prior%2520approaches.%2520We%2520release%2520our%2520extensive%2520codebase%2520at%250Ahttps%253A//github.com/amazon-science/synthesizrr%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation&entry.906535625=Abhishek%20Divekar%20and%20Greg%20Durrett&entry.1292438233=%20%20It%20is%20often%20desirable%20to%20distill%20the%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20into%20smaller%20student%20models%20due%20to%20compute%20and%20memory%20constraints.%20One%0Away%20to%20do%20this%20for%20classification%20tasks%20is%20via%20dataset%20synthesis%2C%20which%20can%20be%0Aaccomplished%20by%20generating%20examples%20of%20each%20label%20from%20the%20LLM.%20Prior%0Aapproaches%20to%20synthesis%20use%20few-shot%20prompting%2C%20which%20relies%20on%20the%20LLM%27s%0Aparametric%20knowledge%20to%20generate%20usable%20examples.%20However%2C%20this%20leads%20to%20issues%0Aof%20repetition%2C%20bias%20towards%20popular%20entities%2C%20and%20stylistic%20differences%20from%0Ahuman%20text.%20In%20this%20work%2C%20we%20propose%20Synthesize%20by%20Retrieval%20and%20Refinement%0A%28SynthesizRR%29%2C%20which%20uses%20retrieval%20augmentation%20to%20introduce%20variety%20into%20the%0Adataset%20synthesis%20process%3A%20as%20retrieved%20passages%20vary%2C%20the%20LLM%20is%20seeded%20with%0Adifferent%20content%20to%20generate%20its%20examples.%20We%20empirically%20study%20the%20synthesis%0Aof%20six%20datasets%2C%20covering%20topic%20classification%2C%20sentiment%20analysis%2C%20tone%0Adetection%2C%20and%20humor%2C%20requiring%20complex%20synthesis%20strategies.%20We%20find%20that%0ASynthesizRR%20greatly%20improves%20lexical%20and%20semantic%20diversity%2C%20similarity%20to%0Ahuman-written%20text%2C%20and%20distillation%20performance%2C%20when%20compared%20to%2032-shot%0Aprompting%20and%20four%20prior%20approaches.%20We%20release%20our%20extensive%20codebase%20at%0Ahttps%3A//github.com/amazon-science/synthesizrr%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10040v2&entry.124074799=Read"},
{"title": "Prominent Roles of Conditionally Invariant Components in Domain\n  Adaptation: Theory and Algorithms", "author": "Keru Wu and Yuansi Chen and Wooseok Ha and Bin Yu", "abstract": "  Domain adaptation (DA) is a statistical learning problem that arises when the\ndistribution of the source data used to train a model differs from that of the\ntarget data used to evaluate the model. While many DA algorithms have\ndemonstrated considerable empirical success, blindly applying these algorithms\ncan often lead to worse performance on new datasets. To address this, it is\ncrucial to clarify the assumptions under which a DA algorithm has good target\nperformance. In this work, we focus on the assumption of the presence of\nconditionally invariant components (CICs), which are relevant for prediction\nand remain conditionally invariant across the source and target data. We\ndemonstrate that CICs, which can be estimated through conditional invariant\npenalty (CIP), play three prominent roles in providing target risk guarantees\nin DA. First, we propose a new algorithm based on CICs, importance-weighted\nconditional invariant penalty (IW-CIP), which has target risk guarantees beyond\nsimple settings such as covariate shift and label shift. Second, we show that\nCICs help identify large discrepancies between source and target risks of other\nDA algorithms. Finally, we demonstrate that incorporating CICs into the domain\ninvariant projection (DIP) algorithm can address its failure scenario caused by\nlabel-flipping features. We support our new algorithms and theoretical findings\nvia numerical experiments on synthetic data, MNIST, CelebA, Camelyon17, and\nDomainNet datasets.\n", "link": "http://arxiv.org/abs/2309.10301v2", "date": "2024-07-08", "relevancy": 1.9325, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prominent%20Roles%20of%20Conditionally%20Invariant%20Components%20in%20Domain%0A%20%20Adaptation%3A%20Theory%20and%20Algorithms&body=Title%3A%20Prominent%20Roles%20of%20Conditionally%20Invariant%20Components%20in%20Domain%0A%20%20Adaptation%3A%20Theory%20and%20Algorithms%0AAuthor%3A%20Keru%20Wu%20and%20Yuansi%20Chen%20and%20Wooseok%20Ha%20and%20Bin%20Yu%0AAbstract%3A%20%20%20Domain%20adaptation%20%28DA%29%20is%20a%20statistical%20learning%20problem%20that%20arises%20when%20the%0Adistribution%20of%20the%20source%20data%20used%20to%20train%20a%20model%20differs%20from%20that%20of%20the%0Atarget%20data%20used%20to%20evaluate%20the%20model.%20While%20many%20DA%20algorithms%20have%0Ademonstrated%20considerable%20empirical%20success%2C%20blindly%20applying%20these%20algorithms%0Acan%20often%20lead%20to%20worse%20performance%20on%20new%20datasets.%20To%20address%20this%2C%20it%20is%0Acrucial%20to%20clarify%20the%20assumptions%20under%20which%20a%20DA%20algorithm%20has%20good%20target%0Aperformance.%20In%20this%20work%2C%20we%20focus%20on%20the%20assumption%20of%20the%20presence%20of%0Aconditionally%20invariant%20components%20%28CICs%29%2C%20which%20are%20relevant%20for%20prediction%0Aand%20remain%20conditionally%20invariant%20across%20the%20source%20and%20target%20data.%20We%0Ademonstrate%20that%20CICs%2C%20which%20can%20be%20estimated%20through%20conditional%20invariant%0Apenalty%20%28CIP%29%2C%20play%20three%20prominent%20roles%20in%20providing%20target%20risk%20guarantees%0Ain%20DA.%20First%2C%20we%20propose%20a%20new%20algorithm%20based%20on%20CICs%2C%20importance-weighted%0Aconditional%20invariant%20penalty%20%28IW-CIP%29%2C%20which%20has%20target%20risk%20guarantees%20beyond%0Asimple%20settings%20such%20as%20covariate%20shift%20and%20label%20shift.%20Second%2C%20we%20show%20that%0ACICs%20help%20identify%20large%20discrepancies%20between%20source%20and%20target%20risks%20of%20other%0ADA%20algorithms.%20Finally%2C%20we%20demonstrate%20that%20incorporating%20CICs%20into%20the%20domain%0Ainvariant%20projection%20%28DIP%29%20algorithm%20can%20address%20its%20failure%20scenario%20caused%20by%0Alabel-flipping%20features.%20We%20support%20our%20new%20algorithms%20and%20theoretical%20findings%0Avia%20numerical%20experiments%20on%20synthetic%20data%2C%20MNIST%2C%20CelebA%2C%20Camelyon17%2C%20and%0ADomainNet%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProminent%2520Roles%2520of%2520Conditionally%2520Invariant%2520Components%2520in%2520Domain%250A%2520%2520Adaptation%253A%2520Theory%2520and%2520Algorithms%26entry.906535625%3DKeru%2520Wu%2520and%2520Yuansi%2520Chen%2520and%2520Wooseok%2520Ha%2520and%2520Bin%2520Yu%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520%2528DA%2529%2520is%2520a%2520statistical%2520learning%2520problem%2520that%2520arises%2520when%2520the%250Adistribution%2520of%2520the%2520source%2520data%2520used%2520to%2520train%2520a%2520model%2520differs%2520from%2520that%2520of%2520the%250Atarget%2520data%2520used%2520to%2520evaluate%2520the%2520model.%2520While%2520many%2520DA%2520algorithms%2520have%250Ademonstrated%2520considerable%2520empirical%2520success%252C%2520blindly%2520applying%2520these%2520algorithms%250Acan%2520often%2520lead%2520to%2520worse%2520performance%2520on%2520new%2520datasets.%2520To%2520address%2520this%252C%2520it%2520is%250Acrucial%2520to%2520clarify%2520the%2520assumptions%2520under%2520which%2520a%2520DA%2520algorithm%2520has%2520good%2520target%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520assumption%2520of%2520the%2520presence%2520of%250Aconditionally%2520invariant%2520components%2520%2528CICs%2529%252C%2520which%2520are%2520relevant%2520for%2520prediction%250Aand%2520remain%2520conditionally%2520invariant%2520across%2520the%2520source%2520and%2520target%2520data.%2520We%250Ademonstrate%2520that%2520CICs%252C%2520which%2520can%2520be%2520estimated%2520through%2520conditional%2520invariant%250Apenalty%2520%2528CIP%2529%252C%2520play%2520three%2520prominent%2520roles%2520in%2520providing%2520target%2520risk%2520guarantees%250Ain%2520DA.%2520First%252C%2520we%2520propose%2520a%2520new%2520algorithm%2520based%2520on%2520CICs%252C%2520importance-weighted%250Aconditional%2520invariant%2520penalty%2520%2528IW-CIP%2529%252C%2520which%2520has%2520target%2520risk%2520guarantees%2520beyond%250Asimple%2520settings%2520such%2520as%2520covariate%2520shift%2520and%2520label%2520shift.%2520Second%252C%2520we%2520show%2520that%250ACICs%2520help%2520identify%2520large%2520discrepancies%2520between%2520source%2520and%2520target%2520risks%2520of%2520other%250ADA%2520algorithms.%2520Finally%252C%2520we%2520demonstrate%2520that%2520incorporating%2520CICs%2520into%2520the%2520domain%250Ainvariant%2520projection%2520%2528DIP%2529%2520algorithm%2520can%2520address%2520its%2520failure%2520scenario%2520caused%2520by%250Alabel-flipping%2520features.%2520We%2520support%2520our%2520new%2520algorithms%2520and%2520theoretical%2520findings%250Avia%2520numerical%2520experiments%2520on%2520synthetic%2520data%252C%2520MNIST%252C%2520CelebA%252C%2520Camelyon17%252C%2520and%250ADomainNet%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prominent%20Roles%20of%20Conditionally%20Invariant%20Components%20in%20Domain%0A%20%20Adaptation%3A%20Theory%20and%20Algorithms&entry.906535625=Keru%20Wu%20and%20Yuansi%20Chen%20and%20Wooseok%20Ha%20and%20Bin%20Yu&entry.1292438233=%20%20Domain%20adaptation%20%28DA%29%20is%20a%20statistical%20learning%20problem%20that%20arises%20when%20the%0Adistribution%20of%20the%20source%20data%20used%20to%20train%20a%20model%20differs%20from%20that%20of%20the%0Atarget%20data%20used%20to%20evaluate%20the%20model.%20While%20many%20DA%20algorithms%20have%0Ademonstrated%20considerable%20empirical%20success%2C%20blindly%20applying%20these%20algorithms%0Acan%20often%20lead%20to%20worse%20performance%20on%20new%20datasets.%20To%20address%20this%2C%20it%20is%0Acrucial%20to%20clarify%20the%20assumptions%20under%20which%20a%20DA%20algorithm%20has%20good%20target%0Aperformance.%20In%20this%20work%2C%20we%20focus%20on%20the%20assumption%20of%20the%20presence%20of%0Aconditionally%20invariant%20components%20%28CICs%29%2C%20which%20are%20relevant%20for%20prediction%0Aand%20remain%20conditionally%20invariant%20across%20the%20source%20and%20target%20data.%20We%0Ademonstrate%20that%20CICs%2C%20which%20can%20be%20estimated%20through%20conditional%20invariant%0Apenalty%20%28CIP%29%2C%20play%20three%20prominent%20roles%20in%20providing%20target%20risk%20guarantees%0Ain%20DA.%20First%2C%20we%20propose%20a%20new%20algorithm%20based%20on%20CICs%2C%20importance-weighted%0Aconditional%20invariant%20penalty%20%28IW-CIP%29%2C%20which%20has%20target%20risk%20guarantees%20beyond%0Asimple%20settings%20such%20as%20covariate%20shift%20and%20label%20shift.%20Second%2C%20we%20show%20that%0ACICs%20help%20identify%20large%20discrepancies%20between%20source%20and%20target%20risks%20of%20other%0ADA%20algorithms.%20Finally%2C%20we%20demonstrate%20that%20incorporating%20CICs%20into%20the%20domain%0Ainvariant%20projection%20%28DIP%29%20algorithm%20can%20address%20its%20failure%20scenario%20caused%20by%0Alabel-flipping%20features.%20We%20support%20our%20new%20algorithms%20and%20theoretical%20findings%0Avia%20numerical%20experiments%20on%20synthetic%20data%2C%20MNIST%2C%20CelebA%2C%20Camelyon17%2C%20and%0ADomainNet%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10301v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


