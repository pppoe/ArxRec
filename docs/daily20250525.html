<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250522.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Training-Free Efficient Video Generation via Dynamic Token Carving", "author": "Yuechen Zhang and Jinbo Xing and Bin Xia and Shaoteng Liu and Bohao Peng and Xin Tao and Pengfei Wan and Eric Lo and Jiaya Jia", "abstract": "  Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga\n", "link": "http://arxiv.org/abs/2505.16864v1", "date": "2025-05-22", "relevancy": 3.3676, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6816}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6725}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Efficient%20Video%20Generation%20via%20Dynamic%20Token%20Carving&body=Title%3A%20Training-Free%20Efficient%20Video%20Generation%20via%20Dynamic%20Token%20Carving%0AAuthor%3A%20Yuechen%20Zhang%20and%20Jinbo%20Xing%20and%20Bin%20Xia%20and%20Shaoteng%20Liu%20and%20Bohao%20Peng%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Eric%20Lo%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20generation%20quality%20of%20video%20Diffusion%20Transformer%0A%28DiT%29%20models%2C%20their%20practical%20deployment%20is%20severely%20hindered%20by%20extensive%0Acomputational%20requirements.%20This%20inefficiency%20stems%20from%20two%20key%20challenges%3A%0Athe%20quadratic%20complexity%20of%20self-attention%20with%20respect%20to%20token%20length%20and%20the%0Amulti-step%20nature%20of%20diffusion%20models.%20To%20address%20these%20limitations%2C%20we%20present%0AJenga%2C%20a%20novel%20inference%20pipeline%20that%20combines%20dynamic%20attention%20carving%20with%0Aprogressive%20resolution%20generation.%20Our%20approach%20leverages%20two%20key%20insights%3A%20%281%29%0Aearly%20denoising%20steps%20do%20not%20require%20high-resolution%20latents%2C%20and%20%282%29%20later%0Asteps%20do%20not%20require%20dense%20attention.%20Jenga%20introduces%20a%20block-wise%20attention%0Amechanism%20that%20dynamically%20selects%20relevant%20token%20interactions%20using%203D%0Aspace-filling%20curves%2C%20alongside%20a%20progressive%20resolution%20strategy%20that%0Agradually%20increases%20latent%20resolution%20during%20generation.%20Experimental%20results%0Ademonstrate%20that%20Jenga%20achieves%20substantial%20speedups%20across%20multiple%0Astate-of-the-art%20video%20diffusion%20models%20while%20maintaining%20comparable%20generation%0Aquality%20%288.83%24%5Ctimes%24%20speedup%20with%200.01%5C%25%20performance%20drop%20on%20VBench%29.%20As%20a%0Aplug-and-play%20solution%2C%20Jenga%20enables%20practical%2C%20high-quality%20video%20generation%0Aon%20modern%20hardware%20by%20reducing%20inference%20time%20from%20minutes%20to%20seconds%20--%0Awithout%20requiring%20model%20retraining.%20Code%3A%0Ahttps%3A//github.com/dvlab-research/Jenga%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Efficient%2520Video%2520Generation%2520via%2520Dynamic%2520Token%2520Carving%26entry.906535625%3DYuechen%2520Zhang%2520and%2520Jinbo%2520Xing%2520and%2520Bin%2520Xia%2520and%2520Shaoteng%2520Liu%2520and%2520Bohao%2520Peng%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Eric%2520Lo%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520generation%2520quality%2520of%2520video%2520Diffusion%2520Transformer%250A%2528DiT%2529%2520models%252C%2520their%2520practical%2520deployment%2520is%2520severely%2520hindered%2520by%2520extensive%250Acomputational%2520requirements.%2520This%2520inefficiency%2520stems%2520from%2520two%2520key%2520challenges%253A%250Athe%2520quadratic%2520complexity%2520of%2520self-attention%2520with%2520respect%2520to%2520token%2520length%2520and%2520the%250Amulti-step%2520nature%2520of%2520diffusion%2520models.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%250AJenga%252C%2520a%2520novel%2520inference%2520pipeline%2520that%2520combines%2520dynamic%2520attention%2520carving%2520with%250Aprogressive%2520resolution%2520generation.%2520Our%2520approach%2520leverages%2520two%2520key%2520insights%253A%2520%25281%2529%250Aearly%2520denoising%2520steps%2520do%2520not%2520require%2520high-resolution%2520latents%252C%2520and%2520%25282%2529%2520later%250Asteps%2520do%2520not%2520require%2520dense%2520attention.%2520Jenga%2520introduces%2520a%2520block-wise%2520attention%250Amechanism%2520that%2520dynamically%2520selects%2520relevant%2520token%2520interactions%2520using%25203D%250Aspace-filling%2520curves%252C%2520alongside%2520a%2520progressive%2520resolution%2520strategy%2520that%250Agradually%2520increases%2520latent%2520resolution%2520during%2520generation.%2520Experimental%2520results%250Ademonstrate%2520that%2520Jenga%2520achieves%2520substantial%2520speedups%2520across%2520multiple%250Astate-of-the-art%2520video%2520diffusion%2520models%2520while%2520maintaining%2520comparable%2520generation%250Aquality%2520%25288.83%2524%255Ctimes%2524%2520speedup%2520with%25200.01%255C%2525%2520performance%2520drop%2520on%2520VBench%2529.%2520As%2520a%250Aplug-and-play%2520solution%252C%2520Jenga%2520enables%2520practical%252C%2520high-quality%2520video%2520generation%250Aon%2520modern%2520hardware%2520by%2520reducing%2520inference%2520time%2520from%2520minutes%2520to%2520seconds%2520--%250Awithout%2520requiring%2520model%2520retraining.%2520Code%253A%250Ahttps%253A//github.com/dvlab-research/Jenga%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Efficient%20Video%20Generation%20via%20Dynamic%20Token%20Carving&entry.906535625=Yuechen%20Zhang%20and%20Jinbo%20Xing%20and%20Bin%20Xia%20and%20Shaoteng%20Liu%20and%20Bohao%20Peng%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Eric%20Lo%20and%20Jiaya%20Jia&entry.1292438233=%20%20Despite%20the%20remarkable%20generation%20quality%20of%20video%20Diffusion%20Transformer%0A%28DiT%29%20models%2C%20their%20practical%20deployment%20is%20severely%20hindered%20by%20extensive%0Acomputational%20requirements.%20This%20inefficiency%20stems%20from%20two%20key%20challenges%3A%0Athe%20quadratic%20complexity%20of%20self-attention%20with%20respect%20to%20token%20length%20and%20the%0Amulti-step%20nature%20of%20diffusion%20models.%20To%20address%20these%20limitations%2C%20we%20present%0AJenga%2C%20a%20novel%20inference%20pipeline%20that%20combines%20dynamic%20attention%20carving%20with%0Aprogressive%20resolution%20generation.%20Our%20approach%20leverages%20two%20key%20insights%3A%20%281%29%0Aearly%20denoising%20steps%20do%20not%20require%20high-resolution%20latents%2C%20and%20%282%29%20later%0Asteps%20do%20not%20require%20dense%20attention.%20Jenga%20introduces%20a%20block-wise%20attention%0Amechanism%20that%20dynamically%20selects%20relevant%20token%20interactions%20using%203D%0Aspace-filling%20curves%2C%20alongside%20a%20progressive%20resolution%20strategy%20that%0Agradually%20increases%20latent%20resolution%20during%20generation.%20Experimental%20results%0Ademonstrate%20that%20Jenga%20achieves%20substantial%20speedups%20across%20multiple%0Astate-of-the-art%20video%20diffusion%20models%20while%20maintaining%20comparable%20generation%0Aquality%20%288.83%24%5Ctimes%24%20speedup%20with%200.01%5C%25%20performance%20drop%20on%20VBench%29.%20As%20a%0Aplug-and-play%20solution%2C%20Jenga%20enables%20practical%2C%20high-quality%20video%20generation%0Aon%20modern%20hardware%20by%20reducing%20inference%20time%20from%20minutes%20to%20seconds%20--%0Awithout%20requiring%20model%20retraining.%20Code%3A%0Ahttps%3A//github.com/dvlab-research/Jenga%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16864v1&entry.124074799=Read"},
{"title": "SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane\n  Deformation and Latent Diffusion", "author": "Asrar Alruwayqi", "abstract": "  We present a novel framework for dynamic 3D scene reconstruction that\nintegrates three key components: an explicit tri-plane deformation field, a\nview-conditioned canonical radiance field with spherical harmonics (SH)\nattention, and a temporally-aware latent diffusion prior. Our method encodes 4D\nscenes using three orthogonal 2D feature planes that evolve over time, enabling\nefficient and compact spatiotemporal representation. These features are\nexplicitly warped into a canonical space via a deformation offset field,\neliminating the need for MLP-based motion modeling.\n  In canonical space, we replace traditional MLP decoders with a structured\nSH-based rendering head that synthesizes view-dependent color via attention\nover learned frequency bands improving both interpretability and rendering\nefficiency. To further enhance fidelity and temporal consistency, we introduce\na transformer-guided latent diffusion module that refines the tri-plane and\ndeformation features in a compressed latent space. This generative module\ndenoises scene representations under ambiguous or out-of-distribution (OOD)\nmotion, improving generalization.\n  Our model is trained in two stages: the diffusion module is first pre-trained\nindependently, and then fine-tuned jointly with the full pipeline using a\ncombination of image reconstruction, diffusion denoising, and temporal\nconsistency losses. We demonstrate state-of-the-art results on synthetic\nbenchmarks, surpassing recent methods such as HexPlane and 4D Gaussian\nSplatting in visual quality, temporal coherence, and robustness to sparse-view\ndynamic inputs.\n", "link": "http://arxiv.org/abs/2505.16535v1", "date": "2025-05-22", "relevancy": 3.2608, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6532}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6532}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHaDe%3A%20Compact%20and%20Consistent%20Dynamic%203D%20Reconstruction%20via%20Tri-Plane%0A%20%20Deformation%20and%20Latent%20Diffusion&body=Title%3A%20SHaDe%3A%20Compact%20and%20Consistent%20Dynamic%203D%20Reconstruction%20via%20Tri-Plane%0A%20%20Deformation%20and%20Latent%20Diffusion%0AAuthor%3A%20Asrar%20Alruwayqi%0AAbstract%3A%20%20%20We%20present%20a%20novel%20framework%20for%20dynamic%203D%20scene%20reconstruction%20that%0Aintegrates%20three%20key%20components%3A%20an%20explicit%20tri-plane%20deformation%20field%2C%20a%0Aview-conditioned%20canonical%20radiance%20field%20with%20spherical%20harmonics%20%28SH%29%0Aattention%2C%20and%20a%20temporally-aware%20latent%20diffusion%20prior.%20Our%20method%20encodes%204D%0Ascenes%20using%20three%20orthogonal%202D%20feature%20planes%20that%20evolve%20over%20time%2C%20enabling%0Aefficient%20and%20compact%20spatiotemporal%20representation.%20These%20features%20are%0Aexplicitly%20warped%20into%20a%20canonical%20space%20via%20a%20deformation%20offset%20field%2C%0Aeliminating%20the%20need%20for%20MLP-based%20motion%20modeling.%0A%20%20In%20canonical%20space%2C%20we%20replace%20traditional%20MLP%20decoders%20with%20a%20structured%0ASH-based%20rendering%20head%20that%20synthesizes%20view-dependent%20color%20via%20attention%0Aover%20learned%20frequency%20bands%20improving%20both%20interpretability%20and%20rendering%0Aefficiency.%20To%20further%20enhance%20fidelity%20and%20temporal%20consistency%2C%20we%20introduce%0Aa%20transformer-guided%20latent%20diffusion%20module%20that%20refines%20the%20tri-plane%20and%0Adeformation%20features%20in%20a%20compressed%20latent%20space.%20This%20generative%20module%0Adenoises%20scene%20representations%20under%20ambiguous%20or%20out-of-distribution%20%28OOD%29%0Amotion%2C%20improving%20generalization.%0A%20%20Our%20model%20is%20trained%20in%20two%20stages%3A%20the%20diffusion%20module%20is%20first%20pre-trained%0Aindependently%2C%20and%20then%20fine-tuned%20jointly%20with%20the%20full%20pipeline%20using%20a%0Acombination%20of%20image%20reconstruction%2C%20diffusion%20denoising%2C%20and%20temporal%0Aconsistency%20losses.%20We%20demonstrate%20state-of-the-art%20results%20on%20synthetic%0Abenchmarks%2C%20surpassing%20recent%20methods%20such%20as%20HexPlane%20and%204D%20Gaussian%0ASplatting%20in%20visual%20quality%2C%20temporal%20coherence%2C%20and%20robustness%20to%20sparse-view%0Adynamic%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHaDe%253A%2520Compact%2520and%2520Consistent%2520Dynamic%25203D%2520Reconstruction%2520via%2520Tri-Plane%250A%2520%2520Deformation%2520and%2520Latent%2520Diffusion%26entry.906535625%3DAsrar%2520Alruwayqi%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520framework%2520for%2520dynamic%25203D%2520scene%2520reconstruction%2520that%250Aintegrates%2520three%2520key%2520components%253A%2520an%2520explicit%2520tri-plane%2520deformation%2520field%252C%2520a%250Aview-conditioned%2520canonical%2520radiance%2520field%2520with%2520spherical%2520harmonics%2520%2528SH%2529%250Aattention%252C%2520and%2520a%2520temporally-aware%2520latent%2520diffusion%2520prior.%2520Our%2520method%2520encodes%25204D%250Ascenes%2520using%2520three%2520orthogonal%25202D%2520feature%2520planes%2520that%2520evolve%2520over%2520time%252C%2520enabling%250Aefficient%2520and%2520compact%2520spatiotemporal%2520representation.%2520These%2520features%2520are%250Aexplicitly%2520warped%2520into%2520a%2520canonical%2520space%2520via%2520a%2520deformation%2520offset%2520field%252C%250Aeliminating%2520the%2520need%2520for%2520MLP-based%2520motion%2520modeling.%250A%2520%2520In%2520canonical%2520space%252C%2520we%2520replace%2520traditional%2520MLP%2520decoders%2520with%2520a%2520structured%250ASH-based%2520rendering%2520head%2520that%2520synthesizes%2520view-dependent%2520color%2520via%2520attention%250Aover%2520learned%2520frequency%2520bands%2520improving%2520both%2520interpretability%2520and%2520rendering%250Aefficiency.%2520To%2520further%2520enhance%2520fidelity%2520and%2520temporal%2520consistency%252C%2520we%2520introduce%250Aa%2520transformer-guided%2520latent%2520diffusion%2520module%2520that%2520refines%2520the%2520tri-plane%2520and%250Adeformation%2520features%2520in%2520a%2520compressed%2520latent%2520space.%2520This%2520generative%2520module%250Adenoises%2520scene%2520representations%2520under%2520ambiguous%2520or%2520out-of-distribution%2520%2528OOD%2529%250Amotion%252C%2520improving%2520generalization.%250A%2520%2520Our%2520model%2520is%2520trained%2520in%2520two%2520stages%253A%2520the%2520diffusion%2520module%2520is%2520first%2520pre-trained%250Aindependently%252C%2520and%2520then%2520fine-tuned%2520jointly%2520with%2520the%2520full%2520pipeline%2520using%2520a%250Acombination%2520of%2520image%2520reconstruction%252C%2520diffusion%2520denoising%252C%2520and%2520temporal%250Aconsistency%2520losses.%2520We%2520demonstrate%2520state-of-the-art%2520results%2520on%2520synthetic%250Abenchmarks%252C%2520surpassing%2520recent%2520methods%2520such%2520as%2520HexPlane%2520and%25204D%2520Gaussian%250ASplatting%2520in%2520visual%2520quality%252C%2520temporal%2520coherence%252C%2520and%2520robustness%2520to%2520sparse-view%250Adynamic%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHaDe%3A%20Compact%20and%20Consistent%20Dynamic%203D%20Reconstruction%20via%20Tri-Plane%0A%20%20Deformation%20and%20Latent%20Diffusion&entry.906535625=Asrar%20Alruwayqi&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20for%20dynamic%203D%20scene%20reconstruction%20that%0Aintegrates%20three%20key%20components%3A%20an%20explicit%20tri-plane%20deformation%20field%2C%20a%0Aview-conditioned%20canonical%20radiance%20field%20with%20spherical%20harmonics%20%28SH%29%0Aattention%2C%20and%20a%20temporally-aware%20latent%20diffusion%20prior.%20Our%20method%20encodes%204D%0Ascenes%20using%20three%20orthogonal%202D%20feature%20planes%20that%20evolve%20over%20time%2C%20enabling%0Aefficient%20and%20compact%20spatiotemporal%20representation.%20These%20features%20are%0Aexplicitly%20warped%20into%20a%20canonical%20space%20via%20a%20deformation%20offset%20field%2C%0Aeliminating%20the%20need%20for%20MLP-based%20motion%20modeling.%0A%20%20In%20canonical%20space%2C%20we%20replace%20traditional%20MLP%20decoders%20with%20a%20structured%0ASH-based%20rendering%20head%20that%20synthesizes%20view-dependent%20color%20via%20attention%0Aover%20learned%20frequency%20bands%20improving%20both%20interpretability%20and%20rendering%0Aefficiency.%20To%20further%20enhance%20fidelity%20and%20temporal%20consistency%2C%20we%20introduce%0Aa%20transformer-guided%20latent%20diffusion%20module%20that%20refines%20the%20tri-plane%20and%0Adeformation%20features%20in%20a%20compressed%20latent%20space.%20This%20generative%20module%0Adenoises%20scene%20representations%20under%20ambiguous%20or%20out-of-distribution%20%28OOD%29%0Amotion%2C%20improving%20generalization.%0A%20%20Our%20model%20is%20trained%20in%20two%20stages%3A%20the%20diffusion%20module%20is%20first%20pre-trained%0Aindependently%2C%20and%20then%20fine-tuned%20jointly%20with%20the%20full%20pipeline%20using%20a%0Acombination%20of%20image%20reconstruction%2C%20diffusion%20denoising%2C%20and%20temporal%0Aconsistency%20losses.%20We%20demonstrate%20state-of-the-art%20results%20on%20synthetic%0Abenchmarks%2C%20surpassing%20recent%20methods%20such%20as%20HexPlane%20and%204D%20Gaussian%0ASplatting%20in%20visual%20quality%2C%20temporal%20coherence%2C%20and%20robustness%20to%20sparse-view%0Adynamic%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16535v1&entry.124074799=Read"},
{"title": "Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video\n  Reconstruction", "author": "Jiacong Chen and Qingyu Mao and Youneng Bao and Xiandong Meng and Fanyang Meng and Ronggang Wang and Yongsheng Liang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient\nparadigm for online free-viewpoint video (FVV) reconstruction, offering viewers\nrapid responsiveness and immersive experiences. However, existing online\nmethods face challenge in prohibitive storage requirements primarily due to\npoint-wise modeling that fails to exploit the motion properties. To address\nthis limitation, we propose a novel Compact Gaussian Streaming (ComGS)\nframework, leveraging the locality and consistency of motion in dynamic scene,\nthat models object-consistent Gaussian point motion through keypoint-driven\nmotion representation. By transmitting only the keypoint attributes, this\nframework provides a more storage-efficient solution. Specifically, we first\nidentify a sparse set of motion-sensitive keypoints localized within motion\nregions using a viewspace gradient difference strategy. Equipped with these\nkeypoints, we propose an adaptive motion-driven mechanism that predicts a\nspatial influence field for propagating keypoint motion to neighboring Gaussian\npoints with similar motion. Moreover, ComGS adopts an error-aware correction\nstrategy for key frame reconstruction that selectively refines erroneous\nregions and mitigates error accumulation without unnecessary overhead. Overall,\nComGS achieves a remarkable storage reduction of over 159 X compared to\n3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining\ncompetitive visual fidelity and rendering speed. Our code will be released.\n", "link": "http://arxiv.org/abs/2505.16533v1", "date": "2025-05-22", "relevancy": 3.2376, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.675}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6639}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20Matters%3A%20Compact%20Gaussian%20Streaming%20for%20Free-Viewpoint%20Video%0A%20%20Reconstruction&body=Title%3A%20Motion%20Matters%3A%20Compact%20Gaussian%20Streaming%20for%20Free-Viewpoint%20Video%0A%20%20Reconstruction%0AAuthor%3A%20Jiacong%20Chen%20and%20Qingyu%20Mao%20and%20Youneng%20Bao%20and%20Xiandong%20Meng%20and%20Fanyang%20Meng%20and%20Ronggang%20Wang%20and%20Yongsheng%20Liang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20high-fidelity%20and%20efficient%0Aparadigm%20for%20online%20free-viewpoint%20video%20%28FVV%29%20reconstruction%2C%20offering%20viewers%0Arapid%20responsiveness%20and%20immersive%20experiences.%20However%2C%20existing%20online%0Amethods%20face%20challenge%20in%20prohibitive%20storage%20requirements%20primarily%20due%20to%0Apoint-wise%20modeling%20that%20fails%20to%20exploit%20the%20motion%20properties.%20To%20address%0Athis%20limitation%2C%20we%20propose%20a%20novel%20Compact%20Gaussian%20Streaming%20%28ComGS%29%0Aframework%2C%20leveraging%20the%20locality%20and%20consistency%20of%20motion%20in%20dynamic%20scene%2C%0Athat%20models%20object-consistent%20Gaussian%20point%20motion%20through%20keypoint-driven%0Amotion%20representation.%20By%20transmitting%20only%20the%20keypoint%20attributes%2C%20this%0Aframework%20provides%20a%20more%20storage-efficient%20solution.%20Specifically%2C%20we%20first%0Aidentify%20a%20sparse%20set%20of%20motion-sensitive%20keypoints%20localized%20within%20motion%0Aregions%20using%20a%20viewspace%20gradient%20difference%20strategy.%20Equipped%20with%20these%0Akeypoints%2C%20we%20propose%20an%20adaptive%20motion-driven%20mechanism%20that%20predicts%20a%0Aspatial%20influence%20field%20for%20propagating%20keypoint%20motion%20to%20neighboring%20Gaussian%0Apoints%20with%20similar%20motion.%20Moreover%2C%20ComGS%20adopts%20an%20error-aware%20correction%0Astrategy%20for%20key%20frame%20reconstruction%20that%20selectively%20refines%20erroneous%0Aregions%20and%20mitigates%20error%20accumulation%20without%20unnecessary%20overhead.%20Overall%2C%0AComGS%20achieves%20a%20remarkable%20storage%20reduction%20of%20over%20159%20X%20compared%20to%0A3DGStream%20and%2014%20X%20compared%20to%20the%20SOTA%20method%20QUEEN%2C%20while%20maintaining%0Acompetitive%20visual%20fidelity%20and%20rendering%20speed.%20Our%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520Matters%253A%2520Compact%2520Gaussian%2520Streaming%2520for%2520Free-Viewpoint%2520Video%250A%2520%2520Reconstruction%26entry.906535625%3DJiacong%2520Chen%2520and%2520Qingyu%2520Mao%2520and%2520Youneng%2520Bao%2520and%2520Xiandong%2520Meng%2520and%2520Fanyang%2520Meng%2520and%2520Ronggang%2520Wang%2520and%2520Yongsheng%2520Liang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520high-fidelity%2520and%2520efficient%250Aparadigm%2520for%2520online%2520free-viewpoint%2520video%2520%2528FVV%2529%2520reconstruction%252C%2520offering%2520viewers%250Arapid%2520responsiveness%2520and%2520immersive%2520experiences.%2520However%252C%2520existing%2520online%250Amethods%2520face%2520challenge%2520in%2520prohibitive%2520storage%2520requirements%2520primarily%2520due%2520to%250Apoint-wise%2520modeling%2520that%2520fails%2520to%2520exploit%2520the%2520motion%2520properties.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520Compact%2520Gaussian%2520Streaming%2520%2528ComGS%2529%250Aframework%252C%2520leveraging%2520the%2520locality%2520and%2520consistency%2520of%2520motion%2520in%2520dynamic%2520scene%252C%250Athat%2520models%2520object-consistent%2520Gaussian%2520point%2520motion%2520through%2520keypoint-driven%250Amotion%2520representation.%2520By%2520transmitting%2520only%2520the%2520keypoint%2520attributes%252C%2520this%250Aframework%2520provides%2520a%2520more%2520storage-efficient%2520solution.%2520Specifically%252C%2520we%2520first%250Aidentify%2520a%2520sparse%2520set%2520of%2520motion-sensitive%2520keypoints%2520localized%2520within%2520motion%250Aregions%2520using%2520a%2520viewspace%2520gradient%2520difference%2520strategy.%2520Equipped%2520with%2520these%250Akeypoints%252C%2520we%2520propose%2520an%2520adaptive%2520motion-driven%2520mechanism%2520that%2520predicts%2520a%250Aspatial%2520influence%2520field%2520for%2520propagating%2520keypoint%2520motion%2520to%2520neighboring%2520Gaussian%250Apoints%2520with%2520similar%2520motion.%2520Moreover%252C%2520ComGS%2520adopts%2520an%2520error-aware%2520correction%250Astrategy%2520for%2520key%2520frame%2520reconstruction%2520that%2520selectively%2520refines%2520erroneous%250Aregions%2520and%2520mitigates%2520error%2520accumulation%2520without%2520unnecessary%2520overhead.%2520Overall%252C%250AComGS%2520achieves%2520a%2520remarkable%2520storage%2520reduction%2520of%2520over%2520159%2520X%2520compared%2520to%250A3DGStream%2520and%252014%2520X%2520compared%2520to%2520the%2520SOTA%2520method%2520QUEEN%252C%2520while%2520maintaining%250Acompetitive%2520visual%2520fidelity%2520and%2520rendering%2520speed.%2520Our%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Matters%3A%20Compact%20Gaussian%20Streaming%20for%20Free-Viewpoint%20Video%0A%20%20Reconstruction&entry.906535625=Jiacong%20Chen%20and%20Qingyu%20Mao%20and%20Youneng%20Bao%20and%20Xiandong%20Meng%20and%20Fanyang%20Meng%20and%20Ronggang%20Wang%20and%20Yongsheng%20Liang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20high-fidelity%20and%20efficient%0Aparadigm%20for%20online%20free-viewpoint%20video%20%28FVV%29%20reconstruction%2C%20offering%20viewers%0Arapid%20responsiveness%20and%20immersive%20experiences.%20However%2C%20existing%20online%0Amethods%20face%20challenge%20in%20prohibitive%20storage%20requirements%20primarily%20due%20to%0Apoint-wise%20modeling%20that%20fails%20to%20exploit%20the%20motion%20properties.%20To%20address%0Athis%20limitation%2C%20we%20propose%20a%20novel%20Compact%20Gaussian%20Streaming%20%28ComGS%29%0Aframework%2C%20leveraging%20the%20locality%20and%20consistency%20of%20motion%20in%20dynamic%20scene%2C%0Athat%20models%20object-consistent%20Gaussian%20point%20motion%20through%20keypoint-driven%0Amotion%20representation.%20By%20transmitting%20only%20the%20keypoint%20attributes%2C%20this%0Aframework%20provides%20a%20more%20storage-efficient%20solution.%20Specifically%2C%20we%20first%0Aidentify%20a%20sparse%20set%20of%20motion-sensitive%20keypoints%20localized%20within%20motion%0Aregions%20using%20a%20viewspace%20gradient%20difference%20strategy.%20Equipped%20with%20these%0Akeypoints%2C%20we%20propose%20an%20adaptive%20motion-driven%20mechanism%20that%20predicts%20a%0Aspatial%20influence%20field%20for%20propagating%20keypoint%20motion%20to%20neighboring%20Gaussian%0Apoints%20with%20similar%20motion.%20Moreover%2C%20ComGS%20adopts%20an%20error-aware%20correction%0Astrategy%20for%20key%20frame%20reconstruction%20that%20selectively%20refines%20erroneous%0Aregions%20and%20mitigates%20error%20accumulation%20without%20unnecessary%20overhead.%20Overall%2C%0AComGS%20achieves%20a%20remarkable%20storage%20reduction%20of%20over%20159%20X%20compared%20to%0A3DGStream%20and%2014%20X%20compared%20to%20the%20SOTA%20method%20QUEEN%2C%20while%20maintaining%0Acompetitive%20visual%20fidelity%20and%20rendering%20speed.%20Our%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16533v1&entry.124074799=Read"},
{"title": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis", "author": "Radek Dan\u011b\u010dek and Carolin Schmitt and Senya Polikovsky and Michael J. Black", "abstract": "  In order to be widely applicable, speech-driven 3D head avatars must\narticulate their lips in accordance with speech, while also conveying the\nappropriate emotions with dynamically changing facial expressions. The key\nproblem is that deterministic models produce high-quality lip-sync but without\nrich expressions, whereas stochastic models generate diverse expressions but\nwith lower lip-sync quality. To get the best of both, we seek a stochastic\nmodel with accurate lip-sync. To that end, we develop a new approach based on\nthe following observation: if a method generates realistic 3D lip motions, it\nshould be possible to infer the spoken audio from the lip motion. The inferred\nspeech should match the original input audio, and erroneous predictions create\na novel supervision signal for training 3D talking head avatars with accurate\nlip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under\nNeural Differentiable Elocution Reconstruction), a 3D talking head avatar\nframework that introduces a novel supervision mechanism via differentiable\nsound production. First, we train a novel mesh-to-speech model that regresses\naudio from facial animation. Then, we incorporate this model into a\ndiffusion-based talking avatar framework. During training, the mesh-to-speech\nmodel takes the generated animation and produces a sound that is compared to\nthe input speech, creating a differentiable analysis-by-audio-synthesis\nsupervision loop. Our extensive qualitative and quantitative experiments\ndemonstrate that THUNDER significantly improves the quality of the lip-sync of\ntalking head avatars while still allowing for generation of diverse,\nhigh-quality, expressive facial animations. The code and models will be\navailable at https://thunder.is.tue.mpg.de/\n", "link": "http://arxiv.org/abs/2504.13386v2", "date": "2025-05-22", "relevancy": 3.1895, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6398}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6369}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervising%203D%20Talking%20Head%20Avatars%20with%20Analysis-by-Audio-Synthesis&body=Title%3A%20Supervising%203D%20Talking%20Head%20Avatars%20with%20Analysis-by-Audio-Synthesis%0AAuthor%3A%20Radek%20Dan%C4%9B%C4%8Dek%20and%20Carolin%20Schmitt%20and%20Senya%20Polikovsky%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20In%20order%20to%20be%20widely%20applicable%2C%20speech-driven%203D%20head%20avatars%20must%0Aarticulate%20their%20lips%20in%20accordance%20with%20speech%2C%20while%20also%20conveying%20the%0Aappropriate%20emotions%20with%20dynamically%20changing%20facial%20expressions.%20The%20key%0Aproblem%20is%20that%20deterministic%20models%20produce%20high-quality%20lip-sync%20but%20without%0Arich%20expressions%2C%20whereas%20stochastic%20models%20generate%20diverse%20expressions%20but%0Awith%20lower%20lip-sync%20quality.%20To%20get%20the%20best%20of%20both%2C%20we%20seek%20a%20stochastic%0Amodel%20with%20accurate%20lip-sync.%20To%20that%20end%2C%20we%20develop%20a%20new%20approach%20based%20on%0Athe%20following%20observation%3A%20if%20a%20method%20generates%20realistic%203D%20lip%20motions%2C%20it%0Ashould%20be%20possible%20to%20infer%20the%20spoken%20audio%20from%20the%20lip%20motion.%20The%20inferred%0Aspeech%20should%20match%20the%20original%20input%20audio%2C%20and%20erroneous%20predictions%20create%0Aa%20novel%20supervision%20signal%20for%20training%203D%20talking%20head%20avatars%20with%20accurate%0Alip-sync.%20To%20demonstrate%20this%20effect%2C%20we%20propose%20THUNDER%20%28Talking%20Heads%20Under%0ANeural%20Differentiable%20Elocution%20Reconstruction%29%2C%20a%203D%20talking%20head%20avatar%0Aframework%20that%20introduces%20a%20novel%20supervision%20mechanism%20via%20differentiable%0Asound%20production.%20First%2C%20we%20train%20a%20novel%20mesh-to-speech%20model%20that%20regresses%0Aaudio%20from%20facial%20animation.%20Then%2C%20we%20incorporate%20this%20model%20into%20a%0Adiffusion-based%20talking%20avatar%20framework.%20During%20training%2C%20the%20mesh-to-speech%0Amodel%20takes%20the%20generated%20animation%20and%20produces%20a%20sound%20that%20is%20compared%20to%0Athe%20input%20speech%2C%20creating%20a%20differentiable%20analysis-by-audio-synthesis%0Asupervision%20loop.%20Our%20extensive%20qualitative%20and%20quantitative%20experiments%0Ademonstrate%20that%20THUNDER%20significantly%20improves%20the%20quality%20of%20the%20lip-sync%20of%0Atalking%20head%20avatars%20while%20still%20allowing%20for%20generation%20of%20diverse%2C%0Ahigh-quality%2C%20expressive%20facial%20animations.%20The%20code%20and%20models%20will%20be%0Aavailable%20at%20https%3A//thunder.is.tue.mpg.de/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervising%25203D%2520Talking%2520Head%2520Avatars%2520with%2520Analysis-by-Audio-Synthesis%26entry.906535625%3DRadek%2520Dan%25C4%259B%25C4%258Dek%2520and%2520Carolin%2520Schmitt%2520and%2520Senya%2520Polikovsky%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520be%2520widely%2520applicable%252C%2520speech-driven%25203D%2520head%2520avatars%2520must%250Aarticulate%2520their%2520lips%2520in%2520accordance%2520with%2520speech%252C%2520while%2520also%2520conveying%2520the%250Aappropriate%2520emotions%2520with%2520dynamically%2520changing%2520facial%2520expressions.%2520The%2520key%250Aproblem%2520is%2520that%2520deterministic%2520models%2520produce%2520high-quality%2520lip-sync%2520but%2520without%250Arich%2520expressions%252C%2520whereas%2520stochastic%2520models%2520generate%2520diverse%2520expressions%2520but%250Awith%2520lower%2520lip-sync%2520quality.%2520To%2520get%2520the%2520best%2520of%2520both%252C%2520we%2520seek%2520a%2520stochastic%250Amodel%2520with%2520accurate%2520lip-sync.%2520To%2520that%2520end%252C%2520we%2520develop%2520a%2520new%2520approach%2520based%2520on%250Athe%2520following%2520observation%253A%2520if%2520a%2520method%2520generates%2520realistic%25203D%2520lip%2520motions%252C%2520it%250Ashould%2520be%2520possible%2520to%2520infer%2520the%2520spoken%2520audio%2520from%2520the%2520lip%2520motion.%2520The%2520inferred%250Aspeech%2520should%2520match%2520the%2520original%2520input%2520audio%252C%2520and%2520erroneous%2520predictions%2520create%250Aa%2520novel%2520supervision%2520signal%2520for%2520training%25203D%2520talking%2520head%2520avatars%2520with%2520accurate%250Alip-sync.%2520To%2520demonstrate%2520this%2520effect%252C%2520we%2520propose%2520THUNDER%2520%2528Talking%2520Heads%2520Under%250ANeural%2520Differentiable%2520Elocution%2520Reconstruction%2529%252C%2520a%25203D%2520talking%2520head%2520avatar%250Aframework%2520that%2520introduces%2520a%2520novel%2520supervision%2520mechanism%2520via%2520differentiable%250Asound%2520production.%2520First%252C%2520we%2520train%2520a%2520novel%2520mesh-to-speech%2520model%2520that%2520regresses%250Aaudio%2520from%2520facial%2520animation.%2520Then%252C%2520we%2520incorporate%2520this%2520model%2520into%2520a%250Adiffusion-based%2520talking%2520avatar%2520framework.%2520During%2520training%252C%2520the%2520mesh-to-speech%250Amodel%2520takes%2520the%2520generated%2520animation%2520and%2520produces%2520a%2520sound%2520that%2520is%2520compared%2520to%250Athe%2520input%2520speech%252C%2520creating%2520a%2520differentiable%2520analysis-by-audio-synthesis%250Asupervision%2520loop.%2520Our%2520extensive%2520qualitative%2520and%2520quantitative%2520experiments%250Ademonstrate%2520that%2520THUNDER%2520significantly%2520improves%2520the%2520quality%2520of%2520the%2520lip-sync%2520of%250Atalking%2520head%2520avatars%2520while%2520still%2520allowing%2520for%2520generation%2520of%2520diverse%252C%250Ahigh-quality%252C%2520expressive%2520facial%2520animations.%2520The%2520code%2520and%2520models%2520will%2520be%250Aavailable%2520at%2520https%253A//thunder.is.tue.mpg.de/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervising%203D%20Talking%20Head%20Avatars%20with%20Analysis-by-Audio-Synthesis&entry.906535625=Radek%20Dan%C4%9B%C4%8Dek%20and%20Carolin%20Schmitt%20and%20Senya%20Polikovsky%20and%20Michael%20J.%20Black&entry.1292438233=%20%20In%20order%20to%20be%20widely%20applicable%2C%20speech-driven%203D%20head%20avatars%20must%0Aarticulate%20their%20lips%20in%20accordance%20with%20speech%2C%20while%20also%20conveying%20the%0Aappropriate%20emotions%20with%20dynamically%20changing%20facial%20expressions.%20The%20key%0Aproblem%20is%20that%20deterministic%20models%20produce%20high-quality%20lip-sync%20but%20without%0Arich%20expressions%2C%20whereas%20stochastic%20models%20generate%20diverse%20expressions%20but%0Awith%20lower%20lip-sync%20quality.%20To%20get%20the%20best%20of%20both%2C%20we%20seek%20a%20stochastic%0Amodel%20with%20accurate%20lip-sync.%20To%20that%20end%2C%20we%20develop%20a%20new%20approach%20based%20on%0Athe%20following%20observation%3A%20if%20a%20method%20generates%20realistic%203D%20lip%20motions%2C%20it%0Ashould%20be%20possible%20to%20infer%20the%20spoken%20audio%20from%20the%20lip%20motion.%20The%20inferred%0Aspeech%20should%20match%20the%20original%20input%20audio%2C%20and%20erroneous%20predictions%20create%0Aa%20novel%20supervision%20signal%20for%20training%203D%20talking%20head%20avatars%20with%20accurate%0Alip-sync.%20To%20demonstrate%20this%20effect%2C%20we%20propose%20THUNDER%20%28Talking%20Heads%20Under%0ANeural%20Differentiable%20Elocution%20Reconstruction%29%2C%20a%203D%20talking%20head%20avatar%0Aframework%20that%20introduces%20a%20novel%20supervision%20mechanism%20via%20differentiable%0Asound%20production.%20First%2C%20we%20train%20a%20novel%20mesh-to-speech%20model%20that%20regresses%0Aaudio%20from%20facial%20animation.%20Then%2C%20we%20incorporate%20this%20model%20into%20a%0Adiffusion-based%20talking%20avatar%20framework.%20During%20training%2C%20the%20mesh-to-speech%0Amodel%20takes%20the%20generated%20animation%20and%20produces%20a%20sound%20that%20is%20compared%20to%0Athe%20input%20speech%2C%20creating%20a%20differentiable%20analysis-by-audio-synthesis%0Asupervision%20loop.%20Our%20extensive%20qualitative%20and%20quantitative%20experiments%0Ademonstrate%20that%20THUNDER%20significantly%20improves%20the%20quality%20of%20the%20lip-sync%20of%0Atalking%20head%20avatars%20while%20still%20allowing%20for%20generation%20of%20diverse%2C%0Ahigh-quality%2C%20expressive%20facial%20animations.%20The%20code%20and%20models%20will%20be%0Aavailable%20at%20https%3A//thunder.is.tue.mpg.de/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13386v2&entry.124074799=Read"},
{"title": "MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion\n  Generation", "author": "Bohan Zhou and Yi Zhan and Zhongbin Zhang and Zongqing Lu", "abstract": "  Egocentric hand-object motion generation is crucial for immersive AR/VR and\nrobotic imitation but remains challenging due to unstable viewpoints,\nself-occlusions, perspective distortion, and noisy ego-motion. Existing methods\nrely on predefined 3D object priors, limiting generalization to novel objects,\nwhich restricts their generalizability to novel objects. Meanwhile, recent\nmultimodal approaches suffer from ambiguous generation from abstract textual\ncues, intricate pipelines for modeling 3D hand-object correlation, and\ncompounding errors in open-loop prediction. We propose MEgoHand, a multimodal\nframework that synthesizes physically plausible hand-object interactions from\negocentric RGB, text, and initial hand pose. MEgoHand introduces a bi-level\narchitecture: a high-level \"cerebrum\" leverages a vision language model (VLM)\nto infer motion priors from visual-textual context and a monocular depth\nestimator for object-agnostic spatial reasoning, while a low-level DiT-based\nflow-matching policy generates fine-grained trajectories with temporal\northogonal filtering to enhance stability. To address dataset inconsistency, we\ndesign a dataset curation paradigm with an Inverse MANO Retargeting Network and\nVirtual RGB-D Renderer, curating a unified dataset of 3.35M RGB-D frames, 24K\ninteractions, and 1.2K objects. Extensive experiments across five in-domain and\ntwo cross-domain datasets demonstrate the effectiveness of MEgoHand, achieving\nsubstantial reductions in wrist translation error (86.9%) and joint rotation\nerror (34.1%), highlighting its capacity to accurately model fine-grained hand\njoint structures and generalize robustly across diverse scenarios.\n", "link": "http://arxiv.org/abs/2505.16602v1", "date": "2025-05-22", "relevancy": 3.0591, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.618}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.612}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEgoHand%3A%20Multimodal%20Egocentric%20Hand-Object%20Interaction%20Motion%0A%20%20Generation&body=Title%3A%20MEgoHand%3A%20Multimodal%20Egocentric%20Hand-Object%20Interaction%20Motion%0A%20%20Generation%0AAuthor%3A%20Bohan%20Zhou%20and%20Yi%20Zhan%20and%20Zhongbin%20Zhang%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20Egocentric%20hand-object%20motion%20generation%20is%20crucial%20for%20immersive%20AR/VR%20and%0Arobotic%20imitation%20but%20remains%20challenging%20due%20to%20unstable%20viewpoints%2C%0Aself-occlusions%2C%20perspective%20distortion%2C%20and%20noisy%20ego-motion.%20Existing%20methods%0Arely%20on%20predefined%203D%20object%20priors%2C%20limiting%20generalization%20to%20novel%20objects%2C%0Awhich%20restricts%20their%20generalizability%20to%20novel%20objects.%20Meanwhile%2C%20recent%0Amultimodal%20approaches%20suffer%20from%20ambiguous%20generation%20from%20abstract%20textual%0Acues%2C%20intricate%20pipelines%20for%20modeling%203D%20hand-object%20correlation%2C%20and%0Acompounding%20errors%20in%20open-loop%20prediction.%20We%20propose%20MEgoHand%2C%20a%20multimodal%0Aframework%20that%20synthesizes%20physically%20plausible%20hand-object%20interactions%20from%0Aegocentric%20RGB%2C%20text%2C%20and%20initial%20hand%20pose.%20MEgoHand%20introduces%20a%20bi-level%0Aarchitecture%3A%20a%20high-level%20%22cerebrum%22%20leverages%20a%20vision%20language%20model%20%28VLM%29%0Ato%20infer%20motion%20priors%20from%20visual-textual%20context%20and%20a%20monocular%20depth%0Aestimator%20for%20object-agnostic%20spatial%20reasoning%2C%20while%20a%20low-level%20DiT-based%0Aflow-matching%20policy%20generates%20fine-grained%20trajectories%20with%20temporal%0Aorthogonal%20filtering%20to%20enhance%20stability.%20To%20address%20dataset%20inconsistency%2C%20we%0Adesign%20a%20dataset%20curation%20paradigm%20with%20an%20Inverse%20MANO%20Retargeting%20Network%20and%0AVirtual%20RGB-D%20Renderer%2C%20curating%20a%20unified%20dataset%20of%203.35M%20RGB-D%20frames%2C%2024K%0Ainteractions%2C%20and%201.2K%20objects.%20Extensive%20experiments%20across%20five%20in-domain%20and%0Atwo%20cross-domain%20datasets%20demonstrate%20the%20effectiveness%20of%20MEgoHand%2C%20achieving%0Asubstantial%20reductions%20in%20wrist%20translation%20error%20%2886.9%25%29%20and%20joint%20rotation%0Aerror%20%2834.1%25%29%2C%20highlighting%20its%20capacity%20to%20accurately%20model%20fine-grained%20hand%0Ajoint%20structures%20and%20generalize%20robustly%20across%20diverse%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEgoHand%253A%2520Multimodal%2520Egocentric%2520Hand-Object%2520Interaction%2520Motion%250A%2520%2520Generation%26entry.906535625%3DBohan%2520Zhou%2520and%2520Yi%2520Zhan%2520and%2520Zhongbin%2520Zhang%2520and%2520Zongqing%2520Lu%26entry.1292438233%3D%2520%2520Egocentric%2520hand-object%2520motion%2520generation%2520is%2520crucial%2520for%2520immersive%2520AR/VR%2520and%250Arobotic%2520imitation%2520but%2520remains%2520challenging%2520due%2520to%2520unstable%2520viewpoints%252C%250Aself-occlusions%252C%2520perspective%2520distortion%252C%2520and%2520noisy%2520ego-motion.%2520Existing%2520methods%250Arely%2520on%2520predefined%25203D%2520object%2520priors%252C%2520limiting%2520generalization%2520to%2520novel%2520objects%252C%250Awhich%2520restricts%2520their%2520generalizability%2520to%2520novel%2520objects.%2520Meanwhile%252C%2520recent%250Amultimodal%2520approaches%2520suffer%2520from%2520ambiguous%2520generation%2520from%2520abstract%2520textual%250Acues%252C%2520intricate%2520pipelines%2520for%2520modeling%25203D%2520hand-object%2520correlation%252C%2520and%250Acompounding%2520errors%2520in%2520open-loop%2520prediction.%2520We%2520propose%2520MEgoHand%252C%2520a%2520multimodal%250Aframework%2520that%2520synthesizes%2520physically%2520plausible%2520hand-object%2520interactions%2520from%250Aegocentric%2520RGB%252C%2520text%252C%2520and%2520initial%2520hand%2520pose.%2520MEgoHand%2520introduces%2520a%2520bi-level%250Aarchitecture%253A%2520a%2520high-level%2520%2522cerebrum%2522%2520leverages%2520a%2520vision%2520language%2520model%2520%2528VLM%2529%250Ato%2520infer%2520motion%2520priors%2520from%2520visual-textual%2520context%2520and%2520a%2520monocular%2520depth%250Aestimator%2520for%2520object-agnostic%2520spatial%2520reasoning%252C%2520while%2520a%2520low-level%2520DiT-based%250Aflow-matching%2520policy%2520generates%2520fine-grained%2520trajectories%2520with%2520temporal%250Aorthogonal%2520filtering%2520to%2520enhance%2520stability.%2520To%2520address%2520dataset%2520inconsistency%252C%2520we%250Adesign%2520a%2520dataset%2520curation%2520paradigm%2520with%2520an%2520Inverse%2520MANO%2520Retargeting%2520Network%2520and%250AVirtual%2520RGB-D%2520Renderer%252C%2520curating%2520a%2520unified%2520dataset%2520of%25203.35M%2520RGB-D%2520frames%252C%252024K%250Ainteractions%252C%2520and%25201.2K%2520objects.%2520Extensive%2520experiments%2520across%2520five%2520in-domain%2520and%250Atwo%2520cross-domain%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520MEgoHand%252C%2520achieving%250Asubstantial%2520reductions%2520in%2520wrist%2520translation%2520error%2520%252886.9%2525%2529%2520and%2520joint%2520rotation%250Aerror%2520%252834.1%2525%2529%252C%2520highlighting%2520its%2520capacity%2520to%2520accurately%2520model%2520fine-grained%2520hand%250Ajoint%2520structures%2520and%2520generalize%2520robustly%2520across%2520diverse%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEgoHand%3A%20Multimodal%20Egocentric%20Hand-Object%20Interaction%20Motion%0A%20%20Generation&entry.906535625=Bohan%20Zhou%20and%20Yi%20Zhan%20and%20Zhongbin%20Zhang%20and%20Zongqing%20Lu&entry.1292438233=%20%20Egocentric%20hand-object%20motion%20generation%20is%20crucial%20for%20immersive%20AR/VR%20and%0Arobotic%20imitation%20but%20remains%20challenging%20due%20to%20unstable%20viewpoints%2C%0Aself-occlusions%2C%20perspective%20distortion%2C%20and%20noisy%20ego-motion.%20Existing%20methods%0Arely%20on%20predefined%203D%20object%20priors%2C%20limiting%20generalization%20to%20novel%20objects%2C%0Awhich%20restricts%20their%20generalizability%20to%20novel%20objects.%20Meanwhile%2C%20recent%0Amultimodal%20approaches%20suffer%20from%20ambiguous%20generation%20from%20abstract%20textual%0Acues%2C%20intricate%20pipelines%20for%20modeling%203D%20hand-object%20correlation%2C%20and%0Acompounding%20errors%20in%20open-loop%20prediction.%20We%20propose%20MEgoHand%2C%20a%20multimodal%0Aframework%20that%20synthesizes%20physically%20plausible%20hand-object%20interactions%20from%0Aegocentric%20RGB%2C%20text%2C%20and%20initial%20hand%20pose.%20MEgoHand%20introduces%20a%20bi-level%0Aarchitecture%3A%20a%20high-level%20%22cerebrum%22%20leverages%20a%20vision%20language%20model%20%28VLM%29%0Ato%20infer%20motion%20priors%20from%20visual-textual%20context%20and%20a%20monocular%20depth%0Aestimator%20for%20object-agnostic%20spatial%20reasoning%2C%20while%20a%20low-level%20DiT-based%0Aflow-matching%20policy%20generates%20fine-grained%20trajectories%20with%20temporal%0Aorthogonal%20filtering%20to%20enhance%20stability.%20To%20address%20dataset%20inconsistency%2C%20we%0Adesign%20a%20dataset%20curation%20paradigm%20with%20an%20Inverse%20MANO%20Retargeting%20Network%20and%0AVirtual%20RGB-D%20Renderer%2C%20curating%20a%20unified%20dataset%20of%203.35M%20RGB-D%20frames%2C%2024K%0Ainteractions%2C%20and%201.2K%20objects.%20Extensive%20experiments%20across%20five%20in-domain%20and%0Atwo%20cross-domain%20datasets%20demonstrate%20the%20effectiveness%20of%20MEgoHand%2C%20achieving%0Asubstantial%20reductions%20in%20wrist%20translation%20error%20%2886.9%25%29%20and%20joint%20rotation%0Aerror%20%2834.1%25%29%2C%20highlighting%20its%20capacity%20to%20accurately%20model%20fine-grained%20hand%0Ajoint%20structures%20and%20generalize%20robustly%20across%20diverse%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16602v1&entry.124074799=Read"},
{"title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding", "author": "Yuan Tang and Xu Han and Xianzhi Li and Qiao Yu and Jinfeng Xu and Yixue Hao and Long Hu and Min Chen", "abstract": "  Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.\n", "link": "http://arxiv.org/abs/2408.15966v3", "date": "2025-05-22", "relevancy": 2.9869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Text%2C%20Less%20Point%3A%20Towards%203D%20Data-Efficient%20Point-Language%0A%20%20Understanding&body=Title%3A%20More%20Text%2C%20Less%20Point%3A%20Towards%203D%20Data-Efficient%20Point-Language%0A%20%20Understanding%0AAuthor%3A%20Yuan%20Tang%20and%20Xu%20Han%20and%20Xianzhi%20Li%20and%20Qiao%20Yu%20and%20Jinfeng%20Xu%20and%20Yixue%20Hao%20and%20Long%20Hu%20and%20Min%20Chen%0AAbstract%3A%20%20%20Enabling%20Large%20Language%20Models%20%28LLMs%29%20to%20comprehend%20the%203D%20physical%20world%0Aremains%20a%20significant%20challenge.%20Due%20to%20the%20lack%20of%20large-scale%203D-text%20pair%0Adatasets%2C%20the%20success%20of%20LLMs%20has%20yet%20to%20be%20replicated%20in%203D%20understanding.%20In%0Athis%20paper%2C%20we%20rethink%20this%20issue%20and%20propose%20a%20new%20task%3A%203D%20Data-Efficient%0APoint-Language%20Understanding.%20The%20goal%20is%20to%20enable%20LLMs%20to%20achieve%20robust%203D%0Aobject%20understanding%20with%20minimal%203D%20point%20cloud%20and%20text%20data%20pairs.%20To%0Aaddress%20this%20task%2C%20we%20introduce%20GreenPLM%2C%20which%20leverages%20more%20text%20data%20to%0Acompensate%20for%20the%20lack%20of%203D%20data.%20First%2C%20inspired%20by%20using%20CLIP%20to%20align%0Aimages%20and%20text%2C%20we%20utilize%20a%20pre-trained%20point%20cloud-text%20encoder%20to%20map%20the%0A3D%20point%20cloud%20space%20to%20the%20text%20space.%20This%20mapping%20leaves%20us%20to%20seamlessly%0Aconnect%20the%20text%20space%20with%20LLMs.%20Once%20the%20point-text-LLM%20connection%20is%0Aestablished%2C%20we%20further%20enhance%20text-LLM%20alignment%20by%20expanding%20the%0Aintermediate%20text%20space%2C%20thereby%20reducing%20the%20reliance%20on%203D%20point%20cloud%20data.%0ASpecifically%2C%20we%20generate%206M%20free-text%20descriptions%20of%203D%20objects%2C%20and%20design%20a%0Athree-stage%20training%20strategy%20to%20help%20LLMs%20better%20explore%20the%20intrinsic%0Aconnections%20between%20different%20modalities.%20To%20achieve%20efficient%20modality%0Aalignment%2C%20we%20design%20a%20zero-parameter%20cross-attention%20module%20for%20token%20pooling.%0AExtensive%20experimental%20results%20show%20that%20GreenPLM%20requires%20only%2012%25%20of%20the%203D%0Atraining%20data%20used%20by%20existing%20state-of-the-art%20models%20to%20achieve%20superior%203D%0Aunderstanding.%20Remarkably%2C%20GreenPLM%20also%20achieves%20competitive%20performance%20using%0Atext-only%20data.%20The%20code%20and%20weights%20are%20available%20at%3A%0Ahttps%3A//github.com/TangYuan96/GreenPLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Text%252C%2520Less%2520Point%253A%2520Towards%25203D%2520Data-Efficient%2520Point-Language%250A%2520%2520Understanding%26entry.906535625%3DYuan%2520Tang%2520and%2520Xu%2520Han%2520and%2520Xianzhi%2520Li%2520and%2520Qiao%2520Yu%2520and%2520Jinfeng%2520Xu%2520and%2520Yixue%2520Hao%2520and%2520Long%2520Hu%2520and%2520Min%2520Chen%26entry.1292438233%3D%2520%2520Enabling%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520comprehend%2520the%25203D%2520physical%2520world%250Aremains%2520a%2520significant%2520challenge.%2520Due%2520to%2520the%2520lack%2520of%2520large-scale%25203D-text%2520pair%250Adatasets%252C%2520the%2520success%2520of%2520LLMs%2520has%2520yet%2520to%2520be%2520replicated%2520in%25203D%2520understanding.%2520In%250Athis%2520paper%252C%2520we%2520rethink%2520this%2520issue%2520and%2520propose%2520a%2520new%2520task%253A%25203D%2520Data-Efficient%250APoint-Language%2520Understanding.%2520The%2520goal%2520is%2520to%2520enable%2520LLMs%2520to%2520achieve%2520robust%25203D%250Aobject%2520understanding%2520with%2520minimal%25203D%2520point%2520cloud%2520and%2520text%2520data%2520pairs.%2520To%250Aaddress%2520this%2520task%252C%2520we%2520introduce%2520GreenPLM%252C%2520which%2520leverages%2520more%2520text%2520data%2520to%250Acompensate%2520for%2520the%2520lack%2520of%25203D%2520data.%2520First%252C%2520inspired%2520by%2520using%2520CLIP%2520to%2520align%250Aimages%2520and%2520text%252C%2520we%2520utilize%2520a%2520pre-trained%2520point%2520cloud-text%2520encoder%2520to%2520map%2520the%250A3D%2520point%2520cloud%2520space%2520to%2520the%2520text%2520space.%2520This%2520mapping%2520leaves%2520us%2520to%2520seamlessly%250Aconnect%2520the%2520text%2520space%2520with%2520LLMs.%2520Once%2520the%2520point-text-LLM%2520connection%2520is%250Aestablished%252C%2520we%2520further%2520enhance%2520text-LLM%2520alignment%2520by%2520expanding%2520the%250Aintermediate%2520text%2520space%252C%2520thereby%2520reducing%2520the%2520reliance%2520on%25203D%2520point%2520cloud%2520data.%250ASpecifically%252C%2520we%2520generate%25206M%2520free-text%2520descriptions%2520of%25203D%2520objects%252C%2520and%2520design%2520a%250Athree-stage%2520training%2520strategy%2520to%2520help%2520LLMs%2520better%2520explore%2520the%2520intrinsic%250Aconnections%2520between%2520different%2520modalities.%2520To%2520achieve%2520efficient%2520modality%250Aalignment%252C%2520we%2520design%2520a%2520zero-parameter%2520cross-attention%2520module%2520for%2520token%2520pooling.%250AExtensive%2520experimental%2520results%2520show%2520that%2520GreenPLM%2520requires%2520only%252012%2525%2520of%2520the%25203D%250Atraining%2520data%2520used%2520by%2520existing%2520state-of-the-art%2520models%2520to%2520achieve%2520superior%25203D%250Aunderstanding.%2520Remarkably%252C%2520GreenPLM%2520also%2520achieves%2520competitive%2520performance%2520using%250Atext-only%2520data.%2520The%2520code%2520and%2520weights%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/TangYuan96/GreenPLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Text%2C%20Less%20Point%3A%20Towards%203D%20Data-Efficient%20Point-Language%0A%20%20Understanding&entry.906535625=Yuan%20Tang%20and%20Xu%20Han%20and%20Xianzhi%20Li%20and%20Qiao%20Yu%20and%20Jinfeng%20Xu%20and%20Yixue%20Hao%20and%20Long%20Hu%20and%20Min%20Chen&entry.1292438233=%20%20Enabling%20Large%20Language%20Models%20%28LLMs%29%20to%20comprehend%20the%203D%20physical%20world%0Aremains%20a%20significant%20challenge.%20Due%20to%20the%20lack%20of%20large-scale%203D-text%20pair%0Adatasets%2C%20the%20success%20of%20LLMs%20has%20yet%20to%20be%20replicated%20in%203D%20understanding.%20In%0Athis%20paper%2C%20we%20rethink%20this%20issue%20and%20propose%20a%20new%20task%3A%203D%20Data-Efficient%0APoint-Language%20Understanding.%20The%20goal%20is%20to%20enable%20LLMs%20to%20achieve%20robust%203D%0Aobject%20understanding%20with%20minimal%203D%20point%20cloud%20and%20text%20data%20pairs.%20To%0Aaddress%20this%20task%2C%20we%20introduce%20GreenPLM%2C%20which%20leverages%20more%20text%20data%20to%0Acompensate%20for%20the%20lack%20of%203D%20data.%20First%2C%20inspired%20by%20using%20CLIP%20to%20align%0Aimages%20and%20text%2C%20we%20utilize%20a%20pre-trained%20point%20cloud-text%20encoder%20to%20map%20the%0A3D%20point%20cloud%20space%20to%20the%20text%20space.%20This%20mapping%20leaves%20us%20to%20seamlessly%0Aconnect%20the%20text%20space%20with%20LLMs.%20Once%20the%20point-text-LLM%20connection%20is%0Aestablished%2C%20we%20further%20enhance%20text-LLM%20alignment%20by%20expanding%20the%0Aintermediate%20text%20space%2C%20thereby%20reducing%20the%20reliance%20on%203D%20point%20cloud%20data.%0ASpecifically%2C%20we%20generate%206M%20free-text%20descriptions%20of%203D%20objects%2C%20and%20design%20a%0Athree-stage%20training%20strategy%20to%20help%20LLMs%20better%20explore%20the%20intrinsic%0Aconnections%20between%20different%20modalities.%20To%20achieve%20efficient%20modality%0Aalignment%2C%20we%20design%20a%20zero-parameter%20cross-attention%20module%20for%20token%20pooling.%0AExtensive%20experimental%20results%20show%20that%20GreenPLM%20requires%20only%2012%25%20of%20the%203D%0Atraining%20data%20used%20by%20existing%20state-of-the-art%20models%20to%20achieve%20superior%203D%0Aunderstanding.%20Remarkably%2C%20GreenPLM%20also%20achieves%20competitive%20performance%20using%0Atext-only%20data.%20The%20code%20and%20weights%20are%20available%20at%3A%0Ahttps%3A//github.com/TangYuan96/GreenPLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15966v3&entry.124074799=Read"},
{"title": "Top-Down Compression: Revisit Efficient Vision Token Projection for\n  Visual Instruction Tuning", "author": "Bonan li and Zicheng Zhang and Songhua Liu and Weihao Yu and Xinchao Wang", "abstract": "  Visual instruction tuning aims to enable large language models to comprehend\nthe visual world, with a pivotal challenge lying in establishing an effective\nvision-to-language projection. However, existing methods often grapple with the\nintractable trade-off between accuracy and efficiency. In this paper, we\npresent LLaVA-Meteor, a novel approach designed to break this deadlock,\nequipped with a novel Top-Down Compression paradigm that strategically\ncompresses visual tokens without compromising core information. Specifically,\nwe construct a trainable Flash Global Fusion module based on efficient\nselective state space operators, which aligns the feature space while enabling\neach token to perceive holistic visual context and instruction preference at\nlow cost. Furthermore, a local-to-single scanning manner is employed to\neffectively capture local dependencies, thereby enhancing the model's\ncapability in vision modeling. To alleviate computational overhead, we explore\na Visual-Native Selection mechanism that independently assesses token\nsignificance by both the visual and native experts, followed by aggregation to\nretain the most critical subset. Extensive experiments show that our approach\nreduces visual tokens by 75--95% while achieving comparable or superior\nperformance across 12 benchmarks, significantly improving efficiency.\n", "link": "http://arxiv.org/abs/2505.11945v2", "date": "2025-05-22", "relevancy": 2.9716, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Top-Down%20Compression%3A%20Revisit%20Efficient%20Vision%20Token%20Projection%20for%0A%20%20Visual%20Instruction%20Tuning&body=Title%3A%20Top-Down%20Compression%3A%20Revisit%20Efficient%20Vision%20Token%20Projection%20for%0A%20%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Bonan%20li%20and%20Zicheng%20Zhang%20and%20Songhua%20Liu%20and%20Weihao%20Yu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Visual%20instruction%20tuning%20aims%20to%20enable%20large%20language%20models%20to%20comprehend%0Athe%20visual%20world%2C%20with%20a%20pivotal%20challenge%20lying%20in%20establishing%20an%20effective%0Avision-to-language%20projection.%20However%2C%20existing%20methods%20often%20grapple%20with%20the%0Aintractable%20trade-off%20between%20accuracy%20and%20efficiency.%20In%20this%20paper%2C%20we%0Apresent%20LLaVA-Meteor%2C%20a%20novel%20approach%20designed%20to%20break%20this%20deadlock%2C%0Aequipped%20with%20a%20novel%20Top-Down%20Compression%20paradigm%20that%20strategically%0Acompresses%20visual%20tokens%20without%20compromising%20core%20information.%20Specifically%2C%0Awe%20construct%20a%20trainable%20Flash%20Global%20Fusion%20module%20based%20on%20efficient%0Aselective%20state%20space%20operators%2C%20which%20aligns%20the%20feature%20space%20while%20enabling%0Aeach%20token%20to%20perceive%20holistic%20visual%20context%20and%20instruction%20preference%20at%0Alow%20cost.%20Furthermore%2C%20a%20local-to-single%20scanning%20manner%20is%20employed%20to%0Aeffectively%20capture%20local%20dependencies%2C%20thereby%20enhancing%20the%20model%27s%0Acapability%20in%20vision%20modeling.%20To%20alleviate%20computational%20overhead%2C%20we%20explore%0Aa%20Visual-Native%20Selection%20mechanism%20that%20independently%20assesses%20token%0Asignificance%20by%20both%20the%20visual%20and%20native%20experts%2C%20followed%20by%20aggregation%20to%0Aretain%20the%20most%20critical%20subset.%20Extensive%20experiments%20show%20that%20our%20approach%0Areduces%20visual%20tokens%20by%2075--95%25%20while%20achieving%20comparable%20or%20superior%0Aperformance%20across%2012%20benchmarks%2C%20significantly%20improving%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTop-Down%2520Compression%253A%2520Revisit%2520Efficient%2520Vision%2520Token%2520Projection%2520for%250A%2520%2520Visual%2520Instruction%2520Tuning%26entry.906535625%3DBonan%2520li%2520and%2520Zicheng%2520Zhang%2520and%2520Songhua%2520Liu%2520and%2520Weihao%2520Yu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520instruction%2520tuning%2520aims%2520to%2520enable%2520large%2520language%2520models%2520to%2520comprehend%250Athe%2520visual%2520world%252C%2520with%2520a%2520pivotal%2520challenge%2520lying%2520in%2520establishing%2520an%2520effective%250Avision-to-language%2520projection.%2520However%252C%2520existing%2520methods%2520often%2520grapple%2520with%2520the%250Aintractable%2520trade-off%2520between%2520accuracy%2520and%2520efficiency.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520LLaVA-Meteor%252C%2520a%2520novel%2520approach%2520designed%2520to%2520break%2520this%2520deadlock%252C%250Aequipped%2520with%2520a%2520novel%2520Top-Down%2520Compression%2520paradigm%2520that%2520strategically%250Acompresses%2520visual%2520tokens%2520without%2520compromising%2520core%2520information.%2520Specifically%252C%250Awe%2520construct%2520a%2520trainable%2520Flash%2520Global%2520Fusion%2520module%2520based%2520on%2520efficient%250Aselective%2520state%2520space%2520operators%252C%2520which%2520aligns%2520the%2520feature%2520space%2520while%2520enabling%250Aeach%2520token%2520to%2520perceive%2520holistic%2520visual%2520context%2520and%2520instruction%2520preference%2520at%250Alow%2520cost.%2520Furthermore%252C%2520a%2520local-to-single%2520scanning%2520manner%2520is%2520employed%2520to%250Aeffectively%2520capture%2520local%2520dependencies%252C%2520thereby%2520enhancing%2520the%2520model%2527s%250Acapability%2520in%2520vision%2520modeling.%2520To%2520alleviate%2520computational%2520overhead%252C%2520we%2520explore%250Aa%2520Visual-Native%2520Selection%2520mechanism%2520that%2520independently%2520assesses%2520token%250Asignificance%2520by%2520both%2520the%2520visual%2520and%2520native%2520experts%252C%2520followed%2520by%2520aggregation%2520to%250Aretain%2520the%2520most%2520critical%2520subset.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%250Areduces%2520visual%2520tokens%2520by%252075--95%2525%2520while%2520achieving%2520comparable%2520or%2520superior%250Aperformance%2520across%252012%2520benchmarks%252C%2520significantly%2520improving%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Top-Down%20Compression%3A%20Revisit%20Efficient%20Vision%20Token%20Projection%20for%0A%20%20Visual%20Instruction%20Tuning&entry.906535625=Bonan%20li%20and%20Zicheng%20Zhang%20and%20Songhua%20Liu%20and%20Weihao%20Yu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Visual%20instruction%20tuning%20aims%20to%20enable%20large%20language%20models%20to%20comprehend%0Athe%20visual%20world%2C%20with%20a%20pivotal%20challenge%20lying%20in%20establishing%20an%20effective%0Avision-to-language%20projection.%20However%2C%20existing%20methods%20often%20grapple%20with%20the%0Aintractable%20trade-off%20between%20accuracy%20and%20efficiency.%20In%20this%20paper%2C%20we%0Apresent%20LLaVA-Meteor%2C%20a%20novel%20approach%20designed%20to%20break%20this%20deadlock%2C%0Aequipped%20with%20a%20novel%20Top-Down%20Compression%20paradigm%20that%20strategically%0Acompresses%20visual%20tokens%20without%20compromising%20core%20information.%20Specifically%2C%0Awe%20construct%20a%20trainable%20Flash%20Global%20Fusion%20module%20based%20on%20efficient%0Aselective%20state%20space%20operators%2C%20which%20aligns%20the%20feature%20space%20while%20enabling%0Aeach%20token%20to%20perceive%20holistic%20visual%20context%20and%20instruction%20preference%20at%0Alow%20cost.%20Furthermore%2C%20a%20local-to-single%20scanning%20manner%20is%20employed%20to%0Aeffectively%20capture%20local%20dependencies%2C%20thereby%20enhancing%20the%20model%27s%0Acapability%20in%20vision%20modeling.%20To%20alleviate%20computational%20overhead%2C%20we%20explore%0Aa%20Visual-Native%20Selection%20mechanism%20that%20independently%20assesses%20token%0Asignificance%20by%20both%20the%20visual%20and%20native%20experts%2C%20followed%20by%20aggregation%20to%0Aretain%20the%20most%20critical%20subset.%20Extensive%20experiments%20show%20that%20our%20approach%0Areduces%20visual%20tokens%20by%2075--95%25%20while%20achieving%20comparable%20or%20superior%0Aperformance%20across%2012%20benchmarks%2C%20significantly%20improving%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11945v2&entry.124074799=Read"},
{"title": "SEDD-PCC: A Single Encoder-Dual Decoder Framework For End-To-End Learned\n  Point Cloud Compression", "author": "Kai Hsiang Hsieh and Monyneath Yim and Jui Chiu Chiang", "abstract": "  To encode point clouds containing both geometry and attributes, most\nlearning-based compression schemes treat geometry and attribute coding\nseparately, employing distinct encoders and decoders. This not only increases\ncomputational complexity but also fails to fully exploit shared features\nbetween geometry and attributes. To address this limitation, we propose\nSEDD-PCC, an end-to-end learning-based framework for lossy point cloud\ncompression that jointly compresses geometry and attributes. SEDD-PCC employs a\nsingle encoder to extract shared geometric and attribute features into a\nunified latent space, followed by dual specialized decoders that sequentially\nreconstruct geometry and attributes. Additionally, we incorporate knowledge\ndistillation to enhance feature representation learning from a teacher model,\nfurther improving coding efficiency. With its simple yet effective design,\nSEDD-PCC provides an efficient and practical solution for point cloud\ncompression. Comparative evaluations against both rule-based and learning-based\nmethods demonstrate its competitive performance, highlighting SEDD-PCC as a\npromising AI-driven compression approach.\n", "link": "http://arxiv.org/abs/2505.16709v1", "date": "2025-05-22", "relevancy": 2.9354, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6476}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5665}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEDD-PCC%3A%20A%20Single%20Encoder-Dual%20Decoder%20Framework%20For%20End-To-End%20Learned%0A%20%20Point%20Cloud%20Compression&body=Title%3A%20SEDD-PCC%3A%20A%20Single%20Encoder-Dual%20Decoder%20Framework%20For%20End-To-End%20Learned%0A%20%20Point%20Cloud%20Compression%0AAuthor%3A%20Kai%20Hsiang%20Hsieh%20and%20Monyneath%20Yim%20and%20Jui%20Chiu%20Chiang%0AAbstract%3A%20%20%20To%20encode%20point%20clouds%20containing%20both%20geometry%20and%20attributes%2C%20most%0Alearning-based%20compression%20schemes%20treat%20geometry%20and%20attribute%20coding%0Aseparately%2C%20employing%20distinct%20encoders%20and%20decoders.%20This%20not%20only%20increases%0Acomputational%20complexity%20but%20also%20fails%20to%20fully%20exploit%20shared%20features%0Abetween%20geometry%20and%20attributes.%20To%20address%20this%20limitation%2C%20we%20propose%0ASEDD-PCC%2C%20an%20end-to-end%20learning-based%20framework%20for%20lossy%20point%20cloud%0Acompression%20that%20jointly%20compresses%20geometry%20and%20attributes.%20SEDD-PCC%20employs%20a%0Asingle%20encoder%20to%20extract%20shared%20geometric%20and%20attribute%20features%20into%20a%0Aunified%20latent%20space%2C%20followed%20by%20dual%20specialized%20decoders%20that%20sequentially%0Areconstruct%20geometry%20and%20attributes.%20Additionally%2C%20we%20incorporate%20knowledge%0Adistillation%20to%20enhance%20feature%20representation%20learning%20from%20a%20teacher%20model%2C%0Afurther%20improving%20coding%20efficiency.%20With%20its%20simple%20yet%20effective%20design%2C%0ASEDD-PCC%20provides%20an%20efficient%20and%20practical%20solution%20for%20point%20cloud%0Acompression.%20Comparative%20evaluations%20against%20both%20rule-based%20and%20learning-based%0Amethods%20demonstrate%20its%20competitive%20performance%2C%20highlighting%20SEDD-PCC%20as%20a%0Apromising%20AI-driven%20compression%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEDD-PCC%253A%2520A%2520Single%2520Encoder-Dual%2520Decoder%2520Framework%2520For%2520End-To-End%2520Learned%250A%2520%2520Point%2520Cloud%2520Compression%26entry.906535625%3DKai%2520Hsiang%2520Hsieh%2520and%2520Monyneath%2520Yim%2520and%2520Jui%2520Chiu%2520Chiang%26entry.1292438233%3D%2520%2520To%2520encode%2520point%2520clouds%2520containing%2520both%2520geometry%2520and%2520attributes%252C%2520most%250Alearning-based%2520compression%2520schemes%2520treat%2520geometry%2520and%2520attribute%2520coding%250Aseparately%252C%2520employing%2520distinct%2520encoders%2520and%2520decoders.%2520This%2520not%2520only%2520increases%250Acomputational%2520complexity%2520but%2520also%2520fails%2520to%2520fully%2520exploit%2520shared%2520features%250Abetween%2520geometry%2520and%2520attributes.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250ASEDD-PCC%252C%2520an%2520end-to-end%2520learning-based%2520framework%2520for%2520lossy%2520point%2520cloud%250Acompression%2520that%2520jointly%2520compresses%2520geometry%2520and%2520attributes.%2520SEDD-PCC%2520employs%2520a%250Asingle%2520encoder%2520to%2520extract%2520shared%2520geometric%2520and%2520attribute%2520features%2520into%2520a%250Aunified%2520latent%2520space%252C%2520followed%2520by%2520dual%2520specialized%2520decoders%2520that%2520sequentially%250Areconstruct%2520geometry%2520and%2520attributes.%2520Additionally%252C%2520we%2520incorporate%2520knowledge%250Adistillation%2520to%2520enhance%2520feature%2520representation%2520learning%2520from%2520a%2520teacher%2520model%252C%250Afurther%2520improving%2520coding%2520efficiency.%2520With%2520its%2520simple%2520yet%2520effective%2520design%252C%250ASEDD-PCC%2520provides%2520an%2520efficient%2520and%2520practical%2520solution%2520for%2520point%2520cloud%250Acompression.%2520Comparative%2520evaluations%2520against%2520both%2520rule-based%2520and%2520learning-based%250Amethods%2520demonstrate%2520its%2520competitive%2520performance%252C%2520highlighting%2520SEDD-PCC%2520as%2520a%250Apromising%2520AI-driven%2520compression%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEDD-PCC%3A%20A%20Single%20Encoder-Dual%20Decoder%20Framework%20For%20End-To-End%20Learned%0A%20%20Point%20Cloud%20Compression&entry.906535625=Kai%20Hsiang%20Hsieh%20and%20Monyneath%20Yim%20and%20Jui%20Chiu%20Chiang&entry.1292438233=%20%20To%20encode%20point%20clouds%20containing%20both%20geometry%20and%20attributes%2C%20most%0Alearning-based%20compression%20schemes%20treat%20geometry%20and%20attribute%20coding%0Aseparately%2C%20employing%20distinct%20encoders%20and%20decoders.%20This%20not%20only%20increases%0Acomputational%20complexity%20but%20also%20fails%20to%20fully%20exploit%20shared%20features%0Abetween%20geometry%20and%20attributes.%20To%20address%20this%20limitation%2C%20we%20propose%0ASEDD-PCC%2C%20an%20end-to-end%20learning-based%20framework%20for%20lossy%20point%20cloud%0Acompression%20that%20jointly%20compresses%20geometry%20and%20attributes.%20SEDD-PCC%20employs%20a%0Asingle%20encoder%20to%20extract%20shared%20geometric%20and%20attribute%20features%20into%20a%0Aunified%20latent%20space%2C%20followed%20by%20dual%20specialized%20decoders%20that%20sequentially%0Areconstruct%20geometry%20and%20attributes.%20Additionally%2C%20we%20incorporate%20knowledge%0Adistillation%20to%20enhance%20feature%20representation%20learning%20from%20a%20teacher%20model%2C%0Afurther%20improving%20coding%20efficiency.%20With%20its%20simple%20yet%20effective%20design%2C%0ASEDD-PCC%20provides%20an%20efficient%20and%20practical%20solution%20for%20point%20cloud%0Acompression.%20Comparative%20evaluations%20against%20both%20rule-based%20and%20learning-based%0Amethods%20demonstrate%20its%20competitive%20performance%2C%20highlighting%20SEDD-PCC%20as%20a%0Apromising%20AI-driven%20compression%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16709v1&entry.124074799=Read"},
{"title": "Remote Sensing Spatio-Temporal Vision-Language Models: A Comprehensive\n  Survey", "author": "Chenyang Liu and Jiafan Zhang and Keyan Chen and Man Wang and Zhengxia Zou and Zhenwei Shi", "abstract": "  The interpretation of multi-temporal remote sensing imagery is critical for\nmonitoring Earth's dynamic processes-yet previous change detection methods,\nwhich produce binary or semantic masks, fall short of providing human-readable\ninsights into changes. Recent advances in Vision-Language Models (VLMs) have\nopened a new frontier by fusing visual and linguistic modalities, enabling\nspatio-temporal vision-language understanding: models that not only capture\nspatial and temporal dependencies to recognize changes but also provide a\nricher interactive semantic analysis of temporal images (e.g., generate\ndescriptive captions and answer natural-language queries). In this survey, we\npresent the first comprehensive review of RS-STVLMs. The survey covers the\nevolution of models from early task-specific models to recent general\nfoundation models that leverage powerful large language models. We discuss\nprogress in representative tasks, such as change captioning, change question\nanswering, and change grounding. Moreover, we systematically dissect the\nfundamental components and key technologies underlying these models, and review\nthe datasets and evaluation metrics that have driven the field. By synthesizing\ntask-level insights with a deep dive into shared architectural patterns, we aim\nto illuminate current achievements and chart promising directions for future\nresearch in spatio-temporal vision-language understanding for remote sensing.\nWe will keep tracing related works at\nhttps://github.com/Chen-Yang-Liu/Awesome-RS-SpatioTemporal-VLMs\n", "link": "http://arxiv.org/abs/2412.02573v2", "date": "2025-05-22", "relevancy": 2.9228, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Remote%20Sensing%20Spatio-Temporal%20Vision-Language%20Models%3A%20A%20Comprehensive%0A%20%20Survey&body=Title%3A%20Remote%20Sensing%20Spatio-Temporal%20Vision-Language%20Models%3A%20A%20Comprehensive%0A%20%20Survey%0AAuthor%3A%20Chenyang%20Liu%20and%20Jiafan%20Zhang%20and%20Keyan%20Chen%20and%20Man%20Wang%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20The%20interpretation%20of%20multi-temporal%20remote%20sensing%20imagery%20is%20critical%20for%0Amonitoring%20Earth%27s%20dynamic%20processes-yet%20previous%20change%20detection%20methods%2C%0Awhich%20produce%20binary%20or%20semantic%20masks%2C%20fall%20short%20of%20providing%20human-readable%0Ainsights%20into%20changes.%20Recent%20advances%20in%20Vision-Language%20Models%20%28VLMs%29%20have%0Aopened%20a%20new%20frontier%20by%20fusing%20visual%20and%20linguistic%20modalities%2C%20enabling%0Aspatio-temporal%20vision-language%20understanding%3A%20models%20that%20not%20only%20capture%0Aspatial%20and%20temporal%20dependencies%20to%20recognize%20changes%20but%20also%20provide%20a%0Aricher%20interactive%20semantic%20analysis%20of%20temporal%20images%20%28e.g.%2C%20generate%0Adescriptive%20captions%20and%20answer%20natural-language%20queries%29.%20In%20this%20survey%2C%20we%0Apresent%20the%20first%20comprehensive%20review%20of%20RS-STVLMs.%20The%20survey%20covers%20the%0Aevolution%20of%20models%20from%20early%20task-specific%20models%20to%20recent%20general%0Afoundation%20models%20that%20leverage%20powerful%20large%20language%20models.%20We%20discuss%0Aprogress%20in%20representative%20tasks%2C%20such%20as%20change%20captioning%2C%20change%20question%0Aanswering%2C%20and%20change%20grounding.%20Moreover%2C%20we%20systematically%20dissect%20the%0Afundamental%20components%20and%20key%20technologies%20underlying%20these%20models%2C%20and%20review%0Athe%20datasets%20and%20evaluation%20metrics%20that%20have%20driven%20the%20field.%20By%20synthesizing%0Atask-level%20insights%20with%20a%20deep%20dive%20into%20shared%20architectural%20patterns%2C%20we%20aim%0Ato%20illuminate%20current%20achievements%20and%20chart%20promising%20directions%20for%20future%0Aresearch%20in%20spatio-temporal%20vision-language%20understanding%20for%20remote%20sensing.%0AWe%20will%20keep%20tracing%20related%20works%20at%0Ahttps%3A//github.com/Chen-Yang-Liu/Awesome-RS-SpatioTemporal-VLMs%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemote%2520Sensing%2520Spatio-Temporal%2520Vision-Language%2520Models%253A%2520A%2520Comprehensive%250A%2520%2520Survey%26entry.906535625%3DChenyang%2520Liu%2520and%2520Jiafan%2520Zhang%2520and%2520Keyan%2520Chen%2520and%2520Man%2520Wang%2520and%2520Zhengxia%2520Zou%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3D%2520%2520The%2520interpretation%2520of%2520multi-temporal%2520remote%2520sensing%2520imagery%2520is%2520critical%2520for%250Amonitoring%2520Earth%2527s%2520dynamic%2520processes-yet%2520previous%2520change%2520detection%2520methods%252C%250Awhich%2520produce%2520binary%2520or%2520semantic%2520masks%252C%2520fall%2520short%2520of%2520providing%2520human-readable%250Ainsights%2520into%2520changes.%2520Recent%2520advances%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%250Aopened%2520a%2520new%2520frontier%2520by%2520fusing%2520visual%2520and%2520linguistic%2520modalities%252C%2520enabling%250Aspatio-temporal%2520vision-language%2520understanding%253A%2520models%2520that%2520not%2520only%2520capture%250Aspatial%2520and%2520temporal%2520dependencies%2520to%2520recognize%2520changes%2520but%2520also%2520provide%2520a%250Aricher%2520interactive%2520semantic%2520analysis%2520of%2520temporal%2520images%2520%2528e.g.%252C%2520generate%250Adescriptive%2520captions%2520and%2520answer%2520natural-language%2520queries%2529.%2520In%2520this%2520survey%252C%2520we%250Apresent%2520the%2520first%2520comprehensive%2520review%2520of%2520RS-STVLMs.%2520The%2520survey%2520covers%2520the%250Aevolution%2520of%2520models%2520from%2520early%2520task-specific%2520models%2520to%2520recent%2520general%250Afoundation%2520models%2520that%2520leverage%2520powerful%2520large%2520language%2520models.%2520We%2520discuss%250Aprogress%2520in%2520representative%2520tasks%252C%2520such%2520as%2520change%2520captioning%252C%2520change%2520question%250Aanswering%252C%2520and%2520change%2520grounding.%2520Moreover%252C%2520we%2520systematically%2520dissect%2520the%250Afundamental%2520components%2520and%2520key%2520technologies%2520underlying%2520these%2520models%252C%2520and%2520review%250Athe%2520datasets%2520and%2520evaluation%2520metrics%2520that%2520have%2520driven%2520the%2520field.%2520By%2520synthesizing%250Atask-level%2520insights%2520with%2520a%2520deep%2520dive%2520into%2520shared%2520architectural%2520patterns%252C%2520we%2520aim%250Ato%2520illuminate%2520current%2520achievements%2520and%2520chart%2520promising%2520directions%2520for%2520future%250Aresearch%2520in%2520spatio-temporal%2520vision-language%2520understanding%2520for%2520remote%2520sensing.%250AWe%2520will%2520keep%2520tracing%2520related%2520works%2520at%250Ahttps%253A//github.com/Chen-Yang-Liu/Awesome-RS-SpatioTemporal-VLMs%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Remote%20Sensing%20Spatio-Temporal%20Vision-Language%20Models%3A%20A%20Comprehensive%0A%20%20Survey&entry.906535625=Chenyang%20Liu%20and%20Jiafan%20Zhang%20and%20Keyan%20Chen%20and%20Man%20Wang%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi&entry.1292438233=%20%20The%20interpretation%20of%20multi-temporal%20remote%20sensing%20imagery%20is%20critical%20for%0Amonitoring%20Earth%27s%20dynamic%20processes-yet%20previous%20change%20detection%20methods%2C%0Awhich%20produce%20binary%20or%20semantic%20masks%2C%20fall%20short%20of%20providing%20human-readable%0Ainsights%20into%20changes.%20Recent%20advances%20in%20Vision-Language%20Models%20%28VLMs%29%20have%0Aopened%20a%20new%20frontier%20by%20fusing%20visual%20and%20linguistic%20modalities%2C%20enabling%0Aspatio-temporal%20vision-language%20understanding%3A%20models%20that%20not%20only%20capture%0Aspatial%20and%20temporal%20dependencies%20to%20recognize%20changes%20but%20also%20provide%20a%0Aricher%20interactive%20semantic%20analysis%20of%20temporal%20images%20%28e.g.%2C%20generate%0Adescriptive%20captions%20and%20answer%20natural-language%20queries%29.%20In%20this%20survey%2C%20we%0Apresent%20the%20first%20comprehensive%20review%20of%20RS-STVLMs.%20The%20survey%20covers%20the%0Aevolution%20of%20models%20from%20early%20task-specific%20models%20to%20recent%20general%0Afoundation%20models%20that%20leverage%20powerful%20large%20language%20models.%20We%20discuss%0Aprogress%20in%20representative%20tasks%2C%20such%20as%20change%20captioning%2C%20change%20question%0Aanswering%2C%20and%20change%20grounding.%20Moreover%2C%20we%20systematically%20dissect%20the%0Afundamental%20components%20and%20key%20technologies%20underlying%20these%20models%2C%20and%20review%0Athe%20datasets%20and%20evaluation%20metrics%20that%20have%20driven%20the%20field.%20By%20synthesizing%0Atask-level%20insights%20with%20a%20deep%20dive%20into%20shared%20architectural%20patterns%2C%20we%20aim%0Ato%20illuminate%20current%20achievements%20and%20chart%20promising%20directions%20for%20future%0Aresearch%20in%20spatio-temporal%20vision-language%20understanding%20for%20remote%20sensing.%0AWe%20will%20keep%20tracing%20related%20works%20at%0Ahttps%3A//github.com/Chen-Yang-Liu/Awesome-RS-SpatioTemporal-VLMs%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02573v2&entry.124074799=Read"},
{"title": "SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous\n  Driving", "author": "Xuesong Chen and Linjiang Huang and Tao Ma and Rongyao Fang and Shaoshuai Shi and Hongsheng Li", "abstract": "  The integration of Vision-Language Models (VLMs) into autonomous driving\nsystems has shown promise in addressing key challenges such as learning\ncomplexity, interpretability, and common-sense reasoning. However, existing\napproaches often struggle with efficient integration and realtime\ndecision-making due to computational demands. In this paper, we introduce\nSOLVE, an innovative framework that synergizes VLMs with end-to-end (E2E)\nmodels to enhance autonomous vehicle planning. Our approach emphasizes\nknowledge sharing at the feature level through a shared visual encoder,\nenabling comprehensive interaction between VLM and E2E components. We propose a\nTrajectory Chain-of-Thought (T-CoT) paradigm, which progressively refines\ntrajectory predictions, reducing uncertainty and improving accuracy. By\nemploying a temporal decoupling strategy, SOLVE achieves efficient cooperation\nby aligning high-quality VLM outputs with E2E real-time performance. Evaluated\non the nuScenes dataset, our method demonstrates significant improvements in\ntrajectory prediction accuracy, paving the way for more robust and reliable\nautonomous driving systems.\n", "link": "http://arxiv.org/abs/2505.16805v1", "date": "2025-05-22", "relevancy": 2.9083, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOLVE%3A%20Synergy%20of%20Language-Vision%20and%20End-to-End%20Networks%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20SOLVE%3A%20Synergy%20of%20Language-Vision%20and%20End-to-End%20Networks%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Xuesong%20Chen%20and%20Linjiang%20Huang%20and%20Tao%20Ma%20and%20Rongyao%20Fang%20and%20Shaoshuai%20Shi%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20The%20integration%20of%20Vision-Language%20Models%20%28VLMs%29%20into%20autonomous%20driving%0Asystems%20has%20shown%20promise%20in%20addressing%20key%20challenges%20such%20as%20learning%0Acomplexity%2C%20interpretability%2C%20and%20common-sense%20reasoning.%20However%2C%20existing%0Aapproaches%20often%20struggle%20with%20efficient%20integration%20and%20realtime%0Adecision-making%20due%20to%20computational%20demands.%20In%20this%20paper%2C%20we%20introduce%0ASOLVE%2C%20an%20innovative%20framework%20that%20synergizes%20VLMs%20with%20end-to-end%20%28E2E%29%0Amodels%20to%20enhance%20autonomous%20vehicle%20planning.%20Our%20approach%20emphasizes%0Aknowledge%20sharing%20at%20the%20feature%20level%20through%20a%20shared%20visual%20encoder%2C%0Aenabling%20comprehensive%20interaction%20between%20VLM%20and%20E2E%20components.%20We%20propose%20a%0ATrajectory%20Chain-of-Thought%20%28T-CoT%29%20paradigm%2C%20which%20progressively%20refines%0Atrajectory%20predictions%2C%20reducing%20uncertainty%20and%20improving%20accuracy.%20By%0Aemploying%20a%20temporal%20decoupling%20strategy%2C%20SOLVE%20achieves%20efficient%20cooperation%0Aby%20aligning%20high-quality%20VLM%20outputs%20with%20E2E%20real-time%20performance.%20Evaluated%0Aon%20the%20nuScenes%20dataset%2C%20our%20method%20demonstrates%20significant%20improvements%20in%0Atrajectory%20prediction%20accuracy%2C%20paving%20the%20way%20for%20more%20robust%20and%20reliable%0Aautonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOLVE%253A%2520Synergy%2520of%2520Language-Vision%2520and%2520End-to-End%2520Networks%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DXuesong%2520Chen%2520and%2520Linjiang%2520Huang%2520and%2520Tao%2520Ma%2520and%2520Rongyao%2520Fang%2520and%2520Shaoshuai%2520Shi%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520into%2520autonomous%2520driving%250Asystems%2520has%2520shown%2520promise%2520in%2520addressing%2520key%2520challenges%2520such%2520as%2520learning%250Acomplexity%252C%2520interpretability%252C%2520and%2520common-sense%2520reasoning.%2520However%252C%2520existing%250Aapproaches%2520often%2520struggle%2520with%2520efficient%2520integration%2520and%2520realtime%250Adecision-making%2520due%2520to%2520computational%2520demands.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ASOLVE%252C%2520an%2520innovative%2520framework%2520that%2520synergizes%2520VLMs%2520with%2520end-to-end%2520%2528E2E%2529%250Amodels%2520to%2520enhance%2520autonomous%2520vehicle%2520planning.%2520Our%2520approach%2520emphasizes%250Aknowledge%2520sharing%2520at%2520the%2520feature%2520level%2520through%2520a%2520shared%2520visual%2520encoder%252C%250Aenabling%2520comprehensive%2520interaction%2520between%2520VLM%2520and%2520E2E%2520components.%2520We%2520propose%2520a%250ATrajectory%2520Chain-of-Thought%2520%2528T-CoT%2529%2520paradigm%252C%2520which%2520progressively%2520refines%250Atrajectory%2520predictions%252C%2520reducing%2520uncertainty%2520and%2520improving%2520accuracy.%2520By%250Aemploying%2520a%2520temporal%2520decoupling%2520strategy%252C%2520SOLVE%2520achieves%2520efficient%2520cooperation%250Aby%2520aligning%2520high-quality%2520VLM%2520outputs%2520with%2520E2E%2520real-time%2520performance.%2520Evaluated%250Aon%2520the%2520nuScenes%2520dataset%252C%2520our%2520method%2520demonstrates%2520significant%2520improvements%2520in%250Atrajectory%2520prediction%2520accuracy%252C%2520paving%2520the%2520way%2520for%2520more%2520robust%2520and%2520reliable%250Aautonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOLVE%3A%20Synergy%20of%20Language-Vision%20and%20End-to-End%20Networks%20for%20Autonomous%0A%20%20Driving&entry.906535625=Xuesong%20Chen%20and%20Linjiang%20Huang%20and%20Tao%20Ma%20and%20Rongyao%20Fang%20and%20Shaoshuai%20Shi%20and%20Hongsheng%20Li&entry.1292438233=%20%20The%20integration%20of%20Vision-Language%20Models%20%28VLMs%29%20into%20autonomous%20driving%0Asystems%20has%20shown%20promise%20in%20addressing%20key%20challenges%20such%20as%20learning%0Acomplexity%2C%20interpretability%2C%20and%20common-sense%20reasoning.%20However%2C%20existing%0Aapproaches%20often%20struggle%20with%20efficient%20integration%20and%20realtime%0Adecision-making%20due%20to%20computational%20demands.%20In%20this%20paper%2C%20we%20introduce%0ASOLVE%2C%20an%20innovative%20framework%20that%20synergizes%20VLMs%20with%20end-to-end%20%28E2E%29%0Amodels%20to%20enhance%20autonomous%20vehicle%20planning.%20Our%20approach%20emphasizes%0Aknowledge%20sharing%20at%20the%20feature%20level%20through%20a%20shared%20visual%20encoder%2C%0Aenabling%20comprehensive%20interaction%20between%20VLM%20and%20E2E%20components.%20We%20propose%20a%0ATrajectory%20Chain-of-Thought%20%28T-CoT%29%20paradigm%2C%20which%20progressively%20refines%0Atrajectory%20predictions%2C%20reducing%20uncertainty%20and%20improving%20accuracy.%20By%0Aemploying%20a%20temporal%20decoupling%20strategy%2C%20SOLVE%20achieves%20efficient%20cooperation%0Aby%20aligning%20high-quality%20VLM%20outputs%20with%20E2E%20real-time%20performance.%20Evaluated%0Aon%20the%20nuScenes%20dataset%2C%20our%20method%20demonstrates%20significant%20improvements%20in%0Atrajectory%20prediction%20accuracy%2C%20paving%20the%20way%20for%20more%20robust%20and%20reliable%0Aautonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16805v1&entry.124074799=Read"},
{"title": "Temporal Object Captioning for Street Scene Videos from LiDAR Tracks", "author": "Vignesh Gopinathan and Urs Zimmermann and Michael Arnold and Matthias Rottmann", "abstract": "  Video captioning models have seen notable advancements in recent years,\nespecially with regard to their ability to capture temporal information. While\nmany research efforts have focused on architectural advancements, such as\ntemporal attention mechanisms, there remains a notable gap in understanding how\nmodels capture and utilize temporal semantics for effective temporal feature\nextraction, especially in the context of Advanced Driver Assistance Systems. We\npropose an automated LiDAR-based captioning procedure that focuses on the\ntemporal dynamics of traffic participants. Our approach uses a rule-based\nsystem to extract essential details such as lane position and relative motion\nfrom object tracks, followed by a template-based caption generation. Our\nfindings show that training SwinBERT, a video captioning model, using only\nfront camera images and supervised with our template-based captions,\nspecifically designed to encapsulate fine-grained temporal behavior, leads to\nimproved temporal understanding consistently across three datasets. In\nconclusion, our results clearly demonstrate that integrating LiDAR-based\ncaption supervision significantly enhances temporal understanding, effectively\naddressing and reducing the inherent visual/static biases prevalent in current\nstate-of-the-art model architectures.\n", "link": "http://arxiv.org/abs/2505.16594v1", "date": "2025-05-22", "relevancy": 2.8616, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Object%20Captioning%20for%20Street%20Scene%20Videos%20from%20LiDAR%20Tracks&body=Title%3A%20Temporal%20Object%20Captioning%20for%20Street%20Scene%20Videos%20from%20LiDAR%20Tracks%0AAuthor%3A%20Vignesh%20Gopinathan%20and%20Urs%20Zimmermann%20and%20Michael%20Arnold%20and%20Matthias%20Rottmann%0AAbstract%3A%20%20%20Video%20captioning%20models%20have%20seen%20notable%20advancements%20in%20recent%20years%2C%0Aespecially%20with%20regard%20to%20their%20ability%20to%20capture%20temporal%20information.%20While%0Amany%20research%20efforts%20have%20focused%20on%20architectural%20advancements%2C%20such%20as%0Atemporal%20attention%20mechanisms%2C%20there%20remains%20a%20notable%20gap%20in%20understanding%20how%0Amodels%20capture%20and%20utilize%20temporal%20semantics%20for%20effective%20temporal%20feature%0Aextraction%2C%20especially%20in%20the%20context%20of%20Advanced%20Driver%20Assistance%20Systems.%20We%0Apropose%20an%20automated%20LiDAR-based%20captioning%20procedure%20that%20focuses%20on%20the%0Atemporal%20dynamics%20of%20traffic%20participants.%20Our%20approach%20uses%20a%20rule-based%0Asystem%20to%20extract%20essential%20details%20such%20as%20lane%20position%20and%20relative%20motion%0Afrom%20object%20tracks%2C%20followed%20by%20a%20template-based%20caption%20generation.%20Our%0Afindings%20show%20that%20training%20SwinBERT%2C%20a%20video%20captioning%20model%2C%20using%20only%0Afront%20camera%20images%20and%20supervised%20with%20our%20template-based%20captions%2C%0Aspecifically%20designed%20to%20encapsulate%20fine-grained%20temporal%20behavior%2C%20leads%20to%0Aimproved%20temporal%20understanding%20consistently%20across%20three%20datasets.%20In%0Aconclusion%2C%20our%20results%20clearly%20demonstrate%20that%20integrating%20LiDAR-based%0Acaption%20supervision%20significantly%20enhances%20temporal%20understanding%2C%20effectively%0Aaddressing%20and%20reducing%20the%20inherent%20visual/static%20biases%20prevalent%20in%20current%0Astate-of-the-art%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Object%2520Captioning%2520for%2520Street%2520Scene%2520Videos%2520from%2520LiDAR%2520Tracks%26entry.906535625%3DVignesh%2520Gopinathan%2520and%2520Urs%2520Zimmermann%2520and%2520Michael%2520Arnold%2520and%2520Matthias%2520Rottmann%26entry.1292438233%3D%2520%2520Video%2520captioning%2520models%2520have%2520seen%2520notable%2520advancements%2520in%2520recent%2520years%252C%250Aespecially%2520with%2520regard%2520to%2520their%2520ability%2520to%2520capture%2520temporal%2520information.%2520While%250Amany%2520research%2520efforts%2520have%2520focused%2520on%2520architectural%2520advancements%252C%2520such%2520as%250Atemporal%2520attention%2520mechanisms%252C%2520there%2520remains%2520a%2520notable%2520gap%2520in%2520understanding%2520how%250Amodels%2520capture%2520and%2520utilize%2520temporal%2520semantics%2520for%2520effective%2520temporal%2520feature%250Aextraction%252C%2520especially%2520in%2520the%2520context%2520of%2520Advanced%2520Driver%2520Assistance%2520Systems.%2520We%250Apropose%2520an%2520automated%2520LiDAR-based%2520captioning%2520procedure%2520that%2520focuses%2520on%2520the%250Atemporal%2520dynamics%2520of%2520traffic%2520participants.%2520Our%2520approach%2520uses%2520a%2520rule-based%250Asystem%2520to%2520extract%2520essential%2520details%2520such%2520as%2520lane%2520position%2520and%2520relative%2520motion%250Afrom%2520object%2520tracks%252C%2520followed%2520by%2520a%2520template-based%2520caption%2520generation.%2520Our%250Afindings%2520show%2520that%2520training%2520SwinBERT%252C%2520a%2520video%2520captioning%2520model%252C%2520using%2520only%250Afront%2520camera%2520images%2520and%2520supervised%2520with%2520our%2520template-based%2520captions%252C%250Aspecifically%2520designed%2520to%2520encapsulate%2520fine-grained%2520temporal%2520behavior%252C%2520leads%2520to%250Aimproved%2520temporal%2520understanding%2520consistently%2520across%2520three%2520datasets.%2520In%250Aconclusion%252C%2520our%2520results%2520clearly%2520demonstrate%2520that%2520integrating%2520LiDAR-based%250Acaption%2520supervision%2520significantly%2520enhances%2520temporal%2520understanding%252C%2520effectively%250Aaddressing%2520and%2520reducing%2520the%2520inherent%2520visual/static%2520biases%2520prevalent%2520in%2520current%250Astate-of-the-art%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Object%20Captioning%20for%20Street%20Scene%20Videos%20from%20LiDAR%20Tracks&entry.906535625=Vignesh%20Gopinathan%20and%20Urs%20Zimmermann%20and%20Michael%20Arnold%20and%20Matthias%20Rottmann&entry.1292438233=%20%20Video%20captioning%20models%20have%20seen%20notable%20advancements%20in%20recent%20years%2C%0Aespecially%20with%20regard%20to%20their%20ability%20to%20capture%20temporal%20information.%20While%0Amany%20research%20efforts%20have%20focused%20on%20architectural%20advancements%2C%20such%20as%0Atemporal%20attention%20mechanisms%2C%20there%20remains%20a%20notable%20gap%20in%20understanding%20how%0Amodels%20capture%20and%20utilize%20temporal%20semantics%20for%20effective%20temporal%20feature%0Aextraction%2C%20especially%20in%20the%20context%20of%20Advanced%20Driver%20Assistance%20Systems.%20We%0Apropose%20an%20automated%20LiDAR-based%20captioning%20procedure%20that%20focuses%20on%20the%0Atemporal%20dynamics%20of%20traffic%20participants.%20Our%20approach%20uses%20a%20rule-based%0Asystem%20to%20extract%20essential%20details%20such%20as%20lane%20position%20and%20relative%20motion%0Afrom%20object%20tracks%2C%20followed%20by%20a%20template-based%20caption%20generation.%20Our%0Afindings%20show%20that%20training%20SwinBERT%2C%20a%20video%20captioning%20model%2C%20using%20only%0Afront%20camera%20images%20and%20supervised%20with%20our%20template-based%20captions%2C%0Aspecifically%20designed%20to%20encapsulate%20fine-grained%20temporal%20behavior%2C%20leads%20to%0Aimproved%20temporal%20understanding%20consistently%20across%20three%20datasets.%20In%0Aconclusion%2C%20our%20results%20clearly%20demonstrate%20that%20integrating%20LiDAR-based%0Acaption%20supervision%20significantly%20enhances%20temporal%20understanding%2C%20effectively%0Aaddressing%20and%20reducing%20the%20inherent%20visual/static%20biases%20prevalent%20in%20current%0Astate-of-the-art%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16594v1&entry.124074799=Read"},
{"title": "Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of\n  Large Models via Differentiated Thinking and Complementary Ensembles", "author": "Jun Xie and Xiongjun Guan and Yingjian Zhu and Zhaoran Zhao and Xinming Wang and Feng Chen and Zhepeng Wang", "abstract": "  In this paper, we present the runner-up solution for the Ego4D EgoSchema\nChallenge at CVPR 2025 (Confirmed on May 20, 2025). Inspired by the success of\nlarge models, we evaluate and leverage leading accessible multimodal large\nmodels and adapt them to video understanding tasks via few-shot learning and\nmodel ensemble strategies. Specifically, diversified prompt styles and process\nparadigms are systematically explored and evaluated to effectively guide the\nattention of large models, fully unleashing their powerful generalization and\nadaptability abilities. Experimental results demonstrate that, with our\ncarefully designed approach, directly utilizing an individual multimodal model\nalready outperforms the previous state-of-the-art (SOTA) method which includes\nseveral additional processes. Besides, an additional stage is further\nintroduced that facilitates the cooperation and ensemble of periodic results,\nwhich achieves impressive performance improvements. We hope this work serves as\na valuable reference for the practical application of large models and inspires\nfuture research in the field.\n", "link": "http://arxiv.org/abs/2505.16784v1", "date": "2025-05-22", "relevancy": 2.8568, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Four%20Eyes%20Are%20Better%20Than%20Two%3A%20Harnessing%20the%20Collaborative%20Potential%20of%0A%20%20Large%20Models%20via%20Differentiated%20Thinking%20and%20Complementary%20Ensembles&body=Title%3A%20Four%20Eyes%20Are%20Better%20Than%20Two%3A%20Harnessing%20the%20Collaborative%20Potential%20of%0A%20%20Large%20Models%20via%20Differentiated%20Thinking%20and%20Complementary%20Ensembles%0AAuthor%3A%20Jun%20Xie%20and%20Xiongjun%20Guan%20and%20Yingjian%20Zhu%20and%20Zhaoran%20Zhao%20and%20Xinming%20Wang%20and%20Feng%20Chen%20and%20Zhepeng%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20the%20runner-up%20solution%20for%20the%20Ego4D%20EgoSchema%0AChallenge%20at%20CVPR%202025%20%28Confirmed%20on%20May%2020%2C%202025%29.%20Inspired%20by%20the%20success%20of%0Alarge%20models%2C%20we%20evaluate%20and%20leverage%20leading%20accessible%20multimodal%20large%0Amodels%20and%20adapt%20them%20to%20video%20understanding%20tasks%20via%20few-shot%20learning%20and%0Amodel%20ensemble%20strategies.%20Specifically%2C%20diversified%20prompt%20styles%20and%20process%0Aparadigms%20are%20systematically%20explored%20and%20evaluated%20to%20effectively%20guide%20the%0Aattention%20of%20large%20models%2C%20fully%20unleashing%20their%20powerful%20generalization%20and%0Aadaptability%20abilities.%20Experimental%20results%20demonstrate%20that%2C%20with%20our%0Acarefully%20designed%20approach%2C%20directly%20utilizing%20an%20individual%20multimodal%20model%0Aalready%20outperforms%20the%20previous%20state-of-the-art%20%28SOTA%29%20method%20which%20includes%0Aseveral%20additional%20processes.%20Besides%2C%20an%20additional%20stage%20is%20further%0Aintroduced%20that%20facilitates%20the%20cooperation%20and%20ensemble%20of%20periodic%20results%2C%0Awhich%20achieves%20impressive%20performance%20improvements.%20We%20hope%20this%20work%20serves%20as%0Aa%20valuable%20reference%20for%20the%20practical%20application%20of%20large%20models%20and%20inspires%0Afuture%20research%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFour%2520Eyes%2520Are%2520Better%2520Than%2520Two%253A%2520Harnessing%2520the%2520Collaborative%2520Potential%2520of%250A%2520%2520Large%2520Models%2520via%2520Differentiated%2520Thinking%2520and%2520Complementary%2520Ensembles%26entry.906535625%3DJun%2520Xie%2520and%2520Xiongjun%2520Guan%2520and%2520Yingjian%2520Zhu%2520and%2520Zhaoran%2520Zhao%2520and%2520Xinming%2520Wang%2520and%2520Feng%2520Chen%2520and%2520Zhepeng%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520runner-up%2520solution%2520for%2520the%2520Ego4D%2520EgoSchema%250AChallenge%2520at%2520CVPR%25202025%2520%2528Confirmed%2520on%2520May%252020%252C%25202025%2529.%2520Inspired%2520by%2520the%2520success%2520of%250Alarge%2520models%252C%2520we%2520evaluate%2520and%2520leverage%2520leading%2520accessible%2520multimodal%2520large%250Amodels%2520and%2520adapt%2520them%2520to%2520video%2520understanding%2520tasks%2520via%2520few-shot%2520learning%2520and%250Amodel%2520ensemble%2520strategies.%2520Specifically%252C%2520diversified%2520prompt%2520styles%2520and%2520process%250Aparadigms%2520are%2520systematically%2520explored%2520and%2520evaluated%2520to%2520effectively%2520guide%2520the%250Aattention%2520of%2520large%2520models%252C%2520fully%2520unleashing%2520their%2520powerful%2520generalization%2520and%250Aadaptability%2520abilities.%2520Experimental%2520results%2520demonstrate%2520that%252C%2520with%2520our%250Acarefully%2520designed%2520approach%252C%2520directly%2520utilizing%2520an%2520individual%2520multimodal%2520model%250Aalready%2520outperforms%2520the%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520method%2520which%2520includes%250Aseveral%2520additional%2520processes.%2520Besides%252C%2520an%2520additional%2520stage%2520is%2520further%250Aintroduced%2520that%2520facilitates%2520the%2520cooperation%2520and%2520ensemble%2520of%2520periodic%2520results%252C%250Awhich%2520achieves%2520impressive%2520performance%2520improvements.%2520We%2520hope%2520this%2520work%2520serves%2520as%250Aa%2520valuable%2520reference%2520for%2520the%2520practical%2520application%2520of%2520large%2520models%2520and%2520inspires%250Afuture%2520research%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Four%20Eyes%20Are%20Better%20Than%20Two%3A%20Harnessing%20the%20Collaborative%20Potential%20of%0A%20%20Large%20Models%20via%20Differentiated%20Thinking%20and%20Complementary%20Ensembles&entry.906535625=Jun%20Xie%20and%20Xiongjun%20Guan%20and%20Yingjian%20Zhu%20and%20Zhaoran%20Zhao%20and%20Xinming%20Wang%20and%20Feng%20Chen%20and%20Zhepeng%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20the%20runner-up%20solution%20for%20the%20Ego4D%20EgoSchema%0AChallenge%20at%20CVPR%202025%20%28Confirmed%20on%20May%2020%2C%202025%29.%20Inspired%20by%20the%20success%20of%0Alarge%20models%2C%20we%20evaluate%20and%20leverage%20leading%20accessible%20multimodal%20large%0Amodels%20and%20adapt%20them%20to%20video%20understanding%20tasks%20via%20few-shot%20learning%20and%0Amodel%20ensemble%20strategies.%20Specifically%2C%20diversified%20prompt%20styles%20and%20process%0Aparadigms%20are%20systematically%20explored%20and%20evaluated%20to%20effectively%20guide%20the%0Aattention%20of%20large%20models%2C%20fully%20unleashing%20their%20powerful%20generalization%20and%0Aadaptability%20abilities.%20Experimental%20results%20demonstrate%20that%2C%20with%20our%0Acarefully%20designed%20approach%2C%20directly%20utilizing%20an%20individual%20multimodal%20model%0Aalready%20outperforms%20the%20previous%20state-of-the-art%20%28SOTA%29%20method%20which%20includes%0Aseveral%20additional%20processes.%20Besides%2C%20an%20additional%20stage%20is%20further%0Aintroduced%20that%20facilitates%20the%20cooperation%20and%20ensemble%20of%20periodic%20results%2C%0Awhich%20achieves%20impressive%20performance%20improvements.%20We%20hope%20this%20work%20serves%20as%0Aa%20valuable%20reference%20for%20the%20practical%20application%20of%20large%20models%20and%20inspires%0Afuture%20research%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16784v1&entry.124074799=Read"},
{"title": "Point, Detect, Count: Multi-Task Medical Image Understanding with\n  Instruction-Tuned Vision-Language Models", "author": "Sushant Gautam and Michael A. Riegler and P\u00e5l Halvorsen", "abstract": "  We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount.\n", "link": "http://arxiv.org/abs/2505.16647v1", "date": "2025-05-22", "relevancy": 2.8352, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%2C%20Detect%2C%20Count%3A%20Multi-Task%20Medical%20Image%20Understanding%20with%0A%20%20Instruction-Tuned%20Vision-Language%20Models&body=Title%3A%20Point%2C%20Detect%2C%20Count%3A%20Multi-Task%20Medical%20Image%20Understanding%20with%0A%20%20Instruction-Tuned%20Vision-Language%20Models%0AAuthor%3A%20Sushant%20Gautam%20and%20Michael%20A.%20Riegler%20and%20P%C3%A5l%20Halvorsen%0AAbstract%3A%20%20%20We%20investigate%20fine-tuning%20Vision-Language%20Models%20%28VLMs%29%20for%20multi-task%0Amedical%20image%20understanding%2C%20focusing%20on%20detection%2C%20localization%2C%20and%20counting%0Aof%20findings%20in%20medical%20images.%20Our%20objective%20is%20to%20evaluate%20whether%0Ainstruction-tuned%20VLMs%20can%20simultaneously%20improve%20these%20tasks%2C%20with%20the%20goal%20of%0Aenhancing%20diagnostic%20accuracy%20and%20efficiency.%20Using%20MedMultiPoints%2C%20a%0Amultimodal%20dataset%20with%20annotations%20from%20endoscopy%20%28polyps%20and%20instruments%29%20and%0Amicroscopy%20%28sperm%20cells%29%2C%20we%20reformulate%20each%20task%20into%20instruction-based%0Aprompts%20suitable%20for%20vision-language%20reasoning.%20We%20fine-tune%0AQwen2.5-VL-7B-Instruct%20using%20Low-Rank%20Adaptation%20%28LoRA%29%20across%20multiple%20task%0Acombinations.%20Results%20show%20that%20multi-task%20training%20improves%20robustness%20and%0Aaccuracy.%20For%20example%2C%20it%20reduces%20the%20Count%20Mean%20Absolute%20Error%20%28MAE%29%20and%0Aincreases%20Matching%20Accuracy%20in%20the%20Counting%20%2B%20Pointing%20task.%20However%2C%0Atrade-offs%20emerge%2C%20such%20as%20more%20zero-case%20point%20predictions%2C%20indicating%20reduced%0Areliability%20in%20edge%20cases%20despite%20overall%20performance%20gains.%20Our%20study%0Ahighlights%20the%20potential%20of%20adapting%20general-purpose%20VLMs%20to%20specialized%0Amedical%20tasks%20via%20prompt-driven%20fine-tuning.%20This%20approach%20mirrors%20clinical%0Aworkflows%2C%20where%20radiologists%20simultaneously%20localize%2C%20count%2C%20and%20describe%0Afindings%20-%20demonstrating%20how%20VLMs%20can%20learn%20composite%20diagnostic%20reasoning%0Apatterns.%20The%20model%20produces%20interpretable%2C%20structured%20outputs%2C%20offering%20a%0Apromising%20step%20toward%20explainable%20and%20versatile%20medical%20AI.%20Code%2C%20model%0Aweights%2C%20and%20scripts%20will%20be%20released%20for%20reproducibility%20at%0Ahttps%3A//github.com/simula/PointDetectCount.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%252C%2520Detect%252C%2520Count%253A%2520Multi-Task%2520Medical%2520Image%2520Understanding%2520with%250A%2520%2520Instruction-Tuned%2520Vision-Language%2520Models%26entry.906535625%3DSushant%2520Gautam%2520and%2520Michael%2520A.%2520Riegler%2520and%2520P%25C3%25A5l%2520Halvorsen%26entry.1292438233%3D%2520%2520We%2520investigate%2520fine-tuning%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520multi-task%250Amedical%2520image%2520understanding%252C%2520focusing%2520on%2520detection%252C%2520localization%252C%2520and%2520counting%250Aof%2520findings%2520in%2520medical%2520images.%2520Our%2520objective%2520is%2520to%2520evaluate%2520whether%250Ainstruction-tuned%2520VLMs%2520can%2520simultaneously%2520improve%2520these%2520tasks%252C%2520with%2520the%2520goal%2520of%250Aenhancing%2520diagnostic%2520accuracy%2520and%2520efficiency.%2520Using%2520MedMultiPoints%252C%2520a%250Amultimodal%2520dataset%2520with%2520annotations%2520from%2520endoscopy%2520%2528polyps%2520and%2520instruments%2529%2520and%250Amicroscopy%2520%2528sperm%2520cells%2529%252C%2520we%2520reformulate%2520each%2520task%2520into%2520instruction-based%250Aprompts%2520suitable%2520for%2520vision-language%2520reasoning.%2520We%2520fine-tune%250AQwen2.5-VL-7B-Instruct%2520using%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520across%2520multiple%2520task%250Acombinations.%2520Results%2520show%2520that%2520multi-task%2520training%2520improves%2520robustness%2520and%250Aaccuracy.%2520For%2520example%252C%2520it%2520reduces%2520the%2520Count%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520and%250Aincreases%2520Matching%2520Accuracy%2520in%2520the%2520Counting%2520%252B%2520Pointing%2520task.%2520However%252C%250Atrade-offs%2520emerge%252C%2520such%2520as%2520more%2520zero-case%2520point%2520predictions%252C%2520indicating%2520reduced%250Areliability%2520in%2520edge%2520cases%2520despite%2520overall%2520performance%2520gains.%2520Our%2520study%250Ahighlights%2520the%2520potential%2520of%2520adapting%2520general-purpose%2520VLMs%2520to%2520specialized%250Amedical%2520tasks%2520via%2520prompt-driven%2520fine-tuning.%2520This%2520approach%2520mirrors%2520clinical%250Aworkflows%252C%2520where%2520radiologists%2520simultaneously%2520localize%252C%2520count%252C%2520and%2520describe%250Afindings%2520-%2520demonstrating%2520how%2520VLMs%2520can%2520learn%2520composite%2520diagnostic%2520reasoning%250Apatterns.%2520The%2520model%2520produces%2520interpretable%252C%2520structured%2520outputs%252C%2520offering%2520a%250Apromising%2520step%2520toward%2520explainable%2520and%2520versatile%2520medical%2520AI.%2520Code%252C%2520model%250Aweights%252C%2520and%2520scripts%2520will%2520be%2520released%2520for%2520reproducibility%2520at%250Ahttps%253A//github.com/simula/PointDetectCount.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%2C%20Detect%2C%20Count%3A%20Multi-Task%20Medical%20Image%20Understanding%20with%0A%20%20Instruction-Tuned%20Vision-Language%20Models&entry.906535625=Sushant%20Gautam%20and%20Michael%20A.%20Riegler%20and%20P%C3%A5l%20Halvorsen&entry.1292438233=%20%20We%20investigate%20fine-tuning%20Vision-Language%20Models%20%28VLMs%29%20for%20multi-task%0Amedical%20image%20understanding%2C%20focusing%20on%20detection%2C%20localization%2C%20and%20counting%0Aof%20findings%20in%20medical%20images.%20Our%20objective%20is%20to%20evaluate%20whether%0Ainstruction-tuned%20VLMs%20can%20simultaneously%20improve%20these%20tasks%2C%20with%20the%20goal%20of%0Aenhancing%20diagnostic%20accuracy%20and%20efficiency.%20Using%20MedMultiPoints%2C%20a%0Amultimodal%20dataset%20with%20annotations%20from%20endoscopy%20%28polyps%20and%20instruments%29%20and%0Amicroscopy%20%28sperm%20cells%29%2C%20we%20reformulate%20each%20task%20into%20instruction-based%0Aprompts%20suitable%20for%20vision-language%20reasoning.%20We%20fine-tune%0AQwen2.5-VL-7B-Instruct%20using%20Low-Rank%20Adaptation%20%28LoRA%29%20across%20multiple%20task%0Acombinations.%20Results%20show%20that%20multi-task%20training%20improves%20robustness%20and%0Aaccuracy.%20For%20example%2C%20it%20reduces%20the%20Count%20Mean%20Absolute%20Error%20%28MAE%29%20and%0Aincreases%20Matching%20Accuracy%20in%20the%20Counting%20%2B%20Pointing%20task.%20However%2C%0Atrade-offs%20emerge%2C%20such%20as%20more%20zero-case%20point%20predictions%2C%20indicating%20reduced%0Areliability%20in%20edge%20cases%20despite%20overall%20performance%20gains.%20Our%20study%0Ahighlights%20the%20potential%20of%20adapting%20general-purpose%20VLMs%20to%20specialized%0Amedical%20tasks%20via%20prompt-driven%20fine-tuning.%20This%20approach%20mirrors%20clinical%0Aworkflows%2C%20where%20radiologists%20simultaneously%20localize%2C%20count%2C%20and%20describe%0Afindings%20-%20demonstrating%20how%20VLMs%20can%20learn%20composite%20diagnostic%20reasoning%0Apatterns.%20The%20model%20produces%20interpretable%2C%20structured%20outputs%2C%20offering%20a%0Apromising%20step%20toward%20explainable%20and%20versatile%20medical%20AI.%20Code%2C%20model%0Aweights%2C%20and%20scripts%20will%20be%20released%20for%20reproducibility%20at%0Ahttps%3A//github.com/simula/PointDetectCount.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16647v1&entry.124074799=Read"},
{"title": "Harnessing the Computation Redundancy in ViTs to Boost Adversarial\n  Transferability", "author": "Jiani Liu and Zhiyuan Wang and Zeliang Zhang and Chao Huang and Susan Liang and Yunlong Tang and Chenliang Xu", "abstract": "  Vision Transformers (ViTs) have demonstrated impressive performance across a\nrange of applications, including many safety-critical tasks. However, their\nunique architectural properties raise new challenges and opportunities in\nadversarial robustness. In particular, we observe that adversarial examples\ncrafted on ViTs exhibit higher transferability compared to those crafted on\nCNNs, suggesting that ViTs contain structural characteristics favorable for\ntransferable attacks. In this work, we investigate the role of computational\nredundancy in ViTs and its impact on adversarial transferability. Unlike prior\nstudies that aim to reduce computation for efficiency, we propose to exploit\nthis redundancy to improve the quality and transferability of adversarial\nexamples. Through a detailed analysis, we identify two forms of redundancy,\nincluding the data-level and model-level, that can be harnessed to amplify\nattack effectiveness. Building on this insight, we design a suite of\ntechniques, including attention sparsity manipulation, attention head\npermutation, clean token regularization, ghost MoE diversification, and\ntest-time adversarial training. Extensive experiments on the ImageNet-1k\ndataset validate the effectiveness of our approach, showing that our methods\nsignificantly outperform existing baselines in both transferability and\ngenerality across diverse model architectures.\n", "link": "http://arxiv.org/abs/2504.10804v2", "date": "2025-05-22", "relevancy": 2.8137, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5684}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5656}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20the%20Computation%20Redundancy%20in%20ViTs%20to%20Boost%20Adversarial%0A%20%20Transferability&body=Title%3A%20Harnessing%20the%20Computation%20Redundancy%20in%20ViTs%20to%20Boost%20Adversarial%0A%20%20Transferability%0AAuthor%3A%20Jiani%20Liu%20and%20Zhiyuan%20Wang%20and%20Zeliang%20Zhang%20and%20Chao%20Huang%20and%20Susan%20Liang%20and%20Yunlong%20Tang%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20impressive%20performance%20across%20a%0Arange%20of%20applications%2C%20including%20many%20safety-critical%20tasks.%20However%2C%20their%0Aunique%20architectural%20properties%20raise%20new%20challenges%20and%20opportunities%20in%0Aadversarial%20robustness.%20In%20particular%2C%20we%20observe%20that%20adversarial%20examples%0Acrafted%20on%20ViTs%20exhibit%20higher%20transferability%20compared%20to%20those%20crafted%20on%0ACNNs%2C%20suggesting%20that%20ViTs%20contain%20structural%20characteristics%20favorable%20for%0Atransferable%20attacks.%20In%20this%20work%2C%20we%20investigate%20the%20role%20of%20computational%0Aredundancy%20in%20ViTs%20and%20its%20impact%20on%20adversarial%20transferability.%20Unlike%20prior%0Astudies%20that%20aim%20to%20reduce%20computation%20for%20efficiency%2C%20we%20propose%20to%20exploit%0Athis%20redundancy%20to%20improve%20the%20quality%20and%20transferability%20of%20adversarial%0Aexamples.%20Through%20a%20detailed%20analysis%2C%20we%20identify%20two%20forms%20of%20redundancy%2C%0Aincluding%20the%20data-level%20and%20model-level%2C%20that%20can%20be%20harnessed%20to%20amplify%0Aattack%20effectiveness.%20Building%20on%20this%20insight%2C%20we%20design%20a%20suite%20of%0Atechniques%2C%20including%20attention%20sparsity%20manipulation%2C%20attention%20head%0Apermutation%2C%20clean%20token%20regularization%2C%20ghost%20MoE%20diversification%2C%20and%0Atest-time%20adversarial%20training.%20Extensive%20experiments%20on%20the%20ImageNet-1k%0Adataset%20validate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20that%20our%20methods%0Asignificantly%20outperform%20existing%20baselines%20in%20both%20transferability%20and%0Agenerality%20across%20diverse%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10804v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520the%2520Computation%2520Redundancy%2520in%2520ViTs%2520to%2520Boost%2520Adversarial%250A%2520%2520Transferability%26entry.906535625%3DJiani%2520Liu%2520and%2520Zhiyuan%2520Wang%2520and%2520Zeliang%2520Zhang%2520and%2520Chao%2520Huang%2520and%2520Susan%2520Liang%2520and%2520Yunlong%2520Tang%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520across%2520a%250Arange%2520of%2520applications%252C%2520including%2520many%2520safety-critical%2520tasks.%2520However%252C%2520their%250Aunique%2520architectural%2520properties%2520raise%2520new%2520challenges%2520and%2520opportunities%2520in%250Aadversarial%2520robustness.%2520In%2520particular%252C%2520we%2520observe%2520that%2520adversarial%2520examples%250Acrafted%2520on%2520ViTs%2520exhibit%2520higher%2520transferability%2520compared%2520to%2520those%2520crafted%2520on%250ACNNs%252C%2520suggesting%2520that%2520ViTs%2520contain%2520structural%2520characteristics%2520favorable%2520for%250Atransferable%2520attacks.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520role%2520of%2520computational%250Aredundancy%2520in%2520ViTs%2520and%2520its%2520impact%2520on%2520adversarial%2520transferability.%2520Unlike%2520prior%250Astudies%2520that%2520aim%2520to%2520reduce%2520computation%2520for%2520efficiency%252C%2520we%2520propose%2520to%2520exploit%250Athis%2520redundancy%2520to%2520improve%2520the%2520quality%2520and%2520transferability%2520of%2520adversarial%250Aexamples.%2520Through%2520a%2520detailed%2520analysis%252C%2520we%2520identify%2520two%2520forms%2520of%2520redundancy%252C%250Aincluding%2520the%2520data-level%2520and%2520model-level%252C%2520that%2520can%2520be%2520harnessed%2520to%2520amplify%250Aattack%2520effectiveness.%2520Building%2520on%2520this%2520insight%252C%2520we%2520design%2520a%2520suite%2520of%250Atechniques%252C%2520including%2520attention%2520sparsity%2520manipulation%252C%2520attention%2520head%250Apermutation%252C%2520clean%2520token%2520regularization%252C%2520ghost%2520MoE%2520diversification%252C%2520and%250Atest-time%2520adversarial%2520training.%2520Extensive%2520experiments%2520on%2520the%2520ImageNet-1k%250Adataset%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520showing%2520that%2520our%2520methods%250Asignificantly%2520outperform%2520existing%2520baselines%2520in%2520both%2520transferability%2520and%250Agenerality%2520across%2520diverse%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10804v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20the%20Computation%20Redundancy%20in%20ViTs%20to%20Boost%20Adversarial%0A%20%20Transferability&entry.906535625=Jiani%20Liu%20and%20Zhiyuan%20Wang%20and%20Zeliang%20Zhang%20and%20Chao%20Huang%20and%20Susan%20Liang%20and%20Yunlong%20Tang%20and%20Chenliang%20Xu&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20impressive%20performance%20across%20a%0Arange%20of%20applications%2C%20including%20many%20safety-critical%20tasks.%20However%2C%20their%0Aunique%20architectural%20properties%20raise%20new%20challenges%20and%20opportunities%20in%0Aadversarial%20robustness.%20In%20particular%2C%20we%20observe%20that%20adversarial%20examples%0Acrafted%20on%20ViTs%20exhibit%20higher%20transferability%20compared%20to%20those%20crafted%20on%0ACNNs%2C%20suggesting%20that%20ViTs%20contain%20structural%20characteristics%20favorable%20for%0Atransferable%20attacks.%20In%20this%20work%2C%20we%20investigate%20the%20role%20of%20computational%0Aredundancy%20in%20ViTs%20and%20its%20impact%20on%20adversarial%20transferability.%20Unlike%20prior%0Astudies%20that%20aim%20to%20reduce%20computation%20for%20efficiency%2C%20we%20propose%20to%20exploit%0Athis%20redundancy%20to%20improve%20the%20quality%20and%20transferability%20of%20adversarial%0Aexamples.%20Through%20a%20detailed%20analysis%2C%20we%20identify%20two%20forms%20of%20redundancy%2C%0Aincluding%20the%20data-level%20and%20model-level%2C%20that%20can%20be%20harnessed%20to%20amplify%0Aattack%20effectiveness.%20Building%20on%20this%20insight%2C%20we%20design%20a%20suite%20of%0Atechniques%2C%20including%20attention%20sparsity%20manipulation%2C%20attention%20head%0Apermutation%2C%20clean%20token%20regularization%2C%20ghost%20MoE%20diversification%2C%20and%0Atest-time%20adversarial%20training.%20Extensive%20experiments%20on%20the%20ImageNet-1k%0Adataset%20validate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20that%20our%20methods%0Asignificantly%20outperform%20existing%20baselines%20in%20both%20transferability%20and%0Agenerality%20across%20diverse%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10804v2&entry.124074799=Read"},
{"title": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual\n  Cross-Attention Mechanisms", "author": "Shilin Yan and Jiaming Han and Joey Tsai and Hongwei Xue and Rongyao Fang and Lingyi Hong and Ziyu Guo and Ray Zhang", "abstract": "  The advent of Large Multimodal Models (LMMs) has significantly enhanced Large\nLanguage Models (LLMs) to process and interpret diverse data modalities (e.g.,\nimage and video). However, as input complexity increases, particularly with\nlong video sequences, the number of required tokens has grown significantly,\nleading to quadratically computational costs. This has made the efficient\ncompression of video tokens in LMMs, while maintaining performance integrity, a\npressing research challenge. In this paper, we introduce CrossLMM, decoupling\nlong video sequences from LMMs via a dual cross-attention mechanism, which\nsubstantially reduces visual token quantity with minimal performance\ndegradation. Specifically, we first implement a significant token reduction\nfrom pretrained visual encoders through a pooling methodology. Then, within LLM\nlayers, we employ a visual-to-visual cross-attention mechanism, wherein the\npooled visual tokens function as queries against the original visual token set.\nThis module enables more efficient token utilization while retaining\nfine-grained informational fidelity. In addition, we introduce a text-to-visual\ncross-attention mechanism, for which the text tokens are enhanced through\ninteraction with the original visual tokens, enriching the visual comprehension\nof the text tokens. Comprehensive empirical evaluation demonstrates that our\napproach achieves comparable or superior performance across diverse video-based\nLMM benchmarks, despite utilizing substantially fewer computational resources.\n", "link": "http://arxiv.org/abs/2505.17020v1", "date": "2025-05-22", "relevancy": 2.8072, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5727}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrossLMM%3A%20Decoupling%20Long%20Video%20Sequences%20from%20LMMs%20via%20Dual%0A%20%20Cross-Attention%20Mechanisms&body=Title%3A%20CrossLMM%3A%20Decoupling%20Long%20Video%20Sequences%20from%20LMMs%20via%20Dual%0A%20%20Cross-Attention%20Mechanisms%0AAuthor%3A%20Shilin%20Yan%20and%20Jiaming%20Han%20and%20Joey%20Tsai%20and%20Hongwei%20Xue%20and%20Rongyao%20Fang%20and%20Lingyi%20Hong%20and%20Ziyu%20Guo%20and%20Ray%20Zhang%0AAbstract%3A%20%20%20The%20advent%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20has%20significantly%20enhanced%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20process%20and%20interpret%20diverse%20data%20modalities%20%28e.g.%2C%0Aimage%20and%20video%29.%20However%2C%20as%20input%20complexity%20increases%2C%20particularly%20with%0Along%20video%20sequences%2C%20the%20number%20of%20required%20tokens%20has%20grown%20significantly%2C%0Aleading%20to%20quadratically%20computational%20costs.%20This%20has%20made%20the%20efficient%0Acompression%20of%20video%20tokens%20in%20LMMs%2C%20while%20maintaining%20performance%20integrity%2C%20a%0Apressing%20research%20challenge.%20In%20this%20paper%2C%20we%20introduce%20CrossLMM%2C%20decoupling%0Along%20video%20sequences%20from%20LMMs%20via%20a%20dual%20cross-attention%20mechanism%2C%20which%0Asubstantially%20reduces%20visual%20token%20quantity%20with%20minimal%20performance%0Adegradation.%20Specifically%2C%20we%20first%20implement%20a%20significant%20token%20reduction%0Afrom%20pretrained%20visual%20encoders%20through%20a%20pooling%20methodology.%20Then%2C%20within%20LLM%0Alayers%2C%20we%20employ%20a%20visual-to-visual%20cross-attention%20mechanism%2C%20wherein%20the%0Apooled%20visual%20tokens%20function%20as%20queries%20against%20the%20original%20visual%20token%20set.%0AThis%20module%20enables%20more%20efficient%20token%20utilization%20while%20retaining%0Afine-grained%20informational%20fidelity.%20In%20addition%2C%20we%20introduce%20a%20text-to-visual%0Across-attention%20mechanism%2C%20for%20which%20the%20text%20tokens%20are%20enhanced%20through%0Ainteraction%20with%20the%20original%20visual%20tokens%2C%20enriching%20the%20visual%20comprehension%0Aof%20the%20text%20tokens.%20Comprehensive%20empirical%20evaluation%20demonstrates%20that%20our%0Aapproach%20achieves%20comparable%20or%20superior%20performance%20across%20diverse%20video-based%0ALMM%20benchmarks%2C%20despite%20utilizing%20substantially%20fewer%20computational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrossLMM%253A%2520Decoupling%2520Long%2520Video%2520Sequences%2520from%2520LMMs%2520via%2520Dual%250A%2520%2520Cross-Attention%2520Mechanisms%26entry.906535625%3DShilin%2520Yan%2520and%2520Jiaming%2520Han%2520and%2520Joey%2520Tsai%2520and%2520Hongwei%2520Xue%2520and%2520Rongyao%2520Fang%2520and%2520Lingyi%2520Hong%2520and%2520Ziyu%2520Guo%2520and%2520Ray%2520Zhang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520has%2520significantly%2520enhanced%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520process%2520and%2520interpret%2520diverse%2520data%2520modalities%2520%2528e.g.%252C%250Aimage%2520and%2520video%2529.%2520However%252C%2520as%2520input%2520complexity%2520increases%252C%2520particularly%2520with%250Along%2520video%2520sequences%252C%2520the%2520number%2520of%2520required%2520tokens%2520has%2520grown%2520significantly%252C%250Aleading%2520to%2520quadratically%2520computational%2520costs.%2520This%2520has%2520made%2520the%2520efficient%250Acompression%2520of%2520video%2520tokens%2520in%2520LMMs%252C%2520while%2520maintaining%2520performance%2520integrity%252C%2520a%250Apressing%2520research%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CrossLMM%252C%2520decoupling%250Along%2520video%2520sequences%2520from%2520LMMs%2520via%2520a%2520dual%2520cross-attention%2520mechanism%252C%2520which%250Asubstantially%2520reduces%2520visual%2520token%2520quantity%2520with%2520minimal%2520performance%250Adegradation.%2520Specifically%252C%2520we%2520first%2520implement%2520a%2520significant%2520token%2520reduction%250Afrom%2520pretrained%2520visual%2520encoders%2520through%2520a%2520pooling%2520methodology.%2520Then%252C%2520within%2520LLM%250Alayers%252C%2520we%2520employ%2520a%2520visual-to-visual%2520cross-attention%2520mechanism%252C%2520wherein%2520the%250Apooled%2520visual%2520tokens%2520function%2520as%2520queries%2520against%2520the%2520original%2520visual%2520token%2520set.%250AThis%2520module%2520enables%2520more%2520efficient%2520token%2520utilization%2520while%2520retaining%250Afine-grained%2520informational%2520fidelity.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520text-to-visual%250Across-attention%2520mechanism%252C%2520for%2520which%2520the%2520text%2520tokens%2520are%2520enhanced%2520through%250Ainteraction%2520with%2520the%2520original%2520visual%2520tokens%252C%2520enriching%2520the%2520visual%2520comprehension%250Aof%2520the%2520text%2520tokens.%2520Comprehensive%2520empirical%2520evaluation%2520demonstrates%2520that%2520our%250Aapproach%2520achieves%2520comparable%2520or%2520superior%2520performance%2520across%2520diverse%2520video-based%250ALMM%2520benchmarks%252C%2520despite%2520utilizing%2520substantially%2520fewer%2520computational%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrossLMM%3A%20Decoupling%20Long%20Video%20Sequences%20from%20LMMs%20via%20Dual%0A%20%20Cross-Attention%20Mechanisms&entry.906535625=Shilin%20Yan%20and%20Jiaming%20Han%20and%20Joey%20Tsai%20and%20Hongwei%20Xue%20and%20Rongyao%20Fang%20and%20Lingyi%20Hong%20and%20Ziyu%20Guo%20and%20Ray%20Zhang&entry.1292438233=%20%20The%20advent%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20has%20significantly%20enhanced%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20process%20and%20interpret%20diverse%20data%20modalities%20%28e.g.%2C%0Aimage%20and%20video%29.%20However%2C%20as%20input%20complexity%20increases%2C%20particularly%20with%0Along%20video%20sequences%2C%20the%20number%20of%20required%20tokens%20has%20grown%20significantly%2C%0Aleading%20to%20quadratically%20computational%20costs.%20This%20has%20made%20the%20efficient%0Acompression%20of%20video%20tokens%20in%20LMMs%2C%20while%20maintaining%20performance%20integrity%2C%20a%0Apressing%20research%20challenge.%20In%20this%20paper%2C%20we%20introduce%20CrossLMM%2C%20decoupling%0Along%20video%20sequences%20from%20LMMs%20via%20a%20dual%20cross-attention%20mechanism%2C%20which%0Asubstantially%20reduces%20visual%20token%20quantity%20with%20minimal%20performance%0Adegradation.%20Specifically%2C%20we%20first%20implement%20a%20significant%20token%20reduction%0Afrom%20pretrained%20visual%20encoders%20through%20a%20pooling%20methodology.%20Then%2C%20within%20LLM%0Alayers%2C%20we%20employ%20a%20visual-to-visual%20cross-attention%20mechanism%2C%20wherein%20the%0Apooled%20visual%20tokens%20function%20as%20queries%20against%20the%20original%20visual%20token%20set.%0AThis%20module%20enables%20more%20efficient%20token%20utilization%20while%20retaining%0Afine-grained%20informational%20fidelity.%20In%20addition%2C%20we%20introduce%20a%20text-to-visual%0Across-attention%20mechanism%2C%20for%20which%20the%20text%20tokens%20are%20enhanced%20through%0Ainteraction%20with%20the%20original%20visual%20tokens%2C%20enriching%20the%20visual%20comprehension%0Aof%20the%20text%20tokens.%20Comprehensive%20empirical%20evaluation%20demonstrates%20that%20our%0Aapproach%20achieves%20comparable%20or%20superior%20performance%20across%20diverse%20video-based%0ALMM%20benchmarks%2C%20despite%20utilizing%20substantially%20fewer%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17020v1&entry.124074799=Read"},
{"title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI\n  Agents", "author": "Yuqi Zhou and Sunhao Dai and Shuai Wang and Kaiwen Zhou and Qinglin Jia and Jun Xu", "abstract": "  Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.\n", "link": "http://arxiv.org/abs/2505.15810v2", "date": "2025-05-22", "relevancy": 2.8049, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5689}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5674}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-G1%3A%20Understanding%20R1-Zero-Like%20Training%20for%20Visual%20Grounding%20in%20GUI%0A%20%20Agents&body=Title%3A%20GUI-G1%3A%20Understanding%20R1-Zero-Like%20Training%20for%20Visual%20Grounding%20in%20GUI%0A%20%20Agents%0AAuthor%3A%20Yuqi%20Zhou%20and%20Sunhao%20Dai%20and%20Shuai%20Wang%20and%20Kaiwen%20Zhou%20and%20Qinglin%20Jia%20and%20Jun%20Xu%0AAbstract%3A%20%20%20Recent%20Graphical%20User%20Interface%20%28GUI%29%20agents%20replicate%20the%20R1-Zero%20paradigm%2C%0Acoupling%20online%20Reinforcement%20Learning%20%28RL%29%20with%20explicit%20chain-of-thought%0Areasoning%20prior%20to%20object%20grounding%20and%20thereby%20achieving%20substantial%0Aperformance%20gains.%20In%20this%20paper%2C%20we%20first%20conduct%20extensive%20analysis%0Aexperiments%20of%20three%20key%20components%20of%20that%20training%20pipeline%3A%20input%20design%2C%0Aoutput%20evaluation%2C%20and%20policy%20update-each%20revealing%20distinct%20challenges%20arising%0Afrom%20blindly%20applying%20general-purpose%20RL%20without%20adapting%20to%20GUI%20grounding%0Atasks.%20Input%20design%3A%20Current%20templates%20encourage%20the%20model%20to%20generate%0Achain-of-thought%20reasoning%2C%20but%20longer%20chains%20unexpectedly%20lead%20to%20worse%0Agrounding%20performance.%20Output%20evaluation%3A%20Reward%20functions%20based%20on%20hit%20signals%0Aor%20box%20area%20allow%20models%20to%20exploit%20box%20size%2C%20leading%20to%20reward%20hacking%20and%0Apoor%20localization%20quality.%20Policy%20update%3A%20Online%20RL%20tends%20to%20overfit%20easy%0Aexamples%20due%20to%20biases%20in%20length%20and%20sample%20difficulty%2C%20leading%20to%0Aunder-optimization%20on%20harder%20cases.%20To%20address%20these%20issues%2C%20we%20propose%20three%0Atargeted%20solutions.%20First%2C%20we%20adopt%20a%20Fast%20Thinking%20Template%20that%20encourages%0Adirect%20answer%20generation%2C%20reducing%20excessive%20reasoning%20during%20training.%20Second%2C%0Awe%20incorporate%20a%20box%20size%20constraint%20into%20the%20reward%20function%20to%20mitigate%0Areward%20hacking.%20Third%2C%20we%20revise%20the%20RL%20objective%20by%20adjusting%20length%0Anormalization%20and%20adding%20a%20difficulty-aware%20scaling%20factor%2C%20enabling%20better%0Aoptimization%20on%20hard%20samples.%20Our%20GUI-G1-3B%2C%20trained%20on%2017K%20public%20samples%20with%0AQwen2.5-VL-3B-Instruct%2C%20achieves%2090.3%25%20accuracy%20on%20ScreenSpot%20and%2037.1%25%20on%0AScreenSpot-Pro.%20This%20surpasses%20all%20prior%20models%20of%20similar%20size%20and%20even%0Aoutperforms%20the%20larger%20UI-TARS-7B%2C%20establishing%20a%20new%20state-of-the-art%20in%20GUI%0Aagent%20grounding.%20The%20project%20repository%20is%20available%20at%0Ahttps%3A//github.com/Yuqi-Zhou/GUI-G1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-G1%253A%2520Understanding%2520R1-Zero-Like%2520Training%2520for%2520Visual%2520Grounding%2520in%2520GUI%250A%2520%2520Agents%26entry.906535625%3DYuqi%2520Zhou%2520and%2520Sunhao%2520Dai%2520and%2520Shuai%2520Wang%2520and%2520Kaiwen%2520Zhou%2520and%2520Qinglin%2520Jia%2520and%2520Jun%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520replicate%2520the%2520R1-Zero%2520paradigm%252C%250Acoupling%2520online%2520Reinforcement%2520Learning%2520%2528RL%2529%2520with%2520explicit%2520chain-of-thought%250Areasoning%2520prior%2520to%2520object%2520grounding%2520and%2520thereby%2520achieving%2520substantial%250Aperformance%2520gains.%2520In%2520this%2520paper%252C%2520we%2520first%2520conduct%2520extensive%2520analysis%250Aexperiments%2520of%2520three%2520key%2520components%2520of%2520that%2520training%2520pipeline%253A%2520input%2520design%252C%250Aoutput%2520evaluation%252C%2520and%2520policy%2520update-each%2520revealing%2520distinct%2520challenges%2520arising%250Afrom%2520blindly%2520applying%2520general-purpose%2520RL%2520without%2520adapting%2520to%2520GUI%2520grounding%250Atasks.%2520Input%2520design%253A%2520Current%2520templates%2520encourage%2520the%2520model%2520to%2520generate%250Achain-of-thought%2520reasoning%252C%2520but%2520longer%2520chains%2520unexpectedly%2520lead%2520to%2520worse%250Agrounding%2520performance.%2520Output%2520evaluation%253A%2520Reward%2520functions%2520based%2520on%2520hit%2520signals%250Aor%2520box%2520area%2520allow%2520models%2520to%2520exploit%2520box%2520size%252C%2520leading%2520to%2520reward%2520hacking%2520and%250Apoor%2520localization%2520quality.%2520Policy%2520update%253A%2520Online%2520RL%2520tends%2520to%2520overfit%2520easy%250Aexamples%2520due%2520to%2520biases%2520in%2520length%2520and%2520sample%2520difficulty%252C%2520leading%2520to%250Aunder-optimization%2520on%2520harder%2520cases.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520three%250Atargeted%2520solutions.%2520First%252C%2520we%2520adopt%2520a%2520Fast%2520Thinking%2520Template%2520that%2520encourages%250Adirect%2520answer%2520generation%252C%2520reducing%2520excessive%2520reasoning%2520during%2520training.%2520Second%252C%250Awe%2520incorporate%2520a%2520box%2520size%2520constraint%2520into%2520the%2520reward%2520function%2520to%2520mitigate%250Areward%2520hacking.%2520Third%252C%2520we%2520revise%2520the%2520RL%2520objective%2520by%2520adjusting%2520length%250Anormalization%2520and%2520adding%2520a%2520difficulty-aware%2520scaling%2520factor%252C%2520enabling%2520better%250Aoptimization%2520on%2520hard%2520samples.%2520Our%2520GUI-G1-3B%252C%2520trained%2520on%252017K%2520public%2520samples%2520with%250AQwen2.5-VL-3B-Instruct%252C%2520achieves%252090.3%2525%2520accuracy%2520on%2520ScreenSpot%2520and%252037.1%2525%2520on%250AScreenSpot-Pro.%2520This%2520surpasses%2520all%2520prior%2520models%2520of%2520similar%2520size%2520and%2520even%250Aoutperforms%2520the%2520larger%2520UI-TARS-7B%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%2520GUI%250Aagent%2520grounding.%2520The%2520project%2520repository%2520is%2520available%2520at%250Ahttps%253A//github.com/Yuqi-Zhou/GUI-G1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-G1%3A%20Understanding%20R1-Zero-Like%20Training%20for%20Visual%20Grounding%20in%20GUI%0A%20%20Agents&entry.906535625=Yuqi%20Zhou%20and%20Sunhao%20Dai%20and%20Shuai%20Wang%20and%20Kaiwen%20Zhou%20and%20Qinglin%20Jia%20and%20Jun%20Xu&entry.1292438233=%20%20Recent%20Graphical%20User%20Interface%20%28GUI%29%20agents%20replicate%20the%20R1-Zero%20paradigm%2C%0Acoupling%20online%20Reinforcement%20Learning%20%28RL%29%20with%20explicit%20chain-of-thought%0Areasoning%20prior%20to%20object%20grounding%20and%20thereby%20achieving%20substantial%0Aperformance%20gains.%20In%20this%20paper%2C%20we%20first%20conduct%20extensive%20analysis%0Aexperiments%20of%20three%20key%20components%20of%20that%20training%20pipeline%3A%20input%20design%2C%0Aoutput%20evaluation%2C%20and%20policy%20update-each%20revealing%20distinct%20challenges%20arising%0Afrom%20blindly%20applying%20general-purpose%20RL%20without%20adapting%20to%20GUI%20grounding%0Atasks.%20Input%20design%3A%20Current%20templates%20encourage%20the%20model%20to%20generate%0Achain-of-thought%20reasoning%2C%20but%20longer%20chains%20unexpectedly%20lead%20to%20worse%0Agrounding%20performance.%20Output%20evaluation%3A%20Reward%20functions%20based%20on%20hit%20signals%0Aor%20box%20area%20allow%20models%20to%20exploit%20box%20size%2C%20leading%20to%20reward%20hacking%20and%0Apoor%20localization%20quality.%20Policy%20update%3A%20Online%20RL%20tends%20to%20overfit%20easy%0Aexamples%20due%20to%20biases%20in%20length%20and%20sample%20difficulty%2C%20leading%20to%0Aunder-optimization%20on%20harder%20cases.%20To%20address%20these%20issues%2C%20we%20propose%20three%0Atargeted%20solutions.%20First%2C%20we%20adopt%20a%20Fast%20Thinking%20Template%20that%20encourages%0Adirect%20answer%20generation%2C%20reducing%20excessive%20reasoning%20during%20training.%20Second%2C%0Awe%20incorporate%20a%20box%20size%20constraint%20into%20the%20reward%20function%20to%20mitigate%0Areward%20hacking.%20Third%2C%20we%20revise%20the%20RL%20objective%20by%20adjusting%20length%0Anormalization%20and%20adding%20a%20difficulty-aware%20scaling%20factor%2C%20enabling%20better%0Aoptimization%20on%20hard%20samples.%20Our%20GUI-G1-3B%2C%20trained%20on%2017K%20public%20samples%20with%0AQwen2.5-VL-3B-Instruct%2C%20achieves%2090.3%25%20accuracy%20on%20ScreenSpot%20and%2037.1%25%20on%0AScreenSpot-Pro.%20This%20surpasses%20all%20prior%20models%20of%20similar%20size%20and%20even%0Aoutperforms%20the%20larger%20UI-TARS-7B%2C%20establishing%20a%20new%20state-of-the-art%20in%20GUI%0Aagent%20grounding.%20The%20project%20repository%20is%20available%20at%0Ahttps%3A//github.com/Yuqi-Zhou/GUI-G1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15810v2&entry.124074799=Read"},
{"title": "Representation Discrepancy Bridging Method for Remote Sensing Image-Text\n  Retrieval", "author": "Hailong Ning and Siying Wang and Tao Lei and Xiaopeng Cao and Huanmin Dou and Bin Zhao and Asoke K. Nandi and Petia Radeva", "abstract": "  Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in\ngeographic information interpretation, disaster monitoring, and urban planning\nby establishing semantic associations between image and textual descriptions.\nExisting Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language\nPre-training (VLP) models typically adopt symmetric adapter structures for\nexploring cross-modal correlations. However, the strong discriminative nature\nof text modality may dominate the optimization process and inhibits image\nrepresentation learning. The nonnegligible imbalanced cross-modal optimization\nremains a bottleneck to enhancing the model performance. To address this issue,\nthis study proposes a Representation Discrepancy Bridging (RDB) method for the\nRSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is\ndesigned to enable modality-specific optimization and improve feature\nalignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text\nSemantic Adapter (TSA). VEA mines fine-grained image features by Differential\nAttention (DA) mechanism, while TSA identifies key textual semantics through\nHierarchical Attention (HA) mechanism. On the other hand, this study extends\nthe traditional single-task retrieval framework to a dual-task optimization\nframework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves\ncross-modal alignment robustness through an adaptive weighted combination of\ncross-modal, classification, and exponential moving average consistency\nconstraints. Experiments on RSICD and RSITMD datasets show that the proposed\nRDB method achieves a 6%-11% improvement in mR metrics compared to\nstate-of-the-art PEFT methods and a 1.15%-2% improvement over the full\nfine-tuned GeoRSCLIP model.\n", "link": "http://arxiv.org/abs/2505.16756v1", "date": "2025-05-22", "relevancy": 2.7942, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Discrepancy%20Bridging%20Method%20for%20Remote%20Sensing%20Image-Text%0A%20%20Retrieval&body=Title%3A%20Representation%20Discrepancy%20Bridging%20Method%20for%20Remote%20Sensing%20Image-Text%0A%20%20Retrieval%0AAuthor%3A%20Hailong%20Ning%20and%20Siying%20Wang%20and%20Tao%20Lei%20and%20Xiaopeng%20Cao%20and%20Huanmin%20Dou%20and%20Bin%20Zhao%20and%20Asoke%20K.%20Nandi%20and%20Petia%20Radeva%0AAbstract%3A%20%20%20Remote%20Sensing%20Image-Text%20Retrieval%20%28RSITR%29%20plays%20a%20critical%20role%20in%0Ageographic%20information%20interpretation%2C%20disaster%20monitoring%2C%20and%20urban%20planning%0Aby%20establishing%20semantic%20associations%20between%20image%20and%20textual%20descriptions.%0AExisting%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20for%20Vision-and-Language%0APre-training%20%28VLP%29%20models%20typically%20adopt%20symmetric%20adapter%20structures%20for%0Aexploring%20cross-modal%20correlations.%20However%2C%20the%20strong%20discriminative%20nature%0Aof%20text%20modality%20may%20dominate%20the%20optimization%20process%20and%20inhibits%20image%0Arepresentation%20learning.%20The%20nonnegligible%20imbalanced%20cross-modal%20optimization%0Aremains%20a%20bottleneck%20to%20enhancing%20the%20model%20performance.%20To%20address%20this%20issue%2C%0Athis%20study%20proposes%20a%20Representation%20Discrepancy%20Bridging%20%28RDB%29%20method%20for%20the%0ARSITR%20task.%20On%20the%20one%20hand%2C%20a%20Cross-Modal%20Asymmetric%20Adapter%20%28CMAA%29%20is%0Adesigned%20to%20enable%20modality-specific%20optimization%20and%20improve%20feature%0Aalignment.%20The%20CMAA%20comprises%20a%20Visual%20Enhancement%20Adapter%20%28VEA%29%20and%20a%20Text%0ASemantic%20Adapter%20%28TSA%29.%20VEA%20mines%20fine-grained%20image%20features%20by%20Differential%0AAttention%20%28DA%29%20mechanism%2C%20while%20TSA%20identifies%20key%20textual%20semantics%20through%0AHierarchical%20Attention%20%28HA%29%20mechanism.%20On%20the%20other%20hand%2C%20this%20study%20extends%0Athe%20traditional%20single-task%20retrieval%20framework%20to%20a%20dual-task%20optimization%0Aframework%20and%20develops%20a%20Dual-Task%20Consistency%20Loss%20%28DTCL%29.%20The%20DTCL%20improves%0Across-modal%20alignment%20robustness%20through%20an%20adaptive%20weighted%20combination%20of%0Across-modal%2C%20classification%2C%20and%20exponential%20moving%20average%20consistency%0Aconstraints.%20Experiments%20on%20RSICD%20and%20RSITMD%20datasets%20show%20that%20the%20proposed%0ARDB%20method%20achieves%20a%206%25-11%25%20improvement%20in%20mR%20metrics%20compared%20to%0Astate-of-the-art%20PEFT%20methods%20and%20a%201.15%25-2%25%20improvement%20over%20the%20full%0Afine-tuned%20GeoRSCLIP%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Discrepancy%2520Bridging%2520Method%2520for%2520Remote%2520Sensing%2520Image-Text%250A%2520%2520Retrieval%26entry.906535625%3DHailong%2520Ning%2520and%2520Siying%2520Wang%2520and%2520Tao%2520Lei%2520and%2520Xiaopeng%2520Cao%2520and%2520Huanmin%2520Dou%2520and%2520Bin%2520Zhao%2520and%2520Asoke%2520K.%2520Nandi%2520and%2520Petia%2520Radeva%26entry.1292438233%3D%2520%2520Remote%2520Sensing%2520Image-Text%2520Retrieval%2520%2528RSITR%2529%2520plays%2520a%2520critical%2520role%2520in%250Ageographic%2520information%2520interpretation%252C%2520disaster%2520monitoring%252C%2520and%2520urban%2520planning%250Aby%2520establishing%2520semantic%2520associations%2520between%2520image%2520and%2520textual%2520descriptions.%250AExisting%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520for%2520Vision-and-Language%250APre-training%2520%2528VLP%2529%2520models%2520typically%2520adopt%2520symmetric%2520adapter%2520structures%2520for%250Aexploring%2520cross-modal%2520correlations.%2520However%252C%2520the%2520strong%2520discriminative%2520nature%250Aof%2520text%2520modality%2520may%2520dominate%2520the%2520optimization%2520process%2520and%2520inhibits%2520image%250Arepresentation%2520learning.%2520The%2520nonnegligible%2520imbalanced%2520cross-modal%2520optimization%250Aremains%2520a%2520bottleneck%2520to%2520enhancing%2520the%2520model%2520performance.%2520To%2520address%2520this%2520issue%252C%250Athis%2520study%2520proposes%2520a%2520Representation%2520Discrepancy%2520Bridging%2520%2528RDB%2529%2520method%2520for%2520the%250ARSITR%2520task.%2520On%2520the%2520one%2520hand%252C%2520a%2520Cross-Modal%2520Asymmetric%2520Adapter%2520%2528CMAA%2529%2520is%250Adesigned%2520to%2520enable%2520modality-specific%2520optimization%2520and%2520improve%2520feature%250Aalignment.%2520The%2520CMAA%2520comprises%2520a%2520Visual%2520Enhancement%2520Adapter%2520%2528VEA%2529%2520and%2520a%2520Text%250ASemantic%2520Adapter%2520%2528TSA%2529.%2520VEA%2520mines%2520fine-grained%2520image%2520features%2520by%2520Differential%250AAttention%2520%2528DA%2529%2520mechanism%252C%2520while%2520TSA%2520identifies%2520key%2520textual%2520semantics%2520through%250AHierarchical%2520Attention%2520%2528HA%2529%2520mechanism.%2520On%2520the%2520other%2520hand%252C%2520this%2520study%2520extends%250Athe%2520traditional%2520single-task%2520retrieval%2520framework%2520to%2520a%2520dual-task%2520optimization%250Aframework%2520and%2520develops%2520a%2520Dual-Task%2520Consistency%2520Loss%2520%2528DTCL%2529.%2520The%2520DTCL%2520improves%250Across-modal%2520alignment%2520robustness%2520through%2520an%2520adaptive%2520weighted%2520combination%2520of%250Across-modal%252C%2520classification%252C%2520and%2520exponential%2520moving%2520average%2520consistency%250Aconstraints.%2520Experiments%2520on%2520RSICD%2520and%2520RSITMD%2520datasets%2520show%2520that%2520the%2520proposed%250ARDB%2520method%2520achieves%2520a%25206%2525-11%2525%2520improvement%2520in%2520mR%2520metrics%2520compared%2520to%250Astate-of-the-art%2520PEFT%2520methods%2520and%2520a%25201.15%2525-2%2525%2520improvement%2520over%2520the%2520full%250Afine-tuned%2520GeoRSCLIP%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Discrepancy%20Bridging%20Method%20for%20Remote%20Sensing%20Image-Text%0A%20%20Retrieval&entry.906535625=Hailong%20Ning%20and%20Siying%20Wang%20and%20Tao%20Lei%20and%20Xiaopeng%20Cao%20and%20Huanmin%20Dou%20and%20Bin%20Zhao%20and%20Asoke%20K.%20Nandi%20and%20Petia%20Radeva&entry.1292438233=%20%20Remote%20Sensing%20Image-Text%20Retrieval%20%28RSITR%29%20plays%20a%20critical%20role%20in%0Ageographic%20information%20interpretation%2C%20disaster%20monitoring%2C%20and%20urban%20planning%0Aby%20establishing%20semantic%20associations%20between%20image%20and%20textual%20descriptions.%0AExisting%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20for%20Vision-and-Language%0APre-training%20%28VLP%29%20models%20typically%20adopt%20symmetric%20adapter%20structures%20for%0Aexploring%20cross-modal%20correlations.%20However%2C%20the%20strong%20discriminative%20nature%0Aof%20text%20modality%20may%20dominate%20the%20optimization%20process%20and%20inhibits%20image%0Arepresentation%20learning.%20The%20nonnegligible%20imbalanced%20cross-modal%20optimization%0Aremains%20a%20bottleneck%20to%20enhancing%20the%20model%20performance.%20To%20address%20this%20issue%2C%0Athis%20study%20proposes%20a%20Representation%20Discrepancy%20Bridging%20%28RDB%29%20method%20for%20the%0ARSITR%20task.%20On%20the%20one%20hand%2C%20a%20Cross-Modal%20Asymmetric%20Adapter%20%28CMAA%29%20is%0Adesigned%20to%20enable%20modality-specific%20optimization%20and%20improve%20feature%0Aalignment.%20The%20CMAA%20comprises%20a%20Visual%20Enhancement%20Adapter%20%28VEA%29%20and%20a%20Text%0ASemantic%20Adapter%20%28TSA%29.%20VEA%20mines%20fine-grained%20image%20features%20by%20Differential%0AAttention%20%28DA%29%20mechanism%2C%20while%20TSA%20identifies%20key%20textual%20semantics%20through%0AHierarchical%20Attention%20%28HA%29%20mechanism.%20On%20the%20other%20hand%2C%20this%20study%20extends%0Athe%20traditional%20single-task%20retrieval%20framework%20to%20a%20dual-task%20optimization%0Aframework%20and%20develops%20a%20Dual-Task%20Consistency%20Loss%20%28DTCL%29.%20The%20DTCL%20improves%0Across-modal%20alignment%20robustness%20through%20an%20adaptive%20weighted%20combination%20of%0Across-modal%2C%20classification%2C%20and%20exponential%20moving%20average%20consistency%0Aconstraints.%20Experiments%20on%20RSICD%20and%20RSITMD%20datasets%20show%20that%20the%20proposed%0ARDB%20method%20achieves%20a%206%25-11%25%20improvement%20in%20mR%20metrics%20compared%20to%0Astate-of-the-art%20PEFT%20methods%20and%20a%201.15%25-2%25%20improvement%20over%20the%20full%0Afine-tuned%20GeoRSCLIP%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16756v1&entry.124074799=Read"},
{"title": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets\n  Visual RAG", "author": "Wenbin Wang and Yongcheng Jing and Liang Ding and Yingjie Wang and Li Shen and Yong Luo and Bo Du and Dacheng Tao", "abstract": "  High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench.\n", "link": "http://arxiv.org/abs/2503.01222v2", "date": "2025-05-22", "relevancy": 2.7862, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-Augmented%20Perception%3A%20High-Resolution%20Image%20Perception%20Meets%0A%20%20Visual%20RAG&body=Title%3A%20Retrieval-Augmented%20Perception%3A%20High-Resolution%20Image%20Perception%20Meets%0A%20%20Visual%20RAG%0AAuthor%3A%20Wenbin%20Wang%20and%20Yongcheng%20Jing%20and%20Liang%20Ding%20and%20Yingjie%20Wang%20and%20Li%20Shen%20and%20Yong%20Luo%20and%20Bo%20Du%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20High-resolution%20%28HR%29%20image%20perception%20remains%20a%20key%20challenge%20in%20multimodal%0Alarge%20language%20models%20%28MLLMs%29.%20To%20overcome%20the%20limitations%20of%20existing%20methods%2C%0Athis%20paper%20shifts%20away%20from%20prior%20dedicated%20heuristic%20approaches%20and%20revisits%0Athe%20most%20fundamental%20idea%20to%20HR%20perception%20by%20enhancing%20the%20long-context%0Acapability%20of%20MLLMs%2C%20driven%20by%20recent%20advances%20in%20long-context%20techniques%20like%0Aretrieval-augmented%20generation%20%28RAG%29%20for%20general%20LLMs.%20Towards%20this%20end%2C%20this%0Apaper%20presents%20the%20first%20study%20exploring%20the%20use%20of%20RAG%20to%20address%20HR%0Aperception%20challenges.%20Specifically%2C%20we%20propose%20Retrieval-Augmented%20Perception%0A%28RAP%29%2C%20a%20training-free%20framework%20that%20retrieves%20and%20fuses%20relevant%20image%20crops%0Awhile%20preserving%20spatial%20context%20using%20the%20proposed%20Spatial-Awareness%20Layout.%0ATo%20accommodate%20different%20tasks%2C%20the%20proposed%20Retrieved-Exploration%20Search%0A%28RE-Search%29%20dynamically%20selects%20the%20optimal%20number%20of%20crops%20based%20on%20model%0Aconfidence%20and%20retrieval%20scores.%20Experimental%20results%20on%20HR%20benchmarks%0Ademonstrate%20the%20significant%20effectiveness%20of%20RAP%2C%20with%20LLaVA-v1.5-13B%20achieving%0Aa%2043%25%20improvement%20on%20%24V%5E%2A%24%20Bench%20and%2019%25%20on%20HR-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-Augmented%2520Perception%253A%2520High-Resolution%2520Image%2520Perception%2520Meets%250A%2520%2520Visual%2520RAG%26entry.906535625%3DWenbin%2520Wang%2520and%2520Yongcheng%2520Jing%2520and%2520Liang%2520Ding%2520and%2520Yingjie%2520Wang%2520and%2520Li%2520Shen%2520and%2520Yong%2520Luo%2520and%2520Bo%2520Du%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520High-resolution%2520%2528HR%2529%2520image%2520perception%2520remains%2520a%2520key%2520challenge%2520in%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529.%2520To%2520overcome%2520the%2520limitations%2520of%2520existing%2520methods%252C%250Athis%2520paper%2520shifts%2520away%2520from%2520prior%2520dedicated%2520heuristic%2520approaches%2520and%2520revisits%250Athe%2520most%2520fundamental%2520idea%2520to%2520HR%2520perception%2520by%2520enhancing%2520the%2520long-context%250Acapability%2520of%2520MLLMs%252C%2520driven%2520by%2520recent%2520advances%2520in%2520long-context%2520techniques%2520like%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520for%2520general%2520LLMs.%2520Towards%2520this%2520end%252C%2520this%250Apaper%2520presents%2520the%2520first%2520study%2520exploring%2520the%2520use%2520of%2520RAG%2520to%2520address%2520HR%250Aperception%2520challenges.%2520Specifically%252C%2520we%2520propose%2520Retrieval-Augmented%2520Perception%250A%2528RAP%2529%252C%2520a%2520training-free%2520framework%2520that%2520retrieves%2520and%2520fuses%2520relevant%2520image%2520crops%250Awhile%2520preserving%2520spatial%2520context%2520using%2520the%2520proposed%2520Spatial-Awareness%2520Layout.%250ATo%2520accommodate%2520different%2520tasks%252C%2520the%2520proposed%2520Retrieved-Exploration%2520Search%250A%2528RE-Search%2529%2520dynamically%2520selects%2520the%2520optimal%2520number%2520of%2520crops%2520based%2520on%2520model%250Aconfidence%2520and%2520retrieval%2520scores.%2520Experimental%2520results%2520on%2520HR%2520benchmarks%250Ademonstrate%2520the%2520significant%2520effectiveness%2520of%2520RAP%252C%2520with%2520LLaVA-v1.5-13B%2520achieving%250Aa%252043%2525%2520improvement%2520on%2520%2524V%255E%252A%2524%2520Bench%2520and%252019%2525%2520on%2520HR-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Augmented%20Perception%3A%20High-Resolution%20Image%20Perception%20Meets%0A%20%20Visual%20RAG&entry.906535625=Wenbin%20Wang%20and%20Yongcheng%20Jing%20and%20Liang%20Ding%20and%20Yingjie%20Wang%20and%20Li%20Shen%20and%20Yong%20Luo%20and%20Bo%20Du%20and%20Dacheng%20Tao&entry.1292438233=%20%20High-resolution%20%28HR%29%20image%20perception%20remains%20a%20key%20challenge%20in%20multimodal%0Alarge%20language%20models%20%28MLLMs%29.%20To%20overcome%20the%20limitations%20of%20existing%20methods%2C%0Athis%20paper%20shifts%20away%20from%20prior%20dedicated%20heuristic%20approaches%20and%20revisits%0Athe%20most%20fundamental%20idea%20to%20HR%20perception%20by%20enhancing%20the%20long-context%0Acapability%20of%20MLLMs%2C%20driven%20by%20recent%20advances%20in%20long-context%20techniques%20like%0Aretrieval-augmented%20generation%20%28RAG%29%20for%20general%20LLMs.%20Towards%20this%20end%2C%20this%0Apaper%20presents%20the%20first%20study%20exploring%20the%20use%20of%20RAG%20to%20address%20HR%0Aperception%20challenges.%20Specifically%2C%20we%20propose%20Retrieval-Augmented%20Perception%0A%28RAP%29%2C%20a%20training-free%20framework%20that%20retrieves%20and%20fuses%20relevant%20image%20crops%0Awhile%20preserving%20spatial%20context%20using%20the%20proposed%20Spatial-Awareness%20Layout.%0ATo%20accommodate%20different%20tasks%2C%20the%20proposed%20Retrieved-Exploration%20Search%0A%28RE-Search%29%20dynamically%20selects%20the%20optimal%20number%20of%20crops%20based%20on%20model%0Aconfidence%20and%20retrieval%20scores.%20Experimental%20results%20on%20HR%20benchmarks%0Ademonstrate%20the%20significant%20effectiveness%20of%20RAP%2C%20with%20LLaVA-v1.5-13B%20achieving%0Aa%2043%25%20improvement%20on%20%24V%5E%2A%24%20Bench%20and%2019%25%20on%20HR-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01222v2&entry.124074799=Read"},
{"title": "DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?", "author": "Qirui Jiao and Daoyuan Chen and Yilun Huang and Xika Lin and Ying Shen and Yaliang Li", "abstract": "  While recent text-to-image (T2I) models show impressive capabilities in\nsynthesizing images from brief descriptions, their performance significantly\ndegrades when confronted with long, detail-intensive prompts required in\nprofessional applications. We present DetailMaster, the first comprehensive\nbenchmark specifically designed to evaluate T2I models' systematical abilities\nto handle extended textual inputs that contain complex compositional\nrequirements. Our benchmark introduces four critical evaluation dimensions:\nCharacter Attributes, Structured Character Locations, Multi-Dimensional Scene\nAttributes, and Explicit Spatial/Interactive Relationships. The benchmark\ncomprises long and detail-rich prompts averaging 284.89 tokens, with high\nquality validated by expert annotators. Evaluation on 7 general-purpose and 5\nlong-prompt-optimized T2I models reveals critical performance limitations:\nstate-of-the-art models achieve merely ~50% accuracy in key dimensions like\nattribute binding and spatial reasoning, while all models showing progressive\nperformance degradation as prompt length increases. Our analysis highlights\nsystemic failures in structural comprehension and detail overload handling,\nmotivating future research into architectures with enhanced compositional\nreasoning. We open-source the dataset, data curation code, and evaluation tools\nto advance detail-rich T2I generation and enable broad applications that would\notherwise be infeasible due to the lack of a dedicated benchmark.\n", "link": "http://arxiv.org/abs/2505.16915v1", "date": "2025-05-22", "relevancy": 2.7713, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DetailMaster%3A%20Can%20Your%20Text-to-Image%20Model%20Handle%20Long%20Prompts%3F&body=Title%3A%20DetailMaster%3A%20Can%20Your%20Text-to-Image%20Model%20Handle%20Long%20Prompts%3F%0AAuthor%3A%20Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Xika%20Lin%20and%20Ying%20Shen%20and%20Yaliang%20Li%0AAbstract%3A%20%20%20While%20recent%20text-to-image%20%28T2I%29%20models%20show%20impressive%20capabilities%20in%0Asynthesizing%20images%20from%20brief%20descriptions%2C%20their%20performance%20significantly%0Adegrades%20when%20confronted%20with%20long%2C%20detail-intensive%20prompts%20required%20in%0Aprofessional%20applications.%20We%20present%20DetailMaster%2C%20the%20first%20comprehensive%0Abenchmark%20specifically%20designed%20to%20evaluate%20T2I%20models%27%20systematical%20abilities%0Ato%20handle%20extended%20textual%20inputs%20that%20contain%20complex%20compositional%0Arequirements.%20Our%20benchmark%20introduces%20four%20critical%20evaluation%20dimensions%3A%0ACharacter%20Attributes%2C%20Structured%20Character%20Locations%2C%20Multi-Dimensional%20Scene%0AAttributes%2C%20and%20Explicit%20Spatial/Interactive%20Relationships.%20The%20benchmark%0Acomprises%20long%20and%20detail-rich%20prompts%20averaging%20284.89%20tokens%2C%20with%20high%0Aquality%20validated%20by%20expert%20annotators.%20Evaluation%20on%207%20general-purpose%20and%205%0Along-prompt-optimized%20T2I%20models%20reveals%20critical%20performance%20limitations%3A%0Astate-of-the-art%20models%20achieve%20merely%20~50%25%20accuracy%20in%20key%20dimensions%20like%0Aattribute%20binding%20and%20spatial%20reasoning%2C%20while%20all%20models%20showing%20progressive%0Aperformance%20degradation%20as%20prompt%20length%20increases.%20Our%20analysis%20highlights%0Asystemic%20failures%20in%20structural%20comprehension%20and%20detail%20overload%20handling%2C%0Amotivating%20future%20research%20into%20architectures%20with%20enhanced%20compositional%0Areasoning.%20We%20open-source%20the%20dataset%2C%20data%20curation%20code%2C%20and%20evaluation%20tools%0Ato%20advance%20detail-rich%20T2I%20generation%20and%20enable%20broad%20applications%20that%20would%0Aotherwise%20be%20infeasible%20due%20to%20the%20lack%20of%20a%20dedicated%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetailMaster%253A%2520Can%2520Your%2520Text-to-Image%2520Model%2520Handle%2520Long%2520Prompts%253F%26entry.906535625%3DQirui%2520Jiao%2520and%2520Daoyuan%2520Chen%2520and%2520Yilun%2520Huang%2520and%2520Xika%2520Lin%2520and%2520Ying%2520Shen%2520and%2520Yaliang%2520Li%26entry.1292438233%3D%2520%2520While%2520recent%2520text-to-image%2520%2528T2I%2529%2520models%2520show%2520impressive%2520capabilities%2520in%250Asynthesizing%2520images%2520from%2520brief%2520descriptions%252C%2520their%2520performance%2520significantly%250Adegrades%2520when%2520confronted%2520with%2520long%252C%2520detail-intensive%2520prompts%2520required%2520in%250Aprofessional%2520applications.%2520We%2520present%2520DetailMaster%252C%2520the%2520first%2520comprehensive%250Abenchmark%2520specifically%2520designed%2520to%2520evaluate%2520T2I%2520models%2527%2520systematical%2520abilities%250Ato%2520handle%2520extended%2520textual%2520inputs%2520that%2520contain%2520complex%2520compositional%250Arequirements.%2520Our%2520benchmark%2520introduces%2520four%2520critical%2520evaluation%2520dimensions%253A%250ACharacter%2520Attributes%252C%2520Structured%2520Character%2520Locations%252C%2520Multi-Dimensional%2520Scene%250AAttributes%252C%2520and%2520Explicit%2520Spatial/Interactive%2520Relationships.%2520The%2520benchmark%250Acomprises%2520long%2520and%2520detail-rich%2520prompts%2520averaging%2520284.89%2520tokens%252C%2520with%2520high%250Aquality%2520validated%2520by%2520expert%2520annotators.%2520Evaluation%2520on%25207%2520general-purpose%2520and%25205%250Along-prompt-optimized%2520T2I%2520models%2520reveals%2520critical%2520performance%2520limitations%253A%250Astate-of-the-art%2520models%2520achieve%2520merely%2520~50%2525%2520accuracy%2520in%2520key%2520dimensions%2520like%250Aattribute%2520binding%2520and%2520spatial%2520reasoning%252C%2520while%2520all%2520models%2520showing%2520progressive%250Aperformance%2520degradation%2520as%2520prompt%2520length%2520increases.%2520Our%2520analysis%2520highlights%250Asystemic%2520failures%2520in%2520structural%2520comprehension%2520and%2520detail%2520overload%2520handling%252C%250Amotivating%2520future%2520research%2520into%2520architectures%2520with%2520enhanced%2520compositional%250Areasoning.%2520We%2520open-source%2520the%2520dataset%252C%2520data%2520curation%2520code%252C%2520and%2520evaluation%2520tools%250Ato%2520advance%2520detail-rich%2520T2I%2520generation%2520and%2520enable%2520broad%2520applications%2520that%2520would%250Aotherwise%2520be%2520infeasible%2520due%2520to%2520the%2520lack%2520of%2520a%2520dedicated%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DetailMaster%3A%20Can%20Your%20Text-to-Image%20Model%20Handle%20Long%20Prompts%3F&entry.906535625=Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Xika%20Lin%20and%20Ying%20Shen%20and%20Yaliang%20Li&entry.1292438233=%20%20While%20recent%20text-to-image%20%28T2I%29%20models%20show%20impressive%20capabilities%20in%0Asynthesizing%20images%20from%20brief%20descriptions%2C%20their%20performance%20significantly%0Adegrades%20when%20confronted%20with%20long%2C%20detail-intensive%20prompts%20required%20in%0Aprofessional%20applications.%20We%20present%20DetailMaster%2C%20the%20first%20comprehensive%0Abenchmark%20specifically%20designed%20to%20evaluate%20T2I%20models%27%20systematical%20abilities%0Ato%20handle%20extended%20textual%20inputs%20that%20contain%20complex%20compositional%0Arequirements.%20Our%20benchmark%20introduces%20four%20critical%20evaluation%20dimensions%3A%0ACharacter%20Attributes%2C%20Structured%20Character%20Locations%2C%20Multi-Dimensional%20Scene%0AAttributes%2C%20and%20Explicit%20Spatial/Interactive%20Relationships.%20The%20benchmark%0Acomprises%20long%20and%20detail-rich%20prompts%20averaging%20284.89%20tokens%2C%20with%20high%0Aquality%20validated%20by%20expert%20annotators.%20Evaluation%20on%207%20general-purpose%20and%205%0Along-prompt-optimized%20T2I%20models%20reveals%20critical%20performance%20limitations%3A%0Astate-of-the-art%20models%20achieve%20merely%20~50%25%20accuracy%20in%20key%20dimensions%20like%0Aattribute%20binding%20and%20spatial%20reasoning%2C%20while%20all%20models%20showing%20progressive%0Aperformance%20degradation%20as%20prompt%20length%20increases.%20Our%20analysis%20highlights%0Asystemic%20failures%20in%20structural%20comprehension%20and%20detail%20overload%20handling%2C%0Amotivating%20future%20research%20into%20architectures%20with%20enhanced%20compositional%0Areasoning.%20We%20open-source%20the%20dataset%2C%20data%20curation%20code%2C%20and%20evaluation%20tools%0Ato%20advance%20detail-rich%20T2I%20generation%20and%20enable%20broad%20applications%20that%20would%0Aotherwise%20be%20infeasible%20due%20to%20the%20lack%20of%20a%20dedicated%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16915v1&entry.124074799=Read"},
{"title": "Progressive Local Alignment for Medical Multimodal Pre-training", "author": "Huimin Yan and Xian Yang and Liang Bai and Jiye Liang", "abstract": "  Local alignment between medical images and text is essential for accurate\ndiagnosis, though it remains challenging due to the absence of natural local\npairings and the limitations of rigid region recognition methods. Traditional\napproaches rely on hard boundaries, which introduce uncertainty, whereas\nmedical imaging demands flexible soft region recognition to handle irregular\nstructures. To overcome these challenges, we propose the Progressive Local\nAlignment Network (PLAN), which designs a novel contrastive learning-based\napproach for local alignment to establish meaningful word-pixel relationships\nand introduces a progressive learning strategy to iteratively refine these\nrelationships, enhancing alignment precision and robustness. By combining these\ntechniques, PLAN effectively improves soft region recognition while suppressing\nnoise interference. Extensive experiments on multiple medical datasets\ndemonstrate that PLAN surpasses state-of-the-art methods in phrase grounding,\nimage-text retrieval, object detection, and zero-shot classification, setting a\nnew benchmark for medical image-text alignment.\n", "link": "http://arxiv.org/abs/2502.18047v2", "date": "2025-05-22", "relevancy": 2.7672, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Local%20Alignment%20for%20Medical%20Multimodal%20Pre-training&body=Title%3A%20Progressive%20Local%20Alignment%20for%20Medical%20Multimodal%20Pre-training%0AAuthor%3A%20Huimin%20Yan%20and%20Xian%20Yang%20and%20Liang%20Bai%20and%20Jiye%20Liang%0AAbstract%3A%20%20%20Local%20alignment%20between%20medical%20images%20and%20text%20is%20essential%20for%20accurate%0Adiagnosis%2C%20though%20it%20remains%20challenging%20due%20to%20the%20absence%20of%20natural%20local%0Apairings%20and%20the%20limitations%20of%20rigid%20region%20recognition%20methods.%20Traditional%0Aapproaches%20rely%20on%20hard%20boundaries%2C%20which%20introduce%20uncertainty%2C%20whereas%0Amedical%20imaging%20demands%20flexible%20soft%20region%20recognition%20to%20handle%20irregular%0Astructures.%20To%20overcome%20these%20challenges%2C%20we%20propose%20the%20Progressive%20Local%0AAlignment%20Network%20%28PLAN%29%2C%20which%20designs%20a%20novel%20contrastive%20learning-based%0Aapproach%20for%20local%20alignment%20to%20establish%20meaningful%20word-pixel%20relationships%0Aand%20introduces%20a%20progressive%20learning%20strategy%20to%20iteratively%20refine%20these%0Arelationships%2C%20enhancing%20alignment%20precision%20and%20robustness.%20By%20combining%20these%0Atechniques%2C%20PLAN%20effectively%20improves%20soft%20region%20recognition%20while%20suppressing%0Anoise%20interference.%20Extensive%20experiments%20on%20multiple%20medical%20datasets%0Ademonstrate%20that%20PLAN%20surpasses%20state-of-the-art%20methods%20in%20phrase%20grounding%2C%0Aimage-text%20retrieval%2C%20object%20detection%2C%20and%20zero-shot%20classification%2C%20setting%20a%0Anew%20benchmark%20for%20medical%20image-text%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Local%2520Alignment%2520for%2520Medical%2520Multimodal%2520Pre-training%26entry.906535625%3DHuimin%2520Yan%2520and%2520Xian%2520Yang%2520and%2520Liang%2520Bai%2520and%2520Jiye%2520Liang%26entry.1292438233%3D%2520%2520Local%2520alignment%2520between%2520medical%2520images%2520and%2520text%2520is%2520essential%2520for%2520accurate%250Adiagnosis%252C%2520though%2520it%2520remains%2520challenging%2520due%2520to%2520the%2520absence%2520of%2520natural%2520local%250Apairings%2520and%2520the%2520limitations%2520of%2520rigid%2520region%2520recognition%2520methods.%2520Traditional%250Aapproaches%2520rely%2520on%2520hard%2520boundaries%252C%2520which%2520introduce%2520uncertainty%252C%2520whereas%250Amedical%2520imaging%2520demands%2520flexible%2520soft%2520region%2520recognition%2520to%2520handle%2520irregular%250Astructures.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520the%2520Progressive%2520Local%250AAlignment%2520Network%2520%2528PLAN%2529%252C%2520which%2520designs%2520a%2520novel%2520contrastive%2520learning-based%250Aapproach%2520for%2520local%2520alignment%2520to%2520establish%2520meaningful%2520word-pixel%2520relationships%250Aand%2520introduces%2520a%2520progressive%2520learning%2520strategy%2520to%2520iteratively%2520refine%2520these%250Arelationships%252C%2520enhancing%2520alignment%2520precision%2520and%2520robustness.%2520By%2520combining%2520these%250Atechniques%252C%2520PLAN%2520effectively%2520improves%2520soft%2520region%2520recognition%2520while%2520suppressing%250Anoise%2520interference.%2520Extensive%2520experiments%2520on%2520multiple%2520medical%2520datasets%250Ademonstrate%2520that%2520PLAN%2520surpasses%2520state-of-the-art%2520methods%2520in%2520phrase%2520grounding%252C%250Aimage-text%2520retrieval%252C%2520object%2520detection%252C%2520and%2520zero-shot%2520classification%252C%2520setting%2520a%250Anew%2520benchmark%2520for%2520medical%2520image-text%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Local%20Alignment%20for%20Medical%20Multimodal%20Pre-training&entry.906535625=Huimin%20Yan%20and%20Xian%20Yang%20and%20Liang%20Bai%20and%20Jiye%20Liang&entry.1292438233=%20%20Local%20alignment%20between%20medical%20images%20and%20text%20is%20essential%20for%20accurate%0Adiagnosis%2C%20though%20it%20remains%20challenging%20due%20to%20the%20absence%20of%20natural%20local%0Apairings%20and%20the%20limitations%20of%20rigid%20region%20recognition%20methods.%20Traditional%0Aapproaches%20rely%20on%20hard%20boundaries%2C%20which%20introduce%20uncertainty%2C%20whereas%0Amedical%20imaging%20demands%20flexible%20soft%20region%20recognition%20to%20handle%20irregular%0Astructures.%20To%20overcome%20these%20challenges%2C%20we%20propose%20the%20Progressive%20Local%0AAlignment%20Network%20%28PLAN%29%2C%20which%20designs%20a%20novel%20contrastive%20learning-based%0Aapproach%20for%20local%20alignment%20to%20establish%20meaningful%20word-pixel%20relationships%0Aand%20introduces%20a%20progressive%20learning%20strategy%20to%20iteratively%20refine%20these%0Arelationships%2C%20enhancing%20alignment%20precision%20and%20robustness.%20By%20combining%20these%0Atechniques%2C%20PLAN%20effectively%20improves%20soft%20region%20recognition%20while%20suppressing%0Anoise%20interference.%20Extensive%20experiments%20on%20multiple%20medical%20datasets%0Ademonstrate%20that%20PLAN%20surpasses%20state-of-the-art%20methods%20in%20phrase%20grounding%2C%0Aimage-text%20retrieval%2C%20object%20detection%2C%20and%20zero-shot%20classification%2C%20setting%20a%0Anew%20benchmark%20for%20medical%20image-text%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18047v2&entry.124074799=Read"},
{"title": "Incorporating Visual Correspondence into Diffusion Model for Virtual\n  Try-On", "author": "Siqi Wan and Jingwen Chen and Yingwei Pan and Ting Yao and Tao Mei", "abstract": "  Diffusion models have shown preliminary success in virtual try-on (VTON)\ntask. The typical dual-branch architecture comprises two UNets for implicit\ngarment deformation and synthesized image generation respectively, and has\nemerged as the recipe for VTON task. Nevertheless, the problem remains\nchallenging to preserve the shape and every detail of the given garment due to\nthe intrinsic stochasticity of diffusion model. To alleviate this issue, we\nnovelly propose to explicitly capitalize on visual correspondence as the prior\nto tame diffusion process instead of simply feeding the whole garment into UNet\nas the appearance reference. Specifically, we interpret the fine-grained\nappearance and texture details as a set of structured semantic points, and\nmatch the semantic points rooted in garment to the ones over target person\nthrough local flow warping. Such 2D points are then augmented into 3D-aware\ncues with depth/normal map of target person. The correspondence mimics the way\nof putting clothing on human body and the 3D-aware cues act as semantic point\nmatching to supervise diffusion model training. A point-focused diffusion loss\nis further devised to fully take the advantage of semantic point matching.\nExtensive experiments demonstrate strong garment detail preservation of our\napproach, evidenced by state-of-the-art VTON performances on both VITON-HD and\nDressCode datasets. Code is publicly available at:\nhttps://github.com/HiDream-ai/SPM-Diff.\n", "link": "http://arxiv.org/abs/2505.16977v1", "date": "2025-05-22", "relevancy": 2.7513, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7208}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.7109}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Visual%20Correspondence%20into%20Diffusion%20Model%20for%20Virtual%0A%20%20Try-On&body=Title%3A%20Incorporating%20Visual%20Correspondence%20into%20Diffusion%20Model%20for%20Virtual%0A%20%20Try-On%0AAuthor%3A%20Siqi%20Wan%20and%20Jingwen%20Chen%20and%20Yingwei%20Pan%20and%20Ting%20Yao%20and%20Tao%20Mei%0AAbstract%3A%20%20%20Diffusion%20models%20have%20shown%20preliminary%20success%20in%20virtual%20try-on%20%28VTON%29%0Atask.%20The%20typical%20dual-branch%20architecture%20comprises%20two%20UNets%20for%20implicit%0Agarment%20deformation%20and%20synthesized%20image%20generation%20respectively%2C%20and%20has%0Aemerged%20as%20the%20recipe%20for%20VTON%20task.%20Nevertheless%2C%20the%20problem%20remains%0Achallenging%20to%20preserve%20the%20shape%20and%20every%20detail%20of%20the%20given%20garment%20due%20to%0Athe%20intrinsic%20stochasticity%20of%20diffusion%20model.%20To%20alleviate%20this%20issue%2C%20we%0Anovelly%20propose%20to%20explicitly%20capitalize%20on%20visual%20correspondence%20as%20the%20prior%0Ato%20tame%20diffusion%20process%20instead%20of%20simply%20feeding%20the%20whole%20garment%20into%20UNet%0Aas%20the%20appearance%20reference.%20Specifically%2C%20we%20interpret%20the%20fine-grained%0Aappearance%20and%20texture%20details%20as%20a%20set%20of%20structured%20semantic%20points%2C%20and%0Amatch%20the%20semantic%20points%20rooted%20in%20garment%20to%20the%20ones%20over%20target%20person%0Athrough%20local%20flow%20warping.%20Such%202D%20points%20are%20then%20augmented%20into%203D-aware%0Acues%20with%20depth/normal%20map%20of%20target%20person.%20The%20correspondence%20mimics%20the%20way%0Aof%20putting%20clothing%20on%20human%20body%20and%20the%203D-aware%20cues%20act%20as%20semantic%20point%0Amatching%20to%20supervise%20diffusion%20model%20training.%20A%20point-focused%20diffusion%20loss%0Ais%20further%20devised%20to%20fully%20take%20the%20advantage%20of%20semantic%20point%20matching.%0AExtensive%20experiments%20demonstrate%20strong%20garment%20detail%20preservation%20of%20our%0Aapproach%2C%20evidenced%20by%20state-of-the-art%20VTON%20performances%20on%20both%20VITON-HD%20and%0ADressCode%20datasets.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/HiDream-ai/SPM-Diff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Visual%2520Correspondence%2520into%2520Diffusion%2520Model%2520for%2520Virtual%250A%2520%2520Try-On%26entry.906535625%3DSiqi%2520Wan%2520and%2520Jingwen%2520Chen%2520and%2520Yingwei%2520Pan%2520and%2520Ting%2520Yao%2520and%2520Tao%2520Mei%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520shown%2520preliminary%2520success%2520in%2520virtual%2520try-on%2520%2528VTON%2529%250Atask.%2520The%2520typical%2520dual-branch%2520architecture%2520comprises%2520two%2520UNets%2520for%2520implicit%250Agarment%2520deformation%2520and%2520synthesized%2520image%2520generation%2520respectively%252C%2520and%2520has%250Aemerged%2520as%2520the%2520recipe%2520for%2520VTON%2520task.%2520Nevertheless%252C%2520the%2520problem%2520remains%250Achallenging%2520to%2520preserve%2520the%2520shape%2520and%2520every%2520detail%2520of%2520the%2520given%2520garment%2520due%2520to%250Athe%2520intrinsic%2520stochasticity%2520of%2520diffusion%2520model.%2520To%2520alleviate%2520this%2520issue%252C%2520we%250Anovelly%2520propose%2520to%2520explicitly%2520capitalize%2520on%2520visual%2520correspondence%2520as%2520the%2520prior%250Ato%2520tame%2520diffusion%2520process%2520instead%2520of%2520simply%2520feeding%2520the%2520whole%2520garment%2520into%2520UNet%250Aas%2520the%2520appearance%2520reference.%2520Specifically%252C%2520we%2520interpret%2520the%2520fine-grained%250Aappearance%2520and%2520texture%2520details%2520as%2520a%2520set%2520of%2520structured%2520semantic%2520points%252C%2520and%250Amatch%2520the%2520semantic%2520points%2520rooted%2520in%2520garment%2520to%2520the%2520ones%2520over%2520target%2520person%250Athrough%2520local%2520flow%2520warping.%2520Such%25202D%2520points%2520are%2520then%2520augmented%2520into%25203D-aware%250Acues%2520with%2520depth/normal%2520map%2520of%2520target%2520person.%2520The%2520correspondence%2520mimics%2520the%2520way%250Aof%2520putting%2520clothing%2520on%2520human%2520body%2520and%2520the%25203D-aware%2520cues%2520act%2520as%2520semantic%2520point%250Amatching%2520to%2520supervise%2520diffusion%2520model%2520training.%2520A%2520point-focused%2520diffusion%2520loss%250Ais%2520further%2520devised%2520to%2520fully%2520take%2520the%2520advantage%2520of%2520semantic%2520point%2520matching.%250AExtensive%2520experiments%2520demonstrate%2520strong%2520garment%2520detail%2520preservation%2520of%2520our%250Aapproach%252C%2520evidenced%2520by%2520state-of-the-art%2520VTON%2520performances%2520on%2520both%2520VITON-HD%2520and%250ADressCode%2520datasets.%2520Code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/HiDream-ai/SPM-Diff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Visual%20Correspondence%20into%20Diffusion%20Model%20for%20Virtual%0A%20%20Try-On&entry.906535625=Siqi%20Wan%20and%20Jingwen%20Chen%20and%20Yingwei%20Pan%20and%20Ting%20Yao%20and%20Tao%20Mei&entry.1292438233=%20%20Diffusion%20models%20have%20shown%20preliminary%20success%20in%20virtual%20try-on%20%28VTON%29%0Atask.%20The%20typical%20dual-branch%20architecture%20comprises%20two%20UNets%20for%20implicit%0Agarment%20deformation%20and%20synthesized%20image%20generation%20respectively%2C%20and%20has%0Aemerged%20as%20the%20recipe%20for%20VTON%20task.%20Nevertheless%2C%20the%20problem%20remains%0Achallenging%20to%20preserve%20the%20shape%20and%20every%20detail%20of%20the%20given%20garment%20due%20to%0Athe%20intrinsic%20stochasticity%20of%20diffusion%20model.%20To%20alleviate%20this%20issue%2C%20we%0Anovelly%20propose%20to%20explicitly%20capitalize%20on%20visual%20correspondence%20as%20the%20prior%0Ato%20tame%20diffusion%20process%20instead%20of%20simply%20feeding%20the%20whole%20garment%20into%20UNet%0Aas%20the%20appearance%20reference.%20Specifically%2C%20we%20interpret%20the%20fine-grained%0Aappearance%20and%20texture%20details%20as%20a%20set%20of%20structured%20semantic%20points%2C%20and%0Amatch%20the%20semantic%20points%20rooted%20in%20garment%20to%20the%20ones%20over%20target%20person%0Athrough%20local%20flow%20warping.%20Such%202D%20points%20are%20then%20augmented%20into%203D-aware%0Acues%20with%20depth/normal%20map%20of%20target%20person.%20The%20correspondence%20mimics%20the%20way%0Aof%20putting%20clothing%20on%20human%20body%20and%20the%203D-aware%20cues%20act%20as%20semantic%20point%0Amatching%20to%20supervise%20diffusion%20model%20training.%20A%20point-focused%20diffusion%20loss%0Ais%20further%20devised%20to%20fully%20take%20the%20advantage%20of%20semantic%20point%20matching.%0AExtensive%20experiments%20demonstrate%20strong%20garment%20detail%20preservation%20of%20our%0Aapproach%2C%20evidenced%20by%20state-of-the-art%20VTON%20performances%20on%20both%20VITON-HD%20and%0ADressCode%20datasets.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/HiDream-ai/SPM-Diff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16977v1&entry.124074799=Read"},
{"title": "Background Matters: A Cross-view Bidirectional Modeling Framework for\n  Semi-supervised Medical Image Segmentation", "author": "Luyang Cao and Jianwei Li and Yinghuan Shi", "abstract": "  Semi-supervised medical image segmentation (SSMIS) leverages unlabeled data\nto reduce reliance on manually annotated images. However, current SOTA\napproaches predominantly focus on foreground-oriented modeling (i.e.,\nsegmenting only the foreground region) and have largely overlooked the\npotential benefits of explicitly modeling the background region. Our study\ntheoretically and empirically demonstrates that highly certain predictions in\nbackground modeling enhance the confidence of corresponding foreground\nmodeling. Building on this insight, we propose the Cross-view Bidirectional\nModeling (CVBM) framework, which introduces a novel perspective by\nincorporating background modeling to improve foreground modeling performance.\nWithin CVBM, background modeling serves as an auxiliary perspective, providing\ncomplementary supervisory signals to enhance the confidence of the foreground\nmodel. Additionally, CVBM introduces an innovative bidirectional consistency\nmechanism, which ensures mutual alignment between foreground predictions and\nbackground-guided predictions. Extensive experiments demonstrate that our\napproach achieves SOTA performance on the LA, Pancreas, ACDC, and HRF datasets.\nNotably, on the Pancreas dataset, CVBM outperforms fully supervised methods\n(i.e., DSC: 84.57% vs. 83.89%) while utilizing only 20% of the labeled data.\nOur code is publicly available at https://github.com/caoluyang0830/CVBM.git.\n", "link": "http://arxiv.org/abs/2505.16625v1", "date": "2025-05-22", "relevancy": 2.7465, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Background%20Matters%3A%20A%20Cross-view%20Bidirectional%20Modeling%20Framework%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation&body=Title%3A%20Background%20Matters%3A%20A%20Cross-view%20Bidirectional%20Modeling%20Framework%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation%0AAuthor%3A%20Luyang%20Cao%20and%20Jianwei%20Li%20and%20Yinghuan%20Shi%0AAbstract%3A%20%20%20Semi-supervised%20medical%20image%20segmentation%20%28SSMIS%29%20leverages%20unlabeled%20data%0Ato%20reduce%20reliance%20on%20manually%20annotated%20images.%20However%2C%20current%20SOTA%0Aapproaches%20predominantly%20focus%20on%20foreground-oriented%20modeling%20%28i.e.%2C%0Asegmenting%20only%20the%20foreground%20region%29%20and%20have%20largely%20overlooked%20the%0Apotential%20benefits%20of%20explicitly%20modeling%20the%20background%20region.%20Our%20study%0Atheoretically%20and%20empirically%20demonstrates%20that%20highly%20certain%20predictions%20in%0Abackground%20modeling%20enhance%20the%20confidence%20of%20corresponding%20foreground%0Amodeling.%20Building%20on%20this%20insight%2C%20we%20propose%20the%20Cross-view%20Bidirectional%0AModeling%20%28CVBM%29%20framework%2C%20which%20introduces%20a%20novel%20perspective%20by%0Aincorporating%20background%20modeling%20to%20improve%20foreground%20modeling%20performance.%0AWithin%20CVBM%2C%20background%20modeling%20serves%20as%20an%20auxiliary%20perspective%2C%20providing%0Acomplementary%20supervisory%20signals%20to%20enhance%20the%20confidence%20of%20the%20foreground%0Amodel.%20Additionally%2C%20CVBM%20introduces%20an%20innovative%20bidirectional%20consistency%0Amechanism%2C%20which%20ensures%20mutual%20alignment%20between%20foreground%20predictions%20and%0Abackground-guided%20predictions.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20achieves%20SOTA%20performance%20on%20the%20LA%2C%20Pancreas%2C%20ACDC%2C%20and%20HRF%20datasets.%0ANotably%2C%20on%20the%20Pancreas%20dataset%2C%20CVBM%20outperforms%20fully%20supervised%20methods%0A%28i.e.%2C%20DSC%3A%2084.57%25%20vs.%2083.89%25%29%20while%20utilizing%20only%2020%25%20of%20the%20labeled%20data.%0AOur%20code%20is%20publicly%20available%20at%20https%3A//github.com/caoluyang0830/CVBM.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackground%2520Matters%253A%2520A%2520Cross-view%2520Bidirectional%2520Modeling%2520Framework%2520for%250A%2520%2520Semi-supervised%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DLuyang%2520Cao%2520and%2520Jianwei%2520Li%2520and%2520Yinghuan%2520Shi%26entry.1292438233%3D%2520%2520Semi-supervised%2520medical%2520image%2520segmentation%2520%2528SSMIS%2529%2520leverages%2520unlabeled%2520data%250Ato%2520reduce%2520reliance%2520on%2520manually%2520annotated%2520images.%2520However%252C%2520current%2520SOTA%250Aapproaches%2520predominantly%2520focus%2520on%2520foreground-oriented%2520modeling%2520%2528i.e.%252C%250Asegmenting%2520only%2520the%2520foreground%2520region%2529%2520and%2520have%2520largely%2520overlooked%2520the%250Apotential%2520benefits%2520of%2520explicitly%2520modeling%2520the%2520background%2520region.%2520Our%2520study%250Atheoretically%2520and%2520empirically%2520demonstrates%2520that%2520highly%2520certain%2520predictions%2520in%250Abackground%2520modeling%2520enhance%2520the%2520confidence%2520of%2520corresponding%2520foreground%250Amodeling.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520the%2520Cross-view%2520Bidirectional%250AModeling%2520%2528CVBM%2529%2520framework%252C%2520which%2520introduces%2520a%2520novel%2520perspective%2520by%250Aincorporating%2520background%2520modeling%2520to%2520improve%2520foreground%2520modeling%2520performance.%250AWithin%2520CVBM%252C%2520background%2520modeling%2520serves%2520as%2520an%2520auxiliary%2520perspective%252C%2520providing%250Acomplementary%2520supervisory%2520signals%2520to%2520enhance%2520the%2520confidence%2520of%2520the%2520foreground%250Amodel.%2520Additionally%252C%2520CVBM%2520introduces%2520an%2520innovative%2520bidirectional%2520consistency%250Amechanism%252C%2520which%2520ensures%2520mutual%2520alignment%2520between%2520foreground%2520predictions%2520and%250Abackground-guided%2520predictions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520SOTA%2520performance%2520on%2520the%2520LA%252C%2520Pancreas%252C%2520ACDC%252C%2520and%2520HRF%2520datasets.%250ANotably%252C%2520on%2520the%2520Pancreas%2520dataset%252C%2520CVBM%2520outperforms%2520fully%2520supervised%2520methods%250A%2528i.e.%252C%2520DSC%253A%252084.57%2525%2520vs.%252083.89%2525%2529%2520while%2520utilizing%2520only%252020%2525%2520of%2520the%2520labeled%2520data.%250AOur%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/caoluyang0830/CVBM.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Background%20Matters%3A%20A%20Cross-view%20Bidirectional%20Modeling%20Framework%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation&entry.906535625=Luyang%20Cao%20and%20Jianwei%20Li%20and%20Yinghuan%20Shi&entry.1292438233=%20%20Semi-supervised%20medical%20image%20segmentation%20%28SSMIS%29%20leverages%20unlabeled%20data%0Ato%20reduce%20reliance%20on%20manually%20annotated%20images.%20However%2C%20current%20SOTA%0Aapproaches%20predominantly%20focus%20on%20foreground-oriented%20modeling%20%28i.e.%2C%0Asegmenting%20only%20the%20foreground%20region%29%20and%20have%20largely%20overlooked%20the%0Apotential%20benefits%20of%20explicitly%20modeling%20the%20background%20region.%20Our%20study%0Atheoretically%20and%20empirically%20demonstrates%20that%20highly%20certain%20predictions%20in%0Abackground%20modeling%20enhance%20the%20confidence%20of%20corresponding%20foreground%0Amodeling.%20Building%20on%20this%20insight%2C%20we%20propose%20the%20Cross-view%20Bidirectional%0AModeling%20%28CVBM%29%20framework%2C%20which%20introduces%20a%20novel%20perspective%20by%0Aincorporating%20background%20modeling%20to%20improve%20foreground%20modeling%20performance.%0AWithin%20CVBM%2C%20background%20modeling%20serves%20as%20an%20auxiliary%20perspective%2C%20providing%0Acomplementary%20supervisory%20signals%20to%20enhance%20the%20confidence%20of%20the%20foreground%0Amodel.%20Additionally%2C%20CVBM%20introduces%20an%20innovative%20bidirectional%20consistency%0Amechanism%2C%20which%20ensures%20mutual%20alignment%20between%20foreground%20predictions%20and%0Abackground-guided%20predictions.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20achieves%20SOTA%20performance%20on%20the%20LA%2C%20Pancreas%2C%20ACDC%2C%20and%20HRF%20datasets.%0ANotably%2C%20on%20the%20Pancreas%20dataset%2C%20CVBM%20outperforms%20fully%20supervised%20methods%0A%28i.e.%2C%20DSC%3A%2084.57%25%20vs.%2083.89%25%29%20while%20utilizing%20only%2020%25%20of%20the%20labeled%20data.%0AOur%20code%20is%20publicly%20available%20at%20https%3A//github.com/caoluyang0830/CVBM.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16625v1&entry.124074799=Read"},
{"title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation", "author": "Inbal Cohen and Boaz Meivar and Peihan Tu and Shai Avidan and Gal Oren", "abstract": "  Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.\n", "link": "http://arxiv.org/abs/2505.16540v1", "date": "2025-05-22", "relevancy": 2.7386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5874}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5307}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextureSAM%3A%20Towards%20a%20Texture%20Aware%20Foundation%20Model%20for%20Segmentation&body=Title%3A%20TextureSAM%3A%20Towards%20a%20Texture%20Aware%20Foundation%20Model%20for%20Segmentation%0AAuthor%3A%20Inbal%20Cohen%20and%20Boaz%20Meivar%20and%20Peihan%20Tu%20and%20Shai%20Avidan%20and%20Gal%20Oren%0AAbstract%3A%20%20%20Segment%20Anything%20Models%20%28SAM%29%20have%20achieved%20remarkable%20success%20in%20object%0Asegmentation%20tasks%20across%20diverse%20datasets.%20However%2C%20these%20models%20are%0Apredominantly%20trained%20on%20large-scale%20semantic%20segmentation%20datasets%2C%20which%0Aintroduce%20a%20bias%20toward%20object%20shape%20rather%20than%20texture%20cues%20in%20the%20image.%0AThis%20limitation%20is%20critical%20in%20domains%20such%20as%20medical%20imaging%2C%20material%0Aclassification%2C%20and%20remote%20sensing%2C%20where%20texture%20changes%20define%20object%0Aboundaries.%20In%20this%20study%2C%20we%20investigate%20SAM%27s%20bias%20toward%20semantics%20over%0Atextures%20and%20introduce%20a%20new%20texture-aware%20foundation%20model%2C%20TextureSAM%2C%20which%0Aperforms%20superior%20segmentation%20in%20texture-dominant%20scenarios.%20To%20achieve%20this%2C%0Awe%20employ%20a%20novel%20fine-tuning%20approach%20that%20incorporates%20texture%20augmentation%0Atechniques%2C%20incrementally%20modifying%20training%20images%20to%20emphasize%20texture%0Afeatures.%20By%20leveraging%20a%20novel%20texture-alternation%20of%20the%20ADE20K%20dataset%2C%20we%0Aguide%20TextureSAM%20to%20prioritize%20texture-defined%20regions%2C%20thereby%20mitigating%20the%0Ainherent%20shape%20bias%20present%20in%20the%20original%20SAM%20model.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20TextureSAM%20significantly%20outperforms%20SAM-2%20on%20both%0Anatural%20%28%2B0.2%20mIoU%29%20and%20synthetic%20%28%2B0.18%20mIoU%29%20texture-based%20segmentation%0Adatasets.%20The%20code%20and%20texture-augmented%20dataset%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextureSAM%253A%2520Towards%2520a%2520Texture%2520Aware%2520Foundation%2520Model%2520for%2520Segmentation%26entry.906535625%3DInbal%2520Cohen%2520and%2520Boaz%2520Meivar%2520and%2520Peihan%2520Tu%2520and%2520Shai%2520Avidan%2520and%2520Gal%2520Oren%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Models%2520%2528SAM%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520object%250Asegmentation%2520tasks%2520across%2520diverse%2520datasets.%2520However%252C%2520these%2520models%2520are%250Apredominantly%2520trained%2520on%2520large-scale%2520semantic%2520segmentation%2520datasets%252C%2520which%250Aintroduce%2520a%2520bias%2520toward%2520object%2520shape%2520rather%2520than%2520texture%2520cues%2520in%2520the%2520image.%250AThis%2520limitation%2520is%2520critical%2520in%2520domains%2520such%2520as%2520medical%2520imaging%252C%2520material%250Aclassification%252C%2520and%2520remote%2520sensing%252C%2520where%2520texture%2520changes%2520define%2520object%250Aboundaries.%2520In%2520this%2520study%252C%2520we%2520investigate%2520SAM%2527s%2520bias%2520toward%2520semantics%2520over%250Atextures%2520and%2520introduce%2520a%2520new%2520texture-aware%2520foundation%2520model%252C%2520TextureSAM%252C%2520which%250Aperforms%2520superior%2520segmentation%2520in%2520texture-dominant%2520scenarios.%2520To%2520achieve%2520this%252C%250Awe%2520employ%2520a%2520novel%2520fine-tuning%2520approach%2520that%2520incorporates%2520texture%2520augmentation%250Atechniques%252C%2520incrementally%2520modifying%2520training%2520images%2520to%2520emphasize%2520texture%250Afeatures.%2520By%2520leveraging%2520a%2520novel%2520texture-alternation%2520of%2520the%2520ADE20K%2520dataset%252C%2520we%250Aguide%2520TextureSAM%2520to%2520prioritize%2520texture-defined%2520regions%252C%2520thereby%2520mitigating%2520the%250Ainherent%2520shape%2520bias%2520present%2520in%2520the%2520original%2520SAM%2520model.%2520Our%2520extensive%250Aexperiments%2520demonstrate%2520that%2520TextureSAM%2520significantly%2520outperforms%2520SAM-2%2520on%2520both%250Anatural%2520%2528%252B0.2%2520mIoU%2529%2520and%2520synthetic%2520%2528%252B0.18%2520mIoU%2529%2520texture-based%2520segmentation%250Adatasets.%2520The%2520code%2520and%2520texture-augmented%2520dataset%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextureSAM%3A%20Towards%20a%20Texture%20Aware%20Foundation%20Model%20for%20Segmentation&entry.906535625=Inbal%20Cohen%20and%20Boaz%20Meivar%20and%20Peihan%20Tu%20and%20Shai%20Avidan%20and%20Gal%20Oren&entry.1292438233=%20%20Segment%20Anything%20Models%20%28SAM%29%20have%20achieved%20remarkable%20success%20in%20object%0Asegmentation%20tasks%20across%20diverse%20datasets.%20However%2C%20these%20models%20are%0Apredominantly%20trained%20on%20large-scale%20semantic%20segmentation%20datasets%2C%20which%0Aintroduce%20a%20bias%20toward%20object%20shape%20rather%20than%20texture%20cues%20in%20the%20image.%0AThis%20limitation%20is%20critical%20in%20domains%20such%20as%20medical%20imaging%2C%20material%0Aclassification%2C%20and%20remote%20sensing%2C%20where%20texture%20changes%20define%20object%0Aboundaries.%20In%20this%20study%2C%20we%20investigate%20SAM%27s%20bias%20toward%20semantics%20over%0Atextures%20and%20introduce%20a%20new%20texture-aware%20foundation%20model%2C%20TextureSAM%2C%20which%0Aperforms%20superior%20segmentation%20in%20texture-dominant%20scenarios.%20To%20achieve%20this%2C%0Awe%20employ%20a%20novel%20fine-tuning%20approach%20that%20incorporates%20texture%20augmentation%0Atechniques%2C%20incrementally%20modifying%20training%20images%20to%20emphasize%20texture%0Afeatures.%20By%20leveraging%20a%20novel%20texture-alternation%20of%20the%20ADE20K%20dataset%2C%20we%0Aguide%20TextureSAM%20to%20prioritize%20texture-defined%20regions%2C%20thereby%20mitigating%20the%0Ainherent%20shape%20bias%20present%20in%20the%20original%20SAM%20model.%20Our%20extensive%0Aexperiments%20demonstrate%20that%20TextureSAM%20significantly%20outperforms%20SAM-2%20on%20both%0Anatural%20%28%2B0.2%20mIoU%29%20and%20synthetic%20%28%2B0.18%20mIoU%29%20texture-based%20segmentation%0Adatasets.%20The%20code%20and%20texture-augmented%20dataset%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16540v1&entry.124074799=Read"},
{"title": "Transferring Textual Preferences to Vision-Language Understanding\n  through Model Merging", "author": "Chen-An Li and Tzu-Han Lin and Yun-Nung Chen and Hung-yi Lee", "abstract": "  Large vision-language models (LVLMs) perform outstandingly across various\nmultimodal tasks. However, their ability to evaluate generated content remains\nlimited, and training vision-language reward models (VLRMs) with preference\ndata is computationally expensive. This paper explores a training-free\nalternative by merging text-based reward models (RMs) with LVLMs to create\nVLRMs. Our approach shows that integrating these models leads to improved\nperformance over LVLMs' scoring and text-based RMs, offering an efficient\nmethod for incorporating textual preferences into LVLMs.\n", "link": "http://arxiv.org/abs/2502.13487v2", "date": "2025-05-22", "relevancy": 2.7358, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5541}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferring%20Textual%20Preferences%20to%20Vision-Language%20Understanding%0A%20%20through%20Model%20Merging&body=Title%3A%20Transferring%20Textual%20Preferences%20to%20Vision-Language%20Understanding%0A%20%20through%20Model%20Merging%0AAuthor%3A%20Chen-An%20Li%20and%20Tzu-Han%20Lin%20and%20Yun-Nung%20Chen%20and%20Hung-yi%20Lee%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20perform%20outstandingly%20across%20various%0Amultimodal%20tasks.%20However%2C%20their%20ability%20to%20evaluate%20generated%20content%20remains%0Alimited%2C%20and%20training%20vision-language%20reward%20models%20%28VLRMs%29%20with%20preference%0Adata%20is%20computationally%20expensive.%20This%20paper%20explores%20a%20training-free%0Aalternative%20by%20merging%20text-based%20reward%20models%20%28RMs%29%20with%20LVLMs%20to%20create%0AVLRMs.%20Our%20approach%20shows%20that%20integrating%20these%20models%20leads%20to%20improved%0Aperformance%20over%20LVLMs%27%20scoring%20and%20text-based%20RMs%2C%20offering%20an%20efficient%0Amethod%20for%20incorporating%20textual%20preferences%20into%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13487v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferring%2520Textual%2520Preferences%2520to%2520Vision-Language%2520Understanding%250A%2520%2520through%2520Model%2520Merging%26entry.906535625%3DChen-An%2520Li%2520and%2520Tzu-Han%2520Lin%2520and%2520Yun-Nung%2520Chen%2520and%2520Hung-yi%2520Lee%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520perform%2520outstandingly%2520across%2520various%250Amultimodal%2520tasks.%2520However%252C%2520their%2520ability%2520to%2520evaluate%2520generated%2520content%2520remains%250Alimited%252C%2520and%2520training%2520vision-language%2520reward%2520models%2520%2528VLRMs%2529%2520with%2520preference%250Adata%2520is%2520computationally%2520expensive.%2520This%2520paper%2520explores%2520a%2520training-free%250Aalternative%2520by%2520merging%2520text-based%2520reward%2520models%2520%2528RMs%2529%2520with%2520LVLMs%2520to%2520create%250AVLRMs.%2520Our%2520approach%2520shows%2520that%2520integrating%2520these%2520models%2520leads%2520to%2520improved%250Aperformance%2520over%2520LVLMs%2527%2520scoring%2520and%2520text-based%2520RMs%252C%2520offering%2520an%2520efficient%250Amethod%2520for%2520incorporating%2520textual%2520preferences%2520into%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13487v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferring%20Textual%20Preferences%20to%20Vision-Language%20Understanding%0A%20%20through%20Model%20Merging&entry.906535625=Chen-An%20Li%20and%20Tzu-Han%20Lin%20and%20Yun-Nung%20Chen%20and%20Hung-yi%20Lee&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20perform%20outstandingly%20across%20various%0Amultimodal%20tasks.%20However%2C%20their%20ability%20to%20evaluate%20generated%20content%20remains%0Alimited%2C%20and%20training%20vision-language%20reward%20models%20%28VLRMs%29%20with%20preference%0Adata%20is%20computationally%20expensive.%20This%20paper%20explores%20a%20training-free%0Aalternative%20by%20merging%20text-based%20reward%20models%20%28RMs%29%20with%20LVLMs%20to%20create%0AVLRMs.%20Our%20approach%20shows%20that%20integrating%20these%20models%20leads%20to%20improved%0Aperformance%20over%20LVLMs%27%20scoring%20and%20text-based%20RMs%2C%20offering%20an%20efficient%0Amethod%20for%20incorporating%20textual%20preferences%20into%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13487v2&entry.124074799=Read"},
{"title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor\n  Segmentation with Missing Modalities", "author": "Junze Wang and Lei Fan and Weipeng Jing and Donglin Di and Yang Song and Sidong Liu and Cong Cong", "abstract": "  Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2\\% in the Dice Similarity\nCoefficient across various tumor regions. Our code is available at ReHyDIL.\n", "link": "http://arxiv.org/abs/2505.16809v1", "date": "2025-05-22", "relevancy": 2.6933, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5916}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypergraph%20Tversky-Aware%20Domain%20Incremental%20Learning%20for%20Brain%20Tumor%0A%20%20Segmentation%20with%20Missing%20Modalities&body=Title%3A%20Hypergraph%20Tversky-Aware%20Domain%20Incremental%20Learning%20for%20Brain%20Tumor%0A%20%20Segmentation%20with%20Missing%20Modalities%0AAuthor%3A%20Junze%20Wang%20and%20Lei%20Fan%20and%20Weipeng%20Jing%20and%20Donglin%20Di%20and%20Yang%20Song%20and%20Sidong%20Liu%20and%20Cong%20Cong%0AAbstract%3A%20%20%20Existing%20methods%20for%20multimodal%20MRI%20segmentation%20with%20missing%20modalities%0Atypically%20assume%20that%20all%20MRI%20modalities%20are%20available%20during%20training.%0AHowever%2C%20in%20clinical%20practice%2C%20some%20modalities%20may%20be%20missing%20due%20to%20the%0Asequential%20nature%20of%20MRI%20acquisition%2C%20leading%20to%20performance%20degradation.%0AFurthermore%2C%20retraining%20models%20to%20accommodate%20newly%20available%20modalities%20can%20be%0Ainefficient%20and%20may%20cause%20overfitting%2C%20potentially%20compromising%20previously%0Alearned%20knowledge.%20To%20address%20these%20challenges%2C%20we%20propose%20Replay-based%0AHypergraph%20Domain%20Incremental%20Learning%20%28ReHyDIL%29%20for%20brain%20tumor%20segmentation%0Awith%20missing%20modalities.%20ReHyDIL%20leverages%20Domain%20Incremental%20Learning%20%28DIL%29%20to%0Aenable%20the%20segmentation%20model%20to%20learn%20from%20newly%20acquired%20MRI%20modalities%0Awithout%20forgetting%20previously%20learned%20information.%20To%20enhance%20segmentation%0Aperformance%20across%20diverse%20patient%20scenarios%2C%20we%20introduce%20the%20Cross-Patient%0AHypergraph%20Segmentation%20Network%20%28CHSNet%29%2C%20which%20utilizes%20hypergraphs%20to%20capture%0Ahigh-order%20associations%20between%20patients.%20Additionally%2C%20we%20incorporate%0ATversky-Aware%20Contrastive%20%28TAC%29%20loss%20to%20effectively%20mitigate%20information%0Aimbalance%20both%20across%20and%20within%20different%20modalities.%20Extensive%20experiments%20on%0Athe%20BraTS2019%20dataset%20demonstrate%20that%20ReHyDIL%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20an%20improvement%20of%20over%202%5C%25%20in%20the%20Dice%20Similarity%0ACoefficient%20across%20various%20tumor%20regions.%20Our%20code%20is%20available%20at%20ReHyDIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypergraph%2520Tversky-Aware%2520Domain%2520Incremental%2520Learning%2520for%2520Brain%2520Tumor%250A%2520%2520Segmentation%2520with%2520Missing%2520Modalities%26entry.906535625%3DJunze%2520Wang%2520and%2520Lei%2520Fan%2520and%2520Weipeng%2520Jing%2520and%2520Donglin%2520Di%2520and%2520Yang%2520Song%2520and%2520Sidong%2520Liu%2520and%2520Cong%2520Cong%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520multimodal%2520MRI%2520segmentation%2520with%2520missing%2520modalities%250Atypically%2520assume%2520that%2520all%2520MRI%2520modalities%2520are%2520available%2520during%2520training.%250AHowever%252C%2520in%2520clinical%2520practice%252C%2520some%2520modalities%2520may%2520be%2520missing%2520due%2520to%2520the%250Asequential%2520nature%2520of%2520MRI%2520acquisition%252C%2520leading%2520to%2520performance%2520degradation.%250AFurthermore%252C%2520retraining%2520models%2520to%2520accommodate%2520newly%2520available%2520modalities%2520can%2520be%250Ainefficient%2520and%2520may%2520cause%2520overfitting%252C%2520potentially%2520compromising%2520previously%250Alearned%2520knowledge.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Replay-based%250AHypergraph%2520Domain%2520Incremental%2520Learning%2520%2528ReHyDIL%2529%2520for%2520brain%2520tumor%2520segmentation%250Awith%2520missing%2520modalities.%2520ReHyDIL%2520leverages%2520Domain%2520Incremental%2520Learning%2520%2528DIL%2529%2520to%250Aenable%2520the%2520segmentation%2520model%2520to%2520learn%2520from%2520newly%2520acquired%2520MRI%2520modalities%250Awithout%2520forgetting%2520previously%2520learned%2520information.%2520To%2520enhance%2520segmentation%250Aperformance%2520across%2520diverse%2520patient%2520scenarios%252C%2520we%2520introduce%2520the%2520Cross-Patient%250AHypergraph%2520Segmentation%2520Network%2520%2528CHSNet%2529%252C%2520which%2520utilizes%2520hypergraphs%2520to%2520capture%250Ahigh-order%2520associations%2520between%2520patients.%2520Additionally%252C%2520we%2520incorporate%250ATversky-Aware%2520Contrastive%2520%2528TAC%2529%2520loss%2520to%2520effectively%2520mitigate%2520information%250Aimbalance%2520both%2520across%2520and%2520within%2520different%2520modalities.%2520Extensive%2520experiments%2520on%250Athe%2520BraTS2019%2520dataset%2520demonstrate%2520that%2520ReHyDIL%2520outperforms%2520state-of-the-art%250Amethods%252C%2520achieving%2520an%2520improvement%2520of%2520over%25202%255C%2525%2520in%2520the%2520Dice%2520Similarity%250ACoefficient%2520across%2520various%2520tumor%2520regions.%2520Our%2520code%2520is%2520available%2520at%2520ReHyDIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraph%20Tversky-Aware%20Domain%20Incremental%20Learning%20for%20Brain%20Tumor%0A%20%20Segmentation%20with%20Missing%20Modalities&entry.906535625=Junze%20Wang%20and%20Lei%20Fan%20and%20Weipeng%20Jing%20and%20Donglin%20Di%20and%20Yang%20Song%20and%20Sidong%20Liu%20and%20Cong%20Cong&entry.1292438233=%20%20Existing%20methods%20for%20multimodal%20MRI%20segmentation%20with%20missing%20modalities%0Atypically%20assume%20that%20all%20MRI%20modalities%20are%20available%20during%20training.%0AHowever%2C%20in%20clinical%20practice%2C%20some%20modalities%20may%20be%20missing%20due%20to%20the%0Asequential%20nature%20of%20MRI%20acquisition%2C%20leading%20to%20performance%20degradation.%0AFurthermore%2C%20retraining%20models%20to%20accommodate%20newly%20available%20modalities%20can%20be%0Ainefficient%20and%20may%20cause%20overfitting%2C%20potentially%20compromising%20previously%0Alearned%20knowledge.%20To%20address%20these%20challenges%2C%20we%20propose%20Replay-based%0AHypergraph%20Domain%20Incremental%20Learning%20%28ReHyDIL%29%20for%20brain%20tumor%20segmentation%0Awith%20missing%20modalities.%20ReHyDIL%20leverages%20Domain%20Incremental%20Learning%20%28DIL%29%20to%0Aenable%20the%20segmentation%20model%20to%20learn%20from%20newly%20acquired%20MRI%20modalities%0Awithout%20forgetting%20previously%20learned%20information.%20To%20enhance%20segmentation%0Aperformance%20across%20diverse%20patient%20scenarios%2C%20we%20introduce%20the%20Cross-Patient%0AHypergraph%20Segmentation%20Network%20%28CHSNet%29%2C%20which%20utilizes%20hypergraphs%20to%20capture%0Ahigh-order%20associations%20between%20patients.%20Additionally%2C%20we%20incorporate%0ATversky-Aware%20Contrastive%20%28TAC%29%20loss%20to%20effectively%20mitigate%20information%0Aimbalance%20both%20across%20and%20within%20different%20modalities.%20Extensive%20experiments%20on%0Athe%20BraTS2019%20dataset%20demonstrate%20that%20ReHyDIL%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20an%20improvement%20of%20over%202%5C%25%20in%20the%20Dice%20Similarity%0ACoefficient%20across%20various%20tumor%20regions.%20Our%20code%20is%20available%20at%20ReHyDIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16809v1&entry.124074799=Read"},
{"title": "Native Segmentation Vision Transformers", "author": "Guillem Bras\u00f3 and Aljo\u0161a O\u0161ep and Laura Leal-Taix\u00e9", "abstract": "  Uniform downsampling remains the de facto standard for reducing spatial\nresolution in vision backbones. In this work, we propose an alternative design\nbuilt around a content-aware spatial grouping layer, that dynamically assigns\ntokens to a reduced set based on image boundaries and their semantic content.\nStacking our grouping layer across consecutive backbone stages results in\nhierarchical segmentation that arises natively in the feature extraction\nprocess, resulting in our coined Native Segmentation Vision Transformer. We\nshow that a careful design of our architecture enables the emergence of strong\nsegmentation masks solely from grouping layers, that is, without additional\nsegmentation-specific heads. This sets the foundation for a new paradigm of\nnative, backbone-level segmentation, which enables strong zero-shot results\nwithout mask supervision, as well as a minimal and efficient standalone model\ndesign for downstream segmentation tasks. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/native-segmentation.\n", "link": "http://arxiv.org/abs/2505.16993v1", "date": "2025-05-22", "relevancy": 2.6905, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Native%20Segmentation%20Vision%20Transformers&body=Title%3A%20Native%20Segmentation%20Vision%20Transformers%0AAuthor%3A%20Guillem%20Bras%C3%B3%20and%20Aljo%C5%A1a%20O%C5%A1ep%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20Uniform%20downsampling%20remains%20the%20de%20facto%20standard%20for%20reducing%20spatial%0Aresolution%20in%20vision%20backbones.%20In%20this%20work%2C%20we%20propose%20an%20alternative%20design%0Abuilt%20around%20a%20content-aware%20spatial%20grouping%20layer%2C%20that%20dynamically%20assigns%0Atokens%20to%20a%20reduced%20set%20based%20on%20image%20boundaries%20and%20their%20semantic%20content.%0AStacking%20our%20grouping%20layer%20across%20consecutive%20backbone%20stages%20results%20in%0Ahierarchical%20segmentation%20that%20arises%20natively%20in%20the%20feature%20extraction%0Aprocess%2C%20resulting%20in%20our%20coined%20Native%20Segmentation%20Vision%20Transformer.%20We%0Ashow%20that%20a%20careful%20design%20of%20our%20architecture%20enables%20the%20emergence%20of%20strong%0Asegmentation%20masks%20solely%20from%20grouping%20layers%2C%20that%20is%2C%20without%20additional%0Asegmentation-specific%20heads.%20This%20sets%20the%20foundation%20for%20a%20new%20paradigm%20of%0Anative%2C%20backbone-level%20segmentation%2C%20which%20enables%20strong%20zero-shot%20results%0Awithout%20mask%20supervision%2C%20as%20well%20as%20a%20minimal%20and%20efficient%20standalone%20model%0Adesign%20for%20downstream%20segmentation%20tasks.%20Our%20project%20page%20is%0Ahttps%3A//research.nvidia.com/labs/dvl/projects/native-segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNative%2520Segmentation%2520Vision%2520Transformers%26entry.906535625%3DGuillem%2520Bras%25C3%25B3%2520and%2520Aljo%25C5%25A1a%2520O%25C5%25A1ep%2520and%2520Laura%2520Leal-Taix%25C3%25A9%26entry.1292438233%3D%2520%2520Uniform%2520downsampling%2520remains%2520the%2520de%2520facto%2520standard%2520for%2520reducing%2520spatial%250Aresolution%2520in%2520vision%2520backbones.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520alternative%2520design%250Abuilt%2520around%2520a%2520content-aware%2520spatial%2520grouping%2520layer%252C%2520that%2520dynamically%2520assigns%250Atokens%2520to%2520a%2520reduced%2520set%2520based%2520on%2520image%2520boundaries%2520and%2520their%2520semantic%2520content.%250AStacking%2520our%2520grouping%2520layer%2520across%2520consecutive%2520backbone%2520stages%2520results%2520in%250Ahierarchical%2520segmentation%2520that%2520arises%2520natively%2520in%2520the%2520feature%2520extraction%250Aprocess%252C%2520resulting%2520in%2520our%2520coined%2520Native%2520Segmentation%2520Vision%2520Transformer.%2520We%250Ashow%2520that%2520a%2520careful%2520design%2520of%2520our%2520architecture%2520enables%2520the%2520emergence%2520of%2520strong%250Asegmentation%2520masks%2520solely%2520from%2520grouping%2520layers%252C%2520that%2520is%252C%2520without%2520additional%250Asegmentation-specific%2520heads.%2520This%2520sets%2520the%2520foundation%2520for%2520a%2520new%2520paradigm%2520of%250Anative%252C%2520backbone-level%2520segmentation%252C%2520which%2520enables%2520strong%2520zero-shot%2520results%250Awithout%2520mask%2520supervision%252C%2520as%2520well%2520as%2520a%2520minimal%2520and%2520efficient%2520standalone%2520model%250Adesign%2520for%2520downstream%2520segmentation%2520tasks.%2520Our%2520project%2520page%2520is%250Ahttps%253A//research.nvidia.com/labs/dvl/projects/native-segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Native%20Segmentation%20Vision%20Transformers&entry.906535625=Guillem%20Bras%C3%B3%20and%20Aljo%C5%A1a%20O%C5%A1ep%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20Uniform%20downsampling%20remains%20the%20de%20facto%20standard%20for%20reducing%20spatial%0Aresolution%20in%20vision%20backbones.%20In%20this%20work%2C%20we%20propose%20an%20alternative%20design%0Abuilt%20around%20a%20content-aware%20spatial%20grouping%20layer%2C%20that%20dynamically%20assigns%0Atokens%20to%20a%20reduced%20set%20based%20on%20image%20boundaries%20and%20their%20semantic%20content.%0AStacking%20our%20grouping%20layer%20across%20consecutive%20backbone%20stages%20results%20in%0Ahierarchical%20segmentation%20that%20arises%20natively%20in%20the%20feature%20extraction%0Aprocess%2C%20resulting%20in%20our%20coined%20Native%20Segmentation%20Vision%20Transformer.%20We%0Ashow%20that%20a%20careful%20design%20of%20our%20architecture%20enables%20the%20emergence%20of%20strong%0Asegmentation%20masks%20solely%20from%20grouping%20layers%2C%20that%20is%2C%20without%20additional%0Asegmentation-specific%20heads.%20This%20sets%20the%20foundation%20for%20a%20new%20paradigm%20of%0Anative%2C%20backbone-level%20segmentation%2C%20which%20enables%20strong%20zero-shot%20results%0Awithout%20mask%20supervision%2C%20as%20well%20as%20a%20minimal%20and%20efficient%20standalone%20model%0Adesign%20for%20downstream%20segmentation%20tasks.%20Our%20project%20page%20is%0Ahttps%3A//research.nvidia.com/labs/dvl/projects/native-segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16993v1&entry.124074799=Read"},
{"title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal\n  Language?", "author": "Jin Jiang and Jianing Wang and Yuchen Yan and Yang Liu and Jianhua Zhu and Mengdi Zhang and Xunliang Cai and Liangcai Gao", "abstract": "  Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.\n", "link": "http://arxiv.org/abs/2505.16998v1", "date": "2025-05-22", "relevancy": 2.6644, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Language%20Models%20Excel%20in%20Complex%20Logical%20Reasoning%20with%20Formal%0A%20%20Language%3F&body=Title%3A%20Do%20Large%20Language%20Models%20Excel%20in%20Complex%20Logical%20Reasoning%20with%20Formal%0A%20%20Language%3F%0AAuthor%3A%20Jin%20Jiang%20and%20Jianing%20Wang%20and%20Yuchen%20Yan%20and%20Yang%20Liu%20and%20Jianhua%20Zhu%20and%20Mengdi%20Zhang%20and%20Xunliang%20Cai%20and%20Liangcai%20Gao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20shown%20to%20achieve%20breakthrough%0Aperformance%20on%20complex%20logical%20reasoning%20tasks.%20Nevertheless%2C%20most%20existing%0Aresearch%20focuses%20on%20employing%20formal%20language%20to%20guide%20LLMs%20to%20derive%20reliable%0Areasoning%20paths%2C%20while%20systematic%20evaluations%20of%20these%20capabilities%20are%20still%0Alimited.%20In%20this%20paper%2C%20we%20aim%20to%20conduct%20a%20comprehensive%20evaluation%20of%20LLMs%0Aacross%20various%20logical%20reasoning%20problems%20utilizing%20formal%20languages.%20From%20the%0Aperspective%20of%20three%20dimensions%2C%20i.e.%2C%20spectrum%20of%20LLMs%2C%20taxonomy%20of%20tasks%2C%20and%0Aformat%20of%20trajectories%2C%20our%20key%20findings%20are%3A%201%29%20Thinking%20models%20significantly%0Aoutperform%20Instruct%20models%2C%20especially%20when%20formal%20language%20is%20employed%3B%202%29%20All%0ALLMs%20exhibit%20limitations%20in%20inductive%20reasoning%20capability%2C%20irrespective%20of%0Awhether%20they%20use%20a%20formal%20language%3B%203%29%20Data%20with%20PoT%20format%20achieves%20the%20best%0Ageneralization%20performance%20across%20other%20languages.%20Additionally%2C%20we%20also%20curate%0Athe%20formal-relative%20training%20data%20to%20further%20enhance%20the%20small%20language%20models%2C%0Aand%20the%20experimental%20results%20indicate%20that%20a%20simple%20rejected%20fine-tuning%20method%0Acan%20better%20enable%20LLMs%20to%20generalize%20across%20formal%20languages%20and%20achieve%20the%0Abest%20overall%20performance.%20Our%20codes%20and%20reports%20are%20available%20at%0Ahttps%3A//github.com/jiangjin1999/FormalEval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Language%2520Models%2520Excel%2520in%2520Complex%2520Logical%2520Reasoning%2520with%2520Formal%250A%2520%2520Language%253F%26entry.906535625%3DJin%2520Jiang%2520and%2520Jianing%2520Wang%2520and%2520Yuchen%2520Yan%2520and%2520Yang%2520Liu%2520and%2520Jianhua%2520Zhu%2520and%2520Mengdi%2520Zhang%2520and%2520Xunliang%2520Cai%2520and%2520Liangcai%2520Gao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520shown%2520to%2520achieve%2520breakthrough%250Aperformance%2520on%2520complex%2520logical%2520reasoning%2520tasks.%2520Nevertheless%252C%2520most%2520existing%250Aresearch%2520focuses%2520on%2520employing%2520formal%2520language%2520to%2520guide%2520LLMs%2520to%2520derive%2520reliable%250Areasoning%2520paths%252C%2520while%2520systematic%2520evaluations%2520of%2520these%2520capabilities%2520are%2520still%250Alimited.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520LLMs%250Aacross%2520various%2520logical%2520reasoning%2520problems%2520utilizing%2520formal%2520languages.%2520From%2520the%250Aperspective%2520of%2520three%2520dimensions%252C%2520i.e.%252C%2520spectrum%2520of%2520LLMs%252C%2520taxonomy%2520of%2520tasks%252C%2520and%250Aformat%2520of%2520trajectories%252C%2520our%2520key%2520findings%2520are%253A%25201%2529%2520Thinking%2520models%2520significantly%250Aoutperform%2520Instruct%2520models%252C%2520especially%2520when%2520formal%2520language%2520is%2520employed%253B%25202%2529%2520All%250ALLMs%2520exhibit%2520limitations%2520in%2520inductive%2520reasoning%2520capability%252C%2520irrespective%2520of%250Awhether%2520they%2520use%2520a%2520formal%2520language%253B%25203%2529%2520Data%2520with%2520PoT%2520format%2520achieves%2520the%2520best%250Ageneralization%2520performance%2520across%2520other%2520languages.%2520Additionally%252C%2520we%2520also%2520curate%250Athe%2520formal-relative%2520training%2520data%2520to%2520further%2520enhance%2520the%2520small%2520language%2520models%252C%250Aand%2520the%2520experimental%2520results%2520indicate%2520that%2520a%2520simple%2520rejected%2520fine-tuning%2520method%250Acan%2520better%2520enable%2520LLMs%2520to%2520generalize%2520across%2520formal%2520languages%2520and%2520achieve%2520the%250Abest%2520overall%2520performance.%2520Our%2520codes%2520and%2520reports%2520are%2520available%2520at%250Ahttps%253A//github.com/jiangjin1999/FormalEval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Language%20Models%20Excel%20in%20Complex%20Logical%20Reasoning%20with%20Formal%0A%20%20Language%3F&entry.906535625=Jin%20Jiang%20and%20Jianing%20Wang%20and%20Yuchen%20Yan%20and%20Yang%20Liu%20and%20Jianhua%20Zhu%20and%20Mengdi%20Zhang%20and%20Xunliang%20Cai%20and%20Liangcai%20Gao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20shown%20to%20achieve%20breakthrough%0Aperformance%20on%20complex%20logical%20reasoning%20tasks.%20Nevertheless%2C%20most%20existing%0Aresearch%20focuses%20on%20employing%20formal%20language%20to%20guide%20LLMs%20to%20derive%20reliable%0Areasoning%20paths%2C%20while%20systematic%20evaluations%20of%20these%20capabilities%20are%20still%0Alimited.%20In%20this%20paper%2C%20we%20aim%20to%20conduct%20a%20comprehensive%20evaluation%20of%20LLMs%0Aacross%20various%20logical%20reasoning%20problems%20utilizing%20formal%20languages.%20From%20the%0Aperspective%20of%20three%20dimensions%2C%20i.e.%2C%20spectrum%20of%20LLMs%2C%20taxonomy%20of%20tasks%2C%20and%0Aformat%20of%20trajectories%2C%20our%20key%20findings%20are%3A%201%29%20Thinking%20models%20significantly%0Aoutperform%20Instruct%20models%2C%20especially%20when%20formal%20language%20is%20employed%3B%202%29%20All%0ALLMs%20exhibit%20limitations%20in%20inductive%20reasoning%20capability%2C%20irrespective%20of%0Awhether%20they%20use%20a%20formal%20language%3B%203%29%20Data%20with%20PoT%20format%20achieves%20the%20best%0Ageneralization%20performance%20across%20other%20languages.%20Additionally%2C%20we%20also%20curate%0Athe%20formal-relative%20training%20data%20to%20further%20enhance%20the%20small%20language%20models%2C%0Aand%20the%20experimental%20results%20indicate%20that%20a%20simple%20rejected%20fine-tuning%20method%0Acan%20better%20enable%20LLMs%20to%20generalize%20across%20formal%20languages%20and%20achieve%20the%0Abest%20overall%20performance.%20Our%20codes%20and%20reports%20are%20available%20at%0Ahttps%3A//github.com/jiangjin1999/FormalEval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16998v1&entry.124074799=Read"},
{"title": "Benchmarking Ophthalmology Foundation Models for Clinically Significant\n  Age Macular Degeneration Detection", "author": "Benjamin A. Cohen and Jonathan Fhima and Meishar Meisel and Baskin Meital and Luis Filipe Nakayama and Eran Berkowitz and Joachim A. Behar", "abstract": "  Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.\n", "link": "http://arxiv.org/abs/2505.05291v2", "date": "2025-05-22", "relevancy": 2.6627, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5434}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Ophthalmology%20Foundation%20Models%20for%20Clinically%20Significant%0A%20%20Age%20Macular%20Degeneration%20Detection&body=Title%3A%20Benchmarking%20Ophthalmology%20Foundation%20Models%20for%20Clinically%20Significant%0A%20%20Age%20Macular%20Degeneration%20Detection%0AAuthor%3A%20Benjamin%20A.%20Cohen%20and%20Jonathan%20Fhima%20and%20Meishar%20Meisel%20and%20Baskin%20Meital%20and%20Luis%20Filipe%20Nakayama%20and%20Eran%20Berkowitz%20and%20Joachim%20A.%20Behar%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20enabled%20Vision%20Transformers%20%28ViTs%29%20to%0Alearn%20robust%20representations%20from%20large-scale%20natural%20image%20datasets%2C%20enhancing%0Atheir%20generalization%20across%20domains.%20In%20retinal%20imaging%2C%20foundation%20models%0Apretrained%20on%20either%20natural%20or%20ophthalmic%20data%20have%20shown%20promise%2C%20but%20the%0Abenefits%20of%20in-domain%20pretraining%20remain%20uncertain.%20To%20investigate%20this%2C%20we%0Abenchmark%20six%20SSL-pretrained%20ViTs%20on%20seven%20digital%20fundus%20image%20%28DFI%29%20datasets%0Atotaling%2070%2C000%20expert-annotated%20images%20for%20the%20task%20of%20moderate-to-late%0Aage-related%20macular%20degeneration%20%28AMD%29%20identification.%20Our%20results%20show%20that%0AiBOT%20pretrained%20on%20natural%20images%20achieves%20the%20highest%20out-of-distribution%0Ageneralization%2C%20with%20AUROCs%20of%200.80-0.97%2C%20outperforming%20domain-specific%20models%2C%0Awhich%20achieved%20AUROCs%20of%200.78-0.96%20and%20a%20baseline%20ViT-L%20with%20no%20pretraining%2C%0Awhich%20achieved%20AUROCs%20of%200.68-0.91.%20These%20findings%20highlight%20the%20value%20of%0Afoundation%20models%20in%20improving%20AMD%20identification%20and%20challenge%20the%20assumption%0Athat%20in-domain%20pretraining%20is%20necessary.%20Furthermore%2C%20we%20release%20BRAMD%2C%20an%0Aopen-access%20dataset%20%28n%3D587%29%20of%20DFIs%20with%20AMD%20labels%20from%20Brazil.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Ophthalmology%2520Foundation%2520Models%2520for%2520Clinically%2520Significant%250A%2520%2520Age%2520Macular%2520Degeneration%2520Detection%26entry.906535625%3DBenjamin%2520A.%2520Cohen%2520and%2520Jonathan%2520Fhima%2520and%2520Meishar%2520Meisel%2520and%2520Baskin%2520Meital%2520and%2520Luis%2520Filipe%2520Nakayama%2520and%2520Eran%2520Berkowitz%2520and%2520Joachim%2520A.%2520Behar%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520enabled%2520Vision%2520Transformers%2520%2528ViTs%2529%2520to%250Alearn%2520robust%2520representations%2520from%2520large-scale%2520natural%2520image%2520datasets%252C%2520enhancing%250Atheir%2520generalization%2520across%2520domains.%2520In%2520retinal%2520imaging%252C%2520foundation%2520models%250Apretrained%2520on%2520either%2520natural%2520or%2520ophthalmic%2520data%2520have%2520shown%2520promise%252C%2520but%2520the%250Abenefits%2520of%2520in-domain%2520pretraining%2520remain%2520uncertain.%2520To%2520investigate%2520this%252C%2520we%250Abenchmark%2520six%2520SSL-pretrained%2520ViTs%2520on%2520seven%2520digital%2520fundus%2520image%2520%2528DFI%2529%2520datasets%250Atotaling%252070%252C000%2520expert-annotated%2520images%2520for%2520the%2520task%2520of%2520moderate-to-late%250Aage-related%2520macular%2520degeneration%2520%2528AMD%2529%2520identification.%2520Our%2520results%2520show%2520that%250AiBOT%2520pretrained%2520on%2520natural%2520images%2520achieves%2520the%2520highest%2520out-of-distribution%250Ageneralization%252C%2520with%2520AUROCs%2520of%25200.80-0.97%252C%2520outperforming%2520domain-specific%2520models%252C%250Awhich%2520achieved%2520AUROCs%2520of%25200.78-0.96%2520and%2520a%2520baseline%2520ViT-L%2520with%2520no%2520pretraining%252C%250Awhich%2520achieved%2520AUROCs%2520of%25200.68-0.91.%2520These%2520findings%2520highlight%2520the%2520value%2520of%250Afoundation%2520models%2520in%2520improving%2520AMD%2520identification%2520and%2520challenge%2520the%2520assumption%250Athat%2520in-domain%2520pretraining%2520is%2520necessary.%2520Furthermore%252C%2520we%2520release%2520BRAMD%252C%2520an%250Aopen-access%2520dataset%2520%2528n%253D587%2529%2520of%2520DFIs%2520with%2520AMD%2520labels%2520from%2520Brazil.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Ophthalmology%20Foundation%20Models%20for%20Clinically%20Significant%0A%20%20Age%20Macular%20Degeneration%20Detection&entry.906535625=Benjamin%20A.%20Cohen%20and%20Jonathan%20Fhima%20and%20Meishar%20Meisel%20and%20Baskin%20Meital%20and%20Luis%20Filipe%20Nakayama%20and%20Eran%20Berkowitz%20and%20Joachim%20A.%20Behar&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20enabled%20Vision%20Transformers%20%28ViTs%29%20to%0Alearn%20robust%20representations%20from%20large-scale%20natural%20image%20datasets%2C%20enhancing%0Atheir%20generalization%20across%20domains.%20In%20retinal%20imaging%2C%20foundation%20models%0Apretrained%20on%20either%20natural%20or%20ophthalmic%20data%20have%20shown%20promise%2C%20but%20the%0Abenefits%20of%20in-domain%20pretraining%20remain%20uncertain.%20To%20investigate%20this%2C%20we%0Abenchmark%20six%20SSL-pretrained%20ViTs%20on%20seven%20digital%20fundus%20image%20%28DFI%29%20datasets%0Atotaling%2070%2C000%20expert-annotated%20images%20for%20the%20task%20of%20moderate-to-late%0Aage-related%20macular%20degeneration%20%28AMD%29%20identification.%20Our%20results%20show%20that%0AiBOT%20pretrained%20on%20natural%20images%20achieves%20the%20highest%20out-of-distribution%0Ageneralization%2C%20with%20AUROCs%20of%200.80-0.97%2C%20outperforming%20domain-specific%20models%2C%0Awhich%20achieved%20AUROCs%20of%200.78-0.96%20and%20a%20baseline%20ViT-L%20with%20no%20pretraining%2C%0Awhich%20achieved%20AUROCs%20of%200.68-0.91.%20These%20findings%20highlight%20the%20value%20of%0Afoundation%20models%20in%20improving%20AMD%20identification%20and%20challenge%20the%20assumption%0Athat%20in-domain%20pretraining%20is%20necessary.%20Furthermore%2C%20we%20release%20BRAMD%2C%20an%0Aopen-access%20dataset%20%28n%3D587%29%20of%20DFIs%20with%20AMD%20labels%20from%20Brazil.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05291v2&entry.124074799=Read"},
{"title": "Stochastic Forward-Forward Learning through Representational\n  Dimensionality Compression", "author": "Zhichao Zhu and Yang Qi and Hengyuan Ma and Wenlian Lu and Jianfeng Feng", "abstract": "  The Forward-Forward (FF) algorithm provides a bottom-up alternative to\nbackpropagation (BP) for training neural networks, relying on a layer-wise\n\"goodness\" function to guide learning. Existing goodness functions, inspired by\nenergy-based learning (EBL), are typically defined as the sum of squared\npost-synaptic activations, neglecting the correlations between neurons. In this\nwork, we propose a novel goodness function termed dimensionality compression\nthat uses the effective dimensionality (ED) of fluctuating neural responses to\nincorporate second-order statistical structure. Our objective minimizes ED for\nclamped inputs when noise is considered while maximizing it across the sample\ndistribution, promoting structured representations without the need to prepare\nnegative samples. We demonstrate that this formulation achieves competitive\nperformance compared to other non-BP methods. Moreover, we show that noise\nplays a constructive role that can enhance generalization and improve inference\nwhen predictions are derived from the mean of squared outputs, which is\nequivalent to making predictions based on the energy term. Our findings\ncontribute to the development of more biologically plausible learning\nalgorithms and suggest a natural fit for neuromorphic computing, where\nstochasticity is a computational resource rather than a nuisance. The code is\navailable at https://github.com/ZhichaoZhu/StochasticForwardForward\n", "link": "http://arxiv.org/abs/2505.16649v1", "date": "2025-05-22", "relevancy": 2.6362, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5591}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5139}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Forward-Forward%20Learning%20through%20Representational%0A%20%20Dimensionality%20Compression&body=Title%3A%20Stochastic%20Forward-Forward%20Learning%20through%20Representational%0A%20%20Dimensionality%20Compression%0AAuthor%3A%20Zhichao%20Zhu%20and%20Yang%20Qi%20and%20Hengyuan%20Ma%20and%20Wenlian%20Lu%20and%20Jianfeng%20Feng%0AAbstract%3A%20%20%20The%20Forward-Forward%20%28FF%29%20algorithm%20provides%20a%20bottom-up%20alternative%20to%0Abackpropagation%20%28BP%29%20for%20training%20neural%20networks%2C%20relying%20on%20a%20layer-wise%0A%22goodness%22%20function%20to%20guide%20learning.%20Existing%20goodness%20functions%2C%20inspired%20by%0Aenergy-based%20learning%20%28EBL%29%2C%20are%20typically%20defined%20as%20the%20sum%20of%20squared%0Apost-synaptic%20activations%2C%20neglecting%20the%20correlations%20between%20neurons.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20goodness%20function%20termed%20dimensionality%20compression%0Athat%20uses%20the%20effective%20dimensionality%20%28ED%29%20of%20fluctuating%20neural%20responses%20to%0Aincorporate%20second-order%20statistical%20structure.%20Our%20objective%20minimizes%20ED%20for%0Aclamped%20inputs%20when%20noise%20is%20considered%20while%20maximizing%20it%20across%20the%20sample%0Adistribution%2C%20promoting%20structured%20representations%20without%20the%20need%20to%20prepare%0Anegative%20samples.%20We%20demonstrate%20that%20this%20formulation%20achieves%20competitive%0Aperformance%20compared%20to%20other%20non-BP%20methods.%20Moreover%2C%20we%20show%20that%20noise%0Aplays%20a%20constructive%20role%20that%20can%20enhance%20generalization%20and%20improve%20inference%0Awhen%20predictions%20are%20derived%20from%20the%20mean%20of%20squared%20outputs%2C%20which%20is%0Aequivalent%20to%20making%20predictions%20based%20on%20the%20energy%20term.%20Our%20findings%0Acontribute%20to%20the%20development%20of%20more%20biologically%20plausible%20learning%0Aalgorithms%20and%20suggest%20a%20natural%20fit%20for%20neuromorphic%20computing%2C%20where%0Astochasticity%20is%20a%20computational%20resource%20rather%20than%20a%20nuisance.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/ZhichaoZhu/StochasticForwardForward%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Forward-Forward%2520Learning%2520through%2520Representational%250A%2520%2520Dimensionality%2520Compression%26entry.906535625%3DZhichao%2520Zhu%2520and%2520Yang%2520Qi%2520and%2520Hengyuan%2520Ma%2520and%2520Wenlian%2520Lu%2520and%2520Jianfeng%2520Feng%26entry.1292438233%3D%2520%2520The%2520Forward-Forward%2520%2528FF%2529%2520algorithm%2520provides%2520a%2520bottom-up%2520alternative%2520to%250Abackpropagation%2520%2528BP%2529%2520for%2520training%2520neural%2520networks%252C%2520relying%2520on%2520a%2520layer-wise%250A%2522goodness%2522%2520function%2520to%2520guide%2520learning.%2520Existing%2520goodness%2520functions%252C%2520inspired%2520by%250Aenergy-based%2520learning%2520%2528EBL%2529%252C%2520are%2520typically%2520defined%2520as%2520the%2520sum%2520of%2520squared%250Apost-synaptic%2520activations%252C%2520neglecting%2520the%2520correlations%2520between%2520neurons.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520novel%2520goodness%2520function%2520termed%2520dimensionality%2520compression%250Athat%2520uses%2520the%2520effective%2520dimensionality%2520%2528ED%2529%2520of%2520fluctuating%2520neural%2520responses%2520to%250Aincorporate%2520second-order%2520statistical%2520structure.%2520Our%2520objective%2520minimizes%2520ED%2520for%250Aclamped%2520inputs%2520when%2520noise%2520is%2520considered%2520while%2520maximizing%2520it%2520across%2520the%2520sample%250Adistribution%252C%2520promoting%2520structured%2520representations%2520without%2520the%2520need%2520to%2520prepare%250Anegative%2520samples.%2520We%2520demonstrate%2520that%2520this%2520formulation%2520achieves%2520competitive%250Aperformance%2520compared%2520to%2520other%2520non-BP%2520methods.%2520Moreover%252C%2520we%2520show%2520that%2520noise%250Aplays%2520a%2520constructive%2520role%2520that%2520can%2520enhance%2520generalization%2520and%2520improve%2520inference%250Awhen%2520predictions%2520are%2520derived%2520from%2520the%2520mean%2520of%2520squared%2520outputs%252C%2520which%2520is%250Aequivalent%2520to%2520making%2520predictions%2520based%2520on%2520the%2520energy%2520term.%2520Our%2520findings%250Acontribute%2520to%2520the%2520development%2520of%2520more%2520biologically%2520plausible%2520learning%250Aalgorithms%2520and%2520suggest%2520a%2520natural%2520fit%2520for%2520neuromorphic%2520computing%252C%2520where%250Astochasticity%2520is%2520a%2520computational%2520resource%2520rather%2520than%2520a%2520nuisance.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/ZhichaoZhu/StochasticForwardForward%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Forward-Forward%20Learning%20through%20Representational%0A%20%20Dimensionality%20Compression&entry.906535625=Zhichao%20Zhu%20and%20Yang%20Qi%20and%20Hengyuan%20Ma%20and%20Wenlian%20Lu%20and%20Jianfeng%20Feng&entry.1292438233=%20%20The%20Forward-Forward%20%28FF%29%20algorithm%20provides%20a%20bottom-up%20alternative%20to%0Abackpropagation%20%28BP%29%20for%20training%20neural%20networks%2C%20relying%20on%20a%20layer-wise%0A%22goodness%22%20function%20to%20guide%20learning.%20Existing%20goodness%20functions%2C%20inspired%20by%0Aenergy-based%20learning%20%28EBL%29%2C%20are%20typically%20defined%20as%20the%20sum%20of%20squared%0Apost-synaptic%20activations%2C%20neglecting%20the%20correlations%20between%20neurons.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20goodness%20function%20termed%20dimensionality%20compression%0Athat%20uses%20the%20effective%20dimensionality%20%28ED%29%20of%20fluctuating%20neural%20responses%20to%0Aincorporate%20second-order%20statistical%20structure.%20Our%20objective%20minimizes%20ED%20for%0Aclamped%20inputs%20when%20noise%20is%20considered%20while%20maximizing%20it%20across%20the%20sample%0Adistribution%2C%20promoting%20structured%20representations%20without%20the%20need%20to%20prepare%0Anegative%20samples.%20We%20demonstrate%20that%20this%20formulation%20achieves%20competitive%0Aperformance%20compared%20to%20other%20non-BP%20methods.%20Moreover%2C%20we%20show%20that%20noise%0Aplays%20a%20constructive%20role%20that%20can%20enhance%20generalization%20and%20improve%20inference%0Awhen%20predictions%20are%20derived%20from%20the%20mean%20of%20squared%20outputs%2C%20which%20is%0Aequivalent%20to%20making%20predictions%20based%20on%20the%20energy%20term.%20Our%20findings%0Acontribute%20to%20the%20development%20of%20more%20biologically%20plausible%20learning%0Aalgorithms%20and%20suggest%20a%20natural%20fit%20for%20neuromorphic%20computing%2C%20where%0Astochasticity%20is%20a%20computational%20resource%20rather%20than%20a%20nuisance.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/ZhichaoZhu/StochasticForwardForward%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16649v1&entry.124074799=Read"},
{"title": "ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation", "author": "Lingfeng Wang and Hualing Lin and Senda Chen and Tao Wang and Changxu Cheng and Yangyang Zhong and Dong Zheng and Wuyue Zhao", "abstract": "  While humans effortlessly draw visual objects and shapes by adaptively\nallocating attention based on their complexity, existing multimodal large\nlanguage models (MLLMs) remain constrained by rigid token representations.\nBridging this gap, we propose ALTo, an adaptive length tokenizer for\nautoregressive mask generation. To achieve this, a novel token length predictor\nis designed, along with a length regularization term and a differentiable token\nchunking strategy. We further build ALToLLM that seamlessly integrates ALTo\ninto MLLM. Preferences on the trade-offs between mask quality and efficiency is\nimplemented by group relative policy optimization (GRPO). Experiments\ndemonstrate that ALToLLM achieves state-of-the-art performance with adaptive\ntoken cost on popular segmentation benchmarks. Code and models are released at\nhttps://github.com/yayafengzi/ALToLLM.\n", "link": "http://arxiv.org/abs/2505.16495v1", "date": "2025-05-22", "relevancy": 2.623, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALTo%3A%20Adaptive-Length%20Tokenizer%20for%20Autoregressive%20Mask%20Generation&body=Title%3A%20ALTo%3A%20Adaptive-Length%20Tokenizer%20for%20Autoregressive%20Mask%20Generation%0AAuthor%3A%20Lingfeng%20Wang%20and%20Hualing%20Lin%20and%20Senda%20Chen%20and%20Tao%20Wang%20and%20Changxu%20Cheng%20and%20Yangyang%20Zhong%20and%20Dong%20Zheng%20and%20Wuyue%20Zhao%0AAbstract%3A%20%20%20While%20humans%20effortlessly%20draw%20visual%20objects%20and%20shapes%20by%20adaptively%0Aallocating%20attention%20based%20on%20their%20complexity%2C%20existing%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20remain%20constrained%20by%20rigid%20token%20representations.%0ABridging%20this%20gap%2C%20we%20propose%20ALTo%2C%20an%20adaptive%20length%20tokenizer%20for%0Aautoregressive%20mask%20generation.%20To%20achieve%20this%2C%20a%20novel%20token%20length%20predictor%0Ais%20designed%2C%20along%20with%20a%20length%20regularization%20term%20and%20a%20differentiable%20token%0Achunking%20strategy.%20We%20further%20build%20ALToLLM%20that%20seamlessly%20integrates%20ALTo%0Ainto%20MLLM.%20Preferences%20on%20the%20trade-offs%20between%20mask%20quality%20and%20efficiency%20is%0Aimplemented%20by%20group%20relative%20policy%20optimization%20%28GRPO%29.%20Experiments%0Ademonstrate%20that%20ALToLLM%20achieves%20state-of-the-art%20performance%20with%20adaptive%0Atoken%20cost%20on%20popular%20segmentation%20benchmarks.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/yayafengzi/ALToLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALTo%253A%2520Adaptive-Length%2520Tokenizer%2520for%2520Autoregressive%2520Mask%2520Generation%26entry.906535625%3DLingfeng%2520Wang%2520and%2520Hualing%2520Lin%2520and%2520Senda%2520Chen%2520and%2520Tao%2520Wang%2520and%2520Changxu%2520Cheng%2520and%2520Yangyang%2520Zhong%2520and%2520Dong%2520Zheng%2520and%2520Wuyue%2520Zhao%26entry.1292438233%3D%2520%2520While%2520humans%2520effortlessly%2520draw%2520visual%2520objects%2520and%2520shapes%2520by%2520adaptively%250Aallocating%2520attention%2520based%2520on%2520their%2520complexity%252C%2520existing%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520remain%2520constrained%2520by%2520rigid%2520token%2520representations.%250ABridging%2520this%2520gap%252C%2520we%2520propose%2520ALTo%252C%2520an%2520adaptive%2520length%2520tokenizer%2520for%250Aautoregressive%2520mask%2520generation.%2520To%2520achieve%2520this%252C%2520a%2520novel%2520token%2520length%2520predictor%250Ais%2520designed%252C%2520along%2520with%2520a%2520length%2520regularization%2520term%2520and%2520a%2520differentiable%2520token%250Achunking%2520strategy.%2520We%2520further%2520build%2520ALToLLM%2520that%2520seamlessly%2520integrates%2520ALTo%250Ainto%2520MLLM.%2520Preferences%2520on%2520the%2520trade-offs%2520between%2520mask%2520quality%2520and%2520efficiency%2520is%250Aimplemented%2520by%2520group%2520relative%2520policy%2520optimization%2520%2528GRPO%2529.%2520Experiments%250Ademonstrate%2520that%2520ALToLLM%2520achieves%2520state-of-the-art%2520performance%2520with%2520adaptive%250Atoken%2520cost%2520on%2520popular%2520segmentation%2520benchmarks.%2520Code%2520and%2520models%2520are%2520released%2520at%250Ahttps%253A//github.com/yayafengzi/ALToLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALTo%3A%20Adaptive-Length%20Tokenizer%20for%20Autoregressive%20Mask%20Generation&entry.906535625=Lingfeng%20Wang%20and%20Hualing%20Lin%20and%20Senda%20Chen%20and%20Tao%20Wang%20and%20Changxu%20Cheng%20and%20Yangyang%20Zhong%20and%20Dong%20Zheng%20and%20Wuyue%20Zhao&entry.1292438233=%20%20While%20humans%20effortlessly%20draw%20visual%20objects%20and%20shapes%20by%20adaptively%0Aallocating%20attention%20based%20on%20their%20complexity%2C%20existing%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29%20remain%20constrained%20by%20rigid%20token%20representations.%0ABridging%20this%20gap%2C%20we%20propose%20ALTo%2C%20an%20adaptive%20length%20tokenizer%20for%0Aautoregressive%20mask%20generation.%20To%20achieve%20this%2C%20a%20novel%20token%20length%20predictor%0Ais%20designed%2C%20along%20with%20a%20length%20regularization%20term%20and%20a%20differentiable%20token%0Achunking%20strategy.%20We%20further%20build%20ALToLLM%20that%20seamlessly%20integrates%20ALTo%0Ainto%20MLLM.%20Preferences%20on%20the%20trade-offs%20between%20mask%20quality%20and%20efficiency%20is%0Aimplemented%20by%20group%20relative%20policy%20optimization%20%28GRPO%29.%20Experiments%0Ademonstrate%20that%20ALToLLM%20achieves%20state-of-the-art%20performance%20with%20adaptive%0Atoken%20cost%20on%20popular%20segmentation%20benchmarks.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/yayafengzi/ALToLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16495v1&entry.124074799=Read"},
{"title": "Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose\n  Interaction", "author": "Dong Li and Wenqi Zhong and Wei Yu and Yingwei Pan and Dingwen Zhang and Ting Yao and Junwei Han and Tao Mei", "abstract": "  Video virtual try-on aims to seamlessly dress a subject in a video with a\nspecific garment. The primary challenge involves preserving the visual\nauthenticity of the garment while dynamically adapting to the pose and physique\nof the subject. While existing methods have predominantly focused on\nimage-based virtual try-on, extending these techniques directly to videos often\nresults in temporal inconsistencies. Most current video virtual try-on\napproaches alleviate this challenge by incorporating temporal modules, yet\nstill overlook the critical spatiotemporal pose interactions between human and\ngarment. Effective pose interactions in videos should not only consider spatial\nalignment between human and garment poses in each frame but also account for\nthe temporal dynamics of human poses throughout the entire video. With such\nmotivation, we propose a new framework, namely Dynamic Pose Interaction\nDiffusion Models (DPIDM), to leverage diffusion models to delve into dynamic\npose interactions for video virtual try-on. Technically, DPIDM introduces a\nskeleton-based pose adapter to integrate synchronized human and garment poses\ninto the denoising network. A hierarchical attention module is then exquisitely\ndesigned to model intra-frame human-garment pose interactions and long-term\nhuman pose dynamics across frames through pose-aware spatial and temporal\nattention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized\nattention loss between consecutive frames to enhance temporal consistency.\nExtensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate\nthe superiority of our DPIDM against the baseline methods. Notably, DPIDM\nachieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over\nthe state-of-the-art GPD-VVTO approach.\n", "link": "http://arxiv.org/abs/2505.16980v1", "date": "2025-05-22", "relevancy": 2.6227, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6596}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6529}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pursuing%20Temporal-Consistent%20Video%20Virtual%20Try-On%20via%20Dynamic%20Pose%0A%20%20Interaction&body=Title%3A%20Pursuing%20Temporal-Consistent%20Video%20Virtual%20Try-On%20via%20Dynamic%20Pose%0A%20%20Interaction%0AAuthor%3A%20Dong%20Li%20and%20Wenqi%20Zhong%20and%20Wei%20Yu%20and%20Yingwei%20Pan%20and%20Dingwen%20Zhang%20and%20Ting%20Yao%20and%20Junwei%20Han%20and%20Tao%20Mei%0AAbstract%3A%20%20%20Video%20virtual%20try-on%20aims%20to%20seamlessly%20dress%20a%20subject%20in%20a%20video%20with%20a%0Aspecific%20garment.%20The%20primary%20challenge%20involves%20preserving%20the%20visual%0Aauthenticity%20of%20the%20garment%20while%20dynamically%20adapting%20to%20the%20pose%20and%20physique%0Aof%20the%20subject.%20While%20existing%20methods%20have%20predominantly%20focused%20on%0Aimage-based%20virtual%20try-on%2C%20extending%20these%20techniques%20directly%20to%20videos%20often%0Aresults%20in%20temporal%20inconsistencies.%20Most%20current%20video%20virtual%20try-on%0Aapproaches%20alleviate%20this%20challenge%20by%20incorporating%20temporal%20modules%2C%20yet%0Astill%20overlook%20the%20critical%20spatiotemporal%20pose%20interactions%20between%20human%20and%0Agarment.%20Effective%20pose%20interactions%20in%20videos%20should%20not%20only%20consider%20spatial%0Aalignment%20between%20human%20and%20garment%20poses%20in%20each%20frame%20but%20also%20account%20for%0Athe%20temporal%20dynamics%20of%20human%20poses%20throughout%20the%20entire%20video.%20With%20such%0Amotivation%2C%20we%20propose%20a%20new%20framework%2C%20namely%20Dynamic%20Pose%20Interaction%0ADiffusion%20Models%20%28DPIDM%29%2C%20to%20leverage%20diffusion%20models%20to%20delve%20into%20dynamic%0Apose%20interactions%20for%20video%20virtual%20try-on.%20Technically%2C%20DPIDM%20introduces%20a%0Askeleton-based%20pose%20adapter%20to%20integrate%20synchronized%20human%20and%20garment%20poses%0Ainto%20the%20denoising%20network.%20A%20hierarchical%20attention%20module%20is%20then%20exquisitely%0Adesigned%20to%20model%20intra-frame%20human-garment%20pose%20interactions%20and%20long-term%0Ahuman%20pose%20dynamics%20across%20frames%20through%20pose-aware%20spatial%20and%20temporal%0Aattention%20mechanisms.%20Moreover%2C%20DPIDM%20capitalizes%20on%20a%20temporal%20regularized%0Aattention%20loss%20between%20consecutive%20frames%20to%20enhance%20temporal%20consistency.%0AExtensive%20experiments%20conducted%20on%20VITON-HD%2C%20VVT%20and%20ViViD%20datasets%20demonstrate%0Athe%20superiority%20of%20our%20DPIDM%20against%20the%20baseline%20methods.%20Notably%2C%20DPIDM%0Aachieves%20VFID%20score%20of%200.506%20on%20VVT%20dataset%2C%20leading%20to%2060.5%25%20improvement%20over%0Athe%20state-of-the-art%20GPD-VVTO%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPursuing%2520Temporal-Consistent%2520Video%2520Virtual%2520Try-On%2520via%2520Dynamic%2520Pose%250A%2520%2520Interaction%26entry.906535625%3DDong%2520Li%2520and%2520Wenqi%2520Zhong%2520and%2520Wei%2520Yu%2520and%2520Yingwei%2520Pan%2520and%2520Dingwen%2520Zhang%2520and%2520Ting%2520Yao%2520and%2520Junwei%2520Han%2520and%2520Tao%2520Mei%26entry.1292438233%3D%2520%2520Video%2520virtual%2520try-on%2520aims%2520to%2520seamlessly%2520dress%2520a%2520subject%2520in%2520a%2520video%2520with%2520a%250Aspecific%2520garment.%2520The%2520primary%2520challenge%2520involves%2520preserving%2520the%2520visual%250Aauthenticity%2520of%2520the%2520garment%2520while%2520dynamically%2520adapting%2520to%2520the%2520pose%2520and%2520physique%250Aof%2520the%2520subject.%2520While%2520existing%2520methods%2520have%2520predominantly%2520focused%2520on%250Aimage-based%2520virtual%2520try-on%252C%2520extending%2520these%2520techniques%2520directly%2520to%2520videos%2520often%250Aresults%2520in%2520temporal%2520inconsistencies.%2520Most%2520current%2520video%2520virtual%2520try-on%250Aapproaches%2520alleviate%2520this%2520challenge%2520by%2520incorporating%2520temporal%2520modules%252C%2520yet%250Astill%2520overlook%2520the%2520critical%2520spatiotemporal%2520pose%2520interactions%2520between%2520human%2520and%250Agarment.%2520Effective%2520pose%2520interactions%2520in%2520videos%2520should%2520not%2520only%2520consider%2520spatial%250Aalignment%2520between%2520human%2520and%2520garment%2520poses%2520in%2520each%2520frame%2520but%2520also%2520account%2520for%250Athe%2520temporal%2520dynamics%2520of%2520human%2520poses%2520throughout%2520the%2520entire%2520video.%2520With%2520such%250Amotivation%252C%2520we%2520propose%2520a%2520new%2520framework%252C%2520namely%2520Dynamic%2520Pose%2520Interaction%250ADiffusion%2520Models%2520%2528DPIDM%2529%252C%2520to%2520leverage%2520diffusion%2520models%2520to%2520delve%2520into%2520dynamic%250Apose%2520interactions%2520for%2520video%2520virtual%2520try-on.%2520Technically%252C%2520DPIDM%2520introduces%2520a%250Askeleton-based%2520pose%2520adapter%2520to%2520integrate%2520synchronized%2520human%2520and%2520garment%2520poses%250Ainto%2520the%2520denoising%2520network.%2520A%2520hierarchical%2520attention%2520module%2520is%2520then%2520exquisitely%250Adesigned%2520to%2520model%2520intra-frame%2520human-garment%2520pose%2520interactions%2520and%2520long-term%250Ahuman%2520pose%2520dynamics%2520across%2520frames%2520through%2520pose-aware%2520spatial%2520and%2520temporal%250Aattention%2520mechanisms.%2520Moreover%252C%2520DPIDM%2520capitalizes%2520on%2520a%2520temporal%2520regularized%250Aattention%2520loss%2520between%2520consecutive%2520frames%2520to%2520enhance%2520temporal%2520consistency.%250AExtensive%2520experiments%2520conducted%2520on%2520VITON-HD%252C%2520VVT%2520and%2520ViViD%2520datasets%2520demonstrate%250Athe%2520superiority%2520of%2520our%2520DPIDM%2520against%2520the%2520baseline%2520methods.%2520Notably%252C%2520DPIDM%250Aachieves%2520VFID%2520score%2520of%25200.506%2520on%2520VVT%2520dataset%252C%2520leading%2520to%252060.5%2525%2520improvement%2520over%250Athe%2520state-of-the-art%2520GPD-VVTO%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pursuing%20Temporal-Consistent%20Video%20Virtual%20Try-On%20via%20Dynamic%20Pose%0A%20%20Interaction&entry.906535625=Dong%20Li%20and%20Wenqi%20Zhong%20and%20Wei%20Yu%20and%20Yingwei%20Pan%20and%20Dingwen%20Zhang%20and%20Ting%20Yao%20and%20Junwei%20Han%20and%20Tao%20Mei&entry.1292438233=%20%20Video%20virtual%20try-on%20aims%20to%20seamlessly%20dress%20a%20subject%20in%20a%20video%20with%20a%0Aspecific%20garment.%20The%20primary%20challenge%20involves%20preserving%20the%20visual%0Aauthenticity%20of%20the%20garment%20while%20dynamically%20adapting%20to%20the%20pose%20and%20physique%0Aof%20the%20subject.%20While%20existing%20methods%20have%20predominantly%20focused%20on%0Aimage-based%20virtual%20try-on%2C%20extending%20these%20techniques%20directly%20to%20videos%20often%0Aresults%20in%20temporal%20inconsistencies.%20Most%20current%20video%20virtual%20try-on%0Aapproaches%20alleviate%20this%20challenge%20by%20incorporating%20temporal%20modules%2C%20yet%0Astill%20overlook%20the%20critical%20spatiotemporal%20pose%20interactions%20between%20human%20and%0Agarment.%20Effective%20pose%20interactions%20in%20videos%20should%20not%20only%20consider%20spatial%0Aalignment%20between%20human%20and%20garment%20poses%20in%20each%20frame%20but%20also%20account%20for%0Athe%20temporal%20dynamics%20of%20human%20poses%20throughout%20the%20entire%20video.%20With%20such%0Amotivation%2C%20we%20propose%20a%20new%20framework%2C%20namely%20Dynamic%20Pose%20Interaction%0ADiffusion%20Models%20%28DPIDM%29%2C%20to%20leverage%20diffusion%20models%20to%20delve%20into%20dynamic%0Apose%20interactions%20for%20video%20virtual%20try-on.%20Technically%2C%20DPIDM%20introduces%20a%0Askeleton-based%20pose%20adapter%20to%20integrate%20synchronized%20human%20and%20garment%20poses%0Ainto%20the%20denoising%20network.%20A%20hierarchical%20attention%20module%20is%20then%20exquisitely%0Adesigned%20to%20model%20intra-frame%20human-garment%20pose%20interactions%20and%20long-term%0Ahuman%20pose%20dynamics%20across%20frames%20through%20pose-aware%20spatial%20and%20temporal%0Aattention%20mechanisms.%20Moreover%2C%20DPIDM%20capitalizes%20on%20a%20temporal%20regularized%0Aattention%20loss%20between%20consecutive%20frames%20to%20enhance%20temporal%20consistency.%0AExtensive%20experiments%20conducted%20on%20VITON-HD%2C%20VVT%20and%20ViViD%20datasets%20demonstrate%0Athe%20superiority%20of%20our%20DPIDM%20against%20the%20baseline%20methods.%20Notably%2C%20DPIDM%0Aachieves%20VFID%20score%20of%200.506%20on%20VVT%20dataset%2C%20leading%20to%2060.5%25%20improvement%20over%0Athe%20state-of-the-art%20GPD-VVTO%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16980v1&entry.124074799=Read"},
{"title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion\n  Representations in LLaMA Models Through Probing", "author": "Dario Di Palma and Alessandro De Bellis and Giovanni Servedio and Vito Walter Anelli and Fedelucio Narducci and Tommaso Di Noia", "abstract": "  Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements.\n", "link": "http://arxiv.org/abs/2505.16491v1", "date": "2025-05-22", "relevancy": 2.6046, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaMAs%20Have%20Feelings%20Too%3A%20Unveiling%20Sentiment%20and%20Emotion%0A%20%20Representations%20in%20LLaMA%20Models%20Through%20Probing&body=Title%3A%20LLaMAs%20Have%20Feelings%20Too%3A%20Unveiling%20Sentiment%20and%20Emotion%0A%20%20Representations%20in%20LLaMA%20Models%20Through%20Probing%0AAuthor%3A%20Dario%20Di%20Palma%20and%20Alessandro%20De%20Bellis%20and%20Giovanni%20Servedio%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20rapidly%20become%20central%20to%20NLP%2C%0Ademonstrating%20their%20ability%20to%20adapt%20to%20various%20tasks%20through%20prompting%0Atechniques%2C%20including%20sentiment%20analysis.%20However%2C%20we%20still%20have%20a%20limited%0Aunderstanding%20of%20how%20these%20models%20capture%20sentiment-related%20information.%20This%0Astudy%20probes%20the%20hidden%20layers%20of%20Llama%20models%20to%20pinpoint%20where%20sentiment%0Afeatures%20are%20most%20represented%20and%20to%20assess%20how%20this%20affects%20sentiment%0Aanalysis.%0A%20%20Using%20probe%20classifiers%2C%20we%20analyze%20sentiment%20encoding%20across%20layers%20and%0Ascales%2C%20identifying%20the%20layers%20and%20pooling%20methods%20that%20best%20capture%20sentiment%0Asignals.%20Our%20results%20show%20that%20sentiment%20information%20is%20most%20concentrated%20in%0Amid-layers%20for%20binary%20polarity%20tasks%2C%20with%20detection%20accuracy%20increasing%20up%20to%0A14%25%20over%20prompting%20techniques.%20Additionally%2C%20we%20find%20that%20in%20decoder-only%0Amodels%2C%20the%20last%20token%20is%20not%20consistently%20the%20most%20informative%20for%20sentiment%0Aencoding.%20Finally%2C%20this%20approach%20enables%20sentiment%20tasks%20to%20be%20performed%20with%0Amemory%20requirements%20reduced%20by%20an%20average%20of%2057%25.%0A%20%20These%20insights%20contribute%20to%20a%20broader%20understanding%20of%20sentiment%20in%20LLMs%2C%0Asuggesting%20layer-specific%20probing%20as%20an%20effective%20approach%20for%20sentiment%20tasks%0Abeyond%20prompting%2C%20with%20potential%20to%20enhance%20model%20utility%20and%20reduce%20memory%0Arequirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaMAs%2520Have%2520Feelings%2520Too%253A%2520Unveiling%2520Sentiment%2520and%2520Emotion%250A%2520%2520Representations%2520in%2520LLaMA%2520Models%2520Through%2520Probing%26entry.906535625%3DDario%2520Di%2520Palma%2520and%2520Alessandro%2520De%2520Bellis%2520and%2520Giovanni%2520Servedio%2520and%2520Vito%2520Walter%2520Anelli%2520and%2520Fedelucio%2520Narducci%2520and%2520Tommaso%2520Di%2520Noia%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520rapidly%2520become%2520central%2520to%2520NLP%252C%250Ademonstrating%2520their%2520ability%2520to%2520adapt%2520to%2520various%2520tasks%2520through%2520prompting%250Atechniques%252C%2520including%2520sentiment%2520analysis.%2520However%252C%2520we%2520still%2520have%2520a%2520limited%250Aunderstanding%2520of%2520how%2520these%2520models%2520capture%2520sentiment-related%2520information.%2520This%250Astudy%2520probes%2520the%2520hidden%2520layers%2520of%2520Llama%2520models%2520to%2520pinpoint%2520where%2520sentiment%250Afeatures%2520are%2520most%2520represented%2520and%2520to%2520assess%2520how%2520this%2520affects%2520sentiment%250Aanalysis.%250A%2520%2520Using%2520probe%2520classifiers%252C%2520we%2520analyze%2520sentiment%2520encoding%2520across%2520layers%2520and%250Ascales%252C%2520identifying%2520the%2520layers%2520and%2520pooling%2520methods%2520that%2520best%2520capture%2520sentiment%250Asignals.%2520Our%2520results%2520show%2520that%2520sentiment%2520information%2520is%2520most%2520concentrated%2520in%250Amid-layers%2520for%2520binary%2520polarity%2520tasks%252C%2520with%2520detection%2520accuracy%2520increasing%2520up%2520to%250A14%2525%2520over%2520prompting%2520techniques.%2520Additionally%252C%2520we%2520find%2520that%2520in%2520decoder-only%250Amodels%252C%2520the%2520last%2520token%2520is%2520not%2520consistently%2520the%2520most%2520informative%2520for%2520sentiment%250Aencoding.%2520Finally%252C%2520this%2520approach%2520enables%2520sentiment%2520tasks%2520to%2520be%2520performed%2520with%250Amemory%2520requirements%2520reduced%2520by%2520an%2520average%2520of%252057%2525.%250A%2520%2520These%2520insights%2520contribute%2520to%2520a%2520broader%2520understanding%2520of%2520sentiment%2520in%2520LLMs%252C%250Asuggesting%2520layer-specific%2520probing%2520as%2520an%2520effective%2520approach%2520for%2520sentiment%2520tasks%250Abeyond%2520prompting%252C%2520with%2520potential%2520to%2520enhance%2520model%2520utility%2520and%2520reduce%2520memory%250Arequirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaMAs%20Have%20Feelings%20Too%3A%20Unveiling%20Sentiment%20and%20Emotion%0A%20%20Representations%20in%20LLaMA%20Models%20Through%20Probing&entry.906535625=Dario%20Di%20Palma%20and%20Alessandro%20De%20Bellis%20and%20Giovanni%20Servedio%20and%20Vito%20Walter%20Anelli%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20rapidly%20become%20central%20to%20NLP%2C%0Ademonstrating%20their%20ability%20to%20adapt%20to%20various%20tasks%20through%20prompting%0Atechniques%2C%20including%20sentiment%20analysis.%20However%2C%20we%20still%20have%20a%20limited%0Aunderstanding%20of%20how%20these%20models%20capture%20sentiment-related%20information.%20This%0Astudy%20probes%20the%20hidden%20layers%20of%20Llama%20models%20to%20pinpoint%20where%20sentiment%0Afeatures%20are%20most%20represented%20and%20to%20assess%20how%20this%20affects%20sentiment%0Aanalysis.%0A%20%20Using%20probe%20classifiers%2C%20we%20analyze%20sentiment%20encoding%20across%20layers%20and%0Ascales%2C%20identifying%20the%20layers%20and%20pooling%20methods%20that%20best%20capture%20sentiment%0Asignals.%20Our%20results%20show%20that%20sentiment%20information%20is%20most%20concentrated%20in%0Amid-layers%20for%20binary%20polarity%20tasks%2C%20with%20detection%20accuracy%20increasing%20up%20to%0A14%25%20over%20prompting%20techniques.%20Additionally%2C%20we%20find%20that%20in%20decoder-only%0Amodels%2C%20the%20last%20token%20is%20not%20consistently%20the%20most%20informative%20for%20sentiment%0Aencoding.%20Finally%2C%20this%20approach%20enables%20sentiment%20tasks%20to%20be%20performed%20with%0Amemory%20requirements%20reduced%20by%20an%20average%20of%2057%25.%0A%20%20These%20insights%20contribute%20to%20a%20broader%20understanding%20of%20sentiment%20in%20LLMs%2C%0Asuggesting%20layer-specific%20probing%20as%20an%20effective%20approach%20for%20sentiment%20tasks%0Abeyond%20prompting%2C%20with%20potential%20to%20enhance%20model%20utility%20and%20reduce%20memory%0Arequirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16491v1&entry.124074799=Read"},
{"title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use\n  Capabilities in Large Language Models", "author": "Xuanqi Gao and Siyi Xie and Juan Zhai and Shqing Ma and Chao Shen", "abstract": "  As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143.\n", "link": "http://arxiv.org/abs/2505.16700v1", "date": "2025-05-22", "relevancy": 2.6026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCP-RADAR%3A%20A%20Multi-Dimensional%20Benchmark%20for%20Evaluating%20Tool%20Use%0A%20%20Capabilities%20in%20Large%20Language%20Models&body=Title%3A%20MCP-RADAR%3A%20A%20Multi-Dimensional%20Benchmark%20for%20Evaluating%20Tool%20Use%0A%20%20Capabilities%20in%20Large%20Language%20Models%0AAuthor%3A%20Xuanqi%20Gao%20and%20Siyi%20Xie%20and%20Juan%20Zhai%20and%20Shqing%20Ma%20and%20Chao%20Shen%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20evolve%20from%20passive%20text%20generators%20to%20active%0Areasoning%20agents%20capable%20of%20tool%20interaction%2C%20the%20Model%20Context%20Protocol%20%28MCP%29%0Ahas%20emerged%20as%20a%20standardized%20framework%20for%20dynamic%20tool%20discovery%20and%0Aorchestration.%20Despite%20widespread%20industry%20adoption%2C%20existing%20evaluation%0Amethodologies%20fail%20to%20adequately%20assess%20tool%20utilization%20capabilities%20within%0Athis%20new%20paradigm.%20This%20paper%20introduces%20MCP-RADAR%2C%20the%20first%20comprehensive%0Abenchmark%20specifically%20designed%20to%20evaluate%20LLM%20performance%20in%20the%20MCP%0Aframework%20through%20a%20novel%20five-dimensional%20approach%20measuring%3A%20answer%20accuracy%2C%0Atool%20selection%20efficiency%2C%20computational%20resource%20efficiency%2C%20parameter%0Aconstruction%20accuracy%2C%20and%20execution%20speed.%20Unlike%20conventional%20benchmarks%20that%0Arely%20on%20subjective%20human%20evaluations%20or%20binary%20success%20metrics%2C%20MCP-RADAR%0Aemploys%20objective%2C%20quantifiable%20measurements%20across%20multiple%20task%20domains%0Aincluding%20software%20engineering%2C%20mathematical%20reasoning%2C%20and%20general%0Aproblem-solving.%20Our%20evaluations%20of%20leading%20commercial%20and%20open-source%20LLMs%0Areveal%20distinctive%20capability%20profiles%20with%20significant%20trade-offs%20between%0Aaccuracy%2C%20efficiency%2C%20and%20speed%2C%20challenging%20traditional%20single-metric%0Aperformance%20rankings.%20Besides%2C%20we%20provide%20valuable%20guidance%20for%20developers%20to%0Aoptimize%20their%20tools%20for%20maximum%20model%20compatibility%20and%20effectiveness.%20While%0Afocused%20on%20MCP%20due%20to%20its%20standardized%20approach%2C%20our%20methodology%20remains%0Aapplicable%20across%20all%20LLM%20agent%20tool%20integration%20frameworks%2C%20providing%20valuable%0Ainsights%20for%20both%20LLM%20developers%20and%20tool%20creators%20to%20optimize%20the%20entire%0ALLM-tool%20interaction%20ecosystem.%20The%20implementation%2C%20configurations%2C%20and%0Adatasets%20used%20in%20our%20evaluation%20are%20publicly%20available%20at%0Ahttps%3A//anonymous.4open.science/r/MCPRadar-B143.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCP-RADAR%253A%2520A%2520Multi-Dimensional%2520Benchmark%2520for%2520Evaluating%2520Tool%2520Use%250A%2520%2520Capabilities%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DXuanqi%2520Gao%2520and%2520Siyi%2520Xie%2520and%2520Juan%2520Zhai%2520and%2520Shqing%2520Ma%2520and%2520Chao%2520Shen%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520evolve%2520from%2520passive%2520text%2520generators%2520to%2520active%250Areasoning%2520agents%2520capable%2520of%2520tool%2520interaction%252C%2520the%2520Model%2520Context%2520Protocol%2520%2528MCP%2529%250Ahas%2520emerged%2520as%2520a%2520standardized%2520framework%2520for%2520dynamic%2520tool%2520discovery%2520and%250Aorchestration.%2520Despite%2520widespread%2520industry%2520adoption%252C%2520existing%2520evaluation%250Amethodologies%2520fail%2520to%2520adequately%2520assess%2520tool%2520utilization%2520capabilities%2520within%250Athis%2520new%2520paradigm.%2520This%2520paper%2520introduces%2520MCP-RADAR%252C%2520the%2520first%2520comprehensive%250Abenchmark%2520specifically%2520designed%2520to%2520evaluate%2520LLM%2520performance%2520in%2520the%2520MCP%250Aframework%2520through%2520a%2520novel%2520five-dimensional%2520approach%2520measuring%253A%2520answer%2520accuracy%252C%250Atool%2520selection%2520efficiency%252C%2520computational%2520resource%2520efficiency%252C%2520parameter%250Aconstruction%2520accuracy%252C%2520and%2520execution%2520speed.%2520Unlike%2520conventional%2520benchmarks%2520that%250Arely%2520on%2520subjective%2520human%2520evaluations%2520or%2520binary%2520success%2520metrics%252C%2520MCP-RADAR%250Aemploys%2520objective%252C%2520quantifiable%2520measurements%2520across%2520multiple%2520task%2520domains%250Aincluding%2520software%2520engineering%252C%2520mathematical%2520reasoning%252C%2520and%2520general%250Aproblem-solving.%2520Our%2520evaluations%2520of%2520leading%2520commercial%2520and%2520open-source%2520LLMs%250Areveal%2520distinctive%2520capability%2520profiles%2520with%2520significant%2520trade-offs%2520between%250Aaccuracy%252C%2520efficiency%252C%2520and%2520speed%252C%2520challenging%2520traditional%2520single-metric%250Aperformance%2520rankings.%2520Besides%252C%2520we%2520provide%2520valuable%2520guidance%2520for%2520developers%2520to%250Aoptimize%2520their%2520tools%2520for%2520maximum%2520model%2520compatibility%2520and%2520effectiveness.%2520While%250Afocused%2520on%2520MCP%2520due%2520to%2520its%2520standardized%2520approach%252C%2520our%2520methodology%2520remains%250Aapplicable%2520across%2520all%2520LLM%2520agent%2520tool%2520integration%2520frameworks%252C%2520providing%2520valuable%250Ainsights%2520for%2520both%2520LLM%2520developers%2520and%2520tool%2520creators%2520to%2520optimize%2520the%2520entire%250ALLM-tool%2520interaction%2520ecosystem.%2520The%2520implementation%252C%2520configurations%252C%2520and%250Adatasets%2520used%2520in%2520our%2520evaluation%2520are%2520publicly%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/MCPRadar-B143.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCP-RADAR%3A%20A%20Multi-Dimensional%20Benchmark%20for%20Evaluating%20Tool%20Use%0A%20%20Capabilities%20in%20Large%20Language%20Models&entry.906535625=Xuanqi%20Gao%20and%20Siyi%20Xie%20and%20Juan%20Zhai%20and%20Shqing%20Ma%20and%20Chao%20Shen&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20evolve%20from%20passive%20text%20generators%20to%20active%0Areasoning%20agents%20capable%20of%20tool%20interaction%2C%20the%20Model%20Context%20Protocol%20%28MCP%29%0Ahas%20emerged%20as%20a%20standardized%20framework%20for%20dynamic%20tool%20discovery%20and%0Aorchestration.%20Despite%20widespread%20industry%20adoption%2C%20existing%20evaluation%0Amethodologies%20fail%20to%20adequately%20assess%20tool%20utilization%20capabilities%20within%0Athis%20new%20paradigm.%20This%20paper%20introduces%20MCP-RADAR%2C%20the%20first%20comprehensive%0Abenchmark%20specifically%20designed%20to%20evaluate%20LLM%20performance%20in%20the%20MCP%0Aframework%20through%20a%20novel%20five-dimensional%20approach%20measuring%3A%20answer%20accuracy%2C%0Atool%20selection%20efficiency%2C%20computational%20resource%20efficiency%2C%20parameter%0Aconstruction%20accuracy%2C%20and%20execution%20speed.%20Unlike%20conventional%20benchmarks%20that%0Arely%20on%20subjective%20human%20evaluations%20or%20binary%20success%20metrics%2C%20MCP-RADAR%0Aemploys%20objective%2C%20quantifiable%20measurements%20across%20multiple%20task%20domains%0Aincluding%20software%20engineering%2C%20mathematical%20reasoning%2C%20and%20general%0Aproblem-solving.%20Our%20evaluations%20of%20leading%20commercial%20and%20open-source%20LLMs%0Areveal%20distinctive%20capability%20profiles%20with%20significant%20trade-offs%20between%0Aaccuracy%2C%20efficiency%2C%20and%20speed%2C%20challenging%20traditional%20single-metric%0Aperformance%20rankings.%20Besides%2C%20we%20provide%20valuable%20guidance%20for%20developers%20to%0Aoptimize%20their%20tools%20for%20maximum%20model%20compatibility%20and%20effectiveness.%20While%0Afocused%20on%20MCP%20due%20to%20its%20standardized%20approach%2C%20our%20methodology%20remains%0Aapplicable%20across%20all%20LLM%20agent%20tool%20integration%20frameworks%2C%20providing%20valuable%0Ainsights%20for%20both%20LLM%20developers%20and%20tool%20creators%20to%20optimize%20the%20entire%0ALLM-tool%20interaction%20ecosystem.%20The%20implementation%2C%20configurations%2C%20and%0Adatasets%20used%20in%20our%20evaluation%20are%20publicly%20available%20at%0Ahttps%3A//anonymous.4open.science/r/MCPRadar-B143.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16700v1&entry.124074799=Read"},
{"title": "Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space\n  Guided Diffusion", "author": "Xingpei Ma and Jiaran Cai and Yuansheng Guan and Shenneng Huang and Qiang Zhang and Shunsi Zhang", "abstract": "  Recent diffusion-based talking face generation models have demonstrated\nimpressive potential in synthesizing videos that accurately match a speech\naudio clip with a given reference identity. However, existing approaches still\nencounter significant challenges due to uncontrollable factors, such as\ninaccurate lip-sync, inappropriate head posture and the lack of fine-grained\ncontrol over facial expressions. In order to introduce more face-guided\nconditions beyond speech audio clips, a novel two-stage training framework\nPlaymate is proposed to generate more lifelike facial expressions and talking\nfaces. In the first stage, we introduce a decoupled implicit 3D representation\nalong with a meticulously designed motion-decoupled module to facilitate more\naccurate attribute disentanglement and generate expressive talking videos\ndirectly from audio cues. Then, in the second stage, we introduce an\nemotion-control module to encode emotion control information into the latent\nspace, enabling fine-grained control over emotions and thereby achieving the\nability to generate talking videos with desired emotion. Extensive experiments\ndemonstrate that Playmate not only outperforms existing state-of-the-art\nmethods in terms of video quality, but also exhibits strong competitiveness in\nlip synchronization while offering improved flexibility in controlling emotion\nand head pose. The code will be available at\nhttps://github.com/Playmate111/Playmate.\n", "link": "http://arxiv.org/abs/2502.07203v3", "date": "2025-05-22", "relevancy": 2.6, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6755}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6405}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Playmate%3A%20Flexible%20Control%20of%20Portrait%20Animation%20via%203D-Implicit%20Space%0A%20%20Guided%20Diffusion&body=Title%3A%20Playmate%3A%20Flexible%20Control%20of%20Portrait%20Animation%20via%203D-Implicit%20Space%0A%20%20Guided%20Diffusion%0AAuthor%3A%20Xingpei%20Ma%20and%20Jiaran%20Cai%20and%20Yuansheng%20Guan%20and%20Shenneng%20Huang%20and%20Qiang%20Zhang%20and%20Shunsi%20Zhang%0AAbstract%3A%20%20%20Recent%20diffusion-based%20talking%20face%20generation%20models%20have%20demonstrated%0Aimpressive%20potential%20in%20synthesizing%20videos%20that%20accurately%20match%20a%20speech%0Aaudio%20clip%20with%20a%20given%20reference%20identity.%20However%2C%20existing%20approaches%20still%0Aencounter%20significant%20challenges%20due%20to%20uncontrollable%20factors%2C%20such%20as%0Ainaccurate%20lip-sync%2C%20inappropriate%20head%20posture%20and%20the%20lack%20of%20fine-grained%0Acontrol%20over%20facial%20expressions.%20In%20order%20to%20introduce%20more%20face-guided%0Aconditions%20beyond%20speech%20audio%20clips%2C%20a%20novel%20two-stage%20training%20framework%0APlaymate%20is%20proposed%20to%20generate%20more%20lifelike%20facial%20expressions%20and%20talking%0Afaces.%20In%20the%20first%20stage%2C%20we%20introduce%20a%20decoupled%20implicit%203D%20representation%0Aalong%20with%20a%20meticulously%20designed%20motion-decoupled%20module%20to%20facilitate%20more%0Aaccurate%20attribute%20disentanglement%20and%20generate%20expressive%20talking%20videos%0Adirectly%20from%20audio%20cues.%20Then%2C%20in%20the%20second%20stage%2C%20we%20introduce%20an%0Aemotion-control%20module%20to%20encode%20emotion%20control%20information%20into%20the%20latent%0Aspace%2C%20enabling%20fine-grained%20control%20over%20emotions%20and%20thereby%20achieving%20the%0Aability%20to%20generate%20talking%20videos%20with%20desired%20emotion.%20Extensive%20experiments%0Ademonstrate%20that%20Playmate%20not%20only%20outperforms%20existing%20state-of-the-art%0Amethods%20in%20terms%20of%20video%20quality%2C%20but%20also%20exhibits%20strong%20competitiveness%20in%0Alip%20synchronization%20while%20offering%20improved%20flexibility%20in%20controlling%20emotion%0Aand%20head%20pose.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Playmate111/Playmate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07203v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlaymate%253A%2520Flexible%2520Control%2520of%2520Portrait%2520Animation%2520via%25203D-Implicit%2520Space%250A%2520%2520Guided%2520Diffusion%26entry.906535625%3DXingpei%2520Ma%2520and%2520Jiaran%2520Cai%2520and%2520Yuansheng%2520Guan%2520and%2520Shenneng%2520Huang%2520and%2520Qiang%2520Zhang%2520and%2520Shunsi%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520diffusion-based%2520talking%2520face%2520generation%2520models%2520have%2520demonstrated%250Aimpressive%2520potential%2520in%2520synthesizing%2520videos%2520that%2520accurately%2520match%2520a%2520speech%250Aaudio%2520clip%2520with%2520a%2520given%2520reference%2520identity.%2520However%252C%2520existing%2520approaches%2520still%250Aencounter%2520significant%2520challenges%2520due%2520to%2520uncontrollable%2520factors%252C%2520such%2520as%250Ainaccurate%2520lip-sync%252C%2520inappropriate%2520head%2520posture%2520and%2520the%2520lack%2520of%2520fine-grained%250Acontrol%2520over%2520facial%2520expressions.%2520In%2520order%2520to%2520introduce%2520more%2520face-guided%250Aconditions%2520beyond%2520speech%2520audio%2520clips%252C%2520a%2520novel%2520two-stage%2520training%2520framework%250APlaymate%2520is%2520proposed%2520to%2520generate%2520more%2520lifelike%2520facial%2520expressions%2520and%2520talking%250Afaces.%2520In%2520the%2520first%2520stage%252C%2520we%2520introduce%2520a%2520decoupled%2520implicit%25203D%2520representation%250Aalong%2520with%2520a%2520meticulously%2520designed%2520motion-decoupled%2520module%2520to%2520facilitate%2520more%250Aaccurate%2520attribute%2520disentanglement%2520and%2520generate%2520expressive%2520talking%2520videos%250Adirectly%2520from%2520audio%2520cues.%2520Then%252C%2520in%2520the%2520second%2520stage%252C%2520we%2520introduce%2520an%250Aemotion-control%2520module%2520to%2520encode%2520emotion%2520control%2520information%2520into%2520the%2520latent%250Aspace%252C%2520enabling%2520fine-grained%2520control%2520over%2520emotions%2520and%2520thereby%2520achieving%2520the%250Aability%2520to%2520generate%2520talking%2520videos%2520with%2520desired%2520emotion.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Playmate%2520not%2520only%2520outperforms%2520existing%2520state-of-the-art%250Amethods%2520in%2520terms%2520of%2520video%2520quality%252C%2520but%2520also%2520exhibits%2520strong%2520competitiveness%2520in%250Alip%2520synchronization%2520while%2520offering%2520improved%2520flexibility%2520in%2520controlling%2520emotion%250Aand%2520head%2520pose.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Playmate111/Playmate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07203v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Playmate%3A%20Flexible%20Control%20of%20Portrait%20Animation%20via%203D-Implicit%20Space%0A%20%20Guided%20Diffusion&entry.906535625=Xingpei%20Ma%20and%20Jiaran%20Cai%20and%20Yuansheng%20Guan%20and%20Shenneng%20Huang%20and%20Qiang%20Zhang%20and%20Shunsi%20Zhang&entry.1292438233=%20%20Recent%20diffusion-based%20talking%20face%20generation%20models%20have%20demonstrated%0Aimpressive%20potential%20in%20synthesizing%20videos%20that%20accurately%20match%20a%20speech%0Aaudio%20clip%20with%20a%20given%20reference%20identity.%20However%2C%20existing%20approaches%20still%0Aencounter%20significant%20challenges%20due%20to%20uncontrollable%20factors%2C%20such%20as%0Ainaccurate%20lip-sync%2C%20inappropriate%20head%20posture%20and%20the%20lack%20of%20fine-grained%0Acontrol%20over%20facial%20expressions.%20In%20order%20to%20introduce%20more%20face-guided%0Aconditions%20beyond%20speech%20audio%20clips%2C%20a%20novel%20two-stage%20training%20framework%0APlaymate%20is%20proposed%20to%20generate%20more%20lifelike%20facial%20expressions%20and%20talking%0Afaces.%20In%20the%20first%20stage%2C%20we%20introduce%20a%20decoupled%20implicit%203D%20representation%0Aalong%20with%20a%20meticulously%20designed%20motion-decoupled%20module%20to%20facilitate%20more%0Aaccurate%20attribute%20disentanglement%20and%20generate%20expressive%20talking%20videos%0Adirectly%20from%20audio%20cues.%20Then%2C%20in%20the%20second%20stage%2C%20we%20introduce%20an%0Aemotion-control%20module%20to%20encode%20emotion%20control%20information%20into%20the%20latent%0Aspace%2C%20enabling%20fine-grained%20control%20over%20emotions%20and%20thereby%20achieving%20the%0Aability%20to%20generate%20talking%20videos%20with%20desired%20emotion.%20Extensive%20experiments%0Ademonstrate%20that%20Playmate%20not%20only%20outperforms%20existing%20state-of-the-art%0Amethods%20in%20terms%20of%20video%20quality%2C%20but%20also%20exhibits%20strong%20competitiveness%20in%0Alip%20synchronization%20while%20offering%20improved%20flexibility%20in%20controlling%20emotion%0Aand%20head%20pose.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Playmate111/Playmate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07203v3&entry.124074799=Read"},
{"title": "Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining\n  vs. ImageNet Transfer Learning for Dermatological Diagnosis", "author": "Iv\u00e1n Matas and Carmen Serrano and Miguel Nogales and David Moreno and Lara Ferr\u00e1ndiz and Teresa Ojeda and Bego\u00f1a Acha", "abstract": "  Deep learning has transformed computer vision but relies heavily on large\nlabeled datasets and computational resources. Transfer learning, particularly\nfine-tuning pretrained models, offers a practical alternative; however, models\npretrained on natural image datasets such as ImageNet may fail to capture\ndomain-specific characteristics in medical imaging. This study introduces an\nunsupervised learning framework that extracts high-value dermatological\nfeatures instead of relying solely on ImageNet-based pretraining. We employ a\nVariational Autoencoder (VAE) trained from scratch on a proprietary\ndermatological dataset, allowing the model to learn a structured and clinically\nrelevant latent space. This self-supervised feature extractor is then compared\nto an ImageNet-pretrained backbone under identical classification conditions,\nhighlighting the trade-offs between general-purpose and domain-specific\npretraining. Our results reveal distinct learning patterns. The self-supervised\nmodel achieves a final validation loss of 0.110 (-33.33%), while the\nImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.\nAccuracy trends confirm this: the self-supervised model improves from 45% to\n65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained\nmodel reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting\ngap increasing to +0.060. These findings suggest that while ImageNet\npretraining accelerates convergence, it also amplifies overfitting on\nnon-clinically relevant features. In contrast, self-supervised learning\nachieves steady improvements, stronger generalization, and superior\nadaptability, underscoring the importance of domain-specific feature extraction\nin medical imaging.\n", "link": "http://arxiv.org/abs/2505.16773v1", "date": "2025-05-22", "relevancy": 2.5953, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.563}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4987}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Overfitting%20in%20Medical%20Imaging%3A%20Self-Supervised%20Pretraining%0A%20%20vs.%20ImageNet%20Transfer%20Learning%20for%20Dermatological%20Diagnosis&body=Title%3A%20Mitigating%20Overfitting%20in%20Medical%20Imaging%3A%20Self-Supervised%20Pretraining%0A%20%20vs.%20ImageNet%20Transfer%20Learning%20for%20Dermatological%20Diagnosis%0AAuthor%3A%20Iv%C3%A1n%20Matas%20and%20Carmen%20Serrano%20and%20Miguel%20Nogales%20and%20David%20Moreno%20and%20Lara%20Ferr%C3%A1ndiz%20and%20Teresa%20Ojeda%20and%20Bego%C3%B1a%20Acha%0AAbstract%3A%20%20%20Deep%20learning%20has%20transformed%20computer%20vision%20but%20relies%20heavily%20on%20large%0Alabeled%20datasets%20and%20computational%20resources.%20Transfer%20learning%2C%20particularly%0Afine-tuning%20pretrained%20models%2C%20offers%20a%20practical%20alternative%3B%20however%2C%20models%0Apretrained%20on%20natural%20image%20datasets%20such%20as%20ImageNet%20may%20fail%20to%20capture%0Adomain-specific%20characteristics%20in%20medical%20imaging.%20This%20study%20introduces%20an%0Aunsupervised%20learning%20framework%20that%20extracts%20high-value%20dermatological%0Afeatures%20instead%20of%20relying%20solely%20on%20ImageNet-based%20pretraining.%20We%20employ%20a%0AVariational%20Autoencoder%20%28VAE%29%20trained%20from%20scratch%20on%20a%20proprietary%0Adermatological%20dataset%2C%20allowing%20the%20model%20to%20learn%20a%20structured%20and%20clinically%0Arelevant%20latent%20space.%20This%20self-supervised%20feature%20extractor%20is%20then%20compared%0Ato%20an%20ImageNet-pretrained%20backbone%20under%20identical%20classification%20conditions%2C%0Ahighlighting%20the%20trade-offs%20between%20general-purpose%20and%20domain-specific%0Apretraining.%20Our%20results%20reveal%20distinct%20learning%20patterns.%20The%20self-supervised%0Amodel%20achieves%20a%20final%20validation%20loss%20of%200.110%20%28-33.33%25%29%2C%20while%20the%0AImageNet-pretrained%20model%20stagnates%20at%200.100%20%28-16.67%25%29%2C%20indicating%20overfitting.%0AAccuracy%20trends%20confirm%20this%3A%20the%20self-supervised%20model%20improves%20from%2045%25%20to%0A65%25%20%28%2B44.44%25%29%20with%20a%20near-zero%20overfitting%20gap%2C%20whereas%20the%20ImageNet-pretrained%0Amodel%20reaches%2087%25%20%28%2B50.00%25%29%20but%20plateaus%20at%2075%25%20%28%2B19.05%25%29%2C%20with%20its%20overfitting%0Agap%20increasing%20to%20%2B0.060.%20These%20findings%20suggest%20that%20while%20ImageNet%0Apretraining%20accelerates%20convergence%2C%20it%20also%20amplifies%20overfitting%20on%0Anon-clinically%20relevant%20features.%20In%20contrast%2C%20self-supervised%20learning%0Aachieves%20steady%20improvements%2C%20stronger%20generalization%2C%20and%20superior%0Aadaptability%2C%20underscoring%20the%20importance%20of%20domain-specific%20feature%20extraction%0Ain%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Overfitting%2520in%2520Medical%2520Imaging%253A%2520Self-Supervised%2520Pretraining%250A%2520%2520vs.%2520ImageNet%2520Transfer%2520Learning%2520for%2520Dermatological%2520Diagnosis%26entry.906535625%3DIv%25C3%25A1n%2520Matas%2520and%2520Carmen%2520Serrano%2520and%2520Miguel%2520Nogales%2520and%2520David%2520Moreno%2520and%2520Lara%2520Ferr%25C3%25A1ndiz%2520and%2520Teresa%2520Ojeda%2520and%2520Bego%25C3%25B1a%2520Acha%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520transformed%2520computer%2520vision%2520but%2520relies%2520heavily%2520on%2520large%250Alabeled%2520datasets%2520and%2520computational%2520resources.%2520Transfer%2520learning%252C%2520particularly%250Afine-tuning%2520pretrained%2520models%252C%2520offers%2520a%2520practical%2520alternative%253B%2520however%252C%2520models%250Apretrained%2520on%2520natural%2520image%2520datasets%2520such%2520as%2520ImageNet%2520may%2520fail%2520to%2520capture%250Adomain-specific%2520characteristics%2520in%2520medical%2520imaging.%2520This%2520study%2520introduces%2520an%250Aunsupervised%2520learning%2520framework%2520that%2520extracts%2520high-value%2520dermatological%250Afeatures%2520instead%2520of%2520relying%2520solely%2520on%2520ImageNet-based%2520pretraining.%2520We%2520employ%2520a%250AVariational%2520Autoencoder%2520%2528VAE%2529%2520trained%2520from%2520scratch%2520on%2520a%2520proprietary%250Adermatological%2520dataset%252C%2520allowing%2520the%2520model%2520to%2520learn%2520a%2520structured%2520and%2520clinically%250Arelevant%2520latent%2520space.%2520This%2520self-supervised%2520feature%2520extractor%2520is%2520then%2520compared%250Ato%2520an%2520ImageNet-pretrained%2520backbone%2520under%2520identical%2520classification%2520conditions%252C%250Ahighlighting%2520the%2520trade-offs%2520between%2520general-purpose%2520and%2520domain-specific%250Apretraining.%2520Our%2520results%2520reveal%2520distinct%2520learning%2520patterns.%2520The%2520self-supervised%250Amodel%2520achieves%2520a%2520final%2520validation%2520loss%2520of%25200.110%2520%2528-33.33%2525%2529%252C%2520while%2520the%250AImageNet-pretrained%2520model%2520stagnates%2520at%25200.100%2520%2528-16.67%2525%2529%252C%2520indicating%2520overfitting.%250AAccuracy%2520trends%2520confirm%2520this%253A%2520the%2520self-supervised%2520model%2520improves%2520from%252045%2525%2520to%250A65%2525%2520%2528%252B44.44%2525%2529%2520with%2520a%2520near-zero%2520overfitting%2520gap%252C%2520whereas%2520the%2520ImageNet-pretrained%250Amodel%2520reaches%252087%2525%2520%2528%252B50.00%2525%2529%2520but%2520plateaus%2520at%252075%2525%2520%2528%252B19.05%2525%2529%252C%2520with%2520its%2520overfitting%250Agap%2520increasing%2520to%2520%252B0.060.%2520These%2520findings%2520suggest%2520that%2520while%2520ImageNet%250Apretraining%2520accelerates%2520convergence%252C%2520it%2520also%2520amplifies%2520overfitting%2520on%250Anon-clinically%2520relevant%2520features.%2520In%2520contrast%252C%2520self-supervised%2520learning%250Aachieves%2520steady%2520improvements%252C%2520stronger%2520generalization%252C%2520and%2520superior%250Aadaptability%252C%2520underscoring%2520the%2520importance%2520of%2520domain-specific%2520feature%2520extraction%250Ain%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Overfitting%20in%20Medical%20Imaging%3A%20Self-Supervised%20Pretraining%0A%20%20vs.%20ImageNet%20Transfer%20Learning%20for%20Dermatological%20Diagnosis&entry.906535625=Iv%C3%A1n%20Matas%20and%20Carmen%20Serrano%20and%20Miguel%20Nogales%20and%20David%20Moreno%20and%20Lara%20Ferr%C3%A1ndiz%20and%20Teresa%20Ojeda%20and%20Bego%C3%B1a%20Acha&entry.1292438233=%20%20Deep%20learning%20has%20transformed%20computer%20vision%20but%20relies%20heavily%20on%20large%0Alabeled%20datasets%20and%20computational%20resources.%20Transfer%20learning%2C%20particularly%0Afine-tuning%20pretrained%20models%2C%20offers%20a%20practical%20alternative%3B%20however%2C%20models%0Apretrained%20on%20natural%20image%20datasets%20such%20as%20ImageNet%20may%20fail%20to%20capture%0Adomain-specific%20characteristics%20in%20medical%20imaging.%20This%20study%20introduces%20an%0Aunsupervised%20learning%20framework%20that%20extracts%20high-value%20dermatological%0Afeatures%20instead%20of%20relying%20solely%20on%20ImageNet-based%20pretraining.%20We%20employ%20a%0AVariational%20Autoencoder%20%28VAE%29%20trained%20from%20scratch%20on%20a%20proprietary%0Adermatological%20dataset%2C%20allowing%20the%20model%20to%20learn%20a%20structured%20and%20clinically%0Arelevant%20latent%20space.%20This%20self-supervised%20feature%20extractor%20is%20then%20compared%0Ato%20an%20ImageNet-pretrained%20backbone%20under%20identical%20classification%20conditions%2C%0Ahighlighting%20the%20trade-offs%20between%20general-purpose%20and%20domain-specific%0Apretraining.%20Our%20results%20reveal%20distinct%20learning%20patterns.%20The%20self-supervised%0Amodel%20achieves%20a%20final%20validation%20loss%20of%200.110%20%28-33.33%25%29%2C%20while%20the%0AImageNet-pretrained%20model%20stagnates%20at%200.100%20%28-16.67%25%29%2C%20indicating%20overfitting.%0AAccuracy%20trends%20confirm%20this%3A%20the%20self-supervised%20model%20improves%20from%2045%25%20to%0A65%25%20%28%2B44.44%25%29%20with%20a%20near-zero%20overfitting%20gap%2C%20whereas%20the%20ImageNet-pretrained%0Amodel%20reaches%2087%25%20%28%2B50.00%25%29%20but%20plateaus%20at%2075%25%20%28%2B19.05%25%29%2C%20with%20its%20overfitting%0Agap%20increasing%20to%20%2B0.060.%20These%20findings%20suggest%20that%20while%20ImageNet%0Apretraining%20accelerates%20convergence%2C%20it%20also%20amplifies%20overfitting%20on%0Anon-clinically%20relevant%20features.%20In%20contrast%2C%20self-supervised%20learning%0Aachieves%20steady%20improvements%2C%20stronger%20generalization%2C%20and%20superior%0Aadaptability%2C%20underscoring%20the%20importance%20of%20domain-specific%20feature%20extraction%0Ain%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16773v1&entry.124074799=Read"},
{"title": "Semantic Aware Linear Transfer by Recycling Pre-trained Language Models\n  for Cross-lingual Transfer", "author": "Seungyoon Lee and Seongtae Hong and Hyeonseok Moon and Heuiseok Lim", "abstract": "  Large Language Models (LLMs) increasingly incorporate multilingual\ncapabilities, fueling the demand to transfer them into target language-specific\nmodels. However, most approaches, which blend the source model's embedding by\nreplacing the source vocabulary with the target language-specific vocabulary,\nmay constrain expressive capacity in the target language since the source model\nis predominantly trained on English data. In this paper, we propose Semantic\nAware Linear Transfer (SALT), a novel cross-lingual transfer technique that\nrecycles embeddings from target language Pre-trained Language Models (PLMs) to\ntransmit the deep representational strengths of PLM-derived embedding to LLMs.\nSALT derives unique regression lines based on the similarity in the overlap of\nthe source and target vocabularies, to handle each non-overlapping token's\nembedding space. Our extensive experiments show that SALT significantly\noutperforms other transfer methods and achieves lower loss with accelerating\nfaster convergence during language adaptation. Notably, SALT obtains remarkable\nperformance in cross-lingual understanding setups compared to other methods.\nFurthermore, we highlight the scalable use of PLMs to enhance the functionality\nof contemporary LLMs by conducting experiments with varying architectures.\n", "link": "http://arxiv.org/abs/2505.10945v2", "date": "2025-05-22", "relevancy": 2.5918, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Aware%20Linear%20Transfer%20by%20Recycling%20Pre-trained%20Language%20Models%0A%20%20for%20Cross-lingual%20Transfer&body=Title%3A%20Semantic%20Aware%20Linear%20Transfer%20by%20Recycling%20Pre-trained%20Language%20Models%0A%20%20for%20Cross-lingual%20Transfer%0AAuthor%3A%20Seungyoon%20Lee%20and%20Seongtae%20Hong%20and%20Hyeonseok%20Moon%20and%20Heuiseok%20Lim%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20increasingly%20incorporate%20multilingual%0Acapabilities%2C%20fueling%20the%20demand%20to%20transfer%20them%20into%20target%20language-specific%0Amodels.%20However%2C%20most%20approaches%2C%20which%20blend%20the%20source%20model%27s%20embedding%20by%0Areplacing%20the%20source%20vocabulary%20with%20the%20target%20language-specific%20vocabulary%2C%0Amay%20constrain%20expressive%20capacity%20in%20the%20target%20language%20since%20the%20source%20model%0Ais%20predominantly%20trained%20on%20English%20data.%20In%20this%20paper%2C%20we%20propose%20Semantic%0AAware%20Linear%20Transfer%20%28SALT%29%2C%20a%20novel%20cross-lingual%20transfer%20technique%20that%0Arecycles%20embeddings%20from%20target%20language%20Pre-trained%20Language%20Models%20%28PLMs%29%20to%0Atransmit%20the%20deep%20representational%20strengths%20of%20PLM-derived%20embedding%20to%20LLMs.%0ASALT%20derives%20unique%20regression%20lines%20based%20on%20the%20similarity%20in%20the%20overlap%20of%0Athe%20source%20and%20target%20vocabularies%2C%20to%20handle%20each%20non-overlapping%20token%27s%0Aembedding%20space.%20Our%20extensive%20experiments%20show%20that%20SALT%20significantly%0Aoutperforms%20other%20transfer%20methods%20and%20achieves%20lower%20loss%20with%20accelerating%0Afaster%20convergence%20during%20language%20adaptation.%20Notably%2C%20SALT%20obtains%20remarkable%0Aperformance%20in%20cross-lingual%20understanding%20setups%20compared%20to%20other%20methods.%0AFurthermore%2C%20we%20highlight%20the%20scalable%20use%20of%20PLMs%20to%20enhance%20the%20functionality%0Aof%20contemporary%20LLMs%20by%20conducting%20experiments%20with%20varying%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Aware%2520Linear%2520Transfer%2520by%2520Recycling%2520Pre-trained%2520Language%2520Models%250A%2520%2520for%2520Cross-lingual%2520Transfer%26entry.906535625%3DSeungyoon%2520Lee%2520and%2520Seongtae%2520Hong%2520and%2520Hyeonseok%2520Moon%2520and%2520Heuiseok%2520Lim%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520increasingly%2520incorporate%2520multilingual%250Acapabilities%252C%2520fueling%2520the%2520demand%2520to%2520transfer%2520them%2520into%2520target%2520language-specific%250Amodels.%2520However%252C%2520most%2520approaches%252C%2520which%2520blend%2520the%2520source%2520model%2527s%2520embedding%2520by%250Areplacing%2520the%2520source%2520vocabulary%2520with%2520the%2520target%2520language-specific%2520vocabulary%252C%250Amay%2520constrain%2520expressive%2520capacity%2520in%2520the%2520target%2520language%2520since%2520the%2520source%2520model%250Ais%2520predominantly%2520trained%2520on%2520English%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Semantic%250AAware%2520Linear%2520Transfer%2520%2528SALT%2529%252C%2520a%2520novel%2520cross-lingual%2520transfer%2520technique%2520that%250Arecycles%2520embeddings%2520from%2520target%2520language%2520Pre-trained%2520Language%2520Models%2520%2528PLMs%2529%2520to%250Atransmit%2520the%2520deep%2520representational%2520strengths%2520of%2520PLM-derived%2520embedding%2520to%2520LLMs.%250ASALT%2520derives%2520unique%2520regression%2520lines%2520based%2520on%2520the%2520similarity%2520in%2520the%2520overlap%2520of%250Athe%2520source%2520and%2520target%2520vocabularies%252C%2520to%2520handle%2520each%2520non-overlapping%2520token%2527s%250Aembedding%2520space.%2520Our%2520extensive%2520experiments%2520show%2520that%2520SALT%2520significantly%250Aoutperforms%2520other%2520transfer%2520methods%2520and%2520achieves%2520lower%2520loss%2520with%2520accelerating%250Afaster%2520convergence%2520during%2520language%2520adaptation.%2520Notably%252C%2520SALT%2520obtains%2520remarkable%250Aperformance%2520in%2520cross-lingual%2520understanding%2520setups%2520compared%2520to%2520other%2520methods.%250AFurthermore%252C%2520we%2520highlight%2520the%2520scalable%2520use%2520of%2520PLMs%2520to%2520enhance%2520the%2520functionality%250Aof%2520contemporary%2520LLMs%2520by%2520conducting%2520experiments%2520with%2520varying%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Aware%20Linear%20Transfer%20by%20Recycling%20Pre-trained%20Language%20Models%0A%20%20for%20Cross-lingual%20Transfer&entry.906535625=Seungyoon%20Lee%20and%20Seongtae%20Hong%20and%20Hyeonseok%20Moon%20and%20Heuiseok%20Lim&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20increasingly%20incorporate%20multilingual%0Acapabilities%2C%20fueling%20the%20demand%20to%20transfer%20them%20into%20target%20language-specific%0Amodels.%20However%2C%20most%20approaches%2C%20which%20blend%20the%20source%20model%27s%20embedding%20by%0Areplacing%20the%20source%20vocabulary%20with%20the%20target%20language-specific%20vocabulary%2C%0Amay%20constrain%20expressive%20capacity%20in%20the%20target%20language%20since%20the%20source%20model%0Ais%20predominantly%20trained%20on%20English%20data.%20In%20this%20paper%2C%20we%20propose%20Semantic%0AAware%20Linear%20Transfer%20%28SALT%29%2C%20a%20novel%20cross-lingual%20transfer%20technique%20that%0Arecycles%20embeddings%20from%20target%20language%20Pre-trained%20Language%20Models%20%28PLMs%29%20to%0Atransmit%20the%20deep%20representational%20strengths%20of%20PLM-derived%20embedding%20to%20LLMs.%0ASALT%20derives%20unique%20regression%20lines%20based%20on%20the%20similarity%20in%20the%20overlap%20of%0Athe%20source%20and%20target%20vocabularies%2C%20to%20handle%20each%20non-overlapping%20token%27s%0Aembedding%20space.%20Our%20extensive%20experiments%20show%20that%20SALT%20significantly%0Aoutperforms%20other%20transfer%20methods%20and%20achieves%20lower%20loss%20with%20accelerating%0Afaster%20convergence%20during%20language%20adaptation.%20Notably%2C%20SALT%20obtains%20remarkable%0Aperformance%20in%20cross-lingual%20understanding%20setups%20compared%20to%20other%20methods.%0AFurthermore%2C%20we%20highlight%20the%20scalable%20use%20of%20PLMs%20to%20enhance%20the%20functionality%0Aof%20contemporary%20LLMs%20by%20conducting%20experiments%20with%20varying%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10945v2&entry.124074799=Read"},
{"title": "Motion by Queries: Identity-Motion Trade-offs in Text-to-Video\n  Generation", "author": "Yuval Atzmon and Rinon Gal and Yoad Tewel and Yoni Kasten and Gal Chechik", "abstract": "  Text-to-video diffusion models have shown remarkable progress in generating\ncoherent video clips from textual descriptions. However, the interplay between\nmotion, structure, and identity representations in these models remains\nunder-explored. Here, we investigate how self-attention query (Q) features\nsimultaneously govern motion, structure, and identity and examine the\nchallenges arising when these representations interact. Our analysis reveals\nthat Q affects not only layout, but that during denoising Q also has a strong\neffect on subject identity, making it hard to transfer motion without the\nside-effect of transferring identity. Understanding this dual role enabled us\nto control query feature injection (Q injection) and demonstrate two\napplications: (1) a zero-shot motion transfer method - implemented with\nVideoCrafter2 and WAN 2.1 - that is 10 times more efficient than existing\napproaches, and (2) a training-free technique for consistent multi-shot video\ngeneration, where characters maintain identity across multiple video shots\nwhile Q injection enhances motion fidelity.\n", "link": "http://arxiv.org/abs/2412.07750v3", "date": "2025-05-22", "relevancy": 2.5865, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6993}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.685}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20by%20Queries%3A%20Identity-Motion%20Trade-offs%20in%20Text-to-Video%0A%20%20Generation&body=Title%3A%20Motion%20by%20Queries%3A%20Identity-Motion%20Trade-offs%20in%20Text-to-Video%0A%20%20Generation%0AAuthor%3A%20Yuval%20Atzmon%20and%20Rinon%20Gal%20and%20Yoad%20Tewel%20and%20Yoni%20Kasten%20and%20Gal%20Chechik%0AAbstract%3A%20%20%20Text-to-video%20diffusion%20models%20have%20shown%20remarkable%20progress%20in%20generating%0Acoherent%20video%20clips%20from%20textual%20descriptions.%20However%2C%20the%20interplay%20between%0Amotion%2C%20structure%2C%20and%20identity%20representations%20in%20these%20models%20remains%0Aunder-explored.%20Here%2C%20we%20investigate%20how%20self-attention%20query%20%28Q%29%20features%0Asimultaneously%20govern%20motion%2C%20structure%2C%20and%20identity%20and%20examine%20the%0Achallenges%20arising%20when%20these%20representations%20interact.%20Our%20analysis%20reveals%0Athat%20Q%20affects%20not%20only%20layout%2C%20but%20that%20during%20denoising%20Q%20also%20has%20a%20strong%0Aeffect%20on%20subject%20identity%2C%20making%20it%20hard%20to%20transfer%20motion%20without%20the%0Aside-effect%20of%20transferring%20identity.%20Understanding%20this%20dual%20role%20enabled%20us%0Ato%20control%20query%20feature%20injection%20%28Q%20injection%29%20and%20demonstrate%20two%0Aapplications%3A%20%281%29%20a%20zero-shot%20motion%20transfer%20method%20-%20implemented%20with%0AVideoCrafter2%20and%20WAN%202.1%20-%20that%20is%2010%20times%20more%20efficient%20than%20existing%0Aapproaches%2C%20and%20%282%29%20a%20training-free%20technique%20for%20consistent%20multi-shot%20video%0Ageneration%2C%20where%20characters%20maintain%20identity%20across%20multiple%20video%20shots%0Awhile%20Q%20injection%20enhances%20motion%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07750v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520by%2520Queries%253A%2520Identity-Motion%2520Trade-offs%2520in%2520Text-to-Video%250A%2520%2520Generation%26entry.906535625%3DYuval%2520Atzmon%2520and%2520Rinon%2520Gal%2520and%2520Yoad%2520Tewel%2520and%2520Yoni%2520Kasten%2520and%2520Gal%2520Chechik%26entry.1292438233%3D%2520%2520Text-to-video%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520progress%2520in%2520generating%250Acoherent%2520video%2520clips%2520from%2520textual%2520descriptions.%2520However%252C%2520the%2520interplay%2520between%250Amotion%252C%2520structure%252C%2520and%2520identity%2520representations%2520in%2520these%2520models%2520remains%250Aunder-explored.%2520Here%252C%2520we%2520investigate%2520how%2520self-attention%2520query%2520%2528Q%2529%2520features%250Asimultaneously%2520govern%2520motion%252C%2520structure%252C%2520and%2520identity%2520and%2520examine%2520the%250Achallenges%2520arising%2520when%2520these%2520representations%2520interact.%2520Our%2520analysis%2520reveals%250Athat%2520Q%2520affects%2520not%2520only%2520layout%252C%2520but%2520that%2520during%2520denoising%2520Q%2520also%2520has%2520a%2520strong%250Aeffect%2520on%2520subject%2520identity%252C%2520making%2520it%2520hard%2520to%2520transfer%2520motion%2520without%2520the%250Aside-effect%2520of%2520transferring%2520identity.%2520Understanding%2520this%2520dual%2520role%2520enabled%2520us%250Ato%2520control%2520query%2520feature%2520injection%2520%2528Q%2520injection%2529%2520and%2520demonstrate%2520two%250Aapplications%253A%2520%25281%2529%2520a%2520zero-shot%2520motion%2520transfer%2520method%2520-%2520implemented%2520with%250AVideoCrafter2%2520and%2520WAN%25202.1%2520-%2520that%2520is%252010%2520times%2520more%2520efficient%2520than%2520existing%250Aapproaches%252C%2520and%2520%25282%2529%2520a%2520training-free%2520technique%2520for%2520consistent%2520multi-shot%2520video%250Ageneration%252C%2520where%2520characters%2520maintain%2520identity%2520across%2520multiple%2520video%2520shots%250Awhile%2520Q%2520injection%2520enhances%2520motion%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07750v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20by%20Queries%3A%20Identity-Motion%20Trade-offs%20in%20Text-to-Video%0A%20%20Generation&entry.906535625=Yuval%20Atzmon%20and%20Rinon%20Gal%20and%20Yoad%20Tewel%20and%20Yoni%20Kasten%20and%20Gal%20Chechik&entry.1292438233=%20%20Text-to-video%20diffusion%20models%20have%20shown%20remarkable%20progress%20in%20generating%0Acoherent%20video%20clips%20from%20textual%20descriptions.%20However%2C%20the%20interplay%20between%0Amotion%2C%20structure%2C%20and%20identity%20representations%20in%20these%20models%20remains%0Aunder-explored.%20Here%2C%20we%20investigate%20how%20self-attention%20query%20%28Q%29%20features%0Asimultaneously%20govern%20motion%2C%20structure%2C%20and%20identity%20and%20examine%20the%0Achallenges%20arising%20when%20these%20representations%20interact.%20Our%20analysis%20reveals%0Athat%20Q%20affects%20not%20only%20layout%2C%20but%20that%20during%20denoising%20Q%20also%20has%20a%20strong%0Aeffect%20on%20subject%20identity%2C%20making%20it%20hard%20to%20transfer%20motion%20without%20the%0Aside-effect%20of%20transferring%20identity.%20Understanding%20this%20dual%20role%20enabled%20us%0Ato%20control%20query%20feature%20injection%20%28Q%20injection%29%20and%20demonstrate%20two%0Aapplications%3A%20%281%29%20a%20zero-shot%20motion%20transfer%20method%20-%20implemented%20with%0AVideoCrafter2%20and%20WAN%202.1%20-%20that%20is%2010%20times%20more%20efficient%20than%20existing%0Aapproaches%2C%20and%20%282%29%20a%20training-free%20technique%20for%20consistent%20multi-shot%20video%0Ageneration%2C%20where%20characters%20maintain%20identity%20across%20multiple%20video%20shots%0Awhile%20Q%20injection%20enhances%20motion%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07750v3&entry.124074799=Read"},
{"title": "Zero-Shot Hyperspectral Pansharpening Using Hysteresis-Based Tuning for\n  Spectral Quality Control", "author": "Giuseppe Guarino and Matteo Ciotola and Gemine Vivone and Giovanni Poggi and Giuseppe Scarpa", "abstract": "  Hyperspectral pansharpening has received much attention in recent years due\nto technological and methodological advances that open the door to new\napplication scenarios. However, research on this topic is only now gaining\nmomentum. The most popular methods are still borrowed from the more mature\nfield of multispectral pansharpening and often overlook the unique challenges\nposed by hyperspectral data fusion, such as i) the very large number of bands,\nii) the overwhelming noise in selected spectral ranges, iii) the significant\nspectral mismatch between panchromatic and hyperspectral components, iv) a\ntypically high resolution ratio. Imprecise data modeling especially affects\nspectral fidelity. Even state-of-the-art methods perform well in certain\nspectral ranges and much worse in others, failing to ensure consistent quality\nacross all bands, with the risk of generating unreliable results. Here, we\npropose a hyperspectral pansharpening method that explicitly addresses this\nproblem and ensures uniform spectral quality. To this end, a single lightweight\nneural network is used, with weights that adapt on the fly to each band. During\nfine-tuning, the spatial loss is turned on and off to ensure a fast convergence\nof the spectral loss to the desired level, according to a hysteresis-like\ndynamic. Furthermore, the spatial loss itself is appropriately redefined to\naccount for nonlinear dependencies between panchromatic and spectral bands.\nOverall, the proposed method is fully unsupervised, with no prior training on\nexternal data, flexible, and low-complexity. Experiments on a recently\npublished benchmarking toolbox show that it ensures excellent sharpening\nquality, competitive with the state-of-the-art, consistently across all bands.\nThe software code and the full set of results are shared online on\nhttps://github.com/giu-guarino/rho-PNN.\n", "link": "http://arxiv.org/abs/2505.16658v1", "date": "2025-05-22", "relevancy": 2.5827, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5131}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Hyperspectral%20Pansharpening%20Using%20Hysteresis-Based%20Tuning%20for%0A%20%20Spectral%20Quality%20Control&body=Title%3A%20Zero-Shot%20Hyperspectral%20Pansharpening%20Using%20Hysteresis-Based%20Tuning%20for%0A%20%20Spectral%20Quality%20Control%0AAuthor%3A%20Giuseppe%20Guarino%20and%20Matteo%20Ciotola%20and%20Gemine%20Vivone%20and%20Giovanni%20Poggi%20and%20Giuseppe%20Scarpa%0AAbstract%3A%20%20%20Hyperspectral%20pansharpening%20has%20received%20much%20attention%20in%20recent%20years%20due%0Ato%20technological%20and%20methodological%20advances%20that%20open%20the%20door%20to%20new%0Aapplication%20scenarios.%20However%2C%20research%20on%20this%20topic%20is%20only%20now%20gaining%0Amomentum.%20The%20most%20popular%20methods%20are%20still%20borrowed%20from%20the%20more%20mature%0Afield%20of%20multispectral%20pansharpening%20and%20often%20overlook%20the%20unique%20challenges%0Aposed%20by%20hyperspectral%20data%20fusion%2C%20such%20as%20i%29%20the%20very%20large%20number%20of%20bands%2C%0Aii%29%20the%20overwhelming%20noise%20in%20selected%20spectral%20ranges%2C%20iii%29%20the%20significant%0Aspectral%20mismatch%20between%20panchromatic%20and%20hyperspectral%20components%2C%20iv%29%20a%0Atypically%20high%20resolution%20ratio.%20Imprecise%20data%20modeling%20especially%20affects%0Aspectral%20fidelity.%20Even%20state-of-the-art%20methods%20perform%20well%20in%20certain%0Aspectral%20ranges%20and%20much%20worse%20in%20others%2C%20failing%20to%20ensure%20consistent%20quality%0Aacross%20all%20bands%2C%20with%20the%20risk%20of%20generating%20unreliable%20results.%20Here%2C%20we%0Apropose%20a%20hyperspectral%20pansharpening%20method%20that%20explicitly%20addresses%20this%0Aproblem%20and%20ensures%20uniform%20spectral%20quality.%20To%20this%20end%2C%20a%20single%20lightweight%0Aneural%20network%20is%20used%2C%20with%20weights%20that%20adapt%20on%20the%20fly%20to%20each%20band.%20During%0Afine-tuning%2C%20the%20spatial%20loss%20is%20turned%20on%20and%20off%20to%20ensure%20a%20fast%20convergence%0Aof%20the%20spectral%20loss%20to%20the%20desired%20level%2C%20according%20to%20a%20hysteresis-like%0Adynamic.%20Furthermore%2C%20the%20spatial%20loss%20itself%20is%20appropriately%20redefined%20to%0Aaccount%20for%20nonlinear%20dependencies%20between%20panchromatic%20and%20spectral%20bands.%0AOverall%2C%20the%20proposed%20method%20is%20fully%20unsupervised%2C%20with%20no%20prior%20training%20on%0Aexternal%20data%2C%20flexible%2C%20and%20low-complexity.%20Experiments%20on%20a%20recently%0Apublished%20benchmarking%20toolbox%20show%20that%20it%20ensures%20excellent%20sharpening%0Aquality%2C%20competitive%20with%20the%20state-of-the-art%2C%20consistently%20across%20all%20bands.%0AThe%20software%20code%20and%20the%20full%20set%20of%20results%20are%20shared%20online%20on%0Ahttps%3A//github.com/giu-guarino/rho-PNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Hyperspectral%2520Pansharpening%2520Using%2520Hysteresis-Based%2520Tuning%2520for%250A%2520%2520Spectral%2520Quality%2520Control%26entry.906535625%3DGiuseppe%2520Guarino%2520and%2520Matteo%2520Ciotola%2520and%2520Gemine%2520Vivone%2520and%2520Giovanni%2520Poggi%2520and%2520Giuseppe%2520Scarpa%26entry.1292438233%3D%2520%2520Hyperspectral%2520pansharpening%2520has%2520received%2520much%2520attention%2520in%2520recent%2520years%2520due%250Ato%2520technological%2520and%2520methodological%2520advances%2520that%2520open%2520the%2520door%2520to%2520new%250Aapplication%2520scenarios.%2520However%252C%2520research%2520on%2520this%2520topic%2520is%2520only%2520now%2520gaining%250Amomentum.%2520The%2520most%2520popular%2520methods%2520are%2520still%2520borrowed%2520from%2520the%2520more%2520mature%250Afield%2520of%2520multispectral%2520pansharpening%2520and%2520often%2520overlook%2520the%2520unique%2520challenges%250Aposed%2520by%2520hyperspectral%2520data%2520fusion%252C%2520such%2520as%2520i%2529%2520the%2520very%2520large%2520number%2520of%2520bands%252C%250Aii%2529%2520the%2520overwhelming%2520noise%2520in%2520selected%2520spectral%2520ranges%252C%2520iii%2529%2520the%2520significant%250Aspectral%2520mismatch%2520between%2520panchromatic%2520and%2520hyperspectral%2520components%252C%2520iv%2529%2520a%250Atypically%2520high%2520resolution%2520ratio.%2520Imprecise%2520data%2520modeling%2520especially%2520affects%250Aspectral%2520fidelity.%2520Even%2520state-of-the-art%2520methods%2520perform%2520well%2520in%2520certain%250Aspectral%2520ranges%2520and%2520much%2520worse%2520in%2520others%252C%2520failing%2520to%2520ensure%2520consistent%2520quality%250Aacross%2520all%2520bands%252C%2520with%2520the%2520risk%2520of%2520generating%2520unreliable%2520results.%2520Here%252C%2520we%250Apropose%2520a%2520hyperspectral%2520pansharpening%2520method%2520that%2520explicitly%2520addresses%2520this%250Aproblem%2520and%2520ensures%2520uniform%2520spectral%2520quality.%2520To%2520this%2520end%252C%2520a%2520single%2520lightweight%250Aneural%2520network%2520is%2520used%252C%2520with%2520weights%2520that%2520adapt%2520on%2520the%2520fly%2520to%2520each%2520band.%2520During%250Afine-tuning%252C%2520the%2520spatial%2520loss%2520is%2520turned%2520on%2520and%2520off%2520to%2520ensure%2520a%2520fast%2520convergence%250Aof%2520the%2520spectral%2520loss%2520to%2520the%2520desired%2520level%252C%2520according%2520to%2520a%2520hysteresis-like%250Adynamic.%2520Furthermore%252C%2520the%2520spatial%2520loss%2520itself%2520is%2520appropriately%2520redefined%2520to%250Aaccount%2520for%2520nonlinear%2520dependencies%2520between%2520panchromatic%2520and%2520spectral%2520bands.%250AOverall%252C%2520the%2520proposed%2520method%2520is%2520fully%2520unsupervised%252C%2520with%2520no%2520prior%2520training%2520on%250Aexternal%2520data%252C%2520flexible%252C%2520and%2520low-complexity.%2520Experiments%2520on%2520a%2520recently%250Apublished%2520benchmarking%2520toolbox%2520show%2520that%2520it%2520ensures%2520excellent%2520sharpening%250Aquality%252C%2520competitive%2520with%2520the%2520state-of-the-art%252C%2520consistently%2520across%2520all%2520bands.%250AThe%2520software%2520code%2520and%2520the%2520full%2520set%2520of%2520results%2520are%2520shared%2520online%2520on%250Ahttps%253A//github.com/giu-guarino/rho-PNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Hyperspectral%20Pansharpening%20Using%20Hysteresis-Based%20Tuning%20for%0A%20%20Spectral%20Quality%20Control&entry.906535625=Giuseppe%20Guarino%20and%20Matteo%20Ciotola%20and%20Gemine%20Vivone%20and%20Giovanni%20Poggi%20and%20Giuseppe%20Scarpa&entry.1292438233=%20%20Hyperspectral%20pansharpening%20has%20received%20much%20attention%20in%20recent%20years%20due%0Ato%20technological%20and%20methodological%20advances%20that%20open%20the%20door%20to%20new%0Aapplication%20scenarios.%20However%2C%20research%20on%20this%20topic%20is%20only%20now%20gaining%0Amomentum.%20The%20most%20popular%20methods%20are%20still%20borrowed%20from%20the%20more%20mature%0Afield%20of%20multispectral%20pansharpening%20and%20often%20overlook%20the%20unique%20challenges%0Aposed%20by%20hyperspectral%20data%20fusion%2C%20such%20as%20i%29%20the%20very%20large%20number%20of%20bands%2C%0Aii%29%20the%20overwhelming%20noise%20in%20selected%20spectral%20ranges%2C%20iii%29%20the%20significant%0Aspectral%20mismatch%20between%20panchromatic%20and%20hyperspectral%20components%2C%20iv%29%20a%0Atypically%20high%20resolution%20ratio.%20Imprecise%20data%20modeling%20especially%20affects%0Aspectral%20fidelity.%20Even%20state-of-the-art%20methods%20perform%20well%20in%20certain%0Aspectral%20ranges%20and%20much%20worse%20in%20others%2C%20failing%20to%20ensure%20consistent%20quality%0Aacross%20all%20bands%2C%20with%20the%20risk%20of%20generating%20unreliable%20results.%20Here%2C%20we%0Apropose%20a%20hyperspectral%20pansharpening%20method%20that%20explicitly%20addresses%20this%0Aproblem%20and%20ensures%20uniform%20spectral%20quality.%20To%20this%20end%2C%20a%20single%20lightweight%0Aneural%20network%20is%20used%2C%20with%20weights%20that%20adapt%20on%20the%20fly%20to%20each%20band.%20During%0Afine-tuning%2C%20the%20spatial%20loss%20is%20turned%20on%20and%20off%20to%20ensure%20a%20fast%20convergence%0Aof%20the%20spectral%20loss%20to%20the%20desired%20level%2C%20according%20to%20a%20hysteresis-like%0Adynamic.%20Furthermore%2C%20the%20spatial%20loss%20itself%20is%20appropriately%20redefined%20to%0Aaccount%20for%20nonlinear%20dependencies%20between%20panchromatic%20and%20spectral%20bands.%0AOverall%2C%20the%20proposed%20method%20is%20fully%20unsupervised%2C%20with%20no%20prior%20training%20on%0Aexternal%20data%2C%20flexible%2C%20and%20low-complexity.%20Experiments%20on%20a%20recently%0Apublished%20benchmarking%20toolbox%20show%20that%20it%20ensures%20excellent%20sharpening%0Aquality%2C%20competitive%20with%20the%20state-of-the-art%2C%20consistently%20across%20all%20bands.%0AThe%20software%20code%20and%20the%20full%20set%20of%20results%20are%20shared%20online%20on%0Ahttps%3A//github.com/giu-guarino/rho-PNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16658v1&entry.124074799=Read"},
{"title": "Unsupervised Network Anomaly Detection with Autoencoders and Traffic\n  Images", "author": "Michael Neri and Sara Baldoni", "abstract": "  Due to the recent increase in the number of connected devices, the need to\npromptly detect security issues is emerging. Moreover, the high number of\ncommunication flows creates the necessity of processing huge amounts of data.\nFurthermore, the connected devices are heterogeneous in nature, having\ndifferent computational capacities. For this reason, in this work we propose an\nimage-based representation of network traffic which allows to realize a compact\nsummary of the current network conditions with 1-second time windows. The\nproposed representation highlights the presence of anomalies thus reducing the\nneed for complex processing architectures. Finally, we present an unsupervised\nlearning approach which effectively detects the presence of anomalies. The code\nand the dataset are available at\nhttps://github.com/michaelneri/image-based-network-traffic-anomaly-detection.\n", "link": "http://arxiv.org/abs/2505.16650v1", "date": "2025-05-22", "relevancy": 2.5649, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5445}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5042}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Network%20Anomaly%20Detection%20with%20Autoencoders%20and%20Traffic%0A%20%20Images&body=Title%3A%20Unsupervised%20Network%20Anomaly%20Detection%20with%20Autoencoders%20and%20Traffic%0A%20%20Images%0AAuthor%3A%20Michael%20Neri%20and%20Sara%20Baldoni%0AAbstract%3A%20%20%20Due%20to%20the%20recent%20increase%20in%20the%20number%20of%20connected%20devices%2C%20the%20need%20to%0Apromptly%20detect%20security%20issues%20is%20emerging.%20Moreover%2C%20the%20high%20number%20of%0Acommunication%20flows%20creates%20the%20necessity%20of%20processing%20huge%20amounts%20of%20data.%0AFurthermore%2C%20the%20connected%20devices%20are%20heterogeneous%20in%20nature%2C%20having%0Adifferent%20computational%20capacities.%20For%20this%20reason%2C%20in%20this%20work%20we%20propose%20an%0Aimage-based%20representation%20of%20network%20traffic%20which%20allows%20to%20realize%20a%20compact%0Asummary%20of%20the%20current%20network%20conditions%20with%201-second%20time%20windows.%20The%0Aproposed%20representation%20highlights%20the%20presence%20of%20anomalies%20thus%20reducing%20the%0Aneed%20for%20complex%20processing%20architectures.%20Finally%2C%20we%20present%20an%20unsupervised%0Alearning%20approach%20which%20effectively%20detects%20the%20presence%20of%20anomalies.%20The%20code%0Aand%20the%20dataset%20are%20available%20at%0Ahttps%3A//github.com/michaelneri/image-based-network-traffic-anomaly-detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Network%2520Anomaly%2520Detection%2520with%2520Autoencoders%2520and%2520Traffic%250A%2520%2520Images%26entry.906535625%3DMichael%2520Neri%2520and%2520Sara%2520Baldoni%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520recent%2520increase%2520in%2520the%2520number%2520of%2520connected%2520devices%252C%2520the%2520need%2520to%250Apromptly%2520detect%2520security%2520issues%2520is%2520emerging.%2520Moreover%252C%2520the%2520high%2520number%2520of%250Acommunication%2520flows%2520creates%2520the%2520necessity%2520of%2520processing%2520huge%2520amounts%2520of%2520data.%250AFurthermore%252C%2520the%2520connected%2520devices%2520are%2520heterogeneous%2520in%2520nature%252C%2520having%250Adifferent%2520computational%2520capacities.%2520For%2520this%2520reason%252C%2520in%2520this%2520work%2520we%2520propose%2520an%250Aimage-based%2520representation%2520of%2520network%2520traffic%2520which%2520allows%2520to%2520realize%2520a%2520compact%250Asummary%2520of%2520the%2520current%2520network%2520conditions%2520with%25201-second%2520time%2520windows.%2520The%250Aproposed%2520representation%2520highlights%2520the%2520presence%2520of%2520anomalies%2520thus%2520reducing%2520the%250Aneed%2520for%2520complex%2520processing%2520architectures.%2520Finally%252C%2520we%2520present%2520an%2520unsupervised%250Alearning%2520approach%2520which%2520effectively%2520detects%2520the%2520presence%2520of%2520anomalies.%2520The%2520code%250Aand%2520the%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/michaelneri/image-based-network-traffic-anomaly-detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Network%20Anomaly%20Detection%20with%20Autoencoders%20and%20Traffic%0A%20%20Images&entry.906535625=Michael%20Neri%20and%20Sara%20Baldoni&entry.1292438233=%20%20Due%20to%20the%20recent%20increase%20in%20the%20number%20of%20connected%20devices%2C%20the%20need%20to%0Apromptly%20detect%20security%20issues%20is%20emerging.%20Moreover%2C%20the%20high%20number%20of%0Acommunication%20flows%20creates%20the%20necessity%20of%20processing%20huge%20amounts%20of%20data.%0AFurthermore%2C%20the%20connected%20devices%20are%20heterogeneous%20in%20nature%2C%20having%0Adifferent%20computational%20capacities.%20For%20this%20reason%2C%20in%20this%20work%20we%20propose%20an%0Aimage-based%20representation%20of%20network%20traffic%20which%20allows%20to%20realize%20a%20compact%0Asummary%20of%20the%20current%20network%20conditions%20with%201-second%20time%20windows.%20The%0Aproposed%20representation%20highlights%20the%20presence%20of%20anomalies%20thus%20reducing%20the%0Aneed%20for%20complex%20processing%20architectures.%20Finally%2C%20we%20present%20an%20unsupervised%0Alearning%20approach%20which%20effectively%20detects%20the%20presence%20of%20anomalies.%20The%20code%0Aand%20the%20dataset%20are%20available%20at%0Ahttps%3A//github.com/michaelneri/image-based-network-traffic-anomaly-detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16650v1&entry.124074799=Read"},
{"title": "Latent Principle Discovery for Language Model Self-Improvement", "author": "Keshav Ramji and Tahira Naseem and Ram\u00f3n Fernandez Astudillo", "abstract": "  When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.\n", "link": "http://arxiv.org/abs/2505.16927v1", "date": "2025-05-22", "relevancy": 2.5594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Principle%20Discovery%20for%20Language%20Model%20Self-Improvement&body=Title%3A%20Latent%20Principle%20Discovery%20for%20Language%20Model%20Self-Improvement%0AAuthor%3A%20Keshav%20Ramji%20and%20Tahira%20Naseem%20and%20Ram%C3%B3n%20Fernandez%20Astudillo%0AAbstract%3A%20%20%20When%20language%20model%20%28LM%29%20users%20aim%20to%20improve%20the%20quality%20of%20its%20generations%2C%0Ait%20is%20crucial%20to%20specify%20concrete%20behavioral%20attributes%20that%20the%20model%20should%0Astrive%20to%20reflect.%20However%2C%20curating%20such%20principles%20across%20many%20domains%2C%20even%0Anon-exhaustively%2C%20requires%20a%20labor-intensive%20annotation%20process.%20To%20automate%0Athis%20process%2C%20we%20propose%20eliciting%20these%20latent%20attributes%20guiding%20model%0Areasoning%20towards%20human-preferred%20responses%20by%20explicitly%20modeling%20them%20in%20a%0Aself-correction%20setting.%20Our%20approach%20mines%20new%20principles%20from%20the%20LM%20itself%0Aand%20compresses%20the%20discovered%20elements%20to%20an%20interpretable%20set%20via%20clustering.%0ASpecifically%2C%20we%20employ%20an%20approximation%20of%20posterior-regularized%20Monte%20Carlo%0AExpectation-Maximization%20to%20both%20identify%20a%20condensed%20set%20of%20the%20most%20effective%0Alatent%20principles%20and%20teach%20the%20LM%20to%20strategically%20invoke%20them%20in%20order%20to%0Aintrinsically%20refine%20its%20responses.%20We%20demonstrate%20that%20bootstrapping%20our%0Aalgorithm%20over%20multiple%20iterations%20enables%20smaller%20language%20models%20%287-8B%0Aparameters%29%20to%20self-improve%2C%20achieving%20%2B8-10%25%20in%20AlpacaEval%20win-rate%2C%20an%0Aaverage%20of%20%2B0.3%20on%20MT-Bench%2C%20and%20%2B19-23%25%20in%20principle-following%20win-rate%20on%0AIFEval.%20We%20also%20show%20that%20clustering%20the%20principles%20yields%20interpretable%20and%0Adiverse%20model-generated%20constitutions%20while%20retaining%20model%20performance.%20The%0Agains%20our%20method%20achieves%20highlight%20the%20potential%20of%20automated%2C%0Aprinciple-driven%20post-training%20recipes%20toward%20continual%20self-improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Principle%2520Discovery%2520for%2520Language%2520Model%2520Self-Improvement%26entry.906535625%3DKeshav%2520Ramji%2520and%2520Tahira%2520Naseem%2520and%2520Ram%25C3%25B3n%2520Fernandez%2520Astudillo%26entry.1292438233%3D%2520%2520When%2520language%2520model%2520%2528LM%2529%2520users%2520aim%2520to%2520improve%2520the%2520quality%2520of%2520its%2520generations%252C%250Ait%2520is%2520crucial%2520to%2520specify%2520concrete%2520behavioral%2520attributes%2520that%2520the%2520model%2520should%250Astrive%2520to%2520reflect.%2520However%252C%2520curating%2520such%2520principles%2520across%2520many%2520domains%252C%2520even%250Anon-exhaustively%252C%2520requires%2520a%2520labor-intensive%2520annotation%2520process.%2520To%2520automate%250Athis%2520process%252C%2520we%2520propose%2520eliciting%2520these%2520latent%2520attributes%2520guiding%2520model%250Areasoning%2520towards%2520human-preferred%2520responses%2520by%2520explicitly%2520modeling%2520them%2520in%2520a%250Aself-correction%2520setting.%2520Our%2520approach%2520mines%2520new%2520principles%2520from%2520the%2520LM%2520itself%250Aand%2520compresses%2520the%2520discovered%2520elements%2520to%2520an%2520interpretable%2520set%2520via%2520clustering.%250ASpecifically%252C%2520we%2520employ%2520an%2520approximation%2520of%2520posterior-regularized%2520Monte%2520Carlo%250AExpectation-Maximization%2520to%2520both%2520identify%2520a%2520condensed%2520set%2520of%2520the%2520most%2520effective%250Alatent%2520principles%2520and%2520teach%2520the%2520LM%2520to%2520strategically%2520invoke%2520them%2520in%2520order%2520to%250Aintrinsically%2520refine%2520its%2520responses.%2520We%2520demonstrate%2520that%2520bootstrapping%2520our%250Aalgorithm%2520over%2520multiple%2520iterations%2520enables%2520smaller%2520language%2520models%2520%25287-8B%250Aparameters%2529%2520to%2520self-improve%252C%2520achieving%2520%252B8-10%2525%2520in%2520AlpacaEval%2520win-rate%252C%2520an%250Aaverage%2520of%2520%252B0.3%2520on%2520MT-Bench%252C%2520and%2520%252B19-23%2525%2520in%2520principle-following%2520win-rate%2520on%250AIFEval.%2520We%2520also%2520show%2520that%2520clustering%2520the%2520principles%2520yields%2520interpretable%2520and%250Adiverse%2520model-generated%2520constitutions%2520while%2520retaining%2520model%2520performance.%2520The%250Agains%2520our%2520method%2520achieves%2520highlight%2520the%2520potential%2520of%2520automated%252C%250Aprinciple-driven%2520post-training%2520recipes%2520toward%2520continual%2520self-improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Principle%20Discovery%20for%20Language%20Model%20Self-Improvement&entry.906535625=Keshav%20Ramji%20and%20Tahira%20Naseem%20and%20Ram%C3%B3n%20Fernandez%20Astudillo&entry.1292438233=%20%20When%20language%20model%20%28LM%29%20users%20aim%20to%20improve%20the%20quality%20of%20its%20generations%2C%0Ait%20is%20crucial%20to%20specify%20concrete%20behavioral%20attributes%20that%20the%20model%20should%0Astrive%20to%20reflect.%20However%2C%20curating%20such%20principles%20across%20many%20domains%2C%20even%0Anon-exhaustively%2C%20requires%20a%20labor-intensive%20annotation%20process.%20To%20automate%0Athis%20process%2C%20we%20propose%20eliciting%20these%20latent%20attributes%20guiding%20model%0Areasoning%20towards%20human-preferred%20responses%20by%20explicitly%20modeling%20them%20in%20a%0Aself-correction%20setting.%20Our%20approach%20mines%20new%20principles%20from%20the%20LM%20itself%0Aand%20compresses%20the%20discovered%20elements%20to%20an%20interpretable%20set%20via%20clustering.%0ASpecifically%2C%20we%20employ%20an%20approximation%20of%20posterior-regularized%20Monte%20Carlo%0AExpectation-Maximization%20to%20both%20identify%20a%20condensed%20set%20of%20the%20most%20effective%0Alatent%20principles%20and%20teach%20the%20LM%20to%20strategically%20invoke%20them%20in%20order%20to%0Aintrinsically%20refine%20its%20responses.%20We%20demonstrate%20that%20bootstrapping%20our%0Aalgorithm%20over%20multiple%20iterations%20enables%20smaller%20language%20models%20%287-8B%0Aparameters%29%20to%20self-improve%2C%20achieving%20%2B8-10%25%20in%20AlpacaEval%20win-rate%2C%20an%0Aaverage%20of%20%2B0.3%20on%20MT-Bench%2C%20and%20%2B19-23%25%20in%20principle-following%20win-rate%20on%0AIFEval.%20We%20also%20show%20that%20clustering%20the%20principles%20yields%20interpretable%20and%0Adiverse%20model-generated%20constitutions%20while%20retaining%20model%20performance.%20The%0Agains%20our%20method%20achieves%20highlight%20the%20potential%20of%20automated%2C%0Aprinciple-driven%20post-training%20recipes%20toward%20continual%20self-improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16927v1&entry.124074799=Read"},
{"title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative\n  Metric-driven Pruning", "author": "Florentin Beck and William Rudman and Carsten Eickhoff", "abstract": "  Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM\n", "link": "http://arxiv.org/abs/2505.16743v1", "date": "2025-05-22", "relevancy": 2.5465, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5388}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4983}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRIM%3A%20Achieving%20Extreme%20Sparsity%20with%20Targeted%20Row-wise%20Iterative%0A%20%20Metric-driven%20Pruning&body=Title%3A%20TRIM%3A%20Achieving%20Extreme%20Sparsity%20with%20Targeted%20Row-wise%20Iterative%0A%20%20Metric-driven%20Pruning%0AAuthor%3A%20Florentin%20Beck%20and%20William%20Rudman%20and%20Carsten%20Eickhoff%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20present%20significant%20computational%20and%20memory%0Achallenges%20due%20to%20their%20extensive%20size%2C%20making%20pruning%20essential%20for%20their%0Aefficient%20deployment.%20Existing%20one-shot%20pruning%20methods%20often%20apply%20uniform%0Asparsity%20constraints%20across%20layers%20or%20within%20each%20layer%2C%20resulting%20in%0Asuboptimal%20performance%2C%20especially%20at%20high%20sparsity%20ratios.%20This%20work%0Aintroduces%20TRIM%20%28Targeted%20Row-wise%20Iterative%20Metric-driven%20pruning%29%2C%20a%20novel%0Aapproach%20that%20applies%20varying%20sparsity%20ratios%20to%20individual%20output%20dimensions%0A%28rows%29%20within%20each%20layer.%20TRIM%20employs%20an%20iterative%20adjustment%20process%20guided%0Aby%20quality%20metrics%20to%20optimize%20dimension-wise%20sparsity%20allocation%2C%20focusing%20on%0Areducing%20variance%20in%20quality%20retention%20across%20outputs%20to%20preserve%20critical%0Ainformation.%20TRIM%20can%20be%20seamlessly%20integrated%20with%20existing%20layer-wise%20pruning%0Astrategies.%20Our%20evaluations%20on%20perplexity%20and%20zero-shot%20tasks%20across%20diverse%0ALLM%20families%20%28Qwen2.5%2C%20LLaMA-2%2C%20and%20OPT%29%20and%20sparsity%20levels%20demonstrate%20that%0ATRIM%20achieves%20new%20state-of-the-art%20results%20and%20enhances%20stability.%20For%0Ainstance%2C%20at%2080%25%20sparsity%2C%20TRIM%20reduces%20perplexity%20by%2048%25%20for%20Qwen2.5-14B%20and%0Aover%2090%25%20for%20OPT-13B%20compared%20to%20baseline%20methods.%20We%20conclude%20that%0Afine-grained%2C%20dimension-wise%20sparsity%20adaptation%20is%20crucial%20for%20pushing%20the%0Alimits%20of%20extreme%20LLM%20compression.%20Code%20available%20at%3A%0Ahttps%3A//github.com/flobk/TRIM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRIM%253A%2520Achieving%2520Extreme%2520Sparsity%2520with%2520Targeted%2520Row-wise%2520Iterative%250A%2520%2520Metric-driven%2520Pruning%26entry.906535625%3DFlorentin%2520Beck%2520and%2520William%2520Rudman%2520and%2520Carsten%2520Eickhoff%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520present%2520significant%2520computational%2520and%2520memory%250Achallenges%2520due%2520to%2520their%2520extensive%2520size%252C%2520making%2520pruning%2520essential%2520for%2520their%250Aefficient%2520deployment.%2520Existing%2520one-shot%2520pruning%2520methods%2520often%2520apply%2520uniform%250Asparsity%2520constraints%2520across%2520layers%2520or%2520within%2520each%2520layer%252C%2520resulting%2520in%250Asuboptimal%2520performance%252C%2520especially%2520at%2520high%2520sparsity%2520ratios.%2520This%2520work%250Aintroduces%2520TRIM%2520%2528Targeted%2520Row-wise%2520Iterative%2520Metric-driven%2520pruning%2529%252C%2520a%2520novel%250Aapproach%2520that%2520applies%2520varying%2520sparsity%2520ratios%2520to%2520individual%2520output%2520dimensions%250A%2528rows%2529%2520within%2520each%2520layer.%2520TRIM%2520employs%2520an%2520iterative%2520adjustment%2520process%2520guided%250Aby%2520quality%2520metrics%2520to%2520optimize%2520dimension-wise%2520sparsity%2520allocation%252C%2520focusing%2520on%250Areducing%2520variance%2520in%2520quality%2520retention%2520across%2520outputs%2520to%2520preserve%2520critical%250Ainformation.%2520TRIM%2520can%2520be%2520seamlessly%2520integrated%2520with%2520existing%2520layer-wise%2520pruning%250Astrategies.%2520Our%2520evaluations%2520on%2520perplexity%2520and%2520zero-shot%2520tasks%2520across%2520diverse%250ALLM%2520families%2520%2528Qwen2.5%252C%2520LLaMA-2%252C%2520and%2520OPT%2529%2520and%2520sparsity%2520levels%2520demonstrate%2520that%250ATRIM%2520achieves%2520new%2520state-of-the-art%2520results%2520and%2520enhances%2520stability.%2520For%250Ainstance%252C%2520at%252080%2525%2520sparsity%252C%2520TRIM%2520reduces%2520perplexity%2520by%252048%2525%2520for%2520Qwen2.5-14B%2520and%250Aover%252090%2525%2520for%2520OPT-13B%2520compared%2520to%2520baseline%2520methods.%2520We%2520conclude%2520that%250Afine-grained%252C%2520dimension-wise%2520sparsity%2520adaptation%2520is%2520crucial%2520for%2520pushing%2520the%250Alimits%2520of%2520extreme%2520LLM%2520compression.%2520Code%2520available%2520at%253A%250Ahttps%253A//github.com/flobk/TRIM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRIM%3A%20Achieving%20Extreme%20Sparsity%20with%20Targeted%20Row-wise%20Iterative%0A%20%20Metric-driven%20Pruning&entry.906535625=Florentin%20Beck%20and%20William%20Rudman%20and%20Carsten%20Eickhoff&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20present%20significant%20computational%20and%20memory%0Achallenges%20due%20to%20their%20extensive%20size%2C%20making%20pruning%20essential%20for%20their%0Aefficient%20deployment.%20Existing%20one-shot%20pruning%20methods%20often%20apply%20uniform%0Asparsity%20constraints%20across%20layers%20or%20within%20each%20layer%2C%20resulting%20in%0Asuboptimal%20performance%2C%20especially%20at%20high%20sparsity%20ratios.%20This%20work%0Aintroduces%20TRIM%20%28Targeted%20Row-wise%20Iterative%20Metric-driven%20pruning%29%2C%20a%20novel%0Aapproach%20that%20applies%20varying%20sparsity%20ratios%20to%20individual%20output%20dimensions%0A%28rows%29%20within%20each%20layer.%20TRIM%20employs%20an%20iterative%20adjustment%20process%20guided%0Aby%20quality%20metrics%20to%20optimize%20dimension-wise%20sparsity%20allocation%2C%20focusing%20on%0Areducing%20variance%20in%20quality%20retention%20across%20outputs%20to%20preserve%20critical%0Ainformation.%20TRIM%20can%20be%20seamlessly%20integrated%20with%20existing%20layer-wise%20pruning%0Astrategies.%20Our%20evaluations%20on%20perplexity%20and%20zero-shot%20tasks%20across%20diverse%0ALLM%20families%20%28Qwen2.5%2C%20LLaMA-2%2C%20and%20OPT%29%20and%20sparsity%20levels%20demonstrate%20that%0ATRIM%20achieves%20new%20state-of-the-art%20results%20and%20enhances%20stability.%20For%0Ainstance%2C%20at%2080%25%20sparsity%2C%20TRIM%20reduces%20perplexity%20by%2048%25%20for%20Qwen2.5-14B%20and%0Aover%2090%25%20for%20OPT-13B%20compared%20to%20baseline%20methods.%20We%20conclude%20that%0Afine-grained%2C%20dimension-wise%20sparsity%20adaptation%20is%20crucial%20for%20pushing%20the%0Alimits%20of%20extreme%20LLM%20compression.%20Code%20available%20at%3A%0Ahttps%3A//github.com/flobk/TRIM%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16743v1&entry.124074799=Read"},
{"title": "Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable\n  Structured Latent Modelling", "author": "Xinxing Shi and Xiaoyu Jiang and Mauricio A. \u00c1lvarez", "abstract": "  Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by\nreplacing the fully factorised Gaussian prior with a GP prior, thereby\ncapturing richer correlations among latent variables. However, performing exact\nGP inference in large-scale GPVAEs is computationally prohibitive, often\nforcing existing approaches to rely on restrictive kernel assumptions or large\nsets of inducing points. In this work, we propose a neighbour-driven\napproximation strategy that exploits local adjacencies in the latent space to\nachieve scalable GPVAE inference. By confining computations to the nearest\nneighbours of each data point, our method preserves essential latent\ndependencies, allowing more flexible kernel choices and mitigating the need for\nnumerous inducing points. Through extensive experiments on tasks including\nrepresentation learning, data imputation, and conditional generation, we\ndemonstrate that our approach outperforms other GPVAE variants in both\npredictive performance and computational efficiency.\n", "link": "http://arxiv.org/abs/2505.16481v1", "date": "2025-05-22", "relevancy": 2.5464, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5169}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5095}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neighbour-Driven%20Gaussian%20Process%20Variational%20Autoencoders%20for%20Scalable%0A%20%20Structured%20Latent%20Modelling&body=Title%3A%20Neighbour-Driven%20Gaussian%20Process%20Variational%20Autoencoders%20for%20Scalable%0A%20%20Structured%20Latent%20Modelling%0AAuthor%3A%20Xinxing%20Shi%20and%20Xiaoyu%20Jiang%20and%20Mauricio%20A.%20%C3%81lvarez%0AAbstract%3A%20%20%20Gaussian%20Process%20%28GP%29%20Variational%20Autoencoders%20%28VAEs%29%20extend%20standard%20VAEs%20by%0Areplacing%20the%20fully%20factorised%20Gaussian%20prior%20with%20a%20GP%20prior%2C%20thereby%0Acapturing%20richer%20correlations%20among%20latent%20variables.%20However%2C%20performing%20exact%0AGP%20inference%20in%20large-scale%20GPVAEs%20is%20computationally%20prohibitive%2C%20often%0Aforcing%20existing%20approaches%20to%20rely%20on%20restrictive%20kernel%20assumptions%20or%20large%0Asets%20of%20inducing%20points.%20In%20this%20work%2C%20we%20propose%20a%20neighbour-driven%0Aapproximation%20strategy%20that%20exploits%20local%20adjacencies%20in%20the%20latent%20space%20to%0Aachieve%20scalable%20GPVAE%20inference.%20By%20confining%20computations%20to%20the%20nearest%0Aneighbours%20of%20each%20data%20point%2C%20our%20method%20preserves%20essential%20latent%0Adependencies%2C%20allowing%20more%20flexible%20kernel%20choices%20and%20mitigating%20the%20need%20for%0Anumerous%20inducing%20points.%20Through%20extensive%20experiments%20on%20tasks%20including%0Arepresentation%20learning%2C%20data%20imputation%2C%20and%20conditional%20generation%2C%20we%0Ademonstrate%20that%20our%20approach%20outperforms%20other%20GPVAE%20variants%20in%20both%0Apredictive%20performance%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeighbour-Driven%2520Gaussian%2520Process%2520Variational%2520Autoencoders%2520for%2520Scalable%250A%2520%2520Structured%2520Latent%2520Modelling%26entry.906535625%3DXinxing%2520Shi%2520and%2520Xiaoyu%2520Jiang%2520and%2520Mauricio%2520A.%2520%25C3%2581lvarez%26entry.1292438233%3D%2520%2520Gaussian%2520Process%2520%2528GP%2529%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520extend%2520standard%2520VAEs%2520by%250Areplacing%2520the%2520fully%2520factorised%2520Gaussian%2520prior%2520with%2520a%2520GP%2520prior%252C%2520thereby%250Acapturing%2520richer%2520correlations%2520among%2520latent%2520variables.%2520However%252C%2520performing%2520exact%250AGP%2520inference%2520in%2520large-scale%2520GPVAEs%2520is%2520computationally%2520prohibitive%252C%2520often%250Aforcing%2520existing%2520approaches%2520to%2520rely%2520on%2520restrictive%2520kernel%2520assumptions%2520or%2520large%250Asets%2520of%2520inducing%2520points.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520neighbour-driven%250Aapproximation%2520strategy%2520that%2520exploits%2520local%2520adjacencies%2520in%2520the%2520latent%2520space%2520to%250Aachieve%2520scalable%2520GPVAE%2520inference.%2520By%2520confining%2520computations%2520to%2520the%2520nearest%250Aneighbours%2520of%2520each%2520data%2520point%252C%2520our%2520method%2520preserves%2520essential%2520latent%250Adependencies%252C%2520allowing%2520more%2520flexible%2520kernel%2520choices%2520and%2520mitigating%2520the%2520need%2520for%250Anumerous%2520inducing%2520points.%2520Through%2520extensive%2520experiments%2520on%2520tasks%2520including%250Arepresentation%2520learning%252C%2520data%2520imputation%252C%2520and%2520conditional%2520generation%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520outperforms%2520other%2520GPVAE%2520variants%2520in%2520both%250Apredictive%2520performance%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neighbour-Driven%20Gaussian%20Process%20Variational%20Autoencoders%20for%20Scalable%0A%20%20Structured%20Latent%20Modelling&entry.906535625=Xinxing%20Shi%20and%20Xiaoyu%20Jiang%20and%20Mauricio%20A.%20%C3%81lvarez&entry.1292438233=%20%20Gaussian%20Process%20%28GP%29%20Variational%20Autoencoders%20%28VAEs%29%20extend%20standard%20VAEs%20by%0Areplacing%20the%20fully%20factorised%20Gaussian%20prior%20with%20a%20GP%20prior%2C%20thereby%0Acapturing%20richer%20correlations%20among%20latent%20variables.%20However%2C%20performing%20exact%0AGP%20inference%20in%20large-scale%20GPVAEs%20is%20computationally%20prohibitive%2C%20often%0Aforcing%20existing%20approaches%20to%20rely%20on%20restrictive%20kernel%20assumptions%20or%20large%0Asets%20of%20inducing%20points.%20In%20this%20work%2C%20we%20propose%20a%20neighbour-driven%0Aapproximation%20strategy%20that%20exploits%20local%20adjacencies%20in%20the%20latent%20space%20to%0Aachieve%20scalable%20GPVAE%20inference.%20By%20confining%20computations%20to%20the%20nearest%0Aneighbours%20of%20each%20data%20point%2C%20our%20method%20preserves%20essential%20latent%0Adependencies%2C%20allowing%20more%20flexible%20kernel%20choices%20and%20mitigating%20the%20need%20for%0Anumerous%20inducing%20points.%20Through%20extensive%20experiments%20on%20tasks%20including%0Arepresentation%20learning%2C%20data%20imputation%2C%20and%20conditional%20generation%2C%20we%0Ademonstrate%20that%20our%20approach%20outperforms%20other%20GPVAE%20variants%20in%20both%0Apredictive%20performance%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16481v1&entry.124074799=Read"},
{"title": "Decoupled Geometric Parameterization and its Application in Deep\n  Homography Estimation", "author": "Yao Huang and Si-Yuan Cao and Yaqing Ding and Hao Yin and Shibin Xie and Shuting Wang and Zhijun Fang and Jiachun Wang and Shen Cai and Junchi Yan and Shuhan Shen", "abstract": "  Planar homography, with eight degrees of freedom (DOFs), is fundamental in\nnumerous computer vision tasks. While the positional offsets of four corners\nare widely adopted (especially in neural network predictions), this\nparameterization lacks geometric interpretability and typically requires\nsolving a linear system to compute the homography matrix. This paper presents a\nnovel geometric parameterization of homographies, leveraging the\nsimilarity-kernel-similarity (SKS) decomposition for projective\ntransformations. Two independent sets of four geometric parameters are\ndecoupled: one for a similarity transformation and the other for the kernel\ntransformation. Additionally, the geometric interpretation linearly relating\nthe four kernel transformation parameters to angular offsets is derived. Our\nproposed parameterization allows for direct homography estimation through\nmatrix multiplication, eliminating the need for solving a linear system, and\nachieves performance comparable to the four-corner positional offsets in deep\nhomography estimation.\n", "link": "http://arxiv.org/abs/2505.16599v1", "date": "2025-05-22", "relevancy": 2.5325, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4953}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupled%20Geometric%20Parameterization%20and%20its%20Application%20in%20Deep%0A%20%20Homography%20Estimation&body=Title%3A%20Decoupled%20Geometric%20Parameterization%20and%20its%20Application%20in%20Deep%0A%20%20Homography%20Estimation%0AAuthor%3A%20Yao%20Huang%20and%20Si-Yuan%20Cao%20and%20Yaqing%20Ding%20and%20Hao%20Yin%20and%20Shibin%20Xie%20and%20Shuting%20Wang%20and%20Zhijun%20Fang%20and%20Jiachun%20Wang%20and%20Shen%20Cai%20and%20Junchi%20Yan%20and%20Shuhan%20Shen%0AAbstract%3A%20%20%20Planar%20homography%2C%20with%20eight%20degrees%20of%20freedom%20%28DOFs%29%2C%20is%20fundamental%20in%0Anumerous%20computer%20vision%20tasks.%20While%20the%20positional%20offsets%20of%20four%20corners%0Aare%20widely%20adopted%20%28especially%20in%20neural%20network%20predictions%29%2C%20this%0Aparameterization%20lacks%20geometric%20interpretability%20and%20typically%20requires%0Asolving%20a%20linear%20system%20to%20compute%20the%20homography%20matrix.%20This%20paper%20presents%20a%0Anovel%20geometric%20parameterization%20of%20homographies%2C%20leveraging%20the%0Asimilarity-kernel-similarity%20%28SKS%29%20decomposition%20for%20projective%0Atransformations.%20Two%20independent%20sets%20of%20four%20geometric%20parameters%20are%0Adecoupled%3A%20one%20for%20a%20similarity%20transformation%20and%20the%20other%20for%20the%20kernel%0Atransformation.%20Additionally%2C%20the%20geometric%20interpretation%20linearly%20relating%0Athe%20four%20kernel%20transformation%20parameters%20to%20angular%20offsets%20is%20derived.%20Our%0Aproposed%20parameterization%20allows%20for%20direct%20homography%20estimation%20through%0Amatrix%20multiplication%2C%20eliminating%20the%20need%20for%20solving%20a%20linear%20system%2C%20and%0Aachieves%20performance%20comparable%20to%20the%20four-corner%20positional%20offsets%20in%20deep%0Ahomography%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupled%2520Geometric%2520Parameterization%2520and%2520its%2520Application%2520in%2520Deep%250A%2520%2520Homography%2520Estimation%26entry.906535625%3DYao%2520Huang%2520and%2520Si-Yuan%2520Cao%2520and%2520Yaqing%2520Ding%2520and%2520Hao%2520Yin%2520and%2520Shibin%2520Xie%2520and%2520Shuting%2520Wang%2520and%2520Zhijun%2520Fang%2520and%2520Jiachun%2520Wang%2520and%2520Shen%2520Cai%2520and%2520Junchi%2520Yan%2520and%2520Shuhan%2520Shen%26entry.1292438233%3D%2520%2520Planar%2520homography%252C%2520with%2520eight%2520degrees%2520of%2520freedom%2520%2528DOFs%2529%252C%2520is%2520fundamental%2520in%250Anumerous%2520computer%2520vision%2520tasks.%2520While%2520the%2520positional%2520offsets%2520of%2520four%2520corners%250Aare%2520widely%2520adopted%2520%2528especially%2520in%2520neural%2520network%2520predictions%2529%252C%2520this%250Aparameterization%2520lacks%2520geometric%2520interpretability%2520and%2520typically%2520requires%250Asolving%2520a%2520linear%2520system%2520to%2520compute%2520the%2520homography%2520matrix.%2520This%2520paper%2520presents%2520a%250Anovel%2520geometric%2520parameterization%2520of%2520homographies%252C%2520leveraging%2520the%250Asimilarity-kernel-similarity%2520%2528SKS%2529%2520decomposition%2520for%2520projective%250Atransformations.%2520Two%2520independent%2520sets%2520of%2520four%2520geometric%2520parameters%2520are%250Adecoupled%253A%2520one%2520for%2520a%2520similarity%2520transformation%2520and%2520the%2520other%2520for%2520the%2520kernel%250Atransformation.%2520Additionally%252C%2520the%2520geometric%2520interpretation%2520linearly%2520relating%250Athe%2520four%2520kernel%2520transformation%2520parameters%2520to%2520angular%2520offsets%2520is%2520derived.%2520Our%250Aproposed%2520parameterization%2520allows%2520for%2520direct%2520homography%2520estimation%2520through%250Amatrix%2520multiplication%252C%2520eliminating%2520the%2520need%2520for%2520solving%2520a%2520linear%2520system%252C%2520and%250Aachieves%2520performance%2520comparable%2520to%2520the%2520four-corner%2520positional%2520offsets%2520in%2520deep%250Ahomography%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Geometric%20Parameterization%20and%20its%20Application%20in%20Deep%0A%20%20Homography%20Estimation&entry.906535625=Yao%20Huang%20and%20Si-Yuan%20Cao%20and%20Yaqing%20Ding%20and%20Hao%20Yin%20and%20Shibin%20Xie%20and%20Shuting%20Wang%20and%20Zhijun%20Fang%20and%20Jiachun%20Wang%20and%20Shen%20Cai%20and%20Junchi%20Yan%20and%20Shuhan%20Shen&entry.1292438233=%20%20Planar%20homography%2C%20with%20eight%20degrees%20of%20freedom%20%28DOFs%29%2C%20is%20fundamental%20in%0Anumerous%20computer%20vision%20tasks.%20While%20the%20positional%20offsets%20of%20four%20corners%0Aare%20widely%20adopted%20%28especially%20in%20neural%20network%20predictions%29%2C%20this%0Aparameterization%20lacks%20geometric%20interpretability%20and%20typically%20requires%0Asolving%20a%20linear%20system%20to%20compute%20the%20homography%20matrix.%20This%20paper%20presents%20a%0Anovel%20geometric%20parameterization%20of%20homographies%2C%20leveraging%20the%0Asimilarity-kernel-similarity%20%28SKS%29%20decomposition%20for%20projective%0Atransformations.%20Two%20independent%20sets%20of%20four%20geometric%20parameters%20are%0Adecoupled%3A%20one%20for%20a%20similarity%20transformation%20and%20the%20other%20for%20the%20kernel%0Atransformation.%20Additionally%2C%20the%20geometric%20interpretation%20linearly%20relating%0Athe%20four%20kernel%20transformation%20parameters%20to%20angular%20offsets%20is%20derived.%20Our%0Aproposed%20parameterization%20allows%20for%20direct%20homography%20estimation%20through%0Amatrix%20multiplication%2C%20eliminating%20the%20need%20for%20solving%20a%20linear%20system%2C%20and%0Aachieves%20performance%20comparable%20to%20the%20four-corner%20positional%20offsets%20in%20deep%0Ahomography%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16599v1&entry.124074799=Read"},
{"title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase\n  Circuit Emergence", "author": "Gouki Minegishi and Hiroki Furuta and Shohei Taniguchi and Yusuke Iwasawa and Yutaka Matsuo", "abstract": "  Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability.\n", "link": "http://arxiv.org/abs/2505.16694v1", "date": "2025-05-22", "relevancy": 2.5246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Induction%20Heads%3A%20In-Context%20Meta%20Learning%20Induces%20Multi-Phase%0A%20%20Circuit%20Emergence&body=Title%3A%20Beyond%20Induction%20Heads%3A%20In-Context%20Meta%20Learning%20Induces%20Multi-Phase%0A%20%20Circuit%20Emergence%0AAuthor%3A%20Gouki%20Minegishi%20and%20Hiroki%20Furuta%20and%20Shohei%20Taniguchi%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo%0AAbstract%3A%20%20%20Transformer-based%20language%20models%20exhibit%20In-Context%20Learning%20%28ICL%29%2C%20where%0Apredictions%20are%20made%20adaptively%20based%20on%20context.%20While%20prior%20work%20links%0Ainduction%20heads%20to%20ICL%20through%20a%20sudden%20jump%20in%20accuracy%2C%20this%20can%20only%20account%0Afor%20ICL%20when%20the%20answer%20is%20included%20within%20the%20context.%20However%2C%20an%20important%0Aproperty%20of%20practical%20ICL%20in%20large%20language%20models%20is%20the%20ability%20to%20meta-learn%0Ahow%20to%20solve%20tasks%20from%20context%2C%20rather%20than%20just%20copying%20answers%20from%20context%3B%0Ahow%20such%20an%20ability%20is%20obtained%20during%20training%20is%20largely%20unexplored.%20In%20this%0Apaper%2C%20we%20experimentally%20clarify%20how%20such%20meta-learning%20ability%20is%20acquired%20by%0Aanalyzing%20the%20dynamics%20of%20the%20model%27s%20circuit%20during%20training.%20Specifically%2C%20we%0Aextend%20the%20copy%20task%20from%20previous%20research%20into%20an%20In-Context%20Meta%20Learning%0Asetting%2C%20where%20models%20must%20infer%20a%20task%20from%20examples%20to%20answer%20queries.%0AInterestingly%2C%20in%20this%20setting%2C%20we%20find%20that%20there%20are%20multiple%20phases%20in%20the%0Aprocess%20of%20acquiring%20such%20abilities%2C%20and%20that%20a%20unique%20circuit%20emerges%20in%20each%0Aphase%2C%20contrasting%20with%20the%20single-phases%20change%20in%20induction%20heads.%20The%0Aemergence%20of%20such%20circuits%20can%20be%20related%20to%20several%20phenomena%20known%20in%20large%0Alanguage%20models%2C%20and%20our%20analysis%20lead%20to%20a%20deeper%20understanding%20of%20the%20source%0Aof%20the%20transformer%27s%20ICL%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Induction%2520Heads%253A%2520In-Context%2520Meta%2520Learning%2520Induces%2520Multi-Phase%250A%2520%2520Circuit%2520Emergence%26entry.906535625%3DGouki%2520Minegishi%2520and%2520Hiroki%2520Furuta%2520and%2520Shohei%2520Taniguchi%2520and%2520Yusuke%2520Iwasawa%2520and%2520Yutaka%2520Matsuo%26entry.1292438233%3D%2520%2520Transformer-based%2520language%2520models%2520exhibit%2520In-Context%2520Learning%2520%2528ICL%2529%252C%2520where%250Apredictions%2520are%2520made%2520adaptively%2520based%2520on%2520context.%2520While%2520prior%2520work%2520links%250Ainduction%2520heads%2520to%2520ICL%2520through%2520a%2520sudden%2520jump%2520in%2520accuracy%252C%2520this%2520can%2520only%2520account%250Afor%2520ICL%2520when%2520the%2520answer%2520is%2520included%2520within%2520the%2520context.%2520However%252C%2520an%2520important%250Aproperty%2520of%2520practical%2520ICL%2520in%2520large%2520language%2520models%2520is%2520the%2520ability%2520to%2520meta-learn%250Ahow%2520to%2520solve%2520tasks%2520from%2520context%252C%2520rather%2520than%2520just%2520copying%2520answers%2520from%2520context%253B%250Ahow%2520such%2520an%2520ability%2520is%2520obtained%2520during%2520training%2520is%2520largely%2520unexplored.%2520In%2520this%250Apaper%252C%2520we%2520experimentally%2520clarify%2520how%2520such%2520meta-learning%2520ability%2520is%2520acquired%2520by%250Aanalyzing%2520the%2520dynamics%2520of%2520the%2520model%2527s%2520circuit%2520during%2520training.%2520Specifically%252C%2520we%250Aextend%2520the%2520copy%2520task%2520from%2520previous%2520research%2520into%2520an%2520In-Context%2520Meta%2520Learning%250Asetting%252C%2520where%2520models%2520must%2520infer%2520a%2520task%2520from%2520examples%2520to%2520answer%2520queries.%250AInterestingly%252C%2520in%2520this%2520setting%252C%2520we%2520find%2520that%2520there%2520are%2520multiple%2520phases%2520in%2520the%250Aprocess%2520of%2520acquiring%2520such%2520abilities%252C%2520and%2520that%2520a%2520unique%2520circuit%2520emerges%2520in%2520each%250Aphase%252C%2520contrasting%2520with%2520the%2520single-phases%2520change%2520in%2520induction%2520heads.%2520The%250Aemergence%2520of%2520such%2520circuits%2520can%2520be%2520related%2520to%2520several%2520phenomena%2520known%2520in%2520large%250Alanguage%2520models%252C%2520and%2520our%2520analysis%2520lead%2520to%2520a%2520deeper%2520understanding%2520of%2520the%2520source%250Aof%2520the%2520transformer%2527s%2520ICL%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Induction%20Heads%3A%20In-Context%20Meta%20Learning%20Induces%20Multi-Phase%0A%20%20Circuit%20Emergence&entry.906535625=Gouki%20Minegishi%20and%20Hiroki%20Furuta%20and%20Shohei%20Taniguchi%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo&entry.1292438233=%20%20Transformer-based%20language%20models%20exhibit%20In-Context%20Learning%20%28ICL%29%2C%20where%0Apredictions%20are%20made%20adaptively%20based%20on%20context.%20While%20prior%20work%20links%0Ainduction%20heads%20to%20ICL%20through%20a%20sudden%20jump%20in%20accuracy%2C%20this%20can%20only%20account%0Afor%20ICL%20when%20the%20answer%20is%20included%20within%20the%20context.%20However%2C%20an%20important%0Aproperty%20of%20practical%20ICL%20in%20large%20language%20models%20is%20the%20ability%20to%20meta-learn%0Ahow%20to%20solve%20tasks%20from%20context%2C%20rather%20than%20just%20copying%20answers%20from%20context%3B%0Ahow%20such%20an%20ability%20is%20obtained%20during%20training%20is%20largely%20unexplored.%20In%20this%0Apaper%2C%20we%20experimentally%20clarify%20how%20such%20meta-learning%20ability%20is%20acquired%20by%0Aanalyzing%20the%20dynamics%20of%20the%20model%27s%20circuit%20during%20training.%20Specifically%2C%20we%0Aextend%20the%20copy%20task%20from%20previous%20research%20into%20an%20In-Context%20Meta%20Learning%0Asetting%2C%20where%20models%20must%20infer%20a%20task%20from%20examples%20to%20answer%20queries.%0AInterestingly%2C%20in%20this%20setting%2C%20we%20find%20that%20there%20are%20multiple%20phases%20in%20the%0Aprocess%20of%20acquiring%20such%20abilities%2C%20and%20that%20a%20unique%20circuit%20emerges%20in%20each%0Aphase%2C%20contrasting%20with%20the%20single-phases%20change%20in%20induction%20heads.%20The%0Aemergence%20of%20such%20circuits%20can%20be%20related%20to%20several%20phenomena%20known%20in%20large%0Alanguage%20models%2C%20and%20our%20analysis%20lead%20to%20a%20deeper%20understanding%20of%20the%20source%0Aof%20the%20transformer%27s%20ICL%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16694v1&entry.124074799=Read"},
{"title": "VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning", "author": "Yuqi Liu and Tianyuan Qu and Zhisheng Zhong and Bohao Peng and Shu Liu and Bei Yu and Jiaya Jia", "abstract": "  Large vision-language models exhibit inherent capabilities to handle diverse\nvisual perception tasks. In this paper, we introduce VisionReasoner, a unified\nframework capable of reasoning and solving multiple visual perception tasks\nwithin a shared model. Specifically, by designing novel multi-object cognitive\nlearning strategies and systematic task reformulation, VisionReasoner enhances\nits reasoning capabilities to analyze visual inputs, and addresses diverse\nperception tasks in a unified framework. The model generates a structured\nreasoning process before delivering the desired outputs responding to user\nqueries. To rigorously assess unified visual perception capabilities, we\nevaluate VisionReasoner on ten diverse tasks spanning three critical domains:\ndetection, segmentation, and counting. Experimental results show that\nVisionReasoner achieves superior performance as a unified model, outperforming\nQwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg\n(segmentation), and 15.3% on CountBench (counting).\n", "link": "http://arxiv.org/abs/2505.12081v3", "date": "2025-05-22", "relevancy": 2.5235, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6409}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionReasoner%3A%20Unified%20Visual%20Perception%20and%20Reasoning%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20VisionReasoner%3A%20Unified%20Visual%20Perception%20and%20Reasoning%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Yuqi%20Liu%20and%20Tianyuan%20Qu%20and%20Zhisheng%20Zhong%20and%20Bohao%20Peng%20and%20Shu%20Liu%20and%20Bei%20Yu%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Large%20vision-language%20models%20exhibit%20inherent%20capabilities%20to%20handle%20diverse%0Avisual%20perception%20tasks.%20In%20this%20paper%2C%20we%20introduce%20VisionReasoner%2C%20a%20unified%0Aframework%20capable%20of%20reasoning%20and%20solving%20multiple%20visual%20perception%20tasks%0Awithin%20a%20shared%20model.%20Specifically%2C%20by%20designing%20novel%20multi-object%20cognitive%0Alearning%20strategies%20and%20systematic%20task%20reformulation%2C%20VisionReasoner%20enhances%0Aits%20reasoning%20capabilities%20to%20analyze%20visual%20inputs%2C%20and%20addresses%20diverse%0Aperception%20tasks%20in%20a%20unified%20framework.%20The%20model%20generates%20a%20structured%0Areasoning%20process%20before%20delivering%20the%20desired%20outputs%20responding%20to%20user%0Aqueries.%20To%20rigorously%20assess%20unified%20visual%20perception%20capabilities%2C%20we%0Aevaluate%20VisionReasoner%20on%20ten%20diverse%20tasks%20spanning%20three%20critical%20domains%3A%0Adetection%2C%20segmentation%2C%20and%20counting.%20Experimental%20results%20show%20that%0AVisionReasoner%20achieves%20superior%20performance%20as%20a%20unified%20model%2C%20outperforming%0AQwen2.5VL%20by%20relative%20margins%20of%2029.1%25%20on%20COCO%20%28detection%29%2C%2022.1%25%20on%20ReasonSeg%0A%28segmentation%29%2C%20and%2015.3%25%20on%20CountBench%20%28counting%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12081v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionReasoner%253A%2520Unified%2520Visual%2520Perception%2520and%2520Reasoning%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DYuqi%2520Liu%2520and%2520Tianyuan%2520Qu%2520and%2520Zhisheng%2520Zhong%2520and%2520Bohao%2520Peng%2520and%2520Shu%2520Liu%2520and%2520Bei%2520Yu%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520exhibit%2520inherent%2520capabilities%2520to%2520handle%2520diverse%250Avisual%2520perception%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VisionReasoner%252C%2520a%2520unified%250Aframework%2520capable%2520of%2520reasoning%2520and%2520solving%2520multiple%2520visual%2520perception%2520tasks%250Awithin%2520a%2520shared%2520model.%2520Specifically%252C%2520by%2520designing%2520novel%2520multi-object%2520cognitive%250Alearning%2520strategies%2520and%2520systematic%2520task%2520reformulation%252C%2520VisionReasoner%2520enhances%250Aits%2520reasoning%2520capabilities%2520to%2520analyze%2520visual%2520inputs%252C%2520and%2520addresses%2520diverse%250Aperception%2520tasks%2520in%2520a%2520unified%2520framework.%2520The%2520model%2520generates%2520a%2520structured%250Areasoning%2520process%2520before%2520delivering%2520the%2520desired%2520outputs%2520responding%2520to%2520user%250Aqueries.%2520To%2520rigorously%2520assess%2520unified%2520visual%2520perception%2520capabilities%252C%2520we%250Aevaluate%2520VisionReasoner%2520on%2520ten%2520diverse%2520tasks%2520spanning%2520three%2520critical%2520domains%253A%250Adetection%252C%2520segmentation%252C%2520and%2520counting.%2520Experimental%2520results%2520show%2520that%250AVisionReasoner%2520achieves%2520superior%2520performance%2520as%2520a%2520unified%2520model%252C%2520outperforming%250AQwen2.5VL%2520by%2520relative%2520margins%2520of%252029.1%2525%2520on%2520COCO%2520%2528detection%2529%252C%252022.1%2525%2520on%2520ReasonSeg%250A%2528segmentation%2529%252C%2520and%252015.3%2525%2520on%2520CountBench%2520%2528counting%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12081v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionReasoner%3A%20Unified%20Visual%20Perception%20and%20Reasoning%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Yuqi%20Liu%20and%20Tianyuan%20Qu%20and%20Zhisheng%20Zhong%20and%20Bohao%20Peng%20and%20Shu%20Liu%20and%20Bei%20Yu%20and%20Jiaya%20Jia&entry.1292438233=%20%20Large%20vision-language%20models%20exhibit%20inherent%20capabilities%20to%20handle%20diverse%0Avisual%20perception%20tasks.%20In%20this%20paper%2C%20we%20introduce%20VisionReasoner%2C%20a%20unified%0Aframework%20capable%20of%20reasoning%20and%20solving%20multiple%20visual%20perception%20tasks%0Awithin%20a%20shared%20model.%20Specifically%2C%20by%20designing%20novel%20multi-object%20cognitive%0Alearning%20strategies%20and%20systematic%20task%20reformulation%2C%20VisionReasoner%20enhances%0Aits%20reasoning%20capabilities%20to%20analyze%20visual%20inputs%2C%20and%20addresses%20diverse%0Aperception%20tasks%20in%20a%20unified%20framework.%20The%20model%20generates%20a%20structured%0Areasoning%20process%20before%20delivering%20the%20desired%20outputs%20responding%20to%20user%0Aqueries.%20To%20rigorously%20assess%20unified%20visual%20perception%20capabilities%2C%20we%0Aevaluate%20VisionReasoner%20on%20ten%20diverse%20tasks%20spanning%20three%20critical%20domains%3A%0Adetection%2C%20segmentation%2C%20and%20counting.%20Experimental%20results%20show%20that%0AVisionReasoner%20achieves%20superior%20performance%20as%20a%20unified%20model%2C%20outperforming%0AQwen2.5VL%20by%20relative%20margins%20of%2029.1%25%20on%20COCO%20%28detection%29%2C%2022.1%25%20on%20ReasonSeg%0A%28segmentation%29%2C%20and%2015.3%25%20on%20CountBench%20%28counting%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12081v3&entry.124074799=Read"},
{"title": "Robust and Fine-Grained Detection of AI Generated Texts", "author": "Ram Mohan Rao Kadiyala and Siddartha Pullakhandam and Kanwal Mehreen and Drishti Sharma and Siddhant Gupta and Jebish Purbey and Ashay Srivastava and Subhasya TippaReddy and Arvind Reddy Bobbili and Suraj Telugara Chandrashekhar and Modabbir Adeeb and Srinadh Vura and Hamza Farooq", "abstract": "  An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.\n", "link": "http://arxiv.org/abs/2504.11952v2", "date": "2025-05-22", "relevancy": 2.5227, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.507}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5068}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Fine-Grained%20Detection%20of%20AI%20Generated%20Texts&body=Title%3A%20Robust%20and%20Fine-Grained%20Detection%20of%20AI%20Generated%20Texts%0AAuthor%3A%20Ram%20Mohan%20Rao%20Kadiyala%20and%20Siddartha%20Pullakhandam%20and%20Kanwal%20Mehreen%20and%20Drishti%20Sharma%20and%20Siddhant%20Gupta%20and%20Jebish%20Purbey%20and%20Ashay%20Srivastava%20and%20Subhasya%20TippaReddy%20and%20Arvind%20Reddy%20Bobbili%20and%20Suraj%20Telugara%20Chandrashekhar%20and%20Modabbir%20Adeeb%20and%20Srinadh%20Vura%20and%20Hamza%20Farooq%0AAbstract%3A%20%20%20An%20ideal%20detection%20system%20for%20machine%20generated%20content%20is%20supposed%20to%20work%0Awell%20on%20any%20generator%20as%20many%20more%20advanced%20LLMs%20come%20into%20existence%20day%20by%0Aday.%20Existing%20systems%20often%20struggle%20with%20accurately%20identifying%20AI-generated%0Acontent%20over%20shorter%20texts.%20Further%2C%20not%20all%20texts%20might%20be%20entirely%20authored%0Aby%20a%20human%20or%20LLM%2C%20hence%20we%20focused%20more%20over%20partial%20cases%20i.e%20human-LLM%0Aco-authored%20texts.%20Our%20paper%20introduces%20a%20set%20of%20models%20built%20for%20the%20task%20of%0Atoken%20classification%20which%20are%20trained%20on%20an%20extensive%20collection%20of%0Ahuman-machine%20co-authored%20texts%2C%20which%20performed%20well%20over%20texts%20of%20unseen%0Adomains%2C%20unseen%20generators%2C%20texts%20by%20non-native%20speakers%20and%20those%20with%0Aadversarial%20inputs.%20We%20also%20introduce%20a%20new%20dataset%20of%20over%202.4M%20such%20texts%0Amostly%20co-authored%20by%20several%20popular%20proprietary%20LLMs%20over%2023%20languages.%20We%0Aalso%20present%20findings%20of%20our%20models%27%20performance%20over%20each%20texts%20of%20each%20domain%0Aand%20generator.%20Additional%20findings%20include%20comparison%20of%20performance%20against%0Aeach%20adversarial%20method%2C%20length%20of%20input%20texts%20and%20characteristics%20of%20generated%0Atexts%20compared%20to%20the%20original%20human%20authored%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Fine-Grained%2520Detection%2520of%2520AI%2520Generated%2520Texts%26entry.906535625%3DRam%2520Mohan%2520Rao%2520Kadiyala%2520and%2520Siddartha%2520Pullakhandam%2520and%2520Kanwal%2520Mehreen%2520and%2520Drishti%2520Sharma%2520and%2520Siddhant%2520Gupta%2520and%2520Jebish%2520Purbey%2520and%2520Ashay%2520Srivastava%2520and%2520Subhasya%2520TippaReddy%2520and%2520Arvind%2520Reddy%2520Bobbili%2520and%2520Suraj%2520Telugara%2520Chandrashekhar%2520and%2520Modabbir%2520Adeeb%2520and%2520Srinadh%2520Vura%2520and%2520Hamza%2520Farooq%26entry.1292438233%3D%2520%2520An%2520ideal%2520detection%2520system%2520for%2520machine%2520generated%2520content%2520is%2520supposed%2520to%2520work%250Awell%2520on%2520any%2520generator%2520as%2520many%2520more%2520advanced%2520LLMs%2520come%2520into%2520existence%2520day%2520by%250Aday.%2520Existing%2520systems%2520often%2520struggle%2520with%2520accurately%2520identifying%2520AI-generated%250Acontent%2520over%2520shorter%2520texts.%2520Further%252C%2520not%2520all%2520texts%2520might%2520be%2520entirely%2520authored%250Aby%2520a%2520human%2520or%2520LLM%252C%2520hence%2520we%2520focused%2520more%2520over%2520partial%2520cases%2520i.e%2520human-LLM%250Aco-authored%2520texts.%2520Our%2520paper%2520introduces%2520a%2520set%2520of%2520models%2520built%2520for%2520the%2520task%2520of%250Atoken%2520classification%2520which%2520are%2520trained%2520on%2520an%2520extensive%2520collection%2520of%250Ahuman-machine%2520co-authored%2520texts%252C%2520which%2520performed%2520well%2520over%2520texts%2520of%2520unseen%250Adomains%252C%2520unseen%2520generators%252C%2520texts%2520by%2520non-native%2520speakers%2520and%2520those%2520with%250Aadversarial%2520inputs.%2520We%2520also%2520introduce%2520a%2520new%2520dataset%2520of%2520over%25202.4M%2520such%2520texts%250Amostly%2520co-authored%2520by%2520several%2520popular%2520proprietary%2520LLMs%2520over%252023%2520languages.%2520We%250Aalso%2520present%2520findings%2520of%2520our%2520models%2527%2520performance%2520over%2520each%2520texts%2520of%2520each%2520domain%250Aand%2520generator.%2520Additional%2520findings%2520include%2520comparison%2520of%2520performance%2520against%250Aeach%2520adversarial%2520method%252C%2520length%2520of%2520input%2520texts%2520and%2520characteristics%2520of%2520generated%250Atexts%2520compared%2520to%2520the%2520original%2520human%2520authored%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Fine-Grained%20Detection%20of%20AI%20Generated%20Texts&entry.906535625=Ram%20Mohan%20Rao%20Kadiyala%20and%20Siddartha%20Pullakhandam%20and%20Kanwal%20Mehreen%20and%20Drishti%20Sharma%20and%20Siddhant%20Gupta%20and%20Jebish%20Purbey%20and%20Ashay%20Srivastava%20and%20Subhasya%20TippaReddy%20and%20Arvind%20Reddy%20Bobbili%20and%20Suraj%20Telugara%20Chandrashekhar%20and%20Modabbir%20Adeeb%20and%20Srinadh%20Vura%20and%20Hamza%20Farooq&entry.1292438233=%20%20An%20ideal%20detection%20system%20for%20machine%20generated%20content%20is%20supposed%20to%20work%0Awell%20on%20any%20generator%20as%20many%20more%20advanced%20LLMs%20come%20into%20existence%20day%20by%0Aday.%20Existing%20systems%20often%20struggle%20with%20accurately%20identifying%20AI-generated%0Acontent%20over%20shorter%20texts.%20Further%2C%20not%20all%20texts%20might%20be%20entirely%20authored%0Aby%20a%20human%20or%20LLM%2C%20hence%20we%20focused%20more%20over%20partial%20cases%20i.e%20human-LLM%0Aco-authored%20texts.%20Our%20paper%20introduces%20a%20set%20of%20models%20built%20for%20the%20task%20of%0Atoken%20classification%20which%20are%20trained%20on%20an%20extensive%20collection%20of%0Ahuman-machine%20co-authored%20texts%2C%20which%20performed%20well%20over%20texts%20of%20unseen%0Adomains%2C%20unseen%20generators%2C%20texts%20by%20non-native%20speakers%20and%20those%20with%0Aadversarial%20inputs.%20We%20also%20introduce%20a%20new%20dataset%20of%20over%202.4M%20such%20texts%0Amostly%20co-authored%20by%20several%20popular%20proprietary%20LLMs%20over%2023%20languages.%20We%0Aalso%20present%20findings%20of%20our%20models%27%20performance%20over%20each%20texts%20of%20each%20domain%0Aand%20generator.%20Additional%20findings%20include%20comparison%20of%20performance%20against%0Aeach%20adversarial%20method%2C%20length%20of%20input%20texts%20and%20characteristics%20of%20generated%0Atexts%20compared%20to%20the%20original%20human%20authored%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11952v2&entry.124074799=Read"},
{"title": "Leveraging Habitat Information for Fine-grained Bird Identification", "author": "Tin Nguyen and Peijie Chen and Anh Totti Nguyen", "abstract": "  Traditional bird classifiers mostly rely on the visual characteristics of\nbirds. Some prior works even train classifiers to be invariant to the\nbackground, completely discarding the living environment of birds. Instead, we\nare the first to explore integrating habitat information, one of the four major\ncues for identifying birds by ornithologists, into modern bird classifiers. We\nfocus on two leading model types: (1) CNNs and ViTs trained on the downstream\nbird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with\nhabitat-augmented data results in an improvement of up to +0.83 and +0.23\npoints on NABirds and CUB-200, respectively. Similarly, adding habitat\ndescriptors to the prompts for CLIP yields a substantial accuracy boost of up\nto +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find\nconsistent accuracy improvement after integrating habitat features into the\nimage augmentation process and into the textual descriptors of vision-language\nCLIP classifiers. Code is available at:\nhttps://anonymous.4open.science/r/reasoning-8B7E/.\n", "link": "http://arxiv.org/abs/2312.14999v3", "date": "2025-05-22", "relevancy": 2.52, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Habitat%20Information%20for%20Fine-grained%20Bird%20Identification&body=Title%3A%20Leveraging%20Habitat%20Information%20for%20Fine-grained%20Bird%20Identification%0AAuthor%3A%20Tin%20Nguyen%20and%20Peijie%20Chen%20and%20Anh%20Totti%20Nguyen%0AAbstract%3A%20%20%20Traditional%20bird%20classifiers%20mostly%20rely%20on%20the%20visual%20characteristics%20of%0Abirds.%20Some%20prior%20works%20even%20train%20classifiers%20to%20be%20invariant%20to%20the%0Abackground%2C%20completely%20discarding%20the%20living%20environment%20of%20birds.%20Instead%2C%20we%0Aare%20the%20first%20to%20explore%20integrating%20habitat%20information%2C%20one%20of%20the%20four%20major%0Acues%20for%20identifying%20birds%20by%20ornithologists%2C%20into%20modern%20bird%20classifiers.%20We%0Afocus%20on%20two%20leading%20model%20types%3A%20%281%29%20CNNs%20and%20ViTs%20trained%20on%20the%20downstream%0Abird%20datasets%3B%20and%20%282%29%20original%2C%20multi-modal%20CLIP.%20Training%20CNNs%20and%20ViTs%20with%0Ahabitat-augmented%20data%20results%20in%20an%20improvement%20of%20up%20to%20%2B0.83%20and%20%2B0.23%0Apoints%20on%20NABirds%20and%20CUB-200%2C%20respectively.%20Similarly%2C%20adding%20habitat%0Adescriptors%20to%20the%20prompts%20for%20CLIP%20yields%20a%20substantial%20accuracy%20boost%20of%20up%0Ato%20%2B0.99%20and%20%2B1.1%20points%20on%20NABirds%20and%20CUB-200%2C%20respectively.%20We%20find%0Aconsistent%20accuracy%20improvement%20after%20integrating%20habitat%20features%20into%20the%0Aimage%20augmentation%20process%20and%20into%20the%20textual%20descriptors%20of%20vision-language%0ACLIP%20classifiers.%20Code%20is%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/reasoning-8B7E/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14999v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Habitat%2520Information%2520for%2520Fine-grained%2520Bird%2520Identification%26entry.906535625%3DTin%2520Nguyen%2520and%2520Peijie%2520Chen%2520and%2520Anh%2520Totti%2520Nguyen%26entry.1292438233%3D%2520%2520Traditional%2520bird%2520classifiers%2520mostly%2520rely%2520on%2520the%2520visual%2520characteristics%2520of%250Abirds.%2520Some%2520prior%2520works%2520even%2520train%2520classifiers%2520to%2520be%2520invariant%2520to%2520the%250Abackground%252C%2520completely%2520discarding%2520the%2520living%2520environment%2520of%2520birds.%2520Instead%252C%2520we%250Aare%2520the%2520first%2520to%2520explore%2520integrating%2520habitat%2520information%252C%2520one%2520of%2520the%2520four%2520major%250Acues%2520for%2520identifying%2520birds%2520by%2520ornithologists%252C%2520into%2520modern%2520bird%2520classifiers.%2520We%250Afocus%2520on%2520two%2520leading%2520model%2520types%253A%2520%25281%2529%2520CNNs%2520and%2520ViTs%2520trained%2520on%2520the%2520downstream%250Abird%2520datasets%253B%2520and%2520%25282%2529%2520original%252C%2520multi-modal%2520CLIP.%2520Training%2520CNNs%2520and%2520ViTs%2520with%250Ahabitat-augmented%2520data%2520results%2520in%2520an%2520improvement%2520of%2520up%2520to%2520%252B0.83%2520and%2520%252B0.23%250Apoints%2520on%2520NABirds%2520and%2520CUB-200%252C%2520respectively.%2520Similarly%252C%2520adding%2520habitat%250Adescriptors%2520to%2520the%2520prompts%2520for%2520CLIP%2520yields%2520a%2520substantial%2520accuracy%2520boost%2520of%2520up%250Ato%2520%252B0.99%2520and%2520%252B1.1%2520points%2520on%2520NABirds%2520and%2520CUB-200%252C%2520respectively.%2520We%2520find%250Aconsistent%2520accuracy%2520improvement%2520after%2520integrating%2520habitat%2520features%2520into%2520the%250Aimage%2520augmentation%2520process%2520and%2520into%2520the%2520textual%2520descriptors%2520of%2520vision-language%250ACLIP%2520classifiers.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//anonymous.4open.science/r/reasoning-8B7E/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14999v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Habitat%20Information%20for%20Fine-grained%20Bird%20Identification&entry.906535625=Tin%20Nguyen%20and%20Peijie%20Chen%20and%20Anh%20Totti%20Nguyen&entry.1292438233=%20%20Traditional%20bird%20classifiers%20mostly%20rely%20on%20the%20visual%20characteristics%20of%0Abirds.%20Some%20prior%20works%20even%20train%20classifiers%20to%20be%20invariant%20to%20the%0Abackground%2C%20completely%20discarding%20the%20living%20environment%20of%20birds.%20Instead%2C%20we%0Aare%20the%20first%20to%20explore%20integrating%20habitat%20information%2C%20one%20of%20the%20four%20major%0Acues%20for%20identifying%20birds%20by%20ornithologists%2C%20into%20modern%20bird%20classifiers.%20We%0Afocus%20on%20two%20leading%20model%20types%3A%20%281%29%20CNNs%20and%20ViTs%20trained%20on%20the%20downstream%0Abird%20datasets%3B%20and%20%282%29%20original%2C%20multi-modal%20CLIP.%20Training%20CNNs%20and%20ViTs%20with%0Ahabitat-augmented%20data%20results%20in%20an%20improvement%20of%20up%20to%20%2B0.83%20and%20%2B0.23%0Apoints%20on%20NABirds%20and%20CUB-200%2C%20respectively.%20Similarly%2C%20adding%20habitat%0Adescriptors%20to%20the%20prompts%20for%20CLIP%20yields%20a%20substantial%20accuracy%20boost%20of%20up%0Ato%20%2B0.99%20and%20%2B1.1%20points%20on%20NABirds%20and%20CUB-200%2C%20respectively.%20We%20find%0Aconsistent%20accuracy%20improvement%20after%20integrating%20habitat%20features%20into%20the%0Aimage%20augmentation%20process%20and%20into%20the%20textual%20descriptors%20of%20vision-language%0ACLIP%20classifiers.%20Code%20is%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/reasoning-8B7E/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14999v3&entry.124074799=Read"},
{"title": "Structure-Aligned Protein Language Model", "author": "Can Chen and David Heurtel-Depeiges and Robert M. Vernon and Christopher James Langmead and Yoshua Bengio and Quentin Fournier", "abstract": "  Protein language models (pLMs) pre-trained on vast protein sequence databases\nexcel at various downstream tasks but lack the structural knowledge essential\nfor many biological applications. To address this, we integrate structural\ninsights from pre-trained protein graph neural networks (pGNNs) into pLMs\nthrough a latent-level contrastive learning task. This task aligns residue\nrepresentations from pLMs with those from pGNNs across multiple proteins,\nenriching pLMs with inter-protein structural knowledge. Additionally, we\nincorporate a physical-level task that infuses intra-protein structural\nknowledge by optimizing pLMs to predict structural tokens. The proposed\ndual-task framework effectively incorporates both inter-protein and\nintra-protein structural knowledge into pLMs. Given the variability in the\nquality of protein structures in PDB, we further introduce a residue loss\nselection module, which uses a small model trained on high-quality structures\nto select reliable yet challenging residue losses for the pLM to learn.\nApplying our structure alignment method to the state-of-the-art ESM2 and\nAMPLIFY results in notable performance gains across a wide range of tasks,\nincluding a 12.7% increase in ESM2 contact prediction. The data, code, and\nresulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.\n", "link": "http://arxiv.org/abs/2505.16896v1", "date": "2025-05-22", "relevancy": 2.5123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Aligned%20Protein%20Language%20Model&body=Title%3A%20Structure-Aligned%20Protein%20Language%20Model%0AAuthor%3A%20Can%20Chen%20and%20David%20Heurtel-Depeiges%20and%20Robert%20M.%20Vernon%20and%20Christopher%20James%20Langmead%20and%20Yoshua%20Bengio%20and%20Quentin%20Fournier%0AAbstract%3A%20%20%20Protein%20language%20models%20%28pLMs%29%20pre-trained%20on%20vast%20protein%20sequence%20databases%0Aexcel%20at%20various%20downstream%20tasks%20but%20lack%20the%20structural%20knowledge%20essential%0Afor%20many%20biological%20applications.%20To%20address%20this%2C%20we%20integrate%20structural%0Ainsights%20from%20pre-trained%20protein%20graph%20neural%20networks%20%28pGNNs%29%20into%20pLMs%0Athrough%20a%20latent-level%20contrastive%20learning%20task.%20This%20task%20aligns%20residue%0Arepresentations%20from%20pLMs%20with%20those%20from%20pGNNs%20across%20multiple%20proteins%2C%0Aenriching%20pLMs%20with%20inter-protein%20structural%20knowledge.%20Additionally%2C%20we%0Aincorporate%20a%20physical-level%20task%20that%20infuses%20intra-protein%20structural%0Aknowledge%20by%20optimizing%20pLMs%20to%20predict%20structural%20tokens.%20The%20proposed%0Adual-task%20framework%20effectively%20incorporates%20both%20inter-protein%20and%0Aintra-protein%20structural%20knowledge%20into%20pLMs.%20Given%20the%20variability%20in%20the%0Aquality%20of%20protein%20structures%20in%20PDB%2C%20we%20further%20introduce%20a%20residue%20loss%0Aselection%20module%2C%20which%20uses%20a%20small%20model%20trained%20on%20high-quality%20structures%0Ato%20select%20reliable%20yet%20challenging%20residue%20losses%20for%20the%20pLM%20to%20learn.%0AApplying%20our%20structure%20alignment%20method%20to%20the%20state-of-the-art%20ESM2%20and%0AAMPLIFY%20results%20in%20notable%20performance%20gains%20across%20a%20wide%20range%20of%20tasks%2C%0Aincluding%20a%2012.7%25%20increase%20in%20ESM2%20contact%20prediction.%20The%20data%2C%20code%2C%20and%0Aresulting%20SaESM2%20and%20SaAMPLIFY%20models%20will%20be%20released%20on%20Hugging%20Face.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Aligned%2520Protein%2520Language%2520Model%26entry.906535625%3DCan%2520Chen%2520and%2520David%2520Heurtel-Depeiges%2520and%2520Robert%2520M.%2520Vernon%2520and%2520Christopher%2520James%2520Langmead%2520and%2520Yoshua%2520Bengio%2520and%2520Quentin%2520Fournier%26entry.1292438233%3D%2520%2520Protein%2520language%2520models%2520%2528pLMs%2529%2520pre-trained%2520on%2520vast%2520protein%2520sequence%2520databases%250Aexcel%2520at%2520various%2520downstream%2520tasks%2520but%2520lack%2520the%2520structural%2520knowledge%2520essential%250Afor%2520many%2520biological%2520applications.%2520To%2520address%2520this%252C%2520we%2520integrate%2520structural%250Ainsights%2520from%2520pre-trained%2520protein%2520graph%2520neural%2520networks%2520%2528pGNNs%2529%2520into%2520pLMs%250Athrough%2520a%2520latent-level%2520contrastive%2520learning%2520task.%2520This%2520task%2520aligns%2520residue%250Arepresentations%2520from%2520pLMs%2520with%2520those%2520from%2520pGNNs%2520across%2520multiple%2520proteins%252C%250Aenriching%2520pLMs%2520with%2520inter-protein%2520structural%2520knowledge.%2520Additionally%252C%2520we%250Aincorporate%2520a%2520physical-level%2520task%2520that%2520infuses%2520intra-protein%2520structural%250Aknowledge%2520by%2520optimizing%2520pLMs%2520to%2520predict%2520structural%2520tokens.%2520The%2520proposed%250Adual-task%2520framework%2520effectively%2520incorporates%2520both%2520inter-protein%2520and%250Aintra-protein%2520structural%2520knowledge%2520into%2520pLMs.%2520Given%2520the%2520variability%2520in%2520the%250Aquality%2520of%2520protein%2520structures%2520in%2520PDB%252C%2520we%2520further%2520introduce%2520a%2520residue%2520loss%250Aselection%2520module%252C%2520which%2520uses%2520a%2520small%2520model%2520trained%2520on%2520high-quality%2520structures%250Ato%2520select%2520reliable%2520yet%2520challenging%2520residue%2520losses%2520for%2520the%2520pLM%2520to%2520learn.%250AApplying%2520our%2520structure%2520alignment%2520method%2520to%2520the%2520state-of-the-art%2520ESM2%2520and%250AAMPLIFY%2520results%2520in%2520notable%2520performance%2520gains%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%250Aincluding%2520a%252012.7%2525%2520increase%2520in%2520ESM2%2520contact%2520prediction.%2520The%2520data%252C%2520code%252C%2520and%250Aresulting%2520SaESM2%2520and%2520SaAMPLIFY%2520models%2520will%2520be%2520released%2520on%2520Hugging%2520Face.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Aligned%20Protein%20Language%20Model&entry.906535625=Can%20Chen%20and%20David%20Heurtel-Depeiges%20and%20Robert%20M.%20Vernon%20and%20Christopher%20James%20Langmead%20and%20Yoshua%20Bengio%20and%20Quentin%20Fournier&entry.1292438233=%20%20Protein%20language%20models%20%28pLMs%29%20pre-trained%20on%20vast%20protein%20sequence%20databases%0Aexcel%20at%20various%20downstream%20tasks%20but%20lack%20the%20structural%20knowledge%20essential%0Afor%20many%20biological%20applications.%20To%20address%20this%2C%20we%20integrate%20structural%0Ainsights%20from%20pre-trained%20protein%20graph%20neural%20networks%20%28pGNNs%29%20into%20pLMs%0Athrough%20a%20latent-level%20contrastive%20learning%20task.%20This%20task%20aligns%20residue%0Arepresentations%20from%20pLMs%20with%20those%20from%20pGNNs%20across%20multiple%20proteins%2C%0Aenriching%20pLMs%20with%20inter-protein%20structural%20knowledge.%20Additionally%2C%20we%0Aincorporate%20a%20physical-level%20task%20that%20infuses%20intra-protein%20structural%0Aknowledge%20by%20optimizing%20pLMs%20to%20predict%20structural%20tokens.%20The%20proposed%0Adual-task%20framework%20effectively%20incorporates%20both%20inter-protein%20and%0Aintra-protein%20structural%20knowledge%20into%20pLMs.%20Given%20the%20variability%20in%20the%0Aquality%20of%20protein%20structures%20in%20PDB%2C%20we%20further%20introduce%20a%20residue%20loss%0Aselection%20module%2C%20which%20uses%20a%20small%20model%20trained%20on%20high-quality%20structures%0Ato%20select%20reliable%20yet%20challenging%20residue%20losses%20for%20the%20pLM%20to%20learn.%0AApplying%20our%20structure%20alignment%20method%20to%20the%20state-of-the-art%20ESM2%20and%0AAMPLIFY%20results%20in%20notable%20performance%20gains%20across%20a%20wide%20range%20of%20tasks%2C%0Aincluding%20a%2012.7%25%20increase%20in%20ESM2%20contact%20prediction.%20The%20data%2C%20code%2C%20and%0Aresulting%20SaESM2%20and%20SaAMPLIFY%20models%20will%20be%20released%20on%20Hugging%20Face.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16896v1&entry.124074799=Read"},
{"title": "Emergent Specialization: Rare Token Neurons in Language Models", "author": "Jing Liu and Haozheng Wang and Yueheng Li", "abstract": "  Large language models struggle with representing and generating rare tokens\ndespite their importance in specialized domains. In this study, we identify\nneuron structures with exceptionally strong influence on language model's\nprediction of rare tokens, termed as rare token neurons, and investigate the\nmechanism for their emergence and behavior. These neurons exhibit a\ncharacteristic three-phase organization (plateau, power-law, and rapid decay)\nthat emerges dynamically during training, evolving from a homogeneous initial\nstate to a functionally differentiated architecture. In the activation space,\nrare token neurons form a coordinated subnetwork that selectively co-activates\nwhile avoiding co-activation with other neurons. This functional specialization\npotentially correlates with the development of heavy-tailed weight\ndistributions, suggesting a statistical mechanical basis for emergent\nspecialization.\n", "link": "http://arxiv.org/abs/2505.12822v2", "date": "2025-05-22", "relevancy": 2.4995, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5017}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5017}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Specialization%3A%20Rare%20Token%20Neurons%20in%20Language%20Models&body=Title%3A%20Emergent%20Specialization%3A%20Rare%20Token%20Neurons%20in%20Language%20Models%0AAuthor%3A%20Jing%20Liu%20and%20Haozheng%20Wang%20and%20Yueheng%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20struggle%20with%20representing%20and%20generating%20rare%20tokens%0Adespite%20their%20importance%20in%20specialized%20domains.%20In%20this%20study%2C%20we%20identify%0Aneuron%20structures%20with%20exceptionally%20strong%20influence%20on%20language%20model%27s%0Aprediction%20of%20rare%20tokens%2C%20termed%20as%20rare%20token%20neurons%2C%20and%20investigate%20the%0Amechanism%20for%20their%20emergence%20and%20behavior.%20These%20neurons%20exhibit%20a%0Acharacteristic%20three-phase%20organization%20%28plateau%2C%20power-law%2C%20and%20rapid%20decay%29%0Athat%20emerges%20dynamically%20during%20training%2C%20evolving%20from%20a%20homogeneous%20initial%0Astate%20to%20a%20functionally%20differentiated%20architecture.%20In%20the%20activation%20space%2C%0Arare%20token%20neurons%20form%20a%20coordinated%20subnetwork%20that%20selectively%20co-activates%0Awhile%20avoiding%20co-activation%20with%20other%20neurons.%20This%20functional%20specialization%0Apotentially%20correlates%20with%20the%20development%20of%20heavy-tailed%20weight%0Adistributions%2C%20suggesting%20a%20statistical%20mechanical%20basis%20for%20emergent%0Aspecialization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Specialization%253A%2520Rare%2520Token%2520Neurons%2520in%2520Language%2520Models%26entry.906535625%3DJing%2520Liu%2520and%2520Haozheng%2520Wang%2520and%2520Yueheng%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520struggle%2520with%2520representing%2520and%2520generating%2520rare%2520tokens%250Adespite%2520their%2520importance%2520in%2520specialized%2520domains.%2520In%2520this%2520study%252C%2520we%2520identify%250Aneuron%2520structures%2520with%2520exceptionally%2520strong%2520influence%2520on%2520language%2520model%2527s%250Aprediction%2520of%2520rare%2520tokens%252C%2520termed%2520as%2520rare%2520token%2520neurons%252C%2520and%2520investigate%2520the%250Amechanism%2520for%2520their%2520emergence%2520and%2520behavior.%2520These%2520neurons%2520exhibit%2520a%250Acharacteristic%2520three-phase%2520organization%2520%2528plateau%252C%2520power-law%252C%2520and%2520rapid%2520decay%2529%250Athat%2520emerges%2520dynamically%2520during%2520training%252C%2520evolving%2520from%2520a%2520homogeneous%2520initial%250Astate%2520to%2520a%2520functionally%2520differentiated%2520architecture.%2520In%2520the%2520activation%2520space%252C%250Arare%2520token%2520neurons%2520form%2520a%2520coordinated%2520subnetwork%2520that%2520selectively%2520co-activates%250Awhile%2520avoiding%2520co-activation%2520with%2520other%2520neurons.%2520This%2520functional%2520specialization%250Apotentially%2520correlates%2520with%2520the%2520development%2520of%2520heavy-tailed%2520weight%250Adistributions%252C%2520suggesting%2520a%2520statistical%2520mechanical%2520basis%2520for%2520emergent%250Aspecialization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Specialization%3A%20Rare%20Token%20Neurons%20in%20Language%20Models&entry.906535625=Jing%20Liu%20and%20Haozheng%20Wang%20and%20Yueheng%20Li&entry.1292438233=%20%20Large%20language%20models%20struggle%20with%20representing%20and%20generating%20rare%20tokens%0Adespite%20their%20importance%20in%20specialized%20domains.%20In%20this%20study%2C%20we%20identify%0Aneuron%20structures%20with%20exceptionally%20strong%20influence%20on%20language%20model%27s%0Aprediction%20of%20rare%20tokens%2C%20termed%20as%20rare%20token%20neurons%2C%20and%20investigate%20the%0Amechanism%20for%20their%20emergence%20and%20behavior.%20These%20neurons%20exhibit%20a%0Acharacteristic%20three-phase%20organization%20%28plateau%2C%20power-law%2C%20and%20rapid%20decay%29%0Athat%20emerges%20dynamically%20during%20training%2C%20evolving%20from%20a%20homogeneous%20initial%0Astate%20to%20a%20functionally%20differentiated%20architecture.%20In%20the%20activation%20space%2C%0Arare%20token%20neurons%20form%20a%20coordinated%20subnetwork%20that%20selectively%20co-activates%0Awhile%20avoiding%20co-activation%20with%20other%20neurons.%20This%20functional%20specialization%0Apotentially%20correlates%20with%20the%20development%20of%20heavy-tailed%20weight%0Adistributions%2C%20suggesting%20a%20statistical%20mechanical%20basis%20for%20emergent%0Aspecialization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12822v2&entry.124074799=Read"},
{"title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology\n  Reports", "author": "Francesco Dalla Serra and Patrick Schrempf and Chaoyang Wang and Zaiqiao Meng and Fani Deligianni and Alison Q. O'Neil", "abstract": "  We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset.\n", "link": "http://arxiv.org/abs/2505.16624v1", "date": "2025-05-22", "relevancy": 2.4853, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Chest%20X-Ray%20Visual%20Question%20Answering%20with%20Generated%20Radiology%0A%20%20Reports&body=Title%3A%20Grounding%20Chest%20X-Ray%20Visual%20Question%20Answering%20with%20Generated%20Radiology%0A%20%20Reports%0AAuthor%3A%20Francesco%20Dalla%20Serra%20and%20Patrick%20Schrempf%20and%20Chaoyang%20Wang%20and%20Zaiqiao%20Meng%20and%20Fani%20Deligianni%20and%20Alison%20Q.%20O%27Neil%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20Chest%20X-ray%20%28CXR%29%20Visual%20Question%20Answering%0A%28VQA%29%2C%20addressing%20both%20single-image%20image-difference%20questions.%20Single-image%0Aquestions%20focus%20on%20abnormalities%20within%20a%20specific%20CXR%20%28%22What%20abnormalities%20are%0Aseen%20in%20image%20X%3F%22%29%2C%20while%20image-difference%20questions%20compare%20two%20longitudinal%0ACXRs%20acquired%20at%20different%20time%20points%20%28%22What%20are%20the%20differences%20between%20image%0AX%20and%20Y%3F%22%29.%20We%20further%20explore%20how%20the%20integration%20of%20radiology%20reports%20can%0Aenhance%20the%20performance%20of%20VQA%20models.%20While%20previous%20approaches%20have%0Ademonstrated%20the%20utility%20of%20radiology%20reports%20during%20the%20pre-training%20phase%2C%20we%0Aextend%20this%20idea%20by%20showing%20that%20the%20reports%20can%20also%20be%20leveraged%20as%0Aadditional%20input%20to%20improve%20the%20VQA%20model%27s%20predicted%20answers.%20First%2C%20we%0Apropose%20a%20unified%20method%20that%20handles%20both%20types%20of%20questions%20and%0Aauto-regressively%20generates%20the%20answers.%20For%20single-image%20questions%2C%20the%20model%0Ais%20provided%20with%20a%20single%20CXR.%20For%20image-difference%20questions%2C%20the%20model%20is%0Aprovided%20with%20two%20CXRs%20from%20the%20same%20patient%2C%20captured%20at%20different%20time%0Apoints%2C%20enabling%20the%20model%20to%20detect%20and%20describe%20temporal%20changes.%20Taking%0Ainspiration%20from%20%27Chain-of-Thought%20reasoning%27%2C%20we%20demonstrate%20that%20performance%0Aon%20the%20CXR%20VQA%20task%20can%20be%20improved%20by%20grounding%20the%20answer%20generator%20module%0Awith%20a%20radiology%20report%20predicted%20for%20the%20same%20CXR.%20In%20our%20approach%2C%20the%20VQA%0Amodel%20is%20divided%20into%20two%20steps%3A%20i%29%20Report%20Generation%20%28RG%29%20and%20ii%29%20Answer%0AGeneration%20%28AG%29.%20Our%20results%20demonstrate%20that%20incorporating%20predicted%20radiology%0Areports%20as%20evidence%20to%20the%20AG%20model%20enhances%20performance%20on%20both%20single-image%0Aand%20image-difference%20questions%2C%20achieving%20state-of-the-art%20results%20on%20the%0AMedical-Diff-VQA%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Chest%2520X-Ray%2520Visual%2520Question%2520Answering%2520with%2520Generated%2520Radiology%250A%2520%2520Reports%26entry.906535625%3DFrancesco%2520Dalla%2520Serra%2520and%2520Patrick%2520Schrempf%2520and%2520Chaoyang%2520Wang%2520and%2520Zaiqiao%2520Meng%2520and%2520Fani%2520Deligianni%2520and%2520Alison%2520Q.%2520O%2527Neil%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520Chest%2520X-ray%2520%2528CXR%2529%2520Visual%2520Question%2520Answering%250A%2528VQA%2529%252C%2520addressing%2520both%2520single-image%2520image-difference%2520questions.%2520Single-image%250Aquestions%2520focus%2520on%2520abnormalities%2520within%2520a%2520specific%2520CXR%2520%2528%2522What%2520abnormalities%2520are%250Aseen%2520in%2520image%2520X%253F%2522%2529%252C%2520while%2520image-difference%2520questions%2520compare%2520two%2520longitudinal%250ACXRs%2520acquired%2520at%2520different%2520time%2520points%2520%2528%2522What%2520are%2520the%2520differences%2520between%2520image%250AX%2520and%2520Y%253F%2522%2529.%2520We%2520further%2520explore%2520how%2520the%2520integration%2520of%2520radiology%2520reports%2520can%250Aenhance%2520the%2520performance%2520of%2520VQA%2520models.%2520While%2520previous%2520approaches%2520have%250Ademonstrated%2520the%2520utility%2520of%2520radiology%2520reports%2520during%2520the%2520pre-training%2520phase%252C%2520we%250Aextend%2520this%2520idea%2520by%2520showing%2520that%2520the%2520reports%2520can%2520also%2520be%2520leveraged%2520as%250Aadditional%2520input%2520to%2520improve%2520the%2520VQA%2520model%2527s%2520predicted%2520answers.%2520First%252C%2520we%250Apropose%2520a%2520unified%2520method%2520that%2520handles%2520both%2520types%2520of%2520questions%2520and%250Aauto-regressively%2520generates%2520the%2520answers.%2520For%2520single-image%2520questions%252C%2520the%2520model%250Ais%2520provided%2520with%2520a%2520single%2520CXR.%2520For%2520image-difference%2520questions%252C%2520the%2520model%2520is%250Aprovided%2520with%2520two%2520CXRs%2520from%2520the%2520same%2520patient%252C%2520captured%2520at%2520different%2520time%250Apoints%252C%2520enabling%2520the%2520model%2520to%2520detect%2520and%2520describe%2520temporal%2520changes.%2520Taking%250Ainspiration%2520from%2520%2527Chain-of-Thought%2520reasoning%2527%252C%2520we%2520demonstrate%2520that%2520performance%250Aon%2520the%2520CXR%2520VQA%2520task%2520can%2520be%2520improved%2520by%2520grounding%2520the%2520answer%2520generator%2520module%250Awith%2520a%2520radiology%2520report%2520predicted%2520for%2520the%2520same%2520CXR.%2520In%2520our%2520approach%252C%2520the%2520VQA%250Amodel%2520is%2520divided%2520into%2520two%2520steps%253A%2520i%2529%2520Report%2520Generation%2520%2528RG%2529%2520and%2520ii%2529%2520Answer%250AGeneration%2520%2528AG%2529.%2520Our%2520results%2520demonstrate%2520that%2520incorporating%2520predicted%2520radiology%250Areports%2520as%2520evidence%2520to%2520the%2520AG%2520model%2520enhances%2520performance%2520on%2520both%2520single-image%250Aand%2520image-difference%2520questions%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520the%250AMedical-Diff-VQA%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Chest%20X-Ray%20Visual%20Question%20Answering%20with%20Generated%20Radiology%0A%20%20Reports&entry.906535625=Francesco%20Dalla%20Serra%20and%20Patrick%20Schrempf%20and%20Chaoyang%20Wang%20and%20Zaiqiao%20Meng%20and%20Fani%20Deligianni%20and%20Alison%20Q.%20O%27Neil&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20Chest%20X-ray%20%28CXR%29%20Visual%20Question%20Answering%0A%28VQA%29%2C%20addressing%20both%20single-image%20image-difference%20questions.%20Single-image%0Aquestions%20focus%20on%20abnormalities%20within%20a%20specific%20CXR%20%28%22What%20abnormalities%20are%0Aseen%20in%20image%20X%3F%22%29%2C%20while%20image-difference%20questions%20compare%20two%20longitudinal%0ACXRs%20acquired%20at%20different%20time%20points%20%28%22What%20are%20the%20differences%20between%20image%0AX%20and%20Y%3F%22%29.%20We%20further%20explore%20how%20the%20integration%20of%20radiology%20reports%20can%0Aenhance%20the%20performance%20of%20VQA%20models.%20While%20previous%20approaches%20have%0Ademonstrated%20the%20utility%20of%20radiology%20reports%20during%20the%20pre-training%20phase%2C%20we%0Aextend%20this%20idea%20by%20showing%20that%20the%20reports%20can%20also%20be%20leveraged%20as%0Aadditional%20input%20to%20improve%20the%20VQA%20model%27s%20predicted%20answers.%20First%2C%20we%0Apropose%20a%20unified%20method%20that%20handles%20both%20types%20of%20questions%20and%0Aauto-regressively%20generates%20the%20answers.%20For%20single-image%20questions%2C%20the%20model%0Ais%20provided%20with%20a%20single%20CXR.%20For%20image-difference%20questions%2C%20the%20model%20is%0Aprovided%20with%20two%20CXRs%20from%20the%20same%20patient%2C%20captured%20at%20different%20time%0Apoints%2C%20enabling%20the%20model%20to%20detect%20and%20describe%20temporal%20changes.%20Taking%0Ainspiration%20from%20%27Chain-of-Thought%20reasoning%27%2C%20we%20demonstrate%20that%20performance%0Aon%20the%20CXR%20VQA%20task%20can%20be%20improved%20by%20grounding%20the%20answer%20generator%20module%0Awith%20a%20radiology%20report%20predicted%20for%20the%20same%20CXR.%20In%20our%20approach%2C%20the%20VQA%0Amodel%20is%20divided%20into%20two%20steps%3A%20i%29%20Report%20Generation%20%28RG%29%20and%20ii%29%20Answer%0AGeneration%20%28AG%29.%20Our%20results%20demonstrate%20that%20incorporating%20predicted%20radiology%0Areports%20as%20evidence%20to%20the%20AG%20model%20enhances%20performance%20on%20both%20single-image%0Aand%20image-difference%20questions%2C%20achieving%20state-of-the-art%20results%20on%20the%0AMedical-Diff-VQA%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16624v1&entry.124074799=Read"},
{"title": "PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for\n  Face-Voice Association", "author": "Abdul Hannan and Muhammad Arslan Manzoor and Shah Nawaz and Muhammad Irzam Liaqat and Markus Schedl and Mubashir Noman", "abstract": "  We study the task of learning association between faces and voices, which is\ngaining interest in the multimodal community lately. These methods suffer from\nthe deliberate crafting of negative mining procedures as well as the reliance\non the distant margin parameter. These issues are addressed by learning a joint\nembedding space in which orthogonality constraints are applied to the fused\nembeddings of faces and voices. However, embedding spaces of faces and voices\npossess different characteristics and require spaces to be aligned before\nfusing them. To this end, we propose a method that accurately aligns the\nembedding spaces and fuses them with an enhanced gated fusion thereby improving\nthe performance of face-voice association. Extensive experiments on the\nVoxCeleb dataset reveals the merits of the proposed approach.\n", "link": "http://arxiv.org/abs/2505.17002v1", "date": "2025-05-22", "relevancy": 2.4817, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAEFF%3A%20Precise%20Alignment%20and%20Enhanced%20Gated%20Feature%20Fusion%20for%0A%20%20Face-Voice%20Association&body=Title%3A%20PAEFF%3A%20Precise%20Alignment%20and%20Enhanced%20Gated%20Feature%20Fusion%20for%0A%20%20Face-Voice%20Association%0AAuthor%3A%20Abdul%20Hannan%20and%20Muhammad%20Arslan%20Manzoor%20and%20Shah%20Nawaz%20and%20Muhammad%20Irzam%20Liaqat%20and%20Markus%20Schedl%20and%20Mubashir%20Noman%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20learning%20association%20between%20faces%20and%20voices%2C%20which%20is%0Againing%20interest%20in%20the%20multimodal%20community%20lately.%20These%20methods%20suffer%20from%0Athe%20deliberate%20crafting%20of%20negative%20mining%20procedures%20as%20well%20as%20the%20reliance%0Aon%20the%20distant%20margin%20parameter.%20These%20issues%20are%20addressed%20by%20learning%20a%20joint%0Aembedding%20space%20in%20which%20orthogonality%20constraints%20are%20applied%20to%20the%20fused%0Aembeddings%20of%20faces%20and%20voices.%20However%2C%20embedding%20spaces%20of%20faces%20and%20voices%0Apossess%20different%20characteristics%20and%20require%20spaces%20to%20be%20aligned%20before%0Afusing%20them.%20To%20this%20end%2C%20we%20propose%20a%20method%20that%20accurately%20aligns%20the%0Aembedding%20spaces%20and%20fuses%20them%20with%20an%20enhanced%20gated%20fusion%20thereby%20improving%0Athe%20performance%20of%20face-voice%20association.%20Extensive%20experiments%20on%20the%0AVoxCeleb%20dataset%20reveals%20the%20merits%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAEFF%253A%2520Precise%2520Alignment%2520and%2520Enhanced%2520Gated%2520Feature%2520Fusion%2520for%250A%2520%2520Face-Voice%2520Association%26entry.906535625%3DAbdul%2520Hannan%2520and%2520Muhammad%2520Arslan%2520Manzoor%2520and%2520Shah%2520Nawaz%2520and%2520Muhammad%2520Irzam%2520Liaqat%2520and%2520Markus%2520Schedl%2520and%2520Mubashir%2520Noman%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520learning%2520association%2520between%2520faces%2520and%2520voices%252C%2520which%2520is%250Againing%2520interest%2520in%2520the%2520multimodal%2520community%2520lately.%2520These%2520methods%2520suffer%2520from%250Athe%2520deliberate%2520crafting%2520of%2520negative%2520mining%2520procedures%2520as%2520well%2520as%2520the%2520reliance%250Aon%2520the%2520distant%2520margin%2520parameter.%2520These%2520issues%2520are%2520addressed%2520by%2520learning%2520a%2520joint%250Aembedding%2520space%2520in%2520which%2520orthogonality%2520constraints%2520are%2520applied%2520to%2520the%2520fused%250Aembeddings%2520of%2520faces%2520and%2520voices.%2520However%252C%2520embedding%2520spaces%2520of%2520faces%2520and%2520voices%250Apossess%2520different%2520characteristics%2520and%2520require%2520spaces%2520to%2520be%2520aligned%2520before%250Afusing%2520them.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520method%2520that%2520accurately%2520aligns%2520the%250Aembedding%2520spaces%2520and%2520fuses%2520them%2520with%2520an%2520enhanced%2520gated%2520fusion%2520thereby%2520improving%250Athe%2520performance%2520of%2520face-voice%2520association.%2520Extensive%2520experiments%2520on%2520the%250AVoxCeleb%2520dataset%2520reveals%2520the%2520merits%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAEFF%3A%20Precise%20Alignment%20and%20Enhanced%20Gated%20Feature%20Fusion%20for%0A%20%20Face-Voice%20Association&entry.906535625=Abdul%20Hannan%20and%20Muhammad%20Arslan%20Manzoor%20and%20Shah%20Nawaz%20and%20Muhammad%20Irzam%20Liaqat%20and%20Markus%20Schedl%20and%20Mubashir%20Noman&entry.1292438233=%20%20We%20study%20the%20task%20of%20learning%20association%20between%20faces%20and%20voices%2C%20which%20is%0Againing%20interest%20in%20the%20multimodal%20community%20lately.%20These%20methods%20suffer%20from%0Athe%20deliberate%20crafting%20of%20negative%20mining%20procedures%20as%20well%20as%20the%20reliance%0Aon%20the%20distant%20margin%20parameter.%20These%20issues%20are%20addressed%20by%20learning%20a%20joint%0Aembedding%20space%20in%20which%20orthogonality%20constraints%20are%20applied%20to%20the%20fused%0Aembeddings%20of%20faces%20and%20voices.%20However%2C%20embedding%20spaces%20of%20faces%20and%20voices%0Apossess%20different%20characteristics%20and%20require%20spaces%20to%20be%20aligned%20before%0Afusing%20them.%20To%20this%20end%2C%20we%20propose%20a%20method%20that%20accurately%20aligns%20the%0Aembedding%20spaces%20and%20fuses%20them%20with%20an%20enhanced%20gated%20fusion%20thereby%20improving%0Athe%20performance%20of%20face-voice%20association.%20Extensive%20experiments%20on%20the%0AVoxCeleb%20dataset%20reveals%20the%20merits%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17002v1&entry.124074799=Read"},
{"title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning", "author": "Mingyang Liu and Gabriele Farina and Asuman Ozdaglar", "abstract": "  Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks.\n", "link": "http://arxiv.org/abs/2505.16984v1", "date": "2025-05-22", "relevancy": 2.4768, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UFT%3A%20Unifying%20Supervised%20and%20Reinforcement%20Fine-Tuning&body=Title%3A%20UFT%3A%20Unifying%20Supervised%20and%20Reinforcement%20Fine-Tuning%0AAuthor%3A%20Mingyang%20Liu%20and%20Gabriele%20Farina%20and%20Asuman%20Ozdaglar%0AAbstract%3A%20%20%20Post-training%20has%20demonstrated%20its%20importance%20in%20enhancing%20the%20reasoning%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20The%20primary%20post-training%20methods%0Acan%20be%20categorized%20into%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%0Afine-tuning%20%28RFT%29.%20SFT%20is%20efficient%20and%20well-suited%20for%20small%20language%20models%2C%0Abut%20it%20may%20lead%20to%20overfitting%20and%20limit%20the%20reasoning%20abilities%20of%20larger%0Amodels.%20In%20contrast%2C%20RFT%20generally%20yields%20better%20generalization%20but%20depends%0Aheavily%20on%20the%20strength%20of%20the%20base%20model.%20To%20address%20the%20limitations%20of%20SFT%0Aand%20RFT%2C%20we%20propose%20Unified%20Fine-Tuning%20%28UFT%29%2C%20a%20novel%20post-training%20paradigm%0Athat%20unifies%20SFT%20and%20RFT%20into%20a%20single%2C%20integrated%20process.%20UFT%20enables%20the%0Amodel%20to%20effectively%20explore%20solutions%20while%20incorporating%20informative%0Asupervision%20signals%2C%20bridging%20the%20gap%20between%20memorizing%20and%20thinking%0Aunderlying%20existing%20methods.%20Notably%2C%20UFT%20outperforms%20both%20SFT%20and%20RFT%20in%0Ageneral%2C%20regardless%20of%20model%20sizes.%20Furthermore%2C%20we%20theoretically%20prove%20that%0AUFT%20breaks%20RFT%27s%20inherent%20exponential%20sample%20complexity%20bottleneck%2C%20showing%20for%0Athe%20first%20time%20that%20unified%20training%20can%20exponentially%20accelerate%20convergence%0Aon%20long-horizon%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUFT%253A%2520Unifying%2520Supervised%2520and%2520Reinforcement%2520Fine-Tuning%26entry.906535625%3DMingyang%2520Liu%2520and%2520Gabriele%2520Farina%2520and%2520Asuman%2520Ozdaglar%26entry.1292438233%3D%2520%2520Post-training%2520has%2520demonstrated%2520its%2520importance%2520in%2520enhancing%2520the%2520reasoning%250Acapabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520primary%2520post-training%2520methods%250Acan%2520be%2520categorized%2520into%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%250Afine-tuning%2520%2528RFT%2529.%2520SFT%2520is%2520efficient%2520and%2520well-suited%2520for%2520small%2520language%2520models%252C%250Abut%2520it%2520may%2520lead%2520to%2520overfitting%2520and%2520limit%2520the%2520reasoning%2520abilities%2520of%2520larger%250Amodels.%2520In%2520contrast%252C%2520RFT%2520generally%2520yields%2520better%2520generalization%2520but%2520depends%250Aheavily%2520on%2520the%2520strength%2520of%2520the%2520base%2520model.%2520To%2520address%2520the%2520limitations%2520of%2520SFT%250Aand%2520RFT%252C%2520we%2520propose%2520Unified%2520Fine-Tuning%2520%2528UFT%2529%252C%2520a%2520novel%2520post-training%2520paradigm%250Athat%2520unifies%2520SFT%2520and%2520RFT%2520into%2520a%2520single%252C%2520integrated%2520process.%2520UFT%2520enables%2520the%250Amodel%2520to%2520effectively%2520explore%2520solutions%2520while%2520incorporating%2520informative%250Asupervision%2520signals%252C%2520bridging%2520the%2520gap%2520between%2520memorizing%2520and%2520thinking%250Aunderlying%2520existing%2520methods.%2520Notably%252C%2520UFT%2520outperforms%2520both%2520SFT%2520and%2520RFT%2520in%250Ageneral%252C%2520regardless%2520of%2520model%2520sizes.%2520Furthermore%252C%2520we%2520theoretically%2520prove%2520that%250AUFT%2520breaks%2520RFT%2527s%2520inherent%2520exponential%2520sample%2520complexity%2520bottleneck%252C%2520showing%2520for%250Athe%2520first%2520time%2520that%2520unified%2520training%2520can%2520exponentially%2520accelerate%2520convergence%250Aon%2520long-horizon%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UFT%3A%20Unifying%20Supervised%20and%20Reinforcement%20Fine-Tuning&entry.906535625=Mingyang%20Liu%20and%20Gabriele%20Farina%20and%20Asuman%20Ozdaglar&entry.1292438233=%20%20Post-training%20has%20demonstrated%20its%20importance%20in%20enhancing%20the%20reasoning%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20The%20primary%20post-training%20methods%0Acan%20be%20categorized%20into%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%0Afine-tuning%20%28RFT%29.%20SFT%20is%20efficient%20and%20well-suited%20for%20small%20language%20models%2C%0Abut%20it%20may%20lead%20to%20overfitting%20and%20limit%20the%20reasoning%20abilities%20of%20larger%0Amodels.%20In%20contrast%2C%20RFT%20generally%20yields%20better%20generalization%20but%20depends%0Aheavily%20on%20the%20strength%20of%20the%20base%20model.%20To%20address%20the%20limitations%20of%20SFT%0Aand%20RFT%2C%20we%20propose%20Unified%20Fine-Tuning%20%28UFT%29%2C%20a%20novel%20post-training%20paradigm%0Athat%20unifies%20SFT%20and%20RFT%20into%20a%20single%2C%20integrated%20process.%20UFT%20enables%20the%0Amodel%20to%20effectively%20explore%20solutions%20while%20incorporating%20informative%0Asupervision%20signals%2C%20bridging%20the%20gap%20between%20memorizing%20and%20thinking%0Aunderlying%20existing%20methods.%20Notably%2C%20UFT%20outperforms%20both%20SFT%20and%20RFT%20in%0Ageneral%2C%20regardless%20of%20model%20sizes.%20Furthermore%2C%20we%20theoretically%20prove%20that%0AUFT%20breaks%20RFT%27s%20inherent%20exponential%20sample%20complexity%20bottleneck%2C%20showing%20for%0Athe%20first%20time%20that%20unified%20training%20can%20exponentially%20accelerate%20convergence%0Aon%20long-horizon%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16984v1&entry.124074799=Read"},
{"title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal\n  Large Language Models", "author": "Runsen Xu and Weiyao Wang and Hao Tang and Xingyu Chen and Xiaodong Wang and Fu-Jen Chu and Dahua Lin and Matt Feiszli and Kevin J. Liang", "abstract": "  Multi-modal large language models (MLLMs) have rapidly advanced in visual\ntasks, yet their spatial understanding remains limited to single images,\nleaving them ill-suited for robotics and other real-world applications that\nrequire multi-frame reasoning. In this paper, we propose a framework to equip\nMLLMs with robust multi-frame spatial understanding by integrating depth\nperception, visual correspondence, and dynamic perception. Central to our\napproach is the MultiSPA dataset, a novel, large-scale collection of more than\n27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we\nintroduce a comprehensive benchmark that tests a wide spectrum of spatial tasks\nunder uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves\nsignificant gains over baselines and proprietary systems, demonstrating\nscalable, generalizable multi-frame reasoning. We further observe multi-task\nbenefits and early indications of emergent capabilities in challenging\nscenarios, and showcase how our model can serve as a multi-frame reward\nannotator for robotics.\n", "link": "http://arxiv.org/abs/2505.17015v1", "date": "2025-05-22", "relevancy": 2.47, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6331}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-SpatialMLLM%3A%20Multi-Frame%20Spatial%20Understanding%20with%20Multi-Modal%0A%20%20Large%20Language%20Models&body=Title%3A%20Multi-SpatialMLLM%3A%20Multi-Frame%20Spatial%20Understanding%20with%20Multi-Modal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Runsen%20Xu%20and%20Weiyao%20Wang%20and%20Hao%20Tang%20and%20Xingyu%20Chen%20and%20Xiaodong%20Wang%20and%20Fu-Jen%20Chu%20and%20Dahua%20Lin%20and%20Matt%20Feiszli%20and%20Kevin%20J.%20Liang%0AAbstract%3A%20%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20rapidly%20advanced%20in%20visual%0Atasks%2C%20yet%20their%20spatial%20understanding%20remains%20limited%20to%20single%20images%2C%0Aleaving%20them%20ill-suited%20for%20robotics%20and%20other%20real-world%20applications%20that%0Arequire%20multi-frame%20reasoning.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20to%20equip%0AMLLMs%20with%20robust%20multi-frame%20spatial%20understanding%20by%20integrating%20depth%0Aperception%2C%20visual%20correspondence%2C%20and%20dynamic%20perception.%20Central%20to%20our%0Aapproach%20is%20the%20MultiSPA%20dataset%2C%20a%20novel%2C%20large-scale%20collection%20of%20more%20than%0A27%20million%20samples%20spanning%20diverse%203D%20and%204D%20scenes.%20Alongside%20MultiSPA%2C%20we%0Aintroduce%20a%20comprehensive%20benchmark%20that%20tests%20a%20wide%20spectrum%20of%20spatial%20tasks%0Aunder%20uniform%20metrics.%20Our%20resulting%20model%2C%20Multi-SpatialMLLM%2C%20achieves%0Asignificant%20gains%20over%20baselines%20and%20proprietary%20systems%2C%20demonstrating%0Ascalable%2C%20generalizable%20multi-frame%20reasoning.%20We%20further%20observe%20multi-task%0Abenefits%20and%20early%20indications%20of%20emergent%20capabilities%20in%20challenging%0Ascenarios%2C%20and%20showcase%20how%20our%20model%20can%20serve%20as%20a%20multi-frame%20reward%0Aannotator%20for%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-SpatialMLLM%253A%2520Multi-Frame%2520Spatial%2520Understanding%2520with%2520Multi-Modal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DRunsen%2520Xu%2520and%2520Weiyao%2520Wang%2520and%2520Hao%2520Tang%2520and%2520Xingyu%2520Chen%2520and%2520Xiaodong%2520Wang%2520and%2520Fu-Jen%2520Chu%2520and%2520Dahua%2520Lin%2520and%2520Matt%2520Feiszli%2520and%2520Kevin%2520J.%2520Liang%26entry.1292438233%3D%2520%2520Multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520rapidly%2520advanced%2520in%2520visual%250Atasks%252C%2520yet%2520their%2520spatial%2520understanding%2520remains%2520limited%2520to%2520single%2520images%252C%250Aleaving%2520them%2520ill-suited%2520for%2520robotics%2520and%2520other%2520real-world%2520applications%2520that%250Arequire%2520multi-frame%2520reasoning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520framework%2520to%2520equip%250AMLLMs%2520with%2520robust%2520multi-frame%2520spatial%2520understanding%2520by%2520integrating%2520depth%250Aperception%252C%2520visual%2520correspondence%252C%2520and%2520dynamic%2520perception.%2520Central%2520to%2520our%250Aapproach%2520is%2520the%2520MultiSPA%2520dataset%252C%2520a%2520novel%252C%2520large-scale%2520collection%2520of%2520more%2520than%250A27%2520million%2520samples%2520spanning%2520diverse%25203D%2520and%25204D%2520scenes.%2520Alongside%2520MultiSPA%252C%2520we%250Aintroduce%2520a%2520comprehensive%2520benchmark%2520that%2520tests%2520a%2520wide%2520spectrum%2520of%2520spatial%2520tasks%250Aunder%2520uniform%2520metrics.%2520Our%2520resulting%2520model%252C%2520Multi-SpatialMLLM%252C%2520achieves%250Asignificant%2520gains%2520over%2520baselines%2520and%2520proprietary%2520systems%252C%2520demonstrating%250Ascalable%252C%2520generalizable%2520multi-frame%2520reasoning.%2520We%2520further%2520observe%2520multi-task%250Abenefits%2520and%2520early%2520indications%2520of%2520emergent%2520capabilities%2520in%2520challenging%250Ascenarios%252C%2520and%2520showcase%2520how%2520our%2520model%2520can%2520serve%2520as%2520a%2520multi-frame%2520reward%250Aannotator%2520for%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-SpatialMLLM%3A%20Multi-Frame%20Spatial%20Understanding%20with%20Multi-Modal%0A%20%20Large%20Language%20Models&entry.906535625=Runsen%20Xu%20and%20Weiyao%20Wang%20and%20Hao%20Tang%20and%20Xingyu%20Chen%20and%20Xiaodong%20Wang%20and%20Fu-Jen%20Chu%20and%20Dahua%20Lin%20and%20Matt%20Feiszli%20and%20Kevin%20J.%20Liang&entry.1292438233=%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20rapidly%20advanced%20in%20visual%0Atasks%2C%20yet%20their%20spatial%20understanding%20remains%20limited%20to%20single%20images%2C%0Aleaving%20them%20ill-suited%20for%20robotics%20and%20other%20real-world%20applications%20that%0Arequire%20multi-frame%20reasoning.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20to%20equip%0AMLLMs%20with%20robust%20multi-frame%20spatial%20understanding%20by%20integrating%20depth%0Aperception%2C%20visual%20correspondence%2C%20and%20dynamic%20perception.%20Central%20to%20our%0Aapproach%20is%20the%20MultiSPA%20dataset%2C%20a%20novel%2C%20large-scale%20collection%20of%20more%20than%0A27%20million%20samples%20spanning%20diverse%203D%20and%204D%20scenes.%20Alongside%20MultiSPA%2C%20we%0Aintroduce%20a%20comprehensive%20benchmark%20that%20tests%20a%20wide%20spectrum%20of%20spatial%20tasks%0Aunder%20uniform%20metrics.%20Our%20resulting%20model%2C%20Multi-SpatialMLLM%2C%20achieves%0Asignificant%20gains%20over%20baselines%20and%20proprietary%20systems%2C%20demonstrating%0Ascalable%2C%20generalizable%20multi-frame%20reasoning.%20We%20further%20observe%20multi-task%0Abenefits%20and%20early%20indications%20of%20emergent%20capabilities%20in%20challenging%0Ascenarios%2C%20and%20showcase%20how%20our%20model%20can%20serve%20as%20a%20multi-frame%20reward%0Aannotator%20for%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17015v1&entry.124074799=Read"},
{"title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning\n  Self-Reflection", "author": "Jiaqi Li and Xinyi Dong and Yang Liu and Zhizhuo Yang and Quansen Wang and Xiaobo Wang and SongChun Zhu and Zixia Jia and Zilong Zheng", "abstract": "  We present a novel pipeline, ReflectEvo, to demonstrate that small language\nmodels (SLMs) can enhance meta introspection through reflection learning. This\nprocess iteratively generates self-reflection for self-training, fostering a\ncontinuous and self-evolving process. Leveraging this pipeline, we construct\nReflectEvo-460k, a large-scale, comprehensive, self-generated reflection\ndataset with broadened instructions and diverse multi-domain tasks. Building\nupon this dataset, we demonstrate the effectiveness of reflection learning to\nimprove SLMs' reasoning abilities using SFT and DPO with remarkable\nperformance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral\nfrom 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the\nreasoning capability of the three prominent open-sourced models on BIG-bench\nwithout distillation from superior models or fine-grained human annotation. We\nfurther conduct a deeper analysis of the high quality of self-generated\nreflections and their impact on error localization and correction. Our work\nhighlights the potential of continuously enhancing the reasoning performance of\nSLMs through iterative reflection learning in the long run.\n", "link": "http://arxiv.org/abs/2505.16475v1", "date": "2025-05-22", "relevancy": 2.4693, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReflectEvo%3A%20Improving%20Meta%20Introspection%20of%20Small%20LLMs%20by%20Learning%0A%20%20Self-Reflection&body=Title%3A%20ReflectEvo%3A%20Improving%20Meta%20Introspection%20of%20Small%20LLMs%20by%20Learning%0A%20%20Self-Reflection%0AAuthor%3A%20Jiaqi%20Li%20and%20Xinyi%20Dong%20and%20Yang%20Liu%20and%20Zhizhuo%20Yang%20and%20Quansen%20Wang%20and%20Xiaobo%20Wang%20and%20SongChun%20Zhu%20and%20Zixia%20Jia%20and%20Zilong%20Zheng%0AAbstract%3A%20%20%20We%20present%20a%20novel%20pipeline%2C%20ReflectEvo%2C%20to%20demonstrate%20that%20small%20language%0Amodels%20%28SLMs%29%20can%20enhance%20meta%20introspection%20through%20reflection%20learning.%20This%0Aprocess%20iteratively%20generates%20self-reflection%20for%20self-training%2C%20fostering%20a%0Acontinuous%20and%20self-evolving%20process.%20Leveraging%20this%20pipeline%2C%20we%20construct%0AReflectEvo-460k%2C%20a%20large-scale%2C%20comprehensive%2C%20self-generated%20reflection%0Adataset%20with%20broadened%20instructions%20and%20diverse%20multi-domain%20tasks.%20Building%0Aupon%20this%20dataset%2C%20we%20demonstrate%20the%20effectiveness%20of%20reflection%20learning%20to%0Aimprove%20SLMs%27%20reasoning%20abilities%20using%20SFT%20and%20DPO%20with%20remarkable%0Aperformance%2C%20substantially%20boosting%20Llama-3%20from%2052.4%25%20to%2071.2%25%20and%20Mistral%0Afrom%2044.4%25%20to%2071.1%25.%20It%20validates%20that%20ReflectEvo%20can%20rival%20or%20even%20surpass%20the%0Areasoning%20capability%20of%20the%20three%20prominent%20open-sourced%20models%20on%20BIG-bench%0Awithout%20distillation%20from%20superior%20models%20or%20fine-grained%20human%20annotation.%20We%0Afurther%20conduct%20a%20deeper%20analysis%20of%20the%20high%20quality%20of%20self-generated%0Areflections%20and%20their%20impact%20on%20error%20localization%20and%20correction.%20Our%20work%0Ahighlights%20the%20potential%20of%20continuously%20enhancing%20the%20reasoning%20performance%20of%0ASLMs%20through%20iterative%20reflection%20learning%20in%20the%20long%20run.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflectEvo%253A%2520Improving%2520Meta%2520Introspection%2520of%2520Small%2520LLMs%2520by%2520Learning%250A%2520%2520Self-Reflection%26entry.906535625%3DJiaqi%2520Li%2520and%2520Xinyi%2520Dong%2520and%2520Yang%2520Liu%2520and%2520Zhizhuo%2520Yang%2520and%2520Quansen%2520Wang%2520and%2520Xiaobo%2520Wang%2520and%2520SongChun%2520Zhu%2520and%2520Zixia%2520Jia%2520and%2520Zilong%2520Zheng%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520pipeline%252C%2520ReflectEvo%252C%2520to%2520demonstrate%2520that%2520small%2520language%250Amodels%2520%2528SLMs%2529%2520can%2520enhance%2520meta%2520introspection%2520through%2520reflection%2520learning.%2520This%250Aprocess%2520iteratively%2520generates%2520self-reflection%2520for%2520self-training%252C%2520fostering%2520a%250Acontinuous%2520and%2520self-evolving%2520process.%2520Leveraging%2520this%2520pipeline%252C%2520we%2520construct%250AReflectEvo-460k%252C%2520a%2520large-scale%252C%2520comprehensive%252C%2520self-generated%2520reflection%250Adataset%2520with%2520broadened%2520instructions%2520and%2520diverse%2520multi-domain%2520tasks.%2520Building%250Aupon%2520this%2520dataset%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520reflection%2520learning%2520to%250Aimprove%2520SLMs%2527%2520reasoning%2520abilities%2520using%2520SFT%2520and%2520DPO%2520with%2520remarkable%250Aperformance%252C%2520substantially%2520boosting%2520Llama-3%2520from%252052.4%2525%2520to%252071.2%2525%2520and%2520Mistral%250Afrom%252044.4%2525%2520to%252071.1%2525.%2520It%2520validates%2520that%2520ReflectEvo%2520can%2520rival%2520or%2520even%2520surpass%2520the%250Areasoning%2520capability%2520of%2520the%2520three%2520prominent%2520open-sourced%2520models%2520on%2520BIG-bench%250Awithout%2520distillation%2520from%2520superior%2520models%2520or%2520fine-grained%2520human%2520annotation.%2520We%250Afurther%2520conduct%2520a%2520deeper%2520analysis%2520of%2520the%2520high%2520quality%2520of%2520self-generated%250Areflections%2520and%2520their%2520impact%2520on%2520error%2520localization%2520and%2520correction.%2520Our%2520work%250Ahighlights%2520the%2520potential%2520of%2520continuously%2520enhancing%2520the%2520reasoning%2520performance%2520of%250ASLMs%2520through%2520iterative%2520reflection%2520learning%2520in%2520the%2520long%2520run.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReflectEvo%3A%20Improving%20Meta%20Introspection%20of%20Small%20LLMs%20by%20Learning%0A%20%20Self-Reflection&entry.906535625=Jiaqi%20Li%20and%20Xinyi%20Dong%20and%20Yang%20Liu%20and%20Zhizhuo%20Yang%20and%20Quansen%20Wang%20and%20Xiaobo%20Wang%20and%20SongChun%20Zhu%20and%20Zixia%20Jia%20and%20Zilong%20Zheng&entry.1292438233=%20%20We%20present%20a%20novel%20pipeline%2C%20ReflectEvo%2C%20to%20demonstrate%20that%20small%20language%0Amodels%20%28SLMs%29%20can%20enhance%20meta%20introspection%20through%20reflection%20learning.%20This%0Aprocess%20iteratively%20generates%20self-reflection%20for%20self-training%2C%20fostering%20a%0Acontinuous%20and%20self-evolving%20process.%20Leveraging%20this%20pipeline%2C%20we%20construct%0AReflectEvo-460k%2C%20a%20large-scale%2C%20comprehensive%2C%20self-generated%20reflection%0Adataset%20with%20broadened%20instructions%20and%20diverse%20multi-domain%20tasks.%20Building%0Aupon%20this%20dataset%2C%20we%20demonstrate%20the%20effectiveness%20of%20reflection%20learning%20to%0Aimprove%20SLMs%27%20reasoning%20abilities%20using%20SFT%20and%20DPO%20with%20remarkable%0Aperformance%2C%20substantially%20boosting%20Llama-3%20from%2052.4%25%20to%2071.2%25%20and%20Mistral%0Afrom%2044.4%25%20to%2071.1%25.%20It%20validates%20that%20ReflectEvo%20can%20rival%20or%20even%20surpass%20the%0Areasoning%20capability%20of%20the%20three%20prominent%20open-sourced%20models%20on%20BIG-bench%0Awithout%20distillation%20from%20superior%20models%20or%20fine-grained%20human%20annotation.%20We%0Afurther%20conduct%20a%20deeper%20analysis%20of%20the%20high%20quality%20of%20self-generated%0Areflections%20and%20their%20impact%20on%20error%20localization%20and%20correction.%20Our%20work%0Ahighlights%20the%20potential%20of%20continuously%20enhancing%20the%20reasoning%20performance%20of%0ASLMs%20through%20iterative%20reflection%20learning%20in%20the%20long%20run.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16475v1&entry.124074799=Read"},
{"title": "SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment\n  for LLMs' Mathematical Problem Solving", "author": "Yujie Hou and Ting Zhang and Mei Wang and Xuetao Ma and Hu Huang", "abstract": "  Large Language Models have achieved remarkable results on a variety of\nmathematical benchmarks. However, concerns remain as to whether these successes\nreflect genuine mathematical reasoning or superficial pattern recognition.\nCommon evaluation metrics, such as final answer accuracy, fail to disentangle\nthe underlying competencies involved, offering limited diagnostic value. To\naddress these limitations, we introduce SMART: a Self-Generating and\nSelf-Validating Multi-Dimensional Assessment Framework. SMART decomposes\nmathematical problem solving into four distinct dimensions: understanding,\nreasoning, arithmetic, and reflection \\& refinement. Each dimension is\nevaluated independently through tailored tasks, enabling interpretable and\nfine-grained analysis of LLM behavior. Crucially, SMART integrates an automated\nself-generating and self-validating mechanism to produce and verify benchmark\ndata, ensuring both scalability and reliability. We apply SMART to 21\nstate-of-the-art open- and closed-source LLMs, uncovering significant\ndiscrepancies in their abilities across different dimensions. Our findings\ndemonstrate the inadequacy of final answer accuracy as a sole metric and\nmotivate a new holistic metric to better capture true problem-solving\ncapabilities. Code and benchmarks will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2505.16646v1", "date": "2025-05-22", "relevancy": 2.4646, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART%3A%20Self-Generating%20and%20Self-Validating%20Multi-Dimensional%20Assessment%0A%20%20for%20LLMs%27%20Mathematical%20Problem%20Solving&body=Title%3A%20SMART%3A%20Self-Generating%20and%20Self-Validating%20Multi-Dimensional%20Assessment%0A%20%20for%20LLMs%27%20Mathematical%20Problem%20Solving%0AAuthor%3A%20Yujie%20Hou%20and%20Ting%20Zhang%20and%20Mei%20Wang%20and%20Xuetao%20Ma%20and%20Hu%20Huang%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20achieved%20remarkable%20results%20on%20a%20variety%20of%0Amathematical%20benchmarks.%20However%2C%20concerns%20remain%20as%20to%20whether%20these%20successes%0Areflect%20genuine%20mathematical%20reasoning%20or%20superficial%20pattern%20recognition.%0ACommon%20evaluation%20metrics%2C%20such%20as%20final%20answer%20accuracy%2C%20fail%20to%20disentangle%0Athe%20underlying%20competencies%20involved%2C%20offering%20limited%20diagnostic%20value.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20SMART%3A%20a%20Self-Generating%20and%0ASelf-Validating%20Multi-Dimensional%20Assessment%20Framework.%20SMART%20decomposes%0Amathematical%20problem%20solving%20into%20four%20distinct%20dimensions%3A%20understanding%2C%0Areasoning%2C%20arithmetic%2C%20and%20reflection%20%5C%26%20refinement.%20Each%20dimension%20is%0Aevaluated%20independently%20through%20tailored%20tasks%2C%20enabling%20interpretable%20and%0Afine-grained%20analysis%20of%20LLM%20behavior.%20Crucially%2C%20SMART%20integrates%20an%20automated%0Aself-generating%20and%20self-validating%20mechanism%20to%20produce%20and%20verify%20benchmark%0Adata%2C%20ensuring%20both%20scalability%20and%20reliability.%20We%20apply%20SMART%20to%2021%0Astate-of-the-art%20open-%20and%20closed-source%20LLMs%2C%20uncovering%20significant%0Adiscrepancies%20in%20their%20abilities%20across%20different%20dimensions.%20Our%20findings%0Ademonstrate%20the%20inadequacy%20of%20final%20answer%20accuracy%20as%20a%20sole%20metric%20and%0Amotivate%20a%20new%20holistic%20metric%20to%20better%20capture%20true%20problem-solving%0Acapabilities.%20Code%20and%20benchmarks%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART%253A%2520Self-Generating%2520and%2520Self-Validating%2520Multi-Dimensional%2520Assessment%250A%2520%2520for%2520LLMs%2527%2520Mathematical%2520Problem%2520Solving%26entry.906535625%3DYujie%2520Hou%2520and%2520Ting%2520Zhang%2520and%2520Mei%2520Wang%2520and%2520Xuetao%2520Ma%2520and%2520Hu%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520achieved%2520remarkable%2520results%2520on%2520a%2520variety%2520of%250Amathematical%2520benchmarks.%2520However%252C%2520concerns%2520remain%2520as%2520to%2520whether%2520these%2520successes%250Areflect%2520genuine%2520mathematical%2520reasoning%2520or%2520superficial%2520pattern%2520recognition.%250ACommon%2520evaluation%2520metrics%252C%2520such%2520as%2520final%2520answer%2520accuracy%252C%2520fail%2520to%2520disentangle%250Athe%2520underlying%2520competencies%2520involved%252C%2520offering%2520limited%2520diagnostic%2520value.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520SMART%253A%2520a%2520Self-Generating%2520and%250ASelf-Validating%2520Multi-Dimensional%2520Assessment%2520Framework.%2520SMART%2520decomposes%250Amathematical%2520problem%2520solving%2520into%2520four%2520distinct%2520dimensions%253A%2520understanding%252C%250Areasoning%252C%2520arithmetic%252C%2520and%2520reflection%2520%255C%2526%2520refinement.%2520Each%2520dimension%2520is%250Aevaluated%2520independently%2520through%2520tailored%2520tasks%252C%2520enabling%2520interpretable%2520and%250Afine-grained%2520analysis%2520of%2520LLM%2520behavior.%2520Crucially%252C%2520SMART%2520integrates%2520an%2520automated%250Aself-generating%2520and%2520self-validating%2520mechanism%2520to%2520produce%2520and%2520verify%2520benchmark%250Adata%252C%2520ensuring%2520both%2520scalability%2520and%2520reliability.%2520We%2520apply%2520SMART%2520to%252021%250Astate-of-the-art%2520open-%2520and%2520closed-source%2520LLMs%252C%2520uncovering%2520significant%250Adiscrepancies%2520in%2520their%2520abilities%2520across%2520different%2520dimensions.%2520Our%2520findings%250Ademonstrate%2520the%2520inadequacy%2520of%2520final%2520answer%2520accuracy%2520as%2520a%2520sole%2520metric%2520and%250Amotivate%2520a%2520new%2520holistic%2520metric%2520to%2520better%2520capture%2520true%2520problem-solving%250Acapabilities.%2520Code%2520and%2520benchmarks%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART%3A%20Self-Generating%20and%20Self-Validating%20Multi-Dimensional%20Assessment%0A%20%20for%20LLMs%27%20Mathematical%20Problem%20Solving&entry.906535625=Yujie%20Hou%20and%20Ting%20Zhang%20and%20Mei%20Wang%20and%20Xuetao%20Ma%20and%20Hu%20Huang&entry.1292438233=%20%20Large%20Language%20Models%20have%20achieved%20remarkable%20results%20on%20a%20variety%20of%0Amathematical%20benchmarks.%20However%2C%20concerns%20remain%20as%20to%20whether%20these%20successes%0Areflect%20genuine%20mathematical%20reasoning%20or%20superficial%20pattern%20recognition.%0ACommon%20evaluation%20metrics%2C%20such%20as%20final%20answer%20accuracy%2C%20fail%20to%20disentangle%0Athe%20underlying%20competencies%20involved%2C%20offering%20limited%20diagnostic%20value.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20SMART%3A%20a%20Self-Generating%20and%0ASelf-Validating%20Multi-Dimensional%20Assessment%20Framework.%20SMART%20decomposes%0Amathematical%20problem%20solving%20into%20four%20distinct%20dimensions%3A%20understanding%2C%0Areasoning%2C%20arithmetic%2C%20and%20reflection%20%5C%26%20refinement.%20Each%20dimension%20is%0Aevaluated%20independently%20through%20tailored%20tasks%2C%20enabling%20interpretable%20and%0Afine-grained%20analysis%20of%20LLM%20behavior.%20Crucially%2C%20SMART%20integrates%20an%20automated%0Aself-generating%20and%20self-validating%20mechanism%20to%20produce%20and%20verify%20benchmark%0Adata%2C%20ensuring%20both%20scalability%20and%20reliability.%20We%20apply%20SMART%20to%2021%0Astate-of-the-art%20open-%20and%20closed-source%20LLMs%2C%20uncovering%20significant%0Adiscrepancies%20in%20their%20abilities%20across%20different%20dimensions.%20Our%20findings%0Ademonstrate%20the%20inadequacy%20of%20final%20answer%20accuracy%20as%20a%20sole%20metric%20and%0Amotivate%20a%20new%20holistic%20metric%20to%20better%20capture%20true%20problem-solving%0Acapabilities.%20Code%20and%20benchmarks%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16646v1&entry.124074799=Read"},
{"title": "ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart\n  Understanding", "author": "Yifan Wu and Lutao Yan and Leixian Shen and Yinan Mei and Jiannan Wang and Yuyu Luo", "abstract": "  The emergence of Multi-modal Large Language Models (MLLMs) presents new\nopportunities for chart understanding. However, due to the fine-grained nature\nof these tasks, applying MLLMs typically requires large, high-quality datasets\nfor task-specific fine-tuning, leading to high data collection and training\ncosts. To address this, we propose ChartCards, a unified chart-metadata\ngeneration framework for multi-task chart understanding. ChartCards\nsystematically synthesizes various chart information, including data tables,\nvisualization code, visual elements, and multi-dimensional semantic captions.\nBy structuring this information into organized metadata, ChartCards enables a\nsingle chart to support multiple downstream tasks, such as text-to-chart\nretrieval, chart summarization, chart-to-table conversion, chart description,\nand chart question answering. Using ChartCards, we further construct MetaChart,\na large-scale high-quality dataset containing 10,862 data tables, 85K charts,\nand 170 K high-quality chart captions. We validate the dataset through\nqualitative crowdsourcing evaluations and quantitative fine-tuning experiments\nacross various chart understanding tasks. Fine-tuning six different models on\nMetaChart resulted in an average performance improvement of 5% across all\ntasks. The most notable improvements are seen in text-to-chart retrieval and\nchart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements\nof 17% and 28%, respectively.\n", "link": "http://arxiv.org/abs/2505.15046v2", "date": "2025-05-22", "relevancy": 2.4631, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartCards%3A%20A%20Chart-Metadata%20Generation%20Framework%20for%20Multi-Task%20Chart%0A%20%20Understanding&body=Title%3A%20ChartCards%3A%20A%20Chart-Metadata%20Generation%20Framework%20for%20Multi-Task%20Chart%0A%20%20Understanding%0AAuthor%3A%20Yifan%20Wu%20and%20Lutao%20Yan%20and%20Leixian%20Shen%20and%20Yinan%20Mei%20and%20Jiannan%20Wang%20and%20Yuyu%20Luo%0AAbstract%3A%20%20%20The%20emergence%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20presents%20new%0Aopportunities%20for%20chart%20understanding.%20However%2C%20due%20to%20the%20fine-grained%20nature%0Aof%20these%20tasks%2C%20applying%20MLLMs%20typically%20requires%20large%2C%20high-quality%20datasets%0Afor%20task-specific%20fine-tuning%2C%20leading%20to%20high%20data%20collection%20and%20training%0Acosts.%20To%20address%20this%2C%20we%20propose%20ChartCards%2C%20a%20unified%20chart-metadata%0Ageneration%20framework%20for%20multi-task%20chart%20understanding.%20ChartCards%0Asystematically%20synthesizes%20various%20chart%20information%2C%20including%20data%20tables%2C%0Avisualization%20code%2C%20visual%20elements%2C%20and%20multi-dimensional%20semantic%20captions.%0ABy%20structuring%20this%20information%20into%20organized%20metadata%2C%20ChartCards%20enables%20a%0Asingle%20chart%20to%20support%20multiple%20downstream%20tasks%2C%20such%20as%20text-to-chart%0Aretrieval%2C%20chart%20summarization%2C%20chart-to-table%20conversion%2C%20chart%20description%2C%0Aand%20chart%20question%20answering.%20Using%20ChartCards%2C%20we%20further%20construct%20MetaChart%2C%0Aa%20large-scale%20high-quality%20dataset%20containing%2010%2C862%20data%20tables%2C%2085K%20charts%2C%0Aand%20170%20K%20high-quality%20chart%20captions.%20We%20validate%20the%20dataset%20through%0Aqualitative%20crowdsourcing%20evaluations%20and%20quantitative%20fine-tuning%20experiments%0Aacross%20various%20chart%20understanding%20tasks.%20Fine-tuning%20six%20different%20models%20on%0AMetaChart%20resulted%20in%20an%20average%20performance%20improvement%20of%205%25%20across%20all%0Atasks.%20The%20most%20notable%20improvements%20are%20seen%20in%20text-to-chart%20retrieval%20and%0Achart-to-table%20tasks%2C%20with%20Long-CLIP%20and%20Llama%203.2-11B%20achieving%20improvements%0Aof%2017%25%20and%2028%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartCards%253A%2520A%2520Chart-Metadata%2520Generation%2520Framework%2520for%2520Multi-Task%2520Chart%250A%2520%2520Understanding%26entry.906535625%3DYifan%2520Wu%2520and%2520Lutao%2520Yan%2520and%2520Leixian%2520Shen%2520and%2520Yinan%2520Mei%2520and%2520Jiannan%2520Wang%2520and%2520Yuyu%2520Luo%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520presents%2520new%250Aopportunities%2520for%2520chart%2520understanding.%2520However%252C%2520due%2520to%2520the%2520fine-grained%2520nature%250Aof%2520these%2520tasks%252C%2520applying%2520MLLMs%2520typically%2520requires%2520large%252C%2520high-quality%2520datasets%250Afor%2520task-specific%2520fine-tuning%252C%2520leading%2520to%2520high%2520data%2520collection%2520and%2520training%250Acosts.%2520To%2520address%2520this%252C%2520we%2520propose%2520ChartCards%252C%2520a%2520unified%2520chart-metadata%250Ageneration%2520framework%2520for%2520multi-task%2520chart%2520understanding.%2520ChartCards%250Asystematically%2520synthesizes%2520various%2520chart%2520information%252C%2520including%2520data%2520tables%252C%250Avisualization%2520code%252C%2520visual%2520elements%252C%2520and%2520multi-dimensional%2520semantic%2520captions.%250ABy%2520structuring%2520this%2520information%2520into%2520organized%2520metadata%252C%2520ChartCards%2520enables%2520a%250Asingle%2520chart%2520to%2520support%2520multiple%2520downstream%2520tasks%252C%2520such%2520as%2520text-to-chart%250Aretrieval%252C%2520chart%2520summarization%252C%2520chart-to-table%2520conversion%252C%2520chart%2520description%252C%250Aand%2520chart%2520question%2520answering.%2520Using%2520ChartCards%252C%2520we%2520further%2520construct%2520MetaChart%252C%250Aa%2520large-scale%2520high-quality%2520dataset%2520containing%252010%252C862%2520data%2520tables%252C%252085K%2520charts%252C%250Aand%2520170%2520K%2520high-quality%2520chart%2520captions.%2520We%2520validate%2520the%2520dataset%2520through%250Aqualitative%2520crowdsourcing%2520evaluations%2520and%2520quantitative%2520fine-tuning%2520experiments%250Aacross%2520various%2520chart%2520understanding%2520tasks.%2520Fine-tuning%2520six%2520different%2520models%2520on%250AMetaChart%2520resulted%2520in%2520an%2520average%2520performance%2520improvement%2520of%25205%2525%2520across%2520all%250Atasks.%2520The%2520most%2520notable%2520improvements%2520are%2520seen%2520in%2520text-to-chart%2520retrieval%2520and%250Achart-to-table%2520tasks%252C%2520with%2520Long-CLIP%2520and%2520Llama%25203.2-11B%2520achieving%2520improvements%250Aof%252017%2525%2520and%252028%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartCards%3A%20A%20Chart-Metadata%20Generation%20Framework%20for%20Multi-Task%20Chart%0A%20%20Understanding&entry.906535625=Yifan%20Wu%20and%20Lutao%20Yan%20and%20Leixian%20Shen%20and%20Yinan%20Mei%20and%20Jiannan%20Wang%20and%20Yuyu%20Luo&entry.1292438233=%20%20The%20emergence%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20presents%20new%0Aopportunities%20for%20chart%20understanding.%20However%2C%20due%20to%20the%20fine-grained%20nature%0Aof%20these%20tasks%2C%20applying%20MLLMs%20typically%20requires%20large%2C%20high-quality%20datasets%0Afor%20task-specific%20fine-tuning%2C%20leading%20to%20high%20data%20collection%20and%20training%0Acosts.%20To%20address%20this%2C%20we%20propose%20ChartCards%2C%20a%20unified%20chart-metadata%0Ageneration%20framework%20for%20multi-task%20chart%20understanding.%20ChartCards%0Asystematically%20synthesizes%20various%20chart%20information%2C%20including%20data%20tables%2C%0Avisualization%20code%2C%20visual%20elements%2C%20and%20multi-dimensional%20semantic%20captions.%0ABy%20structuring%20this%20information%20into%20organized%20metadata%2C%20ChartCards%20enables%20a%0Asingle%20chart%20to%20support%20multiple%20downstream%20tasks%2C%20such%20as%20text-to-chart%0Aretrieval%2C%20chart%20summarization%2C%20chart-to-table%20conversion%2C%20chart%20description%2C%0Aand%20chart%20question%20answering.%20Using%20ChartCards%2C%20we%20further%20construct%20MetaChart%2C%0Aa%20large-scale%20high-quality%20dataset%20containing%2010%2C862%20data%20tables%2C%2085K%20charts%2C%0Aand%20170%20K%20high-quality%20chart%20captions.%20We%20validate%20the%20dataset%20through%0Aqualitative%20crowdsourcing%20evaluations%20and%20quantitative%20fine-tuning%20experiments%0Aacross%20various%20chart%20understanding%20tasks.%20Fine-tuning%20six%20different%20models%20on%0AMetaChart%20resulted%20in%20an%20average%20performance%20improvement%20of%205%25%20across%20all%0Atasks.%20The%20most%20notable%20improvements%20are%20seen%20in%20text-to-chart%20retrieval%20and%0Achart-to-table%20tasks%2C%20with%20Long-CLIP%20and%20Llama%203.2-11B%20achieving%20improvements%0Aof%2017%25%20and%2028%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15046v2&entry.124074799=Read"},
{"title": "Unsupervised Prompting for Graph Neural Networks", "author": "Peyman Baghershahi and Sourav Medya", "abstract": "  Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to\naddress the semantic gap between pre-training and fine-tuning steps. However,\nexisting GNN prompting methods rely on labeled data and involve lightweight\nfine-tuning for downstream tasks. Meanwhile, in-context learning methods for\nLarge Language Models (LLMs) have shown promising performance with no parameter\nupdating and no or minimal labeled data. Inspired by these approaches, in this\nwork, we first introduce a challenging problem setup to evaluate GNN prompting\nmethods. This setup encourages a prompting function to enhance a pre-trained\nGNN's generalization to a target dataset under covariate shift without updating\nthe GNN's parameters and with no labeled data. Next, we propose a fully\nunsupervised prompting method based on consistency regularization through\npseudo-labeling. We use two regularization techniques to align the prompted\ngraphs' distribution with the original data and reduce biased predictions.\nThrough extensive experiments under our problem setting, we demonstrate that\nour unsupervised approach outperforms the state-of-the-art prompting methods\nthat have access to labels.\n", "link": "http://arxiv.org/abs/2505.16903v1", "date": "2025-05-22", "relevancy": 2.4613, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5257}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4845}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Prompting%20for%20Graph%20Neural%20Networks&body=Title%3A%20Unsupervised%20Prompting%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Peyman%20Baghershahi%20and%20Sourav%20Medya%0AAbstract%3A%20%20%20Prompt%20tuning%20methods%20for%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20popular%20to%0Aaddress%20the%20semantic%20gap%20between%20pre-training%20and%20fine-tuning%20steps.%20However%2C%0Aexisting%20GNN%20prompting%20methods%20rely%20on%20labeled%20data%20and%20involve%20lightweight%0Afine-tuning%20for%20downstream%20tasks.%20Meanwhile%2C%20in-context%20learning%20methods%20for%0ALarge%20Language%20Models%20%28LLMs%29%20have%20shown%20promising%20performance%20with%20no%20parameter%0Aupdating%20and%20no%20or%20minimal%20labeled%20data.%20Inspired%20by%20these%20approaches%2C%20in%20this%0Awork%2C%20we%20first%20introduce%20a%20challenging%20problem%20setup%20to%20evaluate%20GNN%20prompting%0Amethods.%20This%20setup%20encourages%20a%20prompting%20function%20to%20enhance%20a%20pre-trained%0AGNN%27s%20generalization%20to%20a%20target%20dataset%20under%20covariate%20shift%20without%20updating%0Athe%20GNN%27s%20parameters%20and%20with%20no%20labeled%20data.%20Next%2C%20we%20propose%20a%20fully%0Aunsupervised%20prompting%20method%20based%20on%20consistency%20regularization%20through%0Apseudo-labeling.%20We%20use%20two%20regularization%20techniques%20to%20align%20the%20prompted%0Agraphs%27%20distribution%20with%20the%20original%20data%20and%20reduce%20biased%20predictions.%0AThrough%20extensive%20experiments%20under%20our%20problem%20setting%2C%20we%20demonstrate%20that%0Aour%20unsupervised%20approach%20outperforms%20the%20state-of-the-art%20prompting%20methods%0Athat%20have%20access%20to%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Prompting%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DPeyman%2520Baghershahi%2520and%2520Sourav%2520Medya%26entry.1292438233%3D%2520%2520Prompt%2520tuning%2520methods%2520for%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520popular%2520to%250Aaddress%2520the%2520semantic%2520gap%2520between%2520pre-training%2520and%2520fine-tuning%2520steps.%2520However%252C%250Aexisting%2520GNN%2520prompting%2520methods%2520rely%2520on%2520labeled%2520data%2520and%2520involve%2520lightweight%250Afine-tuning%2520for%2520downstream%2520tasks.%2520Meanwhile%252C%2520in-context%2520learning%2520methods%2520for%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520promising%2520performance%2520with%2520no%2520parameter%250Aupdating%2520and%2520no%2520or%2520minimal%2520labeled%2520data.%2520Inspired%2520by%2520these%2520approaches%252C%2520in%2520this%250Awork%252C%2520we%2520first%2520introduce%2520a%2520challenging%2520problem%2520setup%2520to%2520evaluate%2520GNN%2520prompting%250Amethods.%2520This%2520setup%2520encourages%2520a%2520prompting%2520function%2520to%2520enhance%2520a%2520pre-trained%250AGNN%2527s%2520generalization%2520to%2520a%2520target%2520dataset%2520under%2520covariate%2520shift%2520without%2520updating%250Athe%2520GNN%2527s%2520parameters%2520and%2520with%2520no%2520labeled%2520data.%2520Next%252C%2520we%2520propose%2520a%2520fully%250Aunsupervised%2520prompting%2520method%2520based%2520on%2520consistency%2520regularization%2520through%250Apseudo-labeling.%2520We%2520use%2520two%2520regularization%2520techniques%2520to%2520align%2520the%2520prompted%250Agraphs%2527%2520distribution%2520with%2520the%2520original%2520data%2520and%2520reduce%2520biased%2520predictions.%250AThrough%2520extensive%2520experiments%2520under%2520our%2520problem%2520setting%252C%2520we%2520demonstrate%2520that%250Aour%2520unsupervised%2520approach%2520outperforms%2520the%2520state-of-the-art%2520prompting%2520methods%250Athat%2520have%2520access%2520to%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Prompting%20for%20Graph%20Neural%20Networks&entry.906535625=Peyman%20Baghershahi%20and%20Sourav%20Medya&entry.1292438233=%20%20Prompt%20tuning%20methods%20for%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20popular%20to%0Aaddress%20the%20semantic%20gap%20between%20pre-training%20and%20fine-tuning%20steps.%20However%2C%0Aexisting%20GNN%20prompting%20methods%20rely%20on%20labeled%20data%20and%20involve%20lightweight%0Afine-tuning%20for%20downstream%20tasks.%20Meanwhile%2C%20in-context%20learning%20methods%20for%0ALarge%20Language%20Models%20%28LLMs%29%20have%20shown%20promising%20performance%20with%20no%20parameter%0Aupdating%20and%20no%20or%20minimal%20labeled%20data.%20Inspired%20by%20these%20approaches%2C%20in%20this%0Awork%2C%20we%20first%20introduce%20a%20challenging%20problem%20setup%20to%20evaluate%20GNN%20prompting%0Amethods.%20This%20setup%20encourages%20a%20prompting%20function%20to%20enhance%20a%20pre-trained%0AGNN%27s%20generalization%20to%20a%20target%20dataset%20under%20covariate%20shift%20without%20updating%0Athe%20GNN%27s%20parameters%20and%20with%20no%20labeled%20data.%20Next%2C%20we%20propose%20a%20fully%0Aunsupervised%20prompting%20method%20based%20on%20consistency%20regularization%20through%0Apseudo-labeling.%20We%20use%20two%20regularization%20techniques%20to%20align%20the%20prompted%0Agraphs%27%20distribution%20with%20the%20original%20data%20and%20reduce%20biased%20predictions.%0AThrough%20extensive%20experiments%20under%20our%20problem%20setting%2C%20we%20demonstrate%20that%0Aour%20unsupervised%20approach%20outperforms%20the%20state-of-the-art%20prompting%20methods%0Athat%20have%20access%20to%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16903v1&entry.124074799=Read"},
{"title": "Learning Genomic Structure from $k$-mers", "author": "Filip Thor and Carl Nettelblad", "abstract": "  Sequencing a genome to determine an individual's DNA produces an enormous\nnumber of short nucleotide subsequences known as reads, which must be\nreassembled to reconstruct the full genome. We present a method for analyzing\nthis type of data using contrastive learning, in which an encoder model is\ntrained to produce embeddings that cluster together sequences from the same\ngenomic region. The sequential nature of genomic regions is preserved in the\nform of trajectories through this embedding space. Trained solely to reflect\nthe structure of the genome, the resulting model provides a general\nrepresentation of $k$-mer sequences, suitable for a range of downstream tasks\ninvolving read data. We apply our framework to learn the structure of the $E.\\\ncoli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read\nmapping and identification of structural variations. Furthermore, we illustrate\nthe potential of using this type of model for metagenomic species\nidentification. We show how incorporating a domain-specific noise model can\nenhance embedding robustness, and how a supervised contrastive learning setting\ncan be adopted when a linear reference genome is available, by introducing a\ndistance thresholding parameter $\\Gamma$. The model can also be trained fully\nself-supervised on read data, enabling analysis without the need to construct a\nfull genome assembly using specialized algorithms. Small prediction heads based\non a pre-trained embedding are shown to perform on par with BWA-aln, the\ncurrent gold standard approach for aDNA mapping, in terms of accuracy and\nruntime for short genomes. Given the method's favorable scaling properties with\nrespect to total genome size, inference using our approach is highly promising\nfor metagenomic applications and for mapping to genomes comparable in size to\nthe human genome.\n", "link": "http://arxiv.org/abs/2505.16680v1", "date": "2025-05-22", "relevancy": 2.4566, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Genomic%20Structure%20from%20%24k%24-mers&body=Title%3A%20Learning%20Genomic%20Structure%20from%20%24k%24-mers%0AAuthor%3A%20Filip%20Thor%20and%20Carl%20Nettelblad%0AAbstract%3A%20%20%20Sequencing%20a%20genome%20to%20determine%20an%20individual%27s%20DNA%20produces%20an%20enormous%0Anumber%20of%20short%20nucleotide%20subsequences%20known%20as%20reads%2C%20which%20must%20be%0Areassembled%20to%20reconstruct%20the%20full%20genome.%20We%20present%20a%20method%20for%20analyzing%0Athis%20type%20of%20data%20using%20contrastive%20learning%2C%20in%20which%20an%20encoder%20model%20is%0Atrained%20to%20produce%20embeddings%20that%20cluster%20together%20sequences%20from%20the%20same%0Agenomic%20region.%20The%20sequential%20nature%20of%20genomic%20regions%20is%20preserved%20in%20the%0Aform%20of%20trajectories%20through%20this%20embedding%20space.%20Trained%20solely%20to%20reflect%0Athe%20structure%20of%20the%20genome%2C%20the%20resulting%20model%20provides%20a%20general%0Arepresentation%20of%20%24k%24-mer%20sequences%2C%20suitable%20for%20a%20range%20of%20downstream%20tasks%0Ainvolving%20read%20data.%20We%20apply%20our%20framework%20to%20learn%20the%20structure%20of%20the%20%24E.%5C%0Acoli%24%20genome%2C%20and%20demonstrate%20its%20use%20in%20simulated%20ancient%20DNA%20%28aDNA%29%20read%0Amapping%20and%20identification%20of%20structural%20variations.%20Furthermore%2C%20we%20illustrate%0Athe%20potential%20of%20using%20this%20type%20of%20model%20for%20metagenomic%20species%0Aidentification.%20We%20show%20how%20incorporating%20a%20domain-specific%20noise%20model%20can%0Aenhance%20embedding%20robustness%2C%20and%20how%20a%20supervised%20contrastive%20learning%20setting%0Acan%20be%20adopted%20when%20a%20linear%20reference%20genome%20is%20available%2C%20by%20introducing%20a%0Adistance%20thresholding%20parameter%20%24%5CGamma%24.%20The%20model%20can%20also%20be%20trained%20fully%0Aself-supervised%20on%20read%20data%2C%20enabling%20analysis%20without%20the%20need%20to%20construct%20a%0Afull%20genome%20assembly%20using%20specialized%20algorithms.%20Small%20prediction%20heads%20based%0Aon%20a%20pre-trained%20embedding%20are%20shown%20to%20perform%20on%20par%20with%20BWA-aln%2C%20the%0Acurrent%20gold%20standard%20approach%20for%20aDNA%20mapping%2C%20in%20terms%20of%20accuracy%20and%0Aruntime%20for%20short%20genomes.%20Given%20the%20method%27s%20favorable%20scaling%20properties%20with%0Arespect%20to%20total%20genome%20size%2C%20inference%20using%20our%20approach%20is%20highly%20promising%0Afor%20metagenomic%20applications%20and%20for%20mapping%20to%20genomes%20comparable%20in%20size%20to%0Athe%20human%20genome.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Genomic%2520Structure%2520from%2520%2524k%2524-mers%26entry.906535625%3DFilip%2520Thor%2520and%2520Carl%2520Nettelblad%26entry.1292438233%3D%2520%2520Sequencing%2520a%2520genome%2520to%2520determine%2520an%2520individual%2527s%2520DNA%2520produces%2520an%2520enormous%250Anumber%2520of%2520short%2520nucleotide%2520subsequences%2520known%2520as%2520reads%252C%2520which%2520must%2520be%250Areassembled%2520to%2520reconstruct%2520the%2520full%2520genome.%2520We%2520present%2520a%2520method%2520for%2520analyzing%250Athis%2520type%2520of%2520data%2520using%2520contrastive%2520learning%252C%2520in%2520which%2520an%2520encoder%2520model%2520is%250Atrained%2520to%2520produce%2520embeddings%2520that%2520cluster%2520together%2520sequences%2520from%2520the%2520same%250Agenomic%2520region.%2520The%2520sequential%2520nature%2520of%2520genomic%2520regions%2520is%2520preserved%2520in%2520the%250Aform%2520of%2520trajectories%2520through%2520this%2520embedding%2520space.%2520Trained%2520solely%2520to%2520reflect%250Athe%2520structure%2520of%2520the%2520genome%252C%2520the%2520resulting%2520model%2520provides%2520a%2520general%250Arepresentation%2520of%2520%2524k%2524-mer%2520sequences%252C%2520suitable%2520for%2520a%2520range%2520of%2520downstream%2520tasks%250Ainvolving%2520read%2520data.%2520We%2520apply%2520our%2520framework%2520to%2520learn%2520the%2520structure%2520of%2520the%2520%2524E.%255C%250Acoli%2524%2520genome%252C%2520and%2520demonstrate%2520its%2520use%2520in%2520simulated%2520ancient%2520DNA%2520%2528aDNA%2529%2520read%250Amapping%2520and%2520identification%2520of%2520structural%2520variations.%2520Furthermore%252C%2520we%2520illustrate%250Athe%2520potential%2520of%2520using%2520this%2520type%2520of%2520model%2520for%2520metagenomic%2520species%250Aidentification.%2520We%2520show%2520how%2520incorporating%2520a%2520domain-specific%2520noise%2520model%2520can%250Aenhance%2520embedding%2520robustness%252C%2520and%2520how%2520a%2520supervised%2520contrastive%2520learning%2520setting%250Acan%2520be%2520adopted%2520when%2520a%2520linear%2520reference%2520genome%2520is%2520available%252C%2520by%2520introducing%2520a%250Adistance%2520thresholding%2520parameter%2520%2524%255CGamma%2524.%2520The%2520model%2520can%2520also%2520be%2520trained%2520fully%250Aself-supervised%2520on%2520read%2520data%252C%2520enabling%2520analysis%2520without%2520the%2520need%2520to%2520construct%2520a%250Afull%2520genome%2520assembly%2520using%2520specialized%2520algorithms.%2520Small%2520prediction%2520heads%2520based%250Aon%2520a%2520pre-trained%2520embedding%2520are%2520shown%2520to%2520perform%2520on%2520par%2520with%2520BWA-aln%252C%2520the%250Acurrent%2520gold%2520standard%2520approach%2520for%2520aDNA%2520mapping%252C%2520in%2520terms%2520of%2520accuracy%2520and%250Aruntime%2520for%2520short%2520genomes.%2520Given%2520the%2520method%2527s%2520favorable%2520scaling%2520properties%2520with%250Arespect%2520to%2520total%2520genome%2520size%252C%2520inference%2520using%2520our%2520approach%2520is%2520highly%2520promising%250Afor%2520metagenomic%2520applications%2520and%2520for%2520mapping%2520to%2520genomes%2520comparable%2520in%2520size%2520to%250Athe%2520human%2520genome.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Genomic%20Structure%20from%20%24k%24-mers&entry.906535625=Filip%20Thor%20and%20Carl%20Nettelblad&entry.1292438233=%20%20Sequencing%20a%20genome%20to%20determine%20an%20individual%27s%20DNA%20produces%20an%20enormous%0Anumber%20of%20short%20nucleotide%20subsequences%20known%20as%20reads%2C%20which%20must%20be%0Areassembled%20to%20reconstruct%20the%20full%20genome.%20We%20present%20a%20method%20for%20analyzing%0Athis%20type%20of%20data%20using%20contrastive%20learning%2C%20in%20which%20an%20encoder%20model%20is%0Atrained%20to%20produce%20embeddings%20that%20cluster%20together%20sequences%20from%20the%20same%0Agenomic%20region.%20The%20sequential%20nature%20of%20genomic%20regions%20is%20preserved%20in%20the%0Aform%20of%20trajectories%20through%20this%20embedding%20space.%20Trained%20solely%20to%20reflect%0Athe%20structure%20of%20the%20genome%2C%20the%20resulting%20model%20provides%20a%20general%0Arepresentation%20of%20%24k%24-mer%20sequences%2C%20suitable%20for%20a%20range%20of%20downstream%20tasks%0Ainvolving%20read%20data.%20We%20apply%20our%20framework%20to%20learn%20the%20structure%20of%20the%20%24E.%5C%0Acoli%24%20genome%2C%20and%20demonstrate%20its%20use%20in%20simulated%20ancient%20DNA%20%28aDNA%29%20read%0Amapping%20and%20identification%20of%20structural%20variations.%20Furthermore%2C%20we%20illustrate%0Athe%20potential%20of%20using%20this%20type%20of%20model%20for%20metagenomic%20species%0Aidentification.%20We%20show%20how%20incorporating%20a%20domain-specific%20noise%20model%20can%0Aenhance%20embedding%20robustness%2C%20and%20how%20a%20supervised%20contrastive%20learning%20setting%0Acan%20be%20adopted%20when%20a%20linear%20reference%20genome%20is%20available%2C%20by%20introducing%20a%0Adistance%20thresholding%20parameter%20%24%5CGamma%24.%20The%20model%20can%20also%20be%20trained%20fully%0Aself-supervised%20on%20read%20data%2C%20enabling%20analysis%20without%20the%20need%20to%20construct%20a%0Afull%20genome%20assembly%20using%20specialized%20algorithms.%20Small%20prediction%20heads%20based%0Aon%20a%20pre-trained%20embedding%20are%20shown%20to%20perform%20on%20par%20with%20BWA-aln%2C%20the%0Acurrent%20gold%20standard%20approach%20for%20aDNA%20mapping%2C%20in%20terms%20of%20accuracy%20and%0Aruntime%20for%20short%20genomes.%20Given%20the%20method%27s%20favorable%20scaling%20properties%20with%0Arespect%20to%20total%20genome%20size%2C%20inference%20using%20our%20approach%20is%20highly%20promising%0Afor%20metagenomic%20applications%20and%20for%20mapping%20to%20genomes%20comparable%20in%20size%20to%0Athe%20human%20genome.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16680v1&entry.124074799=Read"},
{"title": "DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating\n  Semantic Understanding of Dongba Pictograms", "author": "Xiaojun Bi and Shuo Li and Junyao Xing and Ziyue Wang and Fuwen Luo and Weizheng Qiao and Lu Han and Ziwei Sun and Peng Li and Yang Liu", "abstract": "  Dongba pictographic is the only pictographic script still in use in the\nworld. Its pictorial ideographic features carry rich cultural and contextual\ninformation. However, due to the lack of relevant datasets, research on\nsemantic understanding of Dongba hieroglyphs has progressed slowly. To this\nend, we constructed \\textbf{DongbaMIE} - the first dataset focusing on\nmultimodal information extraction of Dongba pictographs. The dataset consists\nof images of Dongba hieroglyphic characters and their corresponding semantic\nannotations in Chinese. It contains 23,530 sentence-level and 2,539\nparagraph-level high-quality text-image pairs. The annotations cover four\nsemantic dimensions: object, action, relation and attribute. Systematic\nevaluation of mainstream multimodal large language models shows that the models\nare difficult to perform information extraction of Dongba hieroglyphs\nefficiently under zero-shot and few-shot learning. Although supervised\nfine-tuning can improve the performance, accurate extraction of complex\nsemantics is still a great challenge at present.\n", "link": "http://arxiv.org/abs/2503.03644v4", "date": "2025-05-22", "relevancy": 2.4507, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DongbaMIE%3A%20A%20Multimodal%20Information%20Extraction%20Dataset%20for%20Evaluating%0A%20%20Semantic%20Understanding%20of%20Dongba%20Pictograms&body=Title%3A%20DongbaMIE%3A%20A%20Multimodal%20Information%20Extraction%20Dataset%20for%20Evaluating%0A%20%20Semantic%20Understanding%20of%20Dongba%20Pictograms%0AAuthor%3A%20Xiaojun%20Bi%20and%20Shuo%20Li%20and%20Junyao%20Xing%20and%20Ziyue%20Wang%20and%20Fuwen%20Luo%20and%20Weizheng%20Qiao%20and%20Lu%20Han%20and%20Ziwei%20Sun%20and%20Peng%20Li%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Dongba%20pictographic%20is%20the%20only%20pictographic%20script%20still%20in%20use%20in%20the%0Aworld.%20Its%20pictorial%20ideographic%20features%20carry%20rich%20cultural%20and%20contextual%0Ainformation.%20However%2C%20due%20to%20the%20lack%20of%20relevant%20datasets%2C%20research%20on%0Asemantic%20understanding%20of%20Dongba%20hieroglyphs%20has%20progressed%20slowly.%20To%20this%0Aend%2C%20we%20constructed%20%5Ctextbf%7BDongbaMIE%7D%20-%20the%20first%20dataset%20focusing%20on%0Amultimodal%20information%20extraction%20of%20Dongba%20pictographs.%20The%20dataset%20consists%0Aof%20images%20of%20Dongba%20hieroglyphic%20characters%20and%20their%20corresponding%20semantic%0Aannotations%20in%20Chinese.%20It%20contains%2023%2C530%20sentence-level%20and%202%2C539%0Aparagraph-level%20high-quality%20text-image%20pairs.%20The%20annotations%20cover%20four%0Asemantic%20dimensions%3A%20object%2C%20action%2C%20relation%20and%20attribute.%20Systematic%0Aevaluation%20of%20mainstream%20multimodal%20large%20language%20models%20shows%20that%20the%20models%0Aare%20difficult%20to%20perform%20information%20extraction%20of%20Dongba%20hieroglyphs%0Aefficiently%20under%20zero-shot%20and%20few-shot%20learning.%20Although%20supervised%0Afine-tuning%20can%20improve%20the%20performance%2C%20accurate%20extraction%20of%20complex%0Asemantics%20is%20still%20a%20great%20challenge%20at%20present.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03644v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDongbaMIE%253A%2520A%2520Multimodal%2520Information%2520Extraction%2520Dataset%2520for%2520Evaluating%250A%2520%2520Semantic%2520Understanding%2520of%2520Dongba%2520Pictograms%26entry.906535625%3DXiaojun%2520Bi%2520and%2520Shuo%2520Li%2520and%2520Junyao%2520Xing%2520and%2520Ziyue%2520Wang%2520and%2520Fuwen%2520Luo%2520and%2520Weizheng%2520Qiao%2520and%2520Lu%2520Han%2520and%2520Ziwei%2520Sun%2520and%2520Peng%2520Li%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Dongba%2520pictographic%2520is%2520the%2520only%2520pictographic%2520script%2520still%2520in%2520use%2520in%2520the%250Aworld.%2520Its%2520pictorial%2520ideographic%2520features%2520carry%2520rich%2520cultural%2520and%2520contextual%250Ainformation.%2520However%252C%2520due%2520to%2520the%2520lack%2520of%2520relevant%2520datasets%252C%2520research%2520on%250Asemantic%2520understanding%2520of%2520Dongba%2520hieroglyphs%2520has%2520progressed%2520slowly.%2520To%2520this%250Aend%252C%2520we%2520constructed%2520%255Ctextbf%257BDongbaMIE%257D%2520-%2520the%2520first%2520dataset%2520focusing%2520on%250Amultimodal%2520information%2520extraction%2520of%2520Dongba%2520pictographs.%2520The%2520dataset%2520consists%250Aof%2520images%2520of%2520Dongba%2520hieroglyphic%2520characters%2520and%2520their%2520corresponding%2520semantic%250Aannotations%2520in%2520Chinese.%2520It%2520contains%252023%252C530%2520sentence-level%2520and%25202%252C539%250Aparagraph-level%2520high-quality%2520text-image%2520pairs.%2520The%2520annotations%2520cover%2520four%250Asemantic%2520dimensions%253A%2520object%252C%2520action%252C%2520relation%2520and%2520attribute.%2520Systematic%250Aevaluation%2520of%2520mainstream%2520multimodal%2520large%2520language%2520models%2520shows%2520that%2520the%2520models%250Aare%2520difficult%2520to%2520perform%2520information%2520extraction%2520of%2520Dongba%2520hieroglyphs%250Aefficiently%2520under%2520zero-shot%2520and%2520few-shot%2520learning.%2520Although%2520supervised%250Afine-tuning%2520can%2520improve%2520the%2520performance%252C%2520accurate%2520extraction%2520of%2520complex%250Asemantics%2520is%2520still%2520a%2520great%2520challenge%2520at%2520present.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03644v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DongbaMIE%3A%20A%20Multimodal%20Information%20Extraction%20Dataset%20for%20Evaluating%0A%20%20Semantic%20Understanding%20of%20Dongba%20Pictograms&entry.906535625=Xiaojun%20Bi%20and%20Shuo%20Li%20and%20Junyao%20Xing%20and%20Ziyue%20Wang%20and%20Fuwen%20Luo%20and%20Weizheng%20Qiao%20and%20Lu%20Han%20and%20Ziwei%20Sun%20and%20Peng%20Li%20and%20Yang%20Liu&entry.1292438233=%20%20Dongba%20pictographic%20is%20the%20only%20pictographic%20script%20still%20in%20use%20in%20the%0Aworld.%20Its%20pictorial%20ideographic%20features%20carry%20rich%20cultural%20and%20contextual%0Ainformation.%20However%2C%20due%20to%20the%20lack%20of%20relevant%20datasets%2C%20research%20on%0Asemantic%20understanding%20of%20Dongba%20hieroglyphs%20has%20progressed%20slowly.%20To%20this%0Aend%2C%20we%20constructed%20%5Ctextbf%7BDongbaMIE%7D%20-%20the%20first%20dataset%20focusing%20on%0Amultimodal%20information%20extraction%20of%20Dongba%20pictographs.%20The%20dataset%20consists%0Aof%20images%20of%20Dongba%20hieroglyphic%20characters%20and%20their%20corresponding%20semantic%0Aannotations%20in%20Chinese.%20It%20contains%2023%2C530%20sentence-level%20and%202%2C539%0Aparagraph-level%20high-quality%20text-image%20pairs.%20The%20annotations%20cover%20four%0Asemantic%20dimensions%3A%20object%2C%20action%2C%20relation%20and%20attribute.%20Systematic%0Aevaluation%20of%20mainstream%20multimodal%20large%20language%20models%20shows%20that%20the%20models%0Aare%20difficult%20to%20perform%20information%20extraction%20of%20Dongba%20hieroglyphs%0Aefficiently%20under%20zero-shot%20and%20few-shot%20learning.%20Although%20supervised%0Afine-tuning%20can%20improve%20the%20performance%2C%20accurate%20extraction%20of%20complex%0Asemantics%20is%20still%20a%20great%20challenge%20at%20present.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03644v4&entry.124074799=Read"},
{"title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models", "author": "Zihao Cheng and Hongru Wang and Zeming Liu and Yuhang Guo and Yuanfang Guo and Yunhong Wang and Haifeng Wang", "abstract": "  While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.\n", "link": "http://arxiv.org/abs/2505.13176v2", "date": "2025-05-22", "relevancy": 2.4402, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToolSpectrum%20%3A%20Towards%20Personalized%20Tool%20Utilization%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20ToolSpectrum%20%3A%20Towards%20Personalized%20Tool%20Utilization%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Zihao%20Cheng%20and%20Hongru%20Wang%20and%20Zeming%20Liu%20and%20Yuhang%20Guo%20and%20Yuanfang%20Guo%20and%20Yunhong%20Wang%20and%20Haifeng%20Wang%0AAbstract%3A%20%20%20While%20integrating%20external%20tools%20into%20large%20language%20models%20%28LLMs%29%20enhances%0Atheir%20ability%20to%20access%20real-time%20information%20and%20domain-specific%20services%2C%0Aexisting%20approaches%20focus%20narrowly%20on%20functional%20tool%20selection%20following%20user%0Ainstructions%2C%20overlooking%20the%20context-aware%20personalization%20in%20tool%20selection.%0AThis%20oversight%20leads%20to%20suboptimal%20user%20satisfaction%20and%20inefficient%20tool%0Autilization%2C%20particularly%20when%20overlapping%20toolsets%20require%20nuanced%20selection%0Abased%20on%20contextual%20factors.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ToolSpectrum%2C%20a%0Abenchmark%20designed%20to%20evaluate%20LLMs%27%20capabilities%20in%20personalized%20tool%0Autilization.%20Specifically%2C%20we%20formalize%20two%20key%20dimensions%20of%20personalization%2C%0Auser%20profile%20and%20environmental%20factors%2C%20and%20analyze%20their%20individual%20and%0Asynergistic%20impacts%20on%20tool%20utilization.%20Through%20extensive%20experiments%20on%0AToolSpectrum%2C%20we%20demonstrate%20that%20personalized%20tool%20utilization%20significantly%0Aimproves%20user%20experience%20across%20diverse%20scenarios.%20However%2C%20even%0Astate-of-the-art%20LLMs%20exhibit%20the%20limited%20ability%20to%20reason%20jointly%20about%20user%0Aprofiles%20and%20environmental%20factors%2C%20often%20prioritizing%20one%20dimension%20at%20the%0Aexpense%20of%20the%20other.%20Our%20findings%20underscore%20the%20necessity%20of%20context-aware%0Apersonalization%20in%20tool-augmented%20LLMs%20and%20reveal%20critical%20limitations%20for%0Acurrent%20models.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Chengziha0/ToolSpectrum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToolSpectrum%2520%253A%2520Towards%2520Personalized%2520Tool%2520Utilization%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DZihao%2520Cheng%2520and%2520Hongru%2520Wang%2520and%2520Zeming%2520Liu%2520and%2520Yuhang%2520Guo%2520and%2520Yuanfang%2520Guo%2520and%2520Yunhong%2520Wang%2520and%2520Haifeng%2520Wang%26entry.1292438233%3D%2520%2520While%2520integrating%2520external%2520tools%2520into%2520large%2520language%2520models%2520%2528LLMs%2529%2520enhances%250Atheir%2520ability%2520to%2520access%2520real-time%2520information%2520and%2520domain-specific%2520services%252C%250Aexisting%2520approaches%2520focus%2520narrowly%2520on%2520functional%2520tool%2520selection%2520following%2520user%250Ainstructions%252C%2520overlooking%2520the%2520context-aware%2520personalization%2520in%2520tool%2520selection.%250AThis%2520oversight%2520leads%2520to%2520suboptimal%2520user%2520satisfaction%2520and%2520inefficient%2520tool%250Autilization%252C%2520particularly%2520when%2520overlapping%2520toolsets%2520require%2520nuanced%2520selection%250Abased%2520on%2520contextual%2520factors.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520ToolSpectrum%252C%2520a%250Abenchmark%2520designed%2520to%2520evaluate%2520LLMs%2527%2520capabilities%2520in%2520personalized%2520tool%250Autilization.%2520Specifically%252C%2520we%2520formalize%2520two%2520key%2520dimensions%2520of%2520personalization%252C%250Auser%2520profile%2520and%2520environmental%2520factors%252C%2520and%2520analyze%2520their%2520individual%2520and%250Asynergistic%2520impacts%2520on%2520tool%2520utilization.%2520Through%2520extensive%2520experiments%2520on%250AToolSpectrum%252C%2520we%2520demonstrate%2520that%2520personalized%2520tool%2520utilization%2520significantly%250Aimproves%2520user%2520experience%2520across%2520diverse%2520scenarios.%2520However%252C%2520even%250Astate-of-the-art%2520LLMs%2520exhibit%2520the%2520limited%2520ability%2520to%2520reason%2520jointly%2520about%2520user%250Aprofiles%2520and%2520environmental%2520factors%252C%2520often%2520prioritizing%2520one%2520dimension%2520at%2520the%250Aexpense%2520of%2520the%2520other.%2520Our%2520findings%2520underscore%2520the%2520necessity%2520of%2520context-aware%250Apersonalization%2520in%2520tool-augmented%2520LLMs%2520and%2520reveal%2520critical%2520limitations%2520for%250Acurrent%2520models.%2520Our%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/Chengziha0/ToolSpectrum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToolSpectrum%20%3A%20Towards%20Personalized%20Tool%20Utilization%20for%20Large%20Language%0A%20%20Models&entry.906535625=Zihao%20Cheng%20and%20Hongru%20Wang%20and%20Zeming%20Liu%20and%20Yuhang%20Guo%20and%20Yuanfang%20Guo%20and%20Yunhong%20Wang%20and%20Haifeng%20Wang&entry.1292438233=%20%20While%20integrating%20external%20tools%20into%20large%20language%20models%20%28LLMs%29%20enhances%0Atheir%20ability%20to%20access%20real-time%20information%20and%20domain-specific%20services%2C%0Aexisting%20approaches%20focus%20narrowly%20on%20functional%20tool%20selection%20following%20user%0Ainstructions%2C%20overlooking%20the%20context-aware%20personalization%20in%20tool%20selection.%0AThis%20oversight%20leads%20to%20suboptimal%20user%20satisfaction%20and%20inefficient%20tool%0Autilization%2C%20particularly%20when%20overlapping%20toolsets%20require%20nuanced%20selection%0Abased%20on%20contextual%20factors.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ToolSpectrum%2C%20a%0Abenchmark%20designed%20to%20evaluate%20LLMs%27%20capabilities%20in%20personalized%20tool%0Autilization.%20Specifically%2C%20we%20formalize%20two%20key%20dimensions%20of%20personalization%2C%0Auser%20profile%20and%20environmental%20factors%2C%20and%20analyze%20their%20individual%20and%0Asynergistic%20impacts%20on%20tool%20utilization.%20Through%20extensive%20experiments%20on%0AToolSpectrum%2C%20we%20demonstrate%20that%20personalized%20tool%20utilization%20significantly%0Aimproves%20user%20experience%20across%20diverse%20scenarios.%20However%2C%20even%0Astate-of-the-art%20LLMs%20exhibit%20the%20limited%20ability%20to%20reason%20jointly%20about%20user%0Aprofiles%20and%20environmental%20factors%2C%20often%20prioritizing%20one%20dimension%20at%20the%0Aexpense%20of%20the%20other.%20Our%20findings%20underscore%20the%20necessity%20of%20context-aware%0Apersonalization%20in%20tool-augmented%20LLMs%20and%20reveal%20critical%20limitations%20for%0Acurrent%20models.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Chengziha0/ToolSpectrum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13176v2&entry.124074799=Read"},
{"title": "HOFT: Householder Orthogonal Fine-tuning", "author": "Alejandro Moreno Arcas and Albert Sanchis and Jorge Civera and Alfons Juan", "abstract": "  Adaptation of foundation models using low-rank methods is a widespread\napproach. Another way to adapt these models is to employ orthogonal fine-tuning\nmethods, which are less time and memory efficient despite their good\ngeneralization properties. In this work, we propose Householder Orthogonal\nFine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to\nalleviate time and space complexity. Moreover, some theoretical properties of\nthe orthogonal fine-tuning paradigm are explored. From this exploration, Scaled\nHouseholder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are\nevaluated in downstream tasks, namely commonsense reasoning, machine\ntranslation, subject-driven generation and mathematical reasoning. Compared\nwith state-of-the-art adaptation methods, HOFT and SHOFT show comparable or\nbetter results.\n", "link": "http://arxiv.org/abs/2505.16531v1", "date": "2025-05-22", "relevancy": 2.4385, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5136}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4774}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOFT%3A%20Householder%20Orthogonal%20Fine-tuning&body=Title%3A%20HOFT%3A%20Householder%20Orthogonal%20Fine-tuning%0AAuthor%3A%20Alejandro%20Moreno%20Arcas%20and%20Albert%20Sanchis%20and%20Jorge%20Civera%20and%20Alfons%20Juan%0AAbstract%3A%20%20%20Adaptation%20of%20foundation%20models%20using%20low-rank%20methods%20is%20a%20widespread%0Aapproach.%20Another%20way%20to%20adapt%20these%20models%20is%20to%20employ%20orthogonal%20fine-tuning%0Amethods%2C%20which%20are%20less%20time%20and%20memory%20efficient%20despite%20their%20good%0Ageneralization%20properties.%20In%20this%20work%2C%20we%20propose%20Householder%20Orthogonal%0AFine-tuning%20%28HOFT%29%2C%20a%20novel%20orthogonal%20fine-tuning%20method%20that%20aims%20to%0Aalleviate%20time%20and%20space%20complexity.%20Moreover%2C%20some%20theoretical%20properties%20of%0Athe%20orthogonal%20fine-tuning%20paradigm%20are%20explored.%20From%20this%20exploration%2C%20Scaled%0AHouseholder%20Orthogonal%20Fine-tuning%20%28SHOFT%29%20is%20proposed.%20Both%20HOFT%20and%20SHOFT%20are%0Aevaluated%20in%20downstream%20tasks%2C%20namely%20commonsense%20reasoning%2C%20machine%0Atranslation%2C%20subject-driven%20generation%20and%20mathematical%20reasoning.%20Compared%0Awith%20state-of-the-art%20adaptation%20methods%2C%20HOFT%20and%20SHOFT%20show%20comparable%20or%0Abetter%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOFT%253A%2520Householder%2520Orthogonal%2520Fine-tuning%26entry.906535625%3DAlejandro%2520Moreno%2520Arcas%2520and%2520Albert%2520Sanchis%2520and%2520Jorge%2520Civera%2520and%2520Alfons%2520Juan%26entry.1292438233%3D%2520%2520Adaptation%2520of%2520foundation%2520models%2520using%2520low-rank%2520methods%2520is%2520a%2520widespread%250Aapproach.%2520Another%2520way%2520to%2520adapt%2520these%2520models%2520is%2520to%2520employ%2520orthogonal%2520fine-tuning%250Amethods%252C%2520which%2520are%2520less%2520time%2520and%2520memory%2520efficient%2520despite%2520their%2520good%250Ageneralization%2520properties.%2520In%2520this%2520work%252C%2520we%2520propose%2520Householder%2520Orthogonal%250AFine-tuning%2520%2528HOFT%2529%252C%2520a%2520novel%2520orthogonal%2520fine-tuning%2520method%2520that%2520aims%2520to%250Aalleviate%2520time%2520and%2520space%2520complexity.%2520Moreover%252C%2520some%2520theoretical%2520properties%2520of%250Athe%2520orthogonal%2520fine-tuning%2520paradigm%2520are%2520explored.%2520From%2520this%2520exploration%252C%2520Scaled%250AHouseholder%2520Orthogonal%2520Fine-tuning%2520%2528SHOFT%2529%2520is%2520proposed.%2520Both%2520HOFT%2520and%2520SHOFT%2520are%250Aevaluated%2520in%2520downstream%2520tasks%252C%2520namely%2520commonsense%2520reasoning%252C%2520machine%250Atranslation%252C%2520subject-driven%2520generation%2520and%2520mathematical%2520reasoning.%2520Compared%250Awith%2520state-of-the-art%2520adaptation%2520methods%252C%2520HOFT%2520and%2520SHOFT%2520show%2520comparable%2520or%250Abetter%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOFT%3A%20Householder%20Orthogonal%20Fine-tuning&entry.906535625=Alejandro%20Moreno%20Arcas%20and%20Albert%20Sanchis%20and%20Jorge%20Civera%20and%20Alfons%20Juan&entry.1292438233=%20%20Adaptation%20of%20foundation%20models%20using%20low-rank%20methods%20is%20a%20widespread%0Aapproach.%20Another%20way%20to%20adapt%20these%20models%20is%20to%20employ%20orthogonal%20fine-tuning%0Amethods%2C%20which%20are%20less%20time%20and%20memory%20efficient%20despite%20their%20good%0Ageneralization%20properties.%20In%20this%20work%2C%20we%20propose%20Householder%20Orthogonal%0AFine-tuning%20%28HOFT%29%2C%20a%20novel%20orthogonal%20fine-tuning%20method%20that%20aims%20to%0Aalleviate%20time%20and%20space%20complexity.%20Moreover%2C%20some%20theoretical%20properties%20of%0Athe%20orthogonal%20fine-tuning%20paradigm%20are%20explored.%20From%20this%20exploration%2C%20Scaled%0AHouseholder%20Orthogonal%20Fine-tuning%20%28SHOFT%29%20is%20proposed.%20Both%20HOFT%20and%20SHOFT%20are%0Aevaluated%20in%20downstream%20tasks%2C%20namely%20commonsense%20reasoning%2C%20machine%0Atranslation%2C%20subject-driven%20generation%20and%20mathematical%20reasoning.%20Compared%0Awith%20state-of-the-art%20adaptation%20methods%2C%20HOFT%20and%20SHOFT%20show%20comparable%20or%0Abetter%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16531v1&entry.124074799=Read"},
{"title": "Fast computation of the TGOSPA metric for multiple target tracking via\n  unbalanced optimal transport", "author": "Viktor Nevelius Wernholm and Alfred W\u00e4rns\u00e4ter and Axel Ringh", "abstract": "  In multiple target tracking, it is important to be able to evaluate the\nperformance of different tracking algorithms. The trajectory generalized\noptimal sub-pattern assignment metric (TGOSPA) is a recently proposed metric\nfor such evaluations. The TGOSPA metric is computed as the solution to an\noptimization problem, but for large tracking scenarios, solving this problem\nbecomes computationally demanding. In this paper, we present an approximation\nalgorithm for evaluating the TGOSPA metric, based on casting the TGOSPA problem\nas an unbalanced multimarginal optimal transport problem. Following recent\nadvances in computational optimal transport, we introduce an entropy\nregularization and derive an iterative scheme for solving the Lagrangian dual\nof the regularized problem. Numerical results suggest that our proposed\nalgorithm is more computationally efficient than the alternative of computing\nthe exact metric using a linear programming solver, while still providing an\nadequate approximation of the metric.\n", "link": "http://arxiv.org/abs/2503.09449v2", "date": "2025-05-22", "relevancy": 2.4367, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.516}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4913}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20computation%20of%20the%20TGOSPA%20metric%20for%20multiple%20target%20tracking%20via%0A%20%20unbalanced%20optimal%20transport&body=Title%3A%20Fast%20computation%20of%20the%20TGOSPA%20metric%20for%20multiple%20target%20tracking%20via%0A%20%20unbalanced%20optimal%20transport%0AAuthor%3A%20Viktor%20Nevelius%20Wernholm%20and%20Alfred%20W%C3%A4rns%C3%A4ter%20and%20Axel%20Ringh%0AAbstract%3A%20%20%20In%20multiple%20target%20tracking%2C%20it%20is%20important%20to%20be%20able%20to%20evaluate%20the%0Aperformance%20of%20different%20tracking%20algorithms.%20The%20trajectory%20generalized%0Aoptimal%20sub-pattern%20assignment%20metric%20%28TGOSPA%29%20is%20a%20recently%20proposed%20metric%0Afor%20such%20evaluations.%20The%20TGOSPA%20metric%20is%20computed%20as%20the%20solution%20to%20an%0Aoptimization%20problem%2C%20but%20for%20large%20tracking%20scenarios%2C%20solving%20this%20problem%0Abecomes%20computationally%20demanding.%20In%20this%20paper%2C%20we%20present%20an%20approximation%0Aalgorithm%20for%20evaluating%20the%20TGOSPA%20metric%2C%20based%20on%20casting%20the%20TGOSPA%20problem%0Aas%20an%20unbalanced%20multimarginal%20optimal%20transport%20problem.%20Following%20recent%0Aadvances%20in%20computational%20optimal%20transport%2C%20we%20introduce%20an%20entropy%0Aregularization%20and%20derive%20an%20iterative%20scheme%20for%20solving%20the%20Lagrangian%20dual%0Aof%20the%20regularized%20problem.%20Numerical%20results%20suggest%20that%20our%20proposed%0Aalgorithm%20is%20more%20computationally%20efficient%20than%20the%20alternative%20of%20computing%0Athe%20exact%20metric%20using%20a%20linear%20programming%20solver%2C%20while%20still%20providing%20an%0Aadequate%20approximation%20of%20the%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09449v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520computation%2520of%2520the%2520TGOSPA%2520metric%2520for%2520multiple%2520target%2520tracking%2520via%250A%2520%2520unbalanced%2520optimal%2520transport%26entry.906535625%3DViktor%2520Nevelius%2520Wernholm%2520and%2520Alfred%2520W%25C3%25A4rns%25C3%25A4ter%2520and%2520Axel%2520Ringh%26entry.1292438233%3D%2520%2520In%2520multiple%2520target%2520tracking%252C%2520it%2520is%2520important%2520to%2520be%2520able%2520to%2520evaluate%2520the%250Aperformance%2520of%2520different%2520tracking%2520algorithms.%2520The%2520trajectory%2520generalized%250Aoptimal%2520sub-pattern%2520assignment%2520metric%2520%2528TGOSPA%2529%2520is%2520a%2520recently%2520proposed%2520metric%250Afor%2520such%2520evaluations.%2520The%2520TGOSPA%2520metric%2520is%2520computed%2520as%2520the%2520solution%2520to%2520an%250Aoptimization%2520problem%252C%2520but%2520for%2520large%2520tracking%2520scenarios%252C%2520solving%2520this%2520problem%250Abecomes%2520computationally%2520demanding.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520approximation%250Aalgorithm%2520for%2520evaluating%2520the%2520TGOSPA%2520metric%252C%2520based%2520on%2520casting%2520the%2520TGOSPA%2520problem%250Aas%2520an%2520unbalanced%2520multimarginal%2520optimal%2520transport%2520problem.%2520Following%2520recent%250Aadvances%2520in%2520computational%2520optimal%2520transport%252C%2520we%2520introduce%2520an%2520entropy%250Aregularization%2520and%2520derive%2520an%2520iterative%2520scheme%2520for%2520solving%2520the%2520Lagrangian%2520dual%250Aof%2520the%2520regularized%2520problem.%2520Numerical%2520results%2520suggest%2520that%2520our%2520proposed%250Aalgorithm%2520is%2520more%2520computationally%2520efficient%2520than%2520the%2520alternative%2520of%2520computing%250Athe%2520exact%2520metric%2520using%2520a%2520linear%2520programming%2520solver%252C%2520while%2520still%2520providing%2520an%250Aadequate%2520approximation%2520of%2520the%2520metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09449v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20computation%20of%20the%20TGOSPA%20metric%20for%20multiple%20target%20tracking%20via%0A%20%20unbalanced%20optimal%20transport&entry.906535625=Viktor%20Nevelius%20Wernholm%20and%20Alfred%20W%C3%A4rns%C3%A4ter%20and%20Axel%20Ringh&entry.1292438233=%20%20In%20multiple%20target%20tracking%2C%20it%20is%20important%20to%20be%20able%20to%20evaluate%20the%0Aperformance%20of%20different%20tracking%20algorithms.%20The%20trajectory%20generalized%0Aoptimal%20sub-pattern%20assignment%20metric%20%28TGOSPA%29%20is%20a%20recently%20proposed%20metric%0Afor%20such%20evaluations.%20The%20TGOSPA%20metric%20is%20computed%20as%20the%20solution%20to%20an%0Aoptimization%20problem%2C%20but%20for%20large%20tracking%20scenarios%2C%20solving%20this%20problem%0Abecomes%20computationally%20demanding.%20In%20this%20paper%2C%20we%20present%20an%20approximation%0Aalgorithm%20for%20evaluating%20the%20TGOSPA%20metric%2C%20based%20on%20casting%20the%20TGOSPA%20problem%0Aas%20an%20unbalanced%20multimarginal%20optimal%20transport%20problem.%20Following%20recent%0Aadvances%20in%20computational%20optimal%20transport%2C%20we%20introduce%20an%20entropy%0Aregularization%20and%20derive%20an%20iterative%20scheme%20for%20solving%20the%20Lagrangian%20dual%0Aof%20the%20regularized%20problem.%20Numerical%20results%20suggest%20that%20our%20proposed%0Aalgorithm%20is%20more%20computationally%20efficient%20than%20the%20alternative%20of%20computing%0Athe%20exact%20metric%20using%20a%20linear%20programming%20solver%2C%20while%20still%20providing%20an%0Aadequate%20approximation%20of%20the%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09449v2&entry.124074799=Read"},
{"title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding", "author": "Shufan Li and Konstantinos Kallidromitis and Hritik Bansal and Akash Gokul and Yusuke Kato and Kazuki Kozuka and Jason Kuen and Zhe Lin and Kai-Wei Chang and Aditya Grover", "abstract": "  Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.\n", "link": "http://arxiv.org/abs/2505.16839v1", "date": "2025-05-22", "relevancy": 2.4365, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.61}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.61}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaViDa%3A%20A%20Large%20Diffusion%20Language%20Model%20for%20Multimodal%20Understanding&body=Title%3A%20LaViDa%3A%20A%20Large%20Diffusion%20Language%20Model%20for%20Multimodal%20Understanding%0AAuthor%3A%20Shufan%20Li%20and%20Konstantinos%20Kallidromitis%20and%20Hritik%20Bansal%20and%20Akash%20Gokul%20and%20Yusuke%20Kato%20and%20Kazuki%20Kozuka%20and%20Jason%20Kuen%20and%20Zhe%20Lin%20and%20Kai-Wei%20Chang%20and%20Aditya%20Grover%0AAbstract%3A%20%20%20Modern%20Vision-Language%20Models%20%28VLMs%29%20can%20solve%20a%20wide%20range%20of%20tasks%0Arequiring%20visual%20reasoning.%20In%20real-world%20scenarios%2C%20desirable%20properties%20for%0AVLMs%20include%20fast%20inference%20and%20controllable%20generation%20%28e.g.%2C%20constraining%0Aoutputs%20to%20adhere%20to%20a%20desired%20format%29.%20However%2C%20existing%20autoregressive%20%28AR%29%0AVLMs%20like%20LLaVA%20struggle%20in%20these%20aspects.%20Discrete%20diffusion%20models%20%28DMs%29%0Aoffer%20a%20promising%20alternative%2C%20enabling%20parallel%20decoding%20for%20faster%20inference%0Aand%20bidirectional%20context%20for%20controllable%20generation%20through%20text-infilling.%0AWhile%20effective%20in%20language-only%20settings%2C%20DMs%27%20potential%20for%20multimodal%20tasks%0Ais%20underexplored.%20We%20introduce%20LaViDa%2C%20a%20family%20of%20VLMs%20built%20on%20DMs.%20We%20build%0ALaViDa%20by%20equipping%20DMs%20with%20a%20vision%20encoder%20and%20jointly%20fine-tune%20the%0Acombined%20parts%20for%20multimodal%20instruction%20following.%20To%20address%20challenges%0Aencountered%2C%20LaViDa%20incorporates%20novel%20techniques%20such%20as%20complementary%20masking%0Afor%20effective%20training%2C%20prefix%20KV%20cache%20for%20efficient%20inference%2C%20and%20timestep%0Ashifting%20for%20high-quality%20sampling.%20Experiments%20show%20that%20LaViDa%20achieves%0Acompetitive%20or%20superior%20performance%20to%20AR%20VLMs%20on%20multi-modal%20benchmarks%20such%0Aas%20MMMU%2C%20while%20offering%20unique%20advantages%20of%20DMs%2C%20including%20flexible%0Aspeed-quality%20tradeoff%2C%20controllability%2C%20and%20bidirectional%20reasoning.%20On%20COCO%0Acaptioning%2C%20LaViDa%20surpasses%20Open-LLaVa-Next-8B%20by%20%2B4.1%20CIDEr%20with%201.92x%0Aspeedup.%20On%20bidirectional%20tasks%2C%20it%20achieves%20%2B59%25%20improvement%20on%20Constrained%0APoem%20Completion.%20These%20results%20demonstrate%20LaViDa%20as%20a%20strong%20alternative%20to%20AR%0AVLMs.%20Code%20and%20models%20will%20be%20released%20in%20the%20camera-ready%20version.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaViDa%253A%2520A%2520Large%2520Diffusion%2520Language%2520Model%2520for%2520Multimodal%2520Understanding%26entry.906535625%3DShufan%2520Li%2520and%2520Konstantinos%2520Kallidromitis%2520and%2520Hritik%2520Bansal%2520and%2520Akash%2520Gokul%2520and%2520Yusuke%2520Kato%2520and%2520Kazuki%2520Kozuka%2520and%2520Jason%2520Kuen%2520and%2520Zhe%2520Lin%2520and%2520Kai-Wei%2520Chang%2520and%2520Aditya%2520Grover%26entry.1292438233%3D%2520%2520Modern%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520can%2520solve%2520a%2520wide%2520range%2520of%2520tasks%250Arequiring%2520visual%2520reasoning.%2520In%2520real-world%2520scenarios%252C%2520desirable%2520properties%2520for%250AVLMs%2520include%2520fast%2520inference%2520and%2520controllable%2520generation%2520%2528e.g.%252C%2520constraining%250Aoutputs%2520to%2520adhere%2520to%2520a%2520desired%2520format%2529.%2520However%252C%2520existing%2520autoregressive%2520%2528AR%2529%250AVLMs%2520like%2520LLaVA%2520struggle%2520in%2520these%2520aspects.%2520Discrete%2520diffusion%2520models%2520%2528DMs%2529%250Aoffer%2520a%2520promising%2520alternative%252C%2520enabling%2520parallel%2520decoding%2520for%2520faster%2520inference%250Aand%2520bidirectional%2520context%2520for%2520controllable%2520generation%2520through%2520text-infilling.%250AWhile%2520effective%2520in%2520language-only%2520settings%252C%2520DMs%2527%2520potential%2520for%2520multimodal%2520tasks%250Ais%2520underexplored.%2520We%2520introduce%2520LaViDa%252C%2520a%2520family%2520of%2520VLMs%2520built%2520on%2520DMs.%2520We%2520build%250ALaViDa%2520by%2520equipping%2520DMs%2520with%2520a%2520vision%2520encoder%2520and%2520jointly%2520fine-tune%2520the%250Acombined%2520parts%2520for%2520multimodal%2520instruction%2520following.%2520To%2520address%2520challenges%250Aencountered%252C%2520LaViDa%2520incorporates%2520novel%2520techniques%2520such%2520as%2520complementary%2520masking%250Afor%2520effective%2520training%252C%2520prefix%2520KV%2520cache%2520for%2520efficient%2520inference%252C%2520and%2520timestep%250Ashifting%2520for%2520high-quality%2520sampling.%2520Experiments%2520show%2520that%2520LaViDa%2520achieves%250Acompetitive%2520or%2520superior%2520performance%2520to%2520AR%2520VLMs%2520on%2520multi-modal%2520benchmarks%2520such%250Aas%2520MMMU%252C%2520while%2520offering%2520unique%2520advantages%2520of%2520DMs%252C%2520including%2520flexible%250Aspeed-quality%2520tradeoff%252C%2520controllability%252C%2520and%2520bidirectional%2520reasoning.%2520On%2520COCO%250Acaptioning%252C%2520LaViDa%2520surpasses%2520Open-LLaVa-Next-8B%2520by%2520%252B4.1%2520CIDEr%2520with%25201.92x%250Aspeedup.%2520On%2520bidirectional%2520tasks%252C%2520it%2520achieves%2520%252B59%2525%2520improvement%2520on%2520Constrained%250APoem%2520Completion.%2520These%2520results%2520demonstrate%2520LaViDa%2520as%2520a%2520strong%2520alternative%2520to%2520AR%250AVLMs.%2520Code%2520and%2520models%2520will%2520be%2520released%2520in%2520the%2520camera-ready%2520version.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaViDa%3A%20A%20Large%20Diffusion%20Language%20Model%20for%20Multimodal%20Understanding&entry.906535625=Shufan%20Li%20and%20Konstantinos%20Kallidromitis%20and%20Hritik%20Bansal%20and%20Akash%20Gokul%20and%20Yusuke%20Kato%20and%20Kazuki%20Kozuka%20and%20Jason%20Kuen%20and%20Zhe%20Lin%20and%20Kai-Wei%20Chang%20and%20Aditya%20Grover&entry.1292438233=%20%20Modern%20Vision-Language%20Models%20%28VLMs%29%20can%20solve%20a%20wide%20range%20of%20tasks%0Arequiring%20visual%20reasoning.%20In%20real-world%20scenarios%2C%20desirable%20properties%20for%0AVLMs%20include%20fast%20inference%20and%20controllable%20generation%20%28e.g.%2C%20constraining%0Aoutputs%20to%20adhere%20to%20a%20desired%20format%29.%20However%2C%20existing%20autoregressive%20%28AR%29%0AVLMs%20like%20LLaVA%20struggle%20in%20these%20aspects.%20Discrete%20diffusion%20models%20%28DMs%29%0Aoffer%20a%20promising%20alternative%2C%20enabling%20parallel%20decoding%20for%20faster%20inference%0Aand%20bidirectional%20context%20for%20controllable%20generation%20through%20text-infilling.%0AWhile%20effective%20in%20language-only%20settings%2C%20DMs%27%20potential%20for%20multimodal%20tasks%0Ais%20underexplored.%20We%20introduce%20LaViDa%2C%20a%20family%20of%20VLMs%20built%20on%20DMs.%20We%20build%0ALaViDa%20by%20equipping%20DMs%20with%20a%20vision%20encoder%20and%20jointly%20fine-tune%20the%0Acombined%20parts%20for%20multimodal%20instruction%20following.%20To%20address%20challenges%0Aencountered%2C%20LaViDa%20incorporates%20novel%20techniques%20such%20as%20complementary%20masking%0Afor%20effective%20training%2C%20prefix%20KV%20cache%20for%20efficient%20inference%2C%20and%20timestep%0Ashifting%20for%20high-quality%20sampling.%20Experiments%20show%20that%20LaViDa%20achieves%0Acompetitive%20or%20superior%20performance%20to%20AR%20VLMs%20on%20multi-modal%20benchmarks%20such%0Aas%20MMMU%2C%20while%20offering%20unique%20advantages%20of%20DMs%2C%20including%20flexible%0Aspeed-quality%20tradeoff%2C%20controllability%2C%20and%20bidirectional%20reasoning.%20On%20COCO%0Acaptioning%2C%20LaViDa%20surpasses%20Open-LLaVa-Next-8B%20by%20%2B4.1%20CIDEr%20with%201.92x%0Aspeedup.%20On%20bidirectional%20tasks%2C%20it%20achieves%20%2B59%25%20improvement%20on%20Constrained%0APoem%20Completion.%20These%20results%20demonstrate%20LaViDa%20as%20a%20strong%20alternative%20to%20AR%0AVLMs.%20Code%20and%20models%20will%20be%20released%20in%20the%20camera-ready%20version.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16839v1&entry.124074799=Read"},
{"title": "Robust LLM Fingerprinting via Domain-Specific Watermarks", "author": "Thibaud Gloaguen and Robin Staab and Nikola Jovanovi\u0107 and Martin Vechev", "abstract": "  As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios.\n", "link": "http://arxiv.org/abs/2505.16723v1", "date": "2025-05-22", "relevancy": 2.4343, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20LLM%20Fingerprinting%20via%20Domain-Specific%20Watermarks&body=Title%3A%20Robust%20LLM%20Fingerprinting%20via%20Domain-Specific%20Watermarks%0AAuthor%3A%20Thibaud%20Gloaguen%20and%20Robin%20Staab%20and%20Nikola%20Jovanovi%C4%87%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20As%20open-source%20language%20models%20%28OSMs%29%20grow%20more%20capable%20and%20are%20widely%20shared%0Aand%20finetuned%2C%20ensuring%20model%20provenance%2C%20i.e.%2C%20identifying%20the%20origin%20of%20a%0Agiven%20model%20instance%2C%20has%20become%20an%20increasingly%20important%20issue.%20At%20the%20same%0Atime%2C%20existing%20backdoor-based%20model%20fingerprinting%20techniques%20often%20fall%20short%0Aof%20achieving%20key%20requirements%20of%20real-world%20model%20ownership%20detection.%20In%20this%0Awork%2C%20we%20build%20on%20the%20observation%20that%20while%20current%20open-source%20model%0Awatermarks%20fail%20to%20achieve%20reliable%20content%20traceability%2C%20they%20can%20be%0Aeffectively%20adapted%20to%20address%20the%20challenge%20of%20model%20provenance.%20To%20this%20end%2C%0Awe%20introduce%20the%20concept%20of%20domain-specific%20watermarking%20for%20model%0Afingerprinting.%20Rather%20than%20watermarking%20all%20generated%20content%2C%20we%20train%20the%0Amodel%20to%20embed%20watermarks%20only%20within%20specified%20subdomains%20%28e.g.%2C%20particular%0Alanguages%20or%20topics%29.%20This%20targeted%20approach%20ensures%20detection%20reliability%2C%0Awhile%20improving%20watermark%20durability%20and%20quality%20under%20a%20range%20of%20real-world%0Adeployment%20settings.%20Our%20evaluations%20show%20that%20domain-specific%20watermarking%0Aenables%20model%20fingerprinting%20with%20strong%20statistical%20guarantees%2C%20controllable%0Afalse%20positive%20rates%2C%20high%20detection%20power%2C%20and%20preserved%20generation%20quality.%0AMoreover%2C%20we%20find%20that%20our%20fingerprints%20are%20inherently%20stealthy%20and%20naturally%0Arobust%20to%20real-world%20variability%20across%20deployment%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520LLM%2520Fingerprinting%2520via%2520Domain-Specific%2520Watermarks%26entry.906535625%3DThibaud%2520Gloaguen%2520and%2520Robin%2520Staab%2520and%2520Nikola%2520Jovanovi%25C4%2587%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520As%2520open-source%2520language%2520models%2520%2528OSMs%2529%2520grow%2520more%2520capable%2520and%2520are%2520widely%2520shared%250Aand%2520finetuned%252C%2520ensuring%2520model%2520provenance%252C%2520i.e.%252C%2520identifying%2520the%2520origin%2520of%2520a%250Agiven%2520model%2520instance%252C%2520has%2520become%2520an%2520increasingly%2520important%2520issue.%2520At%2520the%2520same%250Atime%252C%2520existing%2520backdoor-based%2520model%2520fingerprinting%2520techniques%2520often%2520fall%2520short%250Aof%2520achieving%2520key%2520requirements%2520of%2520real-world%2520model%2520ownership%2520detection.%2520In%2520this%250Awork%252C%2520we%2520build%2520on%2520the%2520observation%2520that%2520while%2520current%2520open-source%2520model%250Awatermarks%2520fail%2520to%2520achieve%2520reliable%2520content%2520traceability%252C%2520they%2520can%2520be%250Aeffectively%2520adapted%2520to%2520address%2520the%2520challenge%2520of%2520model%2520provenance.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520the%2520concept%2520of%2520domain-specific%2520watermarking%2520for%2520model%250Afingerprinting.%2520Rather%2520than%2520watermarking%2520all%2520generated%2520content%252C%2520we%2520train%2520the%250Amodel%2520to%2520embed%2520watermarks%2520only%2520within%2520specified%2520subdomains%2520%2528e.g.%252C%2520particular%250Alanguages%2520or%2520topics%2529.%2520This%2520targeted%2520approach%2520ensures%2520detection%2520reliability%252C%250Awhile%2520improving%2520watermark%2520durability%2520and%2520quality%2520under%2520a%2520range%2520of%2520real-world%250Adeployment%2520settings.%2520Our%2520evaluations%2520show%2520that%2520domain-specific%2520watermarking%250Aenables%2520model%2520fingerprinting%2520with%2520strong%2520statistical%2520guarantees%252C%2520controllable%250Afalse%2520positive%2520rates%252C%2520high%2520detection%2520power%252C%2520and%2520preserved%2520generation%2520quality.%250AMoreover%252C%2520we%2520find%2520that%2520our%2520fingerprints%2520are%2520inherently%2520stealthy%2520and%2520naturally%250Arobust%2520to%2520real-world%2520variability%2520across%2520deployment%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20LLM%20Fingerprinting%20via%20Domain-Specific%20Watermarks&entry.906535625=Thibaud%20Gloaguen%20and%20Robin%20Staab%20and%20Nikola%20Jovanovi%C4%87%20and%20Martin%20Vechev&entry.1292438233=%20%20As%20open-source%20language%20models%20%28OSMs%29%20grow%20more%20capable%20and%20are%20widely%20shared%0Aand%20finetuned%2C%20ensuring%20model%20provenance%2C%20i.e.%2C%20identifying%20the%20origin%20of%20a%0Agiven%20model%20instance%2C%20has%20become%20an%20increasingly%20important%20issue.%20At%20the%20same%0Atime%2C%20existing%20backdoor-based%20model%20fingerprinting%20techniques%20often%20fall%20short%0Aof%20achieving%20key%20requirements%20of%20real-world%20model%20ownership%20detection.%20In%20this%0Awork%2C%20we%20build%20on%20the%20observation%20that%20while%20current%20open-source%20model%0Awatermarks%20fail%20to%20achieve%20reliable%20content%20traceability%2C%20they%20can%20be%0Aeffectively%20adapted%20to%20address%20the%20challenge%20of%20model%20provenance.%20To%20this%20end%2C%0Awe%20introduce%20the%20concept%20of%20domain-specific%20watermarking%20for%20model%0Afingerprinting.%20Rather%20than%20watermarking%20all%20generated%20content%2C%20we%20train%20the%0Amodel%20to%20embed%20watermarks%20only%20within%20specified%20subdomains%20%28e.g.%2C%20particular%0Alanguages%20or%20topics%29.%20This%20targeted%20approach%20ensures%20detection%20reliability%2C%0Awhile%20improving%20watermark%20durability%20and%20quality%20under%20a%20range%20of%20real-world%0Adeployment%20settings.%20Our%20evaluations%20show%20that%20domain-specific%20watermarking%0Aenables%20model%20fingerprinting%20with%20strong%20statistical%20guarantees%2C%20controllable%0Afalse%20positive%20rates%2C%20high%20detection%20power%2C%20and%20preserved%20generation%20quality.%0AMoreover%2C%20we%20find%20that%20our%20fingerprints%20are%20inherently%20stealthy%20and%20naturally%0Arobust%20to%20real-world%20variability%20across%20deployment%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16723v1&entry.124074799=Read"},
{"title": "Incremental Sequence Classification with Temporal Consistency", "author": "Lucas Maystre and Gabriel Barello and Tudor Berariu and Aleix Cambray and Rares Dolga and Alvaro Ortega Gonzalez and Andrei Nica and David Barber", "abstract": "  We address the problem of incremental sequence classification, where\npredictions are updated as new elements in the sequence are revealed. Drawing\non temporal-difference learning from reinforcement learning, we identify a\ntemporal-consistency condition that successive predictions should satisfy. We\nleverage this condition to develop a novel loss function for training\nincremental sequence classifiers. Through a concrete example, we demonstrate\nthat optimizing this loss can offer substantial gains in data efficiency. We\napply our method to text classification tasks and show that it improves\npredictive accuracy over competing approaches on several benchmark datasets. We\nfurther evaluate our approach on the task of verifying large language model\ngenerations for correctness in grade-school math problems. Our results show\nthat models trained with our method are better able to distinguish promising\ngenerations from unpromising ones after observing only a few tokens.\n", "link": "http://arxiv.org/abs/2505.16548v1", "date": "2025-05-22", "relevancy": 2.4316, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5027}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4797}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Sequence%20Classification%20with%20Temporal%20Consistency&body=Title%3A%20Incremental%20Sequence%20Classification%20with%20Temporal%20Consistency%0AAuthor%3A%20Lucas%20Maystre%20and%20Gabriel%20Barello%20and%20Tudor%20Berariu%20and%20Aleix%20Cambray%20and%20Rares%20Dolga%20and%20Alvaro%20Ortega%20Gonzalez%20and%20Andrei%20Nica%20and%20David%20Barber%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20incremental%20sequence%20classification%2C%20where%0Apredictions%20are%20updated%20as%20new%20elements%20in%20the%20sequence%20are%20revealed.%20Drawing%0Aon%20temporal-difference%20learning%20from%20reinforcement%20learning%2C%20we%20identify%20a%0Atemporal-consistency%20condition%20that%20successive%20predictions%20should%20satisfy.%20We%0Aleverage%20this%20condition%20to%20develop%20a%20novel%20loss%20function%20for%20training%0Aincremental%20sequence%20classifiers.%20Through%20a%20concrete%20example%2C%20we%20demonstrate%0Athat%20optimizing%20this%20loss%20can%20offer%20substantial%20gains%20in%20data%20efficiency.%20We%0Aapply%20our%20method%20to%20text%20classification%20tasks%20and%20show%20that%20it%20improves%0Apredictive%20accuracy%20over%20competing%20approaches%20on%20several%20benchmark%20datasets.%20We%0Afurther%20evaluate%20our%20approach%20on%20the%20task%20of%20verifying%20large%20language%20model%0Agenerations%20for%20correctness%20in%20grade-school%20math%20problems.%20Our%20results%20show%0Athat%20models%20trained%20with%20our%20method%20are%20better%20able%20to%20distinguish%20promising%0Agenerations%20from%20unpromising%20ones%20after%20observing%20only%20a%20few%20tokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Sequence%2520Classification%2520with%2520Temporal%2520Consistency%26entry.906535625%3DLucas%2520Maystre%2520and%2520Gabriel%2520Barello%2520and%2520Tudor%2520Berariu%2520and%2520Aleix%2520Cambray%2520and%2520Rares%2520Dolga%2520and%2520Alvaro%2520Ortega%2520Gonzalez%2520and%2520Andrei%2520Nica%2520and%2520David%2520Barber%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520incremental%2520sequence%2520classification%252C%2520where%250Apredictions%2520are%2520updated%2520as%2520new%2520elements%2520in%2520the%2520sequence%2520are%2520revealed.%2520Drawing%250Aon%2520temporal-difference%2520learning%2520from%2520reinforcement%2520learning%252C%2520we%2520identify%2520a%250Atemporal-consistency%2520condition%2520that%2520successive%2520predictions%2520should%2520satisfy.%2520We%250Aleverage%2520this%2520condition%2520to%2520develop%2520a%2520novel%2520loss%2520function%2520for%2520training%250Aincremental%2520sequence%2520classifiers.%2520Through%2520a%2520concrete%2520example%252C%2520we%2520demonstrate%250Athat%2520optimizing%2520this%2520loss%2520can%2520offer%2520substantial%2520gains%2520in%2520data%2520efficiency.%2520We%250Aapply%2520our%2520method%2520to%2520text%2520classification%2520tasks%2520and%2520show%2520that%2520it%2520improves%250Apredictive%2520accuracy%2520over%2520competing%2520approaches%2520on%2520several%2520benchmark%2520datasets.%2520We%250Afurther%2520evaluate%2520our%2520approach%2520on%2520the%2520task%2520of%2520verifying%2520large%2520language%2520model%250Agenerations%2520for%2520correctness%2520in%2520grade-school%2520math%2520problems.%2520Our%2520results%2520show%250Athat%2520models%2520trained%2520with%2520our%2520method%2520are%2520better%2520able%2520to%2520distinguish%2520promising%250Agenerations%2520from%2520unpromising%2520ones%2520after%2520observing%2520only%2520a%2520few%2520tokens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Sequence%20Classification%20with%20Temporal%20Consistency&entry.906535625=Lucas%20Maystre%20and%20Gabriel%20Barello%20and%20Tudor%20Berariu%20and%20Aleix%20Cambray%20and%20Rares%20Dolga%20and%20Alvaro%20Ortega%20Gonzalez%20and%20Andrei%20Nica%20and%20David%20Barber&entry.1292438233=%20%20We%20address%20the%20problem%20of%20incremental%20sequence%20classification%2C%20where%0Apredictions%20are%20updated%20as%20new%20elements%20in%20the%20sequence%20are%20revealed.%20Drawing%0Aon%20temporal-difference%20learning%20from%20reinforcement%20learning%2C%20we%20identify%20a%0Atemporal-consistency%20condition%20that%20successive%20predictions%20should%20satisfy.%20We%0Aleverage%20this%20condition%20to%20develop%20a%20novel%20loss%20function%20for%20training%0Aincremental%20sequence%20classifiers.%20Through%20a%20concrete%20example%2C%20we%20demonstrate%0Athat%20optimizing%20this%20loss%20can%20offer%20substantial%20gains%20in%20data%20efficiency.%20We%0Aapply%20our%20method%20to%20text%20classification%20tasks%20and%20show%20that%20it%20improves%0Apredictive%20accuracy%20over%20competing%20approaches%20on%20several%20benchmark%20datasets.%20We%0Afurther%20evaluate%20our%20approach%20on%20the%20task%20of%20verifying%20large%20language%20model%0Agenerations%20for%20correctness%20in%20grade-school%20math%20problems.%20Our%20results%20show%0Athat%20models%20trained%20with%20our%20method%20are%20better%20able%20to%20distinguish%20promising%0Agenerations%20from%20unpromising%20ones%20after%20observing%20only%20a%20few%20tokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16548v1&entry.124074799=Read"},
{"title": "OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step\n  Visual Reasoning", "author": "Zongyan Han and Jiale Cao and Shuo Chen and Tong Wang and Jorma Laaksonen and Rao Muhammad Anwer", "abstract": "  Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its\ncapacity to generalize segmentation beyond predefined categories. However,\nexisting methods typically predict segmentation masks with simple forward\ninference, lacking explicit reasoning and interpretability. This makes it\nchallenging for OVS model to distinguish similar categories in open-world\nsettings due to the lack of contextual understanding and discriminative visual\ncues. To address this limitation, we propose a step-by-step visual reasoning\nframework for open-vocabulary segmentation, named OpenSeg-R. The proposed\nOpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical\nvisual reasoning before segmentation. Specifically, we generate both generic\nand image-specific reasoning for each image, forming structured triplets that\nexplain the visual reason for objects in a coarse-to-fine manner. Based on\nthese reasoning steps, we can compose detailed description prompts, and feed\nthem to the segmentor to produce more accurate segmentation masks. To the best\nof our knowledge, OpenSeg-R is the first framework to introduce explicit\nstep-by-step visual reasoning into OVS. Experimental results demonstrate that\nOpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary\nsemantic segmentation across five benchmark datasets. Moreover, it achieves\nconsistent gains across all metrics on open-vocabulary panoptic segmentation.\nQualitative results further highlight the effectiveness of our reasoning-guided\nframework in improving both segmentation precision and interpretability. Our\ncode is publicly available at https://github.com/Hanzy1996/OpenSeg-R.\n", "link": "http://arxiv.org/abs/2505.16974v1", "date": "2025-05-22", "relevancy": 2.4306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6175}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6175}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenSeg-R%3A%20Improving%20Open-Vocabulary%20Segmentation%20via%20Step-by-Step%0A%20%20Visual%20Reasoning&body=Title%3A%20OpenSeg-R%3A%20Improving%20Open-Vocabulary%20Segmentation%20via%20Step-by-Step%0A%20%20Visual%20Reasoning%0AAuthor%3A%20Zongyan%20Han%20and%20Jiale%20Cao%20and%20Shuo%20Chen%20and%20Tong%20Wang%20and%20Jorma%20Laaksonen%20and%20Rao%20Muhammad%20Anwer%0AAbstract%3A%20%20%20Open-Vocabulary%20Segmentation%20%28OVS%29%20has%20drawn%20increasing%20attention%20for%20its%0Acapacity%20to%20generalize%20segmentation%20beyond%20predefined%20categories.%20However%2C%0Aexisting%20methods%20typically%20predict%20segmentation%20masks%20with%20simple%20forward%0Ainference%2C%20lacking%20explicit%20reasoning%20and%20interpretability.%20This%20makes%20it%0Achallenging%20for%20OVS%20model%20to%20distinguish%20similar%20categories%20in%20open-world%0Asettings%20due%20to%20the%20lack%20of%20contextual%20understanding%20and%20discriminative%20visual%0Acues.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20step-by-step%20visual%20reasoning%0Aframework%20for%20open-vocabulary%20segmentation%2C%20named%20OpenSeg-R.%20The%20proposed%0AOpenSeg-R%20leverages%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20perform%20hierarchical%0Avisual%20reasoning%20before%20segmentation.%20Specifically%2C%20we%20generate%20both%20generic%0Aand%20image-specific%20reasoning%20for%20each%20image%2C%20forming%20structured%20triplets%20that%0Aexplain%20the%20visual%20reason%20for%20objects%20in%20a%20coarse-to-fine%20manner.%20Based%20on%0Athese%20reasoning%20steps%2C%20we%20can%20compose%20detailed%20description%20prompts%2C%20and%20feed%0Athem%20to%20the%20segmentor%20to%20produce%20more%20accurate%20segmentation%20masks.%20To%20the%20best%0Aof%20our%20knowledge%2C%20OpenSeg-R%20is%20the%20first%20framework%20to%20introduce%20explicit%0Astep-by-step%20visual%20reasoning%20into%20OVS.%20Experimental%20results%20demonstrate%20that%0AOpenSeg-R%20significantly%20outperforms%20state-of-the-art%20methods%20on%20open-vocabulary%0Asemantic%20segmentation%20across%20five%20benchmark%20datasets.%20Moreover%2C%20it%20achieves%0Aconsistent%20gains%20across%20all%20metrics%20on%20open-vocabulary%20panoptic%20segmentation.%0AQualitative%20results%20further%20highlight%20the%20effectiveness%20of%20our%20reasoning-guided%0Aframework%20in%20improving%20both%20segmentation%20precision%20and%20interpretability.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/Hanzy1996/OpenSeg-R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenSeg-R%253A%2520Improving%2520Open-Vocabulary%2520Segmentation%2520via%2520Step-by-Step%250A%2520%2520Visual%2520Reasoning%26entry.906535625%3DZongyan%2520Han%2520and%2520Jiale%2520Cao%2520and%2520Shuo%2520Chen%2520and%2520Tong%2520Wang%2520and%2520Jorma%2520Laaksonen%2520and%2520Rao%2520Muhammad%2520Anwer%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520Segmentation%2520%2528OVS%2529%2520has%2520drawn%2520increasing%2520attention%2520for%2520its%250Acapacity%2520to%2520generalize%2520segmentation%2520beyond%2520predefined%2520categories.%2520However%252C%250Aexisting%2520methods%2520typically%2520predict%2520segmentation%2520masks%2520with%2520simple%2520forward%250Ainference%252C%2520lacking%2520explicit%2520reasoning%2520and%2520interpretability.%2520This%2520makes%2520it%250Achallenging%2520for%2520OVS%2520model%2520to%2520distinguish%2520similar%2520categories%2520in%2520open-world%250Asettings%2520due%2520to%2520the%2520lack%2520of%2520contextual%2520understanding%2520and%2520discriminative%2520visual%250Acues.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520step-by-step%2520visual%2520reasoning%250Aframework%2520for%2520open-vocabulary%2520segmentation%252C%2520named%2520OpenSeg-R.%2520The%2520proposed%250AOpenSeg-R%2520leverages%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520to%2520perform%2520hierarchical%250Avisual%2520reasoning%2520before%2520segmentation.%2520Specifically%252C%2520we%2520generate%2520both%2520generic%250Aand%2520image-specific%2520reasoning%2520for%2520each%2520image%252C%2520forming%2520structured%2520triplets%2520that%250Aexplain%2520the%2520visual%2520reason%2520for%2520objects%2520in%2520a%2520coarse-to-fine%2520manner.%2520Based%2520on%250Athese%2520reasoning%2520steps%252C%2520we%2520can%2520compose%2520detailed%2520description%2520prompts%252C%2520and%2520feed%250Athem%2520to%2520the%2520segmentor%2520to%2520produce%2520more%2520accurate%2520segmentation%2520masks.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520OpenSeg-R%2520is%2520the%2520first%2520framework%2520to%2520introduce%2520explicit%250Astep-by-step%2520visual%2520reasoning%2520into%2520OVS.%2520Experimental%2520results%2520demonstrate%2520that%250AOpenSeg-R%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520on%2520open-vocabulary%250Asemantic%2520segmentation%2520across%2520five%2520benchmark%2520datasets.%2520Moreover%252C%2520it%2520achieves%250Aconsistent%2520gains%2520across%2520all%2520metrics%2520on%2520open-vocabulary%2520panoptic%2520segmentation.%250AQualitative%2520results%2520further%2520highlight%2520the%2520effectiveness%2520of%2520our%2520reasoning-guided%250Aframework%2520in%2520improving%2520both%2520segmentation%2520precision%2520and%2520interpretability.%2520Our%250Acode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Hanzy1996/OpenSeg-R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenSeg-R%3A%20Improving%20Open-Vocabulary%20Segmentation%20via%20Step-by-Step%0A%20%20Visual%20Reasoning&entry.906535625=Zongyan%20Han%20and%20Jiale%20Cao%20and%20Shuo%20Chen%20and%20Tong%20Wang%20and%20Jorma%20Laaksonen%20and%20Rao%20Muhammad%20Anwer&entry.1292438233=%20%20Open-Vocabulary%20Segmentation%20%28OVS%29%20has%20drawn%20increasing%20attention%20for%20its%0Acapacity%20to%20generalize%20segmentation%20beyond%20predefined%20categories.%20However%2C%0Aexisting%20methods%20typically%20predict%20segmentation%20masks%20with%20simple%20forward%0Ainference%2C%20lacking%20explicit%20reasoning%20and%20interpretability.%20This%20makes%20it%0Achallenging%20for%20OVS%20model%20to%20distinguish%20similar%20categories%20in%20open-world%0Asettings%20due%20to%20the%20lack%20of%20contextual%20understanding%20and%20discriminative%20visual%0Acues.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20step-by-step%20visual%20reasoning%0Aframework%20for%20open-vocabulary%20segmentation%2C%20named%20OpenSeg-R.%20The%20proposed%0AOpenSeg-R%20leverages%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20perform%20hierarchical%0Avisual%20reasoning%20before%20segmentation.%20Specifically%2C%20we%20generate%20both%20generic%0Aand%20image-specific%20reasoning%20for%20each%20image%2C%20forming%20structured%20triplets%20that%0Aexplain%20the%20visual%20reason%20for%20objects%20in%20a%20coarse-to-fine%20manner.%20Based%20on%0Athese%20reasoning%20steps%2C%20we%20can%20compose%20detailed%20description%20prompts%2C%20and%20feed%0Athem%20to%20the%20segmentor%20to%20produce%20more%20accurate%20segmentation%20masks.%20To%20the%20best%0Aof%20our%20knowledge%2C%20OpenSeg-R%20is%20the%20first%20framework%20to%20introduce%20explicit%0Astep-by-step%20visual%20reasoning%20into%20OVS.%20Experimental%20results%20demonstrate%20that%0AOpenSeg-R%20significantly%20outperforms%20state-of-the-art%20methods%20on%20open-vocabulary%0Asemantic%20segmentation%20across%20five%20benchmark%20datasets.%20Moreover%2C%20it%20achieves%0Aconsistent%20gains%20across%20all%20metrics%20on%20open-vocabulary%20panoptic%20segmentation.%0AQualitative%20results%20further%20highlight%20the%20effectiveness%20of%20our%20reasoning-guided%0Aframework%20in%20improving%20both%20segmentation%20precision%20and%20interpretability.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/Hanzy1996/OpenSeg-R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16974v1&entry.124074799=Read"},
{"title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding", "author": "Haoning Wu and Xiao Huang and Yaohui Chen and Ya Zhang and Yanfeng Wang and Weidi Xie", "abstract": "  Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.\n", "link": "http://arxiv.org/abs/2505.17012v1", "date": "2025-05-22", "relevancy": 2.4263, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialScore%3A%20Towards%20Unified%20Evaluation%20for%20Multimodal%20Spatial%0A%20%20Understanding&body=Title%3A%20SpatialScore%3A%20Towards%20Unified%20Evaluation%20for%20Multimodal%20Spatial%0A%20%20Understanding%0AAuthor%3A%20Haoning%20Wu%20and%20Xiao%20Huang%20and%20Yaohui%20Chen%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20impressive%20success%20in%0Aquestion-answering%20tasks%2C%20yet%20their%20capabilities%20for%20spatial%20understanding%20are%0Aless%20explored.%20This%20work%20investigates%20a%20critical%20question%3A%20do%20existing%20MLLMs%0Apossess%203D%20spatial%20perception%20and%20understanding%20abilities%3F%20Concretely%2C%20we%20make%0Athe%20following%20contributions%20in%20this%20paper%3A%20%28i%29%20we%20introduce%20VGBench%2C%20a%0Abenchmark%20specifically%20designed%20to%20assess%20MLLMs%20for%20visual%20geometry%20perception%2C%0Ae.g.%2C%20camera%20pose%20and%20motion%20estimation%3B%20%28ii%29%20we%20propose%20SpatialScore%2C%20the%20most%0Acomprehensive%20and%20diverse%20multimodal%20spatial%20understanding%20benchmark%20to%20date%2C%0Aintegrating%20VGBench%20with%20relevant%20data%20from%20the%20other%2011%20existing%20datasets.%0AThis%20benchmark%20comprises%2028K%20samples%20across%20various%20spatial%20understanding%0Atasks%2C%20modalities%2C%20and%20QA%20formats%2C%20along%20with%20a%20carefully%20curated%20challenging%0Asubset%2C%20SpatialScore-Hard%3B%20%28iii%29%20we%20develop%20SpatialAgent%2C%20a%20novel%20multi-agent%0Asystem%20incorporating%209%20specialized%20tools%20for%20spatial%20understanding%2C%20supporting%0Aboth%20Plan-Execute%20and%20ReAct%20reasoning%20paradigms%3B%20%28iv%29%20we%20conduct%20extensive%0Aevaluations%20to%20reveal%20persistent%20challenges%20in%20spatial%20reasoning%20while%0Ademonstrating%20the%20effectiveness%20of%20SpatialAgent.%20We%20believe%20SpatialScore%20will%0Aoffer%20valuable%20insights%20and%20serve%20as%20a%20rigorous%20benchmark%20for%20the%20next%0Aevolution%20of%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialScore%253A%2520Towards%2520Unified%2520Evaluation%2520for%2520Multimodal%2520Spatial%250A%2520%2520Understanding%26entry.906535625%3DHaoning%2520Wu%2520and%2520Xiao%2520Huang%2520and%2520Yaohui%2520Chen%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520achieved%2520impressive%2520success%2520in%250Aquestion-answering%2520tasks%252C%2520yet%2520their%2520capabilities%2520for%2520spatial%2520understanding%2520are%250Aless%2520explored.%2520This%2520work%2520investigates%2520a%2520critical%2520question%253A%2520do%2520existing%2520MLLMs%250Apossess%25203D%2520spatial%2520perception%2520and%2520understanding%2520abilities%253F%2520Concretely%252C%2520we%2520make%250Athe%2520following%2520contributions%2520in%2520this%2520paper%253A%2520%2528i%2529%2520we%2520introduce%2520VGBench%252C%2520a%250Abenchmark%2520specifically%2520designed%2520to%2520assess%2520MLLMs%2520for%2520visual%2520geometry%2520perception%252C%250Ae.g.%252C%2520camera%2520pose%2520and%2520motion%2520estimation%253B%2520%2528ii%2529%2520we%2520propose%2520SpatialScore%252C%2520the%2520most%250Acomprehensive%2520and%2520diverse%2520multimodal%2520spatial%2520understanding%2520benchmark%2520to%2520date%252C%250Aintegrating%2520VGBench%2520with%2520relevant%2520data%2520from%2520the%2520other%252011%2520existing%2520datasets.%250AThis%2520benchmark%2520comprises%252028K%2520samples%2520across%2520various%2520spatial%2520understanding%250Atasks%252C%2520modalities%252C%2520and%2520QA%2520formats%252C%2520along%2520with%2520a%2520carefully%2520curated%2520challenging%250Asubset%252C%2520SpatialScore-Hard%253B%2520%2528iii%2529%2520we%2520develop%2520SpatialAgent%252C%2520a%2520novel%2520multi-agent%250Asystem%2520incorporating%25209%2520specialized%2520tools%2520for%2520spatial%2520understanding%252C%2520supporting%250Aboth%2520Plan-Execute%2520and%2520ReAct%2520reasoning%2520paradigms%253B%2520%2528iv%2529%2520we%2520conduct%2520extensive%250Aevaluations%2520to%2520reveal%2520persistent%2520challenges%2520in%2520spatial%2520reasoning%2520while%250Ademonstrating%2520the%2520effectiveness%2520of%2520SpatialAgent.%2520We%2520believe%2520SpatialScore%2520will%250Aoffer%2520valuable%2520insights%2520and%2520serve%2520as%2520a%2520rigorous%2520benchmark%2520for%2520the%2520next%250Aevolution%2520of%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialScore%3A%20Towards%20Unified%20Evaluation%20for%20Multimodal%20Spatial%0A%20%20Understanding&entry.906535625=Haoning%20Wu%20and%20Xiao%20Huang%20and%20Yaohui%20Chen%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20impressive%20success%20in%0Aquestion-answering%20tasks%2C%20yet%20their%20capabilities%20for%20spatial%20understanding%20are%0Aless%20explored.%20This%20work%20investigates%20a%20critical%20question%3A%20do%20existing%20MLLMs%0Apossess%203D%20spatial%20perception%20and%20understanding%20abilities%3F%20Concretely%2C%20we%20make%0Athe%20following%20contributions%20in%20this%20paper%3A%20%28i%29%20we%20introduce%20VGBench%2C%20a%0Abenchmark%20specifically%20designed%20to%20assess%20MLLMs%20for%20visual%20geometry%20perception%2C%0Ae.g.%2C%20camera%20pose%20and%20motion%20estimation%3B%20%28ii%29%20we%20propose%20SpatialScore%2C%20the%20most%0Acomprehensive%20and%20diverse%20multimodal%20spatial%20understanding%20benchmark%20to%20date%2C%0Aintegrating%20VGBench%20with%20relevant%20data%20from%20the%20other%2011%20existing%20datasets.%0AThis%20benchmark%20comprises%2028K%20samples%20across%20various%20spatial%20understanding%0Atasks%2C%20modalities%2C%20and%20QA%20formats%2C%20along%20with%20a%20carefully%20curated%20challenging%0Asubset%2C%20SpatialScore-Hard%3B%20%28iii%29%20we%20develop%20SpatialAgent%2C%20a%20novel%20multi-agent%0Asystem%20incorporating%209%20specialized%20tools%20for%20spatial%20understanding%2C%20supporting%0Aboth%20Plan-Execute%20and%20ReAct%20reasoning%20paradigms%3B%20%28iv%29%20we%20conduct%20extensive%0Aevaluations%20to%20reveal%20persistent%20challenges%20in%20spatial%20reasoning%20while%0Ademonstrating%20the%20effectiveness%20of%20SpatialAgent.%20We%20believe%20SpatialScore%20will%0Aoffer%20valuable%20insights%20and%20serve%20as%20a%20rigorous%20benchmark%20for%20the%20next%0Aevolution%20of%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17012v1&entry.124074799=Read"},
{"title": "Feature Map Similarity Reduction in Convolutional Neural Networks", "author": "Zakariae Belmekki and Jun Li and Patrick Reuter and David Antonio G\u00f3mez J\u00e1uregui and Karl Jenkins", "abstract": "  It has been observed that Convolutional Neural Networks (CNNs) suffer from\nredundancy in feature maps, leading to inefficient capacity utilization.\nEfforts to address this issue have largely focused on kernel orthogonality\nmethod. In this work, we theoretically and empirically demonstrate that kernel\northogonality does not necessarily lead to a reduction in feature map\nredundancy. Based on this analysis, we propose the Convolutional Similarity\nmethod to reduce feature map similarity, independently of the CNN's input. The\nConvolutional Similarity can be minimized as either a regularization term or an\niterative initialization method. Experimental results show that minimizing\nConvolutional Similarity not only improves classification accuracy but also\naccelerates convergence. Furthermore, our method enables the use of\nsignificantly smaller models to achieve the same level of performance,\npromoting a more efficient use of model capacity. Future work will focus on\ncoupling the iterative initialization method with the optimization momentum\nterm and examining the method's impact on generative frameworks.\n", "link": "http://arxiv.org/abs/2411.03226v2", "date": "2025-05-22", "relevancy": 2.4212, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4966}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Map%20Similarity%20Reduction%20in%20Convolutional%20Neural%20Networks&body=Title%3A%20Feature%20Map%20Similarity%20Reduction%20in%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Zakariae%20Belmekki%20and%20Jun%20Li%20and%20Patrick%20Reuter%20and%20David%20Antonio%20G%C3%B3mez%20J%C3%A1uregui%20and%20Karl%20Jenkins%0AAbstract%3A%20%20%20It%20has%20been%20observed%20that%20Convolutional%20Neural%20Networks%20%28CNNs%29%20suffer%20from%0Aredundancy%20in%20feature%20maps%2C%20leading%20to%20inefficient%20capacity%20utilization.%0AEfforts%20to%20address%20this%20issue%20have%20largely%20focused%20on%20kernel%20orthogonality%0Amethod.%20In%20this%20work%2C%20we%20theoretically%20and%20empirically%20demonstrate%20that%20kernel%0Aorthogonality%20does%20not%20necessarily%20lead%20to%20a%20reduction%20in%20feature%20map%0Aredundancy.%20Based%20on%20this%20analysis%2C%20we%20propose%20the%20Convolutional%20Similarity%0Amethod%20to%20reduce%20feature%20map%20similarity%2C%20independently%20of%20the%20CNN%27s%20input.%20The%0AConvolutional%20Similarity%20can%20be%20minimized%20as%20either%20a%20regularization%20term%20or%20an%0Aiterative%20initialization%20method.%20Experimental%20results%20show%20that%20minimizing%0AConvolutional%20Similarity%20not%20only%20improves%20classification%20accuracy%20but%20also%0Aaccelerates%20convergence.%20Furthermore%2C%20our%20method%20enables%20the%20use%20of%0Asignificantly%20smaller%20models%20to%20achieve%20the%20same%20level%20of%20performance%2C%0Apromoting%20a%20more%20efficient%20use%20of%20model%20capacity.%20Future%20work%20will%20focus%20on%0Acoupling%20the%20iterative%20initialization%20method%20with%20the%20optimization%20momentum%0Aterm%20and%20examining%20the%20method%27s%20impact%20on%20generative%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03226v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Map%2520Similarity%2520Reduction%2520in%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DZakariae%2520Belmekki%2520and%2520Jun%2520Li%2520and%2520Patrick%2520Reuter%2520and%2520David%2520Antonio%2520G%25C3%25B3mez%2520J%25C3%25A1uregui%2520and%2520Karl%2520Jenkins%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520observed%2520that%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520suffer%2520from%250Aredundancy%2520in%2520feature%2520maps%252C%2520leading%2520to%2520inefficient%2520capacity%2520utilization.%250AEfforts%2520to%2520address%2520this%2520issue%2520have%2520largely%2520focused%2520on%2520kernel%2520orthogonality%250Amethod.%2520In%2520this%2520work%252C%2520we%2520theoretically%2520and%2520empirically%2520demonstrate%2520that%2520kernel%250Aorthogonality%2520does%2520not%2520necessarily%2520lead%2520to%2520a%2520reduction%2520in%2520feature%2520map%250Aredundancy.%2520Based%2520on%2520this%2520analysis%252C%2520we%2520propose%2520the%2520Convolutional%2520Similarity%250Amethod%2520to%2520reduce%2520feature%2520map%2520similarity%252C%2520independently%2520of%2520the%2520CNN%2527s%2520input.%2520The%250AConvolutional%2520Similarity%2520can%2520be%2520minimized%2520as%2520either%2520a%2520regularization%2520term%2520or%2520an%250Aiterative%2520initialization%2520method.%2520Experimental%2520results%2520show%2520that%2520minimizing%250AConvolutional%2520Similarity%2520not%2520only%2520improves%2520classification%2520accuracy%2520but%2520also%250Aaccelerates%2520convergence.%2520Furthermore%252C%2520our%2520method%2520enables%2520the%2520use%2520of%250Asignificantly%2520smaller%2520models%2520to%2520achieve%2520the%2520same%2520level%2520of%2520performance%252C%250Apromoting%2520a%2520more%2520efficient%2520use%2520of%2520model%2520capacity.%2520Future%2520work%2520will%2520focus%2520on%250Acoupling%2520the%2520iterative%2520initialization%2520method%2520with%2520the%2520optimization%2520momentum%250Aterm%2520and%2520examining%2520the%2520method%2527s%2520impact%2520on%2520generative%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03226v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Map%20Similarity%20Reduction%20in%20Convolutional%20Neural%20Networks&entry.906535625=Zakariae%20Belmekki%20and%20Jun%20Li%20and%20Patrick%20Reuter%20and%20David%20Antonio%20G%C3%B3mez%20J%C3%A1uregui%20and%20Karl%20Jenkins&entry.1292438233=%20%20It%20has%20been%20observed%20that%20Convolutional%20Neural%20Networks%20%28CNNs%29%20suffer%20from%0Aredundancy%20in%20feature%20maps%2C%20leading%20to%20inefficient%20capacity%20utilization.%0AEfforts%20to%20address%20this%20issue%20have%20largely%20focused%20on%20kernel%20orthogonality%0Amethod.%20In%20this%20work%2C%20we%20theoretically%20and%20empirically%20demonstrate%20that%20kernel%0Aorthogonality%20does%20not%20necessarily%20lead%20to%20a%20reduction%20in%20feature%20map%0Aredundancy.%20Based%20on%20this%20analysis%2C%20we%20propose%20the%20Convolutional%20Similarity%0Amethod%20to%20reduce%20feature%20map%20similarity%2C%20independently%20of%20the%20CNN%27s%20input.%20The%0AConvolutional%20Similarity%20can%20be%20minimized%20as%20either%20a%20regularization%20term%20or%20an%0Aiterative%20initialization%20method.%20Experimental%20results%20show%20that%20minimizing%0AConvolutional%20Similarity%20not%20only%20improves%20classification%20accuracy%20but%20also%0Aaccelerates%20convergence.%20Furthermore%2C%20our%20method%20enables%20the%20use%20of%0Asignificantly%20smaller%20models%20to%20achieve%20the%20same%20level%20of%20performance%2C%0Apromoting%20a%20more%20efficient%20use%20of%20model%20capacity.%20Future%20work%20will%20focus%20on%0Acoupling%20the%20iterative%20initialization%20method%20with%20the%20optimization%20momentum%0Aterm%20and%20examining%20the%20method%27s%20impact%20on%20generative%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03226v2&entry.124074799=Read"},
{"title": "One-Step Diffusion-Based Image Compression with Semantic Distillation", "author": "Naifu Xue and Zhaoyang Jia and Jiahao Li and Bin Li and Yuan Zhang and Yan Lu", "abstract": "  While recent diffusion-based generative image codecs have shown impressive\nperformance, their iterative sampling process introduces unpleasing latency. In\nthis work, we revisit the design of a diffusion-based codec and argue that\nmulti-step sampling is not necessary for generative compression. Based on this\ninsight, we propose OneDC, a One-step Diffusion-based generative image Codec --\nthat integrates a latent compression module with a one-step diffusion\ngenerator. Recognizing the critical role of semantic guidance in one-step\ndiffusion, we propose using the hyperprior as a semantic signal, overcoming the\nlimitations of text prompts in representing complex visual content. To further\nenhance the semantic capability of the hyperprior, we introduce a semantic\ndistillation mechanism that transfers knowledge from a pretrained generative\ntokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and\nlatent-domain optimization to jointly enhance both reconstruction fidelity and\nperceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA\nperceptual quality even with one-step generation, offering over 40% bitrate\nreduction and 20x faster decoding compared to prior multi-step diffusion-based\ncodecs. Code will be released later.\n", "link": "http://arxiv.org/abs/2505.16687v1", "date": "2025-05-22", "relevancy": 2.417, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6896}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5961}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Step%20Diffusion-Based%20Image%20Compression%20with%20Semantic%20Distillation&body=Title%3A%20One-Step%20Diffusion-Based%20Image%20Compression%20with%20Semantic%20Distillation%0AAuthor%3A%20Naifu%20Xue%20and%20Zhaoyang%20Jia%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Yuan%20Zhang%20and%20Yan%20Lu%0AAbstract%3A%20%20%20While%20recent%20diffusion-based%20generative%20image%20codecs%20have%20shown%20impressive%0Aperformance%2C%20their%20iterative%20sampling%20process%20introduces%20unpleasing%20latency.%20In%0Athis%20work%2C%20we%20revisit%20the%20design%20of%20a%20diffusion-based%20codec%20and%20argue%20that%0Amulti-step%20sampling%20is%20not%20necessary%20for%20generative%20compression.%20Based%20on%20this%0Ainsight%2C%20we%20propose%20OneDC%2C%20a%20One-step%20Diffusion-based%20generative%20image%20Codec%20--%0Athat%20integrates%20a%20latent%20compression%20module%20with%20a%20one-step%20diffusion%0Agenerator.%20Recognizing%20the%20critical%20role%20of%20semantic%20guidance%20in%20one-step%0Adiffusion%2C%20we%20propose%20using%20the%20hyperprior%20as%20a%20semantic%20signal%2C%20overcoming%20the%0Alimitations%20of%20text%20prompts%20in%20representing%20complex%20visual%20content.%20To%20further%0Aenhance%20the%20semantic%20capability%20of%20the%20hyperprior%2C%20we%20introduce%20a%20semantic%0Adistillation%20mechanism%20that%20transfers%20knowledge%20from%20a%20pretrained%20generative%0Atokenizer%20to%20the%20hyperprior%20codec.%20Additionally%2C%20we%20adopt%20a%20hybrid%20pixel-%20and%0Alatent-domain%20optimization%20to%20jointly%20enhance%20both%20reconstruction%20fidelity%20and%0Aperceptual%20realism.%20Extensive%20experiments%20demonstrate%20that%20OneDC%20achieves%20SOTA%0Aperceptual%20quality%20even%20with%20one-step%20generation%2C%20offering%20over%2040%25%20bitrate%0Areduction%20and%2020x%20faster%20decoding%20compared%20to%20prior%20multi-step%20diffusion-based%0Acodecs.%20Code%20will%20be%20released%20later.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Step%2520Diffusion-Based%2520Image%2520Compression%2520with%2520Semantic%2520Distillation%26entry.906535625%3DNaifu%2520Xue%2520and%2520Zhaoyang%2520Jia%2520and%2520Jiahao%2520Li%2520and%2520Bin%2520Li%2520and%2520Yuan%2520Zhang%2520and%2520Yan%2520Lu%26entry.1292438233%3D%2520%2520While%2520recent%2520diffusion-based%2520generative%2520image%2520codecs%2520have%2520shown%2520impressive%250Aperformance%252C%2520their%2520iterative%2520sampling%2520process%2520introduces%2520unpleasing%2520latency.%2520In%250Athis%2520work%252C%2520we%2520revisit%2520the%2520design%2520of%2520a%2520diffusion-based%2520codec%2520and%2520argue%2520that%250Amulti-step%2520sampling%2520is%2520not%2520necessary%2520for%2520generative%2520compression.%2520Based%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520OneDC%252C%2520a%2520One-step%2520Diffusion-based%2520generative%2520image%2520Codec%2520--%250Athat%2520integrates%2520a%2520latent%2520compression%2520module%2520with%2520a%2520one-step%2520diffusion%250Agenerator.%2520Recognizing%2520the%2520critical%2520role%2520of%2520semantic%2520guidance%2520in%2520one-step%250Adiffusion%252C%2520we%2520propose%2520using%2520the%2520hyperprior%2520as%2520a%2520semantic%2520signal%252C%2520overcoming%2520the%250Alimitations%2520of%2520text%2520prompts%2520in%2520representing%2520complex%2520visual%2520content.%2520To%2520further%250Aenhance%2520the%2520semantic%2520capability%2520of%2520the%2520hyperprior%252C%2520we%2520introduce%2520a%2520semantic%250Adistillation%2520mechanism%2520that%2520transfers%2520knowledge%2520from%2520a%2520pretrained%2520generative%250Atokenizer%2520to%2520the%2520hyperprior%2520codec.%2520Additionally%252C%2520we%2520adopt%2520a%2520hybrid%2520pixel-%2520and%250Alatent-domain%2520optimization%2520to%2520jointly%2520enhance%2520both%2520reconstruction%2520fidelity%2520and%250Aperceptual%2520realism.%2520Extensive%2520experiments%2520demonstrate%2520that%2520OneDC%2520achieves%2520SOTA%250Aperceptual%2520quality%2520even%2520with%2520one-step%2520generation%252C%2520offering%2520over%252040%2525%2520bitrate%250Areduction%2520and%252020x%2520faster%2520decoding%2520compared%2520to%2520prior%2520multi-step%2520diffusion-based%250Acodecs.%2520Code%2520will%2520be%2520released%2520later.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Step%20Diffusion-Based%20Image%20Compression%20with%20Semantic%20Distillation&entry.906535625=Naifu%20Xue%20and%20Zhaoyang%20Jia%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Yuan%20Zhang%20and%20Yan%20Lu&entry.1292438233=%20%20While%20recent%20diffusion-based%20generative%20image%20codecs%20have%20shown%20impressive%0Aperformance%2C%20their%20iterative%20sampling%20process%20introduces%20unpleasing%20latency.%20In%0Athis%20work%2C%20we%20revisit%20the%20design%20of%20a%20diffusion-based%20codec%20and%20argue%20that%0Amulti-step%20sampling%20is%20not%20necessary%20for%20generative%20compression.%20Based%20on%20this%0Ainsight%2C%20we%20propose%20OneDC%2C%20a%20One-step%20Diffusion-based%20generative%20image%20Codec%20--%0Athat%20integrates%20a%20latent%20compression%20module%20with%20a%20one-step%20diffusion%0Agenerator.%20Recognizing%20the%20critical%20role%20of%20semantic%20guidance%20in%20one-step%0Adiffusion%2C%20we%20propose%20using%20the%20hyperprior%20as%20a%20semantic%20signal%2C%20overcoming%20the%0Alimitations%20of%20text%20prompts%20in%20representing%20complex%20visual%20content.%20To%20further%0Aenhance%20the%20semantic%20capability%20of%20the%20hyperprior%2C%20we%20introduce%20a%20semantic%0Adistillation%20mechanism%20that%20transfers%20knowledge%20from%20a%20pretrained%20generative%0Atokenizer%20to%20the%20hyperprior%20codec.%20Additionally%2C%20we%20adopt%20a%20hybrid%20pixel-%20and%0Alatent-domain%20optimization%20to%20jointly%20enhance%20both%20reconstruction%20fidelity%20and%0Aperceptual%20realism.%20Extensive%20experiments%20demonstrate%20that%20OneDC%20achieves%20SOTA%0Aperceptual%20quality%20even%20with%20one-step%20generation%2C%20offering%20over%2040%25%20bitrate%0Areduction%20and%2020x%20faster%20decoding%20compared%20to%20prior%20multi-step%20diffusion-based%0Acodecs.%20Code%20will%20be%20released%20later.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16687v1&entry.124074799=Read"},
{"title": "Semantic Compression of 3D Objects for Open and Collaborative Virtual\n  Worlds", "author": "Jordan Dotzel and Tony Montes and Mohamed S. Abdelfattah and Zhiru Zhang", "abstract": "  Traditional methods for 3D object compression operate only on structural\ninformation within the object vertices, polygons, and textures. These methods\nare effective at compression rates up to 10x for standard object sizes but\nquickly deteriorate at higher compression rates with texture artifacts,\nlow-polygon counts, and mesh gaps. In contrast, semantic compression ignores\nstructural information and operates directly on the core concepts to push to\nextreme levels of compression. In addition, it uses natural language as its\nstorage format, which makes it natively human-readable and a natural fit for\nemerging applications built around large-scale, collaborative projects within\naugmented and virtual reality. It deprioritizes structural information like\nlocation, size, and orientation and predicts the missing information with\nstate-of-the-art deep generative models. In this work, we construct a pipeline\nfor 3D semantic compression from public generative models and explore the\nquality-compression frontier for 3D object compression. We apply this pipeline\nto achieve rates as high as 105x for 3D objects taken from the Objaverse\ndataset and show that semantic compression can outperform traditional methods\nin the important quality-preserving region around 100x compression.\n", "link": "http://arxiv.org/abs/2505.16679v1", "date": "2025-05-22", "relevancy": 2.4045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6016}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Compression%20of%203D%20Objects%20for%20Open%20and%20Collaborative%20Virtual%0A%20%20Worlds&body=Title%3A%20Semantic%20Compression%20of%203D%20Objects%20for%20Open%20and%20Collaborative%20Virtual%0A%20%20Worlds%0AAuthor%3A%20Jordan%20Dotzel%20and%20Tony%20Montes%20and%20Mohamed%20S.%20Abdelfattah%20and%20Zhiru%20Zhang%0AAbstract%3A%20%20%20Traditional%20methods%20for%203D%20object%20compression%20operate%20only%20on%20structural%0Ainformation%20within%20the%20object%20vertices%2C%20polygons%2C%20and%20textures.%20These%20methods%0Aare%20effective%20at%20compression%20rates%20up%20to%2010x%20for%20standard%20object%20sizes%20but%0Aquickly%20deteriorate%20at%20higher%20compression%20rates%20with%20texture%20artifacts%2C%0Alow-polygon%20counts%2C%20and%20mesh%20gaps.%20In%20contrast%2C%20semantic%20compression%20ignores%0Astructural%20information%20and%20operates%20directly%20on%20the%20core%20concepts%20to%20push%20to%0Aextreme%20levels%20of%20compression.%20In%20addition%2C%20it%20uses%20natural%20language%20as%20its%0Astorage%20format%2C%20which%20makes%20it%20natively%20human-readable%20and%20a%20natural%20fit%20for%0Aemerging%20applications%20built%20around%20large-scale%2C%20collaborative%20projects%20within%0Aaugmented%20and%20virtual%20reality.%20It%20deprioritizes%20structural%20information%20like%0Alocation%2C%20size%2C%20and%20orientation%20and%20predicts%20the%20missing%20information%20with%0Astate-of-the-art%20deep%20generative%20models.%20In%20this%20work%2C%20we%20construct%20a%20pipeline%0Afor%203D%20semantic%20compression%20from%20public%20generative%20models%20and%20explore%20the%0Aquality-compression%20frontier%20for%203D%20object%20compression.%20We%20apply%20this%20pipeline%0Ato%20achieve%20rates%20as%20high%20as%20105x%20for%203D%20objects%20taken%20from%20the%20Objaverse%0Adataset%20and%20show%20that%20semantic%20compression%20can%20outperform%20traditional%20methods%0Ain%20the%20important%20quality-preserving%20region%20around%20100x%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Compression%2520of%25203D%2520Objects%2520for%2520Open%2520and%2520Collaborative%2520Virtual%250A%2520%2520Worlds%26entry.906535625%3DJordan%2520Dotzel%2520and%2520Tony%2520Montes%2520and%2520Mohamed%2520S.%2520Abdelfattah%2520and%2520Zhiru%2520Zhang%26entry.1292438233%3D%2520%2520Traditional%2520methods%2520for%25203D%2520object%2520compression%2520operate%2520only%2520on%2520structural%250Ainformation%2520within%2520the%2520object%2520vertices%252C%2520polygons%252C%2520and%2520textures.%2520These%2520methods%250Aare%2520effective%2520at%2520compression%2520rates%2520up%2520to%252010x%2520for%2520standard%2520object%2520sizes%2520but%250Aquickly%2520deteriorate%2520at%2520higher%2520compression%2520rates%2520with%2520texture%2520artifacts%252C%250Alow-polygon%2520counts%252C%2520and%2520mesh%2520gaps.%2520In%2520contrast%252C%2520semantic%2520compression%2520ignores%250Astructural%2520information%2520and%2520operates%2520directly%2520on%2520the%2520core%2520concepts%2520to%2520push%2520to%250Aextreme%2520levels%2520of%2520compression.%2520In%2520addition%252C%2520it%2520uses%2520natural%2520language%2520as%2520its%250Astorage%2520format%252C%2520which%2520makes%2520it%2520natively%2520human-readable%2520and%2520a%2520natural%2520fit%2520for%250Aemerging%2520applications%2520built%2520around%2520large-scale%252C%2520collaborative%2520projects%2520within%250Aaugmented%2520and%2520virtual%2520reality.%2520It%2520deprioritizes%2520structural%2520information%2520like%250Alocation%252C%2520size%252C%2520and%2520orientation%2520and%2520predicts%2520the%2520missing%2520information%2520with%250Astate-of-the-art%2520deep%2520generative%2520models.%2520In%2520this%2520work%252C%2520we%2520construct%2520a%2520pipeline%250Afor%25203D%2520semantic%2520compression%2520from%2520public%2520generative%2520models%2520and%2520explore%2520the%250Aquality-compression%2520frontier%2520for%25203D%2520object%2520compression.%2520We%2520apply%2520this%2520pipeline%250Ato%2520achieve%2520rates%2520as%2520high%2520as%2520105x%2520for%25203D%2520objects%2520taken%2520from%2520the%2520Objaverse%250Adataset%2520and%2520show%2520that%2520semantic%2520compression%2520can%2520outperform%2520traditional%2520methods%250Ain%2520the%2520important%2520quality-preserving%2520region%2520around%2520100x%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Compression%20of%203D%20Objects%20for%20Open%20and%20Collaborative%20Virtual%0A%20%20Worlds&entry.906535625=Jordan%20Dotzel%20and%20Tony%20Montes%20and%20Mohamed%20S.%20Abdelfattah%20and%20Zhiru%20Zhang&entry.1292438233=%20%20Traditional%20methods%20for%203D%20object%20compression%20operate%20only%20on%20structural%0Ainformation%20within%20the%20object%20vertices%2C%20polygons%2C%20and%20textures.%20These%20methods%0Aare%20effective%20at%20compression%20rates%20up%20to%2010x%20for%20standard%20object%20sizes%20but%0Aquickly%20deteriorate%20at%20higher%20compression%20rates%20with%20texture%20artifacts%2C%0Alow-polygon%20counts%2C%20and%20mesh%20gaps.%20In%20contrast%2C%20semantic%20compression%20ignores%0Astructural%20information%20and%20operates%20directly%20on%20the%20core%20concepts%20to%20push%20to%0Aextreme%20levels%20of%20compression.%20In%20addition%2C%20it%20uses%20natural%20language%20as%20its%0Astorage%20format%2C%20which%20makes%20it%20natively%20human-readable%20and%20a%20natural%20fit%20for%0Aemerging%20applications%20built%20around%20large-scale%2C%20collaborative%20projects%20within%0Aaugmented%20and%20virtual%20reality.%20It%20deprioritizes%20structural%20information%20like%0Alocation%2C%20size%2C%20and%20orientation%20and%20predicts%20the%20missing%20information%20with%0Astate-of-the-art%20deep%20generative%20models.%20In%20this%20work%2C%20we%20construct%20a%20pipeline%0Afor%203D%20semantic%20compression%20from%20public%20generative%20models%20and%20explore%20the%0Aquality-compression%20frontier%20for%203D%20object%20compression.%20We%20apply%20this%20pipeline%0Ato%20achieve%20rates%20as%20high%20as%20105x%20for%203D%20objects%20taken%20from%20the%20Objaverse%0Adataset%20and%20show%20that%20semantic%20compression%20can%20outperform%20traditional%20methods%0Ain%20the%20important%20quality-preserving%20region%20around%20100x%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16679v1&entry.124074799=Read"},
{"title": "Sufficient conditions for offline reactivation in recurrent neural\n  networks", "author": "Nanda H. Krishna and Colin Bredenberg and Daniel Levenstein and Blake A. Richards and Guillaume Lajoie", "abstract": "  During periods of quiescence, such as sleep, neural activity in many brain\ncircuits resembles that observed during periods of task engagement. However,\nthe precise conditions under which task-optimized networks can autonomously\nreactivate the same network states responsible for online behavior is poorly\nunderstood. In this study, we develop a mathematical framework that outlines\nsufficient conditions for the emergence of neural reactivation in circuits that\nencode features of smoothly varying stimuli. We demonstrate mathematically that\nnoisy recurrent networks optimized to track environmental state variables using\nchange-based sensory information naturally develop denoising dynamics, which,\nin the absence of input, cause the network to revisit state configurations\nobserved during periods of online activity. We validate our findings using\nnumerical experiments on two canonical neuroscience tasks: spatial position\nestimation based on self-motion cues, and head direction estimation based on\nangular velocity cues. Overall, our work provides theoretical support for\nmodeling offline reactivation as an emergent consequence of task optimization\nin noisy neural circuits.\n", "link": "http://arxiv.org/abs/2505.17003v1", "date": "2025-05-22", "relevancy": 2.3873, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4826}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4774}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sufficient%20conditions%20for%20offline%20reactivation%20in%20recurrent%20neural%0A%20%20networks&body=Title%3A%20Sufficient%20conditions%20for%20offline%20reactivation%20in%20recurrent%20neural%0A%20%20networks%0AAuthor%3A%20Nanda%20H.%20Krishna%20and%20Colin%20Bredenberg%20and%20Daniel%20Levenstein%20and%20Blake%20A.%20Richards%20and%20Guillaume%20Lajoie%0AAbstract%3A%20%20%20During%20periods%20of%20quiescence%2C%20such%20as%20sleep%2C%20neural%20activity%20in%20many%20brain%0Acircuits%20resembles%20that%20observed%20during%20periods%20of%20task%20engagement.%20However%2C%0Athe%20precise%20conditions%20under%20which%20task-optimized%20networks%20can%20autonomously%0Areactivate%20the%20same%20network%20states%20responsible%20for%20online%20behavior%20is%20poorly%0Aunderstood.%20In%20this%20study%2C%20we%20develop%20a%20mathematical%20framework%20that%20outlines%0Asufficient%20conditions%20for%20the%20emergence%20of%20neural%20reactivation%20in%20circuits%20that%0Aencode%20features%20of%20smoothly%20varying%20stimuli.%20We%20demonstrate%20mathematically%20that%0Anoisy%20recurrent%20networks%20optimized%20to%20track%20environmental%20state%20variables%20using%0Achange-based%20sensory%20information%20naturally%20develop%20denoising%20dynamics%2C%20which%2C%0Ain%20the%20absence%20of%20input%2C%20cause%20the%20network%20to%20revisit%20state%20configurations%0Aobserved%20during%20periods%20of%20online%20activity.%20We%20validate%20our%20findings%20using%0Anumerical%20experiments%20on%20two%20canonical%20neuroscience%20tasks%3A%20spatial%20position%0Aestimation%20based%20on%20self-motion%20cues%2C%20and%20head%20direction%20estimation%20based%20on%0Aangular%20velocity%20cues.%20Overall%2C%20our%20work%20provides%20theoretical%20support%20for%0Amodeling%20offline%20reactivation%20as%20an%20emergent%20consequence%20of%20task%20optimization%0Ain%20noisy%20neural%20circuits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSufficient%2520conditions%2520for%2520offline%2520reactivation%2520in%2520recurrent%2520neural%250A%2520%2520networks%26entry.906535625%3DNanda%2520H.%2520Krishna%2520and%2520Colin%2520Bredenberg%2520and%2520Daniel%2520Levenstein%2520and%2520Blake%2520A.%2520Richards%2520and%2520Guillaume%2520Lajoie%26entry.1292438233%3D%2520%2520During%2520periods%2520of%2520quiescence%252C%2520such%2520as%2520sleep%252C%2520neural%2520activity%2520in%2520many%2520brain%250Acircuits%2520resembles%2520that%2520observed%2520during%2520periods%2520of%2520task%2520engagement.%2520However%252C%250Athe%2520precise%2520conditions%2520under%2520which%2520task-optimized%2520networks%2520can%2520autonomously%250Areactivate%2520the%2520same%2520network%2520states%2520responsible%2520for%2520online%2520behavior%2520is%2520poorly%250Aunderstood.%2520In%2520this%2520study%252C%2520we%2520develop%2520a%2520mathematical%2520framework%2520that%2520outlines%250Asufficient%2520conditions%2520for%2520the%2520emergence%2520of%2520neural%2520reactivation%2520in%2520circuits%2520that%250Aencode%2520features%2520of%2520smoothly%2520varying%2520stimuli.%2520We%2520demonstrate%2520mathematically%2520that%250Anoisy%2520recurrent%2520networks%2520optimized%2520to%2520track%2520environmental%2520state%2520variables%2520using%250Achange-based%2520sensory%2520information%2520naturally%2520develop%2520denoising%2520dynamics%252C%2520which%252C%250Ain%2520the%2520absence%2520of%2520input%252C%2520cause%2520the%2520network%2520to%2520revisit%2520state%2520configurations%250Aobserved%2520during%2520periods%2520of%2520online%2520activity.%2520We%2520validate%2520our%2520findings%2520using%250Anumerical%2520experiments%2520on%2520two%2520canonical%2520neuroscience%2520tasks%253A%2520spatial%2520position%250Aestimation%2520based%2520on%2520self-motion%2520cues%252C%2520and%2520head%2520direction%2520estimation%2520based%2520on%250Aangular%2520velocity%2520cues.%2520Overall%252C%2520our%2520work%2520provides%2520theoretical%2520support%2520for%250Amodeling%2520offline%2520reactivation%2520as%2520an%2520emergent%2520consequence%2520of%2520task%2520optimization%250Ain%2520noisy%2520neural%2520circuits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sufficient%20conditions%20for%20offline%20reactivation%20in%20recurrent%20neural%0A%20%20networks&entry.906535625=Nanda%20H.%20Krishna%20and%20Colin%20Bredenberg%20and%20Daniel%20Levenstein%20and%20Blake%20A.%20Richards%20and%20Guillaume%20Lajoie&entry.1292438233=%20%20During%20periods%20of%20quiescence%2C%20such%20as%20sleep%2C%20neural%20activity%20in%20many%20brain%0Acircuits%20resembles%20that%20observed%20during%20periods%20of%20task%20engagement.%20However%2C%0Athe%20precise%20conditions%20under%20which%20task-optimized%20networks%20can%20autonomously%0Areactivate%20the%20same%20network%20states%20responsible%20for%20online%20behavior%20is%20poorly%0Aunderstood.%20In%20this%20study%2C%20we%20develop%20a%20mathematical%20framework%20that%20outlines%0Asufficient%20conditions%20for%20the%20emergence%20of%20neural%20reactivation%20in%20circuits%20that%0Aencode%20features%20of%20smoothly%20varying%20stimuli.%20We%20demonstrate%20mathematically%20that%0Anoisy%20recurrent%20networks%20optimized%20to%20track%20environmental%20state%20variables%20using%0Achange-based%20sensory%20information%20naturally%20develop%20denoising%20dynamics%2C%20which%2C%0Ain%20the%20absence%20of%20input%2C%20cause%20the%20network%20to%20revisit%20state%20configurations%0Aobserved%20during%20periods%20of%20online%20activity.%20We%20validate%20our%20findings%20using%0Anumerical%20experiments%20on%20two%20canonical%20neuroscience%20tasks%3A%20spatial%20position%0Aestimation%20based%20on%20self-motion%20cues%2C%20and%20head%20direction%20estimation%20based%20on%0Aangular%20velocity%20cues.%20Overall%2C%20our%20work%20provides%20theoretical%20support%20for%0Amodeling%20offline%20reactivation%20as%20an%20emergent%20consequence%20of%20task%20optimization%0Ain%20noisy%20neural%20circuits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17003v1&entry.124074799=Read"},
{"title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for\n  Multimodal Deepfake Detection", "author": "Jiaxin Liu and Jia Wang and Saihui Hou and Min Ren and Huijia Wu and Zhaofeng He", "abstract": "  In recent years, the rapid development of deepfake technology has given rise\nto an emerging and serious threat to public security: diffusion model-based\ndigital human generation. Unlike traditional face manipulation methods, such\nmodels can generate highly realistic videos with consistency through multimodal\ncontrol signals. Their flexibility and covertness pose severe challenges to\nexisting detection strategies. To bridge this gap, we introduce DigiFakeAV, the\nfirst large-scale multimodal digital human forgery dataset based on diffusion\nmodels. Employing five latest digital human generation methods (Sonic, Hallo,\netc.) and voice cloning method, we systematically produce a dataset comprising\n60,000 videos (8.4 million frames), covering multiple nationalities, skin\ntones, genders, and real-world scenarios, significantly enhancing data\ndiversity and realism. User studies show that the confusion rate between forged\nand real videos reaches 68%, and existing state-of-the-art (SOTA) detection\nmodels exhibit large drops in AUC values on DigiFakeAV, highlighting the\nchallenge of the dataset. To address this problem, we further propose\nDigiShield, a detection baseline based on spatiotemporal and cross-modal\nfusion. By jointly modeling the 3D spatiotemporal features of videos and the\nsemantic-acoustic features of audio, DigiShield achieves SOTA performance on\nboth the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method\neffectively identifies covert artifacts through fine-grained analysis of the\ntemporal evolution of facial features in synthetic videos.\n", "link": "http://arxiv.org/abs/2505.16512v1", "date": "2025-05-22", "relevancy": 2.3679, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5995}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5935}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Face%20Swapping%3A%20A%20Diffusion-Based%20Digital%20Human%20Benchmark%20for%0A%20%20Multimodal%20Deepfake%20Detection&body=Title%3A%20Beyond%20Face%20Swapping%3A%20A%20Diffusion-Based%20Digital%20Human%20Benchmark%20for%0A%20%20Multimodal%20Deepfake%20Detection%0AAuthor%3A%20Jiaxin%20Liu%20and%20Jia%20Wang%20and%20Saihui%20Hou%20and%20Min%20Ren%20and%20Huijia%20Wu%20and%20Zhaofeng%20He%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20rapid%20development%20of%20deepfake%20technology%20has%20given%20rise%0Ato%20an%20emerging%20and%20serious%20threat%20to%20public%20security%3A%20diffusion%20model-based%0Adigital%20human%20generation.%20Unlike%20traditional%20face%20manipulation%20methods%2C%20such%0Amodels%20can%20generate%20highly%20realistic%20videos%20with%20consistency%20through%20multimodal%0Acontrol%20signals.%20Their%20flexibility%20and%20covertness%20pose%20severe%20challenges%20to%0Aexisting%20detection%20strategies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DigiFakeAV%2C%20the%0Afirst%20large-scale%20multimodal%20digital%20human%20forgery%20dataset%20based%20on%20diffusion%0Amodels.%20Employing%20five%20latest%20digital%20human%20generation%20methods%20%28Sonic%2C%20Hallo%2C%0Aetc.%29%20and%20voice%20cloning%20method%2C%20we%20systematically%20produce%20a%20dataset%20comprising%0A60%2C000%20videos%20%288.4%20million%20frames%29%2C%20covering%20multiple%20nationalities%2C%20skin%0Atones%2C%20genders%2C%20and%20real-world%20scenarios%2C%20significantly%20enhancing%20data%0Adiversity%20and%20realism.%20User%20studies%20show%20that%20the%20confusion%20rate%20between%20forged%0Aand%20real%20videos%20reaches%2068%25%2C%20and%20existing%20state-of-the-art%20%28SOTA%29%20detection%0Amodels%20exhibit%20large%20drops%20in%20AUC%20values%20on%20DigiFakeAV%2C%20highlighting%20the%0Achallenge%20of%20the%20dataset.%20To%20address%20this%20problem%2C%20we%20further%20propose%0ADigiShield%2C%20a%20detection%20baseline%20based%20on%20spatiotemporal%20and%20cross-modal%0Afusion.%20By%20jointly%20modeling%20the%203D%20spatiotemporal%20features%20of%20videos%20and%20the%0Asemantic-acoustic%20features%20of%20audio%2C%20DigiShield%20achieves%20SOTA%20performance%20on%0Aboth%20the%20DigiFakeAV%20and%20DF-TIMIT%20datasets.%20Experiments%20show%20that%20this%20method%0Aeffectively%20identifies%20covert%20artifacts%20through%20fine-grained%20analysis%20of%20the%0Atemporal%20evolution%20of%20facial%20features%20in%20synthetic%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Face%2520Swapping%253A%2520A%2520Diffusion-Based%2520Digital%2520Human%2520Benchmark%2520for%250A%2520%2520Multimodal%2520Deepfake%2520Detection%26entry.906535625%3DJiaxin%2520Liu%2520and%2520Jia%2520Wang%2520and%2520Saihui%2520Hou%2520and%2520Min%2520Ren%2520and%2520Huijia%2520Wu%2520and%2520Zhaofeng%2520He%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520rapid%2520development%2520of%2520deepfake%2520technology%2520has%2520given%2520rise%250Ato%2520an%2520emerging%2520and%2520serious%2520threat%2520to%2520public%2520security%253A%2520diffusion%2520model-based%250Adigital%2520human%2520generation.%2520Unlike%2520traditional%2520face%2520manipulation%2520methods%252C%2520such%250Amodels%2520can%2520generate%2520highly%2520realistic%2520videos%2520with%2520consistency%2520through%2520multimodal%250Acontrol%2520signals.%2520Their%2520flexibility%2520and%2520covertness%2520pose%2520severe%2520challenges%2520to%250Aexisting%2520detection%2520strategies.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520DigiFakeAV%252C%2520the%250Afirst%2520large-scale%2520multimodal%2520digital%2520human%2520forgery%2520dataset%2520based%2520on%2520diffusion%250Amodels.%2520Employing%2520five%2520latest%2520digital%2520human%2520generation%2520methods%2520%2528Sonic%252C%2520Hallo%252C%250Aetc.%2529%2520and%2520voice%2520cloning%2520method%252C%2520we%2520systematically%2520produce%2520a%2520dataset%2520comprising%250A60%252C000%2520videos%2520%25288.4%2520million%2520frames%2529%252C%2520covering%2520multiple%2520nationalities%252C%2520skin%250Atones%252C%2520genders%252C%2520and%2520real-world%2520scenarios%252C%2520significantly%2520enhancing%2520data%250Adiversity%2520and%2520realism.%2520User%2520studies%2520show%2520that%2520the%2520confusion%2520rate%2520between%2520forged%250Aand%2520real%2520videos%2520reaches%252068%2525%252C%2520and%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520detection%250Amodels%2520exhibit%2520large%2520drops%2520in%2520AUC%2520values%2520on%2520DigiFakeAV%252C%2520highlighting%2520the%250Achallenge%2520of%2520the%2520dataset.%2520To%2520address%2520this%2520problem%252C%2520we%2520further%2520propose%250ADigiShield%252C%2520a%2520detection%2520baseline%2520based%2520on%2520spatiotemporal%2520and%2520cross-modal%250Afusion.%2520By%2520jointly%2520modeling%2520the%25203D%2520spatiotemporal%2520features%2520of%2520videos%2520and%2520the%250Asemantic-acoustic%2520features%2520of%2520audio%252C%2520DigiShield%2520achieves%2520SOTA%2520performance%2520on%250Aboth%2520the%2520DigiFakeAV%2520and%2520DF-TIMIT%2520datasets.%2520Experiments%2520show%2520that%2520this%2520method%250Aeffectively%2520identifies%2520covert%2520artifacts%2520through%2520fine-grained%2520analysis%2520of%2520the%250Atemporal%2520evolution%2520of%2520facial%2520features%2520in%2520synthetic%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Face%20Swapping%3A%20A%20Diffusion-Based%20Digital%20Human%20Benchmark%20for%0A%20%20Multimodal%20Deepfake%20Detection&entry.906535625=Jiaxin%20Liu%20and%20Jia%20Wang%20and%20Saihui%20Hou%20and%20Min%20Ren%20and%20Huijia%20Wu%20and%20Zhaofeng%20He&entry.1292438233=%20%20In%20recent%20years%2C%20the%20rapid%20development%20of%20deepfake%20technology%20has%20given%20rise%0Ato%20an%20emerging%20and%20serious%20threat%20to%20public%20security%3A%20diffusion%20model-based%0Adigital%20human%20generation.%20Unlike%20traditional%20face%20manipulation%20methods%2C%20such%0Amodels%20can%20generate%20highly%20realistic%20videos%20with%20consistency%20through%20multimodal%0Acontrol%20signals.%20Their%20flexibility%20and%20covertness%20pose%20severe%20challenges%20to%0Aexisting%20detection%20strategies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DigiFakeAV%2C%20the%0Afirst%20large-scale%20multimodal%20digital%20human%20forgery%20dataset%20based%20on%20diffusion%0Amodels.%20Employing%20five%20latest%20digital%20human%20generation%20methods%20%28Sonic%2C%20Hallo%2C%0Aetc.%29%20and%20voice%20cloning%20method%2C%20we%20systematically%20produce%20a%20dataset%20comprising%0A60%2C000%20videos%20%288.4%20million%20frames%29%2C%20covering%20multiple%20nationalities%2C%20skin%0Atones%2C%20genders%2C%20and%20real-world%20scenarios%2C%20significantly%20enhancing%20data%0Adiversity%20and%20realism.%20User%20studies%20show%20that%20the%20confusion%20rate%20between%20forged%0Aand%20real%20videos%20reaches%2068%25%2C%20and%20existing%20state-of-the-art%20%28SOTA%29%20detection%0Amodels%20exhibit%20large%20drops%20in%20AUC%20values%20on%20DigiFakeAV%2C%20highlighting%20the%0Achallenge%20of%20the%20dataset.%20To%20address%20this%20problem%2C%20we%20further%20propose%0ADigiShield%2C%20a%20detection%20baseline%20based%20on%20spatiotemporal%20and%20cross-modal%0Afusion.%20By%20jointly%20modeling%20the%203D%20spatiotemporal%20features%20of%20videos%20and%20the%0Asemantic-acoustic%20features%20of%20audio%2C%20DigiShield%20achieves%20SOTA%20performance%20on%0Aboth%20the%20DigiFakeAV%20and%20DF-TIMIT%20datasets.%20Experiments%20show%20that%20this%20method%0Aeffectively%20identifies%20covert%20artifacts%20through%20fine-grained%20analysis%20of%20the%0Atemporal%20evolution%20of%20facial%20features%20in%20synthetic%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16512v1&entry.124074799=Read"},
{"title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs", "author": "Meng-Hao Guo and Xuanyu Chu and Qianrui Yang and Zhe-Han Mo and Yiqing Shen and Pei-lin Li and Xinjie Lin and Jinnian Zhang and Xin-Sheng Chen and Yi Zhang and Kiyohiro Nakayama and Zhengyang Geng and Houwen Peng and Han Hu and Shi-Nin Hu", "abstract": "  The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv\n", "link": "http://arxiv.org/abs/2505.16770v1", "date": "2025-05-22", "relevancy": 2.3678, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RBench-V%3A%20A%20Primary%20Assessment%20for%20Visual%20Reasoning%20Models%20with%0A%20%20Multi-modal%20Outputs&body=Title%3A%20RBench-V%3A%20A%20Primary%20Assessment%20for%20Visual%20Reasoning%20Models%20with%0A%20%20Multi-modal%20Outputs%0AAuthor%3A%20Meng-Hao%20Guo%20and%20Xuanyu%20Chu%20and%20Qianrui%20Yang%20and%20Zhe-Han%20Mo%20and%20Yiqing%20Shen%20and%20Pei-lin%20Li%20and%20Xinjie%20Lin%20and%20Jinnian%20Zhang%20and%20Xin-Sheng%20Chen%20and%20Yi%20Zhang%20and%20Kiyohiro%20Nakayama%20and%20Zhengyang%20Geng%20and%20Houwen%20Peng%20and%20Han%20Hu%20and%20Shi-Nin%20Hu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20native%20multi-modal%20models%20and%20omni-models%2C%0Aexemplified%20by%20GPT-4o%2C%20Gemini%2C%20and%20o3%2C%20with%20their%20capability%20to%20process%20and%0Agenerate%20content%20across%20modalities%20such%20as%20text%20and%20images%2C%20marks%20a%20significant%0Amilestone%20in%20the%20evolution%20of%20intelligence.%20Systematic%20evaluation%20of%20their%0Amulti-modal%20output%20capabilities%20in%20visual%20thinking%20processes%20%28also%20known%20as%0Amulti-modal%20chain%20of%20thought%2C%20M-CoT%29%20becomes%20critically%20important.%20However%2C%0Aexisting%20benchmarks%20for%20evaluating%20multi-modal%20models%20primarily%20focus%20on%0Aassessing%20multi-modal%20inputs%20and%20text-only%20reasoning%20while%20neglecting%20the%0Aimportance%20of%20reasoning%20through%20multi-modal%20outputs.%20In%20this%20paper%2C%20we%20present%0Aa%20benchmark%2C%20dubbed%20RBench-V%2C%20designed%20to%20assess%20models%27%20vision-indispensable%0Areasoning%20abilities.%20To%20construct%20RBench-V%2C%20we%20carefully%20hand-pick%20803%0Aquestions%20covering%20math%2C%20physics%2C%20counting%2C%20and%20games.%20Unlike%20previous%0Abenchmarks%20that%20typically%20specify%20certain%20input%20modalities%2C%20RBench-V%20presents%0Aproblems%20centered%20on%20multi-modal%20outputs%2C%20which%20require%20image%20manipulation%20such%0Aas%20generating%20novel%20images%20and%20constructing%20auxiliary%20lines%20to%20support%20the%0Areasoning%20process.%20We%20evaluate%20numerous%20open-%20and%20closed-source%20models%20on%0ARBench-V%2C%20including%20o3%2C%20Gemini%202.5%20Pro%2C%20Qwen2.5-VL%2C%20etc.%20Even%20the%0Abest-performing%20model%2C%20o3%2C%20achieves%20only%2025.8%25%20accuracy%20on%20RBench-V%2C%20far%20below%0Athe%20human%20score%20of%2082.3%25%2C%20highlighting%20that%20current%20models%20struggle%20to%20leverage%0Amulti-modal%20reasoning.%20Data%20and%20code%20are%20available%20at%0Ahttps%3A//evalmodels.github.io/rbenchv%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRBench-V%253A%2520A%2520Primary%2520Assessment%2520for%2520Visual%2520Reasoning%2520Models%2520with%250A%2520%2520Multi-modal%2520Outputs%26entry.906535625%3DMeng-Hao%2520Guo%2520and%2520Xuanyu%2520Chu%2520and%2520Qianrui%2520Yang%2520and%2520Zhe-Han%2520Mo%2520and%2520Yiqing%2520Shen%2520and%2520Pei-lin%2520Li%2520and%2520Xinjie%2520Lin%2520and%2520Jinnian%2520Zhang%2520and%2520Xin-Sheng%2520Chen%2520and%2520Yi%2520Zhang%2520and%2520Kiyohiro%2520Nakayama%2520and%2520Zhengyang%2520Geng%2520and%2520Houwen%2520Peng%2520and%2520Han%2520Hu%2520and%2520Shi-Nin%2520Hu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520native%2520multi-modal%2520models%2520and%2520omni-models%252C%250Aexemplified%2520by%2520GPT-4o%252C%2520Gemini%252C%2520and%2520o3%252C%2520with%2520their%2520capability%2520to%2520process%2520and%250Agenerate%2520content%2520across%2520modalities%2520such%2520as%2520text%2520and%2520images%252C%2520marks%2520a%2520significant%250Amilestone%2520in%2520the%2520evolution%2520of%2520intelligence.%2520Systematic%2520evaluation%2520of%2520their%250Amulti-modal%2520output%2520capabilities%2520in%2520visual%2520thinking%2520processes%2520%2528also%2520known%2520as%250Amulti-modal%2520chain%2520of%2520thought%252C%2520M-CoT%2529%2520becomes%2520critically%2520important.%2520However%252C%250Aexisting%2520benchmarks%2520for%2520evaluating%2520multi-modal%2520models%2520primarily%2520focus%2520on%250Aassessing%2520multi-modal%2520inputs%2520and%2520text-only%2520reasoning%2520while%2520neglecting%2520the%250Aimportance%2520of%2520reasoning%2520through%2520multi-modal%2520outputs.%2520In%2520this%2520paper%252C%2520we%2520present%250Aa%2520benchmark%252C%2520dubbed%2520RBench-V%252C%2520designed%2520to%2520assess%2520models%2527%2520vision-indispensable%250Areasoning%2520abilities.%2520To%2520construct%2520RBench-V%252C%2520we%2520carefully%2520hand-pick%2520803%250Aquestions%2520covering%2520math%252C%2520physics%252C%2520counting%252C%2520and%2520games.%2520Unlike%2520previous%250Abenchmarks%2520that%2520typically%2520specify%2520certain%2520input%2520modalities%252C%2520RBench-V%2520presents%250Aproblems%2520centered%2520on%2520multi-modal%2520outputs%252C%2520which%2520require%2520image%2520manipulation%2520such%250Aas%2520generating%2520novel%2520images%2520and%2520constructing%2520auxiliary%2520lines%2520to%2520support%2520the%250Areasoning%2520process.%2520We%2520evaluate%2520numerous%2520open-%2520and%2520closed-source%2520models%2520on%250ARBench-V%252C%2520including%2520o3%252C%2520Gemini%25202.5%2520Pro%252C%2520Qwen2.5-VL%252C%2520etc.%2520Even%2520the%250Abest-performing%2520model%252C%2520o3%252C%2520achieves%2520only%252025.8%2525%2520accuracy%2520on%2520RBench-V%252C%2520far%2520below%250Athe%2520human%2520score%2520of%252082.3%2525%252C%2520highlighting%2520that%2520current%2520models%2520struggle%2520to%2520leverage%250Amulti-modal%2520reasoning.%2520Data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//evalmodels.github.io/rbenchv%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RBench-V%3A%20A%20Primary%20Assessment%20for%20Visual%20Reasoning%20Models%20with%0A%20%20Multi-modal%20Outputs&entry.906535625=Meng-Hao%20Guo%20and%20Xuanyu%20Chu%20and%20Qianrui%20Yang%20and%20Zhe-Han%20Mo%20and%20Yiqing%20Shen%20and%20Pei-lin%20Li%20and%20Xinjie%20Lin%20and%20Jinnian%20Zhang%20and%20Xin-Sheng%20Chen%20and%20Yi%20Zhang%20and%20Kiyohiro%20Nakayama%20and%20Zhengyang%20Geng%20and%20Houwen%20Peng%20and%20Han%20Hu%20and%20Shi-Nin%20Hu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20native%20multi-modal%20models%20and%20omni-models%2C%0Aexemplified%20by%20GPT-4o%2C%20Gemini%2C%20and%20o3%2C%20with%20their%20capability%20to%20process%20and%0Agenerate%20content%20across%20modalities%20such%20as%20text%20and%20images%2C%20marks%20a%20significant%0Amilestone%20in%20the%20evolution%20of%20intelligence.%20Systematic%20evaluation%20of%20their%0Amulti-modal%20output%20capabilities%20in%20visual%20thinking%20processes%20%28also%20known%20as%0Amulti-modal%20chain%20of%20thought%2C%20M-CoT%29%20becomes%20critically%20important.%20However%2C%0Aexisting%20benchmarks%20for%20evaluating%20multi-modal%20models%20primarily%20focus%20on%0Aassessing%20multi-modal%20inputs%20and%20text-only%20reasoning%20while%20neglecting%20the%0Aimportance%20of%20reasoning%20through%20multi-modal%20outputs.%20In%20this%20paper%2C%20we%20present%0Aa%20benchmark%2C%20dubbed%20RBench-V%2C%20designed%20to%20assess%20models%27%20vision-indispensable%0Areasoning%20abilities.%20To%20construct%20RBench-V%2C%20we%20carefully%20hand-pick%20803%0Aquestions%20covering%20math%2C%20physics%2C%20counting%2C%20and%20games.%20Unlike%20previous%0Abenchmarks%20that%20typically%20specify%20certain%20input%20modalities%2C%20RBench-V%20presents%0Aproblems%20centered%20on%20multi-modal%20outputs%2C%20which%20require%20image%20manipulation%20such%0Aas%20generating%20novel%20images%20and%20constructing%20auxiliary%20lines%20to%20support%20the%0Areasoning%20process.%20We%20evaluate%20numerous%20open-%20and%20closed-source%20models%20on%0ARBench-V%2C%20including%20o3%2C%20Gemini%202.5%20Pro%2C%20Qwen2.5-VL%2C%20etc.%20Even%20the%0Abest-performing%20model%2C%20o3%2C%20achieves%20only%2025.8%25%20accuracy%20on%20RBench-V%2C%20far%20below%0Athe%20human%20score%20of%2082.3%25%2C%20highlighting%20that%20current%20models%20struggle%20to%20leverage%0Amulti-modal%20reasoning.%20Data%20and%20code%20are%20available%20at%0Ahttps%3A//evalmodels.github.io/rbenchv%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16770v1&entry.124074799=Read"},
{"title": "CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation", "author": "Haihong Hao and Mingfei Han and Changlin Li and Zhihui Li and Xiaojun Chang", "abstract": "  Embodied navigation demands comprehensive scene understanding and precise\nspatial reasoning. While image-text models excel at interpreting pixel-level\ncolor and lighting cues, 3D-text models capture volumetric structure and\nspatial relationships. However, unified fusion approaches that jointly fuse 2D\nimages, 3D point clouds, and textual instructions face challenges in limited\navailability of triple-modality data and difficulty resolving conflicting\nbeliefs among modalities. In this work, we introduce CoNav, a collaborative\ncross-modal reasoning framework where a pretrained 3D-text model explicitly\nguides an image-text navigation agent by providing structured spatial-semantic\nknowledge to resolve ambiguities during navigation. Specifically, we introduce\nCross-Modal Belief Alignment, which operationalizes this cross-modal guidance\nby simply sharing textual hypotheses from the 3D-text model to the navigation\nagent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the\nnavigation agent learns to integrate visual cues with spatial-semantic\nknowledge derived from the 3D-text model, enabling effective reasoning in\nembodied navigation. CoNav achieves significant improvements on four standard\nembodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial\nreasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success\nRate, CoNav often generates shorter paths compared to other methods (as\nmeasured by SPL), showcasing the potential and challenges of fusing data from\ndifferent modalities in embodied navigation. Project Page:\nhttps://oceanhao.github.io/CoNav/\n", "link": "http://arxiv.org/abs/2505.16663v1", "date": "2025-05-22", "relevancy": 2.3669, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoNav%3A%20Collaborative%20Cross-Modal%20Reasoning%20for%20Embodied%20Navigation&body=Title%3A%20CoNav%3A%20Collaborative%20Cross-Modal%20Reasoning%20for%20Embodied%20Navigation%0AAuthor%3A%20Haihong%20Hao%20and%20Mingfei%20Han%20and%20Changlin%20Li%20and%20Zhihui%20Li%20and%20Xiaojun%20Chang%0AAbstract%3A%20%20%20Embodied%20navigation%20demands%20comprehensive%20scene%20understanding%20and%20precise%0Aspatial%20reasoning.%20While%20image-text%20models%20excel%20at%20interpreting%20pixel-level%0Acolor%20and%20lighting%20cues%2C%203D-text%20models%20capture%20volumetric%20structure%20and%0Aspatial%20relationships.%20However%2C%20unified%20fusion%20approaches%20that%20jointly%20fuse%202D%0Aimages%2C%203D%20point%20clouds%2C%20and%20textual%20instructions%20face%20challenges%20in%20limited%0Aavailability%20of%20triple-modality%20data%20and%20difficulty%20resolving%20conflicting%0Abeliefs%20among%20modalities.%20In%20this%20work%2C%20we%20introduce%20CoNav%2C%20a%20collaborative%0Across-modal%20reasoning%20framework%20where%20a%20pretrained%203D-text%20model%20explicitly%0Aguides%20an%20image-text%20navigation%20agent%20by%20providing%20structured%20spatial-semantic%0Aknowledge%20to%20resolve%20ambiguities%20during%20navigation.%20Specifically%2C%20we%20introduce%0ACross-Modal%20Belief%20Alignment%2C%20which%20operationalizes%20this%20cross-modal%20guidance%0Aby%20simply%20sharing%20textual%20hypotheses%20from%20the%203D-text%20model%20to%20the%20navigation%0Aagent.%20Through%20lightweight%20fine-tuning%20on%20a%20small%202D-3D-text%20corpus%2C%20the%0Anavigation%20agent%20learns%20to%20integrate%20visual%20cues%20with%20spatial-semantic%0Aknowledge%20derived%20from%20the%203D-text%20model%2C%20enabling%20effective%20reasoning%20in%0Aembodied%20navigation.%20CoNav%20achieves%20significant%20improvements%20on%20four%20standard%0Aembodied%20navigation%20benchmarks%20%28R2R%2C%20CVDN%2C%20REVERIE%2C%20SOON%29%20and%20two%20spatial%0Areasoning%20benchmarks%20%28ScanQA%2C%20SQA3D%29.%20Moreover%2C%20under%20close%20navigation%20Success%0ARate%2C%20CoNav%20often%20generates%20shorter%20paths%20compared%20to%20other%20methods%20%28as%0Ameasured%20by%20SPL%29%2C%20showcasing%20the%20potential%20and%20challenges%20of%20fusing%20data%20from%0Adifferent%20modalities%20in%20embodied%20navigation.%20Project%20Page%3A%0Ahttps%3A//oceanhao.github.io/CoNav/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoNav%253A%2520Collaborative%2520Cross-Modal%2520Reasoning%2520for%2520Embodied%2520Navigation%26entry.906535625%3DHaihong%2520Hao%2520and%2520Mingfei%2520Han%2520and%2520Changlin%2520Li%2520and%2520Zhihui%2520Li%2520and%2520Xiaojun%2520Chang%26entry.1292438233%3D%2520%2520Embodied%2520navigation%2520demands%2520comprehensive%2520scene%2520understanding%2520and%2520precise%250Aspatial%2520reasoning.%2520While%2520image-text%2520models%2520excel%2520at%2520interpreting%2520pixel-level%250Acolor%2520and%2520lighting%2520cues%252C%25203D-text%2520models%2520capture%2520volumetric%2520structure%2520and%250Aspatial%2520relationships.%2520However%252C%2520unified%2520fusion%2520approaches%2520that%2520jointly%2520fuse%25202D%250Aimages%252C%25203D%2520point%2520clouds%252C%2520and%2520textual%2520instructions%2520face%2520challenges%2520in%2520limited%250Aavailability%2520of%2520triple-modality%2520data%2520and%2520difficulty%2520resolving%2520conflicting%250Abeliefs%2520among%2520modalities.%2520In%2520this%2520work%252C%2520we%2520introduce%2520CoNav%252C%2520a%2520collaborative%250Across-modal%2520reasoning%2520framework%2520where%2520a%2520pretrained%25203D-text%2520model%2520explicitly%250Aguides%2520an%2520image-text%2520navigation%2520agent%2520by%2520providing%2520structured%2520spatial-semantic%250Aknowledge%2520to%2520resolve%2520ambiguities%2520during%2520navigation.%2520Specifically%252C%2520we%2520introduce%250ACross-Modal%2520Belief%2520Alignment%252C%2520which%2520operationalizes%2520this%2520cross-modal%2520guidance%250Aby%2520simply%2520sharing%2520textual%2520hypotheses%2520from%2520the%25203D-text%2520model%2520to%2520the%2520navigation%250Aagent.%2520Through%2520lightweight%2520fine-tuning%2520on%2520a%2520small%25202D-3D-text%2520corpus%252C%2520the%250Anavigation%2520agent%2520learns%2520to%2520integrate%2520visual%2520cues%2520with%2520spatial-semantic%250Aknowledge%2520derived%2520from%2520the%25203D-text%2520model%252C%2520enabling%2520effective%2520reasoning%2520in%250Aembodied%2520navigation.%2520CoNav%2520achieves%2520significant%2520improvements%2520on%2520four%2520standard%250Aembodied%2520navigation%2520benchmarks%2520%2528R2R%252C%2520CVDN%252C%2520REVERIE%252C%2520SOON%2529%2520and%2520two%2520spatial%250Areasoning%2520benchmarks%2520%2528ScanQA%252C%2520SQA3D%2529.%2520Moreover%252C%2520under%2520close%2520navigation%2520Success%250ARate%252C%2520CoNav%2520often%2520generates%2520shorter%2520paths%2520compared%2520to%2520other%2520methods%2520%2528as%250Ameasured%2520by%2520SPL%2529%252C%2520showcasing%2520the%2520potential%2520and%2520challenges%2520of%2520fusing%2520data%2520from%250Adifferent%2520modalities%2520in%2520embodied%2520navigation.%2520Project%2520Page%253A%250Ahttps%253A//oceanhao.github.io/CoNav/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoNav%3A%20Collaborative%20Cross-Modal%20Reasoning%20for%20Embodied%20Navigation&entry.906535625=Haihong%20Hao%20and%20Mingfei%20Han%20and%20Changlin%20Li%20and%20Zhihui%20Li%20and%20Xiaojun%20Chang&entry.1292438233=%20%20Embodied%20navigation%20demands%20comprehensive%20scene%20understanding%20and%20precise%0Aspatial%20reasoning.%20While%20image-text%20models%20excel%20at%20interpreting%20pixel-level%0Acolor%20and%20lighting%20cues%2C%203D-text%20models%20capture%20volumetric%20structure%20and%0Aspatial%20relationships.%20However%2C%20unified%20fusion%20approaches%20that%20jointly%20fuse%202D%0Aimages%2C%203D%20point%20clouds%2C%20and%20textual%20instructions%20face%20challenges%20in%20limited%0Aavailability%20of%20triple-modality%20data%20and%20difficulty%20resolving%20conflicting%0Abeliefs%20among%20modalities.%20In%20this%20work%2C%20we%20introduce%20CoNav%2C%20a%20collaborative%0Across-modal%20reasoning%20framework%20where%20a%20pretrained%203D-text%20model%20explicitly%0Aguides%20an%20image-text%20navigation%20agent%20by%20providing%20structured%20spatial-semantic%0Aknowledge%20to%20resolve%20ambiguities%20during%20navigation.%20Specifically%2C%20we%20introduce%0ACross-Modal%20Belief%20Alignment%2C%20which%20operationalizes%20this%20cross-modal%20guidance%0Aby%20simply%20sharing%20textual%20hypotheses%20from%20the%203D-text%20model%20to%20the%20navigation%0Aagent.%20Through%20lightweight%20fine-tuning%20on%20a%20small%202D-3D-text%20corpus%2C%20the%0Anavigation%20agent%20learns%20to%20integrate%20visual%20cues%20with%20spatial-semantic%0Aknowledge%20derived%20from%20the%203D-text%20model%2C%20enabling%20effective%20reasoning%20in%0Aembodied%20navigation.%20CoNav%20achieves%20significant%20improvements%20on%20four%20standard%0Aembodied%20navigation%20benchmarks%20%28R2R%2C%20CVDN%2C%20REVERIE%2C%20SOON%29%20and%20two%20spatial%0Areasoning%20benchmarks%20%28ScanQA%2C%20SQA3D%29.%20Moreover%2C%20under%20close%20navigation%20Success%0ARate%2C%20CoNav%20often%20generates%20shorter%20paths%20compared%20to%20other%20methods%20%28as%0Ameasured%20by%20SPL%29%2C%20showcasing%20the%20potential%20and%20challenges%20of%20fusing%20data%20from%0Adifferent%20modalities%20in%20embodied%20navigation.%20Project%20Page%3A%0Ahttps%3A//oceanhao.github.io/CoNav/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16663v1&entry.124074799=Read"},
{"title": "Learning Adaptive and Temporally Causal Video Tokenization in a 1D\n  Latent Space", "author": "Yan Li and Changyao Tian and Renqiu Xia and Ning Liao and Weiwei Guo and Junchi Yan and Hongsheng Li and Jifeng Dai and Hao Li and Xue Yang", "abstract": "  We propose AdapTok, an adaptive temporal causal video tokenizer that can\nflexibly allocate tokens for different frames based on video content. AdapTok\nis equipped with a block-wise masking strategy that randomly drops tail tokens\nof each block during training, and a block causal scorer to predict the\nreconstruction quality of video frames using different numbers of tokens.\nDuring inference, an adaptive token allocation strategy based on integer linear\nprogramming is further proposed to adjust token usage given predicted scores.\nSuch design allows for sample-wise, content-aware, and temporally dynamic token\nallocation under a controllable overall budget. Extensive experiments for video\nreconstruction and generation on UCF-101 and Kinetics-600 demonstrate the\neffectiveness of our approach. Without additional image data, AdapTok\nconsistently improves reconstruction quality and generation performance under\ndifferent token budgets, allowing for more scalable and token-efficient\ngenerative video modeling.\n", "link": "http://arxiv.org/abs/2505.17011v1", "date": "2025-05-22", "relevancy": 2.3662, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6108}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5896}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Adaptive%20and%20Temporally%20Causal%20Video%20Tokenization%20in%20a%201D%0A%20%20Latent%20Space&body=Title%3A%20Learning%20Adaptive%20and%20Temporally%20Causal%20Video%20Tokenization%20in%20a%201D%0A%20%20Latent%20Space%0AAuthor%3A%20Yan%20Li%20and%20Changyao%20Tian%20and%20Renqiu%20Xia%20and%20Ning%20Liao%20and%20Weiwei%20Guo%20and%20Junchi%20Yan%20and%20Hongsheng%20Li%20and%20Jifeng%20Dai%20and%20Hao%20Li%20and%20Xue%20Yang%0AAbstract%3A%20%20%20We%20propose%20AdapTok%2C%20an%20adaptive%20temporal%20causal%20video%20tokenizer%20that%20can%0Aflexibly%20allocate%20tokens%20for%20different%20frames%20based%20on%20video%20content.%20AdapTok%0Ais%20equipped%20with%20a%20block-wise%20masking%20strategy%20that%20randomly%20drops%20tail%20tokens%0Aof%20each%20block%20during%20training%2C%20and%20a%20block%20causal%20scorer%20to%20predict%20the%0Areconstruction%20quality%20of%20video%20frames%20using%20different%20numbers%20of%20tokens.%0ADuring%20inference%2C%20an%20adaptive%20token%20allocation%20strategy%20based%20on%20integer%20linear%0Aprogramming%20is%20further%20proposed%20to%20adjust%20token%20usage%20given%20predicted%20scores.%0ASuch%20design%20allows%20for%20sample-wise%2C%20content-aware%2C%20and%20temporally%20dynamic%20token%0Aallocation%20under%20a%20controllable%20overall%20budget.%20Extensive%20experiments%20for%20video%0Areconstruction%20and%20generation%20on%20UCF-101%20and%20Kinetics-600%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach.%20Without%20additional%20image%20data%2C%20AdapTok%0Aconsistently%20improves%20reconstruction%20quality%20and%20generation%20performance%20under%0Adifferent%20token%20budgets%2C%20allowing%20for%20more%20scalable%20and%20token-efficient%0Agenerative%20video%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Adaptive%2520and%2520Temporally%2520Causal%2520Video%2520Tokenization%2520in%2520a%25201D%250A%2520%2520Latent%2520Space%26entry.906535625%3DYan%2520Li%2520and%2520Changyao%2520Tian%2520and%2520Renqiu%2520Xia%2520and%2520Ning%2520Liao%2520and%2520Weiwei%2520Guo%2520and%2520Junchi%2520Yan%2520and%2520Hongsheng%2520Li%2520and%2520Jifeng%2520Dai%2520and%2520Hao%2520Li%2520and%2520Xue%2520Yang%26entry.1292438233%3D%2520%2520We%2520propose%2520AdapTok%252C%2520an%2520adaptive%2520temporal%2520causal%2520video%2520tokenizer%2520that%2520can%250Aflexibly%2520allocate%2520tokens%2520for%2520different%2520frames%2520based%2520on%2520video%2520content.%2520AdapTok%250Ais%2520equipped%2520with%2520a%2520block-wise%2520masking%2520strategy%2520that%2520randomly%2520drops%2520tail%2520tokens%250Aof%2520each%2520block%2520during%2520training%252C%2520and%2520a%2520block%2520causal%2520scorer%2520to%2520predict%2520the%250Areconstruction%2520quality%2520of%2520video%2520frames%2520using%2520different%2520numbers%2520of%2520tokens.%250ADuring%2520inference%252C%2520an%2520adaptive%2520token%2520allocation%2520strategy%2520based%2520on%2520integer%2520linear%250Aprogramming%2520is%2520further%2520proposed%2520to%2520adjust%2520token%2520usage%2520given%2520predicted%2520scores.%250ASuch%2520design%2520allows%2520for%2520sample-wise%252C%2520content-aware%252C%2520and%2520temporally%2520dynamic%2520token%250Aallocation%2520under%2520a%2520controllable%2520overall%2520budget.%2520Extensive%2520experiments%2520for%2520video%250Areconstruction%2520and%2520generation%2520on%2520UCF-101%2520and%2520Kinetics-600%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach.%2520Without%2520additional%2520image%2520data%252C%2520AdapTok%250Aconsistently%2520improves%2520reconstruction%2520quality%2520and%2520generation%2520performance%2520under%250Adifferent%2520token%2520budgets%252C%2520allowing%2520for%2520more%2520scalable%2520and%2520token-efficient%250Agenerative%2520video%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Adaptive%20and%20Temporally%20Causal%20Video%20Tokenization%20in%20a%201D%0A%20%20Latent%20Space&entry.906535625=Yan%20Li%20and%20Changyao%20Tian%20and%20Renqiu%20Xia%20and%20Ning%20Liao%20and%20Weiwei%20Guo%20and%20Junchi%20Yan%20and%20Hongsheng%20Li%20and%20Jifeng%20Dai%20and%20Hao%20Li%20and%20Xue%20Yang&entry.1292438233=%20%20We%20propose%20AdapTok%2C%20an%20adaptive%20temporal%20causal%20video%20tokenizer%20that%20can%0Aflexibly%20allocate%20tokens%20for%20different%20frames%20based%20on%20video%20content.%20AdapTok%0Ais%20equipped%20with%20a%20block-wise%20masking%20strategy%20that%20randomly%20drops%20tail%20tokens%0Aof%20each%20block%20during%20training%2C%20and%20a%20block%20causal%20scorer%20to%20predict%20the%0Areconstruction%20quality%20of%20video%20frames%20using%20different%20numbers%20of%20tokens.%0ADuring%20inference%2C%20an%20adaptive%20token%20allocation%20strategy%20based%20on%20integer%20linear%0Aprogramming%20is%20further%20proposed%20to%20adjust%20token%20usage%20given%20predicted%20scores.%0ASuch%20design%20allows%20for%20sample-wise%2C%20content-aware%2C%20and%20temporally%20dynamic%20token%0Aallocation%20under%20a%20controllable%20overall%20budget.%20Extensive%20experiments%20for%20video%0Areconstruction%20and%20generation%20on%20UCF-101%20and%20Kinetics-600%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach.%20Without%20additional%20image%20data%2C%20AdapTok%0Aconsistently%20improves%20reconstruction%20quality%20and%20generation%20performance%20under%0Adifferent%20token%20budgets%2C%20allowing%20for%20more%20scalable%20and%20token-efficient%0Agenerative%20video%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17011v1&entry.124074799=Read"},
{"title": "ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and\n  Reasoning", "author": "Tajamul Ashraf and Mohammed Mohsen Peerzada and Moloud Abdar and Yutong Xie and Yuyin Zhou and Xiaofeng Liu and Iqra Altaf Gillani and Janibul Bashir", "abstract": "  Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy across decentralized participants.\nAs FL adoption grows, numerous techniques have been proposed to tackle its\npractical challenges. However, the lack of standardized evaluation across key\ndimensions hampers systematic progress and fair comparison of FL methods. In\nthis work, we introduce ATR-Bench, a unified framework for analyzing federated\nlearning through three foundational dimensions: Adaptation, Trust, and\nReasoning. We provide an in-depth examination of the conceptual foundations,\ntask formulations, and open research challenges associated with each theme. We\nhave extensively benchmarked representative methods and datasets for adaptation\nto heterogeneous clients and trustworthiness in adversarial or unreliable\nenvironments. Due to the lack of reliable metrics and models for reasoning in\nFL, we only provide literature-driven insights for this dimension. ATR-Bench\nlays the groundwork for a systematic and holistic evaluation of federated\nlearning with real-world relevance. We will make our complete codebase publicly\naccessible and a curated repository that continuously tracks new developments\nand research in the FL literature.\n", "link": "http://arxiv.org/abs/2505.16850v1", "date": "2025-05-22", "relevancy": 2.3604, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4737}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATR-Bench%3A%20A%20Federated%20Learning%20Benchmark%20for%20Adaptation%2C%20Trust%2C%20and%0A%20%20Reasoning&body=Title%3A%20ATR-Bench%3A%20A%20Federated%20Learning%20Benchmark%20for%20Adaptation%2C%20Trust%2C%20and%0A%20%20Reasoning%0AAuthor%3A%20Tajamul%20Ashraf%20and%20Mohammed%20Mohsen%20Peerzada%20and%20Moloud%20Abdar%20and%20Yutong%20Xie%20and%20Yuyin%20Zhou%20and%20Xiaofeng%20Liu%20and%20Iqra%20Altaf%20Gillani%20and%20Janibul%20Bashir%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%20collaborative%0Amodel%20training%20while%20preserving%20data%20privacy%20across%20decentralized%20participants.%0AAs%20FL%20adoption%20grows%2C%20numerous%20techniques%20have%20been%20proposed%20to%20tackle%20its%0Apractical%20challenges.%20However%2C%20the%20lack%20of%20standardized%20evaluation%20across%20key%0Adimensions%20hampers%20systematic%20progress%20and%20fair%20comparison%20of%20FL%20methods.%20In%0Athis%20work%2C%20we%20introduce%20ATR-Bench%2C%20a%20unified%20framework%20for%20analyzing%20federated%0Alearning%20through%20three%20foundational%20dimensions%3A%20Adaptation%2C%20Trust%2C%20and%0AReasoning.%20We%20provide%20an%20in-depth%20examination%20of%20the%20conceptual%20foundations%2C%0Atask%20formulations%2C%20and%20open%20research%20challenges%20associated%20with%20each%20theme.%20We%0Ahave%20extensively%20benchmarked%20representative%20methods%20and%20datasets%20for%20adaptation%0Ato%20heterogeneous%20clients%20and%20trustworthiness%20in%20adversarial%20or%20unreliable%0Aenvironments.%20Due%20to%20the%20lack%20of%20reliable%20metrics%20and%20models%20for%20reasoning%20in%0AFL%2C%20we%20only%20provide%20literature-driven%20insights%20for%20this%20dimension.%20ATR-Bench%0Alays%20the%20groundwork%20for%20a%20systematic%20and%20holistic%20evaluation%20of%20federated%0Alearning%20with%20real-world%20relevance.%20We%20will%20make%20our%20complete%20codebase%20publicly%0Aaccessible%20and%20a%20curated%20repository%20that%20continuously%20tracks%20new%20developments%0Aand%20research%20in%20the%20FL%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATR-Bench%253A%2520A%2520Federated%2520Learning%2520Benchmark%2520for%2520Adaptation%252C%2520Trust%252C%2520and%250A%2520%2520Reasoning%26entry.906535625%3DTajamul%2520Ashraf%2520and%2520Mohammed%2520Mohsen%2520Peerzada%2520and%2520Moloud%2520Abdar%2520and%2520Yutong%2520Xie%2520and%2520Yuyin%2520Zhou%2520and%2520Xiaofeng%2520Liu%2520and%2520Iqra%2520Altaf%2520Gillani%2520and%2520Janibul%2520Bashir%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520collaborative%250Amodel%2520training%2520while%2520preserving%2520data%2520privacy%2520across%2520decentralized%2520participants.%250AAs%2520FL%2520adoption%2520grows%252C%2520numerous%2520techniques%2520have%2520been%2520proposed%2520to%2520tackle%2520its%250Apractical%2520challenges.%2520However%252C%2520the%2520lack%2520of%2520standardized%2520evaluation%2520across%2520key%250Adimensions%2520hampers%2520systematic%2520progress%2520and%2520fair%2520comparison%2520of%2520FL%2520methods.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520ATR-Bench%252C%2520a%2520unified%2520framework%2520for%2520analyzing%2520federated%250Alearning%2520through%2520three%2520foundational%2520dimensions%253A%2520Adaptation%252C%2520Trust%252C%2520and%250AReasoning.%2520We%2520provide%2520an%2520in-depth%2520examination%2520of%2520the%2520conceptual%2520foundations%252C%250Atask%2520formulations%252C%2520and%2520open%2520research%2520challenges%2520associated%2520with%2520each%2520theme.%2520We%250Ahave%2520extensively%2520benchmarked%2520representative%2520methods%2520and%2520datasets%2520for%2520adaptation%250Ato%2520heterogeneous%2520clients%2520and%2520trustworthiness%2520in%2520adversarial%2520or%2520unreliable%250Aenvironments.%2520Due%2520to%2520the%2520lack%2520of%2520reliable%2520metrics%2520and%2520models%2520for%2520reasoning%2520in%250AFL%252C%2520we%2520only%2520provide%2520literature-driven%2520insights%2520for%2520this%2520dimension.%2520ATR-Bench%250Alays%2520the%2520groundwork%2520for%2520a%2520systematic%2520and%2520holistic%2520evaluation%2520of%2520federated%250Alearning%2520with%2520real-world%2520relevance.%2520We%2520will%2520make%2520our%2520complete%2520codebase%2520publicly%250Aaccessible%2520and%2520a%2520curated%2520repository%2520that%2520continuously%2520tracks%2520new%2520developments%250Aand%2520research%2520in%2520the%2520FL%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATR-Bench%3A%20A%20Federated%20Learning%20Benchmark%20for%20Adaptation%2C%20Trust%2C%20and%0A%20%20Reasoning&entry.906535625=Tajamul%20Ashraf%20and%20Mohammed%20Mohsen%20Peerzada%20and%20Moloud%20Abdar%20and%20Yutong%20Xie%20and%20Yuyin%20Zhou%20and%20Xiaofeng%20Liu%20and%20Iqra%20Altaf%20Gillani%20and%20Janibul%20Bashir&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%20collaborative%0Amodel%20training%20while%20preserving%20data%20privacy%20across%20decentralized%20participants.%0AAs%20FL%20adoption%20grows%2C%20numerous%20techniques%20have%20been%20proposed%20to%20tackle%20its%0Apractical%20challenges.%20However%2C%20the%20lack%20of%20standardized%20evaluation%20across%20key%0Adimensions%20hampers%20systematic%20progress%20and%20fair%20comparison%20of%20FL%20methods.%20In%0Athis%20work%2C%20we%20introduce%20ATR-Bench%2C%20a%20unified%20framework%20for%20analyzing%20federated%0Alearning%20through%20three%20foundational%20dimensions%3A%20Adaptation%2C%20Trust%2C%20and%0AReasoning.%20We%20provide%20an%20in-depth%20examination%20of%20the%20conceptual%20foundations%2C%0Atask%20formulations%2C%20and%20open%20research%20challenges%20associated%20with%20each%20theme.%20We%0Ahave%20extensively%20benchmarked%20representative%20methods%20and%20datasets%20for%20adaptation%0Ato%20heterogeneous%20clients%20and%20trustworthiness%20in%20adversarial%20or%20unreliable%0Aenvironments.%20Due%20to%20the%20lack%20of%20reliable%20metrics%20and%20models%20for%20reasoning%20in%0AFL%2C%20we%20only%20provide%20literature-driven%20insights%20for%20this%20dimension.%20ATR-Bench%0Alays%20the%20groundwork%20for%20a%20systematic%20and%20holistic%20evaluation%20of%20federated%0Alearning%20with%20real-world%20relevance.%20We%20will%20make%20our%20complete%20codebase%20publicly%0Aaccessible%20and%20a%20curated%20repository%20that%20continuously%20tracks%20new%20developments%0Aand%20research%20in%20the%20FL%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16850v1&entry.124074799=Read"},
{"title": "M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo\n  Video Conversion", "author": "Nina Shvetsova and Goutam Bhat and Prune Truong and Hilde Kuehne and Federico Tombari", "abstract": "  We tackle the problem of monocular-to-stereo video conversion and propose a\nnovel architecture for inpainting and refinement of the warped right view\nobtained by depth-based reprojection of the input left view. We extend the\nStable Video Diffusion (SVD) model to utilize the input left video, the warped\nright video, and the disocclusion masks as conditioning input to generate a\nhigh-quality right camera view. In order to effectively exploit information\nfrom neighboring frames for inpainting, we modify the attention layers in SVD\nto compute full attention for discoccluded pixels. Our model is trained to\ngenerate the right view video in an end-to-end manner by minimizing image space\nlosses to ensure high-quality generation. Our approach outperforms previous\nstate-of-the-art methods, obtaining an average rank of 1.43 among the 4\ncompared methods in a user study, while being 6x faster than the second placed\nmethod.\n", "link": "http://arxiv.org/abs/2505.16565v1", "date": "2025-05-22", "relevancy": 2.3435, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6041}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5962}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M2SVid%3A%20End-to-End%20Inpainting%20and%20Refinement%20for%20Monocular-to-Stereo%0A%20%20Video%20Conversion&body=Title%3A%20M2SVid%3A%20End-to-End%20Inpainting%20and%20Refinement%20for%20Monocular-to-Stereo%0A%20%20Video%20Conversion%0AAuthor%3A%20Nina%20Shvetsova%20and%20Goutam%20Bhat%20and%20Prune%20Truong%20and%20Hilde%20Kuehne%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20monocular-to-stereo%20video%20conversion%20and%20propose%20a%0Anovel%20architecture%20for%20inpainting%20and%20refinement%20of%20the%20warped%20right%20view%0Aobtained%20by%20depth-based%20reprojection%20of%20the%20input%20left%20view.%20We%20extend%20the%0AStable%20Video%20Diffusion%20%28SVD%29%20model%20to%20utilize%20the%20input%20left%20video%2C%20the%20warped%0Aright%20video%2C%20and%20the%20disocclusion%20masks%20as%20conditioning%20input%20to%20generate%20a%0Ahigh-quality%20right%20camera%20view.%20In%20order%20to%20effectively%20exploit%20information%0Afrom%20neighboring%20frames%20for%20inpainting%2C%20we%20modify%20the%20attention%20layers%20in%20SVD%0Ato%20compute%20full%20attention%20for%20discoccluded%20pixels.%20Our%20model%20is%20trained%20to%0Agenerate%20the%20right%20view%20video%20in%20an%20end-to-end%20manner%20by%20minimizing%20image%20space%0Alosses%20to%20ensure%20high-quality%20generation.%20Our%20approach%20outperforms%20previous%0Astate-of-the-art%20methods%2C%20obtaining%20an%20average%20rank%20of%201.43%20among%20the%204%0Acompared%20methods%20in%20a%20user%20study%2C%20while%20being%206x%20faster%20than%20the%20second%20placed%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM2SVid%253A%2520End-to-End%2520Inpainting%2520and%2520Refinement%2520for%2520Monocular-to-Stereo%250A%2520%2520Video%2520Conversion%26entry.906535625%3DNina%2520Shvetsova%2520and%2520Goutam%2520Bhat%2520and%2520Prune%2520Truong%2520and%2520Hilde%2520Kuehne%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520monocular-to-stereo%2520video%2520conversion%2520and%2520propose%2520a%250Anovel%2520architecture%2520for%2520inpainting%2520and%2520refinement%2520of%2520the%2520warped%2520right%2520view%250Aobtained%2520by%2520depth-based%2520reprojection%2520of%2520the%2520input%2520left%2520view.%2520We%2520extend%2520the%250AStable%2520Video%2520Diffusion%2520%2528SVD%2529%2520model%2520to%2520utilize%2520the%2520input%2520left%2520video%252C%2520the%2520warped%250Aright%2520video%252C%2520and%2520the%2520disocclusion%2520masks%2520as%2520conditioning%2520input%2520to%2520generate%2520a%250Ahigh-quality%2520right%2520camera%2520view.%2520In%2520order%2520to%2520effectively%2520exploit%2520information%250Afrom%2520neighboring%2520frames%2520for%2520inpainting%252C%2520we%2520modify%2520the%2520attention%2520layers%2520in%2520SVD%250Ato%2520compute%2520full%2520attention%2520for%2520discoccluded%2520pixels.%2520Our%2520model%2520is%2520trained%2520to%250Agenerate%2520the%2520right%2520view%2520video%2520in%2520an%2520end-to-end%2520manner%2520by%2520minimizing%2520image%2520space%250Alosses%2520to%2520ensure%2520high-quality%2520generation.%2520Our%2520approach%2520outperforms%2520previous%250Astate-of-the-art%2520methods%252C%2520obtaining%2520an%2520average%2520rank%2520of%25201.43%2520among%2520the%25204%250Acompared%2520methods%2520in%2520a%2520user%2520study%252C%2520while%2520being%25206x%2520faster%2520than%2520the%2520second%2520placed%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M2SVid%3A%20End-to-End%20Inpainting%20and%20Refinement%20for%20Monocular-to-Stereo%0A%20%20Video%20Conversion&entry.906535625=Nina%20Shvetsova%20and%20Goutam%20Bhat%20and%20Prune%20Truong%20and%20Hilde%20Kuehne%20and%20Federico%20Tombari&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20monocular-to-stereo%20video%20conversion%20and%20propose%20a%0Anovel%20architecture%20for%20inpainting%20and%20refinement%20of%20the%20warped%20right%20view%0Aobtained%20by%20depth-based%20reprojection%20of%20the%20input%20left%20view.%20We%20extend%20the%0AStable%20Video%20Diffusion%20%28SVD%29%20model%20to%20utilize%20the%20input%20left%20video%2C%20the%20warped%0Aright%20video%2C%20and%20the%20disocclusion%20masks%20as%20conditioning%20input%20to%20generate%20a%0Ahigh-quality%20right%20camera%20view.%20In%20order%20to%20effectively%20exploit%20information%0Afrom%20neighboring%20frames%20for%20inpainting%2C%20we%20modify%20the%20attention%20layers%20in%20SVD%0Ato%20compute%20full%20attention%20for%20discoccluded%20pixels.%20Our%20model%20is%20trained%20to%0Agenerate%20the%20right%20view%20video%20in%20an%20end-to-end%20manner%20by%20minimizing%20image%20space%0Alosses%20to%20ensure%20high-quality%20generation.%20Our%20approach%20outperforms%20previous%0Astate-of-the-art%20methods%2C%20obtaining%20an%20average%20rank%20of%201.43%20among%20the%204%0Acompared%20methods%20in%20a%20user%20study%2C%20while%20being%206x%20faster%20than%20the%20second%20placed%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16565v1&entry.124074799=Read"},
{"title": "V2V: Scaling Event-Based Vision through Efficient Video-to-Voxel\n  Simulation", "author": "Hanyue Lou and Jinxiu Liang and Minggui Teng and Yi Wang and Boxin Shi", "abstract": "  Event-based cameras offer unique advantages such as high temporal resolution,\nhigh dynamic range, and low power consumption. However, the massive storage\nrequirements and I/O burdens of existing synthetic data generation pipelines\nand the scarcity of real data prevent event-based training datasets from\nscaling up, limiting the development and generalization capabilities of event\nvision models. To address this challenge, we introduce Video-to-Voxel (V2V), an\napproach that directly converts conventional video frames into event-based\nvoxel grid representations, bypassing the storage-intensive event stream\ngeneration entirely. V2V enables a 150 times reduction in storage requirements\nwhile supporting on-the-fly parameter randomization for enhanced model\nrobustness. Leveraging this efficiency, we train several video reconstruction\nand optical flow estimation model architectures on 10,000 diverse videos\ntotaling 52 hours--an order of magnitude larger than existing event datasets,\nyielding substantial improvements.\n", "link": "http://arxiv.org/abs/2505.16797v1", "date": "2025-05-22", "relevancy": 2.3405, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5891}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.587}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2V%3A%20Scaling%20Event-Based%20Vision%20through%20Efficient%20Video-to-Voxel%0A%20%20Simulation&body=Title%3A%20V2V%3A%20Scaling%20Event-Based%20Vision%20through%20Efficient%20Video-to-Voxel%0A%20%20Simulation%0AAuthor%3A%20Hanyue%20Lou%20and%20Jinxiu%20Liang%20and%20Minggui%20Teng%20and%20Yi%20Wang%20and%20Boxin%20Shi%0AAbstract%3A%20%20%20Event-based%20cameras%20offer%20unique%20advantages%20such%20as%20high%20temporal%20resolution%2C%0Ahigh%20dynamic%20range%2C%20and%20low%20power%20consumption.%20However%2C%20the%20massive%20storage%0Arequirements%20and%20I/O%20burdens%20of%20existing%20synthetic%20data%20generation%20pipelines%0Aand%20the%20scarcity%20of%20real%20data%20prevent%20event-based%20training%20datasets%20from%0Ascaling%20up%2C%20limiting%20the%20development%20and%20generalization%20capabilities%20of%20event%0Avision%20models.%20To%20address%20this%20challenge%2C%20we%20introduce%20Video-to-Voxel%20%28V2V%29%2C%20an%0Aapproach%20that%20directly%20converts%20conventional%20video%20frames%20into%20event-based%0Avoxel%20grid%20representations%2C%20bypassing%20the%20storage-intensive%20event%20stream%0Ageneration%20entirely.%20V2V%20enables%20a%20150%20times%20reduction%20in%20storage%20requirements%0Awhile%20supporting%20on-the-fly%20parameter%20randomization%20for%20enhanced%20model%0Arobustness.%20Leveraging%20this%20efficiency%2C%20we%20train%20several%20video%20reconstruction%0Aand%20optical%20flow%20estimation%20model%20architectures%20on%2010%2C000%20diverse%20videos%0Atotaling%2052%20hours--an%20order%20of%20magnitude%20larger%20than%20existing%20event%20datasets%2C%0Ayielding%20substantial%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2V%253A%2520Scaling%2520Event-Based%2520Vision%2520through%2520Efficient%2520Video-to-Voxel%250A%2520%2520Simulation%26entry.906535625%3DHanyue%2520Lou%2520and%2520Jinxiu%2520Liang%2520and%2520Minggui%2520Teng%2520and%2520Yi%2520Wang%2520and%2520Boxin%2520Shi%26entry.1292438233%3D%2520%2520Event-based%2520cameras%2520offer%2520unique%2520advantages%2520such%2520as%2520high%2520temporal%2520resolution%252C%250Ahigh%2520dynamic%2520range%252C%2520and%2520low%2520power%2520consumption.%2520However%252C%2520the%2520massive%2520storage%250Arequirements%2520and%2520I/O%2520burdens%2520of%2520existing%2520synthetic%2520data%2520generation%2520pipelines%250Aand%2520the%2520scarcity%2520of%2520real%2520data%2520prevent%2520event-based%2520training%2520datasets%2520from%250Ascaling%2520up%252C%2520limiting%2520the%2520development%2520and%2520generalization%2520capabilities%2520of%2520event%250Avision%2520models.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520Video-to-Voxel%2520%2528V2V%2529%252C%2520an%250Aapproach%2520that%2520directly%2520converts%2520conventional%2520video%2520frames%2520into%2520event-based%250Avoxel%2520grid%2520representations%252C%2520bypassing%2520the%2520storage-intensive%2520event%2520stream%250Ageneration%2520entirely.%2520V2V%2520enables%2520a%2520150%2520times%2520reduction%2520in%2520storage%2520requirements%250Awhile%2520supporting%2520on-the-fly%2520parameter%2520randomization%2520for%2520enhanced%2520model%250Arobustness.%2520Leveraging%2520this%2520efficiency%252C%2520we%2520train%2520several%2520video%2520reconstruction%250Aand%2520optical%2520flow%2520estimation%2520model%2520architectures%2520on%252010%252C000%2520diverse%2520videos%250Atotaling%252052%2520hours--an%2520order%2520of%2520magnitude%2520larger%2520than%2520existing%2520event%2520datasets%252C%250Ayielding%2520substantial%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2V%3A%20Scaling%20Event-Based%20Vision%20through%20Efficient%20Video-to-Voxel%0A%20%20Simulation&entry.906535625=Hanyue%20Lou%20and%20Jinxiu%20Liang%20and%20Minggui%20Teng%20and%20Yi%20Wang%20and%20Boxin%20Shi&entry.1292438233=%20%20Event-based%20cameras%20offer%20unique%20advantages%20such%20as%20high%20temporal%20resolution%2C%0Ahigh%20dynamic%20range%2C%20and%20low%20power%20consumption.%20However%2C%20the%20massive%20storage%0Arequirements%20and%20I/O%20burdens%20of%20existing%20synthetic%20data%20generation%20pipelines%0Aand%20the%20scarcity%20of%20real%20data%20prevent%20event-based%20training%20datasets%20from%0Ascaling%20up%2C%20limiting%20the%20development%20and%20generalization%20capabilities%20of%20event%0Avision%20models.%20To%20address%20this%20challenge%2C%20we%20introduce%20Video-to-Voxel%20%28V2V%29%2C%20an%0Aapproach%20that%20directly%20converts%20conventional%20video%20frames%20into%20event-based%0Avoxel%20grid%20representations%2C%20bypassing%20the%20storage-intensive%20event%20stream%0Ageneration%20entirely.%20V2V%20enables%20a%20150%20times%20reduction%20in%20storage%20requirements%0Awhile%20supporting%20on-the-fly%20parameter%20randomization%20for%20enhanced%20model%0Arobustness.%20Leveraging%20this%20efficiency%2C%20we%20train%20several%20video%20reconstruction%0Aand%20optical%20flow%20estimation%20model%20architectures%20on%2010%2C000%20diverse%20videos%0Atotaling%2052%20hours--an%20order%20of%20magnitude%20larger%20than%20existing%20event%20datasets%2C%0Ayielding%20substantial%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16797v1&entry.124074799=Read"},
{"title": "Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement\n  Fine-Tuning", "author": "Jian Liu and Jing Xu and Song Guo and Jing Li and Jingfeng Guo and Jiaao Yu and Haohan Weng and Biwen Lei and Xianghui Yang and Zhuo Chen and Fangqi Zhu and Tao Han and Chunchao Guo", "abstract": "  Existing pretrained models for 3D mesh generation often suffer from data\nbiases and produce low-quality results, while global reinforcement learning\n(RL) methods rely on object-level rewards that struggle to capture local\nstructure details. To address these challenges, we present \\textbf{Mesh-RFT}, a\nnovel fine-grained reinforcement fine-tuning framework that employs Masked\nDirect Preference Optimization (M-DPO) to enable localized refinement via\nquality-aware face masking. To facilitate efficient quality evaluation, we\nintroduce an objective topology-aware scoring system to evaluate geometric\nintegrity and topological regularity at both object and face levels through two\nmetrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating\nthese metrics into a fine-grained RL strategy, Mesh-RFT becomes the first\nmethod to optimize mesh quality at the granularity of individual faces,\nresolving localized errors while preserving global coherence. Experiment\nresults show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\\%\nand improves Topology Score (TS) by 3.8\\% over pre-trained models, while\noutperforming global DPO methods with a 17.4\\% HD reduction and 4.9\\% TS gain.\nThese results demonstrate Mesh-RFT's ability to improve geometric integrity and\ntopological regularity, achieving new state-of-the-art performance in\nproduction-ready mesh generation. Project Page:\n\\href{https://hitcslj.github.io/mesh-rft/}{this https URL}.\n", "link": "http://arxiv.org/abs/2505.16761v1", "date": "2025-05-22", "relevancy": 2.3383, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6184}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5688}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh-RFT%3A%20Enhancing%20Mesh%20Generation%20via%20Fine-grained%20Reinforcement%0A%20%20Fine-Tuning&body=Title%3A%20Mesh-RFT%3A%20Enhancing%20Mesh%20Generation%20via%20Fine-grained%20Reinforcement%0A%20%20Fine-Tuning%0AAuthor%3A%20Jian%20Liu%20and%20Jing%20Xu%20and%20Song%20Guo%20and%20Jing%20Li%20and%20Jingfeng%20Guo%20and%20Jiaao%20Yu%20and%20Haohan%20Weng%20and%20Biwen%20Lei%20and%20Xianghui%20Yang%20and%20Zhuo%20Chen%20and%20Fangqi%20Zhu%20and%20Tao%20Han%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20Existing%20pretrained%20models%20for%203D%20mesh%20generation%20often%20suffer%20from%20data%0Abiases%20and%20produce%20low-quality%20results%2C%20while%20global%20reinforcement%20learning%0A%28RL%29%20methods%20rely%20on%20object-level%20rewards%20that%20struggle%20to%20capture%20local%0Astructure%20details.%20To%20address%20these%20challenges%2C%20we%20present%20%5Ctextbf%7BMesh-RFT%7D%2C%20a%0Anovel%20fine-grained%20reinforcement%20fine-tuning%20framework%20that%20employs%20Masked%0ADirect%20Preference%20Optimization%20%28M-DPO%29%20to%20enable%20localized%20refinement%20via%0Aquality-aware%20face%20masking.%20To%20facilitate%20efficient%20quality%20evaluation%2C%20we%0Aintroduce%20an%20objective%20topology-aware%20scoring%20system%20to%20evaluate%20geometric%0Aintegrity%20and%20topological%20regularity%20at%20both%20object%20and%20face%20levels%20through%20two%0Ametrics%3A%20Boundary%20Edge%20Ratio%20%28BER%29%20and%20Topology%20Score%20%28TS%29.%20By%20integrating%0Athese%20metrics%20into%20a%20fine-grained%20RL%20strategy%2C%20Mesh-RFT%20becomes%20the%20first%0Amethod%20to%20optimize%20mesh%20quality%20at%20the%20granularity%20of%20individual%20faces%2C%0Aresolving%20localized%20errors%20while%20preserving%20global%20coherence.%20Experiment%0Aresults%20show%20that%20our%20M-DPO%20approach%20reduces%20Hausdorff%20Distance%20%28HD%29%20by%2024.6%5C%25%0Aand%20improves%20Topology%20Score%20%28TS%29%20by%203.8%5C%25%20over%20pre-trained%20models%2C%20while%0Aoutperforming%20global%20DPO%20methods%20with%20a%2017.4%5C%25%20HD%20reduction%20and%204.9%5C%25%20TS%20gain.%0AThese%20results%20demonstrate%20Mesh-RFT%27s%20ability%20to%20improve%20geometric%20integrity%20and%0Atopological%20regularity%2C%20achieving%20new%20state-of-the-art%20performance%20in%0Aproduction-ready%20mesh%20generation.%20Project%20Page%3A%0A%5Chref%7Bhttps%3A//hitcslj.github.io/mesh-rft/%7D%7Bthis%20https%20URL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh-RFT%253A%2520Enhancing%2520Mesh%2520Generation%2520via%2520Fine-grained%2520Reinforcement%250A%2520%2520Fine-Tuning%26entry.906535625%3DJian%2520Liu%2520and%2520Jing%2520Xu%2520and%2520Song%2520Guo%2520and%2520Jing%2520Li%2520and%2520Jingfeng%2520Guo%2520and%2520Jiaao%2520Yu%2520and%2520Haohan%2520Weng%2520and%2520Biwen%2520Lei%2520and%2520Xianghui%2520Yang%2520and%2520Zhuo%2520Chen%2520and%2520Fangqi%2520Zhu%2520and%2520Tao%2520Han%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520Existing%2520pretrained%2520models%2520for%25203D%2520mesh%2520generation%2520often%2520suffer%2520from%2520data%250Abiases%2520and%2520produce%2520low-quality%2520results%252C%2520while%2520global%2520reinforcement%2520learning%250A%2528RL%2529%2520methods%2520rely%2520on%2520object-level%2520rewards%2520that%2520struggle%2520to%2520capture%2520local%250Astructure%2520details.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520%255Ctextbf%257BMesh-RFT%257D%252C%2520a%250Anovel%2520fine-grained%2520reinforcement%2520fine-tuning%2520framework%2520that%2520employs%2520Masked%250ADirect%2520Preference%2520Optimization%2520%2528M-DPO%2529%2520to%2520enable%2520localized%2520refinement%2520via%250Aquality-aware%2520face%2520masking.%2520To%2520facilitate%2520efficient%2520quality%2520evaluation%252C%2520we%250Aintroduce%2520an%2520objective%2520topology-aware%2520scoring%2520system%2520to%2520evaluate%2520geometric%250Aintegrity%2520and%2520topological%2520regularity%2520at%2520both%2520object%2520and%2520face%2520levels%2520through%2520two%250Ametrics%253A%2520Boundary%2520Edge%2520Ratio%2520%2528BER%2529%2520and%2520Topology%2520Score%2520%2528TS%2529.%2520By%2520integrating%250Athese%2520metrics%2520into%2520a%2520fine-grained%2520RL%2520strategy%252C%2520Mesh-RFT%2520becomes%2520the%2520first%250Amethod%2520to%2520optimize%2520mesh%2520quality%2520at%2520the%2520granularity%2520of%2520individual%2520faces%252C%250Aresolving%2520localized%2520errors%2520while%2520preserving%2520global%2520coherence.%2520Experiment%250Aresults%2520show%2520that%2520our%2520M-DPO%2520approach%2520reduces%2520Hausdorff%2520Distance%2520%2528HD%2529%2520by%252024.6%255C%2525%250Aand%2520improves%2520Topology%2520Score%2520%2528TS%2529%2520by%25203.8%255C%2525%2520over%2520pre-trained%2520models%252C%2520while%250Aoutperforming%2520global%2520DPO%2520methods%2520with%2520a%252017.4%255C%2525%2520HD%2520reduction%2520and%25204.9%255C%2525%2520TS%2520gain.%250AThese%2520results%2520demonstrate%2520Mesh-RFT%2527s%2520ability%2520to%2520improve%2520geometric%2520integrity%2520and%250Atopological%2520regularity%252C%2520achieving%2520new%2520state-of-the-art%2520performance%2520in%250Aproduction-ready%2520mesh%2520generation.%2520Project%2520Page%253A%250A%255Chref%257Bhttps%253A//hitcslj.github.io/mesh-rft/%257D%257Bthis%2520https%2520URL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh-RFT%3A%20Enhancing%20Mesh%20Generation%20via%20Fine-grained%20Reinforcement%0A%20%20Fine-Tuning&entry.906535625=Jian%20Liu%20and%20Jing%20Xu%20and%20Song%20Guo%20and%20Jing%20Li%20and%20Jingfeng%20Guo%20and%20Jiaao%20Yu%20and%20Haohan%20Weng%20and%20Biwen%20Lei%20and%20Xianghui%20Yang%20and%20Zhuo%20Chen%20and%20Fangqi%20Zhu%20and%20Tao%20Han%20and%20Chunchao%20Guo&entry.1292438233=%20%20Existing%20pretrained%20models%20for%203D%20mesh%20generation%20often%20suffer%20from%20data%0Abiases%20and%20produce%20low-quality%20results%2C%20while%20global%20reinforcement%20learning%0A%28RL%29%20methods%20rely%20on%20object-level%20rewards%20that%20struggle%20to%20capture%20local%0Astructure%20details.%20To%20address%20these%20challenges%2C%20we%20present%20%5Ctextbf%7BMesh-RFT%7D%2C%20a%0Anovel%20fine-grained%20reinforcement%20fine-tuning%20framework%20that%20employs%20Masked%0ADirect%20Preference%20Optimization%20%28M-DPO%29%20to%20enable%20localized%20refinement%20via%0Aquality-aware%20face%20masking.%20To%20facilitate%20efficient%20quality%20evaluation%2C%20we%0Aintroduce%20an%20objective%20topology-aware%20scoring%20system%20to%20evaluate%20geometric%0Aintegrity%20and%20topological%20regularity%20at%20both%20object%20and%20face%20levels%20through%20two%0Ametrics%3A%20Boundary%20Edge%20Ratio%20%28BER%29%20and%20Topology%20Score%20%28TS%29.%20By%20integrating%0Athese%20metrics%20into%20a%20fine-grained%20RL%20strategy%2C%20Mesh-RFT%20becomes%20the%20first%0Amethod%20to%20optimize%20mesh%20quality%20at%20the%20granularity%20of%20individual%20faces%2C%0Aresolving%20localized%20errors%20while%20preserving%20global%20coherence.%20Experiment%0Aresults%20show%20that%20our%20M-DPO%20approach%20reduces%20Hausdorff%20Distance%20%28HD%29%20by%2024.6%5C%25%0Aand%20improves%20Topology%20Score%20%28TS%29%20by%203.8%5C%25%20over%20pre-trained%20models%2C%20while%0Aoutperforming%20global%20DPO%20methods%20with%20a%2017.4%5C%25%20HD%20reduction%20and%204.9%5C%25%20TS%20gain.%0AThese%20results%20demonstrate%20Mesh-RFT%27s%20ability%20to%20improve%20geometric%20integrity%20and%0Atopological%20regularity%2C%20achieving%20new%20state-of-the-art%20performance%20in%0Aproduction-ready%20mesh%20generation.%20Project%20Page%3A%0A%5Chref%7Bhttps%3A//hitcslj.github.io/mesh-rft/%7D%7Bthis%20https%20URL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16761v1&entry.124074799=Read"},
{"title": "InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning", "author": "Ji Zhang and Shihan Wu and Xu Luo and Hao Wu and Lianli Gao and Heng Tao Shen and Jingkuan Song", "abstract": "  Leveraging pretrained Vision-Language Models (VLMs) to map language\ninstruction and visual observations to raw low-level actions,\nVision-Language-Action models (VLAs) hold great promise for achieving\ngeneral-purpose robotic systems. Despite their advancements, existing VLAs tend\nto spuriously correlate task-irrelevant visual features with actions, limiting\ntheir generalization capacity beyond the training data. To tackle this\nchallenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet\neffective approach that mitigates the adverse effects of spurious correlations\nby boosting the spatial reasoning ability of VLAs. Specifically, InSpire\nredirects the VLA's attention to task-relevant factors by prepending the\nquestion \"In which direction is the [object] relative to the robot?\" to the\nlanguage instruction and aligning the answer\n\"right/left/up/down/front/back/grasped\" and predicted actions with the\nground-truth. Notably, InSpire can be used as a plugin to enhance existing\nautoregressive VLAs, requiring no extra training data or interaction with other\nlarge models. Extensive experimental results in both simulation and real-world\nenvironments demonstrate the effectiveness and flexibility of our approach. Our\ncode, pretrained models and demos are publicly available at:\nhttps://Koorye.github.io/proj/Inspire.\n", "link": "http://arxiv.org/abs/2505.13888v2", "date": "2025-05-22", "relevancy": 2.3381, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5923}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InSpire%3A%20Vision-Language-Action%20Models%20with%20Intrinsic%20Spatial%20Reasoning&body=Title%3A%20InSpire%3A%20Vision-Language-Action%20Models%20with%20Intrinsic%20Spatial%20Reasoning%0AAuthor%3A%20Ji%20Zhang%20and%20Shihan%20Wu%20and%20Xu%20Luo%20and%20Hao%20Wu%20and%20Lianli%20Gao%20and%20Heng%20Tao%20Shen%20and%20Jingkuan%20Song%0AAbstract%3A%20%20%20Leveraging%20pretrained%20Vision-Language%20Models%20%28VLMs%29%20to%20map%20language%0Ainstruction%20and%20visual%20observations%20to%20raw%20low-level%20actions%2C%0AVision-Language-Action%20models%20%28VLAs%29%20hold%20great%20promise%20for%20achieving%0Ageneral-purpose%20robotic%20systems.%20Despite%20their%20advancements%2C%20existing%20VLAs%20tend%0Ato%20spuriously%20correlate%20task-irrelevant%20visual%20features%20with%20actions%2C%20limiting%0Atheir%20generalization%20capacity%20beyond%20the%20training%20data.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20Intrinsic%20Spatial%20Reasoning%20%28InSpire%29%2C%20a%20simple%20yet%0Aeffective%20approach%20that%20mitigates%20the%20adverse%20effects%20of%20spurious%20correlations%0Aby%20boosting%20the%20spatial%20reasoning%20ability%20of%20VLAs.%20Specifically%2C%20InSpire%0Aredirects%20the%20VLA%27s%20attention%20to%20task-relevant%20factors%20by%20prepending%20the%0Aquestion%20%22In%20which%20direction%20is%20the%20%5Bobject%5D%20relative%20to%20the%20robot%3F%22%20to%20the%0Alanguage%20instruction%20and%20aligning%20the%20answer%0A%22right/left/up/down/front/back/grasped%22%20and%20predicted%20actions%20with%20the%0Aground-truth.%20Notably%2C%20InSpire%20can%20be%20used%20as%20a%20plugin%20to%20enhance%20existing%0Aautoregressive%20VLAs%2C%20requiring%20no%20extra%20training%20data%20or%20interaction%20with%20other%0Alarge%20models.%20Extensive%20experimental%20results%20in%20both%20simulation%20and%20real-world%0Aenvironments%20demonstrate%20the%20effectiveness%20and%20flexibility%20of%20our%20approach.%20Our%0Acode%2C%20pretrained%20models%20and%20demos%20are%20publicly%20available%20at%3A%0Ahttps%3A//Koorye.github.io/proj/Inspire.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInSpire%253A%2520Vision-Language-Action%2520Models%2520with%2520Intrinsic%2520Spatial%2520Reasoning%26entry.906535625%3DJi%2520Zhang%2520and%2520Shihan%2520Wu%2520and%2520Xu%2520Luo%2520and%2520Hao%2520Wu%2520and%2520Lianli%2520Gao%2520and%2520Heng%2520Tao%2520Shen%2520and%2520Jingkuan%2520Song%26entry.1292438233%3D%2520%2520Leveraging%2520pretrained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520map%2520language%250Ainstruction%2520and%2520visual%2520observations%2520to%2520raw%2520low-level%2520actions%252C%250AVision-Language-Action%2520models%2520%2528VLAs%2529%2520hold%2520great%2520promise%2520for%2520achieving%250Ageneral-purpose%2520robotic%2520systems.%2520Despite%2520their%2520advancements%252C%2520existing%2520VLAs%2520tend%250Ato%2520spuriously%2520correlate%2520task-irrelevant%2520visual%2520features%2520with%2520actions%252C%2520limiting%250Atheir%2520generalization%2520capacity%2520beyond%2520the%2520training%2520data.%2520To%2520tackle%2520this%250Achallenge%252C%2520we%2520propose%2520Intrinsic%2520Spatial%2520Reasoning%2520%2528InSpire%2529%252C%2520a%2520simple%2520yet%250Aeffective%2520approach%2520that%2520mitigates%2520the%2520adverse%2520effects%2520of%2520spurious%2520correlations%250Aby%2520boosting%2520the%2520spatial%2520reasoning%2520ability%2520of%2520VLAs.%2520Specifically%252C%2520InSpire%250Aredirects%2520the%2520VLA%2527s%2520attention%2520to%2520task-relevant%2520factors%2520by%2520prepending%2520the%250Aquestion%2520%2522In%2520which%2520direction%2520is%2520the%2520%255Bobject%255D%2520relative%2520to%2520the%2520robot%253F%2522%2520to%2520the%250Alanguage%2520instruction%2520and%2520aligning%2520the%2520answer%250A%2522right/left/up/down/front/back/grasped%2522%2520and%2520predicted%2520actions%2520with%2520the%250Aground-truth.%2520Notably%252C%2520InSpire%2520can%2520be%2520used%2520as%2520a%2520plugin%2520to%2520enhance%2520existing%250Aautoregressive%2520VLAs%252C%2520requiring%2520no%2520extra%2520training%2520data%2520or%2520interaction%2520with%2520other%250Alarge%2520models.%2520Extensive%2520experimental%2520results%2520in%2520both%2520simulation%2520and%2520real-world%250Aenvironments%2520demonstrate%2520the%2520effectiveness%2520and%2520flexibility%2520of%2520our%2520approach.%2520Our%250Acode%252C%2520pretrained%2520models%2520and%2520demos%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//Koorye.github.io/proj/Inspire.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InSpire%3A%20Vision-Language-Action%20Models%20with%20Intrinsic%20Spatial%20Reasoning&entry.906535625=Ji%20Zhang%20and%20Shihan%20Wu%20and%20Xu%20Luo%20and%20Hao%20Wu%20and%20Lianli%20Gao%20and%20Heng%20Tao%20Shen%20and%20Jingkuan%20Song&entry.1292438233=%20%20Leveraging%20pretrained%20Vision-Language%20Models%20%28VLMs%29%20to%20map%20language%0Ainstruction%20and%20visual%20observations%20to%20raw%20low-level%20actions%2C%0AVision-Language-Action%20models%20%28VLAs%29%20hold%20great%20promise%20for%20achieving%0Ageneral-purpose%20robotic%20systems.%20Despite%20their%20advancements%2C%20existing%20VLAs%20tend%0Ato%20spuriously%20correlate%20task-irrelevant%20visual%20features%20with%20actions%2C%20limiting%0Atheir%20generalization%20capacity%20beyond%20the%20training%20data.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20Intrinsic%20Spatial%20Reasoning%20%28InSpire%29%2C%20a%20simple%20yet%0Aeffective%20approach%20that%20mitigates%20the%20adverse%20effects%20of%20spurious%20correlations%0Aby%20boosting%20the%20spatial%20reasoning%20ability%20of%20VLAs.%20Specifically%2C%20InSpire%0Aredirects%20the%20VLA%27s%20attention%20to%20task-relevant%20factors%20by%20prepending%20the%0Aquestion%20%22In%20which%20direction%20is%20the%20%5Bobject%5D%20relative%20to%20the%20robot%3F%22%20to%20the%0Alanguage%20instruction%20and%20aligning%20the%20answer%0A%22right/left/up/down/front/back/grasped%22%20and%20predicted%20actions%20with%20the%0Aground-truth.%20Notably%2C%20InSpire%20can%20be%20used%20as%20a%20plugin%20to%20enhance%20existing%0Aautoregressive%20VLAs%2C%20requiring%20no%20extra%20training%20data%20or%20interaction%20with%20other%0Alarge%20models.%20Extensive%20experimental%20results%20in%20both%20simulation%20and%20real-world%0Aenvironments%20demonstrate%20the%20effectiveness%20and%20flexibility%20of%20our%20approach.%20Our%0Acode%2C%20pretrained%20models%20and%20demos%20are%20publicly%20available%20at%3A%0Ahttps%3A//Koorye.github.io/proj/Inspire.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13888v2&entry.124074799=Read"},
{"title": "SPAR: Self-supervised Placement-Aware Representation Learning for\n  Multi-Node IoT Systems", "author": "Yizhuo Chen and Tianchen Wang and You Lyu and Yanlan Hu and Jinyang Li and Tomoyoshi Kimura and Hongjue Zhao and Yigong Hu and Denizhan Kara and Tarek Abdelzaher", "abstract": "  This work develops the underpinnings of self-supervised placement-aware\nrepresentation learning given spatially-distributed (multi-view and multimodal)\nsensor observations, motivated by the need to represent external environmental\nstate in multi-sensor IoT systems in a manner that correctly distills spatial\nphenomena from the distributed multi-vantage observations. The objective of\nsensing in IoT systems is, in general, to collectively represent an externally\nobserved environment given multiple vantage points from which sensory\nobservations occur. Pretraining of models that help interpret sensor data must\ntherefore encode the relation between signals observed by sensors and the\nobservers' vantage points in order to attain a representation that encodes the\nobserved spatial phenomena in a manner informed by the specific placement of\nthe measuring instruments, while allowing arbitrary placement. The work\nsignificantly advances self-supervised model pretraining from IoT signals\nbeyond current solutions that often overlook the distinctive spatial nature of\nIoT data. Our framework explicitly learns the dependencies between measurements\nand geometric observer layouts and structural characteristics, guided by a core\ndesign principle: the duality between signals and observer positions. We\nfurther provide theoretical analyses from the perspectives of information\ntheory and occlusion-invariant representation learning to offer insight into\nthe rationale behind our design. Experiments on three real-world\ndatasets--covering vehicle monitoring, human activity recognition, and\nearthquake localization--demonstrate the superior generalizability and\nrobustness of our method across diverse modalities, sensor placements,\napplication-level inference tasks, and spatial scales.\n", "link": "http://arxiv.org/abs/2505.16936v1", "date": "2025-05-22", "relevancy": 2.3379, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6168}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5876}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPAR%3A%20Self-supervised%20Placement-Aware%20Representation%20Learning%20for%0A%20%20Multi-Node%20IoT%20Systems&body=Title%3A%20SPAR%3A%20Self-supervised%20Placement-Aware%20Representation%20Learning%20for%0A%20%20Multi-Node%20IoT%20Systems%0AAuthor%3A%20Yizhuo%20Chen%20and%20Tianchen%20Wang%20and%20You%20Lyu%20and%20Yanlan%20Hu%20and%20Jinyang%20Li%20and%20Tomoyoshi%20Kimura%20and%20Hongjue%20Zhao%20and%20Yigong%20Hu%20and%20Denizhan%20Kara%20and%20Tarek%20Abdelzaher%0AAbstract%3A%20%20%20This%20work%20develops%20the%20underpinnings%20of%20self-supervised%20placement-aware%0Arepresentation%20learning%20given%20spatially-distributed%20%28multi-view%20and%20multimodal%29%0Asensor%20observations%2C%20motivated%20by%20the%20need%20to%20represent%20external%20environmental%0Astate%20in%20multi-sensor%20IoT%20systems%20in%20a%20manner%20that%20correctly%20distills%20spatial%0Aphenomena%20from%20the%20distributed%20multi-vantage%20observations.%20The%20objective%20of%0Asensing%20in%20IoT%20systems%20is%2C%20in%20general%2C%20to%20collectively%20represent%20an%20externally%0Aobserved%20environment%20given%20multiple%20vantage%20points%20from%20which%20sensory%0Aobservations%20occur.%20Pretraining%20of%20models%20that%20help%20interpret%20sensor%20data%20must%0Atherefore%20encode%20the%20relation%20between%20signals%20observed%20by%20sensors%20and%20the%0Aobservers%27%20vantage%20points%20in%20order%20to%20attain%20a%20representation%20that%20encodes%20the%0Aobserved%20spatial%20phenomena%20in%20a%20manner%20informed%20by%20the%20specific%20placement%20of%0Athe%20measuring%20instruments%2C%20while%20allowing%20arbitrary%20placement.%20The%20work%0Asignificantly%20advances%20self-supervised%20model%20pretraining%20from%20IoT%20signals%0Abeyond%20current%20solutions%20that%20often%20overlook%20the%20distinctive%20spatial%20nature%20of%0AIoT%20data.%20Our%20framework%20explicitly%20learns%20the%20dependencies%20between%20measurements%0Aand%20geometric%20observer%20layouts%20and%20structural%20characteristics%2C%20guided%20by%20a%20core%0Adesign%20principle%3A%20the%20duality%20between%20signals%20and%20observer%20positions.%20We%0Afurther%20provide%20theoretical%20analyses%20from%20the%20perspectives%20of%20information%0Atheory%20and%20occlusion-invariant%20representation%20learning%20to%20offer%20insight%20into%0Athe%20rationale%20behind%20our%20design.%20Experiments%20on%20three%20real-world%0Adatasets--covering%20vehicle%20monitoring%2C%20human%20activity%20recognition%2C%20and%0Aearthquake%20localization--demonstrate%20the%20superior%20generalizability%20and%0Arobustness%20of%20our%20method%20across%20diverse%20modalities%2C%20sensor%20placements%2C%0Aapplication-level%20inference%20tasks%2C%20and%20spatial%20scales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPAR%253A%2520Self-supervised%2520Placement-Aware%2520Representation%2520Learning%2520for%250A%2520%2520Multi-Node%2520IoT%2520Systems%26entry.906535625%3DYizhuo%2520Chen%2520and%2520Tianchen%2520Wang%2520and%2520You%2520Lyu%2520and%2520Yanlan%2520Hu%2520and%2520Jinyang%2520Li%2520and%2520Tomoyoshi%2520Kimura%2520and%2520Hongjue%2520Zhao%2520and%2520Yigong%2520Hu%2520and%2520Denizhan%2520Kara%2520and%2520Tarek%2520Abdelzaher%26entry.1292438233%3D%2520%2520This%2520work%2520develops%2520the%2520underpinnings%2520of%2520self-supervised%2520placement-aware%250Arepresentation%2520learning%2520given%2520spatially-distributed%2520%2528multi-view%2520and%2520multimodal%2529%250Asensor%2520observations%252C%2520motivated%2520by%2520the%2520need%2520to%2520represent%2520external%2520environmental%250Astate%2520in%2520multi-sensor%2520IoT%2520systems%2520in%2520a%2520manner%2520that%2520correctly%2520distills%2520spatial%250Aphenomena%2520from%2520the%2520distributed%2520multi-vantage%2520observations.%2520The%2520objective%2520of%250Asensing%2520in%2520IoT%2520systems%2520is%252C%2520in%2520general%252C%2520to%2520collectively%2520represent%2520an%2520externally%250Aobserved%2520environment%2520given%2520multiple%2520vantage%2520points%2520from%2520which%2520sensory%250Aobservations%2520occur.%2520Pretraining%2520of%2520models%2520that%2520help%2520interpret%2520sensor%2520data%2520must%250Atherefore%2520encode%2520the%2520relation%2520between%2520signals%2520observed%2520by%2520sensors%2520and%2520the%250Aobservers%2527%2520vantage%2520points%2520in%2520order%2520to%2520attain%2520a%2520representation%2520that%2520encodes%2520the%250Aobserved%2520spatial%2520phenomena%2520in%2520a%2520manner%2520informed%2520by%2520the%2520specific%2520placement%2520of%250Athe%2520measuring%2520instruments%252C%2520while%2520allowing%2520arbitrary%2520placement.%2520The%2520work%250Asignificantly%2520advances%2520self-supervised%2520model%2520pretraining%2520from%2520IoT%2520signals%250Abeyond%2520current%2520solutions%2520that%2520often%2520overlook%2520the%2520distinctive%2520spatial%2520nature%2520of%250AIoT%2520data.%2520Our%2520framework%2520explicitly%2520learns%2520the%2520dependencies%2520between%2520measurements%250Aand%2520geometric%2520observer%2520layouts%2520and%2520structural%2520characteristics%252C%2520guided%2520by%2520a%2520core%250Adesign%2520principle%253A%2520the%2520duality%2520between%2520signals%2520and%2520observer%2520positions.%2520We%250Afurther%2520provide%2520theoretical%2520analyses%2520from%2520the%2520perspectives%2520of%2520information%250Atheory%2520and%2520occlusion-invariant%2520representation%2520learning%2520to%2520offer%2520insight%2520into%250Athe%2520rationale%2520behind%2520our%2520design.%2520Experiments%2520on%2520three%2520real-world%250Adatasets--covering%2520vehicle%2520monitoring%252C%2520human%2520activity%2520recognition%252C%2520and%250Aearthquake%2520localization--demonstrate%2520the%2520superior%2520generalizability%2520and%250Arobustness%2520of%2520our%2520method%2520across%2520diverse%2520modalities%252C%2520sensor%2520placements%252C%250Aapplication-level%2520inference%2520tasks%252C%2520and%2520spatial%2520scales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPAR%3A%20Self-supervised%20Placement-Aware%20Representation%20Learning%20for%0A%20%20Multi-Node%20IoT%20Systems&entry.906535625=Yizhuo%20Chen%20and%20Tianchen%20Wang%20and%20You%20Lyu%20and%20Yanlan%20Hu%20and%20Jinyang%20Li%20and%20Tomoyoshi%20Kimura%20and%20Hongjue%20Zhao%20and%20Yigong%20Hu%20and%20Denizhan%20Kara%20and%20Tarek%20Abdelzaher&entry.1292438233=%20%20This%20work%20develops%20the%20underpinnings%20of%20self-supervised%20placement-aware%0Arepresentation%20learning%20given%20spatially-distributed%20%28multi-view%20and%20multimodal%29%0Asensor%20observations%2C%20motivated%20by%20the%20need%20to%20represent%20external%20environmental%0Astate%20in%20multi-sensor%20IoT%20systems%20in%20a%20manner%20that%20correctly%20distills%20spatial%0Aphenomena%20from%20the%20distributed%20multi-vantage%20observations.%20The%20objective%20of%0Asensing%20in%20IoT%20systems%20is%2C%20in%20general%2C%20to%20collectively%20represent%20an%20externally%0Aobserved%20environment%20given%20multiple%20vantage%20points%20from%20which%20sensory%0Aobservations%20occur.%20Pretraining%20of%20models%20that%20help%20interpret%20sensor%20data%20must%0Atherefore%20encode%20the%20relation%20between%20signals%20observed%20by%20sensors%20and%20the%0Aobservers%27%20vantage%20points%20in%20order%20to%20attain%20a%20representation%20that%20encodes%20the%0Aobserved%20spatial%20phenomena%20in%20a%20manner%20informed%20by%20the%20specific%20placement%20of%0Athe%20measuring%20instruments%2C%20while%20allowing%20arbitrary%20placement.%20The%20work%0Asignificantly%20advances%20self-supervised%20model%20pretraining%20from%20IoT%20signals%0Abeyond%20current%20solutions%20that%20often%20overlook%20the%20distinctive%20spatial%20nature%20of%0AIoT%20data.%20Our%20framework%20explicitly%20learns%20the%20dependencies%20between%20measurements%0Aand%20geometric%20observer%20layouts%20and%20structural%20characteristics%2C%20guided%20by%20a%20core%0Adesign%20principle%3A%20the%20duality%20between%20signals%20and%20observer%20positions.%20We%0Afurther%20provide%20theoretical%20analyses%20from%20the%20perspectives%20of%20information%0Atheory%20and%20occlusion-invariant%20representation%20learning%20to%20offer%20insight%20into%0Athe%20rationale%20behind%20our%20design.%20Experiments%20on%20three%20real-world%0Adatasets--covering%20vehicle%20monitoring%2C%20human%20activity%20recognition%2C%20and%0Aearthquake%20localization--demonstrate%20the%20superior%20generalizability%20and%0Arobustness%20of%20our%20method%20across%20diverse%20modalities%2C%20sensor%20placements%2C%0Aapplication-level%20inference%20tasks%2C%20and%20spatial%20scales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16936v1&entry.124074799=Read"},
{"title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in\n  Large Language Models While Enhancing Native Performance", "author": "Ram Mohan Rao Kadiyala and Siddartha Pullakhandam and Siddhant Gupta and Drishti Sharma and Jebish Purbey and Kanwal Mehreen and Muhammad Arham and Hamza Farooq", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages.\n", "link": "http://arxiv.org/abs/2504.09753v2", "date": "2025-05-22", "relevancy": 2.3362, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4695}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Multilingual%20Capabilities%20with%20Cultural%20and%20Local%20Knowledge%20in%0A%20%20Large%20Language%20Models%20While%20Enhancing%20Native%20Performance&body=Title%3A%20Improving%20Multilingual%20Capabilities%20with%20Cultural%20and%20Local%20Knowledge%20in%0A%20%20Large%20Language%20Models%20While%20Enhancing%20Native%20Performance%0AAuthor%3A%20Ram%20Mohan%20Rao%20Kadiyala%20and%20Siddartha%20Pullakhandam%20and%20Siddhant%20Gupta%20and%20Drishti%20Sharma%20and%20Jebish%20Purbey%20and%20Kanwal%20Mehreen%20and%20Muhammad%20Arham%20and%20Hamza%20Farooq%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%2C%20but%20their%0Adevelopment%20has%20primarily%20focused%20on%20English%20and%20other%20high-resource%20languages%2C%0Aleaving%20many%20languages%20underserved.%20We%20present%20our%20latest%20Hindi-English%0Abi-lingual%20LLM%20%5Ctextbf%7BMantra-14B%7D%20with%20~3%5C%25%20average%20improvement%20in%20benchmark%0Ascores%20over%20both%20languages%2C%20outperforming%20models%20twice%20its%20size.%20Using%20a%0Acurated%20dataset%20composed%20of%20English%20and%20Hindi%20instruction%20data%20of%20485K%20samples%2C%0Awe%20instruction%20tuned%20models%20such%20as%20Qwen-2.5-14B-Instruct%20and%20Phi-4%20to%20improve%0Aperformance%20over%20both%20English%20and%20Hindi.%20Our%20experiments%20encompassing%20seven%0Adifferent%20LLMs%20of%20varying%20parameter%20sizes%20and%20over%20140%20training%20attempts%20with%0Avarying%20English-Hindi%20training%20data%20ratios%20demonstrated%20that%20it%20is%20possible%20to%0Asignificantly%20improve%20multilingual%20performance%20without%20compromising%20native%0Aperformance.%20Further%2C%20our%20approach%20avoids%20resource-intensive%20techniques%20like%0Avocabulary%20expansion%20or%20architectural%20modifications%2C%20thus%20keeping%20the%20model%0Asize%20small.%20Our%20results%20indicate%20that%20modest%20fine-tuning%20with%20culturally%20and%0Alocally%20informed%20data%20can%20bridge%20performance%20gaps%20without%20incurring%20significant%0Acomputational%20overhead.%20We%20release%20our%20training%20code%2C%20datasets%2C%20and%20models%0Aunder%20mit%20and%20apache%20licenses%20to%20aid%20further%20research%20towards%20under-represented%0Aand%20low-resource%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Multilingual%2520Capabilities%2520with%2520Cultural%2520and%2520Local%2520Knowledge%2520in%250A%2520%2520Large%2520Language%2520Models%2520While%2520Enhancing%2520Native%2520Performance%26entry.906535625%3DRam%2520Mohan%2520Rao%2520Kadiyala%2520and%2520Siddartha%2520Pullakhandam%2520and%2520Siddhant%2520Gupta%2520and%2520Drishti%2520Sharma%2520and%2520Jebish%2520Purbey%2520and%2520Kanwal%2520Mehreen%2520and%2520Muhammad%2520Arham%2520and%2520Hamza%2520Farooq%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%252C%2520but%2520their%250Adevelopment%2520has%2520primarily%2520focused%2520on%2520English%2520and%2520other%2520high-resource%2520languages%252C%250Aleaving%2520many%2520languages%2520underserved.%2520We%2520present%2520our%2520latest%2520Hindi-English%250Abi-lingual%2520LLM%2520%255Ctextbf%257BMantra-14B%257D%2520with%2520~3%255C%2525%2520average%2520improvement%2520in%2520benchmark%250Ascores%2520over%2520both%2520languages%252C%2520outperforming%2520models%2520twice%2520its%2520size.%2520Using%2520a%250Acurated%2520dataset%2520composed%2520of%2520English%2520and%2520Hindi%2520instruction%2520data%2520of%2520485K%2520samples%252C%250Awe%2520instruction%2520tuned%2520models%2520such%2520as%2520Qwen-2.5-14B-Instruct%2520and%2520Phi-4%2520to%2520improve%250Aperformance%2520over%2520both%2520English%2520and%2520Hindi.%2520Our%2520experiments%2520encompassing%2520seven%250Adifferent%2520LLMs%2520of%2520varying%2520parameter%2520sizes%2520and%2520over%2520140%2520training%2520attempts%2520with%250Avarying%2520English-Hindi%2520training%2520data%2520ratios%2520demonstrated%2520that%2520it%2520is%2520possible%2520to%250Asignificantly%2520improve%2520multilingual%2520performance%2520without%2520compromising%2520native%250Aperformance.%2520Further%252C%2520our%2520approach%2520avoids%2520resource-intensive%2520techniques%2520like%250Avocabulary%2520expansion%2520or%2520architectural%2520modifications%252C%2520thus%2520keeping%2520the%2520model%250Asize%2520small.%2520Our%2520results%2520indicate%2520that%2520modest%2520fine-tuning%2520with%2520culturally%2520and%250Alocally%2520informed%2520data%2520can%2520bridge%2520performance%2520gaps%2520without%2520incurring%2520significant%250Acomputational%2520overhead.%2520We%2520release%2520our%2520training%2520code%252C%2520datasets%252C%2520and%2520models%250Aunder%2520mit%2520and%2520apache%2520licenses%2520to%2520aid%2520further%2520research%2520towards%2520under-represented%250Aand%2520low-resource%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Multilingual%20Capabilities%20with%20Cultural%20and%20Local%20Knowledge%20in%0A%20%20Large%20Language%20Models%20While%20Enhancing%20Native%20Performance&entry.906535625=Ram%20Mohan%20Rao%20Kadiyala%20and%20Siddartha%20Pullakhandam%20and%20Siddhant%20Gupta%20and%20Drishti%20Sharma%20and%20Jebish%20Purbey%20and%20Kanwal%20Mehreen%20and%20Muhammad%20Arham%20and%20Hamza%20Farooq&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%2C%20but%20their%0Adevelopment%20has%20primarily%20focused%20on%20English%20and%20other%20high-resource%20languages%2C%0Aleaving%20many%20languages%20underserved.%20We%20present%20our%20latest%20Hindi-English%0Abi-lingual%20LLM%20%5Ctextbf%7BMantra-14B%7D%20with%20~3%5C%25%20average%20improvement%20in%20benchmark%0Ascores%20over%20both%20languages%2C%20outperforming%20models%20twice%20its%20size.%20Using%20a%0Acurated%20dataset%20composed%20of%20English%20and%20Hindi%20instruction%20data%20of%20485K%20samples%2C%0Awe%20instruction%20tuned%20models%20such%20as%20Qwen-2.5-14B-Instruct%20and%20Phi-4%20to%20improve%0Aperformance%20over%20both%20English%20and%20Hindi.%20Our%20experiments%20encompassing%20seven%0Adifferent%20LLMs%20of%20varying%20parameter%20sizes%20and%20over%20140%20training%20attempts%20with%0Avarying%20English-Hindi%20training%20data%20ratios%20demonstrated%20that%20it%20is%20possible%20to%0Asignificantly%20improve%20multilingual%20performance%20without%20compromising%20native%0Aperformance.%20Further%2C%20our%20approach%20avoids%20resource-intensive%20techniques%20like%0Avocabulary%20expansion%20or%20architectural%20modifications%2C%20thus%20keeping%20the%20model%0Asize%20small.%20Our%20results%20indicate%20that%20modest%20fine-tuning%20with%20culturally%20and%0Alocally%20informed%20data%20can%20bridge%20performance%20gaps%20without%20incurring%20significant%0Acomputational%20overhead.%20We%20release%20our%20training%20code%2C%20datasets%2C%20and%20models%0Aunder%20mit%20and%20apache%20licenses%20to%20aid%20further%20research%20towards%20under-represented%0Aand%20low-resource%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09753v2&entry.124074799=Read"},
{"title": "Bridging the Dynamic Perception Gap: Training-Free Draft\n  Chain-of-Thought for Dynamic Multimodal Spatial Reasoning", "author": "Siqu Ou and Hongcheng Liu and Pingjie Wang and Yusheng Liao and Chuan Xuan and Yanfeng Wang and Yu Wang", "abstract": "  While chains-of-thought (CoT) have advanced complex reasoning in multimodal\nlarge language models (MLLMs), existing methods remain confined to text or\nstatic visual domains, often faltering in dynamic spatial reasoning tasks. To\nbridge this gap, we present GRASSLAND, a novel maze navigation benchmark\ndesigned to evaluate dynamic spatial reasoning. Our experiments show that\naugmenting textual reasoning chains with dynamic visual drafts, overlaid on\ninput images, significantly outperforms conventional approaches, offering new\ninsights into spatial reasoning in evolving environments. To generalize this\ncapability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free\nframework that seamlessly integrates textual CoT with corresponding visual\ndrafts into MLLMs. Extensive evaluations demonstrate that D2R consistently\nenhances performance across diverse tasks, establishing a robust baseline for\ndynamic spatial reasoning without requiring model fine-tuning. Project is open\nat https://github.com/Cratileo/D2R.\n", "link": "http://arxiv.org/abs/2505.16579v1", "date": "2025-05-22", "relevancy": 2.3315, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Dynamic%20Perception%20Gap%3A%20Training-Free%20Draft%0A%20%20Chain-of-Thought%20for%20Dynamic%20Multimodal%20Spatial%20Reasoning&body=Title%3A%20Bridging%20the%20Dynamic%20Perception%20Gap%3A%20Training-Free%20Draft%0A%20%20Chain-of-Thought%20for%20Dynamic%20Multimodal%20Spatial%20Reasoning%0AAuthor%3A%20Siqu%20Ou%20and%20Hongcheng%20Liu%20and%20Pingjie%20Wang%20and%20Yusheng%20Liao%20and%20Chuan%20Xuan%20and%20Yanfeng%20Wang%20and%20Yu%20Wang%0AAbstract%3A%20%20%20While%20chains-of-thought%20%28CoT%29%20have%20advanced%20complex%20reasoning%20in%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%2C%20existing%20methods%20remain%20confined%20to%20text%20or%0Astatic%20visual%20domains%2C%20often%20faltering%20in%20dynamic%20spatial%20reasoning%20tasks.%20To%0Abridge%20this%20gap%2C%20we%20present%20GRASSLAND%2C%20a%20novel%20maze%20navigation%20benchmark%0Adesigned%20to%20evaluate%20dynamic%20spatial%20reasoning.%20Our%20experiments%20show%20that%0Aaugmenting%20textual%20reasoning%20chains%20with%20dynamic%20visual%20drafts%2C%20overlaid%20on%0Ainput%20images%2C%20significantly%20outperforms%20conventional%20approaches%2C%20offering%20new%0Ainsights%20into%20spatial%20reasoning%20in%20evolving%20environments.%20To%20generalize%20this%0Acapability%2C%20we%20propose%20D2R%20%28Dynamic%20Draft-Augmented%20Reasoning%29%2C%20a%20training-free%0Aframework%20that%20seamlessly%20integrates%20textual%20CoT%20with%20corresponding%20visual%0Adrafts%20into%20MLLMs.%20Extensive%20evaluations%20demonstrate%20that%20D2R%20consistently%0Aenhances%20performance%20across%20diverse%20tasks%2C%20establishing%20a%20robust%20baseline%20for%0Adynamic%20spatial%20reasoning%20without%20requiring%20model%20fine-tuning.%20Project%20is%20open%0Aat%20https%3A//github.com/Cratileo/D2R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Dynamic%2520Perception%2520Gap%253A%2520Training-Free%2520Draft%250A%2520%2520Chain-of-Thought%2520for%2520Dynamic%2520Multimodal%2520Spatial%2520Reasoning%26entry.906535625%3DSiqu%2520Ou%2520and%2520Hongcheng%2520Liu%2520and%2520Pingjie%2520Wang%2520and%2520Yusheng%2520Liao%2520and%2520Chuan%2520Xuan%2520and%2520Yanfeng%2520Wang%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520While%2520chains-of-thought%2520%2528CoT%2529%2520have%2520advanced%2520complex%2520reasoning%2520in%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%252C%2520existing%2520methods%2520remain%2520confined%2520to%2520text%2520or%250Astatic%2520visual%2520domains%252C%2520often%2520faltering%2520in%2520dynamic%2520spatial%2520reasoning%2520tasks.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520present%2520GRASSLAND%252C%2520a%2520novel%2520maze%2520navigation%2520benchmark%250Adesigned%2520to%2520evaluate%2520dynamic%2520spatial%2520reasoning.%2520Our%2520experiments%2520show%2520that%250Aaugmenting%2520textual%2520reasoning%2520chains%2520with%2520dynamic%2520visual%2520drafts%252C%2520overlaid%2520on%250Ainput%2520images%252C%2520significantly%2520outperforms%2520conventional%2520approaches%252C%2520offering%2520new%250Ainsights%2520into%2520spatial%2520reasoning%2520in%2520evolving%2520environments.%2520To%2520generalize%2520this%250Acapability%252C%2520we%2520propose%2520D2R%2520%2528Dynamic%2520Draft-Augmented%2520Reasoning%2529%252C%2520a%2520training-free%250Aframework%2520that%2520seamlessly%2520integrates%2520textual%2520CoT%2520with%2520corresponding%2520visual%250Adrafts%2520into%2520MLLMs.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520D2R%2520consistently%250Aenhances%2520performance%2520across%2520diverse%2520tasks%252C%2520establishing%2520a%2520robust%2520baseline%2520for%250Adynamic%2520spatial%2520reasoning%2520without%2520requiring%2520model%2520fine-tuning.%2520Project%2520is%2520open%250Aat%2520https%253A//github.com/Cratileo/D2R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Dynamic%20Perception%20Gap%3A%20Training-Free%20Draft%0A%20%20Chain-of-Thought%20for%20Dynamic%20Multimodal%20Spatial%20Reasoning&entry.906535625=Siqu%20Ou%20and%20Hongcheng%20Liu%20and%20Pingjie%20Wang%20and%20Yusheng%20Liao%20and%20Chuan%20Xuan%20and%20Yanfeng%20Wang%20and%20Yu%20Wang&entry.1292438233=%20%20While%20chains-of-thought%20%28CoT%29%20have%20advanced%20complex%20reasoning%20in%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%2C%20existing%20methods%20remain%20confined%20to%20text%20or%0Astatic%20visual%20domains%2C%20often%20faltering%20in%20dynamic%20spatial%20reasoning%20tasks.%20To%0Abridge%20this%20gap%2C%20we%20present%20GRASSLAND%2C%20a%20novel%20maze%20navigation%20benchmark%0Adesigned%20to%20evaluate%20dynamic%20spatial%20reasoning.%20Our%20experiments%20show%20that%0Aaugmenting%20textual%20reasoning%20chains%20with%20dynamic%20visual%20drafts%2C%20overlaid%20on%0Ainput%20images%2C%20significantly%20outperforms%20conventional%20approaches%2C%20offering%20new%0Ainsights%20into%20spatial%20reasoning%20in%20evolving%20environments.%20To%20generalize%20this%0Acapability%2C%20we%20propose%20D2R%20%28Dynamic%20Draft-Augmented%20Reasoning%29%2C%20a%20training-free%0Aframework%20that%20seamlessly%20integrates%20textual%20CoT%20with%20corresponding%20visual%0Adrafts%20into%20MLLMs.%20Extensive%20evaluations%20demonstrate%20that%20D2R%20consistently%0Aenhances%20performance%20across%20diverse%20tasks%2C%20establishing%20a%20robust%20baseline%20for%0Adynamic%20spatial%20reasoning%20without%20requiring%20model%20fine-tuning.%20Project%20is%20open%0Aat%20https%3A//github.com/Cratileo/D2R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16579v1&entry.124074799=Read"},
{"title": "LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple\n  Object Tracking", "author": "Martha Teiko Teye and Ori Maoz and Matthias Rottmann", "abstract": "  Multi-object tracking from LiDAR point clouds presents unique challenges due\nto the sparse and irregular nature of the data, compounded by the need for\ntemporal coherence across frames. Traditional tracking systems often rely on\nhand-crafted features and motion models, which can struggle to maintain\nconsistent object identities in crowded or fast-moving scenes. We present a\nlidar-based two-staged DETR inspired transformer; a smoother and tracker. The\nsmoother stage refines lidar object detections, from any off-the-shelf\ndetector, across a moving temporal window. The tracker stage uses a DETR-based\nattention block to maintain tracks across time by associating tracked objects\nwith the refined detections using the point cloud as context. The model is\ntrained on the datasets nuScenes and KITTI in both online and offline (forward\npeeking) modes demonstrating strong performance across metrics such as\nID-switch and multiple object tracking accuracy (MOTA). The numerical results\nindicate that the online mode outperforms the lidar-only baseline and SOTA\nmodels on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,\nwhile the offline mode provides an additional 3 pp aMOTP.\n", "link": "http://arxiv.org/abs/2505.12753v2", "date": "2025-05-22", "relevancy": 2.322, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5846}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR%20MOT-DETR%3A%20A%20LiDAR-based%20Two-Stage%20Transformer%20for%203D%20Multiple%0A%20%20Object%20Tracking&body=Title%3A%20LiDAR%20MOT-DETR%3A%20A%20LiDAR-based%20Two-Stage%20Transformer%20for%203D%20Multiple%0A%20%20Object%20Tracking%0AAuthor%3A%20Martha%20Teiko%20Teye%20and%20Ori%20Maoz%20and%20Matthias%20Rottmann%0AAbstract%3A%20%20%20Multi-object%20tracking%20from%20LiDAR%20point%20clouds%20presents%20unique%20challenges%20due%0Ato%20the%20sparse%20and%20irregular%20nature%20of%20the%20data%2C%20compounded%20by%20the%20need%20for%0Atemporal%20coherence%20across%20frames.%20Traditional%20tracking%20systems%20often%20rely%20on%0Ahand-crafted%20features%20and%20motion%20models%2C%20which%20can%20struggle%20to%20maintain%0Aconsistent%20object%20identities%20in%20crowded%20or%20fast-moving%20scenes.%20We%20present%20a%0Alidar-based%20two-staged%20DETR%20inspired%20transformer%3B%20a%20smoother%20and%20tracker.%20The%0Asmoother%20stage%20refines%20lidar%20object%20detections%2C%20from%20any%20off-the-shelf%0Adetector%2C%20across%20a%20moving%20temporal%20window.%20The%20tracker%20stage%20uses%20a%20DETR-based%0Aattention%20block%20to%20maintain%20tracks%20across%20time%20by%20associating%20tracked%20objects%0Awith%20the%20refined%20detections%20using%20the%20point%20cloud%20as%20context.%20The%20model%20is%0Atrained%20on%20the%20datasets%20nuScenes%20and%20KITTI%20in%20both%20online%20and%20offline%20%28forward%0Apeeking%29%20modes%20demonstrating%20strong%20performance%20across%20metrics%20such%20as%0AID-switch%20and%20multiple%20object%20tracking%20accuracy%20%28MOTA%29.%20The%20numerical%20results%0Aindicate%20that%20the%20online%20mode%20outperforms%20the%20lidar-only%20baseline%20and%20SOTA%0Amodels%20on%20the%20nuScenes%20dataset%2C%20with%20an%20aMOTA%20of%200.722%20and%20an%20aMOTP%20of%200.475%2C%0Awhile%20the%20offline%20mode%20provides%20an%20additional%203%20pp%20aMOTP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR%2520MOT-DETR%253A%2520A%2520LiDAR-based%2520Two-Stage%2520Transformer%2520for%25203D%2520Multiple%250A%2520%2520Object%2520Tracking%26entry.906535625%3DMartha%2520Teiko%2520Teye%2520and%2520Ori%2520Maoz%2520and%2520Matthias%2520Rottmann%26entry.1292438233%3D%2520%2520Multi-object%2520tracking%2520from%2520LiDAR%2520point%2520clouds%2520presents%2520unique%2520challenges%2520due%250Ato%2520the%2520sparse%2520and%2520irregular%2520nature%2520of%2520the%2520data%252C%2520compounded%2520by%2520the%2520need%2520for%250Atemporal%2520coherence%2520across%2520frames.%2520Traditional%2520tracking%2520systems%2520often%2520rely%2520on%250Ahand-crafted%2520features%2520and%2520motion%2520models%252C%2520which%2520can%2520struggle%2520to%2520maintain%250Aconsistent%2520object%2520identities%2520in%2520crowded%2520or%2520fast-moving%2520scenes.%2520We%2520present%2520a%250Alidar-based%2520two-staged%2520DETR%2520inspired%2520transformer%253B%2520a%2520smoother%2520and%2520tracker.%2520The%250Asmoother%2520stage%2520refines%2520lidar%2520object%2520detections%252C%2520from%2520any%2520off-the-shelf%250Adetector%252C%2520across%2520a%2520moving%2520temporal%2520window.%2520The%2520tracker%2520stage%2520uses%2520a%2520DETR-based%250Aattention%2520block%2520to%2520maintain%2520tracks%2520across%2520time%2520by%2520associating%2520tracked%2520objects%250Awith%2520the%2520refined%2520detections%2520using%2520the%2520point%2520cloud%2520as%2520context.%2520The%2520model%2520is%250Atrained%2520on%2520the%2520datasets%2520nuScenes%2520and%2520KITTI%2520in%2520both%2520online%2520and%2520offline%2520%2528forward%250Apeeking%2529%2520modes%2520demonstrating%2520strong%2520performance%2520across%2520metrics%2520such%2520as%250AID-switch%2520and%2520multiple%2520object%2520tracking%2520accuracy%2520%2528MOTA%2529.%2520The%2520numerical%2520results%250Aindicate%2520that%2520the%2520online%2520mode%2520outperforms%2520the%2520lidar-only%2520baseline%2520and%2520SOTA%250Amodels%2520on%2520the%2520nuScenes%2520dataset%252C%2520with%2520an%2520aMOTA%2520of%25200.722%2520and%2520an%2520aMOTP%2520of%25200.475%252C%250Awhile%2520the%2520offline%2520mode%2520provides%2520an%2520additional%25203%2520pp%2520aMOTP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR%20MOT-DETR%3A%20A%20LiDAR-based%20Two-Stage%20Transformer%20for%203D%20Multiple%0A%20%20Object%20Tracking&entry.906535625=Martha%20Teiko%20Teye%20and%20Ori%20Maoz%20and%20Matthias%20Rottmann&entry.1292438233=%20%20Multi-object%20tracking%20from%20LiDAR%20point%20clouds%20presents%20unique%20challenges%20due%0Ato%20the%20sparse%20and%20irregular%20nature%20of%20the%20data%2C%20compounded%20by%20the%20need%20for%0Atemporal%20coherence%20across%20frames.%20Traditional%20tracking%20systems%20often%20rely%20on%0Ahand-crafted%20features%20and%20motion%20models%2C%20which%20can%20struggle%20to%20maintain%0Aconsistent%20object%20identities%20in%20crowded%20or%20fast-moving%20scenes.%20We%20present%20a%0Alidar-based%20two-staged%20DETR%20inspired%20transformer%3B%20a%20smoother%20and%20tracker.%20The%0Asmoother%20stage%20refines%20lidar%20object%20detections%2C%20from%20any%20off-the-shelf%0Adetector%2C%20across%20a%20moving%20temporal%20window.%20The%20tracker%20stage%20uses%20a%20DETR-based%0Aattention%20block%20to%20maintain%20tracks%20across%20time%20by%20associating%20tracked%20objects%0Awith%20the%20refined%20detections%20using%20the%20point%20cloud%20as%20context.%20The%20model%20is%0Atrained%20on%20the%20datasets%20nuScenes%20and%20KITTI%20in%20both%20online%20and%20offline%20%28forward%0Apeeking%29%20modes%20demonstrating%20strong%20performance%20across%20metrics%20such%20as%0AID-switch%20and%20multiple%20object%20tracking%20accuracy%20%28MOTA%29.%20The%20numerical%20results%0Aindicate%20that%20the%20online%20mode%20outperforms%20the%20lidar-only%20baseline%20and%20SOTA%0Amodels%20on%20the%20nuScenes%20dataset%2C%20with%20an%20aMOTA%20of%200.722%20and%20an%20aMOTP%20of%200.475%2C%0Awhile%20the%20offline%20mode%20provides%20an%20additional%203%20pp%20aMOTP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12753v2&entry.124074799=Read"},
{"title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought\n  Reasoning in Vision-Language Models for Autonomous Driving", "author": "Kangan Qian and Sicong Jiang and Yang Zhong and Ziang Luo and Zilin Huang and Tianze Zhu and Kun Jiang and Mengmeng Yang and Zheng Fu and Jinyu Miao and Yining Shi and He Zhe Lim and Li Liu and Tianbao Zhou and Hongyi Wang and Huang Yu and Yifei Hu and Guang Li and Guang Chen and Hao Ye and Lijun Sun and Diange Yang", "abstract": "  Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models.\n", "link": "http://arxiv.org/abs/2505.15298v2", "date": "2025-05-22", "relevancy": 2.3191, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentThink%3A%20A%20Unified%20Framework%20for%20Tool-Augmented%20Chain-of-Thought%0A%20%20Reasoning%20in%20Vision-Language%20Models%20for%20Autonomous%20Driving&body=Title%3A%20AgentThink%3A%20A%20Unified%20Framework%20for%20Tool-Augmented%20Chain-of-Thought%0A%20%20Reasoning%20in%20Vision-Language%20Models%20for%20Autonomous%20Driving%0AAuthor%3A%20Kangan%20Qian%20and%20Sicong%20Jiang%20and%20Yang%20Zhong%20and%20Ziang%20Luo%20and%20Zilin%20Huang%20and%20Tianze%20Zhu%20and%20Kun%20Jiang%20and%20Mengmeng%20Yang%20and%20Zheng%20Fu%20and%20Jinyu%20Miao%20and%20Yining%20Shi%20and%20He%20Zhe%20Lim%20and%20Li%20Liu%20and%20Tianbao%20Zhou%20and%20Hongyi%20Wang%20and%20Huang%20Yu%20and%20Yifei%20Hu%20and%20Guang%20Li%20and%20Guang%20Chen%20and%20Hao%20Ye%20and%20Lijun%20Sun%20and%20Diange%20Yang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20show%20promise%20for%20autonomous%20driving%2C%20yet%20their%0Astruggle%20with%20hallucinations%2C%20inefficient%20reasoning%2C%20and%20limited%20real-world%0Avalidation%20hinders%20accurate%20perception%20and%20robust%20step-by-step%20reasoning.%20To%0Aovercome%20this%2C%20we%20introduce%20%5Ctextbf%7BAgentThink%7D%2C%20a%20pioneering%20unified%20framework%0Athat%2C%20for%20the%20first%20time%2C%20integrates%20Chain-of-Thought%20%28CoT%29%20reasoning%20with%0Adynamic%2C%20agent-style%20tool%20invocation%20for%20autonomous%20driving%20tasks.%20AgentThink%27s%0Acore%20innovations%20include%3A%20%5Ctextbf%7B%28i%29%20Structured%20Data%20Generation%7D%2C%20by%0Aestablishing%20an%20autonomous%20driving%20tool%20library%20to%20automatically%20construct%0Astructured%2C%20self-verified%20reasoning%20data%20explicitly%20incorporating%20tool%20usage%0Afor%20diverse%20driving%20scenarios%3B%20%5Ctextbf%7B%28ii%29%20A%20Two-stage%20Training%20Pipeline%7D%2C%0Aemploying%20Supervised%20Fine-Tuning%20%28SFT%29%20with%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20to%20equip%20VLMs%20with%20the%20capability%20for%20autonomous%20tool%20invocation%3B%20and%0A%5Ctextbf%7B%28iii%29%20Agent-style%20Tool-Usage%20Evaluation%7D%2C%20introducing%20a%20novel%0Amulti-tool%20assessment%20protocol%20to%20rigorously%20evaluate%20the%20model%27s%20tool%0Ainvocation%20and%20utilization.%20Experiments%20on%20the%20DriveLMM-o1%20benchmark%0Ademonstrate%20AgentThink%20significantly%20boosts%20overall%20reasoning%20scores%20by%0A%5Ctextbf%7B53.91%5C%25%7D%20and%20enhances%20answer%20accuracy%20by%20%5Ctextbf%7B33.54%5C%25%7D%2C%20while%0Amarkedly%20improving%20reasoning%20quality%20and%20consistency.%20Furthermore%2C%20ablation%0Astudies%20and%20robust%20zero-shot/few-shot%20generalization%20experiments%20across%20various%0Abenchmarks%20underscore%20its%20powerful%20capabilities.%20These%20findings%20highlight%20a%0Apromising%20trajectory%20for%20developing%20trustworthy%20and%20tool-aware%20autonomous%0Adriving%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentThink%253A%2520A%2520Unified%2520Framework%2520for%2520Tool-Augmented%2520Chain-of-Thought%250A%2520%2520Reasoning%2520in%2520Vision-Language%2520Models%2520for%2520Autonomous%2520Driving%26entry.906535625%3DKangan%2520Qian%2520and%2520Sicong%2520Jiang%2520and%2520Yang%2520Zhong%2520and%2520Ziang%2520Luo%2520and%2520Zilin%2520Huang%2520and%2520Tianze%2520Zhu%2520and%2520Kun%2520Jiang%2520and%2520Mengmeng%2520Yang%2520and%2520Zheng%2520Fu%2520and%2520Jinyu%2520Miao%2520and%2520Yining%2520Shi%2520and%2520He%2520Zhe%2520Lim%2520and%2520Li%2520Liu%2520and%2520Tianbao%2520Zhou%2520and%2520Hongyi%2520Wang%2520and%2520Huang%2520Yu%2520and%2520Yifei%2520Hu%2520and%2520Guang%2520Li%2520and%2520Guang%2520Chen%2520and%2520Hao%2520Ye%2520and%2520Lijun%2520Sun%2520and%2520Diange%2520Yang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520show%2520promise%2520for%2520autonomous%2520driving%252C%2520yet%2520their%250Astruggle%2520with%2520hallucinations%252C%2520inefficient%2520reasoning%252C%2520and%2520limited%2520real-world%250Avalidation%2520hinders%2520accurate%2520perception%2520and%2520robust%2520step-by-step%2520reasoning.%2520To%250Aovercome%2520this%252C%2520we%2520introduce%2520%255Ctextbf%257BAgentThink%257D%252C%2520a%2520pioneering%2520unified%2520framework%250Athat%252C%2520for%2520the%2520first%2520time%252C%2520integrates%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520with%250Adynamic%252C%2520agent-style%2520tool%2520invocation%2520for%2520autonomous%2520driving%2520tasks.%2520AgentThink%2527s%250Acore%2520innovations%2520include%253A%2520%255Ctextbf%257B%2528i%2529%2520Structured%2520Data%2520Generation%257D%252C%2520by%250Aestablishing%2520an%2520autonomous%2520driving%2520tool%2520library%2520to%2520automatically%2520construct%250Astructured%252C%2520self-verified%2520reasoning%2520data%2520explicitly%2520incorporating%2520tool%2520usage%250Afor%2520diverse%2520driving%2520scenarios%253B%2520%255Ctextbf%257B%2528ii%2529%2520A%2520Two-stage%2520Training%2520Pipeline%257D%252C%250Aemploying%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520with%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%2520to%2520equip%2520VLMs%2520with%2520the%2520capability%2520for%2520autonomous%2520tool%2520invocation%253B%2520and%250A%255Ctextbf%257B%2528iii%2529%2520Agent-style%2520Tool-Usage%2520Evaluation%257D%252C%2520introducing%2520a%2520novel%250Amulti-tool%2520assessment%2520protocol%2520to%2520rigorously%2520evaluate%2520the%2520model%2527s%2520tool%250Ainvocation%2520and%2520utilization.%2520Experiments%2520on%2520the%2520DriveLMM-o1%2520benchmark%250Ademonstrate%2520AgentThink%2520significantly%2520boosts%2520overall%2520reasoning%2520scores%2520by%250A%255Ctextbf%257B53.91%255C%2525%257D%2520and%2520enhances%2520answer%2520accuracy%2520by%2520%255Ctextbf%257B33.54%255C%2525%257D%252C%2520while%250Amarkedly%2520improving%2520reasoning%2520quality%2520and%2520consistency.%2520Furthermore%252C%2520ablation%250Astudies%2520and%2520robust%2520zero-shot/few-shot%2520generalization%2520experiments%2520across%2520various%250Abenchmarks%2520underscore%2520its%2520powerful%2520capabilities.%2520These%2520findings%2520highlight%2520a%250Apromising%2520trajectory%2520for%2520developing%2520trustworthy%2520and%2520tool-aware%2520autonomous%250Adriving%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentThink%3A%20A%20Unified%20Framework%20for%20Tool-Augmented%20Chain-of-Thought%0A%20%20Reasoning%20in%20Vision-Language%20Models%20for%20Autonomous%20Driving&entry.906535625=Kangan%20Qian%20and%20Sicong%20Jiang%20and%20Yang%20Zhong%20and%20Ziang%20Luo%20and%20Zilin%20Huang%20and%20Tianze%20Zhu%20and%20Kun%20Jiang%20and%20Mengmeng%20Yang%20and%20Zheng%20Fu%20and%20Jinyu%20Miao%20and%20Yining%20Shi%20and%20He%20Zhe%20Lim%20and%20Li%20Liu%20and%20Tianbao%20Zhou%20and%20Hongyi%20Wang%20and%20Huang%20Yu%20and%20Yifei%20Hu%20and%20Guang%20Li%20and%20Guang%20Chen%20and%20Hao%20Ye%20and%20Lijun%20Sun%20and%20Diange%20Yang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20show%20promise%20for%20autonomous%20driving%2C%20yet%20their%0Astruggle%20with%20hallucinations%2C%20inefficient%20reasoning%2C%20and%20limited%20real-world%0Avalidation%20hinders%20accurate%20perception%20and%20robust%20step-by-step%20reasoning.%20To%0Aovercome%20this%2C%20we%20introduce%20%5Ctextbf%7BAgentThink%7D%2C%20a%20pioneering%20unified%20framework%0Athat%2C%20for%20the%20first%20time%2C%20integrates%20Chain-of-Thought%20%28CoT%29%20reasoning%20with%0Adynamic%2C%20agent-style%20tool%20invocation%20for%20autonomous%20driving%20tasks.%20AgentThink%27s%0Acore%20innovations%20include%3A%20%5Ctextbf%7B%28i%29%20Structured%20Data%20Generation%7D%2C%20by%0Aestablishing%20an%20autonomous%20driving%20tool%20library%20to%20automatically%20construct%0Astructured%2C%20self-verified%20reasoning%20data%20explicitly%20incorporating%20tool%20usage%0Afor%20diverse%20driving%20scenarios%3B%20%5Ctextbf%7B%28ii%29%20A%20Two-stage%20Training%20Pipeline%7D%2C%0Aemploying%20Supervised%20Fine-Tuning%20%28SFT%29%20with%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20to%20equip%20VLMs%20with%20the%20capability%20for%20autonomous%20tool%20invocation%3B%20and%0A%5Ctextbf%7B%28iii%29%20Agent-style%20Tool-Usage%20Evaluation%7D%2C%20introducing%20a%20novel%0Amulti-tool%20assessment%20protocol%20to%20rigorously%20evaluate%20the%20model%27s%20tool%0Ainvocation%20and%20utilization.%20Experiments%20on%20the%20DriveLMM-o1%20benchmark%0Ademonstrate%20AgentThink%20significantly%20boosts%20overall%20reasoning%20scores%20by%0A%5Ctextbf%7B53.91%5C%25%7D%20and%20enhances%20answer%20accuracy%20by%20%5Ctextbf%7B33.54%5C%25%7D%2C%20while%0Amarkedly%20improving%20reasoning%20quality%20and%20consistency.%20Furthermore%2C%20ablation%0Astudies%20and%20robust%20zero-shot/few-shot%20generalization%20experiments%20across%20various%0Abenchmarks%20underscore%20its%20powerful%20capabilities.%20These%20findings%20highlight%20a%0Apromising%20trajectory%20for%20developing%20trustworthy%20and%20tool-aware%20autonomous%0Adriving%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15298v2&entry.124074799=Read"},
{"title": "Conditional Panoramic Image Generation via Masked Autoregressive\n  Modeling", "author": "Chaoyang Wang and Xiangtai Li and Lu Qi and Xiaofan Lin and Jinbin Bai and Qianyu Zhou and Yunhai Tong", "abstract": "  Recent progress in panoramic image generation has underscored two critical\nlimitations in existing approaches. First, most methods are built upon\ndiffusion models, which are inherently ill-suited for equirectangular\nprojection (ERP) panoramas due to the violation of the identically and\nindependently distributed (i.i.d.) Gaussian noise assumption caused by their\nspherical mapping. Second, these methods often treat text-conditioned\ngeneration (text-to-panorama) and image-conditioned generation (panorama\noutpainting) as separate tasks, relying on distinct architectures and\ntask-specific data. In this work, we propose a unified framework, Panoramic\nAutoRegressive model (PAR), which leverages masked autoregressive modeling to\naddress these challenges. PAR avoids the i.i.d. assumption constraint and\nintegrates text and image conditioning into a cohesive architecture, enabling\nseamless generation across tasks. To address the inherent discontinuity in\nexisting generative models, we introduce circular padding to enhance spatial\ncoherence and propose a consistency alignment strategy to improve generation\nquality. Extensive experiments demonstrate competitive performance in\ntext-to-image generation and panorama outpainting tasks while showcasing\npromising scalability and generalization capabilities.\n", "link": "http://arxiv.org/abs/2505.16862v1", "date": "2025-05-22", "relevancy": 2.3138, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6431}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5683}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Panoramic%20Image%20Generation%20via%20Masked%20Autoregressive%0A%20%20Modeling&body=Title%3A%20Conditional%20Panoramic%20Image%20Generation%20via%20Masked%20Autoregressive%0A%20%20Modeling%0AAuthor%3A%20Chaoyang%20Wang%20and%20Xiangtai%20Li%20and%20Lu%20Qi%20and%20Xiaofan%20Lin%20and%20Jinbin%20Bai%20and%20Qianyu%20Zhou%20and%20Yunhai%20Tong%0AAbstract%3A%20%20%20Recent%20progress%20in%20panoramic%20image%20generation%20has%20underscored%20two%20critical%0Alimitations%20in%20existing%20approaches.%20First%2C%20most%20methods%20are%20built%20upon%0Adiffusion%20models%2C%20which%20are%20inherently%20ill-suited%20for%20equirectangular%0Aprojection%20%28ERP%29%20panoramas%20due%20to%20the%20violation%20of%20the%20identically%20and%0Aindependently%20distributed%20%28i.i.d.%29%20Gaussian%20noise%20assumption%20caused%20by%20their%0Aspherical%20mapping.%20Second%2C%20these%20methods%20often%20treat%20text-conditioned%0Ageneration%20%28text-to-panorama%29%20and%20image-conditioned%20generation%20%28panorama%0Aoutpainting%29%20as%20separate%20tasks%2C%20relying%20on%20distinct%20architectures%20and%0Atask-specific%20data.%20In%20this%20work%2C%20we%20propose%20a%20unified%20framework%2C%20Panoramic%0AAutoRegressive%20model%20%28PAR%29%2C%20which%20leverages%20masked%20autoregressive%20modeling%20to%0Aaddress%20these%20challenges.%20PAR%20avoids%20the%20i.i.d.%20assumption%20constraint%20and%0Aintegrates%20text%20and%20image%20conditioning%20into%20a%20cohesive%20architecture%2C%20enabling%0Aseamless%20generation%20across%20tasks.%20To%20address%20the%20inherent%20discontinuity%20in%0Aexisting%20generative%20models%2C%20we%20introduce%20circular%20padding%20to%20enhance%20spatial%0Acoherence%20and%20propose%20a%20consistency%20alignment%20strategy%20to%20improve%20generation%0Aquality.%20Extensive%20experiments%20demonstrate%20competitive%20performance%20in%0Atext-to-image%20generation%20and%20panorama%20outpainting%20tasks%20while%20showcasing%0Apromising%20scalability%20and%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Panoramic%2520Image%2520Generation%2520via%2520Masked%2520Autoregressive%250A%2520%2520Modeling%26entry.906535625%3DChaoyang%2520Wang%2520and%2520Xiangtai%2520Li%2520and%2520Lu%2520Qi%2520and%2520Xiaofan%2520Lin%2520and%2520Jinbin%2520Bai%2520and%2520Qianyu%2520Zhou%2520and%2520Yunhai%2520Tong%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520panoramic%2520image%2520generation%2520has%2520underscored%2520two%2520critical%250Alimitations%2520in%2520existing%2520approaches.%2520First%252C%2520most%2520methods%2520are%2520built%2520upon%250Adiffusion%2520models%252C%2520which%2520are%2520inherently%2520ill-suited%2520for%2520equirectangular%250Aprojection%2520%2528ERP%2529%2520panoramas%2520due%2520to%2520the%2520violation%2520of%2520the%2520identically%2520and%250Aindependently%2520distributed%2520%2528i.i.d.%2529%2520Gaussian%2520noise%2520assumption%2520caused%2520by%2520their%250Aspherical%2520mapping.%2520Second%252C%2520these%2520methods%2520often%2520treat%2520text-conditioned%250Ageneration%2520%2528text-to-panorama%2529%2520and%2520image-conditioned%2520generation%2520%2528panorama%250Aoutpainting%2529%2520as%2520separate%2520tasks%252C%2520relying%2520on%2520distinct%2520architectures%2520and%250Atask-specific%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520unified%2520framework%252C%2520Panoramic%250AAutoRegressive%2520model%2520%2528PAR%2529%252C%2520which%2520leverages%2520masked%2520autoregressive%2520modeling%2520to%250Aaddress%2520these%2520challenges.%2520PAR%2520avoids%2520the%2520i.i.d.%2520assumption%2520constraint%2520and%250Aintegrates%2520text%2520and%2520image%2520conditioning%2520into%2520a%2520cohesive%2520architecture%252C%2520enabling%250Aseamless%2520generation%2520across%2520tasks.%2520To%2520address%2520the%2520inherent%2520discontinuity%2520in%250Aexisting%2520generative%2520models%252C%2520we%2520introduce%2520circular%2520padding%2520to%2520enhance%2520spatial%250Acoherence%2520and%2520propose%2520a%2520consistency%2520alignment%2520strategy%2520to%2520improve%2520generation%250Aquality.%2520Extensive%2520experiments%2520demonstrate%2520competitive%2520performance%2520in%250Atext-to-image%2520generation%2520and%2520panorama%2520outpainting%2520tasks%2520while%2520showcasing%250Apromising%2520scalability%2520and%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Panoramic%20Image%20Generation%20via%20Masked%20Autoregressive%0A%20%20Modeling&entry.906535625=Chaoyang%20Wang%20and%20Xiangtai%20Li%20and%20Lu%20Qi%20and%20Xiaofan%20Lin%20and%20Jinbin%20Bai%20and%20Qianyu%20Zhou%20and%20Yunhai%20Tong&entry.1292438233=%20%20Recent%20progress%20in%20panoramic%20image%20generation%20has%20underscored%20two%20critical%0Alimitations%20in%20existing%20approaches.%20First%2C%20most%20methods%20are%20built%20upon%0Adiffusion%20models%2C%20which%20are%20inherently%20ill-suited%20for%20equirectangular%0Aprojection%20%28ERP%29%20panoramas%20due%20to%20the%20violation%20of%20the%20identically%20and%0Aindependently%20distributed%20%28i.i.d.%29%20Gaussian%20noise%20assumption%20caused%20by%20their%0Aspherical%20mapping.%20Second%2C%20these%20methods%20often%20treat%20text-conditioned%0Ageneration%20%28text-to-panorama%29%20and%20image-conditioned%20generation%20%28panorama%0Aoutpainting%29%20as%20separate%20tasks%2C%20relying%20on%20distinct%20architectures%20and%0Atask-specific%20data.%20In%20this%20work%2C%20we%20propose%20a%20unified%20framework%2C%20Panoramic%0AAutoRegressive%20model%20%28PAR%29%2C%20which%20leverages%20masked%20autoregressive%20modeling%20to%0Aaddress%20these%20challenges.%20PAR%20avoids%20the%20i.i.d.%20assumption%20constraint%20and%0Aintegrates%20text%20and%20image%20conditioning%20into%20a%20cohesive%20architecture%2C%20enabling%0Aseamless%20generation%20across%20tasks.%20To%20address%20the%20inherent%20discontinuity%20in%0Aexisting%20generative%20models%2C%20we%20introduce%20circular%20padding%20to%20enhance%20spatial%0Acoherence%20and%20propose%20a%20consistency%20alignment%20strategy%20to%20improve%20generation%0Aquality.%20Extensive%20experiments%20demonstrate%20competitive%20performance%20in%0Atext-to-image%20generation%20and%20panorama%20outpainting%20tasks%20while%20showcasing%0Apromising%20scalability%20and%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16862v1&entry.124074799=Read"},
{"title": "Zero-Shot Anomaly Detection in Battery Thermal Images Using Visual\n  Question Answering with Prior Knowledge", "author": "Marcella Astrid and Abdelrahman Shabayek and Djamila Aouada", "abstract": "  Batteries are essential for various applications, including electric vehicles\nand renewable energy storage, making safety and efficiency critical concerns.\nAnomaly detection in battery thermal images helps identify failures early, but\ntraditional deep learning methods require extensive labeled data, which is\ndifficult to obtain, especially for anomalies due to safety risks and high data\ncollection costs. To overcome this, we explore zero-shot anomaly detection\nusing Visual Question Answering (VQA) models, which leverage pretrained\nknowledge and textbased prompts to generalize across vision tasks. By\nincorporating prior knowledge of normal battery thermal behavior, we design\nprompts to detect anomalies without battery-specific training data. We evaluate\nthree VQA models (ChatGPT-4o, LLaVa-13b, and BLIP-2) analyzing their robustness\nto prompt variations, repeated trials, and qualitative outputs. Despite the\nlack of finetuning on battery data, our approach demonstrates competitive\nperformance compared to state-of-the-art models that are trained with the\nbattery data. Our findings highlight the potential of VQA-based zero-shot\nlearning for battery anomaly detection and suggest future directions for\nimproving its effectiveness.\n", "link": "http://arxiv.org/abs/2505.16674v1", "date": "2025-05-22", "relevancy": 2.3134, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Anomaly%20Detection%20in%20Battery%20Thermal%20Images%20Using%20Visual%0A%20%20Question%20Answering%20with%20Prior%20Knowledge&body=Title%3A%20Zero-Shot%20Anomaly%20Detection%20in%20Battery%20Thermal%20Images%20Using%20Visual%0A%20%20Question%20Answering%20with%20Prior%20Knowledge%0AAuthor%3A%20Marcella%20Astrid%20and%20Abdelrahman%20Shabayek%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Batteries%20are%20essential%20for%20various%20applications%2C%20including%20electric%20vehicles%0Aand%20renewable%20energy%20storage%2C%20making%20safety%20and%20efficiency%20critical%20concerns.%0AAnomaly%20detection%20in%20battery%20thermal%20images%20helps%20identify%20failures%20early%2C%20but%0Atraditional%20deep%20learning%20methods%20require%20extensive%20labeled%20data%2C%20which%20is%0Adifficult%20to%20obtain%2C%20especially%20for%20anomalies%20due%20to%20safety%20risks%20and%20high%20data%0Acollection%20costs.%20To%20overcome%20this%2C%20we%20explore%20zero-shot%20anomaly%20detection%0Ausing%20Visual%20Question%20Answering%20%28VQA%29%20models%2C%20which%20leverage%20pretrained%0Aknowledge%20and%20textbased%20prompts%20to%20generalize%20across%20vision%20tasks.%20By%0Aincorporating%20prior%20knowledge%20of%20normal%20battery%20thermal%20behavior%2C%20we%20design%0Aprompts%20to%20detect%20anomalies%20without%20battery-specific%20training%20data.%20We%20evaluate%0Athree%20VQA%20models%20%28ChatGPT-4o%2C%20LLaVa-13b%2C%20and%20BLIP-2%29%20analyzing%20their%20robustness%0Ato%20prompt%20variations%2C%20repeated%20trials%2C%20and%20qualitative%20outputs.%20Despite%20the%0Alack%20of%20finetuning%20on%20battery%20data%2C%20our%20approach%20demonstrates%20competitive%0Aperformance%20compared%20to%20state-of-the-art%20models%20that%20are%20trained%20with%20the%0Abattery%20data.%20Our%20findings%20highlight%20the%20potential%20of%20VQA-based%20zero-shot%0Alearning%20for%20battery%20anomaly%20detection%20and%20suggest%20future%20directions%20for%0Aimproving%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Anomaly%2520Detection%2520in%2520Battery%2520Thermal%2520Images%2520Using%2520Visual%250A%2520%2520Question%2520Answering%2520with%2520Prior%2520Knowledge%26entry.906535625%3DMarcella%2520Astrid%2520and%2520Abdelrahman%2520Shabayek%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Batteries%2520are%2520essential%2520for%2520various%2520applications%252C%2520including%2520electric%2520vehicles%250Aand%2520renewable%2520energy%2520storage%252C%2520making%2520safety%2520and%2520efficiency%2520critical%2520concerns.%250AAnomaly%2520detection%2520in%2520battery%2520thermal%2520images%2520helps%2520identify%2520failures%2520early%252C%2520but%250Atraditional%2520deep%2520learning%2520methods%2520require%2520extensive%2520labeled%2520data%252C%2520which%2520is%250Adifficult%2520to%2520obtain%252C%2520especially%2520for%2520anomalies%2520due%2520to%2520safety%2520risks%2520and%2520high%2520data%250Acollection%2520costs.%2520To%2520overcome%2520this%252C%2520we%2520explore%2520zero-shot%2520anomaly%2520detection%250Ausing%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520models%252C%2520which%2520leverage%2520pretrained%250Aknowledge%2520and%2520textbased%2520prompts%2520to%2520generalize%2520across%2520vision%2520tasks.%2520By%250Aincorporating%2520prior%2520knowledge%2520of%2520normal%2520battery%2520thermal%2520behavior%252C%2520we%2520design%250Aprompts%2520to%2520detect%2520anomalies%2520without%2520battery-specific%2520training%2520data.%2520We%2520evaluate%250Athree%2520VQA%2520models%2520%2528ChatGPT-4o%252C%2520LLaVa-13b%252C%2520and%2520BLIP-2%2529%2520analyzing%2520their%2520robustness%250Ato%2520prompt%2520variations%252C%2520repeated%2520trials%252C%2520and%2520qualitative%2520outputs.%2520Despite%2520the%250Alack%2520of%2520finetuning%2520on%2520battery%2520data%252C%2520our%2520approach%2520demonstrates%2520competitive%250Aperformance%2520compared%2520to%2520state-of-the-art%2520models%2520that%2520are%2520trained%2520with%2520the%250Abattery%2520data.%2520Our%2520findings%2520highlight%2520the%2520potential%2520of%2520VQA-based%2520zero-shot%250Alearning%2520for%2520battery%2520anomaly%2520detection%2520and%2520suggest%2520future%2520directions%2520for%250Aimproving%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Anomaly%20Detection%20in%20Battery%20Thermal%20Images%20Using%20Visual%0A%20%20Question%20Answering%20with%20Prior%20Knowledge&entry.906535625=Marcella%20Astrid%20and%20Abdelrahman%20Shabayek%20and%20Djamila%20Aouada&entry.1292438233=%20%20Batteries%20are%20essential%20for%20various%20applications%2C%20including%20electric%20vehicles%0Aand%20renewable%20energy%20storage%2C%20making%20safety%20and%20efficiency%20critical%20concerns.%0AAnomaly%20detection%20in%20battery%20thermal%20images%20helps%20identify%20failures%20early%2C%20but%0Atraditional%20deep%20learning%20methods%20require%20extensive%20labeled%20data%2C%20which%20is%0Adifficult%20to%20obtain%2C%20especially%20for%20anomalies%20due%20to%20safety%20risks%20and%20high%20data%0Acollection%20costs.%20To%20overcome%20this%2C%20we%20explore%20zero-shot%20anomaly%20detection%0Ausing%20Visual%20Question%20Answering%20%28VQA%29%20models%2C%20which%20leverage%20pretrained%0Aknowledge%20and%20textbased%20prompts%20to%20generalize%20across%20vision%20tasks.%20By%0Aincorporating%20prior%20knowledge%20of%20normal%20battery%20thermal%20behavior%2C%20we%20design%0Aprompts%20to%20detect%20anomalies%20without%20battery-specific%20training%20data.%20We%20evaluate%0Athree%20VQA%20models%20%28ChatGPT-4o%2C%20LLaVa-13b%2C%20and%20BLIP-2%29%20analyzing%20their%20robustness%0Ato%20prompt%20variations%2C%20repeated%20trials%2C%20and%20qualitative%20outputs.%20Despite%20the%0Alack%20of%20finetuning%20on%20battery%20data%2C%20our%20approach%20demonstrates%20competitive%0Aperformance%20compared%20to%20state-of-the-art%20models%20that%20are%20trained%20with%20the%0Abattery%20data.%20Our%20findings%20highlight%20the%20potential%20of%20VQA-based%20zero-shot%0Alearning%20for%20battery%20anomaly%20detection%20and%20suggest%20future%20directions%20for%0Aimproving%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16674v1&entry.124074799=Read"},
{"title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework\n  for Pedagogical Visualization", "author": "Haonian Ji and Shi Qiu and Siyang Xin and Siwei Han and Zhaorun Chen and Hongyi Wang and Dake Zhang and Huaxiu Yao", "abstract": "  While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.\n", "link": "http://arxiv.org/abs/2505.16832v1", "date": "2025-05-22", "relevancy": 2.3051, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5793}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20EduVisBench%20to%20EduVisAgent%3A%20A%20Benchmark%20and%20Multi-Agent%20Framework%0A%20%20for%20Pedagogical%20Visualization&body=Title%3A%20From%20EduVisBench%20to%20EduVisAgent%3A%20A%20Benchmark%20and%20Multi-Agent%20Framework%0A%20%20for%20Pedagogical%20Visualization%0AAuthor%3A%20Haonian%20Ji%20and%20Shi%20Qiu%20and%20Siyang%20Xin%20and%20Siwei%20Han%20and%20Zhaorun%20Chen%20and%20Hongyi%20Wang%20and%20Dake%20Zhang%20and%20Huaxiu%20Yao%0AAbstract%3A%20%20%20While%20foundation%20models%20%28FMs%29%2C%20such%20as%20diffusion%20models%20and%20large%0Avision-language%20models%20%28LVLMs%29%2C%20have%20been%20widely%20applied%20in%20educational%0Acontexts%2C%20their%20ability%20to%20generate%20pedagogically%20effective%20visual%20explanations%0Aremains%20limited.%20Most%20existing%20approaches%20focus%20primarily%20on%20textual%20reasoning%2C%0Aoverlooking%20the%20critical%20role%20of%20structured%20and%20interpretable%20visualizations%20in%0Asupporting%20conceptual%20understanding.%20To%20better%20assess%20the%20visual%20reasoning%0Acapabilities%20of%20FMs%20in%20educational%20settings%2C%20we%20introduce%20EduVisBench%2C%20a%0Amulti-domain%2C%20multi-level%20benchmark.%20EduVisBench%20features%20diverse%20STEM%20problem%0Asets%20requiring%20visually%20grounded%20solutions%2C%20along%20with%20a%20fine-grained%0Aevaluation%20rubric%20informed%20by%20pedagogical%20theory.%20Our%20empirical%20analysis%0Areveals%20that%20existing%20models%20frequently%20struggle%20with%20the%20inherent%20challenge%20of%0Adecomposing%20complex%20reasoning%20and%20translating%20it%20into%20visual%20representations%0Aaligned%20with%20human%20cognitive%20processes.%20To%20address%20these%20limitations%2C%20we%0Apropose%20EduVisAgent%2C%20a%20multi-agent%20collaborative%20framework%20that%20coordinates%0Aspecialized%20agents%20for%20instructional%20planning%2C%20reasoning%20decomposition%2C%0Ametacognitive%20prompting%2C%20and%20visualization%20design.%20Experimental%20results%20show%0Athat%20EduVisAgent%20substantially%20outperforms%20all%20baselines%2C%20achieving%20a%2040.2%25%0Aimprovement%20and%20delivering%20more%20educationally%20aligned%20visualizations.%0AEduVisBench%20and%20EduVisAgent%20are%20available%20at%0Ahttps%3A//github.com/aiming-lab/EduVisBench%20and%0Ahttps%3A//github.com/aiming-lab/EduVisAgent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520EduVisBench%2520to%2520EduVisAgent%253A%2520A%2520Benchmark%2520and%2520Multi-Agent%2520Framework%250A%2520%2520for%2520Pedagogical%2520Visualization%26entry.906535625%3DHaonian%2520Ji%2520and%2520Shi%2520Qiu%2520and%2520Siyang%2520Xin%2520and%2520Siwei%2520Han%2520and%2520Zhaorun%2520Chen%2520and%2520Hongyi%2520Wang%2520and%2520Dake%2520Zhang%2520and%2520Huaxiu%2520Yao%26entry.1292438233%3D%2520%2520While%2520foundation%2520models%2520%2528FMs%2529%252C%2520such%2520as%2520diffusion%2520models%2520and%2520large%250Avision-language%2520models%2520%2528LVLMs%2529%252C%2520have%2520been%2520widely%2520applied%2520in%2520educational%250Acontexts%252C%2520their%2520ability%2520to%2520generate%2520pedagogically%2520effective%2520visual%2520explanations%250Aremains%2520limited.%2520Most%2520existing%2520approaches%2520focus%2520primarily%2520on%2520textual%2520reasoning%252C%250Aoverlooking%2520the%2520critical%2520role%2520of%2520structured%2520and%2520interpretable%2520visualizations%2520in%250Asupporting%2520conceptual%2520understanding.%2520To%2520better%2520assess%2520the%2520visual%2520reasoning%250Acapabilities%2520of%2520FMs%2520in%2520educational%2520settings%252C%2520we%2520introduce%2520EduVisBench%252C%2520a%250Amulti-domain%252C%2520multi-level%2520benchmark.%2520EduVisBench%2520features%2520diverse%2520STEM%2520problem%250Asets%2520requiring%2520visually%2520grounded%2520solutions%252C%2520along%2520with%2520a%2520fine-grained%250Aevaluation%2520rubric%2520informed%2520by%2520pedagogical%2520theory.%2520Our%2520empirical%2520analysis%250Areveals%2520that%2520existing%2520models%2520frequently%2520struggle%2520with%2520the%2520inherent%2520challenge%2520of%250Adecomposing%2520complex%2520reasoning%2520and%2520translating%2520it%2520into%2520visual%2520representations%250Aaligned%2520with%2520human%2520cognitive%2520processes.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520EduVisAgent%252C%2520a%2520multi-agent%2520collaborative%2520framework%2520that%2520coordinates%250Aspecialized%2520agents%2520for%2520instructional%2520planning%252C%2520reasoning%2520decomposition%252C%250Ametacognitive%2520prompting%252C%2520and%2520visualization%2520design.%2520Experimental%2520results%2520show%250Athat%2520EduVisAgent%2520substantially%2520outperforms%2520all%2520baselines%252C%2520achieving%2520a%252040.2%2525%250Aimprovement%2520and%2520delivering%2520more%2520educationally%2520aligned%2520visualizations.%250AEduVisBench%2520and%2520EduVisAgent%2520are%2520available%2520at%250Ahttps%253A//github.com/aiming-lab/EduVisBench%2520and%250Ahttps%253A//github.com/aiming-lab/EduVisAgent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20EduVisBench%20to%20EduVisAgent%3A%20A%20Benchmark%20and%20Multi-Agent%20Framework%0A%20%20for%20Pedagogical%20Visualization&entry.906535625=Haonian%20Ji%20and%20Shi%20Qiu%20and%20Siyang%20Xin%20and%20Siwei%20Han%20and%20Zhaorun%20Chen%20and%20Hongyi%20Wang%20and%20Dake%20Zhang%20and%20Huaxiu%20Yao&entry.1292438233=%20%20While%20foundation%20models%20%28FMs%29%2C%20such%20as%20diffusion%20models%20and%20large%0Avision-language%20models%20%28LVLMs%29%2C%20have%20been%20widely%20applied%20in%20educational%0Acontexts%2C%20their%20ability%20to%20generate%20pedagogically%20effective%20visual%20explanations%0Aremains%20limited.%20Most%20existing%20approaches%20focus%20primarily%20on%20textual%20reasoning%2C%0Aoverlooking%20the%20critical%20role%20of%20structured%20and%20interpretable%20visualizations%20in%0Asupporting%20conceptual%20understanding.%20To%20better%20assess%20the%20visual%20reasoning%0Acapabilities%20of%20FMs%20in%20educational%20settings%2C%20we%20introduce%20EduVisBench%2C%20a%0Amulti-domain%2C%20multi-level%20benchmark.%20EduVisBench%20features%20diverse%20STEM%20problem%0Asets%20requiring%20visually%20grounded%20solutions%2C%20along%20with%20a%20fine-grained%0Aevaluation%20rubric%20informed%20by%20pedagogical%20theory.%20Our%20empirical%20analysis%0Areveals%20that%20existing%20models%20frequently%20struggle%20with%20the%20inherent%20challenge%20of%0Adecomposing%20complex%20reasoning%20and%20translating%20it%20into%20visual%20representations%0Aaligned%20with%20human%20cognitive%20processes.%20To%20address%20these%20limitations%2C%20we%0Apropose%20EduVisAgent%2C%20a%20multi-agent%20collaborative%20framework%20that%20coordinates%0Aspecialized%20agents%20for%20instructional%20planning%2C%20reasoning%20decomposition%2C%0Ametacognitive%20prompting%2C%20and%20visualization%20design.%20Experimental%20results%20show%0Athat%20EduVisAgent%20substantially%20outperforms%20all%20baselines%2C%20achieving%20a%2040.2%25%0Aimprovement%20and%20delivering%20more%20educationally%20aligned%20visualizations.%0AEduVisBench%20and%20EduVisAgent%20are%20available%20at%0Ahttps%3A//github.com/aiming-lab/EduVisBench%20and%0Ahttps%3A//github.com/aiming-lab/EduVisAgent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16832v1&entry.124074799=Read"},
{"title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "author": "Tim Genewein and Kevin Wenliang Li and Jordi Grau-Moya and Anian Ruoss and Laurent Orseau and Marcus Hutter", "abstract": "  Prompting is one of the main ways to adapt a pretrained model to target\ntasks. Besides manually constructing prompts, many prompt optimization methods\nhave been proposed in the literature. Method development is mainly empirically\ndriven, with less emphasis on a conceptual understanding of prompting. In this\npaper we discuss how optimal prompting can be understood through a Bayesian\nview, which also implies some fundamental limitations of prompting that can\nonly be overcome by tuning weights. The paper explains in detail how\nmeta-trained neural networks behave as Bayesian predictors over the pretraining\ndistribution, whose hallmark feature is rapid in-context adaptation. Optimal\nprompting can be studied formally as conditioning these Bayesian predictors,\nyielding criteria for target tasks where optimal prompting is and is not\npossible. We support the theory with educational experiments on LSTMs and\nTransformers, where we compare different versions of prefix-tuning and\ndifferent weight-tuning methods. We also confirm that soft prefixes, which are\nsequences of real-valued vectors outside the token alphabet, can lead to very\neffective prompts for trained and even untrained networks by manipulating\nactivations in ways that are not achievable by hard tokens. This adds an\nimportant mechanistic aspect beyond the conceptual Bayesian theory.\n", "link": "http://arxiv.org/abs/2505.17010v1", "date": "2025-05-22", "relevancy": 2.3025, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4622}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Prompt%20Tuning%20and%20In-Context%20Learning%20via%20Meta-Learning&body=Title%3A%20Understanding%20Prompt%20Tuning%20and%20In-Context%20Learning%20via%20Meta-Learning%0AAuthor%3A%20Tim%20Genewein%20and%20Kevin%20Wenliang%20Li%20and%20Jordi%20Grau-Moya%20and%20Anian%20Ruoss%20and%20Laurent%20Orseau%20and%20Marcus%20Hutter%0AAbstract%3A%20%20%20Prompting%20is%20one%20of%20the%20main%20ways%20to%20adapt%20a%20pretrained%20model%20to%20target%0Atasks.%20Besides%20manually%20constructing%20prompts%2C%20many%20prompt%20optimization%20methods%0Ahave%20been%20proposed%20in%20the%20literature.%20Method%20development%20is%20mainly%20empirically%0Adriven%2C%20with%20less%20emphasis%20on%20a%20conceptual%20understanding%20of%20prompting.%20In%20this%0Apaper%20we%20discuss%20how%20optimal%20prompting%20can%20be%20understood%20through%20a%20Bayesian%0Aview%2C%20which%20also%20implies%20some%20fundamental%20limitations%20of%20prompting%20that%20can%0Aonly%20be%20overcome%20by%20tuning%20weights.%20The%20paper%20explains%20in%20detail%20how%0Ameta-trained%20neural%20networks%20behave%20as%20Bayesian%20predictors%20over%20the%20pretraining%0Adistribution%2C%20whose%20hallmark%20feature%20is%20rapid%20in-context%20adaptation.%20Optimal%0Aprompting%20can%20be%20studied%20formally%20as%20conditioning%20these%20Bayesian%20predictors%2C%0Ayielding%20criteria%20for%20target%20tasks%20where%20optimal%20prompting%20is%20and%20is%20not%0Apossible.%20We%20support%20the%20theory%20with%20educational%20experiments%20on%20LSTMs%20and%0ATransformers%2C%20where%20we%20compare%20different%20versions%20of%20prefix-tuning%20and%0Adifferent%20weight-tuning%20methods.%20We%20also%20confirm%20that%20soft%20prefixes%2C%20which%20are%0Asequences%20of%20real-valued%20vectors%20outside%20the%20token%20alphabet%2C%20can%20lead%20to%20very%0Aeffective%20prompts%20for%20trained%20and%20even%20untrained%20networks%20by%20manipulating%0Aactivations%20in%20ways%20that%20are%20not%20achievable%20by%20hard%20tokens.%20This%20adds%20an%0Aimportant%20mechanistic%20aspect%20beyond%20the%20conceptual%20Bayesian%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Prompt%2520Tuning%2520and%2520In-Context%2520Learning%2520via%2520Meta-Learning%26entry.906535625%3DTim%2520Genewein%2520and%2520Kevin%2520Wenliang%2520Li%2520and%2520Jordi%2520Grau-Moya%2520and%2520Anian%2520Ruoss%2520and%2520Laurent%2520Orseau%2520and%2520Marcus%2520Hutter%26entry.1292438233%3D%2520%2520Prompting%2520is%2520one%2520of%2520the%2520main%2520ways%2520to%2520adapt%2520a%2520pretrained%2520model%2520to%2520target%250Atasks.%2520Besides%2520manually%2520constructing%2520prompts%252C%2520many%2520prompt%2520optimization%2520methods%250Ahave%2520been%2520proposed%2520in%2520the%2520literature.%2520Method%2520development%2520is%2520mainly%2520empirically%250Adriven%252C%2520with%2520less%2520emphasis%2520on%2520a%2520conceptual%2520understanding%2520of%2520prompting.%2520In%2520this%250Apaper%2520we%2520discuss%2520how%2520optimal%2520prompting%2520can%2520be%2520understood%2520through%2520a%2520Bayesian%250Aview%252C%2520which%2520also%2520implies%2520some%2520fundamental%2520limitations%2520of%2520prompting%2520that%2520can%250Aonly%2520be%2520overcome%2520by%2520tuning%2520weights.%2520The%2520paper%2520explains%2520in%2520detail%2520how%250Ameta-trained%2520neural%2520networks%2520behave%2520as%2520Bayesian%2520predictors%2520over%2520the%2520pretraining%250Adistribution%252C%2520whose%2520hallmark%2520feature%2520is%2520rapid%2520in-context%2520adaptation.%2520Optimal%250Aprompting%2520can%2520be%2520studied%2520formally%2520as%2520conditioning%2520these%2520Bayesian%2520predictors%252C%250Ayielding%2520criteria%2520for%2520target%2520tasks%2520where%2520optimal%2520prompting%2520is%2520and%2520is%2520not%250Apossible.%2520We%2520support%2520the%2520theory%2520with%2520educational%2520experiments%2520on%2520LSTMs%2520and%250ATransformers%252C%2520where%2520we%2520compare%2520different%2520versions%2520of%2520prefix-tuning%2520and%250Adifferent%2520weight-tuning%2520methods.%2520We%2520also%2520confirm%2520that%2520soft%2520prefixes%252C%2520which%2520are%250Asequences%2520of%2520real-valued%2520vectors%2520outside%2520the%2520token%2520alphabet%252C%2520can%2520lead%2520to%2520very%250Aeffective%2520prompts%2520for%2520trained%2520and%2520even%2520untrained%2520networks%2520by%2520manipulating%250Aactivations%2520in%2520ways%2520that%2520are%2520not%2520achievable%2520by%2520hard%2520tokens.%2520This%2520adds%2520an%250Aimportant%2520mechanistic%2520aspect%2520beyond%2520the%2520conceptual%2520Bayesian%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Prompt%20Tuning%20and%20In-Context%20Learning%20via%20Meta-Learning&entry.906535625=Tim%20Genewein%20and%20Kevin%20Wenliang%20Li%20and%20Jordi%20Grau-Moya%20and%20Anian%20Ruoss%20and%20Laurent%20Orseau%20and%20Marcus%20Hutter&entry.1292438233=%20%20Prompting%20is%20one%20of%20the%20main%20ways%20to%20adapt%20a%20pretrained%20model%20to%20target%0Atasks.%20Besides%20manually%20constructing%20prompts%2C%20many%20prompt%20optimization%20methods%0Ahave%20been%20proposed%20in%20the%20literature.%20Method%20development%20is%20mainly%20empirically%0Adriven%2C%20with%20less%20emphasis%20on%20a%20conceptual%20understanding%20of%20prompting.%20In%20this%0Apaper%20we%20discuss%20how%20optimal%20prompting%20can%20be%20understood%20through%20a%20Bayesian%0Aview%2C%20which%20also%20implies%20some%20fundamental%20limitations%20of%20prompting%20that%20can%0Aonly%20be%20overcome%20by%20tuning%20weights.%20The%20paper%20explains%20in%20detail%20how%0Ameta-trained%20neural%20networks%20behave%20as%20Bayesian%20predictors%20over%20the%20pretraining%0Adistribution%2C%20whose%20hallmark%20feature%20is%20rapid%20in-context%20adaptation.%20Optimal%0Aprompting%20can%20be%20studied%20formally%20as%20conditioning%20these%20Bayesian%20predictors%2C%0Ayielding%20criteria%20for%20target%20tasks%20where%20optimal%20prompting%20is%20and%20is%20not%0Apossible.%20We%20support%20the%20theory%20with%20educational%20experiments%20on%20LSTMs%20and%0ATransformers%2C%20where%20we%20compare%20different%20versions%20of%20prefix-tuning%20and%0Adifferent%20weight-tuning%20methods.%20We%20also%20confirm%20that%20soft%20prefixes%2C%20which%20are%0Asequences%20of%20real-valued%20vectors%20outside%20the%20token%20alphabet%2C%20can%20lead%20to%20very%0Aeffective%20prompts%20for%20trained%20and%20even%20untrained%20networks%20by%20manipulating%0Aactivations%20in%20ways%20that%20are%20not%20achievable%20by%20hard%20tokens.%20This%20adds%20an%0Aimportant%20mechanistic%20aspect%20beyond%20the%20conceptual%20Bayesian%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17010v1&entry.124074799=Read"},
{"title": "Persistence-based Hough Transform for Line Detection", "author": "Johannes Ferner and Stefan Huber and Saverio Messineo and Angel Pop and Martin Uray", "abstract": "  The Hough transform is a popular and classical technique in computer vision\nfor the detection of lines (or more general objects). It maps a pixel into a\ndual space -- the Hough space: each pixel is mapped to the set of lines through\nthis pixel, which forms a curve in Hough space. The detection of lines then\nbecomes a voting process to find those lines that received many votes by\npixels. However, this voting is done by thresholding, which is susceptible to\nnoise and other artifacts.\n  In this work, we present an alternative voting technique to detect peaks in\nthe Hough space based on persistent homology, which very naturally addresses\nlimitations of simple thresholding. Experiments on synthetic data show that our\nmethod significantly outperforms the original method, while also demonstrating\nenhanced robustness.\n  This work seeks to inspire future research in two key directions. First, we\nhighlight the untapped potential of Topological Data Analysis techniques and\nadvocate for their broader integration into existing methods, including\nwell-established ones. Secondly, we initiate a discussion on the mathematical\nstability of the Hough transform, encouraging exploration of mathematically\ngrounded improvements to enhance its robustness.\n", "link": "http://arxiv.org/abs/2504.16114v2", "date": "2025-05-22", "relevancy": 2.3013, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4624}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4599}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistence-based%20Hough%20Transform%20for%20Line%20Detection&body=Title%3A%20Persistence-based%20Hough%20Transform%20for%20Line%20Detection%0AAuthor%3A%20Johannes%20Ferner%20and%20Stefan%20Huber%20and%20Saverio%20Messineo%20and%20Angel%20Pop%20and%20Martin%20Uray%0AAbstract%3A%20%20%20The%20Hough%20transform%20is%20a%20popular%20and%20classical%20technique%20in%20computer%20vision%0Afor%20the%20detection%20of%20lines%20%28or%20more%20general%20objects%29.%20It%20maps%20a%20pixel%20into%20a%0Adual%20space%20--%20the%20Hough%20space%3A%20each%20pixel%20is%20mapped%20to%20the%20set%20of%20lines%20through%0Athis%20pixel%2C%20which%20forms%20a%20curve%20in%20Hough%20space.%20The%20detection%20of%20lines%20then%0Abecomes%20a%20voting%20process%20to%20find%20those%20lines%20that%20received%20many%20votes%20by%0Apixels.%20However%2C%20this%20voting%20is%20done%20by%20thresholding%2C%20which%20is%20susceptible%20to%0Anoise%20and%20other%20artifacts.%0A%20%20In%20this%20work%2C%20we%20present%20an%20alternative%20voting%20technique%20to%20detect%20peaks%20in%0Athe%20Hough%20space%20based%20on%20persistent%20homology%2C%20which%20very%20naturally%20addresses%0Alimitations%20of%20simple%20thresholding.%20Experiments%20on%20synthetic%20data%20show%20that%20our%0Amethod%20significantly%20outperforms%20the%20original%20method%2C%20while%20also%20demonstrating%0Aenhanced%20robustness.%0A%20%20This%20work%20seeks%20to%20inspire%20future%20research%20in%20two%20key%20directions.%20First%2C%20we%0Ahighlight%20the%20untapped%20potential%20of%20Topological%20Data%20Analysis%20techniques%20and%0Aadvocate%20for%20their%20broader%20integration%20into%20existing%20methods%2C%20including%0Awell-established%20ones.%20Secondly%2C%20we%20initiate%20a%20discussion%20on%20the%20mathematical%0Astability%20of%20the%20Hough%20transform%2C%20encouraging%20exploration%20of%20mathematically%0Agrounded%20improvements%20to%20enhance%20its%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistence-based%2520Hough%2520Transform%2520for%2520Line%2520Detection%26entry.906535625%3DJohannes%2520Ferner%2520and%2520Stefan%2520Huber%2520and%2520Saverio%2520Messineo%2520and%2520Angel%2520Pop%2520and%2520Martin%2520Uray%26entry.1292438233%3D%2520%2520The%2520Hough%2520transform%2520is%2520a%2520popular%2520and%2520classical%2520technique%2520in%2520computer%2520vision%250Afor%2520the%2520detection%2520of%2520lines%2520%2528or%2520more%2520general%2520objects%2529.%2520It%2520maps%2520a%2520pixel%2520into%2520a%250Adual%2520space%2520--%2520the%2520Hough%2520space%253A%2520each%2520pixel%2520is%2520mapped%2520to%2520the%2520set%2520of%2520lines%2520through%250Athis%2520pixel%252C%2520which%2520forms%2520a%2520curve%2520in%2520Hough%2520space.%2520The%2520detection%2520of%2520lines%2520then%250Abecomes%2520a%2520voting%2520process%2520to%2520find%2520those%2520lines%2520that%2520received%2520many%2520votes%2520by%250Apixels.%2520However%252C%2520this%2520voting%2520is%2520done%2520by%2520thresholding%252C%2520which%2520is%2520susceptible%2520to%250Anoise%2520and%2520other%2520artifacts.%250A%2520%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520alternative%2520voting%2520technique%2520to%2520detect%2520peaks%2520in%250Athe%2520Hough%2520space%2520based%2520on%2520persistent%2520homology%252C%2520which%2520very%2520naturally%2520addresses%250Alimitations%2520of%2520simple%2520thresholding.%2520Experiments%2520on%2520synthetic%2520data%2520show%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520the%2520original%2520method%252C%2520while%2520also%2520demonstrating%250Aenhanced%2520robustness.%250A%2520%2520This%2520work%2520seeks%2520to%2520inspire%2520future%2520research%2520in%2520two%2520key%2520directions.%2520First%252C%2520we%250Ahighlight%2520the%2520untapped%2520potential%2520of%2520Topological%2520Data%2520Analysis%2520techniques%2520and%250Aadvocate%2520for%2520their%2520broader%2520integration%2520into%2520existing%2520methods%252C%2520including%250Awell-established%2520ones.%2520Secondly%252C%2520we%2520initiate%2520a%2520discussion%2520on%2520the%2520mathematical%250Astability%2520of%2520the%2520Hough%2520transform%252C%2520encouraging%2520exploration%2520of%2520mathematically%250Agrounded%2520improvements%2520to%2520enhance%2520its%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistence-based%20Hough%20Transform%20for%20Line%20Detection&entry.906535625=Johannes%20Ferner%20and%20Stefan%20Huber%20and%20Saverio%20Messineo%20and%20Angel%20Pop%20and%20Martin%20Uray&entry.1292438233=%20%20The%20Hough%20transform%20is%20a%20popular%20and%20classical%20technique%20in%20computer%20vision%0Afor%20the%20detection%20of%20lines%20%28or%20more%20general%20objects%29.%20It%20maps%20a%20pixel%20into%20a%0Adual%20space%20--%20the%20Hough%20space%3A%20each%20pixel%20is%20mapped%20to%20the%20set%20of%20lines%20through%0Athis%20pixel%2C%20which%20forms%20a%20curve%20in%20Hough%20space.%20The%20detection%20of%20lines%20then%0Abecomes%20a%20voting%20process%20to%20find%20those%20lines%20that%20received%20many%20votes%20by%0Apixels.%20However%2C%20this%20voting%20is%20done%20by%20thresholding%2C%20which%20is%20susceptible%20to%0Anoise%20and%20other%20artifacts.%0A%20%20In%20this%20work%2C%20we%20present%20an%20alternative%20voting%20technique%20to%20detect%20peaks%20in%0Athe%20Hough%20space%20based%20on%20persistent%20homology%2C%20which%20very%20naturally%20addresses%0Alimitations%20of%20simple%20thresholding.%20Experiments%20on%20synthetic%20data%20show%20that%20our%0Amethod%20significantly%20outperforms%20the%20original%20method%2C%20while%20also%20demonstrating%0Aenhanced%20robustness.%0A%20%20This%20work%20seeks%20to%20inspire%20future%20research%20in%20two%20key%20directions.%20First%2C%20we%0Ahighlight%20the%20untapped%20potential%20of%20Topological%20Data%20Analysis%20techniques%20and%0Aadvocate%20for%20their%20broader%20integration%20into%20existing%20methods%2C%20including%0Awell-established%20ones.%20Secondly%2C%20we%20initiate%20a%20discussion%20on%20the%20mathematical%0Astability%20of%20the%20Hough%20transform%2C%20encouraging%20exploration%20of%20mathematically%0Agrounded%20improvements%20to%20enhance%20its%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16114v2&entry.124074799=Read"},
{"title": "Energy Matching: Unifying Flow Matching and Energy-Based Models for\n  Generative Modeling", "author": "Michal Balcerak and Tamaz Amiranashvili and Antonio Terpin and Suprosanna Shit and Lea Bogensperger and Sebastian Kaltenbach and Petros Koumoutsakos and Bjoern Menze", "abstract": "  The most widely used generative models map noise and data distributions by\nmatching flows or scores. However, they struggle to incorporate partial\nobservations and additional priors--something energy-based models (EBMs) handle\nelegantly by simply adding corresponding scalar energy terms. We address this\nissue by proposing Energy Matching, a framework that endows flow-based\napproaches with the flexibility of EBMs. Far from the data manifold, samples\nmove along curl-free, optimal transport paths from noise to data. As they\napproach the data manifold, an entropic energy term guides the system into a\nBoltzmann equilibrium distribution, explicitly capturing the underlying\nlikelihood structure of the data. We parameterize this dynamic with a single\ntime-independent scalar field, which serves as both a powerful generator and a\nflexible prior for effective regularization of inverse problems. Our method\nsubstantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in\nterms of fidelity, while retaining simulation-free training of transport-based\napproaches away from the data manifold. Furthermore, we leverage the method's\nflexibility to introduce an interaction energy that supports diverse mode\nexploration, which we demonstrate in a controlled protein-generation setting.\nOur approach focuses on learning a scalar potential energy--without\ntime-conditioning, auxiliary generators, or additional networks--which marks a\nsignificant departure from recent EBM methods. We believe that this simplified\nframework significantly advances EBMs capabilities and paves the way for their\nwider adoption in generative modeling across diverse domains.\n", "link": "http://arxiv.org/abs/2504.10612v3", "date": "2025-05-22", "relevancy": 1.6639, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5868}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5675}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy%20Matching%3A%20Unifying%20Flow%20Matching%20and%20Energy-Based%20Models%20for%0A%20%20Generative%20Modeling&body=Title%3A%20Energy%20Matching%3A%20Unifying%20Flow%20Matching%20and%20Energy-Based%20Models%20for%0A%20%20Generative%20Modeling%0AAuthor%3A%20Michal%20Balcerak%20and%20Tamaz%20Amiranashvili%20and%20Antonio%20Terpin%20and%20Suprosanna%20Shit%20and%20Lea%20Bogensperger%20and%20Sebastian%20Kaltenbach%20and%20Petros%20Koumoutsakos%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20The%20most%20widely%20used%20generative%20models%20map%20noise%20and%20data%20distributions%20by%0Amatching%20flows%20or%20scores.%20However%2C%20they%20struggle%20to%20incorporate%20partial%0Aobservations%20and%20additional%20priors--something%20energy-based%20models%20%28EBMs%29%20handle%0Aelegantly%20by%20simply%20adding%20corresponding%20scalar%20energy%20terms.%20We%20address%20this%0Aissue%20by%20proposing%20Energy%20Matching%2C%20a%20framework%20that%20endows%20flow-based%0Aapproaches%20with%20the%20flexibility%20of%20EBMs.%20Far%20from%20the%20data%20manifold%2C%20samples%0Amove%20along%20curl-free%2C%20optimal%20transport%20paths%20from%20noise%20to%20data.%20As%20they%0Aapproach%20the%20data%20manifold%2C%20an%20entropic%20energy%20term%20guides%20the%20system%20into%20a%0ABoltzmann%20equilibrium%20distribution%2C%20explicitly%20capturing%20the%20underlying%0Alikelihood%20structure%20of%20the%20data.%20We%20parameterize%20this%20dynamic%20with%20a%20single%0Atime-independent%20scalar%20field%2C%20which%20serves%20as%20both%20a%20powerful%20generator%20and%20a%0Aflexible%20prior%20for%20effective%20regularization%20of%20inverse%20problems.%20Our%20method%0Asubstantially%20outperforms%20existing%20EBMs%20on%20CIFAR-10%20and%20ImageNet%20generation%20in%0Aterms%20of%20fidelity%2C%20while%20retaining%20simulation-free%20training%20of%20transport-based%0Aapproaches%20away%20from%20the%20data%20manifold.%20Furthermore%2C%20we%20leverage%20the%20method%27s%0Aflexibility%20to%20introduce%20an%20interaction%20energy%20that%20supports%20diverse%20mode%0Aexploration%2C%20which%20we%20demonstrate%20in%20a%20controlled%20protein-generation%20setting.%0AOur%20approach%20focuses%20on%20learning%20a%20scalar%20potential%20energy--without%0Atime-conditioning%2C%20auxiliary%20generators%2C%20or%20additional%20networks--which%20marks%20a%0Asignificant%20departure%20from%20recent%20EBM%20methods.%20We%20believe%20that%20this%20simplified%0Aframework%20significantly%20advances%20EBMs%20capabilities%20and%20paves%20the%20way%20for%20their%0Awider%20adoption%20in%20generative%20modeling%20across%20diverse%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10612v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy%2520Matching%253A%2520Unifying%2520Flow%2520Matching%2520and%2520Energy-Based%2520Models%2520for%250A%2520%2520Generative%2520Modeling%26entry.906535625%3DMichal%2520Balcerak%2520and%2520Tamaz%2520Amiranashvili%2520and%2520Antonio%2520Terpin%2520and%2520Suprosanna%2520Shit%2520and%2520Lea%2520Bogensperger%2520and%2520Sebastian%2520Kaltenbach%2520and%2520Petros%2520Koumoutsakos%2520and%2520Bjoern%2520Menze%26entry.1292438233%3D%2520%2520The%2520most%2520widely%2520used%2520generative%2520models%2520map%2520noise%2520and%2520data%2520distributions%2520by%250Amatching%2520flows%2520or%2520scores.%2520However%252C%2520they%2520struggle%2520to%2520incorporate%2520partial%250Aobservations%2520and%2520additional%2520priors--something%2520energy-based%2520models%2520%2528EBMs%2529%2520handle%250Aelegantly%2520by%2520simply%2520adding%2520corresponding%2520scalar%2520energy%2520terms.%2520We%2520address%2520this%250Aissue%2520by%2520proposing%2520Energy%2520Matching%252C%2520a%2520framework%2520that%2520endows%2520flow-based%250Aapproaches%2520with%2520the%2520flexibility%2520of%2520EBMs.%2520Far%2520from%2520the%2520data%2520manifold%252C%2520samples%250Amove%2520along%2520curl-free%252C%2520optimal%2520transport%2520paths%2520from%2520noise%2520to%2520data.%2520As%2520they%250Aapproach%2520the%2520data%2520manifold%252C%2520an%2520entropic%2520energy%2520term%2520guides%2520the%2520system%2520into%2520a%250ABoltzmann%2520equilibrium%2520distribution%252C%2520explicitly%2520capturing%2520the%2520underlying%250Alikelihood%2520structure%2520of%2520the%2520data.%2520We%2520parameterize%2520this%2520dynamic%2520with%2520a%2520single%250Atime-independent%2520scalar%2520field%252C%2520which%2520serves%2520as%2520both%2520a%2520powerful%2520generator%2520and%2520a%250Aflexible%2520prior%2520for%2520effective%2520regularization%2520of%2520inverse%2520problems.%2520Our%2520method%250Asubstantially%2520outperforms%2520existing%2520EBMs%2520on%2520CIFAR-10%2520and%2520ImageNet%2520generation%2520in%250Aterms%2520of%2520fidelity%252C%2520while%2520retaining%2520simulation-free%2520training%2520of%2520transport-based%250Aapproaches%2520away%2520from%2520the%2520data%2520manifold.%2520Furthermore%252C%2520we%2520leverage%2520the%2520method%2527s%250Aflexibility%2520to%2520introduce%2520an%2520interaction%2520energy%2520that%2520supports%2520diverse%2520mode%250Aexploration%252C%2520which%2520we%2520demonstrate%2520in%2520a%2520controlled%2520protein-generation%2520setting.%250AOur%2520approach%2520focuses%2520on%2520learning%2520a%2520scalar%2520potential%2520energy--without%250Atime-conditioning%252C%2520auxiliary%2520generators%252C%2520or%2520additional%2520networks--which%2520marks%2520a%250Asignificant%2520departure%2520from%2520recent%2520EBM%2520methods.%2520We%2520believe%2520that%2520this%2520simplified%250Aframework%2520significantly%2520advances%2520EBMs%2520capabilities%2520and%2520paves%2520the%2520way%2520for%2520their%250Awider%2520adoption%2520in%2520generative%2520modeling%2520across%2520diverse%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10612v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy%20Matching%3A%20Unifying%20Flow%20Matching%20and%20Energy-Based%20Models%20for%0A%20%20Generative%20Modeling&entry.906535625=Michal%20Balcerak%20and%20Tamaz%20Amiranashvili%20and%20Antonio%20Terpin%20and%20Suprosanna%20Shit%20and%20Lea%20Bogensperger%20and%20Sebastian%20Kaltenbach%20and%20Petros%20Koumoutsakos%20and%20Bjoern%20Menze&entry.1292438233=%20%20The%20most%20widely%20used%20generative%20models%20map%20noise%20and%20data%20distributions%20by%0Amatching%20flows%20or%20scores.%20However%2C%20they%20struggle%20to%20incorporate%20partial%0Aobservations%20and%20additional%20priors--something%20energy-based%20models%20%28EBMs%29%20handle%0Aelegantly%20by%20simply%20adding%20corresponding%20scalar%20energy%20terms.%20We%20address%20this%0Aissue%20by%20proposing%20Energy%20Matching%2C%20a%20framework%20that%20endows%20flow-based%0Aapproaches%20with%20the%20flexibility%20of%20EBMs.%20Far%20from%20the%20data%20manifold%2C%20samples%0Amove%20along%20curl-free%2C%20optimal%20transport%20paths%20from%20noise%20to%20data.%20As%20they%0Aapproach%20the%20data%20manifold%2C%20an%20entropic%20energy%20term%20guides%20the%20system%20into%20a%0ABoltzmann%20equilibrium%20distribution%2C%20explicitly%20capturing%20the%20underlying%0Alikelihood%20structure%20of%20the%20data.%20We%20parameterize%20this%20dynamic%20with%20a%20single%0Atime-independent%20scalar%20field%2C%20which%20serves%20as%20both%20a%20powerful%20generator%20and%20a%0Aflexible%20prior%20for%20effective%20regularization%20of%20inverse%20problems.%20Our%20method%0Asubstantially%20outperforms%20existing%20EBMs%20on%20CIFAR-10%20and%20ImageNet%20generation%20in%0Aterms%20of%20fidelity%2C%20while%20retaining%20simulation-free%20training%20of%20transport-based%0Aapproaches%20away%20from%20the%20data%20manifold.%20Furthermore%2C%20we%20leverage%20the%20method%27s%0Aflexibility%20to%20introduce%20an%20interaction%20energy%20that%20supports%20diverse%20mode%0Aexploration%2C%20which%20we%20demonstrate%20in%20a%20controlled%20protein-generation%20setting.%0AOur%20approach%20focuses%20on%20learning%20a%20scalar%20potential%20energy--without%0Atime-conditioning%2C%20auxiliary%20generators%2C%20or%20additional%20networks--which%20marks%20a%0Asignificant%20departure%20from%20recent%20EBM%20methods.%20We%20believe%20that%20this%20simplified%0Aframework%20significantly%20advances%20EBMs%20capabilities%20and%20paves%20the%20way%20for%20their%0Awider%20adoption%20in%20generative%20modeling%20across%20diverse%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10612v3&entry.124074799=Read"},
{"title": "Forward-only Diffusion Probabilistic Models", "author": "Ziwei Luo and Fredrik K. Gustafsson and Jens Sj\u00f6lund and Thomas B. Sch\u00f6n", "abstract": "  This work presents a forward-only diffusion (FoD) approach for generative\nmodelling. In contrast to traditional diffusion models that rely on a coupled\nforward-backward diffusion scheme, FoD directly learns data generation through\na single forward diffusion process, yielding a simple yet efficient generative\nframework. The core of FoD is a state-dependent linear stochastic differential\nequation that involves a mean-reverting term in both the drift and diffusion\nfunctions. This mean-reversion property guarantees the convergence to clean\ndata, naturally simulating a stochastic interpolation between source and target\ndistributions. More importantly, FoD is analytically tractable and is trained\nusing a simple stochastic flow matching objective, enabling a few-step\nnon-Markov chain sampling during inference. The proposed FoD model, despite its\nsimplicity, achieves competitive performance on various image-conditioned\n(e.g., image restoration) and unconditional generation tasks, demonstrating its\neffectiveness in generative modelling. Our code is available at\nhttps://github.com/Algolzw/FoD.\n", "link": "http://arxiv.org/abs/2505.16733v1", "date": "2025-05-22", "relevancy": 1.7994, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6065}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5995}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forward-only%20Diffusion%20Probabilistic%20Models&body=Title%3A%20Forward-only%20Diffusion%20Probabilistic%20Models%0AAuthor%3A%20Ziwei%20Luo%20and%20Fredrik%20K.%20Gustafsson%20and%20Jens%20Sj%C3%B6lund%20and%20Thomas%20B.%20Sch%C3%B6n%0AAbstract%3A%20%20%20This%20work%20presents%20a%20forward-only%20diffusion%20%28FoD%29%20approach%20for%20generative%0Amodelling.%20In%20contrast%20to%20traditional%20diffusion%20models%20that%20rely%20on%20a%20coupled%0Aforward-backward%20diffusion%20scheme%2C%20FoD%20directly%20learns%20data%20generation%20through%0Aa%20single%20forward%20diffusion%20process%2C%20yielding%20a%20simple%20yet%20efficient%20generative%0Aframework.%20The%20core%20of%20FoD%20is%20a%20state-dependent%20linear%20stochastic%20differential%0Aequation%20that%20involves%20a%20mean-reverting%20term%20in%20both%20the%20drift%20and%20diffusion%0Afunctions.%20This%20mean-reversion%20property%20guarantees%20the%20convergence%20to%20clean%0Adata%2C%20naturally%20simulating%20a%20stochastic%20interpolation%20between%20source%20and%20target%0Adistributions.%20More%20importantly%2C%20FoD%20is%20analytically%20tractable%20and%20is%20trained%0Ausing%20a%20simple%20stochastic%20flow%20matching%20objective%2C%20enabling%20a%20few-step%0Anon-Markov%20chain%20sampling%20during%20inference.%20The%20proposed%20FoD%20model%2C%20despite%20its%0Asimplicity%2C%20achieves%20competitive%20performance%20on%20various%20image-conditioned%0A%28e.g.%2C%20image%20restoration%29%20and%20unconditional%20generation%20tasks%2C%20demonstrating%20its%0Aeffectiveness%20in%20generative%20modelling.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Algolzw/FoD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForward-only%2520Diffusion%2520Probabilistic%2520Models%26entry.906535625%3DZiwei%2520Luo%2520and%2520Fredrik%2520K.%2520Gustafsson%2520and%2520Jens%2520Sj%25C3%25B6lund%2520and%2520Thomas%2520B.%2520Sch%25C3%25B6n%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520forward-only%2520diffusion%2520%2528FoD%2529%2520approach%2520for%2520generative%250Amodelling.%2520In%2520contrast%2520to%2520traditional%2520diffusion%2520models%2520that%2520rely%2520on%2520a%2520coupled%250Aforward-backward%2520diffusion%2520scheme%252C%2520FoD%2520directly%2520learns%2520data%2520generation%2520through%250Aa%2520single%2520forward%2520diffusion%2520process%252C%2520yielding%2520a%2520simple%2520yet%2520efficient%2520generative%250Aframework.%2520The%2520core%2520of%2520FoD%2520is%2520a%2520state-dependent%2520linear%2520stochastic%2520differential%250Aequation%2520that%2520involves%2520a%2520mean-reverting%2520term%2520in%2520both%2520the%2520drift%2520and%2520diffusion%250Afunctions.%2520This%2520mean-reversion%2520property%2520guarantees%2520the%2520convergence%2520to%2520clean%250Adata%252C%2520naturally%2520simulating%2520a%2520stochastic%2520interpolation%2520between%2520source%2520and%2520target%250Adistributions.%2520More%2520importantly%252C%2520FoD%2520is%2520analytically%2520tractable%2520and%2520is%2520trained%250Ausing%2520a%2520simple%2520stochastic%2520flow%2520matching%2520objective%252C%2520enabling%2520a%2520few-step%250Anon-Markov%2520chain%2520sampling%2520during%2520inference.%2520The%2520proposed%2520FoD%2520model%252C%2520despite%2520its%250Asimplicity%252C%2520achieves%2520competitive%2520performance%2520on%2520various%2520image-conditioned%250A%2528e.g.%252C%2520image%2520restoration%2529%2520and%2520unconditional%2520generation%2520tasks%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520generative%2520modelling.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Algolzw/FoD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forward-only%20Diffusion%20Probabilistic%20Models&entry.906535625=Ziwei%20Luo%20and%20Fredrik%20K.%20Gustafsson%20and%20Jens%20Sj%C3%B6lund%20and%20Thomas%20B.%20Sch%C3%B6n&entry.1292438233=%20%20This%20work%20presents%20a%20forward-only%20diffusion%20%28FoD%29%20approach%20for%20generative%0Amodelling.%20In%20contrast%20to%20traditional%20diffusion%20models%20that%20rely%20on%20a%20coupled%0Aforward-backward%20diffusion%20scheme%2C%20FoD%20directly%20learns%20data%20generation%20through%0Aa%20single%20forward%20diffusion%20process%2C%20yielding%20a%20simple%20yet%20efficient%20generative%0Aframework.%20The%20core%20of%20FoD%20is%20a%20state-dependent%20linear%20stochastic%20differential%0Aequation%20that%20involves%20a%20mean-reverting%20term%20in%20both%20the%20drift%20and%20diffusion%0Afunctions.%20This%20mean-reversion%20property%20guarantees%20the%20convergence%20to%20clean%0Adata%2C%20naturally%20simulating%20a%20stochastic%20interpolation%20between%20source%20and%20target%0Adistributions.%20More%20importantly%2C%20FoD%20is%20analytically%20tractable%20and%20is%20trained%0Ausing%20a%20simple%20stochastic%20flow%20matching%20objective%2C%20enabling%20a%20few-step%0Anon-Markov%20chain%20sampling%20during%20inference.%20The%20proposed%20FoD%20model%2C%20despite%20its%0Asimplicity%2C%20achieves%20competitive%20performance%20on%20various%20image-conditioned%0A%28e.g.%2C%20image%20restoration%29%20and%20unconditional%20generation%20tasks%2C%20demonstrating%20its%0Aeffectiveness%20in%20generative%20modelling.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Algolzw/FoD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16733v1&entry.124074799=Read"},
{"title": "TTRL: Test-Time Reinforcement Learning", "author": "Yuxin Zuo and Kaiyan Zhang and Li Sheng and Shang Qu and Ganqu Cui and Xuekai Zhu and Haozhan Li and Yuchen Zhang and Xinwei Long and Ermo Hua and Biqing Qi and Youbang Sun and Zhiyuan Ma and Lifan Yuan and Ning Ding and Bowen Zhou", "abstract": "  This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL\n", "link": "http://arxiv.org/abs/2504.16084v2", "date": "2025-05-22", "relevancy": 1.5125, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5174}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTRL%3A%20Test-Time%20Reinforcement%20Learning&body=Title%3A%20TTRL%3A%20Test-Time%20Reinforcement%20Learning%0AAuthor%3A%20Yuxin%20Zuo%20and%20Kaiyan%20Zhang%20and%20Li%20Sheng%20and%20Shang%20Qu%20and%20Ganqu%20Cui%20and%20Xuekai%20Zhu%20and%20Haozhan%20Li%20and%20Yuchen%20Zhang%20and%20Xinwei%20Long%20and%20Ermo%20Hua%20and%20Biqing%20Qi%20and%20Youbang%20Sun%20and%20Zhiyuan%20Ma%20and%20Lifan%20Yuan%20and%20Ning%20Ding%20and%20Bowen%20Zhou%0AAbstract%3A%20%20%20This%20paper%20investigates%20Reinforcement%20Learning%20%28RL%29%20on%20data%20without%20explicit%0Alabels%20for%20reasoning%20tasks%20in%20Large%20Language%20Models%20%28LLMs%29.%20The%20core%20challenge%0Aof%20the%20problem%20is%20reward%20estimation%20during%20inference%20while%20not%20having%20access%20to%0Aground-truth%20information.%20While%20this%20setting%20appears%20elusive%2C%20we%20find%20that%0Acommon%20practices%20in%20Test-Time%20Scaling%20%28TTS%29%2C%20such%20as%20majority%20voting%2C%20yield%0Asurprisingly%20effective%20rewards%20suitable%20for%20driving%20RL%20training.%20In%20this%20work%2C%0Awe%20introduce%20Test-Time%20Reinforcement%20Learning%20%28TTRL%29%2C%20a%20novel%20method%20for%0Atraining%20LLMs%20using%20RL%20on%20unlabeled%20data.%20TTRL%20enables%20self-evolution%20of%20LLMs%0Aby%20utilizing%20the%20priors%20in%20the%20pre-trained%20models.%20Our%20experiments%20demonstrate%0Athat%20TTRL%20consistently%20improves%20performance%20across%20a%20variety%20of%20tasks%20and%0Amodels.%20Notably%2C%20TTRL%20boosts%20the%20pass%401%20performance%20of%20Qwen-2.5-Math-7B%20by%0Aapproximately%20211%25%20on%20the%20AIME%202024%20with%20only%20unlabeled%20test%20data.%20Furthermore%2C%0Aalthough%20TTRL%20is%20only%20supervised%20by%20the%20maj%40n%20metric%2C%20TTRL%20has%20demonstrated%0Aperformance%20to%20consistently%20surpass%20the%20upper%20limit%20of%20the%20initial%20model%20maj%40n%2C%0Aand%20approach%20the%20performance%20of%20models%20trained%20directly%20on%20test%20data%20with%0Aground-truth%20labels.%20Our%20experimental%20findings%20validate%20the%20general%0Aeffectiveness%20of%20TTRL%20across%20various%20tasks%20and%20highlight%20TTRL%27s%20potential%20for%0Abroader%20tasks%20and%20domains.%20GitHub%3A%20https%3A//github.com/PRIME-RL/TTRL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16084v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTRL%253A%2520Test-Time%2520Reinforcement%2520Learning%26entry.906535625%3DYuxin%2520Zuo%2520and%2520Kaiyan%2520Zhang%2520and%2520Li%2520Sheng%2520and%2520Shang%2520Qu%2520and%2520Ganqu%2520Cui%2520and%2520Xuekai%2520Zhu%2520and%2520Haozhan%2520Li%2520and%2520Yuchen%2520Zhang%2520and%2520Xinwei%2520Long%2520and%2520Ermo%2520Hua%2520and%2520Biqing%2520Qi%2520and%2520Youbang%2520Sun%2520and%2520Zhiyuan%2520Ma%2520and%2520Lifan%2520Yuan%2520and%2520Ning%2520Ding%2520and%2520Bowen%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520Reinforcement%2520Learning%2520%2528RL%2529%2520on%2520data%2520without%2520explicit%250Alabels%2520for%2520reasoning%2520tasks%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520The%2520core%2520challenge%250Aof%2520the%2520problem%2520is%2520reward%2520estimation%2520during%2520inference%2520while%2520not%2520having%2520access%2520to%250Aground-truth%2520information.%2520While%2520this%2520setting%2520appears%2520elusive%252C%2520we%2520find%2520that%250Acommon%2520practices%2520in%2520Test-Time%2520Scaling%2520%2528TTS%2529%252C%2520such%2520as%2520majority%2520voting%252C%2520yield%250Asurprisingly%2520effective%2520rewards%2520suitable%2520for%2520driving%2520RL%2520training.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520Test-Time%2520Reinforcement%2520Learning%2520%2528TTRL%2529%252C%2520a%2520novel%2520method%2520for%250Atraining%2520LLMs%2520using%2520RL%2520on%2520unlabeled%2520data.%2520TTRL%2520enables%2520self-evolution%2520of%2520LLMs%250Aby%2520utilizing%2520the%2520priors%2520in%2520the%2520pre-trained%2520models.%2520Our%2520experiments%2520demonstrate%250Athat%2520TTRL%2520consistently%2520improves%2520performance%2520across%2520a%2520variety%2520of%2520tasks%2520and%250Amodels.%2520Notably%252C%2520TTRL%2520boosts%2520the%2520pass%25401%2520performance%2520of%2520Qwen-2.5-Math-7B%2520by%250Aapproximately%2520211%2525%2520on%2520the%2520AIME%25202024%2520with%2520only%2520unlabeled%2520test%2520data.%2520Furthermore%252C%250Aalthough%2520TTRL%2520is%2520only%2520supervised%2520by%2520the%2520maj%2540n%2520metric%252C%2520TTRL%2520has%2520demonstrated%250Aperformance%2520to%2520consistently%2520surpass%2520the%2520upper%2520limit%2520of%2520the%2520initial%2520model%2520maj%2540n%252C%250Aand%2520approach%2520the%2520performance%2520of%2520models%2520trained%2520directly%2520on%2520test%2520data%2520with%250Aground-truth%2520labels.%2520Our%2520experimental%2520findings%2520validate%2520the%2520general%250Aeffectiveness%2520of%2520TTRL%2520across%2520various%2520tasks%2520and%2520highlight%2520TTRL%2527s%2520potential%2520for%250Abroader%2520tasks%2520and%2520domains.%2520GitHub%253A%2520https%253A//github.com/PRIME-RL/TTRL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16084v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTRL%3A%20Test-Time%20Reinforcement%20Learning&entry.906535625=Yuxin%20Zuo%20and%20Kaiyan%20Zhang%20and%20Li%20Sheng%20and%20Shang%20Qu%20and%20Ganqu%20Cui%20and%20Xuekai%20Zhu%20and%20Haozhan%20Li%20and%20Yuchen%20Zhang%20and%20Xinwei%20Long%20and%20Ermo%20Hua%20and%20Biqing%20Qi%20and%20Youbang%20Sun%20and%20Zhiyuan%20Ma%20and%20Lifan%20Yuan%20and%20Ning%20Ding%20and%20Bowen%20Zhou&entry.1292438233=%20%20This%20paper%20investigates%20Reinforcement%20Learning%20%28RL%29%20on%20data%20without%20explicit%0Alabels%20for%20reasoning%20tasks%20in%20Large%20Language%20Models%20%28LLMs%29.%20The%20core%20challenge%0Aof%20the%20problem%20is%20reward%20estimation%20during%20inference%20while%20not%20having%20access%20to%0Aground-truth%20information.%20While%20this%20setting%20appears%20elusive%2C%20we%20find%20that%0Acommon%20practices%20in%20Test-Time%20Scaling%20%28TTS%29%2C%20such%20as%20majority%20voting%2C%20yield%0Asurprisingly%20effective%20rewards%20suitable%20for%20driving%20RL%20training.%20In%20this%20work%2C%0Awe%20introduce%20Test-Time%20Reinforcement%20Learning%20%28TTRL%29%2C%20a%20novel%20method%20for%0Atraining%20LLMs%20using%20RL%20on%20unlabeled%20data.%20TTRL%20enables%20self-evolution%20of%20LLMs%0Aby%20utilizing%20the%20priors%20in%20the%20pre-trained%20models.%20Our%20experiments%20demonstrate%0Athat%20TTRL%20consistently%20improves%20performance%20across%20a%20variety%20of%20tasks%20and%0Amodels.%20Notably%2C%20TTRL%20boosts%20the%20pass%401%20performance%20of%20Qwen-2.5-Math-7B%20by%0Aapproximately%20211%25%20on%20the%20AIME%202024%20with%20only%20unlabeled%20test%20data.%20Furthermore%2C%0Aalthough%20TTRL%20is%20only%20supervised%20by%20the%20maj%40n%20metric%2C%20TTRL%20has%20demonstrated%0Aperformance%20to%20consistently%20surpass%20the%20upper%20limit%20of%20the%20initial%20model%20maj%40n%2C%0Aand%20approach%20the%20performance%20of%20models%20trained%20directly%20on%20test%20data%20with%0Aground-truth%20labels.%20Our%20experimental%20findings%20validate%20the%20general%0Aeffectiveness%20of%20TTRL%20across%20various%20tasks%20and%20highlight%20TTRL%27s%20potential%20for%0Abroader%20tasks%20and%20domains.%20GitHub%3A%20https%3A//github.com/PRIME-RL/TTRL%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16084v2&entry.124074799=Read"},
{"title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in\n  Text-to-Image Generation", "author": "Hongji Yang and Yucheng Zhou and Wencheng Han and Jianbing Shen", "abstract": "  Text-to-image models are powerful for producing high-quality images based on\ngiven text prompts, but crafting these prompts often requires specialized\nvocabulary. To address this, existing methods train rewriting models with\nsupervision from large amounts of manually annotated data and trained aesthetic\nassessment models. To alleviate the dependence on data scale for model training\nand the biases introduced by trained models, we propose a novel prompt\noptimization framework, designed to rephrase a simple user prompt into a\nsophisticated prompt to a text-to-image model. Specifically, we employ the\nlarge vision language models (LVLMs) as the solver to rewrite the user prompt,\nand concurrently, employ LVLMs as a reward model to score the aesthetics and\nalignment of the images generated by the optimized prompt. Instead of laborious\nhuman feedback, we exploit the prior knowledge of the LVLM to provide rewards,\ni.e., AI feedback. Simultaneously, the solver and the reward model are unified\ninto one model and iterated in reinforcement learning to achieve\nself-improvement by giving a solution and judging itself. Results on two\npopular datasets demonstrate that our method outperforms other strong\ncompetitors.\n", "link": "http://arxiv.org/abs/2505.16763v1", "date": "2025-05-22", "relevancy": 1.6928, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5705}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.565}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Rewarding%20Large%20Vision-Language%20Models%20for%20Optimizing%20Prompts%20in%0A%20%20Text-to-Image%20Generation&body=Title%3A%20Self-Rewarding%20Large%20Vision-Language%20Models%20for%20Optimizing%20Prompts%20in%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Hongji%20Yang%20and%20Yucheng%20Zhou%20and%20Wencheng%20Han%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Text-to-image%20models%20are%20powerful%20for%20producing%20high-quality%20images%20based%20on%0Agiven%20text%20prompts%2C%20but%20crafting%20these%20prompts%20often%20requires%20specialized%0Avocabulary.%20To%20address%20this%2C%20existing%20methods%20train%20rewriting%20models%20with%0Asupervision%20from%20large%20amounts%20of%20manually%20annotated%20data%20and%20trained%20aesthetic%0Aassessment%20models.%20To%20alleviate%20the%20dependence%20on%20data%20scale%20for%20model%20training%0Aand%20the%20biases%20introduced%20by%20trained%20models%2C%20we%20propose%20a%20novel%20prompt%0Aoptimization%20framework%2C%20designed%20to%20rephrase%20a%20simple%20user%20prompt%20into%20a%0Asophisticated%20prompt%20to%20a%20text-to-image%20model.%20Specifically%2C%20we%20employ%20the%0Alarge%20vision%20language%20models%20%28LVLMs%29%20as%20the%20solver%20to%20rewrite%20the%20user%20prompt%2C%0Aand%20concurrently%2C%20employ%20LVLMs%20as%20a%20reward%20model%20to%20score%20the%20aesthetics%20and%0Aalignment%20of%20the%20images%20generated%20by%20the%20optimized%20prompt.%20Instead%20of%20laborious%0Ahuman%20feedback%2C%20we%20exploit%20the%20prior%20knowledge%20of%20the%20LVLM%20to%20provide%20rewards%2C%0Ai.e.%2C%20AI%20feedback.%20Simultaneously%2C%20the%20solver%20and%20the%20reward%20model%20are%20unified%0Ainto%20one%20model%20and%20iterated%20in%20reinforcement%20learning%20to%20achieve%0Aself-improvement%20by%20giving%20a%20solution%20and%20judging%20itself.%20Results%20on%20two%0Apopular%20datasets%20demonstrate%20that%20our%20method%20outperforms%20other%20strong%0Acompetitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Rewarding%2520Large%2520Vision-Language%2520Models%2520for%2520Optimizing%2520Prompts%2520in%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DHongji%2520Yang%2520and%2520Yucheng%2520Zhou%2520and%2520Wencheng%2520Han%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Text-to-image%2520models%2520are%2520powerful%2520for%2520producing%2520high-quality%2520images%2520based%2520on%250Agiven%2520text%2520prompts%252C%2520but%2520crafting%2520these%2520prompts%2520often%2520requires%2520specialized%250Avocabulary.%2520To%2520address%2520this%252C%2520existing%2520methods%2520train%2520rewriting%2520models%2520with%250Asupervision%2520from%2520large%2520amounts%2520of%2520manually%2520annotated%2520data%2520and%2520trained%2520aesthetic%250Aassessment%2520models.%2520To%2520alleviate%2520the%2520dependence%2520on%2520data%2520scale%2520for%2520model%2520training%250Aand%2520the%2520biases%2520introduced%2520by%2520trained%2520models%252C%2520we%2520propose%2520a%2520novel%2520prompt%250Aoptimization%2520framework%252C%2520designed%2520to%2520rephrase%2520a%2520simple%2520user%2520prompt%2520into%2520a%250Asophisticated%2520prompt%2520to%2520a%2520text-to-image%2520model.%2520Specifically%252C%2520we%2520employ%2520the%250Alarge%2520vision%2520language%2520models%2520%2528LVLMs%2529%2520as%2520the%2520solver%2520to%2520rewrite%2520the%2520user%2520prompt%252C%250Aand%2520concurrently%252C%2520employ%2520LVLMs%2520as%2520a%2520reward%2520model%2520to%2520score%2520the%2520aesthetics%2520and%250Aalignment%2520of%2520the%2520images%2520generated%2520by%2520the%2520optimized%2520prompt.%2520Instead%2520of%2520laborious%250Ahuman%2520feedback%252C%2520we%2520exploit%2520the%2520prior%2520knowledge%2520of%2520the%2520LVLM%2520to%2520provide%2520rewards%252C%250Ai.e.%252C%2520AI%2520feedback.%2520Simultaneously%252C%2520the%2520solver%2520and%2520the%2520reward%2520model%2520are%2520unified%250Ainto%2520one%2520model%2520and%2520iterated%2520in%2520reinforcement%2520learning%2520to%2520achieve%250Aself-improvement%2520by%2520giving%2520a%2520solution%2520and%2520judging%2520itself.%2520Results%2520on%2520two%250Apopular%2520datasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520other%2520strong%250Acompetitors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Rewarding%20Large%20Vision-Language%20Models%20for%20Optimizing%20Prompts%20in%0A%20%20Text-to-Image%20Generation&entry.906535625=Hongji%20Yang%20and%20Yucheng%20Zhou%20and%20Wencheng%20Han%20and%20Jianbing%20Shen&entry.1292438233=%20%20Text-to-image%20models%20are%20powerful%20for%20producing%20high-quality%20images%20based%20on%0Agiven%20text%20prompts%2C%20but%20crafting%20these%20prompts%20often%20requires%20specialized%0Avocabulary.%20To%20address%20this%2C%20existing%20methods%20train%20rewriting%20models%20with%0Asupervision%20from%20large%20amounts%20of%20manually%20annotated%20data%20and%20trained%20aesthetic%0Aassessment%20models.%20To%20alleviate%20the%20dependence%20on%20data%20scale%20for%20model%20training%0Aand%20the%20biases%20introduced%20by%20trained%20models%2C%20we%20propose%20a%20novel%20prompt%0Aoptimization%20framework%2C%20designed%20to%20rephrase%20a%20simple%20user%20prompt%20into%20a%0Asophisticated%20prompt%20to%20a%20text-to-image%20model.%20Specifically%2C%20we%20employ%20the%0Alarge%20vision%20language%20models%20%28LVLMs%29%20as%20the%20solver%20to%20rewrite%20the%20user%20prompt%2C%0Aand%20concurrently%2C%20employ%20LVLMs%20as%20a%20reward%20model%20to%20score%20the%20aesthetics%20and%0Aalignment%20of%20the%20images%20generated%20by%20the%20optimized%20prompt.%20Instead%20of%20laborious%0Ahuman%20feedback%2C%20we%20exploit%20the%20prior%20knowledge%20of%20the%20LVLM%20to%20provide%20rewards%2C%0Ai.e.%2C%20AI%20feedback.%20Simultaneously%2C%20the%20solver%20and%20the%20reward%20model%20are%20unified%0Ainto%20one%20model%20and%20iterated%20in%20reinforcement%20learning%20to%20achieve%0Aself-improvement%20by%20giving%20a%20solution%20and%20judging%20itself.%20Results%20on%20two%0Apopular%20datasets%20demonstrate%20that%20our%20method%20outperforms%20other%20strong%0Acompetitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16763v1&entry.124074799=Read"},
{"title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model", "author": "Konstantinos Barmpas and Na Lee and Yannis Panagakis and Dimitrios A. Adamos and Nikolaos Laskaris and Stefanos Zafeiriou", "abstract": "  Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs.\n", "link": "http://arxiv.org/abs/2505.16724v1", "date": "2025-05-22", "relevancy": 2.1281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5437}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Brainwave%20Modeling%20with%20a%20Codebook-Based%20Foundation%20Model&body=Title%3A%20Advancing%20Brainwave%20Modeling%20with%20a%20Codebook-Based%20Foundation%20Model%0AAuthor%3A%20Konstantinos%20Barmpas%20and%20Na%20Lee%20and%20Yannis%20Panagakis%20and%20Dimitrios%20A.%20Adamos%20and%20Nikolaos%20Laskaris%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20%20%20Recent%20advances%20in%20large-scale%20pre-trained%20Electroencephalogram%20%28EEG%29%20models%0Ahave%20shown%20great%20promise%2C%20driving%20progress%20in%20Brain-Computer%20Interfaces%20%28BCIs%29%0Aand%20healthcare%20applications.%20However%2C%20despite%20their%20success%2C%20many%20existing%0Apre-trained%20models%20have%20struggled%20to%20fully%20capture%20the%20rich%20information%20content%0Aof%20neural%20oscillations%2C%20a%20limitation%20that%20fundamentally%20constrains%20their%0Aperformance%20and%20generalizability%20across%20diverse%20BCI%20tasks.%20This%20limitation%20is%0Afrequently%20rooted%20in%20suboptimal%20architectural%20design%20choices%20which%20constrain%0Atheir%20representational%20capacity.%20In%20this%20work%2C%20we%20introduce%20LaBraM%2B%2B%2C%20an%0Aenhanced%20Large%20Brainwave%20Foundation%20Model%20%28LBM%29%20that%20incorporates%20principled%0Aimprovements%20grounded%20in%20robust%20signal%20processing%20foundations.%20LaBraM%2B%2B%0Ademonstrates%20substantial%20gains%20across%20a%20variety%20of%20tasks%2C%20consistently%0Aoutperforming%20its%20originally-based%20architecture%20and%20achieving%20competitive%0Aresults%20when%20compared%20to%20other%20open-source%20LBMs.%20Its%20superior%20performance%20and%0Atraining%20efficiency%20highlight%20its%20potential%20as%20a%20strong%20foundation%20for%20future%0Aadvancements%20in%20LBMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Brainwave%2520Modeling%2520with%2520a%2520Codebook-Based%2520Foundation%2520Model%26entry.906535625%3DKonstantinos%2520Barmpas%2520and%2520Na%2520Lee%2520and%2520Yannis%2520Panagakis%2520and%2520Dimitrios%2520A.%2520Adamos%2520and%2520Nikolaos%2520Laskaris%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large-scale%2520pre-trained%2520Electroencephalogram%2520%2528EEG%2529%2520models%250Ahave%2520shown%2520great%2520promise%252C%2520driving%2520progress%2520in%2520Brain-Computer%2520Interfaces%2520%2528BCIs%2529%250Aand%2520healthcare%2520applications.%2520However%252C%2520despite%2520their%2520success%252C%2520many%2520existing%250Apre-trained%2520models%2520have%2520struggled%2520to%2520fully%2520capture%2520the%2520rich%2520information%2520content%250Aof%2520neural%2520oscillations%252C%2520a%2520limitation%2520that%2520fundamentally%2520constrains%2520their%250Aperformance%2520and%2520generalizability%2520across%2520diverse%2520BCI%2520tasks.%2520This%2520limitation%2520is%250Afrequently%2520rooted%2520in%2520suboptimal%2520architectural%2520design%2520choices%2520which%2520constrain%250Atheir%2520representational%2520capacity.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LaBraM%252B%252B%252C%2520an%250Aenhanced%2520Large%2520Brainwave%2520Foundation%2520Model%2520%2528LBM%2529%2520that%2520incorporates%2520principled%250Aimprovements%2520grounded%2520in%2520robust%2520signal%2520processing%2520foundations.%2520LaBraM%252B%252B%250Ademonstrates%2520substantial%2520gains%2520across%2520a%2520variety%2520of%2520tasks%252C%2520consistently%250Aoutperforming%2520its%2520originally-based%2520architecture%2520and%2520achieving%2520competitive%250Aresults%2520when%2520compared%2520to%2520other%2520open-source%2520LBMs.%2520Its%2520superior%2520performance%2520and%250Atraining%2520efficiency%2520highlight%2520its%2520potential%2520as%2520a%2520strong%2520foundation%2520for%2520future%250Aadvancements%2520in%2520LBMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Brainwave%20Modeling%20with%20a%20Codebook-Based%20Foundation%20Model&entry.906535625=Konstantinos%20Barmpas%20and%20Na%20Lee%20and%20Yannis%20Panagakis%20and%20Dimitrios%20A.%20Adamos%20and%20Nikolaos%20Laskaris%20and%20Stefanos%20Zafeiriou&entry.1292438233=%20%20Recent%20advances%20in%20large-scale%20pre-trained%20Electroencephalogram%20%28EEG%29%20models%0Ahave%20shown%20great%20promise%2C%20driving%20progress%20in%20Brain-Computer%20Interfaces%20%28BCIs%29%0Aand%20healthcare%20applications.%20However%2C%20despite%20their%20success%2C%20many%20existing%0Apre-trained%20models%20have%20struggled%20to%20fully%20capture%20the%20rich%20information%20content%0Aof%20neural%20oscillations%2C%20a%20limitation%20that%20fundamentally%20constrains%20their%0Aperformance%20and%20generalizability%20across%20diverse%20BCI%20tasks.%20This%20limitation%20is%0Afrequently%20rooted%20in%20suboptimal%20architectural%20design%20choices%20which%20constrain%0Atheir%20representational%20capacity.%20In%20this%20work%2C%20we%20introduce%20LaBraM%2B%2B%2C%20an%0Aenhanced%20Large%20Brainwave%20Foundation%20Model%20%28LBM%29%20that%20incorporates%20principled%0Aimprovements%20grounded%20in%20robust%20signal%20processing%20foundations.%20LaBraM%2B%2B%0Ademonstrates%20substantial%20gains%20across%20a%20variety%20of%20tasks%2C%20consistently%0Aoutperforming%20its%20originally-based%20architecture%20and%20achieving%20competitive%0Aresults%20when%20compared%20to%20other%20open-source%20LBMs.%20Its%20superior%20performance%20and%0Atraining%20efficiency%20highlight%20its%20potential%20as%20a%20strong%20foundation%20for%20future%0Aadvancements%20in%20LBMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16724v1&entry.124074799=Read"},
{"title": "Large Language Model-Empowered Interactive Load Forecasting", "author": "Yu Zuo and Dalin Qin and Yi Wang", "abstract": "  The growing complexity of power systems has made accurate load forecasting\nmore important than ever. An increasing number of advanced load forecasting\nmethods have been developed. However, the static design of current methods\noffers no mechanism for human-model interaction. As the primary users of\nforecasting models, system operators often find it difficult to understand and\napply these advanced models, which typically requires expertise in artificial\nintelligence (AI). This also prevents them from incorporating their experience\nand real-world contextual understanding into the forecasting process. Recent\nbreakthroughs in large language models (LLMs) offer a new opportunity to\naddress this issue. By leveraging their natural language understanding and\nreasoning capabilities, we propose an LLM-based multi-agent collaboration\nframework to bridge the gap between human operators and forecasting models. A\nset of specialized agents is designed to perform different tasks in the\nforecasting workflow and collaborate via a dedicated communication mechanism.\nThis framework embeds interactive mechanisms throughout the load forecasting\npipeline, reducing the technical threshold for non-expert users and enabling\nthe integration of human experience. Our experiments demonstrate that the\ninteractive load forecasting accuracy can be significantly improved when users\nprovide proper insight in key stages. Our cost analysis shows that the\nframework remains affordable, making it practical for real-world deployment.\n", "link": "http://arxiv.org/abs/2505.16577v1", "date": "2025-05-22", "relevancy": 1.4093, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4758}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model-Empowered%20Interactive%20Load%20Forecasting&body=Title%3A%20Large%20Language%20Model-Empowered%20Interactive%20Load%20Forecasting%0AAuthor%3A%20Yu%20Zuo%20and%20Dalin%20Qin%20and%20Yi%20Wang%0AAbstract%3A%20%20%20The%20growing%20complexity%20of%20power%20systems%20has%20made%20accurate%20load%20forecasting%0Amore%20important%20than%20ever.%20An%20increasing%20number%20of%20advanced%20load%20forecasting%0Amethods%20have%20been%20developed.%20However%2C%20the%20static%20design%20of%20current%20methods%0Aoffers%20no%20mechanism%20for%20human-model%20interaction.%20As%20the%20primary%20users%20of%0Aforecasting%20models%2C%20system%20operators%20often%20find%20it%20difficult%20to%20understand%20and%0Aapply%20these%20advanced%20models%2C%20which%20typically%20requires%20expertise%20in%20artificial%0Aintelligence%20%28AI%29.%20This%20also%20prevents%20them%20from%20incorporating%20their%20experience%0Aand%20real-world%20contextual%20understanding%20into%20the%20forecasting%20process.%20Recent%0Abreakthroughs%20in%20large%20language%20models%20%28LLMs%29%20offer%20a%20new%20opportunity%20to%0Aaddress%20this%20issue.%20By%20leveraging%20their%20natural%20language%20understanding%20and%0Areasoning%20capabilities%2C%20we%20propose%20an%20LLM-based%20multi-agent%20collaboration%0Aframework%20to%20bridge%20the%20gap%20between%20human%20operators%20and%20forecasting%20models.%20A%0Aset%20of%20specialized%20agents%20is%20designed%20to%20perform%20different%20tasks%20in%20the%0Aforecasting%20workflow%20and%20collaborate%20via%20a%20dedicated%20communication%20mechanism.%0AThis%20framework%20embeds%20interactive%20mechanisms%20throughout%20the%20load%20forecasting%0Apipeline%2C%20reducing%20the%20technical%20threshold%20for%20non-expert%20users%20and%20enabling%0Athe%20integration%20of%20human%20experience.%20Our%20experiments%20demonstrate%20that%20the%0Ainteractive%20load%20forecasting%20accuracy%20can%20be%20significantly%20improved%20when%20users%0Aprovide%20proper%20insight%20in%20key%20stages.%20Our%20cost%20analysis%20shows%20that%20the%0Aframework%20remains%20affordable%2C%20making%20it%20practical%20for%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model-Empowered%2520Interactive%2520Load%2520Forecasting%26entry.906535625%3DYu%2520Zuo%2520and%2520Dalin%2520Qin%2520and%2520Yi%2520Wang%26entry.1292438233%3D%2520%2520The%2520growing%2520complexity%2520of%2520power%2520systems%2520has%2520made%2520accurate%2520load%2520forecasting%250Amore%2520important%2520than%2520ever.%2520An%2520increasing%2520number%2520of%2520advanced%2520load%2520forecasting%250Amethods%2520have%2520been%2520developed.%2520However%252C%2520the%2520static%2520design%2520of%2520current%2520methods%250Aoffers%2520no%2520mechanism%2520for%2520human-model%2520interaction.%2520As%2520the%2520primary%2520users%2520of%250Aforecasting%2520models%252C%2520system%2520operators%2520often%2520find%2520it%2520difficult%2520to%2520understand%2520and%250Aapply%2520these%2520advanced%2520models%252C%2520which%2520typically%2520requires%2520expertise%2520in%2520artificial%250Aintelligence%2520%2528AI%2529.%2520This%2520also%2520prevents%2520them%2520from%2520incorporating%2520their%2520experience%250Aand%2520real-world%2520contextual%2520understanding%2520into%2520the%2520forecasting%2520process.%2520Recent%250Abreakthroughs%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520a%2520new%2520opportunity%2520to%250Aaddress%2520this%2520issue.%2520By%2520leveraging%2520their%2520natural%2520language%2520understanding%2520and%250Areasoning%2520capabilities%252C%2520we%2520propose%2520an%2520LLM-based%2520multi-agent%2520collaboration%250Aframework%2520to%2520bridge%2520the%2520gap%2520between%2520human%2520operators%2520and%2520forecasting%2520models.%2520A%250Aset%2520of%2520specialized%2520agents%2520is%2520designed%2520to%2520perform%2520different%2520tasks%2520in%2520the%250Aforecasting%2520workflow%2520and%2520collaborate%2520via%2520a%2520dedicated%2520communication%2520mechanism.%250AThis%2520framework%2520embeds%2520interactive%2520mechanisms%2520throughout%2520the%2520load%2520forecasting%250Apipeline%252C%2520reducing%2520the%2520technical%2520threshold%2520for%2520non-expert%2520users%2520and%2520enabling%250Athe%2520integration%2520of%2520human%2520experience.%2520Our%2520experiments%2520demonstrate%2520that%2520the%250Ainteractive%2520load%2520forecasting%2520accuracy%2520can%2520be%2520significantly%2520improved%2520when%2520users%250Aprovide%2520proper%2520insight%2520in%2520key%2520stages.%2520Our%2520cost%2520analysis%2520shows%2520that%2520the%250Aframework%2520remains%2520affordable%252C%2520making%2520it%2520practical%2520for%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model-Empowered%20Interactive%20Load%20Forecasting&entry.906535625=Yu%20Zuo%20and%20Dalin%20Qin%20and%20Yi%20Wang&entry.1292438233=%20%20The%20growing%20complexity%20of%20power%20systems%20has%20made%20accurate%20load%20forecasting%0Amore%20important%20than%20ever.%20An%20increasing%20number%20of%20advanced%20load%20forecasting%0Amethods%20have%20been%20developed.%20However%2C%20the%20static%20design%20of%20current%20methods%0Aoffers%20no%20mechanism%20for%20human-model%20interaction.%20As%20the%20primary%20users%20of%0Aforecasting%20models%2C%20system%20operators%20often%20find%20it%20difficult%20to%20understand%20and%0Aapply%20these%20advanced%20models%2C%20which%20typically%20requires%20expertise%20in%20artificial%0Aintelligence%20%28AI%29.%20This%20also%20prevents%20them%20from%20incorporating%20their%20experience%0Aand%20real-world%20contextual%20understanding%20into%20the%20forecasting%20process.%20Recent%0Abreakthroughs%20in%20large%20language%20models%20%28LLMs%29%20offer%20a%20new%20opportunity%20to%0Aaddress%20this%20issue.%20By%20leveraging%20their%20natural%20language%20understanding%20and%0Areasoning%20capabilities%2C%20we%20propose%20an%20LLM-based%20multi-agent%20collaboration%0Aframework%20to%20bridge%20the%20gap%20between%20human%20operators%20and%20forecasting%20models.%20A%0Aset%20of%20specialized%20agents%20is%20designed%20to%20perform%20different%20tasks%20in%20the%0Aforecasting%20workflow%20and%20collaborate%20via%20a%20dedicated%20communication%20mechanism.%0AThis%20framework%20embeds%20interactive%20mechanisms%20throughout%20the%20load%20forecasting%0Apipeline%2C%20reducing%20the%20technical%20threshold%20for%20non-expert%20users%20and%20enabling%0Athe%20integration%20of%20human%20experience.%20Our%20experiments%20demonstrate%20that%20the%0Ainteractive%20load%20forecasting%20accuracy%20can%20be%20significantly%20improved%20when%20users%0Aprovide%20proper%20insight%20in%20key%20stages.%20Our%20cost%20analysis%20shows%20that%20the%0Aframework%20remains%20affordable%2C%20making%20it%20practical%20for%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16577v1&entry.124074799=Read"},
{"title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss", "author": "Igor Udovichenko and Olivier Croissant and Anita Toleutaeva and Evgeny Burnaev and Alexander Korotin", "abstract": "  Risk-averse reinforcement learning finds application in various high-stakes\nfields. Unlike classical reinforcement learning, which aims to maximize\nexpected returns, risk-averse agents choose policies that minimize risk,\noccasionally sacrificing expected value. These preferences can be framed\nthrough utility theory. We focus on the specific case of the exponential\nutility function, where we can derive the Bellman equations and employ various\nreinforcement learning algorithms with few modifications. However, these\nmethods suffer from numerical instability due to the need for exponent\ncomputation throughout the process. To address this, we introduce a numerically\nstable and mathematically sound loss function based on the Itakura-Saito\ndivergence for learning state-value and action-value functions. We evaluate our\nproposed loss function against established alternatives, both theoretically and\nempirically. In the experimental section, we explore multiple financial\nscenarios, some with known analytical solutions, and show that our loss\nfunction outperforms the alternatives.\n", "link": "http://arxiv.org/abs/2505.16925v1", "date": "2025-05-22", "relevancy": 1.3574, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4909}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4433}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk-Averse%20Reinforcement%20Learning%20with%20Itakura-Saito%20Loss&body=Title%3A%20Risk-Averse%20Reinforcement%20Learning%20with%20Itakura-Saito%20Loss%0AAuthor%3A%20Igor%20Udovichenko%20and%20Olivier%20Croissant%20and%20Anita%20Toleutaeva%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin%0AAbstract%3A%20%20%20Risk-averse%20reinforcement%20learning%20finds%20application%20in%20various%20high-stakes%0Afields.%20Unlike%20classical%20reinforcement%20learning%2C%20which%20aims%20to%20maximize%0Aexpected%20returns%2C%20risk-averse%20agents%20choose%20policies%20that%20minimize%20risk%2C%0Aoccasionally%20sacrificing%20expected%20value.%20These%20preferences%20can%20be%20framed%0Athrough%20utility%20theory.%20We%20focus%20on%20the%20specific%20case%20of%20the%20exponential%0Autility%20function%2C%20where%20we%20can%20derive%20the%20Bellman%20equations%20and%20employ%20various%0Areinforcement%20learning%20algorithms%20with%20few%20modifications.%20However%2C%20these%0Amethods%20suffer%20from%20numerical%20instability%20due%20to%20the%20need%20for%20exponent%0Acomputation%20throughout%20the%20process.%20To%20address%20this%2C%20we%20introduce%20a%20numerically%0Astable%20and%20mathematically%20sound%20loss%20function%20based%20on%20the%20Itakura-Saito%0Adivergence%20for%20learning%20state-value%20and%20action-value%20functions.%20We%20evaluate%20our%0Aproposed%20loss%20function%20against%20established%20alternatives%2C%20both%20theoretically%20and%0Aempirically.%20In%20the%20experimental%20section%2C%20we%20explore%20multiple%20financial%0Ascenarios%2C%20some%20with%20known%20analytical%20solutions%2C%20and%20show%20that%20our%20loss%0Afunction%20outperforms%20the%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk-Averse%2520Reinforcement%2520Learning%2520with%2520Itakura-Saito%2520Loss%26entry.906535625%3DIgor%2520Udovichenko%2520and%2520Olivier%2520Croissant%2520and%2520Anita%2520Toleutaeva%2520and%2520Evgeny%2520Burnaev%2520and%2520Alexander%2520Korotin%26entry.1292438233%3D%2520%2520Risk-averse%2520reinforcement%2520learning%2520finds%2520application%2520in%2520various%2520high-stakes%250Afields.%2520Unlike%2520classical%2520reinforcement%2520learning%252C%2520which%2520aims%2520to%2520maximize%250Aexpected%2520returns%252C%2520risk-averse%2520agents%2520choose%2520policies%2520that%2520minimize%2520risk%252C%250Aoccasionally%2520sacrificing%2520expected%2520value.%2520These%2520preferences%2520can%2520be%2520framed%250Athrough%2520utility%2520theory.%2520We%2520focus%2520on%2520the%2520specific%2520case%2520of%2520the%2520exponential%250Autility%2520function%252C%2520where%2520we%2520can%2520derive%2520the%2520Bellman%2520equations%2520and%2520employ%2520various%250Areinforcement%2520learning%2520algorithms%2520with%2520few%2520modifications.%2520However%252C%2520these%250Amethods%2520suffer%2520from%2520numerical%2520instability%2520due%2520to%2520the%2520need%2520for%2520exponent%250Acomputation%2520throughout%2520the%2520process.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520numerically%250Astable%2520and%2520mathematically%2520sound%2520loss%2520function%2520based%2520on%2520the%2520Itakura-Saito%250Adivergence%2520for%2520learning%2520state-value%2520and%2520action-value%2520functions.%2520We%2520evaluate%2520our%250Aproposed%2520loss%2520function%2520against%2520established%2520alternatives%252C%2520both%2520theoretically%2520and%250Aempirically.%2520In%2520the%2520experimental%2520section%252C%2520we%2520explore%2520multiple%2520financial%250Ascenarios%252C%2520some%2520with%2520known%2520analytical%2520solutions%252C%2520and%2520show%2520that%2520our%2520loss%250Afunction%2520outperforms%2520the%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk-Averse%20Reinforcement%20Learning%20with%20Itakura-Saito%20Loss&entry.906535625=Igor%20Udovichenko%20and%20Olivier%20Croissant%20and%20Anita%20Toleutaeva%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin&entry.1292438233=%20%20Risk-averse%20reinforcement%20learning%20finds%20application%20in%20various%20high-stakes%0Afields.%20Unlike%20classical%20reinforcement%20learning%2C%20which%20aims%20to%20maximize%0Aexpected%20returns%2C%20risk-averse%20agents%20choose%20policies%20that%20minimize%20risk%2C%0Aoccasionally%20sacrificing%20expected%20value.%20These%20preferences%20can%20be%20framed%0Athrough%20utility%20theory.%20We%20focus%20on%20the%20specific%20case%20of%20the%20exponential%0Autility%20function%2C%20where%20we%20can%20derive%20the%20Bellman%20equations%20and%20employ%20various%0Areinforcement%20learning%20algorithms%20with%20few%20modifications.%20However%2C%20these%0Amethods%20suffer%20from%20numerical%20instability%20due%20to%20the%20need%20for%20exponent%0Acomputation%20throughout%20the%20process.%20To%20address%20this%2C%20we%20introduce%20a%20numerically%0Astable%20and%20mathematically%20sound%20loss%20function%20based%20on%20the%20Itakura-Saito%0Adivergence%20for%20learning%20state-value%20and%20action-value%20functions.%20We%20evaluate%20our%0Aproposed%20loss%20function%20against%20established%20alternatives%2C%20both%20theoretically%20and%0Aempirically.%20In%20the%20experimental%20section%2C%20we%20explore%20multiple%20financial%0Ascenarios%2C%20some%20with%20known%20analytical%20solutions%2C%20and%20show%20that%20our%20loss%0Afunction%20outperforms%20the%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16925v1&entry.124074799=Read"},
{"title": "Strategically Linked Decisions in Long-Term Planning and Reinforcement\n  Learning", "author": "Alihan H\u00fcy\u00fck and Finale Doshi-Velez", "abstract": "  Long-term planning, as in reinforcement learning (RL), involves finding\nstrategies: actions that collectively work toward a goal rather than\nindividually optimizing their immediate outcomes. As part of a strategy, some\nactions are taken at the expense of short-term benefit to enable future actions\nwith even greater returns. These actions are only advantageous if followed up\nby the actions they facilitate, consequently, they would not have been taken if\nthose follow-ups were not available. In this paper, we quantify such\ndependencies between planned actions with strategic link scores: the drop in\nthe likelihood of one decision under the constraint that a follow-up decision\nis no longer available. We demonstrate the utility of strategic link scores\nthrough three practical applications: (i) explaining black-box RL agents by\nidentifying strategically linked pairs among decisions they make, (ii)\nimproving the worst-case performance of decision support systems by\ndistinguishing whether recommended actions can be adopted as standalone\nimprovements or whether they are strategically linked hence requiring a\ncommitment to a broader strategy to be effective, and (iii) characterizing the\nplanning processes of non-RL agents purely through interventions aimed at\nmeasuring strategic link scores - as an example, we consider a realistic\ntraffic simulator and analyze through road closures the effective planning\nhorizon of the emergent routing behavior of many drivers.\n", "link": "http://arxiv.org/abs/2505.16833v1", "date": "2025-05-22", "relevancy": 1.3808, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4798}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strategically%20Linked%20Decisions%20in%20Long-Term%20Planning%20and%20Reinforcement%0A%20%20Learning&body=Title%3A%20Strategically%20Linked%20Decisions%20in%20Long-Term%20Planning%20and%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Alihan%20H%C3%BCy%C3%BCk%20and%20Finale%20Doshi-Velez%0AAbstract%3A%20%20%20Long-term%20planning%2C%20as%20in%20reinforcement%20learning%20%28RL%29%2C%20involves%20finding%0Astrategies%3A%20actions%20that%20collectively%20work%20toward%20a%20goal%20rather%20than%0Aindividually%20optimizing%20their%20immediate%20outcomes.%20As%20part%20of%20a%20strategy%2C%20some%0Aactions%20are%20taken%20at%20the%20expense%20of%20short-term%20benefit%20to%20enable%20future%20actions%0Awith%20even%20greater%20returns.%20These%20actions%20are%20only%20advantageous%20if%20followed%20up%0Aby%20the%20actions%20they%20facilitate%2C%20consequently%2C%20they%20would%20not%20have%20been%20taken%20if%0Athose%20follow-ups%20were%20not%20available.%20In%20this%20paper%2C%20we%20quantify%20such%0Adependencies%20between%20planned%20actions%20with%20strategic%20link%20scores%3A%20the%20drop%20in%0Athe%20likelihood%20of%20one%20decision%20under%20the%20constraint%20that%20a%20follow-up%20decision%0Ais%20no%20longer%20available.%20We%20demonstrate%20the%20utility%20of%20strategic%20link%20scores%0Athrough%20three%20practical%20applications%3A%20%28i%29%20explaining%20black-box%20RL%20agents%20by%0Aidentifying%20strategically%20linked%20pairs%20among%20decisions%20they%20make%2C%20%28ii%29%0Aimproving%20the%20worst-case%20performance%20of%20decision%20support%20systems%20by%0Adistinguishing%20whether%20recommended%20actions%20can%20be%20adopted%20as%20standalone%0Aimprovements%20or%20whether%20they%20are%20strategically%20linked%20hence%20requiring%20a%0Acommitment%20to%20a%20broader%20strategy%20to%20be%20effective%2C%20and%20%28iii%29%20characterizing%20the%0Aplanning%20processes%20of%20non-RL%20agents%20purely%20through%20interventions%20aimed%20at%0Ameasuring%20strategic%20link%20scores%20-%20as%20an%20example%2C%20we%20consider%20a%20realistic%0Atraffic%20simulator%20and%20analyze%20through%20road%20closures%20the%20effective%20planning%0Ahorizon%20of%20the%20emergent%20routing%20behavior%20of%20many%20drivers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrategically%2520Linked%2520Decisions%2520in%2520Long-Term%2520Planning%2520and%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DAlihan%2520H%25C3%25BCy%25C3%25BCk%2520and%2520Finale%2520Doshi-Velez%26entry.1292438233%3D%2520%2520Long-term%2520planning%252C%2520as%2520in%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520involves%2520finding%250Astrategies%253A%2520actions%2520that%2520collectively%2520work%2520toward%2520a%2520goal%2520rather%2520than%250Aindividually%2520optimizing%2520their%2520immediate%2520outcomes.%2520As%2520part%2520of%2520a%2520strategy%252C%2520some%250Aactions%2520are%2520taken%2520at%2520the%2520expense%2520of%2520short-term%2520benefit%2520to%2520enable%2520future%2520actions%250Awith%2520even%2520greater%2520returns.%2520These%2520actions%2520are%2520only%2520advantageous%2520if%2520followed%2520up%250Aby%2520the%2520actions%2520they%2520facilitate%252C%2520consequently%252C%2520they%2520would%2520not%2520have%2520been%2520taken%2520if%250Athose%2520follow-ups%2520were%2520not%2520available.%2520In%2520this%2520paper%252C%2520we%2520quantify%2520such%250Adependencies%2520between%2520planned%2520actions%2520with%2520strategic%2520link%2520scores%253A%2520the%2520drop%2520in%250Athe%2520likelihood%2520of%2520one%2520decision%2520under%2520the%2520constraint%2520that%2520a%2520follow-up%2520decision%250Ais%2520no%2520longer%2520available.%2520We%2520demonstrate%2520the%2520utility%2520of%2520strategic%2520link%2520scores%250Athrough%2520three%2520practical%2520applications%253A%2520%2528i%2529%2520explaining%2520black-box%2520RL%2520agents%2520by%250Aidentifying%2520strategically%2520linked%2520pairs%2520among%2520decisions%2520they%2520make%252C%2520%2528ii%2529%250Aimproving%2520the%2520worst-case%2520performance%2520of%2520decision%2520support%2520systems%2520by%250Adistinguishing%2520whether%2520recommended%2520actions%2520can%2520be%2520adopted%2520as%2520standalone%250Aimprovements%2520or%2520whether%2520they%2520are%2520strategically%2520linked%2520hence%2520requiring%2520a%250Acommitment%2520to%2520a%2520broader%2520strategy%2520to%2520be%2520effective%252C%2520and%2520%2528iii%2529%2520characterizing%2520the%250Aplanning%2520processes%2520of%2520non-RL%2520agents%2520purely%2520through%2520interventions%2520aimed%2520at%250Ameasuring%2520strategic%2520link%2520scores%2520-%2520as%2520an%2520example%252C%2520we%2520consider%2520a%2520realistic%250Atraffic%2520simulator%2520and%2520analyze%2520through%2520road%2520closures%2520the%2520effective%2520planning%250Ahorizon%2520of%2520the%2520emergent%2520routing%2520behavior%2520of%2520many%2520drivers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strategically%20Linked%20Decisions%20in%20Long-Term%20Planning%20and%20Reinforcement%0A%20%20Learning&entry.906535625=Alihan%20H%C3%BCy%C3%BCk%20and%20Finale%20Doshi-Velez&entry.1292438233=%20%20Long-term%20planning%2C%20as%20in%20reinforcement%20learning%20%28RL%29%2C%20involves%20finding%0Astrategies%3A%20actions%20that%20collectively%20work%20toward%20a%20goal%20rather%20than%0Aindividually%20optimizing%20their%20immediate%20outcomes.%20As%20part%20of%20a%20strategy%2C%20some%0Aactions%20are%20taken%20at%20the%20expense%20of%20short-term%20benefit%20to%20enable%20future%20actions%0Awith%20even%20greater%20returns.%20These%20actions%20are%20only%20advantageous%20if%20followed%20up%0Aby%20the%20actions%20they%20facilitate%2C%20consequently%2C%20they%20would%20not%20have%20been%20taken%20if%0Athose%20follow-ups%20were%20not%20available.%20In%20this%20paper%2C%20we%20quantify%20such%0Adependencies%20between%20planned%20actions%20with%20strategic%20link%20scores%3A%20the%20drop%20in%0Athe%20likelihood%20of%20one%20decision%20under%20the%20constraint%20that%20a%20follow-up%20decision%0Ais%20no%20longer%20available.%20We%20demonstrate%20the%20utility%20of%20strategic%20link%20scores%0Athrough%20three%20practical%20applications%3A%20%28i%29%20explaining%20black-box%20RL%20agents%20by%0Aidentifying%20strategically%20linked%20pairs%20among%20decisions%20they%20make%2C%20%28ii%29%0Aimproving%20the%20worst-case%20performance%20of%20decision%20support%20systems%20by%0Adistinguishing%20whether%20recommended%20actions%20can%20be%20adopted%20as%20standalone%0Aimprovements%20or%20whether%20they%20are%20strategically%20linked%20hence%20requiring%20a%0Acommitment%20to%20a%20broader%20strategy%20to%20be%20effective%2C%20and%20%28iii%29%20characterizing%20the%0Aplanning%20processes%20of%20non-RL%20agents%20purely%20through%20interventions%20aimed%20at%0Ameasuring%20strategic%20link%20scores%20-%20as%20an%20example%2C%20we%20consider%20a%20realistic%0Atraffic%20simulator%20and%20analyze%20through%20road%20closures%20the%20effective%20planning%0Ahorizon%20of%20the%20emergent%20routing%20behavior%20of%20many%20drivers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16833v1&entry.124074799=Read"},
{"title": "A Multi-Step Comparative Framework for Anomaly Detection in IoT Data\n  Streams", "author": "Mohammed Al-Qudah and Fadi AlMahamid", "abstract": "  The rapid expansion of Internet of Things (IoT) devices has introduced\ncritical security challenges, underscoring the need for accurate anomaly\ndetection. Although numerous studies have proposed machine learning (ML)\nmethods for this purpose, limited research systematically examines how\ndifferent preprocessing steps--normalization, transformation, and feature\nselection--interact with distinct model architectures. To address this gap,\nthis paper presents a multi-step evaluation framework assessing the combined\nimpact of preprocessing choices on three ML algorithms: RNN-LSTM, autoencoder\nneural networks (ANN), and Gradient Boosting (GBoosting). Experiments on the\nIoTID20 dataset shows that GBoosting consistently delivers superior accuracy\nacross preprocessing configurations, while RNN-LSTM shows notable gains with\nz-score normalization and autoencoders excel in recall, making them well-suited\nfor unsupervised scenarios. By offering a structured analysis of preprocessing\ndecisions and their interplay with various ML techniques, the proposed\nframework provides actionable guidance to enhance anomaly detection performance\nin IoT environments.\n", "link": "http://arxiv.org/abs/2505.16872v1", "date": "2025-05-22", "relevancy": 1.8695, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4905}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4674}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Step%20Comparative%20Framework%20for%20Anomaly%20Detection%20in%20IoT%20Data%0A%20%20Streams&body=Title%3A%20A%20Multi-Step%20Comparative%20Framework%20for%20Anomaly%20Detection%20in%20IoT%20Data%0A%20%20Streams%0AAuthor%3A%20Mohammed%20Al-Qudah%20and%20Fadi%20AlMahamid%0AAbstract%3A%20%20%20The%20rapid%20expansion%20of%20Internet%20of%20Things%20%28IoT%29%20devices%20has%20introduced%0Acritical%20security%20challenges%2C%20underscoring%20the%20need%20for%20accurate%20anomaly%0Adetection.%20Although%20numerous%20studies%20have%20proposed%20machine%20learning%20%28ML%29%0Amethods%20for%20this%20purpose%2C%20limited%20research%20systematically%20examines%20how%0Adifferent%20preprocessing%20steps--normalization%2C%20transformation%2C%20and%20feature%0Aselection--interact%20with%20distinct%20model%20architectures.%20To%20address%20this%20gap%2C%0Athis%20paper%20presents%20a%20multi-step%20evaluation%20framework%20assessing%20the%20combined%0Aimpact%20of%20preprocessing%20choices%20on%20three%20ML%20algorithms%3A%20RNN-LSTM%2C%20autoencoder%0Aneural%20networks%20%28ANN%29%2C%20and%20Gradient%20Boosting%20%28GBoosting%29.%20Experiments%20on%20the%0AIoTID20%20dataset%20shows%20that%20GBoosting%20consistently%20delivers%20superior%20accuracy%0Aacross%20preprocessing%20configurations%2C%20while%20RNN-LSTM%20shows%20notable%20gains%20with%0Az-score%20normalization%20and%20autoencoders%20excel%20in%20recall%2C%20making%20them%20well-suited%0Afor%20unsupervised%20scenarios.%20By%20offering%20a%20structured%20analysis%20of%20preprocessing%0Adecisions%20and%20their%20interplay%20with%20various%20ML%20techniques%2C%20the%20proposed%0Aframework%20provides%20actionable%20guidance%20to%20enhance%20anomaly%20detection%20performance%0Ain%20IoT%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Step%2520Comparative%2520Framework%2520for%2520Anomaly%2520Detection%2520in%2520IoT%2520Data%250A%2520%2520Streams%26entry.906535625%3DMohammed%2520Al-Qudah%2520and%2520Fadi%2520AlMahamid%26entry.1292438233%3D%2520%2520The%2520rapid%2520expansion%2520of%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520devices%2520has%2520introduced%250Acritical%2520security%2520challenges%252C%2520underscoring%2520the%2520need%2520for%2520accurate%2520anomaly%250Adetection.%2520Although%2520numerous%2520studies%2520have%2520proposed%2520machine%2520learning%2520%2528ML%2529%250Amethods%2520for%2520this%2520purpose%252C%2520limited%2520research%2520systematically%2520examines%2520how%250Adifferent%2520preprocessing%2520steps--normalization%252C%2520transformation%252C%2520and%2520feature%250Aselection--interact%2520with%2520distinct%2520model%2520architectures.%2520To%2520address%2520this%2520gap%252C%250Athis%2520paper%2520presents%2520a%2520multi-step%2520evaluation%2520framework%2520assessing%2520the%2520combined%250Aimpact%2520of%2520preprocessing%2520choices%2520on%2520three%2520ML%2520algorithms%253A%2520RNN-LSTM%252C%2520autoencoder%250Aneural%2520networks%2520%2528ANN%2529%252C%2520and%2520Gradient%2520Boosting%2520%2528GBoosting%2529.%2520Experiments%2520on%2520the%250AIoTID20%2520dataset%2520shows%2520that%2520GBoosting%2520consistently%2520delivers%2520superior%2520accuracy%250Aacross%2520preprocessing%2520configurations%252C%2520while%2520RNN-LSTM%2520shows%2520notable%2520gains%2520with%250Az-score%2520normalization%2520and%2520autoencoders%2520excel%2520in%2520recall%252C%2520making%2520them%2520well-suited%250Afor%2520unsupervised%2520scenarios.%2520By%2520offering%2520a%2520structured%2520analysis%2520of%2520preprocessing%250Adecisions%2520and%2520their%2520interplay%2520with%2520various%2520ML%2520techniques%252C%2520the%2520proposed%250Aframework%2520provides%2520actionable%2520guidance%2520to%2520enhance%2520anomaly%2520detection%2520performance%250Ain%2520IoT%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Step%20Comparative%20Framework%20for%20Anomaly%20Detection%20in%20IoT%20Data%0A%20%20Streams&entry.906535625=Mohammed%20Al-Qudah%20and%20Fadi%20AlMahamid&entry.1292438233=%20%20The%20rapid%20expansion%20of%20Internet%20of%20Things%20%28IoT%29%20devices%20has%20introduced%0Acritical%20security%20challenges%2C%20underscoring%20the%20need%20for%20accurate%20anomaly%0Adetection.%20Although%20numerous%20studies%20have%20proposed%20machine%20learning%20%28ML%29%0Amethods%20for%20this%20purpose%2C%20limited%20research%20systematically%20examines%20how%0Adifferent%20preprocessing%20steps--normalization%2C%20transformation%2C%20and%20feature%0Aselection--interact%20with%20distinct%20model%20architectures.%20To%20address%20this%20gap%2C%0Athis%20paper%20presents%20a%20multi-step%20evaluation%20framework%20assessing%20the%20combined%0Aimpact%20of%20preprocessing%20choices%20on%20three%20ML%20algorithms%3A%20RNN-LSTM%2C%20autoencoder%0Aneural%20networks%20%28ANN%29%2C%20and%20Gradient%20Boosting%20%28GBoosting%29.%20Experiments%20on%20the%0AIoTID20%20dataset%20shows%20that%20GBoosting%20consistently%20delivers%20superior%20accuracy%0Aacross%20preprocessing%20configurations%2C%20while%20RNN-LSTM%20shows%20notable%20gains%20with%0Az-score%20normalization%20and%20autoencoders%20excel%20in%20recall%2C%20making%20them%20well-suited%0Afor%20unsupervised%20scenarios.%20By%20offering%20a%20structured%20analysis%20of%20preprocessing%0Adecisions%20and%20their%20interplay%20with%20various%20ML%20techniques%2C%20the%20proposed%0Aframework%20provides%20actionable%20guidance%20to%20enhance%20anomaly%20detection%20performance%0Ain%20IoT%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16872v1&entry.124074799=Read"},
{"title": "GeoBiked: A Dataset with Geometric Features and Automated Labeling\n  Techniques to Enable Deep Generative Models in Engineering Design", "author": "Phillip Mueller and Sebastian Mueller and Lars Mikelsons", "abstract": "  We provide a dataset for enabling Deep Generative Models (DGMs) in\nengineering design and propose methods to automate data labeling by utilizing\nlarge-scale foundation models. GeoBiked is curated to contain 4 355 bicycle\nimages, annotated with structural and technical features and is used to\ninvestigate two automated labeling techniques: The utilization of consolidated\nlatent features (Hyperfeatures) from image-generation models to detect\ngeometric correspondences (e.g. the position of the wheel center) in structural\nimages and the generation of diverse text descriptions for structural images.\nGPT-4o, a vision-language-model (VLM), is instructed to analyze images and\nproduce diverse descriptions aligned with the system-prompt. By representing\ntechnical images as Diffusion-Hyperfeatures, drawing geometric correspondences\nbetween them is possible. The detection accuracy of geometric points in unseen\nsamples is improved by presenting multiple annotated source images. GPT-4o has\nsufficient capabilities to generate accurate descriptions of technical images.\nGrounding the generation only on images leads to diverse descriptions but\ncauses hallucinations, while grounding it on categorical labels restricts the\ndiversity. Using both as input balances creativity and accuracy. Successfully\nusing Hyperfeatures for geometric correspondence suggests that this approach\ncan be used for general point-detection and annotation tasks in technical\nimages. Labeling such images with text descriptions using VLMs is possible, but\ndependent on the models detection capabilities, careful prompt-engineering and\nthe selection of input information. Applying foundation models in engineering\ndesign is largely unexplored. We aim to bridge this gap with a dataset to\nexplore training, finetuning and conditioning DGMs in this field and suggesting\napproaches to bootstrap foundation models to process technical images.\n", "link": "http://arxiv.org/abs/2409.17045v2", "date": "2025-05-22", "relevancy": 2.2665, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5749}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5611}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoBiked%3A%20A%20Dataset%20with%20Geometric%20Features%20and%20Automated%20Labeling%0A%20%20Techniques%20to%20Enable%20Deep%20Generative%20Models%20in%20Engineering%20Design&body=Title%3A%20GeoBiked%3A%20A%20Dataset%20with%20Geometric%20Features%20and%20Automated%20Labeling%0A%20%20Techniques%20to%20Enable%20Deep%20Generative%20Models%20in%20Engineering%20Design%0AAuthor%3A%20Phillip%20Mueller%20and%20Sebastian%20Mueller%20and%20Lars%20Mikelsons%0AAbstract%3A%20%20%20We%20provide%20a%20dataset%20for%20enabling%20Deep%20Generative%20Models%20%28DGMs%29%20in%0Aengineering%20design%20and%20propose%20methods%20to%20automate%20data%20labeling%20by%20utilizing%0Alarge-scale%20foundation%20models.%20GeoBiked%20is%20curated%20to%20contain%204%20355%20bicycle%0Aimages%2C%20annotated%20with%20structural%20and%20technical%20features%20and%20is%20used%20to%0Ainvestigate%20two%20automated%20labeling%20techniques%3A%20The%20utilization%20of%20consolidated%0Alatent%20features%20%28Hyperfeatures%29%20from%20image-generation%20models%20to%20detect%0Ageometric%20correspondences%20%28e.g.%20the%20position%20of%20the%20wheel%20center%29%20in%20structural%0Aimages%20and%20the%20generation%20of%20diverse%20text%20descriptions%20for%20structural%20images.%0AGPT-4o%2C%20a%20vision-language-model%20%28VLM%29%2C%20is%20instructed%20to%20analyze%20images%20and%0Aproduce%20diverse%20descriptions%20aligned%20with%20the%20system-prompt.%20By%20representing%0Atechnical%20images%20as%20Diffusion-Hyperfeatures%2C%20drawing%20geometric%20correspondences%0Abetween%20them%20is%20possible.%20The%20detection%20accuracy%20of%20geometric%20points%20in%20unseen%0Asamples%20is%20improved%20by%20presenting%20multiple%20annotated%20source%20images.%20GPT-4o%20has%0Asufficient%20capabilities%20to%20generate%20accurate%20descriptions%20of%20technical%20images.%0AGrounding%20the%20generation%20only%20on%20images%20leads%20to%20diverse%20descriptions%20but%0Acauses%20hallucinations%2C%20while%20grounding%20it%20on%20categorical%20labels%20restricts%20the%0Adiversity.%20Using%20both%20as%20input%20balances%20creativity%20and%20accuracy.%20Successfully%0Ausing%20Hyperfeatures%20for%20geometric%20correspondence%20suggests%20that%20this%20approach%0Acan%20be%20used%20for%20general%20point-detection%20and%20annotation%20tasks%20in%20technical%0Aimages.%20Labeling%20such%20images%20with%20text%20descriptions%20using%20VLMs%20is%20possible%2C%20but%0Adependent%20on%20the%20models%20detection%20capabilities%2C%20careful%20prompt-engineering%20and%0Athe%20selection%20of%20input%20information.%20Applying%20foundation%20models%20in%20engineering%0Adesign%20is%20largely%20unexplored.%20We%20aim%20to%20bridge%20this%20gap%20with%20a%20dataset%20to%0Aexplore%20training%2C%20finetuning%20and%20conditioning%20DGMs%20in%20this%20field%20and%20suggesting%0Aapproaches%20to%20bootstrap%20foundation%20models%20to%20process%20technical%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17045v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoBiked%253A%2520A%2520Dataset%2520with%2520Geometric%2520Features%2520and%2520Automated%2520Labeling%250A%2520%2520Techniques%2520to%2520Enable%2520Deep%2520Generative%2520Models%2520in%2520Engineering%2520Design%26entry.906535625%3DPhillip%2520Mueller%2520and%2520Sebastian%2520Mueller%2520and%2520Lars%2520Mikelsons%26entry.1292438233%3D%2520%2520We%2520provide%2520a%2520dataset%2520for%2520enabling%2520Deep%2520Generative%2520Models%2520%2528DGMs%2529%2520in%250Aengineering%2520design%2520and%2520propose%2520methods%2520to%2520automate%2520data%2520labeling%2520by%2520utilizing%250Alarge-scale%2520foundation%2520models.%2520GeoBiked%2520is%2520curated%2520to%2520contain%25204%2520355%2520bicycle%250Aimages%252C%2520annotated%2520with%2520structural%2520and%2520technical%2520features%2520and%2520is%2520used%2520to%250Ainvestigate%2520two%2520automated%2520labeling%2520techniques%253A%2520The%2520utilization%2520of%2520consolidated%250Alatent%2520features%2520%2528Hyperfeatures%2529%2520from%2520image-generation%2520models%2520to%2520detect%250Ageometric%2520correspondences%2520%2528e.g.%2520the%2520position%2520of%2520the%2520wheel%2520center%2529%2520in%2520structural%250Aimages%2520and%2520the%2520generation%2520of%2520diverse%2520text%2520descriptions%2520for%2520structural%2520images.%250AGPT-4o%252C%2520a%2520vision-language-model%2520%2528VLM%2529%252C%2520is%2520instructed%2520to%2520analyze%2520images%2520and%250Aproduce%2520diverse%2520descriptions%2520aligned%2520with%2520the%2520system-prompt.%2520By%2520representing%250Atechnical%2520images%2520as%2520Diffusion-Hyperfeatures%252C%2520drawing%2520geometric%2520correspondences%250Abetween%2520them%2520is%2520possible.%2520The%2520detection%2520accuracy%2520of%2520geometric%2520points%2520in%2520unseen%250Asamples%2520is%2520improved%2520by%2520presenting%2520multiple%2520annotated%2520source%2520images.%2520GPT-4o%2520has%250Asufficient%2520capabilities%2520to%2520generate%2520accurate%2520descriptions%2520of%2520technical%2520images.%250AGrounding%2520the%2520generation%2520only%2520on%2520images%2520leads%2520to%2520diverse%2520descriptions%2520but%250Acauses%2520hallucinations%252C%2520while%2520grounding%2520it%2520on%2520categorical%2520labels%2520restricts%2520the%250Adiversity.%2520Using%2520both%2520as%2520input%2520balances%2520creativity%2520and%2520accuracy.%2520Successfully%250Ausing%2520Hyperfeatures%2520for%2520geometric%2520correspondence%2520suggests%2520that%2520this%2520approach%250Acan%2520be%2520used%2520for%2520general%2520point-detection%2520and%2520annotation%2520tasks%2520in%2520technical%250Aimages.%2520Labeling%2520such%2520images%2520with%2520text%2520descriptions%2520using%2520VLMs%2520is%2520possible%252C%2520but%250Adependent%2520on%2520the%2520models%2520detection%2520capabilities%252C%2520careful%2520prompt-engineering%2520and%250Athe%2520selection%2520of%2520input%2520information.%2520Applying%2520foundation%2520models%2520in%2520engineering%250Adesign%2520is%2520largely%2520unexplored.%2520We%2520aim%2520to%2520bridge%2520this%2520gap%2520with%2520a%2520dataset%2520to%250Aexplore%2520training%252C%2520finetuning%2520and%2520conditioning%2520DGMs%2520in%2520this%2520field%2520and%2520suggesting%250Aapproaches%2520to%2520bootstrap%2520foundation%2520models%2520to%2520process%2520technical%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17045v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoBiked%3A%20A%20Dataset%20with%20Geometric%20Features%20and%20Automated%20Labeling%0A%20%20Techniques%20to%20Enable%20Deep%20Generative%20Models%20in%20Engineering%20Design&entry.906535625=Phillip%20Mueller%20and%20Sebastian%20Mueller%20and%20Lars%20Mikelsons&entry.1292438233=%20%20We%20provide%20a%20dataset%20for%20enabling%20Deep%20Generative%20Models%20%28DGMs%29%20in%0Aengineering%20design%20and%20propose%20methods%20to%20automate%20data%20labeling%20by%20utilizing%0Alarge-scale%20foundation%20models.%20GeoBiked%20is%20curated%20to%20contain%204%20355%20bicycle%0Aimages%2C%20annotated%20with%20structural%20and%20technical%20features%20and%20is%20used%20to%0Ainvestigate%20two%20automated%20labeling%20techniques%3A%20The%20utilization%20of%20consolidated%0Alatent%20features%20%28Hyperfeatures%29%20from%20image-generation%20models%20to%20detect%0Ageometric%20correspondences%20%28e.g.%20the%20position%20of%20the%20wheel%20center%29%20in%20structural%0Aimages%20and%20the%20generation%20of%20diverse%20text%20descriptions%20for%20structural%20images.%0AGPT-4o%2C%20a%20vision-language-model%20%28VLM%29%2C%20is%20instructed%20to%20analyze%20images%20and%0Aproduce%20diverse%20descriptions%20aligned%20with%20the%20system-prompt.%20By%20representing%0Atechnical%20images%20as%20Diffusion-Hyperfeatures%2C%20drawing%20geometric%20correspondences%0Abetween%20them%20is%20possible.%20The%20detection%20accuracy%20of%20geometric%20points%20in%20unseen%0Asamples%20is%20improved%20by%20presenting%20multiple%20annotated%20source%20images.%20GPT-4o%20has%0Asufficient%20capabilities%20to%20generate%20accurate%20descriptions%20of%20technical%20images.%0AGrounding%20the%20generation%20only%20on%20images%20leads%20to%20diverse%20descriptions%20but%0Acauses%20hallucinations%2C%20while%20grounding%20it%20on%20categorical%20labels%20restricts%20the%0Adiversity.%20Using%20both%20as%20input%20balances%20creativity%20and%20accuracy.%20Successfully%0Ausing%20Hyperfeatures%20for%20geometric%20correspondence%20suggests%20that%20this%20approach%0Acan%20be%20used%20for%20general%20point-detection%20and%20annotation%20tasks%20in%20technical%0Aimages.%20Labeling%20such%20images%20with%20text%20descriptions%20using%20VLMs%20is%20possible%2C%20but%0Adependent%20on%20the%20models%20detection%20capabilities%2C%20careful%20prompt-engineering%20and%0Athe%20selection%20of%20input%20information.%20Applying%20foundation%20models%20in%20engineering%0Adesign%20is%20largely%20unexplored.%20We%20aim%20to%20bridge%20this%20gap%20with%20a%20dataset%20to%0Aexplore%20training%2C%20finetuning%20and%20conditioning%20DGMs%20in%20this%20field%20and%20suggesting%0Aapproaches%20to%20bootstrap%20foundation%20models%20to%20process%20technical%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17045v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


