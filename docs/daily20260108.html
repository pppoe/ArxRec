<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260106.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "IDESplat: Iterative Depth Probability Estimation for Generalizable 3D Gaussian Splatting", "author": "Wei Long and Haifeng Wu and Shiyin Jiang and Jinhua Zhang and Xinchun Ji and Shuhang Gu", "abstract": "Generalizable 3D Gaussian Splatting aims to directly predict Gaussian parameters using a feed-forward network for scene reconstruction. Among these parameters, Gaussian means are particularly difficult to predict, so depth is usually estimated first and then unprojected to obtain the Gaussian sphere centers. Existing methods typically rely solely on a single warp to estimate depth probability, which hinders their ability to fully leverage cross-view geometric cues, resulting in unstable and coarse depth maps. To address this limitation, we propose IDESplat, which iteratively applies warp operations to boost depth probability estimation for accurate Gaussian mean prediction. First, to eliminate the inherent instability of a single warp, we introduce a Depth Probability Boosting Unit (DPBU) that integrates epipolar attention maps produced by cascading warp operations in a multiplicative manner. Next, we construct an iterative depth estimation process by stacking multiple DPBUs, progressively identifying potential depth candidates with high likelihood. As IDESplat iteratively boosts depth probability estimates and updates the depth candidates, the depth map is gradually refined, resulting in accurate Gaussian means. We conduct experiments on RealEstate10K, ACID, and DL3DV. IDESplat achieves outstanding reconstruction quality and state-of-the-art performance with real-time efficiency. On RE10K, it outperforms DepthSplat by 0.33 dB in PSNR, using only 10.7% of the parameters and 70% of the memory. Additionally, our IDESplat improves PSNR by 2.95 dB over DepthSplat on the DTU dataset in cross-dataset experiments, demonstrating its strong generalization ability.", "link": "http://arxiv.org/abs/2601.03824v1", "date": "2026-01-07", "relevancy": 3.2553, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.686}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6604}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDESplat%3A%20Iterative%20Depth%20Probability%20Estimation%20for%20Generalizable%203D%20Gaussian%20Splatting&body=Title%3A%20IDESplat%3A%20Iterative%20Depth%20Probability%20Estimation%20for%20Generalizable%203D%20Gaussian%20Splatting%0AAuthor%3A%20Wei%20Long%20and%20Haifeng%20Wu%20and%20Shiyin%20Jiang%20and%20Jinhua%20Zhang%20and%20Xinchun%20Ji%20and%20Shuhang%20Gu%0AAbstract%3A%20Generalizable%203D%20Gaussian%20Splatting%20aims%20to%20directly%20predict%20Gaussian%20parameters%20using%20a%20feed-forward%20network%20for%20scene%20reconstruction.%20Among%20these%20parameters%2C%20Gaussian%20means%20are%20particularly%20difficult%20to%20predict%2C%20so%20depth%20is%20usually%20estimated%20first%20and%20then%20unprojected%20to%20obtain%20the%20Gaussian%20sphere%20centers.%20Existing%20methods%20typically%20rely%20solely%20on%20a%20single%20warp%20to%20estimate%20depth%20probability%2C%20which%20hinders%20their%20ability%20to%20fully%20leverage%20cross-view%20geometric%20cues%2C%20resulting%20in%20unstable%20and%20coarse%20depth%20maps.%20To%20address%20this%20limitation%2C%20we%20propose%20IDESplat%2C%20which%20iteratively%20applies%20warp%20operations%20to%20boost%20depth%20probability%20estimation%20for%20accurate%20Gaussian%20mean%20prediction.%20First%2C%20to%20eliminate%20the%20inherent%20instability%20of%20a%20single%20warp%2C%20we%20introduce%20a%20Depth%20Probability%20Boosting%20Unit%20%28DPBU%29%20that%20integrates%20epipolar%20attention%20maps%20produced%20by%20cascading%20warp%20operations%20in%20a%20multiplicative%20manner.%20Next%2C%20we%20construct%20an%20iterative%20depth%20estimation%20process%20by%20stacking%20multiple%20DPBUs%2C%20progressively%20identifying%20potential%20depth%20candidates%20with%20high%20likelihood.%20As%20IDESplat%20iteratively%20boosts%20depth%20probability%20estimates%20and%20updates%20the%20depth%20candidates%2C%20the%20depth%20map%20is%20gradually%20refined%2C%20resulting%20in%20accurate%20Gaussian%20means.%20We%20conduct%20experiments%20on%20RealEstate10K%2C%20ACID%2C%20and%20DL3DV.%20IDESplat%20achieves%20outstanding%20reconstruction%20quality%20and%20state-of-the-art%20performance%20with%20real-time%20efficiency.%20On%20RE10K%2C%20it%20outperforms%20DepthSplat%20by%200.33%20dB%20in%20PSNR%2C%20using%20only%2010.7%25%20of%20the%20parameters%20and%2070%25%20of%20the%20memory.%20Additionally%2C%20our%20IDESplat%20improves%20PSNR%20by%202.95%20dB%20over%20DepthSplat%20on%20the%20DTU%20dataset%20in%20cross-dataset%20experiments%2C%20demonstrating%20its%20strong%20generalization%20ability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDESplat%253A%2520Iterative%2520Depth%2520Probability%2520Estimation%2520for%2520Generalizable%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DWei%2520Long%2520and%2520Haifeng%2520Wu%2520and%2520Shiyin%2520Jiang%2520and%2520Jinhua%2520Zhang%2520and%2520Xinchun%2520Ji%2520and%2520Shuhang%2520Gu%26entry.1292438233%3DGeneralizable%25203D%2520Gaussian%2520Splatting%2520aims%2520to%2520directly%2520predict%2520Gaussian%2520parameters%2520using%2520a%2520feed-forward%2520network%2520for%2520scene%2520reconstruction.%2520Among%2520these%2520parameters%252C%2520Gaussian%2520means%2520are%2520particularly%2520difficult%2520to%2520predict%252C%2520so%2520depth%2520is%2520usually%2520estimated%2520first%2520and%2520then%2520unprojected%2520to%2520obtain%2520the%2520Gaussian%2520sphere%2520centers.%2520Existing%2520methods%2520typically%2520rely%2520solely%2520on%2520a%2520single%2520warp%2520to%2520estimate%2520depth%2520probability%252C%2520which%2520hinders%2520their%2520ability%2520to%2520fully%2520leverage%2520cross-view%2520geometric%2520cues%252C%2520resulting%2520in%2520unstable%2520and%2520coarse%2520depth%2520maps.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520IDESplat%252C%2520which%2520iteratively%2520applies%2520warp%2520operations%2520to%2520boost%2520depth%2520probability%2520estimation%2520for%2520accurate%2520Gaussian%2520mean%2520prediction.%2520First%252C%2520to%2520eliminate%2520the%2520inherent%2520instability%2520of%2520a%2520single%2520warp%252C%2520we%2520introduce%2520a%2520Depth%2520Probability%2520Boosting%2520Unit%2520%2528DPBU%2529%2520that%2520integrates%2520epipolar%2520attention%2520maps%2520produced%2520by%2520cascading%2520warp%2520operations%2520in%2520a%2520multiplicative%2520manner.%2520Next%252C%2520we%2520construct%2520an%2520iterative%2520depth%2520estimation%2520process%2520by%2520stacking%2520multiple%2520DPBUs%252C%2520progressively%2520identifying%2520potential%2520depth%2520candidates%2520with%2520high%2520likelihood.%2520As%2520IDESplat%2520iteratively%2520boosts%2520depth%2520probability%2520estimates%2520and%2520updates%2520the%2520depth%2520candidates%252C%2520the%2520depth%2520map%2520is%2520gradually%2520refined%252C%2520resulting%2520in%2520accurate%2520Gaussian%2520means.%2520We%2520conduct%2520experiments%2520on%2520RealEstate10K%252C%2520ACID%252C%2520and%2520DL3DV.%2520IDESplat%2520achieves%2520outstanding%2520reconstruction%2520quality%2520and%2520state-of-the-art%2520performance%2520with%2520real-time%2520efficiency.%2520On%2520RE10K%252C%2520it%2520outperforms%2520DepthSplat%2520by%25200.33%2520dB%2520in%2520PSNR%252C%2520using%2520only%252010.7%2525%2520of%2520the%2520parameters%2520and%252070%2525%2520of%2520the%2520memory.%2520Additionally%252C%2520our%2520IDESplat%2520improves%2520PSNR%2520by%25202.95%2520dB%2520over%2520DepthSplat%2520on%2520the%2520DTU%2520dataset%2520in%2520cross-dataset%2520experiments%252C%2520demonstrating%2520its%2520strong%2520generalization%2520ability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDESplat%3A%20Iterative%20Depth%20Probability%20Estimation%20for%20Generalizable%203D%20Gaussian%20Splatting&entry.906535625=Wei%20Long%20and%20Haifeng%20Wu%20and%20Shiyin%20Jiang%20and%20Jinhua%20Zhang%20and%20Xinchun%20Ji%20and%20Shuhang%20Gu&entry.1292438233=Generalizable%203D%20Gaussian%20Splatting%20aims%20to%20directly%20predict%20Gaussian%20parameters%20using%20a%20feed-forward%20network%20for%20scene%20reconstruction.%20Among%20these%20parameters%2C%20Gaussian%20means%20are%20particularly%20difficult%20to%20predict%2C%20so%20depth%20is%20usually%20estimated%20first%20and%20then%20unprojected%20to%20obtain%20the%20Gaussian%20sphere%20centers.%20Existing%20methods%20typically%20rely%20solely%20on%20a%20single%20warp%20to%20estimate%20depth%20probability%2C%20which%20hinders%20their%20ability%20to%20fully%20leverage%20cross-view%20geometric%20cues%2C%20resulting%20in%20unstable%20and%20coarse%20depth%20maps.%20To%20address%20this%20limitation%2C%20we%20propose%20IDESplat%2C%20which%20iteratively%20applies%20warp%20operations%20to%20boost%20depth%20probability%20estimation%20for%20accurate%20Gaussian%20mean%20prediction.%20First%2C%20to%20eliminate%20the%20inherent%20instability%20of%20a%20single%20warp%2C%20we%20introduce%20a%20Depth%20Probability%20Boosting%20Unit%20%28DPBU%29%20that%20integrates%20epipolar%20attention%20maps%20produced%20by%20cascading%20warp%20operations%20in%20a%20multiplicative%20manner.%20Next%2C%20we%20construct%20an%20iterative%20depth%20estimation%20process%20by%20stacking%20multiple%20DPBUs%2C%20progressively%20identifying%20potential%20depth%20candidates%20with%20high%20likelihood.%20As%20IDESplat%20iteratively%20boosts%20depth%20probability%20estimates%20and%20updates%20the%20depth%20candidates%2C%20the%20depth%20map%20is%20gradually%20refined%2C%20resulting%20in%20accurate%20Gaussian%20means.%20We%20conduct%20experiments%20on%20RealEstate10K%2C%20ACID%2C%20and%20DL3DV.%20IDESplat%20achieves%20outstanding%20reconstruction%20quality%20and%20state-of-the-art%20performance%20with%20real-time%20efficiency.%20On%20RE10K%2C%20it%20outperforms%20DepthSplat%20by%200.33%20dB%20in%20PSNR%2C%20using%20only%2010.7%25%20of%20the%20parameters%20and%2070%25%20of%20the%20memory.%20Additionally%2C%20our%20IDESplat%20improves%20PSNR%20by%202.95%20dB%20over%20DepthSplat%20on%20the%20DTU%20dataset%20in%20cross-dataset%20experiments%2C%20demonstrating%20its%20strong%20generalization%20ability.&entry.1838667208=http%3A//arxiv.org/abs/2601.03824v1&entry.124074799=Read"},
{"title": "ImLoc: Revisiting Visual Localization with Image-based Representation", "author": "Xudong Jiang and Fangjinhua Wang and Silvano Galliani and Christoph Vogel and Marc Pollefeys", "abstract": "Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.", "link": "http://arxiv.org/abs/2601.04185v1", "date": "2026-01-07", "relevancy": 3.1099, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6795}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6091}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImLoc%3A%20Revisiting%20Visual%20Localization%20with%20Image-based%20Representation&body=Title%3A%20ImLoc%3A%20Revisiting%20Visual%20Localization%20with%20Image-based%20Representation%0AAuthor%3A%20Xudong%20Jiang%20and%20Fangjinhua%20Wang%20and%20Silvano%20Galliani%20and%20Christoph%20Vogel%20and%20Marc%20Pollefeys%0AAbstract%3A%20Existing%20visual%20localization%20methods%20are%20typically%20either%202D%20image-based%2C%20which%20are%20easy%20to%20build%20and%20maintain%20but%20limited%20in%20effective%20geometric%20reasoning%2C%20or%203D%20structure-based%2C%20which%20achieve%20high%20accuracy%20but%20require%20a%20centralized%20reconstruction%20and%20are%20difficult%20to%20update.%20In%20this%20work%2C%20we%20revisit%20visual%20localization%20with%20a%202D%20image-based%20representation%20and%20propose%20to%20augment%20each%20image%20with%20estimated%20depth%20maps%20to%20capture%20the%20geometric%20structure.%20Supported%20by%20the%20effective%20use%20of%20dense%20matchers%2C%20this%20representation%20is%20not%20only%20easy%20to%20build%20and%20maintain%2C%20but%20achieves%20highest%20accuracy%20in%20challenging%20conditions.%20With%20compact%20compression%20and%20a%20GPU-accelerated%20LO-RANSAC%20implementation%2C%20the%20whole%20pipeline%20is%20efficient%20in%20both%20storage%20and%20computation%20and%20allows%20for%20a%20flexible%20trade-off%20between%20accuracy%20and%20highest%20memory%20efficiency.%20Our%20method%20achieves%20a%20new%20state-of-the-art%20accuracy%20on%20various%20standard%20benchmarks%20and%20outperforms%20existing%20memory-efficient%20methods%20at%20comparable%20map%20sizes.%20Code%20will%20be%20available%20at%20https%3A//github.com/cvg/Hierarchical-Localization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImLoc%253A%2520Revisiting%2520Visual%2520Localization%2520with%2520Image-based%2520Representation%26entry.906535625%3DXudong%2520Jiang%2520and%2520Fangjinhua%2520Wang%2520and%2520Silvano%2520Galliani%2520and%2520Christoph%2520Vogel%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3DExisting%2520visual%2520localization%2520methods%2520are%2520typically%2520either%25202D%2520image-based%252C%2520which%2520are%2520easy%2520to%2520build%2520and%2520maintain%2520but%2520limited%2520in%2520effective%2520geometric%2520reasoning%252C%2520or%25203D%2520structure-based%252C%2520which%2520achieve%2520high%2520accuracy%2520but%2520require%2520a%2520centralized%2520reconstruction%2520and%2520are%2520difficult%2520to%2520update.%2520In%2520this%2520work%252C%2520we%2520revisit%2520visual%2520localization%2520with%2520a%25202D%2520image-based%2520representation%2520and%2520propose%2520to%2520augment%2520each%2520image%2520with%2520estimated%2520depth%2520maps%2520to%2520capture%2520the%2520geometric%2520structure.%2520Supported%2520by%2520the%2520effective%2520use%2520of%2520dense%2520matchers%252C%2520this%2520representation%2520is%2520not%2520only%2520easy%2520to%2520build%2520and%2520maintain%252C%2520but%2520achieves%2520highest%2520accuracy%2520in%2520challenging%2520conditions.%2520With%2520compact%2520compression%2520and%2520a%2520GPU-accelerated%2520LO-RANSAC%2520implementation%252C%2520the%2520whole%2520pipeline%2520is%2520efficient%2520in%2520both%2520storage%2520and%2520computation%2520and%2520allows%2520for%2520a%2520flexible%2520trade-off%2520between%2520accuracy%2520and%2520highest%2520memory%2520efficiency.%2520Our%2520method%2520achieves%2520a%2520new%2520state-of-the-art%2520accuracy%2520on%2520various%2520standard%2520benchmarks%2520and%2520outperforms%2520existing%2520memory-efficient%2520methods%2520at%2520comparable%2520map%2520sizes.%2520Code%2520will%2520be%2520available%2520at%2520https%253A//github.com/cvg/Hierarchical-Localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImLoc%3A%20Revisiting%20Visual%20Localization%20with%20Image-based%20Representation&entry.906535625=Xudong%20Jiang%20and%20Fangjinhua%20Wang%20and%20Silvano%20Galliani%20and%20Christoph%20Vogel%20and%20Marc%20Pollefeys&entry.1292438233=Existing%20visual%20localization%20methods%20are%20typically%20either%202D%20image-based%2C%20which%20are%20easy%20to%20build%20and%20maintain%20but%20limited%20in%20effective%20geometric%20reasoning%2C%20or%203D%20structure-based%2C%20which%20achieve%20high%20accuracy%20but%20require%20a%20centralized%20reconstruction%20and%20are%20difficult%20to%20update.%20In%20this%20work%2C%20we%20revisit%20visual%20localization%20with%20a%202D%20image-based%20representation%20and%20propose%20to%20augment%20each%20image%20with%20estimated%20depth%20maps%20to%20capture%20the%20geometric%20structure.%20Supported%20by%20the%20effective%20use%20of%20dense%20matchers%2C%20this%20representation%20is%20not%20only%20easy%20to%20build%20and%20maintain%2C%20but%20achieves%20highest%20accuracy%20in%20challenging%20conditions.%20With%20compact%20compression%20and%20a%20GPU-accelerated%20LO-RANSAC%20implementation%2C%20the%20whole%20pipeline%20is%20efficient%20in%20both%20storage%20and%20computation%20and%20allows%20for%20a%20flexible%20trade-off%20between%20accuracy%20and%20highest%20memory%20efficiency.%20Our%20method%20achieves%20a%20new%20state-of-the-art%20accuracy%20on%20various%20standard%20benchmarks%20and%20outperforms%20existing%20memory-efficient%20methods%20at%20comparable%20map%20sizes.%20Code%20will%20be%20available%20at%20https%3A//github.com/cvg/Hierarchical-Localization.&entry.1838667208=http%3A//arxiv.org/abs/2601.04185v1&entry.124074799=Read"},
{"title": "A Novel Convolution and Attention Mechanism-based Model for 6D Object Pose Estimation", "author": "Alexander Du and Xiujin Liu", "abstract": "This paper proposes PoseLecTr, a graph-based encoder-decoder framework that integrates a novel Legendre convolution with attention mechanisms for six-degree-of-freedom (6-DOF) object pose estimation from monocular RGB images. Conventional learning-based approaches predominantly rely on grid-structured convolutions, which can limit their ability to model higher-order and long-range dependencies among image features, especially in cluttered or occluded scenes. PoseLecTr addresses this limitation by constructing a graph representation from image features, where spatial relationships are explicitly modeled through graph connectivity. The proposed framework incorporates a Legendre convolution layer to improve numerical stability in graph convolution, together with spatial-attention and self-attention distillation to enhance feature selection. Experiments conducted on the LINEMOD, Occluded LINEMOD, and YCB-VIDEO datasets demonstrate that our method achieves competitive performance and shows consistent improvements across a wide range of objects and scene complexities.", "link": "http://arxiv.org/abs/2501.01993v2", "date": "2026-01-07", "relevancy": 2.9438, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5895}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Convolution%20and%20Attention%20Mechanism-based%20Model%20for%206D%20Object%20Pose%20Estimation&body=Title%3A%20A%20Novel%20Convolution%20and%20Attention%20Mechanism-based%20Model%20for%206D%20Object%20Pose%20Estimation%0AAuthor%3A%20Alexander%20Du%20and%20Xiujin%20Liu%0AAbstract%3A%20This%20paper%20proposes%20PoseLecTr%2C%20a%20graph-based%20encoder-decoder%20framework%20that%20integrates%20a%20novel%20Legendre%20convolution%20with%20attention%20mechanisms%20for%20six-degree-of-freedom%20%286-DOF%29%20object%20pose%20estimation%20from%20monocular%20RGB%20images.%20Conventional%20learning-based%20approaches%20predominantly%20rely%20on%20grid-structured%20convolutions%2C%20which%20can%20limit%20their%20ability%20to%20model%20higher-order%20and%20long-range%20dependencies%20among%20image%20features%2C%20especially%20in%20cluttered%20or%20occluded%20scenes.%20PoseLecTr%20addresses%20this%20limitation%20by%20constructing%20a%20graph%20representation%20from%20image%20features%2C%20where%20spatial%20relationships%20are%20explicitly%20modeled%20through%20graph%20connectivity.%20The%20proposed%20framework%20incorporates%20a%20Legendre%20convolution%20layer%20to%20improve%20numerical%20stability%20in%20graph%20convolution%2C%20together%20with%20spatial-attention%20and%20self-attention%20distillation%20to%20enhance%20feature%20selection.%20Experiments%20conducted%20on%20the%20LINEMOD%2C%20Occluded%20LINEMOD%2C%20and%20YCB-VIDEO%20datasets%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20and%20shows%20consistent%20improvements%20across%20a%20wide%20range%20of%20objects%20and%20scene%20complexities.%0ALink%3A%20http%3A//arxiv.org/abs/2501.01993v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Convolution%2520and%2520Attention%2520Mechanism-based%2520Model%2520for%25206D%2520Object%2520Pose%2520Estimation%26entry.906535625%3DAlexander%2520Du%2520and%2520Xiujin%2520Liu%26entry.1292438233%3DThis%2520paper%2520proposes%2520PoseLecTr%252C%2520a%2520graph-based%2520encoder-decoder%2520framework%2520that%2520integrates%2520a%2520novel%2520Legendre%2520convolution%2520with%2520attention%2520mechanisms%2520for%2520six-degree-of-freedom%2520%25286-DOF%2529%2520object%2520pose%2520estimation%2520from%2520monocular%2520RGB%2520images.%2520Conventional%2520learning-based%2520approaches%2520predominantly%2520rely%2520on%2520grid-structured%2520convolutions%252C%2520which%2520can%2520limit%2520their%2520ability%2520to%2520model%2520higher-order%2520and%2520long-range%2520dependencies%2520among%2520image%2520features%252C%2520especially%2520in%2520cluttered%2520or%2520occluded%2520scenes.%2520PoseLecTr%2520addresses%2520this%2520limitation%2520by%2520constructing%2520a%2520graph%2520representation%2520from%2520image%2520features%252C%2520where%2520spatial%2520relationships%2520are%2520explicitly%2520modeled%2520through%2520graph%2520connectivity.%2520The%2520proposed%2520framework%2520incorporates%2520a%2520Legendre%2520convolution%2520layer%2520to%2520improve%2520numerical%2520stability%2520in%2520graph%2520convolution%252C%2520together%2520with%2520spatial-attention%2520and%2520self-attention%2520distillation%2520to%2520enhance%2520feature%2520selection.%2520Experiments%2520conducted%2520on%2520the%2520LINEMOD%252C%2520Occluded%2520LINEMOD%252C%2520and%2520YCB-VIDEO%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%2520performance%2520and%2520shows%2520consistent%2520improvements%2520across%2520a%2520wide%2520range%2520of%2520objects%2520and%2520scene%2520complexities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01993v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Convolution%20and%20Attention%20Mechanism-based%20Model%20for%206D%20Object%20Pose%20Estimation&entry.906535625=Alexander%20Du%20and%20Xiujin%20Liu&entry.1292438233=This%20paper%20proposes%20PoseLecTr%2C%20a%20graph-based%20encoder-decoder%20framework%20that%20integrates%20a%20novel%20Legendre%20convolution%20with%20attention%20mechanisms%20for%20six-degree-of-freedom%20%286-DOF%29%20object%20pose%20estimation%20from%20monocular%20RGB%20images.%20Conventional%20learning-based%20approaches%20predominantly%20rely%20on%20grid-structured%20convolutions%2C%20which%20can%20limit%20their%20ability%20to%20model%20higher-order%20and%20long-range%20dependencies%20among%20image%20features%2C%20especially%20in%20cluttered%20or%20occluded%20scenes.%20PoseLecTr%20addresses%20this%20limitation%20by%20constructing%20a%20graph%20representation%20from%20image%20features%2C%20where%20spatial%20relationships%20are%20explicitly%20modeled%20through%20graph%20connectivity.%20The%20proposed%20framework%20incorporates%20a%20Legendre%20convolution%20layer%20to%20improve%20numerical%20stability%20in%20graph%20convolution%2C%20together%20with%20spatial-attention%20and%20self-attention%20distillation%20to%20enhance%20feature%20selection.%20Experiments%20conducted%20on%20the%20LINEMOD%2C%20Occluded%20LINEMOD%2C%20and%20YCB-VIDEO%20datasets%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20and%20shows%20consistent%20improvements%20across%20a%20wide%20range%20of%20objects%20and%20scene%20complexities.&entry.1838667208=http%3A//arxiv.org/abs/2501.01993v2&entry.124074799=Read"},
{"title": "A Comparative Study of 3D Model Acquisition Methods for Synthetic Data Generation of Agricultural Products", "author": "Steven Moonen and Rob Salaets and Kenneth Batstone and Abdellatif Bey-Temsamani and Nick Michiels", "abstract": "In the manufacturing industry, computer vision systems based on artificial intelligence (AI) are widely used to reduce costs and increase production. Training these AI models requires a large amount of training data that is costly to acquire and annotate, especially in high-variance, low-volume manufacturing environments. A popular approach to reduce the need for real data is the use of synthetic data that is generated by leveraging computer-aided design (CAD) models available in the industry. However, in the agricultural industry these models are not readily available, increasing the difficulty in leveraging synthetic data. In this paper, we present different techniques for substituting CAD files to create synthetic datasets. We measure their relative performance when used to train an AI object detection model to separate stones and potatoes in a bin picking environment. We demonstrate that using highly representative 3D models acquired by scanning or using image-to-3D approaches can be used to generate synthetic data for training object detection models. Finetuning on a small real dataset can significantly improve the performance of the models and even get similar performance when less representative models are used.", "link": "http://arxiv.org/abs/2601.03784v1", "date": "2026-01-07", "relevancy": 2.9102, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5938}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5938}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Study%20of%203D%20Model%20Acquisition%20Methods%20for%20Synthetic%20Data%20Generation%20of%20Agricultural%20Products&body=Title%3A%20A%20Comparative%20Study%20of%203D%20Model%20Acquisition%20Methods%20for%20Synthetic%20Data%20Generation%20of%20Agricultural%20Products%0AAuthor%3A%20Steven%20Moonen%20and%20Rob%20Salaets%20and%20Kenneth%20Batstone%20and%20Abdellatif%20Bey-Temsamani%20and%20Nick%20Michiels%0AAbstract%3A%20In%20the%20manufacturing%20industry%2C%20computer%20vision%20systems%20based%20on%20artificial%20intelligence%20%28AI%29%20are%20widely%20used%20to%20reduce%20costs%20and%20increase%20production.%20Training%20these%20AI%20models%20requires%20a%20large%20amount%20of%20training%20data%20that%20is%20costly%20to%20acquire%20and%20annotate%2C%20especially%20in%20high-variance%2C%20low-volume%20manufacturing%20environments.%20A%20popular%20approach%20to%20reduce%20the%20need%20for%20real%20data%20is%20the%20use%20of%20synthetic%20data%20that%20is%20generated%20by%20leveraging%20computer-aided%20design%20%28CAD%29%20models%20available%20in%20the%20industry.%20However%2C%20in%20the%20agricultural%20industry%20these%20models%20are%20not%20readily%20available%2C%20increasing%20the%20difficulty%20in%20leveraging%20synthetic%20data.%20In%20this%20paper%2C%20we%20present%20different%20techniques%20for%20substituting%20CAD%20files%20to%20create%20synthetic%20datasets.%20We%20measure%20their%20relative%20performance%20when%20used%20to%20train%20an%20AI%20object%20detection%20model%20to%20separate%20stones%20and%20potatoes%20in%20a%20bin%20picking%20environment.%20We%20demonstrate%20that%20using%20highly%20representative%203D%20models%20acquired%20by%20scanning%20or%20using%20image-to-3D%20approaches%20can%20be%20used%20to%20generate%20synthetic%20data%20for%20training%20object%20detection%20models.%20Finetuning%20on%20a%20small%20real%20dataset%20can%20significantly%20improve%20the%20performance%20of%20the%20models%20and%20even%20get%20similar%20performance%20when%20less%20representative%20models%20are%20used.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Study%2520of%25203D%2520Model%2520Acquisition%2520Methods%2520for%2520Synthetic%2520Data%2520Generation%2520of%2520Agricultural%2520Products%26entry.906535625%3DSteven%2520Moonen%2520and%2520Rob%2520Salaets%2520and%2520Kenneth%2520Batstone%2520and%2520Abdellatif%2520Bey-Temsamani%2520and%2520Nick%2520Michiels%26entry.1292438233%3DIn%2520the%2520manufacturing%2520industry%252C%2520computer%2520vision%2520systems%2520based%2520on%2520artificial%2520intelligence%2520%2528AI%2529%2520are%2520widely%2520used%2520to%2520reduce%2520costs%2520and%2520increase%2520production.%2520Training%2520these%2520AI%2520models%2520requires%2520a%2520large%2520amount%2520of%2520training%2520data%2520that%2520is%2520costly%2520to%2520acquire%2520and%2520annotate%252C%2520especially%2520in%2520high-variance%252C%2520low-volume%2520manufacturing%2520environments.%2520A%2520popular%2520approach%2520to%2520reduce%2520the%2520need%2520for%2520real%2520data%2520is%2520the%2520use%2520of%2520synthetic%2520data%2520that%2520is%2520generated%2520by%2520leveraging%2520computer-aided%2520design%2520%2528CAD%2529%2520models%2520available%2520in%2520the%2520industry.%2520However%252C%2520in%2520the%2520agricultural%2520industry%2520these%2520models%2520are%2520not%2520readily%2520available%252C%2520increasing%2520the%2520difficulty%2520in%2520leveraging%2520synthetic%2520data.%2520In%2520this%2520paper%252C%2520we%2520present%2520different%2520techniques%2520for%2520substituting%2520CAD%2520files%2520to%2520create%2520synthetic%2520datasets.%2520We%2520measure%2520their%2520relative%2520performance%2520when%2520used%2520to%2520train%2520an%2520AI%2520object%2520detection%2520model%2520to%2520separate%2520stones%2520and%2520potatoes%2520in%2520a%2520bin%2520picking%2520environment.%2520We%2520demonstrate%2520that%2520using%2520highly%2520representative%25203D%2520models%2520acquired%2520by%2520scanning%2520or%2520using%2520image-to-3D%2520approaches%2520can%2520be%2520used%2520to%2520generate%2520synthetic%2520data%2520for%2520training%2520object%2520detection%2520models.%2520Finetuning%2520on%2520a%2520small%2520real%2520dataset%2520can%2520significantly%2520improve%2520the%2520performance%2520of%2520the%2520models%2520and%2520even%2520get%2520similar%2520performance%2520when%2520less%2520representative%2520models%2520are%2520used.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Study%20of%203D%20Model%20Acquisition%20Methods%20for%20Synthetic%20Data%20Generation%20of%20Agricultural%20Products&entry.906535625=Steven%20Moonen%20and%20Rob%20Salaets%20and%20Kenneth%20Batstone%20and%20Abdellatif%20Bey-Temsamani%20and%20Nick%20Michiels&entry.1292438233=In%20the%20manufacturing%20industry%2C%20computer%20vision%20systems%20based%20on%20artificial%20intelligence%20%28AI%29%20are%20widely%20used%20to%20reduce%20costs%20and%20increase%20production.%20Training%20these%20AI%20models%20requires%20a%20large%20amount%20of%20training%20data%20that%20is%20costly%20to%20acquire%20and%20annotate%2C%20especially%20in%20high-variance%2C%20low-volume%20manufacturing%20environments.%20A%20popular%20approach%20to%20reduce%20the%20need%20for%20real%20data%20is%20the%20use%20of%20synthetic%20data%20that%20is%20generated%20by%20leveraging%20computer-aided%20design%20%28CAD%29%20models%20available%20in%20the%20industry.%20However%2C%20in%20the%20agricultural%20industry%20these%20models%20are%20not%20readily%20available%2C%20increasing%20the%20difficulty%20in%20leveraging%20synthetic%20data.%20In%20this%20paper%2C%20we%20present%20different%20techniques%20for%20substituting%20CAD%20files%20to%20create%20synthetic%20datasets.%20We%20measure%20their%20relative%20performance%20when%20used%20to%20train%20an%20AI%20object%20detection%20model%20to%20separate%20stones%20and%20potatoes%20in%20a%20bin%20picking%20environment.%20We%20demonstrate%20that%20using%20highly%20representative%203D%20models%20acquired%20by%20scanning%20or%20using%20image-to-3D%20approaches%20can%20be%20used%20to%20generate%20synthetic%20data%20for%20training%20object%20detection%20models.%20Finetuning%20on%20a%20small%20real%20dataset%20can%20significantly%20improve%20the%20performance%20of%20the%20models%20and%20even%20get%20similar%20performance%20when%20less%20representative%20models%20are%20used.&entry.1838667208=http%3A//arxiv.org/abs/2601.03784v1&entry.124074799=Read"},
{"title": "Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation", "author": "Mingxing Zhan and Li Zhang and Beibei Wang and Yingjie Wang and Zenglin Shi", "abstract": "Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.", "link": "http://arxiv.org/abs/2601.01457v2", "date": "2026-01-07", "relevancy": 2.8531, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20as%20Prior%2C%20Vision%20as%20Calibration%3A%20Metric%20Scale%20Recovery%20for%20Monocular%20Depth%20Estimation&body=Title%3A%20Language%20as%20Prior%2C%20Vision%20as%20Calibration%3A%20Metric%20Scale%20Recovery%20for%20Monocular%20Depth%20Estimation%0AAuthor%3A%20Mingxing%20Zhan%20and%20Li%20Zhang%20and%20Beibei%20Wang%20and%20Yingjie%20Wang%20and%20Zenglin%20Shi%0AAbstract%3A%20Relative-depth%20foundation%20models%20transfer%20well%2C%20yet%20monocular%20metric%20depth%20remains%20ill-posed%20due%20to%20unidentifiable%20global%20scale%20and%20heightened%20domain-shift%20sensitivity.%20Under%20a%20frozen-backbone%20calibration%20setting%2C%20we%20recover%20metric%20depth%20via%20an%20image-specific%20affine%20transform%20in%20inverse%20depth%20and%20train%20only%20lightweight%20calibration%20heads%20while%20keeping%20the%20relative-depth%20backbone%20and%20the%20CLIP%20text%20encoder%20fixed.%20Since%20captions%20provide%20coarse%20but%20noisy%20scale%20cues%20that%20vary%20with%20phrasing%20and%20missing%20objects%2C%20we%20use%20language%20to%20predict%20an%20uncertainty-aware%20envelope%20that%20bounds%20feasible%20calibration%20parameters%20in%20an%20unconstrained%20space%2C%20rather%20than%20committing%20to%20a%20text-only%20point%20estimate.%20We%20then%20use%20pooled%20multi-scale%20frozen%20visual%20features%20to%20select%20an%20image-specific%20calibration%20within%20this%20envelope.%20During%20training%2C%20a%20closed-form%20least-squares%20oracle%20in%20inverse%20depth%20provides%20per-image%20supervision%20for%20learning%20the%20envelope%20and%20the%20selected%20calibration.%20Experiments%20on%20NYUv2%20and%20KITTI%20improve%20in-domain%20accuracy%2C%20while%20zero-shot%20transfer%20to%20SUN-RGBD%20and%20DDAD%20demonstrates%20improved%20robustness%20over%20strong%20language-only%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01457v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520as%2520Prior%252C%2520Vision%2520as%2520Calibration%253A%2520Metric%2520Scale%2520Recovery%2520for%2520Monocular%2520Depth%2520Estimation%26entry.906535625%3DMingxing%2520Zhan%2520and%2520Li%2520Zhang%2520and%2520Beibei%2520Wang%2520and%2520Yingjie%2520Wang%2520and%2520Zenglin%2520Shi%26entry.1292438233%3DRelative-depth%2520foundation%2520models%2520transfer%2520well%252C%2520yet%2520monocular%2520metric%2520depth%2520remains%2520ill-posed%2520due%2520to%2520unidentifiable%2520global%2520scale%2520and%2520heightened%2520domain-shift%2520sensitivity.%2520Under%2520a%2520frozen-backbone%2520calibration%2520setting%252C%2520we%2520recover%2520metric%2520depth%2520via%2520an%2520image-specific%2520affine%2520transform%2520in%2520inverse%2520depth%2520and%2520train%2520only%2520lightweight%2520calibration%2520heads%2520while%2520keeping%2520the%2520relative-depth%2520backbone%2520and%2520the%2520CLIP%2520text%2520encoder%2520fixed.%2520Since%2520captions%2520provide%2520coarse%2520but%2520noisy%2520scale%2520cues%2520that%2520vary%2520with%2520phrasing%2520and%2520missing%2520objects%252C%2520we%2520use%2520language%2520to%2520predict%2520an%2520uncertainty-aware%2520envelope%2520that%2520bounds%2520feasible%2520calibration%2520parameters%2520in%2520an%2520unconstrained%2520space%252C%2520rather%2520than%2520committing%2520to%2520a%2520text-only%2520point%2520estimate.%2520We%2520then%2520use%2520pooled%2520multi-scale%2520frozen%2520visual%2520features%2520to%2520select%2520an%2520image-specific%2520calibration%2520within%2520this%2520envelope.%2520During%2520training%252C%2520a%2520closed-form%2520least-squares%2520oracle%2520in%2520inverse%2520depth%2520provides%2520per-image%2520supervision%2520for%2520learning%2520the%2520envelope%2520and%2520the%2520selected%2520calibration.%2520Experiments%2520on%2520NYUv2%2520and%2520KITTI%2520improve%2520in-domain%2520accuracy%252C%2520while%2520zero-shot%2520transfer%2520to%2520SUN-RGBD%2520and%2520DDAD%2520demonstrates%2520improved%2520robustness%2520over%2520strong%2520language-only%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01457v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20as%20Prior%2C%20Vision%20as%20Calibration%3A%20Metric%20Scale%20Recovery%20for%20Monocular%20Depth%20Estimation&entry.906535625=Mingxing%20Zhan%20and%20Li%20Zhang%20and%20Beibei%20Wang%20and%20Yingjie%20Wang%20and%20Zenglin%20Shi&entry.1292438233=Relative-depth%20foundation%20models%20transfer%20well%2C%20yet%20monocular%20metric%20depth%20remains%20ill-posed%20due%20to%20unidentifiable%20global%20scale%20and%20heightened%20domain-shift%20sensitivity.%20Under%20a%20frozen-backbone%20calibration%20setting%2C%20we%20recover%20metric%20depth%20via%20an%20image-specific%20affine%20transform%20in%20inverse%20depth%20and%20train%20only%20lightweight%20calibration%20heads%20while%20keeping%20the%20relative-depth%20backbone%20and%20the%20CLIP%20text%20encoder%20fixed.%20Since%20captions%20provide%20coarse%20but%20noisy%20scale%20cues%20that%20vary%20with%20phrasing%20and%20missing%20objects%2C%20we%20use%20language%20to%20predict%20an%20uncertainty-aware%20envelope%20that%20bounds%20feasible%20calibration%20parameters%20in%20an%20unconstrained%20space%2C%20rather%20than%20committing%20to%20a%20text-only%20point%20estimate.%20We%20then%20use%20pooled%20multi-scale%20frozen%20visual%20features%20to%20select%20an%20image-specific%20calibration%20within%20this%20envelope.%20During%20training%2C%20a%20closed-form%20least-squares%20oracle%20in%20inverse%20depth%20provides%20per-image%20supervision%20for%20learning%20the%20envelope%20and%20the%20selected%20calibration.%20Experiments%20on%20NYUv2%20and%20KITTI%20improve%20in-domain%20accuracy%2C%20while%20zero-shot%20transfer%20to%20SUN-RGBD%20and%20DDAD%20demonstrates%20improved%20robustness%20over%20strong%20language-only%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.01457v2&entry.124074799=Read"},
{"title": "Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?", "author": "Zabir Al Nazi and GM Shahariar and Md. Abrar Hossain and Wei Peng", "abstract": "Theory of Mind (ToM) - the ability to attribute beliefs and intents to others - is fundamental for social intelligence, yet Vision-Language Model (VLM) evaluations remain largely Western-centric. In this work, we introduce CulturalToM-VQA, a benchmark of 5,095 visually situated ToM probes across diverse cultural contexts, rituals, and social norms. Constructed through a frontier proprietary MLLM, human-verified pipeline, the dataset spans a taxonomy of six ToM tasks and four complexity levels. We benchmark 10 VLMs (2023-2025) and observe a significant performance leap: while earlier models struggle, frontier models achieve high accuracy (>93%). However, significant limitations persist: models struggle with false belief reasoning (19-83% accuracy) and show high regional variance (20-30% gaps). Crucially, we find that SOTA models exhibit social desirability bias - systematically favoring semantically positive answer choices over negative ones. Ablation experiments reveal that some frontier models rely heavily on parametric social priors, frequently defaulting to safety-aligned predictions. Furthermore, while Chain-of-Thought prompting aids older models, it yields minimal gains for newer ones. Overall, our work provides a testbed for cross-cultural social reasoning, underscoring that despite architectural gains, achieving robust, visually grounded understanding remains an open challenge.", "link": "http://arxiv.org/abs/2512.17394v2", "date": "2026-01-07", "relevancy": 2.8468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Vision%20Language%20Models%20Cross-Cultural%20Theory%20of%20Mind%20Reasoners%3F&body=Title%3A%20Are%20Vision%20Language%20Models%20Cross-Cultural%20Theory%20of%20Mind%20Reasoners%3F%0AAuthor%3A%20Zabir%20Al%20Nazi%20and%20GM%20Shahariar%20and%20Md.%20Abrar%20Hossain%20and%20Wei%20Peng%0AAbstract%3A%20Theory%20of%20Mind%20%28ToM%29%20-%20the%20ability%20to%20attribute%20beliefs%20and%20intents%20to%20others%20-%20is%20fundamental%20for%20social%20intelligence%2C%20yet%20Vision-Language%20Model%20%28VLM%29%20evaluations%20remain%20largely%20Western-centric.%20In%20this%20work%2C%20we%20introduce%20CulturalToM-VQA%2C%20a%20benchmark%20of%205%2C095%20visually%20situated%20ToM%20probes%20across%20diverse%20cultural%20contexts%2C%20rituals%2C%20and%20social%20norms.%20Constructed%20through%20a%20frontier%20proprietary%20MLLM%2C%20human-verified%20pipeline%2C%20the%20dataset%20spans%20a%20taxonomy%20of%20six%20ToM%20tasks%20and%20four%20complexity%20levels.%20We%20benchmark%2010%20VLMs%20%282023-2025%29%20and%20observe%20a%20significant%20performance%20leap%3A%20while%20earlier%20models%20struggle%2C%20frontier%20models%20achieve%20high%20accuracy%20%28%3E93%25%29.%20However%2C%20significant%20limitations%20persist%3A%20models%20struggle%20with%20false%20belief%20reasoning%20%2819-83%25%20accuracy%29%20and%20show%20high%20regional%20variance%20%2820-30%25%20gaps%29.%20Crucially%2C%20we%20find%20that%20SOTA%20models%20exhibit%20social%20desirability%20bias%20-%20systematically%20favoring%20semantically%20positive%20answer%20choices%20over%20negative%20ones.%20Ablation%20experiments%20reveal%20that%20some%20frontier%20models%20rely%20heavily%20on%20parametric%20social%20priors%2C%20frequently%20defaulting%20to%20safety-aligned%20predictions.%20Furthermore%2C%20while%20Chain-of-Thought%20prompting%20aids%20older%20models%2C%20it%20yields%20minimal%20gains%20for%20newer%20ones.%20Overall%2C%20our%20work%20provides%20a%20testbed%20for%20cross-cultural%20social%20reasoning%2C%20underscoring%20that%20despite%20architectural%20gains%2C%20achieving%20robust%2C%20visually%20grounded%20understanding%20remains%20an%20open%20challenge.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Vision%2520Language%2520Models%2520Cross-Cultural%2520Theory%2520of%2520Mind%2520Reasoners%253F%26entry.906535625%3DZabir%2520Al%2520Nazi%2520and%2520GM%2520Shahariar%2520and%2520Md.%2520Abrar%2520Hossain%2520and%2520Wei%2520Peng%26entry.1292438233%3DTheory%2520of%2520Mind%2520%2528ToM%2529%2520-%2520the%2520ability%2520to%2520attribute%2520beliefs%2520and%2520intents%2520to%2520others%2520-%2520is%2520fundamental%2520for%2520social%2520intelligence%252C%2520yet%2520Vision-Language%2520Model%2520%2528VLM%2529%2520evaluations%2520remain%2520largely%2520Western-centric.%2520In%2520this%2520work%252C%2520we%2520introduce%2520CulturalToM-VQA%252C%2520a%2520benchmark%2520of%25205%252C095%2520visually%2520situated%2520ToM%2520probes%2520across%2520diverse%2520cultural%2520contexts%252C%2520rituals%252C%2520and%2520social%2520norms.%2520Constructed%2520through%2520a%2520frontier%2520proprietary%2520MLLM%252C%2520human-verified%2520pipeline%252C%2520the%2520dataset%2520spans%2520a%2520taxonomy%2520of%2520six%2520ToM%2520tasks%2520and%2520four%2520complexity%2520levels.%2520We%2520benchmark%252010%2520VLMs%2520%25282023-2025%2529%2520and%2520observe%2520a%2520significant%2520performance%2520leap%253A%2520while%2520earlier%2520models%2520struggle%252C%2520frontier%2520models%2520achieve%2520high%2520accuracy%2520%2528%253E93%2525%2529.%2520However%252C%2520significant%2520limitations%2520persist%253A%2520models%2520struggle%2520with%2520false%2520belief%2520reasoning%2520%252819-83%2525%2520accuracy%2529%2520and%2520show%2520high%2520regional%2520variance%2520%252820-30%2525%2520gaps%2529.%2520Crucially%252C%2520we%2520find%2520that%2520SOTA%2520models%2520exhibit%2520social%2520desirability%2520bias%2520-%2520systematically%2520favoring%2520semantically%2520positive%2520answer%2520choices%2520over%2520negative%2520ones.%2520Ablation%2520experiments%2520reveal%2520that%2520some%2520frontier%2520models%2520rely%2520heavily%2520on%2520parametric%2520social%2520priors%252C%2520frequently%2520defaulting%2520to%2520safety-aligned%2520predictions.%2520Furthermore%252C%2520while%2520Chain-of-Thought%2520prompting%2520aids%2520older%2520models%252C%2520it%2520yields%2520minimal%2520gains%2520for%2520newer%2520ones.%2520Overall%252C%2520our%2520work%2520provides%2520a%2520testbed%2520for%2520cross-cultural%2520social%2520reasoning%252C%2520underscoring%2520that%2520despite%2520architectural%2520gains%252C%2520achieving%2520robust%252C%2520visually%2520grounded%2520understanding%2520remains%2520an%2520open%2520challenge.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Vision%20Language%20Models%20Cross-Cultural%20Theory%20of%20Mind%20Reasoners%3F&entry.906535625=Zabir%20Al%20Nazi%20and%20GM%20Shahariar%20and%20Md.%20Abrar%20Hossain%20and%20Wei%20Peng&entry.1292438233=Theory%20of%20Mind%20%28ToM%29%20-%20the%20ability%20to%20attribute%20beliefs%20and%20intents%20to%20others%20-%20is%20fundamental%20for%20social%20intelligence%2C%20yet%20Vision-Language%20Model%20%28VLM%29%20evaluations%20remain%20largely%20Western-centric.%20In%20this%20work%2C%20we%20introduce%20CulturalToM-VQA%2C%20a%20benchmark%20of%205%2C095%20visually%20situated%20ToM%20probes%20across%20diverse%20cultural%20contexts%2C%20rituals%2C%20and%20social%20norms.%20Constructed%20through%20a%20frontier%20proprietary%20MLLM%2C%20human-verified%20pipeline%2C%20the%20dataset%20spans%20a%20taxonomy%20of%20six%20ToM%20tasks%20and%20four%20complexity%20levels.%20We%20benchmark%2010%20VLMs%20%282023-2025%29%20and%20observe%20a%20significant%20performance%20leap%3A%20while%20earlier%20models%20struggle%2C%20frontier%20models%20achieve%20high%20accuracy%20%28%3E93%25%29.%20However%2C%20significant%20limitations%20persist%3A%20models%20struggle%20with%20false%20belief%20reasoning%20%2819-83%25%20accuracy%29%20and%20show%20high%20regional%20variance%20%2820-30%25%20gaps%29.%20Crucially%2C%20we%20find%20that%20SOTA%20models%20exhibit%20social%20desirability%20bias%20-%20systematically%20favoring%20semantically%20positive%20answer%20choices%20over%20negative%20ones.%20Ablation%20experiments%20reveal%20that%20some%20frontier%20models%20rely%20heavily%20on%20parametric%20social%20priors%2C%20frequently%20defaulting%20to%20safety-aligned%20predictions.%20Furthermore%2C%20while%20Chain-of-Thought%20prompting%20aids%20older%20models%2C%20it%20yields%20minimal%20gains%20for%20newer%20ones.%20Overall%2C%20our%20work%20provides%20a%20testbed%20for%20cross-cultural%20social%20reasoning%2C%20underscoring%20that%20despite%20architectural%20gains%2C%20achieving%20robust%2C%20visually%20grounded%20understanding%20remains%20an%20open%20challenge.&entry.1838667208=http%3A//arxiv.org/abs/2512.17394v2&entry.124074799=Read"},
{"title": "GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning", "author": "Wenshuai Li and Xiantai Xiang and Zixiao Wen and Guangyao Zhou and Ben Niu and Feng Wang and Lijia Huang and Qiantong Wang and Yuxin Hu", "abstract": "The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.", "link": "http://arxiv.org/abs/2601.04118v1", "date": "2026-01-07", "relevancy": 2.834, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoReason%3A%20Aligning%20Thinking%20And%20Answering%20In%20Remote%20Sensing%20Vision-Language%20Models%20Via%20Logical%20Consistency%20Reinforcement%20Learning&body=Title%3A%20GeoReason%3A%20Aligning%20Thinking%20And%20Answering%20In%20Remote%20Sensing%20Vision-Language%20Models%20Via%20Logical%20Consistency%20Reinforcement%20Learning%0AAuthor%3A%20Wenshuai%20Li%20and%20Xiantai%20Xiang%20and%20Zixiao%20Wen%20and%20Guangyao%20Zhou%20and%20Ben%20Niu%20and%20Feng%20Wang%20and%20Lijia%20Huang%20and%20Qiantong%20Wang%20and%20Yuxin%20Hu%0AAbstract%3A%20The%20evolution%20of%20Remote%20Sensing%20Vision-Language%20Models%28RS-VLMs%29%20emphasizes%20the%20importance%20of%20transitioning%20from%20perception-centric%20recognition%20toward%20high-level%20deductive%20reasoning%20to%20enhance%20cognitive%20reliability%20in%20complex%20spatial%20tasks.%20However%2C%20current%20models%20often%20suffer%20from%20logical%20hallucinations%2C%20where%20correct%20answers%20are%20derived%20from%20flawed%20reasoning%20chains%20or%20rely%20on%20positional%20shortcuts%20rather%20than%20spatial%20logic.%20This%20decoupling%20undermines%20reliability%20in%20strategic%20spatial%20decision-making.%20To%20address%20this%2C%20we%20present%20GeoReason%2C%20a%20framework%20designed%20to%20synchronize%20internal%20thinking%20with%20final%20decisions.%20We%20first%20construct%20GeoReason-Bench%2C%20a%20logic-driven%20dataset%20containing%204%2C000%20reasoning%20trajectories%20synthesized%20from%20geometric%20primitives%20and%20expert%20knowledge.%20We%20then%20formulate%20a%20two-stage%20training%20strategy%3A%20%281%29%20Supervised%20Knowledge%20Initialization%20to%20equip%20the%20model%20with%20reasoning%20syntax%20and%20domain%20expertise%2C%20and%20%282%29%20Consistency-Aware%20Reinforcement%20Learning%20to%20refine%20deductive%20reliability.%20This%20second%20stage%20integrates%20a%20novel%20Logical%20Consistency%20Reward%2C%20which%20penalizes%20logical%20drift%20via%20an%20option%20permutation%20strategy%20to%20anchor%20decisions%20in%20verifiable%20reasoning%20traces.%20Experimental%20results%20demonstrate%20that%20our%20framework%20significantly%20enhances%20the%20cognitive%20reliability%20and%20interpretability%20of%20RS-VLMs%2C%20achieving%20state-of-the-art%20performance%20compared%20to%20other%20advanced%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoReason%253A%2520Aligning%2520Thinking%2520And%2520Answering%2520In%2520Remote%2520Sensing%2520Vision-Language%2520Models%2520Via%2520Logical%2520Consistency%2520Reinforcement%2520Learning%26entry.906535625%3DWenshuai%2520Li%2520and%2520Xiantai%2520Xiang%2520and%2520Zixiao%2520Wen%2520and%2520Guangyao%2520Zhou%2520and%2520Ben%2520Niu%2520and%2520Feng%2520Wang%2520and%2520Lijia%2520Huang%2520and%2520Qiantong%2520Wang%2520and%2520Yuxin%2520Hu%26entry.1292438233%3DThe%2520evolution%2520of%2520Remote%2520Sensing%2520Vision-Language%2520Models%2528RS-VLMs%2529%2520emphasizes%2520the%2520importance%2520of%2520transitioning%2520from%2520perception-centric%2520recognition%2520toward%2520high-level%2520deductive%2520reasoning%2520to%2520enhance%2520cognitive%2520reliability%2520in%2520complex%2520spatial%2520tasks.%2520However%252C%2520current%2520models%2520often%2520suffer%2520from%2520logical%2520hallucinations%252C%2520where%2520correct%2520answers%2520are%2520derived%2520from%2520flawed%2520reasoning%2520chains%2520or%2520rely%2520on%2520positional%2520shortcuts%2520rather%2520than%2520spatial%2520logic.%2520This%2520decoupling%2520undermines%2520reliability%2520in%2520strategic%2520spatial%2520decision-making.%2520To%2520address%2520this%252C%2520we%2520present%2520GeoReason%252C%2520a%2520framework%2520designed%2520to%2520synchronize%2520internal%2520thinking%2520with%2520final%2520decisions.%2520We%2520first%2520construct%2520GeoReason-Bench%252C%2520a%2520logic-driven%2520dataset%2520containing%25204%252C000%2520reasoning%2520trajectories%2520synthesized%2520from%2520geometric%2520primitives%2520and%2520expert%2520knowledge.%2520We%2520then%2520formulate%2520a%2520two-stage%2520training%2520strategy%253A%2520%25281%2529%2520Supervised%2520Knowledge%2520Initialization%2520to%2520equip%2520the%2520model%2520with%2520reasoning%2520syntax%2520and%2520domain%2520expertise%252C%2520and%2520%25282%2529%2520Consistency-Aware%2520Reinforcement%2520Learning%2520to%2520refine%2520deductive%2520reliability.%2520This%2520second%2520stage%2520integrates%2520a%2520novel%2520Logical%2520Consistency%2520Reward%252C%2520which%2520penalizes%2520logical%2520drift%2520via%2520an%2520option%2520permutation%2520strategy%2520to%2520anchor%2520decisions%2520in%2520verifiable%2520reasoning%2520traces.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520framework%2520significantly%2520enhances%2520the%2520cognitive%2520reliability%2520and%2520interpretability%2520of%2520RS-VLMs%252C%2520achieving%2520state-of-the-art%2520performance%2520compared%2520to%2520other%2520advanced%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoReason%3A%20Aligning%20Thinking%20And%20Answering%20In%20Remote%20Sensing%20Vision-Language%20Models%20Via%20Logical%20Consistency%20Reinforcement%20Learning&entry.906535625=Wenshuai%20Li%20and%20Xiantai%20Xiang%20and%20Zixiao%20Wen%20and%20Guangyao%20Zhou%20and%20Ben%20Niu%20and%20Feng%20Wang%20and%20Lijia%20Huang%20and%20Qiantong%20Wang%20and%20Yuxin%20Hu&entry.1292438233=The%20evolution%20of%20Remote%20Sensing%20Vision-Language%20Models%28RS-VLMs%29%20emphasizes%20the%20importance%20of%20transitioning%20from%20perception-centric%20recognition%20toward%20high-level%20deductive%20reasoning%20to%20enhance%20cognitive%20reliability%20in%20complex%20spatial%20tasks.%20However%2C%20current%20models%20often%20suffer%20from%20logical%20hallucinations%2C%20where%20correct%20answers%20are%20derived%20from%20flawed%20reasoning%20chains%20or%20rely%20on%20positional%20shortcuts%20rather%20than%20spatial%20logic.%20This%20decoupling%20undermines%20reliability%20in%20strategic%20spatial%20decision-making.%20To%20address%20this%2C%20we%20present%20GeoReason%2C%20a%20framework%20designed%20to%20synchronize%20internal%20thinking%20with%20final%20decisions.%20We%20first%20construct%20GeoReason-Bench%2C%20a%20logic-driven%20dataset%20containing%204%2C000%20reasoning%20trajectories%20synthesized%20from%20geometric%20primitives%20and%20expert%20knowledge.%20We%20then%20formulate%20a%20two-stage%20training%20strategy%3A%20%281%29%20Supervised%20Knowledge%20Initialization%20to%20equip%20the%20model%20with%20reasoning%20syntax%20and%20domain%20expertise%2C%20and%20%282%29%20Consistency-Aware%20Reinforcement%20Learning%20to%20refine%20deductive%20reliability.%20This%20second%20stage%20integrates%20a%20novel%20Logical%20Consistency%20Reward%2C%20which%20penalizes%20logical%20drift%20via%20an%20option%20permutation%20strategy%20to%20anchor%20decisions%20in%20verifiable%20reasoning%20traces.%20Experimental%20results%20demonstrate%20that%20our%20framework%20significantly%20enhances%20the%20cognitive%20reliability%20and%20interpretability%20of%20RS-VLMs%2C%20achieving%20state-of-the-art%20performance%20compared%20to%20other%20advanced%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.04118v1&entry.124074799=Read"},
{"title": "PM4Bench: Benchmarking Large Vision-Language Models with Parallel Multilingual Multi-Modal Multi-task Corpus", "author": "Junyuan Gao and Jiahe Song and Jiang Wu and Runchuan Zhu and Guanlin Shen and Shasha Wang and Xingjian Wei and Haote Yang and Songyang Zhang and Weijia Li and Bin Wang and Dahua Lin and Lijun Wu and Conghui He", "abstract": "While Large Vision-Language Models (LVLMs) demonstrate promising multilingual capabilities, their evaluation is currently hindered by two critical limitations: (1) the use of non-parallel corpora, which conflates inherent language capability gaps with dataset artifacts, precluding a fair assessment of cross-lingual alignment; and (2) disjointed multimodal inputs, which deviate from real-world scenarios where most texts are embedded within visual contexts. To address these challenges, we propose PM4Bench, the first Multilingual Multi-Modal Multi-task Benchmark constructed on a strictly parallel corpus across 10 languages. By eliminating content divergence, our benchmark enables a fair comparison of model capabilities across different languages. We also introduce a vision setting where textual queries are visually fused into images, compelling models to jointly \"see,\" \"read,\" and \"think\". Extensive evaluation of 10 LVLMs uncover a substantial performance drop in the Vision setting compared to standard inputs. Further analysis reveals that OCR capability is not only a general bottleneck but also contributes to cross-lingual performance disparities, suggesting that improving multilingual OCR is essential for advancing LVLM performance. We will release PM4Bench at https://github.com/opendatalab/PM4Bench .", "link": "http://arxiv.org/abs/2503.18484v2", "date": "2026-01-07", "relevancy": 2.8295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5846}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PM4Bench%3A%20Benchmarking%20Large%20Vision-Language%20Models%20with%20Parallel%20Multilingual%20Multi-Modal%20Multi-task%20Corpus&body=Title%3A%20PM4Bench%3A%20Benchmarking%20Large%20Vision-Language%20Models%20with%20Parallel%20Multilingual%20Multi-Modal%20Multi-task%20Corpus%0AAuthor%3A%20Junyuan%20Gao%20and%20Jiahe%20Song%20and%20Jiang%20Wu%20and%20Runchuan%20Zhu%20and%20Guanlin%20Shen%20and%20Shasha%20Wang%20and%20Xingjian%20Wei%20and%20Haote%20Yang%20and%20Songyang%20Zhang%20and%20Weijia%20Li%20and%20Bin%20Wang%20and%20Dahua%20Lin%20and%20Lijun%20Wu%20and%20Conghui%20He%0AAbstract%3A%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20demonstrate%20promising%20multilingual%20capabilities%2C%20their%20evaluation%20is%20currently%20hindered%20by%20two%20critical%20limitations%3A%20%281%29%20the%20use%20of%20non-parallel%20corpora%2C%20which%20conflates%20inherent%20language%20capability%20gaps%20with%20dataset%20artifacts%2C%20precluding%20a%20fair%20assessment%20of%20cross-lingual%20alignment%3B%20and%20%282%29%20disjointed%20multimodal%20inputs%2C%20which%20deviate%20from%20real-world%20scenarios%20where%20most%20texts%20are%20embedded%20within%20visual%20contexts.%20To%20address%20these%20challenges%2C%20we%20propose%20PM4Bench%2C%20the%20first%20Multilingual%20Multi-Modal%20Multi-task%20Benchmark%20constructed%20on%20a%20strictly%20parallel%20corpus%20across%2010%20languages.%20By%20eliminating%20content%20divergence%2C%20our%20benchmark%20enables%20a%20fair%20comparison%20of%20model%20capabilities%20across%20different%20languages.%20We%20also%20introduce%20a%20vision%20setting%20where%20textual%20queries%20are%20visually%20fused%20into%20images%2C%20compelling%20models%20to%20jointly%20%22see%2C%22%20%22read%2C%22%20and%20%22think%22.%20Extensive%20evaluation%20of%2010%20LVLMs%20uncover%20a%20substantial%20performance%20drop%20in%20the%20Vision%20setting%20compared%20to%20standard%20inputs.%20Further%20analysis%20reveals%20that%20OCR%20capability%20is%20not%20only%20a%20general%20bottleneck%20but%20also%20contributes%20to%20cross-lingual%20performance%20disparities%2C%20suggesting%20that%20improving%20multilingual%20OCR%20is%20essential%20for%20advancing%20LVLM%20performance.%20We%20will%20release%20PM4Bench%20at%20https%3A//github.com/opendatalab/PM4Bench%20.%0ALink%3A%20http%3A//arxiv.org/abs/2503.18484v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPM4Bench%253A%2520Benchmarking%2520Large%2520Vision-Language%2520Models%2520with%2520Parallel%2520Multilingual%2520Multi-Modal%2520Multi-task%2520Corpus%26entry.906535625%3DJunyuan%2520Gao%2520and%2520Jiahe%2520Song%2520and%2520Jiang%2520Wu%2520and%2520Runchuan%2520Zhu%2520and%2520Guanlin%2520Shen%2520and%2520Shasha%2520Wang%2520and%2520Xingjian%2520Wei%2520and%2520Haote%2520Yang%2520and%2520Songyang%2520Zhang%2520and%2520Weijia%2520Li%2520and%2520Bin%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Lijun%2520Wu%2520and%2520Conghui%2520He%26entry.1292438233%3DWhile%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520demonstrate%2520promising%2520multilingual%2520capabilities%252C%2520their%2520evaluation%2520is%2520currently%2520hindered%2520by%2520two%2520critical%2520limitations%253A%2520%25281%2529%2520the%2520use%2520of%2520non-parallel%2520corpora%252C%2520which%2520conflates%2520inherent%2520language%2520capability%2520gaps%2520with%2520dataset%2520artifacts%252C%2520precluding%2520a%2520fair%2520assessment%2520of%2520cross-lingual%2520alignment%253B%2520and%2520%25282%2529%2520disjointed%2520multimodal%2520inputs%252C%2520which%2520deviate%2520from%2520real-world%2520scenarios%2520where%2520most%2520texts%2520are%2520embedded%2520within%2520visual%2520contexts.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520PM4Bench%252C%2520the%2520first%2520Multilingual%2520Multi-Modal%2520Multi-task%2520Benchmark%2520constructed%2520on%2520a%2520strictly%2520parallel%2520corpus%2520across%252010%2520languages.%2520By%2520eliminating%2520content%2520divergence%252C%2520our%2520benchmark%2520enables%2520a%2520fair%2520comparison%2520of%2520model%2520capabilities%2520across%2520different%2520languages.%2520We%2520also%2520introduce%2520a%2520vision%2520setting%2520where%2520textual%2520queries%2520are%2520visually%2520fused%2520into%2520images%252C%2520compelling%2520models%2520to%2520jointly%2520%2522see%252C%2522%2520%2522read%252C%2522%2520and%2520%2522think%2522.%2520Extensive%2520evaluation%2520of%252010%2520LVLMs%2520uncover%2520a%2520substantial%2520performance%2520drop%2520in%2520the%2520Vision%2520setting%2520compared%2520to%2520standard%2520inputs.%2520Further%2520analysis%2520reveals%2520that%2520OCR%2520capability%2520is%2520not%2520only%2520a%2520general%2520bottleneck%2520but%2520also%2520contributes%2520to%2520cross-lingual%2520performance%2520disparities%252C%2520suggesting%2520that%2520improving%2520multilingual%2520OCR%2520is%2520essential%2520for%2520advancing%2520LVLM%2520performance.%2520We%2520will%2520release%2520PM4Bench%2520at%2520https%253A//github.com/opendatalab/PM4Bench%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18484v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PM4Bench%3A%20Benchmarking%20Large%20Vision-Language%20Models%20with%20Parallel%20Multilingual%20Multi-Modal%20Multi-task%20Corpus&entry.906535625=Junyuan%20Gao%20and%20Jiahe%20Song%20and%20Jiang%20Wu%20and%20Runchuan%20Zhu%20and%20Guanlin%20Shen%20and%20Shasha%20Wang%20and%20Xingjian%20Wei%20and%20Haote%20Yang%20and%20Songyang%20Zhang%20and%20Weijia%20Li%20and%20Bin%20Wang%20and%20Dahua%20Lin%20and%20Lijun%20Wu%20and%20Conghui%20He&entry.1292438233=While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20demonstrate%20promising%20multilingual%20capabilities%2C%20their%20evaluation%20is%20currently%20hindered%20by%20two%20critical%20limitations%3A%20%281%29%20the%20use%20of%20non-parallel%20corpora%2C%20which%20conflates%20inherent%20language%20capability%20gaps%20with%20dataset%20artifacts%2C%20precluding%20a%20fair%20assessment%20of%20cross-lingual%20alignment%3B%20and%20%282%29%20disjointed%20multimodal%20inputs%2C%20which%20deviate%20from%20real-world%20scenarios%20where%20most%20texts%20are%20embedded%20within%20visual%20contexts.%20To%20address%20these%20challenges%2C%20we%20propose%20PM4Bench%2C%20the%20first%20Multilingual%20Multi-Modal%20Multi-task%20Benchmark%20constructed%20on%20a%20strictly%20parallel%20corpus%20across%2010%20languages.%20By%20eliminating%20content%20divergence%2C%20our%20benchmark%20enables%20a%20fair%20comparison%20of%20model%20capabilities%20across%20different%20languages.%20We%20also%20introduce%20a%20vision%20setting%20where%20textual%20queries%20are%20visually%20fused%20into%20images%2C%20compelling%20models%20to%20jointly%20%22see%2C%22%20%22read%2C%22%20and%20%22think%22.%20Extensive%20evaluation%20of%2010%20LVLMs%20uncover%20a%20substantial%20performance%20drop%20in%20the%20Vision%20setting%20compared%20to%20standard%20inputs.%20Further%20analysis%20reveals%20that%20OCR%20capability%20is%20not%20only%20a%20general%20bottleneck%20but%20also%20contributes%20to%20cross-lingual%20performance%20disparities%2C%20suggesting%20that%20improving%20multilingual%20OCR%20is%20essential%20for%20advancing%20LVLM%20performance.%20We%20will%20release%20PM4Bench%20at%20https%3A//github.com/opendatalab/PM4Bench%20.&entry.1838667208=http%3A//arxiv.org/abs/2503.18484v2&entry.124074799=Read"},
{"title": "Semantic-E2VID: a Semantic-Enriched Paradigm for Event-to-Video Reconstruction", "author": "Jingqian Wu and Yunbo Jia and Shengpeng Xu and Edmund Y. Lam", "abstract": "Event cameras provide a promising sensing modality for high-speed and high-dynamic-range vision by asynchronously capturing brightness changes. A fundamental task in event-based vision is event-to-video (E2V) reconstruction, which aims to recover intensity videos from event streams. Most existing E2V approaches formulate reconstruction as a temporal--spatial signal recovery problem, relying on temporal aggregation and spatial feature learning to infer intensity frames. While effective to some extent, this formulation overlooks a critical limitation of event data: due to the change-driven sensing mechanism, event streams are inherently semantically under-determined, lacking object-level structure and contextual information that are essential for faithful reconstruction. In this work, we revisit E2V from a semantic perspective and argue that effective reconstruction requires going beyond temporal and spatial modeling to explicitly account for missing semantic information. Based on this insight, we propose \\textit{Semantic-E2VID}, a semantic-enriched end-to-end E2V framework that reformulates reconstruction as a process of semantic learning, fusing and decoding. Our approach first performs semantic abstraction by bridging event representations with semantics extracted from a pretrained Segment Anything Model (SAM), while avoiding modality-induced feature drift. The learned semantics are then fused into the event latent space in a representation-compatible manner, enabling event features to capture object-level structure and contextual cues. Furthermore, semantic-aware supervision is introduced to explicitly guide the reconstruction process toward semantically meaningful regions, complementing conventional pixel-level and temporal objectives. Extensive experiments on six public benchmarks demonstrate that Semantic-E2VID consistently outperforms state-of-the-art E2V methods.", "link": "http://arxiv.org/abs/2510.17347v2", "date": "2026-01-07", "relevancy": 2.7837, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-E2VID%3A%20a%20Semantic-Enriched%20Paradigm%20for%20Event-to-Video%20Reconstruction&body=Title%3A%20Semantic-E2VID%3A%20a%20Semantic-Enriched%20Paradigm%20for%20Event-to-Video%20Reconstruction%0AAuthor%3A%20Jingqian%20Wu%20and%20Yunbo%20Jia%20and%20Shengpeng%20Xu%20and%20Edmund%20Y.%20Lam%0AAbstract%3A%20Event%20cameras%20provide%20a%20promising%20sensing%20modality%20for%20high-speed%20and%20high-dynamic-range%20vision%20by%20asynchronously%20capturing%20brightness%20changes.%20A%20fundamental%20task%20in%20event-based%20vision%20is%20event-to-video%20%28E2V%29%20reconstruction%2C%20which%20aims%20to%20recover%20intensity%20videos%20from%20event%20streams.%20Most%20existing%20E2V%20approaches%20formulate%20reconstruction%20as%20a%20temporal--spatial%20signal%20recovery%20problem%2C%20relying%20on%20temporal%20aggregation%20and%20spatial%20feature%20learning%20to%20infer%20intensity%20frames.%20While%20effective%20to%20some%20extent%2C%20this%20formulation%20overlooks%20a%20critical%20limitation%20of%20event%20data%3A%20due%20to%20the%20change-driven%20sensing%20mechanism%2C%20event%20streams%20are%20inherently%20semantically%20under-determined%2C%20lacking%20object-level%20structure%20and%20contextual%20information%20that%20are%20essential%20for%20faithful%20reconstruction.%20In%20this%20work%2C%20we%20revisit%20E2V%20from%20a%20semantic%20perspective%20and%20argue%20that%20effective%20reconstruction%20requires%20going%20beyond%20temporal%20and%20spatial%20modeling%20to%20explicitly%20account%20for%20missing%20semantic%20information.%20Based%20on%20this%20insight%2C%20we%20propose%20%5Ctextit%7BSemantic-E2VID%7D%2C%20a%20semantic-enriched%20end-to-end%20E2V%20framework%20that%20reformulates%20reconstruction%20as%20a%20process%20of%20semantic%20learning%2C%20fusing%20and%20decoding.%20Our%20approach%20first%20performs%20semantic%20abstraction%20by%20bridging%20event%20representations%20with%20semantics%20extracted%20from%20a%20pretrained%20Segment%20Anything%20Model%20%28SAM%29%2C%20while%20avoiding%20modality-induced%20feature%20drift.%20The%20learned%20semantics%20are%20then%20fused%20into%20the%20event%20latent%20space%20in%20a%20representation-compatible%20manner%2C%20enabling%20event%20features%20to%20capture%20object-level%20structure%20and%20contextual%20cues.%20Furthermore%2C%20semantic-aware%20supervision%20is%20introduced%20to%20explicitly%20guide%20the%20reconstruction%20process%20toward%20semantically%20meaningful%20regions%2C%20complementing%20conventional%20pixel-level%20and%20temporal%20objectives.%20Extensive%20experiments%20on%20six%20public%20benchmarks%20demonstrate%20that%20Semantic-E2VID%20consistently%20outperforms%20state-of-the-art%20E2V%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2510.17347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-E2VID%253A%2520a%2520Semantic-Enriched%2520Paradigm%2520for%2520Event-to-Video%2520Reconstruction%26entry.906535625%3DJingqian%2520Wu%2520and%2520Yunbo%2520Jia%2520and%2520Shengpeng%2520Xu%2520and%2520Edmund%2520Y.%2520Lam%26entry.1292438233%3DEvent%2520cameras%2520provide%2520a%2520promising%2520sensing%2520modality%2520for%2520high-speed%2520and%2520high-dynamic-range%2520vision%2520by%2520asynchronously%2520capturing%2520brightness%2520changes.%2520A%2520fundamental%2520task%2520in%2520event-based%2520vision%2520is%2520event-to-video%2520%2528E2V%2529%2520reconstruction%252C%2520which%2520aims%2520to%2520recover%2520intensity%2520videos%2520from%2520event%2520streams.%2520Most%2520existing%2520E2V%2520approaches%2520formulate%2520reconstruction%2520as%2520a%2520temporal--spatial%2520signal%2520recovery%2520problem%252C%2520relying%2520on%2520temporal%2520aggregation%2520and%2520spatial%2520feature%2520learning%2520to%2520infer%2520intensity%2520frames.%2520While%2520effective%2520to%2520some%2520extent%252C%2520this%2520formulation%2520overlooks%2520a%2520critical%2520limitation%2520of%2520event%2520data%253A%2520due%2520to%2520the%2520change-driven%2520sensing%2520mechanism%252C%2520event%2520streams%2520are%2520inherently%2520semantically%2520under-determined%252C%2520lacking%2520object-level%2520structure%2520and%2520contextual%2520information%2520that%2520are%2520essential%2520for%2520faithful%2520reconstruction.%2520In%2520this%2520work%252C%2520we%2520revisit%2520E2V%2520from%2520a%2520semantic%2520perspective%2520and%2520argue%2520that%2520effective%2520reconstruction%2520requires%2520going%2520beyond%2520temporal%2520and%2520spatial%2520modeling%2520to%2520explicitly%2520account%2520for%2520missing%2520semantic%2520information.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520%255Ctextit%257BSemantic-E2VID%257D%252C%2520a%2520semantic-enriched%2520end-to-end%2520E2V%2520framework%2520that%2520reformulates%2520reconstruction%2520as%2520a%2520process%2520of%2520semantic%2520learning%252C%2520fusing%2520and%2520decoding.%2520Our%2520approach%2520first%2520performs%2520semantic%2520abstraction%2520by%2520bridging%2520event%2520representations%2520with%2520semantics%2520extracted%2520from%2520a%2520pretrained%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520while%2520avoiding%2520modality-induced%2520feature%2520drift.%2520The%2520learned%2520semantics%2520are%2520then%2520fused%2520into%2520the%2520event%2520latent%2520space%2520in%2520a%2520representation-compatible%2520manner%252C%2520enabling%2520event%2520features%2520to%2520capture%2520object-level%2520structure%2520and%2520contextual%2520cues.%2520Furthermore%252C%2520semantic-aware%2520supervision%2520is%2520introduced%2520to%2520explicitly%2520guide%2520the%2520reconstruction%2520process%2520toward%2520semantically%2520meaningful%2520regions%252C%2520complementing%2520conventional%2520pixel-level%2520and%2520temporal%2520objectives.%2520Extensive%2520experiments%2520on%2520six%2520public%2520benchmarks%2520demonstrate%2520that%2520Semantic-E2VID%2520consistently%2520outperforms%2520state-of-the-art%2520E2V%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-E2VID%3A%20a%20Semantic-Enriched%20Paradigm%20for%20Event-to-Video%20Reconstruction&entry.906535625=Jingqian%20Wu%20and%20Yunbo%20Jia%20and%20Shengpeng%20Xu%20and%20Edmund%20Y.%20Lam&entry.1292438233=Event%20cameras%20provide%20a%20promising%20sensing%20modality%20for%20high-speed%20and%20high-dynamic-range%20vision%20by%20asynchronously%20capturing%20brightness%20changes.%20A%20fundamental%20task%20in%20event-based%20vision%20is%20event-to-video%20%28E2V%29%20reconstruction%2C%20which%20aims%20to%20recover%20intensity%20videos%20from%20event%20streams.%20Most%20existing%20E2V%20approaches%20formulate%20reconstruction%20as%20a%20temporal--spatial%20signal%20recovery%20problem%2C%20relying%20on%20temporal%20aggregation%20and%20spatial%20feature%20learning%20to%20infer%20intensity%20frames.%20While%20effective%20to%20some%20extent%2C%20this%20formulation%20overlooks%20a%20critical%20limitation%20of%20event%20data%3A%20due%20to%20the%20change-driven%20sensing%20mechanism%2C%20event%20streams%20are%20inherently%20semantically%20under-determined%2C%20lacking%20object-level%20structure%20and%20contextual%20information%20that%20are%20essential%20for%20faithful%20reconstruction.%20In%20this%20work%2C%20we%20revisit%20E2V%20from%20a%20semantic%20perspective%20and%20argue%20that%20effective%20reconstruction%20requires%20going%20beyond%20temporal%20and%20spatial%20modeling%20to%20explicitly%20account%20for%20missing%20semantic%20information.%20Based%20on%20this%20insight%2C%20we%20propose%20%5Ctextit%7BSemantic-E2VID%7D%2C%20a%20semantic-enriched%20end-to-end%20E2V%20framework%20that%20reformulates%20reconstruction%20as%20a%20process%20of%20semantic%20learning%2C%20fusing%20and%20decoding.%20Our%20approach%20first%20performs%20semantic%20abstraction%20by%20bridging%20event%20representations%20with%20semantics%20extracted%20from%20a%20pretrained%20Segment%20Anything%20Model%20%28SAM%29%2C%20while%20avoiding%20modality-induced%20feature%20drift.%20The%20learned%20semantics%20are%20then%20fused%20into%20the%20event%20latent%20space%20in%20a%20representation-compatible%20manner%2C%20enabling%20event%20features%20to%20capture%20object-level%20structure%20and%20contextual%20cues.%20Furthermore%2C%20semantic-aware%20supervision%20is%20introduced%20to%20explicitly%20guide%20the%20reconstruction%20process%20toward%20semantically%20meaningful%20regions%2C%20complementing%20conventional%20pixel-level%20and%20temporal%20objectives.%20Extensive%20experiments%20on%20six%20public%20benchmarks%20demonstrate%20that%20Semantic-E2VID%20consistently%20outperforms%20state-of-the-art%20E2V%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2510.17347v2&entry.124074799=Read"},
{"title": "Klear: Unified Multi-Task Audio-Video Joint Generation", "author": "Jun Wang and Chunyu Qiang and Yuxin Guo and Yiran Wang and Xijuan Zeng and Chen Zhang and Pengfei Wan", "abstract": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.", "link": "http://arxiv.org/abs/2601.04151v1", "date": "2026-01-07", "relevancy": 2.7755, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5681}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5489}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Klear%3A%20Unified%20Multi-Task%20Audio-Video%20Joint%20Generation&body=Title%3A%20Klear%3A%20Unified%20Multi-Task%20Audio-Video%20Joint%20Generation%0AAuthor%3A%20Jun%20Wang%20and%20Chunyu%20Qiang%20and%20Yuxin%20Guo%20and%20Yiran%20Wang%20and%20Xijuan%20Zeng%20and%20Chen%20Zhang%20and%20Pengfei%20Wan%0AAbstract%3A%20Audio-video%20joint%20generation%20has%20progressed%20rapidly%2C%20yet%20substantial%20challenges%20still%20remain.%20Non-commercial%20approaches%20still%20suffer%20audio-visual%20asynchrony%2C%20poor%20lip-speech%20alignment%2C%20and%20unimodal%20degradation%2C%20which%20can%20be%20stemmed%20from%20weak%20audio-visual%20correspondence%20modeling%2C%20limited%20generalization%2C%20and%20scarce%20high-quality%20dense-caption%20data.%20To%20address%20these%20issues%2C%20we%20introduce%20Klear%20and%20delve%20into%20three%20axes--model%20architecture%2C%20training%20strategy%2C%20and%20data%20curation.%20Architecturally%2C%20we%20adopt%20a%20single-tower%20design%20with%20unified%20DiT%20blocks%20and%20an%20Omni-Full%20Attention%20mechanism%2C%20achieving%20tight%20audio-visual%20alignment%20and%20strong%20scalability.%20Training-wise%2C%20we%20adopt%20a%20progressive%20multitask%20regime--random%20modality%20masking%20to%20joint%20optimization%20across%20tasks%2C%20and%20a%20multistage%20curriculum%2C%20yielding%20robust%20representations%2C%20strengthening%20A-V%20aligned%20world%20knowledge%2C%20and%20preventing%20unimodal%20collapse.%20For%20datasets%2C%20we%20present%20the%20first%20large-scale%20audio-video%20dataset%20with%20dense%20captions%2C%20and%20introduce%20a%20novel%20automated%20data-construction%20pipeline%20which%20annotates%20and%20filters%20millions%20of%20diverse%2C%20high-quality%2C%20strictly%20aligned%20audio-video-caption%20triplets.%20Building%20on%20this%2C%20Klear%20scales%20to%20large%20datasets%2C%20delivering%20high-fidelity%2C%20semantically%20and%20temporally%20aligned%2C%20instruction-following%20generation%20in%20both%20joint%20and%20unimodal%20settings%20while%20generalizing%20robustly%20to%20out-of-distribution%20scenarios.%20Across%20tasks%2C%20it%20substantially%20outperforms%20prior%20methods%20by%20a%20large%20margin%20and%20achieves%20performance%20comparable%20to%20Veo%203%2C%20offering%20a%20unified%2C%20scalable%20path%20toward%20next-generation%20audio-video%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKlear%253A%2520Unified%2520Multi-Task%2520Audio-Video%2520Joint%2520Generation%26entry.906535625%3DJun%2520Wang%2520and%2520Chunyu%2520Qiang%2520and%2520Yuxin%2520Guo%2520and%2520Yiran%2520Wang%2520and%2520Xijuan%2520Zeng%2520and%2520Chen%2520Zhang%2520and%2520Pengfei%2520Wan%26entry.1292438233%3DAudio-video%2520joint%2520generation%2520has%2520progressed%2520rapidly%252C%2520yet%2520substantial%2520challenges%2520still%2520remain.%2520Non-commercial%2520approaches%2520still%2520suffer%2520audio-visual%2520asynchrony%252C%2520poor%2520lip-speech%2520alignment%252C%2520and%2520unimodal%2520degradation%252C%2520which%2520can%2520be%2520stemmed%2520from%2520weak%2520audio-visual%2520correspondence%2520modeling%252C%2520limited%2520generalization%252C%2520and%2520scarce%2520high-quality%2520dense-caption%2520data.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Klear%2520and%2520delve%2520into%2520three%2520axes--model%2520architecture%252C%2520training%2520strategy%252C%2520and%2520data%2520curation.%2520Architecturally%252C%2520we%2520adopt%2520a%2520single-tower%2520design%2520with%2520unified%2520DiT%2520blocks%2520and%2520an%2520Omni-Full%2520Attention%2520mechanism%252C%2520achieving%2520tight%2520audio-visual%2520alignment%2520and%2520strong%2520scalability.%2520Training-wise%252C%2520we%2520adopt%2520a%2520progressive%2520multitask%2520regime--random%2520modality%2520masking%2520to%2520joint%2520optimization%2520across%2520tasks%252C%2520and%2520a%2520multistage%2520curriculum%252C%2520yielding%2520robust%2520representations%252C%2520strengthening%2520A-V%2520aligned%2520world%2520knowledge%252C%2520and%2520preventing%2520unimodal%2520collapse.%2520For%2520datasets%252C%2520we%2520present%2520the%2520first%2520large-scale%2520audio-video%2520dataset%2520with%2520dense%2520captions%252C%2520and%2520introduce%2520a%2520novel%2520automated%2520data-construction%2520pipeline%2520which%2520annotates%2520and%2520filters%2520millions%2520of%2520diverse%252C%2520high-quality%252C%2520strictly%2520aligned%2520audio-video-caption%2520triplets.%2520Building%2520on%2520this%252C%2520Klear%2520scales%2520to%2520large%2520datasets%252C%2520delivering%2520high-fidelity%252C%2520semantically%2520and%2520temporally%2520aligned%252C%2520instruction-following%2520generation%2520in%2520both%2520joint%2520and%2520unimodal%2520settings%2520while%2520generalizing%2520robustly%2520to%2520out-of-distribution%2520scenarios.%2520Across%2520tasks%252C%2520it%2520substantially%2520outperforms%2520prior%2520methods%2520by%2520a%2520large%2520margin%2520and%2520achieves%2520performance%2520comparable%2520to%2520Veo%25203%252C%2520offering%2520a%2520unified%252C%2520scalable%2520path%2520toward%2520next-generation%2520audio-video%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Klear%3A%20Unified%20Multi-Task%20Audio-Video%20Joint%20Generation&entry.906535625=Jun%20Wang%20and%20Chunyu%20Qiang%20and%20Yuxin%20Guo%20and%20Yiran%20Wang%20and%20Xijuan%20Zeng%20and%20Chen%20Zhang%20and%20Pengfei%20Wan&entry.1292438233=Audio-video%20joint%20generation%20has%20progressed%20rapidly%2C%20yet%20substantial%20challenges%20still%20remain.%20Non-commercial%20approaches%20still%20suffer%20audio-visual%20asynchrony%2C%20poor%20lip-speech%20alignment%2C%20and%20unimodal%20degradation%2C%20which%20can%20be%20stemmed%20from%20weak%20audio-visual%20correspondence%20modeling%2C%20limited%20generalization%2C%20and%20scarce%20high-quality%20dense-caption%20data.%20To%20address%20these%20issues%2C%20we%20introduce%20Klear%20and%20delve%20into%20three%20axes--model%20architecture%2C%20training%20strategy%2C%20and%20data%20curation.%20Architecturally%2C%20we%20adopt%20a%20single-tower%20design%20with%20unified%20DiT%20blocks%20and%20an%20Omni-Full%20Attention%20mechanism%2C%20achieving%20tight%20audio-visual%20alignment%20and%20strong%20scalability.%20Training-wise%2C%20we%20adopt%20a%20progressive%20multitask%20regime--random%20modality%20masking%20to%20joint%20optimization%20across%20tasks%2C%20and%20a%20multistage%20curriculum%2C%20yielding%20robust%20representations%2C%20strengthening%20A-V%20aligned%20world%20knowledge%2C%20and%20preventing%20unimodal%20collapse.%20For%20datasets%2C%20we%20present%20the%20first%20large-scale%20audio-video%20dataset%20with%20dense%20captions%2C%20and%20introduce%20a%20novel%20automated%20data-construction%20pipeline%20which%20annotates%20and%20filters%20millions%20of%20diverse%2C%20high-quality%2C%20strictly%20aligned%20audio-video-caption%20triplets.%20Building%20on%20this%2C%20Klear%20scales%20to%20large%20datasets%2C%20delivering%20high-fidelity%2C%20semantically%20and%20temporally%20aligned%2C%20instruction-following%20generation%20in%20both%20joint%20and%20unimodal%20settings%20while%20generalizing%20robustly%20to%20out-of-distribution%20scenarios.%20Across%20tasks%2C%20it%20substantially%20outperforms%20prior%20methods%20by%20a%20large%20margin%20and%20achieves%20performance%20comparable%20to%20Veo%203%2C%20offering%20a%20unified%2C%20scalable%20path%20toward%20next-generation%20audio-video%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2601.04151v1&entry.124074799=Read"},
{"title": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding", "author": "Shengyuan Ye and Bei Ouyang and Tianyi Qian and Liekang Zeng and Mu Yuan and Xiaowen Chu and Weijie Hong and Xu Chen", "abstract": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.", "link": "http://arxiv.org/abs/2512.07344v2", "date": "2026-01-07", "relevancy": 2.7238, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Venus%3A%20An%20Efficient%20Edge%20Memory-and-Retrieval%20System%20for%20VLM-based%20Online%20Video%20Understanding&body=Title%3A%20Venus%3A%20An%20Efficient%20Edge%20Memory-and-Retrieval%20System%20for%20VLM-based%20Online%20Video%20Understanding%0AAuthor%3A%20Shengyuan%20Ye%20and%20Bei%20Ouyang%20and%20Tianyi%20Qian%20and%20Liekang%20Zeng%20and%20Mu%20Yuan%20and%20Xiaowen%20Chu%20and%20Weijie%20Hong%20and%20Xu%20Chen%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20impressive%20multimodal%20comprehension%20capabilities%20and%20are%20being%20deployed%20in%20an%20increasing%20number%20of%20online%20video%20understanding%20applications.%20While%20recent%20efforts%20extensively%20explore%20advancing%20VLMs%27%20reasoning%20power%20in%20these%20cases%2C%20deployment%20constraints%20are%20overlooked%2C%20leading%20to%20overwhelming%20system%20overhead%20in%20real-world%20deployments.%20To%20address%20that%2C%20we%20propose%20Venus%2C%20an%20on-device%20memory-and-retrieval%20system%20for%20efficient%20online%20video%20understanding.%20Venus%20proposes%20an%20edge-cloud%20disaggregated%20architecture%20that%20sinks%20memory%20construction%20and%20keyframe%20retrieval%20from%20cloud%20to%20edge%2C%20operating%20in%20two%20stages.%20In%20the%20ingestion%20stage%2C%20Venus%20continuously%20processes%20streaming%20edge%20videos%20via%20scene%20segmentation%20and%20clustering%2C%20where%20the%20selected%20keyframes%20are%20embedded%20with%20a%20multimodal%20embedding%20model%20to%20build%20a%20hierarchical%20memory%20for%20efficient%20storage%20and%20retrieval.%20In%20the%20querying%20stage%2C%20Venus%20indexes%20incoming%20queries%20from%20memory%2C%20and%20employs%20a%20threshold-based%20progressive%20sampling%20algorithm%20for%20keyframe%20selection%20that%20enhances%20diversity%20and%20adaptively%20balances%20system%20cost%20and%20reasoning%20accuracy.%20Our%20extensive%20evaluation%20shows%20that%20Venus%20achieves%20a%2015x-131x%20speedup%20in%20total%20response%20latency%20compared%20to%20state-of-the-art%20methods%2C%20enabling%20real-time%20responses%20within%20seconds%20while%20maintaining%20comparable%20or%20even%20superior%20reasoning%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVenus%253A%2520An%2520Efficient%2520Edge%2520Memory-and-Retrieval%2520System%2520for%2520VLM-based%2520Online%2520Video%2520Understanding%26entry.906535625%3DShengyuan%2520Ye%2520and%2520Bei%2520Ouyang%2520and%2520Tianyi%2520Qian%2520and%2520Liekang%2520Zeng%2520and%2520Mu%2520Yuan%2520and%2520Xiaowen%2520Chu%2520and%2520Weijie%2520Hong%2520and%2520Xu%2520Chen%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520impressive%2520multimodal%2520comprehension%2520capabilities%2520and%2520are%2520being%2520deployed%2520in%2520an%2520increasing%2520number%2520of%2520online%2520video%2520understanding%2520applications.%2520While%2520recent%2520efforts%2520extensively%2520explore%2520advancing%2520VLMs%2527%2520reasoning%2520power%2520in%2520these%2520cases%252C%2520deployment%2520constraints%2520are%2520overlooked%252C%2520leading%2520to%2520overwhelming%2520system%2520overhead%2520in%2520real-world%2520deployments.%2520To%2520address%2520that%252C%2520we%2520propose%2520Venus%252C%2520an%2520on-device%2520memory-and-retrieval%2520system%2520for%2520efficient%2520online%2520video%2520understanding.%2520Venus%2520proposes%2520an%2520edge-cloud%2520disaggregated%2520architecture%2520that%2520sinks%2520memory%2520construction%2520and%2520keyframe%2520retrieval%2520from%2520cloud%2520to%2520edge%252C%2520operating%2520in%2520two%2520stages.%2520In%2520the%2520ingestion%2520stage%252C%2520Venus%2520continuously%2520processes%2520streaming%2520edge%2520videos%2520via%2520scene%2520segmentation%2520and%2520clustering%252C%2520where%2520the%2520selected%2520keyframes%2520are%2520embedded%2520with%2520a%2520multimodal%2520embedding%2520model%2520to%2520build%2520a%2520hierarchical%2520memory%2520for%2520efficient%2520storage%2520and%2520retrieval.%2520In%2520the%2520querying%2520stage%252C%2520Venus%2520indexes%2520incoming%2520queries%2520from%2520memory%252C%2520and%2520employs%2520a%2520threshold-based%2520progressive%2520sampling%2520algorithm%2520for%2520keyframe%2520selection%2520that%2520enhances%2520diversity%2520and%2520adaptively%2520balances%2520system%2520cost%2520and%2520reasoning%2520accuracy.%2520Our%2520extensive%2520evaluation%2520shows%2520that%2520Venus%2520achieves%2520a%252015x-131x%2520speedup%2520in%2520total%2520response%2520latency%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520enabling%2520real-time%2520responses%2520within%2520seconds%2520while%2520maintaining%2520comparable%2520or%2520even%2520superior%2520reasoning%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Venus%3A%20An%20Efficient%20Edge%20Memory-and-Retrieval%20System%20for%20VLM-based%20Online%20Video%20Understanding&entry.906535625=Shengyuan%20Ye%20and%20Bei%20Ouyang%20and%20Tianyi%20Qian%20and%20Liekang%20Zeng%20and%20Mu%20Yuan%20and%20Xiaowen%20Chu%20and%20Weijie%20Hong%20and%20Xu%20Chen&entry.1292438233=Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20impressive%20multimodal%20comprehension%20capabilities%20and%20are%20being%20deployed%20in%20an%20increasing%20number%20of%20online%20video%20understanding%20applications.%20While%20recent%20efforts%20extensively%20explore%20advancing%20VLMs%27%20reasoning%20power%20in%20these%20cases%2C%20deployment%20constraints%20are%20overlooked%2C%20leading%20to%20overwhelming%20system%20overhead%20in%20real-world%20deployments.%20To%20address%20that%2C%20we%20propose%20Venus%2C%20an%20on-device%20memory-and-retrieval%20system%20for%20efficient%20online%20video%20understanding.%20Venus%20proposes%20an%20edge-cloud%20disaggregated%20architecture%20that%20sinks%20memory%20construction%20and%20keyframe%20retrieval%20from%20cloud%20to%20edge%2C%20operating%20in%20two%20stages.%20In%20the%20ingestion%20stage%2C%20Venus%20continuously%20processes%20streaming%20edge%20videos%20via%20scene%20segmentation%20and%20clustering%2C%20where%20the%20selected%20keyframes%20are%20embedded%20with%20a%20multimodal%20embedding%20model%20to%20build%20a%20hierarchical%20memory%20for%20efficient%20storage%20and%20retrieval.%20In%20the%20querying%20stage%2C%20Venus%20indexes%20incoming%20queries%20from%20memory%2C%20and%20employs%20a%20threshold-based%20progressive%20sampling%20algorithm%20for%20keyframe%20selection%20that%20enhances%20diversity%20and%20adaptively%20balances%20system%20cost%20and%20reasoning%20accuracy.%20Our%20extensive%20evaluation%20shows%20that%20Venus%20achieves%20a%2015x-131x%20speedup%20in%20total%20response%20latency%20compared%20to%20state-of-the-art%20methods%2C%20enabling%20real-time%20responses%20within%20seconds%20while%20maintaining%20comparable%20or%20even%20superior%20reasoning%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2512.07344v2&entry.124074799=Read"},
{"title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic Reasoning", "author": "Benjamin Callewaert and Simon Vandevelde and Joost Vennekens", "abstract": "A recent approach to neurosymbolic reasoning is to explicitly combine the strengths of large language models (LLMs) and symbolic solvers to tackle complex reasoning tasks. However, current approaches face significant limitations, including poor generalizability due to task-specific prompts, inefficiencies caused by the lack of separation between knowledge and queries, and restricted inferential capabilities. These shortcomings hinder their scalability and applicability across diverse domains. In this paper, we introduce VERUS-LM, a novel framework designed to address these challenges. VERUS-LM employs a generic prompting mechanism, clearly separates domain knowledge from queries, and supports a wide range of different logical reasoning tasks. This framework enhances adaptability, reduces computational cost, and allows for richer forms of reasoning, such as optimization and constraint satisfaction. We show that our approach succeeds in diverse reasoning on a novel dataset, markedly outperforming LLMs. Additionally, our system achieves competitive results on common reasoning benchmarks when compared to similar state-of-the-art approaches, and significantly surpasses them on the difficult AR-LSAT dataset. By pushing the boundaries of hybrid reasoning, VERUS-LM represents a significant step towards more versatile neurosymbolic AI systems.", "link": "http://arxiv.org/abs/2501.14540v3", "date": "2026-01-07", "relevancy": 2.7012, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VERUS-LM%3A%20a%20Versatile%20Framework%20for%20Combining%20LLMs%20with%20Symbolic%20Reasoning&body=Title%3A%20VERUS-LM%3A%20a%20Versatile%20Framework%20for%20Combining%20LLMs%20with%20Symbolic%20Reasoning%0AAuthor%3A%20Benjamin%20Callewaert%20and%20Simon%20Vandevelde%20and%20Joost%20Vennekens%0AAbstract%3A%20A%20recent%20approach%20to%20neurosymbolic%20reasoning%20is%20to%20explicitly%20combine%20the%20strengths%20of%20large%20language%20models%20%28LLMs%29%20and%20symbolic%20solvers%20to%20tackle%20complex%20reasoning%20tasks.%20However%2C%20current%20approaches%20face%20significant%20limitations%2C%20including%20poor%20generalizability%20due%20to%20task-specific%20prompts%2C%20inefficiencies%20caused%20by%20the%20lack%20of%20separation%20between%20knowledge%20and%20queries%2C%20and%20restricted%20inferential%20capabilities.%20These%20shortcomings%20hinder%20their%20scalability%20and%20applicability%20across%20diverse%20domains.%20In%20this%20paper%2C%20we%20introduce%20VERUS-LM%2C%20a%20novel%20framework%20designed%20to%20address%20these%20challenges.%20VERUS-LM%20employs%20a%20generic%20prompting%20mechanism%2C%20clearly%20separates%20domain%20knowledge%20from%20queries%2C%20and%20supports%20a%20wide%20range%20of%20different%20logical%20reasoning%20tasks.%20This%20framework%20enhances%20adaptability%2C%20reduces%20computational%20cost%2C%20and%20allows%20for%20richer%20forms%20of%20reasoning%2C%20such%20as%20optimization%20and%20constraint%20satisfaction.%20We%20show%20that%20our%20approach%20succeeds%20in%20diverse%20reasoning%20on%20a%20novel%20dataset%2C%20markedly%20outperforming%20LLMs.%20Additionally%2C%20our%20system%20achieves%20competitive%20results%20on%20common%20reasoning%20benchmarks%20when%20compared%20to%20similar%20state-of-the-art%20approaches%2C%20and%20significantly%20surpasses%20them%20on%20the%20difficult%20AR-LSAT%20dataset.%20By%20pushing%20the%20boundaries%20of%20hybrid%20reasoning%2C%20VERUS-LM%20represents%20a%20significant%20step%20towards%20more%20versatile%20neurosymbolic%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2501.14540v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVERUS-LM%253A%2520a%2520Versatile%2520Framework%2520for%2520Combining%2520LLMs%2520with%2520Symbolic%2520Reasoning%26entry.906535625%3DBenjamin%2520Callewaert%2520and%2520Simon%2520Vandevelde%2520and%2520Joost%2520Vennekens%26entry.1292438233%3DA%2520recent%2520approach%2520to%2520neurosymbolic%2520reasoning%2520is%2520to%2520explicitly%2520combine%2520the%2520strengths%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520symbolic%2520solvers%2520to%2520tackle%2520complex%2520reasoning%2520tasks.%2520However%252C%2520current%2520approaches%2520face%2520significant%2520limitations%252C%2520including%2520poor%2520generalizability%2520due%2520to%2520task-specific%2520prompts%252C%2520inefficiencies%2520caused%2520by%2520the%2520lack%2520of%2520separation%2520between%2520knowledge%2520and%2520queries%252C%2520and%2520restricted%2520inferential%2520capabilities.%2520These%2520shortcomings%2520hinder%2520their%2520scalability%2520and%2520applicability%2520across%2520diverse%2520domains.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VERUS-LM%252C%2520a%2520novel%2520framework%2520designed%2520to%2520address%2520these%2520challenges.%2520VERUS-LM%2520employs%2520a%2520generic%2520prompting%2520mechanism%252C%2520clearly%2520separates%2520domain%2520knowledge%2520from%2520queries%252C%2520and%2520supports%2520a%2520wide%2520range%2520of%2520different%2520logical%2520reasoning%2520tasks.%2520This%2520framework%2520enhances%2520adaptability%252C%2520reduces%2520computational%2520cost%252C%2520and%2520allows%2520for%2520richer%2520forms%2520of%2520reasoning%252C%2520such%2520as%2520optimization%2520and%2520constraint%2520satisfaction.%2520We%2520show%2520that%2520our%2520approach%2520succeeds%2520in%2520diverse%2520reasoning%2520on%2520a%2520novel%2520dataset%252C%2520markedly%2520outperforming%2520LLMs.%2520Additionally%252C%2520our%2520system%2520achieves%2520competitive%2520results%2520on%2520common%2520reasoning%2520benchmarks%2520when%2520compared%2520to%2520similar%2520state-of-the-art%2520approaches%252C%2520and%2520significantly%2520surpasses%2520them%2520on%2520the%2520difficult%2520AR-LSAT%2520dataset.%2520By%2520pushing%2520the%2520boundaries%2520of%2520hybrid%2520reasoning%252C%2520VERUS-LM%2520represents%2520a%2520significant%2520step%2520towards%2520more%2520versatile%2520neurosymbolic%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14540v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VERUS-LM%3A%20a%20Versatile%20Framework%20for%20Combining%20LLMs%20with%20Symbolic%20Reasoning&entry.906535625=Benjamin%20Callewaert%20and%20Simon%20Vandevelde%20and%20Joost%20Vennekens&entry.1292438233=A%20recent%20approach%20to%20neurosymbolic%20reasoning%20is%20to%20explicitly%20combine%20the%20strengths%20of%20large%20language%20models%20%28LLMs%29%20and%20symbolic%20solvers%20to%20tackle%20complex%20reasoning%20tasks.%20However%2C%20current%20approaches%20face%20significant%20limitations%2C%20including%20poor%20generalizability%20due%20to%20task-specific%20prompts%2C%20inefficiencies%20caused%20by%20the%20lack%20of%20separation%20between%20knowledge%20and%20queries%2C%20and%20restricted%20inferential%20capabilities.%20These%20shortcomings%20hinder%20their%20scalability%20and%20applicability%20across%20diverse%20domains.%20In%20this%20paper%2C%20we%20introduce%20VERUS-LM%2C%20a%20novel%20framework%20designed%20to%20address%20these%20challenges.%20VERUS-LM%20employs%20a%20generic%20prompting%20mechanism%2C%20clearly%20separates%20domain%20knowledge%20from%20queries%2C%20and%20supports%20a%20wide%20range%20of%20different%20logical%20reasoning%20tasks.%20This%20framework%20enhances%20adaptability%2C%20reduces%20computational%20cost%2C%20and%20allows%20for%20richer%20forms%20of%20reasoning%2C%20such%20as%20optimization%20and%20constraint%20satisfaction.%20We%20show%20that%20our%20approach%20succeeds%20in%20diverse%20reasoning%20on%20a%20novel%20dataset%2C%20markedly%20outperforming%20LLMs.%20Additionally%2C%20our%20system%20achieves%20competitive%20results%20on%20common%20reasoning%20benchmarks%20when%20compared%20to%20similar%20state-of-the-art%20approaches%2C%20and%20significantly%20surpasses%20them%20on%20the%20difficult%20AR-LSAT%20dataset.%20By%20pushing%20the%20boundaries%20of%20hybrid%20reasoning%2C%20VERUS-LM%20represents%20a%20significant%20step%20towards%20more%20versatile%20neurosymbolic%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2501.14540v3&entry.124074799=Read"},
{"title": "A Differentiable Adversarial Framework for Task-Aware Data Subsampling", "author": "Jiacheng Lyu and Bihua Bao", "abstract": "The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduce the antagonistic soft selection subsampling (ASSS) framework as a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.", "link": "http://arxiv.org/abs/2601.02081v2", "date": "2026-01-07", "relevancy": 2.6907, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5814}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5368}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Differentiable%20Adversarial%20Framework%20for%20Task-Aware%20Data%20Subsampling&body=Title%3A%20A%20Differentiable%20Adversarial%20Framework%20for%20Task-Aware%20Data%20Subsampling%0AAuthor%3A%20Jiacheng%20Lyu%20and%20Bihua%20Bao%0AAbstract%3A%20The%20proliferation%20of%20large-scale%20datasets%20poses%20a%20major%20computational%20challenge%20to%20model%20training.%20The%20traditional%20data%20subsampling%20method%20works%20as%20a%20static%2C%20task%20independent%20preprocessing%20step%20which%20usually%20discards%20information%20that%20is%20critical%20to%20downstream%20prediction.%20In%20this%20paper%2C%20we%20introduce%20the%20antagonistic%20soft%20selection%20subsampling%20%28ASSS%29%20framework%20as%20a%20novel%20paradigm%20that%20reconstructs%20data%20reduction%20into%20a%20differentiable%20end-to-end%20learning%20problem.%20ASSS%20uses%20the%20adversarial%20game%20between%20selector%20network%20and%20task%20network%2C%20and%20selector%20network%20learning%20assigns%20continuous%20importance%20weights%20to%20samples.%20This%20direct%20optimization%20implemented%20by%20Gumbel-Softmax%20relaxation%20allows%20the%20selector%20to%20identify%20and%20retain%20samples%20with%20the%20maximum%20amount%20of%20information%20for%20a%20specific%20task%20target%20under%20the%20guidance%20of%20the%20loss%20function%20that%20balances%20the%20fidelity%20and%20sparsity%20of%20the%20prediction.%20Theoretical%20analysis%20links%20this%20framework%20with%20the%20information%20bottleneck%20principle.%20Comprehensive%20experiments%20on%20four%20large-scale%20real%20world%20datasets%20show%20that%20ASSS%20has%20always%20been%20better%20than%20heuristic%20subsampling%20baselines%20such%20as%20clustering%20and%20nearest%20neighbor%20thinning%20in%20maintaining%20model%20performance.%20It%20is%20worth%20noting%20that%20ASSS%20can%20not%20only%20match%2C%20but%20also%20sometimes%20exceed%20the%20training%20performance%20of%20the%20entire%20dataset%2C%20showcasing%20the%20effect%20of%20intelligent%20denoising.%20This%20work%20establishes%20task%20aware%20data%20subsampling%20as%20a%20learnable%20component%2C%20providing%20a%20principled%20solution%20for%20effective%20large-scale%20data%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Differentiable%2520Adversarial%2520Framework%2520for%2520Task-Aware%2520Data%2520Subsampling%26entry.906535625%3DJiacheng%2520Lyu%2520and%2520Bihua%2520Bao%26entry.1292438233%3DThe%2520proliferation%2520of%2520large-scale%2520datasets%2520poses%2520a%2520major%2520computational%2520challenge%2520to%2520model%2520training.%2520The%2520traditional%2520data%2520subsampling%2520method%2520works%2520as%2520a%2520static%252C%2520task%2520independent%2520preprocessing%2520step%2520which%2520usually%2520discards%2520information%2520that%2520is%2520critical%2520to%2520downstream%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520antagonistic%2520soft%2520selection%2520subsampling%2520%2528ASSS%2529%2520framework%2520as%2520a%2520novel%2520paradigm%2520that%2520reconstructs%2520data%2520reduction%2520into%2520a%2520differentiable%2520end-to-end%2520learning%2520problem.%2520ASSS%2520uses%2520the%2520adversarial%2520game%2520between%2520selector%2520network%2520and%2520task%2520network%252C%2520and%2520selector%2520network%2520learning%2520assigns%2520continuous%2520importance%2520weights%2520to%2520samples.%2520This%2520direct%2520optimization%2520implemented%2520by%2520Gumbel-Softmax%2520relaxation%2520allows%2520the%2520selector%2520to%2520identify%2520and%2520retain%2520samples%2520with%2520the%2520maximum%2520amount%2520of%2520information%2520for%2520a%2520specific%2520task%2520target%2520under%2520the%2520guidance%2520of%2520the%2520loss%2520function%2520that%2520balances%2520the%2520fidelity%2520and%2520sparsity%2520of%2520the%2520prediction.%2520Theoretical%2520analysis%2520links%2520this%2520framework%2520with%2520the%2520information%2520bottleneck%2520principle.%2520Comprehensive%2520experiments%2520on%2520four%2520large-scale%2520real%2520world%2520datasets%2520show%2520that%2520ASSS%2520has%2520always%2520been%2520better%2520than%2520heuristic%2520subsampling%2520baselines%2520such%2520as%2520clustering%2520and%2520nearest%2520neighbor%2520thinning%2520in%2520maintaining%2520model%2520performance.%2520It%2520is%2520worth%2520noting%2520that%2520ASSS%2520can%2520not%2520only%2520match%252C%2520but%2520also%2520sometimes%2520exceed%2520the%2520training%2520performance%2520of%2520the%2520entire%2520dataset%252C%2520showcasing%2520the%2520effect%2520of%2520intelligent%2520denoising.%2520This%2520work%2520establishes%2520task%2520aware%2520data%2520subsampling%2520as%2520a%2520learnable%2520component%252C%2520providing%2520a%2520principled%2520solution%2520for%2520effective%2520large-scale%2520data%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Differentiable%20Adversarial%20Framework%20for%20Task-Aware%20Data%20Subsampling&entry.906535625=Jiacheng%20Lyu%20and%20Bihua%20Bao&entry.1292438233=The%20proliferation%20of%20large-scale%20datasets%20poses%20a%20major%20computational%20challenge%20to%20model%20training.%20The%20traditional%20data%20subsampling%20method%20works%20as%20a%20static%2C%20task%20independent%20preprocessing%20step%20which%20usually%20discards%20information%20that%20is%20critical%20to%20downstream%20prediction.%20In%20this%20paper%2C%20we%20introduce%20the%20antagonistic%20soft%20selection%20subsampling%20%28ASSS%29%20framework%20as%20a%20novel%20paradigm%20that%20reconstructs%20data%20reduction%20into%20a%20differentiable%20end-to-end%20learning%20problem.%20ASSS%20uses%20the%20adversarial%20game%20between%20selector%20network%20and%20task%20network%2C%20and%20selector%20network%20learning%20assigns%20continuous%20importance%20weights%20to%20samples.%20This%20direct%20optimization%20implemented%20by%20Gumbel-Softmax%20relaxation%20allows%20the%20selector%20to%20identify%20and%20retain%20samples%20with%20the%20maximum%20amount%20of%20information%20for%20a%20specific%20task%20target%20under%20the%20guidance%20of%20the%20loss%20function%20that%20balances%20the%20fidelity%20and%20sparsity%20of%20the%20prediction.%20Theoretical%20analysis%20links%20this%20framework%20with%20the%20information%20bottleneck%20principle.%20Comprehensive%20experiments%20on%20four%20large-scale%20real%20world%20datasets%20show%20that%20ASSS%20has%20always%20been%20better%20than%20heuristic%20subsampling%20baselines%20such%20as%20clustering%20and%20nearest%20neighbor%20thinning%20in%20maintaining%20model%20performance.%20It%20is%20worth%20noting%20that%20ASSS%20can%20not%20only%20match%2C%20but%20also%20sometimes%20exceed%20the%20training%20performance%20of%20the%20entire%20dataset%2C%20showcasing%20the%20effect%20of%20intelligent%20denoising.%20This%20work%20establishes%20task%20aware%20data%20subsampling%20as%20a%20learnable%20component%2C%20providing%20a%20principled%20solution%20for%20effective%20large-scale%20data%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2601.02081v2&entry.124074799=Read"},
{"title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "author": "Yuxi Xiao and Longfei Li and Shen Yan and Xinhang Liu and Sida Peng and Yunchao Wei and Xiaowei Zhou and Bingyi Kang", "abstract": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "link": "http://arxiv.org/abs/2512.20617v2", "date": "2026-01-07", "relevancy": 2.6804, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialTree%3A%20How%20Spatial%20Abilities%20Branch%20Out%20in%20MLLMs&body=Title%3A%20SpatialTree%3A%20How%20Spatial%20Abilities%20Branch%20Out%20in%20MLLMs%0AAuthor%3A%20Yuxi%20Xiao%20and%20Longfei%20Li%20and%20Shen%20Yan%20and%20Xinhang%20Liu%20and%20Sida%20Peng%20and%20Yunchao%20Wei%20and%20Xiaowei%20Zhou%20and%20Bingyi%20Kang%0AAbstract%3A%20Cognitive%20science%20suggests%20that%20spatial%20ability%20develops%20progressively-from%20perception%20to%20reasoning%20and%20interaction.%20Yet%20in%20multimodal%20LLMs%20%28MLLMs%29%2C%20this%20hierarchy%20remains%20poorly%20understood%2C%20as%20most%20studies%20focus%20on%20a%20narrow%20set%20of%20tasks.%20We%20introduce%20SpatialTree%2C%20a%20cognitive-science-inspired%20hierarchy%20that%20organizes%20spatial%20abilities%20into%20four%20levels%3A%20low-level%20perception%20%28L1%29%2C%20mental%20mapping%20%28L2%29%2C%20simulation%20%28L3%29%2C%20and%20agentic%20competence%20%28L4%29.%20Based%20on%20this%20taxonomy%2C%20we%20construct%20the%20first%20capability-centric%20hierarchical%20benchmark%2C%20thoroughly%20evaluating%20mainstream%20MLLMs%20across%2027%20sub-abilities.%20The%20evaluation%20results%20reveal%20a%20clear%20structure%3A%20L1%20skills%20are%20largely%20orthogonal%2C%20whereas%20higher-level%20skills%20are%20strongly%20correlated%2C%20indicating%20increasing%20interdependency.%20Through%20targeted%20supervised%20fine-tuning%2C%20we%20uncover%20a%20surprising%20transfer%20dynamic-negative%20transfer%20within%20L1%2C%20but%20strong%20cross-level%20transfer%20from%20low-%20to%20high-level%20abilities%20with%20notable%20synergy.%20Finally%2C%20we%20explore%20how%20to%20improve%20the%20entire%20hierarchy.%20We%20find%20that%20naive%20RL%20that%20encourages%20extensive%20%22thinking%22%20is%20unreliable%3A%20it%20helps%20complex%20reasoning%20but%20hurts%20intuitive%20perception.%20We%20propose%20a%20simple%20auto-think%20strategy%20that%20suppresses%20unnecessary%20deliberation%2C%20enabling%20RL%20to%20consistently%20improve%20performance%20across%20all%20levels.%20By%20building%20SpatialTree%2C%20we%20provide%20a%20proof-of-concept%20framework%20for%20understanding%20and%20systematically%20scaling%20spatial%20abilities%20in%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20617v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialTree%253A%2520How%2520Spatial%2520Abilities%2520Branch%2520Out%2520in%2520MLLMs%26entry.906535625%3DYuxi%2520Xiao%2520and%2520Longfei%2520Li%2520and%2520Shen%2520Yan%2520and%2520Xinhang%2520Liu%2520and%2520Sida%2520Peng%2520and%2520Yunchao%2520Wei%2520and%2520Xiaowei%2520Zhou%2520and%2520Bingyi%2520Kang%26entry.1292438233%3DCognitive%2520science%2520suggests%2520that%2520spatial%2520ability%2520develops%2520progressively-from%2520perception%2520to%2520reasoning%2520and%2520interaction.%2520Yet%2520in%2520multimodal%2520LLMs%2520%2528MLLMs%2529%252C%2520this%2520hierarchy%2520remains%2520poorly%2520understood%252C%2520as%2520most%2520studies%2520focus%2520on%2520a%2520narrow%2520set%2520of%2520tasks.%2520We%2520introduce%2520SpatialTree%252C%2520a%2520cognitive-science-inspired%2520hierarchy%2520that%2520organizes%2520spatial%2520abilities%2520into%2520four%2520levels%253A%2520low-level%2520perception%2520%2528L1%2529%252C%2520mental%2520mapping%2520%2528L2%2529%252C%2520simulation%2520%2528L3%2529%252C%2520and%2520agentic%2520competence%2520%2528L4%2529.%2520Based%2520on%2520this%2520taxonomy%252C%2520we%2520construct%2520the%2520first%2520capability-centric%2520hierarchical%2520benchmark%252C%2520thoroughly%2520evaluating%2520mainstream%2520MLLMs%2520across%252027%2520sub-abilities.%2520The%2520evaluation%2520results%2520reveal%2520a%2520clear%2520structure%253A%2520L1%2520skills%2520are%2520largely%2520orthogonal%252C%2520whereas%2520higher-level%2520skills%2520are%2520strongly%2520correlated%252C%2520indicating%2520increasing%2520interdependency.%2520Through%2520targeted%2520supervised%2520fine-tuning%252C%2520we%2520uncover%2520a%2520surprising%2520transfer%2520dynamic-negative%2520transfer%2520within%2520L1%252C%2520but%2520strong%2520cross-level%2520transfer%2520from%2520low-%2520to%2520high-level%2520abilities%2520with%2520notable%2520synergy.%2520Finally%252C%2520we%2520explore%2520how%2520to%2520improve%2520the%2520entire%2520hierarchy.%2520We%2520find%2520that%2520naive%2520RL%2520that%2520encourages%2520extensive%2520%2522thinking%2522%2520is%2520unreliable%253A%2520it%2520helps%2520complex%2520reasoning%2520but%2520hurts%2520intuitive%2520perception.%2520We%2520propose%2520a%2520simple%2520auto-think%2520strategy%2520that%2520suppresses%2520unnecessary%2520deliberation%252C%2520enabling%2520RL%2520to%2520consistently%2520improve%2520performance%2520across%2520all%2520levels.%2520By%2520building%2520SpatialTree%252C%2520we%2520provide%2520a%2520proof-of-concept%2520framework%2520for%2520understanding%2520and%2520systematically%2520scaling%2520spatial%2520abilities%2520in%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20617v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialTree%3A%20How%20Spatial%20Abilities%20Branch%20Out%20in%20MLLMs&entry.906535625=Yuxi%20Xiao%20and%20Longfei%20Li%20and%20Shen%20Yan%20and%20Xinhang%20Liu%20and%20Sida%20Peng%20and%20Yunchao%20Wei%20and%20Xiaowei%20Zhou%20and%20Bingyi%20Kang&entry.1292438233=Cognitive%20science%20suggests%20that%20spatial%20ability%20develops%20progressively-from%20perception%20to%20reasoning%20and%20interaction.%20Yet%20in%20multimodal%20LLMs%20%28MLLMs%29%2C%20this%20hierarchy%20remains%20poorly%20understood%2C%20as%20most%20studies%20focus%20on%20a%20narrow%20set%20of%20tasks.%20We%20introduce%20SpatialTree%2C%20a%20cognitive-science-inspired%20hierarchy%20that%20organizes%20spatial%20abilities%20into%20four%20levels%3A%20low-level%20perception%20%28L1%29%2C%20mental%20mapping%20%28L2%29%2C%20simulation%20%28L3%29%2C%20and%20agentic%20competence%20%28L4%29.%20Based%20on%20this%20taxonomy%2C%20we%20construct%20the%20first%20capability-centric%20hierarchical%20benchmark%2C%20thoroughly%20evaluating%20mainstream%20MLLMs%20across%2027%20sub-abilities.%20The%20evaluation%20results%20reveal%20a%20clear%20structure%3A%20L1%20skills%20are%20largely%20orthogonal%2C%20whereas%20higher-level%20skills%20are%20strongly%20correlated%2C%20indicating%20increasing%20interdependency.%20Through%20targeted%20supervised%20fine-tuning%2C%20we%20uncover%20a%20surprising%20transfer%20dynamic-negative%20transfer%20within%20L1%2C%20but%20strong%20cross-level%20transfer%20from%20low-%20to%20high-level%20abilities%20with%20notable%20synergy.%20Finally%2C%20we%20explore%20how%20to%20improve%20the%20entire%20hierarchy.%20We%20find%20that%20naive%20RL%20that%20encourages%20extensive%20%22thinking%22%20is%20unreliable%3A%20it%20helps%20complex%20reasoning%20but%20hurts%20intuitive%20perception.%20We%20propose%20a%20simple%20auto-think%20strategy%20that%20suppresses%20unnecessary%20deliberation%2C%20enabling%20RL%20to%20consistently%20improve%20performance%20across%20all%20levels.%20By%20building%20SpatialTree%2C%20we%20provide%20a%20proof-of-concept%20framework%20for%20understanding%20and%20systematically%20scaling%20spatial%20abilities%20in%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.20617v2&entry.124074799=Read"},
{"title": "CktGen: Automated Analog Circuit Design with Generative Artificial Intelligence", "author": "Yuxuan Hou and Hehe Fan and Jianrong Zhang and Yue Zhang and Hua Chen and Min Zhou and Faxin Yu and Roger Zimmermann and Yi Yang", "abstract": "The automatic synthesis of analog circuits presents significant challenges. Most existing approaches formulate the problem as a single-objective optimization task, overlooking that design specifications for a given circuit type vary widely across applications. To address this, we introduce specification-conditioned analog circuit generation, a task that directly generates analog circuits based on target specifications. The motivation is to leverage existing well-designed circuits to improve automation in analog circuit design. Specifically, we propose CktGen, a simple yet effective variational autoencoder that maps discretized specifications and circuits into a joint latent space and reconstructs the circuit from that latent vector. Notably, as a single specification may correspond to multiple valid circuits, naively fusing specification information into the generative model does not capture these one-to-many relationships. To address this, we decouple the encoding of circuits and specifications and align their mapped latent space. Then, we employ contrastive training with a filter mask to maximize differences between encoded circuits and specifications. Furthermore, classifier guidance along with latent feature alignment promotes the clustering of circuits sharing the same specification, avoiding model collapse into trivial one-to-one mappings. By canonicalizing the latent space with respect to specifications, we can search for an optimal circuit that meets valid target specifications. We conduct comprehensive experiments on the open circuit benchmark and introduce metrics to evaluate cross-model consistency. Experimental results demonstrate that CktGen achieves substantial improvements over state-of-the-art methods.", "link": "http://arxiv.org/abs/2410.00995v2", "date": "2026-01-07", "relevancy": 2.6493, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5556}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5232}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CktGen%3A%20Automated%20Analog%20Circuit%20Design%20with%20Generative%20Artificial%20Intelligence&body=Title%3A%20CktGen%3A%20Automated%20Analog%20Circuit%20Design%20with%20Generative%20Artificial%20Intelligence%0AAuthor%3A%20Yuxuan%20Hou%20and%20Hehe%20Fan%20and%20Jianrong%20Zhang%20and%20Yue%20Zhang%20and%20Hua%20Chen%20and%20Min%20Zhou%20and%20Faxin%20Yu%20and%20Roger%20Zimmermann%20and%20Yi%20Yang%0AAbstract%3A%20The%20automatic%20synthesis%20of%20analog%20circuits%20presents%20significant%20challenges.%20Most%20existing%20approaches%20formulate%20the%20problem%20as%20a%20single-objective%20optimization%20task%2C%20overlooking%20that%20design%20specifications%20for%20a%20given%20circuit%20type%20vary%20widely%20across%20applications.%20To%20address%20this%2C%20we%20introduce%20specification-conditioned%20analog%20circuit%20generation%2C%20a%20task%20that%20directly%20generates%20analog%20circuits%20based%20on%20target%20specifications.%20The%20motivation%20is%20to%20leverage%20existing%20well-designed%20circuits%20to%20improve%20automation%20in%20analog%20circuit%20design.%20Specifically%2C%20we%20propose%20CktGen%2C%20a%20simple%20yet%20effective%20variational%20autoencoder%20that%20maps%20discretized%20specifications%20and%20circuits%20into%20a%20joint%20latent%20space%20and%20reconstructs%20the%20circuit%20from%20that%20latent%20vector.%20Notably%2C%20as%20a%20single%20specification%20may%20correspond%20to%20multiple%20valid%20circuits%2C%20naively%20fusing%20specification%20information%20into%20the%20generative%20model%20does%20not%20capture%20these%20one-to-many%20relationships.%20To%20address%20this%2C%20we%20decouple%20the%20encoding%20of%20circuits%20and%20specifications%20and%20align%20their%20mapped%20latent%20space.%20Then%2C%20we%20employ%20contrastive%20training%20with%20a%20filter%20mask%20to%20maximize%20differences%20between%20encoded%20circuits%20and%20specifications.%20Furthermore%2C%20classifier%20guidance%20along%20with%20latent%20feature%20alignment%20promotes%20the%20clustering%20of%20circuits%20sharing%20the%20same%20specification%2C%20avoiding%20model%20collapse%20into%20trivial%20one-to-one%20mappings.%20By%20canonicalizing%20the%20latent%20space%20with%20respect%20to%20specifications%2C%20we%20can%20search%20for%20an%20optimal%20circuit%20that%20meets%20valid%20target%20specifications.%20We%20conduct%20comprehensive%20experiments%20on%20the%20open%20circuit%20benchmark%20and%20introduce%20metrics%20to%20evaluate%20cross-model%20consistency.%20Experimental%20results%20demonstrate%20that%20CktGen%20achieves%20substantial%20improvements%20over%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2410.00995v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCktGen%253A%2520Automated%2520Analog%2520Circuit%2520Design%2520with%2520Generative%2520Artificial%2520Intelligence%26entry.906535625%3DYuxuan%2520Hou%2520and%2520Hehe%2520Fan%2520and%2520Jianrong%2520Zhang%2520and%2520Yue%2520Zhang%2520and%2520Hua%2520Chen%2520and%2520Min%2520Zhou%2520and%2520Faxin%2520Yu%2520and%2520Roger%2520Zimmermann%2520and%2520Yi%2520Yang%26entry.1292438233%3DThe%2520automatic%2520synthesis%2520of%2520analog%2520circuits%2520presents%2520significant%2520challenges.%2520Most%2520existing%2520approaches%2520formulate%2520the%2520problem%2520as%2520a%2520single-objective%2520optimization%2520task%252C%2520overlooking%2520that%2520design%2520specifications%2520for%2520a%2520given%2520circuit%2520type%2520vary%2520widely%2520across%2520applications.%2520To%2520address%2520this%252C%2520we%2520introduce%2520specification-conditioned%2520analog%2520circuit%2520generation%252C%2520a%2520task%2520that%2520directly%2520generates%2520analog%2520circuits%2520based%2520on%2520target%2520specifications.%2520The%2520motivation%2520is%2520to%2520leverage%2520existing%2520well-designed%2520circuits%2520to%2520improve%2520automation%2520in%2520analog%2520circuit%2520design.%2520Specifically%252C%2520we%2520propose%2520CktGen%252C%2520a%2520simple%2520yet%2520effective%2520variational%2520autoencoder%2520that%2520maps%2520discretized%2520specifications%2520and%2520circuits%2520into%2520a%2520joint%2520latent%2520space%2520and%2520reconstructs%2520the%2520circuit%2520from%2520that%2520latent%2520vector.%2520Notably%252C%2520as%2520a%2520single%2520specification%2520may%2520correspond%2520to%2520multiple%2520valid%2520circuits%252C%2520naively%2520fusing%2520specification%2520information%2520into%2520the%2520generative%2520model%2520does%2520not%2520capture%2520these%2520one-to-many%2520relationships.%2520To%2520address%2520this%252C%2520we%2520decouple%2520the%2520encoding%2520of%2520circuits%2520and%2520specifications%2520and%2520align%2520their%2520mapped%2520latent%2520space.%2520Then%252C%2520we%2520employ%2520contrastive%2520training%2520with%2520a%2520filter%2520mask%2520to%2520maximize%2520differences%2520between%2520encoded%2520circuits%2520and%2520specifications.%2520Furthermore%252C%2520classifier%2520guidance%2520along%2520with%2520latent%2520feature%2520alignment%2520promotes%2520the%2520clustering%2520of%2520circuits%2520sharing%2520the%2520same%2520specification%252C%2520avoiding%2520model%2520collapse%2520into%2520trivial%2520one-to-one%2520mappings.%2520By%2520canonicalizing%2520the%2520latent%2520space%2520with%2520respect%2520to%2520specifications%252C%2520we%2520can%2520search%2520for%2520an%2520optimal%2520circuit%2520that%2520meets%2520valid%2520target%2520specifications.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520the%2520open%2520circuit%2520benchmark%2520and%2520introduce%2520metrics%2520to%2520evaluate%2520cross-model%2520consistency.%2520Experimental%2520results%2520demonstrate%2520that%2520CktGen%2520achieves%2520substantial%2520improvements%2520over%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00995v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CktGen%3A%20Automated%20Analog%20Circuit%20Design%20with%20Generative%20Artificial%20Intelligence&entry.906535625=Yuxuan%20Hou%20and%20Hehe%20Fan%20and%20Jianrong%20Zhang%20and%20Yue%20Zhang%20and%20Hua%20Chen%20and%20Min%20Zhou%20and%20Faxin%20Yu%20and%20Roger%20Zimmermann%20and%20Yi%20Yang&entry.1292438233=The%20automatic%20synthesis%20of%20analog%20circuits%20presents%20significant%20challenges.%20Most%20existing%20approaches%20formulate%20the%20problem%20as%20a%20single-objective%20optimization%20task%2C%20overlooking%20that%20design%20specifications%20for%20a%20given%20circuit%20type%20vary%20widely%20across%20applications.%20To%20address%20this%2C%20we%20introduce%20specification-conditioned%20analog%20circuit%20generation%2C%20a%20task%20that%20directly%20generates%20analog%20circuits%20based%20on%20target%20specifications.%20The%20motivation%20is%20to%20leverage%20existing%20well-designed%20circuits%20to%20improve%20automation%20in%20analog%20circuit%20design.%20Specifically%2C%20we%20propose%20CktGen%2C%20a%20simple%20yet%20effective%20variational%20autoencoder%20that%20maps%20discretized%20specifications%20and%20circuits%20into%20a%20joint%20latent%20space%20and%20reconstructs%20the%20circuit%20from%20that%20latent%20vector.%20Notably%2C%20as%20a%20single%20specification%20may%20correspond%20to%20multiple%20valid%20circuits%2C%20naively%20fusing%20specification%20information%20into%20the%20generative%20model%20does%20not%20capture%20these%20one-to-many%20relationships.%20To%20address%20this%2C%20we%20decouple%20the%20encoding%20of%20circuits%20and%20specifications%20and%20align%20their%20mapped%20latent%20space.%20Then%2C%20we%20employ%20contrastive%20training%20with%20a%20filter%20mask%20to%20maximize%20differences%20between%20encoded%20circuits%20and%20specifications.%20Furthermore%2C%20classifier%20guidance%20along%20with%20latent%20feature%20alignment%20promotes%20the%20clustering%20of%20circuits%20sharing%20the%20same%20specification%2C%20avoiding%20model%20collapse%20into%20trivial%20one-to-one%20mappings.%20By%20canonicalizing%20the%20latent%20space%20with%20respect%20to%20specifications%2C%20we%20can%20search%20for%20an%20optimal%20circuit%20that%20meets%20valid%20target%20specifications.%20We%20conduct%20comprehensive%20experiments%20on%20the%20open%20circuit%20benchmark%20and%20introduce%20metrics%20to%20evaluate%20cross-model%20consistency.%20Experimental%20results%20demonstrate%20that%20CktGen%20achieves%20substantial%20improvements%20over%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2410.00995v2&entry.124074799=Read"},
{"title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction", "author": "Jiaxin Huang and Yuanbo Yang and Bangbang Yang and Lin Ma and Yuewen Ma and Yiyi Liao", "abstract": "We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.", "link": "http://arxiv.org/abs/2601.04090v1", "date": "2026-01-07", "relevancy": 2.6216, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6582}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gen3R%3A%203D%20Scene%20Generation%20Meets%20Feed-Forward%20Reconstruction&body=Title%3A%20Gen3R%3A%203D%20Scene%20Generation%20Meets%20Feed-Forward%20Reconstruction%0AAuthor%3A%20Jiaxin%20Huang%20and%20Yuanbo%20Yang%20and%20Bangbang%20Yang%20and%20Lin%20Ma%20and%20Yuewen%20Ma%20and%20Yiyi%20Liao%0AAbstract%3A%20We%20present%20Gen3R%2C%20a%20method%20that%20bridges%20the%20strong%20priors%20of%20foundational%20reconstruction%20models%20and%20video%20diffusion%20models%20for%20scene-level%203D%20generation.%20We%20repurpose%20the%20VGGT%20reconstruction%20model%20to%20produce%20geometric%20latents%20by%20training%20an%20adapter%20on%20its%20tokens%2C%20which%20are%20regularized%20to%20align%20with%20the%20appearance%20latents%20of%20pre-trained%20video%20diffusion%20models.%20By%20jointly%20generating%20these%20disentangled%20yet%20aligned%20latents%2C%20Gen3R%20produces%20both%20RGB%20videos%20and%20corresponding%203D%20geometry%2C%20including%20camera%20poses%2C%20depth%20maps%2C%20and%20global%20point%20clouds.%20Experiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20results%20in%20single-%20and%20multi-image%20conditioned%203D%20scene%20generation.%20Additionally%2C%20our%20method%20can%20enhance%20the%20robustness%20of%20reconstruction%20by%20leveraging%20generative%20priors%2C%20demonstrating%20the%20mutual%20benefit%20of%20tightly%20coupling%20reconstruction%20and%20generative%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGen3R%253A%25203D%2520Scene%2520Generation%2520Meets%2520Feed-Forward%2520Reconstruction%26entry.906535625%3DJiaxin%2520Huang%2520and%2520Yuanbo%2520Yang%2520and%2520Bangbang%2520Yang%2520and%2520Lin%2520Ma%2520and%2520Yuewen%2520Ma%2520and%2520Yiyi%2520Liao%26entry.1292438233%3DWe%2520present%2520Gen3R%252C%2520a%2520method%2520that%2520bridges%2520the%2520strong%2520priors%2520of%2520foundational%2520reconstruction%2520models%2520and%2520video%2520diffusion%2520models%2520for%2520scene-level%25203D%2520generation.%2520We%2520repurpose%2520the%2520VGGT%2520reconstruction%2520model%2520to%2520produce%2520geometric%2520latents%2520by%2520training%2520an%2520adapter%2520on%2520its%2520tokens%252C%2520which%2520are%2520regularized%2520to%2520align%2520with%2520the%2520appearance%2520latents%2520of%2520pre-trained%2520video%2520diffusion%2520models.%2520By%2520jointly%2520generating%2520these%2520disentangled%2520yet%2520aligned%2520latents%252C%2520Gen3R%2520produces%2520both%2520RGB%2520videos%2520and%2520corresponding%25203D%2520geometry%252C%2520including%2520camera%2520poses%252C%2520depth%2520maps%252C%2520and%2520global%2520point%2520clouds.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520results%2520in%2520single-%2520and%2520multi-image%2520conditioned%25203D%2520scene%2520generation.%2520Additionally%252C%2520our%2520method%2520can%2520enhance%2520the%2520robustness%2520of%2520reconstruction%2520by%2520leveraging%2520generative%2520priors%252C%2520demonstrating%2520the%2520mutual%2520benefit%2520of%2520tightly%2520coupling%2520reconstruction%2520and%2520generative%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gen3R%3A%203D%20Scene%20Generation%20Meets%20Feed-Forward%20Reconstruction&entry.906535625=Jiaxin%20Huang%20and%20Yuanbo%20Yang%20and%20Bangbang%20Yang%20and%20Lin%20Ma%20and%20Yuewen%20Ma%20and%20Yiyi%20Liao&entry.1292438233=We%20present%20Gen3R%2C%20a%20method%20that%20bridges%20the%20strong%20priors%20of%20foundational%20reconstruction%20models%20and%20video%20diffusion%20models%20for%20scene-level%203D%20generation.%20We%20repurpose%20the%20VGGT%20reconstruction%20model%20to%20produce%20geometric%20latents%20by%20training%20an%20adapter%20on%20its%20tokens%2C%20which%20are%20regularized%20to%20align%20with%20the%20appearance%20latents%20of%20pre-trained%20video%20diffusion%20models.%20By%20jointly%20generating%20these%20disentangled%20yet%20aligned%20latents%2C%20Gen3R%20produces%20both%20RGB%20videos%20and%20corresponding%203D%20geometry%2C%20including%20camera%20poses%2C%20depth%20maps%2C%20and%20global%20point%20clouds.%20Experiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20results%20in%20single-%20and%20multi-image%20conditioned%203D%20scene%20generation.%20Additionally%2C%20our%20method%20can%20enhance%20the%20robustness%20of%20reconstruction%20by%20leveraging%20generative%20priors%2C%20demonstrating%20the%20mutual%20benefit%20of%20tightly%20coupling%20reconstruction%20and%20generative%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.04090v1&entry.124074799=Read"},
{"title": "HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST", "author": "Shuyu Zhang and Yifan Wei and Xinru Wang and Yanmin Zhu and Yangfan He and Yixuan Weng and Bin Li and Yujie Liu", "abstract": "Zero-shot Dialog State Tracking (zs-DST) is essential for enabling Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly data annotation. A central challenge lies in the semantic misalignment between dynamic dialog contexts and static prompts, leading to inflexible cross-layer coordination, domain interference, and catastrophic forgetting. To tackle this, we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a framework that enhances zero-shot slot inference through robust prompt alignment. It features a hierarchical LoRA architecture for dynamic layer-specific processing (combining lower-layer heuristic grouping and higher-layer full interaction), integrates Spectral Joint Domain-Slot Clustering to identify transferable associations (feeding an Adaptive Linear Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization (SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.", "link": "http://arxiv.org/abs/2509.19742v3", "date": "2026-01-07", "relevancy": 2.6034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiCoLoRA%3A%20Addressing%20Context-Prompt%20Misalignment%20via%20Hierarchical%20Collaborative%20LoRA%20for%20Zero-Shot%20DST&body=Title%3A%20HiCoLoRA%3A%20Addressing%20Context-Prompt%20Misalignment%20via%20Hierarchical%20Collaborative%20LoRA%20for%20Zero-Shot%20DST%0AAuthor%3A%20Shuyu%20Zhang%20and%20Yifan%20Wei%20and%20Xinru%20Wang%20and%20Yanmin%20Zhu%20and%20Yangfan%20He%20and%20Yixuan%20Weng%20and%20Bin%20Li%20and%20Yujie%20Liu%0AAbstract%3A%20Zero-shot%20Dialog%20State%20Tracking%20%28zs-DST%29%20is%20essential%20for%20enabling%20Task-Oriented%20Dialog%20Systems%20%28TODs%29%20to%20generalize%20to%20new%20domains%20without%20costly%20data%20annotation.%20A%20central%20challenge%20lies%20in%20the%20semantic%20misalignment%20between%20dynamic%20dialog%20contexts%20and%20static%20prompts%2C%20leading%20to%20inflexible%20cross-layer%20coordination%2C%20domain%20interference%2C%20and%20catastrophic%20forgetting.%20To%20tackle%20this%2C%20we%20propose%20Hierarchical%20Collaborative%20Low-Rank%20Adaptation%20%28HiCoLoRA%29%2C%20a%20framework%20that%20enhances%20zero-shot%20slot%20inference%20through%20robust%20prompt%20alignment.%20It%20features%20a%20hierarchical%20LoRA%20architecture%20for%20dynamic%20layer-specific%20processing%20%28combining%20lower-layer%20heuristic%20grouping%20and%20higher-layer%20full%20interaction%29%2C%20integrates%20Spectral%20Joint%20Domain-Slot%20Clustering%20to%20identify%20transferable%20associations%20%28feeding%20an%20Adaptive%20Linear%20Fusion%20Mechanism%29%2C%20and%20employs%20Semantic-Enhanced%20SVD%20Initialization%20%28SemSVD-Init%29%20to%20preserve%20pre-trained%20knowledge.%20Experiments%20on%20multi-domain%20datasets%20MultiWOZ%20and%20SGD%20show%20that%20HiCoLoRA%20outperforms%20baselines%2C%20achieving%20SOTA%20in%20zs-DST.%20Code%20is%20available%20at%20https%3A//github.com/carsonz/HiCoLoRA.%0ALink%3A%20http%3A//arxiv.org/abs/2509.19742v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiCoLoRA%253A%2520Addressing%2520Context-Prompt%2520Misalignment%2520via%2520Hierarchical%2520Collaborative%2520LoRA%2520for%2520Zero-Shot%2520DST%26entry.906535625%3DShuyu%2520Zhang%2520and%2520Yifan%2520Wei%2520and%2520Xinru%2520Wang%2520and%2520Yanmin%2520Zhu%2520and%2520Yangfan%2520He%2520and%2520Yixuan%2520Weng%2520and%2520Bin%2520Li%2520and%2520Yujie%2520Liu%26entry.1292438233%3DZero-shot%2520Dialog%2520State%2520Tracking%2520%2528zs-DST%2529%2520is%2520essential%2520for%2520enabling%2520Task-Oriented%2520Dialog%2520Systems%2520%2528TODs%2529%2520to%2520generalize%2520to%2520new%2520domains%2520without%2520costly%2520data%2520annotation.%2520A%2520central%2520challenge%2520lies%2520in%2520the%2520semantic%2520misalignment%2520between%2520dynamic%2520dialog%2520contexts%2520and%2520static%2520prompts%252C%2520leading%2520to%2520inflexible%2520cross-layer%2520coordination%252C%2520domain%2520interference%252C%2520and%2520catastrophic%2520forgetting.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520Hierarchical%2520Collaborative%2520Low-Rank%2520Adaptation%2520%2528HiCoLoRA%2529%252C%2520a%2520framework%2520that%2520enhances%2520zero-shot%2520slot%2520inference%2520through%2520robust%2520prompt%2520alignment.%2520It%2520features%2520a%2520hierarchical%2520LoRA%2520architecture%2520for%2520dynamic%2520layer-specific%2520processing%2520%2528combining%2520lower-layer%2520heuristic%2520grouping%2520and%2520higher-layer%2520full%2520interaction%2529%252C%2520integrates%2520Spectral%2520Joint%2520Domain-Slot%2520Clustering%2520to%2520identify%2520transferable%2520associations%2520%2528feeding%2520an%2520Adaptive%2520Linear%2520Fusion%2520Mechanism%2529%252C%2520and%2520employs%2520Semantic-Enhanced%2520SVD%2520Initialization%2520%2528SemSVD-Init%2529%2520to%2520preserve%2520pre-trained%2520knowledge.%2520Experiments%2520on%2520multi-domain%2520datasets%2520MultiWOZ%2520and%2520SGD%2520show%2520that%2520HiCoLoRA%2520outperforms%2520baselines%252C%2520achieving%2520SOTA%2520in%2520zs-DST.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/carsonz/HiCoLoRA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19742v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiCoLoRA%3A%20Addressing%20Context-Prompt%20Misalignment%20via%20Hierarchical%20Collaborative%20LoRA%20for%20Zero-Shot%20DST&entry.906535625=Shuyu%20Zhang%20and%20Yifan%20Wei%20and%20Xinru%20Wang%20and%20Yanmin%20Zhu%20and%20Yangfan%20He%20and%20Yixuan%20Weng%20and%20Bin%20Li%20and%20Yujie%20Liu&entry.1292438233=Zero-shot%20Dialog%20State%20Tracking%20%28zs-DST%29%20is%20essential%20for%20enabling%20Task-Oriented%20Dialog%20Systems%20%28TODs%29%20to%20generalize%20to%20new%20domains%20without%20costly%20data%20annotation.%20A%20central%20challenge%20lies%20in%20the%20semantic%20misalignment%20between%20dynamic%20dialog%20contexts%20and%20static%20prompts%2C%20leading%20to%20inflexible%20cross-layer%20coordination%2C%20domain%20interference%2C%20and%20catastrophic%20forgetting.%20To%20tackle%20this%2C%20we%20propose%20Hierarchical%20Collaborative%20Low-Rank%20Adaptation%20%28HiCoLoRA%29%2C%20a%20framework%20that%20enhances%20zero-shot%20slot%20inference%20through%20robust%20prompt%20alignment.%20It%20features%20a%20hierarchical%20LoRA%20architecture%20for%20dynamic%20layer-specific%20processing%20%28combining%20lower-layer%20heuristic%20grouping%20and%20higher-layer%20full%20interaction%29%2C%20integrates%20Spectral%20Joint%20Domain-Slot%20Clustering%20to%20identify%20transferable%20associations%20%28feeding%20an%20Adaptive%20Linear%20Fusion%20Mechanism%29%2C%20and%20employs%20Semantic-Enhanced%20SVD%20Initialization%20%28SemSVD-Init%29%20to%20preserve%20pre-trained%20knowledge.%20Experiments%20on%20multi-domain%20datasets%20MultiWOZ%20and%20SGD%20show%20that%20HiCoLoRA%20outperforms%20baselines%2C%20achieving%20SOTA%20in%20zs-DST.%20Code%20is%20available%20at%20https%3A//github.com/carsonz/HiCoLoRA.&entry.1838667208=http%3A//arxiv.org/abs/2509.19742v3&entry.124074799=Read"},
{"title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection", "author": "Mingyu Ouyang and Kevin Qinghong Lin and Mike Zheng Shou and Hwee Tou Ng", "abstract": "Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.", "link": "http://arxiv.org/abs/2601.03928v1", "date": "2026-01-07", "relevancy": 2.5844, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocusUI%3A%20Efficient%20UI%20Grounding%20via%20Position-Preserving%20Visual%20Token%20Selection&body=Title%3A%20FocusUI%3A%20Efficient%20UI%20Grounding%20via%20Position-Preserving%20Visual%20Token%20Selection%0AAuthor%3A%20Mingyu%20Ouyang%20and%20Kevin%20Qinghong%20Lin%20and%20Mike%20Zheng%20Shou%20and%20Hwee%20Tou%20Ng%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20remarkable%20performance%20in%20User%20Interface%20%28UI%29%20grounding%20tasks%2C%20driven%20by%20their%20ability%20to%20process%20increasingly%20high-resolution%20screenshots.%20However%2C%20screenshots%20are%20tokenized%20into%20thousands%20of%20visual%20tokens%20%28e.g.%2C%20about%204700%20for%202K%20resolution%29%2C%20incurring%20significant%20computational%20overhead%20and%20diluting%20attention.%20In%20contrast%2C%20humans%20typically%20focus%20on%20regions%20of%20interest%20when%20interacting%20with%20UI.%20In%20this%20work%2C%20we%20pioneer%20the%20task%20of%20efficient%20UI%20grounding.%20Guided%20by%20practical%20analysis%20of%20the%20task%27s%20characteristics%20and%20challenges%2C%20we%20propose%20FocusUI%2C%20an%20efficient%20UI%20grounding%20framework%20that%20selects%20patches%20most%20relevant%20to%20the%20instruction%20while%20preserving%20positional%20continuity%20for%20precise%20grounding.%20FocusUI%20addresses%20two%20key%20challenges%3A%20%281%29%20Eliminating%20redundant%20tokens%20in%20visual%20encoding.%20We%20construct%20patch-level%20supervision%20by%20fusing%20an%20instruction-conditioned%20score%20with%20a%20rule-based%20UI-graph%20score%20that%20down-weights%20large%20homogeneous%20regions%20to%20select%20distinct%20and%20instruction-relevant%20visual%20tokens.%20%282%29%20Preserving%20positional%20continuity%20during%20visual%20token%20selection.%20We%20find%20that%20general%20visual%20token%20pruning%20methods%20suffer%20from%20severe%20accuracy%20degradation%20on%20UI%20grounding%20tasks%20due%20to%20broken%20positional%20information.%20We%20introduce%20a%20novel%20PosPad%20strategy%2C%20which%20compresses%20each%20contiguous%20sequence%20of%20dropped%20visual%20tokens%20into%20a%20single%20special%20marker%20placed%20at%20the%20sequence%27s%20last%20index%20to%20preserve%20positional%20continuity.%20Comprehensive%20experiments%20on%20four%20grounding%20benchmarks%20demonstrate%20that%20FocusUI%20surpasses%20GUI-specific%20baselines.%20On%20the%20ScreenSpot-Pro%20benchmark%2C%20FocusUI-7B%20achieves%20a%20performance%20improvement%20of%203.7%25%20over%20GUI-Actor-7B.%20Even%20with%20only%2030%25%20visual%20token%20retention%2C%20FocusUI-7B%20drops%20by%20only%203.2%25%20while%20achieving%20up%20to%201.44x%20faster%20inference%20and%2017%25%20lower%20peak%20GPU%20memory.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocusUI%253A%2520Efficient%2520UI%2520Grounding%2520via%2520Position-Preserving%2520Visual%2520Token%2520Selection%26entry.906535625%3DMingyu%2520Ouyang%2520and%2520Kevin%2520Qinghong%2520Lin%2520and%2520Mike%2520Zheng%2520Shou%2520and%2520Hwee%2520Tou%2520Ng%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%2520User%2520Interface%2520%2528UI%2529%2520grounding%2520tasks%252C%2520driven%2520by%2520their%2520ability%2520to%2520process%2520increasingly%2520high-resolution%2520screenshots.%2520However%252C%2520screenshots%2520are%2520tokenized%2520into%2520thousands%2520of%2520visual%2520tokens%2520%2528e.g.%252C%2520about%25204700%2520for%25202K%2520resolution%2529%252C%2520incurring%2520significant%2520computational%2520overhead%2520and%2520diluting%2520attention.%2520In%2520contrast%252C%2520humans%2520typically%2520focus%2520on%2520regions%2520of%2520interest%2520when%2520interacting%2520with%2520UI.%2520In%2520this%2520work%252C%2520we%2520pioneer%2520the%2520task%2520of%2520efficient%2520UI%2520grounding.%2520Guided%2520by%2520practical%2520analysis%2520of%2520the%2520task%2527s%2520characteristics%2520and%2520challenges%252C%2520we%2520propose%2520FocusUI%252C%2520an%2520efficient%2520UI%2520grounding%2520framework%2520that%2520selects%2520patches%2520most%2520relevant%2520to%2520the%2520instruction%2520while%2520preserving%2520positional%2520continuity%2520for%2520precise%2520grounding.%2520FocusUI%2520addresses%2520two%2520key%2520challenges%253A%2520%25281%2529%2520Eliminating%2520redundant%2520tokens%2520in%2520visual%2520encoding.%2520We%2520construct%2520patch-level%2520supervision%2520by%2520fusing%2520an%2520instruction-conditioned%2520score%2520with%2520a%2520rule-based%2520UI-graph%2520score%2520that%2520down-weights%2520large%2520homogeneous%2520regions%2520to%2520select%2520distinct%2520and%2520instruction-relevant%2520visual%2520tokens.%2520%25282%2529%2520Preserving%2520positional%2520continuity%2520during%2520visual%2520token%2520selection.%2520We%2520find%2520that%2520general%2520visual%2520token%2520pruning%2520methods%2520suffer%2520from%2520severe%2520accuracy%2520degradation%2520on%2520UI%2520grounding%2520tasks%2520due%2520to%2520broken%2520positional%2520information.%2520We%2520introduce%2520a%2520novel%2520PosPad%2520strategy%252C%2520which%2520compresses%2520each%2520contiguous%2520sequence%2520of%2520dropped%2520visual%2520tokens%2520into%2520a%2520single%2520special%2520marker%2520placed%2520at%2520the%2520sequence%2527s%2520last%2520index%2520to%2520preserve%2520positional%2520continuity.%2520Comprehensive%2520experiments%2520on%2520four%2520grounding%2520benchmarks%2520demonstrate%2520that%2520FocusUI%2520surpasses%2520GUI-specific%2520baselines.%2520On%2520the%2520ScreenSpot-Pro%2520benchmark%252C%2520FocusUI-7B%2520achieves%2520a%2520performance%2520improvement%2520of%25203.7%2525%2520over%2520GUI-Actor-7B.%2520Even%2520with%2520only%252030%2525%2520visual%2520token%2520retention%252C%2520FocusUI-7B%2520drops%2520by%2520only%25203.2%2525%2520while%2520achieving%2520up%2520to%25201.44x%2520faster%2520inference%2520and%252017%2525%2520lower%2520peak%2520GPU%2520memory.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocusUI%3A%20Efficient%20UI%20Grounding%20via%20Position-Preserving%20Visual%20Token%20Selection&entry.906535625=Mingyu%20Ouyang%20and%20Kevin%20Qinghong%20Lin%20and%20Mike%20Zheng%20Shou%20and%20Hwee%20Tou%20Ng&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20have%20shown%20remarkable%20performance%20in%20User%20Interface%20%28UI%29%20grounding%20tasks%2C%20driven%20by%20their%20ability%20to%20process%20increasingly%20high-resolution%20screenshots.%20However%2C%20screenshots%20are%20tokenized%20into%20thousands%20of%20visual%20tokens%20%28e.g.%2C%20about%204700%20for%202K%20resolution%29%2C%20incurring%20significant%20computational%20overhead%20and%20diluting%20attention.%20In%20contrast%2C%20humans%20typically%20focus%20on%20regions%20of%20interest%20when%20interacting%20with%20UI.%20In%20this%20work%2C%20we%20pioneer%20the%20task%20of%20efficient%20UI%20grounding.%20Guided%20by%20practical%20analysis%20of%20the%20task%27s%20characteristics%20and%20challenges%2C%20we%20propose%20FocusUI%2C%20an%20efficient%20UI%20grounding%20framework%20that%20selects%20patches%20most%20relevant%20to%20the%20instruction%20while%20preserving%20positional%20continuity%20for%20precise%20grounding.%20FocusUI%20addresses%20two%20key%20challenges%3A%20%281%29%20Eliminating%20redundant%20tokens%20in%20visual%20encoding.%20We%20construct%20patch-level%20supervision%20by%20fusing%20an%20instruction-conditioned%20score%20with%20a%20rule-based%20UI-graph%20score%20that%20down-weights%20large%20homogeneous%20regions%20to%20select%20distinct%20and%20instruction-relevant%20visual%20tokens.%20%282%29%20Preserving%20positional%20continuity%20during%20visual%20token%20selection.%20We%20find%20that%20general%20visual%20token%20pruning%20methods%20suffer%20from%20severe%20accuracy%20degradation%20on%20UI%20grounding%20tasks%20due%20to%20broken%20positional%20information.%20We%20introduce%20a%20novel%20PosPad%20strategy%2C%20which%20compresses%20each%20contiguous%20sequence%20of%20dropped%20visual%20tokens%20into%20a%20single%20special%20marker%20placed%20at%20the%20sequence%27s%20last%20index%20to%20preserve%20positional%20continuity.%20Comprehensive%20experiments%20on%20four%20grounding%20benchmarks%20demonstrate%20that%20FocusUI%20surpasses%20GUI-specific%20baselines.%20On%20the%20ScreenSpot-Pro%20benchmark%2C%20FocusUI-7B%20achieves%20a%20performance%20improvement%20of%203.7%25%20over%20GUI-Actor-7B.%20Even%20with%20only%2030%25%20visual%20token%20retention%2C%20FocusUI-7B%20drops%20by%20only%203.2%25%20while%20achieving%20up%20to%201.44x%20faster%20inference%20and%2017%25%20lower%20peak%20GPU%20memory.&entry.1838667208=http%3A//arxiv.org/abs/2601.03928v1&entry.124074799=Read"},
{"title": "Generating Storytelling Images with Rich Chains-of-Reasoning", "author": "Xiujie Song and Qi Jia and Shota Watanabe and Xiaoyi Pang and Ruijie Chen and Mengyue Wu and Kenny Q. Zhu", "abstract": "A single image can convey a compelling story through logically connected visual clues, forming Chains-of-Reasoning (CoRs). We define these semantically rich images as Storytelling Images. By conveying multi-layered information that inspires active interpretation, these images enable a wide range of applications, such as illustration and cognitive screening. Despite their potential, such images are scarce and complex to create. To address this, we introduce the Storytelling Image Generation task and propose StorytellingPainter, a two-stage pipeline combining the reasoning of Large Language Models (LLMs) with Text-to-Image (T2I) synthesis. We also develop a dedicated evaluation framework assessing semantic complexity, diversity, and text-image alignment. Furthermore, given the critical role of story generation in the task, we introduce lightweight Mini-Storytellers to bridge the performance gap between small-scale and proprietary LLMs. Experimental results demonstrate the feasibility of our approaches.", "link": "http://arxiv.org/abs/2512.07198v2", "date": "2026-01-07", "relevancy": 2.5777, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Storytelling%20Images%20with%20Rich%20Chains-of-Reasoning&body=Title%3A%20Generating%20Storytelling%20Images%20with%20Rich%20Chains-of-Reasoning%0AAuthor%3A%20Xiujie%20Song%20and%20Qi%20Jia%20and%20Shota%20Watanabe%20and%20Xiaoyi%20Pang%20and%20Ruijie%20Chen%20and%20Mengyue%20Wu%20and%20Kenny%20Q.%20Zhu%0AAbstract%3A%20A%20single%20image%20can%20convey%20a%20compelling%20story%20through%20logically%20connected%20visual%20clues%2C%20forming%20Chains-of-Reasoning%20%28CoRs%29.%20We%20define%20these%20semantically%20rich%20images%20as%20Storytelling%20Images.%20By%20conveying%20multi-layered%20information%20that%20inspires%20active%20interpretation%2C%20these%20images%20enable%20a%20wide%20range%20of%20applications%2C%20such%20as%20illustration%20and%20cognitive%20screening.%20Despite%20their%20potential%2C%20such%20images%20are%20scarce%20and%20complex%20to%20create.%20To%20address%20this%2C%20we%20introduce%20the%20Storytelling%20Image%20Generation%20task%20and%20propose%20StorytellingPainter%2C%20a%20two-stage%20pipeline%20combining%20the%20reasoning%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20Text-to-Image%20%28T2I%29%20synthesis.%20We%20also%20develop%20a%20dedicated%20evaluation%20framework%20assessing%20semantic%20complexity%2C%20diversity%2C%20and%20text-image%20alignment.%20Furthermore%2C%20given%20the%20critical%20role%20of%20story%20generation%20in%20the%20task%2C%20we%20introduce%20lightweight%20Mini-Storytellers%20to%20bridge%20the%20performance%20gap%20between%20small-scale%20and%20proprietary%20LLMs.%20Experimental%20results%20demonstrate%20the%20feasibility%20of%20our%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07198v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Storytelling%2520Images%2520with%2520Rich%2520Chains-of-Reasoning%26entry.906535625%3DXiujie%2520Song%2520and%2520Qi%2520Jia%2520and%2520Shota%2520Watanabe%2520and%2520Xiaoyi%2520Pang%2520and%2520Ruijie%2520Chen%2520and%2520Mengyue%2520Wu%2520and%2520Kenny%2520Q.%2520Zhu%26entry.1292438233%3DA%2520single%2520image%2520can%2520convey%2520a%2520compelling%2520story%2520through%2520logically%2520connected%2520visual%2520clues%252C%2520forming%2520Chains-of-Reasoning%2520%2528CoRs%2529.%2520We%2520define%2520these%2520semantically%2520rich%2520images%2520as%2520Storytelling%2520Images.%2520By%2520conveying%2520multi-layered%2520information%2520that%2520inspires%2520active%2520interpretation%252C%2520these%2520images%2520enable%2520a%2520wide%2520range%2520of%2520applications%252C%2520such%2520as%2520illustration%2520and%2520cognitive%2520screening.%2520Despite%2520their%2520potential%252C%2520such%2520images%2520are%2520scarce%2520and%2520complex%2520to%2520create.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Storytelling%2520Image%2520Generation%2520task%2520and%2520propose%2520StorytellingPainter%252C%2520a%2520two-stage%2520pipeline%2520combining%2520the%2520reasoning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520Text-to-Image%2520%2528T2I%2529%2520synthesis.%2520We%2520also%2520develop%2520a%2520dedicated%2520evaluation%2520framework%2520assessing%2520semantic%2520complexity%252C%2520diversity%252C%2520and%2520text-image%2520alignment.%2520Furthermore%252C%2520given%2520the%2520critical%2520role%2520of%2520story%2520generation%2520in%2520the%2520task%252C%2520we%2520introduce%2520lightweight%2520Mini-Storytellers%2520to%2520bridge%2520the%2520performance%2520gap%2520between%2520small-scale%2520and%2520proprietary%2520LLMs.%2520Experimental%2520results%2520demonstrate%2520the%2520feasibility%2520of%2520our%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07198v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Storytelling%20Images%20with%20Rich%20Chains-of-Reasoning&entry.906535625=Xiujie%20Song%20and%20Qi%20Jia%20and%20Shota%20Watanabe%20and%20Xiaoyi%20Pang%20and%20Ruijie%20Chen%20and%20Mengyue%20Wu%20and%20Kenny%20Q.%20Zhu&entry.1292438233=A%20single%20image%20can%20convey%20a%20compelling%20story%20through%20logically%20connected%20visual%20clues%2C%20forming%20Chains-of-Reasoning%20%28CoRs%29.%20We%20define%20these%20semantically%20rich%20images%20as%20Storytelling%20Images.%20By%20conveying%20multi-layered%20information%20that%20inspires%20active%20interpretation%2C%20these%20images%20enable%20a%20wide%20range%20of%20applications%2C%20such%20as%20illustration%20and%20cognitive%20screening.%20Despite%20their%20potential%2C%20such%20images%20are%20scarce%20and%20complex%20to%20create.%20To%20address%20this%2C%20we%20introduce%20the%20Storytelling%20Image%20Generation%20task%20and%20propose%20StorytellingPainter%2C%20a%20two-stage%20pipeline%20combining%20the%20reasoning%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20Text-to-Image%20%28T2I%29%20synthesis.%20We%20also%20develop%20a%20dedicated%20evaluation%20framework%20assessing%20semantic%20complexity%2C%20diversity%2C%20and%20text-image%20alignment.%20Furthermore%2C%20given%20the%20critical%20role%20of%20story%20generation%20in%20the%20task%2C%20we%20introduce%20lightweight%20Mini-Storytellers%20to%20bridge%20the%20performance%20gap%20between%20small-scale%20and%20proprietary%20LLMs.%20Experimental%20results%20demonstrate%20the%20feasibility%20of%20our%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2512.07198v2&entry.124074799=Read"},
{"title": "Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs", "author": "Sethupathy Parameswaran and Suresh Sundaram and Yuan Fang", "abstract": "Node classification is a fundamental problem in information retrieval with many real-world applications, such as community detection in social networks, grouping articles published online and product categorization in e-commerce. Zero-shot node classification in text-attributed graphs (TAGs) presents a significant challenge, particularly due to the absence of labeled data. In this paper, we propose a novel Zero-shot Prompt Tuning (ZPT) framework to address this problem by leveraging a Universal Bimodal Conditional Generator (UBCG). Our approach begins with pre-training a graph-language model to capture both the graph structure and the associated textual descriptions of each node. Following this, a conditional generative model is trained to learn the joint distribution of nodes in both graph and text modalities, enabling the generation of synthetic samples for each class based solely on the class name. These synthetic node and text embeddings are subsequently used to perform continuous prompt tuning, facilitating effective node classification in a zero-shot setting. Furthermore, we conduct extensive experiments on multiple benchmark datasets, demonstrating that our framework performs better than existing state-of-the-art baselines. We also provide ablation studies to validate the contribution of the bimodal generator. The code is provided at: https://github.com/Sethup123/ZPT.", "link": "http://arxiv.org/abs/2601.03793v1", "date": "2026-01-07", "relevancy": 2.5716, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5171}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5151}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Tuning%20without%20Labeled%20Samples%20for%20Zero-Shot%20Node%20Classification%20in%20Text-Attributed%20Graphs&body=Title%3A%20Prompt%20Tuning%20without%20Labeled%20Samples%20for%20Zero-Shot%20Node%20Classification%20in%20Text-Attributed%20Graphs%0AAuthor%3A%20Sethupathy%20Parameswaran%20and%20Suresh%20Sundaram%20and%20Yuan%20Fang%0AAbstract%3A%20Node%20classification%20is%20a%20fundamental%20problem%20in%20information%20retrieval%20with%20many%20real-world%20applications%2C%20such%20as%20community%20detection%20in%20social%20networks%2C%20grouping%20articles%20published%20online%20and%20product%20categorization%20in%20e-commerce.%20Zero-shot%20node%20classification%20in%20text-attributed%20graphs%20%28TAGs%29%20presents%20a%20significant%20challenge%2C%20particularly%20due%20to%20the%20absence%20of%20labeled%20data.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Zero-shot%20Prompt%20Tuning%20%28ZPT%29%20framework%20to%20address%20this%20problem%20by%20leveraging%20a%20Universal%20Bimodal%20Conditional%20Generator%20%28UBCG%29.%20Our%20approach%20begins%20with%20pre-training%20a%20graph-language%20model%20to%20capture%20both%20the%20graph%20structure%20and%20the%20associated%20textual%20descriptions%20of%20each%20node.%20Following%20this%2C%20a%20conditional%20generative%20model%20is%20trained%20to%20learn%20the%20joint%20distribution%20of%20nodes%20in%20both%20graph%20and%20text%20modalities%2C%20enabling%20the%20generation%20of%20synthetic%20samples%20for%20each%20class%20based%20solely%20on%20the%20class%20name.%20These%20synthetic%20node%20and%20text%20embeddings%20are%20subsequently%20used%20to%20perform%20continuous%20prompt%20tuning%2C%20facilitating%20effective%20node%20classification%20in%20a%20zero-shot%20setting.%20Furthermore%2C%20we%20conduct%20extensive%20experiments%20on%20multiple%20benchmark%20datasets%2C%20demonstrating%20that%20our%20framework%20performs%20better%20than%20existing%20state-of-the-art%20baselines.%20We%20also%20provide%20ablation%20studies%20to%20validate%20the%20contribution%20of%20the%20bimodal%20generator.%20The%20code%20is%20provided%20at%3A%20https%3A//github.com/Sethup123/ZPT.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Tuning%2520without%2520Labeled%2520Samples%2520for%2520Zero-Shot%2520Node%2520Classification%2520in%2520Text-Attributed%2520Graphs%26entry.906535625%3DSethupathy%2520Parameswaran%2520and%2520Suresh%2520Sundaram%2520and%2520Yuan%2520Fang%26entry.1292438233%3DNode%2520classification%2520is%2520a%2520fundamental%2520problem%2520in%2520information%2520retrieval%2520with%2520many%2520real-world%2520applications%252C%2520such%2520as%2520community%2520detection%2520in%2520social%2520networks%252C%2520grouping%2520articles%2520published%2520online%2520and%2520product%2520categorization%2520in%2520e-commerce.%2520Zero-shot%2520node%2520classification%2520in%2520text-attributed%2520graphs%2520%2528TAGs%2529%2520presents%2520a%2520significant%2520challenge%252C%2520particularly%2520due%2520to%2520the%2520absence%2520of%2520labeled%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Zero-shot%2520Prompt%2520Tuning%2520%2528ZPT%2529%2520framework%2520to%2520address%2520this%2520problem%2520by%2520leveraging%2520a%2520Universal%2520Bimodal%2520Conditional%2520Generator%2520%2528UBCG%2529.%2520Our%2520approach%2520begins%2520with%2520pre-training%2520a%2520graph-language%2520model%2520to%2520capture%2520both%2520the%2520graph%2520structure%2520and%2520the%2520associated%2520textual%2520descriptions%2520of%2520each%2520node.%2520Following%2520this%252C%2520a%2520conditional%2520generative%2520model%2520is%2520trained%2520to%2520learn%2520the%2520joint%2520distribution%2520of%2520nodes%2520in%2520both%2520graph%2520and%2520text%2520modalities%252C%2520enabling%2520the%2520generation%2520of%2520synthetic%2520samples%2520for%2520each%2520class%2520based%2520solely%2520on%2520the%2520class%2520name.%2520These%2520synthetic%2520node%2520and%2520text%2520embeddings%2520are%2520subsequently%2520used%2520to%2520perform%2520continuous%2520prompt%2520tuning%252C%2520facilitating%2520effective%2520node%2520classification%2520in%2520a%2520zero-shot%2520setting.%2520Furthermore%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520multiple%2520benchmark%2520datasets%252C%2520demonstrating%2520that%2520our%2520framework%2520performs%2520better%2520than%2520existing%2520state-of-the-art%2520baselines.%2520We%2520also%2520provide%2520ablation%2520studies%2520to%2520validate%2520the%2520contribution%2520of%2520the%2520bimodal%2520generator.%2520The%2520code%2520is%2520provided%2520at%253A%2520https%253A//github.com/Sethup123/ZPT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Tuning%20without%20Labeled%20Samples%20for%20Zero-Shot%20Node%20Classification%20in%20Text-Attributed%20Graphs&entry.906535625=Sethupathy%20Parameswaran%20and%20Suresh%20Sundaram%20and%20Yuan%20Fang&entry.1292438233=Node%20classification%20is%20a%20fundamental%20problem%20in%20information%20retrieval%20with%20many%20real-world%20applications%2C%20such%20as%20community%20detection%20in%20social%20networks%2C%20grouping%20articles%20published%20online%20and%20product%20categorization%20in%20e-commerce.%20Zero-shot%20node%20classification%20in%20text-attributed%20graphs%20%28TAGs%29%20presents%20a%20significant%20challenge%2C%20particularly%20due%20to%20the%20absence%20of%20labeled%20data.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Zero-shot%20Prompt%20Tuning%20%28ZPT%29%20framework%20to%20address%20this%20problem%20by%20leveraging%20a%20Universal%20Bimodal%20Conditional%20Generator%20%28UBCG%29.%20Our%20approach%20begins%20with%20pre-training%20a%20graph-language%20model%20to%20capture%20both%20the%20graph%20structure%20and%20the%20associated%20textual%20descriptions%20of%20each%20node.%20Following%20this%2C%20a%20conditional%20generative%20model%20is%20trained%20to%20learn%20the%20joint%20distribution%20of%20nodes%20in%20both%20graph%20and%20text%20modalities%2C%20enabling%20the%20generation%20of%20synthetic%20samples%20for%20each%20class%20based%20solely%20on%20the%20class%20name.%20These%20synthetic%20node%20and%20text%20embeddings%20are%20subsequently%20used%20to%20perform%20continuous%20prompt%20tuning%2C%20facilitating%20effective%20node%20classification%20in%20a%20zero-shot%20setting.%20Furthermore%2C%20we%20conduct%20extensive%20experiments%20on%20multiple%20benchmark%20datasets%2C%20demonstrating%20that%20our%20framework%20performs%20better%20than%20existing%20state-of-the-art%20baselines.%20We%20also%20provide%20ablation%20studies%20to%20validate%20the%20contribution%20of%20the%20bimodal%20generator.%20The%20code%20is%20provided%20at%3A%20https%3A//github.com/Sethup123/ZPT.&entry.1838667208=http%3A//arxiv.org/abs/2601.03793v1&entry.124074799=Read"},
{"title": "S2Vec: Self-Supervised Geospatial Embeddings for the Built Environment", "author": "Shushman Choudhury and Elad Aharoni and Chandrakumari Suvarna and Iveel Tsogsuren and Abdul Rahman Kreidieh and Chun-Ta Lu and Neha Arora", "abstract": "Scalable general-purpose representations of the built environment are crucial for geospatial artificial intelligence applications. This paper introduces S2Vec, a novel self-supervised framework for learning such geospatial embeddings. S2Vec uses the S2 Geometry library to partition large areas into discrete S2 cells, rasterizes built environment feature vectors within cells as images, and applies masked autoencoding on these rasterized images to encode the feature vectors. This approach yields task-agnostic embeddings that capture local feature characteristics and broader spatial relationships. We evaluate S2Vec on several large-scale geospatial prediction tasks, both random train/test splits (interpolation) and zero-shot geographic adaptation (extrapolation). Our experiments show S2Vec's competitive performance against several baselines on socioeconomic tasks, especially the geographic adaptation variant, with room for improvement on environmental tasks. We also explore combining S2Vec embeddings with image-based embeddings downstream, showing that such multimodal fusion can often improve performance. Our findings highlight how S2Vec can learn effective general-purpose geospatial representations of the built environment features it is provided, and how it can complement other data modalities in geospatial artificial intelligence.", "link": "http://arxiv.org/abs/2504.16942v2", "date": "2026-01-07", "relevancy": 2.5702, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5423}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5178}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2Vec%3A%20Self-Supervised%20Geospatial%20Embeddings%20for%20the%20Built%20Environment&body=Title%3A%20S2Vec%3A%20Self-Supervised%20Geospatial%20Embeddings%20for%20the%20Built%20Environment%0AAuthor%3A%20Shushman%20Choudhury%20and%20Elad%20Aharoni%20and%20Chandrakumari%20Suvarna%20and%20Iveel%20Tsogsuren%20and%20Abdul%20Rahman%20Kreidieh%20and%20Chun-Ta%20Lu%20and%20Neha%20Arora%0AAbstract%3A%20Scalable%20general-purpose%20representations%20of%20the%20built%20environment%20are%20crucial%20for%20geospatial%20artificial%20intelligence%20applications.%20This%20paper%20introduces%20S2Vec%2C%20a%20novel%20self-supervised%20framework%20for%20learning%20such%20geospatial%20embeddings.%20S2Vec%20uses%20the%20S2%20Geometry%20library%20to%20partition%20large%20areas%20into%20discrete%20S2%20cells%2C%20rasterizes%20built%20environment%20feature%20vectors%20within%20cells%20as%20images%2C%20and%20applies%20masked%20autoencoding%20on%20these%20rasterized%20images%20to%20encode%20the%20feature%20vectors.%20This%20approach%20yields%20task-agnostic%20embeddings%20that%20capture%20local%20feature%20characteristics%20and%20broader%20spatial%20relationships.%20We%20evaluate%20S2Vec%20on%20several%20large-scale%20geospatial%20prediction%20tasks%2C%20both%20random%20train/test%20splits%20%28interpolation%29%20and%20zero-shot%20geographic%20adaptation%20%28extrapolation%29.%20Our%20experiments%20show%20S2Vec%27s%20competitive%20performance%20against%20several%20baselines%20on%20socioeconomic%20tasks%2C%20especially%20the%20geographic%20adaptation%20variant%2C%20with%20room%20for%20improvement%20on%20environmental%20tasks.%20We%20also%20explore%20combining%20S2Vec%20embeddings%20with%20image-based%20embeddings%20downstream%2C%20showing%20that%20such%20multimodal%20fusion%20can%20often%20improve%20performance.%20Our%20findings%20highlight%20how%20S2Vec%20can%20learn%20effective%20general-purpose%20geospatial%20representations%20of%20the%20built%20environment%20features%20it%20is%20provided%2C%20and%20how%20it%20can%20complement%20other%20data%20modalities%20in%20geospatial%20artificial%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2504.16942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2Vec%253A%2520Self-Supervised%2520Geospatial%2520Embeddings%2520for%2520the%2520Built%2520Environment%26entry.906535625%3DShushman%2520Choudhury%2520and%2520Elad%2520Aharoni%2520and%2520Chandrakumari%2520Suvarna%2520and%2520Iveel%2520Tsogsuren%2520and%2520Abdul%2520Rahman%2520Kreidieh%2520and%2520Chun-Ta%2520Lu%2520and%2520Neha%2520Arora%26entry.1292438233%3DScalable%2520general-purpose%2520representations%2520of%2520the%2520built%2520environment%2520are%2520crucial%2520for%2520geospatial%2520artificial%2520intelligence%2520applications.%2520This%2520paper%2520introduces%2520S2Vec%252C%2520a%2520novel%2520self-supervised%2520framework%2520for%2520learning%2520such%2520geospatial%2520embeddings.%2520S2Vec%2520uses%2520the%2520S2%2520Geometry%2520library%2520to%2520partition%2520large%2520areas%2520into%2520discrete%2520S2%2520cells%252C%2520rasterizes%2520built%2520environment%2520feature%2520vectors%2520within%2520cells%2520as%2520images%252C%2520and%2520applies%2520masked%2520autoencoding%2520on%2520these%2520rasterized%2520images%2520to%2520encode%2520the%2520feature%2520vectors.%2520This%2520approach%2520yields%2520task-agnostic%2520embeddings%2520that%2520capture%2520local%2520feature%2520characteristics%2520and%2520broader%2520spatial%2520relationships.%2520We%2520evaluate%2520S2Vec%2520on%2520several%2520large-scale%2520geospatial%2520prediction%2520tasks%252C%2520both%2520random%2520train/test%2520splits%2520%2528interpolation%2529%2520and%2520zero-shot%2520geographic%2520adaptation%2520%2528extrapolation%2529.%2520Our%2520experiments%2520show%2520S2Vec%2527s%2520competitive%2520performance%2520against%2520several%2520baselines%2520on%2520socioeconomic%2520tasks%252C%2520especially%2520the%2520geographic%2520adaptation%2520variant%252C%2520with%2520room%2520for%2520improvement%2520on%2520environmental%2520tasks.%2520We%2520also%2520explore%2520combining%2520S2Vec%2520embeddings%2520with%2520image-based%2520embeddings%2520downstream%252C%2520showing%2520that%2520such%2520multimodal%2520fusion%2520can%2520often%2520improve%2520performance.%2520Our%2520findings%2520highlight%2520how%2520S2Vec%2520can%2520learn%2520effective%2520general-purpose%2520geospatial%2520representations%2520of%2520the%2520built%2520environment%2520features%2520it%2520is%2520provided%252C%2520and%2520how%2520it%2520can%2520complement%2520other%2520data%2520modalities%2520in%2520geospatial%2520artificial%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2Vec%3A%20Self-Supervised%20Geospatial%20Embeddings%20for%20the%20Built%20Environment&entry.906535625=Shushman%20Choudhury%20and%20Elad%20Aharoni%20and%20Chandrakumari%20Suvarna%20and%20Iveel%20Tsogsuren%20and%20Abdul%20Rahman%20Kreidieh%20and%20Chun-Ta%20Lu%20and%20Neha%20Arora&entry.1292438233=Scalable%20general-purpose%20representations%20of%20the%20built%20environment%20are%20crucial%20for%20geospatial%20artificial%20intelligence%20applications.%20This%20paper%20introduces%20S2Vec%2C%20a%20novel%20self-supervised%20framework%20for%20learning%20such%20geospatial%20embeddings.%20S2Vec%20uses%20the%20S2%20Geometry%20library%20to%20partition%20large%20areas%20into%20discrete%20S2%20cells%2C%20rasterizes%20built%20environment%20feature%20vectors%20within%20cells%20as%20images%2C%20and%20applies%20masked%20autoencoding%20on%20these%20rasterized%20images%20to%20encode%20the%20feature%20vectors.%20This%20approach%20yields%20task-agnostic%20embeddings%20that%20capture%20local%20feature%20characteristics%20and%20broader%20spatial%20relationships.%20We%20evaluate%20S2Vec%20on%20several%20large-scale%20geospatial%20prediction%20tasks%2C%20both%20random%20train/test%20splits%20%28interpolation%29%20and%20zero-shot%20geographic%20adaptation%20%28extrapolation%29.%20Our%20experiments%20show%20S2Vec%27s%20competitive%20performance%20against%20several%20baselines%20on%20socioeconomic%20tasks%2C%20especially%20the%20geographic%20adaptation%20variant%2C%20with%20room%20for%20improvement%20on%20environmental%20tasks.%20We%20also%20explore%20combining%20S2Vec%20embeddings%20with%20image-based%20embeddings%20downstream%2C%20showing%20that%20such%20multimodal%20fusion%20can%20often%20improve%20performance.%20Our%20findings%20highlight%20how%20S2Vec%20can%20learn%20effective%20general-purpose%20geospatial%20representations%20of%20the%20built%20environment%20features%20it%20is%20provided%2C%20and%20how%20it%20can%20complement%20other%20data%20modalities%20in%20geospatial%20artificial%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2504.16942v2&entry.124074799=Read"},
{"title": "Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models", "author": "Beier Zhu and Kaihua Tang and Qianru Sun and Hanwang Zhang", "abstract": "Foundation models like CLIP allow zero-shot transfer on various tasks without additional training data. Yet, the zero-shot performance is less competitive than a fully supervised one. Thus, to enhance the performance, fine-tuning and ensembling are also commonly adopted to better fit the downstream tasks. However, we argue that such prior work has overlooked the inherent biases in foundation models. Due to the highly imbalanced Web-scale training set, these foundation models are inevitably skewed toward frequent semantics, and thus the subsequent fine-tuning or ensembling is still biased. In this study, we systematically examine the biases in foundation models and demonstrate the efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that bias estimation in foundation models is challenging, as most pre-train data cannot be explicitly accessed like in traditional long-tailed classification tasks. To this end, GLA has an optimization-based bias estimation approach for debiasing foundation models. As our work resolves a fundamental flaw in the pre-training, the proposed GLA demonstrates significant improvements across a diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on long-tailed classification. Codes are in https://github.com/BeierZhu/GLA.", "link": "http://arxiv.org/abs/2310.08106v4", "date": "2026-01-07", "relevancy": 2.5669, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5234}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5201}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Logit%20Adjustment%3A%20Calibrating%20Fine-tuned%20Models%20by%20Removing%20Label%20Bias%20in%20Foundation%20Models&body=Title%3A%20Generalized%20Logit%20Adjustment%3A%20Calibrating%20Fine-tuned%20Models%20by%20Removing%20Label%20Bias%20in%20Foundation%20Models%0AAuthor%3A%20Beier%20Zhu%20and%20Kaihua%20Tang%20and%20Qianru%20Sun%20and%20Hanwang%20Zhang%0AAbstract%3A%20Foundation%20models%20like%20CLIP%20allow%20zero-shot%20transfer%20on%20various%20tasks%20without%20additional%20training%20data.%20Yet%2C%20the%20zero-shot%20performance%20is%20less%20competitive%20than%20a%20fully%20supervised%20one.%20Thus%2C%20to%20enhance%20the%20performance%2C%20fine-tuning%20and%20ensembling%20are%20also%20commonly%20adopted%20to%20better%20fit%20the%20downstream%20tasks.%20However%2C%20we%20argue%20that%20such%20prior%20work%20has%20overlooked%20the%20inherent%20biases%20in%20foundation%20models.%20Due%20to%20the%20highly%20imbalanced%20Web-scale%20training%20set%2C%20these%20foundation%20models%20are%20inevitably%20skewed%20toward%20frequent%20semantics%2C%20and%20thus%20the%20subsequent%20fine-tuning%20or%20ensembling%20is%20still%20biased.%20In%20this%20study%2C%20we%20systematically%20examine%20the%20biases%20in%20foundation%20models%20and%20demonstrate%20the%20efficacy%20of%20our%20proposed%20Generalized%20Logit%20Adjustment%20%28GLA%29%20method.%20Note%20that%20bias%20estimation%20in%20foundation%20models%20is%20challenging%2C%20as%20most%20pre-train%20data%20cannot%20be%20explicitly%20accessed%20like%20in%20traditional%20long-tailed%20classification%20tasks.%20To%20this%20end%2C%20GLA%20has%20an%20optimization-based%20bias%20estimation%20approach%20for%20debiasing%20foundation%20models.%20As%20our%20work%20resolves%20a%20fundamental%20flaw%20in%20the%20pre-training%2C%20the%20proposed%20GLA%20demonstrates%20significant%20improvements%20across%20a%20diverse%20range%20of%20tasks%3A%20it%20achieves%201.5%20pp%20accuracy%20gains%20on%20ImageNet%2C%20an%20large%20average%20improvement%20%281.4-4.6%20pp%29%20on%2011%20few-shot%20datasets%2C%202.4%20pp%20gains%20on%20long-tailed%20classification.%20Codes%20are%20in%20https%3A//github.com/BeierZhu/GLA.%0ALink%3A%20http%3A//arxiv.org/abs/2310.08106v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Logit%2520Adjustment%253A%2520Calibrating%2520Fine-tuned%2520Models%2520by%2520Removing%2520Label%2520Bias%2520in%2520Foundation%2520Models%26entry.906535625%3DBeier%2520Zhu%2520and%2520Kaihua%2520Tang%2520and%2520Qianru%2520Sun%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3DFoundation%2520models%2520like%2520CLIP%2520allow%2520zero-shot%2520transfer%2520on%2520various%2520tasks%2520without%2520additional%2520training%2520data.%2520Yet%252C%2520the%2520zero-shot%2520performance%2520is%2520less%2520competitive%2520than%2520a%2520fully%2520supervised%2520one.%2520Thus%252C%2520to%2520enhance%2520the%2520performance%252C%2520fine-tuning%2520and%2520ensembling%2520are%2520also%2520commonly%2520adopted%2520to%2520better%2520fit%2520the%2520downstream%2520tasks.%2520However%252C%2520we%2520argue%2520that%2520such%2520prior%2520work%2520has%2520overlooked%2520the%2520inherent%2520biases%2520in%2520foundation%2520models.%2520Due%2520to%2520the%2520highly%2520imbalanced%2520Web-scale%2520training%2520set%252C%2520these%2520foundation%2520models%2520are%2520inevitably%2520skewed%2520toward%2520frequent%2520semantics%252C%2520and%2520thus%2520the%2520subsequent%2520fine-tuning%2520or%2520ensembling%2520is%2520still%2520biased.%2520In%2520this%2520study%252C%2520we%2520systematically%2520examine%2520the%2520biases%2520in%2520foundation%2520models%2520and%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520Generalized%2520Logit%2520Adjustment%2520%2528GLA%2529%2520method.%2520Note%2520that%2520bias%2520estimation%2520in%2520foundation%2520models%2520is%2520challenging%252C%2520as%2520most%2520pre-train%2520data%2520cannot%2520be%2520explicitly%2520accessed%2520like%2520in%2520traditional%2520long-tailed%2520classification%2520tasks.%2520To%2520this%2520end%252C%2520GLA%2520has%2520an%2520optimization-based%2520bias%2520estimation%2520approach%2520for%2520debiasing%2520foundation%2520models.%2520As%2520our%2520work%2520resolves%2520a%2520fundamental%2520flaw%2520in%2520the%2520pre-training%252C%2520the%2520proposed%2520GLA%2520demonstrates%2520significant%2520improvements%2520across%2520a%2520diverse%2520range%2520of%2520tasks%253A%2520it%2520achieves%25201.5%2520pp%2520accuracy%2520gains%2520on%2520ImageNet%252C%2520an%2520large%2520average%2520improvement%2520%25281.4-4.6%2520pp%2529%2520on%252011%2520few-shot%2520datasets%252C%25202.4%2520pp%2520gains%2520on%2520long-tailed%2520classification.%2520Codes%2520are%2520in%2520https%253A//github.com/BeierZhu/GLA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08106v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Logit%20Adjustment%3A%20Calibrating%20Fine-tuned%20Models%20by%20Removing%20Label%20Bias%20in%20Foundation%20Models&entry.906535625=Beier%20Zhu%20and%20Kaihua%20Tang%20and%20Qianru%20Sun%20and%20Hanwang%20Zhang&entry.1292438233=Foundation%20models%20like%20CLIP%20allow%20zero-shot%20transfer%20on%20various%20tasks%20without%20additional%20training%20data.%20Yet%2C%20the%20zero-shot%20performance%20is%20less%20competitive%20than%20a%20fully%20supervised%20one.%20Thus%2C%20to%20enhance%20the%20performance%2C%20fine-tuning%20and%20ensembling%20are%20also%20commonly%20adopted%20to%20better%20fit%20the%20downstream%20tasks.%20However%2C%20we%20argue%20that%20such%20prior%20work%20has%20overlooked%20the%20inherent%20biases%20in%20foundation%20models.%20Due%20to%20the%20highly%20imbalanced%20Web-scale%20training%20set%2C%20these%20foundation%20models%20are%20inevitably%20skewed%20toward%20frequent%20semantics%2C%20and%20thus%20the%20subsequent%20fine-tuning%20or%20ensembling%20is%20still%20biased.%20In%20this%20study%2C%20we%20systematically%20examine%20the%20biases%20in%20foundation%20models%20and%20demonstrate%20the%20efficacy%20of%20our%20proposed%20Generalized%20Logit%20Adjustment%20%28GLA%29%20method.%20Note%20that%20bias%20estimation%20in%20foundation%20models%20is%20challenging%2C%20as%20most%20pre-train%20data%20cannot%20be%20explicitly%20accessed%20like%20in%20traditional%20long-tailed%20classification%20tasks.%20To%20this%20end%2C%20GLA%20has%20an%20optimization-based%20bias%20estimation%20approach%20for%20debiasing%20foundation%20models.%20As%20our%20work%20resolves%20a%20fundamental%20flaw%20in%20the%20pre-training%2C%20the%20proposed%20GLA%20demonstrates%20significant%20improvements%20across%20a%20diverse%20range%20of%20tasks%3A%20it%20achieves%201.5%20pp%20accuracy%20gains%20on%20ImageNet%2C%20an%20large%20average%20improvement%20%281.4-4.6%20pp%29%20on%2011%20few-shot%20datasets%2C%202.4%20pp%20gains%20on%20long-tailed%20classification.%20Codes%20are%20in%20https%3A//github.com/BeierZhu/GLA.&entry.1838667208=http%3A//arxiv.org/abs/2310.08106v4&entry.124074799=Read"},
{"title": "BiLO: Bilevel Local Operator Learning for PDE Inverse Problems", "author": "Ray Zirui Zhang and Christopher E. Miles and Xiaohui Xie and John S. Lowengrub", "abstract": "We propose a new neural network based method for solving inverse problems for partial differential equations (PDEs) by formulating the PDE inverse problem as a bilevel optimization problem. At the upper level, we minimize the data loss with respect to the PDE parameters. At the lower level, we train a neural network to locally approximate the PDE solution operator in the neighborhood of a given set of PDE parameters, which enables an accurate approximation of the descent direction for the upper level optimization problem. The lower level loss function includes the L2 norms of both the residual and its derivative with respect to the PDE parameters. We apply gradient descent simultaneously on both the upper and lower level optimization problems, leading to an effective and fast algorithm. The method, which we refer to as BiLO (Bilevel Local Operator learning), is also able to efficiently infer unknown functions in the PDEs through the introduction of an auxiliary variable. We provide a theoretical analysis that justifies our approach. Through extensive experiments over multiple PDE systems, we demonstrate that our method enforces strong PDE constraints, is robust to sparse and noisy data, and eliminates the need to balance the residual and the data loss, which is inherent to the soft PDE constraints in many existing methods.", "link": "http://arxiv.org/abs/2404.17789v6", "date": "2026-01-07", "relevancy": 2.5653, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5278}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5066}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiLO%3A%20Bilevel%20Local%20Operator%20Learning%20for%20PDE%20Inverse%20Problems&body=Title%3A%20BiLO%3A%20Bilevel%20Local%20Operator%20Learning%20for%20PDE%20Inverse%20Problems%0AAuthor%3A%20Ray%20Zirui%20Zhang%20and%20Christopher%20E.%20Miles%20and%20Xiaohui%20Xie%20and%20John%20S.%20Lowengrub%0AAbstract%3A%20We%20propose%20a%20new%20neural%20network%20based%20method%20for%20solving%20inverse%20problems%20for%20partial%20differential%20equations%20%28PDEs%29%20by%20formulating%20the%20PDE%20inverse%20problem%20as%20a%20bilevel%20optimization%20problem.%20At%20the%20upper%20level%2C%20we%20minimize%20the%20data%20loss%20with%20respect%20to%20the%20PDE%20parameters.%20At%20the%20lower%20level%2C%20we%20train%20a%20neural%20network%20to%20locally%20approximate%20the%20PDE%20solution%20operator%20in%20the%20neighborhood%20of%20a%20given%20set%20of%20PDE%20parameters%2C%20which%20enables%20an%20accurate%20approximation%20of%20the%20descent%20direction%20for%20the%20upper%20level%20optimization%20problem.%20The%20lower%20level%20loss%20function%20includes%20the%20L2%20norms%20of%20both%20the%20residual%20and%20its%20derivative%20with%20respect%20to%20the%20PDE%20parameters.%20We%20apply%20gradient%20descent%20simultaneously%20on%20both%20the%20upper%20and%20lower%20level%20optimization%20problems%2C%20leading%20to%20an%20effective%20and%20fast%20algorithm.%20The%20method%2C%20which%20we%20refer%20to%20as%20BiLO%20%28Bilevel%20Local%20Operator%20learning%29%2C%20is%20also%20able%20to%20efficiently%20infer%20unknown%20functions%20in%20the%20PDEs%20through%20the%20introduction%20of%20an%20auxiliary%20variable.%20We%20provide%20a%20theoretical%20analysis%20that%20justifies%20our%20approach.%20Through%20extensive%20experiments%20over%20multiple%20PDE%20systems%2C%20we%20demonstrate%20that%20our%20method%20enforces%20strong%20PDE%20constraints%2C%20is%20robust%20to%20sparse%20and%20noisy%20data%2C%20and%20eliminates%20the%20need%20to%20balance%20the%20residual%20and%20the%20data%20loss%2C%20which%20is%20inherent%20to%20the%20soft%20PDE%20constraints%20in%20many%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2404.17789v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiLO%253A%2520Bilevel%2520Local%2520Operator%2520Learning%2520for%2520PDE%2520Inverse%2520Problems%26entry.906535625%3DRay%2520Zirui%2520Zhang%2520and%2520Christopher%2520E.%2520Miles%2520and%2520Xiaohui%2520Xie%2520and%2520John%2520S.%2520Lowengrub%26entry.1292438233%3DWe%2520propose%2520a%2520new%2520neural%2520network%2520based%2520method%2520for%2520solving%2520inverse%2520problems%2520for%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520by%2520formulating%2520the%2520PDE%2520inverse%2520problem%2520as%2520a%2520bilevel%2520optimization%2520problem.%2520At%2520the%2520upper%2520level%252C%2520we%2520minimize%2520the%2520data%2520loss%2520with%2520respect%2520to%2520the%2520PDE%2520parameters.%2520At%2520the%2520lower%2520level%252C%2520we%2520train%2520a%2520neural%2520network%2520to%2520locally%2520approximate%2520the%2520PDE%2520solution%2520operator%2520in%2520the%2520neighborhood%2520of%2520a%2520given%2520set%2520of%2520PDE%2520parameters%252C%2520which%2520enables%2520an%2520accurate%2520approximation%2520of%2520the%2520descent%2520direction%2520for%2520the%2520upper%2520level%2520optimization%2520problem.%2520The%2520lower%2520level%2520loss%2520function%2520includes%2520the%2520L2%2520norms%2520of%2520both%2520the%2520residual%2520and%2520its%2520derivative%2520with%2520respect%2520to%2520the%2520PDE%2520parameters.%2520We%2520apply%2520gradient%2520descent%2520simultaneously%2520on%2520both%2520the%2520upper%2520and%2520lower%2520level%2520optimization%2520problems%252C%2520leading%2520to%2520an%2520effective%2520and%2520fast%2520algorithm.%2520The%2520method%252C%2520which%2520we%2520refer%2520to%2520as%2520BiLO%2520%2528Bilevel%2520Local%2520Operator%2520learning%2529%252C%2520is%2520also%2520able%2520to%2520efficiently%2520infer%2520unknown%2520functions%2520in%2520the%2520PDEs%2520through%2520the%2520introduction%2520of%2520an%2520auxiliary%2520variable.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520that%2520justifies%2520our%2520approach.%2520Through%2520extensive%2520experiments%2520over%2520multiple%2520PDE%2520systems%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520enforces%2520strong%2520PDE%2520constraints%252C%2520is%2520robust%2520to%2520sparse%2520and%2520noisy%2520data%252C%2520and%2520eliminates%2520the%2520need%2520to%2520balance%2520the%2520residual%2520and%2520the%2520data%2520loss%252C%2520which%2520is%2520inherent%2520to%2520the%2520soft%2520PDE%2520constraints%2520in%2520many%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17789v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiLO%3A%20Bilevel%20Local%20Operator%20Learning%20for%20PDE%20Inverse%20Problems&entry.906535625=Ray%20Zirui%20Zhang%20and%20Christopher%20E.%20Miles%20and%20Xiaohui%20Xie%20and%20John%20S.%20Lowengrub&entry.1292438233=We%20propose%20a%20new%20neural%20network%20based%20method%20for%20solving%20inverse%20problems%20for%20partial%20differential%20equations%20%28PDEs%29%20by%20formulating%20the%20PDE%20inverse%20problem%20as%20a%20bilevel%20optimization%20problem.%20At%20the%20upper%20level%2C%20we%20minimize%20the%20data%20loss%20with%20respect%20to%20the%20PDE%20parameters.%20At%20the%20lower%20level%2C%20we%20train%20a%20neural%20network%20to%20locally%20approximate%20the%20PDE%20solution%20operator%20in%20the%20neighborhood%20of%20a%20given%20set%20of%20PDE%20parameters%2C%20which%20enables%20an%20accurate%20approximation%20of%20the%20descent%20direction%20for%20the%20upper%20level%20optimization%20problem.%20The%20lower%20level%20loss%20function%20includes%20the%20L2%20norms%20of%20both%20the%20residual%20and%20its%20derivative%20with%20respect%20to%20the%20PDE%20parameters.%20We%20apply%20gradient%20descent%20simultaneously%20on%20both%20the%20upper%20and%20lower%20level%20optimization%20problems%2C%20leading%20to%20an%20effective%20and%20fast%20algorithm.%20The%20method%2C%20which%20we%20refer%20to%20as%20BiLO%20%28Bilevel%20Local%20Operator%20learning%29%2C%20is%20also%20able%20to%20efficiently%20infer%20unknown%20functions%20in%20the%20PDEs%20through%20the%20introduction%20of%20an%20auxiliary%20variable.%20We%20provide%20a%20theoretical%20analysis%20that%20justifies%20our%20approach.%20Through%20extensive%20experiments%20over%20multiple%20PDE%20systems%2C%20we%20demonstrate%20that%20our%20method%20enforces%20strong%20PDE%20constraints%2C%20is%20robust%20to%20sparse%20and%20noisy%20data%2C%20and%20eliminates%20the%20need%20to%20balance%20the%20residual%20and%20the%20data%20loss%2C%20which%20is%20inherent%20to%20the%20soft%20PDE%20constraints%20in%20many%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2404.17789v6&entry.124074799=Read"},
{"title": "FUSION: Full-Body Unified Motion Prior for Body and Hands via Diffusion", "author": "Enes Duran and Nikos Athanasiou and Muhammed Kocabas and Michael J. Black and Omid Taheri", "abstract": "Hands are central to interacting with our surroundings and conveying gestures, making their inclusion essential for full-body motion synthesis. Despite this, existing human motion synthesis methods fall short: some ignore hand motions entirely, while others generate full-body motions only for narrowly scoped tasks under highly constrained settings. A key obstacle is the lack of large-scale datasets that jointly capture diverse full-body motion with detailed hand articulation. While some datasets capture both, they are limited in scale and diversity. Conversely, large-scale datasets typically focus either on body motion without hands or on hand motions without the body. To overcome this, we curate and unify existing hand motion datasets with large-scale body motion data to generate full-body sequences that capture both hand and body. We then propose the first diffusion-based unconditional full-body motion prior, FUSION, which jointly models body and hand motion. Despite using a pose-based motion representation, FUSION surpasses state-of-the-art skeletal control models on the Keypoint Tracking task in the HumanML3D dataset and achieves superior motion naturalness. Beyond standard benchmarks, we demonstrate that FUSION can go beyond typical uses of motion priors through two applications: (1) generating detailed full-body motion including fingers during interaction given the motion of an object, and (2) generating Self-Interaction motions using an LLM to transform natural language cues into actionable motion constraints. For these applications, we develop an optimization pipeline that refines the latent space of our diffusion model to generate task-specific motions. Experiments on these tasks highlight precise control over hand motion while maintaining plausible full-body coordination. The code will be public.", "link": "http://arxiv.org/abs/2601.03959v1", "date": "2026-01-07", "relevancy": 2.5448, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7263}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5795}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FUSION%3A%20Full-Body%20Unified%20Motion%20Prior%20for%20Body%20and%20Hands%20via%20Diffusion&body=Title%3A%20FUSION%3A%20Full-Body%20Unified%20Motion%20Prior%20for%20Body%20and%20Hands%20via%20Diffusion%0AAuthor%3A%20Enes%20Duran%20and%20Nikos%20Athanasiou%20and%20Muhammed%20Kocabas%20and%20Michael%20J.%20Black%20and%20Omid%20Taheri%0AAbstract%3A%20Hands%20are%20central%20to%20interacting%20with%20our%20surroundings%20and%20conveying%20gestures%2C%20making%20their%20inclusion%20essential%20for%20full-body%20motion%20synthesis.%20Despite%20this%2C%20existing%20human%20motion%20synthesis%20methods%20fall%20short%3A%20some%20ignore%20hand%20motions%20entirely%2C%20while%20others%20generate%20full-body%20motions%20only%20for%20narrowly%20scoped%20tasks%20under%20highly%20constrained%20settings.%20A%20key%20obstacle%20is%20the%20lack%20of%20large-scale%20datasets%20that%20jointly%20capture%20diverse%20full-body%20motion%20with%20detailed%20hand%20articulation.%20While%20some%20datasets%20capture%20both%2C%20they%20are%20limited%20in%20scale%20and%20diversity.%20Conversely%2C%20large-scale%20datasets%20typically%20focus%20either%20on%20body%20motion%20without%20hands%20or%20on%20hand%20motions%20without%20the%20body.%20To%20overcome%20this%2C%20we%20curate%20and%20unify%20existing%20hand%20motion%20datasets%20with%20large-scale%20body%20motion%20data%20to%20generate%20full-body%20sequences%20that%20capture%20both%20hand%20and%20body.%20We%20then%20propose%20the%20first%20diffusion-based%20unconditional%20full-body%20motion%20prior%2C%20FUSION%2C%20which%20jointly%20models%20body%20and%20hand%20motion.%20Despite%20using%20a%20pose-based%20motion%20representation%2C%20FUSION%20surpasses%20state-of-the-art%20skeletal%20control%20models%20on%20the%20Keypoint%20Tracking%20task%20in%20the%20HumanML3D%20dataset%20and%20achieves%20superior%20motion%20naturalness.%20Beyond%20standard%20benchmarks%2C%20we%20demonstrate%20that%20FUSION%20can%20go%20beyond%20typical%20uses%20of%20motion%20priors%20through%20two%20applications%3A%20%281%29%20generating%20detailed%20full-body%20motion%20including%20fingers%20during%20interaction%20given%20the%20motion%20of%20an%20object%2C%20and%20%282%29%20generating%20Self-Interaction%20motions%20using%20an%20LLM%20to%20transform%20natural%20language%20cues%20into%20actionable%20motion%20constraints.%20For%20these%20applications%2C%20we%20develop%20an%20optimization%20pipeline%20that%20refines%20the%20latent%20space%20of%20our%20diffusion%20model%20to%20generate%20task-specific%20motions.%20Experiments%20on%20these%20tasks%20highlight%20precise%20control%20over%20hand%20motion%20while%20maintaining%20plausible%20full-body%20coordination.%20The%20code%20will%20be%20public.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFUSION%253A%2520Full-Body%2520Unified%2520Motion%2520Prior%2520for%2520Body%2520and%2520Hands%2520via%2520Diffusion%26entry.906535625%3DEnes%2520Duran%2520and%2520Nikos%2520Athanasiou%2520and%2520Muhammed%2520Kocabas%2520and%2520Michael%2520J.%2520Black%2520and%2520Omid%2520Taheri%26entry.1292438233%3DHands%2520are%2520central%2520to%2520interacting%2520with%2520our%2520surroundings%2520and%2520conveying%2520gestures%252C%2520making%2520their%2520inclusion%2520essential%2520for%2520full-body%2520motion%2520synthesis.%2520Despite%2520this%252C%2520existing%2520human%2520motion%2520synthesis%2520methods%2520fall%2520short%253A%2520some%2520ignore%2520hand%2520motions%2520entirely%252C%2520while%2520others%2520generate%2520full-body%2520motions%2520only%2520for%2520narrowly%2520scoped%2520tasks%2520under%2520highly%2520constrained%2520settings.%2520A%2520key%2520obstacle%2520is%2520the%2520lack%2520of%2520large-scale%2520datasets%2520that%2520jointly%2520capture%2520diverse%2520full-body%2520motion%2520with%2520detailed%2520hand%2520articulation.%2520While%2520some%2520datasets%2520capture%2520both%252C%2520they%2520are%2520limited%2520in%2520scale%2520and%2520diversity.%2520Conversely%252C%2520large-scale%2520datasets%2520typically%2520focus%2520either%2520on%2520body%2520motion%2520without%2520hands%2520or%2520on%2520hand%2520motions%2520without%2520the%2520body.%2520To%2520overcome%2520this%252C%2520we%2520curate%2520and%2520unify%2520existing%2520hand%2520motion%2520datasets%2520with%2520large-scale%2520body%2520motion%2520data%2520to%2520generate%2520full-body%2520sequences%2520that%2520capture%2520both%2520hand%2520and%2520body.%2520We%2520then%2520propose%2520the%2520first%2520diffusion-based%2520unconditional%2520full-body%2520motion%2520prior%252C%2520FUSION%252C%2520which%2520jointly%2520models%2520body%2520and%2520hand%2520motion.%2520Despite%2520using%2520a%2520pose-based%2520motion%2520representation%252C%2520FUSION%2520surpasses%2520state-of-the-art%2520skeletal%2520control%2520models%2520on%2520the%2520Keypoint%2520Tracking%2520task%2520in%2520the%2520HumanML3D%2520dataset%2520and%2520achieves%2520superior%2520motion%2520naturalness.%2520Beyond%2520standard%2520benchmarks%252C%2520we%2520demonstrate%2520that%2520FUSION%2520can%2520go%2520beyond%2520typical%2520uses%2520of%2520motion%2520priors%2520through%2520two%2520applications%253A%2520%25281%2529%2520generating%2520detailed%2520full-body%2520motion%2520including%2520fingers%2520during%2520interaction%2520given%2520the%2520motion%2520of%2520an%2520object%252C%2520and%2520%25282%2529%2520generating%2520Self-Interaction%2520motions%2520using%2520an%2520LLM%2520to%2520transform%2520natural%2520language%2520cues%2520into%2520actionable%2520motion%2520constraints.%2520For%2520these%2520applications%252C%2520we%2520develop%2520an%2520optimization%2520pipeline%2520that%2520refines%2520the%2520latent%2520space%2520of%2520our%2520diffusion%2520model%2520to%2520generate%2520task-specific%2520motions.%2520Experiments%2520on%2520these%2520tasks%2520highlight%2520precise%2520control%2520over%2520hand%2520motion%2520while%2520maintaining%2520plausible%2520full-body%2520coordination.%2520The%2520code%2520will%2520be%2520public.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FUSION%3A%20Full-Body%20Unified%20Motion%20Prior%20for%20Body%20and%20Hands%20via%20Diffusion&entry.906535625=Enes%20Duran%20and%20Nikos%20Athanasiou%20and%20Muhammed%20Kocabas%20and%20Michael%20J.%20Black%20and%20Omid%20Taheri&entry.1292438233=Hands%20are%20central%20to%20interacting%20with%20our%20surroundings%20and%20conveying%20gestures%2C%20making%20their%20inclusion%20essential%20for%20full-body%20motion%20synthesis.%20Despite%20this%2C%20existing%20human%20motion%20synthesis%20methods%20fall%20short%3A%20some%20ignore%20hand%20motions%20entirely%2C%20while%20others%20generate%20full-body%20motions%20only%20for%20narrowly%20scoped%20tasks%20under%20highly%20constrained%20settings.%20A%20key%20obstacle%20is%20the%20lack%20of%20large-scale%20datasets%20that%20jointly%20capture%20diverse%20full-body%20motion%20with%20detailed%20hand%20articulation.%20While%20some%20datasets%20capture%20both%2C%20they%20are%20limited%20in%20scale%20and%20diversity.%20Conversely%2C%20large-scale%20datasets%20typically%20focus%20either%20on%20body%20motion%20without%20hands%20or%20on%20hand%20motions%20without%20the%20body.%20To%20overcome%20this%2C%20we%20curate%20and%20unify%20existing%20hand%20motion%20datasets%20with%20large-scale%20body%20motion%20data%20to%20generate%20full-body%20sequences%20that%20capture%20both%20hand%20and%20body.%20We%20then%20propose%20the%20first%20diffusion-based%20unconditional%20full-body%20motion%20prior%2C%20FUSION%2C%20which%20jointly%20models%20body%20and%20hand%20motion.%20Despite%20using%20a%20pose-based%20motion%20representation%2C%20FUSION%20surpasses%20state-of-the-art%20skeletal%20control%20models%20on%20the%20Keypoint%20Tracking%20task%20in%20the%20HumanML3D%20dataset%20and%20achieves%20superior%20motion%20naturalness.%20Beyond%20standard%20benchmarks%2C%20we%20demonstrate%20that%20FUSION%20can%20go%20beyond%20typical%20uses%20of%20motion%20priors%20through%20two%20applications%3A%20%281%29%20generating%20detailed%20full-body%20motion%20including%20fingers%20during%20interaction%20given%20the%20motion%20of%20an%20object%2C%20and%20%282%29%20generating%20Self-Interaction%20motions%20using%20an%20LLM%20to%20transform%20natural%20language%20cues%20into%20actionable%20motion%20constraints.%20For%20these%20applications%2C%20we%20develop%20an%20optimization%20pipeline%20that%20refines%20the%20latent%20space%20of%20our%20diffusion%20model%20to%20generate%20task-specific%20motions.%20Experiments%20on%20these%20tasks%20highlight%20precise%20control%20over%20hand%20motion%20while%20maintaining%20plausible%20full-body%20coordination.%20The%20code%20will%20be%20public.&entry.1838667208=http%3A//arxiv.org/abs/2601.03959v1&entry.124074799=Read"},
{"title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures", "author": "Nicolas Lacroix and Mireille Blay-Fornarino and S\u00e9bastien Mosser and Frederic Precioso", "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.\n  Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.\n  Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.", "link": "http://arxiv.org/abs/2601.03988v1", "date": "2026-01-07", "relevancy": 2.5333, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Small%20Language%20Models%20to%20Reverse-Engineer%20Machine%20Learning%20Pipelines%20Structures&body=Title%3A%20Using%20Small%20Language%20Models%20to%20Reverse-Engineer%20Machine%20Learning%20Pipelines%20Structures%0AAuthor%3A%20Nicolas%20Lacroix%20and%20Mireille%20Blay-Fornarino%20and%20S%C3%A9bastien%20Mosser%20and%20Frederic%20Precioso%0AAbstract%3A%20Background%3A%20Extracting%20the%20stages%20that%20structure%20Machine%20Learning%20%28ML%29%20pipelines%20from%20source%20code%20is%20key%20for%20gaining%20a%20deeper%20understanding%20of%20data%20science%20practices.%20However%2C%20the%20diversity%20caused%20by%20the%20constant%20evolution%20of%20the%20ML%20ecosystem%20%28e.g.%2C%20algorithms%2C%20libraries%2C%20datasets%29%20makes%20this%20task%20challenging.%20Existing%20approaches%20either%20depend%20on%20non-scalable%2C%20manual%20labeling%2C%20or%20on%20ML%20classifiers%20that%20do%20not%20properly%20support%20the%20diversity%20of%20the%20domain.%20These%20limitations%20highlight%20the%20need%20for%20more%20flexible%20and%20reliable%20solutions.%0A%20%20Objective%3A%20We%20evaluate%20whether%20Small%20Language%20Models%20%28SLMs%29%20can%20leverage%20their%20code%20understanding%20and%20classification%20abilities%20to%20address%20these%20limitations%2C%20and%20subsequently%20how%20they%20can%20advance%20our%20understanding%20of%20data%20science%20practices.%0A%20%20Method%3A%20We%20conduct%20a%20confirmatory%20study%20based%20on%20two%20reference%20works%20selected%20for%20their%20relevance%20regarding%20current%20state-of-the-art%27s%20limitations.%20First%2C%20we%20compare%20several%20SLMs%20using%20Cochran%27s%20Q%20test.%20The%20best-performing%20model%20is%20then%20evaluated%20against%20the%20reference%20studies%20using%20two%20distinct%20McNemar%27s%20tests.%20We%20further%20analyze%20how%20variations%20in%20taxonomy%20definitions%20affect%20performance%20through%20an%20additional%20Cochran%27s%20Q%20test.%20Finally%2C%20a%20goodness-of-fit%20analysis%20is%20conducted%20using%20Pearson%27s%20chi-squared%20tests%20to%20compare%20our%20insights%20on%20data%20science%20practices%20with%20those%20from%20prior%20studies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Small%2520Language%2520Models%2520to%2520Reverse-Engineer%2520Machine%2520Learning%2520Pipelines%2520Structures%26entry.906535625%3DNicolas%2520Lacroix%2520and%2520Mireille%2520Blay-Fornarino%2520and%2520S%25C3%25A9bastien%2520Mosser%2520and%2520Frederic%2520Precioso%26entry.1292438233%3DBackground%253A%2520Extracting%2520the%2520stages%2520that%2520structure%2520Machine%2520Learning%2520%2528ML%2529%2520pipelines%2520from%2520source%2520code%2520is%2520key%2520for%2520gaining%2520a%2520deeper%2520understanding%2520of%2520data%2520science%2520practices.%2520However%252C%2520the%2520diversity%2520caused%2520by%2520the%2520constant%2520evolution%2520of%2520the%2520ML%2520ecosystem%2520%2528e.g.%252C%2520algorithms%252C%2520libraries%252C%2520datasets%2529%2520makes%2520this%2520task%2520challenging.%2520Existing%2520approaches%2520either%2520depend%2520on%2520non-scalable%252C%2520manual%2520labeling%252C%2520or%2520on%2520ML%2520classifiers%2520that%2520do%2520not%2520properly%2520support%2520the%2520diversity%2520of%2520the%2520domain.%2520These%2520limitations%2520highlight%2520the%2520need%2520for%2520more%2520flexible%2520and%2520reliable%2520solutions.%250A%2520%2520Objective%253A%2520We%2520evaluate%2520whether%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520can%2520leverage%2520their%2520code%2520understanding%2520and%2520classification%2520abilities%2520to%2520address%2520these%2520limitations%252C%2520and%2520subsequently%2520how%2520they%2520can%2520advance%2520our%2520understanding%2520of%2520data%2520science%2520practices.%250A%2520%2520Method%253A%2520We%2520conduct%2520a%2520confirmatory%2520study%2520based%2520on%2520two%2520reference%2520works%2520selected%2520for%2520their%2520relevance%2520regarding%2520current%2520state-of-the-art%2527s%2520limitations.%2520First%252C%2520we%2520compare%2520several%2520SLMs%2520using%2520Cochran%2527s%2520Q%2520test.%2520The%2520best-performing%2520model%2520is%2520then%2520evaluated%2520against%2520the%2520reference%2520studies%2520using%2520two%2520distinct%2520McNemar%2527s%2520tests.%2520We%2520further%2520analyze%2520how%2520variations%2520in%2520taxonomy%2520definitions%2520affect%2520performance%2520through%2520an%2520additional%2520Cochran%2527s%2520Q%2520test.%2520Finally%252C%2520a%2520goodness-of-fit%2520analysis%2520is%2520conducted%2520using%2520Pearson%2527s%2520chi-squared%2520tests%2520to%2520compare%2520our%2520insights%2520on%2520data%2520science%2520practices%2520with%2520those%2520from%2520prior%2520studies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Small%20Language%20Models%20to%20Reverse-Engineer%20Machine%20Learning%20Pipelines%20Structures&entry.906535625=Nicolas%20Lacroix%20and%20Mireille%20Blay-Fornarino%20and%20S%C3%A9bastien%20Mosser%20and%20Frederic%20Precioso&entry.1292438233=Background%3A%20Extracting%20the%20stages%20that%20structure%20Machine%20Learning%20%28ML%29%20pipelines%20from%20source%20code%20is%20key%20for%20gaining%20a%20deeper%20understanding%20of%20data%20science%20practices.%20However%2C%20the%20diversity%20caused%20by%20the%20constant%20evolution%20of%20the%20ML%20ecosystem%20%28e.g.%2C%20algorithms%2C%20libraries%2C%20datasets%29%20makes%20this%20task%20challenging.%20Existing%20approaches%20either%20depend%20on%20non-scalable%2C%20manual%20labeling%2C%20or%20on%20ML%20classifiers%20that%20do%20not%20properly%20support%20the%20diversity%20of%20the%20domain.%20These%20limitations%20highlight%20the%20need%20for%20more%20flexible%20and%20reliable%20solutions.%0A%20%20Objective%3A%20We%20evaluate%20whether%20Small%20Language%20Models%20%28SLMs%29%20can%20leverage%20their%20code%20understanding%20and%20classification%20abilities%20to%20address%20these%20limitations%2C%20and%20subsequently%20how%20they%20can%20advance%20our%20understanding%20of%20data%20science%20practices.%0A%20%20Method%3A%20We%20conduct%20a%20confirmatory%20study%20based%20on%20two%20reference%20works%20selected%20for%20their%20relevance%20regarding%20current%20state-of-the-art%27s%20limitations.%20First%2C%20we%20compare%20several%20SLMs%20using%20Cochran%27s%20Q%20test.%20The%20best-performing%20model%20is%20then%20evaluated%20against%20the%20reference%20studies%20using%20two%20distinct%20McNemar%27s%20tests.%20We%20further%20analyze%20how%20variations%20in%20taxonomy%20definitions%20affect%20performance%20through%20an%20additional%20Cochran%27s%20Q%20test.%20Finally%2C%20a%20goodness-of-fit%20analysis%20is%20conducted%20using%20Pearson%27s%20chi-squared%20tests%20to%20compare%20our%20insights%20on%20data%20science%20practices%20with%20those%20from%20prior%20studies.&entry.1838667208=http%3A//arxiv.org/abs/2601.03988v1&entry.124074799=Read"},
{"title": "ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models", "author": "Nikhil Anand and Shwetha Somasundaram and Anirudh Phukan and Apoorv Saxena and Koyel Mukherjee", "abstract": "Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.", "link": "http://arxiv.org/abs/2601.04131v1", "date": "2026-01-07", "relevancy": 2.5313, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextFocus%3A%20Activation%20Steering%20for%20Contextual%20Faithfulness%20in%20Large%20Language%20Models&body=Title%3A%20ContextFocus%3A%20Activation%20Steering%20for%20Contextual%20Faithfulness%20in%20Large%20Language%20Models%0AAuthor%3A%20Nikhil%20Anand%20and%20Shwetha%20Somasundaram%20and%20Anirudh%20Phukan%20and%20Apoorv%20Saxena%20and%20Koyel%20Mukherjee%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20encode%20vast%20amounts%20of%20parametric%20knowledge%20during%20pre-training.%20As%20world%20knowledge%20evolves%2C%20effective%20deployment%20increasingly%20depends%20on%20their%20ability%20to%20faithfully%20follow%20externally%20retrieved%20context.%20When%20such%20evidence%20conflicts%20with%20the%20model%27s%20internal%20knowledge%2C%20LLMs%20often%20default%20to%20memorized%20facts%2C%20producing%20unfaithful%20outputs.%20In%20this%20work%2C%20we%20introduce%20ContextFocus%2C%20a%20lightweight%20activation%20steering%20approach%20that%20improves%20context%20faithfulness%20in%20such%20knowledge-conflict%20settings%20while%20preserving%20fluency%20and%20efficiency.%20Unlike%20prior%20approaches%2C%20our%20solution%20requires%20no%20model%20finetuning%20and%20incurs%20minimal%20inference-time%20overhead%2C%20making%20it%20highly%20efficient.%20We%20evaluate%20ContextFocus%20on%20the%20ConFiQA%20benchmark%2C%20comparing%20it%20against%20strong%20baselines%20including%20ContextDPO%2C%20COIECD%2C%20and%20prompting-based%20methods.%20Furthermore%2C%20we%20show%20that%20our%20method%20is%20complementary%20to%20prompting%20strategies%20and%20remains%20effective%20on%20larger%20models.%20Extensive%20experiments%20show%20that%20ContextFocus%20significantly%20improves%20contextual-faithfulness.%20Our%20results%20highlight%20the%20effectiveness%2C%20robustness%2C%20and%20efficiency%20of%20ContextFocus%20in%20improving%20contextual-faithfulness%20of%20LLM%20outputs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextFocus%253A%2520Activation%2520Steering%2520for%2520Contextual%2520Faithfulness%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DNikhil%2520Anand%2520and%2520Shwetha%2520Somasundaram%2520and%2520Anirudh%2520Phukan%2520and%2520Apoorv%2520Saxena%2520and%2520Koyel%2520Mukherjee%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520encode%2520vast%2520amounts%2520of%2520parametric%2520knowledge%2520during%2520pre-training.%2520As%2520world%2520knowledge%2520evolves%252C%2520effective%2520deployment%2520increasingly%2520depends%2520on%2520their%2520ability%2520to%2520faithfully%2520follow%2520externally%2520retrieved%2520context.%2520When%2520such%2520evidence%2520conflicts%2520with%2520the%2520model%2527s%2520internal%2520knowledge%252C%2520LLMs%2520often%2520default%2520to%2520memorized%2520facts%252C%2520producing%2520unfaithful%2520outputs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ContextFocus%252C%2520a%2520lightweight%2520activation%2520steering%2520approach%2520that%2520improves%2520context%2520faithfulness%2520in%2520such%2520knowledge-conflict%2520settings%2520while%2520preserving%2520fluency%2520and%2520efficiency.%2520Unlike%2520prior%2520approaches%252C%2520our%2520solution%2520requires%2520no%2520model%2520finetuning%2520and%2520incurs%2520minimal%2520inference-time%2520overhead%252C%2520making%2520it%2520highly%2520efficient.%2520We%2520evaluate%2520ContextFocus%2520on%2520the%2520ConFiQA%2520benchmark%252C%2520comparing%2520it%2520against%2520strong%2520baselines%2520including%2520ContextDPO%252C%2520COIECD%252C%2520and%2520prompting-based%2520methods.%2520Furthermore%252C%2520we%2520show%2520that%2520our%2520method%2520is%2520complementary%2520to%2520prompting%2520strategies%2520and%2520remains%2520effective%2520on%2520larger%2520models.%2520Extensive%2520experiments%2520show%2520that%2520ContextFocus%2520significantly%2520improves%2520contextual-faithfulness.%2520Our%2520results%2520highlight%2520the%2520effectiveness%252C%2520robustness%252C%2520and%2520efficiency%2520of%2520ContextFocus%2520in%2520improving%2520contextual-faithfulness%2520of%2520LLM%2520outputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextFocus%3A%20Activation%20Steering%20for%20Contextual%20Faithfulness%20in%20Large%20Language%20Models&entry.906535625=Nikhil%20Anand%20and%20Shwetha%20Somasundaram%20and%20Anirudh%20Phukan%20and%20Apoorv%20Saxena%20and%20Koyel%20Mukherjee&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20encode%20vast%20amounts%20of%20parametric%20knowledge%20during%20pre-training.%20As%20world%20knowledge%20evolves%2C%20effective%20deployment%20increasingly%20depends%20on%20their%20ability%20to%20faithfully%20follow%20externally%20retrieved%20context.%20When%20such%20evidence%20conflicts%20with%20the%20model%27s%20internal%20knowledge%2C%20LLMs%20often%20default%20to%20memorized%20facts%2C%20producing%20unfaithful%20outputs.%20In%20this%20work%2C%20we%20introduce%20ContextFocus%2C%20a%20lightweight%20activation%20steering%20approach%20that%20improves%20context%20faithfulness%20in%20such%20knowledge-conflict%20settings%20while%20preserving%20fluency%20and%20efficiency.%20Unlike%20prior%20approaches%2C%20our%20solution%20requires%20no%20model%20finetuning%20and%20incurs%20minimal%20inference-time%20overhead%2C%20making%20it%20highly%20efficient.%20We%20evaluate%20ContextFocus%20on%20the%20ConFiQA%20benchmark%2C%20comparing%20it%20against%20strong%20baselines%20including%20ContextDPO%2C%20COIECD%2C%20and%20prompting-based%20methods.%20Furthermore%2C%20we%20show%20that%20our%20method%20is%20complementary%20to%20prompting%20strategies%20and%20remains%20effective%20on%20larger%20models.%20Extensive%20experiments%20show%20that%20ContextFocus%20significantly%20improves%20contextual-faithfulness.%20Our%20results%20highlight%20the%20effectiveness%2C%20robustness%2C%20and%20efficiency%20of%20ContextFocus%20in%20improving%20contextual-faithfulness%20of%20LLM%20outputs.&entry.1838667208=http%3A//arxiv.org/abs/2601.04131v1&entry.124074799=Read"},
{"title": "HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps", "author": "Xuchang Zhong and Xu Cao and Jinke Feng and Hao Fang", "abstract": "Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.", "link": "http://arxiv.org/abs/2601.02730v2", "date": "2026-01-07", "relevancy": 2.5228, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6751}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6026}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.59}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOLO%3A%20Homography-Guided%20Pose%20Estimator%20Network%20for%20Fine-Grained%20Visual%20Localization%20on%20SD%20Maps&body=Title%3A%20HOLO%3A%20Homography-Guided%20Pose%20Estimator%20Network%20for%20Fine-Grained%20Visual%20Localization%20on%20SD%20Maps%0AAuthor%3A%20Xuchang%20Zhong%20and%20Xu%20Cao%20and%20Jinke%20Feng%20and%20Hao%20Fang%0AAbstract%3A%20Visual%20localization%20on%20standard-definition%20%28SD%29%20maps%20has%20emerged%20as%20a%20promising%20low-cost%20and%20scalable%20solution%20for%20autonomous%20driving.%20However%2C%20existing%20regression-based%20approaches%20often%20overlook%20inherent%20geometric%20priors%2C%20resulting%20in%20suboptimal%20training%20efficiency%20and%20limited%20localization%20accuracy.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20homography-guided%20pose%20estimator%20network%20for%20fine-grained%20visual%20localization%20between%20multi-view%20images%20and%20standard-definition%20%28SD%29%20maps.%20We%20construct%20input%20pairs%20that%20satisfy%20a%20homography%20constraint%20by%20projecting%20ground-view%20features%20into%20the%20BEV%20domain%20and%20enforcing%20semantic%20alignment%20with%20map%20features.%20Then%20we%20leverage%20homography%20relationships%20to%20guide%20feature%20fusion%20and%20restrict%20the%20pose%20outputs%20to%20a%20valid%20feasible%20region%2C%20which%20significantly%20improves%20training%20efficiency%20and%20localization%20accuracy%20compared%20to%20prior%20methods%20relying%20on%20attention-based%20fusion%20and%20direct%203-DoF%20pose%20regression.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20unify%20BEV%20semantic%20reasoning%20with%20homography%20learning%20for%20image-to-map%20localization.%20Furthermore%2C%20by%20explicitly%20modeling%20homography%20transformations%2C%20the%20proposed%20framework%20naturally%20supports%20cross-resolution%20inputs%2C%20enhancing%20model%20flexibility.%20Extensive%20experiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%20state-of-the-art%20visual%20localization%20methods.%20Code%20and%20pretrained%20models%20will%20be%20publicly%20released%20to%20foster%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOLO%253A%2520Homography-Guided%2520Pose%2520Estimator%2520Network%2520for%2520Fine-Grained%2520Visual%2520Localization%2520on%2520SD%2520Maps%26entry.906535625%3DXuchang%2520Zhong%2520and%2520Xu%2520Cao%2520and%2520Jinke%2520Feng%2520and%2520Hao%2520Fang%26entry.1292438233%3DVisual%2520localization%2520on%2520standard-definition%2520%2528SD%2529%2520maps%2520has%2520emerged%2520as%2520a%2520promising%2520low-cost%2520and%2520scalable%2520solution%2520for%2520autonomous%2520driving.%2520However%252C%2520existing%2520regression-based%2520approaches%2520often%2520overlook%2520inherent%2520geometric%2520priors%252C%2520resulting%2520in%2520suboptimal%2520training%2520efficiency%2520and%2520limited%2520localization%2520accuracy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520homography-guided%2520pose%2520estimator%2520network%2520for%2520fine-grained%2520visual%2520localization%2520between%2520multi-view%2520images%2520and%2520standard-definition%2520%2528SD%2529%2520maps.%2520We%2520construct%2520input%2520pairs%2520that%2520satisfy%2520a%2520homography%2520constraint%2520by%2520projecting%2520ground-view%2520features%2520into%2520the%2520BEV%2520domain%2520and%2520enforcing%2520semantic%2520alignment%2520with%2520map%2520features.%2520Then%2520we%2520leverage%2520homography%2520relationships%2520to%2520guide%2520feature%2520fusion%2520and%2520restrict%2520the%2520pose%2520outputs%2520to%2520a%2520valid%2520feasible%2520region%252C%2520which%2520significantly%2520improves%2520training%2520efficiency%2520and%2520localization%2520accuracy%2520compared%2520to%2520prior%2520methods%2520relying%2520on%2520attention-based%2520fusion%2520and%2520direct%25203-DoF%2520pose%2520regression.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520unify%2520BEV%2520semantic%2520reasoning%2520with%2520homography%2520learning%2520for%2520image-to-map%2520localization.%2520Furthermore%252C%2520by%2520explicitly%2520modeling%2520homography%2520transformations%252C%2520the%2520proposed%2520framework%2520naturally%2520supports%2520cross-resolution%2520inputs%252C%2520enhancing%2520model%2520flexibility.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520dataset%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520visual%2520localization%2520methods.%2520Code%2520and%2520pretrained%2520models%2520will%2520be%2520publicly%2520released%2520to%2520foster%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOLO%3A%20Homography-Guided%20Pose%20Estimator%20Network%20for%20Fine-Grained%20Visual%20Localization%20on%20SD%20Maps&entry.906535625=Xuchang%20Zhong%20and%20Xu%20Cao%20and%20Jinke%20Feng%20and%20Hao%20Fang&entry.1292438233=Visual%20localization%20on%20standard-definition%20%28SD%29%20maps%20has%20emerged%20as%20a%20promising%20low-cost%20and%20scalable%20solution%20for%20autonomous%20driving.%20However%2C%20existing%20regression-based%20approaches%20often%20overlook%20inherent%20geometric%20priors%2C%20resulting%20in%20suboptimal%20training%20efficiency%20and%20limited%20localization%20accuracy.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20homography-guided%20pose%20estimator%20network%20for%20fine-grained%20visual%20localization%20between%20multi-view%20images%20and%20standard-definition%20%28SD%29%20maps.%20We%20construct%20input%20pairs%20that%20satisfy%20a%20homography%20constraint%20by%20projecting%20ground-view%20features%20into%20the%20BEV%20domain%20and%20enforcing%20semantic%20alignment%20with%20map%20features.%20Then%20we%20leverage%20homography%20relationships%20to%20guide%20feature%20fusion%20and%20restrict%20the%20pose%20outputs%20to%20a%20valid%20feasible%20region%2C%20which%20significantly%20improves%20training%20efficiency%20and%20localization%20accuracy%20compared%20to%20prior%20methods%20relying%20on%20attention-based%20fusion%20and%20direct%203-DoF%20pose%20regression.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20unify%20BEV%20semantic%20reasoning%20with%20homography%20learning%20for%20image-to-map%20localization.%20Furthermore%2C%20by%20explicitly%20modeling%20homography%20transformations%2C%20the%20proposed%20framework%20naturally%20supports%20cross-resolution%20inputs%2C%20enhancing%20model%20flexibility.%20Extensive%20experiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%20state-of-the-art%20visual%20localization%20methods.%20Code%20and%20pretrained%20models%20will%20be%20publicly%20released%20to%20foster%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.02730v2&entry.124074799=Read"},
{"title": "FLEx: Language Modeling with Few-shot Language Explanations", "author": "Adar Avsian and Christopher Richardson and Anirudh Sundar and Larry Heck", "abstract": "Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\\textbf{F}$ew-shot $\\textbf{L}$anguage $\\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\\% of CoT's remaining errors.", "link": "http://arxiv.org/abs/2601.04157v1", "date": "2026-01-07", "relevancy": 2.5096, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLEx%3A%20Language%20Modeling%20with%20Few-shot%20Language%20Explanations&body=Title%3A%20FLEx%3A%20Language%20Modeling%20with%20Few-shot%20Language%20Explanations%0AAuthor%3A%20Adar%20Avsian%20and%20Christopher%20Richardson%20and%20Anirudh%20Sundar%20and%20Larry%20Heck%0AAbstract%3A%20Language%20models%20have%20become%20effective%20at%20a%20wide%20range%20of%20tasks%2C%20from%20math%20problem%20solving%20to%20open-domain%20question%20answering.%20However%2C%20they%20still%20make%20mistakes%2C%20and%20these%20mistakes%20are%20often%20repeated%20across%20related%20queries.%20Natural%20language%20explanations%20can%20help%20correct%20these%20errors%2C%20but%20collecting%20them%20at%20scale%20may%20be%20infeasible%2C%20particularly%20in%20domains%20where%20expert%20annotators%20are%20required.%20To%20address%20this%20issue%2C%20we%20introduce%20FLEx%20%28%24%5Ctextbf%7BF%7D%24ew-shot%20%24%5Ctextbf%7BL%7D%24anguage%20%24%5Ctextbf%7BEx%7D%24planations%29%2C%20a%20method%20for%20improving%20model%20behavior%20using%20a%20small%20number%20of%20explanatory%20examples.%20FLEx%20selects%20representative%20model%20errors%20using%20embedding-based%20clustering%2C%20verifies%20that%20the%20associated%20explanations%20correct%20those%20errors%2C%20and%20summarizes%20them%20into%20a%20prompt%20prefix%20that%20is%20prepended%20at%20inference-time.%20This%20summary%20guides%20the%20model%20to%20avoid%20similar%20errors%20on%20new%20inputs%2C%20without%20modifying%20model%20weights.%20We%20evaluate%20FLEx%20on%20CounterBench%2C%20GSM8K%2C%20and%20ReasonIF.%20We%20find%20that%20FLEx%20consistently%20outperforms%20chain-of-thought%20%28CoT%29%20prompting%20across%20all%20three%20datasets%20and%20reduces%20up%20to%2083%5C%25%20of%20CoT%27s%20remaining%20errors.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLEx%253A%2520Language%2520Modeling%2520with%2520Few-shot%2520Language%2520Explanations%26entry.906535625%3DAdar%2520Avsian%2520and%2520Christopher%2520Richardson%2520and%2520Anirudh%2520Sundar%2520and%2520Larry%2520Heck%26entry.1292438233%3DLanguage%2520models%2520have%2520become%2520effective%2520at%2520a%2520wide%2520range%2520of%2520tasks%252C%2520from%2520math%2520problem%2520solving%2520to%2520open-domain%2520question%2520answering.%2520However%252C%2520they%2520still%2520make%2520mistakes%252C%2520and%2520these%2520mistakes%2520are%2520often%2520repeated%2520across%2520related%2520queries.%2520Natural%2520language%2520explanations%2520can%2520help%2520correct%2520these%2520errors%252C%2520but%2520collecting%2520them%2520at%2520scale%2520may%2520be%2520infeasible%252C%2520particularly%2520in%2520domains%2520where%2520expert%2520annotators%2520are%2520required.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520FLEx%2520%2528%2524%255Ctextbf%257BF%257D%2524ew-shot%2520%2524%255Ctextbf%257BL%257D%2524anguage%2520%2524%255Ctextbf%257BEx%257D%2524planations%2529%252C%2520a%2520method%2520for%2520improving%2520model%2520behavior%2520using%2520a%2520small%2520number%2520of%2520explanatory%2520examples.%2520FLEx%2520selects%2520representative%2520model%2520errors%2520using%2520embedding-based%2520clustering%252C%2520verifies%2520that%2520the%2520associated%2520explanations%2520correct%2520those%2520errors%252C%2520and%2520summarizes%2520them%2520into%2520a%2520prompt%2520prefix%2520that%2520is%2520prepended%2520at%2520inference-time.%2520This%2520summary%2520guides%2520the%2520model%2520to%2520avoid%2520similar%2520errors%2520on%2520new%2520inputs%252C%2520without%2520modifying%2520model%2520weights.%2520We%2520evaluate%2520FLEx%2520on%2520CounterBench%252C%2520GSM8K%252C%2520and%2520ReasonIF.%2520We%2520find%2520that%2520FLEx%2520consistently%2520outperforms%2520chain-of-thought%2520%2528CoT%2529%2520prompting%2520across%2520all%2520three%2520datasets%2520and%2520reduces%2520up%2520to%252083%255C%2525%2520of%2520CoT%2527s%2520remaining%2520errors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLEx%3A%20Language%20Modeling%20with%20Few-shot%20Language%20Explanations&entry.906535625=Adar%20Avsian%20and%20Christopher%20Richardson%20and%20Anirudh%20Sundar%20and%20Larry%20Heck&entry.1292438233=Language%20models%20have%20become%20effective%20at%20a%20wide%20range%20of%20tasks%2C%20from%20math%20problem%20solving%20to%20open-domain%20question%20answering.%20However%2C%20they%20still%20make%20mistakes%2C%20and%20these%20mistakes%20are%20often%20repeated%20across%20related%20queries.%20Natural%20language%20explanations%20can%20help%20correct%20these%20errors%2C%20but%20collecting%20them%20at%20scale%20may%20be%20infeasible%2C%20particularly%20in%20domains%20where%20expert%20annotators%20are%20required.%20To%20address%20this%20issue%2C%20we%20introduce%20FLEx%20%28%24%5Ctextbf%7BF%7D%24ew-shot%20%24%5Ctextbf%7BL%7D%24anguage%20%24%5Ctextbf%7BEx%7D%24planations%29%2C%20a%20method%20for%20improving%20model%20behavior%20using%20a%20small%20number%20of%20explanatory%20examples.%20FLEx%20selects%20representative%20model%20errors%20using%20embedding-based%20clustering%2C%20verifies%20that%20the%20associated%20explanations%20correct%20those%20errors%2C%20and%20summarizes%20them%20into%20a%20prompt%20prefix%20that%20is%20prepended%20at%20inference-time.%20This%20summary%20guides%20the%20model%20to%20avoid%20similar%20errors%20on%20new%20inputs%2C%20without%20modifying%20model%20weights.%20We%20evaluate%20FLEx%20on%20CounterBench%2C%20GSM8K%2C%20and%20ReasonIF.%20We%20find%20that%20FLEx%20consistently%20outperforms%20chain-of-thought%20%28CoT%29%20prompting%20across%20all%20three%20datasets%20and%20reduces%20up%20to%2083%5C%25%20of%20CoT%27s%20remaining%20errors.&entry.1838667208=http%3A//arxiv.org/abs/2601.04157v1&entry.124074799=Read"},
{"title": "Scanner-Induced Domain Shifts Undermine the Robustness of Pathology Foundation Models", "author": "Erik Thiringer and Fredrik K. Gustafsson and Kajsa Ledesma Eriksson and Mattias Rantalainen", "abstract": "Pathology foundation models (PFMs) have become central to computational pathology, aiming to offer general encoders for feature extraction from whole-slide images (WSIs). Despite strong benchmark performance, PFM robustness to real-world technical domain shifts, such as variability from whole-slide scanner devices, remains poorly understood. We systematically evaluated the robustness of 14 PFMs to scanner-induced variability, including state-of-the-art models, earlier self-supervised models, and a baseline trained on natural images. Using a multiscanner dataset of 384 breast cancer WSIs scanned on five devices, we isolated scanner effects independently from biological and laboratory confounders. Robustness is assessed via complementary unsupervised embedding analyses and a set of clinicopathological supervised prediction tasks. Our results demonstrate that current PFMs are not invariant to scanner-induced domain shifts. Most models encode pronounced scanner-specific variability in their embedding spaces. While AUC often remains stable, this masks a critical failure mode: scanner variability systematically alters the embedding space and impacts calibration of downstream model predictions, resulting in scanner-dependent bias that can impact reliability in clinical use cases. We further show that robustness is not a simple function of training data scale, model size, or model recency. None of the models provided reliable robustness against scanner-induced variability. While the models trained on the most diverse data, here represented by vision-language models, appear to have an advantage with respect to robustness, they underperformed on downstream supervised tasks. We conclude that development and evaluation of PFMs requires moving beyond accuracy-centric benchmarks toward explicit evaluation and optimisation of embedding stability and calibration under realistic acquisition variability.", "link": "http://arxiv.org/abs/2601.04163v1", "date": "2026-01-07", "relevancy": 2.4934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scanner-Induced%20Domain%20Shifts%20Undermine%20the%20Robustness%20of%20Pathology%20Foundation%20Models&body=Title%3A%20Scanner-Induced%20Domain%20Shifts%20Undermine%20the%20Robustness%20of%20Pathology%20Foundation%20Models%0AAuthor%3A%20Erik%20Thiringer%20and%20Fredrik%20K.%20Gustafsson%20and%20Kajsa%20Ledesma%20Eriksson%20and%20Mattias%20Rantalainen%0AAbstract%3A%20Pathology%20foundation%20models%20%28PFMs%29%20have%20become%20central%20to%20computational%20pathology%2C%20aiming%20to%20offer%20general%20encoders%20for%20feature%20extraction%20from%20whole-slide%20images%20%28WSIs%29.%20Despite%20strong%20benchmark%20performance%2C%20PFM%20robustness%20to%20real-world%20technical%20domain%20shifts%2C%20such%20as%20variability%20from%20whole-slide%20scanner%20devices%2C%20remains%20poorly%20understood.%20We%20systematically%20evaluated%20the%20robustness%20of%2014%20PFMs%20to%20scanner-induced%20variability%2C%20including%20state-of-the-art%20models%2C%20earlier%20self-supervised%20models%2C%20and%20a%20baseline%20trained%20on%20natural%20images.%20Using%20a%20multiscanner%20dataset%20of%20384%20breast%20cancer%20WSIs%20scanned%20on%20five%20devices%2C%20we%20isolated%20scanner%20effects%20independently%20from%20biological%20and%20laboratory%20confounders.%20Robustness%20is%20assessed%20via%20complementary%20unsupervised%20embedding%20analyses%20and%20a%20set%20of%20clinicopathological%20supervised%20prediction%20tasks.%20Our%20results%20demonstrate%20that%20current%20PFMs%20are%20not%20invariant%20to%20scanner-induced%20domain%20shifts.%20Most%20models%20encode%20pronounced%20scanner-specific%20variability%20in%20their%20embedding%20spaces.%20While%20AUC%20often%20remains%20stable%2C%20this%20masks%20a%20critical%20failure%20mode%3A%20scanner%20variability%20systematically%20alters%20the%20embedding%20space%20and%20impacts%20calibration%20of%20downstream%20model%20predictions%2C%20resulting%20in%20scanner-dependent%20bias%20that%20can%20impact%20reliability%20in%20clinical%20use%20cases.%20We%20further%20show%20that%20robustness%20is%20not%20a%20simple%20function%20of%20training%20data%20scale%2C%20model%20size%2C%20or%20model%20recency.%20None%20of%20the%20models%20provided%20reliable%20robustness%20against%20scanner-induced%20variability.%20While%20the%20models%20trained%20on%20the%20most%20diverse%20data%2C%20here%20represented%20by%20vision-language%20models%2C%20appear%20to%20have%20an%20advantage%20with%20respect%20to%20robustness%2C%20they%20underperformed%20on%20downstream%20supervised%20tasks.%20We%20conclude%20that%20development%20and%20evaluation%20of%20PFMs%20requires%20moving%20beyond%20accuracy-centric%20benchmarks%20toward%20explicit%20evaluation%20and%20optimisation%20of%20embedding%20stability%20and%20calibration%20under%20realistic%20acquisition%20variability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScanner-Induced%2520Domain%2520Shifts%2520Undermine%2520the%2520Robustness%2520of%2520Pathology%2520Foundation%2520Models%26entry.906535625%3DErik%2520Thiringer%2520and%2520Fredrik%2520K.%2520Gustafsson%2520and%2520Kajsa%2520Ledesma%2520Eriksson%2520and%2520Mattias%2520Rantalainen%26entry.1292438233%3DPathology%2520foundation%2520models%2520%2528PFMs%2529%2520have%2520become%2520central%2520to%2520computational%2520pathology%252C%2520aiming%2520to%2520offer%2520general%2520encoders%2520for%2520feature%2520extraction%2520from%2520whole-slide%2520images%2520%2528WSIs%2529.%2520Despite%2520strong%2520benchmark%2520performance%252C%2520PFM%2520robustness%2520to%2520real-world%2520technical%2520domain%2520shifts%252C%2520such%2520as%2520variability%2520from%2520whole-slide%2520scanner%2520devices%252C%2520remains%2520poorly%2520understood.%2520We%2520systematically%2520evaluated%2520the%2520robustness%2520of%252014%2520PFMs%2520to%2520scanner-induced%2520variability%252C%2520including%2520state-of-the-art%2520models%252C%2520earlier%2520self-supervised%2520models%252C%2520and%2520a%2520baseline%2520trained%2520on%2520natural%2520images.%2520Using%2520a%2520multiscanner%2520dataset%2520of%2520384%2520breast%2520cancer%2520WSIs%2520scanned%2520on%2520five%2520devices%252C%2520we%2520isolated%2520scanner%2520effects%2520independently%2520from%2520biological%2520and%2520laboratory%2520confounders.%2520Robustness%2520is%2520assessed%2520via%2520complementary%2520unsupervised%2520embedding%2520analyses%2520and%2520a%2520set%2520of%2520clinicopathological%2520supervised%2520prediction%2520tasks.%2520Our%2520results%2520demonstrate%2520that%2520current%2520PFMs%2520are%2520not%2520invariant%2520to%2520scanner-induced%2520domain%2520shifts.%2520Most%2520models%2520encode%2520pronounced%2520scanner-specific%2520variability%2520in%2520their%2520embedding%2520spaces.%2520While%2520AUC%2520often%2520remains%2520stable%252C%2520this%2520masks%2520a%2520critical%2520failure%2520mode%253A%2520scanner%2520variability%2520systematically%2520alters%2520the%2520embedding%2520space%2520and%2520impacts%2520calibration%2520of%2520downstream%2520model%2520predictions%252C%2520resulting%2520in%2520scanner-dependent%2520bias%2520that%2520can%2520impact%2520reliability%2520in%2520clinical%2520use%2520cases.%2520We%2520further%2520show%2520that%2520robustness%2520is%2520not%2520a%2520simple%2520function%2520of%2520training%2520data%2520scale%252C%2520model%2520size%252C%2520or%2520model%2520recency.%2520None%2520of%2520the%2520models%2520provided%2520reliable%2520robustness%2520against%2520scanner-induced%2520variability.%2520While%2520the%2520models%2520trained%2520on%2520the%2520most%2520diverse%2520data%252C%2520here%2520represented%2520by%2520vision-language%2520models%252C%2520appear%2520to%2520have%2520an%2520advantage%2520with%2520respect%2520to%2520robustness%252C%2520they%2520underperformed%2520on%2520downstream%2520supervised%2520tasks.%2520We%2520conclude%2520that%2520development%2520and%2520evaluation%2520of%2520PFMs%2520requires%2520moving%2520beyond%2520accuracy-centric%2520benchmarks%2520toward%2520explicit%2520evaluation%2520and%2520optimisation%2520of%2520embedding%2520stability%2520and%2520calibration%2520under%2520realistic%2520acquisition%2520variability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scanner-Induced%20Domain%20Shifts%20Undermine%20the%20Robustness%20of%20Pathology%20Foundation%20Models&entry.906535625=Erik%20Thiringer%20and%20Fredrik%20K.%20Gustafsson%20and%20Kajsa%20Ledesma%20Eriksson%20and%20Mattias%20Rantalainen&entry.1292438233=Pathology%20foundation%20models%20%28PFMs%29%20have%20become%20central%20to%20computational%20pathology%2C%20aiming%20to%20offer%20general%20encoders%20for%20feature%20extraction%20from%20whole-slide%20images%20%28WSIs%29.%20Despite%20strong%20benchmark%20performance%2C%20PFM%20robustness%20to%20real-world%20technical%20domain%20shifts%2C%20such%20as%20variability%20from%20whole-slide%20scanner%20devices%2C%20remains%20poorly%20understood.%20We%20systematically%20evaluated%20the%20robustness%20of%2014%20PFMs%20to%20scanner-induced%20variability%2C%20including%20state-of-the-art%20models%2C%20earlier%20self-supervised%20models%2C%20and%20a%20baseline%20trained%20on%20natural%20images.%20Using%20a%20multiscanner%20dataset%20of%20384%20breast%20cancer%20WSIs%20scanned%20on%20five%20devices%2C%20we%20isolated%20scanner%20effects%20independently%20from%20biological%20and%20laboratory%20confounders.%20Robustness%20is%20assessed%20via%20complementary%20unsupervised%20embedding%20analyses%20and%20a%20set%20of%20clinicopathological%20supervised%20prediction%20tasks.%20Our%20results%20demonstrate%20that%20current%20PFMs%20are%20not%20invariant%20to%20scanner-induced%20domain%20shifts.%20Most%20models%20encode%20pronounced%20scanner-specific%20variability%20in%20their%20embedding%20spaces.%20While%20AUC%20often%20remains%20stable%2C%20this%20masks%20a%20critical%20failure%20mode%3A%20scanner%20variability%20systematically%20alters%20the%20embedding%20space%20and%20impacts%20calibration%20of%20downstream%20model%20predictions%2C%20resulting%20in%20scanner-dependent%20bias%20that%20can%20impact%20reliability%20in%20clinical%20use%20cases.%20We%20further%20show%20that%20robustness%20is%20not%20a%20simple%20function%20of%20training%20data%20scale%2C%20model%20size%2C%20or%20model%20recency.%20None%20of%20the%20models%20provided%20reliable%20robustness%20against%20scanner-induced%20variability.%20While%20the%20models%20trained%20on%20the%20most%20diverse%20data%2C%20here%20represented%20by%20vision-language%20models%2C%20appear%20to%20have%20an%20advantage%20with%20respect%20to%20robustness%2C%20they%20underperformed%20on%20downstream%20supervised%20tasks.%20We%20conclude%20that%20development%20and%20evaluation%20of%20PFMs%20requires%20moving%20beyond%20accuracy-centric%20benchmarks%20toward%20explicit%20evaluation%20and%20optimisation%20of%20embedding%20stability%20and%20calibration%20under%20realistic%20acquisition%20variability.&entry.1838667208=http%3A//arxiv.org/abs/2601.04163v1&entry.124074799=Read"},
{"title": "HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis", "author": "Julie van Logtestijn and Petru Manescu", "abstract": "Microscopic evaluation of white blood cell morphology is central to leukemia diagnosis, yet current deep learning models often act as black boxes, limiting clinical trust and adoption. We introduce HemBLIP, a vision language model designed to generate interpretable, morphology aware descriptions of peripheral blood cells. Using a newly constructed dataset of 14k healthy and leukemic cells paired with expert-derived attribute captions, we adapt a general-purpose VLM via both full fine-tuning and LoRA based parameter efficient training, and benchmark against the biomedical foundation model MedGEMMA. HemBLIP achieves higher caption quality and morphological accuracy, while LoRA adaptation provides further gains with significantly reduced computational cost. These results highlight the promise of vision language models for transparent and scalable hematological diagnostics.", "link": "http://arxiv.org/abs/2601.03915v1", "date": "2026-01-07", "relevancy": 2.4902, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HemBLIP%3A%20A%20Vision-Language%20Model%20for%20Interpretable%20Leukemia%20Cell%20Morphology%20Analysis&body=Title%3A%20HemBLIP%3A%20A%20Vision-Language%20Model%20for%20Interpretable%20Leukemia%20Cell%20Morphology%20Analysis%0AAuthor%3A%20Julie%20van%20Logtestijn%20and%20Petru%20Manescu%0AAbstract%3A%20Microscopic%20evaluation%20of%20white%20blood%20cell%20morphology%20is%20central%20to%20leukemia%20diagnosis%2C%20yet%20current%20deep%20learning%20models%20often%20act%20as%20black%20boxes%2C%20limiting%20clinical%20trust%20and%20adoption.%20We%20introduce%20HemBLIP%2C%20a%20vision%20language%20model%20designed%20to%20generate%20interpretable%2C%20morphology%20aware%20descriptions%20of%20peripheral%20blood%20cells.%20Using%20a%20newly%20constructed%20dataset%20of%2014k%20healthy%20and%20leukemic%20cells%20paired%20with%20expert-derived%20attribute%20captions%2C%20we%20adapt%20a%20general-purpose%20VLM%20via%20both%20full%20fine-tuning%20and%20LoRA%20based%20parameter%20efficient%20training%2C%20and%20benchmark%20against%20the%20biomedical%20foundation%20model%20MedGEMMA.%20HemBLIP%20achieves%20higher%20caption%20quality%20and%20morphological%20accuracy%2C%20while%20LoRA%20adaptation%20provides%20further%20gains%20with%20significantly%20reduced%20computational%20cost.%20These%20results%20highlight%20the%20promise%20of%20vision%20language%20models%20for%20transparent%20and%20scalable%20hematological%20diagnostics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHemBLIP%253A%2520A%2520Vision-Language%2520Model%2520for%2520Interpretable%2520Leukemia%2520Cell%2520Morphology%2520Analysis%26entry.906535625%3DJulie%2520van%2520Logtestijn%2520and%2520Petru%2520Manescu%26entry.1292438233%3DMicroscopic%2520evaluation%2520of%2520white%2520blood%2520cell%2520morphology%2520is%2520central%2520to%2520leukemia%2520diagnosis%252C%2520yet%2520current%2520deep%2520learning%2520models%2520often%2520act%2520as%2520black%2520boxes%252C%2520limiting%2520clinical%2520trust%2520and%2520adoption.%2520We%2520introduce%2520HemBLIP%252C%2520a%2520vision%2520language%2520model%2520designed%2520to%2520generate%2520interpretable%252C%2520morphology%2520aware%2520descriptions%2520of%2520peripheral%2520blood%2520cells.%2520Using%2520a%2520newly%2520constructed%2520dataset%2520of%252014k%2520healthy%2520and%2520leukemic%2520cells%2520paired%2520with%2520expert-derived%2520attribute%2520captions%252C%2520we%2520adapt%2520a%2520general-purpose%2520VLM%2520via%2520both%2520full%2520fine-tuning%2520and%2520LoRA%2520based%2520parameter%2520efficient%2520training%252C%2520and%2520benchmark%2520against%2520the%2520biomedical%2520foundation%2520model%2520MedGEMMA.%2520HemBLIP%2520achieves%2520higher%2520caption%2520quality%2520and%2520morphological%2520accuracy%252C%2520while%2520LoRA%2520adaptation%2520provides%2520further%2520gains%2520with%2520significantly%2520reduced%2520computational%2520cost.%2520These%2520results%2520highlight%2520the%2520promise%2520of%2520vision%2520language%2520models%2520for%2520transparent%2520and%2520scalable%2520hematological%2520diagnostics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HemBLIP%3A%20A%20Vision-Language%20Model%20for%20Interpretable%20Leukemia%20Cell%20Morphology%20Analysis&entry.906535625=Julie%20van%20Logtestijn%20and%20Petru%20Manescu&entry.1292438233=Microscopic%20evaluation%20of%20white%20blood%20cell%20morphology%20is%20central%20to%20leukemia%20diagnosis%2C%20yet%20current%20deep%20learning%20models%20often%20act%20as%20black%20boxes%2C%20limiting%20clinical%20trust%20and%20adoption.%20We%20introduce%20HemBLIP%2C%20a%20vision%20language%20model%20designed%20to%20generate%20interpretable%2C%20morphology%20aware%20descriptions%20of%20peripheral%20blood%20cells.%20Using%20a%20newly%20constructed%20dataset%20of%2014k%20healthy%20and%20leukemic%20cells%20paired%20with%20expert-derived%20attribute%20captions%2C%20we%20adapt%20a%20general-purpose%20VLM%20via%20both%20full%20fine-tuning%20and%20LoRA%20based%20parameter%20efficient%20training%2C%20and%20benchmark%20against%20the%20biomedical%20foundation%20model%20MedGEMMA.%20HemBLIP%20achieves%20higher%20caption%20quality%20and%20morphological%20accuracy%2C%20while%20LoRA%20adaptation%20provides%20further%20gains%20with%20significantly%20reduced%20computational%20cost.%20These%20results%20highlight%20the%20promise%20of%20vision%20language%20models%20for%20transparent%20and%20scalable%20hematological%20diagnostics.&entry.1838667208=http%3A//arxiv.org/abs/2601.03915v1&entry.124074799=Read"},
{"title": "How Training Data Shapes the Use of Parametric and In-Context Knowledge in Language Models", "author": "Minsung Kim and Dong-Kyum Kim and Jea Kwon and Nakyeong Yang and Kyomin Jung and Meeyoung Cha", "abstract": "Large language models leverage not only parametric knowledge acquired during training but also in-context knowledge provided at inference time, despite the absence of explicit training objectives for using both sources. Prior work has further shown that when these knowledge sources conflict, models resolve the tension based on their internal confidence, preferring parametric knowledge for high-confidence facts while deferring to contextual information for less familiar ones. However, the training conditions that give rise to such knowledge utilization behaviors remain unclear. To address this gap, we conduct controlled experiments in which we train language models while systematically manipulating key properties of the training data. Our results reveal a counterintuitive finding: three properties commonly regarded as detrimental must co-occur for robust knowledge utilization and conflict resolution to emerge: (i) intra-document repetition of information, (ii) a moderate degree of within-document inconsistency, and (iii) a skewed knowledge frequency distribution. We further validate that the same training dynamics observed in our controlled setting also arise during real-world language model pretraining, and we analyze how post-training procedures can reshape models' knowledge preferences. Together, our findings provide concrete empirical guidance for training language models that harmoniously integrate parametric and in-context knowledge.", "link": "http://arxiv.org/abs/2510.02370v2", "date": "2026-01-07", "relevancy": 2.4898, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Training%20Data%20Shapes%20the%20Use%20of%20Parametric%20and%20In-Context%20Knowledge%20in%20Language%20Models&body=Title%3A%20How%20Training%20Data%20Shapes%20the%20Use%20of%20Parametric%20and%20In-Context%20Knowledge%20in%20Language%20Models%0AAuthor%3A%20Minsung%20Kim%20and%20Dong-Kyum%20Kim%20and%20Jea%20Kwon%20and%20Nakyeong%20Yang%20and%20Kyomin%20Jung%20and%20Meeyoung%20Cha%0AAbstract%3A%20Large%20language%20models%20leverage%20not%20only%20parametric%20knowledge%20acquired%20during%20training%20but%20also%20in-context%20knowledge%20provided%20at%20inference%20time%2C%20despite%20the%20absence%20of%20explicit%20training%20objectives%20for%20using%20both%20sources.%20Prior%20work%20has%20further%20shown%20that%20when%20these%20knowledge%20sources%20conflict%2C%20models%20resolve%20the%20tension%20based%20on%20their%20internal%20confidence%2C%20preferring%20parametric%20knowledge%20for%20high-confidence%20facts%20while%20deferring%20to%20contextual%20information%20for%20less%20familiar%20ones.%20However%2C%20the%20training%20conditions%20that%20give%20rise%20to%20such%20knowledge%20utilization%20behaviors%20remain%20unclear.%20To%20address%20this%20gap%2C%20we%20conduct%20controlled%20experiments%20in%20which%20we%20train%20language%20models%20while%20systematically%20manipulating%20key%20properties%20of%20the%20training%20data.%20Our%20results%20reveal%20a%20counterintuitive%20finding%3A%20three%20properties%20commonly%20regarded%20as%20detrimental%20must%20co-occur%20for%20robust%20knowledge%20utilization%20and%20conflict%20resolution%20to%20emerge%3A%20%28i%29%20intra-document%20repetition%20of%20information%2C%20%28ii%29%20a%20moderate%20degree%20of%20within-document%20inconsistency%2C%20and%20%28iii%29%20a%20skewed%20knowledge%20frequency%20distribution.%20We%20further%20validate%20that%20the%20same%20training%20dynamics%20observed%20in%20our%20controlled%20setting%20also%20arise%20during%20real-world%20language%20model%20pretraining%2C%20and%20we%20analyze%20how%20post-training%20procedures%20can%20reshape%20models%27%20knowledge%20preferences.%20Together%2C%20our%20findings%20provide%20concrete%20empirical%20guidance%20for%20training%20language%20models%20that%20harmoniously%20integrate%20parametric%20and%20in-context%20knowledge.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Training%2520Data%2520Shapes%2520the%2520Use%2520of%2520Parametric%2520and%2520In-Context%2520Knowledge%2520in%2520Language%2520Models%26entry.906535625%3DMinsung%2520Kim%2520and%2520Dong-Kyum%2520Kim%2520and%2520Jea%2520Kwon%2520and%2520Nakyeong%2520Yang%2520and%2520Kyomin%2520Jung%2520and%2520Meeyoung%2520Cha%26entry.1292438233%3DLarge%2520language%2520models%2520leverage%2520not%2520only%2520parametric%2520knowledge%2520acquired%2520during%2520training%2520but%2520also%2520in-context%2520knowledge%2520provided%2520at%2520inference%2520time%252C%2520despite%2520the%2520absence%2520of%2520explicit%2520training%2520objectives%2520for%2520using%2520both%2520sources.%2520Prior%2520work%2520has%2520further%2520shown%2520that%2520when%2520these%2520knowledge%2520sources%2520conflict%252C%2520models%2520resolve%2520the%2520tension%2520based%2520on%2520their%2520internal%2520confidence%252C%2520preferring%2520parametric%2520knowledge%2520for%2520high-confidence%2520facts%2520while%2520deferring%2520to%2520contextual%2520information%2520for%2520less%2520familiar%2520ones.%2520However%252C%2520the%2520training%2520conditions%2520that%2520give%2520rise%2520to%2520such%2520knowledge%2520utilization%2520behaviors%2520remain%2520unclear.%2520To%2520address%2520this%2520gap%252C%2520we%2520conduct%2520controlled%2520experiments%2520in%2520which%2520we%2520train%2520language%2520models%2520while%2520systematically%2520manipulating%2520key%2520properties%2520of%2520the%2520training%2520data.%2520Our%2520results%2520reveal%2520a%2520counterintuitive%2520finding%253A%2520three%2520properties%2520commonly%2520regarded%2520as%2520detrimental%2520must%2520co-occur%2520for%2520robust%2520knowledge%2520utilization%2520and%2520conflict%2520resolution%2520to%2520emerge%253A%2520%2528i%2529%2520intra-document%2520repetition%2520of%2520information%252C%2520%2528ii%2529%2520a%2520moderate%2520degree%2520of%2520within-document%2520inconsistency%252C%2520and%2520%2528iii%2529%2520a%2520skewed%2520knowledge%2520frequency%2520distribution.%2520We%2520further%2520validate%2520that%2520the%2520same%2520training%2520dynamics%2520observed%2520in%2520our%2520controlled%2520setting%2520also%2520arise%2520during%2520real-world%2520language%2520model%2520pretraining%252C%2520and%2520we%2520analyze%2520how%2520post-training%2520procedures%2520can%2520reshape%2520models%2527%2520knowledge%2520preferences.%2520Together%252C%2520our%2520findings%2520provide%2520concrete%2520empirical%2520guidance%2520for%2520training%2520language%2520models%2520that%2520harmoniously%2520integrate%2520parametric%2520and%2520in-context%2520knowledge.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Training%20Data%20Shapes%20the%20Use%20of%20Parametric%20and%20In-Context%20Knowledge%20in%20Language%20Models&entry.906535625=Minsung%20Kim%20and%20Dong-Kyum%20Kim%20and%20Jea%20Kwon%20and%20Nakyeong%20Yang%20and%20Kyomin%20Jung%20and%20Meeyoung%20Cha&entry.1292438233=Large%20language%20models%20leverage%20not%20only%20parametric%20knowledge%20acquired%20during%20training%20but%20also%20in-context%20knowledge%20provided%20at%20inference%20time%2C%20despite%20the%20absence%20of%20explicit%20training%20objectives%20for%20using%20both%20sources.%20Prior%20work%20has%20further%20shown%20that%20when%20these%20knowledge%20sources%20conflict%2C%20models%20resolve%20the%20tension%20based%20on%20their%20internal%20confidence%2C%20preferring%20parametric%20knowledge%20for%20high-confidence%20facts%20while%20deferring%20to%20contextual%20information%20for%20less%20familiar%20ones.%20However%2C%20the%20training%20conditions%20that%20give%20rise%20to%20such%20knowledge%20utilization%20behaviors%20remain%20unclear.%20To%20address%20this%20gap%2C%20we%20conduct%20controlled%20experiments%20in%20which%20we%20train%20language%20models%20while%20systematically%20manipulating%20key%20properties%20of%20the%20training%20data.%20Our%20results%20reveal%20a%20counterintuitive%20finding%3A%20three%20properties%20commonly%20regarded%20as%20detrimental%20must%20co-occur%20for%20robust%20knowledge%20utilization%20and%20conflict%20resolution%20to%20emerge%3A%20%28i%29%20intra-document%20repetition%20of%20information%2C%20%28ii%29%20a%20moderate%20degree%20of%20within-document%20inconsistency%2C%20and%20%28iii%29%20a%20skewed%20knowledge%20frequency%20distribution.%20We%20further%20validate%20that%20the%20same%20training%20dynamics%20observed%20in%20our%20controlled%20setting%20also%20arise%20during%20real-world%20language%20model%20pretraining%2C%20and%20we%20analyze%20how%20post-training%20procedures%20can%20reshape%20models%27%20knowledge%20preferences.%20Together%2C%20our%20findings%20provide%20concrete%20empirical%20guidance%20for%20training%20language%20models%20that%20harmoniously%20integrate%20parametric%20and%20in-context%20knowledge.&entry.1838667208=http%3A//arxiv.org/abs/2510.02370v2&entry.124074799=Read"},
{"title": "Choreographing a World of Dynamic Objects", "author": "Yanzhe Lyu and Chen Geng and Karthik Dharmarajan and Yunzhi Zhang and Hadi Alzayer and Shangzhe Wu and Jiajun Wu", "abstract": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord", "link": "http://arxiv.org/abs/2601.04194v1", "date": "2026-01-07", "relevancy": 2.4775, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6294}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6137}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Choreographing%20a%20World%20of%20Dynamic%20Objects&body=Title%3A%20Choreographing%20a%20World%20of%20Dynamic%20Objects%0AAuthor%3A%20Yanzhe%20Lyu%20and%20Chen%20Geng%20and%20Karthik%20Dharmarajan%20and%20Yunzhi%20Zhang%20and%20Hadi%20Alzayer%20and%20Shangzhe%20Wu%20and%20Jiajun%20Wu%0AAbstract%3A%20Dynamic%20objects%20in%20our%20physical%204D%20%283D%20%2B%20time%29%20world%20are%20constantly%20evolving%2C%20deforming%2C%20and%20interacting%20with%20other%20objects%2C%20leading%20to%20diverse%204D%20scene%20dynamics.%20In%20this%20paper%2C%20we%20present%20a%20universal%20generative%20pipeline%2C%20CHORD%2C%20for%20CHOReographing%20Dynamic%20objects%20and%20scenes%20and%20synthesizing%20this%20type%20of%20phenomena.%20Traditional%20rule-based%20graphics%20pipelines%20to%20create%20these%20dynamics%20are%20based%20on%20category-specific%20heuristics%2C%20yet%20are%20labor-intensive%20and%20not%20scalable.%20Recent%20learning-based%20methods%20typically%20demand%20large-scale%20datasets%2C%20which%20may%20not%20cover%20all%20object%20categories%20in%20interest.%20Our%20approach%20instead%20inherits%20the%20universality%20from%20the%20video%20generative%20models%20by%20proposing%20a%20distillation-based%20pipeline%20to%20extract%20the%20rich%20Lagrangian%20motion%20information%20hidden%20in%20the%20Eulerian%20representations%20of%202D%20videos.%20Our%20method%20is%20universal%2C%20versatile%2C%20and%20category-agnostic.%20We%20demonstrate%20its%20effectiveness%20by%20conducting%20experiments%20to%20generate%20a%20diverse%20range%20of%20multi-body%204D%20dynamics%2C%20show%20its%20advantage%20compared%20to%20existing%20methods%2C%20and%20demonstrate%20its%20applicability%20in%20generating%20robotics%20manipulation%20policies.%20Project%20page%3A%20https%3A//yanzhelyu.github.io/chord%0ALink%3A%20http%3A//arxiv.org/abs/2601.04194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChoreographing%2520a%2520World%2520of%2520Dynamic%2520Objects%26entry.906535625%3DYanzhe%2520Lyu%2520and%2520Chen%2520Geng%2520and%2520Karthik%2520Dharmarajan%2520and%2520Yunzhi%2520Zhang%2520and%2520Hadi%2520Alzayer%2520and%2520Shangzhe%2520Wu%2520and%2520Jiajun%2520Wu%26entry.1292438233%3DDynamic%2520objects%2520in%2520our%2520physical%25204D%2520%25283D%2520%252B%2520time%2529%2520world%2520are%2520constantly%2520evolving%252C%2520deforming%252C%2520and%2520interacting%2520with%2520other%2520objects%252C%2520leading%2520to%2520diverse%25204D%2520scene%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520universal%2520generative%2520pipeline%252C%2520CHORD%252C%2520for%2520CHOReographing%2520Dynamic%2520objects%2520and%2520scenes%2520and%2520synthesizing%2520this%2520type%2520of%2520phenomena.%2520Traditional%2520rule-based%2520graphics%2520pipelines%2520to%2520create%2520these%2520dynamics%2520are%2520based%2520on%2520category-specific%2520heuristics%252C%2520yet%2520are%2520labor-intensive%2520and%2520not%2520scalable.%2520Recent%2520learning-based%2520methods%2520typically%2520demand%2520large-scale%2520datasets%252C%2520which%2520may%2520not%2520cover%2520all%2520object%2520categories%2520in%2520interest.%2520Our%2520approach%2520instead%2520inherits%2520the%2520universality%2520from%2520the%2520video%2520generative%2520models%2520by%2520proposing%2520a%2520distillation-based%2520pipeline%2520to%2520extract%2520the%2520rich%2520Lagrangian%2520motion%2520information%2520hidden%2520in%2520the%2520Eulerian%2520representations%2520of%25202D%2520videos.%2520Our%2520method%2520is%2520universal%252C%2520versatile%252C%2520and%2520category-agnostic.%2520We%2520demonstrate%2520its%2520effectiveness%2520by%2520conducting%2520experiments%2520to%2520generate%2520a%2520diverse%2520range%2520of%2520multi-body%25204D%2520dynamics%252C%2520show%2520its%2520advantage%2520compared%2520to%2520existing%2520methods%252C%2520and%2520demonstrate%2520its%2520applicability%2520in%2520generating%2520robotics%2520manipulation%2520policies.%2520Project%2520page%253A%2520https%253A//yanzhelyu.github.io/chord%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Choreographing%20a%20World%20of%20Dynamic%20Objects&entry.906535625=Yanzhe%20Lyu%20and%20Chen%20Geng%20and%20Karthik%20Dharmarajan%20and%20Yunzhi%20Zhang%20and%20Hadi%20Alzayer%20and%20Shangzhe%20Wu%20and%20Jiajun%20Wu&entry.1292438233=Dynamic%20objects%20in%20our%20physical%204D%20%283D%20%2B%20time%29%20world%20are%20constantly%20evolving%2C%20deforming%2C%20and%20interacting%20with%20other%20objects%2C%20leading%20to%20diverse%204D%20scene%20dynamics.%20In%20this%20paper%2C%20we%20present%20a%20universal%20generative%20pipeline%2C%20CHORD%2C%20for%20CHOReographing%20Dynamic%20objects%20and%20scenes%20and%20synthesizing%20this%20type%20of%20phenomena.%20Traditional%20rule-based%20graphics%20pipelines%20to%20create%20these%20dynamics%20are%20based%20on%20category-specific%20heuristics%2C%20yet%20are%20labor-intensive%20and%20not%20scalable.%20Recent%20learning-based%20methods%20typically%20demand%20large-scale%20datasets%2C%20which%20may%20not%20cover%20all%20object%20categories%20in%20interest.%20Our%20approach%20instead%20inherits%20the%20universality%20from%20the%20video%20generative%20models%20by%20proposing%20a%20distillation-based%20pipeline%20to%20extract%20the%20rich%20Lagrangian%20motion%20information%20hidden%20in%20the%20Eulerian%20representations%20of%202D%20videos.%20Our%20method%20is%20universal%2C%20versatile%2C%20and%20category-agnostic.%20We%20demonstrate%20its%20effectiveness%20by%20conducting%20experiments%20to%20generate%20a%20diverse%20range%20of%20multi-body%204D%20dynamics%2C%20show%20its%20advantage%20compared%20to%20existing%20methods%2C%20and%20demonstrate%20its%20applicability%20in%20generating%20robotics%20manipulation%20policies.%20Project%20page%3A%20https%3A//yanzhelyu.github.io/chord&entry.1838667208=http%3A//arxiv.org/abs/2601.04194v1&entry.124074799=Read"},
{"title": "EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging", "author": "Jan Tagscherer and Sarah de Boer and Lena Philipp and Fennie van der Graaf and Dr\u00e9 Peeters and Joeran Bosma and Lars Leijten and Bogdan Obreja and Ewoud Smit and Alessa Hering", "abstract": "Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at https://github.com/DIAGNijmegen/eval-blocks.", "link": "http://arxiv.org/abs/2601.03811v1", "date": "2026-01-07", "relevancy": 2.4718, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4982}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4982}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvalBlocks%3A%20A%20Modular%20Pipeline%20for%20Rapidly%20Evaluating%20Foundation%20Models%20in%20Medical%20Imaging&body=Title%3A%20EvalBlocks%3A%20A%20Modular%20Pipeline%20for%20Rapidly%20Evaluating%20Foundation%20Models%20in%20Medical%20Imaging%0AAuthor%3A%20Jan%20Tagscherer%20and%20Sarah%20de%20Boer%20and%20Lena%20Philipp%20and%20Fennie%20van%20der%20Graaf%20and%20Dr%C3%A9%20Peeters%20and%20Joeran%20Bosma%20and%20Lars%20Leijten%20and%20Bogdan%20Obreja%20and%20Ewoud%20Smit%20and%20Alessa%20Hering%0AAbstract%3A%20Developing%20foundation%20models%20in%20medical%20imaging%20requires%20continuous%20monitoring%20of%20downstream%20performance.%20Researchers%20are%20burdened%20with%20tracking%20numerous%20experiments%2C%20design%20choices%2C%20and%20their%20effects%20on%20performance%2C%20often%20relying%20on%20ad-hoc%2C%20manual%20workflows%20that%20are%20inherently%20slow%20and%20error-prone.%20We%20introduce%20EvalBlocks%2C%20a%20modular%2C%20plug-and-play%20framework%20for%20efficient%20evaluation%20of%20foundation%20models%20during%20development.%20Built%20on%20Snakemake%2C%20EvalBlocks%20supports%20seamless%20integration%20of%20new%20datasets%2C%20foundation%20models%2C%20aggregation%20methods%2C%20and%20evaluation%20strategies.%20All%20experiments%20and%20results%20are%20tracked%20centrally%20and%20are%20reproducible%20with%20a%20single%20command%2C%20while%20efficient%20caching%20and%20parallel%20execution%20enable%20scalable%20use%20on%20shared%20compute%20infrastructure.%20Demonstrated%20on%20five%20state-of-the-art%20foundation%20models%20and%20three%20medical%20imaging%20classification%20tasks%2C%20EvalBlocks%20streamlines%20model%20evaluation%2C%20enabling%20researchers%20to%20iterate%20faster%20and%20focus%20on%20model%20innovation%20rather%20than%20evaluation%20logistics.%20The%20framework%20is%20released%20as%20open%20source%20software%20at%20https%3A//github.com/DIAGNijmegen/eval-blocks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvalBlocks%253A%2520A%2520Modular%2520Pipeline%2520for%2520Rapidly%2520Evaluating%2520Foundation%2520Models%2520in%2520Medical%2520Imaging%26entry.906535625%3DJan%2520Tagscherer%2520and%2520Sarah%2520de%2520Boer%2520and%2520Lena%2520Philipp%2520and%2520Fennie%2520van%2520der%2520Graaf%2520and%2520Dr%25C3%25A9%2520Peeters%2520and%2520Joeran%2520Bosma%2520and%2520Lars%2520Leijten%2520and%2520Bogdan%2520Obreja%2520and%2520Ewoud%2520Smit%2520and%2520Alessa%2520Hering%26entry.1292438233%3DDeveloping%2520foundation%2520models%2520in%2520medical%2520imaging%2520requires%2520continuous%2520monitoring%2520of%2520downstream%2520performance.%2520Researchers%2520are%2520burdened%2520with%2520tracking%2520numerous%2520experiments%252C%2520design%2520choices%252C%2520and%2520their%2520effects%2520on%2520performance%252C%2520often%2520relying%2520on%2520ad-hoc%252C%2520manual%2520workflows%2520that%2520are%2520inherently%2520slow%2520and%2520error-prone.%2520We%2520introduce%2520EvalBlocks%252C%2520a%2520modular%252C%2520plug-and-play%2520framework%2520for%2520efficient%2520evaluation%2520of%2520foundation%2520models%2520during%2520development.%2520Built%2520on%2520Snakemake%252C%2520EvalBlocks%2520supports%2520seamless%2520integration%2520of%2520new%2520datasets%252C%2520foundation%2520models%252C%2520aggregation%2520methods%252C%2520and%2520evaluation%2520strategies.%2520All%2520experiments%2520and%2520results%2520are%2520tracked%2520centrally%2520and%2520are%2520reproducible%2520with%2520a%2520single%2520command%252C%2520while%2520efficient%2520caching%2520and%2520parallel%2520execution%2520enable%2520scalable%2520use%2520on%2520shared%2520compute%2520infrastructure.%2520Demonstrated%2520on%2520five%2520state-of-the-art%2520foundation%2520models%2520and%2520three%2520medical%2520imaging%2520classification%2520tasks%252C%2520EvalBlocks%2520streamlines%2520model%2520evaluation%252C%2520enabling%2520researchers%2520to%2520iterate%2520faster%2520and%2520focus%2520on%2520model%2520innovation%2520rather%2520than%2520evaluation%2520logistics.%2520The%2520framework%2520is%2520released%2520as%2520open%2520source%2520software%2520at%2520https%253A//github.com/DIAGNijmegen/eval-blocks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvalBlocks%3A%20A%20Modular%20Pipeline%20for%20Rapidly%20Evaluating%20Foundation%20Models%20in%20Medical%20Imaging&entry.906535625=Jan%20Tagscherer%20and%20Sarah%20de%20Boer%20and%20Lena%20Philipp%20and%20Fennie%20van%20der%20Graaf%20and%20Dr%C3%A9%20Peeters%20and%20Joeran%20Bosma%20and%20Lars%20Leijten%20and%20Bogdan%20Obreja%20and%20Ewoud%20Smit%20and%20Alessa%20Hering&entry.1292438233=Developing%20foundation%20models%20in%20medical%20imaging%20requires%20continuous%20monitoring%20of%20downstream%20performance.%20Researchers%20are%20burdened%20with%20tracking%20numerous%20experiments%2C%20design%20choices%2C%20and%20their%20effects%20on%20performance%2C%20often%20relying%20on%20ad-hoc%2C%20manual%20workflows%20that%20are%20inherently%20slow%20and%20error-prone.%20We%20introduce%20EvalBlocks%2C%20a%20modular%2C%20plug-and-play%20framework%20for%20efficient%20evaluation%20of%20foundation%20models%20during%20development.%20Built%20on%20Snakemake%2C%20EvalBlocks%20supports%20seamless%20integration%20of%20new%20datasets%2C%20foundation%20models%2C%20aggregation%20methods%2C%20and%20evaluation%20strategies.%20All%20experiments%20and%20results%20are%20tracked%20centrally%20and%20are%20reproducible%20with%20a%20single%20command%2C%20while%20efficient%20caching%20and%20parallel%20execution%20enable%20scalable%20use%20on%20shared%20compute%20infrastructure.%20Demonstrated%20on%20five%20state-of-the-art%20foundation%20models%20and%20three%20medical%20imaging%20classification%20tasks%2C%20EvalBlocks%20streamlines%20model%20evaluation%2C%20enabling%20researchers%20to%20iterate%20faster%20and%20focus%20on%20model%20innovation%20rather%20than%20evaluation%20logistics.%20The%20framework%20is%20released%20as%20open%20source%20software%20at%20https%3A//github.com/DIAGNijmegen/eval-blocks.&entry.1838667208=http%3A//arxiv.org/abs/2601.03811v1&entry.124074799=Read"},
{"title": "Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models", "author": "Zitong Huang and Kaidong Zhang and Yukang Ding and Chao Gao and Rui Ding and Ying Chen and Wangmeng Zuo", "abstract": "Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.", "link": "http://arxiv.org/abs/2601.04068v1", "date": "2026-01-07", "relevancy": 2.4618, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6493}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6262}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Generative%20Details%3A%20Direct%20Localized%20Detail%20Preference%20Optimization%20for%20Video%20Diffusion%20Models&body=Title%3A%20Mind%20the%20Generative%20Details%3A%20Direct%20Localized%20Detail%20Preference%20Optimization%20for%20Video%20Diffusion%20Models%0AAuthor%3A%20Zitong%20Huang%20and%20Kaidong%20Zhang%20and%20Yukang%20Ding%20and%20Chao%20Gao%20and%20Rui%20Ding%20and%20Ying%20Chen%20and%20Wangmeng%20Zuo%0AAbstract%3A%20Aligning%20text-to-video%20diffusion%20models%20with%20human%20preferences%20is%20crucial%20for%20generating%20high-quality%20videos.%20Existing%20Direct%20Preference%20Otimization%20%28DPO%29%20methods%20rely%20on%20multi-sample%20ranking%20and%20task-specific%20critic%20models%2C%20which%20is%20inefficient%20and%20often%20yields%20ambiguous%20global%20supervision.%20To%20address%20these%20limitations%2C%20we%20propose%20LocalDPO%2C%20a%20novel%20post-training%20framework%20that%20constructs%20localized%20preference%20pairs%20from%20real%20videos%20and%20optimizes%20alignment%20at%20the%20spatio-temporal%20region%20level.%20We%20design%20an%20automated%20pipeline%20to%20efficiently%20collect%20preference%20pair%20data%20that%20generates%20preference%20pairs%20with%20a%20single%20inference%20per%20prompt%2C%20eliminating%20the%20need%20for%20external%20critic%20models%20or%20manual%20annotation.%20Specifically%2C%20we%20treat%20high-quality%20real%20videos%20as%20positive%20samples%20and%20generate%20corresponding%20negatives%20by%20locally%20corrupting%20them%20with%20random%20spatio-temporal%20masks%20and%20restoring%20only%20the%20masked%20regions%20using%20the%20frozen%20base%20model.%20During%20training%2C%20we%20introduce%20a%20region-aware%20DPO%20loss%20that%20restricts%20preference%20learning%20to%20corrupted%20areas%20for%20rapid%20convergence.%20Experiments%20on%20Wan2.1%20and%20CogVideoX%20demonstrate%20that%20LocalDPO%20consistently%20improves%20video%20fidelity%2C%20temporal%20coherence%20and%20human%20preference%20scores%20over%20other%20post-training%20approaches%2C%20establishing%20a%20more%20efficient%20and%20fine-grained%20paradigm%20for%20video%20generator%20alignment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Generative%2520Details%253A%2520Direct%2520Localized%2520Detail%2520Preference%2520Optimization%2520for%2520Video%2520Diffusion%2520Models%26entry.906535625%3DZitong%2520Huang%2520and%2520Kaidong%2520Zhang%2520and%2520Yukang%2520Ding%2520and%2520Chao%2520Gao%2520and%2520Rui%2520Ding%2520and%2520Ying%2520Chen%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3DAligning%2520text-to-video%2520diffusion%2520models%2520with%2520human%2520preferences%2520is%2520crucial%2520for%2520generating%2520high-quality%2520videos.%2520Existing%2520Direct%2520Preference%2520Otimization%2520%2528DPO%2529%2520methods%2520rely%2520on%2520multi-sample%2520ranking%2520and%2520task-specific%2520critic%2520models%252C%2520which%2520is%2520inefficient%2520and%2520often%2520yields%2520ambiguous%2520global%2520supervision.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520LocalDPO%252C%2520a%2520novel%2520post-training%2520framework%2520that%2520constructs%2520localized%2520preference%2520pairs%2520from%2520real%2520videos%2520and%2520optimizes%2520alignment%2520at%2520the%2520spatio-temporal%2520region%2520level.%2520We%2520design%2520an%2520automated%2520pipeline%2520to%2520efficiently%2520collect%2520preference%2520pair%2520data%2520that%2520generates%2520preference%2520pairs%2520with%2520a%2520single%2520inference%2520per%2520prompt%252C%2520eliminating%2520the%2520need%2520for%2520external%2520critic%2520models%2520or%2520manual%2520annotation.%2520Specifically%252C%2520we%2520treat%2520high-quality%2520real%2520videos%2520as%2520positive%2520samples%2520and%2520generate%2520corresponding%2520negatives%2520by%2520locally%2520corrupting%2520them%2520with%2520random%2520spatio-temporal%2520masks%2520and%2520restoring%2520only%2520the%2520masked%2520regions%2520using%2520the%2520frozen%2520base%2520model.%2520During%2520training%252C%2520we%2520introduce%2520a%2520region-aware%2520DPO%2520loss%2520that%2520restricts%2520preference%2520learning%2520to%2520corrupted%2520areas%2520for%2520rapid%2520convergence.%2520Experiments%2520on%2520Wan2.1%2520and%2520CogVideoX%2520demonstrate%2520that%2520LocalDPO%2520consistently%2520improves%2520video%2520fidelity%252C%2520temporal%2520coherence%2520and%2520human%2520preference%2520scores%2520over%2520other%2520post-training%2520approaches%252C%2520establishing%2520a%2520more%2520efficient%2520and%2520fine-grained%2520paradigm%2520for%2520video%2520generator%2520alignment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Generative%20Details%3A%20Direct%20Localized%20Detail%20Preference%20Optimization%20for%20Video%20Diffusion%20Models&entry.906535625=Zitong%20Huang%20and%20Kaidong%20Zhang%20and%20Yukang%20Ding%20and%20Chao%20Gao%20and%20Rui%20Ding%20and%20Ying%20Chen%20and%20Wangmeng%20Zuo&entry.1292438233=Aligning%20text-to-video%20diffusion%20models%20with%20human%20preferences%20is%20crucial%20for%20generating%20high-quality%20videos.%20Existing%20Direct%20Preference%20Otimization%20%28DPO%29%20methods%20rely%20on%20multi-sample%20ranking%20and%20task-specific%20critic%20models%2C%20which%20is%20inefficient%20and%20often%20yields%20ambiguous%20global%20supervision.%20To%20address%20these%20limitations%2C%20we%20propose%20LocalDPO%2C%20a%20novel%20post-training%20framework%20that%20constructs%20localized%20preference%20pairs%20from%20real%20videos%20and%20optimizes%20alignment%20at%20the%20spatio-temporal%20region%20level.%20We%20design%20an%20automated%20pipeline%20to%20efficiently%20collect%20preference%20pair%20data%20that%20generates%20preference%20pairs%20with%20a%20single%20inference%20per%20prompt%2C%20eliminating%20the%20need%20for%20external%20critic%20models%20or%20manual%20annotation.%20Specifically%2C%20we%20treat%20high-quality%20real%20videos%20as%20positive%20samples%20and%20generate%20corresponding%20negatives%20by%20locally%20corrupting%20them%20with%20random%20spatio-temporal%20masks%20and%20restoring%20only%20the%20masked%20regions%20using%20the%20frozen%20base%20model.%20During%20training%2C%20we%20introduce%20a%20region-aware%20DPO%20loss%20that%20restricts%20preference%20learning%20to%20corrupted%20areas%20for%20rapid%20convergence.%20Experiments%20on%20Wan2.1%20and%20CogVideoX%20demonstrate%20that%20LocalDPO%20consistently%20improves%20video%20fidelity%2C%20temporal%20coherence%20and%20human%20preference%20scores%20over%20other%20post-training%20approaches%2C%20establishing%20a%20more%20efficient%20and%20fine-grained%20paradigm%20for%20video%20generator%20alignment.&entry.1838667208=http%3A//arxiv.org/abs/2601.04068v1&entry.124074799=Read"},
{"title": "An Algebraic Representation Theorem for Linear GENEOs in Geometric Machine Learning", "author": "Francesco Conti and Patrizio Frosini and Nicola Quercioli", "abstract": "Geometric and Topological Deep Learning are rapidly growing research areas that enhance machine learning through the use of geometric and topological structures. Within this framework, Group Equivariant Non-Expansive Operators (GENEOs) have emerged as a powerful class of operators for encoding symmetries and designing efficient, interpretable neural architectures. Originally introduced in Topological Data Analysis, GENEOs have since found applications in Deep Learning as tools for constructing equivariant models with reduced parameter complexity. GENEOs provide a unifying framework bridging Geometric and Topological Deep Learning and include the operator computing persistence diagrams as a special case. Their theoretical foundations rely on group actions, equivariance, and compactness properties of operator spaces, grounding them in algebra and geometry while enabling both mathematical rigor and practical relevance. While a previous representation theorem characterized linear GENEOs acting on data of the same type, many real-world applications require operators between heterogeneous data spaces. In this work, we address this limitation by introducing a new representation theorem for linear GENEOs acting between different perception pairs, based on generalized T-permutant measures. Under mild assumptions on the data domains and group actions, our result provides a complete characterization of such operators. We also prove the compactness and convexity of the space of linear GENEOs. We further demonstrate the practical impact of this theory by applying the proposed framework to improve the performance of autoencoders, highlighting the relevance of GENEOs in modern machine learning applications.", "link": "http://arxiv.org/abs/2601.03910v1", "date": "2026-01-07", "relevancy": 2.4264, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.487}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4864}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Algebraic%20Representation%20Theorem%20for%20Linear%20GENEOs%20in%20Geometric%20Machine%20Learning&body=Title%3A%20An%20Algebraic%20Representation%20Theorem%20for%20Linear%20GENEOs%20in%20Geometric%20Machine%20Learning%0AAuthor%3A%20Francesco%20Conti%20and%20Patrizio%20Frosini%20and%20Nicola%20Quercioli%0AAbstract%3A%20Geometric%20and%20Topological%20Deep%20Learning%20are%20rapidly%20growing%20research%20areas%20that%20enhance%20machine%20learning%20through%20the%20use%20of%20geometric%20and%20topological%20structures.%20Within%20this%20framework%2C%20Group%20Equivariant%20Non-Expansive%20Operators%20%28GENEOs%29%20have%20emerged%20as%20a%20powerful%20class%20of%20operators%20for%20encoding%20symmetries%20and%20designing%20efficient%2C%20interpretable%20neural%20architectures.%20Originally%20introduced%20in%20Topological%20Data%20Analysis%2C%20GENEOs%20have%20since%20found%20applications%20in%20Deep%20Learning%20as%20tools%20for%20constructing%20equivariant%20models%20with%20reduced%20parameter%20complexity.%20GENEOs%20provide%20a%20unifying%20framework%20bridging%20Geometric%20and%20Topological%20Deep%20Learning%20and%20include%20the%20operator%20computing%20persistence%20diagrams%20as%20a%20special%20case.%20Their%20theoretical%20foundations%20rely%20on%20group%20actions%2C%20equivariance%2C%20and%20compactness%20properties%20of%20operator%20spaces%2C%20grounding%20them%20in%20algebra%20and%20geometry%20while%20enabling%20both%20mathematical%20rigor%20and%20practical%20relevance.%20While%20a%20previous%20representation%20theorem%20characterized%20linear%20GENEOs%20acting%20on%20data%20of%20the%20same%20type%2C%20many%20real-world%20applications%20require%20operators%20between%20heterogeneous%20data%20spaces.%20In%20this%20work%2C%20we%20address%20this%20limitation%20by%20introducing%20a%20new%20representation%20theorem%20for%20linear%20GENEOs%20acting%20between%20different%20perception%20pairs%2C%20based%20on%20generalized%20T-permutant%20measures.%20Under%20mild%20assumptions%20on%20the%20data%20domains%20and%20group%20actions%2C%20our%20result%20provides%20a%20complete%20characterization%20of%20such%20operators.%20We%20also%20prove%20the%20compactness%20and%20convexity%20of%20the%20space%20of%20linear%20GENEOs.%20We%20further%20demonstrate%20the%20practical%20impact%20of%20this%20theory%20by%20applying%20the%20proposed%20framework%20to%20improve%20the%20performance%20of%20autoencoders%2C%20highlighting%20the%20relevance%20of%20GENEOs%20in%20modern%20machine%20learning%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Algebraic%2520Representation%2520Theorem%2520for%2520Linear%2520GENEOs%2520in%2520Geometric%2520Machine%2520Learning%26entry.906535625%3DFrancesco%2520Conti%2520and%2520Patrizio%2520Frosini%2520and%2520Nicola%2520Quercioli%26entry.1292438233%3DGeometric%2520and%2520Topological%2520Deep%2520Learning%2520are%2520rapidly%2520growing%2520research%2520areas%2520that%2520enhance%2520machine%2520learning%2520through%2520the%2520use%2520of%2520geometric%2520and%2520topological%2520structures.%2520Within%2520this%2520framework%252C%2520Group%2520Equivariant%2520Non-Expansive%2520Operators%2520%2528GENEOs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520class%2520of%2520operators%2520for%2520encoding%2520symmetries%2520and%2520designing%2520efficient%252C%2520interpretable%2520neural%2520architectures.%2520Originally%2520introduced%2520in%2520Topological%2520Data%2520Analysis%252C%2520GENEOs%2520have%2520since%2520found%2520applications%2520in%2520Deep%2520Learning%2520as%2520tools%2520for%2520constructing%2520equivariant%2520models%2520with%2520reduced%2520parameter%2520complexity.%2520GENEOs%2520provide%2520a%2520unifying%2520framework%2520bridging%2520Geometric%2520and%2520Topological%2520Deep%2520Learning%2520and%2520include%2520the%2520operator%2520computing%2520persistence%2520diagrams%2520as%2520a%2520special%2520case.%2520Their%2520theoretical%2520foundations%2520rely%2520on%2520group%2520actions%252C%2520equivariance%252C%2520and%2520compactness%2520properties%2520of%2520operator%2520spaces%252C%2520grounding%2520them%2520in%2520algebra%2520and%2520geometry%2520while%2520enabling%2520both%2520mathematical%2520rigor%2520and%2520practical%2520relevance.%2520While%2520a%2520previous%2520representation%2520theorem%2520characterized%2520linear%2520GENEOs%2520acting%2520on%2520data%2520of%2520the%2520same%2520type%252C%2520many%2520real-world%2520applications%2520require%2520operators%2520between%2520heterogeneous%2520data%2520spaces.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520limitation%2520by%2520introducing%2520a%2520new%2520representation%2520theorem%2520for%2520linear%2520GENEOs%2520acting%2520between%2520different%2520perception%2520pairs%252C%2520based%2520on%2520generalized%2520T-permutant%2520measures.%2520Under%2520mild%2520assumptions%2520on%2520the%2520data%2520domains%2520and%2520group%2520actions%252C%2520our%2520result%2520provides%2520a%2520complete%2520characterization%2520of%2520such%2520operators.%2520We%2520also%2520prove%2520the%2520compactness%2520and%2520convexity%2520of%2520the%2520space%2520of%2520linear%2520GENEOs.%2520We%2520further%2520demonstrate%2520the%2520practical%2520impact%2520of%2520this%2520theory%2520by%2520applying%2520the%2520proposed%2520framework%2520to%2520improve%2520the%2520performance%2520of%2520autoencoders%252C%2520highlighting%2520the%2520relevance%2520of%2520GENEOs%2520in%2520modern%2520machine%2520learning%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Algebraic%20Representation%20Theorem%20for%20Linear%20GENEOs%20in%20Geometric%20Machine%20Learning&entry.906535625=Francesco%20Conti%20and%20Patrizio%20Frosini%20and%20Nicola%20Quercioli&entry.1292438233=Geometric%20and%20Topological%20Deep%20Learning%20are%20rapidly%20growing%20research%20areas%20that%20enhance%20machine%20learning%20through%20the%20use%20of%20geometric%20and%20topological%20structures.%20Within%20this%20framework%2C%20Group%20Equivariant%20Non-Expansive%20Operators%20%28GENEOs%29%20have%20emerged%20as%20a%20powerful%20class%20of%20operators%20for%20encoding%20symmetries%20and%20designing%20efficient%2C%20interpretable%20neural%20architectures.%20Originally%20introduced%20in%20Topological%20Data%20Analysis%2C%20GENEOs%20have%20since%20found%20applications%20in%20Deep%20Learning%20as%20tools%20for%20constructing%20equivariant%20models%20with%20reduced%20parameter%20complexity.%20GENEOs%20provide%20a%20unifying%20framework%20bridging%20Geometric%20and%20Topological%20Deep%20Learning%20and%20include%20the%20operator%20computing%20persistence%20diagrams%20as%20a%20special%20case.%20Their%20theoretical%20foundations%20rely%20on%20group%20actions%2C%20equivariance%2C%20and%20compactness%20properties%20of%20operator%20spaces%2C%20grounding%20them%20in%20algebra%20and%20geometry%20while%20enabling%20both%20mathematical%20rigor%20and%20practical%20relevance.%20While%20a%20previous%20representation%20theorem%20characterized%20linear%20GENEOs%20acting%20on%20data%20of%20the%20same%20type%2C%20many%20real-world%20applications%20require%20operators%20between%20heterogeneous%20data%20spaces.%20In%20this%20work%2C%20we%20address%20this%20limitation%20by%20introducing%20a%20new%20representation%20theorem%20for%20linear%20GENEOs%20acting%20between%20different%20perception%20pairs%2C%20based%20on%20generalized%20T-permutant%20measures.%20Under%20mild%20assumptions%20on%20the%20data%20domains%20and%20group%20actions%2C%20our%20result%20provides%20a%20complete%20characterization%20of%20such%20operators.%20We%20also%20prove%20the%20compactness%20and%20convexity%20of%20the%20space%20of%20linear%20GENEOs.%20We%20further%20demonstrate%20the%20practical%20impact%20of%20this%20theory%20by%20applying%20the%20proposed%20framework%20to%20improve%20the%20performance%20of%20autoencoders%2C%20highlighting%20the%20relevance%20of%20GENEOs%20in%20modern%20machine%20learning%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.03910v1&entry.124074799=Read"},
{"title": "Compact Example-Based Explanations for Language Models", "author": "Loris Schoenegger and Benjamin Roth", "abstract": "Training data influence estimation methods quantify the contribution of training documents to a model's output, making them a promising source of information for example-based explanations. As humans cannot interpret thousands of documents, only a small subset of the training data can be presented as an explanation. Although the choice of which documents to include directly affects explanation quality, previous evaluations of such systems have largely ignored any selection strategies. To address this, we propose a novel selection relevance score, a retraining-free metric that quantifies how useful a set of examples is for explaining a model's output. We validate this score through fine-tuning experiments, confirming that it can predict whether a set of examples supports or undermines the model's predictions. Using this metric, we further show that common selection strategies often underperform random selection. Motivated by this finding, we propose a strategy that balances influence and representativeness, enabling better use of selection budgets than naively selecting the highest-ranking examples.", "link": "http://arxiv.org/abs/2601.03786v1", "date": "2026-01-07", "relevancy": 2.4196, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compact%20Example-Based%20Explanations%20for%20Language%20Models&body=Title%3A%20Compact%20Example-Based%20Explanations%20for%20Language%20Models%0AAuthor%3A%20Loris%20Schoenegger%20and%20Benjamin%20Roth%0AAbstract%3A%20Training%20data%20influence%20estimation%20methods%20quantify%20the%20contribution%20of%20training%20documents%20to%20a%20model%27s%20output%2C%20making%20them%20a%20promising%20source%20of%20information%20for%20example-based%20explanations.%20As%20humans%20cannot%20interpret%20thousands%20of%20documents%2C%20only%20a%20small%20subset%20of%20the%20training%20data%20can%20be%20presented%20as%20an%20explanation.%20Although%20the%20choice%20of%20which%20documents%20to%20include%20directly%20affects%20explanation%20quality%2C%20previous%20evaluations%20of%20such%20systems%20have%20largely%20ignored%20any%20selection%20strategies.%20To%20address%20this%2C%20we%20propose%20a%20novel%20selection%20relevance%20score%2C%20a%20retraining-free%20metric%20that%20quantifies%20how%20useful%20a%20set%20of%20examples%20is%20for%20explaining%20a%20model%27s%20output.%20We%20validate%20this%20score%20through%20fine-tuning%20experiments%2C%20confirming%20that%20it%20can%20predict%20whether%20a%20set%20of%20examples%20supports%20or%20undermines%20the%20model%27s%20predictions.%20Using%20this%20metric%2C%20we%20further%20show%20that%20common%20selection%20strategies%20often%20underperform%20random%20selection.%20Motivated%20by%20this%20finding%2C%20we%20propose%20a%20strategy%20that%20balances%20influence%20and%20representativeness%2C%20enabling%20better%20use%20of%20selection%20budgets%20than%20naively%20selecting%20the%20highest-ranking%20examples.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompact%2520Example-Based%2520Explanations%2520for%2520Language%2520Models%26entry.906535625%3DLoris%2520Schoenegger%2520and%2520Benjamin%2520Roth%26entry.1292438233%3DTraining%2520data%2520influence%2520estimation%2520methods%2520quantify%2520the%2520contribution%2520of%2520training%2520documents%2520to%2520a%2520model%2527s%2520output%252C%2520making%2520them%2520a%2520promising%2520source%2520of%2520information%2520for%2520example-based%2520explanations.%2520As%2520humans%2520cannot%2520interpret%2520thousands%2520of%2520documents%252C%2520only%2520a%2520small%2520subset%2520of%2520the%2520training%2520data%2520can%2520be%2520presented%2520as%2520an%2520explanation.%2520Although%2520the%2520choice%2520of%2520which%2520documents%2520to%2520include%2520directly%2520affects%2520explanation%2520quality%252C%2520previous%2520evaluations%2520of%2520such%2520systems%2520have%2520largely%2520ignored%2520any%2520selection%2520strategies.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520selection%2520relevance%2520score%252C%2520a%2520retraining-free%2520metric%2520that%2520quantifies%2520how%2520useful%2520a%2520set%2520of%2520examples%2520is%2520for%2520explaining%2520a%2520model%2527s%2520output.%2520We%2520validate%2520this%2520score%2520through%2520fine-tuning%2520experiments%252C%2520confirming%2520that%2520it%2520can%2520predict%2520whether%2520a%2520set%2520of%2520examples%2520supports%2520or%2520undermines%2520the%2520model%2527s%2520predictions.%2520Using%2520this%2520metric%252C%2520we%2520further%2520show%2520that%2520common%2520selection%2520strategies%2520often%2520underperform%2520random%2520selection.%2520Motivated%2520by%2520this%2520finding%252C%2520we%2520propose%2520a%2520strategy%2520that%2520balances%2520influence%2520and%2520representativeness%252C%2520enabling%2520better%2520use%2520of%2520selection%2520budgets%2520than%2520naively%2520selecting%2520the%2520highest-ranking%2520examples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compact%20Example-Based%20Explanations%20for%20Language%20Models&entry.906535625=Loris%20Schoenegger%20and%20Benjamin%20Roth&entry.1292438233=Training%20data%20influence%20estimation%20methods%20quantify%20the%20contribution%20of%20training%20documents%20to%20a%20model%27s%20output%2C%20making%20them%20a%20promising%20source%20of%20information%20for%20example-based%20explanations.%20As%20humans%20cannot%20interpret%20thousands%20of%20documents%2C%20only%20a%20small%20subset%20of%20the%20training%20data%20can%20be%20presented%20as%20an%20explanation.%20Although%20the%20choice%20of%20which%20documents%20to%20include%20directly%20affects%20explanation%20quality%2C%20previous%20evaluations%20of%20such%20systems%20have%20largely%20ignored%20any%20selection%20strategies.%20To%20address%20this%2C%20we%20propose%20a%20novel%20selection%20relevance%20score%2C%20a%20retraining-free%20metric%20that%20quantifies%20how%20useful%20a%20set%20of%20examples%20is%20for%20explaining%20a%20model%27s%20output.%20We%20validate%20this%20score%20through%20fine-tuning%20experiments%2C%20confirming%20that%20it%20can%20predict%20whether%20a%20set%20of%20examples%20supports%20or%20undermines%20the%20model%27s%20predictions.%20Using%20this%20metric%2C%20we%20further%20show%20that%20common%20selection%20strategies%20often%20underperform%20random%20selection.%20Motivated%20by%20this%20finding%2C%20we%20propose%20a%20strategy%20that%20balances%20influence%20and%20representativeness%2C%20enabling%20better%20use%20of%20selection%20budgets%20than%20naively%20selecting%20the%20highest-ranking%20examples.&entry.1838667208=http%3A//arxiv.org/abs/2601.03786v1&entry.124074799=Read"},
{"title": "Criminal Liability of Generative Artificial Intelligence Providers for User-Generated Child Sexual Abuse Material", "author": "Anamaria Mojica-Hanke and Thomas Goger and Svenja W\u00f6lfel and Brian Valerius and Steffen Herbold", "abstract": "The development of more powerful Generative Artificial Intelligence (GenAI) has expanded its capabilities and the variety of outputs. This has introduced significant legal challenges, including gray areas in various legal systems, such as the assessment of criminal liability for those responsible for these models. Therefore, we conducted a multidisciplinary study utilizing the statutory interpretation of relevant German laws, which, in conjunction with scenarios, provides a perspective on the different properties of GenAI in the context of Child Sexual Abuse Material (CSAM) generation. We found that generating CSAM with GenAI may have criminal and legal consequences not only for the user committing the primary offense but also for individuals responsible for the models, such as independent software developers, researchers, and company representatives. Additionally, the assessment of criminal liability may be affected by contextual and technical factors, including the type of generated image, content moderation policies, and the model's intended purpose. Based on our findings, we discussed the implications for different roles, as well as the requirements when developing such systems.", "link": "http://arxiv.org/abs/2601.03788v1", "date": "2026-01-07", "relevancy": 2.4058, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5041}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4892}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Criminal%20Liability%20of%20Generative%20Artificial%20Intelligence%20Providers%20for%20User-Generated%20Child%20Sexual%20Abuse%20Material&body=Title%3A%20Criminal%20Liability%20of%20Generative%20Artificial%20Intelligence%20Providers%20for%20User-Generated%20Child%20Sexual%20Abuse%20Material%0AAuthor%3A%20Anamaria%20Mojica-Hanke%20and%20Thomas%20Goger%20and%20Svenja%20W%C3%B6lfel%20and%20Brian%20Valerius%20and%20Steffen%20Herbold%0AAbstract%3A%20The%20development%20of%20more%20powerful%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20has%20expanded%20its%20capabilities%20and%20the%20variety%20of%20outputs.%20This%20has%20introduced%20significant%20legal%20challenges%2C%20including%20gray%20areas%20in%20various%20legal%20systems%2C%20such%20as%20the%20assessment%20of%20criminal%20liability%20for%20those%20responsible%20for%20these%20models.%20Therefore%2C%20we%20conducted%20a%20multidisciplinary%20study%20utilizing%20the%20statutory%20interpretation%20of%20relevant%20German%20laws%2C%20which%2C%20in%20conjunction%20with%20scenarios%2C%20provides%20a%20perspective%20on%20the%20different%20properties%20of%20GenAI%20in%20the%20context%20of%20Child%20Sexual%20Abuse%20Material%20%28CSAM%29%20generation.%20We%20found%20that%20generating%20CSAM%20with%20GenAI%20may%20have%20criminal%20and%20legal%20consequences%20not%20only%20for%20the%20user%20committing%20the%20primary%20offense%20but%20also%20for%20individuals%20responsible%20for%20the%20models%2C%20such%20as%20independent%20software%20developers%2C%20researchers%2C%20and%20company%20representatives.%20Additionally%2C%20the%20assessment%20of%20criminal%20liability%20may%20be%20affected%20by%20contextual%20and%20technical%20factors%2C%20including%20the%20type%20of%20generated%20image%2C%20content%20moderation%20policies%2C%20and%20the%20model%27s%20intended%20purpose.%20Based%20on%20our%20findings%2C%20we%20discussed%20the%20implications%20for%20different%20roles%2C%20as%20well%20as%20the%20requirements%20when%20developing%20such%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCriminal%2520Liability%2520of%2520Generative%2520Artificial%2520Intelligence%2520Providers%2520for%2520User-Generated%2520Child%2520Sexual%2520Abuse%2520Material%26entry.906535625%3DAnamaria%2520Mojica-Hanke%2520and%2520Thomas%2520Goger%2520and%2520Svenja%2520W%25C3%25B6lfel%2520and%2520Brian%2520Valerius%2520and%2520Steffen%2520Herbold%26entry.1292438233%3DThe%2520development%2520of%2520more%2520powerful%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520has%2520expanded%2520its%2520capabilities%2520and%2520the%2520variety%2520of%2520outputs.%2520This%2520has%2520introduced%2520significant%2520legal%2520challenges%252C%2520including%2520gray%2520areas%2520in%2520various%2520legal%2520systems%252C%2520such%2520as%2520the%2520assessment%2520of%2520criminal%2520liability%2520for%2520those%2520responsible%2520for%2520these%2520models.%2520Therefore%252C%2520we%2520conducted%2520a%2520multidisciplinary%2520study%2520utilizing%2520the%2520statutory%2520interpretation%2520of%2520relevant%2520German%2520laws%252C%2520which%252C%2520in%2520conjunction%2520with%2520scenarios%252C%2520provides%2520a%2520perspective%2520on%2520the%2520different%2520properties%2520of%2520GenAI%2520in%2520the%2520context%2520of%2520Child%2520Sexual%2520Abuse%2520Material%2520%2528CSAM%2529%2520generation.%2520We%2520found%2520that%2520generating%2520CSAM%2520with%2520GenAI%2520may%2520have%2520criminal%2520and%2520legal%2520consequences%2520not%2520only%2520for%2520the%2520user%2520committing%2520the%2520primary%2520offense%2520but%2520also%2520for%2520individuals%2520responsible%2520for%2520the%2520models%252C%2520such%2520as%2520independent%2520software%2520developers%252C%2520researchers%252C%2520and%2520company%2520representatives.%2520Additionally%252C%2520the%2520assessment%2520of%2520criminal%2520liability%2520may%2520be%2520affected%2520by%2520contextual%2520and%2520technical%2520factors%252C%2520including%2520the%2520type%2520of%2520generated%2520image%252C%2520content%2520moderation%2520policies%252C%2520and%2520the%2520model%2527s%2520intended%2520purpose.%2520Based%2520on%2520our%2520findings%252C%2520we%2520discussed%2520the%2520implications%2520for%2520different%2520roles%252C%2520as%2520well%2520as%2520the%2520requirements%2520when%2520developing%2520such%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Criminal%20Liability%20of%20Generative%20Artificial%20Intelligence%20Providers%20for%20User-Generated%20Child%20Sexual%20Abuse%20Material&entry.906535625=Anamaria%20Mojica-Hanke%20and%20Thomas%20Goger%20and%20Svenja%20W%C3%B6lfel%20and%20Brian%20Valerius%20and%20Steffen%20Herbold&entry.1292438233=The%20development%20of%20more%20powerful%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20has%20expanded%20its%20capabilities%20and%20the%20variety%20of%20outputs.%20This%20has%20introduced%20significant%20legal%20challenges%2C%20including%20gray%20areas%20in%20various%20legal%20systems%2C%20such%20as%20the%20assessment%20of%20criminal%20liability%20for%20those%20responsible%20for%20these%20models.%20Therefore%2C%20we%20conducted%20a%20multidisciplinary%20study%20utilizing%20the%20statutory%20interpretation%20of%20relevant%20German%20laws%2C%20which%2C%20in%20conjunction%20with%20scenarios%2C%20provides%20a%20perspective%20on%20the%20different%20properties%20of%20GenAI%20in%20the%20context%20of%20Child%20Sexual%20Abuse%20Material%20%28CSAM%29%20generation.%20We%20found%20that%20generating%20CSAM%20with%20GenAI%20may%20have%20criminal%20and%20legal%20consequences%20not%20only%20for%20the%20user%20committing%20the%20primary%20offense%20but%20also%20for%20individuals%20responsible%20for%20the%20models%2C%20such%20as%20independent%20software%20developers%2C%20researchers%2C%20and%20company%20representatives.%20Additionally%2C%20the%20assessment%20of%20criminal%20liability%20may%20be%20affected%20by%20contextual%20and%20technical%20factors%2C%20including%20the%20type%20of%20generated%20image%2C%20content%20moderation%20policies%2C%20and%20the%20model%27s%20intended%20purpose.%20Based%20on%20our%20findings%2C%20we%20discussed%20the%20implications%20for%20different%20roles%2C%20as%20well%20as%20the%20requirements%20when%20developing%20such%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.03788v1&entry.124074799=Read"},
{"title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents", "author": "Dehao Tao and Guoliang Ma and Yongfeng Huang and Minghu Jiang", "abstract": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.", "link": "http://arxiv.org/abs/2601.03785v1", "date": "2026-01-07", "relevancy": 2.4014, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4771}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Membox%3A%20Weaving%20Topic%20Continuity%20into%20Long-Range%20Memory%20for%20LLM%20Agents&body=Title%3A%20Membox%3A%20Weaving%20Topic%20Continuity%20into%20Long-Range%20Memory%20for%20LLM%20Agents%0AAuthor%3A%20Dehao%20Tao%20and%20Guoliang%20Ma%20and%20Yongfeng%20Huang%20and%20Minghu%20Jiang%0AAbstract%3A%20Human-agent%20dialogues%20often%20exhibit%20topic%20continuity-a%20stable%20thematic%20frame%20that%20evolves%20through%20temporally%20adjacent%20exchanges-yet%20most%20large%20language%20model%20%28LLM%29%20agent%20memory%20systems%20fail%20to%20preserve%20it.%20Existing%20designs%20follow%20a%20fragmentation-compensation%20paradigm%3A%20they%20first%20break%20dialogue%20streams%20into%20isolated%20utterances%20for%20storage%2C%20then%20attempt%20to%20restore%20coherence%20via%20embedding-based%20retrieval.%20This%20process%20irreversibly%20damages%20narrative%20and%20causal%20flow%2C%20while%20biasing%20retrieval%20towards%20lexical%20similarity.%20We%20introduce%20membox%2C%20a%20hierarchical%20memory%20architecture%20centered%20on%20a%20Topic%20Loom%20that%20continuously%20monitors%20dialogue%20in%20a%20sliding-window%20fashion%2C%20grouping%20consecutive%20same-topic%20turns%20into%20coherent%20%22memory%20boxes%22%20at%20storage%20time.%20Sealed%20boxes%20are%20then%20linked%20by%20a%20Trace%20Weaver%20into%20long-range%20event-timeline%20traces%2C%20recovering%20macro-topic%20recurrences%20across%20discontinuities.%20Experiments%20on%20LoCoMo%20demonstrate%20that%20Membox%20achieves%20up%20to%2068%25%20F1%20improvement%20on%20temporal%20reasoning%20tasks%2C%20outperforming%20competitive%20baselines%20%28e.g.%2C%20Mem0%2C%20A-MEM%29.%20Notably%2C%20Membox%20attains%20these%20gains%20while%20using%20only%20a%20fraction%20of%20the%20context%20tokens%20required%20by%20existing%20methods%2C%20highlighting%20a%20superior%20balance%20between%20efficiency%20and%20effectiveness.%20By%20explicitly%20modeling%20topic%20continuity%2C%20Membox%20offers%20a%20cognitively%20motivated%20mechanism%20for%20enhancing%20both%20coherence%20and%20efficiency%20in%20LLM%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMembox%253A%2520Weaving%2520Topic%2520Continuity%2520into%2520Long-Range%2520Memory%2520for%2520LLM%2520Agents%26entry.906535625%3DDehao%2520Tao%2520and%2520Guoliang%2520Ma%2520and%2520Yongfeng%2520Huang%2520and%2520Minghu%2520Jiang%26entry.1292438233%3DHuman-agent%2520dialogues%2520often%2520exhibit%2520topic%2520continuity-a%2520stable%2520thematic%2520frame%2520that%2520evolves%2520through%2520temporally%2520adjacent%2520exchanges-yet%2520most%2520large%2520language%2520model%2520%2528LLM%2529%2520agent%2520memory%2520systems%2520fail%2520to%2520preserve%2520it.%2520Existing%2520designs%2520follow%2520a%2520fragmentation-compensation%2520paradigm%253A%2520they%2520first%2520break%2520dialogue%2520streams%2520into%2520isolated%2520utterances%2520for%2520storage%252C%2520then%2520attempt%2520to%2520restore%2520coherence%2520via%2520embedding-based%2520retrieval.%2520This%2520process%2520irreversibly%2520damages%2520narrative%2520and%2520causal%2520flow%252C%2520while%2520biasing%2520retrieval%2520towards%2520lexical%2520similarity.%2520We%2520introduce%2520membox%252C%2520a%2520hierarchical%2520memory%2520architecture%2520centered%2520on%2520a%2520Topic%2520Loom%2520that%2520continuously%2520monitors%2520dialogue%2520in%2520a%2520sliding-window%2520fashion%252C%2520grouping%2520consecutive%2520same-topic%2520turns%2520into%2520coherent%2520%2522memory%2520boxes%2522%2520at%2520storage%2520time.%2520Sealed%2520boxes%2520are%2520then%2520linked%2520by%2520a%2520Trace%2520Weaver%2520into%2520long-range%2520event-timeline%2520traces%252C%2520recovering%2520macro-topic%2520recurrences%2520across%2520discontinuities.%2520Experiments%2520on%2520LoCoMo%2520demonstrate%2520that%2520Membox%2520achieves%2520up%2520to%252068%2525%2520F1%2520improvement%2520on%2520temporal%2520reasoning%2520tasks%252C%2520outperforming%2520competitive%2520baselines%2520%2528e.g.%252C%2520Mem0%252C%2520A-MEM%2529.%2520Notably%252C%2520Membox%2520attains%2520these%2520gains%2520while%2520using%2520only%2520a%2520fraction%2520of%2520the%2520context%2520tokens%2520required%2520by%2520existing%2520methods%252C%2520highlighting%2520a%2520superior%2520balance%2520between%2520efficiency%2520and%2520effectiveness.%2520By%2520explicitly%2520modeling%2520topic%2520continuity%252C%2520Membox%2520offers%2520a%2520cognitively%2520motivated%2520mechanism%2520for%2520enhancing%2520both%2520coherence%2520and%2520efficiency%2520in%2520LLM%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Membox%3A%20Weaving%20Topic%20Continuity%20into%20Long-Range%20Memory%20for%20LLM%20Agents&entry.906535625=Dehao%20Tao%20and%20Guoliang%20Ma%20and%20Yongfeng%20Huang%20and%20Minghu%20Jiang&entry.1292438233=Human-agent%20dialogues%20often%20exhibit%20topic%20continuity-a%20stable%20thematic%20frame%20that%20evolves%20through%20temporally%20adjacent%20exchanges-yet%20most%20large%20language%20model%20%28LLM%29%20agent%20memory%20systems%20fail%20to%20preserve%20it.%20Existing%20designs%20follow%20a%20fragmentation-compensation%20paradigm%3A%20they%20first%20break%20dialogue%20streams%20into%20isolated%20utterances%20for%20storage%2C%20then%20attempt%20to%20restore%20coherence%20via%20embedding-based%20retrieval.%20This%20process%20irreversibly%20damages%20narrative%20and%20causal%20flow%2C%20while%20biasing%20retrieval%20towards%20lexical%20similarity.%20We%20introduce%20membox%2C%20a%20hierarchical%20memory%20architecture%20centered%20on%20a%20Topic%20Loom%20that%20continuously%20monitors%20dialogue%20in%20a%20sliding-window%20fashion%2C%20grouping%20consecutive%20same-topic%20turns%20into%20coherent%20%22memory%20boxes%22%20at%20storage%20time.%20Sealed%20boxes%20are%20then%20linked%20by%20a%20Trace%20Weaver%20into%20long-range%20event-timeline%20traces%2C%20recovering%20macro-topic%20recurrences%20across%20discontinuities.%20Experiments%20on%20LoCoMo%20demonstrate%20that%20Membox%20achieves%20up%20to%2068%25%20F1%20improvement%20on%20temporal%20reasoning%20tasks%2C%20outperforming%20competitive%20baselines%20%28e.g.%2C%20Mem0%2C%20A-MEM%29.%20Notably%2C%20Membox%20attains%20these%20gains%20while%20using%20only%20a%20fraction%20of%20the%20context%20tokens%20required%20by%20existing%20methods%2C%20highlighting%20a%20superior%20balance%20between%20efficiency%20and%20effectiveness.%20By%20explicitly%20modeling%20topic%20continuity%2C%20Membox%20offers%20a%20cognitively%20motivated%20mechanism%20for%20enhancing%20both%20coherence%20and%20efficiency%20in%20LLM%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.03785v1&entry.124074799=Read"},
{"title": "UniVideo: Unified Understanding, Generation, and Editing for Videos", "author": "Cong Wei and Quande Liu and Zixuan Ye and Qiulin Wang and Xintao Wang and Pengfei Wan and Kun Gai and Wenhu Chen", "abstract": "Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design preserves the MLLM's original text generation capabilities, enables accurate interpretation of complex multimodal instructions, and maintains visual consistency in the generated content. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as changing the environment or altering materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we released our model and code.", "link": "http://arxiv.org/abs/2510.08377v3", "date": "2026-01-07", "relevancy": 2.3998, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6178}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6093}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos&body=Title%3A%20UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos%0AAuthor%3A%20Cong%20Wei%20and%20Quande%20Liu%20and%20Zixuan%20Ye%20and%20Qiulin%20Wang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Wenhu%20Chen%0AAbstract%3A%20Unified%20multimodal%20models%20have%20shown%20promising%20results%20in%20multimodal%20content%20generation%20and%20editing%20but%20remain%20largely%20limited%20to%20the%20image%20domain.%20In%20this%20work%2C%20we%20present%20UniVideo%2C%20a%20versatile%20framework%20that%20extends%20unified%20modeling%20to%20the%20video%20domain.%20UniVideo%20adopts%20a%20dual-stream%20design%2C%20combining%20a%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20for%20instruction%20understanding%20with%20a%20Multimodal%20DiT%20%28MMDiT%29%20for%20video%20generation.%20This%20design%20preserves%20the%20MLLM%27s%20original%20text%20generation%20capabilities%2C%20enables%20accurate%20interpretation%20of%20complex%20multimodal%20instructions%2C%20and%20maintains%20visual%20consistency%20in%20the%20generated%20content.%20Built%20on%20this%20architecture%2C%20UniVideo%20unifies%20diverse%20video%20generation%20and%20editing%20tasks%20under%20a%20single%20multimodal%20instruction%20paradigm%20and%20is%20jointly%20trained%20across%20them.%20Extensive%20experiments%20demonstrate%20that%20UniVideo%20matches%20or%20surpasses%20state-of-the-art%20task-specific%20baselines%20in%20text/image-to-video%20generation%2C%20in-context%20video%20generation%20and%20in-context%20video%20editing.%20Notably%2C%20the%20unified%20design%20of%20UniVideo%20enables%20two%20forms%20of%20generalization.%20First%2C%20UniVideo%20supports%20task%20composition%2C%20such%20as%20combining%20editing%20with%20style%20transfer%2C%20by%20integrating%20multiple%20capabilities%20within%20a%20single%20instruction.%20Second%2C%20even%20without%20explicit%20training%20on%20free-form%20video%20editing%2C%20UniVideo%20transfers%20its%20editing%20capability%20from%20large-scale%20image%20editing%20data%20to%20this%20setting%2C%20handling%20unseen%20instructions%20such%20as%20changing%20the%20environment%20or%20altering%20materials%20within%20a%20video.%20Beyond%20these%20core%20capabilities%2C%20UniVideo%20also%20supports%20visual-prompt-based%20video%20generation%2C%20where%20the%20MLLM%20interprets%20visual%20prompts%20and%20guides%20the%20MMDiT%20during%20synthesis.%20To%20foster%20future%20research%2C%20we%20released%20our%20model%20and%20code.%0ALink%3A%20http%3A//arxiv.org/abs/2510.08377v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniVideo%253A%2520Unified%2520Understanding%252C%2520Generation%252C%2520and%2520Editing%2520for%2520Videos%26entry.906535625%3DCong%2520Wei%2520and%2520Quande%2520Liu%2520and%2520Zixuan%2520Ye%2520and%2520Qiulin%2520Wang%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%2520and%2520Wenhu%2520Chen%26entry.1292438233%3DUnified%2520multimodal%2520models%2520have%2520shown%2520promising%2520results%2520in%2520multimodal%2520content%2520generation%2520and%2520editing%2520but%2520remain%2520largely%2520limited%2520to%2520the%2520image%2520domain.%2520In%2520this%2520work%252C%2520we%2520present%2520UniVideo%252C%2520a%2520versatile%2520framework%2520that%2520extends%2520unified%2520modeling%2520to%2520the%2520video%2520domain.%2520UniVideo%2520adopts%2520a%2520dual-stream%2520design%252C%2520combining%2520a%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520for%2520instruction%2520understanding%2520with%2520a%2520Multimodal%2520DiT%2520%2528MMDiT%2529%2520for%2520video%2520generation.%2520This%2520design%2520preserves%2520the%2520MLLM%2527s%2520original%2520text%2520generation%2520capabilities%252C%2520enables%2520accurate%2520interpretation%2520of%2520complex%2520multimodal%2520instructions%252C%2520and%2520maintains%2520visual%2520consistency%2520in%2520the%2520generated%2520content.%2520Built%2520on%2520this%2520architecture%252C%2520UniVideo%2520unifies%2520diverse%2520video%2520generation%2520and%2520editing%2520tasks%2520under%2520a%2520single%2520multimodal%2520instruction%2520paradigm%2520and%2520is%2520jointly%2520trained%2520across%2520them.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UniVideo%2520matches%2520or%2520surpasses%2520state-of-the-art%2520task-specific%2520baselines%2520in%2520text/image-to-video%2520generation%252C%2520in-context%2520video%2520generation%2520and%2520in-context%2520video%2520editing.%2520Notably%252C%2520the%2520unified%2520design%2520of%2520UniVideo%2520enables%2520two%2520forms%2520of%2520generalization.%2520First%252C%2520UniVideo%2520supports%2520task%2520composition%252C%2520such%2520as%2520combining%2520editing%2520with%2520style%2520transfer%252C%2520by%2520integrating%2520multiple%2520capabilities%2520within%2520a%2520single%2520instruction.%2520Second%252C%2520even%2520without%2520explicit%2520training%2520on%2520free-form%2520video%2520editing%252C%2520UniVideo%2520transfers%2520its%2520editing%2520capability%2520from%2520large-scale%2520image%2520editing%2520data%2520to%2520this%2520setting%252C%2520handling%2520unseen%2520instructions%2520such%2520as%2520changing%2520the%2520environment%2520or%2520altering%2520materials%2520within%2520a%2520video.%2520Beyond%2520these%2520core%2520capabilities%252C%2520UniVideo%2520also%2520supports%2520visual-prompt-based%2520video%2520generation%252C%2520where%2520the%2520MLLM%2520interprets%2520visual%2520prompts%2520and%2520guides%2520the%2520MMDiT%2520during%2520synthesis.%2520To%2520foster%2520future%2520research%252C%2520we%2520released%2520our%2520model%2520and%2520code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08377v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniVideo%3A%20Unified%20Understanding%2C%20Generation%2C%20and%20Editing%20for%20Videos&entry.906535625=Cong%20Wei%20and%20Quande%20Liu%20and%20Zixuan%20Ye%20and%20Qiulin%20Wang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Wenhu%20Chen&entry.1292438233=Unified%20multimodal%20models%20have%20shown%20promising%20results%20in%20multimodal%20content%20generation%20and%20editing%20but%20remain%20largely%20limited%20to%20the%20image%20domain.%20In%20this%20work%2C%20we%20present%20UniVideo%2C%20a%20versatile%20framework%20that%20extends%20unified%20modeling%20to%20the%20video%20domain.%20UniVideo%20adopts%20a%20dual-stream%20design%2C%20combining%20a%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20for%20instruction%20understanding%20with%20a%20Multimodal%20DiT%20%28MMDiT%29%20for%20video%20generation.%20This%20design%20preserves%20the%20MLLM%27s%20original%20text%20generation%20capabilities%2C%20enables%20accurate%20interpretation%20of%20complex%20multimodal%20instructions%2C%20and%20maintains%20visual%20consistency%20in%20the%20generated%20content.%20Built%20on%20this%20architecture%2C%20UniVideo%20unifies%20diverse%20video%20generation%20and%20editing%20tasks%20under%20a%20single%20multimodal%20instruction%20paradigm%20and%20is%20jointly%20trained%20across%20them.%20Extensive%20experiments%20demonstrate%20that%20UniVideo%20matches%20or%20surpasses%20state-of-the-art%20task-specific%20baselines%20in%20text/image-to-video%20generation%2C%20in-context%20video%20generation%20and%20in-context%20video%20editing.%20Notably%2C%20the%20unified%20design%20of%20UniVideo%20enables%20two%20forms%20of%20generalization.%20First%2C%20UniVideo%20supports%20task%20composition%2C%20such%20as%20combining%20editing%20with%20style%20transfer%2C%20by%20integrating%20multiple%20capabilities%20within%20a%20single%20instruction.%20Second%2C%20even%20without%20explicit%20training%20on%20free-form%20video%20editing%2C%20UniVideo%20transfers%20its%20editing%20capability%20from%20large-scale%20image%20editing%20data%20to%20this%20setting%2C%20handling%20unseen%20instructions%20such%20as%20changing%20the%20environment%20or%20altering%20materials%20within%20a%20video.%20Beyond%20these%20core%20capabilities%2C%20UniVideo%20also%20supports%20visual-prompt-based%20video%20generation%2C%20where%20the%20MLLM%20interprets%20visual%20prompts%20and%20guides%20the%20MMDiT%20during%20synthesis.%20To%20foster%20future%20research%2C%20we%20released%20our%20model%20and%20code.&entry.1838667208=http%3A//arxiv.org/abs/2510.08377v3&entry.124074799=Read"},
{"title": "VISTA: Mitigating Semantic Inertia in Video-LLMs via Training-Free Dynamic Chain-of-Thought Routing", "author": "Hongbo Jin and Jiayu Ding and Siyi Xie and Guibo Luo and Ge Li", "abstract": "Recent advancements in Large Language Models have successfully transitioned towards System 2 reasoning, yet applying these paradigms to video understanding remains challenging. While prevailing research attributes failures in Video-LLMs to perceptual limitations, our empirical analysis reveals a cognitive misalignment termed Semantic Inertia, where models suppress valid visual evidence in favor of dominant language priors. To rectify this, we propose VISTA, a training-free framework designed to align perception with logical deduction. By dynamically routing inference paths and materializing implicit visual features into explicit textual anchors, our approach effectively counterbalances the influence of parametric knowledge. Furthermore, we incorporate a Latent Reasoning Consensus mechanism to mitigate stochastic hallucinations. VISTA showed outstanding results on a wide range of benchmarks, and outperforms its base model by 9.3% on Egochema and 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary models. Our codebase will be publicly available soon.", "link": "http://arxiv.org/abs/2505.11830v3", "date": "2026-01-07", "relevancy": 2.3974, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VISTA%3A%20Mitigating%20Semantic%20Inertia%20in%20Video-LLMs%20via%20Training-Free%20Dynamic%20Chain-of-Thought%20Routing&body=Title%3A%20VISTA%3A%20Mitigating%20Semantic%20Inertia%20in%20Video-LLMs%20via%20Training-Free%20Dynamic%20Chain-of-Thought%20Routing%0AAuthor%3A%20Hongbo%20Jin%20and%20Jiayu%20Ding%20and%20Siyi%20Xie%20and%20Guibo%20Luo%20and%20Ge%20Li%0AAbstract%3A%20Recent%20advancements%20in%20Large%20Language%20Models%20have%20successfully%20transitioned%20towards%20System%202%20reasoning%2C%20yet%20applying%20these%20paradigms%20to%20video%20understanding%20remains%20challenging.%20While%20prevailing%20research%20attributes%20failures%20in%20Video-LLMs%20to%20perceptual%20limitations%2C%20our%20empirical%20analysis%20reveals%20a%20cognitive%20misalignment%20termed%20Semantic%20Inertia%2C%20where%20models%20suppress%20valid%20visual%20evidence%20in%20favor%20of%20dominant%20language%20priors.%20To%20rectify%20this%2C%20we%20propose%20VISTA%2C%20a%20training-free%20framework%20designed%20to%20align%20perception%20with%20logical%20deduction.%20By%20dynamically%20routing%20inference%20paths%20and%20materializing%20implicit%20visual%20features%20into%20explicit%20textual%20anchors%2C%20our%20approach%20effectively%20counterbalances%20the%20influence%20of%20parametric%20knowledge.%20Furthermore%2C%20we%20incorporate%20a%20Latent%20Reasoning%20Consensus%20mechanism%20to%20mitigate%20stochastic%20hallucinations.%20VISTA%20showed%20outstanding%20results%20on%20a%20wide%20range%20of%20benchmarks%2C%20and%20outperforms%20its%20base%20model%20by%209.3%25%20on%20Egochema%20and%205.6%25%20on%20VideoEspresso%2C%20rivalling%20or%20even%20surpassing%20larger%20and%20proprietary%20models.%20Our%20codebase%20will%20be%20publicly%20available%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11830v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVISTA%253A%2520Mitigating%2520Semantic%2520Inertia%2520in%2520Video-LLMs%2520via%2520Training-Free%2520Dynamic%2520Chain-of-Thought%2520Routing%26entry.906535625%3DHongbo%2520Jin%2520and%2520Jiayu%2520Ding%2520and%2520Siyi%2520Xie%2520and%2520Guibo%2520Luo%2520and%2520Ge%2520Li%26entry.1292438233%3DRecent%2520advancements%2520in%2520Large%2520Language%2520Models%2520have%2520successfully%2520transitioned%2520towards%2520System%25202%2520reasoning%252C%2520yet%2520applying%2520these%2520paradigms%2520to%2520video%2520understanding%2520remains%2520challenging.%2520While%2520prevailing%2520research%2520attributes%2520failures%2520in%2520Video-LLMs%2520to%2520perceptual%2520limitations%252C%2520our%2520empirical%2520analysis%2520reveals%2520a%2520cognitive%2520misalignment%2520termed%2520Semantic%2520Inertia%252C%2520where%2520models%2520suppress%2520valid%2520visual%2520evidence%2520in%2520favor%2520of%2520dominant%2520language%2520priors.%2520To%2520rectify%2520this%252C%2520we%2520propose%2520VISTA%252C%2520a%2520training-free%2520framework%2520designed%2520to%2520align%2520perception%2520with%2520logical%2520deduction.%2520By%2520dynamically%2520routing%2520inference%2520paths%2520and%2520materializing%2520implicit%2520visual%2520features%2520into%2520explicit%2520textual%2520anchors%252C%2520our%2520approach%2520effectively%2520counterbalances%2520the%2520influence%2520of%2520parametric%2520knowledge.%2520Furthermore%252C%2520we%2520incorporate%2520a%2520Latent%2520Reasoning%2520Consensus%2520mechanism%2520to%2520mitigate%2520stochastic%2520hallucinations.%2520VISTA%2520showed%2520outstanding%2520results%2520on%2520a%2520wide%2520range%2520of%2520benchmarks%252C%2520and%2520outperforms%2520its%2520base%2520model%2520by%25209.3%2525%2520on%2520Egochema%2520and%25205.6%2525%2520on%2520VideoEspresso%252C%2520rivalling%2520or%2520even%2520surpassing%2520larger%2520and%2520proprietary%2520models.%2520Our%2520codebase%2520will%2520be%2520publicly%2520available%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11830v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VISTA%3A%20Mitigating%20Semantic%20Inertia%20in%20Video-LLMs%20via%20Training-Free%20Dynamic%20Chain-of-Thought%20Routing&entry.906535625=Hongbo%20Jin%20and%20Jiayu%20Ding%20and%20Siyi%20Xie%20and%20Guibo%20Luo%20and%20Ge%20Li&entry.1292438233=Recent%20advancements%20in%20Large%20Language%20Models%20have%20successfully%20transitioned%20towards%20System%202%20reasoning%2C%20yet%20applying%20these%20paradigms%20to%20video%20understanding%20remains%20challenging.%20While%20prevailing%20research%20attributes%20failures%20in%20Video-LLMs%20to%20perceptual%20limitations%2C%20our%20empirical%20analysis%20reveals%20a%20cognitive%20misalignment%20termed%20Semantic%20Inertia%2C%20where%20models%20suppress%20valid%20visual%20evidence%20in%20favor%20of%20dominant%20language%20priors.%20To%20rectify%20this%2C%20we%20propose%20VISTA%2C%20a%20training-free%20framework%20designed%20to%20align%20perception%20with%20logical%20deduction.%20By%20dynamically%20routing%20inference%20paths%20and%20materializing%20implicit%20visual%20features%20into%20explicit%20textual%20anchors%2C%20our%20approach%20effectively%20counterbalances%20the%20influence%20of%20parametric%20knowledge.%20Furthermore%2C%20we%20incorporate%20a%20Latent%20Reasoning%20Consensus%20mechanism%20to%20mitigate%20stochastic%20hallucinations.%20VISTA%20showed%20outstanding%20results%20on%20a%20wide%20range%20of%20benchmarks%2C%20and%20outperforms%20its%20base%20model%20by%209.3%25%20on%20Egochema%20and%205.6%25%20on%20VideoEspresso%2C%20rivalling%20or%20even%20surpassing%20larger%20and%20proprietary%20models.%20Our%20codebase%20will%20be%20publicly%20available%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2505.11830v3&entry.124074799=Read"},
{"title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners", "author": "Kefan Song and Amir Moeini and Peng Wang and Lei Gong and Rohan Chandra and Shangtong Zhang and Yanjun Qi", "abstract": "Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.", "link": "http://arxiv.org/abs/2506.06303v4", "date": "2026-01-07", "relevancy": 2.3563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward%20Is%20Enough%3A%20LLMs%20Are%20In-Context%20Reinforcement%20Learners&body=Title%3A%20Reward%20Is%20Enough%3A%20LLMs%20Are%20In-Context%20Reinforcement%20Learners%0AAuthor%3A%20Kefan%20Song%20and%20Amir%20Moeini%20and%20Peng%20Wang%20and%20Lei%20Gong%20and%20Rohan%20Chandra%20and%20Shangtong%20Zhang%20and%20Yanjun%20Qi%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20is%20a%20framework%20for%20solving%20sequential%20decision-making%20problems.%20In%20this%20work%2C%20we%20demonstrate%20that%2C%20surprisingly%2C%20RL%20emerges%20during%20the%20inference%20time%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20phenomenon%20we%20term%20in-context%20RL%20%28ICRL%29.%20To%20reveal%20this%20capability%2C%20we%20introduce%20a%20simple%20multi-round%20prompting%20framework%2C%20we%20call%20ICRL%20prompting%2C%20for%20inference-time%20self-improvement.%20The%20goal%20of%20ICRL%20prompting%20is%20to%20guide%20LLMs%20to%20perform%20reinforcement%20learning%20during%20inference%20for%20self-improvement%20on%20a%20given%20task.%20After%20each%20response%2C%20the%20model%20receives%20numerical%20scalar%20feedback%2C%20denoted%20as%20a%20reward.%20In%20the%20next%20round%2C%20we%20prompt%20the%20LLM%20again%20together%20with%20a%20context%20that%20concatenates%20all%20prior%20responses%20and%20their%20associated%20rewards.%20We%20consistently%20observe%20that%20response%20quality%20improves%20as%20the%20context%20grows.%20In%20other%20words%2C%20the%20LLM%20can%20optimize%20scalar%20reward%20signals%20during%20inference%2C%20exhibiting%20behavior%20analogous%20to%20reinforcement%20learning.%20We%20evaluate%20ICRL%20prompting%20on%20Game%20of%2024%2C%20creative%20writing%2C%20ScienceWorld%2C%20and%20Olympiad-level%20math%20competitions%20%28AIME%20and%20HMMT%29%2C%20demonstrating%20significant%20improvements%20over%20baselines%20such%20as%20Self-Refine%20and%20Reflexion.%20Notably%2C%20even%20when%20the%20reward%20signals%20are%20generated%20by%20the%20same%20LLM%2C%20ICRL%20prompting%20still%20improves%20performance%2C%20highlighting%20a%20promising%20new%20paradigm%20for%20test-time%20scaling.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06303v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward%2520Is%2520Enough%253A%2520LLMs%2520Are%2520In-Context%2520Reinforcement%2520Learners%26entry.906535625%3DKefan%2520Song%2520and%2520Amir%2520Moeini%2520and%2520Peng%2520Wang%2520and%2520Lei%2520Gong%2520and%2520Rohan%2520Chandra%2520and%2520Shangtong%2520Zhang%2520and%2520Yanjun%2520Qi%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520is%2520a%2520framework%2520for%2520solving%2520sequential%2520decision-making%2520problems.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%252C%2520surprisingly%252C%2520RL%2520emerges%2520during%2520the%2520inference%2520time%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520a%2520phenomenon%2520we%2520term%2520in-context%2520RL%2520%2528ICRL%2529.%2520To%2520reveal%2520this%2520capability%252C%2520we%2520introduce%2520a%2520simple%2520multi-round%2520prompting%2520framework%252C%2520we%2520call%2520ICRL%2520prompting%252C%2520for%2520inference-time%2520self-improvement.%2520The%2520goal%2520of%2520ICRL%2520prompting%2520is%2520to%2520guide%2520LLMs%2520to%2520perform%2520reinforcement%2520learning%2520during%2520inference%2520for%2520self-improvement%2520on%2520a%2520given%2520task.%2520After%2520each%2520response%252C%2520the%2520model%2520receives%2520numerical%2520scalar%2520feedback%252C%2520denoted%2520as%2520a%2520reward.%2520In%2520the%2520next%2520round%252C%2520we%2520prompt%2520the%2520LLM%2520again%2520together%2520with%2520a%2520context%2520that%2520concatenates%2520all%2520prior%2520responses%2520and%2520their%2520associated%2520rewards.%2520We%2520consistently%2520observe%2520that%2520response%2520quality%2520improves%2520as%2520the%2520context%2520grows.%2520In%2520other%2520words%252C%2520the%2520LLM%2520can%2520optimize%2520scalar%2520reward%2520signals%2520during%2520inference%252C%2520exhibiting%2520behavior%2520analogous%2520to%2520reinforcement%2520learning.%2520We%2520evaluate%2520ICRL%2520prompting%2520on%2520Game%2520of%252024%252C%2520creative%2520writing%252C%2520ScienceWorld%252C%2520and%2520Olympiad-level%2520math%2520competitions%2520%2528AIME%2520and%2520HMMT%2529%252C%2520demonstrating%2520significant%2520improvements%2520over%2520baselines%2520such%2520as%2520Self-Refine%2520and%2520Reflexion.%2520Notably%252C%2520even%2520when%2520the%2520reward%2520signals%2520are%2520generated%2520by%2520the%2520same%2520LLM%252C%2520ICRL%2520prompting%2520still%2520improves%2520performance%252C%2520highlighting%2520a%2520promising%2520new%2520paradigm%2520for%2520test-time%2520scaling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06303v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward%20Is%20Enough%3A%20LLMs%20Are%20In-Context%20Reinforcement%20Learners&entry.906535625=Kefan%20Song%20and%20Amir%20Moeini%20and%20Peng%20Wang%20and%20Lei%20Gong%20and%20Rohan%20Chandra%20and%20Shangtong%20Zhang%20and%20Yanjun%20Qi&entry.1292438233=Reinforcement%20learning%20%28RL%29%20is%20a%20framework%20for%20solving%20sequential%20decision-making%20problems.%20In%20this%20work%2C%20we%20demonstrate%20that%2C%20surprisingly%2C%20RL%20emerges%20during%20the%20inference%20time%20of%20large%20language%20models%20%28LLMs%29%2C%20a%20phenomenon%20we%20term%20in-context%20RL%20%28ICRL%29.%20To%20reveal%20this%20capability%2C%20we%20introduce%20a%20simple%20multi-round%20prompting%20framework%2C%20we%20call%20ICRL%20prompting%2C%20for%20inference-time%20self-improvement.%20The%20goal%20of%20ICRL%20prompting%20is%20to%20guide%20LLMs%20to%20perform%20reinforcement%20learning%20during%20inference%20for%20self-improvement%20on%20a%20given%20task.%20After%20each%20response%2C%20the%20model%20receives%20numerical%20scalar%20feedback%2C%20denoted%20as%20a%20reward.%20In%20the%20next%20round%2C%20we%20prompt%20the%20LLM%20again%20together%20with%20a%20context%20that%20concatenates%20all%20prior%20responses%20and%20their%20associated%20rewards.%20We%20consistently%20observe%20that%20response%20quality%20improves%20as%20the%20context%20grows.%20In%20other%20words%2C%20the%20LLM%20can%20optimize%20scalar%20reward%20signals%20during%20inference%2C%20exhibiting%20behavior%20analogous%20to%20reinforcement%20learning.%20We%20evaluate%20ICRL%20prompting%20on%20Game%20of%2024%2C%20creative%20writing%2C%20ScienceWorld%2C%20and%20Olympiad-level%20math%20competitions%20%28AIME%20and%20HMMT%29%2C%20demonstrating%20significant%20improvements%20over%20baselines%20such%20as%20Self-Refine%20and%20Reflexion.%20Notably%2C%20even%20when%20the%20reward%20signals%20are%20generated%20by%20the%20same%20LLM%2C%20ICRL%20prompting%20still%20improves%20performance%2C%20highlighting%20a%20promising%20new%20paradigm%20for%20test-time%20scaling.&entry.1838667208=http%3A//arxiv.org/abs/2506.06303v4&entry.124074799=Read"},
{"title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training", "author": "Ziyun Zhang and Zezhou Wang and Xiaoyi Zhang and Zongyu Guo and Jiahao Li and Bin Li and Yan Lu", "abstract": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.", "link": "http://arxiv.org/abs/2601.04126v1", "date": "2026-01-07", "relevancy": 2.3235, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6567}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5291}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiniteWeb%3A%20Scalable%20Web%20Environment%20Synthesis%20for%20GUI%20Agent%20Training&body=Title%3A%20InfiniteWeb%3A%20Scalable%20Web%20Environment%20Synthesis%20for%20GUI%20Agent%20Training%0AAuthor%3A%20Ziyun%20Zhang%20and%20Zezhou%20Wang%20and%20Xiaoyi%20Zhang%20and%20Zongyu%20Guo%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Yan%20Lu%0AAbstract%3A%20GUI%20agents%20that%20interact%20with%20graphical%20interfaces%20on%20behalf%20of%20users%20represent%20a%20promising%20direction%20for%20practical%20AI%20assistants.%20However%2C%20training%20such%20agents%20is%20hindered%20by%20the%20scarcity%20of%20suitable%20environments.%20We%20present%20InfiniteWeb%2C%20a%20system%20that%20automatically%20generates%20functional%20web%20environments%20at%20scale%20for%20GUI%20agent%20training.%20While%20LLMs%20perform%20well%20on%20generating%20a%20single%20webpage%2C%20building%20a%20realistic%20and%20functional%20website%20with%20many%20interconnected%20pages%20faces%20challenges.%20We%20address%20these%20challenges%20through%20unified%20specification%2C%20task-centric%20test-driven%20development%2C%20and%20a%20combination%20of%20website%20seed%20with%20reference%20design%20image%20to%20ensure%20diversity.%20Our%20system%20also%20generates%20verifiable%20task%20evaluators%20enabling%20dense%20reward%20signals%20for%20reinforcement%20learning.%20Experiments%20show%20that%20InfiniteWeb%20surpasses%20commercial%20coding%20agents%20at%20realistic%20website%20construction%2C%20and%20GUI%20agents%20trained%20on%20our%20generated%20environments%20achieve%20significant%20performance%20improvements%20on%20OSWorld%20and%20Online-Mind2Web%2C%20demonstrating%20the%20effectiveness%20of%20proposed%20system.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiniteWeb%253A%2520Scalable%2520Web%2520Environment%2520Synthesis%2520for%2520GUI%2520Agent%2520Training%26entry.906535625%3DZiyun%2520Zhang%2520and%2520Zezhou%2520Wang%2520and%2520Xiaoyi%2520Zhang%2520and%2520Zongyu%2520Guo%2520and%2520Jiahao%2520Li%2520and%2520Bin%2520Li%2520and%2520Yan%2520Lu%26entry.1292438233%3DGUI%2520agents%2520that%2520interact%2520with%2520graphical%2520interfaces%2520on%2520behalf%2520of%2520users%2520represent%2520a%2520promising%2520direction%2520for%2520practical%2520AI%2520assistants.%2520However%252C%2520training%2520such%2520agents%2520is%2520hindered%2520by%2520the%2520scarcity%2520of%2520suitable%2520environments.%2520We%2520present%2520InfiniteWeb%252C%2520a%2520system%2520that%2520automatically%2520generates%2520functional%2520web%2520environments%2520at%2520scale%2520for%2520GUI%2520agent%2520training.%2520While%2520LLMs%2520perform%2520well%2520on%2520generating%2520a%2520single%2520webpage%252C%2520building%2520a%2520realistic%2520and%2520functional%2520website%2520with%2520many%2520interconnected%2520pages%2520faces%2520challenges.%2520We%2520address%2520these%2520challenges%2520through%2520unified%2520specification%252C%2520task-centric%2520test-driven%2520development%252C%2520and%2520a%2520combination%2520of%2520website%2520seed%2520with%2520reference%2520design%2520image%2520to%2520ensure%2520diversity.%2520Our%2520system%2520also%2520generates%2520verifiable%2520task%2520evaluators%2520enabling%2520dense%2520reward%2520signals%2520for%2520reinforcement%2520learning.%2520Experiments%2520show%2520that%2520InfiniteWeb%2520surpasses%2520commercial%2520coding%2520agents%2520at%2520realistic%2520website%2520construction%252C%2520and%2520GUI%2520agents%2520trained%2520on%2520our%2520generated%2520environments%2520achieve%2520significant%2520performance%2520improvements%2520on%2520OSWorld%2520and%2520Online-Mind2Web%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520proposed%2520system.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiniteWeb%3A%20Scalable%20Web%20Environment%20Synthesis%20for%20GUI%20Agent%20Training&entry.906535625=Ziyun%20Zhang%20and%20Zezhou%20Wang%20and%20Xiaoyi%20Zhang%20and%20Zongyu%20Guo%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Yan%20Lu&entry.1292438233=GUI%20agents%20that%20interact%20with%20graphical%20interfaces%20on%20behalf%20of%20users%20represent%20a%20promising%20direction%20for%20practical%20AI%20assistants.%20However%2C%20training%20such%20agents%20is%20hindered%20by%20the%20scarcity%20of%20suitable%20environments.%20We%20present%20InfiniteWeb%2C%20a%20system%20that%20automatically%20generates%20functional%20web%20environments%20at%20scale%20for%20GUI%20agent%20training.%20While%20LLMs%20perform%20well%20on%20generating%20a%20single%20webpage%2C%20building%20a%20realistic%20and%20functional%20website%20with%20many%20interconnected%20pages%20faces%20challenges.%20We%20address%20these%20challenges%20through%20unified%20specification%2C%20task-centric%20test-driven%20development%2C%20and%20a%20combination%20of%20website%20seed%20with%20reference%20design%20image%20to%20ensure%20diversity.%20Our%20system%20also%20generates%20verifiable%20task%20evaluators%20enabling%20dense%20reward%20signals%20for%20reinforcement%20learning.%20Experiments%20show%20that%20InfiniteWeb%20surpasses%20commercial%20coding%20agents%20at%20realistic%20website%20construction%2C%20and%20GUI%20agents%20trained%20on%20our%20generated%20environments%20achieve%20significant%20performance%20improvements%20on%20OSWorld%20and%20Online-Mind2Web%2C%20demonstrating%20the%20effectiveness%20of%20proposed%20system.&entry.1838667208=http%3A//arxiv.org/abs/2601.04126v1&entry.124074799=Read"},
{"title": "Plasticine: A Traceable Diffusion Model for Medical Image Translation", "author": "Tianyang Zhang and Xinxing Cheng and Jun Cheng and Shaoming Zheng and He Zhao and Huazhu Fu and Alejandro F Frangi and Jiang Liu and Jinming Duan", "abstract": "Domain gaps arising from variations in imaging devices and population distributions pose significant challenges for machine learning in medical image analysis. Existing image-to-image translation methods primarily aim to learn mappings between domains, often generating diverse synthetic data with variations in anatomical scale and shape, but they usually overlook spatial correspondence during the translation process. For clinical applications, traceability, defined as the ability to provide pixel-level correspondences between original and translated images, is equally important. This property enhances clinical interpretability but has been largely overlooked in previous approaches. To address this gap, we propose Plasticine, which is, to the best of our knowledge, the first end-to-end image-to-image translation framework explicitly designed with traceability as a core objective. Our method combines intensity translation and spatial transformation within a denoising diffusion framework. This design enables the generation of synthetic images with interpretable intensity transitions and spatially coherent deformations, supporting pixel-wise traceability throughout the translation process.", "link": "http://arxiv.org/abs/2512.18455v2", "date": "2026-01-07", "relevancy": 2.309, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5804}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5774}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plasticine%3A%20A%20Traceable%20Diffusion%20Model%20for%20Medical%20Image%20Translation&body=Title%3A%20Plasticine%3A%20A%20Traceable%20Diffusion%20Model%20for%20Medical%20Image%20Translation%0AAuthor%3A%20Tianyang%20Zhang%20and%20Xinxing%20Cheng%20and%20Jun%20Cheng%20and%20Shaoming%20Zheng%20and%20He%20Zhao%20and%20Huazhu%20Fu%20and%20Alejandro%20F%20Frangi%20and%20Jiang%20Liu%20and%20Jinming%20Duan%0AAbstract%3A%20Domain%20gaps%20arising%20from%20variations%20in%20imaging%20devices%20and%20population%20distributions%20pose%20significant%20challenges%20for%20machine%20learning%20in%20medical%20image%20analysis.%20Existing%20image-to-image%20translation%20methods%20primarily%20aim%20to%20learn%20mappings%20between%20domains%2C%20often%20generating%20diverse%20synthetic%20data%20with%20variations%20in%20anatomical%20scale%20and%20shape%2C%20but%20they%20usually%20overlook%20spatial%20correspondence%20during%20the%20translation%20process.%20For%20clinical%20applications%2C%20traceability%2C%20defined%20as%20the%20ability%20to%20provide%20pixel-level%20correspondences%20between%20original%20and%20translated%20images%2C%20is%20equally%20important.%20This%20property%20enhances%20clinical%20interpretability%20but%20has%20been%20largely%20overlooked%20in%20previous%20approaches.%20To%20address%20this%20gap%2C%20we%20propose%20Plasticine%2C%20which%20is%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20end-to-end%20image-to-image%20translation%20framework%20explicitly%20designed%20with%20traceability%20as%20a%20core%20objective.%20Our%20method%20combines%20intensity%20translation%20and%20spatial%20transformation%20within%20a%20denoising%20diffusion%20framework.%20This%20design%20enables%20the%20generation%20of%20synthetic%20images%20with%20interpretable%20intensity%20transitions%20and%20spatially%20coherent%20deformations%2C%20supporting%20pixel-wise%20traceability%20throughout%20the%20translation%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18455v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlasticine%253A%2520A%2520Traceable%2520Diffusion%2520Model%2520for%2520Medical%2520Image%2520Translation%26entry.906535625%3DTianyang%2520Zhang%2520and%2520Xinxing%2520Cheng%2520and%2520Jun%2520Cheng%2520and%2520Shaoming%2520Zheng%2520and%2520He%2520Zhao%2520and%2520Huazhu%2520Fu%2520and%2520Alejandro%2520F%2520Frangi%2520and%2520Jiang%2520Liu%2520and%2520Jinming%2520Duan%26entry.1292438233%3DDomain%2520gaps%2520arising%2520from%2520variations%2520in%2520imaging%2520devices%2520and%2520population%2520distributions%2520pose%2520significant%2520challenges%2520for%2520machine%2520learning%2520in%2520medical%2520image%2520analysis.%2520Existing%2520image-to-image%2520translation%2520methods%2520primarily%2520aim%2520to%2520learn%2520mappings%2520between%2520domains%252C%2520often%2520generating%2520diverse%2520synthetic%2520data%2520with%2520variations%2520in%2520anatomical%2520scale%2520and%2520shape%252C%2520but%2520they%2520usually%2520overlook%2520spatial%2520correspondence%2520during%2520the%2520translation%2520process.%2520For%2520clinical%2520applications%252C%2520traceability%252C%2520defined%2520as%2520the%2520ability%2520to%2520provide%2520pixel-level%2520correspondences%2520between%2520original%2520and%2520translated%2520images%252C%2520is%2520equally%2520important.%2520This%2520property%2520enhances%2520clinical%2520interpretability%2520but%2520has%2520been%2520largely%2520overlooked%2520in%2520previous%2520approaches.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520Plasticine%252C%2520which%2520is%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520end-to-end%2520image-to-image%2520translation%2520framework%2520explicitly%2520designed%2520with%2520traceability%2520as%2520a%2520core%2520objective.%2520Our%2520method%2520combines%2520intensity%2520translation%2520and%2520spatial%2520transformation%2520within%2520a%2520denoising%2520diffusion%2520framework.%2520This%2520design%2520enables%2520the%2520generation%2520of%2520synthetic%2520images%2520with%2520interpretable%2520intensity%2520transitions%2520and%2520spatially%2520coherent%2520deformations%252C%2520supporting%2520pixel-wise%2520traceability%2520throughout%2520the%2520translation%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18455v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plasticine%3A%20A%20Traceable%20Diffusion%20Model%20for%20Medical%20Image%20Translation&entry.906535625=Tianyang%20Zhang%20and%20Xinxing%20Cheng%20and%20Jun%20Cheng%20and%20Shaoming%20Zheng%20and%20He%20Zhao%20and%20Huazhu%20Fu%20and%20Alejandro%20F%20Frangi%20and%20Jiang%20Liu%20and%20Jinming%20Duan&entry.1292438233=Domain%20gaps%20arising%20from%20variations%20in%20imaging%20devices%20and%20population%20distributions%20pose%20significant%20challenges%20for%20machine%20learning%20in%20medical%20image%20analysis.%20Existing%20image-to-image%20translation%20methods%20primarily%20aim%20to%20learn%20mappings%20between%20domains%2C%20often%20generating%20diverse%20synthetic%20data%20with%20variations%20in%20anatomical%20scale%20and%20shape%2C%20but%20they%20usually%20overlook%20spatial%20correspondence%20during%20the%20translation%20process.%20For%20clinical%20applications%2C%20traceability%2C%20defined%20as%20the%20ability%20to%20provide%20pixel-level%20correspondences%20between%20original%20and%20translated%20images%2C%20is%20equally%20important.%20This%20property%20enhances%20clinical%20interpretability%20but%20has%20been%20largely%20overlooked%20in%20previous%20approaches.%20To%20address%20this%20gap%2C%20we%20propose%20Plasticine%2C%20which%20is%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20end-to-end%20image-to-image%20translation%20framework%20explicitly%20designed%20with%20traceability%20as%20a%20core%20objective.%20Our%20method%20combines%20intensity%20translation%20and%20spatial%20transformation%20within%20a%20denoising%20diffusion%20framework.%20This%20design%20enables%20the%20generation%20of%20synthetic%20images%20with%20interpretable%20intensity%20transitions%20and%20spatially%20coherent%20deformations%2C%20supporting%20pixel-wise%20traceability%20throughout%20the%20translation%20process.&entry.1838667208=http%3A//arxiv.org/abs/2512.18455v2&entry.124074799=Read"},
{"title": "Exploring Iterative Controllable Summarization with Large Language Models", "author": "Sangwon Ryu and Heejin Do and Daehee Kim and Hwanjo Yu and Dongwoo Kim and Yunsu Kim and Gary Geunbae Lee and Jungseul Ok", "abstract": "Large language models (LLMs) have demonstrated remarkable performance in abstractive summarization tasks. However, their ability to precisely control summary attributes (e.g., length or topic) remains underexplored, limiting their adaptability to specific user preferences. In this paper, we systematically explore the controllability of LLMs. To this end, we revisit summary attribute measurements and introduce iterative evaluation metrics, failure rate and average iteration count to precisely evaluate controllability of LLMs, rather than merely assessing errors. Our findings show that LLMs struggle more with numerical attributes than with linguistic attributes. To address this challenge, we propose a guide-to-explain framework (GTE) for controllable summarization. Our GTE framework enables the model to identify misaligned attributes in the initial draft and guides it in self-explaining errors in the previous output. By allowing the model to reflect on its misalignment, GTE generates well-adjusted summaries that satisfy the desired attributes with robust effectiveness, requiring surprisingly fewer iterations than other iterative approaches.", "link": "http://arxiv.org/abs/2411.12460v3", "date": "2026-01-07", "relevancy": 2.3084, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4692}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Iterative%20Controllable%20Summarization%20with%20Large%20Language%20Models&body=Title%3A%20Exploring%20Iterative%20Controllable%20Summarization%20with%20Large%20Language%20Models%0AAuthor%3A%20Sangwon%20Ryu%20and%20Heejin%20Do%20and%20Daehee%20Kim%20and%20Hwanjo%20Yu%20and%20Dongwoo%20Kim%20and%20Yunsu%20Kim%20and%20Gary%20Geunbae%20Lee%20and%20Jungseul%20Ok%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20in%20abstractive%20summarization%20tasks.%20However%2C%20their%20ability%20to%20precisely%20control%20summary%20attributes%20%28e.g.%2C%20length%20or%20topic%29%20remains%20underexplored%2C%20limiting%20their%20adaptability%20to%20specific%20user%20preferences.%20In%20this%20paper%2C%20we%20systematically%20explore%20the%20controllability%20of%20LLMs.%20To%20this%20end%2C%20we%20revisit%20summary%20attribute%20measurements%20and%20introduce%20iterative%20evaluation%20metrics%2C%20failure%20rate%20and%20average%20iteration%20count%20to%20precisely%20evaluate%20controllability%20of%20LLMs%2C%20rather%20than%20merely%20assessing%20errors.%20Our%20findings%20show%20that%20LLMs%20struggle%20more%20with%20numerical%20attributes%20than%20with%20linguistic%20attributes.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20guide-to-explain%20framework%20%28GTE%29%20for%20controllable%20summarization.%20Our%20GTE%20framework%20enables%20the%20model%20to%20identify%20misaligned%20attributes%20in%20the%20initial%20draft%20and%20guides%20it%20in%20self-explaining%20errors%20in%20the%20previous%20output.%20By%20allowing%20the%20model%20to%20reflect%20on%20its%20misalignment%2C%20GTE%20generates%20well-adjusted%20summaries%20that%20satisfy%20the%20desired%20attributes%20with%20robust%20effectiveness%2C%20requiring%20surprisingly%20fewer%20iterations%20than%20other%20iterative%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2411.12460v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Iterative%2520Controllable%2520Summarization%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DSangwon%2520Ryu%2520and%2520Heejin%2520Do%2520and%2520Daehee%2520Kim%2520and%2520Hwanjo%2520Yu%2520and%2520Dongwoo%2520Kim%2520and%2520Yunsu%2520Kim%2520and%2520Gary%2520Geunbae%2520Lee%2520and%2520Jungseul%2520Ok%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520abstractive%2520summarization%2520tasks.%2520However%252C%2520their%2520ability%2520to%2520precisely%2520control%2520summary%2520attributes%2520%2528e.g.%252C%2520length%2520or%2520topic%2529%2520remains%2520underexplored%252C%2520limiting%2520their%2520adaptability%2520to%2520specific%2520user%2520preferences.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520explore%2520the%2520controllability%2520of%2520LLMs.%2520To%2520this%2520end%252C%2520we%2520revisit%2520summary%2520attribute%2520measurements%2520and%2520introduce%2520iterative%2520evaluation%2520metrics%252C%2520failure%2520rate%2520and%2520average%2520iteration%2520count%2520to%2520precisely%2520evaluate%2520controllability%2520of%2520LLMs%252C%2520rather%2520than%2520merely%2520assessing%2520errors.%2520Our%2520findings%2520show%2520that%2520LLMs%2520struggle%2520more%2520with%2520numerical%2520attributes%2520than%2520with%2520linguistic%2520attributes.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520guide-to-explain%2520framework%2520%2528GTE%2529%2520for%2520controllable%2520summarization.%2520Our%2520GTE%2520framework%2520enables%2520the%2520model%2520to%2520identify%2520misaligned%2520attributes%2520in%2520the%2520initial%2520draft%2520and%2520guides%2520it%2520in%2520self-explaining%2520errors%2520in%2520the%2520previous%2520output.%2520By%2520allowing%2520the%2520model%2520to%2520reflect%2520on%2520its%2520misalignment%252C%2520GTE%2520generates%2520well-adjusted%2520summaries%2520that%2520satisfy%2520the%2520desired%2520attributes%2520with%2520robust%2520effectiveness%252C%2520requiring%2520surprisingly%2520fewer%2520iterations%2520than%2520other%2520iterative%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12460v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Iterative%20Controllable%20Summarization%20with%20Large%20Language%20Models&entry.906535625=Sangwon%20Ryu%20and%20Heejin%20Do%20and%20Daehee%20Kim%20and%20Hwanjo%20Yu%20and%20Dongwoo%20Kim%20and%20Yunsu%20Kim%20and%20Gary%20Geunbae%20Lee%20and%20Jungseul%20Ok&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%20in%20abstractive%20summarization%20tasks.%20However%2C%20their%20ability%20to%20precisely%20control%20summary%20attributes%20%28e.g.%2C%20length%20or%20topic%29%20remains%20underexplored%2C%20limiting%20their%20adaptability%20to%20specific%20user%20preferences.%20In%20this%20paper%2C%20we%20systematically%20explore%20the%20controllability%20of%20LLMs.%20To%20this%20end%2C%20we%20revisit%20summary%20attribute%20measurements%20and%20introduce%20iterative%20evaluation%20metrics%2C%20failure%20rate%20and%20average%20iteration%20count%20to%20precisely%20evaluate%20controllability%20of%20LLMs%2C%20rather%20than%20merely%20assessing%20errors.%20Our%20findings%20show%20that%20LLMs%20struggle%20more%20with%20numerical%20attributes%20than%20with%20linguistic%20attributes.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20guide-to-explain%20framework%20%28GTE%29%20for%20controllable%20summarization.%20Our%20GTE%20framework%20enables%20the%20model%20to%20identify%20misaligned%20attributes%20in%20the%20initial%20draft%20and%20guides%20it%20in%20self-explaining%20errors%20in%20the%20previous%20output.%20By%20allowing%20the%20model%20to%20reflect%20on%20its%20misalignment%2C%20GTE%20generates%20well-adjusted%20summaries%20that%20satisfy%20the%20desired%20attributes%20with%20robust%20effectiveness%2C%20requiring%20surprisingly%20fewer%20iterations%20than%20other%20iterative%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2411.12460v3&entry.124074799=Read"},
{"title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test", "author": "Chun-Kai Fan and Xiaowei Chi and Xiaozhu Ju and Hao Li and Yong Bao and Yu-Kai Wang and Lizhang Chen and Zhiyuan Jiang and Kuangzhi Ge and Ying Li and Weishi Mi and Qingpo Wuwu and Peidong Jia and Yulin Luo and Kevin Zhang and Zhiyuan Qin and Yong Dai and Sirui Han and Yike Guo and Shanghang Zhang and Jian Tang", "abstract": "As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.", "link": "http://arxiv.org/abs/2601.04137v1", "date": "2026-01-07", "relevancy": 2.2981, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5853}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wow%2C%20wo%2C%20val%21%20A%20Comprehensive%20Embodied%20World%20Model%20Evaluation%20Turing%20Test&body=Title%3A%20Wow%2C%20wo%2C%20val%21%20A%20Comprehensive%20Embodied%20World%20Model%20Evaluation%20Turing%20Test%0AAuthor%3A%20Chun-Kai%20Fan%20and%20Xiaowei%20Chi%20and%20Xiaozhu%20Ju%20and%20Hao%20Li%20and%20Yong%20Bao%20and%20Yu-Kai%20Wang%20and%20Lizhang%20Chen%20and%20Zhiyuan%20Jiang%20and%20Kuangzhi%20Ge%20and%20Ying%20Li%20and%20Weishi%20Mi%20and%20Qingpo%20Wuwu%20and%20Peidong%20Jia%20and%20Yulin%20Luo%20and%20Kevin%20Zhang%20and%20Zhiyuan%20Qin%20and%20Yong%20Dai%20and%20Sirui%20Han%20and%20Yike%20Guo%20and%20Shanghang%20Zhang%20and%20Jian%20Tang%0AAbstract%3A%20As%20world%20models%20gain%20momentum%20in%20Embodied%20AI%2C%20an%20increasing%20number%20of%20works%20explore%20using%20video%20foundation%20models%20as%20predictive%20world%20models%20for%20downstream%20embodied%20tasks%20like%203D%20prediction%20or%20interactive%20generation.%20However%2C%20before%20exploring%20these%20downstream%20tasks%2C%20video%20foundation%20models%20still%20have%20two%20critical%20questions%20unanswered%3A%20%281%29%20whether%20their%20generative%20generalization%20is%20sufficient%20to%20maintain%20perceptual%20fidelity%20in%20the%20eyes%20of%20human%20observers%2C%20and%20%282%29%20whether%20they%20are%20robust%20enough%20to%20serve%20as%20a%20universal%20prior%20for%20real-world%20embodied%20agents.%20To%20provide%20a%20standardized%20framework%20for%20answering%20these%20questions%2C%20we%20introduce%20the%20Embodied%20Turing%20Test%20benchmark%3A%20WoW-World-Eval%20%28Wow%2Cwo%2Cval%29.%20Building%20upon%20609%20robot%20manipulation%20data%2C%20Wow-wo-val%20examines%20five%20core%20abilities%2C%20including%20perception%2C%20planning%2C%20prediction%2C%20generalization%2C%20and%20execution.%20We%20propose%20a%20comprehensive%20evaluation%20protocol%20with%2022%20metrics%20to%20assess%20the%20models%27%20generation%20ability%2C%20which%20achieves%20a%20high%20Pearson%20Correlation%20between%20the%20overall%20score%20and%20human%20preference%20%28%3E0.93%29%20and%20establishes%20a%20reliable%20foundation%20for%20the%20Human%20Turing%20Test.%20On%20Wow-wo-val%2C%20models%20achieve%20only%2017.27%20on%20long-horizon%20planning%20and%20at%20best%2068.02%20on%20physical%20consistency%2C%20indicating%20limited%20spatiotemporal%20consistency%20and%20physical%20reasoning.%20For%20the%20Inverse%20Dynamic%20Model%20Turing%20Test%2C%20we%20first%20use%20an%20IDM%20to%20evaluate%20the%20video%20foundation%20models%27%20execution%20accuracy%20in%20the%20real%20world.%20However%2C%20most%20models%20collapse%20to%20%24%5Capprox%24%200%25%20success%2C%20while%20WoW%20maintains%20a%2040.74%25%20success%20rate.%20These%20findings%20point%20to%20a%20noticeable%20gap%20between%20the%20generated%20videos%20and%20the%20real%20world%2C%20highlighting%20the%20urgency%20and%20necessity%20of%20benchmarking%20World%20Model%20in%20Embodied%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWow%252C%2520wo%252C%2520val%2521%2520A%2520Comprehensive%2520Embodied%2520World%2520Model%2520Evaluation%2520Turing%2520Test%26entry.906535625%3DChun-Kai%2520Fan%2520and%2520Xiaowei%2520Chi%2520and%2520Xiaozhu%2520Ju%2520and%2520Hao%2520Li%2520and%2520Yong%2520Bao%2520and%2520Yu-Kai%2520Wang%2520and%2520Lizhang%2520Chen%2520and%2520Zhiyuan%2520Jiang%2520and%2520Kuangzhi%2520Ge%2520and%2520Ying%2520Li%2520and%2520Weishi%2520Mi%2520and%2520Qingpo%2520Wuwu%2520and%2520Peidong%2520Jia%2520and%2520Yulin%2520Luo%2520and%2520Kevin%2520Zhang%2520and%2520Zhiyuan%2520Qin%2520and%2520Yong%2520Dai%2520and%2520Sirui%2520Han%2520and%2520Yike%2520Guo%2520and%2520Shanghang%2520Zhang%2520and%2520Jian%2520Tang%26entry.1292438233%3DAs%2520world%2520models%2520gain%2520momentum%2520in%2520Embodied%2520AI%252C%2520an%2520increasing%2520number%2520of%2520works%2520explore%2520using%2520video%2520foundation%2520models%2520as%2520predictive%2520world%2520models%2520for%2520downstream%2520embodied%2520tasks%2520like%25203D%2520prediction%2520or%2520interactive%2520generation.%2520However%252C%2520before%2520exploring%2520these%2520downstream%2520tasks%252C%2520video%2520foundation%2520models%2520still%2520have%2520two%2520critical%2520questions%2520unanswered%253A%2520%25281%2529%2520whether%2520their%2520generative%2520generalization%2520is%2520sufficient%2520to%2520maintain%2520perceptual%2520fidelity%2520in%2520the%2520eyes%2520of%2520human%2520observers%252C%2520and%2520%25282%2529%2520whether%2520they%2520are%2520robust%2520enough%2520to%2520serve%2520as%2520a%2520universal%2520prior%2520for%2520real-world%2520embodied%2520agents.%2520To%2520provide%2520a%2520standardized%2520framework%2520for%2520answering%2520these%2520questions%252C%2520we%2520introduce%2520the%2520Embodied%2520Turing%2520Test%2520benchmark%253A%2520WoW-World-Eval%2520%2528Wow%252Cwo%252Cval%2529.%2520Building%2520upon%2520609%2520robot%2520manipulation%2520data%252C%2520Wow-wo-val%2520examines%2520five%2520core%2520abilities%252C%2520including%2520perception%252C%2520planning%252C%2520prediction%252C%2520generalization%252C%2520and%2520execution.%2520We%2520propose%2520a%2520comprehensive%2520evaluation%2520protocol%2520with%252022%2520metrics%2520to%2520assess%2520the%2520models%2527%2520generation%2520ability%252C%2520which%2520achieves%2520a%2520high%2520Pearson%2520Correlation%2520between%2520the%2520overall%2520score%2520and%2520human%2520preference%2520%2528%253E0.93%2529%2520and%2520establishes%2520a%2520reliable%2520foundation%2520for%2520the%2520Human%2520Turing%2520Test.%2520On%2520Wow-wo-val%252C%2520models%2520achieve%2520only%252017.27%2520on%2520long-horizon%2520planning%2520and%2520at%2520best%252068.02%2520on%2520physical%2520consistency%252C%2520indicating%2520limited%2520spatiotemporal%2520consistency%2520and%2520physical%2520reasoning.%2520For%2520the%2520Inverse%2520Dynamic%2520Model%2520Turing%2520Test%252C%2520we%2520first%2520use%2520an%2520IDM%2520to%2520evaluate%2520the%2520video%2520foundation%2520models%2527%2520execution%2520accuracy%2520in%2520the%2520real%2520world.%2520However%252C%2520most%2520models%2520collapse%2520to%2520%2524%255Capprox%2524%25200%2525%2520success%252C%2520while%2520WoW%2520maintains%2520a%252040.74%2525%2520success%2520rate.%2520These%2520findings%2520point%2520to%2520a%2520noticeable%2520gap%2520between%2520the%2520generated%2520videos%2520and%2520the%2520real%2520world%252C%2520highlighting%2520the%2520urgency%2520and%2520necessity%2520of%2520benchmarking%2520World%2520Model%2520in%2520Embodied%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wow%2C%20wo%2C%20val%21%20A%20Comprehensive%20Embodied%20World%20Model%20Evaluation%20Turing%20Test&entry.906535625=Chun-Kai%20Fan%20and%20Xiaowei%20Chi%20and%20Xiaozhu%20Ju%20and%20Hao%20Li%20and%20Yong%20Bao%20and%20Yu-Kai%20Wang%20and%20Lizhang%20Chen%20and%20Zhiyuan%20Jiang%20and%20Kuangzhi%20Ge%20and%20Ying%20Li%20and%20Weishi%20Mi%20and%20Qingpo%20Wuwu%20and%20Peidong%20Jia%20and%20Yulin%20Luo%20and%20Kevin%20Zhang%20and%20Zhiyuan%20Qin%20and%20Yong%20Dai%20and%20Sirui%20Han%20and%20Yike%20Guo%20and%20Shanghang%20Zhang%20and%20Jian%20Tang&entry.1292438233=As%20world%20models%20gain%20momentum%20in%20Embodied%20AI%2C%20an%20increasing%20number%20of%20works%20explore%20using%20video%20foundation%20models%20as%20predictive%20world%20models%20for%20downstream%20embodied%20tasks%20like%203D%20prediction%20or%20interactive%20generation.%20However%2C%20before%20exploring%20these%20downstream%20tasks%2C%20video%20foundation%20models%20still%20have%20two%20critical%20questions%20unanswered%3A%20%281%29%20whether%20their%20generative%20generalization%20is%20sufficient%20to%20maintain%20perceptual%20fidelity%20in%20the%20eyes%20of%20human%20observers%2C%20and%20%282%29%20whether%20they%20are%20robust%20enough%20to%20serve%20as%20a%20universal%20prior%20for%20real-world%20embodied%20agents.%20To%20provide%20a%20standardized%20framework%20for%20answering%20these%20questions%2C%20we%20introduce%20the%20Embodied%20Turing%20Test%20benchmark%3A%20WoW-World-Eval%20%28Wow%2Cwo%2Cval%29.%20Building%20upon%20609%20robot%20manipulation%20data%2C%20Wow-wo-val%20examines%20five%20core%20abilities%2C%20including%20perception%2C%20planning%2C%20prediction%2C%20generalization%2C%20and%20execution.%20We%20propose%20a%20comprehensive%20evaluation%20protocol%20with%2022%20metrics%20to%20assess%20the%20models%27%20generation%20ability%2C%20which%20achieves%20a%20high%20Pearson%20Correlation%20between%20the%20overall%20score%20and%20human%20preference%20%28%3E0.93%29%20and%20establishes%20a%20reliable%20foundation%20for%20the%20Human%20Turing%20Test.%20On%20Wow-wo-val%2C%20models%20achieve%20only%2017.27%20on%20long-horizon%20planning%20and%20at%20best%2068.02%20on%20physical%20consistency%2C%20indicating%20limited%20spatiotemporal%20consistency%20and%20physical%20reasoning.%20For%20the%20Inverse%20Dynamic%20Model%20Turing%20Test%2C%20we%20first%20use%20an%20IDM%20to%20evaluate%20the%20video%20foundation%20models%27%20execution%20accuracy%20in%20the%20real%20world.%20However%2C%20most%20models%20collapse%20to%20%24%5Capprox%24%200%25%20success%2C%20while%20WoW%20maintains%20a%2040.74%25%20success%20rate.%20These%20findings%20point%20to%20a%20noticeable%20gap%20between%20the%20generated%20videos%20and%20the%20real%20world%2C%20highlighting%20the%20urgency%20and%20necessity%20of%20benchmarking%20World%20Model%20in%20Embodied%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2601.04137v1&entry.124074799=Read"},
{"title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation", "author": "Xu Zhang and Cheng Da and Huan Yang and Kun Gai and Ming Lu and Zhan Ma", "abstract": "Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.", "link": "http://arxiv.org/abs/2601.03955v1", "date": "2026-01-07", "relevancy": 2.2888, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.623}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5852}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ResTok%3A%20Learning%20Hierarchical%20Residuals%20in%201D%20Visual%20Tokenizers%20for%20Autoregressive%20Image%20Generation&body=Title%3A%20ResTok%3A%20Learning%20Hierarchical%20Residuals%20in%201D%20Visual%20Tokenizers%20for%20Autoregressive%20Image%20Generation%0AAuthor%3A%20Xu%20Zhang%20and%20Cheng%20Da%20and%20Huan%20Yang%20and%20Kun%20Gai%20and%20Ming%20Lu%20and%20Zhan%20Ma%0AAbstract%3A%20Existing%201D%20visual%20tokenizers%20for%20autoregressive%20%28AR%29%20generation%20largely%20follow%20the%20design%20principles%20of%20language%20modeling%2C%20as%20they%20are%20built%20directly%20upon%20transformers%20whose%20priors%20originate%20in%20language%2C%20yielding%20single-hierarchy%20latent%20tokens%20and%20treating%20visual%20data%20as%20flat%20sequential%20token%20streams.%20However%2C%20this%20language-like%20formulation%20overlooks%20key%20properties%20of%20vision%2C%20particularly%20the%20hierarchical%20and%20residual%20network%20designs%20that%20have%20long%20been%20essential%20for%20convergence%20and%20efficiency%20in%20visual%20models.%20To%20bring%20%22vision%22%20back%20to%20vision%2C%20we%20propose%20the%20Residual%20Tokenizer%20%28ResTok%29%2C%20a%201D%20visual%20tokenizer%20that%20builds%20hierarchical%20residuals%20for%20both%20image%20tokens%20and%20latent%20tokens.%20The%20hierarchical%20representations%20obtained%20through%20progressively%20merging%20enable%20cross-level%20feature%20fusion%20at%20each%20layer%2C%20substantially%20enhancing%20representational%20capacity.%20Meanwhile%2C%20the%20semantic%20residuals%20between%20hierarchies%20prevent%20information%20overlap%2C%20yielding%20more%20concentrated%20latent%20distributions%20that%20are%20easier%20for%20AR%20modeling.%20Cross-level%20bindings%20consequently%20emerge%20without%20any%20explicit%20constraints.%20To%20accelerate%20the%20generation%20process%2C%20we%20further%20introduce%20a%20hierarchical%20AR%20generator%20that%20substantially%20reduces%20sampling%20steps%20by%20predicting%20an%20entire%20level%20of%20latent%20tokens%20at%20once%20rather%20than%20generating%20them%20strictly%20token-by-token.%20Extensive%20experiments%20demonstrate%20that%20restoring%20hierarchical%20residual%20priors%20in%20visual%20tokenization%20significantly%20improves%20AR%20image%20generation%2C%20achieving%20a%20gFID%20of%202.34%20on%20ImageNet-256%20with%20only%209%20sampling%20steps.%20Code%20is%20available%20at%20https%3A//github.com/Kwai-Kolors/ResTok.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResTok%253A%2520Learning%2520Hierarchical%2520Residuals%2520in%25201D%2520Visual%2520Tokenizers%2520for%2520Autoregressive%2520Image%2520Generation%26entry.906535625%3DXu%2520Zhang%2520and%2520Cheng%2520Da%2520and%2520Huan%2520Yang%2520and%2520Kun%2520Gai%2520and%2520Ming%2520Lu%2520and%2520Zhan%2520Ma%26entry.1292438233%3DExisting%25201D%2520visual%2520tokenizers%2520for%2520autoregressive%2520%2528AR%2529%2520generation%2520largely%2520follow%2520the%2520design%2520principles%2520of%2520language%2520modeling%252C%2520as%2520they%2520are%2520built%2520directly%2520upon%2520transformers%2520whose%2520priors%2520originate%2520in%2520language%252C%2520yielding%2520single-hierarchy%2520latent%2520tokens%2520and%2520treating%2520visual%2520data%2520as%2520flat%2520sequential%2520token%2520streams.%2520However%252C%2520this%2520language-like%2520formulation%2520overlooks%2520key%2520properties%2520of%2520vision%252C%2520particularly%2520the%2520hierarchical%2520and%2520residual%2520network%2520designs%2520that%2520have%2520long%2520been%2520essential%2520for%2520convergence%2520and%2520efficiency%2520in%2520visual%2520models.%2520To%2520bring%2520%2522vision%2522%2520back%2520to%2520vision%252C%2520we%2520propose%2520the%2520Residual%2520Tokenizer%2520%2528ResTok%2529%252C%2520a%25201D%2520visual%2520tokenizer%2520that%2520builds%2520hierarchical%2520residuals%2520for%2520both%2520image%2520tokens%2520and%2520latent%2520tokens.%2520The%2520hierarchical%2520representations%2520obtained%2520through%2520progressively%2520merging%2520enable%2520cross-level%2520feature%2520fusion%2520at%2520each%2520layer%252C%2520substantially%2520enhancing%2520representational%2520capacity.%2520Meanwhile%252C%2520the%2520semantic%2520residuals%2520between%2520hierarchies%2520prevent%2520information%2520overlap%252C%2520yielding%2520more%2520concentrated%2520latent%2520distributions%2520that%2520are%2520easier%2520for%2520AR%2520modeling.%2520Cross-level%2520bindings%2520consequently%2520emerge%2520without%2520any%2520explicit%2520constraints.%2520To%2520accelerate%2520the%2520generation%2520process%252C%2520we%2520further%2520introduce%2520a%2520hierarchical%2520AR%2520generator%2520that%2520substantially%2520reduces%2520sampling%2520steps%2520by%2520predicting%2520an%2520entire%2520level%2520of%2520latent%2520tokens%2520at%2520once%2520rather%2520than%2520generating%2520them%2520strictly%2520token-by-token.%2520Extensive%2520experiments%2520demonstrate%2520that%2520restoring%2520hierarchical%2520residual%2520priors%2520in%2520visual%2520tokenization%2520significantly%2520improves%2520AR%2520image%2520generation%252C%2520achieving%2520a%2520gFID%2520of%25202.34%2520on%2520ImageNet-256%2520with%2520only%25209%2520sampling%2520steps.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Kwai-Kolors/ResTok.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResTok%3A%20Learning%20Hierarchical%20Residuals%20in%201D%20Visual%20Tokenizers%20for%20Autoregressive%20Image%20Generation&entry.906535625=Xu%20Zhang%20and%20Cheng%20Da%20and%20Huan%20Yang%20and%20Kun%20Gai%20and%20Ming%20Lu%20and%20Zhan%20Ma&entry.1292438233=Existing%201D%20visual%20tokenizers%20for%20autoregressive%20%28AR%29%20generation%20largely%20follow%20the%20design%20principles%20of%20language%20modeling%2C%20as%20they%20are%20built%20directly%20upon%20transformers%20whose%20priors%20originate%20in%20language%2C%20yielding%20single-hierarchy%20latent%20tokens%20and%20treating%20visual%20data%20as%20flat%20sequential%20token%20streams.%20However%2C%20this%20language-like%20formulation%20overlooks%20key%20properties%20of%20vision%2C%20particularly%20the%20hierarchical%20and%20residual%20network%20designs%20that%20have%20long%20been%20essential%20for%20convergence%20and%20efficiency%20in%20visual%20models.%20To%20bring%20%22vision%22%20back%20to%20vision%2C%20we%20propose%20the%20Residual%20Tokenizer%20%28ResTok%29%2C%20a%201D%20visual%20tokenizer%20that%20builds%20hierarchical%20residuals%20for%20both%20image%20tokens%20and%20latent%20tokens.%20The%20hierarchical%20representations%20obtained%20through%20progressively%20merging%20enable%20cross-level%20feature%20fusion%20at%20each%20layer%2C%20substantially%20enhancing%20representational%20capacity.%20Meanwhile%2C%20the%20semantic%20residuals%20between%20hierarchies%20prevent%20information%20overlap%2C%20yielding%20more%20concentrated%20latent%20distributions%20that%20are%20easier%20for%20AR%20modeling.%20Cross-level%20bindings%20consequently%20emerge%20without%20any%20explicit%20constraints.%20To%20accelerate%20the%20generation%20process%2C%20we%20further%20introduce%20a%20hierarchical%20AR%20generator%20that%20substantially%20reduces%20sampling%20steps%20by%20predicting%20an%20entire%20level%20of%20latent%20tokens%20at%20once%20rather%20than%20generating%20them%20strictly%20token-by-token.%20Extensive%20experiments%20demonstrate%20that%20restoring%20hierarchical%20residual%20priors%20in%20visual%20tokenization%20significantly%20improves%20AR%20image%20generation%2C%20achieving%20a%20gFID%20of%202.34%20on%20ImageNet-256%20with%20only%209%20sampling%20steps.%20Code%20is%20available%20at%20https%3A//github.com/Kwai-Kolors/ResTok.&entry.1838667208=http%3A//arxiv.org/abs/2601.03955v1&entry.124074799=Read"},
{"title": "Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images", "author": "Leandro Stival and Ricardo da Silva Torres and Helio Pedrini", "abstract": "Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on", "link": "http://arxiv.org/abs/2601.04127v1", "date": "2026-01-07", "relevancy": 2.2784, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5751}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel-Wise%20Multimodal%20Contrastive%20Learning%20for%20Remote%20Sensing%20Images&body=Title%3A%20Pixel-Wise%20Multimodal%20Contrastive%20Learning%20for%20Remote%20Sensing%20Images%0AAuthor%3A%20Leandro%20Stival%20and%20Ricardo%20da%20Silva%20Torres%20and%20Helio%20Pedrini%0AAbstract%3A%20Satellites%20continuously%20generate%20massive%20volumes%20of%20data%2C%20particularly%20for%20Earth%20observation%2C%20including%20satellite%20image%20time%20series%20%28SITS%29.%20However%2C%20most%20deep%20learning%20models%20are%20designed%20to%20process%20either%20entire%20images%20or%20complete%20time%20series%20sequences%20to%20extract%20meaningful%20features%20for%20downstream%20tasks.%20In%20this%20study%2C%20we%20propose%20a%20novel%20multimodal%20approach%20that%20leverages%20pixel-wise%20two-dimensional%20%282D%29%20representations%20to%20encode%20visual%20property%20variations%20from%20SITS%20more%20effectively.%20Specifically%2C%20we%20generate%20recurrence%20plots%20from%20pixel-based%20vegetation%20index%20time%20series%20%28NDVI%2C%20EVI%2C%20and%20SAVI%29%20as%20an%20alternative%20to%20using%20raw%20pixel%20values%2C%20creating%20more%20informative%20representations.%20Additionally%2C%20we%20introduce%20PIxel-wise%20Multimodal%20Contrastive%20%28PIMC%29%2C%20a%20new%20multimodal%20self-supervision%20approach%20that%20produces%20effective%20encoders%20based%20on%20two-dimensional%20pixel%20time%20series%20representations%20and%20remote%20sensing%20imagery%20%28RSI%29.%20To%20validate%20our%20approach%2C%20we%20assess%20its%20performance%20on%20three%20downstream%20tasks%3A%20pixel-level%20forecasting%20and%20classification%20using%20the%20PASTIS%20dataset%2C%20and%20land%20cover%20classification%20on%20the%20EuroSAT%20dataset.%20Moreover%2C%20we%20compare%20our%20results%20to%20state-of-the-art%20%28SOTA%29%20methods%20on%20all%20downstream%20tasks.%20Our%20experimental%20results%20show%20that%20the%20use%20of%202D%20representations%20significantly%20enhances%20feature%20extraction%20from%20SITS%2C%20while%20contrastive%20learning%20improves%20the%20quality%20of%20representations%20for%20both%20pixel%20time%20series%20and%20RSI.%20These%20findings%20suggest%20that%20our%20multimodal%20method%20outperforms%20existing%20models%20in%20various%20Earth%20observation%20tasks%2C%20establishing%20it%20as%20a%20robust%20self-supervision%20framework%20for%20processing%20both%20SITS%20and%20RSI.%20Code%20avaliable%20on%0ALink%3A%20http%3A//arxiv.org/abs/2601.04127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel-Wise%2520Multimodal%2520Contrastive%2520Learning%2520for%2520Remote%2520Sensing%2520Images%26entry.906535625%3DLeandro%2520Stival%2520and%2520Ricardo%2520da%2520Silva%2520Torres%2520and%2520Helio%2520Pedrini%26entry.1292438233%3DSatellites%2520continuously%2520generate%2520massive%2520volumes%2520of%2520data%252C%2520particularly%2520for%2520Earth%2520observation%252C%2520including%2520satellite%2520image%2520time%2520series%2520%2528SITS%2529.%2520However%252C%2520most%2520deep%2520learning%2520models%2520are%2520designed%2520to%2520process%2520either%2520entire%2520images%2520or%2520complete%2520time%2520series%2520sequences%2520to%2520extract%2520meaningful%2520features%2520for%2520downstream%2520tasks.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520multimodal%2520approach%2520that%2520leverages%2520pixel-wise%2520two-dimensional%2520%25282D%2529%2520representations%2520to%2520encode%2520visual%2520property%2520variations%2520from%2520SITS%2520more%2520effectively.%2520Specifically%252C%2520we%2520generate%2520recurrence%2520plots%2520from%2520pixel-based%2520vegetation%2520index%2520time%2520series%2520%2528NDVI%252C%2520EVI%252C%2520and%2520SAVI%2529%2520as%2520an%2520alternative%2520to%2520using%2520raw%2520pixel%2520values%252C%2520creating%2520more%2520informative%2520representations.%2520Additionally%252C%2520we%2520introduce%2520PIxel-wise%2520Multimodal%2520Contrastive%2520%2528PIMC%2529%252C%2520a%2520new%2520multimodal%2520self-supervision%2520approach%2520that%2520produces%2520effective%2520encoders%2520based%2520on%2520two-dimensional%2520pixel%2520time%2520series%2520representations%2520and%2520remote%2520sensing%2520imagery%2520%2528RSI%2529.%2520To%2520validate%2520our%2520approach%252C%2520we%2520assess%2520its%2520performance%2520on%2520three%2520downstream%2520tasks%253A%2520pixel-level%2520forecasting%2520and%2520classification%2520using%2520the%2520PASTIS%2520dataset%252C%2520and%2520land%2520cover%2520classification%2520on%2520the%2520EuroSAT%2520dataset.%2520Moreover%252C%2520we%2520compare%2520our%2520results%2520to%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520on%2520all%2520downstream%2520tasks.%2520Our%2520experimental%2520results%2520show%2520that%2520the%2520use%2520of%25202D%2520representations%2520significantly%2520enhances%2520feature%2520extraction%2520from%2520SITS%252C%2520while%2520contrastive%2520learning%2520improves%2520the%2520quality%2520of%2520representations%2520for%2520both%2520pixel%2520time%2520series%2520and%2520RSI.%2520These%2520findings%2520suggest%2520that%2520our%2520multimodal%2520method%2520outperforms%2520existing%2520models%2520in%2520various%2520Earth%2520observation%2520tasks%252C%2520establishing%2520it%2520as%2520a%2520robust%2520self-supervision%2520framework%2520for%2520processing%2520both%2520SITS%2520and%2520RSI.%2520Code%2520avaliable%2520on%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Wise%20Multimodal%20Contrastive%20Learning%20for%20Remote%20Sensing%20Images&entry.906535625=Leandro%20Stival%20and%20Ricardo%20da%20Silva%20Torres%20and%20Helio%20Pedrini&entry.1292438233=Satellites%20continuously%20generate%20massive%20volumes%20of%20data%2C%20particularly%20for%20Earth%20observation%2C%20including%20satellite%20image%20time%20series%20%28SITS%29.%20However%2C%20most%20deep%20learning%20models%20are%20designed%20to%20process%20either%20entire%20images%20or%20complete%20time%20series%20sequences%20to%20extract%20meaningful%20features%20for%20downstream%20tasks.%20In%20this%20study%2C%20we%20propose%20a%20novel%20multimodal%20approach%20that%20leverages%20pixel-wise%20two-dimensional%20%282D%29%20representations%20to%20encode%20visual%20property%20variations%20from%20SITS%20more%20effectively.%20Specifically%2C%20we%20generate%20recurrence%20plots%20from%20pixel-based%20vegetation%20index%20time%20series%20%28NDVI%2C%20EVI%2C%20and%20SAVI%29%20as%20an%20alternative%20to%20using%20raw%20pixel%20values%2C%20creating%20more%20informative%20representations.%20Additionally%2C%20we%20introduce%20PIxel-wise%20Multimodal%20Contrastive%20%28PIMC%29%2C%20a%20new%20multimodal%20self-supervision%20approach%20that%20produces%20effective%20encoders%20based%20on%20two-dimensional%20pixel%20time%20series%20representations%20and%20remote%20sensing%20imagery%20%28RSI%29.%20To%20validate%20our%20approach%2C%20we%20assess%20its%20performance%20on%20three%20downstream%20tasks%3A%20pixel-level%20forecasting%20and%20classification%20using%20the%20PASTIS%20dataset%2C%20and%20land%20cover%20classification%20on%20the%20EuroSAT%20dataset.%20Moreover%2C%20we%20compare%20our%20results%20to%20state-of-the-art%20%28SOTA%29%20methods%20on%20all%20downstream%20tasks.%20Our%20experimental%20results%20show%20that%20the%20use%20of%202D%20representations%20significantly%20enhances%20feature%20extraction%20from%20SITS%2C%20while%20contrastive%20learning%20improves%20the%20quality%20of%20representations%20for%20both%20pixel%20time%20series%20and%20RSI.%20These%20findings%20suggest%20that%20our%20multimodal%20method%20outperforms%20existing%20models%20in%20various%20Earth%20observation%20tasks%2C%20establishing%20it%20as%20a%20robust%20self-supervision%20framework%20for%20processing%20both%20SITS%20and%20RSI.%20Code%20avaliable%20on&entry.1838667208=http%3A//arxiv.org/abs/2601.04127v1&entry.124074799=Read"},
{"title": "An Anytime Algorithm for Good Arm Identification", "author": "Marc Jourdan and Andr\u00e9e Delahaye-Duriez and Cl\u00e9mence R\u00e9da", "abstract": "In good arm identification (GAI), the goal is to identify one arm whose average performance exceeds a given threshold, referred to as a good arm, if it exists. Few works have studied GAI in the fixed-budget setting when the sampling budget is fixed beforehand, or in the anytime setting, when a recommendation can be asked at any time. We propose APGAI, an anytime and parameter-free sampling rule for GAI in stochastic bandits. APGAI can be straightforwardly used in fixed-confidence and fixed-budget settings. First, we derive upper bounds on its probability of error at any time. They show that adaptive strategies can be more efficient in detecting the absence of good arms than uniform sampling in several diverse instances. Second, when APGAI is combined with a stopping rule, we prove upper bounds on the expected sampling complexity, holding at any confidence level. Finally, we show the good empirical performance of APGAI on synthetic and real-world data. Our work offers an extensive overview of the GAI problem in all settings.", "link": "http://arxiv.org/abs/2310.10359v3", "date": "2026-01-07", "relevancy": 2.2724, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4876}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4398}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Anytime%20Algorithm%20for%20Good%20Arm%20Identification&body=Title%3A%20An%20Anytime%20Algorithm%20for%20Good%20Arm%20Identification%0AAuthor%3A%20Marc%20Jourdan%20and%20Andr%C3%A9e%20Delahaye-Duriez%20and%20Cl%C3%A9mence%20R%C3%A9da%0AAbstract%3A%20In%20good%20arm%20identification%20%28GAI%29%2C%20the%20goal%20is%20to%20identify%20one%20arm%20whose%20average%20performance%20exceeds%20a%20given%20threshold%2C%20referred%20to%20as%20a%20good%20arm%2C%20if%20it%20exists.%20Few%20works%20have%20studied%20GAI%20in%20the%20fixed-budget%20setting%20when%20the%20sampling%20budget%20is%20fixed%20beforehand%2C%20or%20in%20the%20anytime%20setting%2C%20when%20a%20recommendation%20can%20be%20asked%20at%20any%20time.%20We%20propose%20APGAI%2C%20an%20anytime%20and%20parameter-free%20sampling%20rule%20for%20GAI%20in%20stochastic%20bandits.%20APGAI%20can%20be%20straightforwardly%20used%20in%20fixed-confidence%20and%20fixed-budget%20settings.%20First%2C%20we%20derive%20upper%20bounds%20on%20its%20probability%20of%20error%20at%20any%20time.%20They%20show%20that%20adaptive%20strategies%20can%20be%20more%20efficient%20in%20detecting%20the%20absence%20of%20good%20arms%20than%20uniform%20sampling%20in%20several%20diverse%20instances.%20Second%2C%20when%20APGAI%20is%20combined%20with%20a%20stopping%20rule%2C%20we%20prove%20upper%20bounds%20on%20the%20expected%20sampling%20complexity%2C%20holding%20at%20any%20confidence%20level.%20Finally%2C%20we%20show%20the%20good%20empirical%20performance%20of%20APGAI%20on%20synthetic%20and%20real-world%20data.%20Our%20work%20offers%20an%20extensive%20overview%20of%20the%20GAI%20problem%20in%20all%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2310.10359v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Anytime%2520Algorithm%2520for%2520Good%2520Arm%2520Identification%26entry.906535625%3DMarc%2520Jourdan%2520and%2520Andr%25C3%25A9e%2520Delahaye-Duriez%2520and%2520Cl%25C3%25A9mence%2520R%25C3%25A9da%26entry.1292438233%3DIn%2520good%2520arm%2520identification%2520%2528GAI%2529%252C%2520the%2520goal%2520is%2520to%2520identify%2520one%2520arm%2520whose%2520average%2520performance%2520exceeds%2520a%2520given%2520threshold%252C%2520referred%2520to%2520as%2520a%2520good%2520arm%252C%2520if%2520it%2520exists.%2520Few%2520works%2520have%2520studied%2520GAI%2520in%2520the%2520fixed-budget%2520setting%2520when%2520the%2520sampling%2520budget%2520is%2520fixed%2520beforehand%252C%2520or%2520in%2520the%2520anytime%2520setting%252C%2520when%2520a%2520recommendation%2520can%2520be%2520asked%2520at%2520any%2520time.%2520We%2520propose%2520APGAI%252C%2520an%2520anytime%2520and%2520parameter-free%2520sampling%2520rule%2520for%2520GAI%2520in%2520stochastic%2520bandits.%2520APGAI%2520can%2520be%2520straightforwardly%2520used%2520in%2520fixed-confidence%2520and%2520fixed-budget%2520settings.%2520First%252C%2520we%2520derive%2520upper%2520bounds%2520on%2520its%2520probability%2520of%2520error%2520at%2520any%2520time.%2520They%2520show%2520that%2520adaptive%2520strategies%2520can%2520be%2520more%2520efficient%2520in%2520detecting%2520the%2520absence%2520of%2520good%2520arms%2520than%2520uniform%2520sampling%2520in%2520several%2520diverse%2520instances.%2520Second%252C%2520when%2520APGAI%2520is%2520combined%2520with%2520a%2520stopping%2520rule%252C%2520we%2520prove%2520upper%2520bounds%2520on%2520the%2520expected%2520sampling%2520complexity%252C%2520holding%2520at%2520any%2520confidence%2520level.%2520Finally%252C%2520we%2520show%2520the%2520good%2520empirical%2520performance%2520of%2520APGAI%2520on%2520synthetic%2520and%2520real-world%2520data.%2520Our%2520work%2520offers%2520an%2520extensive%2520overview%2520of%2520the%2520GAI%2520problem%2520in%2520all%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10359v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Anytime%20Algorithm%20for%20Good%20Arm%20Identification&entry.906535625=Marc%20Jourdan%20and%20Andr%C3%A9e%20Delahaye-Duriez%20and%20Cl%C3%A9mence%20R%C3%A9da&entry.1292438233=In%20good%20arm%20identification%20%28GAI%29%2C%20the%20goal%20is%20to%20identify%20one%20arm%20whose%20average%20performance%20exceeds%20a%20given%20threshold%2C%20referred%20to%20as%20a%20good%20arm%2C%20if%20it%20exists.%20Few%20works%20have%20studied%20GAI%20in%20the%20fixed-budget%20setting%20when%20the%20sampling%20budget%20is%20fixed%20beforehand%2C%20or%20in%20the%20anytime%20setting%2C%20when%20a%20recommendation%20can%20be%20asked%20at%20any%20time.%20We%20propose%20APGAI%2C%20an%20anytime%20and%20parameter-free%20sampling%20rule%20for%20GAI%20in%20stochastic%20bandits.%20APGAI%20can%20be%20straightforwardly%20used%20in%20fixed-confidence%20and%20fixed-budget%20settings.%20First%2C%20we%20derive%20upper%20bounds%20on%20its%20probability%20of%20error%20at%20any%20time.%20They%20show%20that%20adaptive%20strategies%20can%20be%20more%20efficient%20in%20detecting%20the%20absence%20of%20good%20arms%20than%20uniform%20sampling%20in%20several%20diverse%20instances.%20Second%2C%20when%20APGAI%20is%20combined%20with%20a%20stopping%20rule%2C%20we%20prove%20upper%20bounds%20on%20the%20expected%20sampling%20complexity%2C%20holding%20at%20any%20confidence%20level.%20Finally%2C%20we%20show%20the%20good%20empirical%20performance%20of%20APGAI%20on%20synthetic%20and%20real-world%20data.%20Our%20work%20offers%20an%20extensive%20overview%20of%20the%20GAI%20problem%20in%20all%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2310.10359v3&entry.124074799=Read"},
{"title": "EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks", "author": "Michael Arbel and David Salinas and Frank Hutter", "abstract": "Recent foundational models for tabular data, such as TabPFN, excel at adapting to new tasks via in-context learning, but remain constrained to a fixed, pre-defined number of target dimensions-often necessitating costly ensembling strategies. We trace this constraint to a deeper architectural shortcoming: these models lack target equivariance, so that permuting target dimension orderings alters their predictions. This deficiency gives rise to an irreducible \"equivariance gap\", an error term that introduces instability in predictions. We eliminate this gap by designing a fully target-equivariant architecture-ensuring permutation invariance via equivariant encoders, decoders, and a bi-attention mechanism. Empirical evaluation on standard classification benchmarks shows that, on datasets with more classes than those seen during pre-training, our model matches or surpasses existing methods while incurring lower computational overhead.", "link": "http://arxiv.org/abs/2502.06684v4", "date": "2026-01-07", "relevancy": 2.2721, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4841}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4407}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EquiTabPFN%3A%20A%20Target-Permutation%20Equivariant%20Prior%20Fitted%20Networks&body=Title%3A%20EquiTabPFN%3A%20A%20Target-Permutation%20Equivariant%20Prior%20Fitted%20Networks%0AAuthor%3A%20Michael%20Arbel%20and%20David%20Salinas%20and%20Frank%20Hutter%0AAbstract%3A%20Recent%20foundational%20models%20for%20tabular%20data%2C%20such%20as%20TabPFN%2C%20excel%20at%20adapting%20to%20new%20tasks%20via%20in-context%20learning%2C%20but%20remain%20constrained%20to%20a%20fixed%2C%20pre-defined%20number%20of%20target%20dimensions-often%20necessitating%20costly%20ensembling%20strategies.%20We%20trace%20this%20constraint%20to%20a%20deeper%20architectural%20shortcoming%3A%20these%20models%20lack%20target%20equivariance%2C%20so%20that%20permuting%20target%20dimension%20orderings%20alters%20their%20predictions.%20This%20deficiency%20gives%20rise%20to%20an%20irreducible%20%22equivariance%20gap%22%2C%20an%20error%20term%20that%20introduces%20instability%20in%20predictions.%20We%20eliminate%20this%20gap%20by%20designing%20a%20fully%20target-equivariant%20architecture-ensuring%20permutation%20invariance%20via%20equivariant%20encoders%2C%20decoders%2C%20and%20a%20bi-attention%20mechanism.%20Empirical%20evaluation%20on%20standard%20classification%20benchmarks%20shows%20that%2C%20on%20datasets%20with%20more%20classes%20than%20those%20seen%20during%20pre-training%2C%20our%20model%20matches%20or%20surpasses%20existing%20methods%20while%20incurring%20lower%20computational%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2502.06684v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquiTabPFN%253A%2520A%2520Target-Permutation%2520Equivariant%2520Prior%2520Fitted%2520Networks%26entry.906535625%3DMichael%2520Arbel%2520and%2520David%2520Salinas%2520and%2520Frank%2520Hutter%26entry.1292438233%3DRecent%2520foundational%2520models%2520for%2520tabular%2520data%252C%2520such%2520as%2520TabPFN%252C%2520excel%2520at%2520adapting%2520to%2520new%2520tasks%2520via%2520in-context%2520learning%252C%2520but%2520remain%2520constrained%2520to%2520a%2520fixed%252C%2520pre-defined%2520number%2520of%2520target%2520dimensions-often%2520necessitating%2520costly%2520ensembling%2520strategies.%2520We%2520trace%2520this%2520constraint%2520to%2520a%2520deeper%2520architectural%2520shortcoming%253A%2520these%2520models%2520lack%2520target%2520equivariance%252C%2520so%2520that%2520permuting%2520target%2520dimension%2520orderings%2520alters%2520their%2520predictions.%2520This%2520deficiency%2520gives%2520rise%2520to%2520an%2520irreducible%2520%2522equivariance%2520gap%2522%252C%2520an%2520error%2520term%2520that%2520introduces%2520instability%2520in%2520predictions.%2520We%2520eliminate%2520this%2520gap%2520by%2520designing%2520a%2520fully%2520target-equivariant%2520architecture-ensuring%2520permutation%2520invariance%2520via%2520equivariant%2520encoders%252C%2520decoders%252C%2520and%2520a%2520bi-attention%2520mechanism.%2520Empirical%2520evaluation%2520on%2520standard%2520classification%2520benchmarks%2520shows%2520that%252C%2520on%2520datasets%2520with%2520more%2520classes%2520than%2520those%2520seen%2520during%2520pre-training%252C%2520our%2520model%2520matches%2520or%2520surpasses%2520existing%2520methods%2520while%2520incurring%2520lower%2520computational%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06684v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EquiTabPFN%3A%20A%20Target-Permutation%20Equivariant%20Prior%20Fitted%20Networks&entry.906535625=Michael%20Arbel%20and%20David%20Salinas%20and%20Frank%20Hutter&entry.1292438233=Recent%20foundational%20models%20for%20tabular%20data%2C%20such%20as%20TabPFN%2C%20excel%20at%20adapting%20to%20new%20tasks%20via%20in-context%20learning%2C%20but%20remain%20constrained%20to%20a%20fixed%2C%20pre-defined%20number%20of%20target%20dimensions-often%20necessitating%20costly%20ensembling%20strategies.%20We%20trace%20this%20constraint%20to%20a%20deeper%20architectural%20shortcoming%3A%20these%20models%20lack%20target%20equivariance%2C%20so%20that%20permuting%20target%20dimension%20orderings%20alters%20their%20predictions.%20This%20deficiency%20gives%20rise%20to%20an%20irreducible%20%22equivariance%20gap%22%2C%20an%20error%20term%20that%20introduces%20instability%20in%20predictions.%20We%20eliminate%20this%20gap%20by%20designing%20a%20fully%20target-equivariant%20architecture-ensuring%20permutation%20invariance%20via%20equivariant%20encoders%2C%20decoders%2C%20and%20a%20bi-attention%20mechanism.%20Empirical%20evaluation%20on%20standard%20classification%20benchmarks%20shows%20that%2C%20on%20datasets%20with%20more%20classes%20than%20those%20seen%20during%20pre-training%2C%20our%20model%20matches%20or%20surpasses%20existing%20methods%20while%20incurring%20lower%20computational%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2502.06684v4&entry.124074799=Read"},
{"title": "MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction", "author": "Xiaokun Sun and Zezhong Wu and Zewen Ding and Linli Xu", "abstract": "Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.", "link": "http://arxiv.org/abs/2601.03781v1", "date": "2026-01-07", "relevancy": 2.2711, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5724}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.57}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVP%3A%20Enhancing%20Video%20Large%20Language%20Models%20via%20Self-supervised%20Masked%20Video%20Prediction&body=Title%3A%20MVP%3A%20Enhancing%20Video%20Large%20Language%20Models%20via%20Self-supervised%20Masked%20Video%20Prediction%0AAuthor%3A%20Xiaokun%20Sun%20and%20Zezhong%20Wu%20and%20Zewen%20Ding%20and%20Linli%20Xu%0AAbstract%3A%20Reinforcement%20learning%20based%20post-training%20paradigms%20for%20Video%20Large%20Language%20Models%20%28VideoLLMs%29%20have%20achieved%20significant%20success%20by%20optimizing%20for%20visual-semantic%20tasks%20such%20as%20captioning%20or%20VideoQA.%20However%2C%20while%20these%20approaches%20effectively%20enhance%20perception%20abilities%2C%20they%20primarily%20target%20holistic%20content%20understanding%2C%20often%20lacking%20explicit%20supervision%20for%20intrinsic%20temporal%20coherence%20and%20inter-frame%20correlations.%20This%20tendency%20limits%20the%20models%27%20ability%20to%20capture%20intricate%20dynamics%20and%20fine-grained%20visual%20causality.%20To%20explicitly%20bridge%20this%20gap%2C%20we%20propose%20a%20novel%20post-training%20objective%3A%20Masked%20Video%20Prediction%20%28MVP%29.%20By%20requiring%20the%20model%20to%20reconstruct%20a%20masked%20continuous%20segment%20from%20a%20set%20of%20challenging%20distractors%2C%20MVP%20forces%20the%20model%20to%20attend%20to%20the%20sequential%20logic%20and%20temporal%20context%20of%20events.%20To%20support%20scalable%20training%2C%20we%20introduce%20a%20scalable%20data%20synthesis%20pipeline%20capable%20of%20transforming%20arbitrary%20video%20corpora%20into%20MVP%20training%20samples%2C%20and%20further%20employ%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20with%20a%20fine-grained%20reward%20function%20to%20enhance%20the%20model%27s%20understanding%20of%20video%20context%20and%20temporal%20properties.%20Comprehensive%20evaluations%20demonstrate%20that%20MVP%20enhances%20video%20reasoning%20capabilities%20by%20directly%20reinforcing%20temporal%20reasoning%20and%20causal%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVP%253A%2520Enhancing%2520Video%2520Large%2520Language%2520Models%2520via%2520Self-supervised%2520Masked%2520Video%2520Prediction%26entry.906535625%3DXiaokun%2520Sun%2520and%2520Zezhong%2520Wu%2520and%2520Zewen%2520Ding%2520and%2520Linli%2520Xu%26entry.1292438233%3DReinforcement%2520learning%2520based%2520post-training%2520paradigms%2520for%2520Video%2520Large%2520Language%2520Models%2520%2528VideoLLMs%2529%2520have%2520achieved%2520significant%2520success%2520by%2520optimizing%2520for%2520visual-semantic%2520tasks%2520such%2520as%2520captioning%2520or%2520VideoQA.%2520However%252C%2520while%2520these%2520approaches%2520effectively%2520enhance%2520perception%2520abilities%252C%2520they%2520primarily%2520target%2520holistic%2520content%2520understanding%252C%2520often%2520lacking%2520explicit%2520supervision%2520for%2520intrinsic%2520temporal%2520coherence%2520and%2520inter-frame%2520correlations.%2520This%2520tendency%2520limits%2520the%2520models%2527%2520ability%2520to%2520capture%2520intricate%2520dynamics%2520and%2520fine-grained%2520visual%2520causality.%2520To%2520explicitly%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520post-training%2520objective%253A%2520Masked%2520Video%2520Prediction%2520%2528MVP%2529.%2520By%2520requiring%2520the%2520model%2520to%2520reconstruct%2520a%2520masked%2520continuous%2520segment%2520from%2520a%2520set%2520of%2520challenging%2520distractors%252C%2520MVP%2520forces%2520the%2520model%2520to%2520attend%2520to%2520the%2520sequential%2520logic%2520and%2520temporal%2520context%2520of%2520events.%2520To%2520support%2520scalable%2520training%252C%2520we%2520introduce%2520a%2520scalable%2520data%2520synthesis%2520pipeline%2520capable%2520of%2520transforming%2520arbitrary%2520video%2520corpora%2520into%2520MVP%2520training%2520samples%252C%2520and%2520further%2520employ%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520with%2520a%2520fine-grained%2520reward%2520function%2520to%2520enhance%2520the%2520model%2527s%2520understanding%2520of%2520video%2520context%2520and%2520temporal%2520properties.%2520Comprehensive%2520evaluations%2520demonstrate%2520that%2520MVP%2520enhances%2520video%2520reasoning%2520capabilities%2520by%2520directly%2520reinforcing%2520temporal%2520reasoning%2520and%2520causal%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVP%3A%20Enhancing%20Video%20Large%20Language%20Models%20via%20Self-supervised%20Masked%20Video%20Prediction&entry.906535625=Xiaokun%20Sun%20and%20Zezhong%20Wu%20and%20Zewen%20Ding%20and%20Linli%20Xu&entry.1292438233=Reinforcement%20learning%20based%20post-training%20paradigms%20for%20Video%20Large%20Language%20Models%20%28VideoLLMs%29%20have%20achieved%20significant%20success%20by%20optimizing%20for%20visual-semantic%20tasks%20such%20as%20captioning%20or%20VideoQA.%20However%2C%20while%20these%20approaches%20effectively%20enhance%20perception%20abilities%2C%20they%20primarily%20target%20holistic%20content%20understanding%2C%20often%20lacking%20explicit%20supervision%20for%20intrinsic%20temporal%20coherence%20and%20inter-frame%20correlations.%20This%20tendency%20limits%20the%20models%27%20ability%20to%20capture%20intricate%20dynamics%20and%20fine-grained%20visual%20causality.%20To%20explicitly%20bridge%20this%20gap%2C%20we%20propose%20a%20novel%20post-training%20objective%3A%20Masked%20Video%20Prediction%20%28MVP%29.%20By%20requiring%20the%20model%20to%20reconstruct%20a%20masked%20continuous%20segment%20from%20a%20set%20of%20challenging%20distractors%2C%20MVP%20forces%20the%20model%20to%20attend%20to%20the%20sequential%20logic%20and%20temporal%20context%20of%20events.%20To%20support%20scalable%20training%2C%20we%20introduce%20a%20scalable%20data%20synthesis%20pipeline%20capable%20of%20transforming%20arbitrary%20video%20corpora%20into%20MVP%20training%20samples%2C%20and%20further%20employ%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20with%20a%20fine-grained%20reward%20function%20to%20enhance%20the%20model%27s%20understanding%20of%20video%20context%20and%20temporal%20properties.%20Comprehensive%20evaluations%20demonstrate%20that%20MVP%20enhances%20video%20reasoning%20capabilities%20by%20directly%20reinforcing%20temporal%20reasoning%20and%20causal%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2601.03781v1&entry.124074799=Read"},
{"title": "Using Legacy Polysomnography Data to Train a Radar System to Quantify Sleep in Older Adults and People living with Dementia", "author": "M. Yin and K. G. Ravindran and C. Hadjipanayi and A. Bannon and A. Rapeaux and C. Della Monica and T. S. Lande and Derk-Jan Dijk and T. G. Constandinou", "abstract": "Objective: Ultra-wideband radar technology offers a promising solution for unobtrusive and cost-effective in-home sleep monitoring. However, the limited availability of radar sleep data poses challenges in building robust models that generalize across diverse cohorts and environments. This study proposes a novel deep transfer learning framework to enhance sleep stage classification using radar data. Methods: An end-to-end neural network was developed to classify sleep stages based on nocturnal respiratory and motion signals. The network was trained using a combination of large-scale polysomnography (PSG) datasets and radar data. A domain adaptation approach employing adversarial learning was utilized to bridge the knowledge gap between PSG and radar signals. Validation was performed on a radar dataset of 47 older adults (mean age: 71.2), including 18 participants with prodromal or mild Alzheimer disease. Results: The proposed network structure achieves an accuracy of 79.5% with a Kappa value of 0.65 when classifying wakefulness, rapid eye movement, light sleep and deep sleep. Experimental results confirm that our deep transfer learning approach significantly enhances automatic sleep staging performance in the target domain. Conclusion: This method effectively addresses challenges associated with data variability and limited sample size, substantially improving the reliability of automatic sleep staging models, especially in contexts where radar data is limited. Significance: The findings underscore the viability of UWB radar as a nonintrusive, forward-looking sleep assessment tool that could significantly benefit care for older people and people with neurodegenerative disorders.", "link": "http://arxiv.org/abs/2601.04057v1", "date": "2026-01-07", "relevancy": 2.2705, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4662}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4549}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Legacy%20Polysomnography%20Data%20to%20Train%20a%20Radar%20System%20to%20Quantify%20Sleep%20in%20Older%20Adults%20and%20People%20living%20with%20Dementia&body=Title%3A%20Using%20Legacy%20Polysomnography%20Data%20to%20Train%20a%20Radar%20System%20to%20Quantify%20Sleep%20in%20Older%20Adults%20and%20People%20living%20with%20Dementia%0AAuthor%3A%20M.%20Yin%20and%20K.%20G.%20Ravindran%20and%20C.%20Hadjipanayi%20and%20A.%20Bannon%20and%20A.%20Rapeaux%20and%20C.%20Della%20Monica%20and%20T.%20S.%20Lande%20and%20Derk-Jan%20Dijk%20and%20T.%20G.%20Constandinou%0AAbstract%3A%20Objective%3A%20Ultra-wideband%20radar%20technology%20offers%20a%20promising%20solution%20for%20unobtrusive%20and%20cost-effective%20in-home%20sleep%20monitoring.%20However%2C%20the%20limited%20availability%20of%20radar%20sleep%20data%20poses%20challenges%20in%20building%20robust%20models%20that%20generalize%20across%20diverse%20cohorts%20and%20environments.%20This%20study%20proposes%20a%20novel%20deep%20transfer%20learning%20framework%20to%20enhance%20sleep%20stage%20classification%20using%20radar%20data.%20Methods%3A%20An%20end-to-end%20neural%20network%20was%20developed%20to%20classify%20sleep%20stages%20based%20on%20nocturnal%20respiratory%20and%20motion%20signals.%20The%20network%20was%20trained%20using%20a%20combination%20of%20large-scale%20polysomnography%20%28PSG%29%20datasets%20and%20radar%20data.%20A%20domain%20adaptation%20approach%20employing%20adversarial%20learning%20was%20utilized%20to%20bridge%20the%20knowledge%20gap%20between%20PSG%20and%20radar%20signals.%20Validation%20was%20performed%20on%20a%20radar%20dataset%20of%2047%20older%20adults%20%28mean%20age%3A%2071.2%29%2C%20including%2018%20participants%20with%20prodromal%20or%20mild%20Alzheimer%20disease.%20Results%3A%20The%20proposed%20network%20structure%20achieves%20an%20accuracy%20of%2079.5%25%20with%20a%20Kappa%20value%20of%200.65%20when%20classifying%20wakefulness%2C%20rapid%20eye%20movement%2C%20light%20sleep%20and%20deep%20sleep.%20Experimental%20results%20confirm%20that%20our%20deep%20transfer%20learning%20approach%20significantly%20enhances%20automatic%20sleep%20staging%20performance%20in%20the%20target%20domain.%20Conclusion%3A%20This%20method%20effectively%20addresses%20challenges%20associated%20with%20data%20variability%20and%20limited%20sample%20size%2C%20substantially%20improving%20the%20reliability%20of%20automatic%20sleep%20staging%20models%2C%20especially%20in%20contexts%20where%20radar%20data%20is%20limited.%20Significance%3A%20The%20findings%20underscore%20the%20viability%20of%20UWB%20radar%20as%20a%20nonintrusive%2C%20forward-looking%20sleep%20assessment%20tool%20that%20could%20significantly%20benefit%20care%20for%20older%20people%20and%20people%20with%20neurodegenerative%20disorders.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Legacy%2520Polysomnography%2520Data%2520to%2520Train%2520a%2520Radar%2520System%2520to%2520Quantify%2520Sleep%2520in%2520Older%2520Adults%2520and%2520People%2520living%2520with%2520Dementia%26entry.906535625%3DM.%2520Yin%2520and%2520K.%2520G.%2520Ravindran%2520and%2520C.%2520Hadjipanayi%2520and%2520A.%2520Bannon%2520and%2520A.%2520Rapeaux%2520and%2520C.%2520Della%2520Monica%2520and%2520T.%2520S.%2520Lande%2520and%2520Derk-Jan%2520Dijk%2520and%2520T.%2520G.%2520Constandinou%26entry.1292438233%3DObjective%253A%2520Ultra-wideband%2520radar%2520technology%2520offers%2520a%2520promising%2520solution%2520for%2520unobtrusive%2520and%2520cost-effective%2520in-home%2520sleep%2520monitoring.%2520However%252C%2520the%2520limited%2520availability%2520of%2520radar%2520sleep%2520data%2520poses%2520challenges%2520in%2520building%2520robust%2520models%2520that%2520generalize%2520across%2520diverse%2520cohorts%2520and%2520environments.%2520This%2520study%2520proposes%2520a%2520novel%2520deep%2520transfer%2520learning%2520framework%2520to%2520enhance%2520sleep%2520stage%2520classification%2520using%2520radar%2520data.%2520Methods%253A%2520An%2520end-to-end%2520neural%2520network%2520was%2520developed%2520to%2520classify%2520sleep%2520stages%2520based%2520on%2520nocturnal%2520respiratory%2520and%2520motion%2520signals.%2520The%2520network%2520was%2520trained%2520using%2520a%2520combination%2520of%2520large-scale%2520polysomnography%2520%2528PSG%2529%2520datasets%2520and%2520radar%2520data.%2520A%2520domain%2520adaptation%2520approach%2520employing%2520adversarial%2520learning%2520was%2520utilized%2520to%2520bridge%2520the%2520knowledge%2520gap%2520between%2520PSG%2520and%2520radar%2520signals.%2520Validation%2520was%2520performed%2520on%2520a%2520radar%2520dataset%2520of%252047%2520older%2520adults%2520%2528mean%2520age%253A%252071.2%2529%252C%2520including%252018%2520participants%2520with%2520prodromal%2520or%2520mild%2520Alzheimer%2520disease.%2520Results%253A%2520The%2520proposed%2520network%2520structure%2520achieves%2520an%2520accuracy%2520of%252079.5%2525%2520with%2520a%2520Kappa%2520value%2520of%25200.65%2520when%2520classifying%2520wakefulness%252C%2520rapid%2520eye%2520movement%252C%2520light%2520sleep%2520and%2520deep%2520sleep.%2520Experimental%2520results%2520confirm%2520that%2520our%2520deep%2520transfer%2520learning%2520approach%2520significantly%2520enhances%2520automatic%2520sleep%2520staging%2520performance%2520in%2520the%2520target%2520domain.%2520Conclusion%253A%2520This%2520method%2520effectively%2520addresses%2520challenges%2520associated%2520with%2520data%2520variability%2520and%2520limited%2520sample%2520size%252C%2520substantially%2520improving%2520the%2520reliability%2520of%2520automatic%2520sleep%2520staging%2520models%252C%2520especially%2520in%2520contexts%2520where%2520radar%2520data%2520is%2520limited.%2520Significance%253A%2520The%2520findings%2520underscore%2520the%2520viability%2520of%2520UWB%2520radar%2520as%2520a%2520nonintrusive%252C%2520forward-looking%2520sleep%2520assessment%2520tool%2520that%2520could%2520significantly%2520benefit%2520care%2520for%2520older%2520people%2520and%2520people%2520with%2520neurodegenerative%2520disorders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Legacy%20Polysomnography%20Data%20to%20Train%20a%20Radar%20System%20to%20Quantify%20Sleep%20in%20Older%20Adults%20and%20People%20living%20with%20Dementia&entry.906535625=M.%20Yin%20and%20K.%20G.%20Ravindran%20and%20C.%20Hadjipanayi%20and%20A.%20Bannon%20and%20A.%20Rapeaux%20and%20C.%20Della%20Monica%20and%20T.%20S.%20Lande%20and%20Derk-Jan%20Dijk%20and%20T.%20G.%20Constandinou&entry.1292438233=Objective%3A%20Ultra-wideband%20radar%20technology%20offers%20a%20promising%20solution%20for%20unobtrusive%20and%20cost-effective%20in-home%20sleep%20monitoring.%20However%2C%20the%20limited%20availability%20of%20radar%20sleep%20data%20poses%20challenges%20in%20building%20robust%20models%20that%20generalize%20across%20diverse%20cohorts%20and%20environments.%20This%20study%20proposes%20a%20novel%20deep%20transfer%20learning%20framework%20to%20enhance%20sleep%20stage%20classification%20using%20radar%20data.%20Methods%3A%20An%20end-to-end%20neural%20network%20was%20developed%20to%20classify%20sleep%20stages%20based%20on%20nocturnal%20respiratory%20and%20motion%20signals.%20The%20network%20was%20trained%20using%20a%20combination%20of%20large-scale%20polysomnography%20%28PSG%29%20datasets%20and%20radar%20data.%20A%20domain%20adaptation%20approach%20employing%20adversarial%20learning%20was%20utilized%20to%20bridge%20the%20knowledge%20gap%20between%20PSG%20and%20radar%20signals.%20Validation%20was%20performed%20on%20a%20radar%20dataset%20of%2047%20older%20adults%20%28mean%20age%3A%2071.2%29%2C%20including%2018%20participants%20with%20prodromal%20or%20mild%20Alzheimer%20disease.%20Results%3A%20The%20proposed%20network%20structure%20achieves%20an%20accuracy%20of%2079.5%25%20with%20a%20Kappa%20value%20of%200.65%20when%20classifying%20wakefulness%2C%20rapid%20eye%20movement%2C%20light%20sleep%20and%20deep%20sleep.%20Experimental%20results%20confirm%20that%20our%20deep%20transfer%20learning%20approach%20significantly%20enhances%20automatic%20sleep%20staging%20performance%20in%20the%20target%20domain.%20Conclusion%3A%20This%20method%20effectively%20addresses%20challenges%20associated%20with%20data%20variability%20and%20limited%20sample%20size%2C%20substantially%20improving%20the%20reliability%20of%20automatic%20sleep%20staging%20models%2C%20especially%20in%20contexts%20where%20radar%20data%20is%20limited.%20Significance%3A%20The%20findings%20underscore%20the%20viability%20of%20UWB%20radar%20as%20a%20nonintrusive%2C%20forward-looking%20sleep%20assessment%20tool%20that%20could%20significantly%20benefit%20care%20for%20older%20people%20and%20people%20with%20neurodegenerative%20disorders.&entry.1838667208=http%3A//arxiv.org/abs/2601.04057v1&entry.124074799=Read"},
{"title": "Stable Language Guidance for Vision-Language-Action Models", "author": "Zhihao Zhan and Yuhao Chen and Jiaying Zhou and Qinhan Lv and Hao Liu and Keze Wang and Liang Lin and Guangrun Wang", "abstract": "Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \\textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \\textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \\textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.", "link": "http://arxiv.org/abs/2601.04052v1", "date": "2026-01-07", "relevancy": 2.2702, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Language%20Guidance%20for%20Vision-Language-Action%20Models&body=Title%3A%20Stable%20Language%20Guidance%20for%20Vision-Language-Action%20Models%0AAuthor%3A%20Zhihao%20Zhan%20and%20Yuhao%20Chen%20and%20Jiaying%20Zhou%20and%20Qinhan%20Lv%20and%20Hao%20Liu%20and%20Keze%20Wang%20and%20Liang%20Lin%20and%20Guangrun%20Wang%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20impressive%20capabilities%20in%20generalized%20robotic%20control%3B%20however%2C%20they%20remain%20notoriously%20brittle%20to%20linguistic%20perturbations.%20We%20identify%20a%20critical%20%60%60modality%20collapse%27%27%20phenomenon%20where%20strong%20visual%20priors%20overwhelm%20sparse%20linguistic%20signals%2C%20causing%20agents%20to%20overfit%20to%20specific%20instruction%20phrasings%20while%20ignoring%20the%20underlying%20semantic%20intent.%20To%20address%20this%2C%20we%20propose%20%5Ctextbf%7BResidual%20Semantic%20Steering%20%28RSS%29%7D%2C%20a%20probabilistic%20framework%20that%20disentangles%20physical%20affordance%20from%20semantic%20execution.%20RSS%20introduces%20two%20theoretical%20innovations%3A%20%281%29%20%5Ctextbf%7BMonte%20Carlo%20Syntactic%20Integration%7D%2C%20which%20approximates%20the%20true%20semantic%20posterior%20via%20dense%2C%20LLM-driven%20distributional%20expansion%2C%20and%20%282%29%20%5Ctextbf%7BResidual%20Affordance%20Steering%7D%2C%20a%20dual-stream%20decoding%20mechanism%20that%20explicitly%20isolates%20the%20causal%20influence%20of%20language%20by%20subtracting%20the%20visual%20affordance%20prior.%20Theoretical%20analysis%20suggests%20that%20RSS%20effectively%20maximizes%20the%20mutual%20information%20between%20action%20and%20intent%20while%20suppressing%20visual%20distractors.%20Empirical%20results%20across%20diverse%20manipulation%20benchmarks%20demonstrate%20that%20RSS%20achieves%20state-of-the-art%20robustness%2C%20maintaining%20performance%20even%20under%20adversarial%20linguistic%20perturbations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Language%2520Guidance%2520for%2520Vision-Language-Action%2520Models%26entry.906535625%3DZhihao%2520Zhan%2520and%2520Yuhao%2520Chen%2520and%2520Jiaying%2520Zhou%2520and%2520Qinhan%2520Lv%2520and%2520Hao%2520Liu%2520and%2520Keze%2520Wang%2520and%2520Liang%2520Lin%2520and%2520Guangrun%2520Wang%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%2520generalized%2520robotic%2520control%253B%2520however%252C%2520they%2520remain%2520notoriously%2520brittle%2520to%2520linguistic%2520perturbations.%2520We%2520identify%2520a%2520critical%2520%2560%2560modality%2520collapse%2527%2527%2520phenomenon%2520where%2520strong%2520visual%2520priors%2520overwhelm%2520sparse%2520linguistic%2520signals%252C%2520causing%2520agents%2520to%2520overfit%2520to%2520specific%2520instruction%2520phrasings%2520while%2520ignoring%2520the%2520underlying%2520semantic%2520intent.%2520To%2520address%2520this%252C%2520we%2520propose%2520%255Ctextbf%257BResidual%2520Semantic%2520Steering%2520%2528RSS%2529%257D%252C%2520a%2520probabilistic%2520framework%2520that%2520disentangles%2520physical%2520affordance%2520from%2520semantic%2520execution.%2520RSS%2520introduces%2520two%2520theoretical%2520innovations%253A%2520%25281%2529%2520%255Ctextbf%257BMonte%2520Carlo%2520Syntactic%2520Integration%257D%252C%2520which%2520approximates%2520the%2520true%2520semantic%2520posterior%2520via%2520dense%252C%2520LLM-driven%2520distributional%2520expansion%252C%2520and%2520%25282%2529%2520%255Ctextbf%257BResidual%2520Affordance%2520Steering%257D%252C%2520a%2520dual-stream%2520decoding%2520mechanism%2520that%2520explicitly%2520isolates%2520the%2520causal%2520influence%2520of%2520language%2520by%2520subtracting%2520the%2520visual%2520affordance%2520prior.%2520Theoretical%2520analysis%2520suggests%2520that%2520RSS%2520effectively%2520maximizes%2520the%2520mutual%2520information%2520between%2520action%2520and%2520intent%2520while%2520suppressing%2520visual%2520distractors.%2520Empirical%2520results%2520across%2520diverse%2520manipulation%2520benchmarks%2520demonstrate%2520that%2520RSS%2520achieves%2520state-of-the-art%2520robustness%252C%2520maintaining%2520performance%2520even%2520under%2520adversarial%2520linguistic%2520perturbations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Language%20Guidance%20for%20Vision-Language-Action%20Models&entry.906535625=Zhihao%20Zhan%20and%20Yuhao%20Chen%20and%20Jiaying%20Zhou%20and%20Qinhan%20Lv%20and%20Hao%20Liu%20and%20Keze%20Wang%20and%20Liang%20Lin%20and%20Guangrun%20Wang&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20impressive%20capabilities%20in%20generalized%20robotic%20control%3B%20however%2C%20they%20remain%20notoriously%20brittle%20to%20linguistic%20perturbations.%20We%20identify%20a%20critical%20%60%60modality%20collapse%27%27%20phenomenon%20where%20strong%20visual%20priors%20overwhelm%20sparse%20linguistic%20signals%2C%20causing%20agents%20to%20overfit%20to%20specific%20instruction%20phrasings%20while%20ignoring%20the%20underlying%20semantic%20intent.%20To%20address%20this%2C%20we%20propose%20%5Ctextbf%7BResidual%20Semantic%20Steering%20%28RSS%29%7D%2C%20a%20probabilistic%20framework%20that%20disentangles%20physical%20affordance%20from%20semantic%20execution.%20RSS%20introduces%20two%20theoretical%20innovations%3A%20%281%29%20%5Ctextbf%7BMonte%20Carlo%20Syntactic%20Integration%7D%2C%20which%20approximates%20the%20true%20semantic%20posterior%20via%20dense%2C%20LLM-driven%20distributional%20expansion%2C%20and%20%282%29%20%5Ctextbf%7BResidual%20Affordance%20Steering%7D%2C%20a%20dual-stream%20decoding%20mechanism%20that%20explicitly%20isolates%20the%20causal%20influence%20of%20language%20by%20subtracting%20the%20visual%20affordance%20prior.%20Theoretical%20analysis%20suggests%20that%20RSS%20effectively%20maximizes%20the%20mutual%20information%20between%20action%20and%20intent%20while%20suppressing%20visual%20distractors.%20Empirical%20results%20across%20diverse%20manipulation%20benchmarks%20demonstrate%20that%20RSS%20achieves%20state-of-the-art%20robustness%2C%20maintaining%20performance%20even%20under%20adversarial%20linguistic%20perturbations.&entry.1838667208=http%3A//arxiv.org/abs/2601.04052v1&entry.124074799=Read"},
{"title": "Do LLMs Really Memorize Personally Identifiable Information? Revisiting PII Leakage with a Cue-Controlled Memorization Framework", "author": "Xiaoyu Luo and Yiyi Chen and Qiongxiu Li and Johannes Bjerva", "abstract": "Large Language Models (LLMs) have been reported to \"leak\" Personally Identifiable Information (PII), with successful PII reconstruction often interpreted as evidence of memorization. We propose a principled revision of memorization evaluation for LLMs, arguing that PII leakage should be evaluated under low lexical cue conditions, where target PII cannot be reconstructed through prompt-induced generalization or pattern completion. We formalize Cue-Resistant Memorization (CRM) as a cue-controlled evaluation framework and a necessary condition for valid memorization evaluation, explicitly conditioning on prompt-target overlap cues. Using CRM, we conduct a large-scale multilingual re-evaluation of PII leakage across 32 languages and multiple memorization paradigms. Revisiting reconstruction-based settings, including verbatim prefix-suffix completion and associative reconstruction, we find that their apparent effectiveness is driven primarily by direct surface-form cues rather than by true memorization. When such cues are controlled for, reconstruction success diminishes substantially. We further examine cue-free generation and membership inference, both of which exhibit extremely low true positive rates. Overall, our results suggest that previously reported PII leakage is better explained by cue-driven behavior than by genuine memorization, highlighting the importance of cue-controlled evaluation for reliably quantifying privacy-relevant memorization in LLMs.", "link": "http://arxiv.org/abs/2601.03791v1", "date": "2026-01-07", "relevancy": 2.2673, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Really%20Memorize%20Personally%20Identifiable%20Information%3F%20Revisiting%20PII%20Leakage%20with%20a%20Cue-Controlled%20Memorization%20Framework&body=Title%3A%20Do%20LLMs%20Really%20Memorize%20Personally%20Identifiable%20Information%3F%20Revisiting%20PII%20Leakage%20with%20a%20Cue-Controlled%20Memorization%20Framework%0AAuthor%3A%20Xiaoyu%20Luo%20and%20Yiyi%20Chen%20and%20Qiongxiu%20Li%20and%20Johannes%20Bjerva%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20reported%20to%20%22leak%22%20Personally%20Identifiable%20Information%20%28PII%29%2C%20with%20successful%20PII%20reconstruction%20often%20interpreted%20as%20evidence%20of%20memorization.%20We%20propose%20a%20principled%20revision%20of%20memorization%20evaluation%20for%20LLMs%2C%20arguing%20that%20PII%20leakage%20should%20be%20evaluated%20under%20low%20lexical%20cue%20conditions%2C%20where%20target%20PII%20cannot%20be%20reconstructed%20through%20prompt-induced%20generalization%20or%20pattern%20completion.%20We%20formalize%20Cue-Resistant%20Memorization%20%28CRM%29%20as%20a%20cue-controlled%20evaluation%20framework%20and%20a%20necessary%20condition%20for%20valid%20memorization%20evaluation%2C%20explicitly%20conditioning%20on%20prompt-target%20overlap%20cues.%20Using%20CRM%2C%20we%20conduct%20a%20large-scale%20multilingual%20re-evaluation%20of%20PII%20leakage%20across%2032%20languages%20and%20multiple%20memorization%20paradigms.%20Revisiting%20reconstruction-based%20settings%2C%20including%20verbatim%20prefix-suffix%20completion%20and%20associative%20reconstruction%2C%20we%20find%20that%20their%20apparent%20effectiveness%20is%20driven%20primarily%20by%20direct%20surface-form%20cues%20rather%20than%20by%20true%20memorization.%20When%20such%20cues%20are%20controlled%20for%2C%20reconstruction%20success%20diminishes%20substantially.%20We%20further%20examine%20cue-free%20generation%20and%20membership%20inference%2C%20both%20of%20which%20exhibit%20extremely%20low%20true%20positive%20rates.%20Overall%2C%20our%20results%20suggest%20that%20previously%20reported%20PII%20leakage%20is%20better%20explained%20by%20cue-driven%20behavior%20than%20by%20genuine%20memorization%2C%20highlighting%20the%20importance%20of%20cue-controlled%20evaluation%20for%20reliably%20quantifying%20privacy-relevant%20memorization%20in%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Really%2520Memorize%2520Personally%2520Identifiable%2520Information%253F%2520Revisiting%2520PII%2520Leakage%2520with%2520a%2520Cue-Controlled%2520Memorization%2520Framework%26entry.906535625%3DXiaoyu%2520Luo%2520and%2520Yiyi%2520Chen%2520and%2520Qiongxiu%2520Li%2520and%2520Johannes%2520Bjerva%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520reported%2520to%2520%2522leak%2522%2520Personally%2520Identifiable%2520Information%2520%2528PII%2529%252C%2520with%2520successful%2520PII%2520reconstruction%2520often%2520interpreted%2520as%2520evidence%2520of%2520memorization.%2520We%2520propose%2520a%2520principled%2520revision%2520of%2520memorization%2520evaluation%2520for%2520LLMs%252C%2520arguing%2520that%2520PII%2520leakage%2520should%2520be%2520evaluated%2520under%2520low%2520lexical%2520cue%2520conditions%252C%2520where%2520target%2520PII%2520cannot%2520be%2520reconstructed%2520through%2520prompt-induced%2520generalization%2520or%2520pattern%2520completion.%2520We%2520formalize%2520Cue-Resistant%2520Memorization%2520%2528CRM%2529%2520as%2520a%2520cue-controlled%2520evaluation%2520framework%2520and%2520a%2520necessary%2520condition%2520for%2520valid%2520memorization%2520evaluation%252C%2520explicitly%2520conditioning%2520on%2520prompt-target%2520overlap%2520cues.%2520Using%2520CRM%252C%2520we%2520conduct%2520a%2520large-scale%2520multilingual%2520re-evaluation%2520of%2520PII%2520leakage%2520across%252032%2520languages%2520and%2520multiple%2520memorization%2520paradigms.%2520Revisiting%2520reconstruction-based%2520settings%252C%2520including%2520verbatim%2520prefix-suffix%2520completion%2520and%2520associative%2520reconstruction%252C%2520we%2520find%2520that%2520their%2520apparent%2520effectiveness%2520is%2520driven%2520primarily%2520by%2520direct%2520surface-form%2520cues%2520rather%2520than%2520by%2520true%2520memorization.%2520When%2520such%2520cues%2520are%2520controlled%2520for%252C%2520reconstruction%2520success%2520diminishes%2520substantially.%2520We%2520further%2520examine%2520cue-free%2520generation%2520and%2520membership%2520inference%252C%2520both%2520of%2520which%2520exhibit%2520extremely%2520low%2520true%2520positive%2520rates.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520previously%2520reported%2520PII%2520leakage%2520is%2520better%2520explained%2520by%2520cue-driven%2520behavior%2520than%2520by%2520genuine%2520memorization%252C%2520highlighting%2520the%2520importance%2520of%2520cue-controlled%2520evaluation%2520for%2520reliably%2520quantifying%2520privacy-relevant%2520memorization%2520in%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Really%20Memorize%20Personally%20Identifiable%20Information%3F%20Revisiting%20PII%20Leakage%20with%20a%20Cue-Controlled%20Memorization%20Framework&entry.906535625=Xiaoyu%20Luo%20and%20Yiyi%20Chen%20and%20Qiongxiu%20Li%20and%20Johannes%20Bjerva&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20been%20reported%20to%20%22leak%22%20Personally%20Identifiable%20Information%20%28PII%29%2C%20with%20successful%20PII%20reconstruction%20often%20interpreted%20as%20evidence%20of%20memorization.%20We%20propose%20a%20principled%20revision%20of%20memorization%20evaluation%20for%20LLMs%2C%20arguing%20that%20PII%20leakage%20should%20be%20evaluated%20under%20low%20lexical%20cue%20conditions%2C%20where%20target%20PII%20cannot%20be%20reconstructed%20through%20prompt-induced%20generalization%20or%20pattern%20completion.%20We%20formalize%20Cue-Resistant%20Memorization%20%28CRM%29%20as%20a%20cue-controlled%20evaluation%20framework%20and%20a%20necessary%20condition%20for%20valid%20memorization%20evaluation%2C%20explicitly%20conditioning%20on%20prompt-target%20overlap%20cues.%20Using%20CRM%2C%20we%20conduct%20a%20large-scale%20multilingual%20re-evaluation%20of%20PII%20leakage%20across%2032%20languages%20and%20multiple%20memorization%20paradigms.%20Revisiting%20reconstruction-based%20settings%2C%20including%20verbatim%20prefix-suffix%20completion%20and%20associative%20reconstruction%2C%20we%20find%20that%20their%20apparent%20effectiveness%20is%20driven%20primarily%20by%20direct%20surface-form%20cues%20rather%20than%20by%20true%20memorization.%20When%20such%20cues%20are%20controlled%20for%2C%20reconstruction%20success%20diminishes%20substantially.%20We%20further%20examine%20cue-free%20generation%20and%20membership%20inference%2C%20both%20of%20which%20exhibit%20extremely%20low%20true%20positive%20rates.%20Overall%2C%20our%20results%20suggest%20that%20previously%20reported%20PII%20leakage%20is%20better%20explained%20by%20cue-driven%20behavior%20than%20by%20genuine%20memorization%2C%20highlighting%20the%20importance%20of%20cue-controlled%20evaluation%20for%20reliably%20quantifying%20privacy-relevant%20memorization%20in%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.03791v1&entry.124074799=Read"},
{"title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation", "author": "Gen Li and Peiyu Liu", "abstract": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.", "link": "http://arxiv.org/abs/2601.01513v2", "date": "2026-01-07", "relevancy": 2.2579, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5781}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5602}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastV-RAG%3A%20Towards%20Fast%20and%20Fine-Grained%20Video%20QA%20with%20Retrieval-Augmented%20Generation&body=Title%3A%20FastV-RAG%3A%20Towards%20Fast%20and%20Fine-Grained%20Video%20QA%20with%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Gen%20Li%20and%20Peiyu%20Liu%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20visual%20reasoning%20but%20still%20struggle%20with%20integrating%20external%20knowledge.%20Retrieval-Augmented%20Generation%20%28RAG%29%20is%20a%20promising%20solution%2C%20but%20current%20methods%20remain%20inefficient%20and%20often%20fail%20to%20maintain%20high%20answer%20quality.%20To%20address%20these%20challenges%2C%20we%20propose%20VideoSpeculateRAG%2C%20an%20efficient%20VLM-based%20RAG%20framework%20built%20on%20two%20key%20ideas.%20First%2C%20we%20introduce%20a%20speculative%20decoding%20pipeline%3A%20a%20lightweight%20draft%20model%20quickly%20generates%20multiple%20answer%20candidates%2C%20which%20are%20then%20verified%20and%20refined%20by%20a%20more%20accurate%20heavyweight%20model%2C%20substantially%20reducing%20inference%20latency%20without%20sacrificing%20correctness.%20Second%2C%20we%20identify%20a%20major%20source%20of%20error%20-%20incorrect%20entity%20recognition%20in%20retrieved%20knowledge%20-%20and%20mitigate%20it%20with%20a%20simple%20yet%20effective%20similarity-based%20filtering%20strategy%20that%20improves%20entity%20alignment%20and%20boosts%20overall%20answer%20accuracy.%20Experiments%20demonstrate%20that%20VideoSpeculateRAG%20achieves%20comparable%20or%20higher%20accuracy%20than%20standard%20RAG%20approaches%20while%20accelerating%20inference%20by%20approximately%202x.%20Our%20framework%20highlights%20the%20potential%20of%20combining%20speculative%20decoding%20with%20retrieval-augmented%20reasoning%20to%20enhance%20efficiency%20and%20reliability%20in%20complex%2C%20knowledge-intensive%20multimodal%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.01513v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastV-RAG%253A%2520Towards%2520Fast%2520and%2520Fine-Grained%2520Video%2520QA%2520with%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DGen%2520Li%2520and%2520Peiyu%2520Liu%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520visual%2520reasoning%2520but%2520still%2520struggle%2520with%2520integrating%2520external%2520knowledge.%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520is%2520a%2520promising%2520solution%252C%2520but%2520current%2520methods%2520remain%2520inefficient%2520and%2520often%2520fail%2520to%2520maintain%2520high%2520answer%2520quality.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520VideoSpeculateRAG%252C%2520an%2520efficient%2520VLM-based%2520RAG%2520framework%2520built%2520on%2520two%2520key%2520ideas.%2520First%252C%2520we%2520introduce%2520a%2520speculative%2520decoding%2520pipeline%253A%2520a%2520lightweight%2520draft%2520model%2520quickly%2520generates%2520multiple%2520answer%2520candidates%252C%2520which%2520are%2520then%2520verified%2520and%2520refined%2520by%2520a%2520more%2520accurate%2520heavyweight%2520model%252C%2520substantially%2520reducing%2520inference%2520latency%2520without%2520sacrificing%2520correctness.%2520Second%252C%2520we%2520identify%2520a%2520major%2520source%2520of%2520error%2520-%2520incorrect%2520entity%2520recognition%2520in%2520retrieved%2520knowledge%2520-%2520and%2520mitigate%2520it%2520with%2520a%2520simple%2520yet%2520effective%2520similarity-based%2520filtering%2520strategy%2520that%2520improves%2520entity%2520alignment%2520and%2520boosts%2520overall%2520answer%2520accuracy.%2520Experiments%2520demonstrate%2520that%2520VideoSpeculateRAG%2520achieves%2520comparable%2520or%2520higher%2520accuracy%2520than%2520standard%2520RAG%2520approaches%2520while%2520accelerating%2520inference%2520by%2520approximately%25202x.%2520Our%2520framework%2520highlights%2520the%2520potential%2520of%2520combining%2520speculative%2520decoding%2520with%2520retrieval-augmented%2520reasoning%2520to%2520enhance%2520efficiency%2520and%2520reliability%2520in%2520complex%252C%2520knowledge-intensive%2520multimodal%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.01513v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastV-RAG%3A%20Towards%20Fast%20and%20Fine-Grained%20Video%20QA%20with%20Retrieval-Augmented%20Generation&entry.906535625=Gen%20Li%20and%20Peiyu%20Liu&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20excel%20at%20visual%20reasoning%20but%20still%20struggle%20with%20integrating%20external%20knowledge.%20Retrieval-Augmented%20Generation%20%28RAG%29%20is%20a%20promising%20solution%2C%20but%20current%20methods%20remain%20inefficient%20and%20often%20fail%20to%20maintain%20high%20answer%20quality.%20To%20address%20these%20challenges%2C%20we%20propose%20VideoSpeculateRAG%2C%20an%20efficient%20VLM-based%20RAG%20framework%20built%20on%20two%20key%20ideas.%20First%2C%20we%20introduce%20a%20speculative%20decoding%20pipeline%3A%20a%20lightweight%20draft%20model%20quickly%20generates%20multiple%20answer%20candidates%2C%20which%20are%20then%20verified%20and%20refined%20by%20a%20more%20accurate%20heavyweight%20model%2C%20substantially%20reducing%20inference%20latency%20without%20sacrificing%20correctness.%20Second%2C%20we%20identify%20a%20major%20source%20of%20error%20-%20incorrect%20entity%20recognition%20in%20retrieved%20knowledge%20-%20and%20mitigate%20it%20with%20a%20simple%20yet%20effective%20similarity-based%20filtering%20strategy%20that%20improves%20entity%20alignment%20and%20boosts%20overall%20answer%20accuracy.%20Experiments%20demonstrate%20that%20VideoSpeculateRAG%20achieves%20comparable%20or%20higher%20accuracy%20than%20standard%20RAG%20approaches%20while%20accelerating%20inference%20by%20approximately%202x.%20Our%20framework%20highlights%20the%20potential%20of%20combining%20speculative%20decoding%20with%20retrieval-augmented%20reasoning%20to%20enhance%20efficiency%20and%20reliability%20in%20complex%2C%20knowledge-intensive%20multimodal%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.01513v2&entry.124074799=Read"},
{"title": "Staged Voxel-Level Deep Reinforcement Learning for 3D Medical Image Segmentation with Noisy Annotations", "author": "Yuyang Fu and Xiuzhen Guo and Ji Shi", "abstract": "Deep learning has achieved significant advancements in medical image segmentation. Currently, obtaining accurate segmentation outcomes is critically reliant on large-scale datasets with high-quality annotations. However, noisy annotations are frequently encountered owing to the complex morphological structures of organs in medical images and variations among different annotators, which can substantially limit the efficacy of segmentation models. Motivated by the fact that medical imaging annotator can correct labeling errors during segmentation based on prior knowledge, we propose an end-to-end Staged Voxel-Level Deep Reinforcement Learning (SVL-DRL) framework for robust medical image segmentation under noisy annotations. This framework employs a dynamic iterative update strategy to automatically mitigate the impact of erroneous labels without requiring manual intervention. The key advancements of SVL-DRL over existing works include: i) formulating noisy annotations as a voxel-dependent problem and addressing it through a novel staged reinforcement learning framework which guarantees robust model convergence; ii) incorporating a voxel-level asynchronous advantage actor-critic (vA3C) module that conceptualizes each voxel as an autonomous agent, which allows each agent to dynamically refine its own state representation during training, thereby directly mitigating the influence of erroneous labels; iii) designing a novel action space for the agents, along with a composite reward function that strategically combines the Dice value and a spatial continuity metric to significantly boost segmentation accuracy while maintain semantic integrity. Experiments on three public medical image datasets demonstrates State-of-The-Art (SoTA) performance under various experimental settings, with an average improvement of over 3\\% in both Dice and IoU scores.", "link": "http://arxiv.org/abs/2601.03875v1", "date": "2026-01-07", "relevancy": 2.2472, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.629}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Staged%20Voxel-Level%20Deep%20Reinforcement%20Learning%20for%203D%20Medical%20Image%20Segmentation%20with%20Noisy%20Annotations&body=Title%3A%20Staged%20Voxel-Level%20Deep%20Reinforcement%20Learning%20for%203D%20Medical%20Image%20Segmentation%20with%20Noisy%20Annotations%0AAuthor%3A%20Yuyang%20Fu%20and%20Xiuzhen%20Guo%20and%20Ji%20Shi%0AAbstract%3A%20Deep%20learning%20has%20achieved%20significant%20advancements%20in%20medical%20image%20segmentation.%20Currently%2C%20obtaining%20accurate%20segmentation%20outcomes%20is%20critically%20reliant%20on%20large-scale%20datasets%20with%20high-quality%20annotations.%20However%2C%20noisy%20annotations%20are%20frequently%20encountered%20owing%20to%20the%20complex%20morphological%20structures%20of%20organs%20in%20medical%20images%20and%20variations%20among%20different%20annotators%2C%20which%20can%20substantially%20limit%20the%20efficacy%20of%20segmentation%20models.%20Motivated%20by%20the%20fact%20that%20medical%20imaging%20annotator%20can%20correct%20labeling%20errors%20during%20segmentation%20based%20on%20prior%20knowledge%2C%20we%20propose%20an%20end-to-end%20Staged%20Voxel-Level%20Deep%20Reinforcement%20Learning%20%28SVL-DRL%29%20framework%20for%20robust%20medical%20image%20segmentation%20under%20noisy%20annotations.%20This%20framework%20employs%20a%20dynamic%20iterative%20update%20strategy%20to%20automatically%20mitigate%20the%20impact%20of%20erroneous%20labels%20without%20requiring%20manual%20intervention.%20The%20key%20advancements%20of%20SVL-DRL%20over%20existing%20works%20include%3A%20i%29%20formulating%20noisy%20annotations%20as%20a%20voxel-dependent%20problem%20and%20addressing%20it%20through%20a%20novel%20staged%20reinforcement%20learning%20framework%20which%20guarantees%20robust%20model%20convergence%3B%20ii%29%20incorporating%20a%20voxel-level%20asynchronous%20advantage%20actor-critic%20%28vA3C%29%20module%20that%20conceptualizes%20each%20voxel%20as%20an%20autonomous%20agent%2C%20which%20allows%20each%20agent%20to%20dynamically%20refine%20its%20own%20state%20representation%20during%20training%2C%20thereby%20directly%20mitigating%20the%20influence%20of%20erroneous%20labels%3B%20iii%29%20designing%20a%20novel%20action%20space%20for%20the%20agents%2C%20along%20with%20a%20composite%20reward%20function%20that%20strategically%20combines%20the%20Dice%20value%20and%20a%20spatial%20continuity%20metric%20to%20significantly%20boost%20segmentation%20accuracy%20while%20maintain%20semantic%20integrity.%20Experiments%20on%20three%20public%20medical%20image%20datasets%20demonstrates%20State-of-The-Art%20%28SoTA%29%20performance%20under%20various%20experimental%20settings%2C%20with%20an%20average%20improvement%20of%20over%203%5C%25%20in%20both%20Dice%20and%20IoU%20scores.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStaged%2520Voxel-Level%2520Deep%2520Reinforcement%2520Learning%2520for%25203D%2520Medical%2520Image%2520Segmentation%2520with%2520Noisy%2520Annotations%26entry.906535625%3DYuyang%2520Fu%2520and%2520Xiuzhen%2520Guo%2520and%2520Ji%2520Shi%26entry.1292438233%3DDeep%2520learning%2520has%2520achieved%2520significant%2520advancements%2520in%2520medical%2520image%2520segmentation.%2520Currently%252C%2520obtaining%2520accurate%2520segmentation%2520outcomes%2520is%2520critically%2520reliant%2520on%2520large-scale%2520datasets%2520with%2520high-quality%2520annotations.%2520However%252C%2520noisy%2520annotations%2520are%2520frequently%2520encountered%2520owing%2520to%2520the%2520complex%2520morphological%2520structures%2520of%2520organs%2520in%2520medical%2520images%2520and%2520variations%2520among%2520different%2520annotators%252C%2520which%2520can%2520substantially%2520limit%2520the%2520efficacy%2520of%2520segmentation%2520models.%2520Motivated%2520by%2520the%2520fact%2520that%2520medical%2520imaging%2520annotator%2520can%2520correct%2520labeling%2520errors%2520during%2520segmentation%2520based%2520on%2520prior%2520knowledge%252C%2520we%2520propose%2520an%2520end-to-end%2520Staged%2520Voxel-Level%2520Deep%2520Reinforcement%2520Learning%2520%2528SVL-DRL%2529%2520framework%2520for%2520robust%2520medical%2520image%2520segmentation%2520under%2520noisy%2520annotations.%2520This%2520framework%2520employs%2520a%2520dynamic%2520iterative%2520update%2520strategy%2520to%2520automatically%2520mitigate%2520the%2520impact%2520of%2520erroneous%2520labels%2520without%2520requiring%2520manual%2520intervention.%2520The%2520key%2520advancements%2520of%2520SVL-DRL%2520over%2520existing%2520works%2520include%253A%2520i%2529%2520formulating%2520noisy%2520annotations%2520as%2520a%2520voxel-dependent%2520problem%2520and%2520addressing%2520it%2520through%2520a%2520novel%2520staged%2520reinforcement%2520learning%2520framework%2520which%2520guarantees%2520robust%2520model%2520convergence%253B%2520ii%2529%2520incorporating%2520a%2520voxel-level%2520asynchronous%2520advantage%2520actor-critic%2520%2528vA3C%2529%2520module%2520that%2520conceptualizes%2520each%2520voxel%2520as%2520an%2520autonomous%2520agent%252C%2520which%2520allows%2520each%2520agent%2520to%2520dynamically%2520refine%2520its%2520own%2520state%2520representation%2520during%2520training%252C%2520thereby%2520directly%2520mitigating%2520the%2520influence%2520of%2520erroneous%2520labels%253B%2520iii%2529%2520designing%2520a%2520novel%2520action%2520space%2520for%2520the%2520agents%252C%2520along%2520with%2520a%2520composite%2520reward%2520function%2520that%2520strategically%2520combines%2520the%2520Dice%2520value%2520and%2520a%2520spatial%2520continuity%2520metric%2520to%2520significantly%2520boost%2520segmentation%2520accuracy%2520while%2520maintain%2520semantic%2520integrity.%2520Experiments%2520on%2520three%2520public%2520medical%2520image%2520datasets%2520demonstrates%2520State-of-The-Art%2520%2528SoTA%2529%2520performance%2520under%2520various%2520experimental%2520settings%252C%2520with%2520an%2520average%2520improvement%2520of%2520over%25203%255C%2525%2520in%2520both%2520Dice%2520and%2520IoU%2520scores.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Staged%20Voxel-Level%20Deep%20Reinforcement%20Learning%20for%203D%20Medical%20Image%20Segmentation%20with%20Noisy%20Annotations&entry.906535625=Yuyang%20Fu%20and%20Xiuzhen%20Guo%20and%20Ji%20Shi&entry.1292438233=Deep%20learning%20has%20achieved%20significant%20advancements%20in%20medical%20image%20segmentation.%20Currently%2C%20obtaining%20accurate%20segmentation%20outcomes%20is%20critically%20reliant%20on%20large-scale%20datasets%20with%20high-quality%20annotations.%20However%2C%20noisy%20annotations%20are%20frequently%20encountered%20owing%20to%20the%20complex%20morphological%20structures%20of%20organs%20in%20medical%20images%20and%20variations%20among%20different%20annotators%2C%20which%20can%20substantially%20limit%20the%20efficacy%20of%20segmentation%20models.%20Motivated%20by%20the%20fact%20that%20medical%20imaging%20annotator%20can%20correct%20labeling%20errors%20during%20segmentation%20based%20on%20prior%20knowledge%2C%20we%20propose%20an%20end-to-end%20Staged%20Voxel-Level%20Deep%20Reinforcement%20Learning%20%28SVL-DRL%29%20framework%20for%20robust%20medical%20image%20segmentation%20under%20noisy%20annotations.%20This%20framework%20employs%20a%20dynamic%20iterative%20update%20strategy%20to%20automatically%20mitigate%20the%20impact%20of%20erroneous%20labels%20without%20requiring%20manual%20intervention.%20The%20key%20advancements%20of%20SVL-DRL%20over%20existing%20works%20include%3A%20i%29%20formulating%20noisy%20annotations%20as%20a%20voxel-dependent%20problem%20and%20addressing%20it%20through%20a%20novel%20staged%20reinforcement%20learning%20framework%20which%20guarantees%20robust%20model%20convergence%3B%20ii%29%20incorporating%20a%20voxel-level%20asynchronous%20advantage%20actor-critic%20%28vA3C%29%20module%20that%20conceptualizes%20each%20voxel%20as%20an%20autonomous%20agent%2C%20which%20allows%20each%20agent%20to%20dynamically%20refine%20its%20own%20state%20representation%20during%20training%2C%20thereby%20directly%20mitigating%20the%20influence%20of%20erroneous%20labels%3B%20iii%29%20designing%20a%20novel%20action%20space%20for%20the%20agents%2C%20along%20with%20a%20composite%20reward%20function%20that%20strategically%20combines%20the%20Dice%20value%20and%20a%20spatial%20continuity%20metric%20to%20significantly%20boost%20segmentation%20accuracy%20while%20maintain%20semantic%20integrity.%20Experiments%20on%20three%20public%20medical%20image%20datasets%20demonstrates%20State-of-The-Art%20%28SoTA%29%20performance%20under%20various%20experimental%20settings%2C%20with%20an%20average%20improvement%20of%20over%203%5C%25%20in%20both%20Dice%20and%20IoU%20scores.&entry.1838667208=http%3A//arxiv.org/abs/2601.03875v1&entry.124074799=Read"},
{"title": "Bayesian Monocular Depth Refinement via Neural Radiance Fields", "author": "Arun Muthukkumar", "abstract": "Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate superior performance on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.", "link": "http://arxiv.org/abs/2601.03869v1", "date": "2026-01-07", "relevancy": 2.2433, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5619}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5611}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Monocular%20Depth%20Refinement%20via%20Neural%20Radiance%20Fields&body=Title%3A%20Bayesian%20Monocular%20Depth%20Refinement%20via%20Neural%20Radiance%20Fields%0AAuthor%3A%20Arun%20Muthukkumar%0AAbstract%3A%20Monocular%20depth%20estimation%20has%20applications%20in%20many%20fields%2C%20such%20as%20autonomous%20navigation%20and%20extended%20reality%2C%20making%20it%20an%20essential%20computer%20vision%20task.%20However%2C%20current%20methods%20often%20produce%20smooth%20depth%20maps%20that%20lack%20the%20fine%20geometric%20detail%20needed%20for%20accurate%20scene%20understanding.%20We%20propose%20MDENeRF%2C%20an%20iterative%20framework%20that%20refines%20monocular%20depth%20estimates%20using%20depth%20information%20from%20Neural%20Radiance%20Fields%20%28NeRFs%29.%20MDENeRF%20consists%20of%20three%20components%3A%20%281%29%20an%20initial%20monocular%20estimate%20for%20global%20structure%2C%20%282%29%20a%20NeRF%20trained%20on%20perturbed%20viewpoints%2C%20with%20per-pixel%20uncertainty%2C%20and%20%283%29%20Bayesian%20fusion%20of%20the%20noisy%20monocular%20and%20NeRF%20depths.%20We%20derive%20NeRF%20uncertainty%20from%20the%20volume%20rendering%20process%20to%20iteratively%20inject%20high-frequency%20fine%20details.%20Meanwhile%2C%20our%20monocular%20prior%20maintains%20global%20structure.%20We%20demonstrate%20superior%20performance%20on%20key%20metrics%20and%20experiments%20using%20indoor%20scenes%20from%20the%20SUN%20RGB-D%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Monocular%2520Depth%2520Refinement%2520via%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DArun%2520Muthukkumar%26entry.1292438233%3DMonocular%2520depth%2520estimation%2520has%2520applications%2520in%2520many%2520fields%252C%2520such%2520as%2520autonomous%2520navigation%2520and%2520extended%2520reality%252C%2520making%2520it%2520an%2520essential%2520computer%2520vision%2520task.%2520However%252C%2520current%2520methods%2520often%2520produce%2520smooth%2520depth%2520maps%2520that%2520lack%2520the%2520fine%2520geometric%2520detail%2520needed%2520for%2520accurate%2520scene%2520understanding.%2520We%2520propose%2520MDENeRF%252C%2520an%2520iterative%2520framework%2520that%2520refines%2520monocular%2520depth%2520estimates%2520using%2520depth%2520information%2520from%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529.%2520MDENeRF%2520consists%2520of%2520three%2520components%253A%2520%25281%2529%2520an%2520initial%2520monocular%2520estimate%2520for%2520global%2520structure%252C%2520%25282%2529%2520a%2520NeRF%2520trained%2520on%2520perturbed%2520viewpoints%252C%2520with%2520per-pixel%2520uncertainty%252C%2520and%2520%25283%2529%2520Bayesian%2520fusion%2520of%2520the%2520noisy%2520monocular%2520and%2520NeRF%2520depths.%2520We%2520derive%2520NeRF%2520uncertainty%2520from%2520the%2520volume%2520rendering%2520process%2520to%2520iteratively%2520inject%2520high-frequency%2520fine%2520details.%2520Meanwhile%252C%2520our%2520monocular%2520prior%2520maintains%2520global%2520structure.%2520We%2520demonstrate%2520superior%2520performance%2520on%2520key%2520metrics%2520and%2520experiments%2520using%2520indoor%2520scenes%2520from%2520the%2520SUN%2520RGB-D%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Monocular%20Depth%20Refinement%20via%20Neural%20Radiance%20Fields&entry.906535625=Arun%20Muthukkumar&entry.1292438233=Monocular%20depth%20estimation%20has%20applications%20in%20many%20fields%2C%20such%20as%20autonomous%20navigation%20and%20extended%20reality%2C%20making%20it%20an%20essential%20computer%20vision%20task.%20However%2C%20current%20methods%20often%20produce%20smooth%20depth%20maps%20that%20lack%20the%20fine%20geometric%20detail%20needed%20for%20accurate%20scene%20understanding.%20We%20propose%20MDENeRF%2C%20an%20iterative%20framework%20that%20refines%20monocular%20depth%20estimates%20using%20depth%20information%20from%20Neural%20Radiance%20Fields%20%28NeRFs%29.%20MDENeRF%20consists%20of%20three%20components%3A%20%281%29%20an%20initial%20monocular%20estimate%20for%20global%20structure%2C%20%282%29%20a%20NeRF%20trained%20on%20perturbed%20viewpoints%2C%20with%20per-pixel%20uncertainty%2C%20and%20%283%29%20Bayesian%20fusion%20of%20the%20noisy%20monocular%20and%20NeRF%20depths.%20We%20derive%20NeRF%20uncertainty%20from%20the%20volume%20rendering%20process%20to%20iteratively%20inject%20high-frequency%20fine%20details.%20Meanwhile%2C%20our%20monocular%20prior%20maintains%20global%20structure.%20We%20demonstrate%20superior%20performance%20on%20key%20metrics%20and%20experiments%20using%20indoor%20scenes%20from%20the%20SUN%20RGB-D%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2601.03869v1&entry.124074799=Read"},
{"title": "FOREVER: Forgetting Curve-Inspired Memory Replay for Language Model Continual Learning", "author": "Yujie Feng and Hao Wang and Jian Li and Xu Chu and Zhaolu Kang and Yiran Liu and Yasha Wang and Philip S. Yu and Xiao-Ming Wu", "abstract": "Continual learning (CL) for large language models (LLMs) aims to enable sequential knowledge acquisition without catastrophic forgetting. Memory replay methods are widely used for their practicality and effectiveness, but most rely on fixed, step-based heuristics that often misalign with the model's actual learning progress, since identical training steps can result in varying degrees of parameter change. Motivated by recent findings that LLM forgetting mirrors the Ebbinghaus human forgetting curve, we propose FOREVER (FORgEtting curVe-inspired mEmory Replay), a novel CL framework that aligns replay schedules with a model-centric notion of time. FOREVER defines model time using the magnitude of optimizer updates, allowing forgetting curve-inspired replay intervals to align with the model's internal evolution rather than raw training steps. Building on this approach, FOREVER incorporates a forgetting curve-based replay scheduler to determine when to replay and an intensity-aware regularization mechanism to adaptively control how to replay. Extensive experiments on three CL benchmarks and models ranging from 0.6B to 13B parameters demonstrate that FOREVER consistently mitigates catastrophic forgetting.", "link": "http://arxiv.org/abs/2601.03938v1", "date": "2026-01-07", "relevancy": 2.2296, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOREVER%3A%20Forgetting%20Curve-Inspired%20Memory%20Replay%20for%20Language%20Model%20Continual%20Learning&body=Title%3A%20FOREVER%3A%20Forgetting%20Curve-Inspired%20Memory%20Replay%20for%20Language%20Model%20Continual%20Learning%0AAuthor%3A%20Yujie%20Feng%20and%20Hao%20Wang%20and%20Jian%20Li%20and%20Xu%20Chu%20and%20Zhaolu%20Kang%20and%20Yiran%20Liu%20and%20Yasha%20Wang%20and%20Philip%20S.%20Yu%20and%20Xiao-Ming%20Wu%0AAbstract%3A%20Continual%20learning%20%28CL%29%20for%20large%20language%20models%20%28LLMs%29%20aims%20to%20enable%20sequential%20knowledge%20acquisition%20without%20catastrophic%20forgetting.%20Memory%20replay%20methods%20are%20widely%20used%20for%20their%20practicality%20and%20effectiveness%2C%20but%20most%20rely%20on%20fixed%2C%20step-based%20heuristics%20that%20often%20misalign%20with%20the%20model%27s%20actual%20learning%20progress%2C%20since%20identical%20training%20steps%20can%20result%20in%20varying%20degrees%20of%20parameter%20change.%20Motivated%20by%20recent%20findings%20that%20LLM%20forgetting%20mirrors%20the%20Ebbinghaus%20human%20forgetting%20curve%2C%20we%20propose%20FOREVER%20%28FORgEtting%20curVe-inspired%20mEmory%20Replay%29%2C%20a%20novel%20CL%20framework%20that%20aligns%20replay%20schedules%20with%20a%20model-centric%20notion%20of%20time.%20FOREVER%20defines%20model%20time%20using%20the%20magnitude%20of%20optimizer%20updates%2C%20allowing%20forgetting%20curve-inspired%20replay%20intervals%20to%20align%20with%20the%20model%27s%20internal%20evolution%20rather%20than%20raw%20training%20steps.%20Building%20on%20this%20approach%2C%20FOREVER%20incorporates%20a%20forgetting%20curve-based%20replay%20scheduler%20to%20determine%20when%20to%20replay%20and%20an%20intensity-aware%20regularization%20mechanism%20to%20adaptively%20control%20how%20to%20replay.%20Extensive%20experiments%20on%20three%20CL%20benchmarks%20and%20models%20ranging%20from%200.6B%20to%2013B%20parameters%20demonstrate%20that%20FOREVER%20consistently%20mitigates%20catastrophic%20forgetting.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOREVER%253A%2520Forgetting%2520Curve-Inspired%2520Memory%2520Replay%2520for%2520Language%2520Model%2520Continual%2520Learning%26entry.906535625%3DYujie%2520Feng%2520and%2520Hao%2520Wang%2520and%2520Jian%2520Li%2520and%2520Xu%2520Chu%2520and%2520Zhaolu%2520Kang%2520and%2520Yiran%2520Liu%2520and%2520Yasha%2520Wang%2520and%2520Philip%2520S.%2520Yu%2520and%2520Xiao-Ming%2520Wu%26entry.1292438233%3DContinual%2520learning%2520%2528CL%2529%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520aims%2520to%2520enable%2520sequential%2520knowledge%2520acquisition%2520without%2520catastrophic%2520forgetting.%2520Memory%2520replay%2520methods%2520are%2520widely%2520used%2520for%2520their%2520practicality%2520and%2520effectiveness%252C%2520but%2520most%2520rely%2520on%2520fixed%252C%2520step-based%2520heuristics%2520that%2520often%2520misalign%2520with%2520the%2520model%2527s%2520actual%2520learning%2520progress%252C%2520since%2520identical%2520training%2520steps%2520can%2520result%2520in%2520varying%2520degrees%2520of%2520parameter%2520change.%2520Motivated%2520by%2520recent%2520findings%2520that%2520LLM%2520forgetting%2520mirrors%2520the%2520Ebbinghaus%2520human%2520forgetting%2520curve%252C%2520we%2520propose%2520FOREVER%2520%2528FORgEtting%2520curVe-inspired%2520mEmory%2520Replay%2529%252C%2520a%2520novel%2520CL%2520framework%2520that%2520aligns%2520replay%2520schedules%2520with%2520a%2520model-centric%2520notion%2520of%2520time.%2520FOREVER%2520defines%2520model%2520time%2520using%2520the%2520magnitude%2520of%2520optimizer%2520updates%252C%2520allowing%2520forgetting%2520curve-inspired%2520replay%2520intervals%2520to%2520align%2520with%2520the%2520model%2527s%2520internal%2520evolution%2520rather%2520than%2520raw%2520training%2520steps.%2520Building%2520on%2520this%2520approach%252C%2520FOREVER%2520incorporates%2520a%2520forgetting%2520curve-based%2520replay%2520scheduler%2520to%2520determine%2520when%2520to%2520replay%2520and%2520an%2520intensity-aware%2520regularization%2520mechanism%2520to%2520adaptively%2520control%2520how%2520to%2520replay.%2520Extensive%2520experiments%2520on%2520three%2520CL%2520benchmarks%2520and%2520models%2520ranging%2520from%25200.6B%2520to%252013B%2520parameters%2520demonstrate%2520that%2520FOREVER%2520consistently%2520mitigates%2520catastrophic%2520forgetting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOREVER%3A%20Forgetting%20Curve-Inspired%20Memory%20Replay%20for%20Language%20Model%20Continual%20Learning&entry.906535625=Yujie%20Feng%20and%20Hao%20Wang%20and%20Jian%20Li%20and%20Xu%20Chu%20and%20Zhaolu%20Kang%20and%20Yiran%20Liu%20and%20Yasha%20Wang%20and%20Philip%20S.%20Yu%20and%20Xiao-Ming%20Wu&entry.1292438233=Continual%20learning%20%28CL%29%20for%20large%20language%20models%20%28LLMs%29%20aims%20to%20enable%20sequential%20knowledge%20acquisition%20without%20catastrophic%20forgetting.%20Memory%20replay%20methods%20are%20widely%20used%20for%20their%20practicality%20and%20effectiveness%2C%20but%20most%20rely%20on%20fixed%2C%20step-based%20heuristics%20that%20often%20misalign%20with%20the%20model%27s%20actual%20learning%20progress%2C%20since%20identical%20training%20steps%20can%20result%20in%20varying%20degrees%20of%20parameter%20change.%20Motivated%20by%20recent%20findings%20that%20LLM%20forgetting%20mirrors%20the%20Ebbinghaus%20human%20forgetting%20curve%2C%20we%20propose%20FOREVER%20%28FORgEtting%20curVe-inspired%20mEmory%20Replay%29%2C%20a%20novel%20CL%20framework%20that%20aligns%20replay%20schedules%20with%20a%20model-centric%20notion%20of%20time.%20FOREVER%20defines%20model%20time%20using%20the%20magnitude%20of%20optimizer%20updates%2C%20allowing%20forgetting%20curve-inspired%20replay%20intervals%20to%20align%20with%20the%20model%27s%20internal%20evolution%20rather%20than%20raw%20training%20steps.%20Building%20on%20this%20approach%2C%20FOREVER%20incorporates%20a%20forgetting%20curve-based%20replay%20scheduler%20to%20determine%20when%20to%20replay%20and%20an%20intensity-aware%20regularization%20mechanism%20to%20adaptively%20control%20how%20to%20replay.%20Extensive%20experiments%20on%20three%20CL%20benchmarks%20and%20models%20ranging%20from%200.6B%20to%2013B%20parameters%20demonstrate%20that%20FOREVER%20consistently%20mitigates%20catastrophic%20forgetting.&entry.1838667208=http%3A//arxiv.org/abs/2601.03938v1&entry.124074799=Read"},
{"title": "Equivariant Neural Networks for Force-Field Models of Lattice Systems", "author": "Yunhao Fan and Gia-Wei Chern", "abstract": "Machine-learning (ML) force fields enable large-scale simulations with near-first-principles accuracy at substantially reduced computational cost. Recent work has extended ML force-field approaches to adiabatic dynamical simulations of condensed-matter lattice models with coupled electronic and structural or magnetic degrees of freedom. However, most existing formulations rely on hand-crafted, symmetry-aware descriptors, whose construction is often system-specific and can hinder generality and transferability across different lattice Hamiltonians. Here we introduce a symmetry-preserving framework based on equivariant neural networks (ENNs) that provides a general, data-driven mapping from local configurations of dynamical variables to the associated on-site forces in a lattice Hamiltonian. In contrast to ENN architectures developed for molecular systems -- where continuous Euclidean symmetries dominate -- our approach aims to embed the discrete point-group and internal symmetries intrinsic to lattice models directly into the neural-network representation of the force field. As a proof of principle, we construct an ENN-based force-field model for the adiabatic dynamics of the Holstein Hamiltonian on a square lattice, a canonical system for electron-lattice physics. The resulting ML-enabled large-scale dynamical simulations faithfully capture mesoscale evolution of the symmetry-breaking phase, illustrating the utility of lattice-equivariant architectures for linking microscopic electronic processes to emergent dynamical behavior in condensed-matter lattice systems.", "link": "http://arxiv.org/abs/2601.04104v1", "date": "2026-01-07", "relevancy": 2.2211, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4484}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4452}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Neural%20Networks%20for%20Force-Field%20Models%20of%20Lattice%20Systems&body=Title%3A%20Equivariant%20Neural%20Networks%20for%20Force-Field%20Models%20of%20Lattice%20Systems%0AAuthor%3A%20Yunhao%20Fan%20and%20Gia-Wei%20Chern%0AAbstract%3A%20Machine-learning%20%28ML%29%20force%20fields%20enable%20large-scale%20simulations%20with%20near-first-principles%20accuracy%20at%20substantially%20reduced%20computational%20cost.%20Recent%20work%20has%20extended%20ML%20force-field%20approaches%20to%20adiabatic%20dynamical%20simulations%20of%20condensed-matter%20lattice%20models%20with%20coupled%20electronic%20and%20structural%20or%20magnetic%20degrees%20of%20freedom.%20However%2C%20most%20existing%20formulations%20rely%20on%20hand-crafted%2C%20symmetry-aware%20descriptors%2C%20whose%20construction%20is%20often%20system-specific%20and%20can%20hinder%20generality%20and%20transferability%20across%20different%20lattice%20Hamiltonians.%20Here%20we%20introduce%20a%20symmetry-preserving%20framework%20based%20on%20equivariant%20neural%20networks%20%28ENNs%29%20that%20provides%20a%20general%2C%20data-driven%20mapping%20from%20local%20configurations%20of%20dynamical%20variables%20to%20the%20associated%20on-site%20forces%20in%20a%20lattice%20Hamiltonian.%20In%20contrast%20to%20ENN%20architectures%20developed%20for%20molecular%20systems%20--%20where%20continuous%20Euclidean%20symmetries%20dominate%20--%20our%20approach%20aims%20to%20embed%20the%20discrete%20point-group%20and%20internal%20symmetries%20intrinsic%20to%20lattice%20models%20directly%20into%20the%20neural-network%20representation%20of%20the%20force%20field.%20As%20a%20proof%20of%20principle%2C%20we%20construct%20an%20ENN-based%20force-field%20model%20for%20the%20adiabatic%20dynamics%20of%20the%20Holstein%20Hamiltonian%20on%20a%20square%20lattice%2C%20a%20canonical%20system%20for%20electron-lattice%20physics.%20The%20resulting%20ML-enabled%20large-scale%20dynamical%20simulations%20faithfully%20capture%20mesoscale%20evolution%20of%20the%20symmetry-breaking%20phase%2C%20illustrating%20the%20utility%20of%20lattice-equivariant%20architectures%20for%20linking%20microscopic%20electronic%20processes%20to%20emergent%20dynamical%20behavior%20in%20condensed-matter%20lattice%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Neural%2520Networks%2520for%2520Force-Field%2520Models%2520of%2520Lattice%2520Systems%26entry.906535625%3DYunhao%2520Fan%2520and%2520Gia-Wei%2520Chern%26entry.1292438233%3DMachine-learning%2520%2528ML%2529%2520force%2520fields%2520enable%2520large-scale%2520simulations%2520with%2520near-first-principles%2520accuracy%2520at%2520substantially%2520reduced%2520computational%2520cost.%2520Recent%2520work%2520has%2520extended%2520ML%2520force-field%2520approaches%2520to%2520adiabatic%2520dynamical%2520simulations%2520of%2520condensed-matter%2520lattice%2520models%2520with%2520coupled%2520electronic%2520and%2520structural%2520or%2520magnetic%2520degrees%2520of%2520freedom.%2520However%252C%2520most%2520existing%2520formulations%2520rely%2520on%2520hand-crafted%252C%2520symmetry-aware%2520descriptors%252C%2520whose%2520construction%2520is%2520often%2520system-specific%2520and%2520can%2520hinder%2520generality%2520and%2520transferability%2520across%2520different%2520lattice%2520Hamiltonians.%2520Here%2520we%2520introduce%2520a%2520symmetry-preserving%2520framework%2520based%2520on%2520equivariant%2520neural%2520networks%2520%2528ENNs%2529%2520that%2520provides%2520a%2520general%252C%2520data-driven%2520mapping%2520from%2520local%2520configurations%2520of%2520dynamical%2520variables%2520to%2520the%2520associated%2520on-site%2520forces%2520in%2520a%2520lattice%2520Hamiltonian.%2520In%2520contrast%2520to%2520ENN%2520architectures%2520developed%2520for%2520molecular%2520systems%2520--%2520where%2520continuous%2520Euclidean%2520symmetries%2520dominate%2520--%2520our%2520approach%2520aims%2520to%2520embed%2520the%2520discrete%2520point-group%2520and%2520internal%2520symmetries%2520intrinsic%2520to%2520lattice%2520models%2520directly%2520into%2520the%2520neural-network%2520representation%2520of%2520the%2520force%2520field.%2520As%2520a%2520proof%2520of%2520principle%252C%2520we%2520construct%2520an%2520ENN-based%2520force-field%2520model%2520for%2520the%2520adiabatic%2520dynamics%2520of%2520the%2520Holstein%2520Hamiltonian%2520on%2520a%2520square%2520lattice%252C%2520a%2520canonical%2520system%2520for%2520electron-lattice%2520physics.%2520The%2520resulting%2520ML-enabled%2520large-scale%2520dynamical%2520simulations%2520faithfully%2520capture%2520mesoscale%2520evolution%2520of%2520the%2520symmetry-breaking%2520phase%252C%2520illustrating%2520the%2520utility%2520of%2520lattice-equivariant%2520architectures%2520for%2520linking%2520microscopic%2520electronic%2520processes%2520to%2520emergent%2520dynamical%2520behavior%2520in%2520condensed-matter%2520lattice%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Neural%20Networks%20for%20Force-Field%20Models%20of%20Lattice%20Systems&entry.906535625=Yunhao%20Fan%20and%20Gia-Wei%20Chern&entry.1292438233=Machine-learning%20%28ML%29%20force%20fields%20enable%20large-scale%20simulations%20with%20near-first-principles%20accuracy%20at%20substantially%20reduced%20computational%20cost.%20Recent%20work%20has%20extended%20ML%20force-field%20approaches%20to%20adiabatic%20dynamical%20simulations%20of%20condensed-matter%20lattice%20models%20with%20coupled%20electronic%20and%20structural%20or%20magnetic%20degrees%20of%20freedom.%20However%2C%20most%20existing%20formulations%20rely%20on%20hand-crafted%2C%20symmetry-aware%20descriptors%2C%20whose%20construction%20is%20often%20system-specific%20and%20can%20hinder%20generality%20and%20transferability%20across%20different%20lattice%20Hamiltonians.%20Here%20we%20introduce%20a%20symmetry-preserving%20framework%20based%20on%20equivariant%20neural%20networks%20%28ENNs%29%20that%20provides%20a%20general%2C%20data-driven%20mapping%20from%20local%20configurations%20of%20dynamical%20variables%20to%20the%20associated%20on-site%20forces%20in%20a%20lattice%20Hamiltonian.%20In%20contrast%20to%20ENN%20architectures%20developed%20for%20molecular%20systems%20--%20where%20continuous%20Euclidean%20symmetries%20dominate%20--%20our%20approach%20aims%20to%20embed%20the%20discrete%20point-group%20and%20internal%20symmetries%20intrinsic%20to%20lattice%20models%20directly%20into%20the%20neural-network%20representation%20of%20the%20force%20field.%20As%20a%20proof%20of%20principle%2C%20we%20construct%20an%20ENN-based%20force-field%20model%20for%20the%20adiabatic%20dynamics%20of%20the%20Holstein%20Hamiltonian%20on%20a%20square%20lattice%2C%20a%20canonical%20system%20for%20electron-lattice%20physics.%20The%20resulting%20ML-enabled%20large-scale%20dynamical%20simulations%20faithfully%20capture%20mesoscale%20evolution%20of%20the%20symmetry-breaking%20phase%2C%20illustrating%20the%20utility%20of%20lattice-equivariant%20architectures%20for%20linking%20microscopic%20electronic%20processes%20to%20emergent%20dynamical%20behavior%20in%20condensed-matter%20lattice%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.04104v1&entry.124074799=Read"},
{"title": "HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense", "author": "Siyuan Li and Xi Lin and Jun Wu and Zehao Liu and Haoyu Li and Tianjie Ju and Xiang Chen and Jianhua Li", "abstract": "Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.", "link": "http://arxiv.org/abs/2601.04034v1", "date": "2026-01-07", "relevancy": 2.2211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4742}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4371}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoneyTrap%3A%20Deceiving%20Large%20Language%20Model%20Attackers%20to%20Honeypot%20Traps%20with%20Resilient%20Multi-Agent%20Defense&body=Title%3A%20HoneyTrap%3A%20Deceiving%20Large%20Language%20Model%20Attackers%20to%20Honeypot%20Traps%20with%20Resilient%20Multi-Agent%20Defense%0AAuthor%3A%20Siyuan%20Li%20and%20Xi%20Lin%20and%20Jun%20Wu%20and%20Zehao%20Liu%20and%20Haoyu%20Li%20and%20Tianjie%20Ju%20and%20Xiang%20Chen%20and%20Jianhua%20Li%0AAbstract%3A%20Jailbreak%20attacks%20pose%20significant%20threats%20to%20large%20language%20models%20%28LLMs%29%2C%20enabling%20attackers%20to%20bypass%20safeguards.%20However%2C%20existing%20reactive%20defense%20approaches%20struggle%20to%20keep%20up%20with%20the%20rapidly%20evolving%20multi-turn%20jailbreaks%2C%20where%20attackers%20continuously%20deepen%20their%20attacks%20to%20exploit%20vulnerabilities.%20To%20address%20this%20critical%20challenge%2C%20we%20propose%20HoneyTrap%2C%20a%20novel%20deceptive%20LLM%20defense%20framework%20leveraging%20collaborative%20defenders%20to%20counter%20jailbreak%20attacks.%20It%20integrates%20four%20defensive%20agents%2C%20Threat%20Interceptor%2C%20Misdirection%20Controller%2C%20Forensic%20Tracker%2C%20and%20System%20Harmonizer%2C%20each%20performing%20a%20specialized%20security%20role%20and%20collaborating%20to%20complete%20a%20deceptive%20defense.%20To%20ensure%20a%20comprehensive%20evaluation%2C%20we%20introduce%20MTJ-Pro%2C%20a%20challenging%20multi-turn%20progressive%20jailbreak%20dataset%20that%20combines%20seven%20advanced%20jailbreak%20strategies%20designed%20to%20gradually%20deepen%20attack%20strategies%20across%20multi-turn%20attacks.%20Besides%2C%20we%20present%20two%20novel%20metrics%3A%20Mislead%20Success%20Rate%20%28MSR%29%20and%20Attack%20Resource%20Consumption%20%28ARC%29%2C%20which%20provide%20more%20nuanced%20assessments%20of%20deceptive%20defense%20beyond%20conventional%20measures.%20Experimental%20results%20on%20GPT-4%2C%20GPT-3.5-turbo%2C%20Gemini-1.5-pro%2C%20and%20LLaMa-3.1%20demonstrate%20that%20HoneyTrap%20achieves%20an%20average%20reduction%20of%2068.77%25%20in%20attack%20success%20rates%20compared%20to%20state-of-the-art%20baselines.%20Notably%2C%20even%20in%20a%20dedicated%20adaptive%20attacker%20setting%20with%20intensified%20conditions%2C%20HoneyTrap%20remains%20resilient%2C%20leveraging%20deceptive%20engagement%20to%20prolong%20interactions%2C%20significantly%20increasing%20the%20time%20and%20computational%20costs%20required%20for%20successful%20exploitation.%20Unlike%20simple%20rejection%2C%20HoneyTrap%20strategically%20wastes%20attacker%20resources%20without%20impacting%20benign%20queries%2C%20improving%20MSR%20and%20ARC%20by%20118.11%25%20and%20149.16%25%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoneyTrap%253A%2520Deceiving%2520Large%2520Language%2520Model%2520Attackers%2520to%2520Honeypot%2520Traps%2520with%2520Resilient%2520Multi-Agent%2520Defense%26entry.906535625%3DSiyuan%2520Li%2520and%2520Xi%2520Lin%2520and%2520Jun%2520Wu%2520and%2520Zehao%2520Liu%2520and%2520Haoyu%2520Li%2520and%2520Tianjie%2520Ju%2520and%2520Xiang%2520Chen%2520and%2520Jianhua%2520Li%26entry.1292438233%3DJailbreak%2520attacks%2520pose%2520significant%2520threats%2520to%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520enabling%2520attackers%2520to%2520bypass%2520safeguards.%2520However%252C%2520existing%2520reactive%2520defense%2520approaches%2520struggle%2520to%2520keep%2520up%2520with%2520the%2520rapidly%2520evolving%2520multi-turn%2520jailbreaks%252C%2520where%2520attackers%2520continuously%2520deepen%2520their%2520attacks%2520to%2520exploit%2520vulnerabilities.%2520To%2520address%2520this%2520critical%2520challenge%252C%2520we%2520propose%2520HoneyTrap%252C%2520a%2520novel%2520deceptive%2520LLM%2520defense%2520framework%2520leveraging%2520collaborative%2520defenders%2520to%2520counter%2520jailbreak%2520attacks.%2520It%2520integrates%2520four%2520defensive%2520agents%252C%2520Threat%2520Interceptor%252C%2520Misdirection%2520Controller%252C%2520Forensic%2520Tracker%252C%2520and%2520System%2520Harmonizer%252C%2520each%2520performing%2520a%2520specialized%2520security%2520role%2520and%2520collaborating%2520to%2520complete%2520a%2520deceptive%2520defense.%2520To%2520ensure%2520a%2520comprehensive%2520evaluation%252C%2520we%2520introduce%2520MTJ-Pro%252C%2520a%2520challenging%2520multi-turn%2520progressive%2520jailbreak%2520dataset%2520that%2520combines%2520seven%2520advanced%2520jailbreak%2520strategies%2520designed%2520to%2520gradually%2520deepen%2520attack%2520strategies%2520across%2520multi-turn%2520attacks.%2520Besides%252C%2520we%2520present%2520two%2520novel%2520metrics%253A%2520Mislead%2520Success%2520Rate%2520%2528MSR%2529%2520and%2520Attack%2520Resource%2520Consumption%2520%2528ARC%2529%252C%2520which%2520provide%2520more%2520nuanced%2520assessments%2520of%2520deceptive%2520defense%2520beyond%2520conventional%2520measures.%2520Experimental%2520results%2520on%2520GPT-4%252C%2520GPT-3.5-turbo%252C%2520Gemini-1.5-pro%252C%2520and%2520LLaMa-3.1%2520demonstrate%2520that%2520HoneyTrap%2520achieves%2520an%2520average%2520reduction%2520of%252068.77%2525%2520in%2520attack%2520success%2520rates%2520compared%2520to%2520state-of-the-art%2520baselines.%2520Notably%252C%2520even%2520in%2520a%2520dedicated%2520adaptive%2520attacker%2520setting%2520with%2520intensified%2520conditions%252C%2520HoneyTrap%2520remains%2520resilient%252C%2520leveraging%2520deceptive%2520engagement%2520to%2520prolong%2520interactions%252C%2520significantly%2520increasing%2520the%2520time%2520and%2520computational%2520costs%2520required%2520for%2520successful%2520exploitation.%2520Unlike%2520simple%2520rejection%252C%2520HoneyTrap%2520strategically%2520wastes%2520attacker%2520resources%2520without%2520impacting%2520benign%2520queries%252C%2520improving%2520MSR%2520and%2520ARC%2520by%2520118.11%2525%2520and%2520149.16%2525%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoneyTrap%3A%20Deceiving%20Large%20Language%20Model%20Attackers%20to%20Honeypot%20Traps%20with%20Resilient%20Multi-Agent%20Defense&entry.906535625=Siyuan%20Li%20and%20Xi%20Lin%20and%20Jun%20Wu%20and%20Zehao%20Liu%20and%20Haoyu%20Li%20and%20Tianjie%20Ju%20and%20Xiang%20Chen%20and%20Jianhua%20Li&entry.1292438233=Jailbreak%20attacks%20pose%20significant%20threats%20to%20large%20language%20models%20%28LLMs%29%2C%20enabling%20attackers%20to%20bypass%20safeguards.%20However%2C%20existing%20reactive%20defense%20approaches%20struggle%20to%20keep%20up%20with%20the%20rapidly%20evolving%20multi-turn%20jailbreaks%2C%20where%20attackers%20continuously%20deepen%20their%20attacks%20to%20exploit%20vulnerabilities.%20To%20address%20this%20critical%20challenge%2C%20we%20propose%20HoneyTrap%2C%20a%20novel%20deceptive%20LLM%20defense%20framework%20leveraging%20collaborative%20defenders%20to%20counter%20jailbreak%20attacks.%20It%20integrates%20four%20defensive%20agents%2C%20Threat%20Interceptor%2C%20Misdirection%20Controller%2C%20Forensic%20Tracker%2C%20and%20System%20Harmonizer%2C%20each%20performing%20a%20specialized%20security%20role%20and%20collaborating%20to%20complete%20a%20deceptive%20defense.%20To%20ensure%20a%20comprehensive%20evaluation%2C%20we%20introduce%20MTJ-Pro%2C%20a%20challenging%20multi-turn%20progressive%20jailbreak%20dataset%20that%20combines%20seven%20advanced%20jailbreak%20strategies%20designed%20to%20gradually%20deepen%20attack%20strategies%20across%20multi-turn%20attacks.%20Besides%2C%20we%20present%20two%20novel%20metrics%3A%20Mislead%20Success%20Rate%20%28MSR%29%20and%20Attack%20Resource%20Consumption%20%28ARC%29%2C%20which%20provide%20more%20nuanced%20assessments%20of%20deceptive%20defense%20beyond%20conventional%20measures.%20Experimental%20results%20on%20GPT-4%2C%20GPT-3.5-turbo%2C%20Gemini-1.5-pro%2C%20and%20LLaMa-3.1%20demonstrate%20that%20HoneyTrap%20achieves%20an%20average%20reduction%20of%2068.77%25%20in%20attack%20success%20rates%20compared%20to%20state-of-the-art%20baselines.%20Notably%2C%20even%20in%20a%20dedicated%20adaptive%20attacker%20setting%20with%20intensified%20conditions%2C%20HoneyTrap%20remains%20resilient%2C%20leveraging%20deceptive%20engagement%20to%20prolong%20interactions%2C%20significantly%20increasing%20the%20time%20and%20computational%20costs%20required%20for%20successful%20exploitation.%20Unlike%20simple%20rejection%2C%20HoneyTrap%20strategically%20wastes%20attacker%20resources%20without%20impacting%20benign%20queries%2C%20improving%20MSR%20and%20ARC%20by%20118.11%25%20and%20149.16%25%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2601.04034v1&entry.124074799=Read"},
{"title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks", "author": "Hao Bai and Alexey Taymanov and Tong Zhang and Aviral Kumar and Spencer Whitehead", "abstract": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.", "link": "http://arxiv.org/abs/2601.02439v2", "date": "2026-01-07", "relevancy": 2.215, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5841}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.552}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WebGym%3A%20Scaling%20Training%20Environments%20for%20Visual%20Web%20Agents%20with%20Realistic%20Tasks&body=Title%3A%20WebGym%3A%20Scaling%20Training%20Environments%20for%20Visual%20Web%20Agents%20with%20Realistic%20Tasks%0AAuthor%3A%20Hao%20Bai%20and%20Alexey%20Taymanov%20and%20Tong%20Zhang%20and%20Aviral%20Kumar%20and%20Spencer%20Whitehead%0AAbstract%3A%20We%20present%20WebGym%2C%20the%20largest-to-date%20open-source%20environment%20for%20training%20realistic%20visual%20web%20agents.%20Real%20websites%20are%20non-stationary%20and%20diverse%2C%20making%20artificial%20or%20small-scale%20task%20sets%20insufficient%20for%20robust%20policy%20learning.%20WebGym%20contains%20nearly%20300%2C000%20tasks%20with%20rubric-based%20evaluations%20across%20diverse%2C%20real-world%20websites%20and%20difficulty%20levels.%20We%20train%20agents%20with%20a%20simple%20reinforcement%20learning%20%28RL%29%20recipe%2C%20which%20trains%20on%20the%20agent%27s%20own%20interaction%20traces%20%28rollouts%29%2C%20using%20task%20rewards%20as%20feedback%20to%20guide%20learning.%20To%20enable%20scaling%20RL%2C%20we%20speed%20up%20sampling%20of%20trajectories%20in%20WebGym%20by%20developing%20a%20high-throughput%20asynchronous%20rollout%20system%2C%20designed%20specifically%20for%20web%20agents.%20Our%20system%20achieves%20a%204-5x%20rollout%20speedup%20compared%20to%20naive%20implementations.%20Second%2C%20we%20scale%20the%20task%20set%20breadth%2C%20depth%2C%20and%20size%2C%20which%20results%20in%20continued%20performance%20improvement.%20Fine-tuning%20a%20strong%20base%20vision-language%20model%2C%20Qwen-3-VL-8B-Instruct%2C%20on%20WebGym%20results%20in%20an%20improvement%20in%20success%20rate%20on%20an%20out-of-distribution%20test%20set%20from%2026.2%25%20to%2042.9%25%2C%20significantly%20outperforming%20agents%20based%20on%20proprietary%20models%20such%20as%20GPT-4o%20and%20GPT-5-Thinking%20that%20achieve%2027.1%25%20and%2029.8%25%2C%20respectively.%20This%20improvement%20is%20substantial%20because%20our%20test%20set%20consists%20only%20of%20tasks%20on%20websites%20never%20seen%20during%20training%2C%20unlike%20many%20other%20prior%20works%20on%20training%20visual%20web%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWebGym%253A%2520Scaling%2520Training%2520Environments%2520for%2520Visual%2520Web%2520Agents%2520with%2520Realistic%2520Tasks%26entry.906535625%3DHao%2520Bai%2520and%2520Alexey%2520Taymanov%2520and%2520Tong%2520Zhang%2520and%2520Aviral%2520Kumar%2520and%2520Spencer%2520Whitehead%26entry.1292438233%3DWe%2520present%2520WebGym%252C%2520the%2520largest-to-date%2520open-source%2520environment%2520for%2520training%2520realistic%2520visual%2520web%2520agents.%2520Real%2520websites%2520are%2520non-stationary%2520and%2520diverse%252C%2520making%2520artificial%2520or%2520small-scale%2520task%2520sets%2520insufficient%2520for%2520robust%2520policy%2520learning.%2520WebGym%2520contains%2520nearly%2520300%252C000%2520tasks%2520with%2520rubric-based%2520evaluations%2520across%2520diverse%252C%2520real-world%2520websites%2520and%2520difficulty%2520levels.%2520We%2520train%2520agents%2520with%2520a%2520simple%2520reinforcement%2520learning%2520%2528RL%2529%2520recipe%252C%2520which%2520trains%2520on%2520the%2520agent%2527s%2520own%2520interaction%2520traces%2520%2528rollouts%2529%252C%2520using%2520task%2520rewards%2520as%2520feedback%2520to%2520guide%2520learning.%2520To%2520enable%2520scaling%2520RL%252C%2520we%2520speed%2520up%2520sampling%2520of%2520trajectories%2520in%2520WebGym%2520by%2520developing%2520a%2520high-throughput%2520asynchronous%2520rollout%2520system%252C%2520designed%2520specifically%2520for%2520web%2520agents.%2520Our%2520system%2520achieves%2520a%25204-5x%2520rollout%2520speedup%2520compared%2520to%2520naive%2520implementations.%2520Second%252C%2520we%2520scale%2520the%2520task%2520set%2520breadth%252C%2520depth%252C%2520and%2520size%252C%2520which%2520results%2520in%2520continued%2520performance%2520improvement.%2520Fine-tuning%2520a%2520strong%2520base%2520vision-language%2520model%252C%2520Qwen-3-VL-8B-Instruct%252C%2520on%2520WebGym%2520results%2520in%2520an%2520improvement%2520in%2520success%2520rate%2520on%2520an%2520out-of-distribution%2520test%2520set%2520from%252026.2%2525%2520to%252042.9%2525%252C%2520significantly%2520outperforming%2520agents%2520based%2520on%2520proprietary%2520models%2520such%2520as%2520GPT-4o%2520and%2520GPT-5-Thinking%2520that%2520achieve%252027.1%2525%2520and%252029.8%2525%252C%2520respectively.%2520This%2520improvement%2520is%2520substantial%2520because%2520our%2520test%2520set%2520consists%2520only%2520of%2520tasks%2520on%2520websites%2520never%2520seen%2520during%2520training%252C%2520unlike%2520many%2520other%2520prior%2520works%2520on%2520training%2520visual%2520web%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WebGym%3A%20Scaling%20Training%20Environments%20for%20Visual%20Web%20Agents%20with%20Realistic%20Tasks&entry.906535625=Hao%20Bai%20and%20Alexey%20Taymanov%20and%20Tong%20Zhang%20and%20Aviral%20Kumar%20and%20Spencer%20Whitehead&entry.1292438233=We%20present%20WebGym%2C%20the%20largest-to-date%20open-source%20environment%20for%20training%20realistic%20visual%20web%20agents.%20Real%20websites%20are%20non-stationary%20and%20diverse%2C%20making%20artificial%20or%20small-scale%20task%20sets%20insufficient%20for%20robust%20policy%20learning.%20WebGym%20contains%20nearly%20300%2C000%20tasks%20with%20rubric-based%20evaluations%20across%20diverse%2C%20real-world%20websites%20and%20difficulty%20levels.%20We%20train%20agents%20with%20a%20simple%20reinforcement%20learning%20%28RL%29%20recipe%2C%20which%20trains%20on%20the%20agent%27s%20own%20interaction%20traces%20%28rollouts%29%2C%20using%20task%20rewards%20as%20feedback%20to%20guide%20learning.%20To%20enable%20scaling%20RL%2C%20we%20speed%20up%20sampling%20of%20trajectories%20in%20WebGym%20by%20developing%20a%20high-throughput%20asynchronous%20rollout%20system%2C%20designed%20specifically%20for%20web%20agents.%20Our%20system%20achieves%20a%204-5x%20rollout%20speedup%20compared%20to%20naive%20implementations.%20Second%2C%20we%20scale%20the%20task%20set%20breadth%2C%20depth%2C%20and%20size%2C%20which%20results%20in%20continued%20performance%20improvement.%20Fine-tuning%20a%20strong%20base%20vision-language%20model%2C%20Qwen-3-VL-8B-Instruct%2C%20on%20WebGym%20results%20in%20an%20improvement%20in%20success%20rate%20on%20an%20out-of-distribution%20test%20set%20from%2026.2%25%20to%2042.9%25%2C%20significantly%20outperforming%20agents%20based%20on%20proprietary%20models%20such%20as%20GPT-4o%20and%20GPT-5-Thinking%20that%20achieve%2027.1%25%20and%2029.8%25%2C%20respectively.%20This%20improvement%20is%20substantial%20because%20our%20test%20set%20consists%20only%20of%20tasks%20on%20websites%20never%20seen%20during%20training%2C%20unlike%20many%20other%20prior%20works%20on%20training%20visual%20web%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.02439v2&entry.124074799=Read"},
{"title": "LinkD: AutoRegressive Diffusion Model for Mechanical Linkage Synthesis", "author": "Yayati Jadhav and Amir Barati Farimani", "abstract": "Designing mechanical linkages to achieve target end-effector trajectories presents a fundamental challenge due to the intricate coupling between continuous node placements, discrete topological configurations, and nonlinear kinematic constraints. The highly nonlinear motion-to-configuration relationship means small perturbations in joint positions drastically alter trajectories, while the combinatorially expanding design space renders conventional optimization and heuristic methods computationally intractable. We introduce an autoregressive diffusion framework that exploits the dyadic nature of linkage assembly by representing mechanisms as sequentially constructed graphs, where nodes correspond to joints and edges to rigid links. Our approach combines a causal transformer with a Denoising Diffusion Probabilistic Model (DDPM), both conditioned on target trajectories encoded via a transformer encoder. The causal transformer autoregressively predicts discrete topology node-by-node, while the DDPM refines each node's spatial coordinates and edge connectivity to previously generated nodes. This sequential generation enables adaptive trial-and-error synthesis where problematic nodes exhibiting kinematic locking or collisions can be selectively regenerated, allowing autonomous correction of degenerate configurations during design. Our graph-based, data-driven methodology surpasses traditional optimization approaches, enabling scalable inverse design that generalizes to mechanisms with arbitrary node counts. We demonstrate successful synthesis of linkage systems containing up to 20 nodes with extensibility to N-node architectures. This work advances autoregressive graph generation methodologies and computational kinematic synthesis, establishing new paradigms for scalable inverse design of complex mechanical systems.", "link": "http://arxiv.org/abs/2601.04054v1", "date": "2026-01-07", "relevancy": 2.187, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5767}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LinkD%3A%20AutoRegressive%20Diffusion%20Model%20for%20Mechanical%20Linkage%20Synthesis&body=Title%3A%20LinkD%3A%20AutoRegressive%20Diffusion%20Model%20for%20Mechanical%20Linkage%20Synthesis%0AAuthor%3A%20Yayati%20Jadhav%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20Designing%20mechanical%20linkages%20to%20achieve%20target%20end-effector%20trajectories%20presents%20a%20fundamental%20challenge%20due%20to%20the%20intricate%20coupling%20between%20continuous%20node%20placements%2C%20discrete%20topological%20configurations%2C%20and%20nonlinear%20kinematic%20constraints.%20The%20highly%20nonlinear%20motion-to-configuration%20relationship%20means%20small%20perturbations%20in%20joint%20positions%20drastically%20alter%20trajectories%2C%20while%20the%20combinatorially%20expanding%20design%20space%20renders%20conventional%20optimization%20and%20heuristic%20methods%20computationally%20intractable.%20We%20introduce%20an%20autoregressive%20diffusion%20framework%20that%20exploits%20the%20dyadic%20nature%20of%20linkage%20assembly%20by%20representing%20mechanisms%20as%20sequentially%20constructed%20graphs%2C%20where%20nodes%20correspond%20to%20joints%20and%20edges%20to%20rigid%20links.%20Our%20approach%20combines%20a%20causal%20transformer%20with%20a%20Denoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%2C%20both%20conditioned%20on%20target%20trajectories%20encoded%20via%20a%20transformer%20encoder.%20The%20causal%20transformer%20autoregressively%20predicts%20discrete%20topology%20node-by-node%2C%20while%20the%20DDPM%20refines%20each%20node%27s%20spatial%20coordinates%20and%20edge%20connectivity%20to%20previously%20generated%20nodes.%20This%20sequential%20generation%20enables%20adaptive%20trial-and-error%20synthesis%20where%20problematic%20nodes%20exhibiting%20kinematic%20locking%20or%20collisions%20can%20be%20selectively%20regenerated%2C%20allowing%20autonomous%20correction%20of%20degenerate%20configurations%20during%20design.%20Our%20graph-based%2C%20data-driven%20methodology%20surpasses%20traditional%20optimization%20approaches%2C%20enabling%20scalable%20inverse%20design%20that%20generalizes%20to%20mechanisms%20with%20arbitrary%20node%20counts.%20We%20demonstrate%20successful%20synthesis%20of%20linkage%20systems%20containing%20up%20to%2020%20nodes%20with%20extensibility%20to%20N-node%20architectures.%20This%20work%20advances%20autoregressive%20graph%20generation%20methodologies%20and%20computational%20kinematic%20synthesis%2C%20establishing%20new%20paradigms%20for%20scalable%20inverse%20design%20of%20complex%20mechanical%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinkD%253A%2520AutoRegressive%2520Diffusion%2520Model%2520for%2520Mechanical%2520Linkage%2520Synthesis%26entry.906535625%3DYayati%2520Jadhav%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3DDesigning%2520mechanical%2520linkages%2520to%2520achieve%2520target%2520end-effector%2520trajectories%2520presents%2520a%2520fundamental%2520challenge%2520due%2520to%2520the%2520intricate%2520coupling%2520between%2520continuous%2520node%2520placements%252C%2520discrete%2520topological%2520configurations%252C%2520and%2520nonlinear%2520kinematic%2520constraints.%2520The%2520highly%2520nonlinear%2520motion-to-configuration%2520relationship%2520means%2520small%2520perturbations%2520in%2520joint%2520positions%2520drastically%2520alter%2520trajectories%252C%2520while%2520the%2520combinatorially%2520expanding%2520design%2520space%2520renders%2520conventional%2520optimization%2520and%2520heuristic%2520methods%2520computationally%2520intractable.%2520We%2520introduce%2520an%2520autoregressive%2520diffusion%2520framework%2520that%2520exploits%2520the%2520dyadic%2520nature%2520of%2520linkage%2520assembly%2520by%2520representing%2520mechanisms%2520as%2520sequentially%2520constructed%2520graphs%252C%2520where%2520nodes%2520correspond%2520to%2520joints%2520and%2520edges%2520to%2520rigid%2520links.%2520Our%2520approach%2520combines%2520a%2520causal%2520transformer%2520with%2520a%2520Denoising%2520Diffusion%2520Probabilistic%2520Model%2520%2528DDPM%2529%252C%2520both%2520conditioned%2520on%2520target%2520trajectories%2520encoded%2520via%2520a%2520transformer%2520encoder.%2520The%2520causal%2520transformer%2520autoregressively%2520predicts%2520discrete%2520topology%2520node-by-node%252C%2520while%2520the%2520DDPM%2520refines%2520each%2520node%2527s%2520spatial%2520coordinates%2520and%2520edge%2520connectivity%2520to%2520previously%2520generated%2520nodes.%2520This%2520sequential%2520generation%2520enables%2520adaptive%2520trial-and-error%2520synthesis%2520where%2520problematic%2520nodes%2520exhibiting%2520kinematic%2520locking%2520or%2520collisions%2520can%2520be%2520selectively%2520regenerated%252C%2520allowing%2520autonomous%2520correction%2520of%2520degenerate%2520configurations%2520during%2520design.%2520Our%2520graph-based%252C%2520data-driven%2520methodology%2520surpasses%2520traditional%2520optimization%2520approaches%252C%2520enabling%2520scalable%2520inverse%2520design%2520that%2520generalizes%2520to%2520mechanisms%2520with%2520arbitrary%2520node%2520counts.%2520We%2520demonstrate%2520successful%2520synthesis%2520of%2520linkage%2520systems%2520containing%2520up%2520to%252020%2520nodes%2520with%2520extensibility%2520to%2520N-node%2520architectures.%2520This%2520work%2520advances%2520autoregressive%2520graph%2520generation%2520methodologies%2520and%2520computational%2520kinematic%2520synthesis%252C%2520establishing%2520new%2520paradigms%2520for%2520scalable%2520inverse%2520design%2520of%2520complex%2520mechanical%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LinkD%3A%20AutoRegressive%20Diffusion%20Model%20for%20Mechanical%20Linkage%20Synthesis&entry.906535625=Yayati%20Jadhav%20and%20Amir%20Barati%20Farimani&entry.1292438233=Designing%20mechanical%20linkages%20to%20achieve%20target%20end-effector%20trajectories%20presents%20a%20fundamental%20challenge%20due%20to%20the%20intricate%20coupling%20between%20continuous%20node%20placements%2C%20discrete%20topological%20configurations%2C%20and%20nonlinear%20kinematic%20constraints.%20The%20highly%20nonlinear%20motion-to-configuration%20relationship%20means%20small%20perturbations%20in%20joint%20positions%20drastically%20alter%20trajectories%2C%20while%20the%20combinatorially%20expanding%20design%20space%20renders%20conventional%20optimization%20and%20heuristic%20methods%20computationally%20intractable.%20We%20introduce%20an%20autoregressive%20diffusion%20framework%20that%20exploits%20the%20dyadic%20nature%20of%20linkage%20assembly%20by%20representing%20mechanisms%20as%20sequentially%20constructed%20graphs%2C%20where%20nodes%20correspond%20to%20joints%20and%20edges%20to%20rigid%20links.%20Our%20approach%20combines%20a%20causal%20transformer%20with%20a%20Denoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%2C%20both%20conditioned%20on%20target%20trajectories%20encoded%20via%20a%20transformer%20encoder.%20The%20causal%20transformer%20autoregressively%20predicts%20discrete%20topology%20node-by-node%2C%20while%20the%20DDPM%20refines%20each%20node%27s%20spatial%20coordinates%20and%20edge%20connectivity%20to%20previously%20generated%20nodes.%20This%20sequential%20generation%20enables%20adaptive%20trial-and-error%20synthesis%20where%20problematic%20nodes%20exhibiting%20kinematic%20locking%20or%20collisions%20can%20be%20selectively%20regenerated%2C%20allowing%20autonomous%20correction%20of%20degenerate%20configurations%20during%20design.%20Our%20graph-based%2C%20data-driven%20methodology%20surpasses%20traditional%20optimization%20approaches%2C%20enabling%20scalable%20inverse%20design%20that%20generalizes%20to%20mechanisms%20with%20arbitrary%20node%20counts.%20We%20demonstrate%20successful%20synthesis%20of%20linkage%20systems%20containing%20up%20to%2020%20nodes%20with%20extensibility%20to%20N-node%20architectures.%20This%20work%20advances%20autoregressive%20graph%20generation%20methodologies%20and%20computational%20kinematic%20synthesis%2C%20establishing%20new%20paradigms%20for%20scalable%20inverse%20design%20of%20complex%20mechanical%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.04054v1&entry.124074799=Read"},
{"title": "ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography", "author": "Vladimir Frants and Sos Agaian and Karen Panetta", "abstract": "Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.", "link": "http://arxiv.org/abs/2601.04159v1", "date": "2026-01-07", "relevancy": 2.1855, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5619}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5418}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToTMNet%3A%20FFT-Accelerated%20Toeplitz%20Temporal%20Mixing%20Network%20for%20Lightweight%20Remote%20Photoplethysmography&body=Title%3A%20ToTMNet%3A%20FFT-Accelerated%20Toeplitz%20Temporal%20Mixing%20Network%20for%20Lightweight%20Remote%20Photoplethysmography%0AAuthor%3A%20Vladimir%20Frants%20and%20Sos%20Agaian%20and%20Karen%20Panetta%0AAbstract%3A%20Remote%20photoplethysmography%20%28rPPG%29%20estimates%20a%20blood%20volume%20pulse%20%28BVP%29%20waveform%20from%20facial%20videos%20captured%20by%20commodity%20cameras.%20Although%20recent%20deep%20models%20improve%20robustness%20compared%20to%20classical%20signal-processing%20approaches%2C%20many%20methods%20increase%20computational%20cost%20and%20parameter%20count%2C%20and%20attention-based%20temporal%20modeling%20introduces%20quadratic%20scaling%20with%20respect%20to%20the%20temporal%20length.%20This%20paper%20proposes%20ToTMNet%2C%20a%20lightweight%20rPPG%20architecture%20that%20replaces%20temporal%20attention%20with%20an%20FFT-accelerated%20Toeplitz%20temporal%20mixing%20layer.%20The%20Toeplitz%20operator%20provides%20full-sequence%20temporal%20receptive%20field%20using%20a%20linear%20number%20of%20parameters%20in%20the%20clip%20length%20and%20can%20be%20applied%20in%20near-linear%20time%20using%20circulant%20embedding%20and%20FFT-based%20convolution.%20ToTMNet%20integrates%20the%20global%20Toeplitz%20temporal%20operator%20into%20a%20compact%20gated%20temporal%20mixer%20that%20combines%20a%20local%20depthwise%20temporal%20convolution%20branch%20with%20gated%20global%20Toeplitz%20mixing%2C%20enabling%20efficient%20long-range%20temporal%20filtering%20while%20only%20having%2063k%20parameters.%20Experiments%20on%20two%20datasets%2C%20UBFC-rPPG%20%28real%20videos%29%20and%20SCAMPS%20%28synthetic%20videos%29%2C%20show%20that%20ToTMNet%20achieves%20strong%20heart-rate%20estimation%20accuracy%20with%20a%20compact%20design.%20On%20UBFC-rPPG%20intra-dataset%20evaluation%2C%20ToTMNet%20reaches%201.055%20bpm%20MAE%20with%20Pearson%20correlation%200.996.%20In%20a%20synthetic-to-real%20setting%20%28SCAMPS%20to%20UBFC-rPPG%29%2C%20ToTMNet%20reaches%201.582%20bpm%20MAE%20with%20Pearson%20correlation%200.994.%20Ablation%20results%20confirm%20that%20the%20gating%20mechanism%20is%20important%20for%20effectively%20using%20global%20Toeplitz%20mixing%2C%20especially%20under%20domain%20shift.%20The%20main%20limitation%20of%20this%20preprint%20study%20is%20the%20use%20of%20only%20two%20datasets%3B%20nevertheless%2C%20the%20results%20indicate%20that%20Toeplitz-structured%20temporal%20mixing%20is%20a%20practical%20and%20efficient%20alternative%20to%20attention%20for%20rPPG.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToTMNet%253A%2520FFT-Accelerated%2520Toeplitz%2520Temporal%2520Mixing%2520Network%2520for%2520Lightweight%2520Remote%2520Photoplethysmography%26entry.906535625%3DVladimir%2520Frants%2520and%2520Sos%2520Agaian%2520and%2520Karen%2520Panetta%26entry.1292438233%3DRemote%2520photoplethysmography%2520%2528rPPG%2529%2520estimates%2520a%2520blood%2520volume%2520pulse%2520%2528BVP%2529%2520waveform%2520from%2520facial%2520videos%2520captured%2520by%2520commodity%2520cameras.%2520Although%2520recent%2520deep%2520models%2520improve%2520robustness%2520compared%2520to%2520classical%2520signal-processing%2520approaches%252C%2520many%2520methods%2520increase%2520computational%2520cost%2520and%2520parameter%2520count%252C%2520and%2520attention-based%2520temporal%2520modeling%2520introduces%2520quadratic%2520scaling%2520with%2520respect%2520to%2520the%2520temporal%2520length.%2520This%2520paper%2520proposes%2520ToTMNet%252C%2520a%2520lightweight%2520rPPG%2520architecture%2520that%2520replaces%2520temporal%2520attention%2520with%2520an%2520FFT-accelerated%2520Toeplitz%2520temporal%2520mixing%2520layer.%2520The%2520Toeplitz%2520operator%2520provides%2520full-sequence%2520temporal%2520receptive%2520field%2520using%2520a%2520linear%2520number%2520of%2520parameters%2520in%2520the%2520clip%2520length%2520and%2520can%2520be%2520applied%2520in%2520near-linear%2520time%2520using%2520circulant%2520embedding%2520and%2520FFT-based%2520convolution.%2520ToTMNet%2520integrates%2520the%2520global%2520Toeplitz%2520temporal%2520operator%2520into%2520a%2520compact%2520gated%2520temporal%2520mixer%2520that%2520combines%2520a%2520local%2520depthwise%2520temporal%2520convolution%2520branch%2520with%2520gated%2520global%2520Toeplitz%2520mixing%252C%2520enabling%2520efficient%2520long-range%2520temporal%2520filtering%2520while%2520only%2520having%252063k%2520parameters.%2520Experiments%2520on%2520two%2520datasets%252C%2520UBFC-rPPG%2520%2528real%2520videos%2529%2520and%2520SCAMPS%2520%2528synthetic%2520videos%2529%252C%2520show%2520that%2520ToTMNet%2520achieves%2520strong%2520heart-rate%2520estimation%2520accuracy%2520with%2520a%2520compact%2520design.%2520On%2520UBFC-rPPG%2520intra-dataset%2520evaluation%252C%2520ToTMNet%2520reaches%25201.055%2520bpm%2520MAE%2520with%2520Pearson%2520correlation%25200.996.%2520In%2520a%2520synthetic-to-real%2520setting%2520%2528SCAMPS%2520to%2520UBFC-rPPG%2529%252C%2520ToTMNet%2520reaches%25201.582%2520bpm%2520MAE%2520with%2520Pearson%2520correlation%25200.994.%2520Ablation%2520results%2520confirm%2520that%2520the%2520gating%2520mechanism%2520is%2520important%2520for%2520effectively%2520using%2520global%2520Toeplitz%2520mixing%252C%2520especially%2520under%2520domain%2520shift.%2520The%2520main%2520limitation%2520of%2520this%2520preprint%2520study%2520is%2520the%2520use%2520of%2520only%2520two%2520datasets%253B%2520nevertheless%252C%2520the%2520results%2520indicate%2520that%2520Toeplitz-structured%2520temporal%2520mixing%2520is%2520a%2520practical%2520and%2520efficient%2520alternative%2520to%2520attention%2520for%2520rPPG.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToTMNet%3A%20FFT-Accelerated%20Toeplitz%20Temporal%20Mixing%20Network%20for%20Lightweight%20Remote%20Photoplethysmography&entry.906535625=Vladimir%20Frants%20and%20Sos%20Agaian%20and%20Karen%20Panetta&entry.1292438233=Remote%20photoplethysmography%20%28rPPG%29%20estimates%20a%20blood%20volume%20pulse%20%28BVP%29%20waveform%20from%20facial%20videos%20captured%20by%20commodity%20cameras.%20Although%20recent%20deep%20models%20improve%20robustness%20compared%20to%20classical%20signal-processing%20approaches%2C%20many%20methods%20increase%20computational%20cost%20and%20parameter%20count%2C%20and%20attention-based%20temporal%20modeling%20introduces%20quadratic%20scaling%20with%20respect%20to%20the%20temporal%20length.%20This%20paper%20proposes%20ToTMNet%2C%20a%20lightweight%20rPPG%20architecture%20that%20replaces%20temporal%20attention%20with%20an%20FFT-accelerated%20Toeplitz%20temporal%20mixing%20layer.%20The%20Toeplitz%20operator%20provides%20full-sequence%20temporal%20receptive%20field%20using%20a%20linear%20number%20of%20parameters%20in%20the%20clip%20length%20and%20can%20be%20applied%20in%20near-linear%20time%20using%20circulant%20embedding%20and%20FFT-based%20convolution.%20ToTMNet%20integrates%20the%20global%20Toeplitz%20temporal%20operator%20into%20a%20compact%20gated%20temporal%20mixer%20that%20combines%20a%20local%20depthwise%20temporal%20convolution%20branch%20with%20gated%20global%20Toeplitz%20mixing%2C%20enabling%20efficient%20long-range%20temporal%20filtering%20while%20only%20having%2063k%20parameters.%20Experiments%20on%20two%20datasets%2C%20UBFC-rPPG%20%28real%20videos%29%20and%20SCAMPS%20%28synthetic%20videos%29%2C%20show%20that%20ToTMNet%20achieves%20strong%20heart-rate%20estimation%20accuracy%20with%20a%20compact%20design.%20On%20UBFC-rPPG%20intra-dataset%20evaluation%2C%20ToTMNet%20reaches%201.055%20bpm%20MAE%20with%20Pearson%20correlation%200.996.%20In%20a%20synthetic-to-real%20setting%20%28SCAMPS%20to%20UBFC-rPPG%29%2C%20ToTMNet%20reaches%201.582%20bpm%20MAE%20with%20Pearson%20correlation%200.994.%20Ablation%20results%20confirm%20that%20the%20gating%20mechanism%20is%20important%20for%20effectively%20using%20global%20Toeplitz%20mixing%2C%20especially%20under%20domain%20shift.%20The%20main%20limitation%20of%20this%20preprint%20study%20is%20the%20use%20of%20only%20two%20datasets%3B%20nevertheless%2C%20the%20results%20indicate%20that%20Toeplitz-structured%20temporal%20mixing%20is%20a%20practical%20and%20efficient%20alternative%20to%20attention%20for%20rPPG.&entry.1838667208=http%3A//arxiv.org/abs/2601.04159v1&entry.124074799=Read"},
{"title": "AI Generated Text Detection", "author": "Adilkhan Alikhanov and Aidar Amangeldi and Diar Demeubay and Dilnaz Akhmetzhan and Nurbek Moldakhmetov and Omar Polat and Galymzhan Zharas", "abstract": "The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.", "link": "http://arxiv.org/abs/2601.03812v1", "date": "2026-01-07", "relevancy": 2.1793, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5477}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.547}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Generated%20Text%20Detection&body=Title%3A%20AI%20Generated%20Text%20Detection%0AAuthor%3A%20Adilkhan%20Alikhanov%20and%20Aidar%20Amangeldi%20and%20Diar%20Demeubay%20and%20Dilnaz%20Akhmetzhan%20and%20Nurbek%20Moldakhmetov%20and%20Omar%20Polat%20and%20Galymzhan%20Zharas%0AAbstract%3A%20The%20rapid%20development%20of%20large%20language%20models%20has%20led%20to%20an%20increase%20in%20AI-generated%20text%2C%20with%20students%20increasingly%20using%20LLM-generated%20content%20as%20their%20own%20work%2C%20which%20violates%20academic%20integrity.%20This%20paper%20presents%20an%20evaluation%20of%20AI%20text%20detection%20methods%2C%20including%20both%20traditional%20machine%20learning%20models%20and%20transformer-based%20architectures.%20We%20utilize%20two%20datasets%2C%20HC3%20and%20DAIGT%20v2%2C%20to%20build%20a%20unified%20benchmark%20and%20apply%20a%20topic-based%20data%20split%20to%20prevent%20information%20leakage.%20This%20approach%20ensures%20robust%20generalization%20across%20unseen%20domains.%20Our%20experiments%20show%20that%20TF-IDF%20logistic%20regression%20achieves%20a%20reasonable%20baseline%20accuracy%20of%2082.87%25.%20However%2C%20deep%20learning%20models%20outperform%20it.%20The%20BiLSTM%20classifier%20achieves%20an%20accuracy%20of%2088.86%25%2C%20while%20DistilBERT%20achieves%20a%20similar%20accuracy%20of%2088.11%25%20with%20the%20highest%20ROC-AUC%20score%20of%200.96%2C%20demonstrating%20the%20strongest%20overall%20performance.%20The%20results%20indicate%20that%20contextual%20semantic%20modeling%20is%20significantly%20superior%20to%20lexical%20features%20and%20highlight%20the%20importance%20of%20mitigating%20topic%20memorization%20through%20appropriate%20evaluation%20protocols.%20The%20limitations%20of%20this%20work%20are%20primarily%20related%20to%20dataset%20diversity%20and%20computational%20constraints.%20In%20future%20work%2C%20we%20plan%20to%20expand%20dataset%20diversity%20and%20utilize%20parameter-efficient%20fine-tuning%20methods%20such%20as%20LoRA.%20We%20also%20plan%20to%20explore%20smaller%20or%20distilled%20models%20and%20employ%20more%20efficient%20batching%20strategies%20and%20hardware-aware%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Generated%2520Text%2520Detection%26entry.906535625%3DAdilkhan%2520Alikhanov%2520and%2520Aidar%2520Amangeldi%2520and%2520Diar%2520Demeubay%2520and%2520Dilnaz%2520Akhmetzhan%2520and%2520Nurbek%2520Moldakhmetov%2520and%2520Omar%2520Polat%2520and%2520Galymzhan%2520Zharas%26entry.1292438233%3DThe%2520rapid%2520development%2520of%2520large%2520language%2520models%2520has%2520led%2520to%2520an%2520increase%2520in%2520AI-generated%2520text%252C%2520with%2520students%2520increasingly%2520using%2520LLM-generated%2520content%2520as%2520their%2520own%2520work%252C%2520which%2520violates%2520academic%2520integrity.%2520This%2520paper%2520presents%2520an%2520evaluation%2520of%2520AI%2520text%2520detection%2520methods%252C%2520including%2520both%2520traditional%2520machine%2520learning%2520models%2520and%2520transformer-based%2520architectures.%2520We%2520utilize%2520two%2520datasets%252C%2520HC3%2520and%2520DAIGT%2520v2%252C%2520to%2520build%2520a%2520unified%2520benchmark%2520and%2520apply%2520a%2520topic-based%2520data%2520split%2520to%2520prevent%2520information%2520leakage.%2520This%2520approach%2520ensures%2520robust%2520generalization%2520across%2520unseen%2520domains.%2520Our%2520experiments%2520show%2520that%2520TF-IDF%2520logistic%2520regression%2520achieves%2520a%2520reasonable%2520baseline%2520accuracy%2520of%252082.87%2525.%2520However%252C%2520deep%2520learning%2520models%2520outperform%2520it.%2520The%2520BiLSTM%2520classifier%2520achieves%2520an%2520accuracy%2520of%252088.86%2525%252C%2520while%2520DistilBERT%2520achieves%2520a%2520similar%2520accuracy%2520of%252088.11%2525%2520with%2520the%2520highest%2520ROC-AUC%2520score%2520of%25200.96%252C%2520demonstrating%2520the%2520strongest%2520overall%2520performance.%2520The%2520results%2520indicate%2520that%2520contextual%2520semantic%2520modeling%2520is%2520significantly%2520superior%2520to%2520lexical%2520features%2520and%2520highlight%2520the%2520importance%2520of%2520mitigating%2520topic%2520memorization%2520through%2520appropriate%2520evaluation%2520protocols.%2520The%2520limitations%2520of%2520this%2520work%2520are%2520primarily%2520related%2520to%2520dataset%2520diversity%2520and%2520computational%2520constraints.%2520In%2520future%2520work%252C%2520we%2520plan%2520to%2520expand%2520dataset%2520diversity%2520and%2520utilize%2520parameter-efficient%2520fine-tuning%2520methods%2520such%2520as%2520LoRA.%2520We%2520also%2520plan%2520to%2520explore%2520smaller%2520or%2520distilled%2520models%2520and%2520employ%2520more%2520efficient%2520batching%2520strategies%2520and%2520hardware-aware%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Generated%20Text%20Detection&entry.906535625=Adilkhan%20Alikhanov%20and%20Aidar%20Amangeldi%20and%20Diar%20Demeubay%20and%20Dilnaz%20Akhmetzhan%20and%20Nurbek%20Moldakhmetov%20and%20Omar%20Polat%20and%20Galymzhan%20Zharas&entry.1292438233=The%20rapid%20development%20of%20large%20language%20models%20has%20led%20to%20an%20increase%20in%20AI-generated%20text%2C%20with%20students%20increasingly%20using%20LLM-generated%20content%20as%20their%20own%20work%2C%20which%20violates%20academic%20integrity.%20This%20paper%20presents%20an%20evaluation%20of%20AI%20text%20detection%20methods%2C%20including%20both%20traditional%20machine%20learning%20models%20and%20transformer-based%20architectures.%20We%20utilize%20two%20datasets%2C%20HC3%20and%20DAIGT%20v2%2C%20to%20build%20a%20unified%20benchmark%20and%20apply%20a%20topic-based%20data%20split%20to%20prevent%20information%20leakage.%20This%20approach%20ensures%20robust%20generalization%20across%20unseen%20domains.%20Our%20experiments%20show%20that%20TF-IDF%20logistic%20regression%20achieves%20a%20reasonable%20baseline%20accuracy%20of%2082.87%25.%20However%2C%20deep%20learning%20models%20outperform%20it.%20The%20BiLSTM%20classifier%20achieves%20an%20accuracy%20of%2088.86%25%2C%20while%20DistilBERT%20achieves%20a%20similar%20accuracy%20of%2088.11%25%20with%20the%20highest%20ROC-AUC%20score%20of%200.96%2C%20demonstrating%20the%20strongest%20overall%20performance.%20The%20results%20indicate%20that%20contextual%20semantic%20modeling%20is%20significantly%20superior%20to%20lexical%20features%20and%20highlight%20the%20importance%20of%20mitigating%20topic%20memorization%20through%20appropriate%20evaluation%20protocols.%20The%20limitations%20of%20this%20work%20are%20primarily%20related%20to%20dataset%20diversity%20and%20computational%20constraints.%20In%20future%20work%2C%20we%20plan%20to%20expand%20dataset%20diversity%20and%20utilize%20parameter-efficient%20fine-tuning%20methods%20such%20as%20LoRA.%20We%20also%20plan%20to%20explore%20smaller%20or%20distilled%20models%20and%20employ%20more%20efficient%20batching%20strategies%20and%20hardware-aware%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2601.03812v1&entry.124074799=Read"},
{"title": "Women Worry, Men Adopt: How Gendered Perceptions Shape the Use of Generative AI", "author": "Fabian Stephany and Jedrzej Duszynski", "abstract": "Generative artificial intelligence (GenAI) is diffusing rapidly, yet its adoption is strikingly unequal. Using nationally representative UK survey data from 2023 to 2024, we show that women adopt GenAI substantially less often than men because they perceive its societal risks differently. We construct a composite index capturing concerns about mental health, privacy, climate impact, and labor market disruption. This index explains between 9 and 18 percent of the variation in GenAI adoption and ranks among the strongest predictors for women across all age groups, surpassing digital literacy and education for young women. Intersectional analyses show that the largest disparities arise among younger, digitally fluent individuals with high societal risk concerns, where gender gaps in personal use exceed 45 percentage points. Using a synthetic twin panel design, we show that increased optimism about AI's societal impact raises GenAI use among young women from 13 percent to 33 percent, substantially narrowing the gender divide. These findings indicate that gendered perceptions of AI's social and ethical consequences, rather than access or capability, are the primary drivers of unequal GenAI adoption, with implications for productivity, skill formation, and economic inequality in an AI enabled economy.", "link": "http://arxiv.org/abs/2601.03880v1", "date": "2026-01-07", "relevancy": 2.1755, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4491}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4316}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Women%20Worry%2C%20Men%20Adopt%3A%20How%20Gendered%20Perceptions%20Shape%20the%20Use%20of%20Generative%20AI&body=Title%3A%20Women%20Worry%2C%20Men%20Adopt%3A%20How%20Gendered%20Perceptions%20Shape%20the%20Use%20of%20Generative%20AI%0AAuthor%3A%20Fabian%20Stephany%20and%20Jedrzej%20Duszynski%0AAbstract%3A%20Generative%20artificial%20intelligence%20%28GenAI%29%20is%20diffusing%20rapidly%2C%20yet%20its%20adoption%20is%20strikingly%20unequal.%20Using%20nationally%20representative%20UK%20survey%20data%20from%202023%20to%202024%2C%20we%20show%20that%20women%20adopt%20GenAI%20substantially%20less%20often%20than%20men%20because%20they%20perceive%20its%20societal%20risks%20differently.%20We%20construct%20a%20composite%20index%20capturing%20concerns%20about%20mental%20health%2C%20privacy%2C%20climate%20impact%2C%20and%20labor%20market%20disruption.%20This%20index%20explains%20between%209%20and%2018%20percent%20of%20the%20variation%20in%20GenAI%20adoption%20and%20ranks%20among%20the%20strongest%20predictors%20for%20women%20across%20all%20age%20groups%2C%20surpassing%20digital%20literacy%20and%20education%20for%20young%20women.%20Intersectional%20analyses%20show%20that%20the%20largest%20disparities%20arise%20among%20younger%2C%20digitally%20fluent%20individuals%20with%20high%20societal%20risk%20concerns%2C%20where%20gender%20gaps%20in%20personal%20use%20exceed%2045%20percentage%20points.%20Using%20a%20synthetic%20twin%20panel%20design%2C%20we%20show%20that%20increased%20optimism%20about%20AI%27s%20societal%20impact%20raises%20GenAI%20use%20among%20young%20women%20from%2013%20percent%20to%2033%20percent%2C%20substantially%20narrowing%20the%20gender%20divide.%20These%20findings%20indicate%20that%20gendered%20perceptions%20of%20AI%27s%20social%20and%20ethical%20consequences%2C%20rather%20than%20access%20or%20capability%2C%20are%20the%20primary%20drivers%20of%20unequal%20GenAI%20adoption%2C%20with%20implications%20for%20productivity%2C%20skill%20formation%2C%20and%20economic%20inequality%20in%20an%20AI%20enabled%20economy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWomen%2520Worry%252C%2520Men%2520Adopt%253A%2520How%2520Gendered%2520Perceptions%2520Shape%2520the%2520Use%2520of%2520Generative%2520AI%26entry.906535625%3DFabian%2520Stephany%2520and%2520Jedrzej%2520Duszynski%26entry.1292438233%3DGenerative%2520artificial%2520intelligence%2520%2528GenAI%2529%2520is%2520diffusing%2520rapidly%252C%2520yet%2520its%2520adoption%2520is%2520strikingly%2520unequal.%2520Using%2520nationally%2520representative%2520UK%2520survey%2520data%2520from%25202023%2520to%25202024%252C%2520we%2520show%2520that%2520women%2520adopt%2520GenAI%2520substantially%2520less%2520often%2520than%2520men%2520because%2520they%2520perceive%2520its%2520societal%2520risks%2520differently.%2520We%2520construct%2520a%2520composite%2520index%2520capturing%2520concerns%2520about%2520mental%2520health%252C%2520privacy%252C%2520climate%2520impact%252C%2520and%2520labor%2520market%2520disruption.%2520This%2520index%2520explains%2520between%25209%2520and%252018%2520percent%2520of%2520the%2520variation%2520in%2520GenAI%2520adoption%2520and%2520ranks%2520among%2520the%2520strongest%2520predictors%2520for%2520women%2520across%2520all%2520age%2520groups%252C%2520surpassing%2520digital%2520literacy%2520and%2520education%2520for%2520young%2520women.%2520Intersectional%2520analyses%2520show%2520that%2520the%2520largest%2520disparities%2520arise%2520among%2520younger%252C%2520digitally%2520fluent%2520individuals%2520with%2520high%2520societal%2520risk%2520concerns%252C%2520where%2520gender%2520gaps%2520in%2520personal%2520use%2520exceed%252045%2520percentage%2520points.%2520Using%2520a%2520synthetic%2520twin%2520panel%2520design%252C%2520we%2520show%2520that%2520increased%2520optimism%2520about%2520AI%2527s%2520societal%2520impact%2520raises%2520GenAI%2520use%2520among%2520young%2520women%2520from%252013%2520percent%2520to%252033%2520percent%252C%2520substantially%2520narrowing%2520the%2520gender%2520divide.%2520These%2520findings%2520indicate%2520that%2520gendered%2520perceptions%2520of%2520AI%2527s%2520social%2520and%2520ethical%2520consequences%252C%2520rather%2520than%2520access%2520or%2520capability%252C%2520are%2520the%2520primary%2520drivers%2520of%2520unequal%2520GenAI%2520adoption%252C%2520with%2520implications%2520for%2520productivity%252C%2520skill%2520formation%252C%2520and%2520economic%2520inequality%2520in%2520an%2520AI%2520enabled%2520economy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Women%20Worry%2C%20Men%20Adopt%3A%20How%20Gendered%20Perceptions%20Shape%20the%20Use%20of%20Generative%20AI&entry.906535625=Fabian%20Stephany%20and%20Jedrzej%20Duszynski&entry.1292438233=Generative%20artificial%20intelligence%20%28GenAI%29%20is%20diffusing%20rapidly%2C%20yet%20its%20adoption%20is%20strikingly%20unequal.%20Using%20nationally%20representative%20UK%20survey%20data%20from%202023%20to%202024%2C%20we%20show%20that%20women%20adopt%20GenAI%20substantially%20less%20often%20than%20men%20because%20they%20perceive%20its%20societal%20risks%20differently.%20We%20construct%20a%20composite%20index%20capturing%20concerns%20about%20mental%20health%2C%20privacy%2C%20climate%20impact%2C%20and%20labor%20market%20disruption.%20This%20index%20explains%20between%209%20and%2018%20percent%20of%20the%20variation%20in%20GenAI%20adoption%20and%20ranks%20among%20the%20strongest%20predictors%20for%20women%20across%20all%20age%20groups%2C%20surpassing%20digital%20literacy%20and%20education%20for%20young%20women.%20Intersectional%20analyses%20show%20that%20the%20largest%20disparities%20arise%20among%20younger%2C%20digitally%20fluent%20individuals%20with%20high%20societal%20risk%20concerns%2C%20where%20gender%20gaps%20in%20personal%20use%20exceed%2045%20percentage%20points.%20Using%20a%20synthetic%20twin%20panel%20design%2C%20we%20show%20that%20increased%20optimism%20about%20AI%27s%20societal%20impact%20raises%20GenAI%20use%20among%20young%20women%20from%2013%20percent%20to%2033%20percent%2C%20substantially%20narrowing%20the%20gender%20divide.%20These%20findings%20indicate%20that%20gendered%20perceptions%20of%20AI%27s%20social%20and%20ethical%20consequences%2C%20rather%20than%20access%20or%20capability%2C%20are%20the%20primary%20drivers%20of%20unequal%20GenAI%20adoption%2C%20with%20implications%20for%20productivity%2C%20skill%20formation%2C%20and%20economic%20inequality%20in%20an%20AI%20enabled%20economy.&entry.1838667208=http%3A//arxiv.org/abs/2601.03880v1&entry.124074799=Read"},
{"title": "Attention Needs to Focus: A Unified Perspective on Attention Allocation", "author": "Zichuan Fu and Wentao Song and Guojing Li and Yejing Wang and Xian Wu and Yimin Deng and Hanyu Yan and Yefeng Zheng and Xiangyu Zhao", "abstract": "The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.", "link": "http://arxiv.org/abs/2601.00919v2", "date": "2026-01-07", "relevancy": 2.1721, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5776}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5402}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Needs%20to%20Focus%3A%20A%20Unified%20Perspective%20on%20Attention%20Allocation&body=Title%3A%20Attention%20Needs%20to%20Focus%3A%20A%20Unified%20Perspective%20on%20Attention%20Allocation%0AAuthor%3A%20Zichuan%20Fu%20and%20Wentao%20Song%20and%20Guojing%20Li%20and%20Yejing%20Wang%20and%20Xian%20Wu%20and%20Yimin%20Deng%20and%20Hanyu%20Yan%20and%20Yefeng%20Zheng%20and%20Xiangyu%20Zhao%0AAbstract%3A%20The%20Transformer%20architecture%2C%20a%20cornerstone%20of%20modern%20Large%20Language%20Models%20%28LLMs%29%2C%20has%20achieved%20extraordinary%20success%20in%20sequence%20modeling%2C%20primarily%20due%20to%20its%20attention%20mechanism.%20However%2C%20despite%20its%20power%2C%20the%20standard%20attention%20mechanism%20is%20plagued%20by%20well-documented%20issues%3A%20representational%20collapse%20and%20attention%20sink.%20Although%20prior%20work%20has%20proposed%20approaches%20for%20these%20issues%2C%20they%20are%20often%20studied%20in%20isolation%2C%20obscuring%20their%20deeper%20connection.%20In%20this%20paper%2C%20we%20present%20a%20unified%20perspective%2C%20arguing%20that%20both%20can%20be%20traced%20to%20a%20common%20root%20--%20improper%20attention%20allocation.%20We%20identify%20two%20failure%20modes%3A%201%29%20Attention%20Overload%2C%20where%20tokens%20receive%20comparable%20high%20weights%2C%20blurring%20semantic%20features%20that%20lead%20to%20representational%20collapse%3B%202%29%20Attention%20Underload%2C%20where%20no%20token%20is%20semantically%20relevant%2C%20yet%20attention%20is%20still%20forced%20to%20distribute%2C%20resulting%20in%20spurious%20focus%20such%20as%20attention%20sink.%20Building%20on%20this%20insight%2C%20we%20introduce%20Lazy%20Attention%2C%20a%20novel%20mechanism%20designed%20for%20a%20more%20focused%20attention%20distribution.%20To%20mitigate%20overload%2C%20it%20employs%20positional%20discrimination%20across%20both%20heads%20and%20dimensions%20to%20sharpen%20token%20distinctions.%20To%20counteract%20underload%2C%20it%20incorporates%20Elastic-Softmax%2C%20a%20modified%20normalization%20function%20that%20relaxes%20the%20standard%20softmax%20constraint%20to%20suppress%20attention%20on%20irrelevant%20tokens.%20Experiments%20on%20the%20FineWeb-Edu%20corpus%2C%20evaluated%20across%20nine%20diverse%20benchmarks%2C%20demonstrate%20that%20Lazy%20Attention%20successfully%20mitigates%20attention%20sink%20and%20achieves%20competitive%20performance%20compared%20to%20both%20standard%20attention%20and%20modern%20architectures%2C%20while%20reaching%20up%20to%2059.58%25%20attention%20sparsity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Needs%2520to%2520Focus%253A%2520A%2520Unified%2520Perspective%2520on%2520Attention%2520Allocation%26entry.906535625%3DZichuan%2520Fu%2520and%2520Wentao%2520Song%2520and%2520Guojing%2520Li%2520and%2520Yejing%2520Wang%2520and%2520Xian%2520Wu%2520and%2520Yimin%2520Deng%2520and%2520Hanyu%2520Yan%2520and%2520Yefeng%2520Zheng%2520and%2520Xiangyu%2520Zhao%26entry.1292438233%3DThe%2520Transformer%2520architecture%252C%2520a%2520cornerstone%2520of%2520modern%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520has%2520achieved%2520extraordinary%2520success%2520in%2520sequence%2520modeling%252C%2520primarily%2520due%2520to%2520its%2520attention%2520mechanism.%2520However%252C%2520despite%2520its%2520power%252C%2520the%2520standard%2520attention%2520mechanism%2520is%2520plagued%2520by%2520well-documented%2520issues%253A%2520representational%2520collapse%2520and%2520attention%2520sink.%2520Although%2520prior%2520work%2520has%2520proposed%2520approaches%2520for%2520these%2520issues%252C%2520they%2520are%2520often%2520studied%2520in%2520isolation%252C%2520obscuring%2520their%2520deeper%2520connection.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520unified%2520perspective%252C%2520arguing%2520that%2520both%2520can%2520be%2520traced%2520to%2520a%2520common%2520root%2520--%2520improper%2520attention%2520allocation.%2520We%2520identify%2520two%2520failure%2520modes%253A%25201%2529%2520Attention%2520Overload%252C%2520where%2520tokens%2520receive%2520comparable%2520high%2520weights%252C%2520blurring%2520semantic%2520features%2520that%2520lead%2520to%2520representational%2520collapse%253B%25202%2529%2520Attention%2520Underload%252C%2520where%2520no%2520token%2520is%2520semantically%2520relevant%252C%2520yet%2520attention%2520is%2520still%2520forced%2520to%2520distribute%252C%2520resulting%2520in%2520spurious%2520focus%2520such%2520as%2520attention%2520sink.%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520Lazy%2520Attention%252C%2520a%2520novel%2520mechanism%2520designed%2520for%2520a%2520more%2520focused%2520attention%2520distribution.%2520To%2520mitigate%2520overload%252C%2520it%2520employs%2520positional%2520discrimination%2520across%2520both%2520heads%2520and%2520dimensions%2520to%2520sharpen%2520token%2520distinctions.%2520To%2520counteract%2520underload%252C%2520it%2520incorporates%2520Elastic-Softmax%252C%2520a%2520modified%2520normalization%2520function%2520that%2520relaxes%2520the%2520standard%2520softmax%2520constraint%2520to%2520suppress%2520attention%2520on%2520irrelevant%2520tokens.%2520Experiments%2520on%2520the%2520FineWeb-Edu%2520corpus%252C%2520evaluated%2520across%2520nine%2520diverse%2520benchmarks%252C%2520demonstrate%2520that%2520Lazy%2520Attention%2520successfully%2520mitigates%2520attention%2520sink%2520and%2520achieves%2520competitive%2520performance%2520compared%2520to%2520both%2520standard%2520attention%2520and%2520modern%2520architectures%252C%2520while%2520reaching%2520up%2520to%252059.58%2525%2520attention%2520sparsity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Needs%20to%20Focus%3A%20A%20Unified%20Perspective%20on%20Attention%20Allocation&entry.906535625=Zichuan%20Fu%20and%20Wentao%20Song%20and%20Guojing%20Li%20and%20Yejing%20Wang%20and%20Xian%20Wu%20and%20Yimin%20Deng%20and%20Hanyu%20Yan%20and%20Yefeng%20Zheng%20and%20Xiangyu%20Zhao&entry.1292438233=The%20Transformer%20architecture%2C%20a%20cornerstone%20of%20modern%20Large%20Language%20Models%20%28LLMs%29%2C%20has%20achieved%20extraordinary%20success%20in%20sequence%20modeling%2C%20primarily%20due%20to%20its%20attention%20mechanism.%20However%2C%20despite%20its%20power%2C%20the%20standard%20attention%20mechanism%20is%20plagued%20by%20well-documented%20issues%3A%20representational%20collapse%20and%20attention%20sink.%20Although%20prior%20work%20has%20proposed%20approaches%20for%20these%20issues%2C%20they%20are%20often%20studied%20in%20isolation%2C%20obscuring%20their%20deeper%20connection.%20In%20this%20paper%2C%20we%20present%20a%20unified%20perspective%2C%20arguing%20that%20both%20can%20be%20traced%20to%20a%20common%20root%20--%20improper%20attention%20allocation.%20We%20identify%20two%20failure%20modes%3A%201%29%20Attention%20Overload%2C%20where%20tokens%20receive%20comparable%20high%20weights%2C%20blurring%20semantic%20features%20that%20lead%20to%20representational%20collapse%3B%202%29%20Attention%20Underload%2C%20where%20no%20token%20is%20semantically%20relevant%2C%20yet%20attention%20is%20still%20forced%20to%20distribute%2C%20resulting%20in%20spurious%20focus%20such%20as%20attention%20sink.%20Building%20on%20this%20insight%2C%20we%20introduce%20Lazy%20Attention%2C%20a%20novel%20mechanism%20designed%20for%20a%20more%20focused%20attention%20distribution.%20To%20mitigate%20overload%2C%20it%20employs%20positional%20discrimination%20across%20both%20heads%20and%20dimensions%20to%20sharpen%20token%20distinctions.%20To%20counteract%20underload%2C%20it%20incorporates%20Elastic-Softmax%2C%20a%20modified%20normalization%20function%20that%20relaxes%20the%20standard%20softmax%20constraint%20to%20suppress%20attention%20on%20irrelevant%20tokens.%20Experiments%20on%20the%20FineWeb-Edu%20corpus%2C%20evaluated%20across%20nine%20diverse%20benchmarks%2C%20demonstrate%20that%20Lazy%20Attention%20successfully%20mitigates%20attention%20sink%20and%20achieves%20competitive%20performance%20compared%20to%20both%20standard%20attention%20and%20modern%20architectures%2C%20while%20reaching%20up%20to%2059.58%25%20attention%20sparsity.&entry.1838667208=http%3A//arxiv.org/abs/2601.00919v2&entry.124074799=Read"},
{"title": "Where meaning lives: Layer-wise accessibility of psycholinguistic features in encoder and decoder language models", "author": "Taisiia Tikhomirova and Dirk U. Wulff", "abstract": "Understanding where transformer language models encode psychologically meaningful aspects of meaning is essential for both theory and practice. We conduct a systematic layer-wise probing study of 58 psycholinguistic features across 10 transformer models, spanning encoder-only and decoder-only architectures, and compare three embedding extraction methods. We find that apparent localization of meaning is strongly method-dependent: contextualized embeddings yield higher feature-specific selectivity and different layer-wise profiles than isolated embeddings. Across models and methods, final-layer representations are rarely optimal for recovering psycholinguistic information with linear probes. Despite these differences, models exhibit a shared depth ordering of meaning dimensions, with lexical properties peaking earlier and experiential and affective dimensions peaking later. Together, these results show that where meaning \"lives\" in transformer models reflects an interaction between methodological choices and architectural constraints.", "link": "http://arxiv.org/abs/2601.03798v1", "date": "2026-01-07", "relevancy": 2.1582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20meaning%20lives%3A%20Layer-wise%20accessibility%20of%20psycholinguistic%20features%20in%20encoder%20and%20decoder%20language%20models&body=Title%3A%20Where%20meaning%20lives%3A%20Layer-wise%20accessibility%20of%20psycholinguistic%20features%20in%20encoder%20and%20decoder%20language%20models%0AAuthor%3A%20Taisiia%20Tikhomirova%20and%20Dirk%20U.%20Wulff%0AAbstract%3A%20Understanding%20where%20transformer%20language%20models%20encode%20psychologically%20meaningful%20aspects%20of%20meaning%20is%20essential%20for%20both%20theory%20and%20practice.%20We%20conduct%20a%20systematic%20layer-wise%20probing%20study%20of%2058%20psycholinguistic%20features%20across%2010%20transformer%20models%2C%20spanning%20encoder-only%20and%20decoder-only%20architectures%2C%20and%20compare%20three%20embedding%20extraction%20methods.%20We%20find%20that%20apparent%20localization%20of%20meaning%20is%20strongly%20method-dependent%3A%20contextualized%20embeddings%20yield%20higher%20feature-specific%20selectivity%20and%20different%20layer-wise%20profiles%20than%20isolated%20embeddings.%20Across%20models%20and%20methods%2C%20final-layer%20representations%20are%20rarely%20optimal%20for%20recovering%20psycholinguistic%20information%20with%20linear%20probes.%20Despite%20these%20differences%2C%20models%20exhibit%20a%20shared%20depth%20ordering%20of%20meaning%20dimensions%2C%20with%20lexical%20properties%20peaking%20earlier%20and%20experiential%20and%20affective%20dimensions%20peaking%20later.%20Together%2C%20these%20results%20show%20that%20where%20meaning%20%22lives%22%20in%20transformer%20models%20reflects%20an%20interaction%20between%20methodological%20choices%20and%20architectural%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520meaning%2520lives%253A%2520Layer-wise%2520accessibility%2520of%2520psycholinguistic%2520features%2520in%2520encoder%2520and%2520decoder%2520language%2520models%26entry.906535625%3DTaisiia%2520Tikhomirova%2520and%2520Dirk%2520U.%2520Wulff%26entry.1292438233%3DUnderstanding%2520where%2520transformer%2520language%2520models%2520encode%2520psychologically%2520meaningful%2520aspects%2520of%2520meaning%2520is%2520essential%2520for%2520both%2520theory%2520and%2520practice.%2520We%2520conduct%2520a%2520systematic%2520layer-wise%2520probing%2520study%2520of%252058%2520psycholinguistic%2520features%2520across%252010%2520transformer%2520models%252C%2520spanning%2520encoder-only%2520and%2520decoder-only%2520architectures%252C%2520and%2520compare%2520three%2520embedding%2520extraction%2520methods.%2520We%2520find%2520that%2520apparent%2520localization%2520of%2520meaning%2520is%2520strongly%2520method-dependent%253A%2520contextualized%2520embeddings%2520yield%2520higher%2520feature-specific%2520selectivity%2520and%2520different%2520layer-wise%2520profiles%2520than%2520isolated%2520embeddings.%2520Across%2520models%2520and%2520methods%252C%2520final-layer%2520representations%2520are%2520rarely%2520optimal%2520for%2520recovering%2520psycholinguistic%2520information%2520with%2520linear%2520probes.%2520Despite%2520these%2520differences%252C%2520models%2520exhibit%2520a%2520shared%2520depth%2520ordering%2520of%2520meaning%2520dimensions%252C%2520with%2520lexical%2520properties%2520peaking%2520earlier%2520and%2520experiential%2520and%2520affective%2520dimensions%2520peaking%2520later.%2520Together%252C%2520these%2520results%2520show%2520that%2520where%2520meaning%2520%2522lives%2522%2520in%2520transformer%2520models%2520reflects%2520an%2520interaction%2520between%2520methodological%2520choices%2520and%2520architectural%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20meaning%20lives%3A%20Layer-wise%20accessibility%20of%20psycholinguistic%20features%20in%20encoder%20and%20decoder%20language%20models&entry.906535625=Taisiia%20Tikhomirova%20and%20Dirk%20U.%20Wulff&entry.1292438233=Understanding%20where%20transformer%20language%20models%20encode%20psychologically%20meaningful%20aspects%20of%20meaning%20is%20essential%20for%20both%20theory%20and%20practice.%20We%20conduct%20a%20systematic%20layer-wise%20probing%20study%20of%2058%20psycholinguistic%20features%20across%2010%20transformer%20models%2C%20spanning%20encoder-only%20and%20decoder-only%20architectures%2C%20and%20compare%20three%20embedding%20extraction%20methods.%20We%20find%20that%20apparent%20localization%20of%20meaning%20is%20strongly%20method-dependent%3A%20contextualized%20embeddings%20yield%20higher%20feature-specific%20selectivity%20and%20different%20layer-wise%20profiles%20than%20isolated%20embeddings.%20Across%20models%20and%20methods%2C%20final-layer%20representations%20are%20rarely%20optimal%20for%20recovering%20psycholinguistic%20information%20with%20linear%20probes.%20Despite%20these%20differences%2C%20models%20exhibit%20a%20shared%20depth%20ordering%20of%20meaning%20dimensions%2C%20with%20lexical%20properties%20peaking%20earlier%20and%20experiential%20and%20affective%20dimensions%20peaking%20later.%20Together%2C%20these%20results%20show%20that%20where%20meaning%20%22lives%22%20in%20transformer%20models%20reflects%20an%20interaction%20between%20methodological%20choices%20and%20architectural%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2601.03798v1&entry.124074799=Read"},
{"title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning", "author": "Jiawei Chen and Xintian Shen and Lihao Zheng and Zhenwei Shao and Handong Cui and Chaoqun Du and Li Gong and Feng Gu and Xuefeng Hao and Wei He and Jiabang He and Yi Hu and Bin Huang and Shanshan Li and Qizhen Li and Jing Luo and Zide Liu and Xiaobo Liu and Ning Mao and Lifu Mu and Xuhao Pan and Zhiheng Qu and Chang Ren and Xudong Rao and Haoyi Sun and Qian Wang and Shuai Wang and Zhichao Wang and Wei Wang and Lian Wen and Jiqing Zhan and Hongfu Yang and Sheng Yang and Jiajun Yang and Pengfei Yu and Hongyuan Zhang and Bin Zhang and Chunpeng Zhou and Zheng Zhou and Shucheng Zhou and Shuo Xie and Yun Zhu and Hao Ma and Tao Wei and Pan Zhou and Wei Chen", "abstract": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.", "link": "http://arxiv.org/abs/2512.23412v2", "date": "2026-01-07", "relevancy": 2.1561, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5412}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MindWatcher%3A%20Toward%20Smarter%20Multimodal%20Tool-Integrated%20Reasoning&body=Title%3A%20MindWatcher%3A%20Toward%20Smarter%20Multimodal%20Tool-Integrated%20Reasoning%0AAuthor%3A%20Jiawei%20Chen%20and%20Xintian%20Shen%20and%20Lihao%20Zheng%20and%20Zhenwei%20Shao%20and%20Handong%20Cui%20and%20Chaoqun%20Du%20and%20Li%20Gong%20and%20Feng%20Gu%20and%20Xuefeng%20Hao%20and%20Wei%20He%20and%20Jiabang%20He%20and%20Yi%20Hu%20and%20Bin%20Huang%20and%20Shanshan%20Li%20and%20Qizhen%20Li%20and%20Jing%20Luo%20and%20Zide%20Liu%20and%20Xiaobo%20Liu%20and%20Ning%20Mao%20and%20Lifu%20Mu%20and%20Xuhao%20Pan%20and%20Zhiheng%20Qu%20and%20Chang%20Ren%20and%20Xudong%20Rao%20and%20Haoyi%20Sun%20and%20Qian%20Wang%20and%20Shuai%20Wang%20and%20Zhichao%20Wang%20and%20Wei%20Wang%20and%20Lian%20Wen%20and%20Jiqing%20Zhan%20and%20Hongfu%20Yang%20and%20Sheng%20Yang%20and%20Jiajun%20Yang%20and%20Pengfei%20Yu%20and%20Hongyuan%20Zhang%20and%20Bin%20Zhang%20and%20Chunpeng%20Zhou%20and%20Zheng%20Zhou%20and%20Shucheng%20Zhou%20and%20Shuo%20Xie%20and%20Yun%20Zhu%20and%20Hao%20Ma%20and%20Tao%20Wei%20and%20Pan%20Zhou%20and%20Wei%20Chen%0AAbstract%3A%20Traditional%20workflow-based%20agents%20exhibit%20limited%20intelligence%20when%20addressing%20real-world%20problems%20requiring%20tool%20invocation.%20Tool-integrated%20reasoning%20%28TIR%29%20agents%20capable%20of%20autonomous%20reasoning%20and%20tool%20invocation%20are%20rapidly%20emerging%20as%20a%20powerful%20approach%20for%20complex%20decision-making%20tasks%20involving%20multi-step%20interactions%20with%20external%20environments.%20In%20this%20work%2C%20we%20introduce%20MindWatcher%2C%20a%20TIR%20agent%20integrating%20interleaved%20thinking%20and%20multimodal%20chain-of-thought%20%28CoT%29%20reasoning.%20MindWatcher%20can%20autonomously%20decide%20whether%20and%20how%20to%20invoke%20diverse%20tools%20and%20coordinate%20their%20use%2C%20without%20relying%20on%20human%20prompts%20or%20workflows.%20The%20interleaved%20thinking%20paradigm%20enables%20the%20model%20to%20switch%20between%20thinking%20and%20tool%20calling%20at%20any%20intermediate%20stage%2C%20while%20its%20multimodal%20CoT%20capability%20allows%20manipulation%20of%20images%20during%20reasoning%20to%20yield%20more%20precise%20search%20results.%20We%20implement%20automated%20data%20auditing%20and%20evaluation%20pipelines%2C%20complemented%20by%20manually%20curated%20high-quality%20datasets%20for%20training%2C%20and%20we%20construct%20a%20benchmark%2C%20called%20MindWatcher-Evaluate%20Bench%20%28MWE-Bench%29%2C%20to%20evaluate%20its%20performance.%20MindWatcher%20is%20equipped%20with%20a%20comprehensive%20suite%20of%20auxiliary%20reasoning%20tools%2C%20enabling%20it%20to%20address%20broad-domain%20multimodal%20problems.%20A%20large-scale%2C%20high-quality%20local%20image%20retrieval%20database%2C%20covering%20eight%20categories%20including%20cars%2C%20animals%2C%20and%20plants%2C%20endows%20model%20with%20robust%20object%20recognition%20despite%20its%20small%20size.%20Finally%2C%20we%20design%20a%20more%20efficient%20training%20infrastructure%20for%20MindWatcher%2C%20enhancing%20training%20speed%20and%20hardware%20utilization.%20Experiments%20not%20only%20demonstrate%20that%20MindWatcher%20matches%20or%20exceeds%20the%20performance%20of%20larger%20or%20more%20recent%20models%20through%20superior%20tool%20invocation%2C%20but%20also%20uncover%20critical%20insights%20for%20agent%20training%2C%20such%20as%20the%20genetic%20inheritance%20phenomenon%20in%20agentic%20RL.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMindWatcher%253A%2520Toward%2520Smarter%2520Multimodal%2520Tool-Integrated%2520Reasoning%26entry.906535625%3DJiawei%2520Chen%2520and%2520Xintian%2520Shen%2520and%2520Lihao%2520Zheng%2520and%2520Zhenwei%2520Shao%2520and%2520Handong%2520Cui%2520and%2520Chaoqun%2520Du%2520and%2520Li%2520Gong%2520and%2520Feng%2520Gu%2520and%2520Xuefeng%2520Hao%2520and%2520Wei%2520He%2520and%2520Jiabang%2520He%2520and%2520Yi%2520Hu%2520and%2520Bin%2520Huang%2520and%2520Shanshan%2520Li%2520and%2520Qizhen%2520Li%2520and%2520Jing%2520Luo%2520and%2520Zide%2520Liu%2520and%2520Xiaobo%2520Liu%2520and%2520Ning%2520Mao%2520and%2520Lifu%2520Mu%2520and%2520Xuhao%2520Pan%2520and%2520Zhiheng%2520Qu%2520and%2520Chang%2520Ren%2520and%2520Xudong%2520Rao%2520and%2520Haoyi%2520Sun%2520and%2520Qian%2520Wang%2520and%2520Shuai%2520Wang%2520and%2520Zhichao%2520Wang%2520and%2520Wei%2520Wang%2520and%2520Lian%2520Wen%2520and%2520Jiqing%2520Zhan%2520and%2520Hongfu%2520Yang%2520and%2520Sheng%2520Yang%2520and%2520Jiajun%2520Yang%2520and%2520Pengfei%2520Yu%2520and%2520Hongyuan%2520Zhang%2520and%2520Bin%2520Zhang%2520and%2520Chunpeng%2520Zhou%2520and%2520Zheng%2520Zhou%2520and%2520Shucheng%2520Zhou%2520and%2520Shuo%2520Xie%2520and%2520Yun%2520Zhu%2520and%2520Hao%2520Ma%2520and%2520Tao%2520Wei%2520and%2520Pan%2520Zhou%2520and%2520Wei%2520Chen%26entry.1292438233%3DTraditional%2520workflow-based%2520agents%2520exhibit%2520limited%2520intelligence%2520when%2520addressing%2520real-world%2520problems%2520requiring%2520tool%2520invocation.%2520Tool-integrated%2520reasoning%2520%2528TIR%2529%2520agents%2520capable%2520of%2520autonomous%2520reasoning%2520and%2520tool%2520invocation%2520are%2520rapidly%2520emerging%2520as%2520a%2520powerful%2520approach%2520for%2520complex%2520decision-making%2520tasks%2520involving%2520multi-step%2520interactions%2520with%2520external%2520environments.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MindWatcher%252C%2520a%2520TIR%2520agent%2520integrating%2520interleaved%2520thinking%2520and%2520multimodal%2520chain-of-thought%2520%2528CoT%2529%2520reasoning.%2520MindWatcher%2520can%2520autonomously%2520decide%2520whether%2520and%2520how%2520to%2520invoke%2520diverse%2520tools%2520and%2520coordinate%2520their%2520use%252C%2520without%2520relying%2520on%2520human%2520prompts%2520or%2520workflows.%2520The%2520interleaved%2520thinking%2520paradigm%2520enables%2520the%2520model%2520to%2520switch%2520between%2520thinking%2520and%2520tool%2520calling%2520at%2520any%2520intermediate%2520stage%252C%2520while%2520its%2520multimodal%2520CoT%2520capability%2520allows%2520manipulation%2520of%2520images%2520during%2520reasoning%2520to%2520yield%2520more%2520precise%2520search%2520results.%2520We%2520implement%2520automated%2520data%2520auditing%2520and%2520evaluation%2520pipelines%252C%2520complemented%2520by%2520manually%2520curated%2520high-quality%2520datasets%2520for%2520training%252C%2520and%2520we%2520construct%2520a%2520benchmark%252C%2520called%2520MindWatcher-Evaluate%2520Bench%2520%2528MWE-Bench%2529%252C%2520to%2520evaluate%2520its%2520performance.%2520MindWatcher%2520is%2520equipped%2520with%2520a%2520comprehensive%2520suite%2520of%2520auxiliary%2520reasoning%2520tools%252C%2520enabling%2520it%2520to%2520address%2520broad-domain%2520multimodal%2520problems.%2520A%2520large-scale%252C%2520high-quality%2520local%2520image%2520retrieval%2520database%252C%2520covering%2520eight%2520categories%2520including%2520cars%252C%2520animals%252C%2520and%2520plants%252C%2520endows%2520model%2520with%2520robust%2520object%2520recognition%2520despite%2520its%2520small%2520size.%2520Finally%252C%2520we%2520design%2520a%2520more%2520efficient%2520training%2520infrastructure%2520for%2520MindWatcher%252C%2520enhancing%2520training%2520speed%2520and%2520hardware%2520utilization.%2520Experiments%2520not%2520only%2520demonstrate%2520that%2520MindWatcher%2520matches%2520or%2520exceeds%2520the%2520performance%2520of%2520larger%2520or%2520more%2520recent%2520models%2520through%2520superior%2520tool%2520invocation%252C%2520but%2520also%2520uncover%2520critical%2520insights%2520for%2520agent%2520training%252C%2520such%2520as%2520the%2520genetic%2520inheritance%2520phenomenon%2520in%2520agentic%2520RL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MindWatcher%3A%20Toward%20Smarter%20Multimodal%20Tool-Integrated%20Reasoning&entry.906535625=Jiawei%20Chen%20and%20Xintian%20Shen%20and%20Lihao%20Zheng%20and%20Zhenwei%20Shao%20and%20Handong%20Cui%20and%20Chaoqun%20Du%20and%20Li%20Gong%20and%20Feng%20Gu%20and%20Xuefeng%20Hao%20and%20Wei%20He%20and%20Jiabang%20He%20and%20Yi%20Hu%20and%20Bin%20Huang%20and%20Shanshan%20Li%20and%20Qizhen%20Li%20and%20Jing%20Luo%20and%20Zide%20Liu%20and%20Xiaobo%20Liu%20and%20Ning%20Mao%20and%20Lifu%20Mu%20and%20Xuhao%20Pan%20and%20Zhiheng%20Qu%20and%20Chang%20Ren%20and%20Xudong%20Rao%20and%20Haoyi%20Sun%20and%20Qian%20Wang%20and%20Shuai%20Wang%20and%20Zhichao%20Wang%20and%20Wei%20Wang%20and%20Lian%20Wen%20and%20Jiqing%20Zhan%20and%20Hongfu%20Yang%20and%20Sheng%20Yang%20and%20Jiajun%20Yang%20and%20Pengfei%20Yu%20and%20Hongyuan%20Zhang%20and%20Bin%20Zhang%20and%20Chunpeng%20Zhou%20and%20Zheng%20Zhou%20and%20Shucheng%20Zhou%20and%20Shuo%20Xie%20and%20Yun%20Zhu%20and%20Hao%20Ma%20and%20Tao%20Wei%20and%20Pan%20Zhou%20and%20Wei%20Chen&entry.1292438233=Traditional%20workflow-based%20agents%20exhibit%20limited%20intelligence%20when%20addressing%20real-world%20problems%20requiring%20tool%20invocation.%20Tool-integrated%20reasoning%20%28TIR%29%20agents%20capable%20of%20autonomous%20reasoning%20and%20tool%20invocation%20are%20rapidly%20emerging%20as%20a%20powerful%20approach%20for%20complex%20decision-making%20tasks%20involving%20multi-step%20interactions%20with%20external%20environments.%20In%20this%20work%2C%20we%20introduce%20MindWatcher%2C%20a%20TIR%20agent%20integrating%20interleaved%20thinking%20and%20multimodal%20chain-of-thought%20%28CoT%29%20reasoning.%20MindWatcher%20can%20autonomously%20decide%20whether%20and%20how%20to%20invoke%20diverse%20tools%20and%20coordinate%20their%20use%2C%20without%20relying%20on%20human%20prompts%20or%20workflows.%20The%20interleaved%20thinking%20paradigm%20enables%20the%20model%20to%20switch%20between%20thinking%20and%20tool%20calling%20at%20any%20intermediate%20stage%2C%20while%20its%20multimodal%20CoT%20capability%20allows%20manipulation%20of%20images%20during%20reasoning%20to%20yield%20more%20precise%20search%20results.%20We%20implement%20automated%20data%20auditing%20and%20evaluation%20pipelines%2C%20complemented%20by%20manually%20curated%20high-quality%20datasets%20for%20training%2C%20and%20we%20construct%20a%20benchmark%2C%20called%20MindWatcher-Evaluate%20Bench%20%28MWE-Bench%29%2C%20to%20evaluate%20its%20performance.%20MindWatcher%20is%20equipped%20with%20a%20comprehensive%20suite%20of%20auxiliary%20reasoning%20tools%2C%20enabling%20it%20to%20address%20broad-domain%20multimodal%20problems.%20A%20large-scale%2C%20high-quality%20local%20image%20retrieval%20database%2C%20covering%20eight%20categories%20including%20cars%2C%20animals%2C%20and%20plants%2C%20endows%20model%20with%20robust%20object%20recognition%20despite%20its%20small%20size.%20Finally%2C%20we%20design%20a%20more%20efficient%20training%20infrastructure%20for%20MindWatcher%2C%20enhancing%20training%20speed%20and%20hardware%20utilization.%20Experiments%20not%20only%20demonstrate%20that%20MindWatcher%20matches%20or%20exceeds%20the%20performance%20of%20larger%20or%20more%20recent%20models%20through%20superior%20tool%20invocation%2C%20but%20also%20uncover%20critical%20insights%20for%20agent%20training%2C%20such%20as%20the%20genetic%20inheritance%20phenomenon%20in%20agentic%20RL.&entry.1838667208=http%3A//arxiv.org/abs/2512.23412v2&entry.124074799=Read"},
{"title": "Hierarchical GNN-Based Multi-Agent Learning for Dynamic Queue-Jump Lane and Emergency Vehicle Corridor Formation", "author": "Haoran Su", "abstract": "Emergency vehicles require rapid passage through congested traffic, yet existing strategies fail to adapt to dynamic conditions. We propose a novel hierarchical graph neural network (GNN)-based multi-agent reinforcement learning framework to coordinate connected vehicles for emergency corridor formation. Our approach uses a high-level planner for global strategy and low-level controllers for trajectory execution, utilizing graph attention networks to scale with variable agent counts. Trained via Multi-Agent Proximal Policy Optimization (MAPPO), the system reduces emergency vehicle travel time by 28.3% compared to baselines and 44.6% compared to uncoordinated traffic in simulations. The design achieves near-zero collision rates (0.3%) while maintaining 81% of background traffic efficiency. Ablation and generalization studies confirm the framework's robustness across diverse scenarios. These results demonstrate the effectiveness of combining GNNs with hierarchical learning for intelligent transportation systems.", "link": "http://arxiv.org/abs/2601.04177v1", "date": "2026-01-07", "relevancy": 2.1176, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5466}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20GNN-Based%20Multi-Agent%20Learning%20for%20Dynamic%20Queue-Jump%20Lane%20and%20Emergency%20Vehicle%20Corridor%20Formation&body=Title%3A%20Hierarchical%20GNN-Based%20Multi-Agent%20Learning%20for%20Dynamic%20Queue-Jump%20Lane%20and%20Emergency%20Vehicle%20Corridor%20Formation%0AAuthor%3A%20Haoran%20Su%0AAbstract%3A%20Emergency%20vehicles%20require%20rapid%20passage%20through%20congested%20traffic%2C%20yet%20existing%20strategies%20fail%20to%20adapt%20to%20dynamic%20conditions.%20We%20propose%20a%20novel%20hierarchical%20graph%20neural%20network%20%28GNN%29-based%20multi-agent%20reinforcement%20learning%20framework%20to%20coordinate%20connected%20vehicles%20for%20emergency%20corridor%20formation.%20Our%20approach%20uses%20a%20high-level%20planner%20for%20global%20strategy%20and%20low-level%20controllers%20for%20trajectory%20execution%2C%20utilizing%20graph%20attention%20networks%20to%20scale%20with%20variable%20agent%20counts.%20Trained%20via%20Multi-Agent%20Proximal%20Policy%20Optimization%20%28MAPPO%29%2C%20the%20system%20reduces%20emergency%20vehicle%20travel%20time%20by%2028.3%25%20compared%20to%20baselines%20and%2044.6%25%20compared%20to%20uncoordinated%20traffic%20in%20simulations.%20The%20design%20achieves%20near-zero%20collision%20rates%20%280.3%25%29%20while%20maintaining%2081%25%20of%20background%20traffic%20efficiency.%20Ablation%20and%20generalization%20studies%20confirm%20the%20framework%27s%20robustness%20across%20diverse%20scenarios.%20These%20results%20demonstrate%20the%20effectiveness%20of%20combining%20GNNs%20with%20hierarchical%20learning%20for%20intelligent%20transportation%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520GNN-Based%2520Multi-Agent%2520Learning%2520for%2520Dynamic%2520Queue-Jump%2520Lane%2520and%2520Emergency%2520Vehicle%2520Corridor%2520Formation%26entry.906535625%3DHaoran%2520Su%26entry.1292438233%3DEmergency%2520vehicles%2520require%2520rapid%2520passage%2520through%2520congested%2520traffic%252C%2520yet%2520existing%2520strategies%2520fail%2520to%2520adapt%2520to%2520dynamic%2520conditions.%2520We%2520propose%2520a%2520novel%2520hierarchical%2520graph%2520neural%2520network%2520%2528GNN%2529-based%2520multi-agent%2520reinforcement%2520learning%2520framework%2520to%2520coordinate%2520connected%2520vehicles%2520for%2520emergency%2520corridor%2520formation.%2520Our%2520approach%2520uses%2520a%2520high-level%2520planner%2520for%2520global%2520strategy%2520and%2520low-level%2520controllers%2520for%2520trajectory%2520execution%252C%2520utilizing%2520graph%2520attention%2520networks%2520to%2520scale%2520with%2520variable%2520agent%2520counts.%2520Trained%2520via%2520Multi-Agent%2520Proximal%2520Policy%2520Optimization%2520%2528MAPPO%2529%252C%2520the%2520system%2520reduces%2520emergency%2520vehicle%2520travel%2520time%2520by%252028.3%2525%2520compared%2520to%2520baselines%2520and%252044.6%2525%2520compared%2520to%2520uncoordinated%2520traffic%2520in%2520simulations.%2520The%2520design%2520achieves%2520near-zero%2520collision%2520rates%2520%25280.3%2525%2529%2520while%2520maintaining%252081%2525%2520of%2520background%2520traffic%2520efficiency.%2520Ablation%2520and%2520generalization%2520studies%2520confirm%2520the%2520framework%2527s%2520robustness%2520across%2520diverse%2520scenarios.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520combining%2520GNNs%2520with%2520hierarchical%2520learning%2520for%2520intelligent%2520transportation%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20GNN-Based%20Multi-Agent%20Learning%20for%20Dynamic%20Queue-Jump%20Lane%20and%20Emergency%20Vehicle%20Corridor%20Formation&entry.906535625=Haoran%20Su&entry.1292438233=Emergency%20vehicles%20require%20rapid%20passage%20through%20congested%20traffic%2C%20yet%20existing%20strategies%20fail%20to%20adapt%20to%20dynamic%20conditions.%20We%20propose%20a%20novel%20hierarchical%20graph%20neural%20network%20%28GNN%29-based%20multi-agent%20reinforcement%20learning%20framework%20to%20coordinate%20connected%20vehicles%20for%20emergency%20corridor%20formation.%20Our%20approach%20uses%20a%20high-level%20planner%20for%20global%20strategy%20and%20low-level%20controllers%20for%20trajectory%20execution%2C%20utilizing%20graph%20attention%20networks%20to%20scale%20with%20variable%20agent%20counts.%20Trained%20via%20Multi-Agent%20Proximal%20Policy%20Optimization%20%28MAPPO%29%2C%20the%20system%20reduces%20emergency%20vehicle%20travel%20time%20by%2028.3%25%20compared%20to%20baselines%20and%2044.6%25%20compared%20to%20uncoordinated%20traffic%20in%20simulations.%20The%20design%20achieves%20near-zero%20collision%20rates%20%280.3%25%29%20while%20maintaining%2081%25%20of%20background%20traffic%20efficiency.%20Ablation%20and%20generalization%20studies%20confirm%20the%20framework%27s%20robustness%20across%20diverse%20scenarios.%20These%20results%20demonstrate%20the%20effectiveness%20of%20combining%20GNNs%20with%20hierarchical%20learning%20for%20intelligent%20transportation%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.04177v1&entry.124074799=Read"},
{"title": "FedDUAL: A Dual-Strategy with Adaptive Loss and Dynamic Aggregation for Mitigating Data Heterogeneity in Federated Learning", "author": "Pranab Sahoo and Ashutosh Tripathi and Sriparna Saha and Samrat Mondal", "abstract": "Federated Learning (FL) marks a transformative approach to distributed model training by combining locally optimized models from various clients into a unified global model. While FL preserves data privacy by eliminating centralized storage, it encounters significant challenges such as performance degradation, slower convergence, and reduced robustness of the global model due to the heterogeneity in client data distributions. Among the various forms of data heterogeneity, label skew emerges as a particularly formidable and prevalent issue, especially in domains such as image classification. To address these challenges, we begin with comprehensive experiments to pinpoint the underlying issues in the FL training process. Based on our findings, we then introduce an innovative dual-strategy approach designed to effectively resolve these issues. First, we introduce an adaptive loss function for client-side training, meticulously crafted to preserve previously acquired knowledge while maintaining an optimal equilibrium between local optimization and global model coherence. Secondly, we develop a dynamic aggregation strategy for aggregating client models at the server. This approach adapts to each client's unique learning patterns, effectively addressing the challenges of diverse data across the network. Our comprehensive evaluation, conducted across three diverse real-world datasets, coupled with theoretical convergence guarantees, demonstrates the superior efficacy of our method compared to several established state-of-the-art approaches.", "link": "http://arxiv.org/abs/2412.04416v2", "date": "2026-01-07", "relevancy": 2.0971, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5294}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5233}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedDUAL%3A%20A%20Dual-Strategy%20with%20Adaptive%20Loss%20and%20Dynamic%20Aggregation%20for%20Mitigating%20Data%20Heterogeneity%20in%20Federated%20Learning&body=Title%3A%20FedDUAL%3A%20A%20Dual-Strategy%20with%20Adaptive%20Loss%20and%20Dynamic%20Aggregation%20for%20Mitigating%20Data%20Heterogeneity%20in%20Federated%20Learning%0AAuthor%3A%20Pranab%20Sahoo%20and%20Ashutosh%20Tripathi%20and%20Sriparna%20Saha%20and%20Samrat%20Mondal%0AAbstract%3A%20Federated%20Learning%20%28FL%29%20marks%20a%20transformative%20approach%20to%20distributed%20model%20training%20by%20combining%20locally%20optimized%20models%20from%20various%20clients%20into%20a%20unified%20global%20model.%20While%20FL%20preserves%20data%20privacy%20by%20eliminating%20centralized%20storage%2C%20it%20encounters%20significant%20challenges%20such%20as%20performance%20degradation%2C%20slower%20convergence%2C%20and%20reduced%20robustness%20of%20the%20global%20model%20due%20to%20the%20heterogeneity%20in%20client%20data%20distributions.%20Among%20the%20various%20forms%20of%20data%20heterogeneity%2C%20label%20skew%20emerges%20as%20a%20particularly%20formidable%20and%20prevalent%20issue%2C%20especially%20in%20domains%20such%20as%20image%20classification.%20To%20address%20these%20challenges%2C%20we%20begin%20with%20comprehensive%20experiments%20to%20pinpoint%20the%20underlying%20issues%20in%20the%20FL%20training%20process.%20Based%20on%20our%20findings%2C%20we%20then%20introduce%20an%20innovative%20dual-strategy%20approach%20designed%20to%20effectively%20resolve%20these%20issues.%20First%2C%20we%20introduce%20an%20adaptive%20loss%20function%20for%20client-side%20training%2C%20meticulously%20crafted%20to%20preserve%20previously%20acquired%20knowledge%20while%20maintaining%20an%20optimal%20equilibrium%20between%20local%20optimization%20and%20global%20model%20coherence.%20Secondly%2C%20we%20develop%20a%20dynamic%20aggregation%20strategy%20for%20aggregating%20client%20models%20at%20the%20server.%20This%20approach%20adapts%20to%20each%20client%27s%20unique%20learning%20patterns%2C%20effectively%20addressing%20the%20challenges%20of%20diverse%20data%20across%20the%20network.%20Our%20comprehensive%20evaluation%2C%20conducted%20across%20three%20diverse%20real-world%20datasets%2C%20coupled%20with%20theoretical%20convergence%20guarantees%2C%20demonstrates%20the%20superior%20efficacy%20of%20our%20method%20compared%20to%20several%20established%20state-of-the-art%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2412.04416v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedDUAL%253A%2520A%2520Dual-Strategy%2520with%2520Adaptive%2520Loss%2520and%2520Dynamic%2520Aggregation%2520for%2520Mitigating%2520Data%2520Heterogeneity%2520in%2520Federated%2520Learning%26entry.906535625%3DPranab%2520Sahoo%2520and%2520Ashutosh%2520Tripathi%2520and%2520Sriparna%2520Saha%2520and%2520Samrat%2520Mondal%26entry.1292438233%3DFederated%2520Learning%2520%2528FL%2529%2520marks%2520a%2520transformative%2520approach%2520to%2520distributed%2520model%2520training%2520by%2520combining%2520locally%2520optimized%2520models%2520from%2520various%2520clients%2520into%2520a%2520unified%2520global%2520model.%2520While%2520FL%2520preserves%2520data%2520privacy%2520by%2520eliminating%2520centralized%2520storage%252C%2520it%2520encounters%2520significant%2520challenges%2520such%2520as%2520performance%2520degradation%252C%2520slower%2520convergence%252C%2520and%2520reduced%2520robustness%2520of%2520the%2520global%2520model%2520due%2520to%2520the%2520heterogeneity%2520in%2520client%2520data%2520distributions.%2520Among%2520the%2520various%2520forms%2520of%2520data%2520heterogeneity%252C%2520label%2520skew%2520emerges%2520as%2520a%2520particularly%2520formidable%2520and%2520prevalent%2520issue%252C%2520especially%2520in%2520domains%2520such%2520as%2520image%2520classification.%2520To%2520address%2520these%2520challenges%252C%2520we%2520begin%2520with%2520comprehensive%2520experiments%2520to%2520pinpoint%2520the%2520underlying%2520issues%2520in%2520the%2520FL%2520training%2520process.%2520Based%2520on%2520our%2520findings%252C%2520we%2520then%2520introduce%2520an%2520innovative%2520dual-strategy%2520approach%2520designed%2520to%2520effectively%2520resolve%2520these%2520issues.%2520First%252C%2520we%2520introduce%2520an%2520adaptive%2520loss%2520function%2520for%2520client-side%2520training%252C%2520meticulously%2520crafted%2520to%2520preserve%2520previously%2520acquired%2520knowledge%2520while%2520maintaining%2520an%2520optimal%2520equilibrium%2520between%2520local%2520optimization%2520and%2520global%2520model%2520coherence.%2520Secondly%252C%2520we%2520develop%2520a%2520dynamic%2520aggregation%2520strategy%2520for%2520aggregating%2520client%2520models%2520at%2520the%2520server.%2520This%2520approach%2520adapts%2520to%2520each%2520client%2527s%2520unique%2520learning%2520patterns%252C%2520effectively%2520addressing%2520the%2520challenges%2520of%2520diverse%2520data%2520across%2520the%2520network.%2520Our%2520comprehensive%2520evaluation%252C%2520conducted%2520across%2520three%2520diverse%2520real-world%2520datasets%252C%2520coupled%2520with%2520theoretical%2520convergence%2520guarantees%252C%2520demonstrates%2520the%2520superior%2520efficacy%2520of%2520our%2520method%2520compared%2520to%2520several%2520established%2520state-of-the-art%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04416v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedDUAL%3A%20A%20Dual-Strategy%20with%20Adaptive%20Loss%20and%20Dynamic%20Aggregation%20for%20Mitigating%20Data%20Heterogeneity%20in%20Federated%20Learning&entry.906535625=Pranab%20Sahoo%20and%20Ashutosh%20Tripathi%20and%20Sriparna%20Saha%20and%20Samrat%20Mondal&entry.1292438233=Federated%20Learning%20%28FL%29%20marks%20a%20transformative%20approach%20to%20distributed%20model%20training%20by%20combining%20locally%20optimized%20models%20from%20various%20clients%20into%20a%20unified%20global%20model.%20While%20FL%20preserves%20data%20privacy%20by%20eliminating%20centralized%20storage%2C%20it%20encounters%20significant%20challenges%20such%20as%20performance%20degradation%2C%20slower%20convergence%2C%20and%20reduced%20robustness%20of%20the%20global%20model%20due%20to%20the%20heterogeneity%20in%20client%20data%20distributions.%20Among%20the%20various%20forms%20of%20data%20heterogeneity%2C%20label%20skew%20emerges%20as%20a%20particularly%20formidable%20and%20prevalent%20issue%2C%20especially%20in%20domains%20such%20as%20image%20classification.%20To%20address%20these%20challenges%2C%20we%20begin%20with%20comprehensive%20experiments%20to%20pinpoint%20the%20underlying%20issues%20in%20the%20FL%20training%20process.%20Based%20on%20our%20findings%2C%20we%20then%20introduce%20an%20innovative%20dual-strategy%20approach%20designed%20to%20effectively%20resolve%20these%20issues.%20First%2C%20we%20introduce%20an%20adaptive%20loss%20function%20for%20client-side%20training%2C%20meticulously%20crafted%20to%20preserve%20previously%20acquired%20knowledge%20while%20maintaining%20an%20optimal%20equilibrium%20between%20local%20optimization%20and%20global%20model%20coherence.%20Secondly%2C%20we%20develop%20a%20dynamic%20aggregation%20strategy%20for%20aggregating%20client%20models%20at%20the%20server.%20This%20approach%20adapts%20to%20each%20client%27s%20unique%20learning%20patterns%2C%20effectively%20addressing%20the%20challenges%20of%20diverse%20data%20across%20the%20network.%20Our%20comprehensive%20evaluation%2C%20conducted%20across%20three%20diverse%20real-world%20datasets%2C%20coupled%20with%20theoretical%20convergence%20guarantees%2C%20demonstrates%20the%20superior%20efficacy%20of%20our%20method%20compared%20to%20several%20established%20state-of-the-art%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2412.04416v2&entry.124074799=Read"},
{"title": "Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schr\u00f6dinger Equation", "author": "Pietro de Oliveira Esteves", "abstract": "We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.", "link": "http://arxiv.org/abs/2601.04176v1", "date": "2026-01-07", "relevancy": 2.0744, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5326}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.532}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Physics%20Discovery%20from%20Highly%20Corrupted%20Data%3A%20A%20PINN%20Framework%20Applied%20to%20the%20Nonlinear%20Schr%C3%B6dinger%20Equation&body=Title%3A%20Robust%20Physics%20Discovery%20from%20Highly%20Corrupted%20Data%3A%20A%20PINN%20Framework%20Applied%20to%20the%20Nonlinear%20Schr%C3%B6dinger%20Equation%0AAuthor%3A%20Pietro%20de%20Oliveira%20Esteves%0AAbstract%3A%20We%20demonstrate%20a%20deep%20learning%20framework%20capable%20of%20recovering%20physical%20parameters%20from%20the%20Nonlinear%20Schrodinger%20Equation%20%28NLSE%29%20under%20severe%20noise%20conditions.%20By%20integrating%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20with%20automatic%20differentiation%2C%20we%20achieve%20reconstruction%20of%20the%20nonlinear%20coefficient%20beta%20with%20less%20than%200.2%20percent%20relative%20error%20using%20only%20500%20sparse%2C%20randomly%20sampled%20data%20points%20corrupted%20by%2020%20percent%20additive%20Gaussian%20noise%2C%20a%20regime%20where%20traditional%20finite%20difference%20methods%20typically%20fail%20due%20to%20noise%20amplification%20in%20numerical%20derivatives.%20We%20validate%20the%20method%27s%20generalization%20capabilities%20across%20different%20physical%20regimes%20%28beta%20between%200.5%20and%202.0%29%20and%20varying%20data%20availability%20%28between%20100%20and%201000%20training%20points%29%2C%20demonstrating%20consistent%20sub-1%20percent%20accuracy.%20Statistical%20analysis%20over%20multiple%20independent%20runs%20confirms%20robustness%20%28standard%20deviation%20less%20than%200.15%20percent%20for%20beta%20equals%201.0%29.%20The%20complete%20pipeline%20executes%20in%20approximately%2080%20minutes%20on%20modest%20cloud%20GPU%20resources%20%28NVIDIA%20Tesla%20T4%29%2C%20making%20the%20approach%20accessible%20for%20widespread%20adoption.%20Our%20results%20indicate%20that%20physics-based%20regularization%20acts%20as%20an%20effective%20filter%20against%20high%20measurement%20uncertainty%2C%20positioning%20PINNs%20as%20a%20viable%20alternative%20to%20traditional%20optimization%20methods%20for%20inverse%20problems%20in%20spatiotemporal%20dynamics%20where%20experimental%20data%20is%20scarce%20and%20noisy.%20All%20code%20is%20made%20publicly%20available%20to%20facilitate%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Physics%2520Discovery%2520from%2520Highly%2520Corrupted%2520Data%253A%2520A%2520PINN%2520Framework%2520Applied%2520to%2520the%2520Nonlinear%2520Schr%25C3%25B6dinger%2520Equation%26entry.906535625%3DPietro%2520de%2520Oliveira%2520Esteves%26entry.1292438233%3DWe%2520demonstrate%2520a%2520deep%2520learning%2520framework%2520capable%2520of%2520recovering%2520physical%2520parameters%2520from%2520the%2520Nonlinear%2520Schrodinger%2520Equation%2520%2528NLSE%2529%2520under%2520severe%2520noise%2520conditions.%2520By%2520integrating%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520with%2520automatic%2520differentiation%252C%2520we%2520achieve%2520reconstruction%2520of%2520the%2520nonlinear%2520coefficient%2520beta%2520with%2520less%2520than%25200.2%2520percent%2520relative%2520error%2520using%2520only%2520500%2520sparse%252C%2520randomly%2520sampled%2520data%2520points%2520corrupted%2520by%252020%2520percent%2520additive%2520Gaussian%2520noise%252C%2520a%2520regime%2520where%2520traditional%2520finite%2520difference%2520methods%2520typically%2520fail%2520due%2520to%2520noise%2520amplification%2520in%2520numerical%2520derivatives.%2520We%2520validate%2520the%2520method%2527s%2520generalization%2520capabilities%2520across%2520different%2520physical%2520regimes%2520%2528beta%2520between%25200.5%2520and%25202.0%2529%2520and%2520varying%2520data%2520availability%2520%2528between%2520100%2520and%25201000%2520training%2520points%2529%252C%2520demonstrating%2520consistent%2520sub-1%2520percent%2520accuracy.%2520Statistical%2520analysis%2520over%2520multiple%2520independent%2520runs%2520confirms%2520robustness%2520%2528standard%2520deviation%2520less%2520than%25200.15%2520percent%2520for%2520beta%2520equals%25201.0%2529.%2520The%2520complete%2520pipeline%2520executes%2520in%2520approximately%252080%2520minutes%2520on%2520modest%2520cloud%2520GPU%2520resources%2520%2528NVIDIA%2520Tesla%2520T4%2529%252C%2520making%2520the%2520approach%2520accessible%2520for%2520widespread%2520adoption.%2520Our%2520results%2520indicate%2520that%2520physics-based%2520regularization%2520acts%2520as%2520an%2520effective%2520filter%2520against%2520high%2520measurement%2520uncertainty%252C%2520positioning%2520PINNs%2520as%2520a%2520viable%2520alternative%2520to%2520traditional%2520optimization%2520methods%2520for%2520inverse%2520problems%2520in%2520spatiotemporal%2520dynamics%2520where%2520experimental%2520data%2520is%2520scarce%2520and%2520noisy.%2520All%2520code%2520is%2520made%2520publicly%2520available%2520to%2520facilitate%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Physics%20Discovery%20from%20Highly%20Corrupted%20Data%3A%20A%20PINN%20Framework%20Applied%20to%20the%20Nonlinear%20Schr%C3%B6dinger%20Equation&entry.906535625=Pietro%20de%20Oliveira%20Esteves&entry.1292438233=We%20demonstrate%20a%20deep%20learning%20framework%20capable%20of%20recovering%20physical%20parameters%20from%20the%20Nonlinear%20Schrodinger%20Equation%20%28NLSE%29%20under%20severe%20noise%20conditions.%20By%20integrating%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20with%20automatic%20differentiation%2C%20we%20achieve%20reconstruction%20of%20the%20nonlinear%20coefficient%20beta%20with%20less%20than%200.2%20percent%20relative%20error%20using%20only%20500%20sparse%2C%20randomly%20sampled%20data%20points%20corrupted%20by%2020%20percent%20additive%20Gaussian%20noise%2C%20a%20regime%20where%20traditional%20finite%20difference%20methods%20typically%20fail%20due%20to%20noise%20amplification%20in%20numerical%20derivatives.%20We%20validate%20the%20method%27s%20generalization%20capabilities%20across%20different%20physical%20regimes%20%28beta%20between%200.5%20and%202.0%29%20and%20varying%20data%20availability%20%28between%20100%20and%201000%20training%20points%29%2C%20demonstrating%20consistent%20sub-1%20percent%20accuracy.%20Statistical%20analysis%20over%20multiple%20independent%20runs%20confirms%20robustness%20%28standard%20deviation%20less%20than%200.15%20percent%20for%20beta%20equals%201.0%29.%20The%20complete%20pipeline%20executes%20in%20approximately%2080%20minutes%20on%20modest%20cloud%20GPU%20resources%20%28NVIDIA%20Tesla%20T4%29%2C%20making%20the%20approach%20accessible%20for%20widespread%20adoption.%20Our%20results%20indicate%20that%20physics-based%20regularization%20acts%20as%20an%20effective%20filter%20against%20high%20measurement%20uncertainty%2C%20positioning%20PINNs%20as%20a%20viable%20alternative%20to%20traditional%20optimization%20methods%20for%20inverse%20problems%20in%20spatiotemporal%20dynamics%20where%20experimental%20data%20is%20scarce%20and%20noisy.%20All%20code%20is%20made%20publicly%20available%20to%20facilitate%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2601.04176v1&entry.124074799=Read"},
{"title": "Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots", "author": "K. Ege de Bruin and Kyrre Glette and Kai Olav Ellefsen", "abstract": "Evolutionary Robotics offers the possibility to design robots to solve a specific task automatically by optimizing their morphology and control together. However, this co-optimization of body and control is challenging, because controllers need some time to adapt to the evolving morphology - which may make it difficult for new and promising designs to enter the evolving population. A solution to this is to add intra-life learning, defined as an additional controller optimization loop, to each individual in the evolving population. A related problem is the lack of diversity often seen in evolving populations as evolution narrows the search down to a few promising designs too quickly. This problem can be mitigated by implementing full generational replacement, where offspring robots replace the whole population. This solution for increasing diversity usually comes at the cost of lower performance compared to using elitism. In this work, we show that combining such generational replacement with intra-life learning can increase diversity while retaining performance. We also highlight the importance of performance metrics when studying learning in morphologically evolving robots, showing that evaluating according to function evaluations versus according to generations of evolution can give different conclusions.", "link": "http://arxiv.org/abs/2601.03807v1", "date": "2026-01-07", "relevancy": 2.0687, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5228}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generational%20Replacement%20and%20Learning%20for%20High-Performing%20and%20Diverse%20Populations%20in%20Evolvable%20Robots&body=Title%3A%20Generational%20Replacement%20and%20Learning%20for%20High-Performing%20and%20Diverse%20Populations%20in%20Evolvable%20Robots%0AAuthor%3A%20K.%20Ege%20de%20Bruin%20and%20Kyrre%20Glette%20and%20Kai%20Olav%20Ellefsen%0AAbstract%3A%20Evolutionary%20Robotics%20offers%20the%20possibility%20to%20design%20robots%20to%20solve%20a%20specific%20task%20automatically%20by%20optimizing%20their%20morphology%20and%20control%20together.%20However%2C%20this%20co-optimization%20of%20body%20and%20control%20is%20challenging%2C%20because%20controllers%20need%20some%20time%20to%20adapt%20to%20the%20evolving%20morphology%20-%20which%20may%20make%20it%20difficult%20for%20new%20and%20promising%20designs%20to%20enter%20the%20evolving%20population.%20A%20solution%20to%20this%20is%20to%20add%20intra-life%20learning%2C%20defined%20as%20an%20additional%20controller%20optimization%20loop%2C%20to%20each%20individual%20in%20the%20evolving%20population.%20A%20related%20problem%20is%20the%20lack%20of%20diversity%20often%20seen%20in%20evolving%20populations%20as%20evolution%20narrows%20the%20search%20down%20to%20a%20few%20promising%20designs%20too%20quickly.%20This%20problem%20can%20be%20mitigated%20by%20implementing%20full%20generational%20replacement%2C%20where%20offspring%20robots%20replace%20the%20whole%20population.%20This%20solution%20for%20increasing%20diversity%20usually%20comes%20at%20the%20cost%20of%20lower%20performance%20compared%20to%20using%20elitism.%20In%20this%20work%2C%20we%20show%20that%20combining%20such%20generational%20replacement%20with%20intra-life%20learning%20can%20increase%20diversity%20while%20retaining%20performance.%20We%20also%20highlight%20the%20importance%20of%20performance%20metrics%20when%20studying%20learning%20in%20morphologically%20evolving%20robots%2C%20showing%20that%20evaluating%20according%20to%20function%20evaluations%20versus%20according%20to%20generations%20of%20evolution%20can%20give%20different%20conclusions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerational%2520Replacement%2520and%2520Learning%2520for%2520High-Performing%2520and%2520Diverse%2520Populations%2520in%2520Evolvable%2520Robots%26entry.906535625%3DK.%2520Ege%2520de%2520Bruin%2520and%2520Kyrre%2520Glette%2520and%2520Kai%2520Olav%2520Ellefsen%26entry.1292438233%3DEvolutionary%2520Robotics%2520offers%2520the%2520possibility%2520to%2520design%2520robots%2520to%2520solve%2520a%2520specific%2520task%2520automatically%2520by%2520optimizing%2520their%2520morphology%2520and%2520control%2520together.%2520However%252C%2520this%2520co-optimization%2520of%2520body%2520and%2520control%2520is%2520challenging%252C%2520because%2520controllers%2520need%2520some%2520time%2520to%2520adapt%2520to%2520the%2520evolving%2520morphology%2520-%2520which%2520may%2520make%2520it%2520difficult%2520for%2520new%2520and%2520promising%2520designs%2520to%2520enter%2520the%2520evolving%2520population.%2520A%2520solution%2520to%2520this%2520is%2520to%2520add%2520intra-life%2520learning%252C%2520defined%2520as%2520an%2520additional%2520controller%2520optimization%2520loop%252C%2520to%2520each%2520individual%2520in%2520the%2520evolving%2520population.%2520A%2520related%2520problem%2520is%2520the%2520lack%2520of%2520diversity%2520often%2520seen%2520in%2520evolving%2520populations%2520as%2520evolution%2520narrows%2520the%2520search%2520down%2520to%2520a%2520few%2520promising%2520designs%2520too%2520quickly.%2520This%2520problem%2520can%2520be%2520mitigated%2520by%2520implementing%2520full%2520generational%2520replacement%252C%2520where%2520offspring%2520robots%2520replace%2520the%2520whole%2520population.%2520This%2520solution%2520for%2520increasing%2520diversity%2520usually%2520comes%2520at%2520the%2520cost%2520of%2520lower%2520performance%2520compared%2520to%2520using%2520elitism.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520combining%2520such%2520generational%2520replacement%2520with%2520intra-life%2520learning%2520can%2520increase%2520diversity%2520while%2520retaining%2520performance.%2520We%2520also%2520highlight%2520the%2520importance%2520of%2520performance%2520metrics%2520when%2520studying%2520learning%2520in%2520morphologically%2520evolving%2520robots%252C%2520showing%2520that%2520evaluating%2520according%2520to%2520function%2520evaluations%2520versus%2520according%2520to%2520generations%2520of%2520evolution%2520can%2520give%2520different%2520conclusions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generational%20Replacement%20and%20Learning%20for%20High-Performing%20and%20Diverse%20Populations%20in%20Evolvable%20Robots&entry.906535625=K.%20Ege%20de%20Bruin%20and%20Kyrre%20Glette%20and%20Kai%20Olav%20Ellefsen&entry.1292438233=Evolutionary%20Robotics%20offers%20the%20possibility%20to%20design%20robots%20to%20solve%20a%20specific%20task%20automatically%20by%20optimizing%20their%20morphology%20and%20control%20together.%20However%2C%20this%20co-optimization%20of%20body%20and%20control%20is%20challenging%2C%20because%20controllers%20need%20some%20time%20to%20adapt%20to%20the%20evolving%20morphology%20-%20which%20may%20make%20it%20difficult%20for%20new%20and%20promising%20designs%20to%20enter%20the%20evolving%20population.%20A%20solution%20to%20this%20is%20to%20add%20intra-life%20learning%2C%20defined%20as%20an%20additional%20controller%20optimization%20loop%2C%20to%20each%20individual%20in%20the%20evolving%20population.%20A%20related%20problem%20is%20the%20lack%20of%20diversity%20often%20seen%20in%20evolving%20populations%20as%20evolution%20narrows%20the%20search%20down%20to%20a%20few%20promising%20designs%20too%20quickly.%20This%20problem%20can%20be%20mitigated%20by%20implementing%20full%20generational%20replacement%2C%20where%20offspring%20robots%20replace%20the%20whole%20population.%20This%20solution%20for%20increasing%20diversity%20usually%20comes%20at%20the%20cost%20of%20lower%20performance%20compared%20to%20using%20elitism.%20In%20this%20work%2C%20we%20show%20that%20combining%20such%20generational%20replacement%20with%20intra-life%20learning%20can%20increase%20diversity%20while%20retaining%20performance.%20We%20also%20highlight%20the%20importance%20of%20performance%20metrics%20when%20studying%20learning%20in%20morphologically%20evolving%20robots%2C%20showing%20that%20evaluating%20according%20to%20function%20evaluations%20versus%20according%20to%20generations%20of%20evolution%20can%20give%20different%20conclusions.&entry.1838667208=http%3A//arxiv.org/abs/2601.03807v1&entry.124074799=Read"},
{"title": "Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models", "author": "Wei Wu and Liyi Chen and Congxi Xiao and Tianfu Wang and Qimeng Wang and Chengqiang Lu and Yan Gao and Yi Wu and Yao Hu and Hui Xiong", "abstract": "Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.", "link": "http://arxiv.org/abs/2601.03969v1", "date": "2026-01-07", "relevancy": 2.0491, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5427}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4953}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anti-Length%20Shift%3A%20Dynamic%20Outlier%20Truncation%20for%20Training%20Efficient%20Reasoning%20Models&body=Title%3A%20Anti-Length%20Shift%3A%20Dynamic%20Outlier%20Truncation%20for%20Training%20Efficient%20Reasoning%20Models%0AAuthor%3A%20Wei%20Wu%20and%20Liyi%20Chen%20and%20Congxi%20Xiao%20and%20Tianfu%20Wang%20and%20Qimeng%20Wang%20and%20Chengqiang%20Lu%20and%20Yan%20Gao%20and%20Yi%20Wu%20and%20Yao%20Hu%20and%20Hui%20Xiong%0AAbstract%3A%20Large%20reasoning%20models%20enhanced%20by%20reinforcement%20learning%20with%20verifiable%20rewards%20have%20achieved%20significant%20performance%20gains%20by%20extending%20their%20chain-of-thought.%20However%2C%20this%20paradigm%20incurs%20substantial%20deployment%20costs%20as%20models%20often%20exhibit%20excessive%20verbosity%20on%20simple%20queries.%20Existing%20efficient%20reasoning%20methods%20relying%20on%20explicit%20length%20penalties%20often%20introduce%20optimization%20conflicts%20and%20leave%20the%20generative%20mechanisms%20driving%20overthinking%20largely%20unexamined.%20In%20this%20paper%2C%20we%20identify%20a%20phenomenon%20termed%20length%20shift%20where%20models%20increasingly%20generate%20unnecessary%20reasoning%20on%20trivial%20inputs%20during%20training.%20To%20address%20this%2C%20we%20introduce%20Dynamic%20Outlier%20Truncation%20%28DOT%29%2C%20a%20training-time%20intervention%20that%20selectively%20suppresses%20redundant%20tokens.%20This%20method%20targets%20only%20the%20extreme%20tail%20of%20response%20lengths%20within%20fully%20correct%20rollout%20groups%20while%20preserving%20long-horizon%20reasoning%20capabilities%20for%20complex%20problems.%20To%20complement%20this%20intervention%20and%20ensure%20stable%20convergence%2C%20we%20further%20incorporate%20auxiliary%20KL%20regularization%20and%20predictive%20dynamic%20sampling.%20Experimental%20results%20across%20multiple%20model%20scales%20demonstrate%20that%20our%20approach%20significantly%20pushes%20the%20efficiency-performance%20Pareto%20frontier%20outward.%20Notably%2C%20on%20the%20AIME-24%2C%20our%20method%20reduces%20inference%20token%20usage%20by%2078%25%20while%20simultaneously%20increasing%20accuracy%20compared%20to%20the%20initial%20policy%20and%20surpassing%20state-of-the-art%20efficient%20reasoning%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnti-Length%2520Shift%253A%2520Dynamic%2520Outlier%2520Truncation%2520for%2520Training%2520Efficient%2520Reasoning%2520Models%26entry.906535625%3DWei%2520Wu%2520and%2520Liyi%2520Chen%2520and%2520Congxi%2520Xiao%2520and%2520Tianfu%2520Wang%2520and%2520Qimeng%2520Wang%2520and%2520Chengqiang%2520Lu%2520and%2520Yan%2520Gao%2520and%2520Yi%2520Wu%2520and%2520Yao%2520Hu%2520and%2520Hui%2520Xiong%26entry.1292438233%3DLarge%2520reasoning%2520models%2520enhanced%2520by%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520have%2520achieved%2520significant%2520performance%2520gains%2520by%2520extending%2520their%2520chain-of-thought.%2520However%252C%2520this%2520paradigm%2520incurs%2520substantial%2520deployment%2520costs%2520as%2520models%2520often%2520exhibit%2520excessive%2520verbosity%2520on%2520simple%2520queries.%2520Existing%2520efficient%2520reasoning%2520methods%2520relying%2520on%2520explicit%2520length%2520penalties%2520often%2520introduce%2520optimization%2520conflicts%2520and%2520leave%2520the%2520generative%2520mechanisms%2520driving%2520overthinking%2520largely%2520unexamined.%2520In%2520this%2520paper%252C%2520we%2520identify%2520a%2520phenomenon%2520termed%2520length%2520shift%2520where%2520models%2520increasingly%2520generate%2520unnecessary%2520reasoning%2520on%2520trivial%2520inputs%2520during%2520training.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Dynamic%2520Outlier%2520Truncation%2520%2528DOT%2529%252C%2520a%2520training-time%2520intervention%2520that%2520selectively%2520suppresses%2520redundant%2520tokens.%2520This%2520method%2520targets%2520only%2520the%2520extreme%2520tail%2520of%2520response%2520lengths%2520within%2520fully%2520correct%2520rollout%2520groups%2520while%2520preserving%2520long-horizon%2520reasoning%2520capabilities%2520for%2520complex%2520problems.%2520To%2520complement%2520this%2520intervention%2520and%2520ensure%2520stable%2520convergence%252C%2520we%2520further%2520incorporate%2520auxiliary%2520KL%2520regularization%2520and%2520predictive%2520dynamic%2520sampling.%2520Experimental%2520results%2520across%2520multiple%2520model%2520scales%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520pushes%2520the%2520efficiency-performance%2520Pareto%2520frontier%2520outward.%2520Notably%252C%2520on%2520the%2520AIME-24%252C%2520our%2520method%2520reduces%2520inference%2520token%2520usage%2520by%252078%2525%2520while%2520simultaneously%2520increasing%2520accuracy%2520compared%2520to%2520the%2520initial%2520policy%2520and%2520surpassing%2520state-of-the-art%2520efficient%2520reasoning%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anti-Length%20Shift%3A%20Dynamic%20Outlier%20Truncation%20for%20Training%20Efficient%20Reasoning%20Models&entry.906535625=Wei%20Wu%20and%20Liyi%20Chen%20and%20Congxi%20Xiao%20and%20Tianfu%20Wang%20and%20Qimeng%20Wang%20and%20Chengqiang%20Lu%20and%20Yan%20Gao%20and%20Yi%20Wu%20and%20Yao%20Hu%20and%20Hui%20Xiong&entry.1292438233=Large%20reasoning%20models%20enhanced%20by%20reinforcement%20learning%20with%20verifiable%20rewards%20have%20achieved%20significant%20performance%20gains%20by%20extending%20their%20chain-of-thought.%20However%2C%20this%20paradigm%20incurs%20substantial%20deployment%20costs%20as%20models%20often%20exhibit%20excessive%20verbosity%20on%20simple%20queries.%20Existing%20efficient%20reasoning%20methods%20relying%20on%20explicit%20length%20penalties%20often%20introduce%20optimization%20conflicts%20and%20leave%20the%20generative%20mechanisms%20driving%20overthinking%20largely%20unexamined.%20In%20this%20paper%2C%20we%20identify%20a%20phenomenon%20termed%20length%20shift%20where%20models%20increasingly%20generate%20unnecessary%20reasoning%20on%20trivial%20inputs%20during%20training.%20To%20address%20this%2C%20we%20introduce%20Dynamic%20Outlier%20Truncation%20%28DOT%29%2C%20a%20training-time%20intervention%20that%20selectively%20suppresses%20redundant%20tokens.%20This%20method%20targets%20only%20the%20extreme%20tail%20of%20response%20lengths%20within%20fully%20correct%20rollout%20groups%20while%20preserving%20long-horizon%20reasoning%20capabilities%20for%20complex%20problems.%20To%20complement%20this%20intervention%20and%20ensure%20stable%20convergence%2C%20we%20further%20incorporate%20auxiliary%20KL%20regularization%20and%20predictive%20dynamic%20sampling.%20Experimental%20results%20across%20multiple%20model%20scales%20demonstrate%20that%20our%20approach%20significantly%20pushes%20the%20efficiency-performance%20Pareto%20frontier%20outward.%20Notably%2C%20on%20the%20AIME-24%2C%20our%20method%20reduces%20inference%20token%20usage%20by%2078%25%20while%20simultaneously%20increasing%20accuracy%20compared%20to%20the%20initial%20policy%20and%20surpassing%20state-of-the-art%20efficient%20reasoning%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.03969v1&entry.124074799=Read"},
{"title": "Feature-Aware One-Shot Federated Learning via Hierarchical Token Sequences", "author": "Shudong Liu and Hanwen Zhang and Xiuling Wang and Yuesheng Zhu and Guibo Luo", "abstract": "One-shot federated learning (OSFL) reduces the communication cost and privacy risks of iterative federated learning by constructing a global model with a single round of communication. However, most existing methods struggle to achieve robust performance on real-world domains such as medical imaging, or are inefficient when handling non-IID (Independent and Identically Distributed) data. To address these limitations, we introduce FALCON, a framework that enhances the effectiveness of OSFL over non-IID image data. The core idea of FALCON is to leverage the feature-aware hierarchical token sequences generation and knowledge distillation into OSFL. First, each client leverages a pretrained visual encoder with hierarchical scale encoding to compress images into hierarchical token sequences, which capture multi-scale semantics. Second, a multi-scale autoregressive transformer generator is used to model the distribution of these token sequences and generate the synthetic sequences. Third, clients upload the synthetic sequences along with the local classifier trained on the real token sequences to the server. Finally, the server incorporates knowledge distillation into global training to reduce reliance on precise distribution modeling. Experiments on medical and natural image datasets validate the effectiveness of FALCON in diverse non-IID scenarios, outperforming the best OSFL baselines by 9.58% in average accuracy.", "link": "http://arxiv.org/abs/2601.03882v1", "date": "2026-01-07", "relevancy": 2.0491, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5188}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5119}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-Aware%20One-Shot%20Federated%20Learning%20via%20Hierarchical%20Token%20Sequences&body=Title%3A%20Feature-Aware%20One-Shot%20Federated%20Learning%20via%20Hierarchical%20Token%20Sequences%0AAuthor%3A%20Shudong%20Liu%20and%20Hanwen%20Zhang%20and%20Xiuling%20Wang%20and%20Yuesheng%20Zhu%20and%20Guibo%20Luo%0AAbstract%3A%20One-shot%20federated%20learning%20%28OSFL%29%20reduces%20the%20communication%20cost%20and%20privacy%20risks%20of%20iterative%20federated%20learning%20by%20constructing%20a%20global%20model%20with%20a%20single%20round%20of%20communication.%20However%2C%20most%20existing%20methods%20struggle%20to%20achieve%20robust%20performance%20on%20real-world%20domains%20such%20as%20medical%20imaging%2C%20or%20are%20inefficient%20when%20handling%20non-IID%20%28Independent%20and%20Identically%20Distributed%29%20data.%20To%20address%20these%20limitations%2C%20we%20introduce%20FALCON%2C%20a%20framework%20that%20enhances%20the%20effectiveness%20of%20OSFL%20over%20non-IID%20image%20data.%20The%20core%20idea%20of%20FALCON%20is%20to%20leverage%20the%20feature-aware%20hierarchical%20token%20sequences%20generation%20and%20knowledge%20distillation%20into%20OSFL.%20First%2C%20each%20client%20leverages%20a%20pretrained%20visual%20encoder%20with%20hierarchical%20scale%20encoding%20to%20compress%20images%20into%20hierarchical%20token%20sequences%2C%20which%20capture%20multi-scale%20semantics.%20Second%2C%20a%20multi-scale%20autoregressive%20transformer%20generator%20is%20used%20to%20model%20the%20distribution%20of%20these%20token%20sequences%20and%20generate%20the%20synthetic%20sequences.%20Third%2C%20clients%20upload%20the%20synthetic%20sequences%20along%20with%20the%20local%20classifier%20trained%20on%20the%20real%20token%20sequences%20to%20the%20server.%20Finally%2C%20the%20server%20incorporates%20knowledge%20distillation%20into%20global%20training%20to%20reduce%20reliance%20on%20precise%20distribution%20modeling.%20Experiments%20on%20medical%20and%20natural%20image%20datasets%20validate%20the%20effectiveness%20of%20FALCON%20in%20diverse%20non-IID%20scenarios%2C%20outperforming%20the%20best%20OSFL%20baselines%20by%209.58%25%20in%20average%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-Aware%2520One-Shot%2520Federated%2520Learning%2520via%2520Hierarchical%2520Token%2520Sequences%26entry.906535625%3DShudong%2520Liu%2520and%2520Hanwen%2520Zhang%2520and%2520Xiuling%2520Wang%2520and%2520Yuesheng%2520Zhu%2520and%2520Guibo%2520Luo%26entry.1292438233%3DOne-shot%2520federated%2520learning%2520%2528OSFL%2529%2520reduces%2520the%2520communication%2520cost%2520and%2520privacy%2520risks%2520of%2520iterative%2520federated%2520learning%2520by%2520constructing%2520a%2520global%2520model%2520with%2520a%2520single%2520round%2520of%2520communication.%2520However%252C%2520most%2520existing%2520methods%2520struggle%2520to%2520achieve%2520robust%2520performance%2520on%2520real-world%2520domains%2520such%2520as%2520medical%2520imaging%252C%2520or%2520are%2520inefficient%2520when%2520handling%2520non-IID%2520%2528Independent%2520and%2520Identically%2520Distributed%2529%2520data.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520FALCON%252C%2520a%2520framework%2520that%2520enhances%2520the%2520effectiveness%2520of%2520OSFL%2520over%2520non-IID%2520image%2520data.%2520The%2520core%2520idea%2520of%2520FALCON%2520is%2520to%2520leverage%2520the%2520feature-aware%2520hierarchical%2520token%2520sequences%2520generation%2520and%2520knowledge%2520distillation%2520into%2520OSFL.%2520First%252C%2520each%2520client%2520leverages%2520a%2520pretrained%2520visual%2520encoder%2520with%2520hierarchical%2520scale%2520encoding%2520to%2520compress%2520images%2520into%2520hierarchical%2520token%2520sequences%252C%2520which%2520capture%2520multi-scale%2520semantics.%2520Second%252C%2520a%2520multi-scale%2520autoregressive%2520transformer%2520generator%2520is%2520used%2520to%2520model%2520the%2520distribution%2520of%2520these%2520token%2520sequences%2520and%2520generate%2520the%2520synthetic%2520sequences.%2520Third%252C%2520clients%2520upload%2520the%2520synthetic%2520sequences%2520along%2520with%2520the%2520local%2520classifier%2520trained%2520on%2520the%2520real%2520token%2520sequences%2520to%2520the%2520server.%2520Finally%252C%2520the%2520server%2520incorporates%2520knowledge%2520distillation%2520into%2520global%2520training%2520to%2520reduce%2520reliance%2520on%2520precise%2520distribution%2520modeling.%2520Experiments%2520on%2520medical%2520and%2520natural%2520image%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520FALCON%2520in%2520diverse%2520non-IID%2520scenarios%252C%2520outperforming%2520the%2520best%2520OSFL%2520baselines%2520by%25209.58%2525%2520in%2520average%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-Aware%20One-Shot%20Federated%20Learning%20via%20Hierarchical%20Token%20Sequences&entry.906535625=Shudong%20Liu%20and%20Hanwen%20Zhang%20and%20Xiuling%20Wang%20and%20Yuesheng%20Zhu%20and%20Guibo%20Luo&entry.1292438233=One-shot%20federated%20learning%20%28OSFL%29%20reduces%20the%20communication%20cost%20and%20privacy%20risks%20of%20iterative%20federated%20learning%20by%20constructing%20a%20global%20model%20with%20a%20single%20round%20of%20communication.%20However%2C%20most%20existing%20methods%20struggle%20to%20achieve%20robust%20performance%20on%20real-world%20domains%20such%20as%20medical%20imaging%2C%20or%20are%20inefficient%20when%20handling%20non-IID%20%28Independent%20and%20Identically%20Distributed%29%20data.%20To%20address%20these%20limitations%2C%20we%20introduce%20FALCON%2C%20a%20framework%20that%20enhances%20the%20effectiveness%20of%20OSFL%20over%20non-IID%20image%20data.%20The%20core%20idea%20of%20FALCON%20is%20to%20leverage%20the%20feature-aware%20hierarchical%20token%20sequences%20generation%20and%20knowledge%20distillation%20into%20OSFL.%20First%2C%20each%20client%20leverages%20a%20pretrained%20visual%20encoder%20with%20hierarchical%20scale%20encoding%20to%20compress%20images%20into%20hierarchical%20token%20sequences%2C%20which%20capture%20multi-scale%20semantics.%20Second%2C%20a%20multi-scale%20autoregressive%20transformer%20generator%20is%20used%20to%20model%20the%20distribution%20of%20these%20token%20sequences%20and%20generate%20the%20synthetic%20sequences.%20Third%2C%20clients%20upload%20the%20synthetic%20sequences%20along%20with%20the%20local%20classifier%20trained%20on%20the%20real%20token%20sequences%20to%20the%20server.%20Finally%2C%20the%20server%20incorporates%20knowledge%20distillation%20into%20global%20training%20to%20reduce%20reliance%20on%20precise%20distribution%20modeling.%20Experiments%20on%20medical%20and%20natural%20image%20datasets%20validate%20the%20effectiveness%20of%20FALCON%20in%20diverse%20non-IID%20scenarios%2C%20outperforming%20the%20best%20OSFL%20baselines%20by%209.58%25%20in%20average%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2601.03882v1&entry.124074799=Read"},
{"title": "Integrating Semantic Communication and Human Decision-Making into an End-to-End Sensing-Decision Framework", "author": "Edgar Beck and Hsuan-Yu Lin and Patrick R\u00fcckert and Yongping Bao and Bettina von Helversen and Sebastian Fehrler and Kirsten Tracht and Armin Dekorsy", "abstract": "As early as 1949, Weaver defined communication in a very broad sense to include all procedures by which one mind or technical system can influence another, thus establishing the idea of semantic communication. With the recent success of machine learning in expert assistance systems where sensed information is wirelessly provided to a human to assist task execution, the need to design effective and efficient communications has become increasingly apparent. In particular, semantic communication aims to convey the meaning behind the sensed information relevant for Human Decision-Making (HDM). Regarding the interplay between semantic communication and HDM, many questions remain, such as how to model the entire end-to-end sensing-decision-making process, how to design semantic communication for the HDM and which information should be provided for HDM. To address these questions, we propose to integrate semantic communication and HDM into one probabilistic end-to-end sensing-decision framework that bridges communications and psychology. In our interdisciplinary framework, we model the human through a HDM process, allowing us to explore how feature extraction from semantic communication can best support HDM both in theory and in simulations. In this sense, our study reveals the fundamental design trade-off between maximizing the relevant semantic information and matching the cognitive capabilities of the HDM model. Our initial analysis shows how semantic communication can balance the level of detail with human cognitive capabilities while demanding less bandwidth, power, and latency.", "link": "http://arxiv.org/abs/2412.05103v4", "date": "2026-01-07", "relevancy": 2.0429, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Semantic%20Communication%20and%20Human%20Decision-Making%20into%20an%20End-to-End%20Sensing-Decision%20Framework&body=Title%3A%20Integrating%20Semantic%20Communication%20and%20Human%20Decision-Making%20into%20an%20End-to-End%20Sensing-Decision%20Framework%0AAuthor%3A%20Edgar%20Beck%20and%20Hsuan-Yu%20Lin%20and%20Patrick%20R%C3%BCckert%20and%20Yongping%20Bao%20and%20Bettina%20von%20Helversen%20and%20Sebastian%20Fehrler%20and%20Kirsten%20Tracht%20and%20Armin%20Dekorsy%0AAbstract%3A%20As%20early%20as%201949%2C%20Weaver%20defined%20communication%20in%20a%20very%20broad%20sense%20to%20include%20all%20procedures%20by%20which%20one%20mind%20or%20technical%20system%20can%20influence%20another%2C%20thus%20establishing%20the%20idea%20of%20semantic%20communication.%20With%20the%20recent%20success%20of%20machine%20learning%20in%20expert%20assistance%20systems%20where%20sensed%20information%20is%20wirelessly%20provided%20to%20a%20human%20to%20assist%20task%20execution%2C%20the%20need%20to%20design%20effective%20and%20efficient%20communications%20has%20become%20increasingly%20apparent.%20In%20particular%2C%20semantic%20communication%20aims%20to%20convey%20the%20meaning%20behind%20the%20sensed%20information%20relevant%20for%20Human%20Decision-Making%20%28HDM%29.%20Regarding%20the%20interplay%20between%20semantic%20communication%20and%20HDM%2C%20many%20questions%20remain%2C%20such%20as%20how%20to%20model%20the%20entire%20end-to-end%20sensing-decision-making%20process%2C%20how%20to%20design%20semantic%20communication%20for%20the%20HDM%20and%20which%20information%20should%20be%20provided%20for%20HDM.%20To%20address%20these%20questions%2C%20we%20propose%20to%20integrate%20semantic%20communication%20and%20HDM%20into%20one%20probabilistic%20end-to-end%20sensing-decision%20framework%20that%20bridges%20communications%20and%20psychology.%20In%20our%20interdisciplinary%20framework%2C%20we%20model%20the%20human%20through%20a%20HDM%20process%2C%20allowing%20us%20to%20explore%20how%20feature%20extraction%20from%20semantic%20communication%20can%20best%20support%20HDM%20both%20in%20theory%20and%20in%20simulations.%20In%20this%20sense%2C%20our%20study%20reveals%20the%20fundamental%20design%20trade-off%20between%20maximizing%20the%20relevant%20semantic%20information%20and%20matching%20the%20cognitive%20capabilities%20of%20the%20HDM%20model.%20Our%20initial%20analysis%20shows%20how%20semantic%20communication%20can%20balance%20the%20level%20of%20detail%20with%20human%20cognitive%20capabilities%20while%20demanding%20less%20bandwidth%2C%20power%2C%20and%20latency.%0ALink%3A%20http%3A//arxiv.org/abs/2412.05103v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Semantic%2520Communication%2520and%2520Human%2520Decision-Making%2520into%2520an%2520End-to-End%2520Sensing-Decision%2520Framework%26entry.906535625%3DEdgar%2520Beck%2520and%2520Hsuan-Yu%2520Lin%2520and%2520Patrick%2520R%25C3%25BCckert%2520and%2520Yongping%2520Bao%2520and%2520Bettina%2520von%2520Helversen%2520and%2520Sebastian%2520Fehrler%2520and%2520Kirsten%2520Tracht%2520and%2520Armin%2520Dekorsy%26entry.1292438233%3DAs%2520early%2520as%25201949%252C%2520Weaver%2520defined%2520communication%2520in%2520a%2520very%2520broad%2520sense%2520to%2520include%2520all%2520procedures%2520by%2520which%2520one%2520mind%2520or%2520technical%2520system%2520can%2520influence%2520another%252C%2520thus%2520establishing%2520the%2520idea%2520of%2520semantic%2520communication.%2520With%2520the%2520recent%2520success%2520of%2520machine%2520learning%2520in%2520expert%2520assistance%2520systems%2520where%2520sensed%2520information%2520is%2520wirelessly%2520provided%2520to%2520a%2520human%2520to%2520assist%2520task%2520execution%252C%2520the%2520need%2520to%2520design%2520effective%2520and%2520efficient%2520communications%2520has%2520become%2520increasingly%2520apparent.%2520In%2520particular%252C%2520semantic%2520communication%2520aims%2520to%2520convey%2520the%2520meaning%2520behind%2520the%2520sensed%2520information%2520relevant%2520for%2520Human%2520Decision-Making%2520%2528HDM%2529.%2520Regarding%2520the%2520interplay%2520between%2520semantic%2520communication%2520and%2520HDM%252C%2520many%2520questions%2520remain%252C%2520such%2520as%2520how%2520to%2520model%2520the%2520entire%2520end-to-end%2520sensing-decision-making%2520process%252C%2520how%2520to%2520design%2520semantic%2520communication%2520for%2520the%2520HDM%2520and%2520which%2520information%2520should%2520be%2520provided%2520for%2520HDM.%2520To%2520address%2520these%2520questions%252C%2520we%2520propose%2520to%2520integrate%2520semantic%2520communication%2520and%2520HDM%2520into%2520one%2520probabilistic%2520end-to-end%2520sensing-decision%2520framework%2520that%2520bridges%2520communications%2520and%2520psychology.%2520In%2520our%2520interdisciplinary%2520framework%252C%2520we%2520model%2520the%2520human%2520through%2520a%2520HDM%2520process%252C%2520allowing%2520us%2520to%2520explore%2520how%2520feature%2520extraction%2520from%2520semantic%2520communication%2520can%2520best%2520support%2520HDM%2520both%2520in%2520theory%2520and%2520in%2520simulations.%2520In%2520this%2520sense%252C%2520our%2520study%2520reveals%2520the%2520fundamental%2520design%2520trade-off%2520between%2520maximizing%2520the%2520relevant%2520semantic%2520information%2520and%2520matching%2520the%2520cognitive%2520capabilities%2520of%2520the%2520HDM%2520model.%2520Our%2520initial%2520analysis%2520shows%2520how%2520semantic%2520communication%2520can%2520balance%2520the%2520level%2520of%2520detail%2520with%2520human%2520cognitive%2520capabilities%2520while%2520demanding%2520less%2520bandwidth%252C%2520power%252C%2520and%2520latency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05103v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Semantic%20Communication%20and%20Human%20Decision-Making%20into%20an%20End-to-End%20Sensing-Decision%20Framework&entry.906535625=Edgar%20Beck%20and%20Hsuan-Yu%20Lin%20and%20Patrick%20R%C3%BCckert%20and%20Yongping%20Bao%20and%20Bettina%20von%20Helversen%20and%20Sebastian%20Fehrler%20and%20Kirsten%20Tracht%20and%20Armin%20Dekorsy&entry.1292438233=As%20early%20as%201949%2C%20Weaver%20defined%20communication%20in%20a%20very%20broad%20sense%20to%20include%20all%20procedures%20by%20which%20one%20mind%20or%20technical%20system%20can%20influence%20another%2C%20thus%20establishing%20the%20idea%20of%20semantic%20communication.%20With%20the%20recent%20success%20of%20machine%20learning%20in%20expert%20assistance%20systems%20where%20sensed%20information%20is%20wirelessly%20provided%20to%20a%20human%20to%20assist%20task%20execution%2C%20the%20need%20to%20design%20effective%20and%20efficient%20communications%20has%20become%20increasingly%20apparent.%20In%20particular%2C%20semantic%20communication%20aims%20to%20convey%20the%20meaning%20behind%20the%20sensed%20information%20relevant%20for%20Human%20Decision-Making%20%28HDM%29.%20Regarding%20the%20interplay%20between%20semantic%20communication%20and%20HDM%2C%20many%20questions%20remain%2C%20such%20as%20how%20to%20model%20the%20entire%20end-to-end%20sensing-decision-making%20process%2C%20how%20to%20design%20semantic%20communication%20for%20the%20HDM%20and%20which%20information%20should%20be%20provided%20for%20HDM.%20To%20address%20these%20questions%2C%20we%20propose%20to%20integrate%20semantic%20communication%20and%20HDM%20into%20one%20probabilistic%20end-to-end%20sensing-decision%20framework%20that%20bridges%20communications%20and%20psychology.%20In%20our%20interdisciplinary%20framework%2C%20we%20model%20the%20human%20through%20a%20HDM%20process%2C%20allowing%20us%20to%20explore%20how%20feature%20extraction%20from%20semantic%20communication%20can%20best%20support%20HDM%20both%20in%20theory%20and%20in%20simulations.%20In%20this%20sense%2C%20our%20study%20reveals%20the%20fundamental%20design%20trade-off%20between%20maximizing%20the%20relevant%20semantic%20information%20and%20matching%20the%20cognitive%20capabilities%20of%20the%20HDM%20model.%20Our%20initial%20analysis%20shows%20how%20semantic%20communication%20can%20balance%20the%20level%20of%20detail%20with%20human%20cognitive%20capabilities%20while%20demanding%20less%20bandwidth%2C%20power%2C%20and%20latency.&entry.1838667208=http%3A//arxiv.org/abs/2412.05103v4&entry.124074799=Read"},
{"title": "Current Agents Fail to Leverage World Model as Tool for Foresight", "author": "Cheng Qian and Emre Can Acikgoz and Bingxuan Li and Xiusi Chen and Yuji Zhang and Bingxiang He and Qinyu Luo and Dilek Hakkani-T\u00fcr and Gokhan Tur and Yunzhu Li and Heng Ji and Heng Ji", "abstract": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.", "link": "http://arxiv.org/abs/2601.03905v1", "date": "2026-01-07", "relevancy": 2.0422, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5337}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Current%20Agents%20Fail%20to%20Leverage%20World%20Model%20as%20Tool%20for%20Foresight&body=Title%3A%20Current%20Agents%20Fail%20to%20Leverage%20World%20Model%20as%20Tool%20for%20Foresight%0AAuthor%3A%20Cheng%20Qian%20and%20Emre%20Can%20Acikgoz%20and%20Bingxuan%20Li%20and%20Xiusi%20Chen%20and%20Yuji%20Zhang%20and%20Bingxiang%20He%20and%20Qinyu%20Luo%20and%20Dilek%20Hakkani-T%C3%BCr%20and%20Gokhan%20Tur%20and%20Yunzhu%20Li%20and%20Heng%20Ji%20and%20Heng%20Ji%0AAbstract%3A%20Agents%20built%20on%20vision-language%20models%20increasingly%20face%20tasks%20that%20demand%20anticipating%20future%20states%20rather%20than%20relying%20on%20short-horizon%20reasoning.%20Generative%20world%20models%20offer%20a%20promising%20remedy%3A%20agents%20could%20use%20them%20as%20external%20simulators%20to%20foresee%20outcomes%20before%20acting.%20This%20paper%20empirically%20examines%20whether%20current%20agents%20can%20leverage%20such%20world%20models%20as%20tools%20to%20enhance%20their%20cognition.%20Across%20diverse%20agentic%20and%20visual%20question%20answering%20tasks%2C%20we%20observe%20that%20some%20agents%20rarely%20invoke%20simulation%20%28fewer%20than%201%25%29%2C%20frequently%20misuse%20predicted%20rollouts%20%28approximately%2015%25%29%2C%20and%20often%20exhibit%20inconsistent%20or%20even%20degraded%20performance%20%28up%20to%205%25%29%20when%20simulation%20is%20available%20or%20enforced.%20Attribution%20analysis%20further%20indicates%20that%20the%20primary%20bottleneck%20lies%20in%20the%20agents%27%20capacity%20to%20decide%20when%20to%20simulate%2C%20how%20to%20interpret%20predicted%20outcomes%2C%20and%20how%20to%20integrate%20foresight%20into%20downstream%20reasoning.%20These%20findings%20underscore%20the%20need%20for%20mechanisms%20that%20foster%20calibrated%2C%20strategic%20interaction%20with%20world%20models%2C%20paving%20the%20way%20toward%20more%20reliable%20anticipatory%20cognition%20in%20future%20agent%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurrent%2520Agents%2520Fail%2520to%2520Leverage%2520World%2520Model%2520as%2520Tool%2520for%2520Foresight%26entry.906535625%3DCheng%2520Qian%2520and%2520Emre%2520Can%2520Acikgoz%2520and%2520Bingxuan%2520Li%2520and%2520Xiusi%2520Chen%2520and%2520Yuji%2520Zhang%2520and%2520Bingxiang%2520He%2520and%2520Qinyu%2520Luo%2520and%2520Dilek%2520Hakkani-T%25C3%25BCr%2520and%2520Gokhan%2520Tur%2520and%2520Yunzhu%2520Li%2520and%2520Heng%2520Ji%2520and%2520Heng%2520Ji%26entry.1292438233%3DAgents%2520built%2520on%2520vision-language%2520models%2520increasingly%2520face%2520tasks%2520that%2520demand%2520anticipating%2520future%2520states%2520rather%2520than%2520relying%2520on%2520short-horizon%2520reasoning.%2520Generative%2520world%2520models%2520offer%2520a%2520promising%2520remedy%253A%2520agents%2520could%2520use%2520them%2520as%2520external%2520simulators%2520to%2520foresee%2520outcomes%2520before%2520acting.%2520This%2520paper%2520empirically%2520examines%2520whether%2520current%2520agents%2520can%2520leverage%2520such%2520world%2520models%2520as%2520tools%2520to%2520enhance%2520their%2520cognition.%2520Across%2520diverse%2520agentic%2520and%2520visual%2520question%2520answering%2520tasks%252C%2520we%2520observe%2520that%2520some%2520agents%2520rarely%2520invoke%2520simulation%2520%2528fewer%2520than%25201%2525%2529%252C%2520frequently%2520misuse%2520predicted%2520rollouts%2520%2528approximately%252015%2525%2529%252C%2520and%2520often%2520exhibit%2520inconsistent%2520or%2520even%2520degraded%2520performance%2520%2528up%2520to%25205%2525%2529%2520when%2520simulation%2520is%2520available%2520or%2520enforced.%2520Attribution%2520analysis%2520further%2520indicates%2520that%2520the%2520primary%2520bottleneck%2520lies%2520in%2520the%2520agents%2527%2520capacity%2520to%2520decide%2520when%2520to%2520simulate%252C%2520how%2520to%2520interpret%2520predicted%2520outcomes%252C%2520and%2520how%2520to%2520integrate%2520foresight%2520into%2520downstream%2520reasoning.%2520These%2520findings%2520underscore%2520the%2520need%2520for%2520mechanisms%2520that%2520foster%2520calibrated%252C%2520strategic%2520interaction%2520with%2520world%2520models%252C%2520paving%2520the%2520way%2520toward%2520more%2520reliable%2520anticipatory%2520cognition%2520in%2520future%2520agent%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Current%20Agents%20Fail%20to%20Leverage%20World%20Model%20as%20Tool%20for%20Foresight&entry.906535625=Cheng%20Qian%20and%20Emre%20Can%20Acikgoz%20and%20Bingxuan%20Li%20and%20Xiusi%20Chen%20and%20Yuji%20Zhang%20and%20Bingxiang%20He%20and%20Qinyu%20Luo%20and%20Dilek%20Hakkani-T%C3%BCr%20and%20Gokhan%20Tur%20and%20Yunzhu%20Li%20and%20Heng%20Ji%20and%20Heng%20Ji&entry.1292438233=Agents%20built%20on%20vision-language%20models%20increasingly%20face%20tasks%20that%20demand%20anticipating%20future%20states%20rather%20than%20relying%20on%20short-horizon%20reasoning.%20Generative%20world%20models%20offer%20a%20promising%20remedy%3A%20agents%20could%20use%20them%20as%20external%20simulators%20to%20foresee%20outcomes%20before%20acting.%20This%20paper%20empirically%20examines%20whether%20current%20agents%20can%20leverage%20such%20world%20models%20as%20tools%20to%20enhance%20their%20cognition.%20Across%20diverse%20agentic%20and%20visual%20question%20answering%20tasks%2C%20we%20observe%20that%20some%20agents%20rarely%20invoke%20simulation%20%28fewer%20than%201%25%29%2C%20frequently%20misuse%20predicted%20rollouts%20%28approximately%2015%25%29%2C%20and%20often%20exhibit%20inconsistent%20or%20even%20degraded%20performance%20%28up%20to%205%25%29%20when%20simulation%20is%20available%20or%20enforced.%20Attribution%20analysis%20further%20indicates%20that%20the%20primary%20bottleneck%20lies%20in%20the%20agents%27%20capacity%20to%20decide%20when%20to%20simulate%2C%20how%20to%20interpret%20predicted%20outcomes%2C%20and%20how%20to%20integrate%20foresight%20into%20downstream%20reasoning.%20These%20findings%20underscore%20the%20need%20for%20mechanisms%20that%20foster%20calibrated%2C%20strategic%20interaction%20with%20world%20models%2C%20paving%20the%20way%20toward%20more%20reliable%20anticipatory%20cognition%20in%20future%20agent%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.03905v1&entry.124074799=Read"},
{"title": "An Event-Based Opto-Tactile Skin", "author": "Mohammadreza Koolani and Simeon Bamford and Petr Trunin and Simon F. M\u00fcller-Cleve and Matteo Lo Preti and Fulvio Mastrogiovanni and Lucia Beccai and Chiara Bartolozzi", "abstract": "This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms.", "link": "http://arxiv.org/abs/2601.03907v1", "date": "2026-01-07", "relevancy": 2.0375, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5466}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5161}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Event-Based%20Opto-Tactile%20Skin&body=Title%3A%20An%20Event-Based%20Opto-Tactile%20Skin%0AAuthor%3A%20Mohammadreza%20Koolani%20and%20Simeon%20Bamford%20and%20Petr%20Trunin%20and%20Simon%20F.%20M%C3%BCller-Cleve%20and%20Matteo%20Lo%20Preti%20and%20Fulvio%20Mastrogiovanni%20and%20Lucia%20Beccai%20and%20Chiara%20Bartolozzi%0AAbstract%3A%20This%20paper%20presents%20a%20neuromorphic%2C%20event-driven%20tactile%20sensing%20system%20for%20soft%2C%20large-area%20skin%2C%20based%20on%20the%20Dynamic%20Vision%20Sensors%20%28DVS%29%20integrated%20with%20a%20flexible%20silicone%20optical%20waveguide%20skin.%20Instead%20of%20repetitively%20scanning%20embedded%20photoreceivers%2C%20this%20design%20uses%20a%20stereo%20vision%20setup%20comprising%20two%20DVS%20cameras%20looking%20sideways%20through%20the%20skin.%20Such%20a%20design%20produces%20events%20as%20changes%20in%20brightness%20are%20detected%2C%20and%20estimates%20press%20positions%20on%20the%202D%20skin%20surface%20through%20triangulation%2C%20utilizing%20Density-Based%20Spatial%20Clustering%20of%20Applications%20with%20Noise%20%28DBSCAN%29%20to%20find%20the%20center%20of%20mass%20of%20contact%20events%20resulting%20from%20pressing%20actions.%20The%20system%20is%20evaluated%20over%20a%204620%20mm2%20probed%20area%20of%20the%20skin%20using%20a%20meander%20raster%20scan.%20Across%2095%20%25%20of%20the%20presses%20visible%20to%20both%20cameras%2C%20the%20press%20localization%20achieved%20a%20Root-Mean-Squared%20Error%20%28RMSE%29%20of%204.66%20mm.%20The%20results%20highlight%20the%20potential%20of%20this%20approach%20for%20wide-area%20flexible%20and%20responsive%20tactile%20sensors%20in%20soft%20robotics%20and%20interactive%20environments.%20Moreover%2C%20we%20examined%20how%20the%20system%20performs%20when%20the%20amount%20of%20event%20data%20is%20strongly%20reduced.%20Using%20stochastic%20down-sampling%2C%20the%20event%20stream%20was%20reduced%20to%201/1024%20of%20its%20original%20size.%20Under%20this%20extreme%20reduction%2C%20the%20average%20localization%20error%20increased%20only%20slightly%20%28from%204.66%20mm%20to%209.33%20mm%29%2C%20and%20the%20system%20still%20produced%20valid%20press%20localizations%20for%2085%20%25%20of%20the%20trials.%20This%20reduction%20in%20pass%20rate%20is%20expected%2C%20as%20some%20presses%20no%20longer%20produce%20enough%20events%20to%20form%20a%20reliable%20cluster%20for%20triangulation.%20These%20results%20show%20that%20the%20sensing%20approach%20remains%20functional%20even%20with%20very%20sparse%20event%20data%2C%20which%20is%20promising%20for%20reducing%20power%20consumption%20and%20computational%20load%20in%20future%20implementations.%20The%20system%20exhibits%20a%20detection%20latency%20distribution%20with%20a%20characteristic%20width%20of%2031%20ms.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Event-Based%2520Opto-Tactile%2520Skin%26entry.906535625%3DMohammadreza%2520Koolani%2520and%2520Simeon%2520Bamford%2520and%2520Petr%2520Trunin%2520and%2520Simon%2520F.%2520M%25C3%25BCller-Cleve%2520and%2520Matteo%2520Lo%2520Preti%2520and%2520Fulvio%2520Mastrogiovanni%2520and%2520Lucia%2520Beccai%2520and%2520Chiara%2520Bartolozzi%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520neuromorphic%252C%2520event-driven%2520tactile%2520sensing%2520system%2520for%2520soft%252C%2520large-area%2520skin%252C%2520based%2520on%2520the%2520Dynamic%2520Vision%2520Sensors%2520%2528DVS%2529%2520integrated%2520with%2520a%2520flexible%2520silicone%2520optical%2520waveguide%2520skin.%2520Instead%2520of%2520repetitively%2520scanning%2520embedded%2520photoreceivers%252C%2520this%2520design%2520uses%2520a%2520stereo%2520vision%2520setup%2520comprising%2520two%2520DVS%2520cameras%2520looking%2520sideways%2520through%2520the%2520skin.%2520Such%2520a%2520design%2520produces%2520events%2520as%2520changes%2520in%2520brightness%2520are%2520detected%252C%2520and%2520estimates%2520press%2520positions%2520on%2520the%25202D%2520skin%2520surface%2520through%2520triangulation%252C%2520utilizing%2520Density-Based%2520Spatial%2520Clustering%2520of%2520Applications%2520with%2520Noise%2520%2528DBSCAN%2529%2520to%2520find%2520the%2520center%2520of%2520mass%2520of%2520contact%2520events%2520resulting%2520from%2520pressing%2520actions.%2520The%2520system%2520is%2520evaluated%2520over%2520a%25204620%2520mm2%2520probed%2520area%2520of%2520the%2520skin%2520using%2520a%2520meander%2520raster%2520scan.%2520Across%252095%2520%2525%2520of%2520the%2520presses%2520visible%2520to%2520both%2520cameras%252C%2520the%2520press%2520localization%2520achieved%2520a%2520Root-Mean-Squared%2520Error%2520%2528RMSE%2529%2520of%25204.66%2520mm.%2520The%2520results%2520highlight%2520the%2520potential%2520of%2520this%2520approach%2520for%2520wide-area%2520flexible%2520and%2520responsive%2520tactile%2520sensors%2520in%2520soft%2520robotics%2520and%2520interactive%2520environments.%2520Moreover%252C%2520we%2520examined%2520how%2520the%2520system%2520performs%2520when%2520the%2520amount%2520of%2520event%2520data%2520is%2520strongly%2520reduced.%2520Using%2520stochastic%2520down-sampling%252C%2520the%2520event%2520stream%2520was%2520reduced%2520to%25201/1024%2520of%2520its%2520original%2520size.%2520Under%2520this%2520extreme%2520reduction%252C%2520the%2520average%2520localization%2520error%2520increased%2520only%2520slightly%2520%2528from%25204.66%2520mm%2520to%25209.33%2520mm%2529%252C%2520and%2520the%2520system%2520still%2520produced%2520valid%2520press%2520localizations%2520for%252085%2520%2525%2520of%2520the%2520trials.%2520This%2520reduction%2520in%2520pass%2520rate%2520is%2520expected%252C%2520as%2520some%2520presses%2520no%2520longer%2520produce%2520enough%2520events%2520to%2520form%2520a%2520reliable%2520cluster%2520for%2520triangulation.%2520These%2520results%2520show%2520that%2520the%2520sensing%2520approach%2520remains%2520functional%2520even%2520with%2520very%2520sparse%2520event%2520data%252C%2520which%2520is%2520promising%2520for%2520reducing%2520power%2520consumption%2520and%2520computational%2520load%2520in%2520future%2520implementations.%2520The%2520system%2520exhibits%2520a%2520detection%2520latency%2520distribution%2520with%2520a%2520characteristic%2520width%2520of%252031%2520ms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Event-Based%20Opto-Tactile%20Skin&entry.906535625=Mohammadreza%20Koolani%20and%20Simeon%20Bamford%20and%20Petr%20Trunin%20and%20Simon%20F.%20M%C3%BCller-Cleve%20and%20Matteo%20Lo%20Preti%20and%20Fulvio%20Mastrogiovanni%20and%20Lucia%20Beccai%20and%20Chiara%20Bartolozzi&entry.1292438233=This%20paper%20presents%20a%20neuromorphic%2C%20event-driven%20tactile%20sensing%20system%20for%20soft%2C%20large-area%20skin%2C%20based%20on%20the%20Dynamic%20Vision%20Sensors%20%28DVS%29%20integrated%20with%20a%20flexible%20silicone%20optical%20waveguide%20skin.%20Instead%20of%20repetitively%20scanning%20embedded%20photoreceivers%2C%20this%20design%20uses%20a%20stereo%20vision%20setup%20comprising%20two%20DVS%20cameras%20looking%20sideways%20through%20the%20skin.%20Such%20a%20design%20produces%20events%20as%20changes%20in%20brightness%20are%20detected%2C%20and%20estimates%20press%20positions%20on%20the%202D%20skin%20surface%20through%20triangulation%2C%20utilizing%20Density-Based%20Spatial%20Clustering%20of%20Applications%20with%20Noise%20%28DBSCAN%29%20to%20find%20the%20center%20of%20mass%20of%20contact%20events%20resulting%20from%20pressing%20actions.%20The%20system%20is%20evaluated%20over%20a%204620%20mm2%20probed%20area%20of%20the%20skin%20using%20a%20meander%20raster%20scan.%20Across%2095%20%25%20of%20the%20presses%20visible%20to%20both%20cameras%2C%20the%20press%20localization%20achieved%20a%20Root-Mean-Squared%20Error%20%28RMSE%29%20of%204.66%20mm.%20The%20results%20highlight%20the%20potential%20of%20this%20approach%20for%20wide-area%20flexible%20and%20responsive%20tactile%20sensors%20in%20soft%20robotics%20and%20interactive%20environments.%20Moreover%2C%20we%20examined%20how%20the%20system%20performs%20when%20the%20amount%20of%20event%20data%20is%20strongly%20reduced.%20Using%20stochastic%20down-sampling%2C%20the%20event%20stream%20was%20reduced%20to%201/1024%20of%20its%20original%20size.%20Under%20this%20extreme%20reduction%2C%20the%20average%20localization%20error%20increased%20only%20slightly%20%28from%204.66%20mm%20to%209.33%20mm%29%2C%20and%20the%20system%20still%20produced%20valid%20press%20localizations%20for%2085%20%25%20of%20the%20trials.%20This%20reduction%20in%20pass%20rate%20is%20expected%2C%20as%20some%20presses%20no%20longer%20produce%20enough%20events%20to%20form%20a%20reliable%20cluster%20for%20triangulation.%20These%20results%20show%20that%20the%20sensing%20approach%20remains%20functional%20even%20with%20very%20sparse%20event%20data%2C%20which%20is%20promising%20for%20reducing%20power%20consumption%20and%20computational%20load%20in%20future%20implementations.%20The%20system%20exhibits%20a%20detection%20latency%20distribution%20with%20a%20characteristic%20width%20of%2031%20ms.&entry.1838667208=http%3A//arxiv.org/abs/2601.03907v1&entry.124074799=Read"},
{"title": "A Single-Loop Bilevel Deep Learning Method for Optimal Control of Obstacle Problems", "author": "Yongcun Song and Shangzhi Zeng and Jin Zhang and Lvgang Zhang", "abstract": "Optimal control of obstacle problems arises in a wide range of applications and is computationally challenging due to its nonsmoothness, nonlinearity, and bilevel structure. Classical numerical approaches rely on mesh-based discretization and typically require solving a sequence of costly subproblems. In this work, we propose a single-loop bilevel deep learning method, which is mesh-free, scalable to high-dimensional and complex domains, and avoids repeated solution of discretized subproblems. The method employs constraint-embedding neural networks to approximate the state and control and preserves the bilevel structure. To train the neural networks efficiently, we propose a Single-Loop Stochastic First-Order Bilevel Algorithm (S2-FOBA), which eliminates nested optimization and does not rely on restrictive lower-level uniqueness assumptions. We analyze the convergence behavior of S2-FOBA under mild assumptions. Numerical experiments on benchmark examples, including distributed and obstacle control problems with regular and irregular obstacles on complex domains, demonstrate that the proposed method achieves satisfactory accuracy while reducing computational cost compared to classical numerical methods.", "link": "http://arxiv.org/abs/2601.04120v1", "date": "2026-01-07", "relevancy": 2.0368, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5325}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Single-Loop%20Bilevel%20Deep%20Learning%20Method%20for%20Optimal%20Control%20of%20Obstacle%20Problems&body=Title%3A%20A%20Single-Loop%20Bilevel%20Deep%20Learning%20Method%20for%20Optimal%20Control%20of%20Obstacle%20Problems%0AAuthor%3A%20Yongcun%20Song%20and%20Shangzhi%20Zeng%20and%20Jin%20Zhang%20and%20Lvgang%20Zhang%0AAbstract%3A%20Optimal%20control%20of%20obstacle%20problems%20arises%20in%20a%20wide%20range%20of%20applications%20and%20is%20computationally%20challenging%20due%20to%20its%20nonsmoothness%2C%20nonlinearity%2C%20and%20bilevel%20structure.%20Classical%20numerical%20approaches%20rely%20on%20mesh-based%20discretization%20and%20typically%20require%20solving%20a%20sequence%20of%20costly%20subproblems.%20In%20this%20work%2C%20we%20propose%20a%20single-loop%20bilevel%20deep%20learning%20method%2C%20which%20is%20mesh-free%2C%20scalable%20to%20high-dimensional%20and%20complex%20domains%2C%20and%20avoids%20repeated%20solution%20of%20discretized%20subproblems.%20The%20method%20employs%20constraint-embedding%20neural%20networks%20to%20approximate%20the%20state%20and%20control%20and%20preserves%20the%20bilevel%20structure.%20To%20train%20the%20neural%20networks%20efficiently%2C%20we%20propose%20a%20Single-Loop%20Stochastic%20First-Order%20Bilevel%20Algorithm%20%28S2-FOBA%29%2C%20which%20eliminates%20nested%20optimization%20and%20does%20not%20rely%20on%20restrictive%20lower-level%20uniqueness%20assumptions.%20We%20analyze%20the%20convergence%20behavior%20of%20S2-FOBA%20under%20mild%20assumptions.%20Numerical%20experiments%20on%20benchmark%20examples%2C%20including%20distributed%20and%20obstacle%20control%20problems%20with%20regular%20and%20irregular%20obstacles%20on%20complex%20domains%2C%20demonstrate%20that%20the%20proposed%20method%20achieves%20satisfactory%20accuracy%20while%20reducing%20computational%20cost%20compared%20to%20classical%20numerical%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Single-Loop%2520Bilevel%2520Deep%2520Learning%2520Method%2520for%2520Optimal%2520Control%2520of%2520Obstacle%2520Problems%26entry.906535625%3DYongcun%2520Song%2520and%2520Shangzhi%2520Zeng%2520and%2520Jin%2520Zhang%2520and%2520Lvgang%2520Zhang%26entry.1292438233%3DOptimal%2520control%2520of%2520obstacle%2520problems%2520arises%2520in%2520a%2520wide%2520range%2520of%2520applications%2520and%2520is%2520computationally%2520challenging%2520due%2520to%2520its%2520nonsmoothness%252C%2520nonlinearity%252C%2520and%2520bilevel%2520structure.%2520Classical%2520numerical%2520approaches%2520rely%2520on%2520mesh-based%2520discretization%2520and%2520typically%2520require%2520solving%2520a%2520sequence%2520of%2520costly%2520subproblems.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520single-loop%2520bilevel%2520deep%2520learning%2520method%252C%2520which%2520is%2520mesh-free%252C%2520scalable%2520to%2520high-dimensional%2520and%2520complex%2520domains%252C%2520and%2520avoids%2520repeated%2520solution%2520of%2520discretized%2520subproblems.%2520The%2520method%2520employs%2520constraint-embedding%2520neural%2520networks%2520to%2520approximate%2520the%2520state%2520and%2520control%2520and%2520preserves%2520the%2520bilevel%2520structure.%2520To%2520train%2520the%2520neural%2520networks%2520efficiently%252C%2520we%2520propose%2520a%2520Single-Loop%2520Stochastic%2520First-Order%2520Bilevel%2520Algorithm%2520%2528S2-FOBA%2529%252C%2520which%2520eliminates%2520nested%2520optimization%2520and%2520does%2520not%2520rely%2520on%2520restrictive%2520lower-level%2520uniqueness%2520assumptions.%2520We%2520analyze%2520the%2520convergence%2520behavior%2520of%2520S2-FOBA%2520under%2520mild%2520assumptions.%2520Numerical%2520experiments%2520on%2520benchmark%2520examples%252C%2520including%2520distributed%2520and%2520obstacle%2520control%2520problems%2520with%2520regular%2520and%2520irregular%2520obstacles%2520on%2520complex%2520domains%252C%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520satisfactory%2520accuracy%2520while%2520reducing%2520computational%2520cost%2520compared%2520to%2520classical%2520numerical%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Single-Loop%20Bilevel%20Deep%20Learning%20Method%20for%20Optimal%20Control%20of%20Obstacle%20Problems&entry.906535625=Yongcun%20Song%20and%20Shangzhi%20Zeng%20and%20Jin%20Zhang%20and%20Lvgang%20Zhang&entry.1292438233=Optimal%20control%20of%20obstacle%20problems%20arises%20in%20a%20wide%20range%20of%20applications%20and%20is%20computationally%20challenging%20due%20to%20its%20nonsmoothness%2C%20nonlinearity%2C%20and%20bilevel%20structure.%20Classical%20numerical%20approaches%20rely%20on%20mesh-based%20discretization%20and%20typically%20require%20solving%20a%20sequence%20of%20costly%20subproblems.%20In%20this%20work%2C%20we%20propose%20a%20single-loop%20bilevel%20deep%20learning%20method%2C%20which%20is%20mesh-free%2C%20scalable%20to%20high-dimensional%20and%20complex%20domains%2C%20and%20avoids%20repeated%20solution%20of%20discretized%20subproblems.%20The%20method%20employs%20constraint-embedding%20neural%20networks%20to%20approximate%20the%20state%20and%20control%20and%20preserves%20the%20bilevel%20structure.%20To%20train%20the%20neural%20networks%20efficiently%2C%20we%20propose%20a%20Single-Loop%20Stochastic%20First-Order%20Bilevel%20Algorithm%20%28S2-FOBA%29%2C%20which%20eliminates%20nested%20optimization%20and%20does%20not%20rely%20on%20restrictive%20lower-level%20uniqueness%20assumptions.%20We%20analyze%20the%20convergence%20behavior%20of%20S2-FOBA%20under%20mild%20assumptions.%20Numerical%20experiments%20on%20benchmark%20examples%2C%20including%20distributed%20and%20obstacle%20control%20problems%20with%20regular%20and%20irregular%20obstacles%20on%20complex%20domains%2C%20demonstrate%20that%20the%20proposed%20method%20achieves%20satisfactory%20accuracy%20while%20reducing%20computational%20cost%20compared%20to%20classical%20numerical%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.04120v1&entry.124074799=Read"},
{"title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring", "author": "Peichun Hua and Hao Li and Shanghao Shi and Zhiyuan Yu and Ning Zhang", "abstract": "Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.", "link": "http://arxiv.org/abs/2512.12069v2", "date": "2026-01-07", "relevancy": 2.0334, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Jailbreak%20Detection%20of%20Large%20Vision%20Language%20Models%20with%20Representational%20Contrastive%20Scoring&body=Title%3A%20Rethinking%20Jailbreak%20Detection%20of%20Large%20Vision%20Language%20Models%20with%20Representational%20Contrastive%20Scoring%0AAuthor%3A%20Peichun%20Hua%20and%20Hao%20Li%20and%20Shanghao%20Shi%20and%20Zhiyuan%20Yu%20and%20Ning%20Zhang%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20vulnerable%20to%20a%20growing%20array%20of%20multimodal%20jailbreak%20attacks%2C%20necessitating%20defenses%20that%20are%20both%20generalizable%20to%20novel%20threats%20and%20efficient%20for%20practical%20deployment.%20Many%20current%20strategies%20fall%20short%2C%20either%20targeting%20specific%20attack%20patterns%2C%20which%20limits%20generalization%2C%20or%20imposing%20high%20computational%20overhead.%20While%20lightweight%20anomaly-detection%20methods%20offer%20a%20promising%20direction%2C%20we%20find%20that%20their%20common%20one-class%20design%20tends%20to%20confuse%20novel%20benign%20inputs%20with%20malicious%20ones%2C%20leading%20to%20unreliable%20over-rejection.%20To%20address%20this%2C%20we%20propose%20Representational%20Contrastive%20Scoring%20%28RCS%29%2C%20a%20framework%20built%20on%20a%20key%20insight%3A%20the%20most%20potent%20safety%20signals%20reside%20within%20the%20LVLM%27s%20own%20internal%20representations.%20Our%20approach%20inspects%20the%20internal%20geometry%20of%20these%20representations%2C%20learning%20a%20lightweight%20projection%20to%20maximally%20separate%20benign%20and%20malicious%20inputs%20in%20safety-critical%20layers.%20This%20enables%20a%20simple%20yet%20powerful%20contrastive%20score%20that%20differentiates%20true%20malicious%20intent%20from%20mere%20novelty.%20Our%20instantiations%2C%20MCD%20%28Mahalanobis%20Contrastive%20Detection%29%20and%20KCD%20%28K-nearest%20Contrastive%20Detection%29%2C%20achieve%20state-of-the-art%20performance%20on%20a%20challenging%20evaluation%20protocol%20designed%20to%20test%20generalization%20to%20unseen%20attack%20types.%20This%20work%20demonstrates%20that%20effective%20jailbreak%20detection%20can%20be%20achieved%20by%20applying%20simple%2C%20interpretable%20statistical%20methods%20to%20the%20appropriate%20internal%20representations%2C%20offering%20a%20practical%20path%20towards%20safer%20LVLM%20deployment.%20Our%20code%20is%20available%20on%20Github%20https%3A//github.com/sarendis56/Jailbreak_Detection_RCS.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Jailbreak%2520Detection%2520of%2520Large%2520Vision%2520Language%2520Models%2520with%2520Representational%2520Contrastive%2520Scoring%26entry.906535625%3DPeichun%2520Hua%2520and%2520Hao%2520Li%2520and%2520Shanghao%2520Shi%2520and%2520Zhiyuan%2520Yu%2520and%2520Ning%2520Zhang%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520are%2520vulnerable%2520to%2520a%2520growing%2520array%2520of%2520multimodal%2520jailbreak%2520attacks%252C%2520necessitating%2520defenses%2520that%2520are%2520both%2520generalizable%2520to%2520novel%2520threats%2520and%2520efficient%2520for%2520practical%2520deployment.%2520Many%2520current%2520strategies%2520fall%2520short%252C%2520either%2520targeting%2520specific%2520attack%2520patterns%252C%2520which%2520limits%2520generalization%252C%2520or%2520imposing%2520high%2520computational%2520overhead.%2520While%2520lightweight%2520anomaly-detection%2520methods%2520offer%2520a%2520promising%2520direction%252C%2520we%2520find%2520that%2520their%2520common%2520one-class%2520design%2520tends%2520to%2520confuse%2520novel%2520benign%2520inputs%2520with%2520malicious%2520ones%252C%2520leading%2520to%2520unreliable%2520over-rejection.%2520To%2520address%2520this%252C%2520we%2520propose%2520Representational%2520Contrastive%2520Scoring%2520%2528RCS%2529%252C%2520a%2520framework%2520built%2520on%2520a%2520key%2520insight%253A%2520the%2520most%2520potent%2520safety%2520signals%2520reside%2520within%2520the%2520LVLM%2527s%2520own%2520internal%2520representations.%2520Our%2520approach%2520inspects%2520the%2520internal%2520geometry%2520of%2520these%2520representations%252C%2520learning%2520a%2520lightweight%2520projection%2520to%2520maximally%2520separate%2520benign%2520and%2520malicious%2520inputs%2520in%2520safety-critical%2520layers.%2520This%2520enables%2520a%2520simple%2520yet%2520powerful%2520contrastive%2520score%2520that%2520differentiates%2520true%2520malicious%2520intent%2520from%2520mere%2520novelty.%2520Our%2520instantiations%252C%2520MCD%2520%2528Mahalanobis%2520Contrastive%2520Detection%2529%2520and%2520KCD%2520%2528K-nearest%2520Contrastive%2520Detection%2529%252C%2520achieve%2520state-of-the-art%2520performance%2520on%2520a%2520challenging%2520evaluation%2520protocol%2520designed%2520to%2520test%2520generalization%2520to%2520unseen%2520attack%2520types.%2520This%2520work%2520demonstrates%2520that%2520effective%2520jailbreak%2520detection%2520can%2520be%2520achieved%2520by%2520applying%2520simple%252C%2520interpretable%2520statistical%2520methods%2520to%2520the%2520appropriate%2520internal%2520representations%252C%2520offering%2520a%2520practical%2520path%2520towards%2520safer%2520LVLM%2520deployment.%2520Our%2520code%2520is%2520available%2520on%2520Github%2520https%253A//github.com/sarendis56/Jailbreak_Detection_RCS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Jailbreak%20Detection%20of%20Large%20Vision%20Language%20Models%20with%20Representational%20Contrastive%20Scoring&entry.906535625=Peichun%20Hua%20and%20Hao%20Li%20and%20Shanghao%20Shi%20and%20Zhiyuan%20Yu%20and%20Ning%20Zhang&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20vulnerable%20to%20a%20growing%20array%20of%20multimodal%20jailbreak%20attacks%2C%20necessitating%20defenses%20that%20are%20both%20generalizable%20to%20novel%20threats%20and%20efficient%20for%20practical%20deployment.%20Many%20current%20strategies%20fall%20short%2C%20either%20targeting%20specific%20attack%20patterns%2C%20which%20limits%20generalization%2C%20or%20imposing%20high%20computational%20overhead.%20While%20lightweight%20anomaly-detection%20methods%20offer%20a%20promising%20direction%2C%20we%20find%20that%20their%20common%20one-class%20design%20tends%20to%20confuse%20novel%20benign%20inputs%20with%20malicious%20ones%2C%20leading%20to%20unreliable%20over-rejection.%20To%20address%20this%2C%20we%20propose%20Representational%20Contrastive%20Scoring%20%28RCS%29%2C%20a%20framework%20built%20on%20a%20key%20insight%3A%20the%20most%20potent%20safety%20signals%20reside%20within%20the%20LVLM%27s%20own%20internal%20representations.%20Our%20approach%20inspects%20the%20internal%20geometry%20of%20these%20representations%2C%20learning%20a%20lightweight%20projection%20to%20maximally%20separate%20benign%20and%20malicious%20inputs%20in%20safety-critical%20layers.%20This%20enables%20a%20simple%20yet%20powerful%20contrastive%20score%20that%20differentiates%20true%20malicious%20intent%20from%20mere%20novelty.%20Our%20instantiations%2C%20MCD%20%28Mahalanobis%20Contrastive%20Detection%29%20and%20KCD%20%28K-nearest%20Contrastive%20Detection%29%2C%20achieve%20state-of-the-art%20performance%20on%20a%20challenging%20evaluation%20protocol%20designed%20to%20test%20generalization%20to%20unseen%20attack%20types.%20This%20work%20demonstrates%20that%20effective%20jailbreak%20detection%20can%20be%20achieved%20by%20applying%20simple%2C%20interpretable%20statistical%20methods%20to%20the%20appropriate%20internal%20representations%2C%20offering%20a%20practical%20path%20towards%20safer%20LVLM%20deployment.%20Our%20code%20is%20available%20on%20Github%20https%3A//github.com/sarendis56/Jailbreak_Detection_RCS.&entry.1838667208=http%3A//arxiv.org/abs/2512.12069v2&entry.124074799=Read"},
{"title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models", "author": "Hongyu Wang and Jiayu Xu and Ruiping Wang and Yan Feng and Yitao Zhai and Peng Pei and Xunliang Cai and Xilin Chen", "abstract": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.", "link": "http://arxiv.org/abs/2506.14435v2", "date": "2026-01-07", "relevancy": 2.0322, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoTE%3A%20Mixture%20of%20Ternary%20Experts%20for%20Memory-efficient%20Large%20Multimodal%20Models&body=Title%3A%20MoTE%3A%20Mixture%20of%20Ternary%20Experts%20for%20Memory-efficient%20Large%20Multimodal%20Models%0AAuthor%3A%20Hongyu%20Wang%20and%20Jiayu%20Xu%20and%20Ruiping%20Wang%20and%20Yan%20Feng%20and%20Yitao%20Zhai%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Xilin%20Chen%0AAbstract%3A%20Large%20multimodal%20Mixture-of-Experts%20%28MoEs%29%20effectively%20scale%20the%20model%20size%20to%20boost%20performance%20while%20maintaining%20fixed%20active%20parameters.%20However%2C%20previous%20works%20primarily%20utilized%20full-precision%20experts%20during%20sparse%20up-cycling.%20Despite%20they%20show%20superior%20performance%20on%20end%20tasks%2C%20the%20large%20amount%20of%20experts%20introduces%20higher%20memory%20footprint%2C%20which%20poses%20significant%20challenges%20for%20the%20deployment%20on%20edge%20devices.%20In%20this%20work%2C%20we%20propose%20MoTE%2C%20a%20scalable%20and%20memory-efficient%20approach%20to%20train%20Mixture-of-Ternary-Experts%20models%20from%20dense%20checkpoint.%20Instead%20of%20training%20fewer%20high-precision%20experts%2C%20we%20propose%20to%20train%20more%20low-precision%20experts%20during%20up-cycling.%20Specifically%2C%20we%20use%20the%20pre-trained%20FFN%20as%20a%20shared%20expert%20and%20train%20ternary%20routed%20experts%20with%20parameters%20in%20%7B-1%2C%200%2C%201%7D.%20Extensive%20experiments%20show%20that%20our%20approach%20has%20promising%20scaling%20trend%20along%20model%20size.%20MoTE%20achieves%20comparable%20performance%20to%20full-precision%20baseline%20MoE-LLaVA%20while%20offering%20lower%20memory%20footprint.%20Furthermore%2C%20our%20approach%20is%20compatible%20with%20post-training%20quantization%20methods%20and%20the%20advantage%20further%20amplifies%20when%20memory-constraint%20goes%20lower.%20Given%20the%20same%20amount%20of%20expert%20memory%20footprint%20of%203.4GB%20and%20combined%20with%20post-training%20quantization%2C%20MoTE%20outperforms%20MoE-LLaVA%20by%20a%20gain%20of%204.3%25%20average%20accuracy%20on%20end%20tasks%2C%20demonstrating%20its%20effectiveness%20and%20potential%20for%20memory-constrained%20devices.%0ALink%3A%20http%3A//arxiv.org/abs/2506.14435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoTE%253A%2520Mixture%2520of%2520Ternary%2520Experts%2520for%2520Memory-efficient%2520Large%2520Multimodal%2520Models%26entry.906535625%3DHongyu%2520Wang%2520and%2520Jiayu%2520Xu%2520and%2520Ruiping%2520Wang%2520and%2520Yan%2520Feng%2520and%2520Yitao%2520Zhai%2520and%2520Peng%2520Pei%2520and%2520Xunliang%2520Cai%2520and%2520Xilin%2520Chen%26entry.1292438233%3DLarge%2520multimodal%2520Mixture-of-Experts%2520%2528MoEs%2529%2520effectively%2520scale%2520the%2520model%2520size%2520to%2520boost%2520performance%2520while%2520maintaining%2520fixed%2520active%2520parameters.%2520However%252C%2520previous%2520works%2520primarily%2520utilized%2520full-precision%2520experts%2520during%2520sparse%2520up-cycling.%2520Despite%2520they%2520show%2520superior%2520performance%2520on%2520end%2520tasks%252C%2520the%2520large%2520amount%2520of%2520experts%2520introduces%2520higher%2520memory%2520footprint%252C%2520which%2520poses%2520significant%2520challenges%2520for%2520the%2520deployment%2520on%2520edge%2520devices.%2520In%2520this%2520work%252C%2520we%2520propose%2520MoTE%252C%2520a%2520scalable%2520and%2520memory-efficient%2520approach%2520to%2520train%2520Mixture-of-Ternary-Experts%2520models%2520from%2520dense%2520checkpoint.%2520Instead%2520of%2520training%2520fewer%2520high-precision%2520experts%252C%2520we%2520propose%2520to%2520train%2520more%2520low-precision%2520experts%2520during%2520up-cycling.%2520Specifically%252C%2520we%2520use%2520the%2520pre-trained%2520FFN%2520as%2520a%2520shared%2520expert%2520and%2520train%2520ternary%2520routed%2520experts%2520with%2520parameters%2520in%2520%257B-1%252C%25200%252C%25201%257D.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%2520has%2520promising%2520scaling%2520trend%2520along%2520model%2520size.%2520MoTE%2520achieves%2520comparable%2520performance%2520to%2520full-precision%2520baseline%2520MoE-LLaVA%2520while%2520offering%2520lower%2520memory%2520footprint.%2520Furthermore%252C%2520our%2520approach%2520is%2520compatible%2520with%2520post-training%2520quantization%2520methods%2520and%2520the%2520advantage%2520further%2520amplifies%2520when%2520memory-constraint%2520goes%2520lower.%2520Given%2520the%2520same%2520amount%2520of%2520expert%2520memory%2520footprint%2520of%25203.4GB%2520and%2520combined%2520with%2520post-training%2520quantization%252C%2520MoTE%2520outperforms%2520MoE-LLaVA%2520by%2520a%2520gain%2520of%25204.3%2525%2520average%2520accuracy%2520on%2520end%2520tasks%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520potential%2520for%2520memory-constrained%2520devices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoTE%3A%20Mixture%20of%20Ternary%20Experts%20for%20Memory-efficient%20Large%20Multimodal%20Models&entry.906535625=Hongyu%20Wang%20and%20Jiayu%20Xu%20and%20Ruiping%20Wang%20and%20Yan%20Feng%20and%20Yitao%20Zhai%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Xilin%20Chen&entry.1292438233=Large%20multimodal%20Mixture-of-Experts%20%28MoEs%29%20effectively%20scale%20the%20model%20size%20to%20boost%20performance%20while%20maintaining%20fixed%20active%20parameters.%20However%2C%20previous%20works%20primarily%20utilized%20full-precision%20experts%20during%20sparse%20up-cycling.%20Despite%20they%20show%20superior%20performance%20on%20end%20tasks%2C%20the%20large%20amount%20of%20experts%20introduces%20higher%20memory%20footprint%2C%20which%20poses%20significant%20challenges%20for%20the%20deployment%20on%20edge%20devices.%20In%20this%20work%2C%20we%20propose%20MoTE%2C%20a%20scalable%20and%20memory-efficient%20approach%20to%20train%20Mixture-of-Ternary-Experts%20models%20from%20dense%20checkpoint.%20Instead%20of%20training%20fewer%20high-precision%20experts%2C%20we%20propose%20to%20train%20more%20low-precision%20experts%20during%20up-cycling.%20Specifically%2C%20we%20use%20the%20pre-trained%20FFN%20as%20a%20shared%20expert%20and%20train%20ternary%20routed%20experts%20with%20parameters%20in%20%7B-1%2C%200%2C%201%7D.%20Extensive%20experiments%20show%20that%20our%20approach%20has%20promising%20scaling%20trend%20along%20model%20size.%20MoTE%20achieves%20comparable%20performance%20to%20full-precision%20baseline%20MoE-LLaVA%20while%20offering%20lower%20memory%20footprint.%20Furthermore%2C%20our%20approach%20is%20compatible%20with%20post-training%20quantization%20methods%20and%20the%20advantage%20further%20amplifies%20when%20memory-constraint%20goes%20lower.%20Given%20the%20same%20amount%20of%20expert%20memory%20footprint%20of%203.4GB%20and%20combined%20with%20post-training%20quantization%2C%20MoTE%20outperforms%20MoE-LLaVA%20by%20a%20gain%20of%204.3%25%20average%20accuracy%20on%20end%20tasks%2C%20demonstrating%20its%20effectiveness%20and%20potential%20for%20memory-constrained%20devices.&entry.1838667208=http%3A//arxiv.org/abs/2506.14435v2&entry.124074799=Read"},
{"title": "MORPHFED: Federated Learning for Cross-institutional Blood Morphology Analysis", "author": "Gabriel Ansah and Eden Ruffell and Delmiro Fernandez-Reyes and Petru Manescu", "abstract": "Automated blood morphology analysis can support hematological diagnostics in low- and middle-income countries (LMICs) but remains sensitive to dataset shifts from staining variability, imaging differences, and rare morphologies. Building centralized datasets to capture this diversity is often infeasible due to privacy regulations and data-sharing restrictions. We introduce a federated learning framework for white blood cell morphology analysis that enables collaborative training across institutions without exchanging training data. Using blood films from multiple clinical sites, our federated models learn robust, domain-invariant representations while preserving complete data privacy. Evaluations across convolutional and transformer-based architectures show that federated training achieves strong cross-site performance and improved generalization to unseen institutions compared to centralized training. These findings highlight federated learning as a practical and privacy-preserving approach for developing equitable, scalable, and generalizable medical imaging AI in resource-limited healthcare environments.", "link": "http://arxiv.org/abs/2601.04121v1", "date": "2026-01-07", "relevancy": 2.0266, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5187}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4986}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MORPHFED%3A%20Federated%20Learning%20for%20Cross-institutional%20Blood%20Morphology%20Analysis&body=Title%3A%20MORPHFED%3A%20Federated%20Learning%20for%20Cross-institutional%20Blood%20Morphology%20Analysis%0AAuthor%3A%20Gabriel%20Ansah%20and%20Eden%20Ruffell%20and%20Delmiro%20Fernandez-Reyes%20and%20Petru%20Manescu%0AAbstract%3A%20Automated%20blood%20morphology%20analysis%20can%20support%20hematological%20diagnostics%20in%20low-%20and%20middle-income%20countries%20%28LMICs%29%20but%20remains%20sensitive%20to%20dataset%20shifts%20from%20staining%20variability%2C%20imaging%20differences%2C%20and%20rare%20morphologies.%20Building%20centralized%20datasets%20to%20capture%20this%20diversity%20is%20often%20infeasible%20due%20to%20privacy%20regulations%20and%20data-sharing%20restrictions.%20We%20introduce%20a%20federated%20learning%20framework%20for%20white%20blood%20cell%20morphology%20analysis%20that%20enables%20collaborative%20training%20across%20institutions%20without%20exchanging%20training%20data.%20Using%20blood%20films%20from%20multiple%20clinical%20sites%2C%20our%20federated%20models%20learn%20robust%2C%20domain-invariant%20representations%20while%20preserving%20complete%20data%20privacy.%20Evaluations%20across%20convolutional%20and%20transformer-based%20architectures%20show%20that%20federated%20training%20achieves%20strong%20cross-site%20performance%20and%20improved%20generalization%20to%20unseen%20institutions%20compared%20to%20centralized%20training.%20These%20findings%20highlight%20federated%20learning%20as%20a%20practical%20and%20privacy-preserving%20approach%20for%20developing%20equitable%2C%20scalable%2C%20and%20generalizable%20medical%20imaging%20AI%20in%20resource-limited%20healthcare%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMORPHFED%253A%2520Federated%2520Learning%2520for%2520Cross-institutional%2520Blood%2520Morphology%2520Analysis%26entry.906535625%3DGabriel%2520Ansah%2520and%2520Eden%2520Ruffell%2520and%2520Delmiro%2520Fernandez-Reyes%2520and%2520Petru%2520Manescu%26entry.1292438233%3DAutomated%2520blood%2520morphology%2520analysis%2520can%2520support%2520hematological%2520diagnostics%2520in%2520low-%2520and%2520middle-income%2520countries%2520%2528LMICs%2529%2520but%2520remains%2520sensitive%2520to%2520dataset%2520shifts%2520from%2520staining%2520variability%252C%2520imaging%2520differences%252C%2520and%2520rare%2520morphologies.%2520Building%2520centralized%2520datasets%2520to%2520capture%2520this%2520diversity%2520is%2520often%2520infeasible%2520due%2520to%2520privacy%2520regulations%2520and%2520data-sharing%2520restrictions.%2520We%2520introduce%2520a%2520federated%2520learning%2520framework%2520for%2520white%2520blood%2520cell%2520morphology%2520analysis%2520that%2520enables%2520collaborative%2520training%2520across%2520institutions%2520without%2520exchanging%2520training%2520data.%2520Using%2520blood%2520films%2520from%2520multiple%2520clinical%2520sites%252C%2520our%2520federated%2520models%2520learn%2520robust%252C%2520domain-invariant%2520representations%2520while%2520preserving%2520complete%2520data%2520privacy.%2520Evaluations%2520across%2520convolutional%2520and%2520transformer-based%2520architectures%2520show%2520that%2520federated%2520training%2520achieves%2520strong%2520cross-site%2520performance%2520and%2520improved%2520generalization%2520to%2520unseen%2520institutions%2520compared%2520to%2520centralized%2520training.%2520These%2520findings%2520highlight%2520federated%2520learning%2520as%2520a%2520practical%2520and%2520privacy-preserving%2520approach%2520for%2520developing%2520equitable%252C%2520scalable%252C%2520and%2520generalizable%2520medical%2520imaging%2520AI%2520in%2520resource-limited%2520healthcare%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MORPHFED%3A%20Federated%20Learning%20for%20Cross-institutional%20Blood%20Morphology%20Analysis&entry.906535625=Gabriel%20Ansah%20and%20Eden%20Ruffell%20and%20Delmiro%20Fernandez-Reyes%20and%20Petru%20Manescu&entry.1292438233=Automated%20blood%20morphology%20analysis%20can%20support%20hematological%20diagnostics%20in%20low-%20and%20middle-income%20countries%20%28LMICs%29%20but%20remains%20sensitive%20to%20dataset%20shifts%20from%20staining%20variability%2C%20imaging%20differences%2C%20and%20rare%20morphologies.%20Building%20centralized%20datasets%20to%20capture%20this%20diversity%20is%20often%20infeasible%20due%20to%20privacy%20regulations%20and%20data-sharing%20restrictions.%20We%20introduce%20a%20federated%20learning%20framework%20for%20white%20blood%20cell%20morphology%20analysis%20that%20enables%20collaborative%20training%20across%20institutions%20without%20exchanging%20training%20data.%20Using%20blood%20films%20from%20multiple%20clinical%20sites%2C%20our%20federated%20models%20learn%20robust%2C%20domain-invariant%20representations%20while%20preserving%20complete%20data%20privacy.%20Evaluations%20across%20convolutional%20and%20transformer-based%20architectures%20show%20that%20federated%20training%20achieves%20strong%20cross-site%20performance%20and%20improved%20generalization%20to%20unseen%20institutions%20compared%20to%20centralized%20training.%20These%20findings%20highlight%20federated%20learning%20as%20a%20practical%20and%20privacy-preserving%20approach%20for%20developing%20equitable%2C%20scalable%2C%20and%20generalizable%20medical%20imaging%20AI%20in%20resource-limited%20healthcare%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.04121v1&entry.124074799=Read"},
{"title": "From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs", "author": "Usha Shrestha and Dmitry Ignatov and Radu Timofte", "abstract": "Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.", "link": "http://arxiv.org/abs/2601.03808v1", "date": "2026-01-07", "relevancy": 2.0045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Brute%20Force%20to%20Semantic%20Insight%3A%20Performance-Guided%20Data%20Transformation%20Design%20with%20LLMs&body=Title%3A%20From%20Brute%20Force%20to%20Semantic%20Insight%3A%20Performance-Guided%20Data%20Transformation%20Design%20with%20LLMs%0AAuthor%3A%20Usha%20Shrestha%20and%20Dmitry%20Ignatov%20and%20Radu%20Timofte%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20notable%20performance%20in%20code%20synthesis%3B%20however%2C%20data-aware%20augmentation%20remains%20a%20limiting%20factor%2C%20handled%20via%20heuristic%20design%20or%20brute-force%20approaches.%20We%20introduce%20a%20performance-aware%2C%20closed-loop%20solution%20in%20the%20NNGPT%20ecosystem%20of%20projects%20that%20enables%20LLMs%20to%20autonomously%20engineer%20optimal%20transformations%20by%20internalizing%20empirical%20performance%20cues.%20We%20fine-tune%20LLMs%20with%20Low-Rank%20Adaptation%20on%20a%20novel%20repository%20of%20more%20than%206%2C000%20empirically%20evaluated%20PyTorch%20augmentation%20functions%2C%20each%20annotated%20solely%20by%20downstream%20model%20accuracy.%20Training%20uses%20pairwise%20performance%20ordering%20%28better-worse%20transformations%29%2C%20enabling%20alignment%20through%20empirical%20feedback%20without%20reinforcement%20learning%2C%20reward%20models%2C%20or%20symbolic%20objectives.%20This%20reduces%20the%20need%20for%20exhaustive%20search%2C%20achieving%20up%20to%20600x%20times%20fewer%20evaluated%20candidates%20than%20brute-force%20discovery%20while%20maintaining%20competitive%20peak%20accuracy%20and%20shifting%20generation%20from%20random%20synthesis%20to%20task-aligned%20design.%20Ablation%20studies%20show%20that%20structured%20Chain-of-Thought%20prompting%20introduces%20syntactic%20noise%20and%20degrades%20performance%2C%20whereas%20direct%20prompting%20ensures%20stable%20optimization%20in%20performance-critical%20code%20tasks.%20Qualitative%20and%20quantitative%20analyses%20demonstrate%20that%20the%20model%20internalizes%20semantic%20performance%20cues%20rather%20than%20memorizing%20syntax.%20These%20results%20show%20that%20LLMs%20can%20exhibit%20task-level%20reasoning%20through%20non-textual%20feedback%20loops%2C%20bypassing%20explicit%20symbolic%20rewards.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Brute%2520Force%2520to%2520Semantic%2520Insight%253A%2520Performance-Guided%2520Data%2520Transformation%2520Design%2520with%2520LLMs%26entry.906535625%3DUsha%2520Shrestha%2520and%2520Dmitry%2520Ignatov%2520and%2520Radu%2520Timofte%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520notable%2520performance%2520in%2520code%2520synthesis%253B%2520however%252C%2520data-aware%2520augmentation%2520remains%2520a%2520limiting%2520factor%252C%2520handled%2520via%2520heuristic%2520design%2520or%2520brute-force%2520approaches.%2520We%2520introduce%2520a%2520performance-aware%252C%2520closed-loop%2520solution%2520in%2520the%2520NNGPT%2520ecosystem%2520of%2520projects%2520that%2520enables%2520LLMs%2520to%2520autonomously%2520engineer%2520optimal%2520transformations%2520by%2520internalizing%2520empirical%2520performance%2520cues.%2520We%2520fine-tune%2520LLMs%2520with%2520Low-Rank%2520Adaptation%2520on%2520a%2520novel%2520repository%2520of%2520more%2520than%25206%252C000%2520empirically%2520evaluated%2520PyTorch%2520augmentation%2520functions%252C%2520each%2520annotated%2520solely%2520by%2520downstream%2520model%2520accuracy.%2520Training%2520uses%2520pairwise%2520performance%2520ordering%2520%2528better-worse%2520transformations%2529%252C%2520enabling%2520alignment%2520through%2520empirical%2520feedback%2520without%2520reinforcement%2520learning%252C%2520reward%2520models%252C%2520or%2520symbolic%2520objectives.%2520This%2520reduces%2520the%2520need%2520for%2520exhaustive%2520search%252C%2520achieving%2520up%2520to%2520600x%2520times%2520fewer%2520evaluated%2520candidates%2520than%2520brute-force%2520discovery%2520while%2520maintaining%2520competitive%2520peak%2520accuracy%2520and%2520shifting%2520generation%2520from%2520random%2520synthesis%2520to%2520task-aligned%2520design.%2520Ablation%2520studies%2520show%2520that%2520structured%2520Chain-of-Thought%2520prompting%2520introduces%2520syntactic%2520noise%2520and%2520degrades%2520performance%252C%2520whereas%2520direct%2520prompting%2520ensures%2520stable%2520optimization%2520in%2520performance-critical%2520code%2520tasks.%2520Qualitative%2520and%2520quantitative%2520analyses%2520demonstrate%2520that%2520the%2520model%2520internalizes%2520semantic%2520performance%2520cues%2520rather%2520than%2520memorizing%2520syntax.%2520These%2520results%2520show%2520that%2520LLMs%2520can%2520exhibit%2520task-level%2520reasoning%2520through%2520non-textual%2520feedback%2520loops%252C%2520bypassing%2520explicit%2520symbolic%2520rewards.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Brute%20Force%20to%20Semantic%20Insight%3A%20Performance-Guided%20Data%20Transformation%20Design%20with%20LLMs&entry.906535625=Usha%20Shrestha%20and%20Dmitry%20Ignatov%20and%20Radu%20Timofte&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20achieved%20notable%20performance%20in%20code%20synthesis%3B%20however%2C%20data-aware%20augmentation%20remains%20a%20limiting%20factor%2C%20handled%20via%20heuristic%20design%20or%20brute-force%20approaches.%20We%20introduce%20a%20performance-aware%2C%20closed-loop%20solution%20in%20the%20NNGPT%20ecosystem%20of%20projects%20that%20enables%20LLMs%20to%20autonomously%20engineer%20optimal%20transformations%20by%20internalizing%20empirical%20performance%20cues.%20We%20fine-tune%20LLMs%20with%20Low-Rank%20Adaptation%20on%20a%20novel%20repository%20of%20more%20than%206%2C000%20empirically%20evaluated%20PyTorch%20augmentation%20functions%2C%20each%20annotated%20solely%20by%20downstream%20model%20accuracy.%20Training%20uses%20pairwise%20performance%20ordering%20%28better-worse%20transformations%29%2C%20enabling%20alignment%20through%20empirical%20feedback%20without%20reinforcement%20learning%2C%20reward%20models%2C%20or%20symbolic%20objectives.%20This%20reduces%20the%20need%20for%20exhaustive%20search%2C%20achieving%20up%20to%20600x%20times%20fewer%20evaluated%20candidates%20than%20brute-force%20discovery%20while%20maintaining%20competitive%20peak%20accuracy%20and%20shifting%20generation%20from%20random%20synthesis%20to%20task-aligned%20design.%20Ablation%20studies%20show%20that%20structured%20Chain-of-Thought%20prompting%20introduces%20syntactic%20noise%20and%20degrades%20performance%2C%20whereas%20direct%20prompting%20ensures%20stable%20optimization%20in%20performance-critical%20code%20tasks.%20Qualitative%20and%20quantitative%20analyses%20demonstrate%20that%20the%20model%20internalizes%20semantic%20performance%20cues%20rather%20than%20memorizing%20syntax.%20These%20results%20show%20that%20LLMs%20can%20exhibit%20task-level%20reasoning%20through%20non-textual%20feedback%20loops%2C%20bypassing%20explicit%20symbolic%20rewards.&entry.1838667208=http%3A//arxiv.org/abs/2601.03808v1&entry.124074799=Read"},
{"title": "SSSD: Simply-Scalable Speculative Decoding", "author": "Michele Marzollo and Jiawei Zhuang and Niklas Roemer and Niklas Zwingenberger and Lorenz K. M\u00fcller and Lukas Cavigelli", "abstract": "Speculative Decoding has emerged as a popular technique for accelerating inference in Large Language Models. However, most existing approaches yield only modest improvements in production serving systems. Methods that achieve substantial speedups typically rely on an additional trained draft model or auxiliary model components, increasing deployment and maintenance complexity. This added complexity reduces flexibility, particularly when serving workloads shift to tasks, domains, or languages that are not well represented in the draft model's training data.\n  We introduce Simply-Scalable Speculative Decoding (SSSD), a training-free method that combines lightweight n-gram matching with hardware-aware speculation. Relative to standard autoregressive decoding, SSSD reduces latency by up to 2.9x. It achieves performance on par with leading training-based approaches across a broad range of benchmarks, while requiring substantially lower adoption effort--no data preparation, training or tuning are needed--and exhibiting superior robustness under language and domain shift, as well as in long-context settings.", "link": "http://arxiv.org/abs/2411.05894v2", "date": "2026-01-07", "relevancy": 2.002, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSSD%3A%20Simply-Scalable%20Speculative%20Decoding&body=Title%3A%20SSSD%3A%20Simply-Scalable%20Speculative%20Decoding%0AAuthor%3A%20Michele%20Marzollo%20and%20Jiawei%20Zhuang%20and%20Niklas%20Roemer%20and%20Niklas%20Zwingenberger%20and%20Lorenz%20K.%20M%C3%BCller%20and%20Lukas%20Cavigelli%0AAbstract%3A%20Speculative%20Decoding%20has%20emerged%20as%20a%20popular%20technique%20for%20accelerating%20inference%20in%20Large%20Language%20Models.%20However%2C%20most%20existing%20approaches%20yield%20only%20modest%20improvements%20in%20production%20serving%20systems.%20Methods%20that%20achieve%20substantial%20speedups%20typically%20rely%20on%20an%20additional%20trained%20draft%20model%20or%20auxiliary%20model%20components%2C%20increasing%20deployment%20and%20maintenance%20complexity.%20This%20added%20complexity%20reduces%20flexibility%2C%20particularly%20when%20serving%20workloads%20shift%20to%20tasks%2C%20domains%2C%20or%20languages%20that%20are%20not%20well%20represented%20in%20the%20draft%20model%27s%20training%20data.%0A%20%20We%20introduce%20Simply-Scalable%20Speculative%20Decoding%20%28SSSD%29%2C%20a%20training-free%20method%20that%20combines%20lightweight%20n-gram%20matching%20with%20hardware-aware%20speculation.%20Relative%20to%20standard%20autoregressive%20decoding%2C%20SSSD%20reduces%20latency%20by%20up%20to%202.9x.%20It%20achieves%20performance%20on%20par%20with%20leading%20training-based%20approaches%20across%20a%20broad%20range%20of%20benchmarks%2C%20while%20requiring%20substantially%20lower%20adoption%20effort--no%20data%20preparation%2C%20training%20or%20tuning%20are%20needed--and%20exhibiting%20superior%20robustness%20under%20language%20and%20domain%20shift%2C%20as%20well%20as%20in%20long-context%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2411.05894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSSD%253A%2520Simply-Scalable%2520Speculative%2520Decoding%26entry.906535625%3DMichele%2520Marzollo%2520and%2520Jiawei%2520Zhuang%2520and%2520Niklas%2520Roemer%2520and%2520Niklas%2520Zwingenberger%2520and%2520Lorenz%2520K.%2520M%25C3%25BCller%2520and%2520Lukas%2520Cavigelli%26entry.1292438233%3DSpeculative%2520Decoding%2520has%2520emerged%2520as%2520a%2520popular%2520technique%2520for%2520accelerating%2520inference%2520in%2520Large%2520Language%2520Models.%2520However%252C%2520most%2520existing%2520approaches%2520yield%2520only%2520modest%2520improvements%2520in%2520production%2520serving%2520systems.%2520Methods%2520that%2520achieve%2520substantial%2520speedups%2520typically%2520rely%2520on%2520an%2520additional%2520trained%2520draft%2520model%2520or%2520auxiliary%2520model%2520components%252C%2520increasing%2520deployment%2520and%2520maintenance%2520complexity.%2520This%2520added%2520complexity%2520reduces%2520flexibility%252C%2520particularly%2520when%2520serving%2520workloads%2520shift%2520to%2520tasks%252C%2520domains%252C%2520or%2520languages%2520that%2520are%2520not%2520well%2520represented%2520in%2520the%2520draft%2520model%2527s%2520training%2520data.%250A%2520%2520We%2520introduce%2520Simply-Scalable%2520Speculative%2520Decoding%2520%2528SSSD%2529%252C%2520a%2520training-free%2520method%2520that%2520combines%2520lightweight%2520n-gram%2520matching%2520with%2520hardware-aware%2520speculation.%2520Relative%2520to%2520standard%2520autoregressive%2520decoding%252C%2520SSSD%2520reduces%2520latency%2520by%2520up%2520to%25202.9x.%2520It%2520achieves%2520performance%2520on%2520par%2520with%2520leading%2520training-based%2520approaches%2520across%2520a%2520broad%2520range%2520of%2520benchmarks%252C%2520while%2520requiring%2520substantially%2520lower%2520adoption%2520effort--no%2520data%2520preparation%252C%2520training%2520or%2520tuning%2520are%2520needed--and%2520exhibiting%2520superior%2520robustness%2520under%2520language%2520and%2520domain%2520shift%252C%2520as%2520well%2520as%2520in%2520long-context%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSSD%3A%20Simply-Scalable%20Speculative%20Decoding&entry.906535625=Michele%20Marzollo%20and%20Jiawei%20Zhuang%20and%20Niklas%20Roemer%20and%20Niklas%20Zwingenberger%20and%20Lorenz%20K.%20M%C3%BCller%20and%20Lukas%20Cavigelli&entry.1292438233=Speculative%20Decoding%20has%20emerged%20as%20a%20popular%20technique%20for%20accelerating%20inference%20in%20Large%20Language%20Models.%20However%2C%20most%20existing%20approaches%20yield%20only%20modest%20improvements%20in%20production%20serving%20systems.%20Methods%20that%20achieve%20substantial%20speedups%20typically%20rely%20on%20an%20additional%20trained%20draft%20model%20or%20auxiliary%20model%20components%2C%20increasing%20deployment%20and%20maintenance%20complexity.%20This%20added%20complexity%20reduces%20flexibility%2C%20particularly%20when%20serving%20workloads%20shift%20to%20tasks%2C%20domains%2C%20or%20languages%20that%20are%20not%20well%20represented%20in%20the%20draft%20model%27s%20training%20data.%0A%20%20We%20introduce%20Simply-Scalable%20Speculative%20Decoding%20%28SSSD%29%2C%20a%20training-free%20method%20that%20combines%20lightweight%20n-gram%20matching%20with%20hardware-aware%20speculation.%20Relative%20to%20standard%20autoregressive%20decoding%2C%20SSSD%20reduces%20latency%20by%20up%20to%202.9x.%20It%20achieves%20performance%20on%20par%20with%20leading%20training-based%20approaches%20across%20a%20broad%20range%20of%20benchmarks%2C%20while%20requiring%20substantially%20lower%20adoption%20effort--no%20data%20preparation%2C%20training%20or%20tuning%20are%20needed--and%20exhibiting%20superior%20robustness%20under%20language%20and%20domain%20shift%2C%20as%20well%20as%20in%20long-context%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2411.05894v2&entry.124074799=Read"},
{"title": "ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding", "author": "Xinbang Dai and Huikang Hu and Yongrui Chen and Jiaqi Li and Rihui Jin and Yuyang Zhang and Xiaoguang Li and Lifeng Shang and Guilin Qi", "abstract": "While large language models (LLMs) excel at many domain-specific tasks, their ability to deeply comprehend and reason about full-length academic papers remains underexplored. Existing benchmarks often fall short of capturing such depth, either due to surface-level question design or unreliable evaluation metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by domain experts to evaluate LLMs' comprehension of artificial intelligence (AI) research papers. Developed through an incentive-driven, adversarial annotation process, ELAIPBench features 403 multiple-choice questions from 137 papers. It spans three difficulty levels and emphasizes non-trivial reasoning rather than shallow retrieval. Our experiments show that the best-performing LLM achieves an accuracy of only 39.95%, far below human performance. Moreover, we observe that frontier LLMs equipped with a thinking mode or a retrieval-augmented generation (RAG) system fail to improve final results-even harming accuracy due to overthinking or noisy retrieval. These findings underscore the significant gap between current LLM capabilities and genuine comprehension of academic papers.", "link": "http://arxiv.org/abs/2510.10549v2", "date": "2026-01-07", "relevancy": 2.0015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5041}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELAIPBench%3A%20A%20Benchmark%20for%20Expert-Level%20Artificial%20Intelligence%20Paper%20Understanding&body=Title%3A%20ELAIPBench%3A%20A%20Benchmark%20for%20Expert-Level%20Artificial%20Intelligence%20Paper%20Understanding%0AAuthor%3A%20Xinbang%20Dai%20and%20Huikang%20Hu%20and%20Yongrui%20Chen%20and%20Jiaqi%20Li%20and%20Rihui%20Jin%20and%20Yuyang%20Zhang%20and%20Xiaoguang%20Li%20and%20Lifeng%20Shang%20and%20Guilin%20Qi%0AAbstract%3A%20While%20large%20language%20models%20%28LLMs%29%20excel%20at%20many%20domain-specific%20tasks%2C%20their%20ability%20to%20deeply%20comprehend%20and%20reason%20about%20full-length%20academic%20papers%20remains%20underexplored.%20Existing%20benchmarks%20often%20fall%20short%20of%20capturing%20such%20depth%2C%20either%20due%20to%20surface-level%20question%20design%20or%20unreliable%20evaluation%20metrics.%20To%20address%20this%20gap%2C%20we%20introduce%20ELAIPBench%2C%20a%20benchmark%20curated%20by%20domain%20experts%20to%20evaluate%20LLMs%27%20comprehension%20of%20artificial%20intelligence%20%28AI%29%20research%20papers.%20Developed%20through%20an%20incentive-driven%2C%20adversarial%20annotation%20process%2C%20ELAIPBench%20features%20403%20multiple-choice%20questions%20from%20137%20papers.%20It%20spans%20three%20difficulty%20levels%20and%20emphasizes%20non-trivial%20reasoning%20rather%20than%20shallow%20retrieval.%20Our%20experiments%20show%20that%20the%20best-performing%20LLM%20achieves%20an%20accuracy%20of%20only%2039.95%25%2C%20far%20below%20human%20performance.%20Moreover%2C%20we%20observe%20that%20frontier%20LLMs%20equipped%20with%20a%20thinking%20mode%20or%20a%20retrieval-augmented%20generation%20%28RAG%29%20system%20fail%20to%20improve%20final%20results-even%20harming%20accuracy%20due%20to%20overthinking%20or%20noisy%20retrieval.%20These%20findings%20underscore%20the%20significant%20gap%20between%20current%20LLM%20capabilities%20and%20genuine%20comprehension%20of%20academic%20papers.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELAIPBench%253A%2520A%2520Benchmark%2520for%2520Expert-Level%2520Artificial%2520Intelligence%2520Paper%2520Understanding%26entry.906535625%3DXinbang%2520Dai%2520and%2520Huikang%2520Hu%2520and%2520Yongrui%2520Chen%2520and%2520Jiaqi%2520Li%2520and%2520Rihui%2520Jin%2520and%2520Yuyang%2520Zhang%2520and%2520Xiaoguang%2520Li%2520and%2520Lifeng%2520Shang%2520and%2520Guilin%2520Qi%26entry.1292438233%3DWhile%2520large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520many%2520domain-specific%2520tasks%252C%2520their%2520ability%2520to%2520deeply%2520comprehend%2520and%2520reason%2520about%2520full-length%2520academic%2520papers%2520remains%2520underexplored.%2520Existing%2520benchmarks%2520often%2520fall%2520short%2520of%2520capturing%2520such%2520depth%252C%2520either%2520due%2520to%2520surface-level%2520question%2520design%2520or%2520unreliable%2520evaluation%2520metrics.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520ELAIPBench%252C%2520a%2520benchmark%2520curated%2520by%2520domain%2520experts%2520to%2520evaluate%2520LLMs%2527%2520comprehension%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520research%2520papers.%2520Developed%2520through%2520an%2520incentive-driven%252C%2520adversarial%2520annotation%2520process%252C%2520ELAIPBench%2520features%2520403%2520multiple-choice%2520questions%2520from%2520137%2520papers.%2520It%2520spans%2520three%2520difficulty%2520levels%2520and%2520emphasizes%2520non-trivial%2520reasoning%2520rather%2520than%2520shallow%2520retrieval.%2520Our%2520experiments%2520show%2520that%2520the%2520best-performing%2520LLM%2520achieves%2520an%2520accuracy%2520of%2520only%252039.95%2525%252C%2520far%2520below%2520human%2520performance.%2520Moreover%252C%2520we%2520observe%2520that%2520frontier%2520LLMs%2520equipped%2520with%2520a%2520thinking%2520mode%2520or%2520a%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520system%2520fail%2520to%2520improve%2520final%2520results-even%2520harming%2520accuracy%2520due%2520to%2520overthinking%2520or%2520noisy%2520retrieval.%2520These%2520findings%2520underscore%2520the%2520significant%2520gap%2520between%2520current%2520LLM%2520capabilities%2520and%2520genuine%2520comprehension%2520of%2520academic%2520papers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELAIPBench%3A%20A%20Benchmark%20for%20Expert-Level%20Artificial%20Intelligence%20Paper%20Understanding&entry.906535625=Xinbang%20Dai%20and%20Huikang%20Hu%20and%20Yongrui%20Chen%20and%20Jiaqi%20Li%20and%20Rihui%20Jin%20and%20Yuyang%20Zhang%20and%20Xiaoguang%20Li%20and%20Lifeng%20Shang%20and%20Guilin%20Qi&entry.1292438233=While%20large%20language%20models%20%28LLMs%29%20excel%20at%20many%20domain-specific%20tasks%2C%20their%20ability%20to%20deeply%20comprehend%20and%20reason%20about%20full-length%20academic%20papers%20remains%20underexplored.%20Existing%20benchmarks%20often%20fall%20short%20of%20capturing%20such%20depth%2C%20either%20due%20to%20surface-level%20question%20design%20or%20unreliable%20evaluation%20metrics.%20To%20address%20this%20gap%2C%20we%20introduce%20ELAIPBench%2C%20a%20benchmark%20curated%20by%20domain%20experts%20to%20evaluate%20LLMs%27%20comprehension%20of%20artificial%20intelligence%20%28AI%29%20research%20papers.%20Developed%20through%20an%20incentive-driven%2C%20adversarial%20annotation%20process%2C%20ELAIPBench%20features%20403%20multiple-choice%20questions%20from%20137%20papers.%20It%20spans%20three%20difficulty%20levels%20and%20emphasizes%20non-trivial%20reasoning%20rather%20than%20shallow%20retrieval.%20Our%20experiments%20show%20that%20the%20best-performing%20LLM%20achieves%20an%20accuracy%20of%20only%2039.95%25%2C%20far%20below%20human%20performance.%20Moreover%2C%20we%20observe%20that%20frontier%20LLMs%20equipped%20with%20a%20thinking%20mode%20or%20a%20retrieval-augmented%20generation%20%28RAG%29%20system%20fail%20to%20improve%20final%20results-even%20harming%20accuracy%20due%20to%20overthinking%20or%20noisy%20retrieval.%20These%20findings%20underscore%20the%20significant%20gap%20between%20current%20LLM%20capabilities%20and%20genuine%20comprehension%20of%20academic%20papers.&entry.1838667208=http%3A//arxiv.org/abs/2510.10549v2&entry.124074799=Read"},
{"title": "Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training", "author": "Chi Liu and Xin Chen", "abstract": "Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.", "link": "http://arxiv.org/abs/2601.03895v1", "date": "2026-01-07", "relevancy": 2.0012, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5056}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5019}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive-Boundary-Clipping%20GRPO%3A%20Ensuring%20Bounded%20Ratios%20for%20Stable%20and%20Generalizable%20Training&body=Title%3A%20Adaptive-Boundary-Clipping%20GRPO%3A%20Ensuring%20Bounded%20Ratios%20for%20Stable%20and%20Generalizable%20Training%0AAuthor%3A%20Chi%20Liu%20and%20Xin%20Chen%0AAbstract%3A%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20has%20emerged%20as%20a%20popular%20algorithm%20for%20reinforcement%20learning%20with%20large%20language%20models%20%28LLMs%29.%20However%2C%20upon%20analyzing%20its%20clipping%20mechanism%2C%20we%20argue%20that%20it%20is%20suboptimal%20in%20certain%20scenarios.%20With%20appropriate%20modifications%2C%20GRPO%20can%20be%20significantly%20enhanced%20to%20improve%20both%20flexibility%20and%20generalization.%20To%20this%20end%2C%20we%20propose%20Adaptive-Boundary-Clipping%20GRPO%20%28ABC-GRPO%29%2C%20an%20asymmetric%20and%20adaptive%20refinement%20of%20the%20original%20GRPO%20framework.%20We%20demonstrate%20that%20ABC-GRPO%20achieves%20superior%20performance%20over%20standard%20GRPO%20on%20mathematical%20reasoning%20tasks%20using%20the%20Qwen3%20LLMs.%20Moreover%2C%20ABC-GRPO%20maintains%20substantially%20higher%20entropy%20throughout%20training%2C%20thereby%20preserving%20the%20model%27s%20exploration%20capacity%20and%20mitigating%20premature%20convergence.%20The%20implementation%20code%20is%20available%20online%20to%20ease%20reproducibility%20https%3A//github.com/chi2liu/ABC-GRPO.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive-Boundary-Clipping%2520GRPO%253A%2520Ensuring%2520Bounded%2520Ratios%2520for%2520Stable%2520and%2520Generalizable%2520Training%26entry.906535625%3DChi%2520Liu%2520and%2520Xin%2520Chen%26entry.1292438233%3DGroup%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520has%2520emerged%2520as%2520a%2520popular%2520algorithm%2520for%2520reinforcement%2520learning%2520with%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520upon%2520analyzing%2520its%2520clipping%2520mechanism%252C%2520we%2520argue%2520that%2520it%2520is%2520suboptimal%2520in%2520certain%2520scenarios.%2520With%2520appropriate%2520modifications%252C%2520GRPO%2520can%2520be%2520significantly%2520enhanced%2520to%2520improve%2520both%2520flexibility%2520and%2520generalization.%2520To%2520this%2520end%252C%2520we%2520propose%2520Adaptive-Boundary-Clipping%2520GRPO%2520%2528ABC-GRPO%2529%252C%2520an%2520asymmetric%2520and%2520adaptive%2520refinement%2520of%2520the%2520original%2520GRPO%2520framework.%2520We%2520demonstrate%2520that%2520ABC-GRPO%2520achieves%2520superior%2520performance%2520over%2520standard%2520GRPO%2520on%2520mathematical%2520reasoning%2520tasks%2520using%2520the%2520Qwen3%2520LLMs.%2520Moreover%252C%2520ABC-GRPO%2520maintains%2520substantially%2520higher%2520entropy%2520throughout%2520training%252C%2520thereby%2520preserving%2520the%2520model%2527s%2520exploration%2520capacity%2520and%2520mitigating%2520premature%2520convergence.%2520The%2520implementation%2520code%2520is%2520available%2520online%2520to%2520ease%2520reproducibility%2520https%253A//github.com/chi2liu/ABC-GRPO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive-Boundary-Clipping%20GRPO%3A%20Ensuring%20Bounded%20Ratios%20for%20Stable%20and%20Generalizable%20Training&entry.906535625=Chi%20Liu%20and%20Xin%20Chen&entry.1292438233=Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20has%20emerged%20as%20a%20popular%20algorithm%20for%20reinforcement%20learning%20with%20large%20language%20models%20%28LLMs%29.%20However%2C%20upon%20analyzing%20its%20clipping%20mechanism%2C%20we%20argue%20that%20it%20is%20suboptimal%20in%20certain%20scenarios.%20With%20appropriate%20modifications%2C%20GRPO%20can%20be%20significantly%20enhanced%20to%20improve%20both%20flexibility%20and%20generalization.%20To%20this%20end%2C%20we%20propose%20Adaptive-Boundary-Clipping%20GRPO%20%28ABC-GRPO%29%2C%20an%20asymmetric%20and%20adaptive%20refinement%20of%20the%20original%20GRPO%20framework.%20We%20demonstrate%20that%20ABC-GRPO%20achieves%20superior%20performance%20over%20standard%20GRPO%20on%20mathematical%20reasoning%20tasks%20using%20the%20Qwen3%20LLMs.%20Moreover%2C%20ABC-GRPO%20maintains%20substantially%20higher%20entropy%20throughout%20training%2C%20thereby%20preserving%20the%20model%27s%20exploration%20capacity%20and%20mitigating%20premature%20convergence.%20The%20implementation%20code%20is%20available%20online%20to%20ease%20reproducibility%20https%3A//github.com/chi2liu/ABC-GRPO.&entry.1838667208=http%3A//arxiv.org/abs/2601.03895v1&entry.124074799=Read"},
{"title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations", "author": "Adeshola Okubena and Yusuf Ali Mohammed and Moe Elbadawi", "abstract": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.", "link": "http://arxiv.org/abs/2601.02071v2", "date": "2026-01-07", "relevancy": 1.9993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.502}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FormuLLA%3A%20A%20Large%20Language%20Model%20Approach%20to%20Generating%20Novel%203D%20Printable%20Formulations&body=Title%3A%20FormuLLA%3A%20A%20Large%20Language%20Model%20Approach%20to%20Generating%20Novel%203D%20Printable%20Formulations%0AAuthor%3A%20Adeshola%20Okubena%20and%20Yusuf%20Ali%20Mohammed%20and%20Moe%20Elbadawi%0AAbstract%3A%20Pharmaceutical%20three-dimensional%20%283D%29%20printing%20is%20an%20advanced%20fabrication%20technology%20with%20the%20potential%20to%20enable%20truly%20personalised%20dosage%20forms.%20Recent%20studies%20have%20integrated%20artificial%20intelligence%20%28AI%29%20to%20accelerate%20formulation%20and%20process%20development%2C%20drastically%20transforming%20current%20approaches%20to%20pharmaceutical%203D%20printing.%20To%20date%2C%20most%20AI-driven%20efforts%20remain%20narrowly%20focused%2C%20while%20failing%20to%20account%20for%20the%20broader%20formulation%20challenges%20inherent%20to%20the%20technology.%20Recent%20advances%20in%20AI%20have%20introduced%20artificial%20general%20intelligence%20concepts%2C%20wherein%20systems%20extend%20beyond%20conventional%20predictive%20modelling%20toward%20more%20generalised%2C%20human-like%20reasoning.%20In%20this%20work%2C%20we%20investigate%20the%20application%20of%20large%20language%20models%20%28LLMs%29%2C%20fine-tuned%20on%20a%20fused%20deposition%20modelling%20%28FDM%29%20dataset%20comprising%20over%201400%20formulations%2C%20to%20recommend%20suitable%20excipients%20based%20on%20active%20pharmaceutical%20ingredient%20%28API%29%20dose%2C%20and%20predict%20filament%20mechanical%20properties.%20Four%20LLM%20architectures%20were%20fine-tuned%2C%20with%20systematic%20evaluation%20of%20both%20fine-tuning%20and%20generative%20parameter%20configurations.%20Our%20results%20demonstrate%20that%20Llama2%20was%20best%20suited%20for%20recommending%20excipients%20for%20FDM%20formulations.%20Additionally%2C%20model%20selection%20and%20parameterisation%20significantly%20influence%20performance%2C%20with%20smaller%20LLMs%20exhibiting%20instances%20of%20catastrophic%20forgetting.%20Furthermore%2C%20we%20demonstrate%3A%20%28i%29%20even%20with%20relatively%20small%20dataset%20of%20over%201400%20formulations%2C%20it%20can%20lead%20to%20model%20catastrophic%20forgetting%3B%20%28ii%29%20standard%20LLM%20metrics%20only%20evaluate%20linguistic%20performance%20but%20not%20formulation%20processability%3B%20and%20%28iii%29%20LLMs%20trained%20on%20biomedically-related%20data%20do%20not%20always%20produce%20the%20best%20results.%20Addressing%20these%20challenges%20is%20essential%20to%20advancing%20LLMs%20beyond%20linguistic%20proficiency%20and%20toward%20reliable%20systems%20for%20pharmaceutical%20formulation%20development.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02071v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFormuLLA%253A%2520A%2520Large%2520Language%2520Model%2520Approach%2520to%2520Generating%2520Novel%25203D%2520Printable%2520Formulations%26entry.906535625%3DAdeshola%2520Okubena%2520and%2520Yusuf%2520Ali%2520Mohammed%2520and%2520Moe%2520Elbadawi%26entry.1292438233%3DPharmaceutical%2520three-dimensional%2520%25283D%2529%2520printing%2520is%2520an%2520advanced%2520fabrication%2520technology%2520with%2520the%2520potential%2520to%2520enable%2520truly%2520personalised%2520dosage%2520forms.%2520Recent%2520studies%2520have%2520integrated%2520artificial%2520intelligence%2520%2528AI%2529%2520to%2520accelerate%2520formulation%2520and%2520process%2520development%252C%2520drastically%2520transforming%2520current%2520approaches%2520to%2520pharmaceutical%25203D%2520printing.%2520To%2520date%252C%2520most%2520AI-driven%2520efforts%2520remain%2520narrowly%2520focused%252C%2520while%2520failing%2520to%2520account%2520for%2520the%2520broader%2520formulation%2520challenges%2520inherent%2520to%2520the%2520technology.%2520Recent%2520advances%2520in%2520AI%2520have%2520introduced%2520artificial%2520general%2520intelligence%2520concepts%252C%2520wherein%2520systems%2520extend%2520beyond%2520conventional%2520predictive%2520modelling%2520toward%2520more%2520generalised%252C%2520human-like%2520reasoning.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520application%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520fine-tuned%2520on%2520a%2520fused%2520deposition%2520modelling%2520%2528FDM%2529%2520dataset%2520comprising%2520over%25201400%2520formulations%252C%2520to%2520recommend%2520suitable%2520excipients%2520based%2520on%2520active%2520pharmaceutical%2520ingredient%2520%2528API%2529%2520dose%252C%2520and%2520predict%2520filament%2520mechanical%2520properties.%2520Four%2520LLM%2520architectures%2520were%2520fine-tuned%252C%2520with%2520systematic%2520evaluation%2520of%2520both%2520fine-tuning%2520and%2520generative%2520parameter%2520configurations.%2520Our%2520results%2520demonstrate%2520that%2520Llama2%2520was%2520best%2520suited%2520for%2520recommending%2520excipients%2520for%2520FDM%2520formulations.%2520Additionally%252C%2520model%2520selection%2520and%2520parameterisation%2520significantly%2520influence%2520performance%252C%2520with%2520smaller%2520LLMs%2520exhibiting%2520instances%2520of%2520catastrophic%2520forgetting.%2520Furthermore%252C%2520we%2520demonstrate%253A%2520%2528i%2529%2520even%2520with%2520relatively%2520small%2520dataset%2520of%2520over%25201400%2520formulations%252C%2520it%2520can%2520lead%2520to%2520model%2520catastrophic%2520forgetting%253B%2520%2528ii%2529%2520standard%2520LLM%2520metrics%2520only%2520evaluate%2520linguistic%2520performance%2520but%2520not%2520formulation%2520processability%253B%2520and%2520%2528iii%2529%2520LLMs%2520trained%2520on%2520biomedically-related%2520data%2520do%2520not%2520always%2520produce%2520the%2520best%2520results.%2520Addressing%2520these%2520challenges%2520is%2520essential%2520to%2520advancing%2520LLMs%2520beyond%2520linguistic%2520proficiency%2520and%2520toward%2520reliable%2520systems%2520for%2520pharmaceutical%2520formulation%2520development.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02071v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FormuLLA%3A%20A%20Large%20Language%20Model%20Approach%20to%20Generating%20Novel%203D%20Printable%20Formulations&entry.906535625=Adeshola%20Okubena%20and%20Yusuf%20Ali%20Mohammed%20and%20Moe%20Elbadawi&entry.1292438233=Pharmaceutical%20three-dimensional%20%283D%29%20printing%20is%20an%20advanced%20fabrication%20technology%20with%20the%20potential%20to%20enable%20truly%20personalised%20dosage%20forms.%20Recent%20studies%20have%20integrated%20artificial%20intelligence%20%28AI%29%20to%20accelerate%20formulation%20and%20process%20development%2C%20drastically%20transforming%20current%20approaches%20to%20pharmaceutical%203D%20printing.%20To%20date%2C%20most%20AI-driven%20efforts%20remain%20narrowly%20focused%2C%20while%20failing%20to%20account%20for%20the%20broader%20formulation%20challenges%20inherent%20to%20the%20technology.%20Recent%20advances%20in%20AI%20have%20introduced%20artificial%20general%20intelligence%20concepts%2C%20wherein%20systems%20extend%20beyond%20conventional%20predictive%20modelling%20toward%20more%20generalised%2C%20human-like%20reasoning.%20In%20this%20work%2C%20we%20investigate%20the%20application%20of%20large%20language%20models%20%28LLMs%29%2C%20fine-tuned%20on%20a%20fused%20deposition%20modelling%20%28FDM%29%20dataset%20comprising%20over%201400%20formulations%2C%20to%20recommend%20suitable%20excipients%20based%20on%20active%20pharmaceutical%20ingredient%20%28API%29%20dose%2C%20and%20predict%20filament%20mechanical%20properties.%20Four%20LLM%20architectures%20were%20fine-tuned%2C%20with%20systematic%20evaluation%20of%20both%20fine-tuning%20and%20generative%20parameter%20configurations.%20Our%20results%20demonstrate%20that%20Llama2%20was%20best%20suited%20for%20recommending%20excipients%20for%20FDM%20formulations.%20Additionally%2C%20model%20selection%20and%20parameterisation%20significantly%20influence%20performance%2C%20with%20smaller%20LLMs%20exhibiting%20instances%20of%20catastrophic%20forgetting.%20Furthermore%2C%20we%20demonstrate%3A%20%28i%29%20even%20with%20relatively%20small%20dataset%20of%20over%201400%20formulations%2C%20it%20can%20lead%20to%20model%20catastrophic%20forgetting%3B%20%28ii%29%20standard%20LLM%20metrics%20only%20evaluate%20linguistic%20performance%20but%20not%20formulation%20processability%3B%20and%20%28iii%29%20LLMs%20trained%20on%20biomedically-related%20data%20do%20not%20always%20produce%20the%20best%20results.%20Addressing%20these%20challenges%20is%20essential%20to%20advancing%20LLMs%20beyond%20linguistic%20proficiency%20and%20toward%20reliable%20systems%20for%20pharmaceutical%20formulation%20development.&entry.1838667208=http%3A//arxiv.org/abs/2601.02071v2&entry.124074799=Read"},
{"title": "Big Reasoning with Small Models: Instruction Retrieval at Inference Time", "author": "Kenan Alkiek and David Jurgens and Vinod Vydiswaran", "abstract": "Small language models (SLMs) enable low-cost, private, on-device inference, but they often fail on problems that require specialized domain knowledge or multi-step reasoning. Existing approaches for improving reasoning either rely on scale (e.g., chain-of-thought prompting), require task-specific training that limits reuse and generality (e.g., distillation), or retrieve unstructured information that still leaves the SLM to determine an appropriate reasoning strategy. We propose instruction retrieval, an inference-time intervention that augments an SLM with structured, reusable reasoning procedures rather than raw passages. We construct an Instruction Corpus by clustering similar training questions and using a teacher model to generate generalizable guides that pair domain background with explicit step-by-step procedures. At inference, the SLM retrieves the instructions most relevant to a given query and executes the associated procedures without any additional fine-tuning. Across three challenging domains: medicine, law, and mathematics, instruction retrieval yields consistent gains for models with at least 3B parameters, improving accuracy by 9.4%, 7.9%, and 5.1%, respectively, with the strongest 14B model surpassing GPT-4o's zero-shot performance on knowledge-intensive tasks.", "link": "http://arxiv.org/abs/2510.13935v2", "date": "2026-01-07", "relevancy": 1.9966, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Big%20Reasoning%20with%20Small%20Models%3A%20Instruction%20Retrieval%20at%20Inference%20Time&body=Title%3A%20Big%20Reasoning%20with%20Small%20Models%3A%20Instruction%20Retrieval%20at%20Inference%20Time%0AAuthor%3A%20Kenan%20Alkiek%20and%20David%20Jurgens%20and%20Vinod%20Vydiswaran%0AAbstract%3A%20Small%20language%20models%20%28SLMs%29%20enable%20low-cost%2C%20private%2C%20on-device%20inference%2C%20but%20they%20often%20fail%20on%20problems%20that%20require%20specialized%20domain%20knowledge%20or%20multi-step%20reasoning.%20Existing%20approaches%20for%20improving%20reasoning%20either%20rely%20on%20scale%20%28e.g.%2C%20chain-of-thought%20prompting%29%2C%20require%20task-specific%20training%20that%20limits%20reuse%20and%20generality%20%28e.g.%2C%20distillation%29%2C%20or%20retrieve%20unstructured%20information%20that%20still%20leaves%20the%20SLM%20to%20determine%20an%20appropriate%20reasoning%20strategy.%20We%20propose%20instruction%20retrieval%2C%20an%20inference-time%20intervention%20that%20augments%20an%20SLM%20with%20structured%2C%20reusable%20reasoning%20procedures%20rather%20than%20raw%20passages.%20We%20construct%20an%20Instruction%20Corpus%20by%20clustering%20similar%20training%20questions%20and%20using%20a%20teacher%20model%20to%20generate%20generalizable%20guides%20that%20pair%20domain%20background%20with%20explicit%20step-by-step%20procedures.%20At%20inference%2C%20the%20SLM%20retrieves%20the%20instructions%20most%20relevant%20to%20a%20given%20query%20and%20executes%20the%20associated%20procedures%20without%20any%20additional%20fine-tuning.%20Across%20three%20challenging%20domains%3A%20medicine%2C%20law%2C%20and%20mathematics%2C%20instruction%20retrieval%20yields%20consistent%20gains%20for%20models%20with%20at%20least%203B%20parameters%2C%20improving%20accuracy%20by%209.4%25%2C%207.9%25%2C%20and%205.1%25%2C%20respectively%2C%20with%20the%20strongest%2014B%20model%20surpassing%20GPT-4o%27s%20zero-shot%20performance%20on%20knowledge-intensive%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBig%2520Reasoning%2520with%2520Small%2520Models%253A%2520Instruction%2520Retrieval%2520at%2520Inference%2520Time%26entry.906535625%3DKenan%2520Alkiek%2520and%2520David%2520Jurgens%2520and%2520Vinod%2520Vydiswaran%26entry.1292438233%3DSmall%2520language%2520models%2520%2528SLMs%2529%2520enable%2520low-cost%252C%2520private%252C%2520on-device%2520inference%252C%2520but%2520they%2520often%2520fail%2520on%2520problems%2520that%2520require%2520specialized%2520domain%2520knowledge%2520or%2520multi-step%2520reasoning.%2520Existing%2520approaches%2520for%2520improving%2520reasoning%2520either%2520rely%2520on%2520scale%2520%2528e.g.%252C%2520chain-of-thought%2520prompting%2529%252C%2520require%2520task-specific%2520training%2520that%2520limits%2520reuse%2520and%2520generality%2520%2528e.g.%252C%2520distillation%2529%252C%2520or%2520retrieve%2520unstructured%2520information%2520that%2520still%2520leaves%2520the%2520SLM%2520to%2520determine%2520an%2520appropriate%2520reasoning%2520strategy.%2520We%2520propose%2520instruction%2520retrieval%252C%2520an%2520inference-time%2520intervention%2520that%2520augments%2520an%2520SLM%2520with%2520structured%252C%2520reusable%2520reasoning%2520procedures%2520rather%2520than%2520raw%2520passages.%2520We%2520construct%2520an%2520Instruction%2520Corpus%2520by%2520clustering%2520similar%2520training%2520questions%2520and%2520using%2520a%2520teacher%2520model%2520to%2520generate%2520generalizable%2520guides%2520that%2520pair%2520domain%2520background%2520with%2520explicit%2520step-by-step%2520procedures.%2520At%2520inference%252C%2520the%2520SLM%2520retrieves%2520the%2520instructions%2520most%2520relevant%2520to%2520a%2520given%2520query%2520and%2520executes%2520the%2520associated%2520procedures%2520without%2520any%2520additional%2520fine-tuning.%2520Across%2520three%2520challenging%2520domains%253A%2520medicine%252C%2520law%252C%2520and%2520mathematics%252C%2520instruction%2520retrieval%2520yields%2520consistent%2520gains%2520for%2520models%2520with%2520at%2520least%25203B%2520parameters%252C%2520improving%2520accuracy%2520by%25209.4%2525%252C%25207.9%2525%252C%2520and%25205.1%2525%252C%2520respectively%252C%2520with%2520the%2520strongest%252014B%2520model%2520surpassing%2520GPT-4o%2527s%2520zero-shot%2520performance%2520on%2520knowledge-intensive%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Big%20Reasoning%20with%20Small%20Models%3A%20Instruction%20Retrieval%20at%20Inference%20Time&entry.906535625=Kenan%20Alkiek%20and%20David%20Jurgens%20and%20Vinod%20Vydiswaran&entry.1292438233=Small%20language%20models%20%28SLMs%29%20enable%20low-cost%2C%20private%2C%20on-device%20inference%2C%20but%20they%20often%20fail%20on%20problems%20that%20require%20specialized%20domain%20knowledge%20or%20multi-step%20reasoning.%20Existing%20approaches%20for%20improving%20reasoning%20either%20rely%20on%20scale%20%28e.g.%2C%20chain-of-thought%20prompting%29%2C%20require%20task-specific%20training%20that%20limits%20reuse%20and%20generality%20%28e.g.%2C%20distillation%29%2C%20or%20retrieve%20unstructured%20information%20that%20still%20leaves%20the%20SLM%20to%20determine%20an%20appropriate%20reasoning%20strategy.%20We%20propose%20instruction%20retrieval%2C%20an%20inference-time%20intervention%20that%20augments%20an%20SLM%20with%20structured%2C%20reusable%20reasoning%20procedures%20rather%20than%20raw%20passages.%20We%20construct%20an%20Instruction%20Corpus%20by%20clustering%20similar%20training%20questions%20and%20using%20a%20teacher%20model%20to%20generate%20generalizable%20guides%20that%20pair%20domain%20background%20with%20explicit%20step-by-step%20procedures.%20At%20inference%2C%20the%20SLM%20retrieves%20the%20instructions%20most%20relevant%20to%20a%20given%20query%20and%20executes%20the%20associated%20procedures%20without%20any%20additional%20fine-tuning.%20Across%20three%20challenging%20domains%3A%20medicine%2C%20law%2C%20and%20mathematics%2C%20instruction%20retrieval%20yields%20consistent%20gains%20for%20models%20with%20at%20least%203B%20parameters%2C%20improving%20accuracy%20by%209.4%25%2C%207.9%25%2C%20and%205.1%25%2C%20respectively%2C%20with%20the%20strongest%2014B%20model%20surpassing%20GPT-4o%27s%20zero-shot%20performance%20on%20knowledge-intensive%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2510.13935v2&entry.124074799=Read"},
{"title": "Large-Scale Aspect-Based Sentiment Analysis with Reasoning-Infused LLMs", "author": "Pawe\u0142 Liskowski and Krzysztof Jankowski", "abstract": "We introduce Arctic-ABSA, a collection of powerful models for real-life aspect-based sentiment analysis (ABSA). Our models are tailored to commercial needs, trained on a large corpus of public data alongside carefully generated synthetic data, resulting in a dataset 20 times larger than SemEval14. We extend typical ABSA models by expanding the number of sentiment classes from the standard three (positive, negative, neutral) to five, adding mixed and unknown classes, while also jointly predicting overall text sentiment and supporting multiple languages. We experiment with reasoning injection by fine-tuning on Chain-of-Thought (CoT) examples and introduce a novel reasoning pretraining technique for encoder-only models that significantly improves downstream fine-tuning and generalization. Our 395M-parameter encoder and 8B-parameter decoder achieve up to 10 percentage points higher accuracy than GPT-4o and Claude 3.5 Sonnet, while setting new state-of-the-art results on the SemEval14 benchmark. A single multilingual model maintains 87-91% accuracy across six languages without degrading English performance. We release ABSA-mix, a large-scale benchmark aggregating 17 public ABSA datasets across 92 domains.", "link": "http://arxiv.org/abs/2601.03940v1", "date": "2026-01-07", "relevancy": 1.9952, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5019}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Scale%20Aspect-Based%20Sentiment%20Analysis%20with%20Reasoning-Infused%20LLMs&body=Title%3A%20Large-Scale%20Aspect-Based%20Sentiment%20Analysis%20with%20Reasoning-Infused%20LLMs%0AAuthor%3A%20Pawe%C5%82%20Liskowski%20and%20Krzysztof%20Jankowski%0AAbstract%3A%20We%20introduce%20Arctic-ABSA%2C%20a%20collection%20of%20powerful%20models%20for%20real-life%20aspect-based%20sentiment%20analysis%20%28ABSA%29.%20Our%20models%20are%20tailored%20to%20commercial%20needs%2C%20trained%20on%20a%20large%20corpus%20of%20public%20data%20alongside%20carefully%20generated%20synthetic%20data%2C%20resulting%20in%20a%20dataset%2020%20times%20larger%20than%20SemEval14.%20We%20extend%20typical%20ABSA%20models%20by%20expanding%20the%20number%20of%20sentiment%20classes%20from%20the%20standard%20three%20%28positive%2C%20negative%2C%20neutral%29%20to%20five%2C%20adding%20mixed%20and%20unknown%20classes%2C%20while%20also%20jointly%20predicting%20overall%20text%20sentiment%20and%20supporting%20multiple%20languages.%20We%20experiment%20with%20reasoning%20injection%20by%20fine-tuning%20on%20Chain-of-Thought%20%28CoT%29%20examples%20and%20introduce%20a%20novel%20reasoning%20pretraining%20technique%20for%20encoder-only%20models%20that%20significantly%20improves%20downstream%20fine-tuning%20and%20generalization.%20Our%20395M-parameter%20encoder%20and%208B-parameter%20decoder%20achieve%20up%20to%2010%20percentage%20points%20higher%20accuracy%20than%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%20while%20setting%20new%20state-of-the-art%20results%20on%20the%20SemEval14%20benchmark.%20A%20single%20multilingual%20model%20maintains%2087-91%25%20accuracy%20across%20six%20languages%20without%20degrading%20English%20performance.%20We%20release%20ABSA-mix%2C%20a%20large-scale%20benchmark%20aggregating%2017%20public%20ABSA%20datasets%20across%2092%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Scale%2520Aspect-Based%2520Sentiment%2520Analysis%2520with%2520Reasoning-Infused%2520LLMs%26entry.906535625%3DPawe%25C5%2582%2520Liskowski%2520and%2520Krzysztof%2520Jankowski%26entry.1292438233%3DWe%2520introduce%2520Arctic-ABSA%252C%2520a%2520collection%2520of%2520powerful%2520models%2520for%2520real-life%2520aspect-based%2520sentiment%2520analysis%2520%2528ABSA%2529.%2520Our%2520models%2520are%2520tailored%2520to%2520commercial%2520needs%252C%2520trained%2520on%2520a%2520large%2520corpus%2520of%2520public%2520data%2520alongside%2520carefully%2520generated%2520synthetic%2520data%252C%2520resulting%2520in%2520a%2520dataset%252020%2520times%2520larger%2520than%2520SemEval14.%2520We%2520extend%2520typical%2520ABSA%2520models%2520by%2520expanding%2520the%2520number%2520of%2520sentiment%2520classes%2520from%2520the%2520standard%2520three%2520%2528positive%252C%2520negative%252C%2520neutral%2529%2520to%2520five%252C%2520adding%2520mixed%2520and%2520unknown%2520classes%252C%2520while%2520also%2520jointly%2520predicting%2520overall%2520text%2520sentiment%2520and%2520supporting%2520multiple%2520languages.%2520We%2520experiment%2520with%2520reasoning%2520injection%2520by%2520fine-tuning%2520on%2520Chain-of-Thought%2520%2528CoT%2529%2520examples%2520and%2520introduce%2520a%2520novel%2520reasoning%2520pretraining%2520technique%2520for%2520encoder-only%2520models%2520that%2520significantly%2520improves%2520downstream%2520fine-tuning%2520and%2520generalization.%2520Our%2520395M-parameter%2520encoder%2520and%25208B-parameter%2520decoder%2520achieve%2520up%2520to%252010%2520percentage%2520points%2520higher%2520accuracy%2520than%2520GPT-4o%2520and%2520Claude%25203.5%2520Sonnet%252C%2520while%2520setting%2520new%2520state-of-the-art%2520results%2520on%2520the%2520SemEval14%2520benchmark.%2520A%2520single%2520multilingual%2520model%2520maintains%252087-91%2525%2520accuracy%2520across%2520six%2520languages%2520without%2520degrading%2520English%2520performance.%2520We%2520release%2520ABSA-mix%252C%2520a%2520large-scale%2520benchmark%2520aggregating%252017%2520public%2520ABSA%2520datasets%2520across%252092%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Scale%20Aspect-Based%20Sentiment%20Analysis%20with%20Reasoning-Infused%20LLMs&entry.906535625=Pawe%C5%82%20Liskowski%20and%20Krzysztof%20Jankowski&entry.1292438233=We%20introduce%20Arctic-ABSA%2C%20a%20collection%20of%20powerful%20models%20for%20real-life%20aspect-based%20sentiment%20analysis%20%28ABSA%29.%20Our%20models%20are%20tailored%20to%20commercial%20needs%2C%20trained%20on%20a%20large%20corpus%20of%20public%20data%20alongside%20carefully%20generated%20synthetic%20data%2C%20resulting%20in%20a%20dataset%2020%20times%20larger%20than%20SemEval14.%20We%20extend%20typical%20ABSA%20models%20by%20expanding%20the%20number%20of%20sentiment%20classes%20from%20the%20standard%20three%20%28positive%2C%20negative%2C%20neutral%29%20to%20five%2C%20adding%20mixed%20and%20unknown%20classes%2C%20while%20also%20jointly%20predicting%20overall%20text%20sentiment%20and%20supporting%20multiple%20languages.%20We%20experiment%20with%20reasoning%20injection%20by%20fine-tuning%20on%20Chain-of-Thought%20%28CoT%29%20examples%20and%20introduce%20a%20novel%20reasoning%20pretraining%20technique%20for%20encoder-only%20models%20that%20significantly%20improves%20downstream%20fine-tuning%20and%20generalization.%20Our%20395M-parameter%20encoder%20and%208B-parameter%20decoder%20achieve%20up%20to%2010%20percentage%20points%20higher%20accuracy%20than%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%20while%20setting%20new%20state-of-the-art%20results%20on%20the%20SemEval14%20benchmark.%20A%20single%20multilingual%20model%20maintains%2087-91%25%20accuracy%20across%20six%20languages%20without%20degrading%20English%20performance.%20We%20release%20ABSA-mix%2C%20a%20large-scale%20benchmark%20aggregating%2017%20public%20ABSA%20datasets%20across%2092%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.03940v1&entry.124074799=Read"},
{"title": "EngTrace: A Symbolic Benchmark for Verifiable Process Supervision of Engineering Reasoning", "author": "Ayesha Gull and Muhammad Usman Safder and Rania Elbadry and Fan Zhang and Veselin Stoyanov and Preslav Nakov and Zhuohan Xie", "abstract": "Large Language Models (LLMs) are increasingly entering specialized, safety-critical engineering workflows governed by strict quantitative standards and immutable physical laws, making rigorous evaluation of their reasoning capabilities imperative. However, existing benchmarks such as MMLU, MATH, and HumanEval assess isolated cognitive skills, failing to capture the physically grounded reasoning central to engineering, where scientific principles, quantitative modeling, and practical constraints must converge. To enable verifiable process supervision in engineering, we introduce EngTrace, a symbolic benchmark comprising 90 templates across three major engineering branches, nine core domains and 20 distinct areas. Through domain-aware parameterization, we generate 1,350 unique, contamination-resistant test cases to stress-test generalization. Moving beyond outcome matching, we introduce a verifiable two-stage evaluation framework that uses a tiered protocol to validate intermediate reasoning traces alongside final answers through automated procedural checks and a heterogeneous AI Tribunal. Our evaluation of 24 leading LLMs reveals a distinct trade-off between numeric precision and trace fidelity, identifying a complexity cliff where abstract mathematical pre-training fails to translate into the integrative reasoning required for advanced engineering tasks.", "link": "http://arxiv.org/abs/2511.01650v2", "date": "2026-01-07", "relevancy": 1.9881, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EngTrace%3A%20A%20Symbolic%20Benchmark%20for%20Verifiable%20Process%20Supervision%20of%20Engineering%20Reasoning&body=Title%3A%20EngTrace%3A%20A%20Symbolic%20Benchmark%20for%20Verifiable%20Process%20Supervision%20of%20Engineering%20Reasoning%0AAuthor%3A%20Ayesha%20Gull%20and%20Muhammad%20Usman%20Safder%20and%20Rania%20Elbadry%20and%20Fan%20Zhang%20and%20Veselin%20Stoyanov%20and%20Preslav%20Nakov%20and%20Zhuohan%20Xie%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20entering%20specialized%2C%20safety-critical%20engineering%20workflows%20governed%20by%20strict%20quantitative%20standards%20and%20immutable%20physical%20laws%2C%20making%20rigorous%20evaluation%20of%20their%20reasoning%20capabilities%20imperative.%20However%2C%20existing%20benchmarks%20such%20as%20MMLU%2C%20MATH%2C%20and%20HumanEval%20assess%20isolated%20cognitive%20skills%2C%20failing%20to%20capture%20the%20physically%20grounded%20reasoning%20central%20to%20engineering%2C%20where%20scientific%20principles%2C%20quantitative%20modeling%2C%20and%20practical%20constraints%20must%20converge.%20To%20enable%20verifiable%20process%20supervision%20in%20engineering%2C%20we%20introduce%20EngTrace%2C%20a%20symbolic%20benchmark%20comprising%2090%20templates%20across%20three%20major%20engineering%20branches%2C%20nine%20core%20domains%20and%2020%20distinct%20areas.%20Through%20domain-aware%20parameterization%2C%20we%20generate%201%2C350%20unique%2C%20contamination-resistant%20test%20cases%20to%20stress-test%20generalization.%20Moving%20beyond%20outcome%20matching%2C%20we%20introduce%20a%20verifiable%20two-stage%20evaluation%20framework%20that%20uses%20a%20tiered%20protocol%20to%20validate%20intermediate%20reasoning%20traces%20alongside%20final%20answers%20through%20automated%20procedural%20checks%20and%20a%20heterogeneous%20AI%20Tribunal.%20Our%20evaluation%20of%2024%20leading%20LLMs%20reveals%20a%20distinct%20trade-off%20between%20numeric%20precision%20and%20trace%20fidelity%2C%20identifying%20a%20complexity%20cliff%20where%20abstract%20mathematical%20pre-training%20fails%20to%20translate%20into%20the%20integrative%20reasoning%20required%20for%20advanced%20engineering%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01650v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEngTrace%253A%2520A%2520Symbolic%2520Benchmark%2520for%2520Verifiable%2520Process%2520Supervision%2520of%2520Engineering%2520Reasoning%26entry.906535625%3DAyesha%2520Gull%2520and%2520Muhammad%2520Usman%2520Safder%2520and%2520Rania%2520Elbadry%2520and%2520Fan%2520Zhang%2520and%2520Veselin%2520Stoyanov%2520and%2520Preslav%2520Nakov%2520and%2520Zhuohan%2520Xie%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520entering%2520specialized%252C%2520safety-critical%2520engineering%2520workflows%2520governed%2520by%2520strict%2520quantitative%2520standards%2520and%2520immutable%2520physical%2520laws%252C%2520making%2520rigorous%2520evaluation%2520of%2520their%2520reasoning%2520capabilities%2520imperative.%2520However%252C%2520existing%2520benchmarks%2520such%2520as%2520MMLU%252C%2520MATH%252C%2520and%2520HumanEval%2520assess%2520isolated%2520cognitive%2520skills%252C%2520failing%2520to%2520capture%2520the%2520physically%2520grounded%2520reasoning%2520central%2520to%2520engineering%252C%2520where%2520scientific%2520principles%252C%2520quantitative%2520modeling%252C%2520and%2520practical%2520constraints%2520must%2520converge.%2520To%2520enable%2520verifiable%2520process%2520supervision%2520in%2520engineering%252C%2520we%2520introduce%2520EngTrace%252C%2520a%2520symbolic%2520benchmark%2520comprising%252090%2520templates%2520across%2520three%2520major%2520engineering%2520branches%252C%2520nine%2520core%2520domains%2520and%252020%2520distinct%2520areas.%2520Through%2520domain-aware%2520parameterization%252C%2520we%2520generate%25201%252C350%2520unique%252C%2520contamination-resistant%2520test%2520cases%2520to%2520stress-test%2520generalization.%2520Moving%2520beyond%2520outcome%2520matching%252C%2520we%2520introduce%2520a%2520verifiable%2520two-stage%2520evaluation%2520framework%2520that%2520uses%2520a%2520tiered%2520protocol%2520to%2520validate%2520intermediate%2520reasoning%2520traces%2520alongside%2520final%2520answers%2520through%2520automated%2520procedural%2520checks%2520and%2520a%2520heterogeneous%2520AI%2520Tribunal.%2520Our%2520evaluation%2520of%252024%2520leading%2520LLMs%2520reveals%2520a%2520distinct%2520trade-off%2520between%2520numeric%2520precision%2520and%2520trace%2520fidelity%252C%2520identifying%2520a%2520complexity%2520cliff%2520where%2520abstract%2520mathematical%2520pre-training%2520fails%2520to%2520translate%2520into%2520the%2520integrative%2520reasoning%2520required%2520for%2520advanced%2520engineering%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01650v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EngTrace%3A%20A%20Symbolic%20Benchmark%20for%20Verifiable%20Process%20Supervision%20of%20Engineering%20Reasoning&entry.906535625=Ayesha%20Gull%20and%20Muhammad%20Usman%20Safder%20and%20Rania%20Elbadry%20and%20Fan%20Zhang%20and%20Veselin%20Stoyanov%20and%20Preslav%20Nakov%20and%20Zhuohan%20Xie&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20entering%20specialized%2C%20safety-critical%20engineering%20workflows%20governed%20by%20strict%20quantitative%20standards%20and%20immutable%20physical%20laws%2C%20making%20rigorous%20evaluation%20of%20their%20reasoning%20capabilities%20imperative.%20However%2C%20existing%20benchmarks%20such%20as%20MMLU%2C%20MATH%2C%20and%20HumanEval%20assess%20isolated%20cognitive%20skills%2C%20failing%20to%20capture%20the%20physically%20grounded%20reasoning%20central%20to%20engineering%2C%20where%20scientific%20principles%2C%20quantitative%20modeling%2C%20and%20practical%20constraints%20must%20converge.%20To%20enable%20verifiable%20process%20supervision%20in%20engineering%2C%20we%20introduce%20EngTrace%2C%20a%20symbolic%20benchmark%20comprising%2090%20templates%20across%20three%20major%20engineering%20branches%2C%20nine%20core%20domains%20and%2020%20distinct%20areas.%20Through%20domain-aware%20parameterization%2C%20we%20generate%201%2C350%20unique%2C%20contamination-resistant%20test%20cases%20to%20stress-test%20generalization.%20Moving%20beyond%20outcome%20matching%2C%20we%20introduce%20a%20verifiable%20two-stage%20evaluation%20framework%20that%20uses%20a%20tiered%20protocol%20to%20validate%20intermediate%20reasoning%20traces%20alongside%20final%20answers%20through%20automated%20procedural%20checks%20and%20a%20heterogeneous%20AI%20Tribunal.%20Our%20evaluation%20of%2024%20leading%20LLMs%20reveals%20a%20distinct%20trade-off%20between%20numeric%20precision%20and%20trace%20fidelity%2C%20identifying%20a%20complexity%20cliff%20where%20abstract%20mathematical%20pre-training%20fails%20to%20translate%20into%20the%20integrative%20reasoning%20required%20for%20advanced%20engineering%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.01650v2&entry.124074799=Read"},
{"title": "Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware", "author": "Korbinian Moller and Glenn Johannes Tungka and Lucas J\u00fcrgens and Johannes Betz", "abstract": "Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning", "link": "http://arxiv.org/abs/2601.03904v1", "date": "2026-01-07", "relevancy": 1.9856, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5185}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4913}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Safe%20Autonomous%20Driving%3A%20A%20Real-Time%20Motion%20Planning%20Algorithm%20on%20Embedded%20Hardware&body=Title%3A%20Towards%20Safe%20Autonomous%20Driving%3A%20A%20Real-Time%20Motion%20Planning%20Algorithm%20on%20Embedded%20Hardware%0AAuthor%3A%20Korbinian%20Moller%20and%20Glenn%20Johannes%20Tungka%20and%20Lucas%20J%C3%BCrgens%20and%20Johannes%20Betz%0AAbstract%3A%20Ensuring%20the%20functional%20safety%20of%20Autonomous%20Vehicles%20%28AVs%29%20requires%20motion%20planning%20modules%20that%20not%20only%20operate%20within%20strict%20real-time%20constraints%20but%20also%20maintain%20controllability%20in%20case%20of%20system%20faults.%20Existing%20safeguarding%20concepts%2C%20such%20as%20Online%20Verification%20%28OV%29%2C%20provide%20safety%20layers%20that%20detect%20infeasible%20planning%20outputs.%20However%2C%20they%20lack%20an%20active%20mechanism%20to%20ensure%20safe%20operation%20in%20the%20event%20that%20the%20main%20planner%20fails.%20This%20paper%20presents%20a%20first%20step%20toward%20an%20active%20safety%20extension%20for%20fail-operational%20Autonomous%20Driving%20%28AD%29.%20We%20deploy%20a%20lightweight%20sampling-based%20trajectory%20planner%20on%20an%20automotive-grade%2C%20embedded%20platform%20running%20a%20Real-Time%20Operating%20System%20%28RTOS%29.%20The%20planner%20continuously%20computes%20trajectories%20under%20constrained%20computational%20resources%2C%20forming%20the%20foundation%20for%20future%20emergency%20planning%20architectures.%20Experimental%20results%20demonstrate%20deterministic%20timing%20behavior%20with%20bounded%20latency%20and%20minimal%20jitter%2C%20validating%20the%20feasibility%20of%20trajectory%20planning%20on%20safety-certifiable%20hardware.%20The%20study%20highlights%20both%20the%20potential%20and%20the%20remaining%20challenges%20of%20integrating%20active%20fallback%20mechanisms%20as%20an%20integral%20part%20of%20next-generation%20safeguarding%20frameworks.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/TUM-AVS/real-time-motion-planning%0ALink%3A%20http%3A//arxiv.org/abs/2601.03904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Safe%2520Autonomous%2520Driving%253A%2520A%2520Real-Time%2520Motion%2520Planning%2520Algorithm%2520on%2520Embedded%2520Hardware%26entry.906535625%3DKorbinian%2520Moller%2520and%2520Glenn%2520Johannes%2520Tungka%2520and%2520Lucas%2520J%25C3%25BCrgens%2520and%2520Johannes%2520Betz%26entry.1292438233%3DEnsuring%2520the%2520functional%2520safety%2520of%2520Autonomous%2520Vehicles%2520%2528AVs%2529%2520requires%2520motion%2520planning%2520modules%2520that%2520not%2520only%2520operate%2520within%2520strict%2520real-time%2520constraints%2520but%2520also%2520maintain%2520controllability%2520in%2520case%2520of%2520system%2520faults.%2520Existing%2520safeguarding%2520concepts%252C%2520such%2520as%2520Online%2520Verification%2520%2528OV%2529%252C%2520provide%2520safety%2520layers%2520that%2520detect%2520infeasible%2520planning%2520outputs.%2520However%252C%2520they%2520lack%2520an%2520active%2520mechanism%2520to%2520ensure%2520safe%2520operation%2520in%2520the%2520event%2520that%2520the%2520main%2520planner%2520fails.%2520This%2520paper%2520presents%2520a%2520first%2520step%2520toward%2520an%2520active%2520safety%2520extension%2520for%2520fail-operational%2520Autonomous%2520Driving%2520%2528AD%2529.%2520We%2520deploy%2520a%2520lightweight%2520sampling-based%2520trajectory%2520planner%2520on%2520an%2520automotive-grade%252C%2520embedded%2520platform%2520running%2520a%2520Real-Time%2520Operating%2520System%2520%2528RTOS%2529.%2520The%2520planner%2520continuously%2520computes%2520trajectories%2520under%2520constrained%2520computational%2520resources%252C%2520forming%2520the%2520foundation%2520for%2520future%2520emergency%2520planning%2520architectures.%2520Experimental%2520results%2520demonstrate%2520deterministic%2520timing%2520behavior%2520with%2520bounded%2520latency%2520and%2520minimal%2520jitter%252C%2520validating%2520the%2520feasibility%2520of%2520trajectory%2520planning%2520on%2520safety-certifiable%2520hardware.%2520The%2520study%2520highlights%2520both%2520the%2520potential%2520and%2520the%2520remaining%2520challenges%2520of%2520integrating%2520active%2520fallback%2520mechanisms%2520as%2520an%2520integral%2520part%2520of%2520next-generation%2520safeguarding%2520frameworks.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/TUM-AVS/real-time-motion-planning%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Safe%20Autonomous%20Driving%3A%20A%20Real-Time%20Motion%20Planning%20Algorithm%20on%20Embedded%20Hardware&entry.906535625=Korbinian%20Moller%20and%20Glenn%20Johannes%20Tungka%20and%20Lucas%20J%C3%BCrgens%20and%20Johannes%20Betz&entry.1292438233=Ensuring%20the%20functional%20safety%20of%20Autonomous%20Vehicles%20%28AVs%29%20requires%20motion%20planning%20modules%20that%20not%20only%20operate%20within%20strict%20real-time%20constraints%20but%20also%20maintain%20controllability%20in%20case%20of%20system%20faults.%20Existing%20safeguarding%20concepts%2C%20such%20as%20Online%20Verification%20%28OV%29%2C%20provide%20safety%20layers%20that%20detect%20infeasible%20planning%20outputs.%20However%2C%20they%20lack%20an%20active%20mechanism%20to%20ensure%20safe%20operation%20in%20the%20event%20that%20the%20main%20planner%20fails.%20This%20paper%20presents%20a%20first%20step%20toward%20an%20active%20safety%20extension%20for%20fail-operational%20Autonomous%20Driving%20%28AD%29.%20We%20deploy%20a%20lightweight%20sampling-based%20trajectory%20planner%20on%20an%20automotive-grade%2C%20embedded%20platform%20running%20a%20Real-Time%20Operating%20System%20%28RTOS%29.%20The%20planner%20continuously%20computes%20trajectories%20under%20constrained%20computational%20resources%2C%20forming%20the%20foundation%20for%20future%20emergency%20planning%20architectures.%20Experimental%20results%20demonstrate%20deterministic%20timing%20behavior%20with%20bounded%20latency%20and%20minimal%20jitter%2C%20validating%20the%20feasibility%20of%20trajectory%20planning%20on%20safety-certifiable%20hardware.%20The%20study%20highlights%20both%20the%20potential%20and%20the%20remaining%20challenges%20of%20integrating%20active%20fallback%20mechanisms%20as%20an%20integral%20part%20of%20next-generation%20safeguarding%20frameworks.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/TUM-AVS/real-time-motion-planning&entry.1838667208=http%3A//arxiv.org/abs/2601.03904v1&entry.124074799=Read"},
{"title": "An Overview of Prototype Formulations for Interpretable Deep Learning", "author": "Maximilian Xiling Li and Korbinian Franz Rudolf and Paul Mattes and Nils Blank and Rudolf Lioutikov", "abstract": "Prototypical part networks offer interpretable alternatives to black-box deep learning models by learning visual prototypes for classification. This work provides a comprehensive analysis of prototype formulations, comparing point-based and probabilistic approaches in both Euclidean and hyperspherical latent spaces.\n  We introduce HyperPG, a probabilistic prototype representation using Gaussian distributions on hyperspheres. Experiments on CUB-200-2011, Stanford Cars, and Oxford Flowers datasets show that hyperspherical prototypes outperform standard Euclidean formulations. Critically, hyperspherical prototypes maintain competitive performance under simplified training schemes, while Euclidean prototypes require extensive hyperparameter tuning.", "link": "http://arxiv.org/abs/2410.08925v4", "date": "2026-01-07", "relevancy": 1.9819, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Overview%20of%20Prototype%20Formulations%20for%20Interpretable%20Deep%20Learning&body=Title%3A%20An%20Overview%20of%20Prototype%20Formulations%20for%20Interpretable%20Deep%20Learning%0AAuthor%3A%20Maximilian%20Xiling%20Li%20and%20Korbinian%20Franz%20Rudolf%20and%20Paul%20Mattes%20and%20Nils%20Blank%20and%20Rudolf%20Lioutikov%0AAbstract%3A%20Prototypical%20part%20networks%20offer%20interpretable%20alternatives%20to%20black-box%20deep%20learning%20models%20by%20learning%20visual%20prototypes%20for%20classification.%20This%20work%20provides%20a%20comprehensive%20analysis%20of%20prototype%20formulations%2C%20comparing%20point-based%20and%20probabilistic%20approaches%20in%20both%20Euclidean%20and%20hyperspherical%20latent%20spaces.%0A%20%20We%20introduce%20HyperPG%2C%20a%20probabilistic%20prototype%20representation%20using%20Gaussian%20distributions%20on%20hyperspheres.%20Experiments%20on%20CUB-200-2011%2C%20Stanford%20Cars%2C%20and%20Oxford%20Flowers%20datasets%20show%20that%20hyperspherical%20prototypes%20outperform%20standard%20Euclidean%20formulations.%20Critically%2C%20hyperspherical%20prototypes%20maintain%20competitive%20performance%20under%20simplified%20training%20schemes%2C%20while%20Euclidean%20prototypes%20require%20extensive%20hyperparameter%20tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2410.08925v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Overview%2520of%2520Prototype%2520Formulations%2520for%2520Interpretable%2520Deep%2520Learning%26entry.906535625%3DMaximilian%2520Xiling%2520Li%2520and%2520Korbinian%2520Franz%2520Rudolf%2520and%2520Paul%2520Mattes%2520and%2520Nils%2520Blank%2520and%2520Rudolf%2520Lioutikov%26entry.1292438233%3DPrototypical%2520part%2520networks%2520offer%2520interpretable%2520alternatives%2520to%2520black-box%2520deep%2520learning%2520models%2520by%2520learning%2520visual%2520prototypes%2520for%2520classification.%2520This%2520work%2520provides%2520a%2520comprehensive%2520analysis%2520of%2520prototype%2520formulations%252C%2520comparing%2520point-based%2520and%2520probabilistic%2520approaches%2520in%2520both%2520Euclidean%2520and%2520hyperspherical%2520latent%2520spaces.%250A%2520%2520We%2520introduce%2520HyperPG%252C%2520a%2520probabilistic%2520prototype%2520representation%2520using%2520Gaussian%2520distributions%2520on%2520hyperspheres.%2520Experiments%2520on%2520CUB-200-2011%252C%2520Stanford%2520Cars%252C%2520and%2520Oxford%2520Flowers%2520datasets%2520show%2520that%2520hyperspherical%2520prototypes%2520outperform%2520standard%2520Euclidean%2520formulations.%2520Critically%252C%2520hyperspherical%2520prototypes%2520maintain%2520competitive%2520performance%2520under%2520simplified%2520training%2520schemes%252C%2520while%2520Euclidean%2520prototypes%2520require%2520extensive%2520hyperparameter%2520tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08925v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Overview%20of%20Prototype%20Formulations%20for%20Interpretable%20Deep%20Learning&entry.906535625=Maximilian%20Xiling%20Li%20and%20Korbinian%20Franz%20Rudolf%20and%20Paul%20Mattes%20and%20Nils%20Blank%20and%20Rudolf%20Lioutikov&entry.1292438233=Prototypical%20part%20networks%20offer%20interpretable%20alternatives%20to%20black-box%20deep%20learning%20models%20by%20learning%20visual%20prototypes%20for%20classification.%20This%20work%20provides%20a%20comprehensive%20analysis%20of%20prototype%20formulations%2C%20comparing%20point-based%20and%20probabilistic%20approaches%20in%20both%20Euclidean%20and%20hyperspherical%20latent%20spaces.%0A%20%20We%20introduce%20HyperPG%2C%20a%20probabilistic%20prototype%20representation%20using%20Gaussian%20distributions%20on%20hyperspheres.%20Experiments%20on%20CUB-200-2011%2C%20Stanford%20Cars%2C%20and%20Oxford%20Flowers%20datasets%20show%20that%20hyperspherical%20prototypes%20outperform%20standard%20Euclidean%20formulations.%20Critically%2C%20hyperspherical%20prototypes%20maintain%20competitive%20performance%20under%20simplified%20training%20schemes%2C%20while%20Euclidean%20prototypes%20require%20extensive%20hyperparameter%20tuning.&entry.1838667208=http%3A//arxiv.org/abs/2410.08925v4&entry.124074799=Read"},
{"title": "PhysDepth: Plug-and-Play Physical Refinement for Monocular Depth Estimation in Challenging Environments", "author": "Kebin Peng and Haotang Li and Zhenyu Qi and Huashan Chen and Zi Wang and Wei Zhang and Sen He and Huanrui Yang and Qing Guo", "abstract": "State-of-the-art monocular depth estimation (MDE) models often struggle in challenging environments, primarily because they overlook robust physical information. To demonstrate this, we first conduct an empirical study by computing the covariance between a model's prediction error and atmospheric attenuation. We find that the error of existing SOTAs increases with atmospheric attenuation. Based on this finding, we propose PhysDepth, a plug-and-play framework that solves this fragility by infusing physical priors into modern SOTA backbones. PhysDepth incorporates two key components: a Physical Prior Module (PPM) that leverages Rayleigh Scattering theory to extract robust features from the high-SNR red channel, and a physics-derived Red Channel Attenuation Loss (RCA) that enforces model to learn the Beer-Lambert law. Extensive evaluations demonstrate that PhysDepth achieves SOTA accuracy in challenging conditions.", "link": "http://arxiv.org/abs/2412.04666v3", "date": "2026-01-07", "relevancy": 1.6199, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5805}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.54}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysDepth%3A%20Plug-and-Play%20Physical%20Refinement%20for%20Monocular%20Depth%20Estimation%20in%20Challenging%20Environments&body=Title%3A%20PhysDepth%3A%20Plug-and-Play%20Physical%20Refinement%20for%20Monocular%20Depth%20Estimation%20in%20Challenging%20Environments%0AAuthor%3A%20Kebin%20Peng%20and%20Haotang%20Li%20and%20Zhenyu%20Qi%20and%20Huashan%20Chen%20and%20Zi%20Wang%20and%20Wei%20Zhang%20and%20Sen%20He%20and%20Huanrui%20Yang%20and%20Qing%20Guo%0AAbstract%3A%20State-of-the-art%20monocular%20depth%20estimation%20%28MDE%29%20models%20often%20struggle%20in%20challenging%20environments%2C%20primarily%20because%20they%20overlook%20robust%20physical%20information.%20To%20demonstrate%20this%2C%20we%20first%20conduct%20an%20empirical%20study%20by%20computing%20the%20covariance%20between%20a%20model%27s%20prediction%20error%20and%20atmospheric%20attenuation.%20We%20find%20that%20the%20error%20of%20existing%20SOTAs%20increases%20with%20atmospheric%20attenuation.%20Based%20on%20this%20finding%2C%20we%20propose%20PhysDepth%2C%20a%20plug-and-play%20framework%20that%20solves%20this%20fragility%20by%20infusing%20physical%20priors%20into%20modern%20SOTA%20backbones.%20PhysDepth%20incorporates%20two%20key%20components%3A%20a%20Physical%20Prior%20Module%20%28PPM%29%20that%20leverages%20Rayleigh%20Scattering%20theory%20to%20extract%20robust%20features%20from%20the%20high-SNR%20red%20channel%2C%20and%20a%20physics-derived%20Red%20Channel%20Attenuation%20Loss%20%28RCA%29%20that%20enforces%20model%20to%20learn%20the%20Beer-Lambert%20law.%20Extensive%20evaluations%20demonstrate%20that%20PhysDepth%20achieves%20SOTA%20accuracy%20in%20challenging%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2412.04666v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysDepth%253A%2520Plug-and-Play%2520Physical%2520Refinement%2520for%2520Monocular%2520Depth%2520Estimation%2520in%2520Challenging%2520Environments%26entry.906535625%3DKebin%2520Peng%2520and%2520Haotang%2520Li%2520and%2520Zhenyu%2520Qi%2520and%2520Huashan%2520Chen%2520and%2520Zi%2520Wang%2520and%2520Wei%2520Zhang%2520and%2520Sen%2520He%2520and%2520Huanrui%2520Yang%2520and%2520Qing%2520Guo%26entry.1292438233%3DState-of-the-art%2520monocular%2520depth%2520estimation%2520%2528MDE%2529%2520models%2520often%2520struggle%2520in%2520challenging%2520environments%252C%2520primarily%2520because%2520they%2520overlook%2520robust%2520physical%2520information.%2520To%2520demonstrate%2520this%252C%2520we%2520first%2520conduct%2520an%2520empirical%2520study%2520by%2520computing%2520the%2520covariance%2520between%2520a%2520model%2527s%2520prediction%2520error%2520and%2520atmospheric%2520attenuation.%2520We%2520find%2520that%2520the%2520error%2520of%2520existing%2520SOTAs%2520increases%2520with%2520atmospheric%2520attenuation.%2520Based%2520on%2520this%2520finding%252C%2520we%2520propose%2520PhysDepth%252C%2520a%2520plug-and-play%2520framework%2520that%2520solves%2520this%2520fragility%2520by%2520infusing%2520physical%2520priors%2520into%2520modern%2520SOTA%2520backbones.%2520PhysDepth%2520incorporates%2520two%2520key%2520components%253A%2520a%2520Physical%2520Prior%2520Module%2520%2528PPM%2529%2520that%2520leverages%2520Rayleigh%2520Scattering%2520theory%2520to%2520extract%2520robust%2520features%2520from%2520the%2520high-SNR%2520red%2520channel%252C%2520and%2520a%2520physics-derived%2520Red%2520Channel%2520Attenuation%2520Loss%2520%2528RCA%2529%2520that%2520enforces%2520model%2520to%2520learn%2520the%2520Beer-Lambert%2520law.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520PhysDepth%2520achieves%2520SOTA%2520accuracy%2520in%2520challenging%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04666v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysDepth%3A%20Plug-and-Play%20Physical%20Refinement%20for%20Monocular%20Depth%20Estimation%20in%20Challenging%20Environments&entry.906535625=Kebin%20Peng%20and%20Haotang%20Li%20and%20Zhenyu%20Qi%20and%20Huashan%20Chen%20and%20Zi%20Wang%20and%20Wei%20Zhang%20and%20Sen%20He%20and%20Huanrui%20Yang%20and%20Qing%20Guo&entry.1292438233=State-of-the-art%20monocular%20depth%20estimation%20%28MDE%29%20models%20often%20struggle%20in%20challenging%20environments%2C%20primarily%20because%20they%20overlook%20robust%20physical%20information.%20To%20demonstrate%20this%2C%20we%20first%20conduct%20an%20empirical%20study%20by%20computing%20the%20covariance%20between%20a%20model%27s%20prediction%20error%20and%20atmospheric%20attenuation.%20We%20find%20that%20the%20error%20of%20existing%20SOTAs%20increases%20with%20atmospheric%20attenuation.%20Based%20on%20this%20finding%2C%20we%20propose%20PhysDepth%2C%20a%20plug-and-play%20framework%20that%20solves%20this%20fragility%20by%20infusing%20physical%20priors%20into%20modern%20SOTA%20backbones.%20PhysDepth%20incorporates%20two%20key%20components%3A%20a%20Physical%20Prior%20Module%20%28PPM%29%20that%20leverages%20Rayleigh%20Scattering%20theory%20to%20extract%20robust%20features%20from%20the%20high-SNR%20red%20channel%2C%20and%20a%20physics-derived%20Red%20Channel%20Attenuation%20Loss%20%28RCA%29%20that%20enforces%20model%20to%20learn%20the%20Beer-Lambert%20law.%20Extensive%20evaluations%20demonstrate%20that%20PhysDepth%20achieves%20SOTA%20accuracy%20in%20challenging%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2412.04666v3&entry.124074799=Read"},
{"title": "Implementing the First-Order Logic of Here and There", "author": "Jens Otten and Torsten Schaub", "abstract": "We present automated theorem provers for the first-order logic of here and there (HT). They are based on a native sequent calculus for the logic of HT and an axiomatic embedding of the logic of HT into intuitionistic logic. The analytic proof search in the sequent calculus is optimized by using free variables and skolemization. The embedding is used in combination with sequent, tableau and connection calculi for intuitionistic first-order logic. All provers are evaluated on a large benchmark set of first-order formulas, providing a foundation for the development of more efficient HT provers.\n", "link": "http://arxiv.org/abs/2601.03848v1", "date": "2026-01-07", "relevancy": 1.4958, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implementing%20the%20First-Order%20Logic%20of%20Here%20and%20There&body=Title%3A%20Implementing%20the%20First-Order%20Logic%20of%20Here%20and%20There%0AAuthor%3A%20Jens%20Otten%20and%20Torsten%20Schaub%0AAbstract%3A%20We%20present%20automated%20theorem%20provers%20for%20the%20first-order%20logic%20of%20here%20and%20there%20%28HT%29.%20They%20are%20based%20on%20a%20native%20sequent%20calculus%20for%20the%20logic%20of%20HT%20and%20an%20axiomatic%20embedding%20of%20the%20logic%20of%20HT%20into%20intuitionistic%20logic.%20The%20analytic%20proof%20search%20in%20the%20sequent%20calculus%20is%20optimized%20by%20using%20free%20variables%20and%20skolemization.%20The%20embedding%20is%20used%20in%20combination%20with%20sequent%2C%20tableau%20and%20connection%20calculi%20for%20intuitionistic%20first-order%20logic.%20All%20provers%20are%20evaluated%20on%20a%20large%20benchmark%20set%20of%20first-order%20formulas%2C%20providing%20a%20foundation%20for%20the%20development%20of%20more%20efficient%20HT%20provers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2601.03848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplementing%2520the%2520First-Order%2520Logic%2520of%2520Here%2520and%2520There%26entry.906535625%3DJens%2520Otten%2520and%2520Torsten%2520Schaub%26entry.1292438233%3DWe%2520present%2520automated%2520theorem%2520provers%2520for%2520the%2520first-order%2520logic%2520of%2520here%2520and%2520there%2520%2528HT%2529.%2520They%2520are%2520based%2520on%2520a%2520native%2520sequent%2520calculus%2520for%2520the%2520logic%2520of%2520HT%2520and%2520an%2520axiomatic%2520embedding%2520of%2520the%2520logic%2520of%2520HT%2520into%2520intuitionistic%2520logic.%2520The%2520analytic%2520proof%2520search%2520in%2520the%2520sequent%2520calculus%2520is%2520optimized%2520by%2520using%2520free%2520variables%2520and%2520skolemization.%2520The%2520embedding%2520is%2520used%2520in%2520combination%2520with%2520sequent%252C%2520tableau%2520and%2520connection%2520calculi%2520for%2520intuitionistic%2520first-order%2520logic.%2520All%2520provers%2520are%2520evaluated%2520on%2520a%2520large%2520benchmark%2520set%2520of%2520first-order%2520formulas%252C%2520providing%2520a%2520foundation%2520for%2520the%2520development%2520of%2520more%2520efficient%2520HT%2520provers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implementing%20the%20First-Order%20Logic%20of%20Here%20and%20There&entry.906535625=Jens%20Otten%20and%20Torsten%20Schaub&entry.1292438233=We%20present%20automated%20theorem%20provers%20for%20the%20first-order%20logic%20of%20here%20and%20there%20%28HT%29.%20They%20are%20based%20on%20a%20native%20sequent%20calculus%20for%20the%20logic%20of%20HT%20and%20an%20axiomatic%20embedding%20of%20the%20logic%20of%20HT%20into%20intuitionistic%20logic.%20The%20analytic%20proof%20search%20in%20the%20sequent%20calculus%20is%20optimized%20by%20using%20free%20variables%20and%20skolemization.%20The%20embedding%20is%20used%20in%20combination%20with%20sequent%2C%20tableau%20and%20connection%20calculi%20for%20intuitionistic%20first-order%20logic.%20All%20provers%20are%20evaluated%20on%20a%20large%20benchmark%20set%20of%20first-order%20formulas%2C%20providing%20a%20foundation%20for%20the%20development%20of%20more%20efficient%20HT%20provers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2601.03848v1&entry.124074799=Read"},
{"title": "Stage-specific cancer survival prediction enriched by explainable machine learning", "author": "Parisa Poorhasani and Bogdan Iancu", "abstract": "Despite the fact that cancer survivability rates vary greatly between stages, traditional survival prediction models have frequently been trained and assessed using examples from all combined phases of the disease. This method may result in an overestimation of performance and ignore the stage-specific variations. Using the SEER dataset, we created and verified explainable machine learning (ML) models to predict stage-specific cancer survivability in colorectal, stomach, and liver cancers. ML-based cancer survival analysis has been a long-standing topic in the literature; however, studies involving the explainability and transparency of ML survivability models are limited. Our use of explainability techniques, including SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), enabled us to illustrate significant feature-cancer stage interactions that would have remained hidden in traditional black-box models. We identified how certain demographic and clinical variables influenced survival differently across cancer stages and types. These insights provide not only transparency but also clinical relevance, supporting personalized treatment planning. By focusing on stage-specific models, this study provides new insights into the most important factors at each stage of cancer, offering transparency and potential clinical relevance to support personalized treatment planning.", "link": "http://arxiv.org/abs/2601.03977v1", "date": "2026-01-07", "relevancy": 1.2424, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4317}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4146}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stage-specific%20cancer%20survival%20prediction%20enriched%20by%20explainable%20machine%20learning&body=Title%3A%20Stage-specific%20cancer%20survival%20prediction%20enriched%20by%20explainable%20machine%20learning%0AAuthor%3A%20Parisa%20Poorhasani%20and%20Bogdan%20Iancu%0AAbstract%3A%20Despite%20the%20fact%20that%20cancer%20survivability%20rates%20vary%20greatly%20between%20stages%2C%20traditional%20survival%20prediction%20models%20have%20frequently%20been%20trained%20and%20assessed%20using%20examples%20from%20all%20combined%20phases%20of%20the%20disease.%20This%20method%20may%20result%20in%20an%20overestimation%20of%20performance%20and%20ignore%20the%20stage-specific%20variations.%20Using%20the%20SEER%20dataset%2C%20we%20created%20and%20verified%20explainable%20machine%20learning%20%28ML%29%20models%20to%20predict%20stage-specific%20cancer%20survivability%20in%20colorectal%2C%20stomach%2C%20and%20liver%20cancers.%20ML-based%20cancer%20survival%20analysis%20has%20been%20a%20long-standing%20topic%20in%20the%20literature%3B%20however%2C%20studies%20involving%20the%20explainability%20and%20transparency%20of%20ML%20survivability%20models%20are%20limited.%20Our%20use%20of%20explainability%20techniques%2C%20including%20SHapley%20Additive%20exPlanations%20%28SHAP%29%20and%20Local%20Interpretable%20Model-agnostic%20Explanations%20%28LIME%29%2C%20enabled%20us%20to%20illustrate%20significant%20feature-cancer%20stage%20interactions%20that%20would%20have%20remained%20hidden%20in%20traditional%20black-box%20models.%20We%20identified%20how%20certain%20demographic%20and%20clinical%20variables%20influenced%20survival%20differently%20across%20cancer%20stages%20and%20types.%20These%20insights%20provide%20not%20only%20transparency%20but%20also%20clinical%20relevance%2C%20supporting%20personalized%20treatment%20planning.%20By%20focusing%20on%20stage-specific%20models%2C%20this%20study%20provides%20new%20insights%20into%20the%20most%20important%20factors%20at%20each%20stage%20of%20cancer%2C%20offering%20transparency%20and%20potential%20clinical%20relevance%20to%20support%20personalized%20treatment%20planning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStage-specific%2520cancer%2520survival%2520prediction%2520enriched%2520by%2520explainable%2520machine%2520learning%26entry.906535625%3DParisa%2520Poorhasani%2520and%2520Bogdan%2520Iancu%26entry.1292438233%3DDespite%2520the%2520fact%2520that%2520cancer%2520survivability%2520rates%2520vary%2520greatly%2520between%2520stages%252C%2520traditional%2520survival%2520prediction%2520models%2520have%2520frequently%2520been%2520trained%2520and%2520assessed%2520using%2520examples%2520from%2520all%2520combined%2520phases%2520of%2520the%2520disease.%2520This%2520method%2520may%2520result%2520in%2520an%2520overestimation%2520of%2520performance%2520and%2520ignore%2520the%2520stage-specific%2520variations.%2520Using%2520the%2520SEER%2520dataset%252C%2520we%2520created%2520and%2520verified%2520explainable%2520machine%2520learning%2520%2528ML%2529%2520models%2520to%2520predict%2520stage-specific%2520cancer%2520survivability%2520in%2520colorectal%252C%2520stomach%252C%2520and%2520liver%2520cancers.%2520ML-based%2520cancer%2520survival%2520analysis%2520has%2520been%2520a%2520long-standing%2520topic%2520in%2520the%2520literature%253B%2520however%252C%2520studies%2520involving%2520the%2520explainability%2520and%2520transparency%2520of%2520ML%2520survivability%2520models%2520are%2520limited.%2520Our%2520use%2520of%2520explainability%2520techniques%252C%2520including%2520SHapley%2520Additive%2520exPlanations%2520%2528SHAP%2529%2520and%2520Local%2520Interpretable%2520Model-agnostic%2520Explanations%2520%2528LIME%2529%252C%2520enabled%2520us%2520to%2520illustrate%2520significant%2520feature-cancer%2520stage%2520interactions%2520that%2520would%2520have%2520remained%2520hidden%2520in%2520traditional%2520black-box%2520models.%2520We%2520identified%2520how%2520certain%2520demographic%2520and%2520clinical%2520variables%2520influenced%2520survival%2520differently%2520across%2520cancer%2520stages%2520and%2520types.%2520These%2520insights%2520provide%2520not%2520only%2520transparency%2520but%2520also%2520clinical%2520relevance%252C%2520supporting%2520personalized%2520treatment%2520planning.%2520By%2520focusing%2520on%2520stage-specific%2520models%252C%2520this%2520study%2520provides%2520new%2520insights%2520into%2520the%2520most%2520important%2520factors%2520at%2520each%2520stage%2520of%2520cancer%252C%2520offering%2520transparency%2520and%2520potential%2520clinical%2520relevance%2520to%2520support%2520personalized%2520treatment%2520planning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stage-specific%20cancer%20survival%20prediction%20enriched%20by%20explainable%20machine%20learning&entry.906535625=Parisa%20Poorhasani%20and%20Bogdan%20Iancu&entry.1292438233=Despite%20the%20fact%20that%20cancer%20survivability%20rates%20vary%20greatly%20between%20stages%2C%20traditional%20survival%20prediction%20models%20have%20frequently%20been%20trained%20and%20assessed%20using%20examples%20from%20all%20combined%20phases%20of%20the%20disease.%20This%20method%20may%20result%20in%20an%20overestimation%20of%20performance%20and%20ignore%20the%20stage-specific%20variations.%20Using%20the%20SEER%20dataset%2C%20we%20created%20and%20verified%20explainable%20machine%20learning%20%28ML%29%20models%20to%20predict%20stage-specific%20cancer%20survivability%20in%20colorectal%2C%20stomach%2C%20and%20liver%20cancers.%20ML-based%20cancer%20survival%20analysis%20has%20been%20a%20long-standing%20topic%20in%20the%20literature%3B%20however%2C%20studies%20involving%20the%20explainability%20and%20transparency%20of%20ML%20survivability%20models%20are%20limited.%20Our%20use%20of%20explainability%20techniques%2C%20including%20SHapley%20Additive%20exPlanations%20%28SHAP%29%20and%20Local%20Interpretable%20Model-agnostic%20Explanations%20%28LIME%29%2C%20enabled%20us%20to%20illustrate%20significant%20feature-cancer%20stage%20interactions%20that%20would%20have%20remained%20hidden%20in%20traditional%20black-box%20models.%20We%20identified%20how%20certain%20demographic%20and%20clinical%20variables%20influenced%20survival%20differently%20across%20cancer%20stages%20and%20types.%20These%20insights%20provide%20not%20only%20transparency%20but%20also%20clinical%20relevance%2C%20supporting%20personalized%20treatment%20planning.%20By%20focusing%20on%20stage-specific%20models%2C%20this%20study%20provides%20new%20insights%20into%20the%20most%20important%20factors%20at%20each%20stage%20of%20cancer%2C%20offering%20transparency%20and%20potential%20clinical%20relevance%20to%20support%20personalized%20treatment%20planning.&entry.1838667208=http%3A//arxiv.org/abs/2601.03977v1&entry.124074799=Read"},
{"title": "IndexTTS 2.5 Technical Report", "author": "Yunpei Li and Xun Zhou and Jinchao Wang and Lu Wang and Yong Wu and Siyi Zhou and Yiquan Zhou and Jingchen Shu", "abstract": "In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.", "link": "http://arxiv.org/abs/2601.03888v1", "date": "2026-01-07", "relevancy": 1.4449, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.491}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4808}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IndexTTS%202.5%20Technical%20Report&body=Title%3A%20IndexTTS%202.5%20Technical%20Report%0AAuthor%3A%20Yunpei%20Li%20and%20Xun%20Zhou%20and%20Jinchao%20Wang%20and%20Lu%20Wang%20and%20Yong%20Wu%20and%20Siyi%20Zhou%20and%20Yiquan%20Zhou%20and%20Jingchen%20Shu%0AAbstract%3A%20In%20prior%20work%2C%20we%20introduced%20IndexTTS%202%2C%20a%20zero-shot%20neural%20text-to-speech%20foundation%20model%20comprising%20two%20core%20components%3A%20a%20transformer-based%20Text-to-Semantic%20%28T2S%29%20module%20and%20a%20non-autoregressive%20Semantic-to-Mel%20%28S2M%29%20module%2C%20which%20together%20enable%20faithful%20emotion%20replication%20and%20establish%20the%20first%20autoregressive%20duration-controllable%20generative%20paradigm.%20Building%20upon%20this%2C%20we%20present%20IndexTTS%202.5%2C%20which%20significantly%20enhances%20multilingual%20coverage%2C%20inference%20speed%2C%20and%20overall%20synthesis%20quality%20through%20four%20key%20improvements%3A%201%29%20Semantic%20Codec%20Compression%3A%20we%20reduce%20the%20semantic%20codec%20frame%20rate%20from%2050%20Hz%20to%2025%20Hz%2C%20halving%20sequence%20length%20and%20substantially%20lowering%20both%20training%20and%20inference%20costs%3B%202%29%20Architectural%20Upgrade%3A%20we%20replace%20the%20U-DiT-based%20backbone%20of%20the%20S2M%20module%20with%20a%20more%20efficient%20Zipformer-based%20modeling%20architecture%2C%20achieving%20notable%20parameter%20reduction%20and%20faster%20mel-spectrogram%20generation%3B%203%29%20Multilingual%20Extension%3A%20We%20propose%20three%20explicit%20cross-lingual%20modeling%20strategies%2C%20boundary-aware%20alignment%2C%20token-level%20concatenation%2C%20and%20instruction-guided%20generation%2C%20establishing%20practical%20design%20principles%20for%20zero-shot%20multilingual%20emotional%20TTS%20that%20supports%20Chinese%2C%20English%2C%20Japanese%2C%20and%20Spanish%2C%20and%20enables%20robust%20emotion%20transfer%20even%20without%20target-language%20emotional%20training%20data%3B%204%29%20Reinforcement%20Learning%20Optimization%3A%20we%20apply%20GRPO%20in%20post-training%20of%20the%20T2S%20module%2C%20improving%20pronunciation%20accuracy%20and%20natrualness.%20Experiments%20show%20that%20IndexTTS%202.5%20not%20only%20supports%20broader%20language%20coverage%20but%20also%20replicates%20emotional%20prosody%20in%20unseen%20languages%20under%20the%20same%20zero-shot%20setting.%20IndexTTS%202.5%20achieves%20a%202.28%20times%20improvement%20in%20RTF%20while%20maintaining%20comparable%20WER%20and%20speaker%20similarity%20to%20IndexTTS%202.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndexTTS%25202.5%2520Technical%2520Report%26entry.906535625%3DYunpei%2520Li%2520and%2520Xun%2520Zhou%2520and%2520Jinchao%2520Wang%2520and%2520Lu%2520Wang%2520and%2520Yong%2520Wu%2520and%2520Siyi%2520Zhou%2520and%2520Yiquan%2520Zhou%2520and%2520Jingchen%2520Shu%26entry.1292438233%3DIn%2520prior%2520work%252C%2520we%2520introduced%2520IndexTTS%25202%252C%2520a%2520zero-shot%2520neural%2520text-to-speech%2520foundation%2520model%2520comprising%2520two%2520core%2520components%253A%2520a%2520transformer-based%2520Text-to-Semantic%2520%2528T2S%2529%2520module%2520and%2520a%2520non-autoregressive%2520Semantic-to-Mel%2520%2528S2M%2529%2520module%252C%2520which%2520together%2520enable%2520faithful%2520emotion%2520replication%2520and%2520establish%2520the%2520first%2520autoregressive%2520duration-controllable%2520generative%2520paradigm.%2520Building%2520upon%2520this%252C%2520we%2520present%2520IndexTTS%25202.5%252C%2520which%2520significantly%2520enhances%2520multilingual%2520coverage%252C%2520inference%2520speed%252C%2520and%2520overall%2520synthesis%2520quality%2520through%2520four%2520key%2520improvements%253A%25201%2529%2520Semantic%2520Codec%2520Compression%253A%2520we%2520reduce%2520the%2520semantic%2520codec%2520frame%2520rate%2520from%252050%2520Hz%2520to%252025%2520Hz%252C%2520halving%2520sequence%2520length%2520and%2520substantially%2520lowering%2520both%2520training%2520and%2520inference%2520costs%253B%25202%2529%2520Architectural%2520Upgrade%253A%2520we%2520replace%2520the%2520U-DiT-based%2520backbone%2520of%2520the%2520S2M%2520module%2520with%2520a%2520more%2520efficient%2520Zipformer-based%2520modeling%2520architecture%252C%2520achieving%2520notable%2520parameter%2520reduction%2520and%2520faster%2520mel-spectrogram%2520generation%253B%25203%2529%2520Multilingual%2520Extension%253A%2520We%2520propose%2520three%2520explicit%2520cross-lingual%2520modeling%2520strategies%252C%2520boundary-aware%2520alignment%252C%2520token-level%2520concatenation%252C%2520and%2520instruction-guided%2520generation%252C%2520establishing%2520practical%2520design%2520principles%2520for%2520zero-shot%2520multilingual%2520emotional%2520TTS%2520that%2520supports%2520Chinese%252C%2520English%252C%2520Japanese%252C%2520and%2520Spanish%252C%2520and%2520enables%2520robust%2520emotion%2520transfer%2520even%2520without%2520target-language%2520emotional%2520training%2520data%253B%25204%2529%2520Reinforcement%2520Learning%2520Optimization%253A%2520we%2520apply%2520GRPO%2520in%2520post-training%2520of%2520the%2520T2S%2520module%252C%2520improving%2520pronunciation%2520accuracy%2520and%2520natrualness.%2520Experiments%2520show%2520that%2520IndexTTS%25202.5%2520not%2520only%2520supports%2520broader%2520language%2520coverage%2520but%2520also%2520replicates%2520emotional%2520prosody%2520in%2520unseen%2520languages%2520under%2520the%2520same%2520zero-shot%2520setting.%2520IndexTTS%25202.5%2520achieves%2520a%25202.28%2520times%2520improvement%2520in%2520RTF%2520while%2520maintaining%2520comparable%2520WER%2520and%2520speaker%2520similarity%2520to%2520IndexTTS%25202.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IndexTTS%202.5%20Technical%20Report&entry.906535625=Yunpei%20Li%20and%20Xun%20Zhou%20and%20Jinchao%20Wang%20and%20Lu%20Wang%20and%20Yong%20Wu%20and%20Siyi%20Zhou%20and%20Yiquan%20Zhou%20and%20Jingchen%20Shu&entry.1292438233=In%20prior%20work%2C%20we%20introduced%20IndexTTS%202%2C%20a%20zero-shot%20neural%20text-to-speech%20foundation%20model%20comprising%20two%20core%20components%3A%20a%20transformer-based%20Text-to-Semantic%20%28T2S%29%20module%20and%20a%20non-autoregressive%20Semantic-to-Mel%20%28S2M%29%20module%2C%20which%20together%20enable%20faithful%20emotion%20replication%20and%20establish%20the%20first%20autoregressive%20duration-controllable%20generative%20paradigm.%20Building%20upon%20this%2C%20we%20present%20IndexTTS%202.5%2C%20which%20significantly%20enhances%20multilingual%20coverage%2C%20inference%20speed%2C%20and%20overall%20synthesis%20quality%20through%20four%20key%20improvements%3A%201%29%20Semantic%20Codec%20Compression%3A%20we%20reduce%20the%20semantic%20codec%20frame%20rate%20from%2050%20Hz%20to%2025%20Hz%2C%20halving%20sequence%20length%20and%20substantially%20lowering%20both%20training%20and%20inference%20costs%3B%202%29%20Architectural%20Upgrade%3A%20we%20replace%20the%20U-DiT-based%20backbone%20of%20the%20S2M%20module%20with%20a%20more%20efficient%20Zipformer-based%20modeling%20architecture%2C%20achieving%20notable%20parameter%20reduction%20and%20faster%20mel-spectrogram%20generation%3B%203%29%20Multilingual%20Extension%3A%20We%20propose%20three%20explicit%20cross-lingual%20modeling%20strategies%2C%20boundary-aware%20alignment%2C%20token-level%20concatenation%2C%20and%20instruction-guided%20generation%2C%20establishing%20practical%20design%20principles%20for%20zero-shot%20multilingual%20emotional%20TTS%20that%20supports%20Chinese%2C%20English%2C%20Japanese%2C%20and%20Spanish%2C%20and%20enables%20robust%20emotion%20transfer%20even%20without%20target-language%20emotional%20training%20data%3B%204%29%20Reinforcement%20Learning%20Optimization%3A%20we%20apply%20GRPO%20in%20post-training%20of%20the%20T2S%20module%2C%20improving%20pronunciation%20accuracy%20and%20natrualness.%20Experiments%20show%20that%20IndexTTS%202.5%20not%20only%20supports%20broader%20language%20coverage%20but%20also%20replicates%20emotional%20prosody%20in%20unseen%20languages%20under%20the%20same%20zero-shot%20setting.%20IndexTTS%202.5%20achieves%20a%202.28%20times%20improvement%20in%20RTF%20while%20maintaining%20comparable%20WER%20and%20speaker%20similarity%20to%20IndexTTS%202.&entry.1838667208=http%3A//arxiv.org/abs/2601.03888v1&entry.124074799=Read"},
{"title": "Modeling Behavioral Patterns in News Recommendations Using Fuzzy Neural Networks", "author": "Kevin Innerebner and Stephan Bartl and Markus Reiter-Haas and Elisabeth Lex", "abstract": "News recommender systems are increasingly driven by black-box models, offering little transparency for editorial decision-making. In this work, we introduce a transparent recommender system that uses fuzzy neural networks to learn human-readable rules from behavioral data for predicting article clicks. By extracting the rules at configurable thresholds, we can control rule complexity and thus, the level of interpretability. We evaluate our approach on two publicly available news datasets (i.e., MIND and EB-NeRD) and show that we can accurately predict click behavior compared to several established baselines, while learning human-readable rules. Furthermore, we show that the learned rules reveal news consumption patterns, enabling editors to align content curation goals with target audience behavior.", "link": "http://arxiv.org/abs/2601.04019v1", "date": "2026-01-07", "relevancy": 1.9776, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5003}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4905}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Behavioral%20Patterns%20in%20News%20Recommendations%20Using%20Fuzzy%20Neural%20Networks&body=Title%3A%20Modeling%20Behavioral%20Patterns%20in%20News%20Recommendations%20Using%20Fuzzy%20Neural%20Networks%0AAuthor%3A%20Kevin%20Innerebner%20and%20Stephan%20Bartl%20and%20Markus%20Reiter-Haas%20and%20Elisabeth%20Lex%0AAbstract%3A%20News%20recommender%20systems%20are%20increasingly%20driven%20by%20black-box%20models%2C%20offering%20little%20transparency%20for%20editorial%20decision-making.%20In%20this%20work%2C%20we%20introduce%20a%20transparent%20recommender%20system%20that%20uses%20fuzzy%20neural%20networks%20to%20learn%20human-readable%20rules%20from%20behavioral%20data%20for%20predicting%20article%20clicks.%20By%20extracting%20the%20rules%20at%20configurable%20thresholds%2C%20we%20can%20control%20rule%20complexity%20and%20thus%2C%20the%20level%20of%20interpretability.%20We%20evaluate%20our%20approach%20on%20two%20publicly%20available%20news%20datasets%20%28i.e.%2C%20MIND%20and%20EB-NeRD%29%20and%20show%20that%20we%20can%20accurately%20predict%20click%20behavior%20compared%20to%20several%20established%20baselines%2C%20while%20learning%20human-readable%20rules.%20Furthermore%2C%20we%20show%20that%20the%20learned%20rules%20reveal%20news%20consumption%20patterns%2C%20enabling%20editors%20to%20align%20content%20curation%20goals%20with%20target%20audience%20behavior.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Behavioral%2520Patterns%2520in%2520News%2520Recommendations%2520Using%2520Fuzzy%2520Neural%2520Networks%26entry.906535625%3DKevin%2520Innerebner%2520and%2520Stephan%2520Bartl%2520and%2520Markus%2520Reiter-Haas%2520and%2520Elisabeth%2520Lex%26entry.1292438233%3DNews%2520recommender%2520systems%2520are%2520increasingly%2520driven%2520by%2520black-box%2520models%252C%2520offering%2520little%2520transparency%2520for%2520editorial%2520decision-making.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520transparent%2520recommender%2520system%2520that%2520uses%2520fuzzy%2520neural%2520networks%2520to%2520learn%2520human-readable%2520rules%2520from%2520behavioral%2520data%2520for%2520predicting%2520article%2520clicks.%2520By%2520extracting%2520the%2520rules%2520at%2520configurable%2520thresholds%252C%2520we%2520can%2520control%2520rule%2520complexity%2520and%2520thus%252C%2520the%2520level%2520of%2520interpretability.%2520We%2520evaluate%2520our%2520approach%2520on%2520two%2520publicly%2520available%2520news%2520datasets%2520%2528i.e.%252C%2520MIND%2520and%2520EB-NeRD%2529%2520and%2520show%2520that%2520we%2520can%2520accurately%2520predict%2520click%2520behavior%2520compared%2520to%2520several%2520established%2520baselines%252C%2520while%2520learning%2520human-readable%2520rules.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520learned%2520rules%2520reveal%2520news%2520consumption%2520patterns%252C%2520enabling%2520editors%2520to%2520align%2520content%2520curation%2520goals%2520with%2520target%2520audience%2520behavior.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Behavioral%20Patterns%20in%20News%20Recommendations%20Using%20Fuzzy%20Neural%20Networks&entry.906535625=Kevin%20Innerebner%20and%20Stephan%20Bartl%20and%20Markus%20Reiter-Haas%20and%20Elisabeth%20Lex&entry.1292438233=News%20recommender%20systems%20are%20increasingly%20driven%20by%20black-box%20models%2C%20offering%20little%20transparency%20for%20editorial%20decision-making.%20In%20this%20work%2C%20we%20introduce%20a%20transparent%20recommender%20system%20that%20uses%20fuzzy%20neural%20networks%20to%20learn%20human-readable%20rules%20from%20behavioral%20data%20for%20predicting%20article%20clicks.%20By%20extracting%20the%20rules%20at%20configurable%20thresholds%2C%20we%20can%20control%20rule%20complexity%20and%20thus%2C%20the%20level%20of%20interpretability.%20We%20evaluate%20our%20approach%20on%20two%20publicly%20available%20news%20datasets%20%28i.e.%2C%20MIND%20and%20EB-NeRD%29%20and%20show%20that%20we%20can%20accurately%20predict%20click%20behavior%20compared%20to%20several%20established%20baselines%2C%20while%20learning%20human-readable%20rules.%20Furthermore%2C%20we%20show%20that%20the%20learned%20rules%20reveal%20news%20consumption%20patterns%2C%20enabling%20editors%20to%20align%20content%20curation%20goals%20with%20target%20audience%20behavior.&entry.1838667208=http%3A//arxiv.org/abs/2601.04019v1&entry.124074799=Read"},
{"title": "Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition", "author": "Nia Touko and Matthew O A Ellis and Cristiano Capone and Alessio Burrello and Elisa Donati and Luca Manneschi", "abstract": "Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We propose a lightweight framework for Test-Time Adaptation (TTA) using a Temporal Convolutional Network (TCN) backbone. We introduce three deployment-ready strategies: (i) causal adaptive batch normalization for real-time statistical alignment; (ii) a Gaussian Mixture Model (GMM) alignment with experience replay to prevent forgetting; and (iii) meta-learning for rapid, few-shot calibration. Evaluated on the NinaPro DB6 multi-session dataset, our framework significantly bridges the inter-session accuracy gap with minimal overhead. Our results show that experience-replay updates yield superior stability under limited data, while meta-learning achieves competitive performance in one- and two-shot regimes using only a fraction of the data required by current benchmarks. This work establishes a path toward robust, \"plug-and-play\" myoelectric control for long-term prosthetic use.", "link": "http://arxiv.org/abs/2601.04181v1", "date": "2026-01-07", "relevancy": 1.5829, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Test-Time%20Adaptation%20for%20EMG-Based%20Gesture%20Recognition&body=Title%3A%20Lightweight%20Test-Time%20Adaptation%20for%20EMG-Based%20Gesture%20Recognition%0AAuthor%3A%20Nia%20Touko%20and%20Matthew%20O%20A%20Ellis%20and%20Cristiano%20Capone%20and%20Alessio%20Burrello%20and%20Elisa%20Donati%20and%20Luca%20Manneschi%0AAbstract%3A%20Reliable%20long-term%20decoding%20of%20surface%20electromyography%20%28EMG%29%20is%20hindered%20by%20signal%20drift%20caused%20by%20electrode%20shifts%2C%20muscle%20fatigue%2C%20and%20posture%20changes.%20While%20state-of-the-art%20models%20achieve%20high%20intra-session%20accuracy%2C%20their%20performance%20often%20degrades%20sharply.%20Existing%20solutions%20typically%20demand%20large%20datasets%20or%20high-compute%20pipelines%20that%20are%20impractical%20for%20energy-efficient%20wearables.%20We%20propose%20a%20lightweight%20framework%20for%20Test-Time%20Adaptation%20%28TTA%29%20using%20a%20Temporal%20Convolutional%20Network%20%28TCN%29%20backbone.%20We%20introduce%20three%20deployment-ready%20strategies%3A%20%28i%29%20causal%20adaptive%20batch%20normalization%20for%20real-time%20statistical%20alignment%3B%20%28ii%29%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20alignment%20with%20experience%20replay%20to%20prevent%20forgetting%3B%20and%20%28iii%29%20meta-learning%20for%20rapid%2C%20few-shot%20calibration.%20Evaluated%20on%20the%20NinaPro%20DB6%20multi-session%20dataset%2C%20our%20framework%20significantly%20bridges%20the%20inter-session%20accuracy%20gap%20with%20minimal%20overhead.%20Our%20results%20show%20that%20experience-replay%20updates%20yield%20superior%20stability%20under%20limited%20data%2C%20while%20meta-learning%20achieves%20competitive%20performance%20in%20one-%20and%20two-shot%20regimes%20using%20only%20a%20fraction%20of%20the%20data%20required%20by%20current%20benchmarks.%20This%20work%20establishes%20a%20path%20toward%20robust%2C%20%22plug-and-play%22%20myoelectric%20control%20for%20long-term%20prosthetic%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Test-Time%2520Adaptation%2520for%2520EMG-Based%2520Gesture%2520Recognition%26entry.906535625%3DNia%2520Touko%2520and%2520Matthew%2520O%2520A%2520Ellis%2520and%2520Cristiano%2520Capone%2520and%2520Alessio%2520Burrello%2520and%2520Elisa%2520Donati%2520and%2520Luca%2520Manneschi%26entry.1292438233%3DReliable%2520long-term%2520decoding%2520of%2520surface%2520electromyography%2520%2528EMG%2529%2520is%2520hindered%2520by%2520signal%2520drift%2520caused%2520by%2520electrode%2520shifts%252C%2520muscle%2520fatigue%252C%2520and%2520posture%2520changes.%2520While%2520state-of-the-art%2520models%2520achieve%2520high%2520intra-session%2520accuracy%252C%2520their%2520performance%2520often%2520degrades%2520sharply.%2520Existing%2520solutions%2520typically%2520demand%2520large%2520datasets%2520or%2520high-compute%2520pipelines%2520that%2520are%2520impractical%2520for%2520energy-efficient%2520wearables.%2520We%2520propose%2520a%2520lightweight%2520framework%2520for%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520using%2520a%2520Temporal%2520Convolutional%2520Network%2520%2528TCN%2529%2520backbone.%2520We%2520introduce%2520three%2520deployment-ready%2520strategies%253A%2520%2528i%2529%2520causal%2520adaptive%2520batch%2520normalization%2520for%2520real-time%2520statistical%2520alignment%253B%2520%2528ii%2529%2520a%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520alignment%2520with%2520experience%2520replay%2520to%2520prevent%2520forgetting%253B%2520and%2520%2528iii%2529%2520meta-learning%2520for%2520rapid%252C%2520few-shot%2520calibration.%2520Evaluated%2520on%2520the%2520NinaPro%2520DB6%2520multi-session%2520dataset%252C%2520our%2520framework%2520significantly%2520bridges%2520the%2520inter-session%2520accuracy%2520gap%2520with%2520minimal%2520overhead.%2520Our%2520results%2520show%2520that%2520experience-replay%2520updates%2520yield%2520superior%2520stability%2520under%2520limited%2520data%252C%2520while%2520meta-learning%2520achieves%2520competitive%2520performance%2520in%2520one-%2520and%2520two-shot%2520regimes%2520using%2520only%2520a%2520fraction%2520of%2520the%2520data%2520required%2520by%2520current%2520benchmarks.%2520This%2520work%2520establishes%2520a%2520path%2520toward%2520robust%252C%2520%2522plug-and-play%2522%2520myoelectric%2520control%2520for%2520long-term%2520prosthetic%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Test-Time%20Adaptation%20for%20EMG-Based%20Gesture%20Recognition&entry.906535625=Nia%20Touko%20and%20Matthew%20O%20A%20Ellis%20and%20Cristiano%20Capone%20and%20Alessio%20Burrello%20and%20Elisa%20Donati%20and%20Luca%20Manneschi&entry.1292438233=Reliable%20long-term%20decoding%20of%20surface%20electromyography%20%28EMG%29%20is%20hindered%20by%20signal%20drift%20caused%20by%20electrode%20shifts%2C%20muscle%20fatigue%2C%20and%20posture%20changes.%20While%20state-of-the-art%20models%20achieve%20high%20intra-session%20accuracy%2C%20their%20performance%20often%20degrades%20sharply.%20Existing%20solutions%20typically%20demand%20large%20datasets%20or%20high-compute%20pipelines%20that%20are%20impractical%20for%20energy-efficient%20wearables.%20We%20propose%20a%20lightweight%20framework%20for%20Test-Time%20Adaptation%20%28TTA%29%20using%20a%20Temporal%20Convolutional%20Network%20%28TCN%29%20backbone.%20We%20introduce%20three%20deployment-ready%20strategies%3A%20%28i%29%20causal%20adaptive%20batch%20normalization%20for%20real-time%20statistical%20alignment%3B%20%28ii%29%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20alignment%20with%20experience%20replay%20to%20prevent%20forgetting%3B%20and%20%28iii%29%20meta-learning%20for%20rapid%2C%20few-shot%20calibration.%20Evaluated%20on%20the%20NinaPro%20DB6%20multi-session%20dataset%2C%20our%20framework%20significantly%20bridges%20the%20inter-session%20accuracy%20gap%20with%20minimal%20overhead.%20Our%20results%20show%20that%20experience-replay%20updates%20yield%20superior%20stability%20under%20limited%20data%2C%20while%20meta-learning%20achieves%20competitive%20performance%20in%20one-%20and%20two-shot%20regimes%20using%20only%20a%20fraction%20of%20the%20data%20required%20by%20current%20benchmarks.%20This%20work%20establishes%20a%20path%20toward%20robust%2C%20%22plug-and-play%22%20myoelectric%20control%20for%20long-term%20prosthetic%20use.&entry.1838667208=http%3A//arxiv.org/abs/2601.04181v1&entry.124074799=Read"},
{"title": "DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems", "author": "Shuyu Zhang and Yifan Wei and Jialuo Yuan and Xinru Wang and Yanmin Zhu and Bin Li and Yujie Liu", "abstract": "Task oriented dialog systems often rely on static exploration strategies that do not adapt to dynamic dialog contexts, leading to inefficient exploration and suboptimal performance. We propose DyBBT, a novel dialog policy learning framework that formalizes the exploration challenge through a structured cognitive state space capturing dialog progression, user uncertainty, and slot dependency. DyBBT proposes a bandit inspired meta-controller that dynamically switches between a fast intuitive inference (System 1) and a slow deliberative reasoner (System 2) based on real-time cognitive states and visitation counts. Extensive experiments on single- and multi-domain benchmarks show that DyBBT achieves state-of-the-art performance in success rate, efficiency, and generalization, with human evaluations confirming its decisions are well aligned with expert judgment. Code is available at https://github.com/carsonz/DyBBT.", "link": "http://arxiv.org/abs/2509.19695v2", "date": "2026-01-07", "relevancy": 1.5224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5363}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5112}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyBBT%3A%20Dynamic%20Balance%20via%20Bandit%20inspired%20Targeting%20for%20Dialog%20Policy%20with%20Cognitive%20Dual-Systems&body=Title%3A%20DyBBT%3A%20Dynamic%20Balance%20via%20Bandit%20inspired%20Targeting%20for%20Dialog%20Policy%20with%20Cognitive%20Dual-Systems%0AAuthor%3A%20Shuyu%20Zhang%20and%20Yifan%20Wei%20and%20Jialuo%20Yuan%20and%20Xinru%20Wang%20and%20Yanmin%20Zhu%20and%20Bin%20Li%20and%20Yujie%20Liu%0AAbstract%3A%20Task%20oriented%20dialog%20systems%20often%20rely%20on%20static%20exploration%20strategies%20that%20do%20not%20adapt%20to%20dynamic%20dialog%20contexts%2C%20leading%20to%20inefficient%20exploration%20and%20suboptimal%20performance.%20We%20propose%20DyBBT%2C%20a%20novel%20dialog%20policy%20learning%20framework%20that%20formalizes%20the%20exploration%20challenge%20through%20a%20structured%20cognitive%20state%20space%20capturing%20dialog%20progression%2C%20user%20uncertainty%2C%20and%20slot%20dependency.%20DyBBT%20proposes%20a%20bandit%20inspired%20meta-controller%20that%20dynamically%20switches%20between%20a%20fast%20intuitive%20inference%20%28System%201%29%20and%20a%20slow%20deliberative%20reasoner%20%28System%202%29%20based%20on%20real-time%20cognitive%20states%20and%20visitation%20counts.%20Extensive%20experiments%20on%20single-%20and%20multi-domain%20benchmarks%20show%20that%20DyBBT%20achieves%20state-of-the-art%20performance%20in%20success%20rate%2C%20efficiency%2C%20and%20generalization%2C%20with%20human%20evaluations%20confirming%20its%20decisions%20are%20well%20aligned%20with%20expert%20judgment.%20Code%20is%20available%20at%20https%3A//github.com/carsonz/DyBBT.%0ALink%3A%20http%3A//arxiv.org/abs/2509.19695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyBBT%253A%2520Dynamic%2520Balance%2520via%2520Bandit%2520inspired%2520Targeting%2520for%2520Dialog%2520Policy%2520with%2520Cognitive%2520Dual-Systems%26entry.906535625%3DShuyu%2520Zhang%2520and%2520Yifan%2520Wei%2520and%2520Jialuo%2520Yuan%2520and%2520Xinru%2520Wang%2520and%2520Yanmin%2520Zhu%2520and%2520Bin%2520Li%2520and%2520Yujie%2520Liu%26entry.1292438233%3DTask%2520oriented%2520dialog%2520systems%2520often%2520rely%2520on%2520static%2520exploration%2520strategies%2520that%2520do%2520not%2520adapt%2520to%2520dynamic%2520dialog%2520contexts%252C%2520leading%2520to%2520inefficient%2520exploration%2520and%2520suboptimal%2520performance.%2520We%2520propose%2520DyBBT%252C%2520a%2520novel%2520dialog%2520policy%2520learning%2520framework%2520that%2520formalizes%2520the%2520exploration%2520challenge%2520through%2520a%2520structured%2520cognitive%2520state%2520space%2520capturing%2520dialog%2520progression%252C%2520user%2520uncertainty%252C%2520and%2520slot%2520dependency.%2520DyBBT%2520proposes%2520a%2520bandit%2520inspired%2520meta-controller%2520that%2520dynamically%2520switches%2520between%2520a%2520fast%2520intuitive%2520inference%2520%2528System%25201%2529%2520and%2520a%2520slow%2520deliberative%2520reasoner%2520%2528System%25202%2529%2520based%2520on%2520real-time%2520cognitive%2520states%2520and%2520visitation%2520counts.%2520Extensive%2520experiments%2520on%2520single-%2520and%2520multi-domain%2520benchmarks%2520show%2520that%2520DyBBT%2520achieves%2520state-of-the-art%2520performance%2520in%2520success%2520rate%252C%2520efficiency%252C%2520and%2520generalization%252C%2520with%2520human%2520evaluations%2520confirming%2520its%2520decisions%2520are%2520well%2520aligned%2520with%2520expert%2520judgment.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/carsonz/DyBBT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyBBT%3A%20Dynamic%20Balance%20via%20Bandit%20inspired%20Targeting%20for%20Dialog%20Policy%20with%20Cognitive%20Dual-Systems&entry.906535625=Shuyu%20Zhang%20and%20Yifan%20Wei%20and%20Jialuo%20Yuan%20and%20Xinru%20Wang%20and%20Yanmin%20Zhu%20and%20Bin%20Li%20and%20Yujie%20Liu&entry.1292438233=Task%20oriented%20dialog%20systems%20often%20rely%20on%20static%20exploration%20strategies%20that%20do%20not%20adapt%20to%20dynamic%20dialog%20contexts%2C%20leading%20to%20inefficient%20exploration%20and%20suboptimal%20performance.%20We%20propose%20DyBBT%2C%20a%20novel%20dialog%20policy%20learning%20framework%20that%20formalizes%20the%20exploration%20challenge%20through%20a%20structured%20cognitive%20state%20space%20capturing%20dialog%20progression%2C%20user%20uncertainty%2C%20and%20slot%20dependency.%20DyBBT%20proposes%20a%20bandit%20inspired%20meta-controller%20that%20dynamically%20switches%20between%20a%20fast%20intuitive%20inference%20%28System%201%29%20and%20a%20slow%20deliberative%20reasoner%20%28System%202%29%20based%20on%20real-time%20cognitive%20states%20and%20visitation%20counts.%20Extensive%20experiments%20on%20single-%20and%20multi-domain%20benchmarks%20show%20that%20DyBBT%20achieves%20state-of-the-art%20performance%20in%20success%20rate%2C%20efficiency%2C%20and%20generalization%2C%20with%20human%20evaluations%20confirming%20its%20decisions%20are%20well%20aligned%20with%20expert%20judgment.%20Code%20is%20available%20at%20https%3A//github.com/carsonz/DyBBT.&entry.1838667208=http%3A//arxiv.org/abs/2509.19695v2&entry.124074799=Read"},
{"title": "Detecting Semantic Backdoors in a Mystery Shopping Scenario", "author": "Arpad Berta and Gabor Danner and Istvan Hegedus and Mark Jelasity", "abstract": "Detecting semantic backdoors in classification models--where some classes can be activated by certain natural, but out-of-distribution inputs--is an important problem that has received relatively little attention. Semantic backdoors are significantly harder to detect than backdoors that are based on trigger patterns due to the lack of such clearly identifiable patterns. We tackle this problem under the assumption that the clean training dataset and the training recipe of the model are both known. These assumptions are motivated by a consumer protection scenario, in which the responsible authority performs mystery shopping to test a machine learning service provider. In this scenario, the authority uses the provider's resources and tools to train a model on a given dataset and tests whether the provider included a backdoor. In our proposed approach, the authority creates a reference model pool by training a small number of clean and poisoned models using trusted infrastructure, and calibrates a model distance threshold to identify clean models. We propose and experimentally analyze a number of approaches to compute model distances and we also test a scenario where the provider performs an adaptive attack to avoid detection. The most reliable method is based on requesting adversarial training from the provider. The model distance is best measured using a set of input samples generated by inverting the models in such a way as to maximize the distance from clean samples. With these settings, our method can often completely separate clean and poisoned models, and it proves to be superior to state-of-the-art backdoor detectors as well.", "link": "http://arxiv.org/abs/2601.03805v1", "date": "2026-01-07", "relevancy": 1.8836, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4699}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Semantic%20Backdoors%20in%20a%20Mystery%20Shopping%20Scenario&body=Title%3A%20Detecting%20Semantic%20Backdoors%20in%20a%20Mystery%20Shopping%20Scenario%0AAuthor%3A%20Arpad%20Berta%20and%20Gabor%20Danner%20and%20Istvan%20Hegedus%20and%20Mark%20Jelasity%0AAbstract%3A%20Detecting%20semantic%20backdoors%20in%20classification%20models--where%20some%20classes%20can%20be%20activated%20by%20certain%20natural%2C%20but%20out-of-distribution%20inputs--is%20an%20important%20problem%20that%20has%20received%20relatively%20little%20attention.%20Semantic%20backdoors%20are%20significantly%20harder%20to%20detect%20than%20backdoors%20that%20are%20based%20on%20trigger%20patterns%20due%20to%20the%20lack%20of%20such%20clearly%20identifiable%20patterns.%20We%20tackle%20this%20problem%20under%20the%20assumption%20that%20the%20clean%20training%20dataset%20and%20the%20training%20recipe%20of%20the%20model%20are%20both%20known.%20These%20assumptions%20are%20motivated%20by%20a%20consumer%20protection%20scenario%2C%20in%20which%20the%20responsible%20authority%20performs%20mystery%20shopping%20to%20test%20a%20machine%20learning%20service%20provider.%20In%20this%20scenario%2C%20the%20authority%20uses%20the%20provider%27s%20resources%20and%20tools%20to%20train%20a%20model%20on%20a%20given%20dataset%20and%20tests%20whether%20the%20provider%20included%20a%20backdoor.%20In%20our%20proposed%20approach%2C%20the%20authority%20creates%20a%20reference%20model%20pool%20by%20training%20a%20small%20number%20of%20clean%20and%20poisoned%20models%20using%20trusted%20infrastructure%2C%20and%20calibrates%20a%20model%20distance%20threshold%20to%20identify%20clean%20models.%20We%20propose%20and%20experimentally%20analyze%20a%20number%20of%20approaches%20to%20compute%20model%20distances%20and%20we%20also%20test%20a%20scenario%20where%20the%20provider%20performs%20an%20adaptive%20attack%20to%20avoid%20detection.%20The%20most%20reliable%20method%20is%20based%20on%20requesting%20adversarial%20training%20from%20the%20provider.%20The%20model%20distance%20is%20best%20measured%20using%20a%20set%20of%20input%20samples%20generated%20by%20inverting%20the%20models%20in%20such%20a%20way%20as%20to%20maximize%20the%20distance%20from%20clean%20samples.%20With%20these%20settings%2C%20our%20method%20can%20often%20completely%20separate%20clean%20and%20poisoned%20models%2C%20and%20it%20proves%20to%20be%20superior%20to%20state-of-the-art%20backdoor%20detectors%20as%20well.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Semantic%2520Backdoors%2520in%2520a%2520Mystery%2520Shopping%2520Scenario%26entry.906535625%3DArpad%2520Berta%2520and%2520Gabor%2520Danner%2520and%2520Istvan%2520Hegedus%2520and%2520Mark%2520Jelasity%26entry.1292438233%3DDetecting%2520semantic%2520backdoors%2520in%2520classification%2520models--where%2520some%2520classes%2520can%2520be%2520activated%2520by%2520certain%2520natural%252C%2520but%2520out-of-distribution%2520inputs--is%2520an%2520important%2520problem%2520that%2520has%2520received%2520relatively%2520little%2520attention.%2520Semantic%2520backdoors%2520are%2520significantly%2520harder%2520to%2520detect%2520than%2520backdoors%2520that%2520are%2520based%2520on%2520trigger%2520patterns%2520due%2520to%2520the%2520lack%2520of%2520such%2520clearly%2520identifiable%2520patterns.%2520We%2520tackle%2520this%2520problem%2520under%2520the%2520assumption%2520that%2520the%2520clean%2520training%2520dataset%2520and%2520the%2520training%2520recipe%2520of%2520the%2520model%2520are%2520both%2520known.%2520These%2520assumptions%2520are%2520motivated%2520by%2520a%2520consumer%2520protection%2520scenario%252C%2520in%2520which%2520the%2520responsible%2520authority%2520performs%2520mystery%2520shopping%2520to%2520test%2520a%2520machine%2520learning%2520service%2520provider.%2520In%2520this%2520scenario%252C%2520the%2520authority%2520uses%2520the%2520provider%2527s%2520resources%2520and%2520tools%2520to%2520train%2520a%2520model%2520on%2520a%2520given%2520dataset%2520and%2520tests%2520whether%2520the%2520provider%2520included%2520a%2520backdoor.%2520In%2520our%2520proposed%2520approach%252C%2520the%2520authority%2520creates%2520a%2520reference%2520model%2520pool%2520by%2520training%2520a%2520small%2520number%2520of%2520clean%2520and%2520poisoned%2520models%2520using%2520trusted%2520infrastructure%252C%2520and%2520calibrates%2520a%2520model%2520distance%2520threshold%2520to%2520identify%2520clean%2520models.%2520We%2520propose%2520and%2520experimentally%2520analyze%2520a%2520number%2520of%2520approaches%2520to%2520compute%2520model%2520distances%2520and%2520we%2520also%2520test%2520a%2520scenario%2520where%2520the%2520provider%2520performs%2520an%2520adaptive%2520attack%2520to%2520avoid%2520detection.%2520The%2520most%2520reliable%2520method%2520is%2520based%2520on%2520requesting%2520adversarial%2520training%2520from%2520the%2520provider.%2520The%2520model%2520distance%2520is%2520best%2520measured%2520using%2520a%2520set%2520of%2520input%2520samples%2520generated%2520by%2520inverting%2520the%2520models%2520in%2520such%2520a%2520way%2520as%2520to%2520maximize%2520the%2520distance%2520from%2520clean%2520samples.%2520With%2520these%2520settings%252C%2520our%2520method%2520can%2520often%2520completely%2520separate%2520clean%2520and%2520poisoned%2520models%252C%2520and%2520it%2520proves%2520to%2520be%2520superior%2520to%2520state-of-the-art%2520backdoor%2520detectors%2520as%2520well.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Semantic%20Backdoors%20in%20a%20Mystery%20Shopping%20Scenario&entry.906535625=Arpad%20Berta%20and%20Gabor%20Danner%20and%20Istvan%20Hegedus%20and%20Mark%20Jelasity&entry.1292438233=Detecting%20semantic%20backdoors%20in%20classification%20models--where%20some%20classes%20can%20be%20activated%20by%20certain%20natural%2C%20but%20out-of-distribution%20inputs--is%20an%20important%20problem%20that%20has%20received%20relatively%20little%20attention.%20Semantic%20backdoors%20are%20significantly%20harder%20to%20detect%20than%20backdoors%20that%20are%20based%20on%20trigger%20patterns%20due%20to%20the%20lack%20of%20such%20clearly%20identifiable%20patterns.%20We%20tackle%20this%20problem%20under%20the%20assumption%20that%20the%20clean%20training%20dataset%20and%20the%20training%20recipe%20of%20the%20model%20are%20both%20known.%20These%20assumptions%20are%20motivated%20by%20a%20consumer%20protection%20scenario%2C%20in%20which%20the%20responsible%20authority%20performs%20mystery%20shopping%20to%20test%20a%20machine%20learning%20service%20provider.%20In%20this%20scenario%2C%20the%20authority%20uses%20the%20provider%27s%20resources%20and%20tools%20to%20train%20a%20model%20on%20a%20given%20dataset%20and%20tests%20whether%20the%20provider%20included%20a%20backdoor.%20In%20our%20proposed%20approach%2C%20the%20authority%20creates%20a%20reference%20model%20pool%20by%20training%20a%20small%20number%20of%20clean%20and%20poisoned%20models%20using%20trusted%20infrastructure%2C%20and%20calibrates%20a%20model%20distance%20threshold%20to%20identify%20clean%20models.%20We%20propose%20and%20experimentally%20analyze%20a%20number%20of%20approaches%20to%20compute%20model%20distances%20and%20we%20also%20test%20a%20scenario%20where%20the%20provider%20performs%20an%20adaptive%20attack%20to%20avoid%20detection.%20The%20most%20reliable%20method%20is%20based%20on%20requesting%20adversarial%20training%20from%20the%20provider.%20The%20model%20distance%20is%20best%20measured%20using%20a%20set%20of%20input%20samples%20generated%20by%20inverting%20the%20models%20in%20such%20a%20way%20as%20to%20maximize%20the%20distance%20from%20clean%20samples.%20With%20these%20settings%2C%20our%20method%20can%20often%20completely%20separate%20clean%20and%20poisoned%20models%2C%20and%20it%20proves%20to%20be%20superior%20to%20state-of-the-art%20backdoor%20detectors%20as%20well.&entry.1838667208=http%3A//arxiv.org/abs/2601.03805v1&entry.124074799=Read"},
{"title": "An ASP-Based Framework for MUSes", "author": "Mohimenul Kabir and Kuldeep S Meel", "abstract": "Given an unsatisfiable formula, understanding the core reason for unsatisfiability is crucial in several applications. One effective way to capture this is through the minimal unsatisfiable subset (MUS), the subset-minimal set of clauses that remains unsatisfiable. Current research broadly focuses on two directions: (i) enumerating as many MUSes as possible within a given time limit, and (ii) counting the total number of MUSes for a given unsatisfiable formula.\n  In this paper, we introduce an answer set programming-based framework, named MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for its strengths in knowledge representation and is particularly suitable for specifying complex combinatorial problems. By translating MUS enumeration into answer set solving, MUS-ASP leverages the computational efficiency of state-of-the-art ASP systems. Our extensive experimental evaluation demonstrates the effectiveness of MUS-ASP and highlights the acceleration in both MUS enumeration and counting tasks, particularly when integrated within hybrid solvers, including the framework proposed in this paper.", "link": "http://arxiv.org/abs/2507.03929v2", "date": "2026-01-07", "relevancy": 1.5659, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4156}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20ASP-Based%20Framework%20for%20MUSes&body=Title%3A%20An%20ASP-Based%20Framework%20for%20MUSes%0AAuthor%3A%20Mohimenul%20Kabir%20and%20Kuldeep%20S%20Meel%0AAbstract%3A%20Given%20an%20unsatisfiable%20formula%2C%20understanding%20the%20core%20reason%20for%20unsatisfiability%20is%20crucial%20in%20several%20applications.%20One%20effective%20way%20to%20capture%20this%20is%20through%20the%20minimal%20unsatisfiable%20subset%20%28MUS%29%2C%20the%20subset-minimal%20set%20of%20clauses%20that%20remains%20unsatisfiable.%20Current%20research%20broadly%20focuses%20on%20two%20directions%3A%20%28i%29%20enumerating%20as%20many%20MUSes%20as%20possible%20within%20a%20given%20time%20limit%2C%20and%20%28ii%29%20counting%20the%20total%20number%20of%20MUSes%20for%20a%20given%20unsatisfiable%20formula.%0A%20%20In%20this%20paper%2C%20we%20introduce%20an%20answer%20set%20programming-based%20framework%2C%20named%20MUS-ASP%2C%20designed%20for%20online%20enumeration%20of%20MUSes.%20ASP%20is%20a%20powerful%20tool%20for%20its%20strengths%20in%20knowledge%20representation%20and%20is%20particularly%20suitable%20for%20specifying%20complex%20combinatorial%20problems.%20By%20translating%20MUS%20enumeration%20into%20answer%20set%20solving%2C%20MUS-ASP%20leverages%20the%20computational%20efficiency%20of%20state-of-the-art%20ASP%20systems.%20Our%20extensive%20experimental%20evaluation%20demonstrates%20the%20effectiveness%20of%20MUS-ASP%20and%20highlights%20the%20acceleration%20in%20both%20MUS%20enumeration%20and%20counting%20tasks%2C%20particularly%20when%20integrated%20within%20hybrid%20solvers%2C%20including%20the%20framework%20proposed%20in%20this%20paper.%0ALink%3A%20http%3A//arxiv.org/abs/2507.03929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520ASP-Based%2520Framework%2520for%2520MUSes%26entry.906535625%3DMohimenul%2520Kabir%2520and%2520Kuldeep%2520S%2520Meel%26entry.1292438233%3DGiven%2520an%2520unsatisfiable%2520formula%252C%2520understanding%2520the%2520core%2520reason%2520for%2520unsatisfiability%2520is%2520crucial%2520in%2520several%2520applications.%2520One%2520effective%2520way%2520to%2520capture%2520this%2520is%2520through%2520the%2520minimal%2520unsatisfiable%2520subset%2520%2528MUS%2529%252C%2520the%2520subset-minimal%2520set%2520of%2520clauses%2520that%2520remains%2520unsatisfiable.%2520Current%2520research%2520broadly%2520focuses%2520on%2520two%2520directions%253A%2520%2528i%2529%2520enumerating%2520as%2520many%2520MUSes%2520as%2520possible%2520within%2520a%2520given%2520time%2520limit%252C%2520and%2520%2528ii%2529%2520counting%2520the%2520total%2520number%2520of%2520MUSes%2520for%2520a%2520given%2520unsatisfiable%2520formula.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520answer%2520set%2520programming-based%2520framework%252C%2520named%2520MUS-ASP%252C%2520designed%2520for%2520online%2520enumeration%2520of%2520MUSes.%2520ASP%2520is%2520a%2520powerful%2520tool%2520for%2520its%2520strengths%2520in%2520knowledge%2520representation%2520and%2520is%2520particularly%2520suitable%2520for%2520specifying%2520complex%2520combinatorial%2520problems.%2520By%2520translating%2520MUS%2520enumeration%2520into%2520answer%2520set%2520solving%252C%2520MUS-ASP%2520leverages%2520the%2520computational%2520efficiency%2520of%2520state-of-the-art%2520ASP%2520systems.%2520Our%2520extensive%2520experimental%2520evaluation%2520demonstrates%2520the%2520effectiveness%2520of%2520MUS-ASP%2520and%2520highlights%2520the%2520acceleration%2520in%2520both%2520MUS%2520enumeration%2520and%2520counting%2520tasks%252C%2520particularly%2520when%2520integrated%2520within%2520hybrid%2520solvers%252C%2520including%2520the%2520framework%2520proposed%2520in%2520this%2520paper.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20ASP-Based%20Framework%20for%20MUSes&entry.906535625=Mohimenul%20Kabir%20and%20Kuldeep%20S%20Meel&entry.1292438233=Given%20an%20unsatisfiable%20formula%2C%20understanding%20the%20core%20reason%20for%20unsatisfiability%20is%20crucial%20in%20several%20applications.%20One%20effective%20way%20to%20capture%20this%20is%20through%20the%20minimal%20unsatisfiable%20subset%20%28MUS%29%2C%20the%20subset-minimal%20set%20of%20clauses%20that%20remains%20unsatisfiable.%20Current%20research%20broadly%20focuses%20on%20two%20directions%3A%20%28i%29%20enumerating%20as%20many%20MUSes%20as%20possible%20within%20a%20given%20time%20limit%2C%20and%20%28ii%29%20counting%20the%20total%20number%20of%20MUSes%20for%20a%20given%20unsatisfiable%20formula.%0A%20%20In%20this%20paper%2C%20we%20introduce%20an%20answer%20set%20programming-based%20framework%2C%20named%20MUS-ASP%2C%20designed%20for%20online%20enumeration%20of%20MUSes.%20ASP%20is%20a%20powerful%20tool%20for%20its%20strengths%20in%20knowledge%20representation%20and%20is%20particularly%20suitable%20for%20specifying%20complex%20combinatorial%20problems.%20By%20translating%20MUS%20enumeration%20into%20answer%20set%20solving%2C%20MUS-ASP%20leverages%20the%20computational%20efficiency%20of%20state-of-the-art%20ASP%20systems.%20Our%20extensive%20experimental%20evaluation%20demonstrates%20the%20effectiveness%20of%20MUS-ASP%20and%20highlights%20the%20acceleration%20in%20both%20MUS%20enumeration%20and%20counting%20tasks%2C%20particularly%20when%20integrated%20within%20hybrid%20solvers%2C%20including%20the%20framework%20proposed%20in%20this%20paper.&entry.1838667208=http%3A//arxiv.org/abs/2507.03929v2&entry.124074799=Read"},
{"title": "Bridging Prediction and Intervention Problems in Social Systems", "author": "Lydia T. Liu and Inioluwa Deborah Raji and Angela Zhou and Luke Guerdan and Jessica Hullman and Daniel Malinsky and Bryan Wilder and Simone Zhang and Hammaad Adam and Amanda Coston and Ben Laufer and Ezinne Nwankwo and Michael Zanger-Tishler and Eli Ben-Michael and Solon Barocas and Avi Feller and Marissa Gerchick and Talia Gillis and Shion Guha and Daniel Ho and Lily Hu and Kosuke Imai and Sayash Kapoor and Joshua Loftus and Razieh Nabi and Arvind Narayanan and Ben Recht and Juan Carlos Perdomo and Matthew Salganik and Mark Sendak and Alexander Tolbert and Berk Ustun and Suresh Venkatasubramanian and Angelina Wang and Ashia Wilson", "abstract": "Many automated decision systems (ADS) are designed to solve prediction problems -- where the goal is to learn patterns from a sample of the population and apply them to individuals from the same population. In reality, these prediction systems operationalize holistic policy interventions in deployment. Once deployed, ADS can shape impacted population outcomes through an effective policy change in how decision-makers operate, while also being defined by past and present interactions between stakeholders and the limitations of existing organizational, as well as societal, infrastructure and context. In this work, we consider the ways in which we must shift from a prediction-focused paradigm to an intervention-oriented paradigm when considering the impact of ADS within social systems. We argue this requires a new default problem setup for ADS beyond prediction, to instead consider predictions as decision support, final decisions, and outcomes. We highlight how this perspective unifies modern statistical frameworks and other tools to study the design, implementation, and evaluation of ADS systems, and point to the research directions necessary to operationalize this paradigm shift. Using these tools, we characterize the limitations of focusing on isolated prediction tasks, and lay the foundation for a more intervention-oriented approach to developing and deploying ADS.", "link": "http://arxiv.org/abs/2507.05216v3", "date": "2026-01-07", "relevancy": 1.3087, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4489}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4351}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Prediction%20and%20Intervention%20Problems%20in%20Social%20Systems&body=Title%3A%20Bridging%20Prediction%20and%20Intervention%20Problems%20in%20Social%20Systems%0AAuthor%3A%20Lydia%20T.%20Liu%20and%20Inioluwa%20Deborah%20Raji%20and%20Angela%20Zhou%20and%20Luke%20Guerdan%20and%20Jessica%20Hullman%20and%20Daniel%20Malinsky%20and%20Bryan%20Wilder%20and%20Simone%20Zhang%20and%20Hammaad%20Adam%20and%20Amanda%20Coston%20and%20Ben%20Laufer%20and%20Ezinne%20Nwankwo%20and%20Michael%20Zanger-Tishler%20and%20Eli%20Ben-Michael%20and%20Solon%20Barocas%20and%20Avi%20Feller%20and%20Marissa%20Gerchick%20and%20Talia%20Gillis%20and%20Shion%20Guha%20and%20Daniel%20Ho%20and%20Lily%20Hu%20and%20Kosuke%20Imai%20and%20Sayash%20Kapoor%20and%20Joshua%20Loftus%20and%20Razieh%20Nabi%20and%20Arvind%20Narayanan%20and%20Ben%20Recht%20and%20Juan%20Carlos%20Perdomo%20and%20Matthew%20Salganik%20and%20Mark%20Sendak%20and%20Alexander%20Tolbert%20and%20Berk%20Ustun%20and%20Suresh%20Venkatasubramanian%20and%20Angelina%20Wang%20and%20Ashia%20Wilson%0AAbstract%3A%20Many%20automated%20decision%20systems%20%28ADS%29%20are%20designed%20to%20solve%20prediction%20problems%20--%20where%20the%20goal%20is%20to%20learn%20patterns%20from%20a%20sample%20of%20the%20population%20and%20apply%20them%20to%20individuals%20from%20the%20same%20population.%20In%20reality%2C%20these%20prediction%20systems%20operationalize%20holistic%20policy%20interventions%20in%20deployment.%20Once%20deployed%2C%20ADS%20can%20shape%20impacted%20population%20outcomes%20through%20an%20effective%20policy%20change%20in%20how%20decision-makers%20operate%2C%20while%20also%20being%20defined%20by%20past%20and%20present%20interactions%20between%20stakeholders%20and%20the%20limitations%20of%20existing%20organizational%2C%20as%20well%20as%20societal%2C%20infrastructure%20and%20context.%20In%20this%20work%2C%20we%20consider%20the%20ways%20in%20which%20we%20must%20shift%20from%20a%20prediction-focused%20paradigm%20to%20an%20intervention-oriented%20paradigm%20when%20considering%20the%20impact%20of%20ADS%20within%20social%20systems.%20We%20argue%20this%20requires%20a%20new%20default%20problem%20setup%20for%20ADS%20beyond%20prediction%2C%20to%20instead%20consider%20predictions%20as%20decision%20support%2C%20final%20decisions%2C%20and%20outcomes.%20We%20highlight%20how%20this%20perspective%20unifies%20modern%20statistical%20frameworks%20and%20other%20tools%20to%20study%20the%20design%2C%20implementation%2C%20and%20evaluation%20of%20ADS%20systems%2C%20and%20point%20to%20the%20research%20directions%20necessary%20to%20operationalize%20this%20paradigm%20shift.%20Using%20these%20tools%2C%20we%20characterize%20the%20limitations%20of%20focusing%20on%20isolated%20prediction%20tasks%2C%20and%20lay%20the%20foundation%20for%20a%20more%20intervention-oriented%20approach%20to%20developing%20and%20deploying%20ADS.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05216v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Prediction%2520and%2520Intervention%2520Problems%2520in%2520Social%2520Systems%26entry.906535625%3DLydia%2520T.%2520Liu%2520and%2520Inioluwa%2520Deborah%2520Raji%2520and%2520Angela%2520Zhou%2520and%2520Luke%2520Guerdan%2520and%2520Jessica%2520Hullman%2520and%2520Daniel%2520Malinsky%2520and%2520Bryan%2520Wilder%2520and%2520Simone%2520Zhang%2520and%2520Hammaad%2520Adam%2520and%2520Amanda%2520Coston%2520and%2520Ben%2520Laufer%2520and%2520Ezinne%2520Nwankwo%2520and%2520Michael%2520Zanger-Tishler%2520and%2520Eli%2520Ben-Michael%2520and%2520Solon%2520Barocas%2520and%2520Avi%2520Feller%2520and%2520Marissa%2520Gerchick%2520and%2520Talia%2520Gillis%2520and%2520Shion%2520Guha%2520and%2520Daniel%2520Ho%2520and%2520Lily%2520Hu%2520and%2520Kosuke%2520Imai%2520and%2520Sayash%2520Kapoor%2520and%2520Joshua%2520Loftus%2520and%2520Razieh%2520Nabi%2520and%2520Arvind%2520Narayanan%2520and%2520Ben%2520Recht%2520and%2520Juan%2520Carlos%2520Perdomo%2520and%2520Matthew%2520Salganik%2520and%2520Mark%2520Sendak%2520and%2520Alexander%2520Tolbert%2520and%2520Berk%2520Ustun%2520and%2520Suresh%2520Venkatasubramanian%2520and%2520Angelina%2520Wang%2520and%2520Ashia%2520Wilson%26entry.1292438233%3DMany%2520automated%2520decision%2520systems%2520%2528ADS%2529%2520are%2520designed%2520to%2520solve%2520prediction%2520problems%2520--%2520where%2520the%2520goal%2520is%2520to%2520learn%2520patterns%2520from%2520a%2520sample%2520of%2520the%2520population%2520and%2520apply%2520them%2520to%2520individuals%2520from%2520the%2520same%2520population.%2520In%2520reality%252C%2520these%2520prediction%2520systems%2520operationalize%2520holistic%2520policy%2520interventions%2520in%2520deployment.%2520Once%2520deployed%252C%2520ADS%2520can%2520shape%2520impacted%2520population%2520outcomes%2520through%2520an%2520effective%2520policy%2520change%2520in%2520how%2520decision-makers%2520operate%252C%2520while%2520also%2520being%2520defined%2520by%2520past%2520and%2520present%2520interactions%2520between%2520stakeholders%2520and%2520the%2520limitations%2520of%2520existing%2520organizational%252C%2520as%2520well%2520as%2520societal%252C%2520infrastructure%2520and%2520context.%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520ways%2520in%2520which%2520we%2520must%2520shift%2520from%2520a%2520prediction-focused%2520paradigm%2520to%2520an%2520intervention-oriented%2520paradigm%2520when%2520considering%2520the%2520impact%2520of%2520ADS%2520within%2520social%2520systems.%2520We%2520argue%2520this%2520requires%2520a%2520new%2520default%2520problem%2520setup%2520for%2520ADS%2520beyond%2520prediction%252C%2520to%2520instead%2520consider%2520predictions%2520as%2520decision%2520support%252C%2520final%2520decisions%252C%2520and%2520outcomes.%2520We%2520highlight%2520how%2520this%2520perspective%2520unifies%2520modern%2520statistical%2520frameworks%2520and%2520other%2520tools%2520to%2520study%2520the%2520design%252C%2520implementation%252C%2520and%2520evaluation%2520of%2520ADS%2520systems%252C%2520and%2520point%2520to%2520the%2520research%2520directions%2520necessary%2520to%2520operationalize%2520this%2520paradigm%2520shift.%2520Using%2520these%2520tools%252C%2520we%2520characterize%2520the%2520limitations%2520of%2520focusing%2520on%2520isolated%2520prediction%2520tasks%252C%2520and%2520lay%2520the%2520foundation%2520for%2520a%2520more%2520intervention-oriented%2520approach%2520to%2520developing%2520and%2520deploying%2520ADS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05216v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Prediction%20and%20Intervention%20Problems%20in%20Social%20Systems&entry.906535625=Lydia%20T.%20Liu%20and%20Inioluwa%20Deborah%20Raji%20and%20Angela%20Zhou%20and%20Luke%20Guerdan%20and%20Jessica%20Hullman%20and%20Daniel%20Malinsky%20and%20Bryan%20Wilder%20and%20Simone%20Zhang%20and%20Hammaad%20Adam%20and%20Amanda%20Coston%20and%20Ben%20Laufer%20and%20Ezinne%20Nwankwo%20and%20Michael%20Zanger-Tishler%20and%20Eli%20Ben-Michael%20and%20Solon%20Barocas%20and%20Avi%20Feller%20and%20Marissa%20Gerchick%20and%20Talia%20Gillis%20and%20Shion%20Guha%20and%20Daniel%20Ho%20and%20Lily%20Hu%20and%20Kosuke%20Imai%20and%20Sayash%20Kapoor%20and%20Joshua%20Loftus%20and%20Razieh%20Nabi%20and%20Arvind%20Narayanan%20and%20Ben%20Recht%20and%20Juan%20Carlos%20Perdomo%20and%20Matthew%20Salganik%20and%20Mark%20Sendak%20and%20Alexander%20Tolbert%20and%20Berk%20Ustun%20and%20Suresh%20Venkatasubramanian%20and%20Angelina%20Wang%20and%20Ashia%20Wilson&entry.1292438233=Many%20automated%20decision%20systems%20%28ADS%29%20are%20designed%20to%20solve%20prediction%20problems%20--%20where%20the%20goal%20is%20to%20learn%20patterns%20from%20a%20sample%20of%20the%20population%20and%20apply%20them%20to%20individuals%20from%20the%20same%20population.%20In%20reality%2C%20these%20prediction%20systems%20operationalize%20holistic%20policy%20interventions%20in%20deployment.%20Once%20deployed%2C%20ADS%20can%20shape%20impacted%20population%20outcomes%20through%20an%20effective%20policy%20change%20in%20how%20decision-makers%20operate%2C%20while%20also%20being%20defined%20by%20past%20and%20present%20interactions%20between%20stakeholders%20and%20the%20limitations%20of%20existing%20organizational%2C%20as%20well%20as%20societal%2C%20infrastructure%20and%20context.%20In%20this%20work%2C%20we%20consider%20the%20ways%20in%20which%20we%20must%20shift%20from%20a%20prediction-focused%20paradigm%20to%20an%20intervention-oriented%20paradigm%20when%20considering%20the%20impact%20of%20ADS%20within%20social%20systems.%20We%20argue%20this%20requires%20a%20new%20default%20problem%20setup%20for%20ADS%20beyond%20prediction%2C%20to%20instead%20consider%20predictions%20as%20decision%20support%2C%20final%20decisions%2C%20and%20outcomes.%20We%20highlight%20how%20this%20perspective%20unifies%20modern%20statistical%20frameworks%20and%20other%20tools%20to%20study%20the%20design%2C%20implementation%2C%20and%20evaluation%20of%20ADS%20systems%2C%20and%20point%20to%20the%20research%20directions%20necessary%20to%20operationalize%20this%20paradigm%20shift.%20Using%20these%20tools%2C%20we%20characterize%20the%20limitations%20of%20focusing%20on%20isolated%20prediction%20tasks%2C%20and%20lay%20the%20foundation%20for%20a%20more%20intervention-oriented%20approach%20to%20developing%20and%20deploying%20ADS.&entry.1838667208=http%3A//arxiv.org/abs/2507.05216v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


