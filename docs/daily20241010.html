<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241008.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior", "author": "David Svitov and Pietro Morerio and Lourdes Agapito and Alessio Del Bue", "abstract": "  We present HAHA - a novel approach for animatable human avatar generation\nfrom monocular input videos. The proposed method relies on learning the\ntrade-off between the use of Gaussian splatting and a textured mesh for\nefficient and high fidelity rendering. We demonstrate its efficiency to animate\nand render full-body human avatars controlled via the SMPL-X parametric model.\nOur model learns to apply Gaussian splatting only in areas of the SMPL-X mesh\nwhere it is necessary, like hair and out-of-mesh clothing. This results in a\nminimal number of Gaussians being used to represent the full avatar, and\nreduced rendering artifacts. This allows us to handle the animation of small\nbody parts such as fingers that are traditionally disregarded. We demonstrate\nthe effectiveness of our approach on two open datasets: SnapshotPeople and\nX-Humans. Our method demonstrates on par reconstruction quality to the\nstate-of-the-art on SnapshotPeople, while using less than a third of Gaussians.\nHAHA outperforms previous state-of-the-art on novel poses from X-Humans both\nquantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2404.01053v2", "date": "2024-10-09", "relevancy": 3.5627, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7223}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7223}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAHA%3A%20Highly%20Articulated%20Gaussian%20Human%20Avatars%20with%20Textured%20Mesh%20Prior&body=Title%3A%20HAHA%3A%20Highly%20Articulated%20Gaussian%20Human%20Avatars%20with%20Textured%20Mesh%20Prior%0AAuthor%3A%20David%20Svitov%20and%20Pietro%20Morerio%20and%20Lourdes%20Agapito%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20We%20present%20HAHA%20-%20a%20novel%20approach%20for%20animatable%20human%20avatar%20generation%0Afrom%20monocular%20input%20videos.%20The%20proposed%20method%20relies%20on%20learning%20the%0Atrade-off%20between%20the%20use%20of%20Gaussian%20splatting%20and%20a%20textured%20mesh%20for%0Aefficient%20and%20high%20fidelity%20rendering.%20We%20demonstrate%20its%20efficiency%20to%20animate%0Aand%20render%20full-body%20human%20avatars%20controlled%20via%20the%20SMPL-X%20parametric%20model.%0AOur%20model%20learns%20to%20apply%20Gaussian%20splatting%20only%20in%20areas%20of%20the%20SMPL-X%20mesh%0Awhere%20it%20is%20necessary%2C%20like%20hair%20and%20out-of-mesh%20clothing.%20This%20results%20in%20a%0Aminimal%20number%20of%20Gaussians%20being%20used%20to%20represent%20the%20full%20avatar%2C%20and%0Areduced%20rendering%20artifacts.%20This%20allows%20us%20to%20handle%20the%20animation%20of%20small%0Abody%20parts%20such%20as%20fingers%20that%20are%20traditionally%20disregarded.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20approach%20on%20two%20open%20datasets%3A%20SnapshotPeople%20and%0AX-Humans.%20Our%20method%20demonstrates%20on%20par%20reconstruction%20quality%20to%20the%0Astate-of-the-art%20on%20SnapshotPeople%2C%20while%20using%20less%20than%20a%20third%20of%20Gaussians.%0AHAHA%20outperforms%20previous%20state-of-the-art%20on%20novel%20poses%20from%20X-Humans%20both%0Aquantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01053v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAHA%253A%2520Highly%2520Articulated%2520Gaussian%2520Human%2520Avatars%2520with%2520Textured%2520Mesh%2520Prior%26entry.906535625%3DDavid%2520Svitov%2520and%2520Pietro%2520Morerio%2520and%2520Lourdes%2520Agapito%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520We%2520present%2520HAHA%2520-%2520a%2520novel%2520approach%2520for%2520animatable%2520human%2520avatar%2520generation%250Afrom%2520monocular%2520input%2520videos.%2520The%2520proposed%2520method%2520relies%2520on%2520learning%2520the%250Atrade-off%2520between%2520the%2520use%2520of%2520Gaussian%2520splatting%2520and%2520a%2520textured%2520mesh%2520for%250Aefficient%2520and%2520high%2520fidelity%2520rendering.%2520We%2520demonstrate%2520its%2520efficiency%2520to%2520animate%250Aand%2520render%2520full-body%2520human%2520avatars%2520controlled%2520via%2520the%2520SMPL-X%2520parametric%2520model.%250AOur%2520model%2520learns%2520to%2520apply%2520Gaussian%2520splatting%2520only%2520in%2520areas%2520of%2520the%2520SMPL-X%2520mesh%250Awhere%2520it%2520is%2520necessary%252C%2520like%2520hair%2520and%2520out-of-mesh%2520clothing.%2520This%2520results%2520in%2520a%250Aminimal%2520number%2520of%2520Gaussians%2520being%2520used%2520to%2520represent%2520the%2520full%2520avatar%252C%2520and%250Areduced%2520rendering%2520artifacts.%2520This%2520allows%2520us%2520to%2520handle%2520the%2520animation%2520of%2520small%250Abody%2520parts%2520such%2520as%2520fingers%2520that%2520are%2520traditionally%2520disregarded.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520approach%2520on%2520two%2520open%2520datasets%253A%2520SnapshotPeople%2520and%250AX-Humans.%2520Our%2520method%2520demonstrates%2520on%2520par%2520reconstruction%2520quality%2520to%2520the%250Astate-of-the-art%2520on%2520SnapshotPeople%252C%2520while%2520using%2520less%2520than%2520a%2520third%2520of%2520Gaussians.%250AHAHA%2520outperforms%2520previous%2520state-of-the-art%2520on%2520novel%2520poses%2520from%2520X-Humans%2520both%250Aquantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01053v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAHA%3A%20Highly%20Articulated%20Gaussian%20Human%20Avatars%20with%20Textured%20Mesh%20Prior&entry.906535625=David%20Svitov%20and%20Pietro%20Morerio%20and%20Lourdes%20Agapito%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20We%20present%20HAHA%20-%20a%20novel%20approach%20for%20animatable%20human%20avatar%20generation%0Afrom%20monocular%20input%20videos.%20The%20proposed%20method%20relies%20on%20learning%20the%0Atrade-off%20between%20the%20use%20of%20Gaussian%20splatting%20and%20a%20textured%20mesh%20for%0Aefficient%20and%20high%20fidelity%20rendering.%20We%20demonstrate%20its%20efficiency%20to%20animate%0Aand%20render%20full-body%20human%20avatars%20controlled%20via%20the%20SMPL-X%20parametric%20model.%0AOur%20model%20learns%20to%20apply%20Gaussian%20splatting%20only%20in%20areas%20of%20the%20SMPL-X%20mesh%0Awhere%20it%20is%20necessary%2C%20like%20hair%20and%20out-of-mesh%20clothing.%20This%20results%20in%20a%0Aminimal%20number%20of%20Gaussians%20being%20used%20to%20represent%20the%20full%20avatar%2C%20and%0Areduced%20rendering%20artifacts.%20This%20allows%20us%20to%20handle%20the%20animation%20of%20small%0Abody%20parts%20such%20as%20fingers%20that%20are%20traditionally%20disregarded.%20We%20demonstrate%0Athe%20effectiveness%20of%20our%20approach%20on%20two%20open%20datasets%3A%20SnapshotPeople%20and%0AX-Humans.%20Our%20method%20demonstrates%20on%20par%20reconstruction%20quality%20to%20the%0Astate-of-the-art%20on%20SnapshotPeople%2C%20while%20using%20less%20than%20a%20third%20of%20Gaussians.%0AHAHA%20outperforms%20previous%20state-of-the-art%20on%20novel%20poses%20from%20X-Humans%20both%0Aquantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01053v2&entry.124074799=Read"},
{"title": "DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh\n  Hybrid Representation", "author": "Zhiqi Li and Yiming Chen and Peidong Liu", "abstract": "  Recent advancements in 2D/3D generative techniques have facilitated the\ngeneration of dynamic 3D objects from monocular videos. Previous methods mainly\nrely on the implicit neural radiance fields (NeRF) or explicit Gaussian\nSplatting as the underlying representation, and struggle to achieve\nsatisfactory spatial-temporal consistency and surface appearance. Drawing\ninspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a\nnovel framework combining mesh representation with geometric skinning technique\nto generate high-quality 4D object from a monocular video. Instead of utilizing\nclassical texture map for appearance, we bind Gaussian splats to triangle face\nof mesh for differentiable optimization of both the texture and mesh vertices.\nIn particular, DreamMesh4D begins with a coarse mesh obtained through an\nimage-to-3D generation procedure. Sparse points are then uniformly sampled\nacross the mesh surface, and are used to build a deformation graph to drive the\nmotion of the 3D object for the sake of computational efficiency and providing\nadditional constraint. For each step, transformations of sparse control points\nare predicted using a deformation network, and the mesh vertices as well as the\nsurface Gaussians are deformed via a novel geometric skinning algorithm, which\nis a hybrid approach combining LBS (linear blending skinning) and DQS\n(dual-quaternion skinning), mitigating drawbacks associated with both\napproaches. The static surface Gaussians and mesh vertices as well as the\ndeformation network are learned via reference view photometric loss, score\ndistillation loss as well as other regularizers in a two-stage manner.\nExtensive experiments demonstrate superior performance of our method.\nFurthermore, our method is compatible with modern graphic pipelines, showcasing\nits potential in the 3D gaming and film industry.\n", "link": "http://arxiv.org/abs/2410.06756v1", "date": "2024-10-09", "relevancy": 3.4285, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7731}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6423}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamMesh4D%3A%20Video-to-4D%20Generation%20with%20Sparse-Controlled%20Gaussian-Mesh%0A%20%20Hybrid%20Representation&body=Title%3A%20DreamMesh4D%3A%20Video-to-4D%20Generation%20with%20Sparse-Controlled%20Gaussian-Mesh%0A%20%20Hybrid%20Representation%0AAuthor%3A%20Zhiqi%20Li%20and%20Yiming%20Chen%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%202D/3D%20generative%20techniques%20have%20facilitated%20the%0Ageneration%20of%20dynamic%203D%20objects%20from%20monocular%20videos.%20Previous%20methods%20mainly%0Arely%20on%20the%20implicit%20neural%20radiance%20fields%20%28NeRF%29%20or%20explicit%20Gaussian%0ASplatting%20as%20the%20underlying%20representation%2C%20and%20struggle%20to%20achieve%0Asatisfactory%20spatial-temporal%20consistency%20and%20surface%20appearance.%20Drawing%0Ainspiration%20from%20modern%203D%20animation%20pipelines%2C%20we%20introduce%20DreamMesh4D%2C%20a%0Anovel%20framework%20combining%20mesh%20representation%20with%20geometric%20skinning%20technique%0Ato%20generate%20high-quality%204D%20object%20from%20a%20monocular%20video.%20Instead%20of%20utilizing%0Aclassical%20texture%20map%20for%20appearance%2C%20we%20bind%20Gaussian%20splats%20to%20triangle%20face%0Aof%20mesh%20for%20differentiable%20optimization%20of%20both%20the%20texture%20and%20mesh%20vertices.%0AIn%20particular%2C%20DreamMesh4D%20begins%20with%20a%20coarse%20mesh%20obtained%20through%20an%0Aimage-to-3D%20generation%20procedure.%20Sparse%20points%20are%20then%20uniformly%20sampled%0Aacross%20the%20mesh%20surface%2C%20and%20are%20used%20to%20build%20a%20deformation%20graph%20to%20drive%20the%0Amotion%20of%20the%203D%20object%20for%20the%20sake%20of%20computational%20efficiency%20and%20providing%0Aadditional%20constraint.%20For%20each%20step%2C%20transformations%20of%20sparse%20control%20points%0Aare%20predicted%20using%20a%20deformation%20network%2C%20and%20the%20mesh%20vertices%20as%20well%20as%20the%0Asurface%20Gaussians%20are%20deformed%20via%20a%20novel%20geometric%20skinning%20algorithm%2C%20which%0Ais%20a%20hybrid%20approach%20combining%20LBS%20%28linear%20blending%20skinning%29%20and%20DQS%0A%28dual-quaternion%20skinning%29%2C%20mitigating%20drawbacks%20associated%20with%20both%0Aapproaches.%20The%20static%20surface%20Gaussians%20and%20mesh%20vertices%20as%20well%20as%20the%0Adeformation%20network%20are%20learned%20via%20reference%20view%20photometric%20loss%2C%20score%0Adistillation%20loss%20as%20well%20as%20other%20regularizers%20in%20a%20two-stage%20manner.%0AExtensive%20experiments%20demonstrate%20superior%20performance%20of%20our%20method.%0AFurthermore%2C%20our%20method%20is%20compatible%20with%20modern%20graphic%20pipelines%2C%20showcasing%0Aits%20potential%20in%20the%203D%20gaming%20and%20film%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamMesh4D%253A%2520Video-to-4D%2520Generation%2520with%2520Sparse-Controlled%2520Gaussian-Mesh%250A%2520%2520Hybrid%2520Representation%26entry.906535625%3DZhiqi%2520Li%2520and%2520Yiming%2520Chen%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25202D/3D%2520generative%2520techniques%2520have%2520facilitated%2520the%250Ageneration%2520of%2520dynamic%25203D%2520objects%2520from%2520monocular%2520videos.%2520Previous%2520methods%2520mainly%250Arely%2520on%2520the%2520implicit%2520neural%2520radiance%2520fields%2520%2528NeRF%2529%2520or%2520explicit%2520Gaussian%250ASplatting%2520as%2520the%2520underlying%2520representation%252C%2520and%2520struggle%2520to%2520achieve%250Asatisfactory%2520spatial-temporal%2520consistency%2520and%2520surface%2520appearance.%2520Drawing%250Ainspiration%2520from%2520modern%25203D%2520animation%2520pipelines%252C%2520we%2520introduce%2520DreamMesh4D%252C%2520a%250Anovel%2520framework%2520combining%2520mesh%2520representation%2520with%2520geometric%2520skinning%2520technique%250Ato%2520generate%2520high-quality%25204D%2520object%2520from%2520a%2520monocular%2520video.%2520Instead%2520of%2520utilizing%250Aclassical%2520texture%2520map%2520for%2520appearance%252C%2520we%2520bind%2520Gaussian%2520splats%2520to%2520triangle%2520face%250Aof%2520mesh%2520for%2520differentiable%2520optimization%2520of%2520both%2520the%2520texture%2520and%2520mesh%2520vertices.%250AIn%2520particular%252C%2520DreamMesh4D%2520begins%2520with%2520a%2520coarse%2520mesh%2520obtained%2520through%2520an%250Aimage-to-3D%2520generation%2520procedure.%2520Sparse%2520points%2520are%2520then%2520uniformly%2520sampled%250Aacross%2520the%2520mesh%2520surface%252C%2520and%2520are%2520used%2520to%2520build%2520a%2520deformation%2520graph%2520to%2520drive%2520the%250Amotion%2520of%2520the%25203D%2520object%2520for%2520the%2520sake%2520of%2520computational%2520efficiency%2520and%2520providing%250Aadditional%2520constraint.%2520For%2520each%2520step%252C%2520transformations%2520of%2520sparse%2520control%2520points%250Aare%2520predicted%2520using%2520a%2520deformation%2520network%252C%2520and%2520the%2520mesh%2520vertices%2520as%2520well%2520as%2520the%250Asurface%2520Gaussians%2520are%2520deformed%2520via%2520a%2520novel%2520geometric%2520skinning%2520algorithm%252C%2520which%250Ais%2520a%2520hybrid%2520approach%2520combining%2520LBS%2520%2528linear%2520blending%2520skinning%2529%2520and%2520DQS%250A%2528dual-quaternion%2520skinning%2529%252C%2520mitigating%2520drawbacks%2520associated%2520with%2520both%250Aapproaches.%2520The%2520static%2520surface%2520Gaussians%2520and%2520mesh%2520vertices%2520as%2520well%2520as%2520the%250Adeformation%2520network%2520are%2520learned%2520via%2520reference%2520view%2520photometric%2520loss%252C%2520score%250Adistillation%2520loss%2520as%2520well%2520as%2520other%2520regularizers%2520in%2520a%2520two-stage%2520manner.%250AExtensive%2520experiments%2520demonstrate%2520superior%2520performance%2520of%2520our%2520method.%250AFurthermore%252C%2520our%2520method%2520is%2520compatible%2520with%2520modern%2520graphic%2520pipelines%252C%2520showcasing%250Aits%2520potential%2520in%2520the%25203D%2520gaming%2520and%2520film%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamMesh4D%3A%20Video-to-4D%20Generation%20with%20Sparse-Controlled%20Gaussian-Mesh%0A%20%20Hybrid%20Representation&entry.906535625=Zhiqi%20Li%20and%20Yiming%20Chen%20and%20Peidong%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%202D/3D%20generative%20techniques%20have%20facilitated%20the%0Ageneration%20of%20dynamic%203D%20objects%20from%20monocular%20videos.%20Previous%20methods%20mainly%0Arely%20on%20the%20implicit%20neural%20radiance%20fields%20%28NeRF%29%20or%20explicit%20Gaussian%0ASplatting%20as%20the%20underlying%20representation%2C%20and%20struggle%20to%20achieve%0Asatisfactory%20spatial-temporal%20consistency%20and%20surface%20appearance.%20Drawing%0Ainspiration%20from%20modern%203D%20animation%20pipelines%2C%20we%20introduce%20DreamMesh4D%2C%20a%0Anovel%20framework%20combining%20mesh%20representation%20with%20geometric%20skinning%20technique%0Ato%20generate%20high-quality%204D%20object%20from%20a%20monocular%20video.%20Instead%20of%20utilizing%0Aclassical%20texture%20map%20for%20appearance%2C%20we%20bind%20Gaussian%20splats%20to%20triangle%20face%0Aof%20mesh%20for%20differentiable%20optimization%20of%20both%20the%20texture%20and%20mesh%20vertices.%0AIn%20particular%2C%20DreamMesh4D%20begins%20with%20a%20coarse%20mesh%20obtained%20through%20an%0Aimage-to-3D%20generation%20procedure.%20Sparse%20points%20are%20then%20uniformly%20sampled%0Aacross%20the%20mesh%20surface%2C%20and%20are%20used%20to%20build%20a%20deformation%20graph%20to%20drive%20the%0Amotion%20of%20the%203D%20object%20for%20the%20sake%20of%20computational%20efficiency%20and%20providing%0Aadditional%20constraint.%20For%20each%20step%2C%20transformations%20of%20sparse%20control%20points%0Aare%20predicted%20using%20a%20deformation%20network%2C%20and%20the%20mesh%20vertices%20as%20well%20as%20the%0Asurface%20Gaussians%20are%20deformed%20via%20a%20novel%20geometric%20skinning%20algorithm%2C%20which%0Ais%20a%20hybrid%20approach%20combining%20LBS%20%28linear%20blending%20skinning%29%20and%20DQS%0A%28dual-quaternion%20skinning%29%2C%20mitigating%20drawbacks%20associated%20with%20both%0Aapproaches.%20The%20static%20surface%20Gaussians%20and%20mesh%20vertices%20as%20well%20as%20the%0Adeformation%20network%20are%20learned%20via%20reference%20view%20photometric%20loss%2C%20score%0Adistillation%20loss%20as%20well%20as%20other%20regularizers%20in%20a%20two-stage%20manner.%0AExtensive%20experiments%20demonstrate%20superior%20performance%20of%20our%20method.%0AFurthermore%2C%20our%20method%20is%20compatible%20with%20modern%20graphic%20pipelines%2C%20showcasing%0Aits%20potential%20in%20the%203D%20gaming%20and%20film%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06756v1&entry.124074799=Read"},
{"title": "HGS-Planner: Hierarchical Planning Framework for Active Scene\n  Reconstruction Using 3D Gaussian Splatting", "author": "Zijun Xu and Rui Jin and Ke Wu and Yi Zhao and Zhiwei Zhang and Jieru Zhao and Fei Gao and Zhongxue Gan and Wenchao Ding", "abstract": "  In complex missions such as search and rescue,robots must make intelligent\ndecisions in unknown environments, relying on their ability to perceive and\nunderstand their surroundings. High-quality and real-time reconstruction\nenhances situational awareness and is crucial for intelligent robotics.\nTraditional methods often struggle with poor scene representation or are too\nslow for real-time use. Inspired by the efficacy of 3D Gaussian Splatting\n(3DGS), we propose a hierarchical planning framework for fast and high-fidelity\nactive reconstruction. Our method evaluates completion and quality gain to\nadaptively guide reconstruction, integrating global and local planning for\nefficiency. Experiments in simulated and real-world environments show our\napproach outperforms existing real-time methods.\n", "link": "http://arxiv.org/abs/2409.17624v2", "date": "2024-10-09", "relevancy": 3.3372, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7141}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6589}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGS-Planner%3A%20Hierarchical%20Planning%20Framework%20for%20Active%20Scene%0A%20%20Reconstruction%20Using%203D%20Gaussian%20Splatting&body=Title%3A%20HGS-Planner%3A%20Hierarchical%20Planning%20Framework%20for%20Active%20Scene%0A%20%20Reconstruction%20Using%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zijun%20Xu%20and%20Rui%20Jin%20and%20Ke%20Wu%20and%20Yi%20Zhao%20and%20Zhiwei%20Zhang%20and%20Jieru%20Zhao%20and%20Fei%20Gao%20and%20Zhongxue%20Gan%20and%20Wenchao%20Ding%0AAbstract%3A%20%20%20In%20complex%20missions%20such%20as%20search%20and%20rescue%2Crobots%20must%20make%20intelligent%0Adecisions%20in%20unknown%20environments%2C%20relying%20on%20their%20ability%20to%20perceive%20and%0Aunderstand%20their%20surroundings.%20High-quality%20and%20real-time%20reconstruction%0Aenhances%20situational%20awareness%20and%20is%20crucial%20for%20intelligent%20robotics.%0ATraditional%20methods%20often%20struggle%20with%20poor%20scene%20representation%20or%20are%20too%0Aslow%20for%20real-time%20use.%20Inspired%20by%20the%20efficacy%20of%203D%20Gaussian%20Splatting%0A%283DGS%29%2C%20we%20propose%20a%20hierarchical%20planning%20framework%20for%20fast%20and%20high-fidelity%0Aactive%20reconstruction.%20Our%20method%20evaluates%20completion%20and%20quality%20gain%20to%0Aadaptively%20guide%20reconstruction%2C%20integrating%20global%20and%20local%20planning%20for%0Aefficiency.%20Experiments%20in%20simulated%20and%20real-world%20environments%20show%20our%0Aapproach%20outperforms%20existing%20real-time%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGS-Planner%253A%2520Hierarchical%2520Planning%2520Framework%2520for%2520Active%2520Scene%250A%2520%2520Reconstruction%2520Using%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZijun%2520Xu%2520and%2520Rui%2520Jin%2520and%2520Ke%2520Wu%2520and%2520Yi%2520Zhao%2520and%2520Zhiwei%2520Zhang%2520and%2520Jieru%2520Zhao%2520and%2520Fei%2520Gao%2520and%2520Zhongxue%2520Gan%2520and%2520Wenchao%2520Ding%26entry.1292438233%3D%2520%2520In%2520complex%2520missions%2520such%2520as%2520search%2520and%2520rescue%252Crobots%2520must%2520make%2520intelligent%250Adecisions%2520in%2520unknown%2520environments%252C%2520relying%2520on%2520their%2520ability%2520to%2520perceive%2520and%250Aunderstand%2520their%2520surroundings.%2520High-quality%2520and%2520real-time%2520reconstruction%250Aenhances%2520situational%2520awareness%2520and%2520is%2520crucial%2520for%2520intelligent%2520robotics.%250ATraditional%2520methods%2520often%2520struggle%2520with%2520poor%2520scene%2520representation%2520or%2520are%2520too%250Aslow%2520for%2520real-time%2520use.%2520Inspired%2520by%2520the%2520efficacy%2520of%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%252C%2520we%2520propose%2520a%2520hierarchical%2520planning%2520framework%2520for%2520fast%2520and%2520high-fidelity%250Aactive%2520reconstruction.%2520Our%2520method%2520evaluates%2520completion%2520and%2520quality%2520gain%2520to%250Aadaptively%2520guide%2520reconstruction%252C%2520integrating%2520global%2520and%2520local%2520planning%2520for%250Aefficiency.%2520Experiments%2520in%2520simulated%2520and%2520real-world%2520environments%2520show%2520our%250Aapproach%2520outperforms%2520existing%2520real-time%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGS-Planner%3A%20Hierarchical%20Planning%20Framework%20for%20Active%20Scene%0A%20%20Reconstruction%20Using%203D%20Gaussian%20Splatting&entry.906535625=Zijun%20Xu%20and%20Rui%20Jin%20and%20Ke%20Wu%20and%20Yi%20Zhao%20and%20Zhiwei%20Zhang%20and%20Jieru%20Zhao%20and%20Fei%20Gao%20and%20Zhongxue%20Gan%20and%20Wenchao%20Ding&entry.1292438233=%20%20In%20complex%20missions%20such%20as%20search%20and%20rescue%2Crobots%20must%20make%20intelligent%0Adecisions%20in%20unknown%20environments%2C%20relying%20on%20their%20ability%20to%20perceive%20and%0Aunderstand%20their%20surroundings.%20High-quality%20and%20real-time%20reconstruction%0Aenhances%20situational%20awareness%20and%20is%20crucial%20for%20intelligent%20robotics.%0ATraditional%20methods%20often%20struggle%20with%20poor%20scene%20representation%20or%20are%20too%0Aslow%20for%20real-time%20use.%20Inspired%20by%20the%20efficacy%20of%203D%20Gaussian%20Splatting%0A%283DGS%29%2C%20we%20propose%20a%20hierarchical%20planning%20framework%20for%20fast%20and%20high-fidelity%0Aactive%20reconstruction.%20Our%20method%20evaluates%20completion%20and%20quality%20gain%20to%0Aadaptively%20guide%20reconstruction%2C%20integrating%20global%20and%20local%20planning%20for%0Aefficiency.%20Experiments%20in%20simulated%20and%20real-world%20environments%20show%20our%0Aapproach%20outperforms%20existing%20real-time%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17624v2&entry.124074799=Read"},
{"title": "StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time\n  Rendering", "author": "Lukas Radl and Michael Steiner and Mathias Parger and Alexander Weinrauch and Bernhard Kerbl and Markus Steinberger", "abstract": "  Gaussian Splatting has emerged as a prominent model for constructing 3D\nrepresentations from images across diverse domains. However, the efficiency of\nthe 3D Gaussian Splatting rendering pipeline relies on several simplifications.\nNotably, reducing Gaussian to 2D splats with a single view-space depth\nintroduces popping and blending artifacts during view rotation. Addressing this\nissue requires accurate per-pixel depth computation, yet a full per-pixel sort\nproves excessively costly compared to a global sort operation. In this paper,\nwe present a novel hierarchical rasterization approach that systematically\nresorts and culls splats with minimal processing overhead. Our software\nrasterizer effectively eliminates popping artifacts and view inconsistencies,\nas demonstrated through both quantitative and qualitative measurements.\nSimultaneously, our method mitigates the potential for cheating view-dependent\neffects with popping, ensuring a more authentic representation. Despite the\nelimination of cheating, our approach achieves comparable quantitative results\nfor test images, while increasing the consistency for novel view synthesis in\nmotion. Due to its design, our hierarchical approach is only 4% slower on\naverage than the original Gaussian Splatting. Notably, enforcing consistency\nenables a reduction in the number of Gaussians by approximately half with\nnearly identical quality and view-consistency. Consequently, rendering\nperformance is nearly doubled, making our approach 1.6x faster than the\noriginal Gaussian Splatting, with a 50% reduction in memory requirements.\n", "link": "http://arxiv.org/abs/2402.00525v3", "date": "2024-10-09", "relevancy": 3.3347, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.717}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6436}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StopThePop%3A%20Sorted%20Gaussian%20Splatting%20for%20View-Consistent%20Real-time%0A%20%20Rendering&body=Title%3A%20StopThePop%3A%20Sorted%20Gaussian%20Splatting%20for%20View-Consistent%20Real-time%0A%20%20Rendering%0AAuthor%3A%20Lukas%20Radl%20and%20Michael%20Steiner%20and%20Mathias%20Parger%20and%20Alexander%20Weinrauch%20and%20Bernhard%20Kerbl%20and%20Markus%20Steinberger%0AAbstract%3A%20%20%20Gaussian%20Splatting%20has%20emerged%20as%20a%20prominent%20model%20for%20constructing%203D%0Arepresentations%20from%20images%20across%20diverse%20domains.%20However%2C%20the%20efficiency%20of%0Athe%203D%20Gaussian%20Splatting%20rendering%20pipeline%20relies%20on%20several%20simplifications.%0ANotably%2C%20reducing%20Gaussian%20to%202D%20splats%20with%20a%20single%20view-space%20depth%0Aintroduces%20popping%20and%20blending%20artifacts%20during%20view%20rotation.%20Addressing%20this%0Aissue%20requires%20accurate%20per-pixel%20depth%20computation%2C%20yet%20a%20full%20per-pixel%20sort%0Aproves%20excessively%20costly%20compared%20to%20a%20global%20sort%20operation.%20In%20this%20paper%2C%0Awe%20present%20a%20novel%20hierarchical%20rasterization%20approach%20that%20systematically%0Aresorts%20and%20culls%20splats%20with%20minimal%20processing%20overhead.%20Our%20software%0Arasterizer%20effectively%20eliminates%20popping%20artifacts%20and%20view%20inconsistencies%2C%0Aas%20demonstrated%20through%20both%20quantitative%20and%20qualitative%20measurements.%0ASimultaneously%2C%20our%20method%20mitigates%20the%20potential%20for%20cheating%20view-dependent%0Aeffects%20with%20popping%2C%20ensuring%20a%20more%20authentic%20representation.%20Despite%20the%0Aelimination%20of%20cheating%2C%20our%20approach%20achieves%20comparable%20quantitative%20results%0Afor%20test%20images%2C%20while%20increasing%20the%20consistency%20for%20novel%20view%20synthesis%20in%0Amotion.%20Due%20to%20its%20design%2C%20our%20hierarchical%20approach%20is%20only%204%25%20slower%20on%0Aaverage%20than%20the%20original%20Gaussian%20Splatting.%20Notably%2C%20enforcing%20consistency%0Aenables%20a%20reduction%20in%20the%20number%20of%20Gaussians%20by%20approximately%20half%20with%0Anearly%20identical%20quality%20and%20view-consistency.%20Consequently%2C%20rendering%0Aperformance%20is%20nearly%20doubled%2C%20making%20our%20approach%201.6x%20faster%20than%20the%0Aoriginal%20Gaussian%20Splatting%2C%20with%20a%2050%25%20reduction%20in%20memory%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00525v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStopThePop%253A%2520Sorted%2520Gaussian%2520Splatting%2520for%2520View-Consistent%2520Real-time%250A%2520%2520Rendering%26entry.906535625%3DLukas%2520Radl%2520and%2520Michael%2520Steiner%2520and%2520Mathias%2520Parger%2520and%2520Alexander%2520Weinrauch%2520and%2520Bernhard%2520Kerbl%2520and%2520Markus%2520Steinberger%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520a%2520prominent%2520model%2520for%2520constructing%25203D%250Arepresentations%2520from%2520images%2520across%2520diverse%2520domains.%2520However%252C%2520the%2520efficiency%2520of%250Athe%25203D%2520Gaussian%2520Splatting%2520rendering%2520pipeline%2520relies%2520on%2520several%2520simplifications.%250ANotably%252C%2520reducing%2520Gaussian%2520to%25202D%2520splats%2520with%2520a%2520single%2520view-space%2520depth%250Aintroduces%2520popping%2520and%2520blending%2520artifacts%2520during%2520view%2520rotation.%2520Addressing%2520this%250Aissue%2520requires%2520accurate%2520per-pixel%2520depth%2520computation%252C%2520yet%2520a%2520full%2520per-pixel%2520sort%250Aproves%2520excessively%2520costly%2520compared%2520to%2520a%2520global%2520sort%2520operation.%2520In%2520this%2520paper%252C%250Awe%2520present%2520a%2520novel%2520hierarchical%2520rasterization%2520approach%2520that%2520systematically%250Aresorts%2520and%2520culls%2520splats%2520with%2520minimal%2520processing%2520overhead.%2520Our%2520software%250Arasterizer%2520effectively%2520eliminates%2520popping%2520artifacts%2520and%2520view%2520inconsistencies%252C%250Aas%2520demonstrated%2520through%2520both%2520quantitative%2520and%2520qualitative%2520measurements.%250ASimultaneously%252C%2520our%2520method%2520mitigates%2520the%2520potential%2520for%2520cheating%2520view-dependent%250Aeffects%2520with%2520popping%252C%2520ensuring%2520a%2520more%2520authentic%2520representation.%2520Despite%2520the%250Aelimination%2520of%2520cheating%252C%2520our%2520approach%2520achieves%2520comparable%2520quantitative%2520results%250Afor%2520test%2520images%252C%2520while%2520increasing%2520the%2520consistency%2520for%2520novel%2520view%2520synthesis%2520in%250Amotion.%2520Due%2520to%2520its%2520design%252C%2520our%2520hierarchical%2520approach%2520is%2520only%25204%2525%2520slower%2520on%250Aaverage%2520than%2520the%2520original%2520Gaussian%2520Splatting.%2520Notably%252C%2520enforcing%2520consistency%250Aenables%2520a%2520reduction%2520in%2520the%2520number%2520of%2520Gaussians%2520by%2520approximately%2520half%2520with%250Anearly%2520identical%2520quality%2520and%2520view-consistency.%2520Consequently%252C%2520rendering%250Aperformance%2520is%2520nearly%2520doubled%252C%2520making%2520our%2520approach%25201.6x%2520faster%2520than%2520the%250Aoriginal%2520Gaussian%2520Splatting%252C%2520with%2520a%252050%2525%2520reduction%2520in%2520memory%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00525v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StopThePop%3A%20Sorted%20Gaussian%20Splatting%20for%20View-Consistent%20Real-time%0A%20%20Rendering&entry.906535625=Lukas%20Radl%20and%20Michael%20Steiner%20and%20Mathias%20Parger%20and%20Alexander%20Weinrauch%20and%20Bernhard%20Kerbl%20and%20Markus%20Steinberger&entry.1292438233=%20%20Gaussian%20Splatting%20has%20emerged%20as%20a%20prominent%20model%20for%20constructing%203D%0Arepresentations%20from%20images%20across%20diverse%20domains.%20However%2C%20the%20efficiency%20of%0Athe%203D%20Gaussian%20Splatting%20rendering%20pipeline%20relies%20on%20several%20simplifications.%0ANotably%2C%20reducing%20Gaussian%20to%202D%20splats%20with%20a%20single%20view-space%20depth%0Aintroduces%20popping%20and%20blending%20artifacts%20during%20view%20rotation.%20Addressing%20this%0Aissue%20requires%20accurate%20per-pixel%20depth%20computation%2C%20yet%20a%20full%20per-pixel%20sort%0Aproves%20excessively%20costly%20compared%20to%20a%20global%20sort%20operation.%20In%20this%20paper%2C%0Awe%20present%20a%20novel%20hierarchical%20rasterization%20approach%20that%20systematically%0Aresorts%20and%20culls%20splats%20with%20minimal%20processing%20overhead.%20Our%20software%0Arasterizer%20effectively%20eliminates%20popping%20artifacts%20and%20view%20inconsistencies%2C%0Aas%20demonstrated%20through%20both%20quantitative%20and%20qualitative%20measurements.%0ASimultaneously%2C%20our%20method%20mitigates%20the%20potential%20for%20cheating%20view-dependent%0Aeffects%20with%20popping%2C%20ensuring%20a%20more%20authentic%20representation.%20Despite%20the%0Aelimination%20of%20cheating%2C%20our%20approach%20achieves%20comparable%20quantitative%20results%0Afor%20test%20images%2C%20while%20increasing%20the%20consistency%20for%20novel%20view%20synthesis%20in%0Amotion.%20Due%20to%20its%20design%2C%20our%20hierarchical%20approach%20is%20only%204%25%20slower%20on%0Aaverage%20than%20the%20original%20Gaussian%20Splatting.%20Notably%2C%20enforcing%20consistency%0Aenables%20a%20reduction%20in%20the%20number%20of%20Gaussians%20by%20approximately%20half%20with%0Anearly%20identical%20quality%20and%20view-consistency.%20Consequently%2C%20rendering%0Aperformance%20is%20nearly%20doubled%2C%20making%20our%20approach%201.6x%20faster%20than%20the%0Aoriginal%20Gaussian%20Splatting%2C%20with%20a%2050%25%20reduction%20in%20memory%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00525v3&entry.124074799=Read"},
{"title": "AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation", "author": "Yukang Cao and Liang Pan and Kai Han and Kwan-Yee K. Wong and Ziwei Liu", "abstract": "  Recent advancements in diffusion models have led to significant improvements\nin the generation and animation of 4D full-body human-object interactions\n(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion\ngeneration, which is limited by the scarcity of realistic large-scale\ninteraction data. This constraint affects their ability to create everyday HOI\nscenes. This paper addresses this challenge using a zero-shot approach with a\npre-trained diffusion model. Despite this potential, achieving our goals is\ndifficult due to the diffusion model's lack of understanding of ''where'' and\n''how'' objects interact with the human body. To tackle these issues, we\nintroduce AvatarGO, a novel framework designed to generate animatable 4D HOI\nscenes directly from textual inputs. Specifically, 1) for the ''where''\nchallenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to\nidentify the contact body part from text prompts, ensuring precise\nrepresentation of human-object spatial relations. 2) For the ''how'' challenge,\nwe introduce correspondence-aware motion optimization that constructs motion\nfields for both human and object models using the linear blend skinning\nfunction from SMPL-X. Our framework not only generates coherent compositional\nmotions, but also exhibits greater robustness in handling penetration issues.\nExtensive experiments with existing methods validate AvatarGO's superior\ngeneration and animation capabilities on a variety of human-object pairs and\ndiverse poses. As the first attempt to synthesize 4D avatars with object\ninteractions, we hope AvatarGO could open new doors for human-centric 4D\ncontent creation.\n", "link": "http://arxiv.org/abs/2410.07164v1", "date": "2024-10-09", "relevancy": 3.2132, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6632}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.652}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AvatarGO%3A%20Zero-shot%204D%20Human-Object%20Interaction%20Generation%20and%20Animation&body=Title%3A%20AvatarGO%3A%20Zero-shot%204D%20Human-Object%20Interaction%20Generation%20and%20Animation%0AAuthor%3A%20Yukang%20Cao%20and%20Liang%20Pan%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20diffusion%20models%20have%20led%20to%20significant%20improvements%0Ain%20the%20generation%20and%20animation%20of%204D%20full-body%20human-object%20interactions%0A%28HOI%29.%20Nevertheless%2C%20existing%20methods%20primarily%20focus%20on%20SMPL-based%20motion%0Ageneration%2C%20which%20is%20limited%20by%20the%20scarcity%20of%20realistic%20large-scale%0Ainteraction%20data.%20This%20constraint%20affects%20their%20ability%20to%20create%20everyday%20HOI%0Ascenes.%20This%20paper%20addresses%20this%20challenge%20using%20a%20zero-shot%20approach%20with%20a%0Apre-trained%20diffusion%20model.%20Despite%20this%20potential%2C%20achieving%20our%20goals%20is%0Adifficult%20due%20to%20the%20diffusion%20model%27s%20lack%20of%20understanding%20of%20%27%27where%27%27%20and%0A%27%27how%27%27%20objects%20interact%20with%20the%20human%20body.%20To%20tackle%20these%20issues%2C%20we%0Aintroduce%20AvatarGO%2C%20a%20novel%20framework%20designed%20to%20generate%20animatable%204D%20HOI%0Ascenes%20directly%20from%20textual%20inputs.%20Specifically%2C%201%29%20for%20the%20%27%27where%27%27%0Achallenge%2C%20we%20propose%20LLM-guided%20contact%20retargeting%2C%20which%20employs%20Lang-SAM%20to%0Aidentify%20the%20contact%20body%20part%20from%20text%20prompts%2C%20ensuring%20precise%0Arepresentation%20of%20human-object%20spatial%20relations.%202%29%20For%20the%20%27%27how%27%27%20challenge%2C%0Awe%20introduce%20correspondence-aware%20motion%20optimization%20that%20constructs%20motion%0Afields%20for%20both%20human%20and%20object%20models%20using%20the%20linear%20blend%20skinning%0Afunction%20from%20SMPL-X.%20Our%20framework%20not%20only%20generates%20coherent%20compositional%0Amotions%2C%20but%20also%20exhibits%20greater%20robustness%20in%20handling%20penetration%20issues.%0AExtensive%20experiments%20with%20existing%20methods%20validate%20AvatarGO%27s%20superior%0Ageneration%20and%20animation%20capabilities%20on%20a%20variety%20of%20human-object%20pairs%20and%0Adiverse%20poses.%20As%20the%20first%20attempt%20to%20synthesize%204D%20avatars%20with%20object%0Ainteractions%2C%20we%20hope%20AvatarGO%20could%20open%20new%20doors%20for%20human-centric%204D%0Acontent%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvatarGO%253A%2520Zero-shot%25204D%2520Human-Object%2520Interaction%2520Generation%2520and%2520Animation%26entry.906535625%3DYukang%2520Cao%2520and%2520Liang%2520Pan%2520and%2520Kai%2520Han%2520and%2520Kwan-Yee%2520K.%2520Wong%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520diffusion%2520models%2520have%2520led%2520to%2520significant%2520improvements%250Ain%2520the%2520generation%2520and%2520animation%2520of%25204D%2520full-body%2520human-object%2520interactions%250A%2528HOI%2529.%2520Nevertheless%252C%2520existing%2520methods%2520primarily%2520focus%2520on%2520SMPL-based%2520motion%250Ageneration%252C%2520which%2520is%2520limited%2520by%2520the%2520scarcity%2520of%2520realistic%2520large-scale%250Ainteraction%2520data.%2520This%2520constraint%2520affects%2520their%2520ability%2520to%2520create%2520everyday%2520HOI%250Ascenes.%2520This%2520paper%2520addresses%2520this%2520challenge%2520using%2520a%2520zero-shot%2520approach%2520with%2520a%250Apre-trained%2520diffusion%2520model.%2520Despite%2520this%2520potential%252C%2520achieving%2520our%2520goals%2520is%250Adifficult%2520due%2520to%2520the%2520diffusion%2520model%2527s%2520lack%2520of%2520understanding%2520of%2520%2527%2527where%2527%2527%2520and%250A%2527%2527how%2527%2527%2520objects%2520interact%2520with%2520the%2520human%2520body.%2520To%2520tackle%2520these%2520issues%252C%2520we%250Aintroduce%2520AvatarGO%252C%2520a%2520novel%2520framework%2520designed%2520to%2520generate%2520animatable%25204D%2520HOI%250Ascenes%2520directly%2520from%2520textual%2520inputs.%2520Specifically%252C%25201%2529%2520for%2520the%2520%2527%2527where%2527%2527%250Achallenge%252C%2520we%2520propose%2520LLM-guided%2520contact%2520retargeting%252C%2520which%2520employs%2520Lang-SAM%2520to%250Aidentify%2520the%2520contact%2520body%2520part%2520from%2520text%2520prompts%252C%2520ensuring%2520precise%250Arepresentation%2520of%2520human-object%2520spatial%2520relations.%25202%2529%2520For%2520the%2520%2527%2527how%2527%2527%2520challenge%252C%250Awe%2520introduce%2520correspondence-aware%2520motion%2520optimization%2520that%2520constructs%2520motion%250Afields%2520for%2520both%2520human%2520and%2520object%2520models%2520using%2520the%2520linear%2520blend%2520skinning%250Afunction%2520from%2520SMPL-X.%2520Our%2520framework%2520not%2520only%2520generates%2520coherent%2520compositional%250Amotions%252C%2520but%2520also%2520exhibits%2520greater%2520robustness%2520in%2520handling%2520penetration%2520issues.%250AExtensive%2520experiments%2520with%2520existing%2520methods%2520validate%2520AvatarGO%2527s%2520superior%250Ageneration%2520and%2520animation%2520capabilities%2520on%2520a%2520variety%2520of%2520human-object%2520pairs%2520and%250Adiverse%2520poses.%2520As%2520the%2520first%2520attempt%2520to%2520synthesize%25204D%2520avatars%2520with%2520object%250Ainteractions%252C%2520we%2520hope%2520AvatarGO%2520could%2520open%2520new%2520doors%2520for%2520human-centric%25204D%250Acontent%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AvatarGO%3A%20Zero-shot%204D%20Human-Object%20Interaction%20Generation%20and%20Animation&entry.906535625=Yukang%20Cao%20and%20Liang%20Pan%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong%20and%20Ziwei%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20diffusion%20models%20have%20led%20to%20significant%20improvements%0Ain%20the%20generation%20and%20animation%20of%204D%20full-body%20human-object%20interactions%0A%28HOI%29.%20Nevertheless%2C%20existing%20methods%20primarily%20focus%20on%20SMPL-based%20motion%0Ageneration%2C%20which%20is%20limited%20by%20the%20scarcity%20of%20realistic%20large-scale%0Ainteraction%20data.%20This%20constraint%20affects%20their%20ability%20to%20create%20everyday%20HOI%0Ascenes.%20This%20paper%20addresses%20this%20challenge%20using%20a%20zero-shot%20approach%20with%20a%0Apre-trained%20diffusion%20model.%20Despite%20this%20potential%2C%20achieving%20our%20goals%20is%0Adifficult%20due%20to%20the%20diffusion%20model%27s%20lack%20of%20understanding%20of%20%27%27where%27%27%20and%0A%27%27how%27%27%20objects%20interact%20with%20the%20human%20body.%20To%20tackle%20these%20issues%2C%20we%0Aintroduce%20AvatarGO%2C%20a%20novel%20framework%20designed%20to%20generate%20animatable%204D%20HOI%0Ascenes%20directly%20from%20textual%20inputs.%20Specifically%2C%201%29%20for%20the%20%27%27where%27%27%0Achallenge%2C%20we%20propose%20LLM-guided%20contact%20retargeting%2C%20which%20employs%20Lang-SAM%20to%0Aidentify%20the%20contact%20body%20part%20from%20text%20prompts%2C%20ensuring%20precise%0Arepresentation%20of%20human-object%20spatial%20relations.%202%29%20For%20the%20%27%27how%27%27%20challenge%2C%0Awe%20introduce%20correspondence-aware%20motion%20optimization%20that%20constructs%20motion%0Afields%20for%20both%20human%20and%20object%20models%20using%20the%20linear%20blend%20skinning%0Afunction%20from%20SMPL-X.%20Our%20framework%20not%20only%20generates%20coherent%20compositional%0Amotions%2C%20but%20also%20exhibits%20greater%20robustness%20in%20handling%20penetration%20issues.%0AExtensive%20experiments%20with%20existing%20methods%20validate%20AvatarGO%27s%20superior%0Ageneration%20and%20animation%20capabilities%20on%20a%20variety%20of%20human-object%20pairs%20and%0Adiverse%20poses.%20As%20the%20first%20attempt%20to%20synthesize%204D%20avatars%20with%20object%0Ainteractions%2C%20we%20hope%20AvatarGO%20could%20open%20new%20doors%20for%20human-centric%204D%0Acontent%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07164v1&entry.124074799=Read"},
{"title": "Hi-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical\n  Gaussian Splatting", "author": "Boying Li and Zhixi Cai and Yuan-Fang Li and Ian Reid and Hamid Rezatofighi", "abstract": "  We propose Hi-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a\nnovel hierarchical categorical representation, which enables accurate global 3D\nsemantic mapping, scaling-up capability, and explicit semantic label prediction\nin the 3D world. The parameter usage in semantic SLAM systems increases\nsignificantly with the growing complexity of the environment, making it\nparticularly challenging and costly for scene understanding. To address this\nproblem, we introduce a novel hierarchical representation that encodes semantic\ninformation in a compact form into 3D Gaussian Splatting, leveraging the\ncapabilities of large language models (LLMs). We further introduce a novel\nsemantic loss designed to optimize hierarchical semantic information through\nboth inter-level and cross-level optimization. Furthermore, we enhance the\nwhole SLAM system, resulting in improved tracking and mapping performance. Our\nHi-SLAM outperforms existing dense SLAM methods in both mapping and tracking\naccuracy, while achieving a 2x operation speed-up. Additionally, it exhibits\ncompetitive performance in rendering semantic segmentation in small synthetic\nscenes, with significantly reduced storage and training time requirements.\nRendering FPS impressively reaches 2,000 with semantic information and 3,000\nwithout it. Most notably, it showcases the capability of handling the complex\nreal-world scene with more than 500 semantic classes, highlighting its valuable\nscaling-up capability.\n", "link": "http://arxiv.org/abs/2409.12518v2", "date": "2024-10-09", "relevancy": 3.1885, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7228}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6058}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hi-SLAM%3A%20Scaling-up%20Semantics%20in%20SLAM%20with%20a%20Hierarchically%20Categorical%0A%20%20Gaussian%20Splatting&body=Title%3A%20Hi-SLAM%3A%20Scaling-up%20Semantics%20in%20SLAM%20with%20a%20Hierarchically%20Categorical%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Boying%20Li%20and%20Zhixi%20Cai%20and%20Yuan-Fang%20Li%20and%20Ian%20Reid%20and%20Hamid%20Rezatofighi%0AAbstract%3A%20%20%20We%20propose%20Hi-SLAM%2C%20a%20semantic%203D%20Gaussian%20Splatting%20SLAM%20method%20featuring%20a%0Anovel%20hierarchical%20categorical%20representation%2C%20which%20enables%20accurate%20global%203D%0Asemantic%20mapping%2C%20scaling-up%20capability%2C%20and%20explicit%20semantic%20label%20prediction%0Ain%20the%203D%20world.%20The%20parameter%20usage%20in%20semantic%20SLAM%20systems%20increases%0Asignificantly%20with%20the%20growing%20complexity%20of%20the%20environment%2C%20making%20it%0Aparticularly%20challenging%20and%20costly%20for%20scene%20understanding.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20novel%20hierarchical%20representation%20that%20encodes%20semantic%0Ainformation%20in%20a%20compact%20form%20into%203D%20Gaussian%20Splatting%2C%20leveraging%20the%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20We%20further%20introduce%20a%20novel%0Asemantic%20loss%20designed%20to%20optimize%20hierarchical%20semantic%20information%20through%0Aboth%20inter-level%20and%20cross-level%20optimization.%20Furthermore%2C%20we%20enhance%20the%0Awhole%20SLAM%20system%2C%20resulting%20in%20improved%20tracking%20and%20mapping%20performance.%20Our%0AHi-SLAM%20outperforms%20existing%20dense%20SLAM%20methods%20in%20both%20mapping%20and%20tracking%0Aaccuracy%2C%20while%20achieving%20a%202x%20operation%20speed-up.%20Additionally%2C%20it%20exhibits%0Acompetitive%20performance%20in%20rendering%20semantic%20segmentation%20in%20small%20synthetic%0Ascenes%2C%20with%20significantly%20reduced%20storage%20and%20training%20time%20requirements.%0ARendering%20FPS%20impressively%20reaches%202%2C000%20with%20semantic%20information%20and%203%2C000%0Awithout%20it.%20Most%20notably%2C%20it%20showcases%20the%20capability%20of%20handling%20the%20complex%0Areal-world%20scene%20with%20more%20than%20500%20semantic%20classes%2C%20highlighting%20its%20valuable%0Ascaling-up%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12518v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHi-SLAM%253A%2520Scaling-up%2520Semantics%2520in%2520SLAM%2520with%2520a%2520Hierarchically%2520Categorical%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DBoying%2520Li%2520and%2520Zhixi%2520Cai%2520and%2520Yuan-Fang%2520Li%2520and%2520Ian%2520Reid%2520and%2520Hamid%2520Rezatofighi%26entry.1292438233%3D%2520%2520We%2520propose%2520Hi-SLAM%252C%2520a%2520semantic%25203D%2520Gaussian%2520Splatting%2520SLAM%2520method%2520featuring%2520a%250Anovel%2520hierarchical%2520categorical%2520representation%252C%2520which%2520enables%2520accurate%2520global%25203D%250Asemantic%2520mapping%252C%2520scaling-up%2520capability%252C%2520and%2520explicit%2520semantic%2520label%2520prediction%250Ain%2520the%25203D%2520world.%2520The%2520parameter%2520usage%2520in%2520semantic%2520SLAM%2520systems%2520increases%250Asignificantly%2520with%2520the%2520growing%2520complexity%2520of%2520the%2520environment%252C%2520making%2520it%250Aparticularly%2520challenging%2520and%2520costly%2520for%2520scene%2520understanding.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520introduce%2520a%2520novel%2520hierarchical%2520representation%2520that%2520encodes%2520semantic%250Ainformation%2520in%2520a%2520compact%2520form%2520into%25203D%2520Gaussian%2520Splatting%252C%2520leveraging%2520the%250Acapabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%2520further%2520introduce%2520a%2520novel%250Asemantic%2520loss%2520designed%2520to%2520optimize%2520hierarchical%2520semantic%2520information%2520through%250Aboth%2520inter-level%2520and%2520cross-level%2520optimization.%2520Furthermore%252C%2520we%2520enhance%2520the%250Awhole%2520SLAM%2520system%252C%2520resulting%2520in%2520improved%2520tracking%2520and%2520mapping%2520performance.%2520Our%250AHi-SLAM%2520outperforms%2520existing%2520dense%2520SLAM%2520methods%2520in%2520both%2520mapping%2520and%2520tracking%250Aaccuracy%252C%2520while%2520achieving%2520a%25202x%2520operation%2520speed-up.%2520Additionally%252C%2520it%2520exhibits%250Acompetitive%2520performance%2520in%2520rendering%2520semantic%2520segmentation%2520in%2520small%2520synthetic%250Ascenes%252C%2520with%2520significantly%2520reduced%2520storage%2520and%2520training%2520time%2520requirements.%250ARendering%2520FPS%2520impressively%2520reaches%25202%252C000%2520with%2520semantic%2520information%2520and%25203%252C000%250Awithout%2520it.%2520Most%2520notably%252C%2520it%2520showcases%2520the%2520capability%2520of%2520handling%2520the%2520complex%250Areal-world%2520scene%2520with%2520more%2520than%2520500%2520semantic%2520classes%252C%2520highlighting%2520its%2520valuable%250Ascaling-up%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12518v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hi-SLAM%3A%20Scaling-up%20Semantics%20in%20SLAM%20with%20a%20Hierarchically%20Categorical%0A%20%20Gaussian%20Splatting&entry.906535625=Boying%20Li%20and%20Zhixi%20Cai%20and%20Yuan-Fang%20Li%20and%20Ian%20Reid%20and%20Hamid%20Rezatofighi&entry.1292438233=%20%20We%20propose%20Hi-SLAM%2C%20a%20semantic%203D%20Gaussian%20Splatting%20SLAM%20method%20featuring%20a%0Anovel%20hierarchical%20categorical%20representation%2C%20which%20enables%20accurate%20global%203D%0Asemantic%20mapping%2C%20scaling-up%20capability%2C%20and%20explicit%20semantic%20label%20prediction%0Ain%20the%203D%20world.%20The%20parameter%20usage%20in%20semantic%20SLAM%20systems%20increases%0Asignificantly%20with%20the%20growing%20complexity%20of%20the%20environment%2C%20making%20it%0Aparticularly%20challenging%20and%20costly%20for%20scene%20understanding.%20To%20address%20this%0Aproblem%2C%20we%20introduce%20a%20novel%20hierarchical%20representation%20that%20encodes%20semantic%0Ainformation%20in%20a%20compact%20form%20into%203D%20Gaussian%20Splatting%2C%20leveraging%20the%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20We%20further%20introduce%20a%20novel%0Asemantic%20loss%20designed%20to%20optimize%20hierarchical%20semantic%20information%20through%0Aboth%20inter-level%20and%20cross-level%20optimization.%20Furthermore%2C%20we%20enhance%20the%0Awhole%20SLAM%20system%2C%20resulting%20in%20improved%20tracking%20and%20mapping%20performance.%20Our%0AHi-SLAM%20outperforms%20existing%20dense%20SLAM%20methods%20in%20both%20mapping%20and%20tracking%0Aaccuracy%2C%20while%20achieving%20a%202x%20operation%20speed-up.%20Additionally%2C%20it%20exhibits%0Acompetitive%20performance%20in%20rendering%20semantic%20segmentation%20in%20small%20synthetic%0Ascenes%2C%20with%20significantly%20reduced%20storage%20and%20training%20time%20requirements.%0ARendering%20FPS%20impressively%20reaches%202%2C000%20with%20semantic%20information%20and%203%2C000%0Awithout%20it.%20Most%20notably%2C%20it%20showcases%20the%20capability%20of%20handling%20the%20complex%0Areal-world%20scene%20with%20more%20than%20500%20semantic%20classes%2C%20highlighting%20its%20valuable%0Ascaling-up%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12518v2&entry.124074799=Read"},
{"title": "Thing2Reality: Transforming 2D Content into Conditioned Multiviews and\n  3D Gaussian Objects for XR Communication", "author": "Erzhen Hu and Mingyi Li and Jungtaek Hong and Xun Qian and Alex Olwal and David Kim and Seongkook Heo and Ruofei Du", "abstract": "  During remote communication, participants often share both digital and\nphysical content, such as product designs, digital assets, and environments, to\nenhance mutual understanding. Recent advances in augmented communication have\nfacilitated users to swiftly create and share digital 2D copies of physical\nobjects from video feeds into a shared space. However, conventional 2D\nrepresentations of digital objects restricts users' ability to spatially\nreference items in a shared immersive environment. To address this, we propose\nThing2Reality, an Extended Reality (XR) communication platform that enhances\nspontaneous discussions of both digital and physical items during remote\nsessions. With Thing2Reality, users can quickly materialize ideas or physical\nobjects in immersive environments and share them as conditioned multiview\nrenderings or 3D Gaussians. Thing2Reality enables users to interact with remote\nobjects or discuss concepts in a collaborative manner. Our user study revealed\nthat the ability to interact with and manipulate 3D representations of objects\nsignificantly enhances the efficiency of discussions, with the potential to\naugment discussion of 2D artifacts.\n", "link": "http://arxiv.org/abs/2410.07119v1", "date": "2024-10-09", "relevancy": 3.0548, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6295}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6295}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thing2Reality%3A%20Transforming%202D%20Content%20into%20Conditioned%20Multiviews%20and%0A%20%203D%20Gaussian%20Objects%20for%20XR%20Communication&body=Title%3A%20Thing2Reality%3A%20Transforming%202D%20Content%20into%20Conditioned%20Multiviews%20and%0A%20%203D%20Gaussian%20Objects%20for%20XR%20Communication%0AAuthor%3A%20Erzhen%20Hu%20and%20Mingyi%20Li%20and%20Jungtaek%20Hong%20and%20Xun%20Qian%20and%20Alex%20Olwal%20and%20David%20Kim%20and%20Seongkook%20Heo%20and%20Ruofei%20Du%0AAbstract%3A%20%20%20During%20remote%20communication%2C%20participants%20often%20share%20both%20digital%20and%0Aphysical%20content%2C%20such%20as%20product%20designs%2C%20digital%20assets%2C%20and%20environments%2C%20to%0Aenhance%20mutual%20understanding.%20Recent%20advances%20in%20augmented%20communication%20have%0Afacilitated%20users%20to%20swiftly%20create%20and%20share%20digital%202D%20copies%20of%20physical%0Aobjects%20from%20video%20feeds%20into%20a%20shared%20space.%20However%2C%20conventional%202D%0Arepresentations%20of%20digital%20objects%20restricts%20users%27%20ability%20to%20spatially%0Areference%20items%20in%20a%20shared%20immersive%20environment.%20To%20address%20this%2C%20we%20propose%0AThing2Reality%2C%20an%20Extended%20Reality%20%28XR%29%20communication%20platform%20that%20enhances%0Aspontaneous%20discussions%20of%20both%20digital%20and%20physical%20items%20during%20remote%0Asessions.%20With%20Thing2Reality%2C%20users%20can%20quickly%20materialize%20ideas%20or%20physical%0Aobjects%20in%20immersive%20environments%20and%20share%20them%20as%20conditioned%20multiview%0Arenderings%20or%203D%20Gaussians.%20Thing2Reality%20enables%20users%20to%20interact%20with%20remote%0Aobjects%20or%20discuss%20concepts%20in%20a%20collaborative%20manner.%20Our%20user%20study%20revealed%0Athat%20the%20ability%20to%20interact%20with%20and%20manipulate%203D%20representations%20of%20objects%0Asignificantly%20enhances%20the%20efficiency%20of%20discussions%2C%20with%20the%20potential%20to%0Aaugment%20discussion%20of%202D%20artifacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThing2Reality%253A%2520Transforming%25202D%2520Content%2520into%2520Conditioned%2520Multiviews%2520and%250A%2520%25203D%2520Gaussian%2520Objects%2520for%2520XR%2520Communication%26entry.906535625%3DErzhen%2520Hu%2520and%2520Mingyi%2520Li%2520and%2520Jungtaek%2520Hong%2520and%2520Xun%2520Qian%2520and%2520Alex%2520Olwal%2520and%2520David%2520Kim%2520and%2520Seongkook%2520Heo%2520and%2520Ruofei%2520Du%26entry.1292438233%3D%2520%2520During%2520remote%2520communication%252C%2520participants%2520often%2520share%2520both%2520digital%2520and%250Aphysical%2520content%252C%2520such%2520as%2520product%2520designs%252C%2520digital%2520assets%252C%2520and%2520environments%252C%2520to%250Aenhance%2520mutual%2520understanding.%2520Recent%2520advances%2520in%2520augmented%2520communication%2520have%250Afacilitated%2520users%2520to%2520swiftly%2520create%2520and%2520share%2520digital%25202D%2520copies%2520of%2520physical%250Aobjects%2520from%2520video%2520feeds%2520into%2520a%2520shared%2520space.%2520However%252C%2520conventional%25202D%250Arepresentations%2520of%2520digital%2520objects%2520restricts%2520users%2527%2520ability%2520to%2520spatially%250Areference%2520items%2520in%2520a%2520shared%2520immersive%2520environment.%2520To%2520address%2520this%252C%2520we%2520propose%250AThing2Reality%252C%2520an%2520Extended%2520Reality%2520%2528XR%2529%2520communication%2520platform%2520that%2520enhances%250Aspontaneous%2520discussions%2520of%2520both%2520digital%2520and%2520physical%2520items%2520during%2520remote%250Asessions.%2520With%2520Thing2Reality%252C%2520users%2520can%2520quickly%2520materialize%2520ideas%2520or%2520physical%250Aobjects%2520in%2520immersive%2520environments%2520and%2520share%2520them%2520as%2520conditioned%2520multiview%250Arenderings%2520or%25203D%2520Gaussians.%2520Thing2Reality%2520enables%2520users%2520to%2520interact%2520with%2520remote%250Aobjects%2520or%2520discuss%2520concepts%2520in%2520a%2520collaborative%2520manner.%2520Our%2520user%2520study%2520revealed%250Athat%2520the%2520ability%2520to%2520interact%2520with%2520and%2520manipulate%25203D%2520representations%2520of%2520objects%250Asignificantly%2520enhances%2520the%2520efficiency%2520of%2520discussions%252C%2520with%2520the%2520potential%2520to%250Aaugment%2520discussion%2520of%25202D%2520artifacts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thing2Reality%3A%20Transforming%202D%20Content%20into%20Conditioned%20Multiviews%20and%0A%20%203D%20Gaussian%20Objects%20for%20XR%20Communication&entry.906535625=Erzhen%20Hu%20and%20Mingyi%20Li%20and%20Jungtaek%20Hong%20and%20Xun%20Qian%20and%20Alex%20Olwal%20and%20David%20Kim%20and%20Seongkook%20Heo%20and%20Ruofei%20Du&entry.1292438233=%20%20During%20remote%20communication%2C%20participants%20often%20share%20both%20digital%20and%0Aphysical%20content%2C%20such%20as%20product%20designs%2C%20digital%20assets%2C%20and%20environments%2C%20to%0Aenhance%20mutual%20understanding.%20Recent%20advances%20in%20augmented%20communication%20have%0Afacilitated%20users%20to%20swiftly%20create%20and%20share%20digital%202D%20copies%20of%20physical%0Aobjects%20from%20video%20feeds%20into%20a%20shared%20space.%20However%2C%20conventional%202D%0Arepresentations%20of%20digital%20objects%20restricts%20users%27%20ability%20to%20spatially%0Areference%20items%20in%20a%20shared%20immersive%20environment.%20To%20address%20this%2C%20we%20propose%0AThing2Reality%2C%20an%20Extended%20Reality%20%28XR%29%20communication%20platform%20that%20enhances%0Aspontaneous%20discussions%20of%20both%20digital%20and%20physical%20items%20during%20remote%0Asessions.%20With%20Thing2Reality%2C%20users%20can%20quickly%20materialize%20ideas%20or%20physical%0Aobjects%20in%20immersive%20environments%20and%20share%20them%20as%20conditioned%20multiview%0Arenderings%20or%203D%20Gaussians.%20Thing2Reality%20enables%20users%20to%20interact%20with%20remote%0Aobjects%20or%20discuss%20concepts%20in%20a%20collaborative%20manner.%20Our%20user%20study%20revealed%0Athat%20the%20ability%20to%20interact%20with%20and%20manipulate%203D%20representations%20of%20objects%0Asignificantly%20enhances%20the%20efficiency%20of%20discussions%2C%20with%20the%20potential%20to%0Aaugment%20discussion%20of%202D%20artifacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07119v1&entry.124074799=Read"},
{"title": "Towards Interpreting Visual Information Processing in Vision-Language\n  Models", "author": "Clement Neo and Luke Ong and Philip Torr and Mor Geva and David Krueger and Fazl Barez", "abstract": "  Vision-Language Models (VLMs) are powerful tools for processing and\nunderstanding text and images. We study the processing of visual tokens in the\nlanguage model component of LLaVA, a prominent VLM. Our approach focuses on\nanalyzing the localization of object information, the evolution of visual token\nrepresentations across layers, and the mechanism of integrating visual\ninformation for predictions. Through ablation studies, we demonstrated that\nobject identification accuracy drops by over 70\\% when object-specific tokens\nare removed. We observed that visual token representations become increasingly\ninterpretable in the vocabulary space across layers, suggesting an alignment\nwith textual tokens corresponding to image content. Finally, we found that the\nmodel extracts object information from these refined representations at the\nlast token position for prediction, mirroring the process in text-only language\nmodels for factual association tasks. These findings provide crucial insights\ninto how VLMs process and integrate visual information, bridging the gap\nbetween our understanding of language and vision models, and paving the way for\nmore interpretable and controllable multimodal systems.\n", "link": "http://arxiv.org/abs/2410.07149v1", "date": "2024-10-09", "relevancy": 2.9892, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.621}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interpreting%20Visual%20Information%20Processing%20in%20Vision-Language%0A%20%20Models&body=Title%3A%20Towards%20Interpreting%20Visual%20Information%20Processing%20in%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Clement%20Neo%20and%20Luke%20Ong%20and%20Philip%20Torr%20and%20Mor%20Geva%20and%20David%20Krueger%20and%20Fazl%20Barez%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20are%20powerful%20tools%20for%20processing%20and%0Aunderstanding%20text%20and%20images.%20We%20study%20the%20processing%20of%20visual%20tokens%20in%20the%0Alanguage%20model%20component%20of%20LLaVA%2C%20a%20prominent%20VLM.%20Our%20approach%20focuses%20on%0Aanalyzing%20the%20localization%20of%20object%20information%2C%20the%20evolution%20of%20visual%20token%0Arepresentations%20across%20layers%2C%20and%20the%20mechanism%20of%20integrating%20visual%0Ainformation%20for%20predictions.%20Through%20ablation%20studies%2C%20we%20demonstrated%20that%0Aobject%20identification%20accuracy%20drops%20by%20over%2070%5C%25%20when%20object-specific%20tokens%0Aare%20removed.%20We%20observed%20that%20visual%20token%20representations%20become%20increasingly%0Ainterpretable%20in%20the%20vocabulary%20space%20across%20layers%2C%20suggesting%20an%20alignment%0Awith%20textual%20tokens%20corresponding%20to%20image%20content.%20Finally%2C%20we%20found%20that%20the%0Amodel%20extracts%20object%20information%20from%20these%20refined%20representations%20at%20the%0Alast%20token%20position%20for%20prediction%2C%20mirroring%20the%20process%20in%20text-only%20language%0Amodels%20for%20factual%20association%20tasks.%20These%20findings%20provide%20crucial%20insights%0Ainto%20how%20VLMs%20process%20and%20integrate%20visual%20information%2C%20bridging%20the%20gap%0Abetween%20our%20understanding%20of%20language%20and%20vision%20models%2C%20and%20paving%20the%20way%20for%0Amore%20interpretable%20and%20controllable%20multimodal%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interpreting%2520Visual%2520Information%2520Processing%2520in%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DClement%2520Neo%2520and%2520Luke%2520Ong%2520and%2520Philip%2520Torr%2520and%2520Mor%2520Geva%2520and%2520David%2520Krueger%2520and%2520Fazl%2520Barez%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%2520powerful%2520tools%2520for%2520processing%2520and%250Aunderstanding%2520text%2520and%2520images.%2520We%2520study%2520the%2520processing%2520of%2520visual%2520tokens%2520in%2520the%250Alanguage%2520model%2520component%2520of%2520LLaVA%252C%2520a%2520prominent%2520VLM.%2520Our%2520approach%2520focuses%2520on%250Aanalyzing%2520the%2520localization%2520of%2520object%2520information%252C%2520the%2520evolution%2520of%2520visual%2520token%250Arepresentations%2520across%2520layers%252C%2520and%2520the%2520mechanism%2520of%2520integrating%2520visual%250Ainformation%2520for%2520predictions.%2520Through%2520ablation%2520studies%252C%2520we%2520demonstrated%2520that%250Aobject%2520identification%2520accuracy%2520drops%2520by%2520over%252070%255C%2525%2520when%2520object-specific%2520tokens%250Aare%2520removed.%2520We%2520observed%2520that%2520visual%2520token%2520representations%2520become%2520increasingly%250Ainterpretable%2520in%2520the%2520vocabulary%2520space%2520across%2520layers%252C%2520suggesting%2520an%2520alignment%250Awith%2520textual%2520tokens%2520corresponding%2520to%2520image%2520content.%2520Finally%252C%2520we%2520found%2520that%2520the%250Amodel%2520extracts%2520object%2520information%2520from%2520these%2520refined%2520representations%2520at%2520the%250Alast%2520token%2520position%2520for%2520prediction%252C%2520mirroring%2520the%2520process%2520in%2520text-only%2520language%250Amodels%2520for%2520factual%2520association%2520tasks.%2520These%2520findings%2520provide%2520crucial%2520insights%250Ainto%2520how%2520VLMs%2520process%2520and%2520integrate%2520visual%2520information%252C%2520bridging%2520the%2520gap%250Abetween%2520our%2520understanding%2520of%2520language%2520and%2520vision%2520models%252C%2520and%2520paving%2520the%2520way%2520for%250Amore%2520interpretable%2520and%2520controllable%2520multimodal%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interpreting%20Visual%20Information%20Processing%20in%20Vision-Language%0A%20%20Models&entry.906535625=Clement%20Neo%20and%20Luke%20Ong%20and%20Philip%20Torr%20and%20Mor%20Geva%20and%20David%20Krueger%20and%20Fazl%20Barez&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20are%20powerful%20tools%20for%20processing%20and%0Aunderstanding%20text%20and%20images.%20We%20study%20the%20processing%20of%20visual%20tokens%20in%20the%0Alanguage%20model%20component%20of%20LLaVA%2C%20a%20prominent%20VLM.%20Our%20approach%20focuses%20on%0Aanalyzing%20the%20localization%20of%20object%20information%2C%20the%20evolution%20of%20visual%20token%0Arepresentations%20across%20layers%2C%20and%20the%20mechanism%20of%20integrating%20visual%0Ainformation%20for%20predictions.%20Through%20ablation%20studies%2C%20we%20demonstrated%20that%0Aobject%20identification%20accuracy%20drops%20by%20over%2070%5C%25%20when%20object-specific%20tokens%0Aare%20removed.%20We%20observed%20that%20visual%20token%20representations%20become%20increasingly%0Ainterpretable%20in%20the%20vocabulary%20space%20across%20layers%2C%20suggesting%20an%20alignment%0Awith%20textual%20tokens%20corresponding%20to%20image%20content.%20Finally%2C%20we%20found%20that%20the%0Amodel%20extracts%20object%20information%20from%20these%20refined%20representations%20at%20the%0Alast%20token%20position%20for%20prediction%2C%20mirroring%20the%20process%20in%20text-only%20language%0Amodels%20for%20factual%20association%20tasks.%20These%20findings%20provide%20crucial%20insights%0Ainto%20how%20VLMs%20process%20and%20integrate%20visual%20information%2C%20bridging%20the%20gap%0Abetween%20our%20understanding%20of%20language%20and%20vision%20models%2C%20and%20paving%20the%20way%20for%0Amore%20interpretable%20and%20controllable%20multimodal%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07149v1&entry.124074799=Read"},
{"title": "Compositional Entailment Learning for Hyperbolic Vision-Language Models", "author": "Avik Pal and Max van Spengler and Guido Maria D'Amely di Melendugno and Alessandro Flaborea and Fabio Galasso and Pascal Mettes", "abstract": "  Image-text representation learning forms a cornerstone in vision-language\nmodels, where pairs of images and textual descriptions are contrastively\naligned in a shared embedding space. Since visual and textual concepts are\nnaturally hierarchical, recent work has shown that hyperbolic space can serve\nas a high-potential manifold to learn vision-language representation with\nstrong downstream performance. In this work, for the first time we show how to\nfully leverage the innate hierarchical nature of hyperbolic embeddings by\nlooking beyond individual image-text pairs. We propose Compositional Entailment\nLearning for hyperbolic vision-language models. The idea is that an image is\nnot only described by a sentence but is itself a composition of multiple object\nboxes, each with their own textual description. Such information can be\nobtained freely by extracting nouns from sentences and using openly available\nlocalized grounding models. We show how to hierarchically organize images,\nimage boxes, and their textual descriptions through contrastive and\nentailment-based objectives. Empirical evaluation on a hyperbolic\nvision-language model trained with millions of image-text pairs shows that the\nproposed compositional learning approach outperforms conventional Euclidean\nCLIP learning, as well as recent hyperbolic alternatives, with better zero-shot\nand retrieval generalization and clearly stronger hierarchical performance.\n", "link": "http://arxiv.org/abs/2410.06912v1", "date": "2024-10-09", "relevancy": 2.9248, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compositional%20Entailment%20Learning%20for%20Hyperbolic%20Vision-Language%20Models&body=Title%3A%20Compositional%20Entailment%20Learning%20for%20Hyperbolic%20Vision-Language%20Models%0AAuthor%3A%20Avik%20Pal%20and%20Max%20van%20Spengler%20and%20Guido%20Maria%20D%27Amely%20di%20Melendugno%20and%20Alessandro%20Flaborea%20and%20Fabio%20Galasso%20and%20Pascal%20Mettes%0AAbstract%3A%20%20%20Image-text%20representation%20learning%20forms%20a%20cornerstone%20in%20vision-language%0Amodels%2C%20where%20pairs%20of%20images%20and%20textual%20descriptions%20are%20contrastively%0Aaligned%20in%20a%20shared%20embedding%20space.%20Since%20visual%20and%20textual%20concepts%20are%0Anaturally%20hierarchical%2C%20recent%20work%20has%20shown%20that%20hyperbolic%20space%20can%20serve%0Aas%20a%20high-potential%20manifold%20to%20learn%20vision-language%20representation%20with%0Astrong%20downstream%20performance.%20In%20this%20work%2C%20for%20the%20first%20time%20we%20show%20how%20to%0Afully%20leverage%20the%20innate%20hierarchical%20nature%20of%20hyperbolic%20embeddings%20by%0Alooking%20beyond%20individual%20image-text%20pairs.%20We%20propose%20Compositional%20Entailment%0ALearning%20for%20hyperbolic%20vision-language%20models.%20The%20idea%20is%20that%20an%20image%20is%0Anot%20only%20described%20by%20a%20sentence%20but%20is%20itself%20a%20composition%20of%20multiple%20object%0Aboxes%2C%20each%20with%20their%20own%20textual%20description.%20Such%20information%20can%20be%0Aobtained%20freely%20by%20extracting%20nouns%20from%20sentences%20and%20using%20openly%20available%0Alocalized%20grounding%20models.%20We%20show%20how%20to%20hierarchically%20organize%20images%2C%0Aimage%20boxes%2C%20and%20their%20textual%20descriptions%20through%20contrastive%20and%0Aentailment-based%20objectives.%20Empirical%20evaluation%20on%20a%20hyperbolic%0Avision-language%20model%20trained%20with%20millions%20of%20image-text%20pairs%20shows%20that%20the%0Aproposed%20compositional%20learning%20approach%20outperforms%20conventional%20Euclidean%0ACLIP%20learning%2C%20as%20well%20as%20recent%20hyperbolic%20alternatives%2C%20with%20better%20zero-shot%0Aand%20retrieval%20generalization%20and%20clearly%20stronger%20hierarchical%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompositional%2520Entailment%2520Learning%2520for%2520Hyperbolic%2520Vision-Language%2520Models%26entry.906535625%3DAvik%2520Pal%2520and%2520Max%2520van%2520Spengler%2520and%2520Guido%2520Maria%2520D%2527Amely%2520di%2520Melendugno%2520and%2520Alessandro%2520Flaborea%2520and%2520Fabio%2520Galasso%2520and%2520Pascal%2520Mettes%26entry.1292438233%3D%2520%2520Image-text%2520representation%2520learning%2520forms%2520a%2520cornerstone%2520in%2520vision-language%250Amodels%252C%2520where%2520pairs%2520of%2520images%2520and%2520textual%2520descriptions%2520are%2520contrastively%250Aaligned%2520in%2520a%2520shared%2520embedding%2520space.%2520Since%2520visual%2520and%2520textual%2520concepts%2520are%250Anaturally%2520hierarchical%252C%2520recent%2520work%2520has%2520shown%2520that%2520hyperbolic%2520space%2520can%2520serve%250Aas%2520a%2520high-potential%2520manifold%2520to%2520learn%2520vision-language%2520representation%2520with%250Astrong%2520downstream%2520performance.%2520In%2520this%2520work%252C%2520for%2520the%2520first%2520time%2520we%2520show%2520how%2520to%250Afully%2520leverage%2520the%2520innate%2520hierarchical%2520nature%2520of%2520hyperbolic%2520embeddings%2520by%250Alooking%2520beyond%2520individual%2520image-text%2520pairs.%2520We%2520propose%2520Compositional%2520Entailment%250ALearning%2520for%2520hyperbolic%2520vision-language%2520models.%2520The%2520idea%2520is%2520that%2520an%2520image%2520is%250Anot%2520only%2520described%2520by%2520a%2520sentence%2520but%2520is%2520itself%2520a%2520composition%2520of%2520multiple%2520object%250Aboxes%252C%2520each%2520with%2520their%2520own%2520textual%2520description.%2520Such%2520information%2520can%2520be%250Aobtained%2520freely%2520by%2520extracting%2520nouns%2520from%2520sentences%2520and%2520using%2520openly%2520available%250Alocalized%2520grounding%2520models.%2520We%2520show%2520how%2520to%2520hierarchically%2520organize%2520images%252C%250Aimage%2520boxes%252C%2520and%2520their%2520textual%2520descriptions%2520through%2520contrastive%2520and%250Aentailment-based%2520objectives.%2520Empirical%2520evaluation%2520on%2520a%2520hyperbolic%250Avision-language%2520model%2520trained%2520with%2520millions%2520of%2520image-text%2520pairs%2520shows%2520that%2520the%250Aproposed%2520compositional%2520learning%2520approach%2520outperforms%2520conventional%2520Euclidean%250ACLIP%2520learning%252C%2520as%2520well%2520as%2520recent%2520hyperbolic%2520alternatives%252C%2520with%2520better%2520zero-shot%250Aand%2520retrieval%2520generalization%2520and%2520clearly%2520stronger%2520hierarchical%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%20Entailment%20Learning%20for%20Hyperbolic%20Vision-Language%20Models&entry.906535625=Avik%20Pal%20and%20Max%20van%20Spengler%20and%20Guido%20Maria%20D%27Amely%20di%20Melendugno%20and%20Alessandro%20Flaborea%20and%20Fabio%20Galasso%20and%20Pascal%20Mettes&entry.1292438233=%20%20Image-text%20representation%20learning%20forms%20a%20cornerstone%20in%20vision-language%0Amodels%2C%20where%20pairs%20of%20images%20and%20textual%20descriptions%20are%20contrastively%0Aaligned%20in%20a%20shared%20embedding%20space.%20Since%20visual%20and%20textual%20concepts%20are%0Anaturally%20hierarchical%2C%20recent%20work%20has%20shown%20that%20hyperbolic%20space%20can%20serve%0Aas%20a%20high-potential%20manifold%20to%20learn%20vision-language%20representation%20with%0Astrong%20downstream%20performance.%20In%20this%20work%2C%20for%20the%20first%20time%20we%20show%20how%20to%0Afully%20leverage%20the%20innate%20hierarchical%20nature%20of%20hyperbolic%20embeddings%20by%0Alooking%20beyond%20individual%20image-text%20pairs.%20We%20propose%20Compositional%20Entailment%0ALearning%20for%20hyperbolic%20vision-language%20models.%20The%20idea%20is%20that%20an%20image%20is%0Anot%20only%20described%20by%20a%20sentence%20but%20is%20itself%20a%20composition%20of%20multiple%20object%0Aboxes%2C%20each%20with%20their%20own%20textual%20description.%20Such%20information%20can%20be%0Aobtained%20freely%20by%20extracting%20nouns%20from%20sentences%20and%20using%20openly%20available%0Alocalized%20grounding%20models.%20We%20show%20how%20to%20hierarchically%20organize%20images%2C%0Aimage%20boxes%2C%20and%20their%20textual%20descriptions%20through%20contrastive%20and%0Aentailment-based%20objectives.%20Empirical%20evaluation%20on%20a%20hyperbolic%0Avision-language%20model%20trained%20with%20millions%20of%20image-text%20pairs%20shows%20that%20the%0Aproposed%20compositional%20learning%20approach%20outperforms%20conventional%20Euclidean%0ACLIP%20learning%2C%20as%20well%20as%20recent%20hyperbolic%20alternatives%2C%20with%20better%20zero-shot%0Aand%20retrieval%20generalization%20and%20clearly%20stronger%20hierarchical%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06912v1&entry.124074799=Read"},
{"title": "Do better language models have crisper vision?", "author": "Jona Ruthardt and Gertjan J. Burghouts and Serge Belongie and Yuki M. Asano", "abstract": "  How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.\n", "link": "http://arxiv.org/abs/2410.07173v1", "date": "2024-10-09", "relevancy": 2.9099, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20better%20language%20models%20have%20crisper%20vision%3F&body=Title%3A%20Do%20better%20language%20models%20have%20crisper%20vision%3F%0AAuthor%3A%20Jona%20Ruthardt%20and%20Gertjan%20J.%20Burghouts%20and%20Serge%20Belongie%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20How%20well%20do%20text-only%20Large%20Language%20Models%20%28LLMs%29%20grasp%20the%20visual%20world%3F%20As%0ALLMs%20are%20increasingly%20used%20in%20computer%20vision%2C%20addressing%20this%20question%20becomes%0Aboth%20fundamental%20and%20pertinent.%20However%2C%20existing%20studies%20have%20primarily%0Afocused%20on%20limited%20scenarios%2C%20such%20as%20their%20ability%20to%20generate%20visual%20content%0Aor%20cluster%20multimodal%20data.%20To%20this%20end%2C%20we%20propose%20the%20Visual%20Text%0ARepresentation%20Benchmark%20%28ViTeRB%29%20to%20isolate%20key%20properties%20that%20make%20language%0Amodels%20well-aligned%20with%20the%20visual%20world.%20With%20this%2C%20we%20identify%20large-scale%0Adecoder-based%20LLMs%20as%20ideal%20candidates%20for%20representing%20text%20in%20vision-centric%0Acontexts%2C%20counter%20to%20the%20current%20practice%20of%20utilizing%20text%20encoders.%20Building%0Aon%20these%20findings%2C%20we%20propose%20ShareLock%2C%20an%20ultra-lightweight%20CLIP-like%20model.%0ABy%20leveraging%20precomputable%20frozen%20features%20from%20strong%20vision%20and%20language%0Amodels%2C%20ShareLock%20achieves%20an%20impressive%2051%25%20accuracy%20on%20ImageNet%20despite%0Autilizing%20just%20563k%20image-caption%20pairs.%20Moreover%2C%20training%20requires%20only%201%20GPU%0Ahour%20%28or%2010%20hours%20including%20the%20precomputation%20of%20features%29%20-%20orders%20of%0Amagnitude%20less%20than%20prior%20methods.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520better%2520language%2520models%2520have%2520crisper%2520vision%253F%26entry.906535625%3DJona%2520Ruthardt%2520and%2520Gertjan%2520J.%2520Burghouts%2520and%2520Serge%2520Belongie%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520How%2520well%2520do%2520text-only%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520grasp%2520the%2520visual%2520world%253F%2520As%250ALLMs%2520are%2520increasingly%2520used%2520in%2520computer%2520vision%252C%2520addressing%2520this%2520question%2520becomes%250Aboth%2520fundamental%2520and%2520pertinent.%2520However%252C%2520existing%2520studies%2520have%2520primarily%250Afocused%2520on%2520limited%2520scenarios%252C%2520such%2520as%2520their%2520ability%2520to%2520generate%2520visual%2520content%250Aor%2520cluster%2520multimodal%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520the%2520Visual%2520Text%250ARepresentation%2520Benchmark%2520%2528ViTeRB%2529%2520to%2520isolate%2520key%2520properties%2520that%2520make%2520language%250Amodels%2520well-aligned%2520with%2520the%2520visual%2520world.%2520With%2520this%252C%2520we%2520identify%2520large-scale%250Adecoder-based%2520LLMs%2520as%2520ideal%2520candidates%2520for%2520representing%2520text%2520in%2520vision-centric%250Acontexts%252C%2520counter%2520to%2520the%2520current%2520practice%2520of%2520utilizing%2520text%2520encoders.%2520Building%250Aon%2520these%2520findings%252C%2520we%2520propose%2520ShareLock%252C%2520an%2520ultra-lightweight%2520CLIP-like%2520model.%250ABy%2520leveraging%2520precomputable%2520frozen%2520features%2520from%2520strong%2520vision%2520and%2520language%250Amodels%252C%2520ShareLock%2520achieves%2520an%2520impressive%252051%2525%2520accuracy%2520on%2520ImageNet%2520despite%250Autilizing%2520just%2520563k%2520image-caption%2520pairs.%2520Moreover%252C%2520training%2520requires%2520only%25201%2520GPU%250Ahour%2520%2528or%252010%2520hours%2520including%2520the%2520precomputation%2520of%2520features%2529%2520-%2520orders%2520of%250Amagnitude%2520less%2520than%2520prior%2520methods.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20better%20language%20models%20have%20crisper%20vision%3F&entry.906535625=Jona%20Ruthardt%20and%20Gertjan%20J.%20Burghouts%20and%20Serge%20Belongie%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20How%20well%20do%20text-only%20Large%20Language%20Models%20%28LLMs%29%20grasp%20the%20visual%20world%3F%20As%0ALLMs%20are%20increasingly%20used%20in%20computer%20vision%2C%20addressing%20this%20question%20becomes%0Aboth%20fundamental%20and%20pertinent.%20However%2C%20existing%20studies%20have%20primarily%0Afocused%20on%20limited%20scenarios%2C%20such%20as%20their%20ability%20to%20generate%20visual%20content%0Aor%20cluster%20multimodal%20data.%20To%20this%20end%2C%20we%20propose%20the%20Visual%20Text%0ARepresentation%20Benchmark%20%28ViTeRB%29%20to%20isolate%20key%20properties%20that%20make%20language%0Amodels%20well-aligned%20with%20the%20visual%20world.%20With%20this%2C%20we%20identify%20large-scale%0Adecoder-based%20LLMs%20as%20ideal%20candidates%20for%20representing%20text%20in%20vision-centric%0Acontexts%2C%20counter%20to%20the%20current%20practice%20of%20utilizing%20text%20encoders.%20Building%0Aon%20these%20findings%2C%20we%20propose%20ShareLock%2C%20an%20ultra-lightweight%20CLIP-like%20model.%0ABy%20leveraging%20precomputable%20frozen%20features%20from%20strong%20vision%20and%20language%0Amodels%2C%20ShareLock%20achieves%20an%20impressive%2051%25%20accuracy%20on%20ImageNet%20despite%0Autilizing%20just%20563k%20image-caption%20pairs.%20Moreover%2C%20training%20requires%20only%201%20GPU%0Ahour%20%28or%2010%20hours%20including%20the%20precomputation%20of%20features%29%20-%20orders%20of%0Amagnitude%20less%20than%20prior%20methods.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07173v1&entry.124074799=Read"},
{"title": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM", "author": "Xuchen Li and Shiyu Hu and Xiaokun Feng and Dailing Zhang and Meiqi Wu and Jing Zhang and Kaiqi Huang", "abstract": "  Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.\n", "link": "http://arxiv.org/abs/2410.02492v2", "date": "2024-10-09", "relevancy": 2.9073, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5936}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DTVLT%3A%20A%20Multi-modal%20Diverse%20Text%20Benchmark%20for%20Visual%20Language%20Tracking%0A%20%20Based%20on%20LLM&body=Title%3A%20DTVLT%3A%20A%20Multi-modal%20Diverse%20Text%20Benchmark%20for%20Visual%20Language%20Tracking%0A%20%20Based%20on%20LLM%0AAuthor%3A%20Xuchen%20Li%20and%20Shiyu%20Hu%20and%20Xiaokun%20Feng%20and%20Dailing%20Zhang%20and%20Meiqi%20Wu%20and%20Jing%20Zhang%20and%20Kaiqi%20Huang%0AAbstract%3A%20%20%20Visual%20language%20tracking%20%28VLT%29%20has%20emerged%20as%20a%20cutting-edge%20research%20area%2C%0Aharnessing%20linguistic%20data%20to%20enhance%20algorithms%20with%20multi-modal%20inputs%20and%0Abroadening%20the%20scope%20of%20traditional%20single%20object%20tracking%20%28SOT%29%20to%20encompass%0Avideo%20understanding%20applications.%20Despite%20this%2C%20most%20VLT%20benchmarks%20still%0Adepend%20on%20succinct%2C%20human-annotated%20text%20descriptions%20for%20each%20video.%20These%0Adescriptions%20often%20fall%20short%20in%20capturing%20the%20nuances%20of%20video%20content%0Adynamics%20and%20lack%20stylistic%20variety%20in%20language%2C%20constrained%20by%20their%20uniform%0Alevel%20of%20detail%20and%20a%20fixed%20annotation%20frequency.%20As%20a%20result%2C%20algorithms%20tend%0Ato%20default%20to%20a%20%22memorize%20the%20answer%22%20strategy%2C%20diverging%20from%20the%20core%0Aobjective%20of%20achieving%20a%20deeper%20understanding%20of%20video%20content.%20Fortunately%2C%0Athe%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20enabled%20the%20generation%20of%0Adiverse%20text.%20This%20work%20utilizes%20LLMs%20to%20generate%20varied%20semantic%20annotations%0A%28in%20terms%20of%20text%20lengths%20and%20granularities%29%20for%20representative%20SOT%20benchmarks%2C%0Athereby%20establishing%20a%20novel%20multi-modal%20benchmark.%20Specifically%2C%20we%20%281%29%0Apropose%20a%20new%20visual%20language%20tracking%20benchmark%20with%20diverse%20texts%2C%20named%0ADTVLT%2C%20based%20on%20five%20prominent%20VLT%20and%20SOT%20benchmarks%2C%20including%20three%0Asub-tasks%3A%20short-term%20tracking%2C%20long-term%20tracking%2C%20and%20global%20instance%0Atracking.%20%282%29%20We%20offer%20four%20granularity%20texts%20in%20our%20benchmark%2C%20considering%20the%0Aextent%20and%20density%20of%20semantic%20information.%20We%20expect%20this%20multi-granular%0Ageneration%20strategy%20to%20foster%20a%20favorable%20environment%20for%20VLT%20and%20video%0Aunderstanding%20research.%20%283%29%20We%20conduct%20comprehensive%20experimental%20analyses%20on%0ADTVLT%2C%20evaluating%20the%20impact%20of%20diverse%20text%20on%20tracking%20performance%20and%20hope%0Athe%20identified%20performance%20bottlenecks%20of%20existing%20algorithms%20can%20support%0Afurther%20research%20in%20VLT%20and%20video%20understanding.%20The%20proposed%20benchmark%2C%0Aexperimental%20results%20and%20toolkit%20will%20be%20released%20gradually%20on%0Ahttp%3A//videocube.aitestunion.com/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDTVLT%253A%2520A%2520Multi-modal%2520Diverse%2520Text%2520Benchmark%2520for%2520Visual%2520Language%2520Tracking%250A%2520%2520Based%2520on%2520LLM%26entry.906535625%3DXuchen%2520Li%2520and%2520Shiyu%2520Hu%2520and%2520Xiaokun%2520Feng%2520and%2520Dailing%2520Zhang%2520and%2520Meiqi%2520Wu%2520and%2520Jing%2520Zhang%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3D%2520%2520Visual%2520language%2520tracking%2520%2528VLT%2529%2520has%2520emerged%2520as%2520a%2520cutting-edge%2520research%2520area%252C%250Aharnessing%2520linguistic%2520data%2520to%2520enhance%2520algorithms%2520with%2520multi-modal%2520inputs%2520and%250Abroadening%2520the%2520scope%2520of%2520traditional%2520single%2520object%2520tracking%2520%2528SOT%2529%2520to%2520encompass%250Avideo%2520understanding%2520applications.%2520Despite%2520this%252C%2520most%2520VLT%2520benchmarks%2520still%250Adepend%2520on%2520succinct%252C%2520human-annotated%2520text%2520descriptions%2520for%2520each%2520video.%2520These%250Adescriptions%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520nuances%2520of%2520video%2520content%250Adynamics%2520and%2520lack%2520stylistic%2520variety%2520in%2520language%252C%2520constrained%2520by%2520their%2520uniform%250Alevel%2520of%2520detail%2520and%2520a%2520fixed%2520annotation%2520frequency.%2520As%2520a%2520result%252C%2520algorithms%2520tend%250Ato%2520default%2520to%2520a%2520%2522memorize%2520the%2520answer%2522%2520strategy%252C%2520diverging%2520from%2520the%2520core%250Aobjective%2520of%2520achieving%2520a%2520deeper%2520understanding%2520of%2520video%2520content.%2520Fortunately%252C%250Athe%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520enabled%2520the%2520generation%2520of%250Adiverse%2520text.%2520This%2520work%2520utilizes%2520LLMs%2520to%2520generate%2520varied%2520semantic%2520annotations%250A%2528in%2520terms%2520of%2520text%2520lengths%2520and%2520granularities%2529%2520for%2520representative%2520SOT%2520benchmarks%252C%250Athereby%2520establishing%2520a%2520novel%2520multi-modal%2520benchmark.%2520Specifically%252C%2520we%2520%25281%2529%250Apropose%2520a%2520new%2520visual%2520language%2520tracking%2520benchmark%2520with%2520diverse%2520texts%252C%2520named%250ADTVLT%252C%2520based%2520on%2520five%2520prominent%2520VLT%2520and%2520SOT%2520benchmarks%252C%2520including%2520three%250Asub-tasks%253A%2520short-term%2520tracking%252C%2520long-term%2520tracking%252C%2520and%2520global%2520instance%250Atracking.%2520%25282%2529%2520We%2520offer%2520four%2520granularity%2520texts%2520in%2520our%2520benchmark%252C%2520considering%2520the%250Aextent%2520and%2520density%2520of%2520semantic%2520information.%2520We%2520expect%2520this%2520multi-granular%250Ageneration%2520strategy%2520to%2520foster%2520a%2520favorable%2520environment%2520for%2520VLT%2520and%2520video%250Aunderstanding%2520research.%2520%25283%2529%2520We%2520conduct%2520comprehensive%2520experimental%2520analyses%2520on%250ADTVLT%252C%2520evaluating%2520the%2520impact%2520of%2520diverse%2520text%2520on%2520tracking%2520performance%2520and%2520hope%250Athe%2520identified%2520performance%2520bottlenecks%2520of%2520existing%2520algorithms%2520can%2520support%250Afurther%2520research%2520in%2520VLT%2520and%2520video%2520understanding.%2520The%2520proposed%2520benchmark%252C%250Aexperimental%2520results%2520and%2520toolkit%2520will%2520be%2520released%2520gradually%2520on%250Ahttp%253A//videocube.aitestunion.com/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DTVLT%3A%20A%20Multi-modal%20Diverse%20Text%20Benchmark%20for%20Visual%20Language%20Tracking%0A%20%20Based%20on%20LLM&entry.906535625=Xuchen%20Li%20and%20Shiyu%20Hu%20and%20Xiaokun%20Feng%20and%20Dailing%20Zhang%20and%20Meiqi%20Wu%20and%20Jing%20Zhang%20and%20Kaiqi%20Huang&entry.1292438233=%20%20Visual%20language%20tracking%20%28VLT%29%20has%20emerged%20as%20a%20cutting-edge%20research%20area%2C%0Aharnessing%20linguistic%20data%20to%20enhance%20algorithms%20with%20multi-modal%20inputs%20and%0Abroadening%20the%20scope%20of%20traditional%20single%20object%20tracking%20%28SOT%29%20to%20encompass%0Avideo%20understanding%20applications.%20Despite%20this%2C%20most%20VLT%20benchmarks%20still%0Adepend%20on%20succinct%2C%20human-annotated%20text%20descriptions%20for%20each%20video.%20These%0Adescriptions%20often%20fall%20short%20in%20capturing%20the%20nuances%20of%20video%20content%0Adynamics%20and%20lack%20stylistic%20variety%20in%20language%2C%20constrained%20by%20their%20uniform%0Alevel%20of%20detail%20and%20a%20fixed%20annotation%20frequency.%20As%20a%20result%2C%20algorithms%20tend%0Ato%20default%20to%20a%20%22memorize%20the%20answer%22%20strategy%2C%20diverging%20from%20the%20core%0Aobjective%20of%20achieving%20a%20deeper%20understanding%20of%20video%20content.%20Fortunately%2C%0Athe%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20enabled%20the%20generation%20of%0Adiverse%20text.%20This%20work%20utilizes%20LLMs%20to%20generate%20varied%20semantic%20annotations%0A%28in%20terms%20of%20text%20lengths%20and%20granularities%29%20for%20representative%20SOT%20benchmarks%2C%0Athereby%20establishing%20a%20novel%20multi-modal%20benchmark.%20Specifically%2C%20we%20%281%29%0Apropose%20a%20new%20visual%20language%20tracking%20benchmark%20with%20diverse%20texts%2C%20named%0ADTVLT%2C%20based%20on%20five%20prominent%20VLT%20and%20SOT%20benchmarks%2C%20including%20three%0Asub-tasks%3A%20short-term%20tracking%2C%20long-term%20tracking%2C%20and%20global%20instance%0Atracking.%20%282%29%20We%20offer%20four%20granularity%20texts%20in%20our%20benchmark%2C%20considering%20the%0Aextent%20and%20density%20of%20semantic%20information.%20We%20expect%20this%20multi-granular%0Ageneration%20strategy%20to%20foster%20a%20favorable%20environment%20for%20VLT%20and%20video%0Aunderstanding%20research.%20%283%29%20We%20conduct%20comprehensive%20experimental%20analyses%20on%0ADTVLT%2C%20evaluating%20the%20impact%20of%20diverse%20text%20on%20tracking%20performance%20and%20hope%0Athe%20identified%20performance%20bottlenecks%20of%20existing%20algorithms%20can%20support%0Afurther%20research%20in%20VLT%20and%20video%20understanding.%20The%20proposed%20benchmark%2C%0Aexperimental%20results%20and%20toolkit%20will%20be%20released%20gradually%20on%0Ahttp%3A//videocube.aitestunion.com/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02492v2&entry.124074799=Read"},
{"title": "GMSR:Gradient-Guided Mamba for Spectral Reconstruction from RGB Images", "author": "Xinying Wang and Zhixiong Huang and Sifan Zhang and Jiawen Zhu and Paolo Gamba and Lin Feng", "abstract": "  Mainstream approaches to spectral reconstruction (SR) primarily focus on\ndesigning Convolution- and Transformer-based architectures. However, CNN\nmethods often face challenges in handling long-range dependencies, whereas\nTransformers are constrained by computational efficiency limitations. Recent\nbreakthroughs in state-space model (e.g., Mamba) has attracted significant\nattention due to its near-linear computational efficiency and superior\nperformance, prompting our investigation into its potential for SR problem. To\nthis end, we propose the Gradient-guided Mamba for Spectral Reconstruction from\nRGB Images, dubbed GMSR-Net. GMSR-Net is a lightweight model characterized by a\nglobal receptive field and linear computational complexity. Its core comprises\nmultiple stacked Gradient Mamba (GM) blocks, each featuring a tri-branch\nstructure. In addition to benefiting from efficient global feature\nrepresentation by Mamba block, we further innovatively introduce spatial\ngradient attention and spectral gradient attention to guide the reconstruction\nof spatial and spectral cues. GMSR-Net demonstrates a significant\naccuracy-efficiency trade-off, achieving state-of-the-art performance while\nmarkedly reducing the number of parameters and computational burdens. Compared\nto existing approaches, GMSR-Net slashes parameters and FLOPS by substantial\nmargins of 10 times and 20 times, respectively. Code is available at\nhttps://github.com/wxy11-27/GMSR.\n", "link": "http://arxiv.org/abs/2405.07777v2", "date": "2024-10-09", "relevancy": 2.8867, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6295}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5653}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMSR%3AGradient-Guided%20Mamba%20for%20Spectral%20Reconstruction%20from%20RGB%20Images&body=Title%3A%20GMSR%3AGradient-Guided%20Mamba%20for%20Spectral%20Reconstruction%20from%20RGB%20Images%0AAuthor%3A%20Xinying%20Wang%20and%20Zhixiong%20Huang%20and%20Sifan%20Zhang%20and%20Jiawen%20Zhu%20and%20Paolo%20Gamba%20and%20Lin%20Feng%0AAbstract%3A%20%20%20Mainstream%20approaches%20to%20spectral%20reconstruction%20%28SR%29%20primarily%20focus%20on%0Adesigning%20Convolution-%20and%20Transformer-based%20architectures.%20However%2C%20CNN%0Amethods%20often%20face%20challenges%20in%20handling%20long-range%20dependencies%2C%20whereas%0ATransformers%20are%20constrained%20by%20computational%20efficiency%20limitations.%20Recent%0Abreakthroughs%20in%20state-space%20model%20%28e.g.%2C%20Mamba%29%20has%20attracted%20significant%0Aattention%20due%20to%20its%20near-linear%20computational%20efficiency%20and%20superior%0Aperformance%2C%20prompting%20our%20investigation%20into%20its%20potential%20for%20SR%20problem.%20To%0Athis%20end%2C%20we%20propose%20the%20Gradient-guided%20Mamba%20for%20Spectral%20Reconstruction%20from%0ARGB%20Images%2C%20dubbed%20GMSR-Net.%20GMSR-Net%20is%20a%20lightweight%20model%20characterized%20by%20a%0Aglobal%20receptive%20field%20and%20linear%20computational%20complexity.%20Its%20core%20comprises%0Amultiple%20stacked%20Gradient%20Mamba%20%28GM%29%20blocks%2C%20each%20featuring%20a%20tri-branch%0Astructure.%20In%20addition%20to%20benefiting%20from%20efficient%20global%20feature%0Arepresentation%20by%20Mamba%20block%2C%20we%20further%20innovatively%20introduce%20spatial%0Agradient%20attention%20and%20spectral%20gradient%20attention%20to%20guide%20the%20reconstruction%0Aof%20spatial%20and%20spectral%20cues.%20GMSR-Net%20demonstrates%20a%20significant%0Aaccuracy-efficiency%20trade-off%2C%20achieving%20state-of-the-art%20performance%20while%0Amarkedly%20reducing%20the%20number%20of%20parameters%20and%20computational%20burdens.%20Compared%0Ato%20existing%20approaches%2C%20GMSR-Net%20slashes%20parameters%20and%20FLOPS%20by%20substantial%0Amargins%20of%2010%20times%20and%2020%20times%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/wxy11-27/GMSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMSR%253AGradient-Guided%2520Mamba%2520for%2520Spectral%2520Reconstruction%2520from%2520RGB%2520Images%26entry.906535625%3DXinying%2520Wang%2520and%2520Zhixiong%2520Huang%2520and%2520Sifan%2520Zhang%2520and%2520Jiawen%2520Zhu%2520and%2520Paolo%2520Gamba%2520and%2520Lin%2520Feng%26entry.1292438233%3D%2520%2520Mainstream%2520approaches%2520to%2520spectral%2520reconstruction%2520%2528SR%2529%2520primarily%2520focus%2520on%250Adesigning%2520Convolution-%2520and%2520Transformer-based%2520architectures.%2520However%252C%2520CNN%250Amethods%2520often%2520face%2520challenges%2520in%2520handling%2520long-range%2520dependencies%252C%2520whereas%250ATransformers%2520are%2520constrained%2520by%2520computational%2520efficiency%2520limitations.%2520Recent%250Abreakthroughs%2520in%2520state-space%2520model%2520%2528e.g.%252C%2520Mamba%2529%2520has%2520attracted%2520significant%250Aattention%2520due%2520to%2520its%2520near-linear%2520computational%2520efficiency%2520and%2520superior%250Aperformance%252C%2520prompting%2520our%2520investigation%2520into%2520its%2520potential%2520for%2520SR%2520problem.%2520To%250Athis%2520end%252C%2520we%2520propose%2520the%2520Gradient-guided%2520Mamba%2520for%2520Spectral%2520Reconstruction%2520from%250ARGB%2520Images%252C%2520dubbed%2520GMSR-Net.%2520GMSR-Net%2520is%2520a%2520lightweight%2520model%2520characterized%2520by%2520a%250Aglobal%2520receptive%2520field%2520and%2520linear%2520computational%2520complexity.%2520Its%2520core%2520comprises%250Amultiple%2520stacked%2520Gradient%2520Mamba%2520%2528GM%2529%2520blocks%252C%2520each%2520featuring%2520a%2520tri-branch%250Astructure.%2520In%2520addition%2520to%2520benefiting%2520from%2520efficient%2520global%2520feature%250Arepresentation%2520by%2520Mamba%2520block%252C%2520we%2520further%2520innovatively%2520introduce%2520spatial%250Agradient%2520attention%2520and%2520spectral%2520gradient%2520attention%2520to%2520guide%2520the%2520reconstruction%250Aof%2520spatial%2520and%2520spectral%2520cues.%2520GMSR-Net%2520demonstrates%2520a%2520significant%250Aaccuracy-efficiency%2520trade-off%252C%2520achieving%2520state-of-the-art%2520performance%2520while%250Amarkedly%2520reducing%2520the%2520number%2520of%2520parameters%2520and%2520computational%2520burdens.%2520Compared%250Ato%2520existing%2520approaches%252C%2520GMSR-Net%2520slashes%2520parameters%2520and%2520FLOPS%2520by%2520substantial%250Amargins%2520of%252010%2520times%2520and%252020%2520times%252C%2520respectively.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/wxy11-27/GMSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMSR%3AGradient-Guided%20Mamba%20for%20Spectral%20Reconstruction%20from%20RGB%20Images&entry.906535625=Xinying%20Wang%20and%20Zhixiong%20Huang%20and%20Sifan%20Zhang%20and%20Jiawen%20Zhu%20and%20Paolo%20Gamba%20and%20Lin%20Feng&entry.1292438233=%20%20Mainstream%20approaches%20to%20spectral%20reconstruction%20%28SR%29%20primarily%20focus%20on%0Adesigning%20Convolution-%20and%20Transformer-based%20architectures.%20However%2C%20CNN%0Amethods%20often%20face%20challenges%20in%20handling%20long-range%20dependencies%2C%20whereas%0ATransformers%20are%20constrained%20by%20computational%20efficiency%20limitations.%20Recent%0Abreakthroughs%20in%20state-space%20model%20%28e.g.%2C%20Mamba%29%20has%20attracted%20significant%0Aattention%20due%20to%20its%20near-linear%20computational%20efficiency%20and%20superior%0Aperformance%2C%20prompting%20our%20investigation%20into%20its%20potential%20for%20SR%20problem.%20To%0Athis%20end%2C%20we%20propose%20the%20Gradient-guided%20Mamba%20for%20Spectral%20Reconstruction%20from%0ARGB%20Images%2C%20dubbed%20GMSR-Net.%20GMSR-Net%20is%20a%20lightweight%20model%20characterized%20by%20a%0Aglobal%20receptive%20field%20and%20linear%20computational%20complexity.%20Its%20core%20comprises%0Amultiple%20stacked%20Gradient%20Mamba%20%28GM%29%20blocks%2C%20each%20featuring%20a%20tri-branch%0Astructure.%20In%20addition%20to%20benefiting%20from%20efficient%20global%20feature%0Arepresentation%20by%20Mamba%20block%2C%20we%20further%20innovatively%20introduce%20spatial%0Agradient%20attention%20and%20spectral%20gradient%20attention%20to%20guide%20the%20reconstruction%0Aof%20spatial%20and%20spectral%20cues.%20GMSR-Net%20demonstrates%20a%20significant%0Aaccuracy-efficiency%20trade-off%2C%20achieving%20state-of-the-art%20performance%20while%0Amarkedly%20reducing%20the%20number%20of%20parameters%20and%20computational%20burdens.%20Compared%0Ato%20existing%20approaches%2C%20GMSR-Net%20slashes%20parameters%20and%20FLOPS%20by%20substantial%0Amargins%20of%2010%20times%20and%2020%20times%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/wxy11-27/GMSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07777v2&entry.124074799=Read"},
{"title": "Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark,\n  and Methodology", "author": "Xiangyu Wang and Donglin Yang and Ziqin Wang and Hohin Kwan and Jinyu Chen and Wenjun Wu and Hongsheng Li and Yue Liao and Si Liu", "abstract": "  Developing agents capable of navigating to a target location based on\nlanguage instructions and visual information, known as vision-language\nnavigation (VLN), has attracted widespread interest. Most research has focused\non ground-based agents, while UAV-based VLN remains relatively underexplored.\nRecent efforts in UAV vision-language navigation predominantly adopt\nground-based VLN settings, relying on predefined discrete action spaces and\nneglecting the inherent disparities in agent movement dynamics and the\ncomplexity of navigation tasks between ground and aerial environments. To\naddress these disparities and challenges, we propose solutions from three\nperspectives: platform, benchmark, and methodology. To enable realistic UAV\ntrajectory simulation in VLN tasks, we propose the OpenUAV platform, which\nfeatures diverse environments, realistic flight control, and extensive\nalgorithmic support. We further construct a target-oriented VLN dataset\nconsisting of approximately 12k trajectories on this platform, serving as the\nfirst dataset specifically designed for realistic UAV VLN tasks. To tackle the\nchallenges posed by complex aerial environments, we propose an assistant-guided\nUAV object search benchmark called UAV-Need-Help, which provides varying levels\nof guidance information to help UAVs better accomplish realistic VLN tasks. We\nalso propose a UAV navigation LLM that, given multi-view images, task\ndescriptions, and assistant instructions, leverages the multimodal\nunderstanding capabilities of the MLLM to jointly process visual and textual\ninformation, and performs hierarchical trajectory generation. The evaluation\nresults of our method significantly outperform the baseline models, while there\nremains a considerable gap between our results and those achieved by human\noperators, underscoring the challenge presented by the UAV-Need-Help task.\n", "link": "http://arxiv.org/abs/2410.07087v1", "date": "2024-10-09", "relevancy": 2.8624, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5901}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Realistic%20UAV%20Vision-Language%20Navigation%3A%20Platform%2C%20Benchmark%2C%0A%20%20and%20Methodology&body=Title%3A%20Towards%20Realistic%20UAV%20Vision-Language%20Navigation%3A%20Platform%2C%20Benchmark%2C%0A%20%20and%20Methodology%0AAuthor%3A%20Xiangyu%20Wang%20and%20Donglin%20Yang%20and%20Ziqin%20Wang%20and%20Hohin%20Kwan%20and%20Jinyu%20Chen%20and%20Wenjun%20Wu%20and%20Hongsheng%20Li%20and%20Yue%20Liao%20and%20Si%20Liu%0AAbstract%3A%20%20%20Developing%20agents%20capable%20of%20navigating%20to%20a%20target%20location%20based%20on%0Alanguage%20instructions%20and%20visual%20information%2C%20known%20as%20vision-language%0Anavigation%20%28VLN%29%2C%20has%20attracted%20widespread%20interest.%20Most%20research%20has%20focused%0Aon%20ground-based%20agents%2C%20while%20UAV-based%20VLN%20remains%20relatively%20underexplored.%0ARecent%20efforts%20in%20UAV%20vision-language%20navigation%20predominantly%20adopt%0Aground-based%20VLN%20settings%2C%20relying%20on%20predefined%20discrete%20action%20spaces%20and%0Aneglecting%20the%20inherent%20disparities%20in%20agent%20movement%20dynamics%20and%20the%0Acomplexity%20of%20navigation%20tasks%20between%20ground%20and%20aerial%20environments.%20To%0Aaddress%20these%20disparities%20and%20challenges%2C%20we%20propose%20solutions%20from%20three%0Aperspectives%3A%20platform%2C%20benchmark%2C%20and%20methodology.%20To%20enable%20realistic%20UAV%0Atrajectory%20simulation%20in%20VLN%20tasks%2C%20we%20propose%20the%20OpenUAV%20platform%2C%20which%0Afeatures%20diverse%20environments%2C%20realistic%20flight%20control%2C%20and%20extensive%0Aalgorithmic%20support.%20We%20further%20construct%20a%20target-oriented%20VLN%20dataset%0Aconsisting%20of%20approximately%2012k%20trajectories%20on%20this%20platform%2C%20serving%20as%20the%0Afirst%20dataset%20specifically%20designed%20for%20realistic%20UAV%20VLN%20tasks.%20To%20tackle%20the%0Achallenges%20posed%20by%20complex%20aerial%20environments%2C%20we%20propose%20an%20assistant-guided%0AUAV%20object%20search%20benchmark%20called%20UAV-Need-Help%2C%20which%20provides%20varying%20levels%0Aof%20guidance%20information%20to%20help%20UAVs%20better%20accomplish%20realistic%20VLN%20tasks.%20We%0Aalso%20propose%20a%20UAV%20navigation%20LLM%20that%2C%20given%20multi-view%20images%2C%20task%0Adescriptions%2C%20and%20assistant%20instructions%2C%20leverages%20the%20multimodal%0Aunderstanding%20capabilities%20of%20the%20MLLM%20to%20jointly%20process%20visual%20and%20textual%0Ainformation%2C%20and%20performs%20hierarchical%20trajectory%20generation.%20The%20evaluation%0Aresults%20of%20our%20method%20significantly%20outperform%20the%20baseline%20models%2C%20while%20there%0Aremains%20a%20considerable%20gap%20between%20our%20results%20and%20those%20achieved%20by%20human%0Aoperators%2C%20underscoring%20the%20challenge%20presented%20by%20the%20UAV-Need-Help%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Realistic%2520UAV%2520Vision-Language%2520Navigation%253A%2520Platform%252C%2520Benchmark%252C%250A%2520%2520and%2520Methodology%26entry.906535625%3DXiangyu%2520Wang%2520and%2520Donglin%2520Yang%2520and%2520Ziqin%2520Wang%2520and%2520Hohin%2520Kwan%2520and%2520Jinyu%2520Chen%2520and%2520Wenjun%2520Wu%2520and%2520Hongsheng%2520Li%2520and%2520Yue%2520Liao%2520and%2520Si%2520Liu%26entry.1292438233%3D%2520%2520Developing%2520agents%2520capable%2520of%2520navigating%2520to%2520a%2520target%2520location%2520based%2520on%250Alanguage%2520instructions%2520and%2520visual%2520information%252C%2520known%2520as%2520vision-language%250Anavigation%2520%2528VLN%2529%252C%2520has%2520attracted%2520widespread%2520interest.%2520Most%2520research%2520has%2520focused%250Aon%2520ground-based%2520agents%252C%2520while%2520UAV-based%2520VLN%2520remains%2520relatively%2520underexplored.%250ARecent%2520efforts%2520in%2520UAV%2520vision-language%2520navigation%2520predominantly%2520adopt%250Aground-based%2520VLN%2520settings%252C%2520relying%2520on%2520predefined%2520discrete%2520action%2520spaces%2520and%250Aneglecting%2520the%2520inherent%2520disparities%2520in%2520agent%2520movement%2520dynamics%2520and%2520the%250Acomplexity%2520of%2520navigation%2520tasks%2520between%2520ground%2520and%2520aerial%2520environments.%2520To%250Aaddress%2520these%2520disparities%2520and%2520challenges%252C%2520we%2520propose%2520solutions%2520from%2520three%250Aperspectives%253A%2520platform%252C%2520benchmark%252C%2520and%2520methodology.%2520To%2520enable%2520realistic%2520UAV%250Atrajectory%2520simulation%2520in%2520VLN%2520tasks%252C%2520we%2520propose%2520the%2520OpenUAV%2520platform%252C%2520which%250Afeatures%2520diverse%2520environments%252C%2520realistic%2520flight%2520control%252C%2520and%2520extensive%250Aalgorithmic%2520support.%2520We%2520further%2520construct%2520a%2520target-oriented%2520VLN%2520dataset%250Aconsisting%2520of%2520approximately%252012k%2520trajectories%2520on%2520this%2520platform%252C%2520serving%2520as%2520the%250Afirst%2520dataset%2520specifically%2520designed%2520for%2520realistic%2520UAV%2520VLN%2520tasks.%2520To%2520tackle%2520the%250Achallenges%2520posed%2520by%2520complex%2520aerial%2520environments%252C%2520we%2520propose%2520an%2520assistant-guided%250AUAV%2520object%2520search%2520benchmark%2520called%2520UAV-Need-Help%252C%2520which%2520provides%2520varying%2520levels%250Aof%2520guidance%2520information%2520to%2520help%2520UAVs%2520better%2520accomplish%2520realistic%2520VLN%2520tasks.%2520We%250Aalso%2520propose%2520a%2520UAV%2520navigation%2520LLM%2520that%252C%2520given%2520multi-view%2520images%252C%2520task%250Adescriptions%252C%2520and%2520assistant%2520instructions%252C%2520leverages%2520the%2520multimodal%250Aunderstanding%2520capabilities%2520of%2520the%2520MLLM%2520to%2520jointly%2520process%2520visual%2520and%2520textual%250Ainformation%252C%2520and%2520performs%2520hierarchical%2520trajectory%2520generation.%2520The%2520evaluation%250Aresults%2520of%2520our%2520method%2520significantly%2520outperform%2520the%2520baseline%2520models%252C%2520while%2520there%250Aremains%2520a%2520considerable%2520gap%2520between%2520our%2520results%2520and%2520those%2520achieved%2520by%2520human%250Aoperators%252C%2520underscoring%2520the%2520challenge%2520presented%2520by%2520the%2520UAV-Need-Help%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Realistic%20UAV%20Vision-Language%20Navigation%3A%20Platform%2C%20Benchmark%2C%0A%20%20and%20Methodology&entry.906535625=Xiangyu%20Wang%20and%20Donglin%20Yang%20and%20Ziqin%20Wang%20and%20Hohin%20Kwan%20and%20Jinyu%20Chen%20and%20Wenjun%20Wu%20and%20Hongsheng%20Li%20and%20Yue%20Liao%20and%20Si%20Liu&entry.1292438233=%20%20Developing%20agents%20capable%20of%20navigating%20to%20a%20target%20location%20based%20on%0Alanguage%20instructions%20and%20visual%20information%2C%20known%20as%20vision-language%0Anavigation%20%28VLN%29%2C%20has%20attracted%20widespread%20interest.%20Most%20research%20has%20focused%0Aon%20ground-based%20agents%2C%20while%20UAV-based%20VLN%20remains%20relatively%20underexplored.%0ARecent%20efforts%20in%20UAV%20vision-language%20navigation%20predominantly%20adopt%0Aground-based%20VLN%20settings%2C%20relying%20on%20predefined%20discrete%20action%20spaces%20and%0Aneglecting%20the%20inherent%20disparities%20in%20agent%20movement%20dynamics%20and%20the%0Acomplexity%20of%20navigation%20tasks%20between%20ground%20and%20aerial%20environments.%20To%0Aaddress%20these%20disparities%20and%20challenges%2C%20we%20propose%20solutions%20from%20three%0Aperspectives%3A%20platform%2C%20benchmark%2C%20and%20methodology.%20To%20enable%20realistic%20UAV%0Atrajectory%20simulation%20in%20VLN%20tasks%2C%20we%20propose%20the%20OpenUAV%20platform%2C%20which%0Afeatures%20diverse%20environments%2C%20realistic%20flight%20control%2C%20and%20extensive%0Aalgorithmic%20support.%20We%20further%20construct%20a%20target-oriented%20VLN%20dataset%0Aconsisting%20of%20approximately%2012k%20trajectories%20on%20this%20platform%2C%20serving%20as%20the%0Afirst%20dataset%20specifically%20designed%20for%20realistic%20UAV%20VLN%20tasks.%20To%20tackle%20the%0Achallenges%20posed%20by%20complex%20aerial%20environments%2C%20we%20propose%20an%20assistant-guided%0AUAV%20object%20search%20benchmark%20called%20UAV-Need-Help%2C%20which%20provides%20varying%20levels%0Aof%20guidance%20information%20to%20help%20UAVs%20better%20accomplish%20realistic%20VLN%20tasks.%20We%0Aalso%20propose%20a%20UAV%20navigation%20LLM%20that%2C%20given%20multi-view%20images%2C%20task%0Adescriptions%2C%20and%20assistant%20instructions%2C%20leverages%20the%20multimodal%0Aunderstanding%20capabilities%20of%20the%20MLLM%20to%20jointly%20process%20visual%20and%20textual%0Ainformation%2C%20and%20performs%20hierarchical%20trajectory%20generation.%20The%20evaluation%0Aresults%20of%20our%20method%20significantly%20outperform%20the%20baseline%20models%2C%20while%20there%0Aremains%20a%20considerable%20gap%20between%20our%20results%20and%20those%20achieved%20by%20human%0Aoperators%2C%20underscoring%20the%20challenge%20presented%20by%20the%20UAV-Need-Help%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07087v1&entry.124074799=Read"},
{"title": "TURTLMap: Real-time Localization and Dense Mapping of Low-texture\n  Underwater Environments with a Low-cost Unmanned Underwater Vehicle", "author": "Jingyu Song and Onur Bagoren and Razan Andigani and Advaith Venkatramanan Sethuraman and Katherine A. Skinner", "abstract": "  Significant work has been done on advancing localization and mapping in\nunderwater environments. Still, state-of-the-art methods are challenged by\nlow-texture environments, which is common for underwater settings. This makes\nit difficult to use existing methods in diverse, real-world scenes. In this\npaper, we present TURTLMap, a novel solution that focuses on textureless\nunderwater environments through a real-time localization and mapping method. We\nshow that this method is low-cost, and capable of tracking the robot\naccurately, while constructing a dense map of a low-textured environment in\nreal-time. We evaluate the proposed method using real-world data collected in\nan indoor water tank with a motion capture system and ground truth reference\nmap. Qualitative and quantitative results validate the proposed system achieves\naccurate and robust localization and precise dense mapping, even when subject\nto wave conditions. The project page for TURTLMap is\nhttps://umfieldrobotics.github.io/TURTLMap.\n", "link": "http://arxiv.org/abs/2408.01569v2", "date": "2024-10-09", "relevancy": 2.848, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TURTLMap%3A%20Real-time%20Localization%20and%20Dense%20Mapping%20of%20Low-texture%0A%20%20Underwater%20Environments%20with%20a%20Low-cost%20Unmanned%20Underwater%20Vehicle&body=Title%3A%20TURTLMap%3A%20Real-time%20Localization%20and%20Dense%20Mapping%20of%20Low-texture%0A%20%20Underwater%20Environments%20with%20a%20Low-cost%20Unmanned%20Underwater%20Vehicle%0AAuthor%3A%20Jingyu%20Song%20and%20Onur%20Bagoren%20and%20Razan%20Andigani%20and%20Advaith%20Venkatramanan%20Sethuraman%20and%20Katherine%20A.%20Skinner%0AAbstract%3A%20%20%20Significant%20work%20has%20been%20done%20on%20advancing%20localization%20and%20mapping%20in%0Aunderwater%20environments.%20Still%2C%20state-of-the-art%20methods%20are%20challenged%20by%0Alow-texture%20environments%2C%20which%20is%20common%20for%20underwater%20settings.%20This%20makes%0Ait%20difficult%20to%20use%20existing%20methods%20in%20diverse%2C%20real-world%20scenes.%20In%20this%0Apaper%2C%20we%20present%20TURTLMap%2C%20a%20novel%20solution%20that%20focuses%20on%20textureless%0Aunderwater%20environments%20through%20a%20real-time%20localization%20and%20mapping%20method.%20We%0Ashow%20that%20this%20method%20is%20low-cost%2C%20and%20capable%20of%20tracking%20the%20robot%0Aaccurately%2C%20while%20constructing%20a%20dense%20map%20of%20a%20low-textured%20environment%20in%0Areal-time.%20We%20evaluate%20the%20proposed%20method%20using%20real-world%20data%20collected%20in%0Aan%20indoor%20water%20tank%20with%20a%20motion%20capture%20system%20and%20ground%20truth%20reference%0Amap.%20Qualitative%20and%20quantitative%20results%20validate%20the%20proposed%20system%20achieves%0Aaccurate%20and%20robust%20localization%20and%20precise%20dense%20mapping%2C%20even%20when%20subject%0Ato%20wave%20conditions.%20The%20project%20page%20for%20TURTLMap%20is%0Ahttps%3A//umfieldrobotics.github.io/TURTLMap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTURTLMap%253A%2520Real-time%2520Localization%2520and%2520Dense%2520Mapping%2520of%2520Low-texture%250A%2520%2520Underwater%2520Environments%2520with%2520a%2520Low-cost%2520Unmanned%2520Underwater%2520Vehicle%26entry.906535625%3DJingyu%2520Song%2520and%2520Onur%2520Bagoren%2520and%2520Razan%2520Andigani%2520and%2520Advaith%2520Venkatramanan%2520Sethuraman%2520and%2520Katherine%2520A.%2520Skinner%26entry.1292438233%3D%2520%2520Significant%2520work%2520has%2520been%2520done%2520on%2520advancing%2520localization%2520and%2520mapping%2520in%250Aunderwater%2520environments.%2520Still%252C%2520state-of-the-art%2520methods%2520are%2520challenged%2520by%250Alow-texture%2520environments%252C%2520which%2520is%2520common%2520for%2520underwater%2520settings.%2520This%2520makes%250Ait%2520difficult%2520to%2520use%2520existing%2520methods%2520in%2520diverse%252C%2520real-world%2520scenes.%2520In%2520this%250Apaper%252C%2520we%2520present%2520TURTLMap%252C%2520a%2520novel%2520solution%2520that%2520focuses%2520on%2520textureless%250Aunderwater%2520environments%2520through%2520a%2520real-time%2520localization%2520and%2520mapping%2520method.%2520We%250Ashow%2520that%2520this%2520method%2520is%2520low-cost%252C%2520and%2520capable%2520of%2520tracking%2520the%2520robot%250Aaccurately%252C%2520while%2520constructing%2520a%2520dense%2520map%2520of%2520a%2520low-textured%2520environment%2520in%250Areal-time.%2520We%2520evaluate%2520the%2520proposed%2520method%2520using%2520real-world%2520data%2520collected%2520in%250Aan%2520indoor%2520water%2520tank%2520with%2520a%2520motion%2520capture%2520system%2520and%2520ground%2520truth%2520reference%250Amap.%2520Qualitative%2520and%2520quantitative%2520results%2520validate%2520the%2520proposed%2520system%2520achieves%250Aaccurate%2520and%2520robust%2520localization%2520and%2520precise%2520dense%2520mapping%252C%2520even%2520when%2520subject%250Ato%2520wave%2520conditions.%2520The%2520project%2520page%2520for%2520TURTLMap%2520is%250Ahttps%253A//umfieldrobotics.github.io/TURTLMap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TURTLMap%3A%20Real-time%20Localization%20and%20Dense%20Mapping%20of%20Low-texture%0A%20%20Underwater%20Environments%20with%20a%20Low-cost%20Unmanned%20Underwater%20Vehicle&entry.906535625=Jingyu%20Song%20and%20Onur%20Bagoren%20and%20Razan%20Andigani%20and%20Advaith%20Venkatramanan%20Sethuraman%20and%20Katherine%20A.%20Skinner&entry.1292438233=%20%20Significant%20work%20has%20been%20done%20on%20advancing%20localization%20and%20mapping%20in%0Aunderwater%20environments.%20Still%2C%20state-of-the-art%20methods%20are%20challenged%20by%0Alow-texture%20environments%2C%20which%20is%20common%20for%20underwater%20settings.%20This%20makes%0Ait%20difficult%20to%20use%20existing%20methods%20in%20diverse%2C%20real-world%20scenes.%20In%20this%0Apaper%2C%20we%20present%20TURTLMap%2C%20a%20novel%20solution%20that%20focuses%20on%20textureless%0Aunderwater%20environments%20through%20a%20real-time%20localization%20and%20mapping%20method.%20We%0Ashow%20that%20this%20method%20is%20low-cost%2C%20and%20capable%20of%20tracking%20the%20robot%0Aaccurately%2C%20while%20constructing%20a%20dense%20map%20of%20a%20low-textured%20environment%20in%0Areal-time.%20We%20evaluate%20the%20proposed%20method%20using%20real-world%20data%20collected%20in%0Aan%20indoor%20water%20tank%20with%20a%20motion%20capture%20system%20and%20ground%20truth%20reference%0Amap.%20Qualitative%20and%20quantitative%20results%20validate%20the%20proposed%20system%20achieves%0Aaccurate%20and%20robust%20localization%20and%20precise%20dense%20mapping%2C%20even%20when%20subject%0Ato%20wave%20conditions.%20The%20project%20page%20for%20TURTLMap%20is%0Ahttps%3A//umfieldrobotics.github.io/TURTLMap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01569v2&entry.124074799=Read"},
{"title": "HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric\n  Understanding", "author": "Keliang Li and Zaifei Yang and Jiahe Zhao and Hongze Shen and Ruibing Hou and Hong Chang and Shiguang Shan and Xilin Chen", "abstract": "  The significant advancements in visual understanding and instruction\nfollowing from Multimodal Large Language Models (MLLMs) have opened up more\npossibilities for broader applications in diverse and universal human-centric\nscenarios. However, existing image-text data may not support the precise\nmodality alignment and integration of multi-grained information, which is\ncrucial for human-centric visual understanding. In this paper, we introduce\nHERM-Bench, a benchmark for evaluating the human-centric understanding\ncapabilities of MLLMs. Our work reveals the limitations of existing MLLMs in\nunderstanding complex human-centric scenarios. To address these challenges, we\npresent HERM-100K, a comprehensive dataset with multi-level human-centric\nannotations, aimed at enhancing MLLMs' training. Furthermore, we develop\nHERM-7B, a MLLM that leverages enhanced training data from HERM-100K.\nEvaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms\nexisting MLLMs across various human-centric dimensions, reflecting the current\ninadequacy of data annotations used in MLLM training for human-centric visual\nunderstanding. This research emphasizes the importance of specialized datasets\nand benchmarks in advancing the MLLMs' capabilities for human-centric\nunderstanding.\n", "link": "http://arxiv.org/abs/2410.06777v1", "date": "2024-10-09", "relevancy": 2.8434, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HERM%3A%20Benchmarking%20and%20Enhancing%20Multimodal%20LLMs%20for%20Human-Centric%0A%20%20Understanding&body=Title%3A%20HERM%3A%20Benchmarking%20and%20Enhancing%20Multimodal%20LLMs%20for%20Human-Centric%0A%20%20Understanding%0AAuthor%3A%20Keliang%20Li%20and%20Zaifei%20Yang%20and%20Jiahe%20Zhao%20and%20Hongze%20Shen%20and%20Ruibing%20Hou%20and%20Hong%20Chang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20The%20significant%20advancements%20in%20visual%20understanding%20and%20instruction%0Afollowing%20from%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20opened%20up%20more%0Apossibilities%20for%20broader%20applications%20in%20diverse%20and%20universal%20human-centric%0Ascenarios.%20However%2C%20existing%20image-text%20data%20may%20not%20support%20the%20precise%0Amodality%20alignment%20and%20integration%20of%20multi-grained%20information%2C%20which%20is%0Acrucial%20for%20human-centric%20visual%20understanding.%20In%20this%20paper%2C%20we%20introduce%0AHERM-Bench%2C%20a%20benchmark%20for%20evaluating%20the%20human-centric%20understanding%0Acapabilities%20of%20MLLMs.%20Our%20work%20reveals%20the%20limitations%20of%20existing%20MLLMs%20in%0Aunderstanding%20complex%20human-centric%20scenarios.%20To%20address%20these%20challenges%2C%20we%0Apresent%20HERM-100K%2C%20a%20comprehensive%20dataset%20with%20multi-level%20human-centric%0Aannotations%2C%20aimed%20at%20enhancing%20MLLMs%27%20training.%20Furthermore%2C%20we%20develop%0AHERM-7B%2C%20a%20MLLM%20that%20leverages%20enhanced%20training%20data%20from%20HERM-100K.%0AEvaluations%20on%20HERM-Bench%20demonstrate%20that%20HERM-7B%20significantly%20outperforms%0Aexisting%20MLLMs%20across%20various%20human-centric%20dimensions%2C%20reflecting%20the%20current%0Ainadequacy%20of%20data%20annotations%20used%20in%20MLLM%20training%20for%20human-centric%20visual%0Aunderstanding.%20This%20research%20emphasizes%20the%20importance%20of%20specialized%20datasets%0Aand%20benchmarks%20in%20advancing%20the%20MLLMs%27%20capabilities%20for%20human-centric%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHERM%253A%2520Benchmarking%2520and%2520Enhancing%2520Multimodal%2520LLMs%2520for%2520Human-Centric%250A%2520%2520Understanding%26entry.906535625%3DKeliang%2520Li%2520and%2520Zaifei%2520Yang%2520and%2520Jiahe%2520Zhao%2520and%2520Hongze%2520Shen%2520and%2520Ruibing%2520Hou%2520and%2520Hong%2520Chang%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3D%2520%2520The%2520significant%2520advancements%2520in%2520visual%2520understanding%2520and%2520instruction%250Afollowing%2520from%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520opened%2520up%2520more%250Apossibilities%2520for%2520broader%2520applications%2520in%2520diverse%2520and%2520universal%2520human-centric%250Ascenarios.%2520However%252C%2520existing%2520image-text%2520data%2520may%2520not%2520support%2520the%2520precise%250Amodality%2520alignment%2520and%2520integration%2520of%2520multi-grained%2520information%252C%2520which%2520is%250Acrucial%2520for%2520human-centric%2520visual%2520understanding.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AHERM-Bench%252C%2520a%2520benchmark%2520for%2520evaluating%2520the%2520human-centric%2520understanding%250Acapabilities%2520of%2520MLLMs.%2520Our%2520work%2520reveals%2520the%2520limitations%2520of%2520existing%2520MLLMs%2520in%250Aunderstanding%2520complex%2520human-centric%2520scenarios.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apresent%2520HERM-100K%252C%2520a%2520comprehensive%2520dataset%2520with%2520multi-level%2520human-centric%250Aannotations%252C%2520aimed%2520at%2520enhancing%2520MLLMs%2527%2520training.%2520Furthermore%252C%2520we%2520develop%250AHERM-7B%252C%2520a%2520MLLM%2520that%2520leverages%2520enhanced%2520training%2520data%2520from%2520HERM-100K.%250AEvaluations%2520on%2520HERM-Bench%2520demonstrate%2520that%2520HERM-7B%2520significantly%2520outperforms%250Aexisting%2520MLLMs%2520across%2520various%2520human-centric%2520dimensions%252C%2520reflecting%2520the%2520current%250Ainadequacy%2520of%2520data%2520annotations%2520used%2520in%2520MLLM%2520training%2520for%2520human-centric%2520visual%250Aunderstanding.%2520This%2520research%2520emphasizes%2520the%2520importance%2520of%2520specialized%2520datasets%250Aand%2520benchmarks%2520in%2520advancing%2520the%2520MLLMs%2527%2520capabilities%2520for%2520human-centric%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HERM%3A%20Benchmarking%20and%20Enhancing%20Multimodal%20LLMs%20for%20Human-Centric%0A%20%20Understanding&entry.906535625=Keliang%20Li%20and%20Zaifei%20Yang%20and%20Jiahe%20Zhao%20and%20Hongze%20Shen%20and%20Ruibing%20Hou%20and%20Hong%20Chang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=%20%20The%20significant%20advancements%20in%20visual%20understanding%20and%20instruction%0Afollowing%20from%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20opened%20up%20more%0Apossibilities%20for%20broader%20applications%20in%20diverse%20and%20universal%20human-centric%0Ascenarios.%20However%2C%20existing%20image-text%20data%20may%20not%20support%20the%20precise%0Amodality%20alignment%20and%20integration%20of%20multi-grained%20information%2C%20which%20is%0Acrucial%20for%20human-centric%20visual%20understanding.%20In%20this%20paper%2C%20we%20introduce%0AHERM-Bench%2C%20a%20benchmark%20for%20evaluating%20the%20human-centric%20understanding%0Acapabilities%20of%20MLLMs.%20Our%20work%20reveals%20the%20limitations%20of%20existing%20MLLMs%20in%0Aunderstanding%20complex%20human-centric%20scenarios.%20To%20address%20these%20challenges%2C%20we%0Apresent%20HERM-100K%2C%20a%20comprehensive%20dataset%20with%20multi-level%20human-centric%0Aannotations%2C%20aimed%20at%20enhancing%20MLLMs%27%20training.%20Furthermore%2C%20we%20develop%0AHERM-7B%2C%20a%20MLLM%20that%20leverages%20enhanced%20training%20data%20from%20HERM-100K.%0AEvaluations%20on%20HERM-Bench%20demonstrate%20that%20HERM-7B%20significantly%20outperforms%0Aexisting%20MLLMs%20across%20various%20human-centric%20dimensions%2C%20reflecting%20the%20current%0Ainadequacy%20of%20data%20annotations%20used%20in%20MLLM%20training%20for%20human-centric%20visual%0Aunderstanding.%20This%20research%20emphasizes%20the%20importance%20of%20specialized%20datasets%0Aand%20benchmarks%20in%20advancing%20the%20MLLMs%27%20capabilities%20for%20human-centric%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06777v1&entry.124074799=Read"},
{"title": "To Preserve or To Compress: An In-Depth Study of Connector Selection in\n  Multimodal Large Language Models", "author": "Junyan Lin and Haoran Chen and Dawei Zhu and Xiaoyu Shen", "abstract": "  In recent years, multimodal large language models (MLLMs) have garnered\nsignificant attention from both industry and academia. However, there is still\nconsiderable debate on constructing MLLM architectures, particularly regarding\nthe selection of appropriate connectors for perception tasks of varying\ngranularities. This paper systematically investigates the impact of connectors\non MLLM performance. Specifically, we classify connectors into\nfeature-preserving and feature-compressing types. Utilizing a unified\nclassification standard, we categorize sub-tasks from three comprehensive\nbenchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained\nperception, fine-grained perception, and reasoning, and evaluate the\nperformance. Our findings reveal that feature-preserving connectors excel in\n\\emph{fine-grained perception} tasks due to their ability to retain detailed\nvisual information. In contrast, feature-compressing connectors, while less\neffective in fine-grained perception tasks, offer significant speed advantages\nand perform comparably in \\emph{coarse-grained perception} and \\emph{reasoning}\ntasks. These insights are crucial for guiding MLLM architecture design and\nadvancing the optimization of MLLM architectures.\n", "link": "http://arxiv.org/abs/2410.06765v1", "date": "2024-10-09", "relevancy": 2.8433, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5788}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Preserve%20or%20To%20Compress%3A%20An%20In-Depth%20Study%20of%20Connector%20Selection%20in%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20To%20Preserve%20or%20To%20Compress%3A%20An%20In-Depth%20Study%20of%20Connector%20Selection%20in%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Junyan%20Lin%20and%20Haoran%20Chen%20and%20Dawei%20Zhu%20and%20Xiaoyu%20Shen%0AAbstract%3A%20%20%20In%20recent%20years%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20garnered%0Asignificant%20attention%20from%20both%20industry%20and%20academia.%20However%2C%20there%20is%20still%0Aconsiderable%20debate%20on%20constructing%20MLLM%20architectures%2C%20particularly%20regarding%0Athe%20selection%20of%20appropriate%20connectors%20for%20perception%20tasks%20of%20varying%0Agranularities.%20This%20paper%20systematically%20investigates%20the%20impact%20of%20connectors%0Aon%20MLLM%20performance.%20Specifically%2C%20we%20classify%20connectors%20into%0Afeature-preserving%20and%20feature-compressing%20types.%20Utilizing%20a%20unified%0Aclassification%20standard%2C%20we%20categorize%20sub-tasks%20from%20three%20comprehensive%0Abenchmarks%2C%20MMBench%2C%20MME%2C%20and%20SEED-Bench%2C%20into%20three%20task%20types%3A%20coarse-grained%0Aperception%2C%20fine-grained%20perception%2C%20and%20reasoning%2C%20and%20evaluate%20the%0Aperformance.%20Our%20findings%20reveal%20that%20feature-preserving%20connectors%20excel%20in%0A%5Cemph%7Bfine-grained%20perception%7D%20tasks%20due%20to%20their%20ability%20to%20retain%20detailed%0Avisual%20information.%20In%20contrast%2C%20feature-compressing%20connectors%2C%20while%20less%0Aeffective%20in%20fine-grained%20perception%20tasks%2C%20offer%20significant%20speed%20advantages%0Aand%20perform%20comparably%20in%20%5Cemph%7Bcoarse-grained%20perception%7D%20and%20%5Cemph%7Breasoning%7D%0Atasks.%20These%20insights%20are%20crucial%20for%20guiding%20MLLM%20architecture%20design%20and%0Aadvancing%20the%20optimization%20of%20MLLM%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Preserve%2520or%2520To%2520Compress%253A%2520An%2520In-Depth%2520Study%2520of%2520Connector%2520Selection%2520in%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DJunyan%2520Lin%2520and%2520Haoran%2520Chen%2520and%2520Dawei%2520Zhu%2520and%2520Xiaoyu%2520Shen%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520garnered%250Asignificant%2520attention%2520from%2520both%2520industry%2520and%2520academia.%2520However%252C%2520there%2520is%2520still%250Aconsiderable%2520debate%2520on%2520constructing%2520MLLM%2520architectures%252C%2520particularly%2520regarding%250Athe%2520selection%2520of%2520appropriate%2520connectors%2520for%2520perception%2520tasks%2520of%2520varying%250Agranularities.%2520This%2520paper%2520systematically%2520investigates%2520the%2520impact%2520of%2520connectors%250Aon%2520MLLM%2520performance.%2520Specifically%252C%2520we%2520classify%2520connectors%2520into%250Afeature-preserving%2520and%2520feature-compressing%2520types.%2520Utilizing%2520a%2520unified%250Aclassification%2520standard%252C%2520we%2520categorize%2520sub-tasks%2520from%2520three%2520comprehensive%250Abenchmarks%252C%2520MMBench%252C%2520MME%252C%2520and%2520SEED-Bench%252C%2520into%2520three%2520task%2520types%253A%2520coarse-grained%250Aperception%252C%2520fine-grained%2520perception%252C%2520and%2520reasoning%252C%2520and%2520evaluate%2520the%250Aperformance.%2520Our%2520findings%2520reveal%2520that%2520feature-preserving%2520connectors%2520excel%2520in%250A%255Cemph%257Bfine-grained%2520perception%257D%2520tasks%2520due%2520to%2520their%2520ability%2520to%2520retain%2520detailed%250Avisual%2520information.%2520In%2520contrast%252C%2520feature-compressing%2520connectors%252C%2520while%2520less%250Aeffective%2520in%2520fine-grained%2520perception%2520tasks%252C%2520offer%2520significant%2520speed%2520advantages%250Aand%2520perform%2520comparably%2520in%2520%255Cemph%257Bcoarse-grained%2520perception%257D%2520and%2520%255Cemph%257Breasoning%257D%250Atasks.%2520These%2520insights%2520are%2520crucial%2520for%2520guiding%2520MLLM%2520architecture%2520design%2520and%250Aadvancing%2520the%2520optimization%2520of%2520MLLM%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Preserve%20or%20To%20Compress%3A%20An%20In-Depth%20Study%20of%20Connector%20Selection%20in%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Junyan%20Lin%20and%20Haoran%20Chen%20and%20Dawei%20Zhu%20and%20Xiaoyu%20Shen&entry.1292438233=%20%20In%20recent%20years%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20garnered%0Asignificant%20attention%20from%20both%20industry%20and%20academia.%20However%2C%20there%20is%20still%0Aconsiderable%20debate%20on%20constructing%20MLLM%20architectures%2C%20particularly%20regarding%0Athe%20selection%20of%20appropriate%20connectors%20for%20perception%20tasks%20of%20varying%0Agranularities.%20This%20paper%20systematically%20investigates%20the%20impact%20of%20connectors%0Aon%20MLLM%20performance.%20Specifically%2C%20we%20classify%20connectors%20into%0Afeature-preserving%20and%20feature-compressing%20types.%20Utilizing%20a%20unified%0Aclassification%20standard%2C%20we%20categorize%20sub-tasks%20from%20three%20comprehensive%0Abenchmarks%2C%20MMBench%2C%20MME%2C%20and%20SEED-Bench%2C%20into%20three%20task%20types%3A%20coarse-grained%0Aperception%2C%20fine-grained%20perception%2C%20and%20reasoning%2C%20and%20evaluate%20the%0Aperformance.%20Our%20findings%20reveal%20that%20feature-preserving%20connectors%20excel%20in%0A%5Cemph%7Bfine-grained%20perception%7D%20tasks%20due%20to%20their%20ability%20to%20retain%20detailed%0Avisual%20information.%20In%20contrast%2C%20feature-compressing%20connectors%2C%20while%20less%0Aeffective%20in%20fine-grained%20perception%20tasks%2C%20offer%20significant%20speed%20advantages%0Aand%20perform%20comparably%20in%20%5Cemph%7Bcoarse-grained%20perception%7D%20and%20%5Cemph%7Breasoning%7D%0Atasks.%20These%20insights%20are%20crucial%20for%20guiding%20MLLM%20architecture%20design%20and%0Aadvancing%20the%20optimization%20of%20MLLM%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06765v1&entry.124074799=Read"},
{"title": "DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on\n  LLM", "author": "Xuchen Li and Xiaokun Feng and Shiyu Hu and Meiqi Wu and Dailing Zhang and Jing Zhang and Kaiqi Huang", "abstract": "  Visual Language Tracking (VLT) enhances single object tracking (SOT) by\nintegrating natural language descriptions from a video, for the precise\ntracking of a specified object. By leveraging high-level semantic information,\nVLT guides object tracking, alleviating the constraints associated with relying\non a visual modality. Nevertheless, most VLT benchmarks are annotated in a\nsingle granularity and lack a coherent semantic framework to provide scientific\nguidance. Moreover, coordinating human annotators for high-quality annotations\nis laborious and time-consuming. To address these challenges, we introduce\nDTLLM-VLT, which automatically generates extensive and multi-granularity text\nto enhance environmental diversity. (1) DTLLM-VLT generates scientific and\nmulti-granularity text descriptions using a cohesive prompt framework. Its\nsuccinct and highly adaptable design allows seamless integration into various\nvisual tracking benchmarks. (2) We select three prominent benchmarks to deploy\nour approach: short-term tracking, long-term tracking, and global instance\ntracking. We offer four granularity combinations for these benchmarks,\nconsidering the extent and density of semantic information, thereby showcasing\nthe practicality and versatility of DTLLM-VLT. (3) We conduct comparative\nexperiments on VLT benchmarks with different text granularities, evaluating and\nanalyzing the impact of diverse text on tracking performance. Conclusionally,\nthis work leverages LLM to provide multi-granularity semantic information for\nVLT task from efficient and diverse perspectives, enabling fine-grained\nevaluation of multi-modal trackers. In the future, we believe this work can be\nextended to more datasets to support vision datasets understanding.\n", "link": "http://arxiv.org/abs/2405.12139v2", "date": "2024-10-09", "relevancy": 2.8348, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DTLLM-VLT%3A%20Diverse%20Text%20Generation%20for%20Visual%20Language%20Tracking%20Based%20on%0A%20%20LLM&body=Title%3A%20DTLLM-VLT%3A%20Diverse%20Text%20Generation%20for%20Visual%20Language%20Tracking%20Based%20on%0A%20%20LLM%0AAuthor%3A%20Xuchen%20Li%20and%20Xiaokun%20Feng%20and%20Shiyu%20Hu%20and%20Meiqi%20Wu%20and%20Dailing%20Zhang%20and%20Jing%20Zhang%20and%20Kaiqi%20Huang%0AAbstract%3A%20%20%20Visual%20Language%20Tracking%20%28VLT%29%20enhances%20single%20object%20tracking%20%28SOT%29%20by%0Aintegrating%20natural%20language%20descriptions%20from%20a%20video%2C%20for%20the%20precise%0Atracking%20of%20a%20specified%20object.%20By%20leveraging%20high-level%20semantic%20information%2C%0AVLT%20guides%20object%20tracking%2C%20alleviating%20the%20constraints%20associated%20with%20relying%0Aon%20a%20visual%20modality.%20Nevertheless%2C%20most%20VLT%20benchmarks%20are%20annotated%20in%20a%0Asingle%20granularity%20and%20lack%20a%20coherent%20semantic%20framework%20to%20provide%20scientific%0Aguidance.%20Moreover%2C%20coordinating%20human%20annotators%20for%20high-quality%20annotations%0Ais%20laborious%20and%20time-consuming.%20To%20address%20these%20challenges%2C%20we%20introduce%0ADTLLM-VLT%2C%20which%20automatically%20generates%20extensive%20and%20multi-granularity%20text%0Ato%20enhance%20environmental%20diversity.%20%281%29%20DTLLM-VLT%20generates%20scientific%20and%0Amulti-granularity%20text%20descriptions%20using%20a%20cohesive%20prompt%20framework.%20Its%0Asuccinct%20and%20highly%20adaptable%20design%20allows%20seamless%20integration%20into%20various%0Avisual%20tracking%20benchmarks.%20%282%29%20We%20select%20three%20prominent%20benchmarks%20to%20deploy%0Aour%20approach%3A%20short-term%20tracking%2C%20long-term%20tracking%2C%20and%20global%20instance%0Atracking.%20We%20offer%20four%20granularity%20combinations%20for%20these%20benchmarks%2C%0Aconsidering%20the%20extent%20and%20density%20of%20semantic%20information%2C%20thereby%20showcasing%0Athe%20practicality%20and%20versatility%20of%20DTLLM-VLT.%20%283%29%20We%20conduct%20comparative%0Aexperiments%20on%20VLT%20benchmarks%20with%20different%20text%20granularities%2C%20evaluating%20and%0Aanalyzing%20the%20impact%20of%20diverse%20text%20on%20tracking%20performance.%20Conclusionally%2C%0Athis%20work%20leverages%20LLM%20to%20provide%20multi-granularity%20semantic%20information%20for%0AVLT%20task%20from%20efficient%20and%20diverse%20perspectives%2C%20enabling%20fine-grained%0Aevaluation%20of%20multi-modal%20trackers.%20In%20the%20future%2C%20we%20believe%20this%20work%20can%20be%0Aextended%20to%20more%20datasets%20to%20support%20vision%20datasets%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDTLLM-VLT%253A%2520Diverse%2520Text%2520Generation%2520for%2520Visual%2520Language%2520Tracking%2520Based%2520on%250A%2520%2520LLM%26entry.906535625%3DXuchen%2520Li%2520and%2520Xiaokun%2520Feng%2520and%2520Shiyu%2520Hu%2520and%2520Meiqi%2520Wu%2520and%2520Dailing%2520Zhang%2520and%2520Jing%2520Zhang%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3D%2520%2520Visual%2520Language%2520Tracking%2520%2528VLT%2529%2520enhances%2520single%2520object%2520tracking%2520%2528SOT%2529%2520by%250Aintegrating%2520natural%2520language%2520descriptions%2520from%2520a%2520video%252C%2520for%2520the%2520precise%250Atracking%2520of%2520a%2520specified%2520object.%2520By%2520leveraging%2520high-level%2520semantic%2520information%252C%250AVLT%2520guides%2520object%2520tracking%252C%2520alleviating%2520the%2520constraints%2520associated%2520with%2520relying%250Aon%2520a%2520visual%2520modality.%2520Nevertheless%252C%2520most%2520VLT%2520benchmarks%2520are%2520annotated%2520in%2520a%250Asingle%2520granularity%2520and%2520lack%2520a%2520coherent%2520semantic%2520framework%2520to%2520provide%2520scientific%250Aguidance.%2520Moreover%252C%2520coordinating%2520human%2520annotators%2520for%2520high-quality%2520annotations%250Ais%2520laborious%2520and%2520time-consuming.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250ADTLLM-VLT%252C%2520which%2520automatically%2520generates%2520extensive%2520and%2520multi-granularity%2520text%250Ato%2520enhance%2520environmental%2520diversity.%2520%25281%2529%2520DTLLM-VLT%2520generates%2520scientific%2520and%250Amulti-granularity%2520text%2520descriptions%2520using%2520a%2520cohesive%2520prompt%2520framework.%2520Its%250Asuccinct%2520and%2520highly%2520adaptable%2520design%2520allows%2520seamless%2520integration%2520into%2520various%250Avisual%2520tracking%2520benchmarks.%2520%25282%2529%2520We%2520select%2520three%2520prominent%2520benchmarks%2520to%2520deploy%250Aour%2520approach%253A%2520short-term%2520tracking%252C%2520long-term%2520tracking%252C%2520and%2520global%2520instance%250Atracking.%2520We%2520offer%2520four%2520granularity%2520combinations%2520for%2520these%2520benchmarks%252C%250Aconsidering%2520the%2520extent%2520and%2520density%2520of%2520semantic%2520information%252C%2520thereby%2520showcasing%250Athe%2520practicality%2520and%2520versatility%2520of%2520DTLLM-VLT.%2520%25283%2529%2520We%2520conduct%2520comparative%250Aexperiments%2520on%2520VLT%2520benchmarks%2520with%2520different%2520text%2520granularities%252C%2520evaluating%2520and%250Aanalyzing%2520the%2520impact%2520of%2520diverse%2520text%2520on%2520tracking%2520performance.%2520Conclusionally%252C%250Athis%2520work%2520leverages%2520LLM%2520to%2520provide%2520multi-granularity%2520semantic%2520information%2520for%250AVLT%2520task%2520from%2520efficient%2520and%2520diverse%2520perspectives%252C%2520enabling%2520fine-grained%250Aevaluation%2520of%2520multi-modal%2520trackers.%2520In%2520the%2520future%252C%2520we%2520believe%2520this%2520work%2520can%2520be%250Aextended%2520to%2520more%2520datasets%2520to%2520support%2520vision%2520datasets%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DTLLM-VLT%3A%20Diverse%20Text%20Generation%20for%20Visual%20Language%20Tracking%20Based%20on%0A%20%20LLM&entry.906535625=Xuchen%20Li%20and%20Xiaokun%20Feng%20and%20Shiyu%20Hu%20and%20Meiqi%20Wu%20and%20Dailing%20Zhang%20and%20Jing%20Zhang%20and%20Kaiqi%20Huang&entry.1292438233=%20%20Visual%20Language%20Tracking%20%28VLT%29%20enhances%20single%20object%20tracking%20%28SOT%29%20by%0Aintegrating%20natural%20language%20descriptions%20from%20a%20video%2C%20for%20the%20precise%0Atracking%20of%20a%20specified%20object.%20By%20leveraging%20high-level%20semantic%20information%2C%0AVLT%20guides%20object%20tracking%2C%20alleviating%20the%20constraints%20associated%20with%20relying%0Aon%20a%20visual%20modality.%20Nevertheless%2C%20most%20VLT%20benchmarks%20are%20annotated%20in%20a%0Asingle%20granularity%20and%20lack%20a%20coherent%20semantic%20framework%20to%20provide%20scientific%0Aguidance.%20Moreover%2C%20coordinating%20human%20annotators%20for%20high-quality%20annotations%0Ais%20laborious%20and%20time-consuming.%20To%20address%20these%20challenges%2C%20we%20introduce%0ADTLLM-VLT%2C%20which%20automatically%20generates%20extensive%20and%20multi-granularity%20text%0Ato%20enhance%20environmental%20diversity.%20%281%29%20DTLLM-VLT%20generates%20scientific%20and%0Amulti-granularity%20text%20descriptions%20using%20a%20cohesive%20prompt%20framework.%20Its%0Asuccinct%20and%20highly%20adaptable%20design%20allows%20seamless%20integration%20into%20various%0Avisual%20tracking%20benchmarks.%20%282%29%20We%20select%20three%20prominent%20benchmarks%20to%20deploy%0Aour%20approach%3A%20short-term%20tracking%2C%20long-term%20tracking%2C%20and%20global%20instance%0Atracking.%20We%20offer%20four%20granularity%20combinations%20for%20these%20benchmarks%2C%0Aconsidering%20the%20extent%20and%20density%20of%20semantic%20information%2C%20thereby%20showcasing%0Athe%20practicality%20and%20versatility%20of%20DTLLM-VLT.%20%283%29%20We%20conduct%20comparative%0Aexperiments%20on%20VLT%20benchmarks%20with%20different%20text%20granularities%2C%20evaluating%20and%0Aanalyzing%20the%20impact%20of%20diverse%20text%20on%20tracking%20performance.%20Conclusionally%2C%0Athis%20work%20leverages%20LLM%20to%20provide%20multi-granularity%20semantic%20information%20for%0AVLT%20task%20from%20efficient%20and%20diverse%20perspectives%2C%20enabling%20fine-grained%0Aevaluation%20of%20multi-modal%20trackers.%20In%20the%20future%2C%20we%20believe%20this%20work%20can%20be%0Aextended%20to%20more%20datasets%20to%20support%20vision%20datasets%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12139v2&entry.124074799=Read"},
{"title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate", "author": "Qidong Huang and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Jiaqi Wang and Dahua Lin and Weiming Zhang and Nenghai Yu", "abstract": "  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n", "link": "http://arxiv.org/abs/2410.07167v1", "date": "2024-10-09", "relevancy": 2.8178, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deciphering%20Cross-Modal%20Alignment%20in%20Large%20Vision-Language%20Models%20with%0A%20%20Modality%20Integration%20Rate&body=Title%3A%20Deciphering%20Cross-Modal%20Alignment%20in%20Large%20Vision-Language%20Models%20with%0A%20%20Modality%20Integration%20Rate%0AAuthor%3A%20Qidong%20Huang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%20and%20Weiming%20Zhang%20and%20Nenghai%20Yu%0AAbstract%3A%20%20%20We%20present%20the%20Modality%20Integration%20Rate%20%28MIR%29%2C%20an%20effective%2C%20robust%2C%20and%0Ageneralized%20metric%20to%20indicate%20the%20multi-modal%20pre-training%20quality%20of%20Large%0AVision%20Language%20Models%20%28LVLMs%29.%20Large-scale%20pre-training%20plays%20a%20critical%20role%0Ain%20building%20capable%20LVLMs%2C%20while%20evaluating%20its%20training%20quality%20without%20the%0Acostly%20supervised%20fine-tuning%20stage%20is%20under-explored.%20Loss%2C%20perplexity%2C%20and%0Ain-context%20evaluation%20results%20are%20commonly%20used%20pre-training%20metrics%20for%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20while%20we%20observed%20that%20these%20metrics%20are%20less%0Aindicative%20when%20aligning%20a%20well-trained%20LLM%20with%20a%20new%20modality.%20Due%20to%20the%0Alack%20of%20proper%20metrics%2C%20the%20research%20of%20LVLMs%20in%20the%20critical%20pre-training%0Astage%20is%20hindered%20greatly%2C%20including%20the%20training%20data%20choice%2C%20efficient%20module%0Adesign%2C%20etc.%20In%20this%20paper%2C%20we%20propose%20evaluating%20the%20pre-training%20quality%20from%0Athe%20inter-modal%20distribution%20distance%20perspective%20and%20present%20MIR%2C%20the%20Modality%0AIntegration%20Rate%2C%20which%20is%201%29%20%5Ctextbf%7BEffective%7D%20to%20represent%20the%20pre-training%0Aquality%20and%20show%20a%20positive%20relation%20with%20the%20benchmark%20performance%20after%0Asupervised%20fine-tuning.%202%29%20%5Ctextbf%7BRobust%7D%20toward%20different%20training/evaluation%0Adata.%203%29%20%5Ctextbf%7BGeneralize%7D%20across%20training%20configurations%20and%20architecture%0Achoices.%20We%20conduct%20a%20series%20of%20pre-training%20experiments%20to%20explore%20the%0Aeffectiveness%20of%20MIR%20and%20observe%20satisfactory%20results%20that%20MIR%20is%20indicative%0Aabout%20training%20data%20selection%2C%20training%20strategy%20schedule%2C%20and%20model%0Aarchitecture%20design%20to%20get%20better%20pre-training%20results.%20We%20hope%20MIR%20could%20be%20a%0Ahelpful%20metric%20for%20building%20capable%20LVLMs%20and%20inspire%20the%20following%20research%0Aabout%20modality%20alignment%20in%20different%20areas.%20Our%20code%20is%20at%3A%0Ahttps%3A//github.com/shikiw/Modality-Integration-Rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeciphering%2520Cross-Modal%2520Alignment%2520in%2520Large%2520Vision-Language%2520Models%2520with%250A%2520%2520Modality%2520Integration%2520Rate%26entry.906535625%3DQidong%2520Huang%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Jiaqi%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Weiming%2520Zhang%2520and%2520Nenghai%2520Yu%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Modality%2520Integration%2520Rate%2520%2528MIR%2529%252C%2520an%2520effective%252C%2520robust%252C%2520and%250Ageneralized%2520metric%2520to%2520indicate%2520the%2520multi-modal%2520pre-training%2520quality%2520of%2520Large%250AVision%2520Language%2520Models%2520%2528LVLMs%2529.%2520Large-scale%2520pre-training%2520plays%2520a%2520critical%2520role%250Ain%2520building%2520capable%2520LVLMs%252C%2520while%2520evaluating%2520its%2520training%2520quality%2520without%2520the%250Acostly%2520supervised%2520fine-tuning%2520stage%2520is%2520under-explored.%2520Loss%252C%2520perplexity%252C%2520and%250Ain-context%2520evaluation%2520results%2520are%2520commonly%2520used%2520pre-training%2520metrics%2520for%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520while%2520we%2520observed%2520that%2520these%2520metrics%2520are%2520less%250Aindicative%2520when%2520aligning%2520a%2520well-trained%2520LLM%2520with%2520a%2520new%2520modality.%2520Due%2520to%2520the%250Alack%2520of%2520proper%2520metrics%252C%2520the%2520research%2520of%2520LVLMs%2520in%2520the%2520critical%2520pre-training%250Astage%2520is%2520hindered%2520greatly%252C%2520including%2520the%2520training%2520data%2520choice%252C%2520efficient%2520module%250Adesign%252C%2520etc.%2520In%2520this%2520paper%252C%2520we%2520propose%2520evaluating%2520the%2520pre-training%2520quality%2520from%250Athe%2520inter-modal%2520distribution%2520distance%2520perspective%2520and%2520present%2520MIR%252C%2520the%2520Modality%250AIntegration%2520Rate%252C%2520which%2520is%25201%2529%2520%255Ctextbf%257BEffective%257D%2520to%2520represent%2520the%2520pre-training%250Aquality%2520and%2520show%2520a%2520positive%2520relation%2520with%2520the%2520benchmark%2520performance%2520after%250Asupervised%2520fine-tuning.%25202%2529%2520%255Ctextbf%257BRobust%257D%2520toward%2520different%2520training/evaluation%250Adata.%25203%2529%2520%255Ctextbf%257BGeneralize%257D%2520across%2520training%2520configurations%2520and%2520architecture%250Achoices.%2520We%2520conduct%2520a%2520series%2520of%2520pre-training%2520experiments%2520to%2520explore%2520the%250Aeffectiveness%2520of%2520MIR%2520and%2520observe%2520satisfactory%2520results%2520that%2520MIR%2520is%2520indicative%250Aabout%2520training%2520data%2520selection%252C%2520training%2520strategy%2520schedule%252C%2520and%2520model%250Aarchitecture%2520design%2520to%2520get%2520better%2520pre-training%2520results.%2520We%2520hope%2520MIR%2520could%2520be%2520a%250Ahelpful%2520metric%2520for%2520building%2520capable%2520LVLMs%2520and%2520inspire%2520the%2520following%2520research%250Aabout%2520modality%2520alignment%2520in%2520different%2520areas.%2520Our%2520code%2520is%2520at%253A%250Ahttps%253A//github.com/shikiw/Modality-Integration-Rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20Cross-Modal%20Alignment%20in%20Large%20Vision-Language%20Models%20with%0A%20%20Modality%20Integration%20Rate&entry.906535625=Qidong%20Huang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%20and%20Weiming%20Zhang%20and%20Nenghai%20Yu&entry.1292438233=%20%20We%20present%20the%20Modality%20Integration%20Rate%20%28MIR%29%2C%20an%20effective%2C%20robust%2C%20and%0Ageneralized%20metric%20to%20indicate%20the%20multi-modal%20pre-training%20quality%20of%20Large%0AVision%20Language%20Models%20%28LVLMs%29.%20Large-scale%20pre-training%20plays%20a%20critical%20role%0Ain%20building%20capable%20LVLMs%2C%20while%20evaluating%20its%20training%20quality%20without%20the%0Acostly%20supervised%20fine-tuning%20stage%20is%20under-explored.%20Loss%2C%20perplexity%2C%20and%0Ain-context%20evaluation%20results%20are%20commonly%20used%20pre-training%20metrics%20for%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20while%20we%20observed%20that%20these%20metrics%20are%20less%0Aindicative%20when%20aligning%20a%20well-trained%20LLM%20with%20a%20new%20modality.%20Due%20to%20the%0Alack%20of%20proper%20metrics%2C%20the%20research%20of%20LVLMs%20in%20the%20critical%20pre-training%0Astage%20is%20hindered%20greatly%2C%20including%20the%20training%20data%20choice%2C%20efficient%20module%0Adesign%2C%20etc.%20In%20this%20paper%2C%20we%20propose%20evaluating%20the%20pre-training%20quality%20from%0Athe%20inter-modal%20distribution%20distance%20perspective%20and%20present%20MIR%2C%20the%20Modality%0AIntegration%20Rate%2C%20which%20is%201%29%20%5Ctextbf%7BEffective%7D%20to%20represent%20the%20pre-training%0Aquality%20and%20show%20a%20positive%20relation%20with%20the%20benchmark%20performance%20after%0Asupervised%20fine-tuning.%202%29%20%5Ctextbf%7BRobust%7D%20toward%20different%20training/evaluation%0Adata.%203%29%20%5Ctextbf%7BGeneralize%7D%20across%20training%20configurations%20and%20architecture%0Achoices.%20We%20conduct%20a%20series%20of%20pre-training%20experiments%20to%20explore%20the%0Aeffectiveness%20of%20MIR%20and%20observe%20satisfactory%20results%20that%20MIR%20is%20indicative%0Aabout%20training%20data%20selection%2C%20training%20strategy%20schedule%2C%20and%20model%0Aarchitecture%20design%20to%20get%20better%20pre-training%20results.%20We%20hope%20MIR%20could%20be%20a%0Ahelpful%20metric%20for%20building%20capable%20LVLMs%20and%20inspire%20the%20following%20research%0Aabout%20modality%20alignment%20in%20different%20areas.%20Our%20code%20is%20at%3A%0Ahttps%3A//github.com/shikiw/Modality-Integration-Rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07167v1&entry.124074799=Read"},
{"title": "SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric\n  hybrid solution", "author": "Hala Djeghim and Nathan Piasco and Moussab Bennehar and Luis Rold\u00e3o and Dzmitry Tsishkou and D\u00e9sir\u00e9 Sidib\u00e9", "abstract": "  Neural implicit surface representation methods have recently shown impressive\n3D reconstruction results. However, existing solutions struggle to reconstruct\nurban outdoor scenes due to their large, unbounded, and highly detailed nature.\nHence, to achieve accurate reconstructions, additional supervision data such as\nLiDAR, strong geometric priors, and long training times are required. To tackle\nsuch issues, we present SCILLA, a new hybrid implicit surface learning method\nto reconstruct large driving scenes from 2D images. SCILLA's hybrid\narchitecture models two separate implicit fields: one for the volumetric\ndensity and another for the signed distance to the surface. To accurately\nrepresent urban outdoor scenarios, we introduce a novel volume-rendering\nstrategy that relies on self-supervised probabilistic density estimation to\nsample points near the surface and transition progressively from volumetric to\nsurface representation. Our solution permits a proper and fast initialization\nof the signed distance field without relying on any geometric prior on the\nscene, compared to concurrent methods. By conducting extensive experiments on\nfour outdoor driving datasets, we show that SCILLA can learn an accurate and\ndetailed 3D surface scene representation in various urban scenarios while being\ntwo times faster to train compared to previous state-of-the-art solutions.\n", "link": "http://arxiv.org/abs/2403.10344v3", "date": "2024-10-09", "relevancy": 2.8042, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5781}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5638}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCILLA%3A%20SurfaCe%20Implicit%20Learning%20for%20Large%20Urban%20Area%2C%20a%20volumetric%0A%20%20hybrid%20solution&body=Title%3A%20SCILLA%3A%20SurfaCe%20Implicit%20Learning%20for%20Large%20Urban%20Area%2C%20a%20volumetric%0A%20%20hybrid%20solution%0AAuthor%3A%20Hala%20Djeghim%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%20and%20D%C3%A9sir%C3%A9%20Sidib%C3%A9%0AAbstract%3A%20%20%20Neural%20implicit%20surface%20representation%20methods%20have%20recently%20shown%20impressive%0A3D%20reconstruction%20results.%20However%2C%20existing%20solutions%20struggle%20to%20reconstruct%0Aurban%20outdoor%20scenes%20due%20to%20their%20large%2C%20unbounded%2C%20and%20highly%20detailed%20nature.%0AHence%2C%20to%20achieve%20accurate%20reconstructions%2C%20additional%20supervision%20data%20such%20as%0ALiDAR%2C%20strong%20geometric%20priors%2C%20and%20long%20training%20times%20are%20required.%20To%20tackle%0Asuch%20issues%2C%20we%20present%20SCILLA%2C%20a%20new%20hybrid%20implicit%20surface%20learning%20method%0Ato%20reconstruct%20large%20driving%20scenes%20from%202D%20images.%20SCILLA%27s%20hybrid%0Aarchitecture%20models%20two%20separate%20implicit%20fields%3A%20one%20for%20the%20volumetric%0Adensity%20and%20another%20for%20the%20signed%20distance%20to%20the%20surface.%20To%20accurately%0Arepresent%20urban%20outdoor%20scenarios%2C%20we%20introduce%20a%20novel%20volume-rendering%0Astrategy%20that%20relies%20on%20self-supervised%20probabilistic%20density%20estimation%20to%0Asample%20points%20near%20the%20surface%20and%20transition%20progressively%20from%20volumetric%20to%0Asurface%20representation.%20Our%20solution%20permits%20a%20proper%20and%20fast%20initialization%0Aof%20the%20signed%20distance%20field%20without%20relying%20on%20any%20geometric%20prior%20on%20the%0Ascene%2C%20compared%20to%20concurrent%20methods.%20By%20conducting%20extensive%20experiments%20on%0Afour%20outdoor%20driving%20datasets%2C%20we%20show%20that%20SCILLA%20can%20learn%20an%20accurate%20and%0Adetailed%203D%20surface%20scene%20representation%20in%20various%20urban%20scenarios%20while%20being%0Atwo%20times%20faster%20to%20train%20compared%20to%20previous%20state-of-the-art%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCILLA%253A%2520SurfaCe%2520Implicit%2520Learning%2520for%2520Large%2520Urban%2520Area%252C%2520a%2520volumetric%250A%2520%2520hybrid%2520solution%26entry.906535625%3DHala%2520Djeghim%2520and%2520Nathan%2520Piasco%2520and%2520Moussab%2520Bennehar%2520and%2520Luis%2520Rold%25C3%25A3o%2520and%2520Dzmitry%2520Tsishkou%2520and%2520D%25C3%25A9sir%25C3%25A9%2520Sidib%25C3%25A9%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520surface%2520representation%2520methods%2520have%2520recently%2520shown%2520impressive%250A3D%2520reconstruction%2520results.%2520However%252C%2520existing%2520solutions%2520struggle%2520to%2520reconstruct%250Aurban%2520outdoor%2520scenes%2520due%2520to%2520their%2520large%252C%2520unbounded%252C%2520and%2520highly%2520detailed%2520nature.%250AHence%252C%2520to%2520achieve%2520accurate%2520reconstructions%252C%2520additional%2520supervision%2520data%2520such%2520as%250ALiDAR%252C%2520strong%2520geometric%2520priors%252C%2520and%2520long%2520training%2520times%2520are%2520required.%2520To%2520tackle%250Asuch%2520issues%252C%2520we%2520present%2520SCILLA%252C%2520a%2520new%2520hybrid%2520implicit%2520surface%2520learning%2520method%250Ato%2520reconstruct%2520large%2520driving%2520scenes%2520from%25202D%2520images.%2520SCILLA%2527s%2520hybrid%250Aarchitecture%2520models%2520two%2520separate%2520implicit%2520fields%253A%2520one%2520for%2520the%2520volumetric%250Adensity%2520and%2520another%2520for%2520the%2520signed%2520distance%2520to%2520the%2520surface.%2520To%2520accurately%250Arepresent%2520urban%2520outdoor%2520scenarios%252C%2520we%2520introduce%2520a%2520novel%2520volume-rendering%250Astrategy%2520that%2520relies%2520on%2520self-supervised%2520probabilistic%2520density%2520estimation%2520to%250Asample%2520points%2520near%2520the%2520surface%2520and%2520transition%2520progressively%2520from%2520volumetric%2520to%250Asurface%2520representation.%2520Our%2520solution%2520permits%2520a%2520proper%2520and%2520fast%2520initialization%250Aof%2520the%2520signed%2520distance%2520field%2520without%2520relying%2520on%2520any%2520geometric%2520prior%2520on%2520the%250Ascene%252C%2520compared%2520to%2520concurrent%2520methods.%2520By%2520conducting%2520extensive%2520experiments%2520on%250Afour%2520outdoor%2520driving%2520datasets%252C%2520we%2520show%2520that%2520SCILLA%2520can%2520learn%2520an%2520accurate%2520and%250Adetailed%25203D%2520surface%2520scene%2520representation%2520in%2520various%2520urban%2520scenarios%2520while%2520being%250Atwo%2520times%2520faster%2520to%2520train%2520compared%2520to%2520previous%2520state-of-the-art%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCILLA%3A%20SurfaCe%20Implicit%20Learning%20for%20Large%20Urban%20Area%2C%20a%20volumetric%0A%20%20hybrid%20solution&entry.906535625=Hala%20Djeghim%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%20and%20D%C3%A9sir%C3%A9%20Sidib%C3%A9&entry.1292438233=%20%20Neural%20implicit%20surface%20representation%20methods%20have%20recently%20shown%20impressive%0A3D%20reconstruction%20results.%20However%2C%20existing%20solutions%20struggle%20to%20reconstruct%0Aurban%20outdoor%20scenes%20due%20to%20their%20large%2C%20unbounded%2C%20and%20highly%20detailed%20nature.%0AHence%2C%20to%20achieve%20accurate%20reconstructions%2C%20additional%20supervision%20data%20such%20as%0ALiDAR%2C%20strong%20geometric%20priors%2C%20and%20long%20training%20times%20are%20required.%20To%20tackle%0Asuch%20issues%2C%20we%20present%20SCILLA%2C%20a%20new%20hybrid%20implicit%20surface%20learning%20method%0Ato%20reconstruct%20large%20driving%20scenes%20from%202D%20images.%20SCILLA%27s%20hybrid%0Aarchitecture%20models%20two%20separate%20implicit%20fields%3A%20one%20for%20the%20volumetric%0Adensity%20and%20another%20for%20the%20signed%20distance%20to%20the%20surface.%20To%20accurately%0Arepresent%20urban%20outdoor%20scenarios%2C%20we%20introduce%20a%20novel%20volume-rendering%0Astrategy%20that%20relies%20on%20self-supervised%20probabilistic%20density%20estimation%20to%0Asample%20points%20near%20the%20surface%20and%20transition%20progressively%20from%20volumetric%20to%0Asurface%20representation.%20Our%20solution%20permits%20a%20proper%20and%20fast%20initialization%0Aof%20the%20signed%20distance%20field%20without%20relying%20on%20any%20geometric%20prior%20on%20the%0Ascene%2C%20compared%20to%20concurrent%20methods.%20By%20conducting%20extensive%20experiments%20on%0Afour%20outdoor%20driving%20datasets%2C%20we%20show%20that%20SCILLA%20can%20learn%20an%20accurate%20and%0Adetailed%203D%20surface%20scene%20representation%20in%20various%20urban%20scenarios%20while%20being%0Atwo%20times%20faster%20to%20train%20compared%20to%20previous%20state-of-the-art%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10344v3&entry.124074799=Read"},
{"title": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language\n  Model Inference", "author": "Yuan Zhang and Chun-Kai Fan and Junpeng Ma and Wenzhao Zheng and Tao Huang and Kuan Cheng and Denis Gudovskiy and Tomoyuki Okuno and Yohei Nakata and Kurt Keutzer and Shanghang Zhang", "abstract": "  In vision-language models (VLMs), visual tokens usually consume a significant\namount of computational overhead, despite their sparser information density\ncompared to text tokens. To address this, most existing methods learn a network\nto prune redundant visual tokens and require additional training data.\nDifferently, we propose an efficient training-free token optimization mechanism\ndubbed SparseVLM without extra parameters or fine-tuning costs. Concretely,\ngiven that visual tokens complement text tokens in VLMs for linguistic\nreasoning, we select visual-relevant text tokens to rate the significance of\nvision tokens within the self-attention matrix extracted from the VLMs. Then we\nprogressively prune irrelevant tokens. To maximize sparsity while retaining\nessential information, we introduce a rank-based strategy to adaptively\ndetermine the sparsification ratio for each layer, alongside a token recycling\nmethod that compresses pruned tokens into more compact representations.\nExperimental results show that our SparseVLM improves the efficiency of various\nVLMs across a range of image and video understanding tasks. In particular,\nLLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio\nof 78% while maintaining 93% of the accuracy. Our code is available at\nhttps://github.com/Gumpest/SparseVLMs.\n", "link": "http://arxiv.org/abs/2410.04417v2", "date": "2024-10-09", "relevancy": 2.7589, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseVLM%3A%20Visual%20Token%20Sparsification%20for%20Efficient%20Vision-Language%0A%20%20Model%20Inference&body=Title%3A%20SparseVLM%3A%20Visual%20Token%20Sparsification%20for%20Efficient%20Vision-Language%0A%20%20Model%20Inference%0AAuthor%3A%20Yuan%20Zhang%20and%20Chun-Kai%20Fan%20and%20Junpeng%20Ma%20and%20Wenzhao%20Zheng%20and%20Tao%20Huang%20and%20Kuan%20Cheng%20and%20Denis%20Gudovskiy%20and%20Tomoyuki%20Okuno%20and%20Yohei%20Nakata%20and%20Kurt%20Keutzer%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20In%20vision-language%20models%20%28VLMs%29%2C%20visual%20tokens%20usually%20consume%20a%20significant%0Aamount%20of%20computational%20overhead%2C%20despite%20their%20sparser%20information%20density%0Acompared%20to%20text%20tokens.%20To%20address%20this%2C%20most%20existing%20methods%20learn%20a%20network%0Ato%20prune%20redundant%20visual%20tokens%20and%20require%20additional%20training%20data.%0ADifferently%2C%20we%20propose%20an%20efficient%20training-free%20token%20optimization%20mechanism%0Adubbed%20SparseVLM%20without%20extra%20parameters%20or%20fine-tuning%20costs.%20Concretely%2C%0Agiven%20that%20visual%20tokens%20complement%20text%20tokens%20in%20VLMs%20for%20linguistic%0Areasoning%2C%20we%20select%20visual-relevant%20text%20tokens%20to%20rate%20the%20significance%20of%0Avision%20tokens%20within%20the%20self-attention%20matrix%20extracted%20from%20the%20VLMs.%20Then%20we%0Aprogressively%20prune%20irrelevant%20tokens.%20To%20maximize%20sparsity%20while%20retaining%0Aessential%20information%2C%20we%20introduce%20a%20rank-based%20strategy%20to%20adaptively%0Adetermine%20the%20sparsification%20ratio%20for%20each%20layer%2C%20alongside%20a%20token%20recycling%0Amethod%20that%20compresses%20pruned%20tokens%20into%20more%20compact%20representations.%0AExperimental%20results%20show%20that%20our%20SparseVLM%20improves%20the%20efficiency%20of%20various%0AVLMs%20across%20a%20range%20of%20image%20and%20video%20understanding%20tasks.%20In%20particular%2C%0ALLaVA%20equipped%20with%20SparseVLM%20reduces%2061%25%20to%2067%25%20FLOPs%20with%20a%20compression%20ratio%0Aof%2078%25%20while%20maintaining%2093%25%20of%20the%20accuracy.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Gumpest/SparseVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseVLM%253A%2520Visual%2520Token%2520Sparsification%2520for%2520Efficient%2520Vision-Language%250A%2520%2520Model%2520Inference%26entry.906535625%3DYuan%2520Zhang%2520and%2520Chun-Kai%2520Fan%2520and%2520Junpeng%2520Ma%2520and%2520Wenzhao%2520Zheng%2520and%2520Tao%2520Huang%2520and%2520Kuan%2520Cheng%2520and%2520Denis%2520Gudovskiy%2520and%2520Tomoyuki%2520Okuno%2520and%2520Yohei%2520Nakata%2520and%2520Kurt%2520Keutzer%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520In%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520visual%2520tokens%2520usually%2520consume%2520a%2520significant%250Aamount%2520of%2520computational%2520overhead%252C%2520despite%2520their%2520sparser%2520information%2520density%250Acompared%2520to%2520text%2520tokens.%2520To%2520address%2520this%252C%2520most%2520existing%2520methods%2520learn%2520a%2520network%250Ato%2520prune%2520redundant%2520visual%2520tokens%2520and%2520require%2520additional%2520training%2520data.%250ADifferently%252C%2520we%2520propose%2520an%2520efficient%2520training-free%2520token%2520optimization%2520mechanism%250Adubbed%2520SparseVLM%2520without%2520extra%2520parameters%2520or%2520fine-tuning%2520costs.%2520Concretely%252C%250Agiven%2520that%2520visual%2520tokens%2520complement%2520text%2520tokens%2520in%2520VLMs%2520for%2520linguistic%250Areasoning%252C%2520we%2520select%2520visual-relevant%2520text%2520tokens%2520to%2520rate%2520the%2520significance%2520of%250Avision%2520tokens%2520within%2520the%2520self-attention%2520matrix%2520extracted%2520from%2520the%2520VLMs.%2520Then%2520we%250Aprogressively%2520prune%2520irrelevant%2520tokens.%2520To%2520maximize%2520sparsity%2520while%2520retaining%250Aessential%2520information%252C%2520we%2520introduce%2520a%2520rank-based%2520strategy%2520to%2520adaptively%250Adetermine%2520the%2520sparsification%2520ratio%2520for%2520each%2520layer%252C%2520alongside%2520a%2520token%2520recycling%250Amethod%2520that%2520compresses%2520pruned%2520tokens%2520into%2520more%2520compact%2520representations.%250AExperimental%2520results%2520show%2520that%2520our%2520SparseVLM%2520improves%2520the%2520efficiency%2520of%2520various%250AVLMs%2520across%2520a%2520range%2520of%2520image%2520and%2520video%2520understanding%2520tasks.%2520In%2520particular%252C%250ALLaVA%2520equipped%2520with%2520SparseVLM%2520reduces%252061%2525%2520to%252067%2525%2520FLOPs%2520with%2520a%2520compression%2520ratio%250Aof%252078%2525%2520while%2520maintaining%252093%2525%2520of%2520the%2520accuracy.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Gumpest/SparseVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseVLM%3A%20Visual%20Token%20Sparsification%20for%20Efficient%20Vision-Language%0A%20%20Model%20Inference&entry.906535625=Yuan%20Zhang%20and%20Chun-Kai%20Fan%20and%20Junpeng%20Ma%20and%20Wenzhao%20Zheng%20and%20Tao%20Huang%20and%20Kuan%20Cheng%20and%20Denis%20Gudovskiy%20and%20Tomoyuki%20Okuno%20and%20Yohei%20Nakata%20and%20Kurt%20Keutzer%20and%20Shanghang%20Zhang&entry.1292438233=%20%20In%20vision-language%20models%20%28VLMs%29%2C%20visual%20tokens%20usually%20consume%20a%20significant%0Aamount%20of%20computational%20overhead%2C%20despite%20their%20sparser%20information%20density%0Acompared%20to%20text%20tokens.%20To%20address%20this%2C%20most%20existing%20methods%20learn%20a%20network%0Ato%20prune%20redundant%20visual%20tokens%20and%20require%20additional%20training%20data.%0ADifferently%2C%20we%20propose%20an%20efficient%20training-free%20token%20optimization%20mechanism%0Adubbed%20SparseVLM%20without%20extra%20parameters%20or%20fine-tuning%20costs.%20Concretely%2C%0Agiven%20that%20visual%20tokens%20complement%20text%20tokens%20in%20VLMs%20for%20linguistic%0Areasoning%2C%20we%20select%20visual-relevant%20text%20tokens%20to%20rate%20the%20significance%20of%0Avision%20tokens%20within%20the%20self-attention%20matrix%20extracted%20from%20the%20VLMs.%20Then%20we%0Aprogressively%20prune%20irrelevant%20tokens.%20To%20maximize%20sparsity%20while%20retaining%0Aessential%20information%2C%20we%20introduce%20a%20rank-based%20strategy%20to%20adaptively%0Adetermine%20the%20sparsification%20ratio%20for%20each%20layer%2C%20alongside%20a%20token%20recycling%0Amethod%20that%20compresses%20pruned%20tokens%20into%20more%20compact%20representations.%0AExperimental%20results%20show%20that%20our%20SparseVLM%20improves%20the%20efficiency%20of%20various%0AVLMs%20across%20a%20range%20of%20image%20and%20video%20understanding%20tasks.%20In%20particular%2C%0ALLaVA%20equipped%20with%20SparseVLM%20reduces%2061%25%20to%2067%25%20FLOPs%20with%20a%20compression%20ratio%0Aof%2078%25%20while%20maintaining%2093%25%20of%20the%20accuracy.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Gumpest/SparseVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04417v2&entry.124074799=Read"},
{"title": "VHELM: A Holistic Evaluation of Vision Language Models", "author": "Tony Lee and Haoqin Tu and Chi Heem Wong and Wenhao Zheng and Yiyang Zhou and Yifan Mai and Josselin Somerville Roberts and Michihiro Yasunaga and Huaxiu Yao and Cihang Xie and Percy Liang", "abstract": "  Current benchmarks for assessing vision-language models (VLMs) often focus on\ntheir perception or problem-solving capabilities and neglect other critical\naspects such as fairness, multilinguality, or toxicity. Furthermore, they\ndiffer in their evaluation procedures and the scope of the evaluation, making\nit difficult to compare models. To address these issues, we extend the HELM\nframework to VLMs to present the Holistic Evaluation of Vision Language Models\n(VHELM). VHELM aggregates various datasets to cover one or more of the 9\naspects: visual perception, knowledge, reasoning, bias, fairness,\nmultilinguality, robustness, toxicity, and safety. In doing so, we produce a\ncomprehensive, multi-dimensional view of the capabilities of the VLMs across\nthese important factors. In addition, we standardize the standard inference\nparameters, methods of prompting, and evaluation metrics to enable fair\ncomparisons across models. Our framework is designed to be lightweight and\nautomatic so that evaluation runs are cheap and fast. Our initial run evaluates\n22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.\nWe uncover new key findings, such as the fact that efficiency-focused models\n(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than\ntheir full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark\nbut not when evaluated on the other aspects. For transparency, we release the\nraw model generations and complete results on our website\n(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living\nbenchmark, and we hope to continue adding new datasets and models over time.\n", "link": "http://arxiv.org/abs/2410.07112v1", "date": "2024-10-09", "relevancy": 2.7466, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VHELM%3A%20A%20Holistic%20Evaluation%20of%20Vision%20Language%20Models&body=Title%3A%20VHELM%3A%20A%20Holistic%20Evaluation%20of%20Vision%20Language%20Models%0AAuthor%3A%20Tony%20Lee%20and%20Haoqin%20Tu%20and%20Chi%20Heem%20Wong%20and%20Wenhao%20Zheng%20and%20Yiyang%20Zhou%20and%20Yifan%20Mai%20and%20Josselin%20Somerville%20Roberts%20and%20Michihiro%20Yasunaga%20and%20Huaxiu%20Yao%20and%20Cihang%20Xie%20and%20Percy%20Liang%0AAbstract%3A%20%20%20Current%20benchmarks%20for%20assessing%20vision-language%20models%20%28VLMs%29%20often%20focus%20on%0Atheir%20perception%20or%20problem-solving%20capabilities%20and%20neglect%20other%20critical%0Aaspects%20such%20as%20fairness%2C%20multilinguality%2C%20or%20toxicity.%20Furthermore%2C%20they%0Adiffer%20in%20their%20evaluation%20procedures%20and%20the%20scope%20of%20the%20evaluation%2C%20making%0Ait%20difficult%20to%20compare%20models.%20To%20address%20these%20issues%2C%20we%20extend%20the%20HELM%0Aframework%20to%20VLMs%20to%20present%20the%20Holistic%20Evaluation%20of%20Vision%20Language%20Models%0A%28VHELM%29.%20VHELM%20aggregates%20various%20datasets%20to%20cover%20one%20or%20more%20of%20the%209%0Aaspects%3A%20visual%20perception%2C%20knowledge%2C%20reasoning%2C%20bias%2C%20fairness%2C%0Amultilinguality%2C%20robustness%2C%20toxicity%2C%20and%20safety.%20In%20doing%20so%2C%20we%20produce%20a%0Acomprehensive%2C%20multi-dimensional%20view%20of%20the%20capabilities%20of%20the%20VLMs%20across%0Athese%20important%20factors.%20In%20addition%2C%20we%20standardize%20the%20standard%20inference%0Aparameters%2C%20methods%20of%20prompting%2C%20and%20evaluation%20metrics%20to%20enable%20fair%0Acomparisons%20across%20models.%20Our%20framework%20is%20designed%20to%20be%20lightweight%20and%0Aautomatic%20so%20that%20evaluation%20runs%20are%20cheap%20and%20fast.%20Our%20initial%20run%20evaluates%0A22%20VLMs%20on%2021%20existing%20datasets%20to%20provide%20a%20holistic%20snapshot%20of%20the%20models.%0AWe%20uncover%20new%20key%20findings%2C%20such%20as%20the%20fact%20that%20efficiency-focused%20models%0A%28e.g.%2C%20Claude%203%20Haiku%20or%20Gemini%201.5%20Flash%29%20perform%20significantly%20worse%20than%0Atheir%20full%20models%20%28e.g.%2C%20Claude%203%20Opus%20or%20Gemini%201.5%20Pro%29%20on%20the%20bias%20benchmark%0Abut%20not%20when%20evaluated%20on%20the%20other%20aspects.%20For%20transparency%2C%20we%20release%20the%0Araw%20model%20generations%20and%20complete%20results%20on%20our%20website%0A%28https%3A//crfm.stanford.edu/helm/vhelm/v2.0.1%29.%20VHELM%20is%20intended%20to%20be%20a%20living%0Abenchmark%2C%20and%20we%20hope%20to%20continue%20adding%20new%20datasets%20and%20models%20over%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVHELM%253A%2520A%2520Holistic%2520Evaluation%2520of%2520Vision%2520Language%2520Models%26entry.906535625%3DTony%2520Lee%2520and%2520Haoqin%2520Tu%2520and%2520Chi%2520Heem%2520Wong%2520and%2520Wenhao%2520Zheng%2520and%2520Yiyang%2520Zhou%2520and%2520Yifan%2520Mai%2520and%2520Josselin%2520Somerville%2520Roberts%2520and%2520Michihiro%2520Yasunaga%2520and%2520Huaxiu%2520Yao%2520and%2520Cihang%2520Xie%2520and%2520Percy%2520Liang%26entry.1292438233%3D%2520%2520Current%2520benchmarks%2520for%2520assessing%2520vision-language%2520models%2520%2528VLMs%2529%2520often%2520focus%2520on%250Atheir%2520perception%2520or%2520problem-solving%2520capabilities%2520and%2520neglect%2520other%2520critical%250Aaspects%2520such%2520as%2520fairness%252C%2520multilinguality%252C%2520or%2520toxicity.%2520Furthermore%252C%2520they%250Adiffer%2520in%2520their%2520evaluation%2520procedures%2520and%2520the%2520scope%2520of%2520the%2520evaluation%252C%2520making%250Ait%2520difficult%2520to%2520compare%2520models.%2520To%2520address%2520these%2520issues%252C%2520we%2520extend%2520the%2520HELM%250Aframework%2520to%2520VLMs%2520to%2520present%2520the%2520Holistic%2520Evaluation%2520of%2520Vision%2520Language%2520Models%250A%2528VHELM%2529.%2520VHELM%2520aggregates%2520various%2520datasets%2520to%2520cover%2520one%2520or%2520more%2520of%2520the%25209%250Aaspects%253A%2520visual%2520perception%252C%2520knowledge%252C%2520reasoning%252C%2520bias%252C%2520fairness%252C%250Amultilinguality%252C%2520robustness%252C%2520toxicity%252C%2520and%2520safety.%2520In%2520doing%2520so%252C%2520we%2520produce%2520a%250Acomprehensive%252C%2520multi-dimensional%2520view%2520of%2520the%2520capabilities%2520of%2520the%2520VLMs%2520across%250Athese%2520important%2520factors.%2520In%2520addition%252C%2520we%2520standardize%2520the%2520standard%2520inference%250Aparameters%252C%2520methods%2520of%2520prompting%252C%2520and%2520evaluation%2520metrics%2520to%2520enable%2520fair%250Acomparisons%2520across%2520models.%2520Our%2520framework%2520is%2520designed%2520to%2520be%2520lightweight%2520and%250Aautomatic%2520so%2520that%2520evaluation%2520runs%2520are%2520cheap%2520and%2520fast.%2520Our%2520initial%2520run%2520evaluates%250A22%2520VLMs%2520on%252021%2520existing%2520datasets%2520to%2520provide%2520a%2520holistic%2520snapshot%2520of%2520the%2520models.%250AWe%2520uncover%2520new%2520key%2520findings%252C%2520such%2520as%2520the%2520fact%2520that%2520efficiency-focused%2520models%250A%2528e.g.%252C%2520Claude%25203%2520Haiku%2520or%2520Gemini%25201.5%2520Flash%2529%2520perform%2520significantly%2520worse%2520than%250Atheir%2520full%2520models%2520%2528e.g.%252C%2520Claude%25203%2520Opus%2520or%2520Gemini%25201.5%2520Pro%2529%2520on%2520the%2520bias%2520benchmark%250Abut%2520not%2520when%2520evaluated%2520on%2520the%2520other%2520aspects.%2520For%2520transparency%252C%2520we%2520release%2520the%250Araw%2520model%2520generations%2520and%2520complete%2520results%2520on%2520our%2520website%250A%2528https%253A//crfm.stanford.edu/helm/vhelm/v2.0.1%2529.%2520VHELM%2520is%2520intended%2520to%2520be%2520a%2520living%250Abenchmark%252C%2520and%2520we%2520hope%2520to%2520continue%2520adding%2520new%2520datasets%2520and%2520models%2520over%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VHELM%3A%20A%20Holistic%20Evaluation%20of%20Vision%20Language%20Models&entry.906535625=Tony%20Lee%20and%20Haoqin%20Tu%20and%20Chi%20Heem%20Wong%20and%20Wenhao%20Zheng%20and%20Yiyang%20Zhou%20and%20Yifan%20Mai%20and%20Josselin%20Somerville%20Roberts%20and%20Michihiro%20Yasunaga%20and%20Huaxiu%20Yao%20and%20Cihang%20Xie%20and%20Percy%20Liang&entry.1292438233=%20%20Current%20benchmarks%20for%20assessing%20vision-language%20models%20%28VLMs%29%20often%20focus%20on%0Atheir%20perception%20or%20problem-solving%20capabilities%20and%20neglect%20other%20critical%0Aaspects%20such%20as%20fairness%2C%20multilinguality%2C%20or%20toxicity.%20Furthermore%2C%20they%0Adiffer%20in%20their%20evaluation%20procedures%20and%20the%20scope%20of%20the%20evaluation%2C%20making%0Ait%20difficult%20to%20compare%20models.%20To%20address%20these%20issues%2C%20we%20extend%20the%20HELM%0Aframework%20to%20VLMs%20to%20present%20the%20Holistic%20Evaluation%20of%20Vision%20Language%20Models%0A%28VHELM%29.%20VHELM%20aggregates%20various%20datasets%20to%20cover%20one%20or%20more%20of%20the%209%0Aaspects%3A%20visual%20perception%2C%20knowledge%2C%20reasoning%2C%20bias%2C%20fairness%2C%0Amultilinguality%2C%20robustness%2C%20toxicity%2C%20and%20safety.%20In%20doing%20so%2C%20we%20produce%20a%0Acomprehensive%2C%20multi-dimensional%20view%20of%20the%20capabilities%20of%20the%20VLMs%20across%0Athese%20important%20factors.%20In%20addition%2C%20we%20standardize%20the%20standard%20inference%0Aparameters%2C%20methods%20of%20prompting%2C%20and%20evaluation%20metrics%20to%20enable%20fair%0Acomparisons%20across%20models.%20Our%20framework%20is%20designed%20to%20be%20lightweight%20and%0Aautomatic%20so%20that%20evaluation%20runs%20are%20cheap%20and%20fast.%20Our%20initial%20run%20evaluates%0A22%20VLMs%20on%2021%20existing%20datasets%20to%20provide%20a%20holistic%20snapshot%20of%20the%20models.%0AWe%20uncover%20new%20key%20findings%2C%20such%20as%20the%20fact%20that%20efficiency-focused%20models%0A%28e.g.%2C%20Claude%203%20Haiku%20or%20Gemini%201.5%20Flash%29%20perform%20significantly%20worse%20than%0Atheir%20full%20models%20%28e.g.%2C%20Claude%203%20Opus%20or%20Gemini%201.5%20Pro%29%20on%20the%20bias%20benchmark%0Abut%20not%20when%20evaluated%20on%20the%20other%20aspects.%20For%20transparency%2C%20we%20release%20the%0Araw%20model%20generations%20and%20complete%20results%20on%20our%20website%0A%28https%3A//crfm.stanford.edu/helm/vhelm/v2.0.1%29.%20VHELM%20is%20intended%20to%20be%20a%20living%0Abenchmark%2C%20and%20we%20hope%20to%20continue%20adding%20new%20datasets%20and%20models%20over%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07112v1&entry.124074799=Read"},
{"title": "Pixtral 12B", "author": "Pravesh Agrawal and Szymon Antoniak and Emma Bou Hanna and Devendra Chaplot and Jessica Chudnovsky and Saurabh Garg and Theophile Gervet and Soham Ghosh and Am\u00e9lie H\u00e9liou and Paul Jacob and Albert Q. Jiang and Timoth\u00e9e Lacroix and Guillaume Lample and Diego Las Casas and Thibaut Lavril and Teven Le Scao and Andy Lo and William Marshall and Louis Martin and Arthur Mensch and Pavankumar Muddireddy and Valera Nemychnikova and Marie Pellat and Patrick Von Platen and Nikhil Raghuraman and Baptiste Rozi\u00e8re and Alexandre Sablayrolles and Lucile Saulnier and Romain Sauvestre and Wendy Shang and Roman Soletskyi and Lawrence Stewart and Pierre Stock and Joachim Studnia and Sandeep Subramanian and Sagar Vaze and Thomas Wang", "abstract": "  We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.\n", "link": "http://arxiv.org/abs/2410.07073v1", "date": "2024-10-09", "relevancy": 2.7396, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixtral%2012B&body=Title%3A%20Pixtral%2012B%0AAuthor%3A%20Pravesh%20Agrawal%20and%20Szymon%20Antoniak%20and%20Emma%20Bou%20Hanna%20and%20Devendra%20Chaplot%20and%20Jessica%20Chudnovsky%20and%20Saurabh%20Garg%20and%20Theophile%20Gervet%20and%20Soham%20Ghosh%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Paul%20Jacob%20and%20Albert%20Q.%20Jiang%20and%20Timoth%C3%A9e%20Lacroix%20and%20Guillaume%20Lample%20and%20Diego%20Las%20Casas%20and%20Thibaut%20Lavril%20and%20Teven%20Le%20Scao%20and%20Andy%20Lo%20and%20William%20Marshall%20and%20Louis%20Martin%20and%20Arthur%20Mensch%20and%20Pavankumar%20Muddireddy%20and%20Valera%20Nemychnikova%20and%20Marie%20Pellat%20and%20Patrick%20Von%20Platen%20and%20Nikhil%20Raghuraman%20and%20Baptiste%20Rozi%C3%A8re%20and%20Alexandre%20Sablayrolles%20and%20Lucile%20Saulnier%20and%20Romain%20Sauvestre%20and%20Wendy%20Shang%20and%20Roman%20Soletskyi%20and%20Lawrence%20Stewart%20and%20Pierre%20Stock%20and%20Joachim%20Studnia%20and%20Sandeep%20Subramanian%20and%20Sagar%20Vaze%20and%20Thomas%20Wang%0AAbstract%3A%20%20%20We%20introduce%20Pixtral-12B%2C%20a%2012--billion-parameter%20multimodal%20language%20model.%0APixtral-12B%20is%20trained%20to%20understand%20both%20natural%20images%20and%20documents%2C%0Aachieving%20leading%20performance%20on%20various%20multimodal%20benchmarks%2C%20surpassing%20a%0Anumber%20of%20larger%20models.%20Unlike%20many%20open-source%20models%2C%20Pixtral%20is%20also%20a%0Acutting-edge%20text%20model%20for%20its%20size%2C%20and%20does%20not%20compromise%20on%20natural%0Alanguage%20performance%20to%20excel%20in%20multimodal%20tasks.%20Pixtral%20uses%20a%20new%20vision%0Aencoder%20trained%20from%20scratch%2C%20which%20allows%20it%20to%20ingest%20images%20at%20their%20natural%0Aresolution%20and%20aspect%20ratio.%20This%20gives%20users%20flexibility%20on%20the%20number%20of%0Atokens%20used%20to%20process%20an%20image.%20Pixtral%20is%20also%20able%20to%20process%20any%20number%20of%0Aimages%20in%20its%20long%20context%20window%20of%20128K%20tokens.%20Pixtral%2012B%20substanially%0Aoutperforms%20other%20open%20models%20of%20similar%20sizes%20%28Llama-3.2%2011B%20%5C%26%20Qwen-2-VL%207B%29.%0AIt%20also%20outperforms%20much%20larger%20open%20models%20like%20Llama-3.2%2090B%20while%20being%207x%0Asmaller.%20We%20further%20contribute%20an%20open-source%20benchmark%2C%20MM-MT-Bench%2C%20for%0Aevaluating%20vision-language%20models%20in%20practical%20scenarios%2C%20and%20provide%20detailed%0Aanalysis%20and%20code%20for%20standardized%20evaluation%20protocols%20for%20multimodal%20LLMs.%0APixtral-12B%20is%20released%20under%20Apache%202.0%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixtral%252012B%26entry.906535625%3DPravesh%2520Agrawal%2520and%2520Szymon%2520Antoniak%2520and%2520Emma%2520Bou%2520Hanna%2520and%2520Devendra%2520Chaplot%2520and%2520Jessica%2520Chudnovsky%2520and%2520Saurabh%2520Garg%2520and%2520Theophile%2520Gervet%2520and%2520Soham%2520Ghosh%2520and%2520Am%25C3%25A9lie%2520H%25C3%25A9liou%2520and%2520Paul%2520Jacob%2520and%2520Albert%2520Q.%2520Jiang%2520and%2520Timoth%25C3%25A9e%2520Lacroix%2520and%2520Guillaume%2520Lample%2520and%2520Diego%2520Las%2520Casas%2520and%2520Thibaut%2520Lavril%2520and%2520Teven%2520Le%2520Scao%2520and%2520Andy%2520Lo%2520and%2520William%2520Marshall%2520and%2520Louis%2520Martin%2520and%2520Arthur%2520Mensch%2520and%2520Pavankumar%2520Muddireddy%2520and%2520Valera%2520Nemychnikova%2520and%2520Marie%2520Pellat%2520and%2520Patrick%2520Von%2520Platen%2520and%2520Nikhil%2520Raghuraman%2520and%2520Baptiste%2520Rozi%25C3%25A8re%2520and%2520Alexandre%2520Sablayrolles%2520and%2520Lucile%2520Saulnier%2520and%2520Romain%2520Sauvestre%2520and%2520Wendy%2520Shang%2520and%2520Roman%2520Soletskyi%2520and%2520Lawrence%2520Stewart%2520and%2520Pierre%2520Stock%2520and%2520Joachim%2520Studnia%2520and%2520Sandeep%2520Subramanian%2520and%2520Sagar%2520Vaze%2520and%2520Thomas%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Pixtral-12B%252C%2520a%252012--billion-parameter%2520multimodal%2520language%2520model.%250APixtral-12B%2520is%2520trained%2520to%2520understand%2520both%2520natural%2520images%2520and%2520documents%252C%250Aachieving%2520leading%2520performance%2520on%2520various%2520multimodal%2520benchmarks%252C%2520surpassing%2520a%250Anumber%2520of%2520larger%2520models.%2520Unlike%2520many%2520open-source%2520models%252C%2520Pixtral%2520is%2520also%2520a%250Acutting-edge%2520text%2520model%2520for%2520its%2520size%252C%2520and%2520does%2520not%2520compromise%2520on%2520natural%250Alanguage%2520performance%2520to%2520excel%2520in%2520multimodal%2520tasks.%2520Pixtral%2520uses%2520a%2520new%2520vision%250Aencoder%2520trained%2520from%2520scratch%252C%2520which%2520allows%2520it%2520to%2520ingest%2520images%2520at%2520their%2520natural%250Aresolution%2520and%2520aspect%2520ratio.%2520This%2520gives%2520users%2520flexibility%2520on%2520the%2520number%2520of%250Atokens%2520used%2520to%2520process%2520an%2520image.%2520Pixtral%2520is%2520also%2520able%2520to%2520process%2520any%2520number%2520of%250Aimages%2520in%2520its%2520long%2520context%2520window%2520of%2520128K%2520tokens.%2520Pixtral%252012B%2520substanially%250Aoutperforms%2520other%2520open%2520models%2520of%2520similar%2520sizes%2520%2528Llama-3.2%252011B%2520%255C%2526%2520Qwen-2-VL%25207B%2529.%250AIt%2520also%2520outperforms%2520much%2520larger%2520open%2520models%2520like%2520Llama-3.2%252090B%2520while%2520being%25207x%250Asmaller.%2520We%2520further%2520contribute%2520an%2520open-source%2520benchmark%252C%2520MM-MT-Bench%252C%2520for%250Aevaluating%2520vision-language%2520models%2520in%2520practical%2520scenarios%252C%2520and%2520provide%2520detailed%250Aanalysis%2520and%2520code%2520for%2520standardized%2520evaluation%2520protocols%2520for%2520multimodal%2520LLMs.%250APixtral-12B%2520is%2520released%2520under%2520Apache%25202.0%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixtral%2012B&entry.906535625=Pravesh%20Agrawal%20and%20Szymon%20Antoniak%20and%20Emma%20Bou%20Hanna%20and%20Devendra%20Chaplot%20and%20Jessica%20Chudnovsky%20and%20Saurabh%20Garg%20and%20Theophile%20Gervet%20and%20Soham%20Ghosh%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Paul%20Jacob%20and%20Albert%20Q.%20Jiang%20and%20Timoth%C3%A9e%20Lacroix%20and%20Guillaume%20Lample%20and%20Diego%20Las%20Casas%20and%20Thibaut%20Lavril%20and%20Teven%20Le%20Scao%20and%20Andy%20Lo%20and%20William%20Marshall%20and%20Louis%20Martin%20and%20Arthur%20Mensch%20and%20Pavankumar%20Muddireddy%20and%20Valera%20Nemychnikova%20and%20Marie%20Pellat%20and%20Patrick%20Von%20Platen%20and%20Nikhil%20Raghuraman%20and%20Baptiste%20Rozi%C3%A8re%20and%20Alexandre%20Sablayrolles%20and%20Lucile%20Saulnier%20and%20Romain%20Sauvestre%20and%20Wendy%20Shang%20and%20Roman%20Soletskyi%20and%20Lawrence%20Stewart%20and%20Pierre%20Stock%20and%20Joachim%20Studnia%20and%20Sandeep%20Subramanian%20and%20Sagar%20Vaze%20and%20Thomas%20Wang&entry.1292438233=%20%20We%20introduce%20Pixtral-12B%2C%20a%2012--billion-parameter%20multimodal%20language%20model.%0APixtral-12B%20is%20trained%20to%20understand%20both%20natural%20images%20and%20documents%2C%0Aachieving%20leading%20performance%20on%20various%20multimodal%20benchmarks%2C%20surpassing%20a%0Anumber%20of%20larger%20models.%20Unlike%20many%20open-source%20models%2C%20Pixtral%20is%20also%20a%0Acutting-edge%20text%20model%20for%20its%20size%2C%20and%20does%20not%20compromise%20on%20natural%0Alanguage%20performance%20to%20excel%20in%20multimodal%20tasks.%20Pixtral%20uses%20a%20new%20vision%0Aencoder%20trained%20from%20scratch%2C%20which%20allows%20it%20to%20ingest%20images%20at%20their%20natural%0Aresolution%20and%20aspect%20ratio.%20This%20gives%20users%20flexibility%20on%20the%20number%20of%0Atokens%20used%20to%20process%20an%20image.%20Pixtral%20is%20also%20able%20to%20process%20any%20number%20of%0Aimages%20in%20its%20long%20context%20window%20of%20128K%20tokens.%20Pixtral%2012B%20substanially%0Aoutperforms%20other%20open%20models%20of%20similar%20sizes%20%28Llama-3.2%2011B%20%5C%26%20Qwen-2-VL%207B%29.%0AIt%20also%20outperforms%20much%20larger%20open%20models%20like%20Llama-3.2%2090B%20while%20being%207x%0Asmaller.%20We%20further%20contribute%20an%20open-source%20benchmark%2C%20MM-MT-Bench%2C%20for%0Aevaluating%20vision-language%20models%20in%20practical%20scenarios%2C%20and%20provide%20detailed%0Aanalysis%20and%20code%20for%20standardized%20evaluation%20protocols%20for%20multimodal%20LLMs.%0APixtral-12B%20is%20released%20under%20Apache%202.0%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07073v1&entry.124074799=Read"},
{"title": "The BRAVO Semantic Segmentation Challenge Results in UNCV2024", "author": "Tuan-Hung Vu and Eduardo Valle and Andrei Bursuc and Tommie Kerssies and Daan de Geus and Gijs Dubbelman and Long Qian and Bingke Zhu and Yingying Chen and Ming Tang and Jinqiao Wang and Tom\u00e1\u0161 Voj\u00ed\u0159 and Jan \u0160ochman and Ji\u0159\u00ed Matas and Michael Smith and Frank Ferrie and Shamik Basu and Christos Sakaridis and Luc Van Gool", "abstract": "  We propose the unified BRAVO challenge to benchmark the reliability of\nsemantic segmentation models under realistic perturbations and unknown\nout-of-distribution (OOD) scenarios. We define two categories of reliability:\n(1) semantic reliability, which reflects the model's accuracy and calibration\nwhen exposed to various perturbations; and (2) OOD reliability, which measures\nthe model's ability to detect object classes that are unknown during training.\nThe challenge attracted nearly 100 submissions from international teams\nrepresenting notable research institutions. The results reveal interesting\ninsights into the importance of large-scale pre-training and minimal\narchitectural design in developing robust and reliable semantic segmentation\nmodels.\n", "link": "http://arxiv.org/abs/2409.15107v2", "date": "2024-10-09", "relevancy": 2.7327, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5624}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20BRAVO%20Semantic%20Segmentation%20Challenge%20Results%20in%20UNCV2024&body=Title%3A%20The%20BRAVO%20Semantic%20Segmentation%20Challenge%20Results%20in%20UNCV2024%0AAuthor%3A%20Tuan-Hung%20Vu%20and%20Eduardo%20Valle%20and%20Andrei%20Bursuc%20and%20Tommie%20Kerssies%20and%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman%20and%20Long%20Qian%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%20and%20Tom%C3%A1%C5%A1%20Voj%C3%AD%C5%99%20and%20Jan%20%C5%A0ochman%20and%20Ji%C5%99%C3%AD%20Matas%20and%20Michael%20Smith%20and%20Frank%20Ferrie%20and%20Shamik%20Basu%20and%20Christos%20Sakaridis%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20We%20propose%20the%20unified%20BRAVO%20challenge%20to%20benchmark%20the%20reliability%20of%0Asemantic%20segmentation%20models%20under%20realistic%20perturbations%20and%20unknown%0Aout-of-distribution%20%28OOD%29%20scenarios.%20We%20define%20two%20categories%20of%20reliability%3A%0A%281%29%20semantic%20reliability%2C%20which%20reflects%20the%20model%27s%20accuracy%20and%20calibration%0Awhen%20exposed%20to%20various%20perturbations%3B%20and%20%282%29%20OOD%20reliability%2C%20which%20measures%0Athe%20model%27s%20ability%20to%20detect%20object%20classes%20that%20are%20unknown%20during%20training.%0AThe%20challenge%20attracted%20nearly%20100%20submissions%20from%20international%20teams%0Arepresenting%20notable%20research%20institutions.%20The%20results%20reveal%20interesting%0Ainsights%20into%20the%20importance%20of%20large-scale%20pre-training%20and%20minimal%0Aarchitectural%20design%20in%20developing%20robust%20and%20reliable%20semantic%20segmentation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520BRAVO%2520Semantic%2520Segmentation%2520Challenge%2520Results%2520in%2520UNCV2024%26entry.906535625%3DTuan-Hung%2520Vu%2520and%2520Eduardo%2520Valle%2520and%2520Andrei%2520Bursuc%2520and%2520Tommie%2520Kerssies%2520and%2520Daan%2520de%2520Geus%2520and%2520Gijs%2520Dubbelman%2520and%2520Long%2520Qian%2520and%2520Bingke%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Voj%25C3%25AD%25C5%2599%2520and%2520Jan%2520%25C5%25A0ochman%2520and%2520Ji%25C5%2599%25C3%25AD%2520Matas%2520and%2520Michael%2520Smith%2520and%2520Frank%2520Ferrie%2520and%2520Shamik%2520Basu%2520and%2520Christos%2520Sakaridis%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520unified%2520BRAVO%2520challenge%2520to%2520benchmark%2520the%2520reliability%2520of%250Asemantic%2520segmentation%2520models%2520under%2520realistic%2520perturbations%2520and%2520unknown%250Aout-of-distribution%2520%2528OOD%2529%2520scenarios.%2520We%2520define%2520two%2520categories%2520of%2520reliability%253A%250A%25281%2529%2520semantic%2520reliability%252C%2520which%2520reflects%2520the%2520model%2527s%2520accuracy%2520and%2520calibration%250Awhen%2520exposed%2520to%2520various%2520perturbations%253B%2520and%2520%25282%2529%2520OOD%2520reliability%252C%2520which%2520measures%250Athe%2520model%2527s%2520ability%2520to%2520detect%2520object%2520classes%2520that%2520are%2520unknown%2520during%2520training.%250AThe%2520challenge%2520attracted%2520nearly%2520100%2520submissions%2520from%2520international%2520teams%250Arepresenting%2520notable%2520research%2520institutions.%2520The%2520results%2520reveal%2520interesting%250Ainsights%2520into%2520the%2520importance%2520of%2520large-scale%2520pre-training%2520and%2520minimal%250Aarchitectural%2520design%2520in%2520developing%2520robust%2520and%2520reliable%2520semantic%2520segmentation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20BRAVO%20Semantic%20Segmentation%20Challenge%20Results%20in%20UNCV2024&entry.906535625=Tuan-Hung%20Vu%20and%20Eduardo%20Valle%20and%20Andrei%20Bursuc%20and%20Tommie%20Kerssies%20and%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman%20and%20Long%20Qian%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%20and%20Tom%C3%A1%C5%A1%20Voj%C3%AD%C5%99%20and%20Jan%20%C5%A0ochman%20and%20Ji%C5%99%C3%AD%20Matas%20and%20Michael%20Smith%20and%20Frank%20Ferrie%20and%20Shamik%20Basu%20and%20Christos%20Sakaridis%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20We%20propose%20the%20unified%20BRAVO%20challenge%20to%20benchmark%20the%20reliability%20of%0Asemantic%20segmentation%20models%20under%20realistic%20perturbations%20and%20unknown%0Aout-of-distribution%20%28OOD%29%20scenarios.%20We%20define%20two%20categories%20of%20reliability%3A%0A%281%29%20semantic%20reliability%2C%20which%20reflects%20the%20model%27s%20accuracy%20and%20calibration%0Awhen%20exposed%20to%20various%20perturbations%3B%20and%20%282%29%20OOD%20reliability%2C%20which%20measures%0Athe%20model%27s%20ability%20to%20detect%20object%20classes%20that%20are%20unknown%20during%20training.%0AThe%20challenge%20attracted%20nearly%20100%20submissions%20from%20international%20teams%0Arepresenting%20notable%20research%20institutions.%20The%20results%20reveal%20interesting%0Ainsights%20into%20the%20importance%20of%20large-scale%20pre-training%20and%20minimal%0Aarchitectural%20design%20in%20developing%20robust%20and%20reliable%20semantic%20segmentation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15107v2&entry.124074799=Read"},
{"title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment", "author": "Juhyun Oh and Eunsu Kim and Jiseon Kim and Wenda Xu and Inha Cha and William Yang Wang and Alice Oh", "abstract": "  Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment.\n", "link": "http://arxiv.org/abs/2410.06965v1", "date": "2024-10-09", "relevancy": 2.7001, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Factor%20Level%20Preferences%20to%20Improve%20Human-Model%20Alignment&body=Title%3A%20Uncovering%20Factor%20Level%20Preferences%20to%20Improve%20Human-Model%20Alignment%0AAuthor%3A%20Juhyun%20Oh%20and%20Eunsu%20Kim%20and%20Jiseon%20Kim%20and%20Wenda%20Xu%20and%20Inha%20Cha%20and%20William%20Yang%20Wang%20and%20Alice%20Oh%0AAbstract%3A%20%20%20Despite%20advancements%20in%20Large%20Language%20Model%20%28LLM%29%20alignment%2C%20understanding%0Athe%20reasons%20behind%20LLM%20preferences%20remains%20crucial%20for%20bridging%20the%20gap%20between%0Adesired%20and%20actual%20behavior.%20LLMs%20often%20exhibit%20biases%20or%20tendencies%20that%0Adiverge%20from%20human%20preferences%2C%20such%20as%20favoring%20certain%20writing%20styles%20or%0Aproducing%20overly%20verbose%20outputs.%20However%2C%20current%20methods%20for%20evaluating%0Apreference%20alignment%20often%20lack%20explainability%2C%20relying%20on%20coarse-grained%0Acomparisons.%20To%20address%20this%2C%20we%20introduce%20PROFILE%20%28PRObing%20Factors%20of%0AInfLuence%20for%20Explainability%29%2C%20a%20novel%20framework%20that%20uncovers%20and%20quantifies%0Athe%20influence%20of%20specific%20factors%20driving%20preferences.%20PROFILE%27s%20factor%20level%0Aanalysis%20explains%20the%20%27why%27%20behind%20human-model%20alignment%20and%20misalignment%2C%0Aoffering%20insights%20into%20the%20direction%20of%20model%20improvement.%20We%20apply%20PROFILE%20to%0Aanalyze%20human%20and%20LLM%20preferences%20across%20three%20tasks%3A%20summarization%2C%20helpful%0Aresponse%20generation%2C%20and%20document-based%20question-answering.%20Our%20factor%20level%0Aanalysis%20reveals%20a%20substantial%20discrepancy%20between%20human%20and%20LLM%20preferences%20in%0Ageneration%20tasks%2C%20whereas%20LLMs%20show%20strong%20alignment%20with%20human%20preferences%20in%0Aevaluation%20tasks.%20We%20demonstrate%20how%20leveraging%20factor%20level%20insights%2C%0Aincluding%20addressing%20misaligned%20factors%20or%20exploiting%20the%20generation-evaluation%0Agap%2C%20can%20improve%20alignment%20with%20human%20preferences.%20This%20work%20underscores%20the%0Aimportance%20of%20explainable%20preference%20analysis%20and%20highlights%20PROFILE%27s%0Apotential%20to%20provide%20valuable%20training%20signals%2C%20driving%20further%20improvements%20in%0Ahuman-model%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Factor%2520Level%2520Preferences%2520to%2520Improve%2520Human-Model%2520Alignment%26entry.906535625%3DJuhyun%2520Oh%2520and%2520Eunsu%2520Kim%2520and%2520Jiseon%2520Kim%2520and%2520Wenda%2520Xu%2520and%2520Inha%2520Cha%2520and%2520William%2520Yang%2520Wang%2520and%2520Alice%2520Oh%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520alignment%252C%2520understanding%250Athe%2520reasons%2520behind%2520LLM%2520preferences%2520remains%2520crucial%2520for%2520bridging%2520the%2520gap%2520between%250Adesired%2520and%2520actual%2520behavior.%2520LLMs%2520often%2520exhibit%2520biases%2520or%2520tendencies%2520that%250Adiverge%2520from%2520human%2520preferences%252C%2520such%2520as%2520favoring%2520certain%2520writing%2520styles%2520or%250Aproducing%2520overly%2520verbose%2520outputs.%2520However%252C%2520current%2520methods%2520for%2520evaluating%250Apreference%2520alignment%2520often%2520lack%2520explainability%252C%2520relying%2520on%2520coarse-grained%250Acomparisons.%2520To%2520address%2520this%252C%2520we%2520introduce%2520PROFILE%2520%2528PRObing%2520Factors%2520of%250AInfLuence%2520for%2520Explainability%2529%252C%2520a%2520novel%2520framework%2520that%2520uncovers%2520and%2520quantifies%250Athe%2520influence%2520of%2520specific%2520factors%2520driving%2520preferences.%2520PROFILE%2527s%2520factor%2520level%250Aanalysis%2520explains%2520the%2520%2527why%2527%2520behind%2520human-model%2520alignment%2520and%2520misalignment%252C%250Aoffering%2520insights%2520into%2520the%2520direction%2520of%2520model%2520improvement.%2520We%2520apply%2520PROFILE%2520to%250Aanalyze%2520human%2520and%2520LLM%2520preferences%2520across%2520three%2520tasks%253A%2520summarization%252C%2520helpful%250Aresponse%2520generation%252C%2520and%2520document-based%2520question-answering.%2520Our%2520factor%2520level%250Aanalysis%2520reveals%2520a%2520substantial%2520discrepancy%2520between%2520human%2520and%2520LLM%2520preferences%2520in%250Ageneration%2520tasks%252C%2520whereas%2520LLMs%2520show%2520strong%2520alignment%2520with%2520human%2520preferences%2520in%250Aevaluation%2520tasks.%2520We%2520demonstrate%2520how%2520leveraging%2520factor%2520level%2520insights%252C%250Aincluding%2520addressing%2520misaligned%2520factors%2520or%2520exploiting%2520the%2520generation-evaluation%250Agap%252C%2520can%2520improve%2520alignment%2520with%2520human%2520preferences.%2520This%2520work%2520underscores%2520the%250Aimportance%2520of%2520explainable%2520preference%2520analysis%2520and%2520highlights%2520PROFILE%2527s%250Apotential%2520to%2520provide%2520valuable%2520training%2520signals%252C%2520driving%2520further%2520improvements%2520in%250Ahuman-model%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Factor%20Level%20Preferences%20to%20Improve%20Human-Model%20Alignment&entry.906535625=Juhyun%20Oh%20and%20Eunsu%20Kim%20and%20Jiseon%20Kim%20and%20Wenda%20Xu%20and%20Inha%20Cha%20and%20William%20Yang%20Wang%20and%20Alice%20Oh&entry.1292438233=%20%20Despite%20advancements%20in%20Large%20Language%20Model%20%28LLM%29%20alignment%2C%20understanding%0Athe%20reasons%20behind%20LLM%20preferences%20remains%20crucial%20for%20bridging%20the%20gap%20between%0Adesired%20and%20actual%20behavior.%20LLMs%20often%20exhibit%20biases%20or%20tendencies%20that%0Adiverge%20from%20human%20preferences%2C%20such%20as%20favoring%20certain%20writing%20styles%20or%0Aproducing%20overly%20verbose%20outputs.%20However%2C%20current%20methods%20for%20evaluating%0Apreference%20alignment%20often%20lack%20explainability%2C%20relying%20on%20coarse-grained%0Acomparisons.%20To%20address%20this%2C%20we%20introduce%20PROFILE%20%28PRObing%20Factors%20of%0AInfLuence%20for%20Explainability%29%2C%20a%20novel%20framework%20that%20uncovers%20and%20quantifies%0Athe%20influence%20of%20specific%20factors%20driving%20preferences.%20PROFILE%27s%20factor%20level%0Aanalysis%20explains%20the%20%27why%27%20behind%20human-model%20alignment%20and%20misalignment%2C%0Aoffering%20insights%20into%20the%20direction%20of%20model%20improvement.%20We%20apply%20PROFILE%20to%0Aanalyze%20human%20and%20LLM%20preferences%20across%20three%20tasks%3A%20summarization%2C%20helpful%0Aresponse%20generation%2C%20and%20document-based%20question-answering.%20Our%20factor%20level%0Aanalysis%20reveals%20a%20substantial%20discrepancy%20between%20human%20and%20LLM%20preferences%20in%0Ageneration%20tasks%2C%20whereas%20LLMs%20show%20strong%20alignment%20with%20human%20preferences%20in%0Aevaluation%20tasks.%20We%20demonstrate%20how%20leveraging%20factor%20level%20insights%2C%0Aincluding%20addressing%20misaligned%20factors%20or%20exploiting%20the%20generation-evaluation%0Agap%2C%20can%20improve%20alignment%20with%20human%20preferences.%20This%20work%20underscores%20the%0Aimportance%20of%20explainable%20preference%20analysis%20and%20highlights%20PROFILE%27s%0Apotential%20to%20provide%20valuable%20training%20signals%2C%20driving%20further%20improvements%20in%0Ahuman-model%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06965v1&entry.124074799=Read"},
{"title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models", "author": "Michael Lan and Philip Torr and Austin Meek and Ashkan Khakzar and David Krueger and Fazl Barez", "abstract": "  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones. This makes it\ndifficult to disentangle and match features across different models. To address\nthis issue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics like Singular Value Canonical Correlation Analysis to\nanalyze these SAE features across different LLMs. Our experiments reveal\nsignificant similarities in SAE feature spaces across various LLMs, providing\nnew evidence for feature universality.\n", "link": "http://arxiv.org/abs/2410.06981v1", "date": "2024-10-09", "relevancy": 2.6924, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Autoencoders%20Reveal%20Universal%20Feature%20Spaces%20Across%20Large%0A%20%20Language%20Models&body=Title%3A%20Sparse%20Autoencoders%20Reveal%20Universal%20Feature%20Spaces%20Across%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Michael%20Lan%20and%20Philip%20Torr%20and%20Austin%20Meek%20and%20Ashkan%20Khakzar%20and%20David%20Krueger%20and%20Fazl%20Barez%0AAbstract%3A%20%20%20We%20investigate%20feature%20universality%20in%20large%20language%20models%20%28LLMs%29%2C%20a%0Aresearch%20field%20that%20aims%20to%20understand%20how%20different%20models%20similarly%20represent%0Aconcepts%20in%20the%20latent%20spaces%20of%20their%20intermediate%20layers.%20Demonstrating%0Afeature%20universality%20allows%20discoveries%20about%20latent%20representations%20to%0Ageneralize%20across%20several%20models.%20However%2C%20comparing%20features%20across%20LLMs%20is%0Achallenging%20due%20to%20polysemanticity%2C%20in%20which%20individual%20neurons%20often%0Acorrespond%20to%20multiple%20features%20rather%20than%20distinct%20ones.%20This%20makes%20it%0Adifficult%20to%20disentangle%20and%20match%20features%20across%20different%20models.%20To%20address%0Athis%20issue%2C%20we%20employ%20a%20method%20known%20as%20dictionary%20learning%20by%20using%20sparse%0Aautoencoders%20%28SAEs%29%20to%20transform%20LLM%20activations%20into%20more%20interpretable%20spaces%0Aspanned%20by%20neurons%20corresponding%20to%20individual%20features.%20After%20matching%20feature%0Aneurons%20across%20models%20via%20activation%20correlation%2C%20we%20apply%20representational%0Aspace%20similarity%20metrics%20like%20Singular%20Value%20Canonical%20Correlation%20Analysis%20to%0Aanalyze%20these%20SAE%20features%20across%20different%20LLMs.%20Our%20experiments%20reveal%0Asignificant%20similarities%20in%20SAE%20feature%20spaces%20across%20various%20LLMs%2C%20providing%0Anew%20evidence%20for%20feature%20universality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Autoencoders%2520Reveal%2520Universal%2520Feature%2520Spaces%2520Across%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DMichael%2520Lan%2520and%2520Philip%2520Torr%2520and%2520Austin%2520Meek%2520and%2520Ashkan%2520Khakzar%2520and%2520David%2520Krueger%2520and%2520Fazl%2520Barez%26entry.1292438233%3D%2520%2520We%2520investigate%2520feature%2520universality%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520a%250Aresearch%2520field%2520that%2520aims%2520to%2520understand%2520how%2520different%2520models%2520similarly%2520represent%250Aconcepts%2520in%2520the%2520latent%2520spaces%2520of%2520their%2520intermediate%2520layers.%2520Demonstrating%250Afeature%2520universality%2520allows%2520discoveries%2520about%2520latent%2520representations%2520to%250Ageneralize%2520across%2520several%2520models.%2520However%252C%2520comparing%2520features%2520across%2520LLMs%2520is%250Achallenging%2520due%2520to%2520polysemanticity%252C%2520in%2520which%2520individual%2520neurons%2520often%250Acorrespond%2520to%2520multiple%2520features%2520rather%2520than%2520distinct%2520ones.%2520This%2520makes%2520it%250Adifficult%2520to%2520disentangle%2520and%2520match%2520features%2520across%2520different%2520models.%2520To%2520address%250Athis%2520issue%252C%2520we%2520employ%2520a%2520method%2520known%2520as%2520dictionary%2520learning%2520by%2520using%2520sparse%250Aautoencoders%2520%2528SAEs%2529%2520to%2520transform%2520LLM%2520activations%2520into%2520more%2520interpretable%2520spaces%250Aspanned%2520by%2520neurons%2520corresponding%2520to%2520individual%2520features.%2520After%2520matching%2520feature%250Aneurons%2520across%2520models%2520via%2520activation%2520correlation%252C%2520we%2520apply%2520representational%250Aspace%2520similarity%2520metrics%2520like%2520Singular%2520Value%2520Canonical%2520Correlation%2520Analysis%2520to%250Aanalyze%2520these%2520SAE%2520features%2520across%2520different%2520LLMs.%2520Our%2520experiments%2520reveal%250Asignificant%2520similarities%2520in%2520SAE%2520feature%2520spaces%2520across%2520various%2520LLMs%252C%2520providing%250Anew%2520evidence%2520for%2520feature%2520universality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Autoencoders%20Reveal%20Universal%20Feature%20Spaces%20Across%20Large%0A%20%20Language%20Models&entry.906535625=Michael%20Lan%20and%20Philip%20Torr%20and%20Austin%20Meek%20and%20Ashkan%20Khakzar%20and%20David%20Krueger%20and%20Fazl%20Barez&entry.1292438233=%20%20We%20investigate%20feature%20universality%20in%20large%20language%20models%20%28LLMs%29%2C%20a%0Aresearch%20field%20that%20aims%20to%20understand%20how%20different%20models%20similarly%20represent%0Aconcepts%20in%20the%20latent%20spaces%20of%20their%20intermediate%20layers.%20Demonstrating%0Afeature%20universality%20allows%20discoveries%20about%20latent%20representations%20to%0Ageneralize%20across%20several%20models.%20However%2C%20comparing%20features%20across%20LLMs%20is%0Achallenging%20due%20to%20polysemanticity%2C%20in%20which%20individual%20neurons%20often%0Acorrespond%20to%20multiple%20features%20rather%20than%20distinct%20ones.%20This%20makes%20it%0Adifficult%20to%20disentangle%20and%20match%20features%20across%20different%20models.%20To%20address%0Athis%20issue%2C%20we%20employ%20a%20method%20known%20as%20dictionary%20learning%20by%20using%20sparse%0Aautoencoders%20%28SAEs%29%20to%20transform%20LLM%20activations%20into%20more%20interpretable%20spaces%0Aspanned%20by%20neurons%20corresponding%20to%20individual%20features.%20After%20matching%20feature%0Aneurons%20across%20models%20via%20activation%20correlation%2C%20we%20apply%20representational%0Aspace%20similarity%20metrics%20like%20Singular%20Value%20Canonical%20Correlation%20Analysis%20to%0Aanalyze%20these%20SAE%20features%20across%20different%20LLMs.%20Our%20experiments%20reveal%0Asignificant%20similarities%20in%20SAE%20feature%20spaces%20across%20various%20LLMs%2C%20providing%0Anew%20evidence%20for%20feature%20universality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06981v1&entry.124074799=Read"},
{"title": "CursorCore: Assist Programming through Aligning Anything", "author": "Hao Jiang and Qi Liu and Rui Li and Shengyu Ye and Shijin Wang", "abstract": "  Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore.\n", "link": "http://arxiv.org/abs/2410.07002v1", "date": "2024-10-09", "relevancy": 2.671, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CursorCore%3A%20Assist%20Programming%20through%20Aligning%20Anything&body=Title%3A%20CursorCore%3A%20Assist%20Programming%20through%20Aligning%20Anything%0AAuthor%3A%20Hao%20Jiang%20and%20Qi%20Liu%20and%20Rui%20Li%20and%20Shengyu%20Ye%20and%20Shijin%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20been%20successfully%20applied%20to%20programming%0Aassistance%20tasks%2C%20such%20as%20code%20completion%2C%20code%20insertion%2C%20and%20instructional%0Acode%20editing.%20However%2C%20these%20applications%20remain%20insufficiently%20automated%20and%0Astruggle%20to%20effectively%20integrate%20various%20types%20of%20information%20during%20the%0Aprogramming%20process%2C%20including%20coding%20history%2C%20current%20code%2C%20and%20user%0Ainstructions.%20In%20this%20work%2C%20we%20propose%20a%20new%20conversational%20framework%20that%0Acomprehensively%20integrates%20these%20information%20sources%2C%20collect%20data%20to%20train%20our%0Amodels%20and%20evaluate%20their%20performance.%20Firstly%2C%20to%20thoroughly%20evaluate%20how%20well%0Amodels%20align%20with%20different%20types%20of%20information%20and%20the%20quality%20of%20their%0Aoutputs%2C%20we%20introduce%20a%20new%20benchmark%2C%20APEval%20%28Assist%20Programming%20Eval%29%2C%20to%0Acomprehensively%20assess%20the%20performance%20of%20models%20in%20programming%20assistance%0Atasks.%20Then%2C%20for%20data%20collection%2C%20we%20develop%20a%20data%20generation%20pipeline%2C%0AProgramming-Instruct%2C%20which%20synthesizes%20training%20data%20from%20diverse%20sources%2C%0Asuch%20as%20GitHub%20and%20online%20judge%20platforms.%20This%20pipeline%20can%20automatically%0Agenerate%20various%20types%20of%20messages%20throughout%20the%20programming%20process.%20Finally%2C%0Ausing%20this%20pipeline%2C%20we%20generate%20219K%20samples%2C%20fine-tune%20multiple%20models%2C%20and%0Adevelop%20the%20CursorCore%20series.%20We%20show%20that%20CursorCore%20outperforms%20other%20models%0Aof%20comparable%20size.%20This%20framework%20unifies%20applications%20such%20as%20inline%20chat%20and%0Aautomated%20editing%2C%20contributes%20to%20the%20advancement%20of%20coding%20assistants.%20Code%2C%0Amodels%20and%20data%20are%20freely%20available%20at%0Ahttps%3A//github.com/TechxGenus/CursorCore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCursorCore%253A%2520Assist%2520Programming%2520through%2520Aligning%2520Anything%26entry.906535625%3DHao%2520Jiang%2520and%2520Qi%2520Liu%2520and%2520Rui%2520Li%2520and%2520Shengyu%2520Ye%2520and%2520Shijin%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520been%2520successfully%2520applied%2520to%2520programming%250Aassistance%2520tasks%252C%2520such%2520as%2520code%2520completion%252C%2520code%2520insertion%252C%2520and%2520instructional%250Acode%2520editing.%2520However%252C%2520these%2520applications%2520remain%2520insufficiently%2520automated%2520and%250Astruggle%2520to%2520effectively%2520integrate%2520various%2520types%2520of%2520information%2520during%2520the%250Aprogramming%2520process%252C%2520including%2520coding%2520history%252C%2520current%2520code%252C%2520and%2520user%250Ainstructions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520conversational%2520framework%2520that%250Acomprehensively%2520integrates%2520these%2520information%2520sources%252C%2520collect%2520data%2520to%2520train%2520our%250Amodels%2520and%2520evaluate%2520their%2520performance.%2520Firstly%252C%2520to%2520thoroughly%2520evaluate%2520how%2520well%250Amodels%2520align%2520with%2520different%2520types%2520of%2520information%2520and%2520the%2520quality%2520of%2520their%250Aoutputs%252C%2520we%2520introduce%2520a%2520new%2520benchmark%252C%2520APEval%2520%2528Assist%2520Programming%2520Eval%2529%252C%2520to%250Acomprehensively%2520assess%2520the%2520performance%2520of%2520models%2520in%2520programming%2520assistance%250Atasks.%2520Then%252C%2520for%2520data%2520collection%252C%2520we%2520develop%2520a%2520data%2520generation%2520pipeline%252C%250AProgramming-Instruct%252C%2520which%2520synthesizes%2520training%2520data%2520from%2520diverse%2520sources%252C%250Asuch%2520as%2520GitHub%2520and%2520online%2520judge%2520platforms.%2520This%2520pipeline%2520can%2520automatically%250Agenerate%2520various%2520types%2520of%2520messages%2520throughout%2520the%2520programming%2520process.%2520Finally%252C%250Ausing%2520this%2520pipeline%252C%2520we%2520generate%2520219K%2520samples%252C%2520fine-tune%2520multiple%2520models%252C%2520and%250Adevelop%2520the%2520CursorCore%2520series.%2520We%2520show%2520that%2520CursorCore%2520outperforms%2520other%2520models%250Aof%2520comparable%2520size.%2520This%2520framework%2520unifies%2520applications%2520such%2520as%2520inline%2520chat%2520and%250Aautomated%2520editing%252C%2520contributes%2520to%2520the%2520advancement%2520of%2520coding%2520assistants.%2520Code%252C%250Amodels%2520and%2520data%2520are%2520freely%2520available%2520at%250Ahttps%253A//github.com/TechxGenus/CursorCore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CursorCore%3A%20Assist%20Programming%20through%20Aligning%20Anything&entry.906535625=Hao%20Jiang%20and%20Qi%20Liu%20and%20Rui%20Li%20and%20Shengyu%20Ye%20and%20Shijin%20Wang&entry.1292438233=%20%20Large%20language%20models%20have%20been%20successfully%20applied%20to%20programming%0Aassistance%20tasks%2C%20such%20as%20code%20completion%2C%20code%20insertion%2C%20and%20instructional%0Acode%20editing.%20However%2C%20these%20applications%20remain%20insufficiently%20automated%20and%0Astruggle%20to%20effectively%20integrate%20various%20types%20of%20information%20during%20the%0Aprogramming%20process%2C%20including%20coding%20history%2C%20current%20code%2C%20and%20user%0Ainstructions.%20In%20this%20work%2C%20we%20propose%20a%20new%20conversational%20framework%20that%0Acomprehensively%20integrates%20these%20information%20sources%2C%20collect%20data%20to%20train%20our%0Amodels%20and%20evaluate%20their%20performance.%20Firstly%2C%20to%20thoroughly%20evaluate%20how%20well%0Amodels%20align%20with%20different%20types%20of%20information%20and%20the%20quality%20of%20their%0Aoutputs%2C%20we%20introduce%20a%20new%20benchmark%2C%20APEval%20%28Assist%20Programming%20Eval%29%2C%20to%0Acomprehensively%20assess%20the%20performance%20of%20models%20in%20programming%20assistance%0Atasks.%20Then%2C%20for%20data%20collection%2C%20we%20develop%20a%20data%20generation%20pipeline%2C%0AProgramming-Instruct%2C%20which%20synthesizes%20training%20data%20from%20diverse%20sources%2C%0Asuch%20as%20GitHub%20and%20online%20judge%20platforms.%20This%20pipeline%20can%20automatically%0Agenerate%20various%20types%20of%20messages%20throughout%20the%20programming%20process.%20Finally%2C%0Ausing%20this%20pipeline%2C%20we%20generate%20219K%20samples%2C%20fine-tune%20multiple%20models%2C%20and%0Adevelop%20the%20CursorCore%20series.%20We%20show%20that%20CursorCore%20outperforms%20other%20models%0Aof%20comparable%20size.%20This%20framework%20unifies%20applications%20such%20as%20inline%20chat%20and%0Aautomated%20editing%2C%20contributes%20to%20the%20advancement%20of%20coding%20assistants.%20Code%2C%0Amodels%20and%20data%20are%20freely%20available%20at%0Ahttps%3A//github.com/TechxGenus/CursorCore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07002v1&entry.124074799=Read"},
{"title": "Continual Learning: Less Forgetting, More OOD Generalization via\n  Adaptive Contrastive Replay", "author": "Hossein Rezaei and Mohammad Sabokrou", "abstract": "  Machine learning models often suffer from catastrophic forgetting of\npreviously learned knowledge when learning new classes. Various methods have\nbeen proposed to mitigate this issue. However, rehearsal-based learning, which\nretains samples from previous classes, typically achieves good performance but\ntends to memorize specific instances, struggling with Out-of-Distribution (OOD)\ngeneralization. This often leads to high forgetting rates and poor\ngeneralization. Surprisingly, the OOD generalization capabilities of these\nmethods have been largely unexplored. In this paper, we highlight this issue\nand propose a simple yet effective strategy inspired by contrastive learning\nand data-centric principles to address it. We introduce Adaptive Contrastive\nReplay (ACR), a method that employs dual optimization to simultaneously train\nboth the encoder and the classifier. ACR adaptively populates the replay buffer\nwith misclassified samples while ensuring a balanced representation of classes\nand tasks. By refining the decision boundary in this way, ACR achieves a\nbalance between stability and plasticity. Our method significantly outperforms\nprevious approaches in terms of OOD generalization, achieving an improvement of\n13.41\\% on Split CIFAR-100, 9.91\\% on Split Mini-ImageNet, and 5.98\\% on Split\nTiny-ImageNet.\n", "link": "http://arxiv.org/abs/2410.07110v1", "date": "2024-10-09", "relevancy": 2.6643, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5445}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5389}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%3A%20Less%20Forgetting%2C%20More%20OOD%20Generalization%20via%0A%20%20Adaptive%20Contrastive%20Replay&body=Title%3A%20Continual%20Learning%3A%20Less%20Forgetting%2C%20More%20OOD%20Generalization%20via%0A%20%20Adaptive%20Contrastive%20Replay%0AAuthor%3A%20Hossein%20Rezaei%20and%20Mohammad%20Sabokrou%0AAbstract%3A%20%20%20Machine%20learning%20models%20often%20suffer%20from%20catastrophic%20forgetting%20of%0Apreviously%20learned%20knowledge%20when%20learning%20new%20classes.%20Various%20methods%20have%0Abeen%20proposed%20to%20mitigate%20this%20issue.%20However%2C%20rehearsal-based%20learning%2C%20which%0Aretains%20samples%20from%20previous%20classes%2C%20typically%20achieves%20good%20performance%20but%0Atends%20to%20memorize%20specific%20instances%2C%20struggling%20with%20Out-of-Distribution%20%28OOD%29%0Ageneralization.%20This%20often%20leads%20to%20high%20forgetting%20rates%20and%20poor%0Ageneralization.%20Surprisingly%2C%20the%20OOD%20generalization%20capabilities%20of%20these%0Amethods%20have%20been%20largely%20unexplored.%20In%20this%20paper%2C%20we%20highlight%20this%20issue%0Aand%20propose%20a%20simple%20yet%20effective%20strategy%20inspired%20by%20contrastive%20learning%0Aand%20data-centric%20principles%20to%20address%20it.%20We%20introduce%20Adaptive%20Contrastive%0AReplay%20%28ACR%29%2C%20a%20method%20that%20employs%20dual%20optimization%20to%20simultaneously%20train%0Aboth%20the%20encoder%20and%20the%20classifier.%20ACR%20adaptively%20populates%20the%20replay%20buffer%0Awith%20misclassified%20samples%20while%20ensuring%20a%20balanced%20representation%20of%20classes%0Aand%20tasks.%20By%20refining%20the%20decision%20boundary%20in%20this%20way%2C%20ACR%20achieves%20a%0Abalance%20between%20stability%20and%20plasticity.%20Our%20method%20significantly%20outperforms%0Aprevious%20approaches%20in%20terms%20of%20OOD%20generalization%2C%20achieving%20an%20improvement%20of%0A13.41%5C%25%20on%20Split%20CIFAR-100%2C%209.91%5C%25%20on%20Split%20Mini-ImageNet%2C%20and%205.98%5C%25%20on%20Split%0ATiny-ImageNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%253A%2520Less%2520Forgetting%252C%2520More%2520OOD%2520Generalization%2520via%250A%2520%2520Adaptive%2520Contrastive%2520Replay%26entry.906535625%3DHossein%2520Rezaei%2520and%2520Mohammad%2520Sabokrou%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520often%2520suffer%2520from%2520catastrophic%2520forgetting%2520of%250Apreviously%2520learned%2520knowledge%2520when%2520learning%2520new%2520classes.%2520Various%2520methods%2520have%250Abeen%2520proposed%2520to%2520mitigate%2520this%2520issue.%2520However%252C%2520rehearsal-based%2520learning%252C%2520which%250Aretains%2520samples%2520from%2520previous%2520classes%252C%2520typically%2520achieves%2520good%2520performance%2520but%250Atends%2520to%2520memorize%2520specific%2520instances%252C%2520struggling%2520with%2520Out-of-Distribution%2520%2528OOD%2529%250Ageneralization.%2520This%2520often%2520leads%2520to%2520high%2520forgetting%2520rates%2520and%2520poor%250Ageneralization.%2520Surprisingly%252C%2520the%2520OOD%2520generalization%2520capabilities%2520of%2520these%250Amethods%2520have%2520been%2520largely%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520highlight%2520this%2520issue%250Aand%2520propose%2520a%2520simple%2520yet%2520effective%2520strategy%2520inspired%2520by%2520contrastive%2520learning%250Aand%2520data-centric%2520principles%2520to%2520address%2520it.%2520We%2520introduce%2520Adaptive%2520Contrastive%250AReplay%2520%2528ACR%2529%252C%2520a%2520method%2520that%2520employs%2520dual%2520optimization%2520to%2520simultaneously%2520train%250Aboth%2520the%2520encoder%2520and%2520the%2520classifier.%2520ACR%2520adaptively%2520populates%2520the%2520replay%2520buffer%250Awith%2520misclassified%2520samples%2520while%2520ensuring%2520a%2520balanced%2520representation%2520of%2520classes%250Aand%2520tasks.%2520By%2520refining%2520the%2520decision%2520boundary%2520in%2520this%2520way%252C%2520ACR%2520achieves%2520a%250Abalance%2520between%2520stability%2520and%2520plasticity.%2520Our%2520method%2520significantly%2520outperforms%250Aprevious%2520approaches%2520in%2520terms%2520of%2520OOD%2520generalization%252C%2520achieving%2520an%2520improvement%2520of%250A13.41%255C%2525%2520on%2520Split%2520CIFAR-100%252C%25209.91%255C%2525%2520on%2520Split%2520Mini-ImageNet%252C%2520and%25205.98%255C%2525%2520on%2520Split%250ATiny-ImageNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%3A%20Less%20Forgetting%2C%20More%20OOD%20Generalization%20via%0A%20%20Adaptive%20Contrastive%20Replay&entry.906535625=Hossein%20Rezaei%20and%20Mohammad%20Sabokrou&entry.1292438233=%20%20Machine%20learning%20models%20often%20suffer%20from%20catastrophic%20forgetting%20of%0Apreviously%20learned%20knowledge%20when%20learning%20new%20classes.%20Various%20methods%20have%0Abeen%20proposed%20to%20mitigate%20this%20issue.%20However%2C%20rehearsal-based%20learning%2C%20which%0Aretains%20samples%20from%20previous%20classes%2C%20typically%20achieves%20good%20performance%20but%0Atends%20to%20memorize%20specific%20instances%2C%20struggling%20with%20Out-of-Distribution%20%28OOD%29%0Ageneralization.%20This%20often%20leads%20to%20high%20forgetting%20rates%20and%20poor%0Ageneralization.%20Surprisingly%2C%20the%20OOD%20generalization%20capabilities%20of%20these%0Amethods%20have%20been%20largely%20unexplored.%20In%20this%20paper%2C%20we%20highlight%20this%20issue%0Aand%20propose%20a%20simple%20yet%20effective%20strategy%20inspired%20by%20contrastive%20learning%0Aand%20data-centric%20principles%20to%20address%20it.%20We%20introduce%20Adaptive%20Contrastive%0AReplay%20%28ACR%29%2C%20a%20method%20that%20employs%20dual%20optimization%20to%20simultaneously%20train%0Aboth%20the%20encoder%20and%20the%20classifier.%20ACR%20adaptively%20populates%20the%20replay%20buffer%0Awith%20misclassified%20samples%20while%20ensuring%20a%20balanced%20representation%20of%20classes%0Aand%20tasks.%20By%20refining%20the%20decision%20boundary%20in%20this%20way%2C%20ACR%20achieves%20a%0Abalance%20between%20stability%20and%20plasticity.%20Our%20method%20significantly%20outperforms%0Aprevious%20approaches%20in%20terms%20of%20OOD%20generalization%2C%20achieving%20an%20improvement%20of%0A13.41%5C%25%20on%20Split%20CIFAR-100%2C%209.91%5C%25%20on%20Split%20Mini-ImageNet%2C%20and%205.98%5C%25%20on%20Split%0ATiny-ImageNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07110v1&entry.124074799=Read"},
{"title": "A Gentle Introduction and Tutorial on Deep Generative Models in\n  Transportation Research", "author": "Seongjin Choi and Zhixiong Jin and Seungwoo Ham and Jiwon Kim and Lijun Sun", "abstract": "  Deep Generative Models (DGMs) have rapidly advanced in recent years, becoming\nessential tools in various fields due to their ability to learn complex data\ndistributions and generate synthetic data. Their importance in transportation\nresearch is increasingly recognized, particularly for applications like traffic\ndata generation, prediction, and feature extraction. This paper offers a\ncomprehensive introduction and tutorial on DGMs, with a focus on their\napplications in transportation. It begins with an overview of generative\nmodels, followed by detailed explanations of fundamental models, a systematic\nreview of the literature, and practical tutorial code to aid implementation.\nThe paper also discusses current challenges and opportunities, highlighting how\nthese models can be effectively utilized and further developed in\ntransportation research. This paper serves as a valuable reference, guiding\nresearchers and practitioners from foundational knowledge to advanced\napplications of DGMs in transportation research.\n", "link": "http://arxiv.org/abs/2410.07066v1", "date": "2024-10-09", "relevancy": 2.6107, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5339}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5193}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Gentle%20Introduction%20and%20Tutorial%20on%20Deep%20Generative%20Models%20in%0A%20%20Transportation%20Research&body=Title%3A%20A%20Gentle%20Introduction%20and%20Tutorial%20on%20Deep%20Generative%20Models%20in%0A%20%20Transportation%20Research%0AAuthor%3A%20Seongjin%20Choi%20and%20Zhixiong%20Jin%20and%20Seungwoo%20Ham%20and%20Jiwon%20Kim%20and%20Lijun%20Sun%0AAbstract%3A%20%20%20Deep%20Generative%20Models%20%28DGMs%29%20have%20rapidly%20advanced%20in%20recent%20years%2C%20becoming%0Aessential%20tools%20in%20various%20fields%20due%20to%20their%20ability%20to%20learn%20complex%20data%0Adistributions%20and%20generate%20synthetic%20data.%20Their%20importance%20in%20transportation%0Aresearch%20is%20increasingly%20recognized%2C%20particularly%20for%20applications%20like%20traffic%0Adata%20generation%2C%20prediction%2C%20and%20feature%20extraction.%20This%20paper%20offers%20a%0Acomprehensive%20introduction%20and%20tutorial%20on%20DGMs%2C%20with%20a%20focus%20on%20their%0Aapplications%20in%20transportation.%20It%20begins%20with%20an%20overview%20of%20generative%0Amodels%2C%20followed%20by%20detailed%20explanations%20of%20fundamental%20models%2C%20a%20systematic%0Areview%20of%20the%20literature%2C%20and%20practical%20tutorial%20code%20to%20aid%20implementation.%0AThe%20paper%20also%20discusses%20current%20challenges%20and%20opportunities%2C%20highlighting%20how%0Athese%20models%20can%20be%20effectively%20utilized%20and%20further%20developed%20in%0Atransportation%20research.%20This%20paper%20serves%20as%20a%20valuable%20reference%2C%20guiding%0Aresearchers%20and%20practitioners%20from%20foundational%20knowledge%20to%20advanced%0Aapplications%20of%20DGMs%20in%20transportation%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Gentle%2520Introduction%2520and%2520Tutorial%2520on%2520Deep%2520Generative%2520Models%2520in%250A%2520%2520Transportation%2520Research%26entry.906535625%3DSeongjin%2520Choi%2520and%2520Zhixiong%2520Jin%2520and%2520Seungwoo%2520Ham%2520and%2520Jiwon%2520Kim%2520and%2520Lijun%2520Sun%26entry.1292438233%3D%2520%2520Deep%2520Generative%2520Models%2520%2528DGMs%2529%2520have%2520rapidly%2520advanced%2520in%2520recent%2520years%252C%2520becoming%250Aessential%2520tools%2520in%2520various%2520fields%2520due%2520to%2520their%2520ability%2520to%2520learn%2520complex%2520data%250Adistributions%2520and%2520generate%2520synthetic%2520data.%2520Their%2520importance%2520in%2520transportation%250Aresearch%2520is%2520increasingly%2520recognized%252C%2520particularly%2520for%2520applications%2520like%2520traffic%250Adata%2520generation%252C%2520prediction%252C%2520and%2520feature%2520extraction.%2520This%2520paper%2520offers%2520a%250Acomprehensive%2520introduction%2520and%2520tutorial%2520on%2520DGMs%252C%2520with%2520a%2520focus%2520on%2520their%250Aapplications%2520in%2520transportation.%2520It%2520begins%2520with%2520an%2520overview%2520of%2520generative%250Amodels%252C%2520followed%2520by%2520detailed%2520explanations%2520of%2520fundamental%2520models%252C%2520a%2520systematic%250Areview%2520of%2520the%2520literature%252C%2520and%2520practical%2520tutorial%2520code%2520to%2520aid%2520implementation.%250AThe%2520paper%2520also%2520discusses%2520current%2520challenges%2520and%2520opportunities%252C%2520highlighting%2520how%250Athese%2520models%2520can%2520be%2520effectively%2520utilized%2520and%2520further%2520developed%2520in%250Atransportation%2520research.%2520This%2520paper%2520serves%2520as%2520a%2520valuable%2520reference%252C%2520guiding%250Aresearchers%2520and%2520practitioners%2520from%2520foundational%2520knowledge%2520to%2520advanced%250Aapplications%2520of%2520DGMs%2520in%2520transportation%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Gentle%20Introduction%20and%20Tutorial%20on%20Deep%20Generative%20Models%20in%0A%20%20Transportation%20Research&entry.906535625=Seongjin%20Choi%20and%20Zhixiong%20Jin%20and%20Seungwoo%20Ham%20and%20Jiwon%20Kim%20and%20Lijun%20Sun&entry.1292438233=%20%20Deep%20Generative%20Models%20%28DGMs%29%20have%20rapidly%20advanced%20in%20recent%20years%2C%20becoming%0Aessential%20tools%20in%20various%20fields%20due%20to%20their%20ability%20to%20learn%20complex%20data%0Adistributions%20and%20generate%20synthetic%20data.%20Their%20importance%20in%20transportation%0Aresearch%20is%20increasingly%20recognized%2C%20particularly%20for%20applications%20like%20traffic%0Adata%20generation%2C%20prediction%2C%20and%20feature%20extraction.%20This%20paper%20offers%20a%0Acomprehensive%20introduction%20and%20tutorial%20on%20DGMs%2C%20with%20a%20focus%20on%20their%0Aapplications%20in%20transportation.%20It%20begins%20with%20an%20overview%20of%20generative%0Amodels%2C%20followed%20by%20detailed%20explanations%20of%20fundamental%20models%2C%20a%20systematic%0Areview%20of%20the%20literature%2C%20and%20practical%20tutorial%20code%20to%20aid%20implementation.%0AThe%20paper%20also%20discusses%20current%20challenges%20and%20opportunities%2C%20highlighting%20how%0Athese%20models%20can%20be%20effectively%20utilized%20and%20further%20developed%20in%0Atransportation%20research.%20This%20paper%20serves%20as%20a%20valuable%20reference%2C%20guiding%0Aresearchers%20and%20practitioners%20from%20foundational%20knowledge%20to%20advanced%0Aapplications%20of%20DGMs%20in%20transportation%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07066v1&entry.124074799=Read"},
{"title": "OpenGraph: Towards Open Graph Foundation Models", "author": "Lianghao Xia and Ben Kao and Chao Huang", "abstract": "  Graph learning has become essential in various domains, including\nrecommendation systems and social network analysis. Graph Neural Networks\n(GNNs) have emerged as promising techniques for encoding structural information\nand improving performance in tasks like link prediction and node\nclassification. However, a key challenge remains: the difficulty of\ngeneralizing to unseen graph data with different properties. In this work, we\npropose a novel graph foundation model, called OpenGraph, to address this\nchallenge. Our approach tackles several technical obstacles. Firstly, we\nenhance data augmentation using a large language model (LLM) to overcome data\nscarcity in real-world scenarios. Secondly, we introduce a unified graph\ntokenizer that enables the model to generalize effectively to diverse graph\ndata, even when encountering unseen properties during training. Thirdly, our\ndeveloped scalable graph transformer captures node-wise dependencies within the\nglobal topological context. Extensive experiments validate the effectiveness of\nour framework. By adapting OpenGraph to new graph characteristics and\ncomprehending diverse graphs, our approach achieves remarkable zero-shot graph\nlearning performance across various settings. We release the model\nimplementation at https://github.com/HKUDS/OpenGraph.\n", "link": "http://arxiv.org/abs/2403.01121v4", "date": "2024-10-09", "relevancy": 2.6095, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenGraph%3A%20Towards%20Open%20Graph%20Foundation%20Models&body=Title%3A%20OpenGraph%3A%20Towards%20Open%20Graph%20Foundation%20Models%0AAuthor%3A%20Lianghao%20Xia%20and%20Ben%20Kao%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Graph%20learning%20has%20become%20essential%20in%20various%20domains%2C%20including%0Arecommendation%20systems%20and%20social%20network%20analysis.%20Graph%20Neural%20Networks%0A%28GNNs%29%20have%20emerged%20as%20promising%20techniques%20for%20encoding%20structural%20information%0Aand%20improving%20performance%20in%20tasks%20like%20link%20prediction%20and%20node%0Aclassification.%20However%2C%20a%20key%20challenge%20remains%3A%20the%20difficulty%20of%0Ageneralizing%20to%20unseen%20graph%20data%20with%20different%20properties.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20graph%20foundation%20model%2C%20called%20OpenGraph%2C%20to%20address%20this%0Achallenge.%20Our%20approach%20tackles%20several%20technical%20obstacles.%20Firstly%2C%20we%0Aenhance%20data%20augmentation%20using%20a%20large%20language%20model%20%28LLM%29%20to%20overcome%20data%0Ascarcity%20in%20real-world%20scenarios.%20Secondly%2C%20we%20introduce%20a%20unified%20graph%0Atokenizer%20that%20enables%20the%20model%20to%20generalize%20effectively%20to%20diverse%20graph%0Adata%2C%20even%20when%20encountering%20unseen%20properties%20during%20training.%20Thirdly%2C%20our%0Adeveloped%20scalable%20graph%20transformer%20captures%20node-wise%20dependencies%20within%20the%0Aglobal%20topological%20context.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%0Aour%20framework.%20By%20adapting%20OpenGraph%20to%20new%20graph%20characteristics%20and%0Acomprehending%20diverse%20graphs%2C%20our%20approach%20achieves%20remarkable%20zero-shot%20graph%0Alearning%20performance%20across%20various%20settings.%20We%20release%20the%20model%0Aimplementation%20at%20https%3A//github.com/HKUDS/OpenGraph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01121v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenGraph%253A%2520Towards%2520Open%2520Graph%2520Foundation%2520Models%26entry.906535625%3DLianghao%2520Xia%2520and%2520Ben%2520Kao%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Graph%2520learning%2520has%2520become%2520essential%2520in%2520various%2520domains%252C%2520including%250Arecommendation%2520systems%2520and%2520social%2520network%2520analysis.%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529%2520have%2520emerged%2520as%2520promising%2520techniques%2520for%2520encoding%2520structural%2520information%250Aand%2520improving%2520performance%2520in%2520tasks%2520like%2520link%2520prediction%2520and%2520node%250Aclassification.%2520However%252C%2520a%2520key%2520challenge%2520remains%253A%2520the%2520difficulty%2520of%250Ageneralizing%2520to%2520unseen%2520graph%2520data%2520with%2520different%2520properties.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520graph%2520foundation%2520model%252C%2520called%2520OpenGraph%252C%2520to%2520address%2520this%250Achallenge.%2520Our%2520approach%2520tackles%2520several%2520technical%2520obstacles.%2520Firstly%252C%2520we%250Aenhance%2520data%2520augmentation%2520using%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520overcome%2520data%250Ascarcity%2520in%2520real-world%2520scenarios.%2520Secondly%252C%2520we%2520introduce%2520a%2520unified%2520graph%250Atokenizer%2520that%2520enables%2520the%2520model%2520to%2520generalize%2520effectively%2520to%2520diverse%2520graph%250Adata%252C%2520even%2520when%2520encountering%2520unseen%2520properties%2520during%2520training.%2520Thirdly%252C%2520our%250Adeveloped%2520scalable%2520graph%2520transformer%2520captures%2520node-wise%2520dependencies%2520within%2520the%250Aglobal%2520topological%2520context.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%250Aour%2520framework.%2520By%2520adapting%2520OpenGraph%2520to%2520new%2520graph%2520characteristics%2520and%250Acomprehending%2520diverse%2520graphs%252C%2520our%2520approach%2520achieves%2520remarkable%2520zero-shot%2520graph%250Alearning%2520performance%2520across%2520various%2520settings.%2520We%2520release%2520the%2520model%250Aimplementation%2520at%2520https%253A//github.com/HKUDS/OpenGraph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01121v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenGraph%3A%20Towards%20Open%20Graph%20Foundation%20Models&entry.906535625=Lianghao%20Xia%20and%20Ben%20Kao%20and%20Chao%20Huang&entry.1292438233=%20%20Graph%20learning%20has%20become%20essential%20in%20various%20domains%2C%20including%0Arecommendation%20systems%20and%20social%20network%20analysis.%20Graph%20Neural%20Networks%0A%28GNNs%29%20have%20emerged%20as%20promising%20techniques%20for%20encoding%20structural%20information%0Aand%20improving%20performance%20in%20tasks%20like%20link%20prediction%20and%20node%0Aclassification.%20However%2C%20a%20key%20challenge%20remains%3A%20the%20difficulty%20of%0Ageneralizing%20to%20unseen%20graph%20data%20with%20different%20properties.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20graph%20foundation%20model%2C%20called%20OpenGraph%2C%20to%20address%20this%0Achallenge.%20Our%20approach%20tackles%20several%20technical%20obstacles.%20Firstly%2C%20we%0Aenhance%20data%20augmentation%20using%20a%20large%20language%20model%20%28LLM%29%20to%20overcome%20data%0Ascarcity%20in%20real-world%20scenarios.%20Secondly%2C%20we%20introduce%20a%20unified%20graph%0Atokenizer%20that%20enables%20the%20model%20to%20generalize%20effectively%20to%20diverse%20graph%0Adata%2C%20even%20when%20encountering%20unseen%20properties%20during%20training.%20Thirdly%2C%20our%0Adeveloped%20scalable%20graph%20transformer%20captures%20node-wise%20dependencies%20within%20the%0Aglobal%20topological%20context.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%0Aour%20framework.%20By%20adapting%20OpenGraph%20to%20new%20graph%20characteristics%20and%0Acomprehending%20diverse%20graphs%2C%20our%20approach%20achieves%20remarkable%20zero-shot%20graph%0Alearning%20performance%20across%20various%20settings.%20We%20release%20the%20model%0Aimplementation%20at%20https%3A//github.com/HKUDS/OpenGraph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01121v4&entry.124074799=Read"},
{"title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM", "author": "Shengqiong Wu and Hao Fei and Xiangtai Li and Jiayi Ji and Hanwang Zhang and Tat-Seng Chua and Shuicheng Yan", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.\n", "link": "http://arxiv.org/abs/2406.05127v3", "date": "2024-10-09", "relevancy": 2.5953, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM&body=Title%3A%20Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM%0AAuthor%3A%20Shengqiong%20Wu%20and%20Hao%20Fei%20and%20Xiangtai%20Li%20and%20Jiayi%20Ji%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20exceptional%0Acapabilities%20in%20processing%20vision-language%20tasks.%20One%20of%20the%20crux%20of%20MLLMs%20lies%0Ain%20vision%20tokenization%2C%20which%20involves%20efficiently%20transforming%20input%20visual%0Asignals%20into%20feature%20representations%20that%20are%20most%20beneficial%20for%20LLMs.%0AHowever%2C%20existing%20vision%20tokenizers%2C%20essential%20for%20semantic%20alignment%20between%0Avision%20and%20language%2C%20remain%20problematic.%20Existing%20methods%20aggressively%20fragment%0Avisual%20input%2C%20corrupting%20the%20visual%20semantic%20integrity.%20To%20address%20this%2C%20this%0Apaper%20proposes%20a%20novel%20dynamic%20Semantic-Equivalent%20Vision%20Tokenizer%20%28SeTok%29%2C%0Awhich%20groups%20visual%20features%20into%20semantic%20units%20via%20a%20dynamic%20clustering%0Aalgorithm%2C%20flexibly%20determining%20the%20number%20of%20tokens%20based%20on%20image%20complexity.%0AThe%20resulting%20vision%20tokens%20effectively%20preserve%20semantic%20integrity%20and%20capture%0Aboth%20low-frequency%20and%20high-frequency%20visual%20features.%20The%20proposed%20MLLM%0A%28Setokim%29%20equipped%20with%20SeTok%20significantly%20demonstrates%20superior%20performance%0Aacross%20various%20tasks%2C%20as%20evidenced%20by%20our%20experimental%20results.%20The%20project%0Apage%20is%20at%20https%3A//chocowu.github.io/SeTok-web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05127v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Semantic%2520Equivalence%2520of%2520Tokenization%2520in%2520Multimodal%2520LLM%26entry.906535625%3DShengqiong%2520Wu%2520and%2520Hao%2520Fei%2520and%2520Xiangtai%2520Li%2520and%2520Jiayi%2520Ji%2520and%2520Hanwang%2520Zhang%2520and%2520Tat-Seng%2520Chua%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520exceptional%250Acapabilities%2520in%2520processing%2520vision-language%2520tasks.%2520One%2520of%2520the%2520crux%2520of%2520MLLMs%2520lies%250Ain%2520vision%2520tokenization%252C%2520which%2520involves%2520efficiently%2520transforming%2520input%2520visual%250Asignals%2520into%2520feature%2520representations%2520that%2520are%2520most%2520beneficial%2520for%2520LLMs.%250AHowever%252C%2520existing%2520vision%2520tokenizers%252C%2520essential%2520for%2520semantic%2520alignment%2520between%250Avision%2520and%2520language%252C%2520remain%2520problematic.%2520Existing%2520methods%2520aggressively%2520fragment%250Avisual%2520input%252C%2520corrupting%2520the%2520visual%2520semantic%2520integrity.%2520To%2520address%2520this%252C%2520this%250Apaper%2520proposes%2520a%2520novel%2520dynamic%2520Semantic-Equivalent%2520Vision%2520Tokenizer%2520%2528SeTok%2529%252C%250Awhich%2520groups%2520visual%2520features%2520into%2520semantic%2520units%2520via%2520a%2520dynamic%2520clustering%250Aalgorithm%252C%2520flexibly%2520determining%2520the%2520number%2520of%2520tokens%2520based%2520on%2520image%2520complexity.%250AThe%2520resulting%2520vision%2520tokens%2520effectively%2520preserve%2520semantic%2520integrity%2520and%2520capture%250Aboth%2520low-frequency%2520and%2520high-frequency%2520visual%2520features.%2520The%2520proposed%2520MLLM%250A%2528Setokim%2529%2520equipped%2520with%2520SeTok%2520significantly%2520demonstrates%2520superior%2520performance%250Aacross%2520various%2520tasks%252C%2520as%2520evidenced%2520by%2520our%2520experimental%2520results.%2520The%2520project%250Apage%2520is%2520at%2520https%253A//chocowu.github.io/SeTok-web/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05127v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Semantic%20Equivalence%20of%20Tokenization%20in%20Multimodal%20LLM&entry.906535625=Shengqiong%20Wu%20and%20Hao%20Fei%20and%20Xiangtai%20Li%20and%20Jiayi%20Ji%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20exceptional%0Acapabilities%20in%20processing%20vision-language%20tasks.%20One%20of%20the%20crux%20of%20MLLMs%20lies%0Ain%20vision%20tokenization%2C%20which%20involves%20efficiently%20transforming%20input%20visual%0Asignals%20into%20feature%20representations%20that%20are%20most%20beneficial%20for%20LLMs.%0AHowever%2C%20existing%20vision%20tokenizers%2C%20essential%20for%20semantic%20alignment%20between%0Avision%20and%20language%2C%20remain%20problematic.%20Existing%20methods%20aggressively%20fragment%0Avisual%20input%2C%20corrupting%20the%20visual%20semantic%20integrity.%20To%20address%20this%2C%20this%0Apaper%20proposes%20a%20novel%20dynamic%20Semantic-Equivalent%20Vision%20Tokenizer%20%28SeTok%29%2C%0Awhich%20groups%20visual%20features%20into%20semantic%20units%20via%20a%20dynamic%20clustering%0Aalgorithm%2C%20flexibly%20determining%20the%20number%20of%20tokens%20based%20on%20image%20complexity.%0AThe%20resulting%20vision%20tokens%20effectively%20preserve%20semantic%20integrity%20and%20capture%0Aboth%20low-frequency%20and%20high-frequency%20visual%20features.%20The%20proposed%20MLLM%0A%28Setokim%29%20equipped%20with%20SeTok%20significantly%20demonstrates%20superior%20performance%0Aacross%20various%20tasks%2C%20as%20evidenced%20by%20our%20experimental%20results.%20The%20project%0Apage%20is%20at%20https%3A//chocowu.github.io/SeTok-web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05127v3&entry.124074799=Read"},
{"title": "Which Programming Language and What Features at Pre-training Stage\n  Affect Downstream Logical Inference Performance?", "author": "Fumiya Uchiyama and Takeshi Kojima and Andrew Gambardella and Qi Cao and Yusuke Iwasawa and Yutaka Matsuo", "abstract": "  Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs.\n", "link": "http://arxiv.org/abs/2410.06735v1", "date": "2024-10-09", "relevancy": 2.5685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Which%20Programming%20Language%20and%20What%20Features%20at%20Pre-training%20Stage%0A%20%20Affect%20Downstream%20Logical%20Inference%20Performance%3F&body=Title%3A%20Which%20Programming%20Language%20and%20What%20Features%20at%20Pre-training%20Stage%0A%20%20Affect%20Downstream%20Logical%20Inference%20Performance%3F%0AAuthor%3A%20Fumiya%20Uchiyama%20and%20Takeshi%20Kojima%20and%20Andrew%20Gambardella%20and%20Qi%20Cao%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo%0AAbstract%3A%20%20%20Recent%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Ageneralization%20abilities%20in%20mathematics%20and%20logical%20reasoning%20tasks.%20Prior%0Aresearch%20indicates%20that%20LLMs%20pre-trained%20with%20programming%20language%20data%20exhibit%0Ahigh%20mathematical%20and%20reasoning%20abilities%3B%20however%2C%20this%20causal%20relationship%0Ahas%20not%20been%20rigorously%20tested.%20Our%20research%20aims%20to%20verify%20which%20programming%0Alanguages%20and%20features%20during%20pre-training%20affect%20logical%20inference%0Aperformance.%20Specifically%2C%20we%20pre-trained%20decoder-based%20language%20models%20from%0Ascratch%20using%20datasets%20from%20ten%20programming%20languages%20%28e.g.%2C%20Python%2C%20C%2C%20Java%29%0Aand%20three%20natural%20language%20datasets%20%28Wikipedia%2C%20Fineweb%2C%20C4%29%20under%20identical%0Aconditions.%20Thereafter%2C%20we%20evaluated%20the%20trained%20models%20in%20a%20few-shot%0Ain-context%20learning%20setting%20on%20logical%20reasoning%20tasks%3A%20FLD%20and%20bAbi%2C%20which%20do%0Anot%20require%20commonsense%20or%20world%20knowledge.%20The%20results%20demonstrate%20that%20nearly%0Aall%20models%20trained%20with%20programming%20languages%20consistently%20outperform%20those%0Atrained%20with%20natural%20languages%2C%20indicating%20that%20programming%20languages%20contain%0Afactors%20that%20elicit%20logic%20inference%20performance.%20In%20addition%2C%20we%20found%20that%0Amodels%20trained%20with%20programming%20languages%20exhibit%20a%20better%20ability%20to%20follow%0Ainstructions%20compared%20to%20those%20trained%20with%20natural%20languages.%20Further%20analysis%0Areveals%20that%20the%20depth%20of%20Abstract%20Syntax%20Trees%20representing%20parsed%20results%20of%0Aprograms%20also%20affects%20logical%20reasoning%20performance.%20These%20findings%20will%20offer%0Ainsights%20into%20the%20essential%20elements%20of%20pre-training%20for%20acquiring%20the%0Afoundational%20abilities%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhich%2520Programming%2520Language%2520and%2520What%2520Features%2520at%2520Pre-training%2520Stage%250A%2520%2520Affect%2520Downstream%2520Logical%2520Inference%2520Performance%253F%26entry.906535625%3DFumiya%2520Uchiyama%2520and%2520Takeshi%2520Kojima%2520and%2520Andrew%2520Gambardella%2520and%2520Qi%2520Cao%2520and%2520Yusuke%2520Iwasawa%2520and%2520Yutaka%2520Matsuo%26entry.1292438233%3D%2520%2520Recent%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%250Ageneralization%2520abilities%2520in%2520mathematics%2520and%2520logical%2520reasoning%2520tasks.%2520Prior%250Aresearch%2520indicates%2520that%2520LLMs%2520pre-trained%2520with%2520programming%2520language%2520data%2520exhibit%250Ahigh%2520mathematical%2520and%2520reasoning%2520abilities%253B%2520however%252C%2520this%2520causal%2520relationship%250Ahas%2520not%2520been%2520rigorously%2520tested.%2520Our%2520research%2520aims%2520to%2520verify%2520which%2520programming%250Alanguages%2520and%2520features%2520during%2520pre-training%2520affect%2520logical%2520inference%250Aperformance.%2520Specifically%252C%2520we%2520pre-trained%2520decoder-based%2520language%2520models%2520from%250Ascratch%2520using%2520datasets%2520from%2520ten%2520programming%2520languages%2520%2528e.g.%252C%2520Python%252C%2520C%252C%2520Java%2529%250Aand%2520three%2520natural%2520language%2520datasets%2520%2528Wikipedia%252C%2520Fineweb%252C%2520C4%2529%2520under%2520identical%250Aconditions.%2520Thereafter%252C%2520we%2520evaluated%2520the%2520trained%2520models%2520in%2520a%2520few-shot%250Ain-context%2520learning%2520setting%2520on%2520logical%2520reasoning%2520tasks%253A%2520FLD%2520and%2520bAbi%252C%2520which%2520do%250Anot%2520require%2520commonsense%2520or%2520world%2520knowledge.%2520The%2520results%2520demonstrate%2520that%2520nearly%250Aall%2520models%2520trained%2520with%2520programming%2520languages%2520consistently%2520outperform%2520those%250Atrained%2520with%2520natural%2520languages%252C%2520indicating%2520that%2520programming%2520languages%2520contain%250Afactors%2520that%2520elicit%2520logic%2520inference%2520performance.%2520In%2520addition%252C%2520we%2520found%2520that%250Amodels%2520trained%2520with%2520programming%2520languages%2520exhibit%2520a%2520better%2520ability%2520to%2520follow%250Ainstructions%2520compared%2520to%2520those%2520trained%2520with%2520natural%2520languages.%2520Further%2520analysis%250Areveals%2520that%2520the%2520depth%2520of%2520Abstract%2520Syntax%2520Trees%2520representing%2520parsed%2520results%2520of%250Aprograms%2520also%2520affects%2520logical%2520reasoning%2520performance.%2520These%2520findings%2520will%2520offer%250Ainsights%2520into%2520the%2520essential%2520elements%2520of%2520pre-training%2520for%2520acquiring%2520the%250Afoundational%2520abilities%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Which%20Programming%20Language%20and%20What%20Features%20at%20Pre-training%20Stage%0A%20%20Affect%20Downstream%20Logical%20Inference%20Performance%3F&entry.906535625=Fumiya%20Uchiyama%20and%20Takeshi%20Kojima%20and%20Andrew%20Gambardella%20and%20Qi%20Cao%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo&entry.1292438233=%20%20Recent%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%0Ageneralization%20abilities%20in%20mathematics%20and%20logical%20reasoning%20tasks.%20Prior%0Aresearch%20indicates%20that%20LLMs%20pre-trained%20with%20programming%20language%20data%20exhibit%0Ahigh%20mathematical%20and%20reasoning%20abilities%3B%20however%2C%20this%20causal%20relationship%0Ahas%20not%20been%20rigorously%20tested.%20Our%20research%20aims%20to%20verify%20which%20programming%0Alanguages%20and%20features%20during%20pre-training%20affect%20logical%20inference%0Aperformance.%20Specifically%2C%20we%20pre-trained%20decoder-based%20language%20models%20from%0Ascratch%20using%20datasets%20from%20ten%20programming%20languages%20%28e.g.%2C%20Python%2C%20C%2C%20Java%29%0Aand%20three%20natural%20language%20datasets%20%28Wikipedia%2C%20Fineweb%2C%20C4%29%20under%20identical%0Aconditions.%20Thereafter%2C%20we%20evaluated%20the%20trained%20models%20in%20a%20few-shot%0Ain-context%20learning%20setting%20on%20logical%20reasoning%20tasks%3A%20FLD%20and%20bAbi%2C%20which%20do%0Anot%20require%20commonsense%20or%20world%20knowledge.%20The%20results%20demonstrate%20that%20nearly%0Aall%20models%20trained%20with%20programming%20languages%20consistently%20outperform%20those%0Atrained%20with%20natural%20languages%2C%20indicating%20that%20programming%20languages%20contain%0Afactors%20that%20elicit%20logic%20inference%20performance.%20In%20addition%2C%20we%20found%20that%0Amodels%20trained%20with%20programming%20languages%20exhibit%20a%20better%20ability%20to%20follow%0Ainstructions%20compared%20to%20those%20trained%20with%20natural%20languages.%20Further%20analysis%0Areveals%20that%20the%20depth%20of%20Abstract%20Syntax%20Trees%20representing%20parsed%20results%20of%0Aprograms%20also%20affects%20logical%20reasoning%20performance.%20These%20findings%20will%20offer%0Ainsights%20into%20the%20essential%20elements%20of%20pre-training%20for%20acquiring%20the%0Afoundational%20abilities%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06735v1&entry.124074799=Read"},
{"title": "AdaRC: Mitigating Graph Structure Shifts during Test-Time", "author": "Wenxuan Bao and Zhichen Zeng and Zhining Liu and Hanghang Tong and Jingrui He", "abstract": "  Powerful as they are, graph neural networks (GNNs) are known to be vulnerable\nto distribution shifts. Recently, test-time adaptation (TTA) has attracted\nattention due to its ability to adapt a pre-trained model to a target domain\nwithout re-accessing the source domain. However, existing TTA algorithms are\nprimarily designed for attribute shifts in vision tasks, where samples are\nindependent. These methods perform poorly on graph data that experience\nstructure shifts, where node connectivity differs between source and target\ngraphs. We attribute this performance gap to the distinct impact of node\nattribute shifts versus graph structure shifts: the latter significantly\ndegrades the quality of node representations and blurs the boundaries between\ndifferent node categories. To address structure shifts in graphs, we propose\nAdaRC, an innovative framework designed for effective and efficient adaptation\nto structure shifts by adjusting the hop-aggregation parameters in GNNs. To\nenhance the representation quality, we design a prediction-informed clustering\nloss to encourage the formation of distinct clusters for different node\ncategories. Additionally, AdaRC seamlessly integrates with existing TTA\nalgorithms, allowing it to handle attribute shifts effectively while improving\noverall performance under combined structure and attribute shifts. We validate\nthe effectiveness of AdaRC on both synthetic and real-world datasets,\ndemonstrating its robustness across various combinations of structure and\nattribute shifts.\n", "link": "http://arxiv.org/abs/2410.06976v1", "date": "2024-10-09", "relevancy": 2.5247, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5155}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaRC%3A%20Mitigating%20Graph%20Structure%20Shifts%20during%20Test-Time&body=Title%3A%20AdaRC%3A%20Mitigating%20Graph%20Structure%20Shifts%20during%20Test-Time%0AAuthor%3A%20Wenxuan%20Bao%20and%20Zhichen%20Zeng%20and%20Zhining%20Liu%20and%20Hanghang%20Tong%20and%20Jingrui%20He%0AAbstract%3A%20%20%20Powerful%20as%20they%20are%2C%20graph%20neural%20networks%20%28GNNs%29%20are%20known%20to%20be%20vulnerable%0Ato%20distribution%20shifts.%20Recently%2C%20test-time%20adaptation%20%28TTA%29%20has%20attracted%0Aattention%20due%20to%20its%20ability%20to%20adapt%20a%20pre-trained%20model%20to%20a%20target%20domain%0Awithout%20re-accessing%20the%20source%20domain.%20However%2C%20existing%20TTA%20algorithms%20are%0Aprimarily%20designed%20for%20attribute%20shifts%20in%20vision%20tasks%2C%20where%20samples%20are%0Aindependent.%20These%20methods%20perform%20poorly%20on%20graph%20data%20that%20experience%0Astructure%20shifts%2C%20where%20node%20connectivity%20differs%20between%20source%20and%20target%0Agraphs.%20We%20attribute%20this%20performance%20gap%20to%20the%20distinct%20impact%20of%20node%0Aattribute%20shifts%20versus%20graph%20structure%20shifts%3A%20the%20latter%20significantly%0Adegrades%20the%20quality%20of%20node%20representations%20and%20blurs%20the%20boundaries%20between%0Adifferent%20node%20categories.%20To%20address%20structure%20shifts%20in%20graphs%2C%20we%20propose%0AAdaRC%2C%20an%20innovative%20framework%20designed%20for%20effective%20and%20efficient%20adaptation%0Ato%20structure%20shifts%20by%20adjusting%20the%20hop-aggregation%20parameters%20in%20GNNs.%20To%0Aenhance%20the%20representation%20quality%2C%20we%20design%20a%20prediction-informed%20clustering%0Aloss%20to%20encourage%20the%20formation%20of%20distinct%20clusters%20for%20different%20node%0Acategories.%20Additionally%2C%20AdaRC%20seamlessly%20integrates%20with%20existing%20TTA%0Aalgorithms%2C%20allowing%20it%20to%20handle%20attribute%20shifts%20effectively%20while%20improving%0Aoverall%20performance%20under%20combined%20structure%20and%20attribute%20shifts.%20We%20validate%0Athe%20effectiveness%20of%20AdaRC%20on%20both%20synthetic%20and%20real-world%20datasets%2C%0Ademonstrating%20its%20robustness%20across%20various%20combinations%20of%20structure%20and%0Aattribute%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaRC%253A%2520Mitigating%2520Graph%2520Structure%2520Shifts%2520during%2520Test-Time%26entry.906535625%3DWenxuan%2520Bao%2520and%2520Zhichen%2520Zeng%2520and%2520Zhining%2520Liu%2520and%2520Hanghang%2520Tong%2520and%2520Jingrui%2520He%26entry.1292438233%3D%2520%2520Powerful%2520as%2520they%2520are%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520known%2520to%2520be%2520vulnerable%250Ato%2520distribution%2520shifts.%2520Recently%252C%2520test-time%2520adaptation%2520%2528TTA%2529%2520has%2520attracted%250Aattention%2520due%2520to%2520its%2520ability%2520to%2520adapt%2520a%2520pre-trained%2520model%2520to%2520a%2520target%2520domain%250Awithout%2520re-accessing%2520the%2520source%2520domain.%2520However%252C%2520existing%2520TTA%2520algorithms%2520are%250Aprimarily%2520designed%2520for%2520attribute%2520shifts%2520in%2520vision%2520tasks%252C%2520where%2520samples%2520are%250Aindependent.%2520These%2520methods%2520perform%2520poorly%2520on%2520graph%2520data%2520that%2520experience%250Astructure%2520shifts%252C%2520where%2520node%2520connectivity%2520differs%2520between%2520source%2520and%2520target%250Agraphs.%2520We%2520attribute%2520this%2520performance%2520gap%2520to%2520the%2520distinct%2520impact%2520of%2520node%250Aattribute%2520shifts%2520versus%2520graph%2520structure%2520shifts%253A%2520the%2520latter%2520significantly%250Adegrades%2520the%2520quality%2520of%2520node%2520representations%2520and%2520blurs%2520the%2520boundaries%2520between%250Adifferent%2520node%2520categories.%2520To%2520address%2520structure%2520shifts%2520in%2520graphs%252C%2520we%2520propose%250AAdaRC%252C%2520an%2520innovative%2520framework%2520designed%2520for%2520effective%2520and%2520efficient%2520adaptation%250Ato%2520structure%2520shifts%2520by%2520adjusting%2520the%2520hop-aggregation%2520parameters%2520in%2520GNNs.%2520To%250Aenhance%2520the%2520representation%2520quality%252C%2520we%2520design%2520a%2520prediction-informed%2520clustering%250Aloss%2520to%2520encourage%2520the%2520formation%2520of%2520distinct%2520clusters%2520for%2520different%2520node%250Acategories.%2520Additionally%252C%2520AdaRC%2520seamlessly%2520integrates%2520with%2520existing%2520TTA%250Aalgorithms%252C%2520allowing%2520it%2520to%2520handle%2520attribute%2520shifts%2520effectively%2520while%2520improving%250Aoverall%2520performance%2520under%2520combined%2520structure%2520and%2520attribute%2520shifts.%2520We%2520validate%250Athe%2520effectiveness%2520of%2520AdaRC%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%250Ademonstrating%2520its%2520robustness%2520across%2520various%2520combinations%2520of%2520structure%2520and%250Aattribute%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaRC%3A%20Mitigating%20Graph%20Structure%20Shifts%20during%20Test-Time&entry.906535625=Wenxuan%20Bao%20and%20Zhichen%20Zeng%20and%20Zhining%20Liu%20and%20Hanghang%20Tong%20and%20Jingrui%20He&entry.1292438233=%20%20Powerful%20as%20they%20are%2C%20graph%20neural%20networks%20%28GNNs%29%20are%20known%20to%20be%20vulnerable%0Ato%20distribution%20shifts.%20Recently%2C%20test-time%20adaptation%20%28TTA%29%20has%20attracted%0Aattention%20due%20to%20its%20ability%20to%20adapt%20a%20pre-trained%20model%20to%20a%20target%20domain%0Awithout%20re-accessing%20the%20source%20domain.%20However%2C%20existing%20TTA%20algorithms%20are%0Aprimarily%20designed%20for%20attribute%20shifts%20in%20vision%20tasks%2C%20where%20samples%20are%0Aindependent.%20These%20methods%20perform%20poorly%20on%20graph%20data%20that%20experience%0Astructure%20shifts%2C%20where%20node%20connectivity%20differs%20between%20source%20and%20target%0Agraphs.%20We%20attribute%20this%20performance%20gap%20to%20the%20distinct%20impact%20of%20node%0Aattribute%20shifts%20versus%20graph%20structure%20shifts%3A%20the%20latter%20significantly%0Adegrades%20the%20quality%20of%20node%20representations%20and%20blurs%20the%20boundaries%20between%0Adifferent%20node%20categories.%20To%20address%20structure%20shifts%20in%20graphs%2C%20we%20propose%0AAdaRC%2C%20an%20innovative%20framework%20designed%20for%20effective%20and%20efficient%20adaptation%0Ato%20structure%20shifts%20by%20adjusting%20the%20hop-aggregation%20parameters%20in%20GNNs.%20To%0Aenhance%20the%20representation%20quality%2C%20we%20design%20a%20prediction-informed%20clustering%0Aloss%20to%20encourage%20the%20formation%20of%20distinct%20clusters%20for%20different%20node%0Acategories.%20Additionally%2C%20AdaRC%20seamlessly%20integrates%20with%20existing%20TTA%0Aalgorithms%2C%20allowing%20it%20to%20handle%20attribute%20shifts%20effectively%20while%20improving%0Aoverall%20performance%20under%20combined%20structure%20and%20attribute%20shifts.%20We%20validate%0Athe%20effectiveness%20of%20AdaRC%20on%20both%20synthetic%20and%20real-world%20datasets%2C%0Ademonstrating%20its%20robustness%20across%20various%20combinations%20of%20structure%20and%0Aattribute%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06976v1&entry.124074799=Read"},
{"title": "The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed\n  Learning", "author": "Youssef Allouah and Rachid Guerraoui and Nirupam Gupta and Ahmed Jellouli and Geovani Rizk and John Stephan", "abstract": "  Byzantine-resilient distributed machine learning seeks to achieve robust\nlearning performance in the presence of misbehaving or adversarial workers.\nWhile state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD)\nmethods were proven theoretically optimal, their empirical success has often\nrelied on pre-aggregation gradient clipping. However, the currently considered\nstatic clipping strategy exhibits mixed results: improving robustness against\nsome attacks while being ineffective or detrimental against others. We address\nthis gap by proposing a principled adaptive clipping strategy, termed Adaptive\nRobust Clipping (ARC). We show that ARC consistently enhances the empirical\nrobustness of SOTA Robust-DGD methods, while preserving the theoretical\nrobustness guarantees. Our analysis shows that ARC provably improves the\nasymptotic convergence guarantee of Robust-DGD in the case when the model is\nwell-initialized. We validate this theoretical insight through an exhaustive\nset of experiments on benchmark image classification tasks. We observe that the\nimprovement induced by ARC is more pronounced in highly heterogeneous and\nadversarial settings.\n", "link": "http://arxiv.org/abs/2405.14432v4", "date": "2024-10-09", "relevancy": 2.512, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5164}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4959}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Vital%20Role%20of%20Gradient%20Clipping%20in%20Byzantine-Resilient%20Distributed%0A%20%20Learning&body=Title%3A%20The%20Vital%20Role%20of%20Gradient%20Clipping%20in%20Byzantine-Resilient%20Distributed%0A%20%20Learning%0AAuthor%3A%20Youssef%20Allouah%20and%20Rachid%20Guerraoui%20and%20Nirupam%20Gupta%20and%20Ahmed%20Jellouli%20and%20Geovani%20Rizk%20and%20John%20Stephan%0AAbstract%3A%20%20%20Byzantine-resilient%20distributed%20machine%20learning%20seeks%20to%20achieve%20robust%0Alearning%20performance%20in%20the%20presence%20of%20misbehaving%20or%20adversarial%20workers.%0AWhile%20state-of-the-art%20%28SOTA%29%20robust%20distributed%20gradient%20descent%20%28Robust-DGD%29%0Amethods%20were%20proven%20theoretically%20optimal%2C%20their%20empirical%20success%20has%20often%0Arelied%20on%20pre-aggregation%20gradient%20clipping.%20However%2C%20the%20currently%20considered%0Astatic%20clipping%20strategy%20exhibits%20mixed%20results%3A%20improving%20robustness%20against%0Asome%20attacks%20while%20being%20ineffective%20or%20detrimental%20against%20others.%20We%20address%0Athis%20gap%20by%20proposing%20a%20principled%20adaptive%20clipping%20strategy%2C%20termed%20Adaptive%0ARobust%20Clipping%20%28ARC%29.%20We%20show%20that%20ARC%20consistently%20enhances%20the%20empirical%0Arobustness%20of%20SOTA%20Robust-DGD%20methods%2C%20while%20preserving%20the%20theoretical%0Arobustness%20guarantees.%20Our%20analysis%20shows%20that%20ARC%20provably%20improves%20the%0Aasymptotic%20convergence%20guarantee%20of%20Robust-DGD%20in%20the%20case%20when%20the%20model%20is%0Awell-initialized.%20We%20validate%20this%20theoretical%20insight%20through%20an%20exhaustive%0Aset%20of%20experiments%20on%20benchmark%20image%20classification%20tasks.%20We%20observe%20that%20the%0Aimprovement%20induced%20by%20ARC%20is%20more%20pronounced%20in%20highly%20heterogeneous%20and%0Aadversarial%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14432v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Vital%2520Role%2520of%2520Gradient%2520Clipping%2520in%2520Byzantine-Resilient%2520Distributed%250A%2520%2520Learning%26entry.906535625%3DYoussef%2520Allouah%2520and%2520Rachid%2520Guerraoui%2520and%2520Nirupam%2520Gupta%2520and%2520Ahmed%2520Jellouli%2520and%2520Geovani%2520Rizk%2520and%2520John%2520Stephan%26entry.1292438233%3D%2520%2520Byzantine-resilient%2520distributed%2520machine%2520learning%2520seeks%2520to%2520achieve%2520robust%250Alearning%2520performance%2520in%2520the%2520presence%2520of%2520misbehaving%2520or%2520adversarial%2520workers.%250AWhile%2520state-of-the-art%2520%2528SOTA%2529%2520robust%2520distributed%2520gradient%2520descent%2520%2528Robust-DGD%2529%250Amethods%2520were%2520proven%2520theoretically%2520optimal%252C%2520their%2520empirical%2520success%2520has%2520often%250Arelied%2520on%2520pre-aggregation%2520gradient%2520clipping.%2520However%252C%2520the%2520currently%2520considered%250Astatic%2520clipping%2520strategy%2520exhibits%2520mixed%2520results%253A%2520improving%2520robustness%2520against%250Asome%2520attacks%2520while%2520being%2520ineffective%2520or%2520detrimental%2520against%2520others.%2520We%2520address%250Athis%2520gap%2520by%2520proposing%2520a%2520principled%2520adaptive%2520clipping%2520strategy%252C%2520termed%2520Adaptive%250ARobust%2520Clipping%2520%2528ARC%2529.%2520We%2520show%2520that%2520ARC%2520consistently%2520enhances%2520the%2520empirical%250Arobustness%2520of%2520SOTA%2520Robust-DGD%2520methods%252C%2520while%2520preserving%2520the%2520theoretical%250Arobustness%2520guarantees.%2520Our%2520analysis%2520shows%2520that%2520ARC%2520provably%2520improves%2520the%250Aasymptotic%2520convergence%2520guarantee%2520of%2520Robust-DGD%2520in%2520the%2520case%2520when%2520the%2520model%2520is%250Awell-initialized.%2520We%2520validate%2520this%2520theoretical%2520insight%2520through%2520an%2520exhaustive%250Aset%2520of%2520experiments%2520on%2520benchmark%2520image%2520classification%2520tasks.%2520We%2520observe%2520that%2520the%250Aimprovement%2520induced%2520by%2520ARC%2520is%2520more%2520pronounced%2520in%2520highly%2520heterogeneous%2520and%250Aadversarial%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14432v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Vital%20Role%20of%20Gradient%20Clipping%20in%20Byzantine-Resilient%20Distributed%0A%20%20Learning&entry.906535625=Youssef%20Allouah%20and%20Rachid%20Guerraoui%20and%20Nirupam%20Gupta%20and%20Ahmed%20Jellouli%20and%20Geovani%20Rizk%20and%20John%20Stephan&entry.1292438233=%20%20Byzantine-resilient%20distributed%20machine%20learning%20seeks%20to%20achieve%20robust%0Alearning%20performance%20in%20the%20presence%20of%20misbehaving%20or%20adversarial%20workers.%0AWhile%20state-of-the-art%20%28SOTA%29%20robust%20distributed%20gradient%20descent%20%28Robust-DGD%29%0Amethods%20were%20proven%20theoretically%20optimal%2C%20their%20empirical%20success%20has%20often%0Arelied%20on%20pre-aggregation%20gradient%20clipping.%20However%2C%20the%20currently%20considered%0Astatic%20clipping%20strategy%20exhibits%20mixed%20results%3A%20improving%20robustness%20against%0Asome%20attacks%20while%20being%20ineffective%20or%20detrimental%20against%20others.%20We%20address%0Athis%20gap%20by%20proposing%20a%20principled%20adaptive%20clipping%20strategy%2C%20termed%20Adaptive%0ARobust%20Clipping%20%28ARC%29.%20We%20show%20that%20ARC%20consistently%20enhances%20the%20empirical%0Arobustness%20of%20SOTA%20Robust-DGD%20methods%2C%20while%20preserving%20the%20theoretical%0Arobustness%20guarantees.%20Our%20analysis%20shows%20that%20ARC%20provably%20improves%20the%0Aasymptotic%20convergence%20guarantee%20of%20Robust-DGD%20in%20the%20case%20when%20the%20model%20is%0Awell-initialized.%20We%20validate%20this%20theoretical%20insight%20through%20an%20exhaustive%0Aset%20of%20experiments%20on%20benchmark%20image%20classification%20tasks.%20We%20observe%20that%20the%0Aimprovement%20induced%20by%20ARC%20is%20more%20pronounced%20in%20highly%20heterogeneous%20and%0Aadversarial%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14432v4&entry.124074799=Read"},
{"title": "Self-Boosting Large Language Models with Synthetic Preference Data", "author": "Qingxiu Dong and Li Dong and Xingxing Zhang and Zhifang Sui and Furu Wei", "abstract": "  Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.\n", "link": "http://arxiv.org/abs/2410.06961v1", "date": "2024-10-09", "relevancy": 2.4886, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Boosting%20Large%20Language%20Models%20with%20Synthetic%20Preference%20Data&body=Title%3A%20Self-Boosting%20Large%20Language%20Models%20with%20Synthetic%20Preference%20Data%0AAuthor%3A%20Qingxiu%20Dong%20and%20Li%20Dong%20and%20Xingxing%20Zhang%20and%20Zhifang%20Sui%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Through%20alignment%20with%20human%20preferences%2C%20Large%20Language%20Models%20%28LLMs%29%20have%0Aadvanced%20significantly%20in%20generating%20honest%2C%20harmless%2C%20and%20helpful%20responses.%0AHowever%2C%20collecting%20high-quality%20preference%20data%20is%20a%20resource-intensive%20and%0Acreativity-demanding%20process%2C%20especially%20for%20the%20continual%20improvement%20of%20LLMs.%0AWe%20introduce%20SynPO%2C%20a%20self-boosting%20paradigm%20that%20leverages%20synthetic%0Apreference%20data%20for%20model%20alignment.%20SynPO%20employs%20an%20iterative%20mechanism%0Awherein%20a%20self-prompt%20generator%20creates%20diverse%20prompts%2C%20and%20a%20response%0Aimprover%20refines%20model%20responses%20progressively.%20This%20approach%20trains%20LLMs%20to%0Aautonomously%20learn%20the%20generative%20rewards%20for%20their%20own%20outputs%20and%20eliminates%0Athe%20need%20for%20large-scale%20annotation%20of%20prompts%20and%20human%20preferences.%20After%0Afour%20SynPO%20iterations%2C%20Llama3-8B%20and%20Mistral-7B%20show%20significant%20enhancements%0Ain%20instruction-following%20abilities%2C%20achieving%20over%2022.1%25%20win%20rate%20improvements%0Aon%20AlpacaEval%202.0%20and%20ArenaHard.%20Simultaneously%2C%20SynPO%20improves%20the%20general%0Aperformance%20of%20LLMs%20on%20various%20tasks%2C%20validated%20by%20a%203.2%20to%205.0%20average%20score%0Aincrease%20on%20the%20well-recognized%20Open%20LLM%20leaderboard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Boosting%2520Large%2520Language%2520Models%2520with%2520Synthetic%2520Preference%2520Data%26entry.906535625%3DQingxiu%2520Dong%2520and%2520Li%2520Dong%2520and%2520Xingxing%2520Zhang%2520and%2520Zhifang%2520Sui%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Through%2520alignment%2520with%2520human%2520preferences%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Aadvanced%2520significantly%2520in%2520generating%2520honest%252C%2520harmless%252C%2520and%2520helpful%2520responses.%250AHowever%252C%2520collecting%2520high-quality%2520preference%2520data%2520is%2520a%2520resource-intensive%2520and%250Acreativity-demanding%2520process%252C%2520especially%2520for%2520the%2520continual%2520improvement%2520of%2520LLMs.%250AWe%2520introduce%2520SynPO%252C%2520a%2520self-boosting%2520paradigm%2520that%2520leverages%2520synthetic%250Apreference%2520data%2520for%2520model%2520alignment.%2520SynPO%2520employs%2520an%2520iterative%2520mechanism%250Awherein%2520a%2520self-prompt%2520generator%2520creates%2520diverse%2520prompts%252C%2520and%2520a%2520response%250Aimprover%2520refines%2520model%2520responses%2520progressively.%2520This%2520approach%2520trains%2520LLMs%2520to%250Aautonomously%2520learn%2520the%2520generative%2520rewards%2520for%2520their%2520own%2520outputs%2520and%2520eliminates%250Athe%2520need%2520for%2520large-scale%2520annotation%2520of%2520prompts%2520and%2520human%2520preferences.%2520After%250Afour%2520SynPO%2520iterations%252C%2520Llama3-8B%2520and%2520Mistral-7B%2520show%2520significant%2520enhancements%250Ain%2520instruction-following%2520abilities%252C%2520achieving%2520over%252022.1%2525%2520win%2520rate%2520improvements%250Aon%2520AlpacaEval%25202.0%2520and%2520ArenaHard.%2520Simultaneously%252C%2520SynPO%2520improves%2520the%2520general%250Aperformance%2520of%2520LLMs%2520on%2520various%2520tasks%252C%2520validated%2520by%2520a%25203.2%2520to%25205.0%2520average%2520score%250Aincrease%2520on%2520the%2520well-recognized%2520Open%2520LLM%2520leaderboard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Boosting%20Large%20Language%20Models%20with%20Synthetic%20Preference%20Data&entry.906535625=Qingxiu%20Dong%20and%20Li%20Dong%20and%20Xingxing%20Zhang%20and%20Zhifang%20Sui%20and%20Furu%20Wei&entry.1292438233=%20%20Through%20alignment%20with%20human%20preferences%2C%20Large%20Language%20Models%20%28LLMs%29%20have%0Aadvanced%20significantly%20in%20generating%20honest%2C%20harmless%2C%20and%20helpful%20responses.%0AHowever%2C%20collecting%20high-quality%20preference%20data%20is%20a%20resource-intensive%20and%0Acreativity-demanding%20process%2C%20especially%20for%20the%20continual%20improvement%20of%20LLMs.%0AWe%20introduce%20SynPO%2C%20a%20self-boosting%20paradigm%20that%20leverages%20synthetic%0Apreference%20data%20for%20model%20alignment.%20SynPO%20employs%20an%20iterative%20mechanism%0Awherein%20a%20self-prompt%20generator%20creates%20diverse%20prompts%2C%20and%20a%20response%0Aimprover%20refines%20model%20responses%20progressively.%20This%20approach%20trains%20LLMs%20to%0Aautonomously%20learn%20the%20generative%20rewards%20for%20their%20own%20outputs%20and%20eliminates%0Athe%20need%20for%20large-scale%20annotation%20of%20prompts%20and%20human%20preferences.%20After%0Afour%20SynPO%20iterations%2C%20Llama3-8B%20and%20Mistral-7B%20show%20significant%20enhancements%0Ain%20instruction-following%20abilities%2C%20achieving%20over%2022.1%25%20win%20rate%20improvements%0Aon%20AlpacaEval%202.0%20and%20ArenaHard.%20Simultaneously%2C%20SynPO%20improves%20the%20general%0Aperformance%20of%20LLMs%20on%20various%20tasks%2C%20validated%20by%20a%203.2%20to%205.0%20average%20score%0Aincrease%20on%20the%20well-recognized%20Open%20LLM%20leaderboard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06961v1&entry.124074799=Read"},
{"title": "Rethinking the Evaluation of Visible and Infrared Image Fusion", "author": "Dayan Guan and Yixuan Wu and Tianzhu Liu and Alex C. Kot and Yanfeng Gu", "abstract": "  Visible and Infrared Image Fusion (VIF) has garnered significant interest\nacross a wide range of high-level vision tasks, such as object detection and\nsemantic segmentation. However, the evaluation of VIF methods remains\nchallenging due to the absence of ground truth. This paper proposes a\nSegmentation-oriented Evaluation Approach (SEA) to assess VIF methods by\nincorporating the semantic segmentation task and leveraging segmentation labels\navailable in latest VIF datasets. Specifically, SEA utilizes universal\nsegmentation models, capable of handling diverse images and classes, to predict\nsegmentation outputs from fused images and compare these outputs with\nsegmentation labels. Our evaluation of recent VIF methods using SEA reveals\nthat their performance is comparable or even inferior to using visible images\nonly, despite nearly half of the infrared images demonstrating better\nperformance than visible images. Further analysis indicates that the two\nmetrics most correlated to our SEA are the gradient-based fusion metric\n$Q_{\\text{ABF}}$ and the visual information fidelity metric $Q_{\\text{VIFF}}$\nin conventional VIF evaluation metrics, which can serve as proxies when\nsegmentation labels are unavailable. We hope that our evaluation will guide the\ndevelopment of novel and practical VIF methods. The code has been released in\n\\url{https://github.com/Yixuan-2002/SEA/}.\n", "link": "http://arxiv.org/abs/2410.06811v1", "date": "2024-10-09", "relevancy": 2.474, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Evaluation%20of%20Visible%20and%20Infrared%20Image%20Fusion&body=Title%3A%20Rethinking%20the%20Evaluation%20of%20Visible%20and%20Infrared%20Image%20Fusion%0AAuthor%3A%20Dayan%20Guan%20and%20Yixuan%20Wu%20and%20Tianzhu%20Liu%20and%20Alex%20C.%20Kot%20and%20Yanfeng%20Gu%0AAbstract%3A%20%20%20Visible%20and%20Infrared%20Image%20Fusion%20%28VIF%29%20has%20garnered%20significant%20interest%0Aacross%20a%20wide%20range%20of%20high-level%20vision%20tasks%2C%20such%20as%20object%20detection%20and%0Asemantic%20segmentation.%20However%2C%20the%20evaluation%20of%20VIF%20methods%20remains%0Achallenging%20due%20to%20the%20absence%20of%20ground%20truth.%20This%20paper%20proposes%20a%0ASegmentation-oriented%20Evaluation%20Approach%20%28SEA%29%20to%20assess%20VIF%20methods%20by%0Aincorporating%20the%20semantic%20segmentation%20task%20and%20leveraging%20segmentation%20labels%0Aavailable%20in%20latest%20VIF%20datasets.%20Specifically%2C%20SEA%20utilizes%20universal%0Asegmentation%20models%2C%20capable%20of%20handling%20diverse%20images%20and%20classes%2C%20to%20predict%0Asegmentation%20outputs%20from%20fused%20images%20and%20compare%20these%20outputs%20with%0Asegmentation%20labels.%20Our%20evaluation%20of%20recent%20VIF%20methods%20using%20SEA%20reveals%0Athat%20their%20performance%20is%20comparable%20or%20even%20inferior%20to%20using%20visible%20images%0Aonly%2C%20despite%20nearly%20half%20of%20the%20infrared%20images%20demonstrating%20better%0Aperformance%20than%20visible%20images.%20Further%20analysis%20indicates%20that%20the%20two%0Ametrics%20most%20correlated%20to%20our%20SEA%20are%20the%20gradient-based%20fusion%20metric%0A%24Q_%7B%5Ctext%7BABF%7D%7D%24%20and%20the%20visual%20information%20fidelity%20metric%20%24Q_%7B%5Ctext%7BVIFF%7D%7D%24%0Ain%20conventional%20VIF%20evaluation%20metrics%2C%20which%20can%20serve%20as%20proxies%20when%0Asegmentation%20labels%20are%20unavailable.%20We%20hope%20that%20our%20evaluation%20will%20guide%20the%0Adevelopment%20of%20novel%20and%20practical%20VIF%20methods.%20The%20code%20has%20been%20released%20in%0A%5Curl%7Bhttps%3A//github.com/Yixuan-2002/SEA/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Evaluation%2520of%2520Visible%2520and%2520Infrared%2520Image%2520Fusion%26entry.906535625%3DDayan%2520Guan%2520and%2520Yixuan%2520Wu%2520and%2520Tianzhu%2520Liu%2520and%2520Alex%2520C.%2520Kot%2520and%2520Yanfeng%2520Gu%26entry.1292438233%3D%2520%2520Visible%2520and%2520Infrared%2520Image%2520Fusion%2520%2528VIF%2529%2520has%2520garnered%2520significant%2520interest%250Aacross%2520a%2520wide%2520range%2520of%2520high-level%2520vision%2520tasks%252C%2520such%2520as%2520object%2520detection%2520and%250Asemantic%2520segmentation.%2520However%252C%2520the%2520evaluation%2520of%2520VIF%2520methods%2520remains%250Achallenging%2520due%2520to%2520the%2520absence%2520of%2520ground%2520truth.%2520This%2520paper%2520proposes%2520a%250ASegmentation-oriented%2520Evaluation%2520Approach%2520%2528SEA%2529%2520to%2520assess%2520VIF%2520methods%2520by%250Aincorporating%2520the%2520semantic%2520segmentation%2520task%2520and%2520leveraging%2520segmentation%2520labels%250Aavailable%2520in%2520latest%2520VIF%2520datasets.%2520Specifically%252C%2520SEA%2520utilizes%2520universal%250Asegmentation%2520models%252C%2520capable%2520of%2520handling%2520diverse%2520images%2520and%2520classes%252C%2520to%2520predict%250Asegmentation%2520outputs%2520from%2520fused%2520images%2520and%2520compare%2520these%2520outputs%2520with%250Asegmentation%2520labels.%2520Our%2520evaluation%2520of%2520recent%2520VIF%2520methods%2520using%2520SEA%2520reveals%250Athat%2520their%2520performance%2520is%2520comparable%2520or%2520even%2520inferior%2520to%2520using%2520visible%2520images%250Aonly%252C%2520despite%2520nearly%2520half%2520of%2520the%2520infrared%2520images%2520demonstrating%2520better%250Aperformance%2520than%2520visible%2520images.%2520Further%2520analysis%2520indicates%2520that%2520the%2520two%250Ametrics%2520most%2520correlated%2520to%2520our%2520SEA%2520are%2520the%2520gradient-based%2520fusion%2520metric%250A%2524Q_%257B%255Ctext%257BABF%257D%257D%2524%2520and%2520the%2520visual%2520information%2520fidelity%2520metric%2520%2524Q_%257B%255Ctext%257BVIFF%257D%257D%2524%250Ain%2520conventional%2520VIF%2520evaluation%2520metrics%252C%2520which%2520can%2520serve%2520as%2520proxies%2520when%250Asegmentation%2520labels%2520are%2520unavailable.%2520We%2520hope%2520that%2520our%2520evaluation%2520will%2520guide%2520the%250Adevelopment%2520of%2520novel%2520and%2520practical%2520VIF%2520methods.%2520The%2520code%2520has%2520been%2520released%2520in%250A%255Curl%257Bhttps%253A//github.com/Yixuan-2002/SEA/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Evaluation%20of%20Visible%20and%20Infrared%20Image%20Fusion&entry.906535625=Dayan%20Guan%20and%20Yixuan%20Wu%20and%20Tianzhu%20Liu%20and%20Alex%20C.%20Kot%20and%20Yanfeng%20Gu&entry.1292438233=%20%20Visible%20and%20Infrared%20Image%20Fusion%20%28VIF%29%20has%20garnered%20significant%20interest%0Aacross%20a%20wide%20range%20of%20high-level%20vision%20tasks%2C%20such%20as%20object%20detection%20and%0Asemantic%20segmentation.%20However%2C%20the%20evaluation%20of%20VIF%20methods%20remains%0Achallenging%20due%20to%20the%20absence%20of%20ground%20truth.%20This%20paper%20proposes%20a%0ASegmentation-oriented%20Evaluation%20Approach%20%28SEA%29%20to%20assess%20VIF%20methods%20by%0Aincorporating%20the%20semantic%20segmentation%20task%20and%20leveraging%20segmentation%20labels%0Aavailable%20in%20latest%20VIF%20datasets.%20Specifically%2C%20SEA%20utilizes%20universal%0Asegmentation%20models%2C%20capable%20of%20handling%20diverse%20images%20and%20classes%2C%20to%20predict%0Asegmentation%20outputs%20from%20fused%20images%20and%20compare%20these%20outputs%20with%0Asegmentation%20labels.%20Our%20evaluation%20of%20recent%20VIF%20methods%20using%20SEA%20reveals%0Athat%20their%20performance%20is%20comparable%20or%20even%20inferior%20to%20using%20visible%20images%0Aonly%2C%20despite%20nearly%20half%20of%20the%20infrared%20images%20demonstrating%20better%0Aperformance%20than%20visible%20images.%20Further%20analysis%20indicates%20that%20the%20two%0Ametrics%20most%20correlated%20to%20our%20SEA%20are%20the%20gradient-based%20fusion%20metric%0A%24Q_%7B%5Ctext%7BABF%7D%7D%24%20and%20the%20visual%20information%20fidelity%20metric%20%24Q_%7B%5Ctext%7BVIFF%7D%7D%24%0Ain%20conventional%20VIF%20evaluation%20metrics%2C%20which%20can%20serve%20as%20proxies%20when%0Asegmentation%20labels%20are%20unavailable.%20We%20hope%20that%20our%20evaluation%20will%20guide%20the%0Adevelopment%20of%20novel%20and%20practical%20VIF%20methods.%20The%20code%20has%20been%20released%20in%0A%5Curl%7Bhttps%3A//github.com/Yixuan-2002/SEA/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06811v1&entry.124074799=Read"},
{"title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness", "author": "Zekun Wang and Feiyu Duan and Yibo Zhang and Wangchunshu Zhou and Ke Xu and Wenhao Huang and Jie Fu", "abstract": "  Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.\n", "link": "http://arxiv.org/abs/2410.07035v1", "date": "2024-10-09", "relevancy": 2.471, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PositionID%3A%20LLMs%20can%20Control%20Lengths%2C%20Copy%20and%20Paste%20with%20Explicit%0A%20%20Positional%20Awareness&body=Title%3A%20PositionID%3A%20LLMs%20can%20Control%20Lengths%2C%20Copy%20and%20Paste%20with%20Explicit%0A%20%20Positional%20Awareness%0AAuthor%3A%20Zekun%20Wang%20and%20Feiyu%20Duan%20and%20Yibo%20Zhang%20and%20Wangchunshu%20Zhou%20and%20Ke%20Xu%20and%20Wenhao%20Huang%20and%20Jie%20Fu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20impressive%20capabilities%20across%0Avarious%20domains%2C%20including%20role-playing%2C%20creative%20writing%2C%20mathematical%0Areasoning%2C%20and%20coding.%20Despite%20these%20advancements%2C%20LLMs%20still%20encounter%0Achallenges%20with%20length%20control%2C%20frequently%20failing%20to%20adhere%20to%20specific%20length%0Aconstraints%20due%20to%20their%20token-level%20operations%20and%20insufficient%20training%20on%0Adata%20with%20strict%20length%20limitations.%20We%20identify%20this%20issue%20as%20stemming%20from%20a%0Alack%20of%20positional%20awareness%20and%20propose%20novel%20approaches--PositionID%20Prompting%0Aand%20PositionID%20Fine-Tuning--to%20address%20it.%20These%20methods%20enhance%20the%20model%27s%0Aability%20to%20continuously%20monitor%20and%20manage%20text%20length%20during%20generation.%0AAdditionally%2C%20we%20introduce%20PositionID%20CP%20Prompting%20to%20enable%20LLMs%20to%20perform%0Acopy%20and%20paste%20operations%20accurately.%20Furthermore%2C%20we%20develop%20two%20benchmarks%0Afor%20evaluating%20length%20control%20and%20copy-paste%20abilities.%20Our%20experiments%0Ademonstrate%20that%20our%20methods%20significantly%20improve%20the%20model%27s%20adherence%20to%0Alength%20constraints%20and%20copy-paste%20accuracy%20without%20compromising%20response%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositionID%253A%2520LLMs%2520can%2520Control%2520Lengths%252C%2520Copy%2520and%2520Paste%2520with%2520Explicit%250A%2520%2520Positional%2520Awareness%26entry.906535625%3DZekun%2520Wang%2520and%2520Feiyu%2520Duan%2520and%2520Yibo%2520Zhang%2520and%2520Wangchunshu%2520Zhou%2520and%2520Ke%2520Xu%2520and%2520Wenhao%2520Huang%2520and%2520Jie%2520Fu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520impressive%2520capabilities%2520across%250Avarious%2520domains%252C%2520including%2520role-playing%252C%2520creative%2520writing%252C%2520mathematical%250Areasoning%252C%2520and%2520coding.%2520Despite%2520these%2520advancements%252C%2520LLMs%2520still%2520encounter%250Achallenges%2520with%2520length%2520control%252C%2520frequently%2520failing%2520to%2520adhere%2520to%2520specific%2520length%250Aconstraints%2520due%2520to%2520their%2520token-level%2520operations%2520and%2520insufficient%2520training%2520on%250Adata%2520with%2520strict%2520length%2520limitations.%2520We%2520identify%2520this%2520issue%2520as%2520stemming%2520from%2520a%250Alack%2520of%2520positional%2520awareness%2520and%2520propose%2520novel%2520approaches--PositionID%2520Prompting%250Aand%2520PositionID%2520Fine-Tuning--to%2520address%2520it.%2520These%2520methods%2520enhance%2520the%2520model%2527s%250Aability%2520to%2520continuously%2520monitor%2520and%2520manage%2520text%2520length%2520during%2520generation.%250AAdditionally%252C%2520we%2520introduce%2520PositionID%2520CP%2520Prompting%2520to%2520enable%2520LLMs%2520to%2520perform%250Acopy%2520and%2520paste%2520operations%2520accurately.%2520Furthermore%252C%2520we%2520develop%2520two%2520benchmarks%250Afor%2520evaluating%2520length%2520control%2520and%2520copy-paste%2520abilities.%2520Our%2520experiments%250Ademonstrate%2520that%2520our%2520methods%2520significantly%2520improve%2520the%2520model%2527s%2520adherence%2520to%250Alength%2520constraints%2520and%2520copy-paste%2520accuracy%2520without%2520compromising%2520response%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PositionID%3A%20LLMs%20can%20Control%20Lengths%2C%20Copy%20and%20Paste%20with%20Explicit%0A%20%20Positional%20Awareness&entry.906535625=Zekun%20Wang%20and%20Feiyu%20Duan%20and%20Yibo%20Zhang%20and%20Wangchunshu%20Zhou%20and%20Ke%20Xu%20and%20Wenhao%20Huang%20and%20Jie%20Fu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20impressive%20capabilities%20across%0Avarious%20domains%2C%20including%20role-playing%2C%20creative%20writing%2C%20mathematical%0Areasoning%2C%20and%20coding.%20Despite%20these%20advancements%2C%20LLMs%20still%20encounter%0Achallenges%20with%20length%20control%2C%20frequently%20failing%20to%20adhere%20to%20specific%20length%0Aconstraints%20due%20to%20their%20token-level%20operations%20and%20insufficient%20training%20on%0Adata%20with%20strict%20length%20limitations.%20We%20identify%20this%20issue%20as%20stemming%20from%20a%0Alack%20of%20positional%20awareness%20and%20propose%20novel%20approaches--PositionID%20Prompting%0Aand%20PositionID%20Fine-Tuning--to%20address%20it.%20These%20methods%20enhance%20the%20model%27s%0Aability%20to%20continuously%20monitor%20and%20manage%20text%20length%20during%20generation.%0AAdditionally%2C%20we%20introduce%20PositionID%20CP%20Prompting%20to%20enable%20LLMs%20to%20perform%0Acopy%20and%20paste%20operations%20accurately.%20Furthermore%2C%20we%20develop%20two%20benchmarks%0Afor%20evaluating%20length%20control%20and%20copy-paste%20abilities.%20Our%20experiments%0Ademonstrate%20that%20our%20methods%20significantly%20improve%20the%20model%27s%20adherence%20to%0Alength%20constraints%20and%20copy-paste%20accuracy%20without%20compromising%20response%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07035v1&entry.124074799=Read"},
{"title": "Boosting Few-Shot Detection with Large Language Models and\n  Layout-to-Image Synthesis", "author": "Ahmed Abdullah and Nikolas Ebert and Oliver Wasenm\u00fcller", "abstract": "  Recent advancements in diffusion models have enabled a wide range of works\nexploiting their ability to generate high-volume, high-quality data for use in\nvarious downstream tasks. One subclass of such models, dubbed Layout-to-Image\nSynthesis (LIS), learns to generate images conditioned on a spatial layout\n(bounding boxes, masks, poses, etc.) and has shown a promising ability to\ngenerate realistic images, albeit with limited layout-adherence. Moreover, the\nquestion of how to effectively transfer those models for scalable augmentation\nof few-shot detection data remains unanswered. Thus, we propose a collaborative\nframework employing a Large Language Model (LLM) and an LIS model for enhancing\nfew-shot detection beyond state-of-the-art generative augmentation approaches.\nWe leverage LLM's reasoning ability to extrapolate the spatial prior of the\nannotation space by generating new bounding boxes given only a few example\nannotations. Additionally, we introduce our novel layout-aware CLIP score for\nsample ranking, enabling tight coupling between generated layouts and images.\nSignificant improvements on COCO few-shot benchmarks are observed. With our\napproach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on\nthe COCO 5-,10-, and 30-shot settings, respectively.\n", "link": "http://arxiv.org/abs/2410.06841v1", "date": "2024-10-09", "relevancy": 2.4589, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6626}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5874}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Few-Shot%20Detection%20with%20Large%20Language%20Models%20and%0A%20%20Layout-to-Image%20Synthesis&body=Title%3A%20Boosting%20Few-Shot%20Detection%20with%20Large%20Language%20Models%20and%0A%20%20Layout-to-Image%20Synthesis%0AAuthor%3A%20Ahmed%20Abdullah%20and%20Nikolas%20Ebert%20and%20Oliver%20Wasenm%C3%BCller%0AAbstract%3A%20%20%20Recent%20advancements%20in%20diffusion%20models%20have%20enabled%20a%20wide%20range%20of%20works%0Aexploiting%20their%20ability%20to%20generate%20high-volume%2C%20high-quality%20data%20for%20use%20in%0Avarious%20downstream%20tasks.%20One%20subclass%20of%20such%20models%2C%20dubbed%20Layout-to-Image%0ASynthesis%20%28LIS%29%2C%20learns%20to%20generate%20images%20conditioned%20on%20a%20spatial%20layout%0A%28bounding%20boxes%2C%20masks%2C%20poses%2C%20etc.%29%20and%20has%20shown%20a%20promising%20ability%20to%0Agenerate%20realistic%20images%2C%20albeit%20with%20limited%20layout-adherence.%20Moreover%2C%20the%0Aquestion%20of%20how%20to%20effectively%20transfer%20those%20models%20for%20scalable%20augmentation%0Aof%20few-shot%20detection%20data%20remains%20unanswered.%20Thus%2C%20we%20propose%20a%20collaborative%0Aframework%20employing%20a%20Large%20Language%20Model%20%28LLM%29%20and%20an%20LIS%20model%20for%20enhancing%0Afew-shot%20detection%20beyond%20state-of-the-art%20generative%20augmentation%20approaches.%0AWe%20leverage%20LLM%27s%20reasoning%20ability%20to%20extrapolate%20the%20spatial%20prior%20of%20the%0Aannotation%20space%20by%20generating%20new%20bounding%20boxes%20given%20only%20a%20few%20example%0Aannotations.%20Additionally%2C%20we%20introduce%20our%20novel%20layout-aware%20CLIP%20score%20for%0Asample%20ranking%2C%20enabling%20tight%20coupling%20between%20generated%20layouts%20and%20images.%0ASignificant%20improvements%20on%20COCO%20few-shot%20benchmarks%20are%20observed.%20With%20our%0Aapproach%2C%20a%20YOLOX-S%20baseline%20is%20boosted%20by%20more%20than%20140%25%2C%2050%25%2C%2035%25%20in%20mAP%20on%0Athe%20COCO%205-%2C10-%2C%20and%2030-shot%20settings%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Few-Shot%2520Detection%2520with%2520Large%2520Language%2520Models%2520and%250A%2520%2520Layout-to-Image%2520Synthesis%26entry.906535625%3DAhmed%2520Abdullah%2520and%2520Nikolas%2520Ebert%2520and%2520Oliver%2520Wasenm%25C3%25BCller%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520diffusion%2520models%2520have%2520enabled%2520a%2520wide%2520range%2520of%2520works%250Aexploiting%2520their%2520ability%2520to%2520generate%2520high-volume%252C%2520high-quality%2520data%2520for%2520use%2520in%250Avarious%2520downstream%2520tasks.%2520One%2520subclass%2520of%2520such%2520models%252C%2520dubbed%2520Layout-to-Image%250ASynthesis%2520%2528LIS%2529%252C%2520learns%2520to%2520generate%2520images%2520conditioned%2520on%2520a%2520spatial%2520layout%250A%2528bounding%2520boxes%252C%2520masks%252C%2520poses%252C%2520etc.%2529%2520and%2520has%2520shown%2520a%2520promising%2520ability%2520to%250Agenerate%2520realistic%2520images%252C%2520albeit%2520with%2520limited%2520layout-adherence.%2520Moreover%252C%2520the%250Aquestion%2520of%2520how%2520to%2520effectively%2520transfer%2520those%2520models%2520for%2520scalable%2520augmentation%250Aof%2520few-shot%2520detection%2520data%2520remains%2520unanswered.%2520Thus%252C%2520we%2520propose%2520a%2520collaborative%250Aframework%2520employing%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520and%2520an%2520LIS%2520model%2520for%2520enhancing%250Afew-shot%2520detection%2520beyond%2520state-of-the-art%2520generative%2520augmentation%2520approaches.%250AWe%2520leverage%2520LLM%2527s%2520reasoning%2520ability%2520to%2520extrapolate%2520the%2520spatial%2520prior%2520of%2520the%250Aannotation%2520space%2520by%2520generating%2520new%2520bounding%2520boxes%2520given%2520only%2520a%2520few%2520example%250Aannotations.%2520Additionally%252C%2520we%2520introduce%2520our%2520novel%2520layout-aware%2520CLIP%2520score%2520for%250Asample%2520ranking%252C%2520enabling%2520tight%2520coupling%2520between%2520generated%2520layouts%2520and%2520images.%250ASignificant%2520improvements%2520on%2520COCO%2520few-shot%2520benchmarks%2520are%2520observed.%2520With%2520our%250Aapproach%252C%2520a%2520YOLOX-S%2520baseline%2520is%2520boosted%2520by%2520more%2520than%2520140%2525%252C%252050%2525%252C%252035%2525%2520in%2520mAP%2520on%250Athe%2520COCO%25205-%252C10-%252C%2520and%252030-shot%2520settings%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Few-Shot%20Detection%20with%20Large%20Language%20Models%20and%0A%20%20Layout-to-Image%20Synthesis&entry.906535625=Ahmed%20Abdullah%20and%20Nikolas%20Ebert%20and%20Oliver%20Wasenm%C3%BCller&entry.1292438233=%20%20Recent%20advancements%20in%20diffusion%20models%20have%20enabled%20a%20wide%20range%20of%20works%0Aexploiting%20their%20ability%20to%20generate%20high-volume%2C%20high-quality%20data%20for%20use%20in%0Avarious%20downstream%20tasks.%20One%20subclass%20of%20such%20models%2C%20dubbed%20Layout-to-Image%0ASynthesis%20%28LIS%29%2C%20learns%20to%20generate%20images%20conditioned%20on%20a%20spatial%20layout%0A%28bounding%20boxes%2C%20masks%2C%20poses%2C%20etc.%29%20and%20has%20shown%20a%20promising%20ability%20to%0Agenerate%20realistic%20images%2C%20albeit%20with%20limited%20layout-adherence.%20Moreover%2C%20the%0Aquestion%20of%20how%20to%20effectively%20transfer%20those%20models%20for%20scalable%20augmentation%0Aof%20few-shot%20detection%20data%20remains%20unanswered.%20Thus%2C%20we%20propose%20a%20collaborative%0Aframework%20employing%20a%20Large%20Language%20Model%20%28LLM%29%20and%20an%20LIS%20model%20for%20enhancing%0Afew-shot%20detection%20beyond%20state-of-the-art%20generative%20augmentation%20approaches.%0AWe%20leverage%20LLM%27s%20reasoning%20ability%20to%20extrapolate%20the%20spatial%20prior%20of%20the%0Aannotation%20space%20by%20generating%20new%20bounding%20boxes%20given%20only%20a%20few%20example%0Aannotations.%20Additionally%2C%20we%20introduce%20our%20novel%20layout-aware%20CLIP%20score%20for%0Asample%20ranking%2C%20enabling%20tight%20coupling%20between%20generated%20layouts%20and%20images.%0ASignificant%20improvements%20on%20COCO%20few-shot%20benchmarks%20are%20observed.%20With%20our%0Aapproach%2C%20a%20YOLOX-S%20baseline%20is%20boosted%20by%20more%20than%20140%25%2C%2050%25%2C%2035%25%20in%20mAP%20on%0Athe%20COCO%205-%2C10-%2C%20and%2030-shot%20settings%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06841v1&entry.124074799=Read"},
{"title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized\n  Attention", "author": "Siyuan Huang and Yunchong Song and Jiayue Zhou and Zhouhan Lin", "abstract": "  In the realm of graph learning, there is a category of methods that\nconceptualize graphs as hierarchical structures, utilizing node clustering to\ncapture broader structural information. While generally effective, these\nmethods often rely on a fixed graph coarsening routine, leading to overly\nhomogeneous cluster representations and loss of node-level information. In this\npaper, we envision the graph as a network of interconnected node sets without\ncompressing each cluster into a single embedding. To enable effective\ninformation transfer among these node sets, we propose the Node-to-Cluster\nAttention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple\nKernel Learning into the kernelized attention framework, effectively capturing\ninformation at both node and cluster levels. We then devise an efficient form\nfor N2C-Attn using the cluster-wise message-passing framework, achieving linear\ntime complexity. We further analyze how N2C-Attn combines bi-level feature maps\nof queries and keys, demonstrating its capability to merge dual-granularity\ninformation. The resulting architecture, Cluster-wise Graph Transformer\n(Cluster-GT), which uses node clusters as tokens and employs our proposed\nN2C-Attn module, shows superior performance on various graph-level tasks. Code\nis available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.\n", "link": "http://arxiv.org/abs/2410.06746v1", "date": "2024-10-09", "relevancy": 2.4368, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention&body=Title%3A%20Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention%0AAuthor%3A%20Siyuan%20Huang%20and%20Yunchong%20Song%20and%20Jiayue%20Zhou%20and%20Zhouhan%20Lin%0AAbstract%3A%20%20%20In%20the%20realm%20of%20graph%20learning%2C%20there%20is%20a%20category%20of%20methods%20that%0Aconceptualize%20graphs%20as%20hierarchical%20structures%2C%20utilizing%20node%20clustering%20to%0Acapture%20broader%20structural%20information.%20While%20generally%20effective%2C%20these%0Amethods%20often%20rely%20on%20a%20fixed%20graph%20coarsening%20routine%2C%20leading%20to%20overly%0Ahomogeneous%20cluster%20representations%20and%20loss%20of%20node-level%20information.%20In%20this%0Apaper%2C%20we%20envision%20the%20graph%20as%20a%20network%20of%20interconnected%20node%20sets%20without%0Acompressing%20each%20cluster%20into%20a%20single%20embedding.%20To%20enable%20effective%0Ainformation%20transfer%20among%20these%20node%20sets%2C%20we%20propose%20the%20Node-to-Cluster%0AAttention%20%28N2C-Attn%29%20mechanism.%20N2C-Attn%20incorporates%20techniques%20from%20Multiple%0AKernel%20Learning%20into%20the%20kernelized%20attention%20framework%2C%20effectively%20capturing%0Ainformation%20at%20both%20node%20and%20cluster%20levels.%20We%20then%20devise%20an%20efficient%20form%0Afor%20N2C-Attn%20using%20the%20cluster-wise%20message-passing%20framework%2C%20achieving%20linear%0Atime%20complexity.%20We%20further%20analyze%20how%20N2C-Attn%20combines%20bi-level%20feature%20maps%0Aof%20queries%20and%20keys%2C%20demonstrating%20its%20capability%20to%20merge%20dual-granularity%0Ainformation.%20The%20resulting%20architecture%2C%20Cluster-wise%20Graph%20Transformer%0A%28Cluster-GT%29%2C%20which%20uses%20node%20clusters%20as%20tokens%20and%20employs%20our%20proposed%0AN2C-Attn%20module%2C%20shows%20superior%20performance%20on%20various%20graph-level%20tasks.%20Code%0Ais%20available%20at%20https%3A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster-wise%2520Graph%2520Transformer%2520with%2520Dual-granularity%2520Kernelized%250A%2520%2520Attention%26entry.906535625%3DSiyuan%2520Huang%2520and%2520Yunchong%2520Song%2520and%2520Jiayue%2520Zhou%2520and%2520Zhouhan%2520Lin%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520graph%2520learning%252C%2520there%2520is%2520a%2520category%2520of%2520methods%2520that%250Aconceptualize%2520graphs%2520as%2520hierarchical%2520structures%252C%2520utilizing%2520node%2520clustering%2520to%250Acapture%2520broader%2520structural%2520information.%2520While%2520generally%2520effective%252C%2520these%250Amethods%2520often%2520rely%2520on%2520a%2520fixed%2520graph%2520coarsening%2520routine%252C%2520leading%2520to%2520overly%250Ahomogeneous%2520cluster%2520representations%2520and%2520loss%2520of%2520node-level%2520information.%2520In%2520this%250Apaper%252C%2520we%2520envision%2520the%2520graph%2520as%2520a%2520network%2520of%2520interconnected%2520node%2520sets%2520without%250Acompressing%2520each%2520cluster%2520into%2520a%2520single%2520embedding.%2520To%2520enable%2520effective%250Ainformation%2520transfer%2520among%2520these%2520node%2520sets%252C%2520we%2520propose%2520the%2520Node-to-Cluster%250AAttention%2520%2528N2C-Attn%2529%2520mechanism.%2520N2C-Attn%2520incorporates%2520techniques%2520from%2520Multiple%250AKernel%2520Learning%2520into%2520the%2520kernelized%2520attention%2520framework%252C%2520effectively%2520capturing%250Ainformation%2520at%2520both%2520node%2520and%2520cluster%2520levels.%2520We%2520then%2520devise%2520an%2520efficient%2520form%250Afor%2520N2C-Attn%2520using%2520the%2520cluster-wise%2520message-passing%2520framework%252C%2520achieving%2520linear%250Atime%2520complexity.%2520We%2520further%2520analyze%2520how%2520N2C-Attn%2520combines%2520bi-level%2520feature%2520maps%250Aof%2520queries%2520and%2520keys%252C%2520demonstrating%2520its%2520capability%2520to%2520merge%2520dual-granularity%250Ainformation.%2520The%2520resulting%2520architecture%252C%2520Cluster-wise%2520Graph%2520Transformer%250A%2528Cluster-GT%2529%252C%2520which%2520uses%2520node%2520clusters%2520as%2520tokens%2520and%2520employs%2520our%2520proposed%250AN2C-Attn%2520module%252C%2520shows%2520superior%2520performance%2520on%2520various%2520graph-level%2520tasks.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster-wise%20Graph%20Transformer%20with%20Dual-granularity%20Kernelized%0A%20%20Attention&entry.906535625=Siyuan%20Huang%20and%20Yunchong%20Song%20and%20Jiayue%20Zhou%20and%20Zhouhan%20Lin&entry.1292438233=%20%20In%20the%20realm%20of%20graph%20learning%2C%20there%20is%20a%20category%20of%20methods%20that%0Aconceptualize%20graphs%20as%20hierarchical%20structures%2C%20utilizing%20node%20clustering%20to%0Acapture%20broader%20structural%20information.%20While%20generally%20effective%2C%20these%0Amethods%20often%20rely%20on%20a%20fixed%20graph%20coarsening%20routine%2C%20leading%20to%20overly%0Ahomogeneous%20cluster%20representations%20and%20loss%20of%20node-level%20information.%20In%20this%0Apaper%2C%20we%20envision%20the%20graph%20as%20a%20network%20of%20interconnected%20node%20sets%20without%0Acompressing%20each%20cluster%20into%20a%20single%20embedding.%20To%20enable%20effective%0Ainformation%20transfer%20among%20these%20node%20sets%2C%20we%20propose%20the%20Node-to-Cluster%0AAttention%20%28N2C-Attn%29%20mechanism.%20N2C-Attn%20incorporates%20techniques%20from%20Multiple%0AKernel%20Learning%20into%20the%20kernelized%20attention%20framework%2C%20effectively%20capturing%0Ainformation%20at%20both%20node%20and%20cluster%20levels.%20We%20then%20devise%20an%20efficient%20form%0Afor%20N2C-Attn%20using%20the%20cluster-wise%20message-passing%20framework%2C%20achieving%20linear%0Atime%20complexity.%20We%20further%20analyze%20how%20N2C-Attn%20combines%20bi-level%20feature%20maps%0Aof%20queries%20and%20keys%2C%20demonstrating%20its%20capability%20to%20merge%20dual-granularity%0Ainformation.%20The%20resulting%20architecture%2C%20Cluster-wise%20Graph%20Transformer%0A%28Cluster-GT%29%2C%20which%20uses%20node%20clusters%20as%20tokens%20and%20employs%20our%20proposed%0AN2C-Attn%20module%2C%20shows%20superior%20performance%20on%20various%20graph-level%20tasks.%20Code%0Ais%20available%20at%20https%3A//github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06746v1&entry.124074799=Read"},
{"title": "Structure-Centric Robust Monocular Depth Estimation via Knowledge\n  Distillation", "author": "Runze Chen and Haiyong Luo and Fang Zhao and Jingze Yu and Yupeng Jia and Juan Wang and Xuepeng Ma", "abstract": "  Monocular depth estimation, enabled by self-supervised learning, is a key\ntechnique for 3D perception in computer vision. However, it faces significant\nchallenges in real-world scenarios, which encompass adverse weather variations,\nmotion blur, as well as scenes with poor lighting conditions at night. Our\nresearch reveals that we can divide monocular depth estimation into three\nsub-problems: depth structure consistency, local texture disambiguation, and\nsemantic-structural correlation. Our approach tackles the non-robustness of\nexisting self-supervised monocular depth estimation models to interference\ntextures by adopting a structure-centered perspective and utilizing the scene\nstructure characteristics demonstrated by semantics and illumination. We devise\na novel approach to reduce over-reliance on local textures, enhancing\nrobustness against missing or interfering patterns. Additionally, we\nincorporate a semantic expert model as the teacher and construct inter-model\nfeature dependencies via learnable isomorphic graphs to enable aggregation of\nsemantic structural knowledge. Our approach achieves state-of-the-art\nout-of-distribution monocular depth estimation performance across a range of\npublic adverse scenario datasets. It demonstrates notable scalability and\ncompatibility, without necessitating extensive model engineering. This\nshowcases the potential for customizing models for diverse industrial\napplications.\n", "link": "http://arxiv.org/abs/2410.06982v1", "date": "2024-10-09", "relevancy": 2.4252, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Centric%20Robust%20Monocular%20Depth%20Estimation%20via%20Knowledge%0A%20%20Distillation&body=Title%3A%20Structure-Centric%20Robust%20Monocular%20Depth%20Estimation%20via%20Knowledge%0A%20%20Distillation%0AAuthor%3A%20Runze%20Chen%20and%20Haiyong%20Luo%20and%20Fang%20Zhao%20and%20Jingze%20Yu%20and%20Yupeng%20Jia%20and%20Juan%20Wang%20and%20Xuepeng%20Ma%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%2C%20enabled%20by%20self-supervised%20learning%2C%20is%20a%20key%0Atechnique%20for%203D%20perception%20in%20computer%20vision.%20However%2C%20it%20faces%20significant%0Achallenges%20in%20real-world%20scenarios%2C%20which%20encompass%20adverse%20weather%20variations%2C%0Amotion%20blur%2C%20as%20well%20as%20scenes%20with%20poor%20lighting%20conditions%20at%20night.%20Our%0Aresearch%20reveals%20that%20we%20can%20divide%20monocular%20depth%20estimation%20into%20three%0Asub-problems%3A%20depth%20structure%20consistency%2C%20local%20texture%20disambiguation%2C%20and%0Asemantic-structural%20correlation.%20Our%20approach%20tackles%20the%20non-robustness%20of%0Aexisting%20self-supervised%20monocular%20depth%20estimation%20models%20to%20interference%0Atextures%20by%20adopting%20a%20structure-centered%20perspective%20and%20utilizing%20the%20scene%0Astructure%20characteristics%20demonstrated%20by%20semantics%20and%20illumination.%20We%20devise%0Aa%20novel%20approach%20to%20reduce%20over-reliance%20on%20local%20textures%2C%20enhancing%0Arobustness%20against%20missing%20or%20interfering%20patterns.%20Additionally%2C%20we%0Aincorporate%20a%20semantic%20expert%20model%20as%20the%20teacher%20and%20construct%20inter-model%0Afeature%20dependencies%20via%20learnable%20isomorphic%20graphs%20to%20enable%20aggregation%20of%0Asemantic%20structural%20knowledge.%20Our%20approach%20achieves%20state-of-the-art%0Aout-of-distribution%20monocular%20depth%20estimation%20performance%20across%20a%20range%20of%0Apublic%20adverse%20scenario%20datasets.%20It%20demonstrates%20notable%20scalability%20and%0Acompatibility%2C%20without%20necessitating%20extensive%20model%20engineering.%20This%0Ashowcases%20the%20potential%20for%20customizing%20models%20for%20diverse%20industrial%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Centric%2520Robust%2520Monocular%2520Depth%2520Estimation%2520via%2520Knowledge%250A%2520%2520Distillation%26entry.906535625%3DRunze%2520Chen%2520and%2520Haiyong%2520Luo%2520and%2520Fang%2520Zhao%2520and%2520Jingze%2520Yu%2520and%2520Yupeng%2520Jia%2520and%2520Juan%2520Wang%2520and%2520Xuepeng%2520Ma%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%252C%2520enabled%2520by%2520self-supervised%2520learning%252C%2520is%2520a%2520key%250Atechnique%2520for%25203D%2520perception%2520in%2520computer%2520vision.%2520However%252C%2520it%2520faces%2520significant%250Achallenges%2520in%2520real-world%2520scenarios%252C%2520which%2520encompass%2520adverse%2520weather%2520variations%252C%250Amotion%2520blur%252C%2520as%2520well%2520as%2520scenes%2520with%2520poor%2520lighting%2520conditions%2520at%2520night.%2520Our%250Aresearch%2520reveals%2520that%2520we%2520can%2520divide%2520monocular%2520depth%2520estimation%2520into%2520three%250Asub-problems%253A%2520depth%2520structure%2520consistency%252C%2520local%2520texture%2520disambiguation%252C%2520and%250Asemantic-structural%2520correlation.%2520Our%2520approach%2520tackles%2520the%2520non-robustness%2520of%250Aexisting%2520self-supervised%2520monocular%2520depth%2520estimation%2520models%2520to%2520interference%250Atextures%2520by%2520adopting%2520a%2520structure-centered%2520perspective%2520and%2520utilizing%2520the%2520scene%250Astructure%2520characteristics%2520demonstrated%2520by%2520semantics%2520and%2520illumination.%2520We%2520devise%250Aa%2520novel%2520approach%2520to%2520reduce%2520over-reliance%2520on%2520local%2520textures%252C%2520enhancing%250Arobustness%2520against%2520missing%2520or%2520interfering%2520patterns.%2520Additionally%252C%2520we%250Aincorporate%2520a%2520semantic%2520expert%2520model%2520as%2520the%2520teacher%2520and%2520construct%2520inter-model%250Afeature%2520dependencies%2520via%2520learnable%2520isomorphic%2520graphs%2520to%2520enable%2520aggregation%2520of%250Asemantic%2520structural%2520knowledge.%2520Our%2520approach%2520achieves%2520state-of-the-art%250Aout-of-distribution%2520monocular%2520depth%2520estimation%2520performance%2520across%2520a%2520range%2520of%250Apublic%2520adverse%2520scenario%2520datasets.%2520It%2520demonstrates%2520notable%2520scalability%2520and%250Acompatibility%252C%2520without%2520necessitating%2520extensive%2520model%2520engineering.%2520This%250Ashowcases%2520the%2520potential%2520for%2520customizing%2520models%2520for%2520diverse%2520industrial%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Centric%20Robust%20Monocular%20Depth%20Estimation%20via%20Knowledge%0A%20%20Distillation&entry.906535625=Runze%20Chen%20and%20Haiyong%20Luo%20and%20Fang%20Zhao%20and%20Jingze%20Yu%20and%20Yupeng%20Jia%20and%20Juan%20Wang%20and%20Xuepeng%20Ma&entry.1292438233=%20%20Monocular%20depth%20estimation%2C%20enabled%20by%20self-supervised%20learning%2C%20is%20a%20key%0Atechnique%20for%203D%20perception%20in%20computer%20vision.%20However%2C%20it%20faces%20significant%0Achallenges%20in%20real-world%20scenarios%2C%20which%20encompass%20adverse%20weather%20variations%2C%0Amotion%20blur%2C%20as%20well%20as%20scenes%20with%20poor%20lighting%20conditions%20at%20night.%20Our%0Aresearch%20reveals%20that%20we%20can%20divide%20monocular%20depth%20estimation%20into%20three%0Asub-problems%3A%20depth%20structure%20consistency%2C%20local%20texture%20disambiguation%2C%20and%0Asemantic-structural%20correlation.%20Our%20approach%20tackles%20the%20non-robustness%20of%0Aexisting%20self-supervised%20monocular%20depth%20estimation%20models%20to%20interference%0Atextures%20by%20adopting%20a%20structure-centered%20perspective%20and%20utilizing%20the%20scene%0Astructure%20characteristics%20demonstrated%20by%20semantics%20and%20illumination.%20We%20devise%0Aa%20novel%20approach%20to%20reduce%20over-reliance%20on%20local%20textures%2C%20enhancing%0Arobustness%20against%20missing%20or%20interfering%20patterns.%20Additionally%2C%20we%0Aincorporate%20a%20semantic%20expert%20model%20as%20the%20teacher%20and%20construct%20inter-model%0Afeature%20dependencies%20via%20learnable%20isomorphic%20graphs%20to%20enable%20aggregation%20of%0Asemantic%20structural%20knowledge.%20Our%20approach%20achieves%20state-of-the-art%0Aout-of-distribution%20monocular%20depth%20estimation%20performance%20across%20a%20range%20of%0Apublic%20adverse%20scenario%20datasets.%20It%20demonstrates%20notable%20scalability%20and%0Acompatibility%2C%20without%20necessitating%20extensive%20model%20engineering.%20This%0Ashowcases%20the%20potential%20for%20customizing%20models%20for%20diverse%20industrial%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06982v1&entry.124074799=Read"},
{"title": "Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention", "author": "Tongzhou Liao and Barnab\u00e1s P\u00f3czos", "abstract": "  Graph Neural Networks (GNNs) have become important tools for machine learning\non graph-structured data. In this paper, we explore the synergistic combination\nof graph encoding, graph rewiring, and graph attention, by introducing Graph\nAttention with Stochastic Structures (GRASS), a novel GNN architecture. GRASS\nutilizes relative random walk probabilities (RRWP) encoding and a novel\ndecomposed variant (D-RRWP) to efficiently capture structural information. It\nrewires the input graph by superimposing a random regular graph to enhance\nlong-range information propagation. It also employs a novel additive attention\nmechanism tailored for graph-structured data. Our empirical evaluations\ndemonstrate that GRASS achieves state-of-the-art performance on multiple\nbenchmark datasets, including a 20.3% reduction in mean absolute error on the\nZINC dataset.\n", "link": "http://arxiv.org/abs/2407.05649v3", "date": "2024-10-09", "relevancy": 2.4224, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4826}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Greener%20GRASS%3A%20Enhancing%20GNNs%20with%20Encoding%2C%20Rewiring%2C%20and%20Attention&body=Title%3A%20Greener%20GRASS%3A%20Enhancing%20GNNs%20with%20Encoding%2C%20Rewiring%2C%20and%20Attention%0AAuthor%3A%20Tongzhou%20Liao%20and%20Barnab%C3%A1s%20P%C3%B3czos%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20important%20tools%20for%20machine%20learning%0Aon%20graph-structured%20data.%20In%20this%20paper%2C%20we%20explore%20the%20synergistic%20combination%0Aof%20graph%20encoding%2C%20graph%20rewiring%2C%20and%20graph%20attention%2C%20by%20introducing%20Graph%0AAttention%20with%20Stochastic%20Structures%20%28GRASS%29%2C%20a%20novel%20GNN%20architecture.%20GRASS%0Autilizes%20relative%20random%20walk%20probabilities%20%28RRWP%29%20encoding%20and%20a%20novel%0Adecomposed%20variant%20%28D-RRWP%29%20to%20efficiently%20capture%20structural%20information.%20It%0Arewires%20the%20input%20graph%20by%20superimposing%20a%20random%20regular%20graph%20to%20enhance%0Along-range%20information%20propagation.%20It%20also%20employs%20a%20novel%20additive%20attention%0Amechanism%20tailored%20for%20graph-structured%20data.%20Our%20empirical%20evaluations%0Ademonstrate%20that%20GRASS%20achieves%20state-of-the-art%20performance%20on%20multiple%0Abenchmark%20datasets%2C%20including%20a%2020.3%25%20reduction%20in%20mean%20absolute%20error%20on%20the%0AZINC%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05649v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreener%2520GRASS%253A%2520Enhancing%2520GNNs%2520with%2520Encoding%252C%2520Rewiring%252C%2520and%2520Attention%26entry.906535625%3DTongzhou%2520Liao%2520and%2520Barnab%25C3%25A1s%2520P%25C3%25B3czos%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520important%2520tools%2520for%2520machine%2520learning%250Aon%2520graph-structured%2520data.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520synergistic%2520combination%250Aof%2520graph%2520encoding%252C%2520graph%2520rewiring%252C%2520and%2520graph%2520attention%252C%2520by%2520introducing%2520Graph%250AAttention%2520with%2520Stochastic%2520Structures%2520%2528GRASS%2529%252C%2520a%2520novel%2520GNN%2520architecture.%2520GRASS%250Autilizes%2520relative%2520random%2520walk%2520probabilities%2520%2528RRWP%2529%2520encoding%2520and%2520a%2520novel%250Adecomposed%2520variant%2520%2528D-RRWP%2529%2520to%2520efficiently%2520capture%2520structural%2520information.%2520It%250Arewires%2520the%2520input%2520graph%2520by%2520superimposing%2520a%2520random%2520regular%2520graph%2520to%2520enhance%250Along-range%2520information%2520propagation.%2520It%2520also%2520employs%2520a%2520novel%2520additive%2520attention%250Amechanism%2520tailored%2520for%2520graph-structured%2520data.%2520Our%2520empirical%2520evaluations%250Ademonstrate%2520that%2520GRASS%2520achieves%2520state-of-the-art%2520performance%2520on%2520multiple%250Abenchmark%2520datasets%252C%2520including%2520a%252020.3%2525%2520reduction%2520in%2520mean%2520absolute%2520error%2520on%2520the%250AZINC%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05649v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Greener%20GRASS%3A%20Enhancing%20GNNs%20with%20Encoding%2C%20Rewiring%2C%20and%20Attention&entry.906535625=Tongzhou%20Liao%20and%20Barnab%C3%A1s%20P%C3%B3czos&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20important%20tools%20for%20machine%20learning%0Aon%20graph-structured%20data.%20In%20this%20paper%2C%20we%20explore%20the%20synergistic%20combination%0Aof%20graph%20encoding%2C%20graph%20rewiring%2C%20and%20graph%20attention%2C%20by%20introducing%20Graph%0AAttention%20with%20Stochastic%20Structures%20%28GRASS%29%2C%20a%20novel%20GNN%20architecture.%20GRASS%0Autilizes%20relative%20random%20walk%20probabilities%20%28RRWP%29%20encoding%20and%20a%20novel%0Adecomposed%20variant%20%28D-RRWP%29%20to%20efficiently%20capture%20structural%20information.%20It%0Arewires%20the%20input%20graph%20by%20superimposing%20a%20random%20regular%20graph%20to%20enhance%0Along-range%20information%20propagation.%20It%20also%20employs%20a%20novel%20additive%20attention%0Amechanism%20tailored%20for%20graph-structured%20data.%20Our%20empirical%20evaluations%0Ademonstrate%20that%20GRASS%20achieves%20state-of-the-art%20performance%20on%20multiple%0Abenchmark%20datasets%2C%20including%20a%2020.3%25%20reduction%20in%20mean%20absolute%20error%20on%20the%0AZINC%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05649v3&entry.124074799=Read"},
{"title": "DLGNet: Hyperedge Classification through Directed Line Graphs for\n  Chemical Reactions", "author": "Stefano Fiorini and Giulia M. Bovolenta and Stefano Coniglio and Michele Ciavotta and Pietro Morerio and Michele Parrinello and Alessio Del Bue", "abstract": "  Graphs and hypergraphs provide powerful abstractions for modeling\ninteractions among a set of entities of interest and have been attracting a\ngrowing interest in the literature thanks to many successful applications in\nseveral fields. In particular, they are rapidly expanding in domains such as\nchemistry and biology, especially in the areas of drug discovery and molecule\ngeneration. One of the areas witnessing the fasted growth is the chemical\nreactions field, where chemical reactions can be naturally encoded as directed\nhyperedges of a hypergraph. In this paper, we address the chemical reaction\nclassification problem by introducing the notation of a Directed Line Graph\n(DGL) associated with a given directed hypergraph. On top of it, we build the\nDirected Line Graph Network (DLGNet), the first spectral-based Graph Neural\nNetwork (GNN) expressly designed to operate on a hypergraph via its DLG\ntransformation. The foundation of DLGNet is a novel Hermitian matrix, the\nDirected Line Graph Laplacian, which compactly encodes the directionality of\nthe interactions taking place within the directed hyperedges of the hypergraph\nthanks to the DLG representation. The Directed Line Graph Laplacian enjoys many\ndesirable properties, including admitting an eigenvalue decomposition and being\npositive semidefinite, which make it well-suited for its adoption within a\nspectral-based GNN. Through extensive experiments on chemical reaction\ndatasets, we show that DGLNet significantly outperforms the existing\napproaches, achieving on a collection of real-world datasets an average\nrelative-percentage-difference improvement of 33.01%, with a maximum\nimprovement of 37.71%.\n", "link": "http://arxiv.org/abs/2410.06969v1", "date": "2024-10-09", "relevancy": 2.411, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5075}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.48}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DLGNet%3A%20Hyperedge%20Classification%20through%20Directed%20Line%20Graphs%20for%0A%20%20Chemical%20Reactions&body=Title%3A%20DLGNet%3A%20Hyperedge%20Classification%20through%20Directed%20Line%20Graphs%20for%0A%20%20Chemical%20Reactions%0AAuthor%3A%20Stefano%20Fiorini%20and%20Giulia%20M.%20Bovolenta%20and%20Stefano%20Coniglio%20and%20Michele%20Ciavotta%20and%20Pietro%20Morerio%20and%20Michele%20Parrinello%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20Graphs%20and%20hypergraphs%20provide%20powerful%20abstractions%20for%20modeling%0Ainteractions%20among%20a%20set%20of%20entities%20of%20interest%20and%20have%20been%20attracting%20a%0Agrowing%20interest%20in%20the%20literature%20thanks%20to%20many%20successful%20applications%20in%0Aseveral%20fields.%20In%20particular%2C%20they%20are%20rapidly%20expanding%20in%20domains%20such%20as%0Achemistry%20and%20biology%2C%20especially%20in%20the%20areas%20of%20drug%20discovery%20and%20molecule%0Ageneration.%20One%20of%20the%20areas%20witnessing%20the%20fasted%20growth%20is%20the%20chemical%0Areactions%20field%2C%20where%20chemical%20reactions%20can%20be%20naturally%20encoded%20as%20directed%0Ahyperedges%20of%20a%20hypergraph.%20In%20this%20paper%2C%20we%20address%20the%20chemical%20reaction%0Aclassification%20problem%20by%20introducing%20the%20notation%20of%20a%20Directed%20Line%20Graph%0A%28DGL%29%20associated%20with%20a%20given%20directed%20hypergraph.%20On%20top%20of%20it%2C%20we%20build%20the%0ADirected%20Line%20Graph%20Network%20%28DLGNet%29%2C%20the%20first%20spectral-based%20Graph%20Neural%0ANetwork%20%28GNN%29%20expressly%20designed%20to%20operate%20on%20a%20hypergraph%20via%20its%20DLG%0Atransformation.%20The%20foundation%20of%20DLGNet%20is%20a%20novel%20Hermitian%20matrix%2C%20the%0ADirected%20Line%20Graph%20Laplacian%2C%20which%20compactly%20encodes%20the%20directionality%20of%0Athe%20interactions%20taking%20place%20within%20the%20directed%20hyperedges%20of%20the%20hypergraph%0Athanks%20to%20the%20DLG%20representation.%20The%20Directed%20Line%20Graph%20Laplacian%20enjoys%20many%0Adesirable%20properties%2C%20including%20admitting%20an%20eigenvalue%20decomposition%20and%20being%0Apositive%20semidefinite%2C%20which%20make%20it%20well-suited%20for%20its%20adoption%20within%20a%0Aspectral-based%20GNN.%20Through%20extensive%20experiments%20on%20chemical%20reaction%0Adatasets%2C%20we%20show%20that%20DGLNet%20significantly%20outperforms%20the%20existing%0Aapproaches%2C%20achieving%20on%20a%20collection%20of%20real-world%20datasets%20an%20average%0Arelative-percentage-difference%20improvement%20of%2033.01%25%2C%20with%20a%20maximum%0Aimprovement%20of%2037.71%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDLGNet%253A%2520Hyperedge%2520Classification%2520through%2520Directed%2520Line%2520Graphs%2520for%250A%2520%2520Chemical%2520Reactions%26entry.906535625%3DStefano%2520Fiorini%2520and%2520Giulia%2520M.%2520Bovolenta%2520and%2520Stefano%2520Coniglio%2520and%2520Michele%2520Ciavotta%2520and%2520Pietro%2520Morerio%2520and%2520Michele%2520Parrinello%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520Graphs%2520and%2520hypergraphs%2520provide%2520powerful%2520abstractions%2520for%2520modeling%250Ainteractions%2520among%2520a%2520set%2520of%2520entities%2520of%2520interest%2520and%2520have%2520been%2520attracting%2520a%250Agrowing%2520interest%2520in%2520the%2520literature%2520thanks%2520to%2520many%2520successful%2520applications%2520in%250Aseveral%2520fields.%2520In%2520particular%252C%2520they%2520are%2520rapidly%2520expanding%2520in%2520domains%2520such%2520as%250Achemistry%2520and%2520biology%252C%2520especially%2520in%2520the%2520areas%2520of%2520drug%2520discovery%2520and%2520molecule%250Ageneration.%2520One%2520of%2520the%2520areas%2520witnessing%2520the%2520fasted%2520growth%2520is%2520the%2520chemical%250Areactions%2520field%252C%2520where%2520chemical%2520reactions%2520can%2520be%2520naturally%2520encoded%2520as%2520directed%250Ahyperedges%2520of%2520a%2520hypergraph.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520chemical%2520reaction%250Aclassification%2520problem%2520by%2520introducing%2520the%2520notation%2520of%2520a%2520Directed%2520Line%2520Graph%250A%2528DGL%2529%2520associated%2520with%2520a%2520given%2520directed%2520hypergraph.%2520On%2520top%2520of%2520it%252C%2520we%2520build%2520the%250ADirected%2520Line%2520Graph%2520Network%2520%2528DLGNet%2529%252C%2520the%2520first%2520spectral-based%2520Graph%2520Neural%250ANetwork%2520%2528GNN%2529%2520expressly%2520designed%2520to%2520operate%2520on%2520a%2520hypergraph%2520via%2520its%2520DLG%250Atransformation.%2520The%2520foundation%2520of%2520DLGNet%2520is%2520a%2520novel%2520Hermitian%2520matrix%252C%2520the%250ADirected%2520Line%2520Graph%2520Laplacian%252C%2520which%2520compactly%2520encodes%2520the%2520directionality%2520of%250Athe%2520interactions%2520taking%2520place%2520within%2520the%2520directed%2520hyperedges%2520of%2520the%2520hypergraph%250Athanks%2520to%2520the%2520DLG%2520representation.%2520The%2520Directed%2520Line%2520Graph%2520Laplacian%2520enjoys%2520many%250Adesirable%2520properties%252C%2520including%2520admitting%2520an%2520eigenvalue%2520decomposition%2520and%2520being%250Apositive%2520semidefinite%252C%2520which%2520make%2520it%2520well-suited%2520for%2520its%2520adoption%2520within%2520a%250Aspectral-based%2520GNN.%2520Through%2520extensive%2520experiments%2520on%2520chemical%2520reaction%250Adatasets%252C%2520we%2520show%2520that%2520DGLNet%2520significantly%2520outperforms%2520the%2520existing%250Aapproaches%252C%2520achieving%2520on%2520a%2520collection%2520of%2520real-world%2520datasets%2520an%2520average%250Arelative-percentage-difference%2520improvement%2520of%252033.01%2525%252C%2520with%2520a%2520maximum%250Aimprovement%2520of%252037.71%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DLGNet%3A%20Hyperedge%20Classification%20through%20Directed%20Line%20Graphs%20for%0A%20%20Chemical%20Reactions&entry.906535625=Stefano%20Fiorini%20and%20Giulia%20M.%20Bovolenta%20and%20Stefano%20Coniglio%20and%20Michele%20Ciavotta%20and%20Pietro%20Morerio%20and%20Michele%20Parrinello%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20Graphs%20and%20hypergraphs%20provide%20powerful%20abstractions%20for%20modeling%0Ainteractions%20among%20a%20set%20of%20entities%20of%20interest%20and%20have%20been%20attracting%20a%0Agrowing%20interest%20in%20the%20literature%20thanks%20to%20many%20successful%20applications%20in%0Aseveral%20fields.%20In%20particular%2C%20they%20are%20rapidly%20expanding%20in%20domains%20such%20as%0Achemistry%20and%20biology%2C%20especially%20in%20the%20areas%20of%20drug%20discovery%20and%20molecule%0Ageneration.%20One%20of%20the%20areas%20witnessing%20the%20fasted%20growth%20is%20the%20chemical%0Areactions%20field%2C%20where%20chemical%20reactions%20can%20be%20naturally%20encoded%20as%20directed%0Ahyperedges%20of%20a%20hypergraph.%20In%20this%20paper%2C%20we%20address%20the%20chemical%20reaction%0Aclassification%20problem%20by%20introducing%20the%20notation%20of%20a%20Directed%20Line%20Graph%0A%28DGL%29%20associated%20with%20a%20given%20directed%20hypergraph.%20On%20top%20of%20it%2C%20we%20build%20the%0ADirected%20Line%20Graph%20Network%20%28DLGNet%29%2C%20the%20first%20spectral-based%20Graph%20Neural%0ANetwork%20%28GNN%29%20expressly%20designed%20to%20operate%20on%20a%20hypergraph%20via%20its%20DLG%0Atransformation.%20The%20foundation%20of%20DLGNet%20is%20a%20novel%20Hermitian%20matrix%2C%20the%0ADirected%20Line%20Graph%20Laplacian%2C%20which%20compactly%20encodes%20the%20directionality%20of%0Athe%20interactions%20taking%20place%20within%20the%20directed%20hyperedges%20of%20the%20hypergraph%0Athanks%20to%20the%20DLG%20representation.%20The%20Directed%20Line%20Graph%20Laplacian%20enjoys%20many%0Adesirable%20properties%2C%20including%20admitting%20an%20eigenvalue%20decomposition%20and%20being%0Apositive%20semidefinite%2C%20which%20make%20it%20well-suited%20for%20its%20adoption%20within%20a%0Aspectral-based%20GNN.%20Through%20extensive%20experiments%20on%20chemical%20reaction%0Adatasets%2C%20we%20show%20that%20DGLNet%20significantly%20outperforms%20the%20existing%0Aapproaches%2C%20achieving%20on%20a%20collection%20of%20real-world%20datasets%20an%20average%0Arelative-percentage-difference%20improvement%20of%2033.01%25%2C%20with%20a%20maximum%0Aimprovement%20of%2037.71%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06969v1&entry.124074799=Read"},
{"title": "Trans4D: Realistic Geometry-Aware Transition for Compositional\n  Text-to-4D Synthesis", "author": "Bohan Zeng and Ling Yang and Siyu Li and Jiaming Liu and Zixiang Zhang and Juanxi Tian and Kaixin Zhu and Yongzhen Guo and Fu-Yun Wang and Minkai Xu and Stefano Ermon and Wentao Zhang", "abstract": "  Recent advances in diffusion models have demonstrated exceptional\ncapabilities in image and video generation, further improving the effectiveness\nof 4D synthesis. Existing 4D generation methods can generate high-quality 4D\nobjects or scenes based on user-friendly conditions, benefiting the gaming and\nvideo industries. However, these methods struggle to synthesize significant\nobject deformation of complex 4D transitions and interactions within scenes. To\naddress this challenge, we propose Trans4D, a novel text-to-4D synthesis\nframework that enables realistic complex scene transitions. Specifically, we\nfirst use multi-modal large language models (MLLMs) to produce a physic-aware\nscene description for 4D scene initialization and effective transition timing\nplanning. Then we propose a geometry-aware 4D transition network to realize a\ncomplex scene-level 4D transition based on the plan, which involves expressive\ngeometrical object deformation. Extensive experiments demonstrate that Trans4D\nconsistently outperforms existing state-of-the-art methods in generating 4D\nscenes with accurate and high-quality transitions, validating its\neffectiveness. Code: https://github.com/YangLing0818/Trans4D\n", "link": "http://arxiv.org/abs/2410.07155v1", "date": "2024-10-09", "relevancy": 2.4019, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6024}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6001}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trans4D%3A%20Realistic%20Geometry-Aware%20Transition%20for%20Compositional%0A%20%20Text-to-4D%20Synthesis&body=Title%3A%20Trans4D%3A%20Realistic%20Geometry-Aware%20Transition%20for%20Compositional%0A%20%20Text-to-4D%20Synthesis%0AAuthor%3A%20Bohan%20Zeng%20and%20Ling%20Yang%20and%20Siyu%20Li%20and%20Jiaming%20Liu%20and%20Zixiang%20Zhang%20and%20Juanxi%20Tian%20and%20Kaixin%20Zhu%20and%20Yongzhen%20Guo%20and%20Fu-Yun%20Wang%20and%20Minkai%20Xu%20and%20Stefano%20Ermon%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20demonstrated%20exceptional%0Acapabilities%20in%20image%20and%20video%20generation%2C%20further%20improving%20the%20effectiveness%0Aof%204D%20synthesis.%20Existing%204D%20generation%20methods%20can%20generate%20high-quality%204D%0Aobjects%20or%20scenes%20based%20on%20user-friendly%20conditions%2C%20benefiting%20the%20gaming%20and%0Avideo%20industries.%20However%2C%20these%20methods%20struggle%20to%20synthesize%20significant%0Aobject%20deformation%20of%20complex%204D%20transitions%20and%20interactions%20within%20scenes.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20Trans4D%2C%20a%20novel%20text-to-4D%20synthesis%0Aframework%20that%20enables%20realistic%20complex%20scene%20transitions.%20Specifically%2C%20we%0Afirst%20use%20multi-modal%20large%20language%20models%20%28MLLMs%29%20to%20produce%20a%20physic-aware%0Ascene%20description%20for%204D%20scene%20initialization%20and%20effective%20transition%20timing%0Aplanning.%20Then%20we%20propose%20a%20geometry-aware%204D%20transition%20network%20to%20realize%20a%0Acomplex%20scene-level%204D%20transition%20based%20on%20the%20plan%2C%20which%20involves%20expressive%0Ageometrical%20object%20deformation.%20Extensive%20experiments%20demonstrate%20that%20Trans4D%0Aconsistently%20outperforms%20existing%20state-of-the-art%20methods%20in%20generating%204D%0Ascenes%20with%20accurate%20and%20high-quality%20transitions%2C%20validating%20its%0Aeffectiveness.%20Code%3A%20https%3A//github.com/YangLing0818/Trans4D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrans4D%253A%2520Realistic%2520Geometry-Aware%2520Transition%2520for%2520Compositional%250A%2520%2520Text-to-4D%2520Synthesis%26entry.906535625%3DBohan%2520Zeng%2520and%2520Ling%2520Yang%2520and%2520Siyu%2520Li%2520and%2520Jiaming%2520Liu%2520and%2520Zixiang%2520Zhang%2520and%2520Juanxi%2520Tian%2520and%2520Kaixin%2520Zhu%2520and%2520Yongzhen%2520Guo%2520and%2520Fu-Yun%2520Wang%2520and%2520Minkai%2520Xu%2520and%2520Stefano%2520Ermon%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520demonstrated%2520exceptional%250Acapabilities%2520in%2520image%2520and%2520video%2520generation%252C%2520further%2520improving%2520the%2520effectiveness%250Aof%25204D%2520synthesis.%2520Existing%25204D%2520generation%2520methods%2520can%2520generate%2520high-quality%25204D%250Aobjects%2520or%2520scenes%2520based%2520on%2520user-friendly%2520conditions%252C%2520benefiting%2520the%2520gaming%2520and%250Avideo%2520industries.%2520However%252C%2520these%2520methods%2520struggle%2520to%2520synthesize%2520significant%250Aobject%2520deformation%2520of%2520complex%25204D%2520transitions%2520and%2520interactions%2520within%2520scenes.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520Trans4D%252C%2520a%2520novel%2520text-to-4D%2520synthesis%250Aframework%2520that%2520enables%2520realistic%2520complex%2520scene%2520transitions.%2520Specifically%252C%2520we%250Afirst%2520use%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520produce%2520a%2520physic-aware%250Ascene%2520description%2520for%25204D%2520scene%2520initialization%2520and%2520effective%2520transition%2520timing%250Aplanning.%2520Then%2520we%2520propose%2520a%2520geometry-aware%25204D%2520transition%2520network%2520to%2520realize%2520a%250Acomplex%2520scene-level%25204D%2520transition%2520based%2520on%2520the%2520plan%252C%2520which%2520involves%2520expressive%250Ageometrical%2520object%2520deformation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Trans4D%250Aconsistently%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520generating%25204D%250Ascenes%2520with%2520accurate%2520and%2520high-quality%2520transitions%252C%2520validating%2520its%250Aeffectiveness.%2520Code%253A%2520https%253A//github.com/YangLing0818/Trans4D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trans4D%3A%20Realistic%20Geometry-Aware%20Transition%20for%20Compositional%0A%20%20Text-to-4D%20Synthesis&entry.906535625=Bohan%20Zeng%20and%20Ling%20Yang%20and%20Siyu%20Li%20and%20Jiaming%20Liu%20and%20Zixiang%20Zhang%20and%20Juanxi%20Tian%20and%20Kaixin%20Zhu%20and%20Yongzhen%20Guo%20and%20Fu-Yun%20Wang%20and%20Minkai%20Xu%20and%20Stefano%20Ermon%20and%20Wentao%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20demonstrated%20exceptional%0Acapabilities%20in%20image%20and%20video%20generation%2C%20further%20improving%20the%20effectiveness%0Aof%204D%20synthesis.%20Existing%204D%20generation%20methods%20can%20generate%20high-quality%204D%0Aobjects%20or%20scenes%20based%20on%20user-friendly%20conditions%2C%20benefiting%20the%20gaming%20and%0Avideo%20industries.%20However%2C%20these%20methods%20struggle%20to%20synthesize%20significant%0Aobject%20deformation%20of%20complex%204D%20transitions%20and%20interactions%20within%20scenes.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20Trans4D%2C%20a%20novel%20text-to-4D%20synthesis%0Aframework%20that%20enables%20realistic%20complex%20scene%20transitions.%20Specifically%2C%20we%0Afirst%20use%20multi-modal%20large%20language%20models%20%28MLLMs%29%20to%20produce%20a%20physic-aware%0Ascene%20description%20for%204D%20scene%20initialization%20and%20effective%20transition%20timing%0Aplanning.%20Then%20we%20propose%20a%20geometry-aware%204D%20transition%20network%20to%20realize%20a%0Acomplex%20scene-level%204D%20transition%20based%20on%20the%20plan%2C%20which%20involves%20expressive%0Ageometrical%20object%20deformation.%20Extensive%20experiments%20demonstrate%20that%20Trans4D%0Aconsistently%20outperforms%20existing%20state-of-the-art%20methods%20in%20generating%204D%0Ascenes%20with%20accurate%20and%20high-quality%20transitions%2C%20validating%20its%0Aeffectiveness.%20Code%3A%20https%3A//github.com/YangLing0818/Trans4D%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07155v1&entry.124074799=Read"},
{"title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning", "author": "Zhengyu Hu and Yichuan Li and Zhengyu Chen and Jingang Wang and Han Liu and Kyumin Lee and Kaize Ding", "abstract": "  Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.\n", "link": "http://arxiv.org/abs/2410.07074v1", "date": "2024-10-09", "relevancy": 2.4002, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4892}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Ask%20GNN%3A%20Empowering%20Large%20Language%20Model%20for%20Graph%20In-Context%0A%20%20Learning&body=Title%3A%20Let%27s%20Ask%20GNN%3A%20Empowering%20Large%20Language%20Model%20for%20Graph%20In-Context%0A%20%20Learning%0AAuthor%3A%20Zhengyu%20Hu%20and%20Yichuan%20Li%20and%20Zhengyu%20Chen%20and%20Jingang%20Wang%20and%20Han%20Liu%20and%20Kyumin%20Lee%20and%20Kaize%20Ding%0AAbstract%3A%20%20%20Textual%20Attributed%20Graphs%20%28TAGs%29%20are%20crucial%20for%20modeling%20complex%20real-world%0Asystems%2C%20yet%20leveraging%20large%20language%20models%20%28LLMs%29%20for%20TAGs%20presents%20unique%0Achallenges%20due%20to%20the%20gap%20between%20sequential%20text%20processing%20and%0Agraph-structured%20data.%20We%20introduce%20AskGNN%2C%20a%20novel%20approach%20that%20bridges%20this%0Agap%20by%20leveraging%20In-Context%20Learning%20%28ICL%29%20to%20integrate%20graph%20data%20and%0Atask-specific%20information%20into%20LLMs.%20AskGNN%20employs%20a%20Graph%20Neural%20Network%0A%28GNN%29-powered%20structure-enhanced%20retriever%20to%20select%20labeled%20nodes%20across%0Agraphs%2C%20incorporating%20complex%20graph%20structures%20and%20their%20supervision%20signals.%0AOur%20learning-to-retrieve%20algorithm%20optimizes%20the%20retriever%20to%20select%20example%0Anodes%20that%20maximize%20LLM%20performance%20on%20graph.%20Experiments%20across%20three%20tasks%0Aand%20seven%20LLMs%20demonstrate%20AskGNN%27s%20superior%20effectiveness%20in%20graph%20task%0Aperformance%2C%20opening%20new%20avenues%20for%20applying%20LLMs%20to%20graph-structured%20data%0Awithout%20extensive%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Ask%2520GNN%253A%2520Empowering%2520Large%2520Language%2520Model%2520for%2520Graph%2520In-Context%250A%2520%2520Learning%26entry.906535625%3DZhengyu%2520Hu%2520and%2520Yichuan%2520Li%2520and%2520Zhengyu%2520Chen%2520and%2520Jingang%2520Wang%2520and%2520Han%2520Liu%2520and%2520Kyumin%2520Lee%2520and%2520Kaize%2520Ding%26entry.1292438233%3D%2520%2520Textual%2520Attributed%2520Graphs%2520%2528TAGs%2529%2520are%2520crucial%2520for%2520modeling%2520complex%2520real-world%250Asystems%252C%2520yet%2520leveraging%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520TAGs%2520presents%2520unique%250Achallenges%2520due%2520to%2520the%2520gap%2520between%2520sequential%2520text%2520processing%2520and%250Agraph-structured%2520data.%2520We%2520introduce%2520AskGNN%252C%2520a%2520novel%2520approach%2520that%2520bridges%2520this%250Agap%2520by%2520leveraging%2520In-Context%2520Learning%2520%2528ICL%2529%2520to%2520integrate%2520graph%2520data%2520and%250Atask-specific%2520information%2520into%2520LLMs.%2520AskGNN%2520employs%2520a%2520Graph%2520Neural%2520Network%250A%2528GNN%2529-powered%2520structure-enhanced%2520retriever%2520to%2520select%2520labeled%2520nodes%2520across%250Agraphs%252C%2520incorporating%2520complex%2520graph%2520structures%2520and%2520their%2520supervision%2520signals.%250AOur%2520learning-to-retrieve%2520algorithm%2520optimizes%2520the%2520retriever%2520to%2520select%2520example%250Anodes%2520that%2520maximize%2520LLM%2520performance%2520on%2520graph.%2520Experiments%2520across%2520three%2520tasks%250Aand%2520seven%2520LLMs%2520demonstrate%2520AskGNN%2527s%2520superior%2520effectiveness%2520in%2520graph%2520task%250Aperformance%252C%2520opening%2520new%2520avenues%2520for%2520applying%2520LLMs%2520to%2520graph-structured%2520data%250Awithout%2520extensive%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Ask%20GNN%3A%20Empowering%20Large%20Language%20Model%20for%20Graph%20In-Context%0A%20%20Learning&entry.906535625=Zhengyu%20Hu%20and%20Yichuan%20Li%20and%20Zhengyu%20Chen%20and%20Jingang%20Wang%20and%20Han%20Liu%20and%20Kyumin%20Lee%20and%20Kaize%20Ding&entry.1292438233=%20%20Textual%20Attributed%20Graphs%20%28TAGs%29%20are%20crucial%20for%20modeling%20complex%20real-world%0Asystems%2C%20yet%20leveraging%20large%20language%20models%20%28LLMs%29%20for%20TAGs%20presents%20unique%0Achallenges%20due%20to%20the%20gap%20between%20sequential%20text%20processing%20and%0Agraph-structured%20data.%20We%20introduce%20AskGNN%2C%20a%20novel%20approach%20that%20bridges%20this%0Agap%20by%20leveraging%20In-Context%20Learning%20%28ICL%29%20to%20integrate%20graph%20data%20and%0Atask-specific%20information%20into%20LLMs.%20AskGNN%20employs%20a%20Graph%20Neural%20Network%0A%28GNN%29-powered%20structure-enhanced%20retriever%20to%20select%20labeled%20nodes%20across%0Agraphs%2C%20incorporating%20complex%20graph%20structures%20and%20their%20supervision%20signals.%0AOur%20learning-to-retrieve%20algorithm%20optimizes%20the%20retriever%20to%20select%20example%0Anodes%20that%20maximize%20LLM%20performance%20on%20graph.%20Experiments%20across%20three%20tasks%0Aand%20seven%20LLMs%20demonstrate%20AskGNN%27s%20superior%20effectiveness%20in%20graph%20task%0Aperformance%2C%20opening%20new%20avenues%20for%20applying%20LLMs%20to%20graph-structured%20data%0Awithout%20extensive%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07074v1&entry.124074799=Read"},
{"title": "Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear\n  Diffusive Parametric PDEs on Multiple Domains", "author": "Shane E. Loeffler and Zan Ahmad and Syed Yusuf Ali and Carolyna Yamamoto and Dan M. Popescu and Alana Yee and Yash Lal and Natalia Trayanova and Mauro Maggioni", "abstract": "  Predicting time-dependent dynamics of complex systems governed by non-linear\npartial differential equations (PDEs) with varying parameters and domains is a\nchallenging task motivated by applications across various fields. We introduce\na novel family of neural operators based on our Graph Fourier Neural Kernels,\ndesigned to learn solution generators for nonlinear PDEs in which the\nhighest-order term is diffusive, across multiple domains and parameters. G-FuNK\ncombines components that are parameter- and domain-adapted with others that are\nnot. The domain-adapted components are constructed using a weighted graph on\nthe discretized domain, where the graph Laplacian approximates the\nhighest-order diffusive term, ensuring boundary condition compliance and\ncapturing the parameter and domain-specific behavior. Meanwhile, the learned\ncomponents transfer across domains and parameters using our variant Fourier\nNeural Operators. This approach naturally embeds geometric and directional\ninformation, improving generalization to new test domains without need for\nretraining the network. To handle temporal dynamics, our method incorporates an\nintegrated ODE solver to predict the evolution of the system. Experiments show\nG-FuNK's capability to accurately approximate heat, reaction diffusion, and\ncardiac electrophysiology equations across various geometries and anisotropic\ndiffusivity fields. G-FuNK achieves low relative errors on unseen domains and\nfiber fields, significantly accelerating predictions compared to traditional\nfinite-element solvers.\n", "link": "http://arxiv.org/abs/2410.04655v2", "date": "2024-10-09", "relevancy": 2.3886, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4824}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4784}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Fourier%20Neural%20Kernels%20%28G-FuNK%29%3A%20Learning%20Solutions%20of%20Nonlinear%0A%20%20Diffusive%20Parametric%20PDEs%20on%20Multiple%20Domains&body=Title%3A%20Graph%20Fourier%20Neural%20Kernels%20%28G-FuNK%29%3A%20Learning%20Solutions%20of%20Nonlinear%0A%20%20Diffusive%20Parametric%20PDEs%20on%20Multiple%20Domains%0AAuthor%3A%20Shane%20E.%20Loeffler%20and%20Zan%20Ahmad%20and%20Syed%20Yusuf%20Ali%20and%20Carolyna%20Yamamoto%20and%20Dan%20M.%20Popescu%20and%20Alana%20Yee%20and%20Yash%20Lal%20and%20Natalia%20Trayanova%20and%20Mauro%20Maggioni%0AAbstract%3A%20%20%20Predicting%20time-dependent%20dynamics%20of%20complex%20systems%20governed%20by%20non-linear%0Apartial%20differential%20equations%20%28PDEs%29%20with%20varying%20parameters%20and%20domains%20is%20a%0Achallenging%20task%20motivated%20by%20applications%20across%20various%20fields.%20We%20introduce%0Aa%20novel%20family%20of%20neural%20operators%20based%20on%20our%20Graph%20Fourier%20Neural%20Kernels%2C%0Adesigned%20to%20learn%20solution%20generators%20for%20nonlinear%20PDEs%20in%20which%20the%0Ahighest-order%20term%20is%20diffusive%2C%20across%20multiple%20domains%20and%20parameters.%20G-FuNK%0Acombines%20components%20that%20are%20parameter-%20and%20domain-adapted%20with%20others%20that%20are%0Anot.%20The%20domain-adapted%20components%20are%20constructed%20using%20a%20weighted%20graph%20on%0Athe%20discretized%20domain%2C%20where%20the%20graph%20Laplacian%20approximates%20the%0Ahighest-order%20diffusive%20term%2C%20ensuring%20boundary%20condition%20compliance%20and%0Acapturing%20the%20parameter%20and%20domain-specific%20behavior.%20Meanwhile%2C%20the%20learned%0Acomponents%20transfer%20across%20domains%20and%20parameters%20using%20our%20variant%20Fourier%0ANeural%20Operators.%20This%20approach%20naturally%20embeds%20geometric%20and%20directional%0Ainformation%2C%20improving%20generalization%20to%20new%20test%20domains%20without%20need%20for%0Aretraining%20the%20network.%20To%20handle%20temporal%20dynamics%2C%20our%20method%20incorporates%20an%0Aintegrated%20ODE%20solver%20to%20predict%20the%20evolution%20of%20the%20system.%20Experiments%20show%0AG-FuNK%27s%20capability%20to%20accurately%20approximate%20heat%2C%20reaction%20diffusion%2C%20and%0Acardiac%20electrophysiology%20equations%20across%20various%20geometries%20and%20anisotropic%0Adiffusivity%20fields.%20G-FuNK%20achieves%20low%20relative%20errors%20on%20unseen%20domains%20and%0Afiber%20fields%2C%20significantly%20accelerating%20predictions%20compared%20to%20traditional%0Afinite-element%20solvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Fourier%2520Neural%2520Kernels%2520%2528G-FuNK%2529%253A%2520Learning%2520Solutions%2520of%2520Nonlinear%250A%2520%2520Diffusive%2520Parametric%2520PDEs%2520on%2520Multiple%2520Domains%26entry.906535625%3DShane%2520E.%2520Loeffler%2520and%2520Zan%2520Ahmad%2520and%2520Syed%2520Yusuf%2520Ali%2520and%2520Carolyna%2520Yamamoto%2520and%2520Dan%2520M.%2520Popescu%2520and%2520Alana%2520Yee%2520and%2520Yash%2520Lal%2520and%2520Natalia%2520Trayanova%2520and%2520Mauro%2520Maggioni%26entry.1292438233%3D%2520%2520Predicting%2520time-dependent%2520dynamics%2520of%2520complex%2520systems%2520governed%2520by%2520non-linear%250Apartial%2520differential%2520equations%2520%2528PDEs%2529%2520with%2520varying%2520parameters%2520and%2520domains%2520is%2520a%250Achallenging%2520task%2520motivated%2520by%2520applications%2520across%2520various%2520fields.%2520We%2520introduce%250Aa%2520novel%2520family%2520of%2520neural%2520operators%2520based%2520on%2520our%2520Graph%2520Fourier%2520Neural%2520Kernels%252C%250Adesigned%2520to%2520learn%2520solution%2520generators%2520for%2520nonlinear%2520PDEs%2520in%2520which%2520the%250Ahighest-order%2520term%2520is%2520diffusive%252C%2520across%2520multiple%2520domains%2520and%2520parameters.%2520G-FuNK%250Acombines%2520components%2520that%2520are%2520parameter-%2520and%2520domain-adapted%2520with%2520others%2520that%2520are%250Anot.%2520The%2520domain-adapted%2520components%2520are%2520constructed%2520using%2520a%2520weighted%2520graph%2520on%250Athe%2520discretized%2520domain%252C%2520where%2520the%2520graph%2520Laplacian%2520approximates%2520the%250Ahighest-order%2520diffusive%2520term%252C%2520ensuring%2520boundary%2520condition%2520compliance%2520and%250Acapturing%2520the%2520parameter%2520and%2520domain-specific%2520behavior.%2520Meanwhile%252C%2520the%2520learned%250Acomponents%2520transfer%2520across%2520domains%2520and%2520parameters%2520using%2520our%2520variant%2520Fourier%250ANeural%2520Operators.%2520This%2520approach%2520naturally%2520embeds%2520geometric%2520and%2520directional%250Ainformation%252C%2520improving%2520generalization%2520to%2520new%2520test%2520domains%2520without%2520need%2520for%250Aretraining%2520the%2520network.%2520To%2520handle%2520temporal%2520dynamics%252C%2520our%2520method%2520incorporates%2520an%250Aintegrated%2520ODE%2520solver%2520to%2520predict%2520the%2520evolution%2520of%2520the%2520system.%2520Experiments%2520show%250AG-FuNK%2527s%2520capability%2520to%2520accurately%2520approximate%2520heat%252C%2520reaction%2520diffusion%252C%2520and%250Acardiac%2520electrophysiology%2520equations%2520across%2520various%2520geometries%2520and%2520anisotropic%250Adiffusivity%2520fields.%2520G-FuNK%2520achieves%2520low%2520relative%2520errors%2520on%2520unseen%2520domains%2520and%250Afiber%2520fields%252C%2520significantly%2520accelerating%2520predictions%2520compared%2520to%2520traditional%250Afinite-element%2520solvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Fourier%20Neural%20Kernels%20%28G-FuNK%29%3A%20Learning%20Solutions%20of%20Nonlinear%0A%20%20Diffusive%20Parametric%20PDEs%20on%20Multiple%20Domains&entry.906535625=Shane%20E.%20Loeffler%20and%20Zan%20Ahmad%20and%20Syed%20Yusuf%20Ali%20and%20Carolyna%20Yamamoto%20and%20Dan%20M.%20Popescu%20and%20Alana%20Yee%20and%20Yash%20Lal%20and%20Natalia%20Trayanova%20and%20Mauro%20Maggioni&entry.1292438233=%20%20Predicting%20time-dependent%20dynamics%20of%20complex%20systems%20governed%20by%20non-linear%0Apartial%20differential%20equations%20%28PDEs%29%20with%20varying%20parameters%20and%20domains%20is%20a%0Achallenging%20task%20motivated%20by%20applications%20across%20various%20fields.%20We%20introduce%0Aa%20novel%20family%20of%20neural%20operators%20based%20on%20our%20Graph%20Fourier%20Neural%20Kernels%2C%0Adesigned%20to%20learn%20solution%20generators%20for%20nonlinear%20PDEs%20in%20which%20the%0Ahighest-order%20term%20is%20diffusive%2C%20across%20multiple%20domains%20and%20parameters.%20G-FuNK%0Acombines%20components%20that%20are%20parameter-%20and%20domain-adapted%20with%20others%20that%20are%0Anot.%20The%20domain-adapted%20components%20are%20constructed%20using%20a%20weighted%20graph%20on%0Athe%20discretized%20domain%2C%20where%20the%20graph%20Laplacian%20approximates%20the%0Ahighest-order%20diffusive%20term%2C%20ensuring%20boundary%20condition%20compliance%20and%0Acapturing%20the%20parameter%20and%20domain-specific%20behavior.%20Meanwhile%2C%20the%20learned%0Acomponents%20transfer%20across%20domains%20and%20parameters%20using%20our%20variant%20Fourier%0ANeural%20Operators.%20This%20approach%20naturally%20embeds%20geometric%20and%20directional%0Ainformation%2C%20improving%20generalization%20to%20new%20test%20domains%20without%20need%20for%0Aretraining%20the%20network.%20To%20handle%20temporal%20dynamics%2C%20our%20method%20incorporates%20an%0Aintegrated%20ODE%20solver%20to%20predict%20the%20evolution%20of%20the%20system.%20Experiments%20show%0AG-FuNK%27s%20capability%20to%20accurately%20approximate%20heat%2C%20reaction%20diffusion%2C%20and%0Acardiac%20electrophysiology%20equations%20across%20various%20geometries%20and%20anisotropic%0Adiffusivity%20fields.%20G-FuNK%20achieves%20low%20relative%20errors%20on%20unseen%20domains%20and%0Afiber%20fields%2C%20significantly%20accelerating%20predictions%20compared%20to%20traditional%0Afinite-element%20solvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04655v2&entry.124074799=Read"},
{"title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning", "author": "Chongyu Fan and Jiancheng Liu and Licong Lin and Jinghan Jia and Ruiqi Zhang and Song Mei and Sijia Liu", "abstract": "  In this work, we address the problem of large language model (LLM)\nunlearning, aiming to remove unwanted data influences and associated model\ncapabilities (e.g., copyrighted data or harmful content generation) while\npreserving essential model utilities, without the need for retraining from\nscratch. Despite the growing need for LLM unlearning, a principled optimization\nframework remains lacking. To this end, we revisit the state-of-the-art\napproach, negative preference optimization (NPO), and identify the issue of\nreference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a\nsimple yet effective unlearning optimization framework, called SimNPO, showing\nthat 'simplicity' in removing the reliance on a reference model (through the\nlens of simple preference optimization) benefits unlearning. We also provide\ndeeper insights into SimNPO's advantages, supported by analysis using mixtures\nof Markov chains. Furthermore, we present extensive experiments validating\nSimNPO's superiority over existing unlearning baselines in benchmarks like TOFU\nand MUSE, and robustness against relearning attacks. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Simple.\n", "link": "http://arxiv.org/abs/2410.07163v1", "date": "2024-10-09", "relevancy": 2.3814, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplicity%20Prevails%3A%20Rethinking%20Negative%20Preference%20Optimization%20for%20LLM%0A%20%20Unlearning&body=Title%3A%20Simplicity%20Prevails%3A%20Rethinking%20Negative%20Preference%20Optimization%20for%20LLM%0A%20%20Unlearning%0AAuthor%3A%20Chongyu%20Fan%20and%20Jiancheng%20Liu%20and%20Licong%20Lin%20and%20Jinghan%20Jia%20and%20Ruiqi%20Zhang%20and%20Song%20Mei%20and%20Sijia%20Liu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20large%20language%20model%20%28LLM%29%0Aunlearning%2C%20aiming%20to%20remove%20unwanted%20data%20influences%20and%20associated%20model%0Acapabilities%20%28e.g.%2C%20copyrighted%20data%20or%20harmful%20content%20generation%29%20while%0Apreserving%20essential%20model%20utilities%2C%20without%20the%20need%20for%20retraining%20from%0Ascratch.%20Despite%20the%20growing%20need%20for%20LLM%20unlearning%2C%20a%20principled%20optimization%0Aframework%20remains%20lacking.%20To%20this%20end%2C%20we%20revisit%20the%20state-of-the-art%0Aapproach%2C%20negative%20preference%20optimization%20%28NPO%29%2C%20and%20identify%20the%20issue%20of%0Areference%20model%20bias%2C%20which%20could%20undermine%20NPO%27s%20effectiveness%2C%20particularly%0Awhen%20unlearning%20forget%20data%20of%20varying%20difficulty.%20Given%20that%2C%20we%20propose%20a%0Asimple%20yet%20effective%20unlearning%20optimization%20framework%2C%20called%20SimNPO%2C%20showing%0Athat%20%27simplicity%27%20in%20removing%20the%20reliance%20on%20a%20reference%20model%20%28through%20the%0Alens%20of%20simple%20preference%20optimization%29%20benefits%20unlearning.%20We%20also%20provide%0Adeeper%20insights%20into%20SimNPO%27s%20advantages%2C%20supported%20by%20analysis%20using%20mixtures%0Aof%20Markov%20chains.%20Furthermore%2C%20we%20present%20extensive%20experiments%20validating%0ASimNPO%27s%20superiority%20over%20existing%20unlearning%20baselines%20in%20benchmarks%20like%20TOFU%0Aand%20MUSE%2C%20and%20robustness%20against%20relearning%20attacks.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/OPTML-Group/Unlearn-Simple.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplicity%2520Prevails%253A%2520Rethinking%2520Negative%2520Preference%2520Optimization%2520for%2520LLM%250A%2520%2520Unlearning%26entry.906535625%3DChongyu%2520Fan%2520and%2520Jiancheng%2520Liu%2520and%2520Licong%2520Lin%2520and%2520Jinghan%2520Jia%2520and%2520Ruiqi%2520Zhang%2520and%2520Song%2520Mei%2520and%2520Sijia%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520problem%2520of%2520large%2520language%2520model%2520%2528LLM%2529%250Aunlearning%252C%2520aiming%2520to%2520remove%2520unwanted%2520data%2520influences%2520and%2520associated%2520model%250Acapabilities%2520%2528e.g.%252C%2520copyrighted%2520data%2520or%2520harmful%2520content%2520generation%2529%2520while%250Apreserving%2520essential%2520model%2520utilities%252C%2520without%2520the%2520need%2520for%2520retraining%2520from%250Ascratch.%2520Despite%2520the%2520growing%2520need%2520for%2520LLM%2520unlearning%252C%2520a%2520principled%2520optimization%250Aframework%2520remains%2520lacking.%2520To%2520this%2520end%252C%2520we%2520revisit%2520the%2520state-of-the-art%250Aapproach%252C%2520negative%2520preference%2520optimization%2520%2528NPO%2529%252C%2520and%2520identify%2520the%2520issue%2520of%250Areference%2520model%2520bias%252C%2520which%2520could%2520undermine%2520NPO%2527s%2520effectiveness%252C%2520particularly%250Awhen%2520unlearning%2520forget%2520data%2520of%2520varying%2520difficulty.%2520Given%2520that%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520unlearning%2520optimization%2520framework%252C%2520called%2520SimNPO%252C%2520showing%250Athat%2520%2527simplicity%2527%2520in%2520removing%2520the%2520reliance%2520on%2520a%2520reference%2520model%2520%2528through%2520the%250Alens%2520of%2520simple%2520preference%2520optimization%2529%2520benefits%2520unlearning.%2520We%2520also%2520provide%250Adeeper%2520insights%2520into%2520SimNPO%2527s%2520advantages%252C%2520supported%2520by%2520analysis%2520using%2520mixtures%250Aof%2520Markov%2520chains.%2520Furthermore%252C%2520we%2520present%2520extensive%2520experiments%2520validating%250ASimNPO%2527s%2520superiority%2520over%2520existing%2520unlearning%2520baselines%2520in%2520benchmarks%2520like%2520TOFU%250Aand%2520MUSE%252C%2520and%2520robustness%2520against%2520relearning%2520attacks.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/OPTML-Group/Unlearn-Simple.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplicity%20Prevails%3A%20Rethinking%20Negative%20Preference%20Optimization%20for%20LLM%0A%20%20Unlearning&entry.906535625=Chongyu%20Fan%20and%20Jiancheng%20Liu%20and%20Licong%20Lin%20and%20Jinghan%20Jia%20and%20Ruiqi%20Zhang%20and%20Song%20Mei%20and%20Sijia%20Liu&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20large%20language%20model%20%28LLM%29%0Aunlearning%2C%20aiming%20to%20remove%20unwanted%20data%20influences%20and%20associated%20model%0Acapabilities%20%28e.g.%2C%20copyrighted%20data%20or%20harmful%20content%20generation%29%20while%0Apreserving%20essential%20model%20utilities%2C%20without%20the%20need%20for%20retraining%20from%0Ascratch.%20Despite%20the%20growing%20need%20for%20LLM%20unlearning%2C%20a%20principled%20optimization%0Aframework%20remains%20lacking.%20To%20this%20end%2C%20we%20revisit%20the%20state-of-the-art%0Aapproach%2C%20negative%20preference%20optimization%20%28NPO%29%2C%20and%20identify%20the%20issue%20of%0Areference%20model%20bias%2C%20which%20could%20undermine%20NPO%27s%20effectiveness%2C%20particularly%0Awhen%20unlearning%20forget%20data%20of%20varying%20difficulty.%20Given%20that%2C%20we%20propose%20a%0Asimple%20yet%20effective%20unlearning%20optimization%20framework%2C%20called%20SimNPO%2C%20showing%0Athat%20%27simplicity%27%20in%20removing%20the%20reliance%20on%20a%20reference%20model%20%28through%20the%0Alens%20of%20simple%20preference%20optimization%29%20benefits%20unlearning.%20We%20also%20provide%0Adeeper%20insights%20into%20SimNPO%27s%20advantages%2C%20supported%20by%20analysis%20using%20mixtures%0Aof%20Markov%20chains.%20Furthermore%2C%20we%20present%20extensive%20experiments%20validating%0ASimNPO%27s%20superiority%20over%20existing%20unlearning%20baselines%20in%20benchmarks%20like%20TOFU%0Aand%20MUSE%2C%20and%20robustness%20against%20relearning%20attacks.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/OPTML-Group/Unlearn-Simple.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07163v1&entry.124074799=Read"},
{"title": "MedLSAM: Localize and Segment Anything Model for 3D CT Images", "author": "Wenhui Lei and Xu Wei and Xiaofan Zhang and Kang Li and Shaoting Zhang", "abstract": "  Recent advancements in foundation models have shown significant potential in\nmedical image analysis. However, there is still a gap in models specifically\ndesigned for medical image localization. To address this, we introduce MedLAM,\na 3D medical foundation localization model that accurately identifies any\nanatomical part within the body using only a few template scans. MedLAM employs\ntwo self-supervision tasks: unified anatomical mapping (UAM) and multi-scale\nsimilarity (MSS) across a comprehensive dataset of 14,012 CT scans.\nFurthermore, we developed MedLSAM by integrating MedLAM with the Segment\nAnything Model (SAM). This innovative framework requires extreme point\nannotations across three directions on several templates to enable MedLAM to\nlocate the target anatomical structure in the image, with SAM performing the\nsegmentation. It significantly reduces the amount of manual annotation required\nby SAM in 3D medical imaging scenarios. We conducted extensive experiments on\ntwo 3D datasets covering 38 distinct organs. Our findings are twofold: 1)\nMedLAM can directly localize anatomical structures using just a few template\nscans, achieving performance comparable to fully supervised models; 2) MedLSAM\nclosely matches the performance of SAM and its specialized medical adaptations\nwith manual prompts, while minimizing the need for extensive point annotations\nacross the entire dataset. Moreover, MedLAM has the potential to be seamlessly\nintegrated with future 3D SAM models, paving the way for enhanced segmentation\nperformance. Our code is public at \\href{https://github.com/openmedlab/MedLSAM}\n", "link": "http://arxiv.org/abs/2306.14752v4", "date": "2024-10-09", "relevancy": 2.3783, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6213}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5895}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedLSAM%3A%20Localize%20and%20Segment%20Anything%20Model%20for%203D%20CT%20Images&body=Title%3A%20MedLSAM%3A%20Localize%20and%20Segment%20Anything%20Model%20for%203D%20CT%20Images%0AAuthor%3A%20Wenhui%20Lei%20and%20Xu%20Wei%20and%20Xiaofan%20Zhang%20and%20Kang%20Li%20and%20Shaoting%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20foundation%20models%20have%20shown%20significant%20potential%20in%0Amedical%20image%20analysis.%20However%2C%20there%20is%20still%20a%20gap%20in%20models%20specifically%0Adesigned%20for%20medical%20image%20localization.%20To%20address%20this%2C%20we%20introduce%20MedLAM%2C%0Aa%203D%20medical%20foundation%20localization%20model%20that%20accurately%20identifies%20any%0Aanatomical%20part%20within%20the%20body%20using%20only%20a%20few%20template%20scans.%20MedLAM%20employs%0Atwo%20self-supervision%20tasks%3A%20unified%20anatomical%20mapping%20%28UAM%29%20and%20multi-scale%0Asimilarity%20%28MSS%29%20across%20a%20comprehensive%20dataset%20of%2014%2C012%20CT%20scans.%0AFurthermore%2C%20we%20developed%20MedLSAM%20by%20integrating%20MedLAM%20with%20the%20Segment%0AAnything%20Model%20%28SAM%29.%20This%20innovative%20framework%20requires%20extreme%20point%0Aannotations%20across%20three%20directions%20on%20several%20templates%20to%20enable%20MedLAM%20to%0Alocate%20the%20target%20anatomical%20structure%20in%20the%20image%2C%20with%20SAM%20performing%20the%0Asegmentation.%20It%20significantly%20reduces%20the%20amount%20of%20manual%20annotation%20required%0Aby%20SAM%20in%203D%20medical%20imaging%20scenarios.%20We%20conducted%20extensive%20experiments%20on%0Atwo%203D%20datasets%20covering%2038%20distinct%20organs.%20Our%20findings%20are%20twofold%3A%201%29%0AMedLAM%20can%20directly%20localize%20anatomical%20structures%20using%20just%20a%20few%20template%0Ascans%2C%20achieving%20performance%20comparable%20to%20fully%20supervised%20models%3B%202%29%20MedLSAM%0Aclosely%20matches%20the%20performance%20of%20SAM%20and%20its%20specialized%20medical%20adaptations%0Awith%20manual%20prompts%2C%20while%20minimizing%20the%20need%20for%20extensive%20point%20annotations%0Aacross%20the%20entire%20dataset.%20Moreover%2C%20MedLAM%20has%20the%20potential%20to%20be%20seamlessly%0Aintegrated%20with%20future%203D%20SAM%20models%2C%20paving%20the%20way%20for%20enhanced%20segmentation%0Aperformance.%20Our%20code%20is%20public%20at%20%5Chref%7Bhttps%3A//github.com/openmedlab/MedLSAM%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.14752v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedLSAM%253A%2520Localize%2520and%2520Segment%2520Anything%2520Model%2520for%25203D%2520CT%2520Images%26entry.906535625%3DWenhui%2520Lei%2520and%2520Xu%2520Wei%2520and%2520Xiaofan%2520Zhang%2520and%2520Kang%2520Li%2520and%2520Shaoting%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520foundation%2520models%2520have%2520shown%2520significant%2520potential%2520in%250Amedical%2520image%2520analysis.%2520However%252C%2520there%2520is%2520still%2520a%2520gap%2520in%2520models%2520specifically%250Adesigned%2520for%2520medical%2520image%2520localization.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MedLAM%252C%250Aa%25203D%2520medical%2520foundation%2520localization%2520model%2520that%2520accurately%2520identifies%2520any%250Aanatomical%2520part%2520within%2520the%2520body%2520using%2520only%2520a%2520few%2520template%2520scans.%2520MedLAM%2520employs%250Atwo%2520self-supervision%2520tasks%253A%2520unified%2520anatomical%2520mapping%2520%2528UAM%2529%2520and%2520multi-scale%250Asimilarity%2520%2528MSS%2529%2520across%2520a%2520comprehensive%2520dataset%2520of%252014%252C012%2520CT%2520scans.%250AFurthermore%252C%2520we%2520developed%2520MedLSAM%2520by%2520integrating%2520MedLAM%2520with%2520the%2520Segment%250AAnything%2520Model%2520%2528SAM%2529.%2520This%2520innovative%2520framework%2520requires%2520extreme%2520point%250Aannotations%2520across%2520three%2520directions%2520on%2520several%2520templates%2520to%2520enable%2520MedLAM%2520to%250Alocate%2520the%2520target%2520anatomical%2520structure%2520in%2520the%2520image%252C%2520with%2520SAM%2520performing%2520the%250Asegmentation.%2520It%2520significantly%2520reduces%2520the%2520amount%2520of%2520manual%2520annotation%2520required%250Aby%2520SAM%2520in%25203D%2520medical%2520imaging%2520scenarios.%2520We%2520conducted%2520extensive%2520experiments%2520on%250Atwo%25203D%2520datasets%2520covering%252038%2520distinct%2520organs.%2520Our%2520findings%2520are%2520twofold%253A%25201%2529%250AMedLAM%2520can%2520directly%2520localize%2520anatomical%2520structures%2520using%2520just%2520a%2520few%2520template%250Ascans%252C%2520achieving%2520performance%2520comparable%2520to%2520fully%2520supervised%2520models%253B%25202%2529%2520MedLSAM%250Aclosely%2520matches%2520the%2520performance%2520of%2520SAM%2520and%2520its%2520specialized%2520medical%2520adaptations%250Awith%2520manual%2520prompts%252C%2520while%2520minimizing%2520the%2520need%2520for%2520extensive%2520point%2520annotations%250Aacross%2520the%2520entire%2520dataset.%2520Moreover%252C%2520MedLAM%2520has%2520the%2520potential%2520to%2520be%2520seamlessly%250Aintegrated%2520with%2520future%25203D%2520SAM%2520models%252C%2520paving%2520the%2520way%2520for%2520enhanced%2520segmentation%250Aperformance.%2520Our%2520code%2520is%2520public%2520at%2520%255Chref%257Bhttps%253A//github.com/openmedlab/MedLSAM%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.14752v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedLSAM%3A%20Localize%20and%20Segment%20Anything%20Model%20for%203D%20CT%20Images&entry.906535625=Wenhui%20Lei%20and%20Xu%20Wei%20and%20Xiaofan%20Zhang%20and%20Kang%20Li%20and%20Shaoting%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20foundation%20models%20have%20shown%20significant%20potential%20in%0Amedical%20image%20analysis.%20However%2C%20there%20is%20still%20a%20gap%20in%20models%20specifically%0Adesigned%20for%20medical%20image%20localization.%20To%20address%20this%2C%20we%20introduce%20MedLAM%2C%0Aa%203D%20medical%20foundation%20localization%20model%20that%20accurately%20identifies%20any%0Aanatomical%20part%20within%20the%20body%20using%20only%20a%20few%20template%20scans.%20MedLAM%20employs%0Atwo%20self-supervision%20tasks%3A%20unified%20anatomical%20mapping%20%28UAM%29%20and%20multi-scale%0Asimilarity%20%28MSS%29%20across%20a%20comprehensive%20dataset%20of%2014%2C012%20CT%20scans.%0AFurthermore%2C%20we%20developed%20MedLSAM%20by%20integrating%20MedLAM%20with%20the%20Segment%0AAnything%20Model%20%28SAM%29.%20This%20innovative%20framework%20requires%20extreme%20point%0Aannotations%20across%20three%20directions%20on%20several%20templates%20to%20enable%20MedLAM%20to%0Alocate%20the%20target%20anatomical%20structure%20in%20the%20image%2C%20with%20SAM%20performing%20the%0Asegmentation.%20It%20significantly%20reduces%20the%20amount%20of%20manual%20annotation%20required%0Aby%20SAM%20in%203D%20medical%20imaging%20scenarios.%20We%20conducted%20extensive%20experiments%20on%0Atwo%203D%20datasets%20covering%2038%20distinct%20organs.%20Our%20findings%20are%20twofold%3A%201%29%0AMedLAM%20can%20directly%20localize%20anatomical%20structures%20using%20just%20a%20few%20template%0Ascans%2C%20achieving%20performance%20comparable%20to%20fully%20supervised%20models%3B%202%29%20MedLSAM%0Aclosely%20matches%20the%20performance%20of%20SAM%20and%20its%20specialized%20medical%20adaptations%0Awith%20manual%20prompts%2C%20while%20minimizing%20the%20need%20for%20extensive%20point%20annotations%0Aacross%20the%20entire%20dataset.%20Moreover%2C%20MedLAM%20has%20the%20potential%20to%20be%20seamlessly%0Aintegrated%20with%20future%203D%20SAM%20models%2C%20paving%20the%20way%20for%20enhanced%20segmentation%0Aperformance.%20Our%20code%20is%20public%20at%20%5Chref%7Bhttps%3A//github.com/openmedlab/MedLSAM%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14752v4&entry.124074799=Read"},
{"title": "Diagnosis of Malignant Lymphoma Cancer Using Hybrid Optimized Techniques\n  Based on Dense Neural Networks", "author": "Salah A. Aly and Ali Bakhiet and Mazen Balat", "abstract": "  Lymphoma diagnosis, particularly distinguishing between subtypes, is critical\nfor effective treatment but remains challenging due to the subtle morphological\ndifferences in histopathological images. This study presents a novel hybrid\ndeep learning framework that combines DenseNet201 for feature extraction with a\nDense Neural Network (DNN) for classification, optimized using the Harris Hawks\nOptimization (HHO) algorithm. The model was trained on a dataset of 15,000\nbiopsy images, spanning three lymphoma subtypes: Chronic Lymphocytic Leukemia\n(CLL), Follicular Lymphoma (FL), and Mantle Cell Lymphoma (MCL). Our approach\nachieved a testing accuracy of 99.33\\%, demonstrating significant improvements\nin both accuracy and model interpretability. Comprehensive evaluation using\nprecision, recall, F1-score, and ROC-AUC underscores the model's robustness and\npotential for clinical adoption. This framework offers a scalable solution for\nimproving diagnostic accuracy and efficiency in oncology.\n", "link": "http://arxiv.org/abs/2410.06974v1", "date": "2024-10-09", "relevancy": 2.3735, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5189}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4552}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagnosis%20of%20Malignant%20Lymphoma%20Cancer%20Using%20Hybrid%20Optimized%20Techniques%0A%20%20Based%20on%20Dense%20Neural%20Networks&body=Title%3A%20Diagnosis%20of%20Malignant%20Lymphoma%20Cancer%20Using%20Hybrid%20Optimized%20Techniques%0A%20%20Based%20on%20Dense%20Neural%20Networks%0AAuthor%3A%20Salah%20A.%20Aly%20and%20Ali%20Bakhiet%20and%20Mazen%20Balat%0AAbstract%3A%20%20%20Lymphoma%20diagnosis%2C%20particularly%20distinguishing%20between%20subtypes%2C%20is%20critical%0Afor%20effective%20treatment%20but%20remains%20challenging%20due%20to%20the%20subtle%20morphological%0Adifferences%20in%20histopathological%20images.%20This%20study%20presents%20a%20novel%20hybrid%0Adeep%20learning%20framework%20that%20combines%20DenseNet201%20for%20feature%20extraction%20with%20a%0ADense%20Neural%20Network%20%28DNN%29%20for%20classification%2C%20optimized%20using%20the%20Harris%20Hawks%0AOptimization%20%28HHO%29%20algorithm.%20The%20model%20was%20trained%20on%20a%20dataset%20of%2015%2C000%0Abiopsy%20images%2C%20spanning%20three%20lymphoma%20subtypes%3A%20Chronic%20Lymphocytic%20Leukemia%0A%28CLL%29%2C%20Follicular%20Lymphoma%20%28FL%29%2C%20and%20Mantle%20Cell%20Lymphoma%20%28MCL%29.%20Our%20approach%0Aachieved%20a%20testing%20accuracy%20of%2099.33%5C%25%2C%20demonstrating%20significant%20improvements%0Ain%20both%20accuracy%20and%20model%20interpretability.%20Comprehensive%20evaluation%20using%0Aprecision%2C%20recall%2C%20F1-score%2C%20and%20ROC-AUC%20underscores%20the%20model%27s%20robustness%20and%0Apotential%20for%20clinical%20adoption.%20This%20framework%20offers%20a%20scalable%20solution%20for%0Aimproving%20diagnostic%20accuracy%20and%20efficiency%20in%20oncology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagnosis%2520of%2520Malignant%2520Lymphoma%2520Cancer%2520Using%2520Hybrid%2520Optimized%2520Techniques%250A%2520%2520Based%2520on%2520Dense%2520Neural%2520Networks%26entry.906535625%3DSalah%2520A.%2520Aly%2520and%2520Ali%2520Bakhiet%2520and%2520Mazen%2520Balat%26entry.1292438233%3D%2520%2520Lymphoma%2520diagnosis%252C%2520particularly%2520distinguishing%2520between%2520subtypes%252C%2520is%2520critical%250Afor%2520effective%2520treatment%2520but%2520remains%2520challenging%2520due%2520to%2520the%2520subtle%2520morphological%250Adifferences%2520in%2520histopathological%2520images.%2520This%2520study%2520presents%2520a%2520novel%2520hybrid%250Adeep%2520learning%2520framework%2520that%2520combines%2520DenseNet201%2520for%2520feature%2520extraction%2520with%2520a%250ADense%2520Neural%2520Network%2520%2528DNN%2529%2520for%2520classification%252C%2520optimized%2520using%2520the%2520Harris%2520Hawks%250AOptimization%2520%2528HHO%2529%2520algorithm.%2520The%2520model%2520was%2520trained%2520on%2520a%2520dataset%2520of%252015%252C000%250Abiopsy%2520images%252C%2520spanning%2520three%2520lymphoma%2520subtypes%253A%2520Chronic%2520Lymphocytic%2520Leukemia%250A%2528CLL%2529%252C%2520Follicular%2520Lymphoma%2520%2528FL%2529%252C%2520and%2520Mantle%2520Cell%2520Lymphoma%2520%2528MCL%2529.%2520Our%2520approach%250Aachieved%2520a%2520testing%2520accuracy%2520of%252099.33%255C%2525%252C%2520demonstrating%2520significant%2520improvements%250Ain%2520both%2520accuracy%2520and%2520model%2520interpretability.%2520Comprehensive%2520evaluation%2520using%250Aprecision%252C%2520recall%252C%2520F1-score%252C%2520and%2520ROC-AUC%2520underscores%2520the%2520model%2527s%2520robustness%2520and%250Apotential%2520for%2520clinical%2520adoption.%2520This%2520framework%2520offers%2520a%2520scalable%2520solution%2520for%250Aimproving%2520diagnostic%2520accuracy%2520and%2520efficiency%2520in%2520oncology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagnosis%20of%20Malignant%20Lymphoma%20Cancer%20Using%20Hybrid%20Optimized%20Techniques%0A%20%20Based%20on%20Dense%20Neural%20Networks&entry.906535625=Salah%20A.%20Aly%20and%20Ali%20Bakhiet%20and%20Mazen%20Balat&entry.1292438233=%20%20Lymphoma%20diagnosis%2C%20particularly%20distinguishing%20between%20subtypes%2C%20is%20critical%0Afor%20effective%20treatment%20but%20remains%20challenging%20due%20to%20the%20subtle%20morphological%0Adifferences%20in%20histopathological%20images.%20This%20study%20presents%20a%20novel%20hybrid%0Adeep%20learning%20framework%20that%20combines%20DenseNet201%20for%20feature%20extraction%20with%20a%0ADense%20Neural%20Network%20%28DNN%29%20for%20classification%2C%20optimized%20using%20the%20Harris%20Hawks%0AOptimization%20%28HHO%29%20algorithm.%20The%20model%20was%20trained%20on%20a%20dataset%20of%2015%2C000%0Abiopsy%20images%2C%20spanning%20three%20lymphoma%20subtypes%3A%20Chronic%20Lymphocytic%20Leukemia%0A%28CLL%29%2C%20Follicular%20Lymphoma%20%28FL%29%2C%20and%20Mantle%20Cell%20Lymphoma%20%28MCL%29.%20Our%20approach%0Aachieved%20a%20testing%20accuracy%20of%2099.33%5C%25%2C%20demonstrating%20significant%20improvements%0Ain%20both%20accuracy%20and%20model%20interpretability.%20Comprehensive%20evaluation%20using%0Aprecision%2C%20recall%2C%20F1-score%2C%20and%20ROC-AUC%20underscores%20the%20model%27s%20robustness%20and%0Apotential%20for%20clinical%20adoption.%20This%20framework%20offers%20a%20scalable%20solution%20for%0Aimproving%20diagnostic%20accuracy%20and%20efficiency%20in%20oncology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06974v1&entry.124074799=Read"},
{"title": "Deep End-to-End Survival Analysis with Temporal Consistency", "author": "Mariana Vargas Vieyra and Pascal Frossard", "abstract": "  In this study, we present a novel Survival Analysis algorithm designed to\nefficiently handle large-scale longitudinal data. Our approach draws\ninspiration from Reinforcement Learning principles, particularly the Deep\nQ-Network paradigm, extending Temporal Learning concepts to Survival\nRegression. A central idea in our method is temporal consistency, a hypothesis\nthat past and future outcomes in the data evolve smoothly over time. Our\nframework uniquely incorporates temporal consistency into large datasets by\nproviding a stable training signal that captures long-term temporal\nrelationships and ensures reliable updates. Additionally, the method supports\narbitrarily complex architectures, enabling the modeling of intricate temporal\ndependencies, and allows for end-to-end training. Through numerous experiments\nwe provide empirical evidence demonstrating our framework's ability to exploit\ntemporal consistency across datasets of varying sizes. Moreover, our algorithm\noutperforms benchmarks on datasets with long sequences, demonstrating its\nability to capture long-term patterns. Finally, ablation studies show how our\nmethod enhances training stability.\n", "link": "http://arxiv.org/abs/2410.06786v1", "date": "2024-10-09", "relevancy": 2.37, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4994}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4641}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20End-to-End%20Survival%20Analysis%20with%20Temporal%20Consistency&body=Title%3A%20Deep%20End-to-End%20Survival%20Analysis%20with%20Temporal%20Consistency%0AAuthor%3A%20Mariana%20Vargas%20Vieyra%20and%20Pascal%20Frossard%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20present%20a%20novel%20Survival%20Analysis%20algorithm%20designed%20to%0Aefficiently%20handle%20large-scale%20longitudinal%20data.%20Our%20approach%20draws%0Ainspiration%20from%20Reinforcement%20Learning%20principles%2C%20particularly%20the%20Deep%0AQ-Network%20paradigm%2C%20extending%20Temporal%20Learning%20concepts%20to%20Survival%0ARegression.%20A%20central%20idea%20in%20our%20method%20is%20temporal%20consistency%2C%20a%20hypothesis%0Athat%20past%20and%20future%20outcomes%20in%20the%20data%20evolve%20smoothly%20over%20time.%20Our%0Aframework%20uniquely%20incorporates%20temporal%20consistency%20into%20large%20datasets%20by%0Aproviding%20a%20stable%20training%20signal%20that%20captures%20long-term%20temporal%0Arelationships%20and%20ensures%20reliable%20updates.%20Additionally%2C%20the%20method%20supports%0Aarbitrarily%20complex%20architectures%2C%20enabling%20the%20modeling%20of%20intricate%20temporal%0Adependencies%2C%20and%20allows%20for%20end-to-end%20training.%20Through%20numerous%20experiments%0Awe%20provide%20empirical%20evidence%20demonstrating%20our%20framework%27s%20ability%20to%20exploit%0Atemporal%20consistency%20across%20datasets%20of%20varying%20sizes.%20Moreover%2C%20our%20algorithm%0Aoutperforms%20benchmarks%20on%20datasets%20with%20long%20sequences%2C%20demonstrating%20its%0Aability%20to%20capture%20long-term%20patterns.%20Finally%2C%20ablation%20studies%20show%20how%20our%0Amethod%20enhances%20training%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520End-to-End%2520Survival%2520Analysis%2520with%2520Temporal%2520Consistency%26entry.906535625%3DMariana%2520Vargas%2520Vieyra%2520and%2520Pascal%2520Frossard%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520novel%2520Survival%2520Analysis%2520algorithm%2520designed%2520to%250Aefficiently%2520handle%2520large-scale%2520longitudinal%2520data.%2520Our%2520approach%2520draws%250Ainspiration%2520from%2520Reinforcement%2520Learning%2520principles%252C%2520particularly%2520the%2520Deep%250AQ-Network%2520paradigm%252C%2520extending%2520Temporal%2520Learning%2520concepts%2520to%2520Survival%250ARegression.%2520A%2520central%2520idea%2520in%2520our%2520method%2520is%2520temporal%2520consistency%252C%2520a%2520hypothesis%250Athat%2520past%2520and%2520future%2520outcomes%2520in%2520the%2520data%2520evolve%2520smoothly%2520over%2520time.%2520Our%250Aframework%2520uniquely%2520incorporates%2520temporal%2520consistency%2520into%2520large%2520datasets%2520by%250Aproviding%2520a%2520stable%2520training%2520signal%2520that%2520captures%2520long-term%2520temporal%250Arelationships%2520and%2520ensures%2520reliable%2520updates.%2520Additionally%252C%2520the%2520method%2520supports%250Aarbitrarily%2520complex%2520architectures%252C%2520enabling%2520the%2520modeling%2520of%2520intricate%2520temporal%250Adependencies%252C%2520and%2520allows%2520for%2520end-to-end%2520training.%2520Through%2520numerous%2520experiments%250Awe%2520provide%2520empirical%2520evidence%2520demonstrating%2520our%2520framework%2527s%2520ability%2520to%2520exploit%250Atemporal%2520consistency%2520across%2520datasets%2520of%2520varying%2520sizes.%2520Moreover%252C%2520our%2520algorithm%250Aoutperforms%2520benchmarks%2520on%2520datasets%2520with%2520long%2520sequences%252C%2520demonstrating%2520its%250Aability%2520to%2520capture%2520long-term%2520patterns.%2520Finally%252C%2520ablation%2520studies%2520show%2520how%2520our%250Amethod%2520enhances%2520training%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20End-to-End%20Survival%20Analysis%20with%20Temporal%20Consistency&entry.906535625=Mariana%20Vargas%20Vieyra%20and%20Pascal%20Frossard&entry.1292438233=%20%20In%20this%20study%2C%20we%20present%20a%20novel%20Survival%20Analysis%20algorithm%20designed%20to%0Aefficiently%20handle%20large-scale%20longitudinal%20data.%20Our%20approach%20draws%0Ainspiration%20from%20Reinforcement%20Learning%20principles%2C%20particularly%20the%20Deep%0AQ-Network%20paradigm%2C%20extending%20Temporal%20Learning%20concepts%20to%20Survival%0ARegression.%20A%20central%20idea%20in%20our%20method%20is%20temporal%20consistency%2C%20a%20hypothesis%0Athat%20past%20and%20future%20outcomes%20in%20the%20data%20evolve%20smoothly%20over%20time.%20Our%0Aframework%20uniquely%20incorporates%20temporal%20consistency%20into%20large%20datasets%20by%0Aproviding%20a%20stable%20training%20signal%20that%20captures%20long-term%20temporal%0Arelationships%20and%20ensures%20reliable%20updates.%20Additionally%2C%20the%20method%20supports%0Aarbitrarily%20complex%20architectures%2C%20enabling%20the%20modeling%20of%20intricate%20temporal%0Adependencies%2C%20and%20allows%20for%20end-to-end%20training.%20Through%20numerous%20experiments%0Awe%20provide%20empirical%20evidence%20demonstrating%20our%20framework%27s%20ability%20to%20exploit%0Atemporal%20consistency%20across%20datasets%20of%20varying%20sizes.%20Moreover%2C%20our%20algorithm%0Aoutperforms%20benchmarks%20on%20datasets%20with%20long%20sequences%2C%20demonstrating%20its%0Aability%20to%20capture%20long-term%20patterns.%20Finally%2C%20ablation%20studies%20show%20how%20our%0Amethod%20enhances%20training%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06786v1&entry.124074799=Read"},
{"title": "MimicTalk: Mimicking a personalized and expressive 3D talking face in\n  minutes", "author": "Zhenhui Ye and Tianyun Zhong and Yi Ren and Ziyue Jiang and Jiawei Huang and Rongjie Huang and Jinglin Liu and Jinzheng He and Chen Zhang and Zehan Wang and Xize Chen and Xiang Yin and Zhou Zhao", "abstract": "  Talking face generation (TFG) aims to animate a target identity's face to\ncreate realistic talking videos. Personalized TFG is a variant that emphasizes\nthe perceptual identity similarity of the synthesized result (from the\nperspective of appearance and talking style). While previous works typically\nsolve this problem by learning an individual neural radiance field (NeRF) for\neach identity to implicitly store its static and dynamic information, we find\nit inefficient and non-generalized due to the per-identity-per-training\nframework and the limited training data. To this end, we propose MimicTalk, the\nfirst attempt that exploits the rich knowledge from a NeRF-based\nperson-agnostic generic model for improving the efficiency and robustness of\npersonalized TFG. To be specific, (1) we first come up with a person-agnostic\n3D TFG model as the base model and propose to adapt it into a specific\nidentity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help\nthe model learn the personalized static appearance and facial dynamic features;\n(3) To generate the facial motion of the personalized talking style, we propose\nan in-context stylized audio-to-motion model that mimics the implicit talking\nstyle provided in the reference video without information loss by an explicit\nstyle representation. The adaptation process to an unseen identity can be\nperformed in 15 minutes, which is 47 times faster than previous\nperson-dependent methods. Experiments show that our MimicTalk surpasses\nprevious baselines regarding video quality, efficiency, and expressiveness.\nSource code and video samples are available at https://mimictalk.github.io .\n", "link": "http://arxiv.org/abs/2410.06734v1", "date": "2024-10-09", "relevancy": 2.3672, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6204}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6115}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MimicTalk%3A%20Mimicking%20a%20personalized%20and%20expressive%203D%20talking%20face%20in%0A%20%20minutes&body=Title%3A%20MimicTalk%3A%20Mimicking%20a%20personalized%20and%20expressive%203D%20talking%20face%20in%0A%20%20minutes%0AAuthor%3A%20Zhenhui%20Ye%20and%20Tianyun%20Zhong%20and%20Yi%20Ren%20and%20Ziyue%20Jiang%20and%20Jiawei%20Huang%20and%20Rongjie%20Huang%20and%20Jinglin%20Liu%20and%20Jinzheng%20He%20and%20Chen%20Zhang%20and%20Zehan%20Wang%20and%20Xize%20Chen%20and%20Xiang%20Yin%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Talking%20face%20generation%20%28TFG%29%20aims%20to%20animate%20a%20target%20identity%27s%20face%20to%0Acreate%20realistic%20talking%20videos.%20Personalized%20TFG%20is%20a%20variant%20that%20emphasizes%0Athe%20perceptual%20identity%20similarity%20of%20the%20synthesized%20result%20%28from%20the%0Aperspective%20of%20appearance%20and%20talking%20style%29.%20While%20previous%20works%20typically%0Asolve%20this%20problem%20by%20learning%20an%20individual%20neural%20radiance%20field%20%28NeRF%29%20for%0Aeach%20identity%20to%20implicitly%20store%20its%20static%20and%20dynamic%20information%2C%20we%20find%0Ait%20inefficient%20and%20non-generalized%20due%20to%20the%20per-identity-per-training%0Aframework%20and%20the%20limited%20training%20data.%20To%20this%20end%2C%20we%20propose%20MimicTalk%2C%20the%0Afirst%20attempt%20that%20exploits%20the%20rich%20knowledge%20from%20a%20NeRF-based%0Aperson-agnostic%20generic%20model%20for%20improving%20the%20efficiency%20and%20robustness%20of%0Apersonalized%20TFG.%20To%20be%20specific%2C%20%281%29%20we%20first%20come%20up%20with%20a%20person-agnostic%0A3D%20TFG%20model%20as%20the%20base%20model%20and%20propose%20to%20adapt%20it%20into%20a%20specific%0Aidentity%3B%20%282%29%20we%20propose%20a%20static-dynamic-hybrid%20adaptation%20pipeline%20to%20help%0Athe%20model%20learn%20the%20personalized%20static%20appearance%20and%20facial%20dynamic%20features%3B%0A%283%29%20To%20generate%20the%20facial%20motion%20of%20the%20personalized%20talking%20style%2C%20we%20propose%0Aan%20in-context%20stylized%20audio-to-motion%20model%20that%20mimics%20the%20implicit%20talking%0Astyle%20provided%20in%20the%20reference%20video%20without%20information%20loss%20by%20an%20explicit%0Astyle%20representation.%20The%20adaptation%20process%20to%20an%20unseen%20identity%20can%20be%0Aperformed%20in%2015%20minutes%2C%20which%20is%2047%20times%20faster%20than%20previous%0Aperson-dependent%20methods.%20Experiments%20show%20that%20our%20MimicTalk%20surpasses%0Aprevious%20baselines%20regarding%20video%20quality%2C%20efficiency%2C%20and%20expressiveness.%0ASource%20code%20and%20video%20samples%20are%20available%20at%20https%3A//mimictalk.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimicTalk%253A%2520Mimicking%2520a%2520personalized%2520and%2520expressive%25203D%2520talking%2520face%2520in%250A%2520%2520minutes%26entry.906535625%3DZhenhui%2520Ye%2520and%2520Tianyun%2520Zhong%2520and%2520Yi%2520Ren%2520and%2520Ziyue%2520Jiang%2520and%2520Jiawei%2520Huang%2520and%2520Rongjie%2520Huang%2520and%2520Jinglin%2520Liu%2520and%2520Jinzheng%2520He%2520and%2520Chen%2520Zhang%2520and%2520Zehan%2520Wang%2520and%2520Xize%2520Chen%2520and%2520Xiang%2520Yin%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Talking%2520face%2520generation%2520%2528TFG%2529%2520aims%2520to%2520animate%2520a%2520target%2520identity%2527s%2520face%2520to%250Acreate%2520realistic%2520talking%2520videos.%2520Personalized%2520TFG%2520is%2520a%2520variant%2520that%2520emphasizes%250Athe%2520perceptual%2520identity%2520similarity%2520of%2520the%2520synthesized%2520result%2520%2528from%2520the%250Aperspective%2520of%2520appearance%2520and%2520talking%2520style%2529.%2520While%2520previous%2520works%2520typically%250Asolve%2520this%2520problem%2520by%2520learning%2520an%2520individual%2520neural%2520radiance%2520field%2520%2528NeRF%2529%2520for%250Aeach%2520identity%2520to%2520implicitly%2520store%2520its%2520static%2520and%2520dynamic%2520information%252C%2520we%2520find%250Ait%2520inefficient%2520and%2520non-generalized%2520due%2520to%2520the%2520per-identity-per-training%250Aframework%2520and%2520the%2520limited%2520training%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520MimicTalk%252C%2520the%250Afirst%2520attempt%2520that%2520exploits%2520the%2520rich%2520knowledge%2520from%2520a%2520NeRF-based%250Aperson-agnostic%2520generic%2520model%2520for%2520improving%2520the%2520efficiency%2520and%2520robustness%2520of%250Apersonalized%2520TFG.%2520To%2520be%2520specific%252C%2520%25281%2529%2520we%2520first%2520come%2520up%2520with%2520a%2520person-agnostic%250A3D%2520TFG%2520model%2520as%2520the%2520base%2520model%2520and%2520propose%2520to%2520adapt%2520it%2520into%2520a%2520specific%250Aidentity%253B%2520%25282%2529%2520we%2520propose%2520a%2520static-dynamic-hybrid%2520adaptation%2520pipeline%2520to%2520help%250Athe%2520model%2520learn%2520the%2520personalized%2520static%2520appearance%2520and%2520facial%2520dynamic%2520features%253B%250A%25283%2529%2520To%2520generate%2520the%2520facial%2520motion%2520of%2520the%2520personalized%2520talking%2520style%252C%2520we%2520propose%250Aan%2520in-context%2520stylized%2520audio-to-motion%2520model%2520that%2520mimics%2520the%2520implicit%2520talking%250Astyle%2520provided%2520in%2520the%2520reference%2520video%2520without%2520information%2520loss%2520by%2520an%2520explicit%250Astyle%2520representation.%2520The%2520adaptation%2520process%2520to%2520an%2520unseen%2520identity%2520can%2520be%250Aperformed%2520in%252015%2520minutes%252C%2520which%2520is%252047%2520times%2520faster%2520than%2520previous%250Aperson-dependent%2520methods.%2520Experiments%2520show%2520that%2520our%2520MimicTalk%2520surpasses%250Aprevious%2520baselines%2520regarding%2520video%2520quality%252C%2520efficiency%252C%2520and%2520expressiveness.%250ASource%2520code%2520and%2520video%2520samples%2520are%2520available%2520at%2520https%253A//mimictalk.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MimicTalk%3A%20Mimicking%20a%20personalized%20and%20expressive%203D%20talking%20face%20in%0A%20%20minutes&entry.906535625=Zhenhui%20Ye%20and%20Tianyun%20Zhong%20and%20Yi%20Ren%20and%20Ziyue%20Jiang%20and%20Jiawei%20Huang%20and%20Rongjie%20Huang%20and%20Jinglin%20Liu%20and%20Jinzheng%20He%20and%20Chen%20Zhang%20and%20Zehan%20Wang%20and%20Xize%20Chen%20and%20Xiang%20Yin%20and%20Zhou%20Zhao&entry.1292438233=%20%20Talking%20face%20generation%20%28TFG%29%20aims%20to%20animate%20a%20target%20identity%27s%20face%20to%0Acreate%20realistic%20talking%20videos.%20Personalized%20TFG%20is%20a%20variant%20that%20emphasizes%0Athe%20perceptual%20identity%20similarity%20of%20the%20synthesized%20result%20%28from%20the%0Aperspective%20of%20appearance%20and%20talking%20style%29.%20While%20previous%20works%20typically%0Asolve%20this%20problem%20by%20learning%20an%20individual%20neural%20radiance%20field%20%28NeRF%29%20for%0Aeach%20identity%20to%20implicitly%20store%20its%20static%20and%20dynamic%20information%2C%20we%20find%0Ait%20inefficient%20and%20non-generalized%20due%20to%20the%20per-identity-per-training%0Aframework%20and%20the%20limited%20training%20data.%20To%20this%20end%2C%20we%20propose%20MimicTalk%2C%20the%0Afirst%20attempt%20that%20exploits%20the%20rich%20knowledge%20from%20a%20NeRF-based%0Aperson-agnostic%20generic%20model%20for%20improving%20the%20efficiency%20and%20robustness%20of%0Apersonalized%20TFG.%20To%20be%20specific%2C%20%281%29%20we%20first%20come%20up%20with%20a%20person-agnostic%0A3D%20TFG%20model%20as%20the%20base%20model%20and%20propose%20to%20adapt%20it%20into%20a%20specific%0Aidentity%3B%20%282%29%20we%20propose%20a%20static-dynamic-hybrid%20adaptation%20pipeline%20to%20help%0Athe%20model%20learn%20the%20personalized%20static%20appearance%20and%20facial%20dynamic%20features%3B%0A%283%29%20To%20generate%20the%20facial%20motion%20of%20the%20personalized%20talking%20style%2C%20we%20propose%0Aan%20in-context%20stylized%20audio-to-motion%20model%20that%20mimics%20the%20implicit%20talking%0Astyle%20provided%20in%20the%20reference%20video%20without%20information%20loss%20by%20an%20explicit%0Astyle%20representation.%20The%20adaptation%20process%20to%20an%20unseen%20identity%20can%20be%0Aperformed%20in%2015%20minutes%2C%20which%20is%2047%20times%20faster%20than%20previous%0Aperson-dependent%20methods.%20Experiments%20show%20that%20our%20MimicTalk%20surpasses%0Aprevious%20baselines%20regarding%20video%20quality%2C%20efficiency%2C%20and%20expressiveness.%0ASource%20code%20and%20video%20samples%20are%20available%20at%20https%3A//mimictalk.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06734v1&entry.124074799=Read"},
{"title": "Students' Perceptions and Use of Generative AI Tools for Programming\n  Across Different Computing Courses", "author": "Hieke Keuning and Isaac Alpizar-Chacon and Ioanna Lykourentzou and Lauren Beehler and Christian K\u00f6ppe and Imke de Jong and Sergey Sosnovsky", "abstract": "  Investigation of students' perceptions and opinions on the use of generative\nartificial intelligence (GenAI) in education is a topic gaining much interest.\nStudies addressing this are typically conducted with large heterogeneous\ngroups, at one moment in time. However, how students perceive and use GenAI\ntools can potentially depend on many factors, including their background\nknowledge, familiarity with the tools, and the learning goals and policies of\nthe courses they are taking.\n  In this study we explore how students following computing courses use GenAI\nfor programming-related tasks across different programs and courses: Bachelor\nand Master, in courses in which learning programming is the learning goal,\ncourses that require programming as a means to achieve another goal, and in\ncourses in which programming is optional, but can be useful. We are also\ninterested in changes over time, since GenAI capabilities are changing at a\nfast pace, and users are adopting GenAI increasingly.\n  We conducted three consecutive surveys (fall `23, winter `23, and spring `24)\namong students of all computing programs of a large European research\nuniversity. We asked questions on the use in education, ethics, and job\nprospects, and we included specific questions on the (dis)allowed use of GenAI\ntools in the courses they were taking at the time.\n  We received 264 responses, which we quantitatively and qualitatively\nanalyzed, to find out how students have employed GenAI tools across 59\ndifferent computing courses, and whether the opinion of an average student\nabout these tools evolves over time. Our study contributes to the emerging\ndiscussion of how to differentiate GenAI use across different courses, and how\nto align its use with the learning goals of a computing course.\n", "link": "http://arxiv.org/abs/2410.06865v1", "date": "2024-10-09", "relevancy": 2.3671, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.493}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4704}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Students%27%20Perceptions%20and%20Use%20of%20Generative%20AI%20Tools%20for%20Programming%0A%20%20Across%20Different%20Computing%20Courses&body=Title%3A%20Students%27%20Perceptions%20and%20Use%20of%20Generative%20AI%20Tools%20for%20Programming%0A%20%20Across%20Different%20Computing%20Courses%0AAuthor%3A%20Hieke%20Keuning%20and%20Isaac%20Alpizar-Chacon%20and%20Ioanna%20Lykourentzou%20and%20Lauren%20Beehler%20and%20Christian%20K%C3%B6ppe%20and%20Imke%20de%20Jong%20and%20Sergey%20Sosnovsky%0AAbstract%3A%20%20%20Investigation%20of%20students%27%20perceptions%20and%20opinions%20on%20the%20use%20of%20generative%0Aartificial%20intelligence%20%28GenAI%29%20in%20education%20is%20a%20topic%20gaining%20much%20interest.%0AStudies%20addressing%20this%20are%20typically%20conducted%20with%20large%20heterogeneous%0Agroups%2C%20at%20one%20moment%20in%20time.%20However%2C%20how%20students%20perceive%20and%20use%20GenAI%0Atools%20can%20potentially%20depend%20on%20many%20factors%2C%20including%20their%20background%0Aknowledge%2C%20familiarity%20with%20the%20tools%2C%20and%20the%20learning%20goals%20and%20policies%20of%0Athe%20courses%20they%20are%20taking.%0A%20%20In%20this%20study%20we%20explore%20how%20students%20following%20computing%20courses%20use%20GenAI%0Afor%20programming-related%20tasks%20across%20different%20programs%20and%20courses%3A%20Bachelor%0Aand%20Master%2C%20in%20courses%20in%20which%20learning%20programming%20is%20the%20learning%20goal%2C%0Acourses%20that%20require%20programming%20as%20a%20means%20to%20achieve%20another%20goal%2C%20and%20in%0Acourses%20in%20which%20programming%20is%20optional%2C%20but%20can%20be%20useful.%20We%20are%20also%0Ainterested%20in%20changes%20over%20time%2C%20since%20GenAI%20capabilities%20are%20changing%20at%20a%0Afast%20pace%2C%20and%20users%20are%20adopting%20GenAI%20increasingly.%0A%20%20We%20conducted%20three%20consecutive%20surveys%20%28fall%20%6023%2C%20winter%20%6023%2C%20and%20spring%20%6024%29%0Aamong%20students%20of%20all%20computing%20programs%20of%20a%20large%20European%20research%0Auniversity.%20We%20asked%20questions%20on%20the%20use%20in%20education%2C%20ethics%2C%20and%20job%0Aprospects%2C%20and%20we%20included%20specific%20questions%20on%20the%20%28dis%29allowed%20use%20of%20GenAI%0Atools%20in%20the%20courses%20they%20were%20taking%20at%20the%20time.%0A%20%20We%20received%20264%20responses%2C%20which%20we%20quantitatively%20and%20qualitatively%0Aanalyzed%2C%20to%20find%20out%20how%20students%20have%20employed%20GenAI%20tools%20across%2059%0Adifferent%20computing%20courses%2C%20and%20whether%20the%20opinion%20of%20an%20average%20student%0Aabout%20these%20tools%20evolves%20over%20time.%20Our%20study%20contributes%20to%20the%20emerging%0Adiscussion%20of%20how%20to%20differentiate%20GenAI%20use%20across%20different%20courses%2C%20and%20how%0Ato%20align%20its%20use%20with%20the%20learning%20goals%20of%20a%20computing%20course.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudents%2527%2520Perceptions%2520and%2520Use%2520of%2520Generative%2520AI%2520Tools%2520for%2520Programming%250A%2520%2520Across%2520Different%2520Computing%2520Courses%26entry.906535625%3DHieke%2520Keuning%2520and%2520Isaac%2520Alpizar-Chacon%2520and%2520Ioanna%2520Lykourentzou%2520and%2520Lauren%2520Beehler%2520and%2520Christian%2520K%25C3%25B6ppe%2520and%2520Imke%2520de%2520Jong%2520and%2520Sergey%2520Sosnovsky%26entry.1292438233%3D%2520%2520Investigation%2520of%2520students%2527%2520perceptions%2520and%2520opinions%2520on%2520the%2520use%2520of%2520generative%250Aartificial%2520intelligence%2520%2528GenAI%2529%2520in%2520education%2520is%2520a%2520topic%2520gaining%2520much%2520interest.%250AStudies%2520addressing%2520this%2520are%2520typically%2520conducted%2520with%2520large%2520heterogeneous%250Agroups%252C%2520at%2520one%2520moment%2520in%2520time.%2520However%252C%2520how%2520students%2520perceive%2520and%2520use%2520GenAI%250Atools%2520can%2520potentially%2520depend%2520on%2520many%2520factors%252C%2520including%2520their%2520background%250Aknowledge%252C%2520familiarity%2520with%2520the%2520tools%252C%2520and%2520the%2520learning%2520goals%2520and%2520policies%2520of%250Athe%2520courses%2520they%2520are%2520taking.%250A%2520%2520In%2520this%2520study%2520we%2520explore%2520how%2520students%2520following%2520computing%2520courses%2520use%2520GenAI%250Afor%2520programming-related%2520tasks%2520across%2520different%2520programs%2520and%2520courses%253A%2520Bachelor%250Aand%2520Master%252C%2520in%2520courses%2520in%2520which%2520learning%2520programming%2520is%2520the%2520learning%2520goal%252C%250Acourses%2520that%2520require%2520programming%2520as%2520a%2520means%2520to%2520achieve%2520another%2520goal%252C%2520and%2520in%250Acourses%2520in%2520which%2520programming%2520is%2520optional%252C%2520but%2520can%2520be%2520useful.%2520We%2520are%2520also%250Ainterested%2520in%2520changes%2520over%2520time%252C%2520since%2520GenAI%2520capabilities%2520are%2520changing%2520at%2520a%250Afast%2520pace%252C%2520and%2520users%2520are%2520adopting%2520GenAI%2520increasingly.%250A%2520%2520We%2520conducted%2520three%2520consecutive%2520surveys%2520%2528fall%2520%256023%252C%2520winter%2520%256023%252C%2520and%2520spring%2520%256024%2529%250Aamong%2520students%2520of%2520all%2520computing%2520programs%2520of%2520a%2520large%2520European%2520research%250Auniversity.%2520We%2520asked%2520questions%2520on%2520the%2520use%2520in%2520education%252C%2520ethics%252C%2520and%2520job%250Aprospects%252C%2520and%2520we%2520included%2520specific%2520questions%2520on%2520the%2520%2528dis%2529allowed%2520use%2520of%2520GenAI%250Atools%2520in%2520the%2520courses%2520they%2520were%2520taking%2520at%2520the%2520time.%250A%2520%2520We%2520received%2520264%2520responses%252C%2520which%2520we%2520quantitatively%2520and%2520qualitatively%250Aanalyzed%252C%2520to%2520find%2520out%2520how%2520students%2520have%2520employed%2520GenAI%2520tools%2520across%252059%250Adifferent%2520computing%2520courses%252C%2520and%2520whether%2520the%2520opinion%2520of%2520an%2520average%2520student%250Aabout%2520these%2520tools%2520evolves%2520over%2520time.%2520Our%2520study%2520contributes%2520to%2520the%2520emerging%250Adiscussion%2520of%2520how%2520to%2520differentiate%2520GenAI%2520use%2520across%2520different%2520courses%252C%2520and%2520how%250Ato%2520align%2520its%2520use%2520with%2520the%2520learning%2520goals%2520of%2520a%2520computing%2520course.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Students%27%20Perceptions%20and%20Use%20of%20Generative%20AI%20Tools%20for%20Programming%0A%20%20Across%20Different%20Computing%20Courses&entry.906535625=Hieke%20Keuning%20and%20Isaac%20Alpizar-Chacon%20and%20Ioanna%20Lykourentzou%20and%20Lauren%20Beehler%20and%20Christian%20K%C3%B6ppe%20and%20Imke%20de%20Jong%20and%20Sergey%20Sosnovsky&entry.1292438233=%20%20Investigation%20of%20students%27%20perceptions%20and%20opinions%20on%20the%20use%20of%20generative%0Aartificial%20intelligence%20%28GenAI%29%20in%20education%20is%20a%20topic%20gaining%20much%20interest.%0AStudies%20addressing%20this%20are%20typically%20conducted%20with%20large%20heterogeneous%0Agroups%2C%20at%20one%20moment%20in%20time.%20However%2C%20how%20students%20perceive%20and%20use%20GenAI%0Atools%20can%20potentially%20depend%20on%20many%20factors%2C%20including%20their%20background%0Aknowledge%2C%20familiarity%20with%20the%20tools%2C%20and%20the%20learning%20goals%20and%20policies%20of%0Athe%20courses%20they%20are%20taking.%0A%20%20In%20this%20study%20we%20explore%20how%20students%20following%20computing%20courses%20use%20GenAI%0Afor%20programming-related%20tasks%20across%20different%20programs%20and%20courses%3A%20Bachelor%0Aand%20Master%2C%20in%20courses%20in%20which%20learning%20programming%20is%20the%20learning%20goal%2C%0Acourses%20that%20require%20programming%20as%20a%20means%20to%20achieve%20another%20goal%2C%20and%20in%0Acourses%20in%20which%20programming%20is%20optional%2C%20but%20can%20be%20useful.%20We%20are%20also%0Ainterested%20in%20changes%20over%20time%2C%20since%20GenAI%20capabilities%20are%20changing%20at%20a%0Afast%20pace%2C%20and%20users%20are%20adopting%20GenAI%20increasingly.%0A%20%20We%20conducted%20three%20consecutive%20surveys%20%28fall%20%6023%2C%20winter%20%6023%2C%20and%20spring%20%6024%29%0Aamong%20students%20of%20all%20computing%20programs%20of%20a%20large%20European%20research%0Auniversity.%20We%20asked%20questions%20on%20the%20use%20in%20education%2C%20ethics%2C%20and%20job%0Aprospects%2C%20and%20we%20included%20specific%20questions%20on%20the%20%28dis%29allowed%20use%20of%20GenAI%0Atools%20in%20the%20courses%20they%20were%20taking%20at%20the%20time.%0A%20%20We%20received%20264%20responses%2C%20which%20we%20quantitatively%20and%20qualitatively%0Aanalyzed%2C%20to%20find%20out%20how%20students%20have%20employed%20GenAI%20tools%20across%2059%0Adifferent%20computing%20courses%2C%20and%20whether%20the%20opinion%20of%20an%20average%20student%0Aabout%20these%20tools%20evolves%20over%20time.%20Our%20study%20contributes%20to%20the%20emerging%0Adiscussion%20of%20how%20to%20differentiate%20GenAI%20use%20across%20different%20courses%2C%20and%20how%0Ato%20align%20its%20use%20with%20the%20learning%20goals%20of%20a%20computing%20course.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06865v1&entry.124074799=Read"},
{"title": "Online Epsilon Net and Piercing Set for Geometric Concepts", "author": "Sujoy Bhore and Devdan Dey and Satyam Singh", "abstract": "  VC-dimension and $\\varepsilon$-nets are key concepts in Statistical Learning\nTheory. Intuitively, VC-dimension is a measure of the size of a class of sets.\nThe famous $\\varepsilon$-net theorem, a fundamental result in Discrete\nGeometry, asserts that if the VC-dimension of a set system is bounded, then a\nsmall sample exists that intersects all sufficiently large sets.\n  In online learning scenarios where data arrives sequentially, the\nVC-dimension helps to bound the complexity of the set system, and\n$\\varepsilon$-nets ensure the selection of a small representative set. This\nsampling framework is crucial in various domains, including spatial data\nanalysis, motion planning in dynamic environments, optimization of sensor\nnetworks, and feature extraction in computer vision, among others. Motivated by\nthese applications, we study the online $\\varepsilon$-net problem for geometric\nconcepts with bounded VC-dimension. While the offline version of this problem\nhas been extensively studied, surprisingly, there are no known theoretical\nresults for the online version to date. We present the first deterministic\nonline algorithm with an optimal competitive ratio for intervals in\n$\\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal\ncompetitive ratio for axis-aligned boxes in $\\mathbb{R}^d$, for $d\\le 3$.\nFurthermore, we introduce a novel technique to analyze similar-sized objects of\nconstant description complexity in $\\mathbb{R}^d$, which may be of independent\ninterest. Next, we focus on the continuous version of this problem, where\nranges of the set system are geometric concepts in $\\mathbb{R}^d$ arriving in\nan online manner, but the universe is the entire space, and the objective is to\nchoose a small sample that intersects all the ranges.\n", "link": "http://arxiv.org/abs/2410.07059v1", "date": "2024-10-09", "relevancy": 2.3443, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Epsilon%20Net%20and%20Piercing%20Set%20for%20Geometric%20Concepts&body=Title%3A%20Online%20Epsilon%20Net%20and%20Piercing%20Set%20for%20Geometric%20Concepts%0AAuthor%3A%20Sujoy%20Bhore%20and%20Devdan%20Dey%20and%20Satyam%20Singh%0AAbstract%3A%20%20%20VC-dimension%20and%20%24%5Cvarepsilon%24-nets%20are%20key%20concepts%20in%20Statistical%20Learning%0ATheory.%20Intuitively%2C%20VC-dimension%20is%20a%20measure%20of%20the%20size%20of%20a%20class%20of%20sets.%0AThe%20famous%20%24%5Cvarepsilon%24-net%20theorem%2C%20a%20fundamental%20result%20in%20Discrete%0AGeometry%2C%20asserts%20that%20if%20the%20VC-dimension%20of%20a%20set%20system%20is%20bounded%2C%20then%20a%0Asmall%20sample%20exists%20that%20intersects%20all%20sufficiently%20large%20sets.%0A%20%20In%20online%20learning%20scenarios%20where%20data%20arrives%20sequentially%2C%20the%0AVC-dimension%20helps%20to%20bound%20the%20complexity%20of%20the%20set%20system%2C%20and%0A%24%5Cvarepsilon%24-nets%20ensure%20the%20selection%20of%20a%20small%20representative%20set.%20This%0Asampling%20framework%20is%20crucial%20in%20various%20domains%2C%20including%20spatial%20data%0Aanalysis%2C%20motion%20planning%20in%20dynamic%20environments%2C%20optimization%20of%20sensor%0Anetworks%2C%20and%20feature%20extraction%20in%20computer%20vision%2C%20among%20others.%20Motivated%20by%0Athese%20applications%2C%20we%20study%20the%20online%20%24%5Cvarepsilon%24-net%20problem%20for%20geometric%0Aconcepts%20with%20bounded%20VC-dimension.%20While%20the%20offline%20version%20of%20this%20problem%0Ahas%20been%20extensively%20studied%2C%20surprisingly%2C%20there%20are%20no%20known%20theoretical%0Aresults%20for%20the%20online%20version%20to%20date.%20We%20present%20the%20first%20deterministic%0Aonline%20algorithm%20with%20an%20optimal%20competitive%20ratio%20for%20intervals%20in%0A%24%5Cmathbb%7BR%7D%24.%20Next%2C%20we%20give%20a%20randomized%20online%20algorithm%20with%20a%20near-optimal%0Acompetitive%20ratio%20for%20axis-aligned%20boxes%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20for%20%24d%5Cle%203%24.%0AFurthermore%2C%20we%20introduce%20a%20novel%20technique%20to%20analyze%20similar-sized%20objects%20of%0Aconstant%20description%20complexity%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20which%20may%20be%20of%20independent%0Ainterest.%20Next%2C%20we%20focus%20on%20the%20continuous%20version%20of%20this%20problem%2C%20where%0Aranges%20of%20the%20set%20system%20are%20geometric%20concepts%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20arriving%20in%0Aan%20online%20manner%2C%20but%20the%20universe%20is%20the%20entire%20space%2C%20and%20the%20objective%20is%20to%0Achoose%20a%20small%20sample%20that%20intersects%20all%20the%20ranges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Epsilon%2520Net%2520and%2520Piercing%2520Set%2520for%2520Geometric%2520Concepts%26entry.906535625%3DSujoy%2520Bhore%2520and%2520Devdan%2520Dey%2520and%2520Satyam%2520Singh%26entry.1292438233%3D%2520%2520VC-dimension%2520and%2520%2524%255Cvarepsilon%2524-nets%2520are%2520key%2520concepts%2520in%2520Statistical%2520Learning%250ATheory.%2520Intuitively%252C%2520VC-dimension%2520is%2520a%2520measure%2520of%2520the%2520size%2520of%2520a%2520class%2520of%2520sets.%250AThe%2520famous%2520%2524%255Cvarepsilon%2524-net%2520theorem%252C%2520a%2520fundamental%2520result%2520in%2520Discrete%250AGeometry%252C%2520asserts%2520that%2520if%2520the%2520VC-dimension%2520of%2520a%2520set%2520system%2520is%2520bounded%252C%2520then%2520a%250Asmall%2520sample%2520exists%2520that%2520intersects%2520all%2520sufficiently%2520large%2520sets.%250A%2520%2520In%2520online%2520learning%2520scenarios%2520where%2520data%2520arrives%2520sequentially%252C%2520the%250AVC-dimension%2520helps%2520to%2520bound%2520the%2520complexity%2520of%2520the%2520set%2520system%252C%2520and%250A%2524%255Cvarepsilon%2524-nets%2520ensure%2520the%2520selection%2520of%2520a%2520small%2520representative%2520set.%2520This%250Asampling%2520framework%2520is%2520crucial%2520in%2520various%2520domains%252C%2520including%2520spatial%2520data%250Aanalysis%252C%2520motion%2520planning%2520in%2520dynamic%2520environments%252C%2520optimization%2520of%2520sensor%250Anetworks%252C%2520and%2520feature%2520extraction%2520in%2520computer%2520vision%252C%2520among%2520others.%2520Motivated%2520by%250Athese%2520applications%252C%2520we%2520study%2520the%2520online%2520%2524%255Cvarepsilon%2524-net%2520problem%2520for%2520geometric%250Aconcepts%2520with%2520bounded%2520VC-dimension.%2520While%2520the%2520offline%2520version%2520of%2520this%2520problem%250Ahas%2520been%2520extensively%2520studied%252C%2520surprisingly%252C%2520there%2520are%2520no%2520known%2520theoretical%250Aresults%2520for%2520the%2520online%2520version%2520to%2520date.%2520We%2520present%2520the%2520first%2520deterministic%250Aonline%2520algorithm%2520with%2520an%2520optimal%2520competitive%2520ratio%2520for%2520intervals%2520in%250A%2524%255Cmathbb%257BR%257D%2524.%2520Next%252C%2520we%2520give%2520a%2520randomized%2520online%2520algorithm%2520with%2520a%2520near-optimal%250Acompetitive%2520ratio%2520for%2520axis-aligned%2520boxes%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520for%2520%2524d%255Cle%25203%2524.%250AFurthermore%252C%2520we%2520introduce%2520a%2520novel%2520technique%2520to%2520analyze%2520similar-sized%2520objects%2520of%250Aconstant%2520description%2520complexity%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520which%2520may%2520be%2520of%2520independent%250Ainterest.%2520Next%252C%2520we%2520focus%2520on%2520the%2520continuous%2520version%2520of%2520this%2520problem%252C%2520where%250Aranges%2520of%2520the%2520set%2520system%2520are%2520geometric%2520concepts%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%2520arriving%2520in%250Aan%2520online%2520manner%252C%2520but%2520the%2520universe%2520is%2520the%2520entire%2520space%252C%2520and%2520the%2520objective%2520is%2520to%250Achoose%2520a%2520small%2520sample%2520that%2520intersects%2520all%2520the%2520ranges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Epsilon%20Net%20and%20Piercing%20Set%20for%20Geometric%20Concepts&entry.906535625=Sujoy%20Bhore%20and%20Devdan%20Dey%20and%20Satyam%20Singh&entry.1292438233=%20%20VC-dimension%20and%20%24%5Cvarepsilon%24-nets%20are%20key%20concepts%20in%20Statistical%20Learning%0ATheory.%20Intuitively%2C%20VC-dimension%20is%20a%20measure%20of%20the%20size%20of%20a%20class%20of%20sets.%0AThe%20famous%20%24%5Cvarepsilon%24-net%20theorem%2C%20a%20fundamental%20result%20in%20Discrete%0AGeometry%2C%20asserts%20that%20if%20the%20VC-dimension%20of%20a%20set%20system%20is%20bounded%2C%20then%20a%0Asmall%20sample%20exists%20that%20intersects%20all%20sufficiently%20large%20sets.%0A%20%20In%20online%20learning%20scenarios%20where%20data%20arrives%20sequentially%2C%20the%0AVC-dimension%20helps%20to%20bound%20the%20complexity%20of%20the%20set%20system%2C%20and%0A%24%5Cvarepsilon%24-nets%20ensure%20the%20selection%20of%20a%20small%20representative%20set.%20This%0Asampling%20framework%20is%20crucial%20in%20various%20domains%2C%20including%20spatial%20data%0Aanalysis%2C%20motion%20planning%20in%20dynamic%20environments%2C%20optimization%20of%20sensor%0Anetworks%2C%20and%20feature%20extraction%20in%20computer%20vision%2C%20among%20others.%20Motivated%20by%0Athese%20applications%2C%20we%20study%20the%20online%20%24%5Cvarepsilon%24-net%20problem%20for%20geometric%0Aconcepts%20with%20bounded%20VC-dimension.%20While%20the%20offline%20version%20of%20this%20problem%0Ahas%20been%20extensively%20studied%2C%20surprisingly%2C%20there%20are%20no%20known%20theoretical%0Aresults%20for%20the%20online%20version%20to%20date.%20We%20present%20the%20first%20deterministic%0Aonline%20algorithm%20with%20an%20optimal%20competitive%20ratio%20for%20intervals%20in%0A%24%5Cmathbb%7BR%7D%24.%20Next%2C%20we%20give%20a%20randomized%20online%20algorithm%20with%20a%20near-optimal%0Acompetitive%20ratio%20for%20axis-aligned%20boxes%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20for%20%24d%5Cle%203%24.%0AFurthermore%2C%20we%20introduce%20a%20novel%20technique%20to%20analyze%20similar-sized%20objects%20of%0Aconstant%20description%20complexity%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20which%20may%20be%20of%20independent%0Ainterest.%20Next%2C%20we%20focus%20on%20the%20continuous%20version%20of%20this%20problem%2C%20where%0Aranges%20of%20the%20set%20system%20are%20geometric%20concepts%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20arriving%20in%0Aan%20online%20manner%2C%20but%20the%20universe%20is%20the%20entire%20space%2C%20and%20the%20objective%20is%20to%0Achoose%20a%20small%20sample%20that%20intersects%20all%20the%20ranges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07059v1&entry.124074799=Read"},
{"title": "Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR\n  Semantic Segmentation", "author": "Seungho Lee and Hwijeong Lee and Hyunjung Shim", "abstract": "  We address the challenges of the semi-supervised LiDAR segmentation (SSLS)\nproblem, particularly in low-budget scenarios. The two main issues in\nlow-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the\nperformance drops due to the significant imbalance between ground-truth and\npseudo-labels. This imbalance leads to a vicious training cycle. To overcome\nthese challenges, we leverage the spatio-temporal prior by recognizing the\nsubstantial overlap between temporally adjacent LiDAR scans. We propose a\nproximity-based label estimation, which generates highly accurate pseudo-labels\nfor unlabeled data by utilizing semantic consistency with adjacent labeled\ndata. Additionally, we enhance this method by progressively expanding the\npseudo-labels from the nearest unlabeled scans, which helps significantly\nreduce errors linked to dynamic classes. Additionally, we employ a dual-branch\nstructure to mitigate performance degradation caused by data imbalance.\nExperimental results demonstrate remarkable performance in low-budget settings\n(i.e., <= 5%) and meaningful improvements in normal budget settings (i.e., 5 -\n50%). Finally, our method has achieved new state-of-the-art results on\nSemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5%\nlabeled data, it offers competitive results against fully-supervised\ncounterparts. Moreover, it surpasses the performance of the previous\nstate-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data\n(76.0%) on nuScenes. The code is available on https://github.com/halbielee/PLE.\n", "link": "http://arxiv.org/abs/2410.06893v1", "date": "2024-10-09", "relevancy": 2.3427, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6011}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5769}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Spatio-temporal%20Correlation%20for%20Semi-Supervised%20LiDAR%0A%20%20Semantic%20Segmentation&body=Title%3A%20Learning%20from%20Spatio-temporal%20Correlation%20for%20Semi-Supervised%20LiDAR%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Seungho%20Lee%20and%20Hwijeong%20Lee%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20We%20address%20the%20challenges%20of%20the%20semi-supervised%20LiDAR%20segmentation%20%28SSLS%29%0Aproblem%2C%20particularly%20in%20low-budget%20scenarios.%20The%20two%20main%20issues%20in%0Alow-budget%20SSLS%20are%20the%20poor-quality%20pseudo-labels%20for%20unlabeled%20data%2C%20and%20the%0Aperformance%20drops%20due%20to%20the%20significant%20imbalance%20between%20ground-truth%20and%0Apseudo-labels.%20This%20imbalance%20leads%20to%20a%20vicious%20training%20cycle.%20To%20overcome%0Athese%20challenges%2C%20we%20leverage%20the%20spatio-temporal%20prior%20by%20recognizing%20the%0Asubstantial%20overlap%20between%20temporally%20adjacent%20LiDAR%20scans.%20We%20propose%20a%0Aproximity-based%20label%20estimation%2C%20which%20generates%20highly%20accurate%20pseudo-labels%0Afor%20unlabeled%20data%20by%20utilizing%20semantic%20consistency%20with%20adjacent%20labeled%0Adata.%20Additionally%2C%20we%20enhance%20this%20method%20by%20progressively%20expanding%20the%0Apseudo-labels%20from%20the%20nearest%20unlabeled%20scans%2C%20which%20helps%20significantly%0Areduce%20errors%20linked%20to%20dynamic%20classes.%20Additionally%2C%20we%20employ%20a%20dual-branch%0Astructure%20to%20mitigate%20performance%20degradation%20caused%20by%20data%20imbalance.%0AExperimental%20results%20demonstrate%20remarkable%20performance%20in%20low-budget%20settings%0A%28i.e.%2C%20%3C%3D%205%25%29%20and%20meaningful%20improvements%20in%20normal%20budget%20settings%20%28i.e.%2C%205%20-%0A50%25%29.%20Finally%2C%20our%20method%20has%20achieved%20new%20state-of-the-art%20results%20on%0ASemanticKITTI%20and%20nuScenes%20in%20semi-supervised%20LiDAR%20segmentation.%20With%20only%205%25%0Alabeled%20data%2C%20it%20offers%20competitive%20results%20against%20fully-supervised%0Acounterparts.%20Moreover%2C%20it%20surpasses%20the%20performance%20of%20the%20previous%0Astate-of-the-art%20at%20100%25%20labeled%20data%20%2875.2%25%29%20using%20only%2020%25%20of%20labeled%20data%0A%2876.0%25%29%20on%20nuScenes.%20The%20code%20is%20available%20on%20https%3A//github.com/halbielee/PLE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Spatio-temporal%2520Correlation%2520for%2520Semi-Supervised%2520LiDAR%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DSeungho%2520Lee%2520and%2520Hwijeong%2520Lee%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenges%2520of%2520the%2520semi-supervised%2520LiDAR%2520segmentation%2520%2528SSLS%2529%250Aproblem%252C%2520particularly%2520in%2520low-budget%2520scenarios.%2520The%2520two%2520main%2520issues%2520in%250Alow-budget%2520SSLS%2520are%2520the%2520poor-quality%2520pseudo-labels%2520for%2520unlabeled%2520data%252C%2520and%2520the%250Aperformance%2520drops%2520due%2520to%2520the%2520significant%2520imbalance%2520between%2520ground-truth%2520and%250Apseudo-labels.%2520This%2520imbalance%2520leads%2520to%2520a%2520vicious%2520training%2520cycle.%2520To%2520overcome%250Athese%2520challenges%252C%2520we%2520leverage%2520the%2520spatio-temporal%2520prior%2520by%2520recognizing%2520the%250Asubstantial%2520overlap%2520between%2520temporally%2520adjacent%2520LiDAR%2520scans.%2520We%2520propose%2520a%250Aproximity-based%2520label%2520estimation%252C%2520which%2520generates%2520highly%2520accurate%2520pseudo-labels%250Afor%2520unlabeled%2520data%2520by%2520utilizing%2520semantic%2520consistency%2520with%2520adjacent%2520labeled%250Adata.%2520Additionally%252C%2520we%2520enhance%2520this%2520method%2520by%2520progressively%2520expanding%2520the%250Apseudo-labels%2520from%2520the%2520nearest%2520unlabeled%2520scans%252C%2520which%2520helps%2520significantly%250Areduce%2520errors%2520linked%2520to%2520dynamic%2520classes.%2520Additionally%252C%2520we%2520employ%2520a%2520dual-branch%250Astructure%2520to%2520mitigate%2520performance%2520degradation%2520caused%2520by%2520data%2520imbalance.%250AExperimental%2520results%2520demonstrate%2520remarkable%2520performance%2520in%2520low-budget%2520settings%250A%2528i.e.%252C%2520%253C%253D%25205%2525%2529%2520and%2520meaningful%2520improvements%2520in%2520normal%2520budget%2520settings%2520%2528i.e.%252C%25205%2520-%250A50%2525%2529.%2520Finally%252C%2520our%2520method%2520has%2520achieved%2520new%2520state-of-the-art%2520results%2520on%250ASemanticKITTI%2520and%2520nuScenes%2520in%2520semi-supervised%2520LiDAR%2520segmentation.%2520With%2520only%25205%2525%250Alabeled%2520data%252C%2520it%2520offers%2520competitive%2520results%2520against%2520fully-supervised%250Acounterparts.%2520Moreover%252C%2520it%2520surpasses%2520the%2520performance%2520of%2520the%2520previous%250Astate-of-the-art%2520at%2520100%2525%2520labeled%2520data%2520%252875.2%2525%2529%2520using%2520only%252020%2525%2520of%2520labeled%2520data%250A%252876.0%2525%2529%2520on%2520nuScenes.%2520The%2520code%2520is%2520available%2520on%2520https%253A//github.com/halbielee/PLE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Spatio-temporal%20Correlation%20for%20Semi-Supervised%20LiDAR%0A%20%20Semantic%20Segmentation&entry.906535625=Seungho%20Lee%20and%20Hwijeong%20Lee%20and%20Hyunjung%20Shim&entry.1292438233=%20%20We%20address%20the%20challenges%20of%20the%20semi-supervised%20LiDAR%20segmentation%20%28SSLS%29%0Aproblem%2C%20particularly%20in%20low-budget%20scenarios.%20The%20two%20main%20issues%20in%0Alow-budget%20SSLS%20are%20the%20poor-quality%20pseudo-labels%20for%20unlabeled%20data%2C%20and%20the%0Aperformance%20drops%20due%20to%20the%20significant%20imbalance%20between%20ground-truth%20and%0Apseudo-labels.%20This%20imbalance%20leads%20to%20a%20vicious%20training%20cycle.%20To%20overcome%0Athese%20challenges%2C%20we%20leverage%20the%20spatio-temporal%20prior%20by%20recognizing%20the%0Asubstantial%20overlap%20between%20temporally%20adjacent%20LiDAR%20scans.%20We%20propose%20a%0Aproximity-based%20label%20estimation%2C%20which%20generates%20highly%20accurate%20pseudo-labels%0Afor%20unlabeled%20data%20by%20utilizing%20semantic%20consistency%20with%20adjacent%20labeled%0Adata.%20Additionally%2C%20we%20enhance%20this%20method%20by%20progressively%20expanding%20the%0Apseudo-labels%20from%20the%20nearest%20unlabeled%20scans%2C%20which%20helps%20significantly%0Areduce%20errors%20linked%20to%20dynamic%20classes.%20Additionally%2C%20we%20employ%20a%20dual-branch%0Astructure%20to%20mitigate%20performance%20degradation%20caused%20by%20data%20imbalance.%0AExperimental%20results%20demonstrate%20remarkable%20performance%20in%20low-budget%20settings%0A%28i.e.%2C%20%3C%3D%205%25%29%20and%20meaningful%20improvements%20in%20normal%20budget%20settings%20%28i.e.%2C%205%20-%0A50%25%29.%20Finally%2C%20our%20method%20has%20achieved%20new%20state-of-the-art%20results%20on%0ASemanticKITTI%20and%20nuScenes%20in%20semi-supervised%20LiDAR%20segmentation.%20With%20only%205%25%0Alabeled%20data%2C%20it%20offers%20competitive%20results%20against%20fully-supervised%0Acounterparts.%20Moreover%2C%20it%20surpasses%20the%20performance%20of%20the%20previous%0Astate-of-the-art%20at%20100%25%20labeled%20data%20%2875.2%25%29%20using%20only%2020%25%20of%20labeled%20data%0A%2876.0%25%29%20on%20nuScenes.%20The%20code%20is%20available%20on%20https%3A//github.com/halbielee/PLE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06893v1&entry.124074799=Read"},
{"title": "MM-Ego: Towards Building Egocentric Multimodal LLMs", "author": "Hanrong Ye and Haotian Zhang and Erik Daxberger and Lin Chen and Zongyu Lin and Yanghao Li and Bowen Zhang and Haoxuan You and Dan Xu and Zhe Gan and Jiasen Lu and Yinfei Yang", "abstract": "  This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.\n", "link": "http://arxiv.org/abs/2410.07177v1", "date": "2024-10-09", "relevancy": 2.3354, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-Ego%3A%20Towards%20Building%20Egocentric%20Multimodal%20LLMs&body=Title%3A%20MM-Ego%3A%20Towards%20Building%20Egocentric%20Multimodal%20LLMs%0AAuthor%3A%20Hanrong%20Ye%20and%20Haotian%20Zhang%20and%20Erik%20Daxberger%20and%20Lin%20Chen%20and%20Zongyu%20Lin%20and%20Yanghao%20Li%20and%20Bowen%20Zhang%20and%20Haoxuan%20You%20and%20Dan%20Xu%20and%20Zhe%20Gan%20and%20Jiasen%20Lu%20and%20Yinfei%20Yang%0AAbstract%3A%20%20%20This%20research%20aims%20to%20comprehensively%20explore%20building%20a%20multimodal%0Afoundation%20model%20for%20egocentric%20video%20understanding.%20To%20achieve%20this%20goal%2C%20we%0Awork%20on%20three%20fronts.%20First%2C%20as%20there%20is%20a%20lack%20of%20QA%20data%20for%20egocentric%20video%0Aunderstanding%2C%20we%20develop%20a%20data%20engine%20that%20efficiently%20generates%207M%0Ahigh-quality%20QA%20samples%20for%20egocentric%20videos%20ranging%20from%2030%20seconds%20to%20one%0Ahour%20long%2C%20based%20on%20human-annotated%20data.%20This%20is%20currently%20the%20largest%0Aegocentric%20QA%20dataset.%20Second%2C%20we%20contribute%20a%20challenging%20egocentric%20QA%0Abenchmark%20with%20629%20videos%20and%207%2C026%20questions%20to%20evaluate%20the%20models%27%20ability%0Ain%20recognizing%20and%20memorizing%20visual%20details%20across%20videos%20of%20varying%20lengths.%0AWe%20introduce%20a%20new%20de-biasing%20evaluation%20method%20to%20help%20mitigate%20the%0Aunavoidable%20language%20bias%20present%20in%20the%20models%20being%20evaluated.%20Third%2C%20we%0Apropose%20a%20specialized%20multimodal%20architecture%20featuring%20a%20novel%20%22Memory%20Pointer%0APrompting%22%20mechanism.%20This%20design%20includes%20a%20global%20glimpse%20step%20to%20gain%20an%0Aoverarching%20understanding%20of%20the%20entire%20video%20and%20identify%20key%20visual%0Ainformation%2C%20followed%20by%20a%20fallback%20step%20that%20utilizes%20the%20key%20visual%0Ainformation%20to%20generate%20responses.%20This%20enables%20the%20model%20to%20more%20effectively%0Acomprehend%20extended%20video%20content.%20With%20the%20data%2C%20benchmark%2C%20and%20model%2C%20we%0Asuccessfully%20build%20MM-Ego%2C%20an%20egocentric%20multimodal%20LLM%20that%20shows%20powerful%0Aperformance%20on%20egocentric%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-Ego%253A%2520Towards%2520Building%2520Egocentric%2520Multimodal%2520LLMs%26entry.906535625%3DHanrong%2520Ye%2520and%2520Haotian%2520Zhang%2520and%2520Erik%2520Daxberger%2520and%2520Lin%2520Chen%2520and%2520Zongyu%2520Lin%2520and%2520Yanghao%2520Li%2520and%2520Bowen%2520Zhang%2520and%2520Haoxuan%2520You%2520and%2520Dan%2520Xu%2520and%2520Zhe%2520Gan%2520and%2520Jiasen%2520Lu%2520and%2520Yinfei%2520Yang%26entry.1292438233%3D%2520%2520This%2520research%2520aims%2520to%2520comprehensively%2520explore%2520building%2520a%2520multimodal%250Afoundation%2520model%2520for%2520egocentric%2520video%2520understanding.%2520To%2520achieve%2520this%2520goal%252C%2520we%250Awork%2520on%2520three%2520fronts.%2520First%252C%2520as%2520there%2520is%2520a%2520lack%2520of%2520QA%2520data%2520for%2520egocentric%2520video%250Aunderstanding%252C%2520we%2520develop%2520a%2520data%2520engine%2520that%2520efficiently%2520generates%25207M%250Ahigh-quality%2520QA%2520samples%2520for%2520egocentric%2520videos%2520ranging%2520from%252030%2520seconds%2520to%2520one%250Ahour%2520long%252C%2520based%2520on%2520human-annotated%2520data.%2520This%2520is%2520currently%2520the%2520largest%250Aegocentric%2520QA%2520dataset.%2520Second%252C%2520we%2520contribute%2520a%2520challenging%2520egocentric%2520QA%250Abenchmark%2520with%2520629%2520videos%2520and%25207%252C026%2520questions%2520to%2520evaluate%2520the%2520models%2527%2520ability%250Ain%2520recognizing%2520and%2520memorizing%2520visual%2520details%2520across%2520videos%2520of%2520varying%2520lengths.%250AWe%2520introduce%2520a%2520new%2520de-biasing%2520evaluation%2520method%2520to%2520help%2520mitigate%2520the%250Aunavoidable%2520language%2520bias%2520present%2520in%2520the%2520models%2520being%2520evaluated.%2520Third%252C%2520we%250Apropose%2520a%2520specialized%2520multimodal%2520architecture%2520featuring%2520a%2520novel%2520%2522Memory%2520Pointer%250APrompting%2522%2520mechanism.%2520This%2520design%2520includes%2520a%2520global%2520glimpse%2520step%2520to%2520gain%2520an%250Aoverarching%2520understanding%2520of%2520the%2520entire%2520video%2520and%2520identify%2520key%2520visual%250Ainformation%252C%2520followed%2520by%2520a%2520fallback%2520step%2520that%2520utilizes%2520the%2520key%2520visual%250Ainformation%2520to%2520generate%2520responses.%2520This%2520enables%2520the%2520model%2520to%2520more%2520effectively%250Acomprehend%2520extended%2520video%2520content.%2520With%2520the%2520data%252C%2520benchmark%252C%2520and%2520model%252C%2520we%250Asuccessfully%2520build%2520MM-Ego%252C%2520an%2520egocentric%2520multimodal%2520LLM%2520that%2520shows%2520powerful%250Aperformance%2520on%2520egocentric%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Ego%3A%20Towards%20Building%20Egocentric%20Multimodal%20LLMs&entry.906535625=Hanrong%20Ye%20and%20Haotian%20Zhang%20and%20Erik%20Daxberger%20and%20Lin%20Chen%20and%20Zongyu%20Lin%20and%20Yanghao%20Li%20and%20Bowen%20Zhang%20and%20Haoxuan%20You%20and%20Dan%20Xu%20and%20Zhe%20Gan%20and%20Jiasen%20Lu%20and%20Yinfei%20Yang&entry.1292438233=%20%20This%20research%20aims%20to%20comprehensively%20explore%20building%20a%20multimodal%0Afoundation%20model%20for%20egocentric%20video%20understanding.%20To%20achieve%20this%20goal%2C%20we%0Awork%20on%20three%20fronts.%20First%2C%20as%20there%20is%20a%20lack%20of%20QA%20data%20for%20egocentric%20video%0Aunderstanding%2C%20we%20develop%20a%20data%20engine%20that%20efficiently%20generates%207M%0Ahigh-quality%20QA%20samples%20for%20egocentric%20videos%20ranging%20from%2030%20seconds%20to%20one%0Ahour%20long%2C%20based%20on%20human-annotated%20data.%20This%20is%20currently%20the%20largest%0Aegocentric%20QA%20dataset.%20Second%2C%20we%20contribute%20a%20challenging%20egocentric%20QA%0Abenchmark%20with%20629%20videos%20and%207%2C026%20questions%20to%20evaluate%20the%20models%27%20ability%0Ain%20recognizing%20and%20memorizing%20visual%20details%20across%20videos%20of%20varying%20lengths.%0AWe%20introduce%20a%20new%20de-biasing%20evaluation%20method%20to%20help%20mitigate%20the%0Aunavoidable%20language%20bias%20present%20in%20the%20models%20being%20evaluated.%20Third%2C%20we%0Apropose%20a%20specialized%20multimodal%20architecture%20featuring%20a%20novel%20%22Memory%20Pointer%0APrompting%22%20mechanism.%20This%20design%20includes%20a%20global%20glimpse%20step%20to%20gain%20an%0Aoverarching%20understanding%20of%20the%20entire%20video%20and%20identify%20key%20visual%0Ainformation%2C%20followed%20by%20a%20fallback%20step%20that%20utilizes%20the%20key%20visual%0Ainformation%20to%20generate%20responses.%20This%20enables%20the%20model%20to%20more%20effectively%0Acomprehend%20extended%20video%20content.%20With%20the%20data%2C%20benchmark%2C%20and%20model%2C%20we%0Asuccessfully%20build%20MM-Ego%2C%20an%20egocentric%20multimodal%20LLM%20that%20shows%20powerful%0Aperformance%20on%20egocentric%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07177v1&entry.124074799=Read"},
{"title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large\n  Vision-Language Models", "author": "Rui Zhao and Hangjie Yuan and Yujie Wei and Shiwei Zhang and Yuchao Gu and Lingmin Ran and Xiang Wang and Zhangjie Wu and Junhao Zhang and Yingya Zhang and Mike Zheng Shou", "abstract": "  Recent advancements in generation models have showcased remarkable\ncapabilities in generating fantastic content. However, most of them are trained\non proprietary high-quality data, and some models withhold their parameters and\nonly provide accessible application programming interfaces (APIs), limiting\ntheir benefits for downstream tasks. To explore the feasibility of training a\ntext-to-image generation model comparable to advanced models using publicly\navailable resources, we introduce EvolveDirector. This framework interacts with\nadvanced models through their public APIs to obtain text-image data pairs to\ntrain a base model. Our experiments with extensive data indicate that the model\ntrained on generated data of the advanced model can approximate its generation\ncapability. However, it requires large-scale samples of 10 million or more.\nThis incurs significant expenses in time, computational resources, and\nespecially the costs associated with calling fee-based APIs. To address this\nproblem, we leverage pre-trained large vision-language models (VLMs) to guide\nthe evolution of the base model. VLM continuously evaluates the base model\nduring training and dynamically updates and refines the training dataset by the\ndiscrimination, expansion, deletion, and mutation operations. Experimental\nresults show that this paradigm significantly reduces the required data volume.\nFurthermore, when approaching multiple advanced models, EvolveDirector can\nselect the best samples generated by them to learn powerful and balanced\nabilities. The final trained model Edgen is demonstrated to outperform these\nadvanced models. The code and model weights are available at\nhttps://github.com/showlab/EvolveDirector.\n", "link": "http://arxiv.org/abs/2410.07133v1", "date": "2024-10-09", "relevancy": 2.3333, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6027}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvolveDirector%3A%20Approaching%20Advanced%20Text-to-Image%20Generation%20with%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20EvolveDirector%3A%20Approaching%20Advanced%20Text-to-Image%20Generation%20with%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Rui%20Zhao%20and%20Hangjie%20Yuan%20and%20Yujie%20Wei%20and%20Shiwei%20Zhang%20and%20Yuchao%20Gu%20and%20Lingmin%20Ran%20and%20Xiang%20Wang%20and%20Zhangjie%20Wu%20and%20Junhao%20Zhang%20and%20Yingya%20Zhang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generation%20models%20have%20showcased%20remarkable%0Acapabilities%20in%20generating%20fantastic%20content.%20However%2C%20most%20of%20them%20are%20trained%0Aon%20proprietary%20high-quality%20data%2C%20and%20some%20models%20withhold%20their%20parameters%20and%0Aonly%20provide%20accessible%20application%20programming%20interfaces%20%28APIs%29%2C%20limiting%0Atheir%20benefits%20for%20downstream%20tasks.%20To%20explore%20the%20feasibility%20of%20training%20a%0Atext-to-image%20generation%20model%20comparable%20to%20advanced%20models%20using%20publicly%0Aavailable%20resources%2C%20we%20introduce%20EvolveDirector.%20This%20framework%20interacts%20with%0Aadvanced%20models%20through%20their%20public%20APIs%20to%20obtain%20text-image%20data%20pairs%20to%0Atrain%20a%20base%20model.%20Our%20experiments%20with%20extensive%20data%20indicate%20that%20the%20model%0Atrained%20on%20generated%20data%20of%20the%20advanced%20model%20can%20approximate%20its%20generation%0Acapability.%20However%2C%20it%20requires%20large-scale%20samples%20of%2010%20million%20or%20more.%0AThis%20incurs%20significant%20expenses%20in%20time%2C%20computational%20resources%2C%20and%0Aespecially%20the%20costs%20associated%20with%20calling%20fee-based%20APIs.%20To%20address%20this%0Aproblem%2C%20we%20leverage%20pre-trained%20large%20vision-language%20models%20%28VLMs%29%20to%20guide%0Athe%20evolution%20of%20the%20base%20model.%20VLM%20continuously%20evaluates%20the%20base%20model%0Aduring%20training%20and%20dynamically%20updates%20and%20refines%20the%20training%20dataset%20by%20the%0Adiscrimination%2C%20expansion%2C%20deletion%2C%20and%20mutation%20operations.%20Experimental%0Aresults%20show%20that%20this%20paradigm%20significantly%20reduces%20the%20required%20data%20volume.%0AFurthermore%2C%20when%20approaching%20multiple%20advanced%20models%2C%20EvolveDirector%20can%0Aselect%20the%20best%20samples%20generated%20by%20them%20to%20learn%20powerful%20and%20balanced%0Aabilities.%20The%20final%20trained%20model%20Edgen%20is%20demonstrated%20to%20outperform%20these%0Aadvanced%20models.%20The%20code%20and%20model%20weights%20are%20available%20at%0Ahttps%3A//github.com/showlab/EvolveDirector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolveDirector%253A%2520Approaching%2520Advanced%2520Text-to-Image%2520Generation%2520with%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DRui%2520Zhao%2520and%2520Hangjie%2520Yuan%2520and%2520Yujie%2520Wei%2520and%2520Shiwei%2520Zhang%2520and%2520Yuchao%2520Gu%2520and%2520Lingmin%2520Ran%2520and%2520Xiang%2520Wang%2520and%2520Zhangjie%2520Wu%2520and%2520Junhao%2520Zhang%2520and%2520Yingya%2520Zhang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generation%2520models%2520have%2520showcased%2520remarkable%250Acapabilities%2520in%2520generating%2520fantastic%2520content.%2520However%252C%2520most%2520of%2520them%2520are%2520trained%250Aon%2520proprietary%2520high-quality%2520data%252C%2520and%2520some%2520models%2520withhold%2520their%2520parameters%2520and%250Aonly%2520provide%2520accessible%2520application%2520programming%2520interfaces%2520%2528APIs%2529%252C%2520limiting%250Atheir%2520benefits%2520for%2520downstream%2520tasks.%2520To%2520explore%2520the%2520feasibility%2520of%2520training%2520a%250Atext-to-image%2520generation%2520model%2520comparable%2520to%2520advanced%2520models%2520using%2520publicly%250Aavailable%2520resources%252C%2520we%2520introduce%2520EvolveDirector.%2520This%2520framework%2520interacts%2520with%250Aadvanced%2520models%2520through%2520their%2520public%2520APIs%2520to%2520obtain%2520text-image%2520data%2520pairs%2520to%250Atrain%2520a%2520base%2520model.%2520Our%2520experiments%2520with%2520extensive%2520data%2520indicate%2520that%2520the%2520model%250Atrained%2520on%2520generated%2520data%2520of%2520the%2520advanced%2520model%2520can%2520approximate%2520its%2520generation%250Acapability.%2520However%252C%2520it%2520requires%2520large-scale%2520samples%2520of%252010%2520million%2520or%2520more.%250AThis%2520incurs%2520significant%2520expenses%2520in%2520time%252C%2520computational%2520resources%252C%2520and%250Aespecially%2520the%2520costs%2520associated%2520with%2520calling%2520fee-based%2520APIs.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520leverage%2520pre-trained%2520large%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520guide%250Athe%2520evolution%2520of%2520the%2520base%2520model.%2520VLM%2520continuously%2520evaluates%2520the%2520base%2520model%250Aduring%2520training%2520and%2520dynamically%2520updates%2520and%2520refines%2520the%2520training%2520dataset%2520by%2520the%250Adiscrimination%252C%2520expansion%252C%2520deletion%252C%2520and%2520mutation%2520operations.%2520Experimental%250Aresults%2520show%2520that%2520this%2520paradigm%2520significantly%2520reduces%2520the%2520required%2520data%2520volume.%250AFurthermore%252C%2520when%2520approaching%2520multiple%2520advanced%2520models%252C%2520EvolveDirector%2520can%250Aselect%2520the%2520best%2520samples%2520generated%2520by%2520them%2520to%2520learn%2520powerful%2520and%2520balanced%250Aabilities.%2520The%2520final%2520trained%2520model%2520Edgen%2520is%2520demonstrated%2520to%2520outperform%2520these%250Aadvanced%2520models.%2520The%2520code%2520and%2520model%2520weights%2520are%2520available%2520at%250Ahttps%253A//github.com/showlab/EvolveDirector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvolveDirector%3A%20Approaching%20Advanced%20Text-to-Image%20Generation%20with%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Rui%20Zhao%20and%20Hangjie%20Yuan%20and%20Yujie%20Wei%20and%20Shiwei%20Zhang%20and%20Yuchao%20Gu%20and%20Lingmin%20Ran%20and%20Xiang%20Wang%20and%20Zhangjie%20Wu%20and%20Junhao%20Zhang%20and%20Yingya%20Zhang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Recent%20advancements%20in%20generation%20models%20have%20showcased%20remarkable%0Acapabilities%20in%20generating%20fantastic%20content.%20However%2C%20most%20of%20them%20are%20trained%0Aon%20proprietary%20high-quality%20data%2C%20and%20some%20models%20withhold%20their%20parameters%20and%0Aonly%20provide%20accessible%20application%20programming%20interfaces%20%28APIs%29%2C%20limiting%0Atheir%20benefits%20for%20downstream%20tasks.%20To%20explore%20the%20feasibility%20of%20training%20a%0Atext-to-image%20generation%20model%20comparable%20to%20advanced%20models%20using%20publicly%0Aavailable%20resources%2C%20we%20introduce%20EvolveDirector.%20This%20framework%20interacts%20with%0Aadvanced%20models%20through%20their%20public%20APIs%20to%20obtain%20text-image%20data%20pairs%20to%0Atrain%20a%20base%20model.%20Our%20experiments%20with%20extensive%20data%20indicate%20that%20the%20model%0Atrained%20on%20generated%20data%20of%20the%20advanced%20model%20can%20approximate%20its%20generation%0Acapability.%20However%2C%20it%20requires%20large-scale%20samples%20of%2010%20million%20or%20more.%0AThis%20incurs%20significant%20expenses%20in%20time%2C%20computational%20resources%2C%20and%0Aespecially%20the%20costs%20associated%20with%20calling%20fee-based%20APIs.%20To%20address%20this%0Aproblem%2C%20we%20leverage%20pre-trained%20large%20vision-language%20models%20%28VLMs%29%20to%20guide%0Athe%20evolution%20of%20the%20base%20model.%20VLM%20continuously%20evaluates%20the%20base%20model%0Aduring%20training%20and%20dynamically%20updates%20and%20refines%20the%20training%20dataset%20by%20the%0Adiscrimination%2C%20expansion%2C%20deletion%2C%20and%20mutation%20operations.%20Experimental%0Aresults%20show%20that%20this%20paradigm%20significantly%20reduces%20the%20required%20data%20volume.%0AFurthermore%2C%20when%20approaching%20multiple%20advanced%20models%2C%20EvolveDirector%20can%0Aselect%20the%20best%20samples%20generated%20by%20them%20to%20learn%20powerful%20and%20balanced%0Aabilities.%20The%20final%20trained%20model%20Edgen%20is%20demonstrated%20to%20outperform%20these%0Aadvanced%20models.%20The%20code%20and%20model%20weights%20are%20available%20at%0Ahttps%3A//github.com/showlab/EvolveDirector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07133v1&entry.124074799=Read"},
{"title": "From Pixels to Tokens: Revisiting Object Hallucinations in Large\n  Vision-Language Models", "author": "Yuying Shang and Xinyi Zeng and Yutao Zhu and Xiao Yang and Zhengwei Fang and Jingyuan Zhang and Jiawei Chen and Zinan Liu and Yu Tian", "abstract": "  Hallucinations in large vision-language models (LVLMs) are a significant\nchallenge, i.e., generating objects that are not presented in the visual input,\nwhich impairs their reliability. Recent studies often attribute hallucinations\nto a lack of understanding of visual input, yet ignore a more fundamental\nissue: the model's inability to effectively extract or decouple visual\nfeatures. In this paper, we revisit the hallucinations in LVLMs from an\narchitectural perspective, investigating whether the primary cause lies in the\nvisual encoder (feature extraction) or the modal alignment module (feature\ndecoupling). Motivated by our findings on the preliminary investigation, we\npropose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.\nThis plug-and-play method can be integrated into various LVLMs, utilizing\nadaptive virtual tokens to extract object features from bounding boxes, thereby\naddressing hallucinations caused by insufficient decoupling of visual features.\nPATCH achieves state-of-the-art performance on multiple multi-modal\nhallucination datasets. We hope this approach provides researchers with deeper\ninsights into the underlying causes of hallucinations in LVLMs, fostering\nfurther advancements and innovation in this field.\n", "link": "http://arxiv.org/abs/2410.06795v1", "date": "2024-10-09", "relevancy": 2.3317, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Pixels%20to%20Tokens%3A%20Revisiting%20Object%20Hallucinations%20in%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20From%20Pixels%20to%20Tokens%3A%20Revisiting%20Object%20Hallucinations%20in%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Yuying%20Shang%20and%20Xinyi%20Zeng%20and%20Yutao%20Zhu%20and%20Xiao%20Yang%20and%20Zhengwei%20Fang%20and%20Jingyuan%20Zhang%20and%20Jiawei%20Chen%20and%20Zinan%20Liu%20and%20Yu%20Tian%0AAbstract%3A%20%20%20Hallucinations%20in%20large%20vision-language%20models%20%28LVLMs%29%20are%20a%20significant%0Achallenge%2C%20i.e.%2C%20generating%20objects%20that%20are%20not%20presented%20in%20the%20visual%20input%2C%0Awhich%20impairs%20their%20reliability.%20Recent%20studies%20often%20attribute%20hallucinations%0Ato%20a%20lack%20of%20understanding%20of%20visual%20input%2C%20yet%20ignore%20a%20more%20fundamental%0Aissue%3A%20the%20model%27s%20inability%20to%20effectively%20extract%20or%20decouple%20visual%0Afeatures.%20In%20this%20paper%2C%20we%20revisit%20the%20hallucinations%20in%20LVLMs%20from%20an%0Aarchitectural%20perspective%2C%20investigating%20whether%20the%20primary%20cause%20lies%20in%20the%0Avisual%20encoder%20%28feature%20extraction%29%20or%20the%20modal%20alignment%20module%20%28feature%0Adecoupling%29.%20Motivated%20by%20our%20findings%20on%20the%20preliminary%20investigation%2C%20we%0Apropose%20a%20novel%20tuning%20strategy%2C%20PATCH%2C%20to%20mitigate%20hallucinations%20in%20LVLMs.%0AThis%20plug-and-play%20method%20can%20be%20integrated%20into%20various%20LVLMs%2C%20utilizing%0Aadaptive%20virtual%20tokens%20to%20extract%20object%20features%20from%20bounding%20boxes%2C%20thereby%0Aaddressing%20hallucinations%20caused%20by%20insufficient%20decoupling%20of%20visual%20features.%0APATCH%20achieves%20state-of-the-art%20performance%20on%20multiple%20multi-modal%0Ahallucination%20datasets.%20We%20hope%20this%20approach%20provides%20researchers%20with%20deeper%0Ainsights%20into%20the%20underlying%20causes%20of%20hallucinations%20in%20LVLMs%2C%20fostering%0Afurther%20advancements%20and%20innovation%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Pixels%2520to%2520Tokens%253A%2520Revisiting%2520Object%2520Hallucinations%2520in%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DYuying%2520Shang%2520and%2520Xinyi%2520Zeng%2520and%2520Yutao%2520Zhu%2520and%2520Xiao%2520Yang%2520and%2520Zhengwei%2520Fang%2520and%2520Jingyuan%2520Zhang%2520and%2520Jiawei%2520Chen%2520and%2520Zinan%2520Liu%2520and%2520Yu%2520Tian%26entry.1292438233%3D%2520%2520Hallucinations%2520in%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520a%2520significant%250Achallenge%252C%2520i.e.%252C%2520generating%2520objects%2520that%2520are%2520not%2520presented%2520in%2520the%2520visual%2520input%252C%250Awhich%2520impairs%2520their%2520reliability.%2520Recent%2520studies%2520often%2520attribute%2520hallucinations%250Ato%2520a%2520lack%2520of%2520understanding%2520of%2520visual%2520input%252C%2520yet%2520ignore%2520a%2520more%2520fundamental%250Aissue%253A%2520the%2520model%2527s%2520inability%2520to%2520effectively%2520extract%2520or%2520decouple%2520visual%250Afeatures.%2520In%2520this%2520paper%252C%2520we%2520revisit%2520the%2520hallucinations%2520in%2520LVLMs%2520from%2520an%250Aarchitectural%2520perspective%252C%2520investigating%2520whether%2520the%2520primary%2520cause%2520lies%2520in%2520the%250Avisual%2520encoder%2520%2528feature%2520extraction%2529%2520or%2520the%2520modal%2520alignment%2520module%2520%2528feature%250Adecoupling%2529.%2520Motivated%2520by%2520our%2520findings%2520on%2520the%2520preliminary%2520investigation%252C%2520we%250Apropose%2520a%2520novel%2520tuning%2520strategy%252C%2520PATCH%252C%2520to%2520mitigate%2520hallucinations%2520in%2520LVLMs.%250AThis%2520plug-and-play%2520method%2520can%2520be%2520integrated%2520into%2520various%2520LVLMs%252C%2520utilizing%250Aadaptive%2520virtual%2520tokens%2520to%2520extract%2520object%2520features%2520from%2520bounding%2520boxes%252C%2520thereby%250Aaddressing%2520hallucinations%2520caused%2520by%2520insufficient%2520decoupling%2520of%2520visual%2520features.%250APATCH%2520achieves%2520state-of-the-art%2520performance%2520on%2520multiple%2520multi-modal%250Ahallucination%2520datasets.%2520We%2520hope%2520this%2520approach%2520provides%2520researchers%2520with%2520deeper%250Ainsights%2520into%2520the%2520underlying%2520causes%2520of%2520hallucinations%2520in%2520LVLMs%252C%2520fostering%250Afurther%2520advancements%2520and%2520innovation%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Pixels%20to%20Tokens%3A%20Revisiting%20Object%20Hallucinations%20in%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Yuying%20Shang%20and%20Xinyi%20Zeng%20and%20Yutao%20Zhu%20and%20Xiao%20Yang%20and%20Zhengwei%20Fang%20and%20Jingyuan%20Zhang%20and%20Jiawei%20Chen%20and%20Zinan%20Liu%20and%20Yu%20Tian&entry.1292438233=%20%20Hallucinations%20in%20large%20vision-language%20models%20%28LVLMs%29%20are%20a%20significant%0Achallenge%2C%20i.e.%2C%20generating%20objects%20that%20are%20not%20presented%20in%20the%20visual%20input%2C%0Awhich%20impairs%20their%20reliability.%20Recent%20studies%20often%20attribute%20hallucinations%0Ato%20a%20lack%20of%20understanding%20of%20visual%20input%2C%20yet%20ignore%20a%20more%20fundamental%0Aissue%3A%20the%20model%27s%20inability%20to%20effectively%20extract%20or%20decouple%20visual%0Afeatures.%20In%20this%20paper%2C%20we%20revisit%20the%20hallucinations%20in%20LVLMs%20from%20an%0Aarchitectural%20perspective%2C%20investigating%20whether%20the%20primary%20cause%20lies%20in%20the%0Avisual%20encoder%20%28feature%20extraction%29%20or%20the%20modal%20alignment%20module%20%28feature%0Adecoupling%29.%20Motivated%20by%20our%20findings%20on%20the%20preliminary%20investigation%2C%20we%0Apropose%20a%20novel%20tuning%20strategy%2C%20PATCH%2C%20to%20mitigate%20hallucinations%20in%20LVLMs.%0AThis%20plug-and-play%20method%20can%20be%20integrated%20into%20various%20LVLMs%2C%20utilizing%0Aadaptive%20virtual%20tokens%20to%20extract%20object%20features%20from%20bounding%20boxes%2C%20thereby%0Aaddressing%20hallucinations%20caused%20by%20insufficient%20decoupling%20of%20visual%20features.%0APATCH%20achieves%20state-of-the-art%20performance%20on%20multiple%20multi-modal%0Ahallucination%20datasets.%20We%20hope%20this%20approach%20provides%20researchers%20with%20deeper%0Ainsights%20into%20the%20underlying%20causes%20of%20hallucinations%20in%20LVLMs%2C%20fostering%0Afurther%20advancements%20and%20innovation%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06795v1&entry.124074799=Read"},
{"title": "Degree Distribution based Spiking Graph Networks for Domain Adaptation", "author": "Yingxu Wang and Siwei Liu and Mengzhu Wang and Shangsong Liang and Nan Yin", "abstract": "  Spiking Graph Networks (SGNs) have garnered significant attraction from both\nresearchers and industry due to their ability to address energy consumption\nchallenges in graph classification. However, SGNs are only effective for\nin-distribution data and cannot tackle out-of-distribution data. In this paper,\nwe first propose the domain adaptation problem in SGNs, and introduce a novel\nframework named Degree-aware Spiking Graph Domain Adaptation for\nClassification. The proposed DeSGDA addresses the spiking graph domain\nadaptation problem by three aspects: node degree-aware personalized spiking\nrepresentation, adversarial feature distribution alignment, and pseudo-label\ndistillation. First, we introduce the personalized spiking representation\nmethod for generating degree-dependent spiking signals. Specifically, the\nthreshold of triggering a spike is determined by the node degree, allowing this\npersonalized approach to capture more expressive information for\nclassification. Then, we propose the graph feature distribution alignment\nmodule that is adversarially trained using membrane potential against a domain\ndiscriminator. Such an alignment module can efficiently maintain high\nperformance and low energy consumption in the case of inconsistent\ndistribution. Additionally, we extract consistent predictions across two spaces\nto create reliable pseudo-labels, effectively leveraging unlabeled data to\nenhance graph classification performance. Extensive experiments on benchmark\ndatasets validate the superiority of the proposed DeSGDA compared with\ncompetitive baselines.\n", "link": "http://arxiv.org/abs/2410.06883v1", "date": "2024-10-09", "relevancy": 2.322, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4664}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4642}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degree%20Distribution%20based%20Spiking%20Graph%20Networks%20for%20Domain%20Adaptation&body=Title%3A%20Degree%20Distribution%20based%20Spiking%20Graph%20Networks%20for%20Domain%20Adaptation%0AAuthor%3A%20Yingxu%20Wang%20and%20Siwei%20Liu%20and%20Mengzhu%20Wang%20and%20Shangsong%20Liang%20and%20Nan%20Yin%0AAbstract%3A%20%20%20Spiking%20Graph%20Networks%20%28SGNs%29%20have%20garnered%20significant%20attraction%20from%20both%0Aresearchers%20and%20industry%20due%20to%20their%20ability%20to%20address%20energy%20consumption%0Achallenges%20in%20graph%20classification.%20However%2C%20SGNs%20are%20only%20effective%20for%0Ain-distribution%20data%20and%20cannot%20tackle%20out-of-distribution%20data.%20In%20this%20paper%2C%0Awe%20first%20propose%20the%20domain%20adaptation%20problem%20in%20SGNs%2C%20and%20introduce%20a%20novel%0Aframework%20named%20Degree-aware%20Spiking%20Graph%20Domain%20Adaptation%20for%0AClassification.%20The%20proposed%20DeSGDA%20addresses%20the%20spiking%20graph%20domain%0Aadaptation%20problem%20by%20three%20aspects%3A%20node%20degree-aware%20personalized%20spiking%0Arepresentation%2C%20adversarial%20feature%20distribution%20alignment%2C%20and%20pseudo-label%0Adistillation.%20First%2C%20we%20introduce%20the%20personalized%20spiking%20representation%0Amethod%20for%20generating%20degree-dependent%20spiking%20signals.%20Specifically%2C%20the%0Athreshold%20of%20triggering%20a%20spike%20is%20determined%20by%20the%20node%20degree%2C%20allowing%20this%0Apersonalized%20approach%20to%20capture%20more%20expressive%20information%20for%0Aclassification.%20Then%2C%20we%20propose%20the%20graph%20feature%20distribution%20alignment%0Amodule%20that%20is%20adversarially%20trained%20using%20membrane%20potential%20against%20a%20domain%0Adiscriminator.%20Such%20an%20alignment%20module%20can%20efficiently%20maintain%20high%0Aperformance%20and%20low%20energy%20consumption%20in%20the%20case%20of%20inconsistent%0Adistribution.%20Additionally%2C%20we%20extract%20consistent%20predictions%20across%20two%20spaces%0Ato%20create%20reliable%20pseudo-labels%2C%20effectively%20leveraging%20unlabeled%20data%20to%0Aenhance%20graph%20classification%20performance.%20Extensive%20experiments%20on%20benchmark%0Adatasets%20validate%20the%20superiority%20of%20the%20proposed%20DeSGDA%20compared%20with%0Acompetitive%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegree%2520Distribution%2520based%2520Spiking%2520Graph%2520Networks%2520for%2520Domain%2520Adaptation%26entry.906535625%3DYingxu%2520Wang%2520and%2520Siwei%2520Liu%2520and%2520Mengzhu%2520Wang%2520and%2520Shangsong%2520Liang%2520and%2520Nan%2520Yin%26entry.1292438233%3D%2520%2520Spiking%2520Graph%2520Networks%2520%2528SGNs%2529%2520have%2520garnered%2520significant%2520attraction%2520from%2520both%250Aresearchers%2520and%2520industry%2520due%2520to%2520their%2520ability%2520to%2520address%2520energy%2520consumption%250Achallenges%2520in%2520graph%2520classification.%2520However%252C%2520SGNs%2520are%2520only%2520effective%2520for%250Ain-distribution%2520data%2520and%2520cannot%2520tackle%2520out-of-distribution%2520data.%2520In%2520this%2520paper%252C%250Awe%2520first%2520propose%2520the%2520domain%2520adaptation%2520problem%2520in%2520SGNs%252C%2520and%2520introduce%2520a%2520novel%250Aframework%2520named%2520Degree-aware%2520Spiking%2520Graph%2520Domain%2520Adaptation%2520for%250AClassification.%2520The%2520proposed%2520DeSGDA%2520addresses%2520the%2520spiking%2520graph%2520domain%250Aadaptation%2520problem%2520by%2520three%2520aspects%253A%2520node%2520degree-aware%2520personalized%2520spiking%250Arepresentation%252C%2520adversarial%2520feature%2520distribution%2520alignment%252C%2520and%2520pseudo-label%250Adistillation.%2520First%252C%2520we%2520introduce%2520the%2520personalized%2520spiking%2520representation%250Amethod%2520for%2520generating%2520degree-dependent%2520spiking%2520signals.%2520Specifically%252C%2520the%250Athreshold%2520of%2520triggering%2520a%2520spike%2520is%2520determined%2520by%2520the%2520node%2520degree%252C%2520allowing%2520this%250Apersonalized%2520approach%2520to%2520capture%2520more%2520expressive%2520information%2520for%250Aclassification.%2520Then%252C%2520we%2520propose%2520the%2520graph%2520feature%2520distribution%2520alignment%250Amodule%2520that%2520is%2520adversarially%2520trained%2520using%2520membrane%2520potential%2520against%2520a%2520domain%250Adiscriminator.%2520Such%2520an%2520alignment%2520module%2520can%2520efficiently%2520maintain%2520high%250Aperformance%2520and%2520low%2520energy%2520consumption%2520in%2520the%2520case%2520of%2520inconsistent%250Adistribution.%2520Additionally%252C%2520we%2520extract%2520consistent%2520predictions%2520across%2520two%2520spaces%250Ato%2520create%2520reliable%2520pseudo-labels%252C%2520effectively%2520leveraging%2520unlabeled%2520data%2520to%250Aenhance%2520graph%2520classification%2520performance.%2520Extensive%2520experiments%2520on%2520benchmark%250Adatasets%2520validate%2520the%2520superiority%2520of%2520the%2520proposed%2520DeSGDA%2520compared%2520with%250Acompetitive%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degree%20Distribution%20based%20Spiking%20Graph%20Networks%20for%20Domain%20Adaptation&entry.906535625=Yingxu%20Wang%20and%20Siwei%20Liu%20and%20Mengzhu%20Wang%20and%20Shangsong%20Liang%20and%20Nan%20Yin&entry.1292438233=%20%20Spiking%20Graph%20Networks%20%28SGNs%29%20have%20garnered%20significant%20attraction%20from%20both%0Aresearchers%20and%20industry%20due%20to%20their%20ability%20to%20address%20energy%20consumption%0Achallenges%20in%20graph%20classification.%20However%2C%20SGNs%20are%20only%20effective%20for%0Ain-distribution%20data%20and%20cannot%20tackle%20out-of-distribution%20data.%20In%20this%20paper%2C%0Awe%20first%20propose%20the%20domain%20adaptation%20problem%20in%20SGNs%2C%20and%20introduce%20a%20novel%0Aframework%20named%20Degree-aware%20Spiking%20Graph%20Domain%20Adaptation%20for%0AClassification.%20The%20proposed%20DeSGDA%20addresses%20the%20spiking%20graph%20domain%0Aadaptation%20problem%20by%20three%20aspects%3A%20node%20degree-aware%20personalized%20spiking%0Arepresentation%2C%20adversarial%20feature%20distribution%20alignment%2C%20and%20pseudo-label%0Adistillation.%20First%2C%20we%20introduce%20the%20personalized%20spiking%20representation%0Amethod%20for%20generating%20degree-dependent%20spiking%20signals.%20Specifically%2C%20the%0Athreshold%20of%20triggering%20a%20spike%20is%20determined%20by%20the%20node%20degree%2C%20allowing%20this%0Apersonalized%20approach%20to%20capture%20more%20expressive%20information%20for%0Aclassification.%20Then%2C%20we%20propose%20the%20graph%20feature%20distribution%20alignment%0Amodule%20that%20is%20adversarially%20trained%20using%20membrane%20potential%20against%20a%20domain%0Adiscriminator.%20Such%20an%20alignment%20module%20can%20efficiently%20maintain%20high%0Aperformance%20and%20low%20energy%20consumption%20in%20the%20case%20of%20inconsistent%0Adistribution.%20Additionally%2C%20we%20extract%20consistent%20predictions%20across%20two%20spaces%0Ato%20create%20reliable%20pseudo-labels%2C%20effectively%20leveraging%20unlabeled%20data%20to%0Aenhance%20graph%20classification%20performance.%20Extensive%20experiments%20on%20benchmark%0Adatasets%20validate%20the%20superiority%20of%20the%20proposed%20DeSGDA%20compared%20with%0Acompetitive%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06883v1&entry.124074799=Read"},
{"title": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling", "author": "Deok-Kyeong Jang and Dongseok Yang and Deok-Yun Jang and Byeoli Choi and Donghoon Shin and Sung-hee Lee", "abstract": "  This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}\n", "link": "http://arxiv.org/abs/2410.06963v1", "date": "2024-10-09", "relevancy": 2.3123, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.591}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5737}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELMO%3A%20Enhanced%20Real-time%20LiDAR%20Motion%20Capture%20through%20Upsampling&body=Title%3A%20ELMO%3A%20Enhanced%20Real-time%20LiDAR%20Motion%20Capture%20through%20Upsampling%0AAuthor%3A%20Deok-Kyeong%20Jang%20and%20Dongseok%20Yang%20and%20Deok-Yun%20Jang%20and%20Byeoli%20Choi%20and%20Donghoon%20Shin%20and%20Sung-hee%20Lee%0AAbstract%3A%20%20%20This%20paper%20introduces%20ELMO%2C%20a%20real-time%20upsampling%20motion%20capture%20framework%0Adesigned%20for%20a%20single%20LiDAR%20sensor.%20Modeled%20as%20a%20conditional%20autoregressive%0Atransformer-based%20upsampling%20motion%20generator%2C%20ELMO%20achieves%2060%20fps%20motion%0Acapture%20from%20a%2020%20fps%20LiDAR%20point%20cloud%20sequence.%20The%20key%20feature%20of%20ELMO%20is%0Athe%20coupling%20of%20the%20self-attention%20mechanism%20with%20thoughtfully%20designed%0Aembedding%20modules%20for%20motion%20and%20point%20clouds%2C%20significantly%20elevating%20the%0Amotion%20quality.%20To%20facilitate%20accurate%20motion%20capture%2C%20we%20develop%20a%20one-time%0Askeleton%20calibration%20model%20capable%20of%20predicting%20user%20skeleton%20offsets%20from%20a%0Asingle-frame%20point%20cloud.%20Additionally%2C%20we%20introduce%20a%20novel%20data%20augmentation%0Atechnique%20utilizing%20a%20LiDAR%20simulator%2C%20which%20enhances%20global%20root%20tracking%20to%0Aimprove%20environmental%20understanding.%20To%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%2C%20we%20compare%20ELMO%20with%20state-of-the-art%20methods%20in%20both%20image-based%20and%0Apoint%20cloud-based%20motion%20capture.%20We%20further%20conduct%20an%20ablation%20study%20to%0Avalidate%20our%20design%20principles.%20ELMO%27s%20fast%20inference%20time%20makes%20it%20well-suited%0Afor%20real-time%20applications%2C%20exemplified%20in%20our%20demo%20video%20featuring%20live%0Astreaming%20and%20interactive%20gaming%20scenarios.%20Furthermore%2C%20we%20contribute%20a%0Ahigh-quality%20LiDAR-mocap%20synchronized%20dataset%20comprising%2020%20different%20subjects%0Aperforming%20a%20range%20of%20motions%2C%20which%20can%20serve%20as%20a%20valuable%20resource%20for%0Afuture%20research.%20The%20dataset%20and%20evaluation%20code%20are%20available%20at%20%7B%5Cblue%0A%5Curl%7Bhttps%3A//movin3d.github.io/ELMO_SIGASIA2024/%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELMO%253A%2520Enhanced%2520Real-time%2520LiDAR%2520Motion%2520Capture%2520through%2520Upsampling%26entry.906535625%3DDeok-Kyeong%2520Jang%2520and%2520Dongseok%2520Yang%2520and%2520Deok-Yun%2520Jang%2520and%2520Byeoli%2520Choi%2520and%2520Donghoon%2520Shin%2520and%2520Sung-hee%2520Lee%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520ELMO%252C%2520a%2520real-time%2520upsampling%2520motion%2520capture%2520framework%250Adesigned%2520for%2520a%2520single%2520LiDAR%2520sensor.%2520Modeled%2520as%2520a%2520conditional%2520autoregressive%250Atransformer-based%2520upsampling%2520motion%2520generator%252C%2520ELMO%2520achieves%252060%2520fps%2520motion%250Acapture%2520from%2520a%252020%2520fps%2520LiDAR%2520point%2520cloud%2520sequence.%2520The%2520key%2520feature%2520of%2520ELMO%2520is%250Athe%2520coupling%2520of%2520the%2520self-attention%2520mechanism%2520with%2520thoughtfully%2520designed%250Aembedding%2520modules%2520for%2520motion%2520and%2520point%2520clouds%252C%2520significantly%2520elevating%2520the%250Amotion%2520quality.%2520To%2520facilitate%2520accurate%2520motion%2520capture%252C%2520we%2520develop%2520a%2520one-time%250Askeleton%2520calibration%2520model%2520capable%2520of%2520predicting%2520user%2520skeleton%2520offsets%2520from%2520a%250Asingle-frame%2520point%2520cloud.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520data%2520augmentation%250Atechnique%2520utilizing%2520a%2520LiDAR%2520simulator%252C%2520which%2520enhances%2520global%2520root%2520tracking%2520to%250Aimprove%2520environmental%2520understanding.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%252C%2520we%2520compare%2520ELMO%2520with%2520state-of-the-art%2520methods%2520in%2520both%2520image-based%2520and%250Apoint%2520cloud-based%2520motion%2520capture.%2520We%2520further%2520conduct%2520an%2520ablation%2520study%2520to%250Avalidate%2520our%2520design%2520principles.%2520ELMO%2527s%2520fast%2520inference%2520time%2520makes%2520it%2520well-suited%250Afor%2520real-time%2520applications%252C%2520exemplified%2520in%2520our%2520demo%2520video%2520featuring%2520live%250Astreaming%2520and%2520interactive%2520gaming%2520scenarios.%2520Furthermore%252C%2520we%2520contribute%2520a%250Ahigh-quality%2520LiDAR-mocap%2520synchronized%2520dataset%2520comprising%252020%2520different%2520subjects%250Aperforming%2520a%2520range%2520of%2520motions%252C%2520which%2520can%2520serve%2520as%2520a%2520valuable%2520resource%2520for%250Afuture%2520research.%2520The%2520dataset%2520and%2520evaluation%2520code%2520are%2520available%2520at%2520%257B%255Cblue%250A%255Curl%257Bhttps%253A//movin3d.github.io/ELMO_SIGASIA2024/%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELMO%3A%20Enhanced%20Real-time%20LiDAR%20Motion%20Capture%20through%20Upsampling&entry.906535625=Deok-Kyeong%20Jang%20and%20Dongseok%20Yang%20and%20Deok-Yun%20Jang%20and%20Byeoli%20Choi%20and%20Donghoon%20Shin%20and%20Sung-hee%20Lee&entry.1292438233=%20%20This%20paper%20introduces%20ELMO%2C%20a%20real-time%20upsampling%20motion%20capture%20framework%0Adesigned%20for%20a%20single%20LiDAR%20sensor.%20Modeled%20as%20a%20conditional%20autoregressive%0Atransformer-based%20upsampling%20motion%20generator%2C%20ELMO%20achieves%2060%20fps%20motion%0Acapture%20from%20a%2020%20fps%20LiDAR%20point%20cloud%20sequence.%20The%20key%20feature%20of%20ELMO%20is%0Athe%20coupling%20of%20the%20self-attention%20mechanism%20with%20thoughtfully%20designed%0Aembedding%20modules%20for%20motion%20and%20point%20clouds%2C%20significantly%20elevating%20the%0Amotion%20quality.%20To%20facilitate%20accurate%20motion%20capture%2C%20we%20develop%20a%20one-time%0Askeleton%20calibration%20model%20capable%20of%20predicting%20user%20skeleton%20offsets%20from%20a%0Asingle-frame%20point%20cloud.%20Additionally%2C%20we%20introduce%20a%20novel%20data%20augmentation%0Atechnique%20utilizing%20a%20LiDAR%20simulator%2C%20which%20enhances%20global%20root%20tracking%20to%0Aimprove%20environmental%20understanding.%20To%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%2C%20we%20compare%20ELMO%20with%20state-of-the-art%20methods%20in%20both%20image-based%20and%0Apoint%20cloud-based%20motion%20capture.%20We%20further%20conduct%20an%20ablation%20study%20to%0Avalidate%20our%20design%20principles.%20ELMO%27s%20fast%20inference%20time%20makes%20it%20well-suited%0Afor%20real-time%20applications%2C%20exemplified%20in%20our%20demo%20video%20featuring%20live%0Astreaming%20and%20interactive%20gaming%20scenarios.%20Furthermore%2C%20we%20contribute%20a%0Ahigh-quality%20LiDAR-mocap%20synchronized%20dataset%20comprising%2020%20different%20subjects%0Aperforming%20a%20range%20of%20motions%2C%20which%20can%20serve%20as%20a%20valuable%20resource%20for%0Afuture%20research.%20The%20dataset%20and%20evaluation%20code%20are%20available%20at%20%7B%5Cblue%0A%5Curl%7Bhttps%3A//movin3d.github.io/ELMO_SIGASIA2024/%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06963v1&entry.124074799=Read"},
{"title": "Gaitor: Learning a Unified Representation Across Gaits for Real-World\n  Quadruped Locomotion", "author": "Alexander L. Mitchell and Wolfgang Merkt and Aristotelis Papatheodorou and Ioannis Havoutis and Ingmar Posner", "abstract": "  The current state-of-the-art in quadruped locomotion is able to produce a\nvariety of complex motions. These methods either rely on switching between a\ndiscrete set of skills or learn a distribution across gaits using complex\nblack-box models. Alternatively, we present Gaitor, which learns a disentangled\nand 2D representation across locomotion gaits. This learnt representation forms\na planning space for closed-loop control delivering continuous gait transitions\nand perceptive terrain traversal. Gaitor's latent space is readily\ninterpretable and we discover that during gait transitions, novel unseen gaits\nemerge. The latent space is disentangled with respect to footswing heights and\nlengths. This means that these gait characteristics can be varied independently\nin the 2D latent representation. Together with a simple terrain encoding and a\nlearnt planner operating in the latent space, Gaitor can take motion commands\nincluding desired gait type and swing characteristics all while reacting to\nuneven terrain. We evaluate Gaitor in both simulation and the real world on the\nANYmal C platform. To the best of our knowledge, this is the first work\nlearning a unified and interpretable latent space for multiple gaits, resulting\nin continuous blending between different locomotion modes on a real quadruped\nrobot. An overview of the methods and results in this paper is found at\nhttps://youtu.be/eVFQbRyilCA.\n", "link": "http://arxiv.org/abs/2405.19452v2", "date": "2024-10-09", "relevancy": 2.3049, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.621}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6016}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaitor%3A%20Learning%20a%20Unified%20Representation%20Across%20Gaits%20for%20Real-World%0A%20%20Quadruped%20Locomotion&body=Title%3A%20Gaitor%3A%20Learning%20a%20Unified%20Representation%20Across%20Gaits%20for%20Real-World%0A%20%20Quadruped%20Locomotion%0AAuthor%3A%20Alexander%20L.%20Mitchell%20and%20Wolfgang%20Merkt%20and%20Aristotelis%20Papatheodorou%20and%20Ioannis%20Havoutis%20and%20Ingmar%20Posner%0AAbstract%3A%20%20%20The%20current%20state-of-the-art%20in%20quadruped%20locomotion%20is%20able%20to%20produce%20a%0Avariety%20of%20complex%20motions.%20These%20methods%20either%20rely%20on%20switching%20between%20a%0Adiscrete%20set%20of%20skills%20or%20learn%20a%20distribution%20across%20gaits%20using%20complex%0Ablack-box%20models.%20Alternatively%2C%20we%20present%20Gaitor%2C%20which%20learns%20a%20disentangled%0Aand%202D%20representation%20across%20locomotion%20gaits.%20This%20learnt%20representation%20forms%0Aa%20planning%20space%20for%20closed-loop%20control%20delivering%20continuous%20gait%20transitions%0Aand%20perceptive%20terrain%20traversal.%20Gaitor%27s%20latent%20space%20is%20readily%0Ainterpretable%20and%20we%20discover%20that%20during%20gait%20transitions%2C%20novel%20unseen%20gaits%0Aemerge.%20The%20latent%20space%20is%20disentangled%20with%20respect%20to%20footswing%20heights%20and%0Alengths.%20This%20means%20that%20these%20gait%20characteristics%20can%20be%20varied%20independently%0Ain%20the%202D%20latent%20representation.%20Together%20with%20a%20simple%20terrain%20encoding%20and%20a%0Alearnt%20planner%20operating%20in%20the%20latent%20space%2C%20Gaitor%20can%20take%20motion%20commands%0Aincluding%20desired%20gait%20type%20and%20swing%20characteristics%20all%20while%20reacting%20to%0Auneven%20terrain.%20We%20evaluate%20Gaitor%20in%20both%20simulation%20and%20the%20real%20world%20on%20the%0AANYmal%20C%20platform.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%0Alearning%20a%20unified%20and%20interpretable%20latent%20space%20for%20multiple%20gaits%2C%20resulting%0Ain%20continuous%20blending%20between%20different%20locomotion%20modes%20on%20a%20real%20quadruped%0Arobot.%20An%20overview%20of%20the%20methods%20and%20results%20in%20this%20paper%20is%20found%20at%0Ahttps%3A//youtu.be/eVFQbRyilCA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19452v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaitor%253A%2520Learning%2520a%2520Unified%2520Representation%2520Across%2520Gaits%2520for%2520Real-World%250A%2520%2520Quadruped%2520Locomotion%26entry.906535625%3DAlexander%2520L.%2520Mitchell%2520and%2520Wolfgang%2520Merkt%2520and%2520Aristotelis%2520Papatheodorou%2520and%2520Ioannis%2520Havoutis%2520and%2520Ingmar%2520Posner%26entry.1292438233%3D%2520%2520The%2520current%2520state-of-the-art%2520in%2520quadruped%2520locomotion%2520is%2520able%2520to%2520produce%2520a%250Avariety%2520of%2520complex%2520motions.%2520These%2520methods%2520either%2520rely%2520on%2520switching%2520between%2520a%250Adiscrete%2520set%2520of%2520skills%2520or%2520learn%2520a%2520distribution%2520across%2520gaits%2520using%2520complex%250Ablack-box%2520models.%2520Alternatively%252C%2520we%2520present%2520Gaitor%252C%2520which%2520learns%2520a%2520disentangled%250Aand%25202D%2520representation%2520across%2520locomotion%2520gaits.%2520This%2520learnt%2520representation%2520forms%250Aa%2520planning%2520space%2520for%2520closed-loop%2520control%2520delivering%2520continuous%2520gait%2520transitions%250Aand%2520perceptive%2520terrain%2520traversal.%2520Gaitor%2527s%2520latent%2520space%2520is%2520readily%250Ainterpretable%2520and%2520we%2520discover%2520that%2520during%2520gait%2520transitions%252C%2520novel%2520unseen%2520gaits%250Aemerge.%2520The%2520latent%2520space%2520is%2520disentangled%2520with%2520respect%2520to%2520footswing%2520heights%2520and%250Alengths.%2520This%2520means%2520that%2520these%2520gait%2520characteristics%2520can%2520be%2520varied%2520independently%250Ain%2520the%25202D%2520latent%2520representation.%2520Together%2520with%2520a%2520simple%2520terrain%2520encoding%2520and%2520a%250Alearnt%2520planner%2520operating%2520in%2520the%2520latent%2520space%252C%2520Gaitor%2520can%2520take%2520motion%2520commands%250Aincluding%2520desired%2520gait%2520type%2520and%2520swing%2520characteristics%2520all%2520while%2520reacting%2520to%250Auneven%2520terrain.%2520We%2520evaluate%2520Gaitor%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world%2520on%2520the%250AANYmal%2520C%2520platform.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%250Alearning%2520a%2520unified%2520and%2520interpretable%2520latent%2520space%2520for%2520multiple%2520gaits%252C%2520resulting%250Ain%2520continuous%2520blending%2520between%2520different%2520locomotion%2520modes%2520on%2520a%2520real%2520quadruped%250Arobot.%2520An%2520overview%2520of%2520the%2520methods%2520and%2520results%2520in%2520this%2520paper%2520is%2520found%2520at%250Ahttps%253A//youtu.be/eVFQbRyilCA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19452v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaitor%3A%20Learning%20a%20Unified%20Representation%20Across%20Gaits%20for%20Real-World%0A%20%20Quadruped%20Locomotion&entry.906535625=Alexander%20L.%20Mitchell%20and%20Wolfgang%20Merkt%20and%20Aristotelis%20Papatheodorou%20and%20Ioannis%20Havoutis%20and%20Ingmar%20Posner&entry.1292438233=%20%20The%20current%20state-of-the-art%20in%20quadruped%20locomotion%20is%20able%20to%20produce%20a%0Avariety%20of%20complex%20motions.%20These%20methods%20either%20rely%20on%20switching%20between%20a%0Adiscrete%20set%20of%20skills%20or%20learn%20a%20distribution%20across%20gaits%20using%20complex%0Ablack-box%20models.%20Alternatively%2C%20we%20present%20Gaitor%2C%20which%20learns%20a%20disentangled%0Aand%202D%20representation%20across%20locomotion%20gaits.%20This%20learnt%20representation%20forms%0Aa%20planning%20space%20for%20closed-loop%20control%20delivering%20continuous%20gait%20transitions%0Aand%20perceptive%20terrain%20traversal.%20Gaitor%27s%20latent%20space%20is%20readily%0Ainterpretable%20and%20we%20discover%20that%20during%20gait%20transitions%2C%20novel%20unseen%20gaits%0Aemerge.%20The%20latent%20space%20is%20disentangled%20with%20respect%20to%20footswing%20heights%20and%0Alengths.%20This%20means%20that%20these%20gait%20characteristics%20can%20be%20varied%20independently%0Ain%20the%202D%20latent%20representation.%20Together%20with%20a%20simple%20terrain%20encoding%20and%20a%0Alearnt%20planner%20operating%20in%20the%20latent%20space%2C%20Gaitor%20can%20take%20motion%20commands%0Aincluding%20desired%20gait%20type%20and%20swing%20characteristics%20all%20while%20reacting%20to%0Auneven%20terrain.%20We%20evaluate%20Gaitor%20in%20both%20simulation%20and%20the%20real%20world%20on%20the%0AANYmal%20C%20platform.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%0Alearning%20a%20unified%20and%20interpretable%20latent%20space%20for%20multiple%20gaits%2C%20resulting%0Ain%20continuous%20blending%20between%20different%20locomotion%20modes%20on%20a%20real%20quadruped%0Arobot.%20An%20overview%20of%20the%20methods%20and%20results%20in%20this%20paper%20is%20found%20at%0Ahttps%3A//youtu.be/eVFQbRyilCA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19452v2&entry.124074799=Read"},
{"title": "Transesophageal Echocardiography Generation using Anatomical Models", "author": "Emmanuel Oladokun and Musa Abdulkareem and Jurica \u0160prem and Vicente Grau", "abstract": "  Through automation, deep learning (DL) can enhance the analysis of\ntransesophageal echocardiography (TEE) images. However, DL methods require\nlarge amounts of high-quality data to produce accurate results, which is\ndifficult to satisfy. Data augmentation is commonly used to tackle this issue.\nIn this work, we develop a pipeline to generate synthetic TEE images and\ncorresponding semantic labels. The proposed data generation pipeline expands on\nan existing pipeline that generates synthetic transthoracic echocardiography\nimages by transforming slices from anatomical models into synthetic images. We\nalso demonstrate that such images can improve DL network performance through a\nleft-ventricle semantic segmentation task. For the pipeline's unpaired\nimage-to-image (I2I) translation section, we explore two generative methods:\nCycleGAN and contrastive unpaired translation. Next, we evaluate the synthetic\nimages quantitatively using the Fr\\'echet Inception Distance (FID) Score and\nqualitatively through a human perception quiz involving expert cardiologists\nand the average researcher.\n  In this study, we achieve a dice score improvement of up to 10% when we\naugment datasets with our synthetic images. Furthermore, we compare established\nmethods of assessing unpaired I2I translation and observe a disagreement when\nevaluating the synthetic images. Finally, we see which metric better predicts\nthe generated data's efficacy when used for data augmentation.\n", "link": "http://arxiv.org/abs/2410.06781v1", "date": "2024-10-09", "relevancy": 2.3041, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6291}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.567}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transesophageal%20Echocardiography%20Generation%20using%20Anatomical%20Models&body=Title%3A%20Transesophageal%20Echocardiography%20Generation%20using%20Anatomical%20Models%0AAuthor%3A%20Emmanuel%20Oladokun%20and%20Musa%20Abdulkareem%20and%20Jurica%20%C5%A0prem%20and%20Vicente%20Grau%0AAbstract%3A%20%20%20Through%20automation%2C%20deep%20learning%20%28DL%29%20can%20enhance%20the%20analysis%20of%0Atransesophageal%20echocardiography%20%28TEE%29%20images.%20However%2C%20DL%20methods%20require%0Alarge%20amounts%20of%20high-quality%20data%20to%20produce%20accurate%20results%2C%20which%20is%0Adifficult%20to%20satisfy.%20Data%20augmentation%20is%20commonly%20used%20to%20tackle%20this%20issue.%0AIn%20this%20work%2C%20we%20develop%20a%20pipeline%20to%20generate%20synthetic%20TEE%20images%20and%0Acorresponding%20semantic%20labels.%20The%20proposed%20data%20generation%20pipeline%20expands%20on%0Aan%20existing%20pipeline%20that%20generates%20synthetic%20transthoracic%20echocardiography%0Aimages%20by%20transforming%20slices%20from%20anatomical%20models%20into%20synthetic%20images.%20We%0Aalso%20demonstrate%20that%20such%20images%20can%20improve%20DL%20network%20performance%20through%20a%0Aleft-ventricle%20semantic%20segmentation%20task.%20For%20the%20pipeline%27s%20unpaired%0Aimage-to-image%20%28I2I%29%20translation%20section%2C%20we%20explore%20two%20generative%20methods%3A%0ACycleGAN%20and%20contrastive%20unpaired%20translation.%20Next%2C%20we%20evaluate%20the%20synthetic%0Aimages%20quantitatively%20using%20the%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20Score%20and%0Aqualitatively%20through%20a%20human%20perception%20quiz%20involving%20expert%20cardiologists%0Aand%20the%20average%20researcher.%0A%20%20In%20this%20study%2C%20we%20achieve%20a%20dice%20score%20improvement%20of%20up%20to%2010%25%20when%20we%0Aaugment%20datasets%20with%20our%20synthetic%20images.%20Furthermore%2C%20we%20compare%20established%0Amethods%20of%20assessing%20unpaired%20I2I%20translation%20and%20observe%20a%20disagreement%20when%0Aevaluating%20the%20synthetic%20images.%20Finally%2C%20we%20see%20which%20metric%20better%20predicts%0Athe%20generated%20data%27s%20efficacy%20when%20used%20for%20data%20augmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransesophageal%2520Echocardiography%2520Generation%2520using%2520Anatomical%2520Models%26entry.906535625%3DEmmanuel%2520Oladokun%2520and%2520Musa%2520Abdulkareem%2520and%2520Jurica%2520%25C5%25A0prem%2520and%2520Vicente%2520Grau%26entry.1292438233%3D%2520%2520Through%2520automation%252C%2520deep%2520learning%2520%2528DL%2529%2520can%2520enhance%2520the%2520analysis%2520of%250Atransesophageal%2520echocardiography%2520%2528TEE%2529%2520images.%2520However%252C%2520DL%2520methods%2520require%250Alarge%2520amounts%2520of%2520high-quality%2520data%2520to%2520produce%2520accurate%2520results%252C%2520which%2520is%250Adifficult%2520to%2520satisfy.%2520Data%2520augmentation%2520is%2520commonly%2520used%2520to%2520tackle%2520this%2520issue.%250AIn%2520this%2520work%252C%2520we%2520develop%2520a%2520pipeline%2520to%2520generate%2520synthetic%2520TEE%2520images%2520and%250Acorresponding%2520semantic%2520labels.%2520The%2520proposed%2520data%2520generation%2520pipeline%2520expands%2520on%250Aan%2520existing%2520pipeline%2520that%2520generates%2520synthetic%2520transthoracic%2520echocardiography%250Aimages%2520by%2520transforming%2520slices%2520from%2520anatomical%2520models%2520into%2520synthetic%2520images.%2520We%250Aalso%2520demonstrate%2520that%2520such%2520images%2520can%2520improve%2520DL%2520network%2520performance%2520through%2520a%250Aleft-ventricle%2520semantic%2520segmentation%2520task.%2520For%2520the%2520pipeline%2527s%2520unpaired%250Aimage-to-image%2520%2528I2I%2529%2520translation%2520section%252C%2520we%2520explore%2520two%2520generative%2520methods%253A%250ACycleGAN%2520and%2520contrastive%2520unpaired%2520translation.%2520Next%252C%2520we%2520evaluate%2520the%2520synthetic%250Aimages%2520quantitatively%2520using%2520the%2520Fr%255C%2527echet%2520Inception%2520Distance%2520%2528FID%2529%2520Score%2520and%250Aqualitatively%2520through%2520a%2520human%2520perception%2520quiz%2520involving%2520expert%2520cardiologists%250Aand%2520the%2520average%2520researcher.%250A%2520%2520In%2520this%2520study%252C%2520we%2520achieve%2520a%2520dice%2520score%2520improvement%2520of%2520up%2520to%252010%2525%2520when%2520we%250Aaugment%2520datasets%2520with%2520our%2520synthetic%2520images.%2520Furthermore%252C%2520we%2520compare%2520established%250Amethods%2520of%2520assessing%2520unpaired%2520I2I%2520translation%2520and%2520observe%2520a%2520disagreement%2520when%250Aevaluating%2520the%2520synthetic%2520images.%2520Finally%252C%2520we%2520see%2520which%2520metric%2520better%2520predicts%250Athe%2520generated%2520data%2527s%2520efficacy%2520when%2520used%2520for%2520data%2520augmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transesophageal%20Echocardiography%20Generation%20using%20Anatomical%20Models&entry.906535625=Emmanuel%20Oladokun%20and%20Musa%20Abdulkareem%20and%20Jurica%20%C5%A0prem%20and%20Vicente%20Grau&entry.1292438233=%20%20Through%20automation%2C%20deep%20learning%20%28DL%29%20can%20enhance%20the%20analysis%20of%0Atransesophageal%20echocardiography%20%28TEE%29%20images.%20However%2C%20DL%20methods%20require%0Alarge%20amounts%20of%20high-quality%20data%20to%20produce%20accurate%20results%2C%20which%20is%0Adifficult%20to%20satisfy.%20Data%20augmentation%20is%20commonly%20used%20to%20tackle%20this%20issue.%0AIn%20this%20work%2C%20we%20develop%20a%20pipeline%20to%20generate%20synthetic%20TEE%20images%20and%0Acorresponding%20semantic%20labels.%20The%20proposed%20data%20generation%20pipeline%20expands%20on%0Aan%20existing%20pipeline%20that%20generates%20synthetic%20transthoracic%20echocardiography%0Aimages%20by%20transforming%20slices%20from%20anatomical%20models%20into%20synthetic%20images.%20We%0Aalso%20demonstrate%20that%20such%20images%20can%20improve%20DL%20network%20performance%20through%20a%0Aleft-ventricle%20semantic%20segmentation%20task.%20For%20the%20pipeline%27s%20unpaired%0Aimage-to-image%20%28I2I%29%20translation%20section%2C%20we%20explore%20two%20generative%20methods%3A%0ACycleGAN%20and%20contrastive%20unpaired%20translation.%20Next%2C%20we%20evaluate%20the%20synthetic%0Aimages%20quantitatively%20using%20the%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20Score%20and%0Aqualitatively%20through%20a%20human%20perception%20quiz%20involving%20expert%20cardiologists%0Aand%20the%20average%20researcher.%0A%20%20In%20this%20study%2C%20we%20achieve%20a%20dice%20score%20improvement%20of%20up%20to%2010%25%20when%20we%0Aaugment%20datasets%20with%20our%20synthetic%20images.%20Furthermore%2C%20we%20compare%20established%0Amethods%20of%20assessing%20unpaired%20I2I%20translation%20and%20observe%20a%20disagreement%20when%0Aevaluating%20the%20synthetic%20images.%20Finally%2C%20we%20see%20which%20metric%20better%20predicts%0Athe%20generated%20data%27s%20efficacy%20when%20used%20for%20data%20augmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06781v1&entry.124074799=Read"},
{"title": "The FIX Benchmark: Extracting Features Interpretable to eXperts", "author": "Helen Jin and Shreya Havaldar and Chaehyeon Kim and Anton Xue and Weiqiu You and Helen Qu and Marco Gatti and Daniel A Hashimoto and Bhuvnesh Jain and Amin Madani and Masao Sako and Lyle Ungar and Eric Wong", "abstract": "  Feature-based methods are commonly used to explain model predictions, but\nthese methods often implicitly assume that interpretable features are readily\navailable. However, this is often not the case for high-dimensional data, and\nit can be hard even for domain experts to mathematically specify which features\nare important. Can we instead automatically extract collections or groups of\nfeatures that are aligned with expert knowledge? To address this gap, we\npresent FIX (Features Interpretable to eXperts), a benchmark for measuring how\nwell a collection of features aligns with expert knowledge. In collaboration\nwith domain experts, we propose FIXScore, a unified expert alignment measure\napplicable to diverse real-world settings across cosmology, psychology, and\nmedicine domains in vision, language and time series data modalities. With\nFIXScore, we find that popular feature-based explanation methods have poor\nalignment with expert-specified knowledge, highlighting the need for new\nmethods that can better identify features interpretable to experts.\n", "link": "http://arxiv.org/abs/2409.13684v2", "date": "2024-10-09", "relevancy": 2.2957, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20FIX%20Benchmark%3A%20Extracting%20Features%20Interpretable%20to%20eXperts&body=Title%3A%20The%20FIX%20Benchmark%3A%20Extracting%20Features%20Interpretable%20to%20eXperts%0AAuthor%3A%20Helen%20Jin%20and%20Shreya%20Havaldar%20and%20Chaehyeon%20Kim%20and%20Anton%20Xue%20and%20Weiqiu%20You%20and%20Helen%20Qu%20and%20Marco%20Gatti%20and%20Daniel%20A%20Hashimoto%20and%20Bhuvnesh%20Jain%20and%20Amin%20Madani%20and%20Masao%20Sako%20and%20Lyle%20Ungar%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Feature-based%20methods%20are%20commonly%20used%20to%20explain%20model%20predictions%2C%20but%0Athese%20methods%20often%20implicitly%20assume%20that%20interpretable%20features%20are%20readily%0Aavailable.%20However%2C%20this%20is%20often%20not%20the%20case%20for%20high-dimensional%20data%2C%20and%0Ait%20can%20be%20hard%20even%20for%20domain%20experts%20to%20mathematically%20specify%20which%20features%0Aare%20important.%20Can%20we%20instead%20automatically%20extract%20collections%20or%20groups%20of%0Afeatures%20that%20are%20aligned%20with%20expert%20knowledge%3F%20To%20address%20this%20gap%2C%20we%0Apresent%20FIX%20%28Features%20Interpretable%20to%20eXperts%29%2C%20a%20benchmark%20for%20measuring%20how%0Awell%20a%20collection%20of%20features%20aligns%20with%20expert%20knowledge.%20In%20collaboration%0Awith%20domain%20experts%2C%20we%20propose%20FIXScore%2C%20a%20unified%20expert%20alignment%20measure%0Aapplicable%20to%20diverse%20real-world%20settings%20across%20cosmology%2C%20psychology%2C%20and%0Amedicine%20domains%20in%20vision%2C%20language%20and%20time%20series%20data%20modalities.%20With%0AFIXScore%2C%20we%20find%20that%20popular%20feature-based%20explanation%20methods%20have%20poor%0Aalignment%20with%20expert-specified%20knowledge%2C%20highlighting%20the%20need%20for%20new%0Amethods%20that%20can%20better%20identify%20features%20interpretable%20to%20experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520FIX%2520Benchmark%253A%2520Extracting%2520Features%2520Interpretable%2520to%2520eXperts%26entry.906535625%3DHelen%2520Jin%2520and%2520Shreya%2520Havaldar%2520and%2520Chaehyeon%2520Kim%2520and%2520Anton%2520Xue%2520and%2520Weiqiu%2520You%2520and%2520Helen%2520Qu%2520and%2520Marco%2520Gatti%2520and%2520Daniel%2520A%2520Hashimoto%2520and%2520Bhuvnesh%2520Jain%2520and%2520Amin%2520Madani%2520and%2520Masao%2520Sako%2520and%2520Lyle%2520Ungar%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Feature-based%2520methods%2520are%2520commonly%2520used%2520to%2520explain%2520model%2520predictions%252C%2520but%250Athese%2520methods%2520often%2520implicitly%2520assume%2520that%2520interpretable%2520features%2520are%2520readily%250Aavailable.%2520However%252C%2520this%2520is%2520often%2520not%2520the%2520case%2520for%2520high-dimensional%2520data%252C%2520and%250Ait%2520can%2520be%2520hard%2520even%2520for%2520domain%2520experts%2520to%2520mathematically%2520specify%2520which%2520features%250Aare%2520important.%2520Can%2520we%2520instead%2520automatically%2520extract%2520collections%2520or%2520groups%2520of%250Afeatures%2520that%2520are%2520aligned%2520with%2520expert%2520knowledge%253F%2520To%2520address%2520this%2520gap%252C%2520we%250Apresent%2520FIX%2520%2528Features%2520Interpretable%2520to%2520eXperts%2529%252C%2520a%2520benchmark%2520for%2520measuring%2520how%250Awell%2520a%2520collection%2520of%2520features%2520aligns%2520with%2520expert%2520knowledge.%2520In%2520collaboration%250Awith%2520domain%2520experts%252C%2520we%2520propose%2520FIXScore%252C%2520a%2520unified%2520expert%2520alignment%2520measure%250Aapplicable%2520to%2520diverse%2520real-world%2520settings%2520across%2520cosmology%252C%2520psychology%252C%2520and%250Amedicine%2520domains%2520in%2520vision%252C%2520language%2520and%2520time%2520series%2520data%2520modalities.%2520With%250AFIXScore%252C%2520we%2520find%2520that%2520popular%2520feature-based%2520explanation%2520methods%2520have%2520poor%250Aalignment%2520with%2520expert-specified%2520knowledge%252C%2520highlighting%2520the%2520need%2520for%2520new%250Amethods%2520that%2520can%2520better%2520identify%2520features%2520interpretable%2520to%2520experts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20FIX%20Benchmark%3A%20Extracting%20Features%20Interpretable%20to%20eXperts&entry.906535625=Helen%20Jin%20and%20Shreya%20Havaldar%20and%20Chaehyeon%20Kim%20and%20Anton%20Xue%20and%20Weiqiu%20You%20and%20Helen%20Qu%20and%20Marco%20Gatti%20and%20Daniel%20A%20Hashimoto%20and%20Bhuvnesh%20Jain%20and%20Amin%20Madani%20and%20Masao%20Sako%20and%20Lyle%20Ungar%20and%20Eric%20Wong&entry.1292438233=%20%20Feature-based%20methods%20are%20commonly%20used%20to%20explain%20model%20predictions%2C%20but%0Athese%20methods%20often%20implicitly%20assume%20that%20interpretable%20features%20are%20readily%0Aavailable.%20However%2C%20this%20is%20often%20not%20the%20case%20for%20high-dimensional%20data%2C%20and%0Ait%20can%20be%20hard%20even%20for%20domain%20experts%20to%20mathematically%20specify%20which%20features%0Aare%20important.%20Can%20we%20instead%20automatically%20extract%20collections%20or%20groups%20of%0Afeatures%20that%20are%20aligned%20with%20expert%20knowledge%3F%20To%20address%20this%20gap%2C%20we%0Apresent%20FIX%20%28Features%20Interpretable%20to%20eXperts%29%2C%20a%20benchmark%20for%20measuring%20how%0Awell%20a%20collection%20of%20features%20aligns%20with%20expert%20knowledge.%20In%20collaboration%0Awith%20domain%20experts%2C%20we%20propose%20FIXScore%2C%20a%20unified%20expert%20alignment%20measure%0Aapplicable%20to%20diverse%20real-world%20settings%20across%20cosmology%2C%20psychology%2C%20and%0Amedicine%20domains%20in%20vision%2C%20language%20and%20time%20series%20data%20modalities.%20With%0AFIXScore%2C%20we%20find%20that%20popular%20feature-based%20explanation%20methods%20have%20poor%0Aalignment%20with%20expert-specified%20knowledge%2C%20highlighting%20the%20need%20for%20new%0Amethods%20that%20can%20better%20identify%20features%20interpretable%20to%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13684v2&entry.124074799=Read"},
{"title": "Spectral and Rhythm Features for Audio Classification with Deep\n  Convolutional Neural Networks", "author": "Friedrich Wolf-Monheim", "abstract": "  Convolutional neural networks (CNNs) are widely used in computer vision. They\ncan be used not only for conventional digital image material to recognize\npatterns, but also for feature extraction from digital imagery representing\nspectral and rhythm features extracted from time-domain digital audio signals\nfor the acoustic classification of sounds. Different spectral and rhythm\nfeature representations like mel-scaled spectrograms, mel-frequency cepstral\ncoefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)\nchromagrams, constant-Q transform (CQT) chromagrams and chroma energy\nnormalized statistics (CENS) chromagrams are investigated in terms of the audio\nclassification performance using a deep convolutional neural network. It can be\nclearly shown that the mel-scaled spectrograms and the mel-frequency cepstral\ncoefficients (MFCCs) perform significantly better then the other spectral and\nrhythm features investigated in this research for audio classification tasks\nusing deep CNNs. The experiments were carried out with the aid of the ESC-50\ndataset with 2,000 labeled environmental audio recordings.\n", "link": "http://arxiv.org/abs/2410.06927v1", "date": "2024-10-09", "relevancy": 2.2852, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4765}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4476}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20and%20Rhythm%20Features%20for%20Audio%20Classification%20with%20Deep%0A%20%20Convolutional%20Neural%20Networks&body=Title%3A%20Spectral%20and%20Rhythm%20Features%20for%20Audio%20Classification%20with%20Deep%0A%20%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Friedrich%20Wolf-Monheim%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20widely%20used%20in%20computer%20vision.%20They%0Acan%20be%20used%20not%20only%20for%20conventional%20digital%20image%20material%20to%20recognize%0Apatterns%2C%20but%20also%20for%20feature%20extraction%20from%20digital%20imagery%20representing%0Aspectral%20and%20rhythm%20features%20extracted%20from%20time-domain%20digital%20audio%20signals%0Afor%20the%20acoustic%20classification%20of%20sounds.%20Different%20spectral%20and%20rhythm%0Afeature%20representations%20like%20mel-scaled%20spectrograms%2C%20mel-frequency%20cepstral%0Acoefficients%20%28MFCCs%29%2C%20cyclic%20tempograms%2C%20short-time%20Fourier%20transform%20%28STFT%29%0Achromagrams%2C%20constant-Q%20transform%20%28CQT%29%20chromagrams%20and%20chroma%20energy%0Anormalized%20statistics%20%28CENS%29%20chromagrams%20are%20investigated%20in%20terms%20of%20the%20audio%0Aclassification%20performance%20using%20a%20deep%20convolutional%20neural%20network.%20It%20can%20be%0Aclearly%20shown%20that%20the%20mel-scaled%20spectrograms%20and%20the%20mel-frequency%20cepstral%0Acoefficients%20%28MFCCs%29%20perform%20significantly%20better%20then%20the%20other%20spectral%20and%0Arhythm%20features%20investigated%20in%20this%20research%20for%20audio%20classification%20tasks%0Ausing%20deep%20CNNs.%20The%20experiments%20were%20carried%20out%20with%20the%20aid%20of%20the%20ESC-50%0Adataset%20with%202%2C000%20labeled%20environmental%20audio%20recordings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520and%2520Rhythm%2520Features%2520for%2520Audio%2520Classification%2520with%2520Deep%250A%2520%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DFriedrich%2520Wolf-Monheim%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520are%2520widely%2520used%2520in%2520computer%2520vision.%2520They%250Acan%2520be%2520used%2520not%2520only%2520for%2520conventional%2520digital%2520image%2520material%2520to%2520recognize%250Apatterns%252C%2520but%2520also%2520for%2520feature%2520extraction%2520from%2520digital%2520imagery%2520representing%250Aspectral%2520and%2520rhythm%2520features%2520extracted%2520from%2520time-domain%2520digital%2520audio%2520signals%250Afor%2520the%2520acoustic%2520classification%2520of%2520sounds.%2520Different%2520spectral%2520and%2520rhythm%250Afeature%2520representations%2520like%2520mel-scaled%2520spectrograms%252C%2520mel-frequency%2520cepstral%250Acoefficients%2520%2528MFCCs%2529%252C%2520cyclic%2520tempograms%252C%2520short-time%2520Fourier%2520transform%2520%2528STFT%2529%250Achromagrams%252C%2520constant-Q%2520transform%2520%2528CQT%2529%2520chromagrams%2520and%2520chroma%2520energy%250Anormalized%2520statistics%2520%2528CENS%2529%2520chromagrams%2520are%2520investigated%2520in%2520terms%2520of%2520the%2520audio%250Aclassification%2520performance%2520using%2520a%2520deep%2520convolutional%2520neural%2520network.%2520It%2520can%2520be%250Aclearly%2520shown%2520that%2520the%2520mel-scaled%2520spectrograms%2520and%2520the%2520mel-frequency%2520cepstral%250Acoefficients%2520%2528MFCCs%2529%2520perform%2520significantly%2520better%2520then%2520the%2520other%2520spectral%2520and%250Arhythm%2520features%2520investigated%2520in%2520this%2520research%2520for%2520audio%2520classification%2520tasks%250Ausing%2520deep%2520CNNs.%2520The%2520experiments%2520were%2520carried%2520out%2520with%2520the%2520aid%2520of%2520the%2520ESC-50%250Adataset%2520with%25202%252C000%2520labeled%2520environmental%2520audio%2520recordings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20and%20Rhythm%20Features%20for%20Audio%20Classification%20with%20Deep%0A%20%20Convolutional%20Neural%20Networks&entry.906535625=Friedrich%20Wolf-Monheim&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20widely%20used%20in%20computer%20vision.%20They%0Acan%20be%20used%20not%20only%20for%20conventional%20digital%20image%20material%20to%20recognize%0Apatterns%2C%20but%20also%20for%20feature%20extraction%20from%20digital%20imagery%20representing%0Aspectral%20and%20rhythm%20features%20extracted%20from%20time-domain%20digital%20audio%20signals%0Afor%20the%20acoustic%20classification%20of%20sounds.%20Different%20spectral%20and%20rhythm%0Afeature%20representations%20like%20mel-scaled%20spectrograms%2C%20mel-frequency%20cepstral%0Acoefficients%20%28MFCCs%29%2C%20cyclic%20tempograms%2C%20short-time%20Fourier%20transform%20%28STFT%29%0Achromagrams%2C%20constant-Q%20transform%20%28CQT%29%20chromagrams%20and%20chroma%20energy%0Anormalized%20statistics%20%28CENS%29%20chromagrams%20are%20investigated%20in%20terms%20of%20the%20audio%0Aclassification%20performance%20using%20a%20deep%20convolutional%20neural%20network.%20It%20can%20be%0Aclearly%20shown%20that%20the%20mel-scaled%20spectrograms%20and%20the%20mel-frequency%20cepstral%0Acoefficients%20%28MFCCs%29%20perform%20significantly%20better%20then%20the%20other%20spectral%20and%0Arhythm%20features%20investigated%20in%20this%20research%20for%20audio%20classification%20tasks%0Ausing%20deep%20CNNs.%20The%20experiments%20were%20carried%20out%20with%20the%20aid%20of%20the%20ESC-50%0Adataset%20with%202%2C000%20labeled%20environmental%20audio%20recordings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06927v1&entry.124074799=Read"},
{"title": "Multi-Neuron Unleashes Expressivity of ReLU Networks Under Convex\n  Relaxation", "author": "Yuhao Mao and Yani Zhang and Martin Vechev", "abstract": "  Neural work certification has established itself as a crucial tool for\nensuring the robustness of neural networks. Certification methods typically\nrely on convex relaxations of the feasible output set to provide sound bounds.\nHowever, complete certification requires exact bounds, which strongly limits\nthe expressivity of ReLU networks: even for the simple ``$\\max$'' function in\n$\\mathbb{R}^2$, there does not exist a ReLU network that expresses this\nfunction and can be exactly bounded by single-neuron relaxation methods. This\nraises the question whether there exists a convex relaxation that can provide\nexact bounds for general continuous piecewise linear functions in\n$\\mathbb{R}^n$. In this work, we answer this question affirmatively by showing\nthat (layer-wise) multi-neuron relaxation provides complete certification for\ngeneral ReLU networks. Based on this novel result, we show that the\nexpressivity of ReLU networks is no longer limited under multi-neuron\nrelaxation. To the best of our knowledge, this is the first positive result on\nthe completeness of convex relaxations, shedding light on the practice of\ncertified robustness.\n", "link": "http://arxiv.org/abs/2410.06816v1", "date": "2024-10-09", "relevancy": 2.2821, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4597}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Neuron%20Unleashes%20Expressivity%20of%20ReLU%20Networks%20Under%20Convex%0A%20%20Relaxation&body=Title%3A%20Multi-Neuron%20Unleashes%20Expressivity%20of%20ReLU%20Networks%20Under%20Convex%0A%20%20Relaxation%0AAuthor%3A%20Yuhao%20Mao%20and%20Yani%20Zhang%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Neural%20work%20certification%20has%20established%20itself%20as%20a%20crucial%20tool%20for%0Aensuring%20the%20robustness%20of%20neural%20networks.%20Certification%20methods%20typically%0Arely%20on%20convex%20relaxations%20of%20the%20feasible%20output%20set%20to%20provide%20sound%20bounds.%0AHowever%2C%20complete%20certification%20requires%20exact%20bounds%2C%20which%20strongly%20limits%0Athe%20expressivity%20of%20ReLU%20networks%3A%20even%20for%20the%20simple%20%60%60%24%5Cmax%24%27%27%20function%20in%0A%24%5Cmathbb%7BR%7D%5E2%24%2C%20there%20does%20not%20exist%20a%20ReLU%20network%20that%20expresses%20this%0Afunction%20and%20can%20be%20exactly%20bounded%20by%20single-neuron%20relaxation%20methods.%20This%0Araises%20the%20question%20whether%20there%20exists%20a%20convex%20relaxation%20that%20can%20provide%0Aexact%20bounds%20for%20general%20continuous%20piecewise%20linear%20functions%20in%0A%24%5Cmathbb%7BR%7D%5En%24.%20In%20this%20work%2C%20we%20answer%20this%20question%20affirmatively%20by%20showing%0Athat%20%28layer-wise%29%20multi-neuron%20relaxation%20provides%20complete%20certification%20for%0Ageneral%20ReLU%20networks.%20Based%20on%20this%20novel%20result%2C%20we%20show%20that%20the%0Aexpressivity%20of%20ReLU%20networks%20is%20no%20longer%20limited%20under%20multi-neuron%0Arelaxation.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20positive%20result%20on%0Athe%20completeness%20of%20convex%20relaxations%2C%20shedding%20light%20on%20the%20practice%20of%0Acertified%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Neuron%2520Unleashes%2520Expressivity%2520of%2520ReLU%2520Networks%2520Under%2520Convex%250A%2520%2520Relaxation%26entry.906535625%3DYuhao%2520Mao%2520and%2520Yani%2520Zhang%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Neural%2520work%2520certification%2520has%2520established%2520itself%2520as%2520a%2520crucial%2520tool%2520for%250Aensuring%2520the%2520robustness%2520of%2520neural%2520networks.%2520Certification%2520methods%2520typically%250Arely%2520on%2520convex%2520relaxations%2520of%2520the%2520feasible%2520output%2520set%2520to%2520provide%2520sound%2520bounds.%250AHowever%252C%2520complete%2520certification%2520requires%2520exact%2520bounds%252C%2520which%2520strongly%2520limits%250Athe%2520expressivity%2520of%2520ReLU%2520networks%253A%2520even%2520for%2520the%2520simple%2520%2560%2560%2524%255Cmax%2524%2527%2527%2520function%2520in%250A%2524%255Cmathbb%257BR%257D%255E2%2524%252C%2520there%2520does%2520not%2520exist%2520a%2520ReLU%2520network%2520that%2520expresses%2520this%250Afunction%2520and%2520can%2520be%2520exactly%2520bounded%2520by%2520single-neuron%2520relaxation%2520methods.%2520This%250Araises%2520the%2520question%2520whether%2520there%2520exists%2520a%2520convex%2520relaxation%2520that%2520can%2520provide%250Aexact%2520bounds%2520for%2520general%2520continuous%2520piecewise%2520linear%2520functions%2520in%250A%2524%255Cmathbb%257BR%257D%255En%2524.%2520In%2520this%2520work%252C%2520we%2520answer%2520this%2520question%2520affirmatively%2520by%2520showing%250Athat%2520%2528layer-wise%2529%2520multi-neuron%2520relaxation%2520provides%2520complete%2520certification%2520for%250Ageneral%2520ReLU%2520networks.%2520Based%2520on%2520this%2520novel%2520result%252C%2520we%2520show%2520that%2520the%250Aexpressivity%2520of%2520ReLU%2520networks%2520is%2520no%2520longer%2520limited%2520under%2520multi-neuron%250Arelaxation.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520positive%2520result%2520on%250Athe%2520completeness%2520of%2520convex%2520relaxations%252C%2520shedding%2520light%2520on%2520the%2520practice%2520of%250Acertified%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Neuron%20Unleashes%20Expressivity%20of%20ReLU%20Networks%20Under%20Convex%0A%20%20Relaxation&entry.906535625=Yuhao%20Mao%20and%20Yani%20Zhang%20and%20Martin%20Vechev&entry.1292438233=%20%20Neural%20work%20certification%20has%20established%20itself%20as%20a%20crucial%20tool%20for%0Aensuring%20the%20robustness%20of%20neural%20networks.%20Certification%20methods%20typically%0Arely%20on%20convex%20relaxations%20of%20the%20feasible%20output%20set%20to%20provide%20sound%20bounds.%0AHowever%2C%20complete%20certification%20requires%20exact%20bounds%2C%20which%20strongly%20limits%0Athe%20expressivity%20of%20ReLU%20networks%3A%20even%20for%20the%20simple%20%60%60%24%5Cmax%24%27%27%20function%20in%0A%24%5Cmathbb%7BR%7D%5E2%24%2C%20there%20does%20not%20exist%20a%20ReLU%20network%20that%20expresses%20this%0Afunction%20and%20can%20be%20exactly%20bounded%20by%20single-neuron%20relaxation%20methods.%20This%0Araises%20the%20question%20whether%20there%20exists%20a%20convex%20relaxation%20that%20can%20provide%0Aexact%20bounds%20for%20general%20continuous%20piecewise%20linear%20functions%20in%0A%24%5Cmathbb%7BR%7D%5En%24.%20In%20this%20work%2C%20we%20answer%20this%20question%20affirmatively%20by%20showing%0Athat%20%28layer-wise%29%20multi-neuron%20relaxation%20provides%20complete%20certification%20for%0Ageneral%20ReLU%20networks.%20Based%20on%20this%20novel%20result%2C%20we%20show%20that%20the%0Aexpressivity%20of%20ReLU%20networks%20is%20no%20longer%20limited%20under%20multi-neuron%0Arelaxation.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20positive%20result%20on%0Athe%20completeness%20of%20convex%20relaxations%2C%20shedding%20light%20on%20the%20practice%20of%0Acertified%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06816v1&entry.124074799=Read"},
{"title": "A Unified Generative Framework for Realistic Lidar Simulation in\n  Autonomous Driving Systems", "author": "Hamed Haghighi and Mehrdad Dianati and Valentina Donzella and Kurt Debattista", "abstract": "  Simulation models for perception sensors are integral components of\nautomotive simulators used for the virtual Verification and Validation (V\\&V)\nof Autonomous Driving Systems (ADS). These models also serve as powerful tools\nfor generating synthetic datasets to train deep learning-based perception\nmodels. Lidar is a widely used sensor type among the perception sensors for ADS\ndue to its high precision in 3D environment scanning. However, developing\nrealistic Lidar simulation models is a significant technical challenge. In\nparticular, unrealistic models can result in a large gap between the\nsynthesised and real-world point clouds, limiting their effectiveness in ADS\napplications. Recently, deep generative models have emerged as promising\nsolutions to synthesise realistic sensory data. However, for Lidar simulation,\ndeep generative models have been primarily hybridised with conventional\nalgorithms, leaving unified generative approaches largely unexplored in the\nliterature. Motivated by this research gap, we propose a unified generative\nframework to enhance Lidar simulation fidelity. Our proposed framework projects\nLidar point clouds into depth-reflectance images via a lossless transformation,\nand employs our novel Controllable Lidar point cloud Generative model, CoLiGen,\nto translate the images. We extensively evaluate our CoLiGen model, comparing\nit with the state-of-the-art image-to-image translation models using various\nmetrics to assess the realness, faithfulness, and performance of a downstream\nperception model. Our results show that CoLiGen exhibits superior performance\nacross most metrics. The dataset and source code for this research are\navailable at https://github.com/hamedhaghighi/CoLiGen.git.\n", "link": "http://arxiv.org/abs/2312.15817v2", "date": "2024-10-09", "relevancy": 2.2761, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5912}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5716}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Generative%20Framework%20for%20Realistic%20Lidar%20Simulation%20in%0A%20%20Autonomous%20Driving%20Systems&body=Title%3A%20A%20Unified%20Generative%20Framework%20for%20Realistic%20Lidar%20Simulation%20in%0A%20%20Autonomous%20Driving%20Systems%0AAuthor%3A%20Hamed%20Haghighi%20and%20Mehrdad%20Dianati%20and%20Valentina%20Donzella%20and%20Kurt%20Debattista%0AAbstract%3A%20%20%20Simulation%20models%20for%20perception%20sensors%20are%20integral%20components%20of%0Aautomotive%20simulators%20used%20for%20the%20virtual%20Verification%20and%20Validation%20%28V%5C%26V%29%0Aof%20Autonomous%20Driving%20Systems%20%28ADS%29.%20These%20models%20also%20serve%20as%20powerful%20tools%0Afor%20generating%20synthetic%20datasets%20to%20train%20deep%20learning-based%20perception%0Amodels.%20Lidar%20is%20a%20widely%20used%20sensor%20type%20among%20the%20perception%20sensors%20for%20ADS%0Adue%20to%20its%20high%20precision%20in%203D%20environment%20scanning.%20However%2C%20developing%0Arealistic%20Lidar%20simulation%20models%20is%20a%20significant%20technical%20challenge.%20In%0Aparticular%2C%20unrealistic%20models%20can%20result%20in%20a%20large%20gap%20between%20the%0Asynthesised%20and%20real-world%20point%20clouds%2C%20limiting%20their%20effectiveness%20in%20ADS%0Aapplications.%20Recently%2C%20deep%20generative%20models%20have%20emerged%20as%20promising%0Asolutions%20to%20synthesise%20realistic%20sensory%20data.%20However%2C%20for%20Lidar%20simulation%2C%0Adeep%20generative%20models%20have%20been%20primarily%20hybridised%20with%20conventional%0Aalgorithms%2C%20leaving%20unified%20generative%20approaches%20largely%20unexplored%20in%20the%0Aliterature.%20Motivated%20by%20this%20research%20gap%2C%20we%20propose%20a%20unified%20generative%0Aframework%20to%20enhance%20Lidar%20simulation%20fidelity.%20Our%20proposed%20framework%20projects%0ALidar%20point%20clouds%20into%20depth-reflectance%20images%20via%20a%20lossless%20transformation%2C%0Aand%20employs%20our%20novel%20Controllable%20Lidar%20point%20cloud%20Generative%20model%2C%20CoLiGen%2C%0Ato%20translate%20the%20images.%20We%20extensively%20evaluate%20our%20CoLiGen%20model%2C%20comparing%0Ait%20with%20the%20state-of-the-art%20image-to-image%20translation%20models%20using%20various%0Ametrics%20to%20assess%20the%20realness%2C%20faithfulness%2C%20and%20performance%20of%20a%20downstream%0Aperception%20model.%20Our%20results%20show%20that%20CoLiGen%20exhibits%20superior%20performance%0Aacross%20most%20metrics.%20The%20dataset%20and%20source%20code%20for%20this%20research%20are%0Aavailable%20at%20https%3A//github.com/hamedhaghighi/CoLiGen.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15817v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Generative%2520Framework%2520for%2520Realistic%2520Lidar%2520Simulation%2520in%250A%2520%2520Autonomous%2520Driving%2520Systems%26entry.906535625%3DHamed%2520Haghighi%2520and%2520Mehrdad%2520Dianati%2520and%2520Valentina%2520Donzella%2520and%2520Kurt%2520Debattista%26entry.1292438233%3D%2520%2520Simulation%2520models%2520for%2520perception%2520sensors%2520are%2520integral%2520components%2520of%250Aautomotive%2520simulators%2520used%2520for%2520the%2520virtual%2520Verification%2520and%2520Validation%2520%2528V%255C%2526V%2529%250Aof%2520Autonomous%2520Driving%2520Systems%2520%2528ADS%2529.%2520These%2520models%2520also%2520serve%2520as%2520powerful%2520tools%250Afor%2520generating%2520synthetic%2520datasets%2520to%2520train%2520deep%2520learning-based%2520perception%250Amodels.%2520Lidar%2520is%2520a%2520widely%2520used%2520sensor%2520type%2520among%2520the%2520perception%2520sensors%2520for%2520ADS%250Adue%2520to%2520its%2520high%2520precision%2520in%25203D%2520environment%2520scanning.%2520However%252C%2520developing%250Arealistic%2520Lidar%2520simulation%2520models%2520is%2520a%2520significant%2520technical%2520challenge.%2520In%250Aparticular%252C%2520unrealistic%2520models%2520can%2520result%2520in%2520a%2520large%2520gap%2520between%2520the%250Asynthesised%2520and%2520real-world%2520point%2520clouds%252C%2520limiting%2520their%2520effectiveness%2520in%2520ADS%250Aapplications.%2520Recently%252C%2520deep%2520generative%2520models%2520have%2520emerged%2520as%2520promising%250Asolutions%2520to%2520synthesise%2520realistic%2520sensory%2520data.%2520However%252C%2520for%2520Lidar%2520simulation%252C%250Adeep%2520generative%2520models%2520have%2520been%2520primarily%2520hybridised%2520with%2520conventional%250Aalgorithms%252C%2520leaving%2520unified%2520generative%2520approaches%2520largely%2520unexplored%2520in%2520the%250Aliterature.%2520Motivated%2520by%2520this%2520research%2520gap%252C%2520we%2520propose%2520a%2520unified%2520generative%250Aframework%2520to%2520enhance%2520Lidar%2520simulation%2520fidelity.%2520Our%2520proposed%2520framework%2520projects%250ALidar%2520point%2520clouds%2520into%2520depth-reflectance%2520images%2520via%2520a%2520lossless%2520transformation%252C%250Aand%2520employs%2520our%2520novel%2520Controllable%2520Lidar%2520point%2520cloud%2520Generative%2520model%252C%2520CoLiGen%252C%250Ato%2520translate%2520the%2520images.%2520We%2520extensively%2520evaluate%2520our%2520CoLiGen%2520model%252C%2520comparing%250Ait%2520with%2520the%2520state-of-the-art%2520image-to-image%2520translation%2520models%2520using%2520various%250Ametrics%2520to%2520assess%2520the%2520realness%252C%2520faithfulness%252C%2520and%2520performance%2520of%2520a%2520downstream%250Aperception%2520model.%2520Our%2520results%2520show%2520that%2520CoLiGen%2520exhibits%2520superior%2520performance%250Aacross%2520most%2520metrics.%2520The%2520dataset%2520and%2520source%2520code%2520for%2520this%2520research%2520are%250Aavailable%2520at%2520https%253A//github.com/hamedhaghighi/CoLiGen.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15817v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Generative%20Framework%20for%20Realistic%20Lidar%20Simulation%20in%0A%20%20Autonomous%20Driving%20Systems&entry.906535625=Hamed%20Haghighi%20and%20Mehrdad%20Dianati%20and%20Valentina%20Donzella%20and%20Kurt%20Debattista&entry.1292438233=%20%20Simulation%20models%20for%20perception%20sensors%20are%20integral%20components%20of%0Aautomotive%20simulators%20used%20for%20the%20virtual%20Verification%20and%20Validation%20%28V%5C%26V%29%0Aof%20Autonomous%20Driving%20Systems%20%28ADS%29.%20These%20models%20also%20serve%20as%20powerful%20tools%0Afor%20generating%20synthetic%20datasets%20to%20train%20deep%20learning-based%20perception%0Amodels.%20Lidar%20is%20a%20widely%20used%20sensor%20type%20among%20the%20perception%20sensors%20for%20ADS%0Adue%20to%20its%20high%20precision%20in%203D%20environment%20scanning.%20However%2C%20developing%0Arealistic%20Lidar%20simulation%20models%20is%20a%20significant%20technical%20challenge.%20In%0Aparticular%2C%20unrealistic%20models%20can%20result%20in%20a%20large%20gap%20between%20the%0Asynthesised%20and%20real-world%20point%20clouds%2C%20limiting%20their%20effectiveness%20in%20ADS%0Aapplications.%20Recently%2C%20deep%20generative%20models%20have%20emerged%20as%20promising%0Asolutions%20to%20synthesise%20realistic%20sensory%20data.%20However%2C%20for%20Lidar%20simulation%2C%0Adeep%20generative%20models%20have%20been%20primarily%20hybridised%20with%20conventional%0Aalgorithms%2C%20leaving%20unified%20generative%20approaches%20largely%20unexplored%20in%20the%0Aliterature.%20Motivated%20by%20this%20research%20gap%2C%20we%20propose%20a%20unified%20generative%0Aframework%20to%20enhance%20Lidar%20simulation%20fidelity.%20Our%20proposed%20framework%20projects%0ALidar%20point%20clouds%20into%20depth-reflectance%20images%20via%20a%20lossless%20transformation%2C%0Aand%20employs%20our%20novel%20Controllable%20Lidar%20point%20cloud%20Generative%20model%2C%20CoLiGen%2C%0Ato%20translate%20the%20images.%20We%20extensively%20evaluate%20our%20CoLiGen%20model%2C%20comparing%0Ait%20with%20the%20state-of-the-art%20image-to-image%20translation%20models%20using%20various%0Ametrics%20to%20assess%20the%20realness%2C%20faithfulness%2C%20and%20performance%20of%20a%20downstream%0Aperception%20model.%20Our%20results%20show%20that%20CoLiGen%20exhibits%20superior%20performance%0Aacross%20most%20metrics.%20The%20dataset%20and%20source%20code%20for%20this%20research%20are%0Aavailable%20at%20https%3A//github.com/hamedhaghighi/CoLiGen.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15817v2&entry.124074799=Read"},
{"title": "Personalized Visual Instruction Tuning", "author": "Renjie Pi and Jianshu Zhang and Tianyang Han and Jipeng Zhang and Rui Pan and Tong Zhang", "abstract": "  Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated significant progress; however, these models exhibit a notable\nlimitation, which we refer to as \"face blindness\". Specifically, they can\nengage in general conversations but fail to conduct personalized dialogues\ntargeting at specific individuals. This deficiency hinders the application of\nMLLMs in personalized settings, such as tailored visual assistants on mobile\ndevices, or domestic robots that need to recognize members of the family. In\nthis paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel\ndata curation and training framework designed to enable MLLMs to identify\ntarget individuals within an image and engage in personalized and coherent\ndialogues. Our approach involves the development of a sophisticated pipeline\nthat autonomously generates training data containing personalized\nconversations. This pipeline leverages the capabilities of various visual\nexperts, image generation models, and (multi-modal) large language models. To\nevaluate the personalized potential of MLLMs, we present a benchmark called\nP-Bench, which encompasses various question types with different levels of\ndifficulty. The experiments demonstrate a substantial personalized performance\nenhancement after fine-tuning with our curated dataset.\n", "link": "http://arxiv.org/abs/2410.07113v1", "date": "2024-10-09", "relevancy": 2.2624, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5669}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5662}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Visual%20Instruction%20Tuning&body=Title%3A%20Personalized%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Renjie%20Pi%20and%20Jianshu%20Zhang%20and%20Tianyang%20Han%20and%20Jipeng%20Zhang%20and%20Rui%20Pan%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Ademonstrated%20significant%20progress%3B%20however%2C%20these%20models%20exhibit%20a%20notable%0Alimitation%2C%20which%20we%20refer%20to%20as%20%22face%20blindness%22.%20Specifically%2C%20they%20can%0Aengage%20in%20general%20conversations%20but%20fail%20to%20conduct%20personalized%20dialogues%0Atargeting%20at%20specific%20individuals.%20This%20deficiency%20hinders%20the%20application%20of%0AMLLMs%20in%20personalized%20settings%2C%20such%20as%20tailored%20visual%20assistants%20on%20mobile%0Adevices%2C%20or%20domestic%20robots%20that%20need%20to%20recognize%20members%20of%20the%20family.%20In%0Athis%20paper%2C%20we%20introduce%20Personalized%20Visual%20Instruction%20Tuning%20%28PVIT%29%2C%20a%20novel%0Adata%20curation%20and%20training%20framework%20designed%20to%20enable%20MLLMs%20to%20identify%0Atarget%20individuals%20within%20an%20image%20and%20engage%20in%20personalized%20and%20coherent%0Adialogues.%20Our%20approach%20involves%20the%20development%20of%20a%20sophisticated%20pipeline%0Athat%20autonomously%20generates%20training%20data%20containing%20personalized%0Aconversations.%20This%20pipeline%20leverages%20the%20capabilities%20of%20various%20visual%0Aexperts%2C%20image%20generation%20models%2C%20and%20%28multi-modal%29%20large%20language%20models.%20To%0Aevaluate%20the%20personalized%20potential%20of%20MLLMs%2C%20we%20present%20a%20benchmark%20called%0AP-Bench%2C%20which%20encompasses%20various%20question%20types%20with%20different%20levels%20of%0Adifficulty.%20The%20experiments%20demonstrate%20a%20substantial%20personalized%20performance%0Aenhancement%20after%20fine-tuning%20with%20our%20curated%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Visual%2520Instruction%2520Tuning%26entry.906535625%3DRenjie%2520Pi%2520and%2520Jianshu%2520Zhang%2520and%2520Tianyang%2520Han%2520and%2520Jipeng%2520Zhang%2520and%2520Rui%2520Pan%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%250Ademonstrated%2520significant%2520progress%253B%2520however%252C%2520these%2520models%2520exhibit%2520a%2520notable%250Alimitation%252C%2520which%2520we%2520refer%2520to%2520as%2520%2522face%2520blindness%2522.%2520Specifically%252C%2520they%2520can%250Aengage%2520in%2520general%2520conversations%2520but%2520fail%2520to%2520conduct%2520personalized%2520dialogues%250Atargeting%2520at%2520specific%2520individuals.%2520This%2520deficiency%2520hinders%2520the%2520application%2520of%250AMLLMs%2520in%2520personalized%2520settings%252C%2520such%2520as%2520tailored%2520visual%2520assistants%2520on%2520mobile%250Adevices%252C%2520or%2520domestic%2520robots%2520that%2520need%2520to%2520recognize%2520members%2520of%2520the%2520family.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520Personalized%2520Visual%2520Instruction%2520Tuning%2520%2528PVIT%2529%252C%2520a%2520novel%250Adata%2520curation%2520and%2520training%2520framework%2520designed%2520to%2520enable%2520MLLMs%2520to%2520identify%250Atarget%2520individuals%2520within%2520an%2520image%2520and%2520engage%2520in%2520personalized%2520and%2520coherent%250Adialogues.%2520Our%2520approach%2520involves%2520the%2520development%2520of%2520a%2520sophisticated%2520pipeline%250Athat%2520autonomously%2520generates%2520training%2520data%2520containing%2520personalized%250Aconversations.%2520This%2520pipeline%2520leverages%2520the%2520capabilities%2520of%2520various%2520visual%250Aexperts%252C%2520image%2520generation%2520models%252C%2520and%2520%2528multi-modal%2529%2520large%2520language%2520models.%2520To%250Aevaluate%2520the%2520personalized%2520potential%2520of%2520MLLMs%252C%2520we%2520present%2520a%2520benchmark%2520called%250AP-Bench%252C%2520which%2520encompasses%2520various%2520question%2520types%2520with%2520different%2520levels%2520of%250Adifficulty.%2520The%2520experiments%2520demonstrate%2520a%2520substantial%2520personalized%2520performance%250Aenhancement%2520after%2520fine-tuning%2520with%2520our%2520curated%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Visual%20Instruction%20Tuning&entry.906535625=Renjie%20Pi%20and%20Jianshu%20Zhang%20and%20Tianyang%20Han%20and%20Jipeng%20Zhang%20and%20Rui%20Pan%20and%20Tong%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Ademonstrated%20significant%20progress%3B%20however%2C%20these%20models%20exhibit%20a%20notable%0Alimitation%2C%20which%20we%20refer%20to%20as%20%22face%20blindness%22.%20Specifically%2C%20they%20can%0Aengage%20in%20general%20conversations%20but%20fail%20to%20conduct%20personalized%20dialogues%0Atargeting%20at%20specific%20individuals.%20This%20deficiency%20hinders%20the%20application%20of%0AMLLMs%20in%20personalized%20settings%2C%20such%20as%20tailored%20visual%20assistants%20on%20mobile%0Adevices%2C%20or%20domestic%20robots%20that%20need%20to%20recognize%20members%20of%20the%20family.%20In%0Athis%20paper%2C%20we%20introduce%20Personalized%20Visual%20Instruction%20Tuning%20%28PVIT%29%2C%20a%20novel%0Adata%20curation%20and%20training%20framework%20designed%20to%20enable%20MLLMs%20to%20identify%0Atarget%20individuals%20within%20an%20image%20and%20engage%20in%20personalized%20and%20coherent%0Adialogues.%20Our%20approach%20involves%20the%20development%20of%20a%20sophisticated%20pipeline%0Athat%20autonomously%20generates%20training%20data%20containing%20personalized%0Aconversations.%20This%20pipeline%20leverages%20the%20capabilities%20of%20various%20visual%0Aexperts%2C%20image%20generation%20models%2C%20and%20%28multi-modal%29%20large%20language%20models.%20To%0Aevaluate%20the%20personalized%20potential%20of%20MLLMs%2C%20we%20present%20a%20benchmark%20called%0AP-Bench%2C%20which%20encompasses%20various%20question%20types%20with%20different%20levels%20of%0Adifficulty.%20The%20experiments%20demonstrate%20a%20substantial%20personalized%20performance%0Aenhancement%20after%20fine-tuning%20with%20our%20curated%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07113v1&entry.124074799=Read"},
{"title": "Personal Intelligence System UniLM: Hybrid On-Device Small Language\n  Model and Server-Based Large Language Model for Malay Nusantara", "author": "Azree Nazri and Olalekan Agbolade and Faisal Aziz", "abstract": "  In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B.\n", "link": "http://arxiv.org/abs/2410.06973v1", "date": "2024-10-09", "relevancy": 2.247, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personal%20Intelligence%20System%20UniLM%3A%20Hybrid%20On-Device%20Small%20Language%0A%20%20Model%20and%20Server-Based%20Large%20Language%20Model%20for%20Malay%20Nusantara&body=Title%3A%20Personal%20Intelligence%20System%20UniLM%3A%20Hybrid%20On-Device%20Small%20Language%0A%20%20Model%20and%20Server-Based%20Large%20Language%20Model%20for%20Malay%20Nusantara%0AAuthor%3A%20Azree%20Nazri%20and%20Olalekan%20Agbolade%20and%20Faisal%20Aziz%0AAbstract%3A%20%20%20In%20contexts%20with%20limited%20computational%20and%20data%20resources%2C%20high-resource%0Alanguage%20models%20often%20prove%20inadequate%2C%20particularly%20when%20addressing%20the%0Aspecific%20needs%20of%20Malay%20languages.%20This%20paper%20introduces%20a%20Personal%0AIntelligence%20System%20designed%20to%20efficiently%20integrate%20both%20on-device%20and%0Aserver-based%20models.%20The%20system%20incorporates%20SLiM-34M%20for%20on-device%20processing%2C%0Aoptimized%20for%20low%20memory%20and%20power%20usage%2C%20and%20MANYAK-1.3B%20for%20server-based%0Atasks%2C%20allowing%20for%20scalable%2C%20high-performance%20language%20processing.%20The%20models%0Aachieve%20significant%20results%20across%20various%20tasks%2C%20such%20as%20machine%20translation%2C%0Aquestion-answering%2C%20and%20translate%20IndoMMLU.%20Particularly%20noteworthy%20is%0ASLiM-34M%27s%20ability%20to%20achieve%20a%20high%20improvement%20in%20accuracy%20compared%20to%20other%0ALLMs%20while%20using%202%20times%20fewer%20pre-training%20tokens.%20This%20work%20challenges%20the%0Aprevailing%20assumption%20that%20large-scale%20computational%20resources%20are%20necessary%20to%0Abuild%20effective%20language%20models%2C%20contributing%20to%20the%20development%20of%0Aresource-efficient%20models%20for%20the%20Malay%20language%20with%20the%20unique%20orchestration%0Abetween%20SLiM-34M%20and%20MANYAK-1.3B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonal%2520Intelligence%2520System%2520UniLM%253A%2520Hybrid%2520On-Device%2520Small%2520Language%250A%2520%2520Model%2520and%2520Server-Based%2520Large%2520Language%2520Model%2520for%2520Malay%2520Nusantara%26entry.906535625%3DAzree%2520Nazri%2520and%2520Olalekan%2520Agbolade%2520and%2520Faisal%2520Aziz%26entry.1292438233%3D%2520%2520In%2520contexts%2520with%2520limited%2520computational%2520and%2520data%2520resources%252C%2520high-resource%250Alanguage%2520models%2520often%2520prove%2520inadequate%252C%2520particularly%2520when%2520addressing%2520the%250Aspecific%2520needs%2520of%2520Malay%2520languages.%2520This%2520paper%2520introduces%2520a%2520Personal%250AIntelligence%2520System%2520designed%2520to%2520efficiently%2520integrate%2520both%2520on-device%2520and%250Aserver-based%2520models.%2520The%2520system%2520incorporates%2520SLiM-34M%2520for%2520on-device%2520processing%252C%250Aoptimized%2520for%2520low%2520memory%2520and%2520power%2520usage%252C%2520and%2520MANYAK-1.3B%2520for%2520server-based%250Atasks%252C%2520allowing%2520for%2520scalable%252C%2520high-performance%2520language%2520processing.%2520The%2520models%250Aachieve%2520significant%2520results%2520across%2520various%2520tasks%252C%2520such%2520as%2520machine%2520translation%252C%250Aquestion-answering%252C%2520and%2520translate%2520IndoMMLU.%2520Particularly%2520noteworthy%2520is%250ASLiM-34M%2527s%2520ability%2520to%2520achieve%2520a%2520high%2520improvement%2520in%2520accuracy%2520compared%2520to%2520other%250ALLMs%2520while%2520using%25202%2520times%2520fewer%2520pre-training%2520tokens.%2520This%2520work%2520challenges%2520the%250Aprevailing%2520assumption%2520that%2520large-scale%2520computational%2520resources%2520are%2520necessary%2520to%250Abuild%2520effective%2520language%2520models%252C%2520contributing%2520to%2520the%2520development%2520of%250Aresource-efficient%2520models%2520for%2520the%2520Malay%2520language%2520with%2520the%2520unique%2520orchestration%250Abetween%2520SLiM-34M%2520and%2520MANYAK-1.3B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personal%20Intelligence%20System%20UniLM%3A%20Hybrid%20On-Device%20Small%20Language%0A%20%20Model%20and%20Server-Based%20Large%20Language%20Model%20for%20Malay%20Nusantara&entry.906535625=Azree%20Nazri%20and%20Olalekan%20Agbolade%20and%20Faisal%20Aziz&entry.1292438233=%20%20In%20contexts%20with%20limited%20computational%20and%20data%20resources%2C%20high-resource%0Alanguage%20models%20often%20prove%20inadequate%2C%20particularly%20when%20addressing%20the%0Aspecific%20needs%20of%20Malay%20languages.%20This%20paper%20introduces%20a%20Personal%0AIntelligence%20System%20designed%20to%20efficiently%20integrate%20both%20on-device%20and%0Aserver-based%20models.%20The%20system%20incorporates%20SLiM-34M%20for%20on-device%20processing%2C%0Aoptimized%20for%20low%20memory%20and%20power%20usage%2C%20and%20MANYAK-1.3B%20for%20server-based%0Atasks%2C%20allowing%20for%20scalable%2C%20high-performance%20language%20processing.%20The%20models%0Aachieve%20significant%20results%20across%20various%20tasks%2C%20such%20as%20machine%20translation%2C%0Aquestion-answering%2C%20and%20translate%20IndoMMLU.%20Particularly%20noteworthy%20is%0ASLiM-34M%27s%20ability%20to%20achieve%20a%20high%20improvement%20in%20accuracy%20compared%20to%20other%0ALLMs%20while%20using%202%20times%20fewer%20pre-training%20tokens.%20This%20work%20challenges%20the%0Aprevailing%20assumption%20that%20large-scale%20computational%20resources%20are%20necessary%20to%0Abuild%20effective%20language%20models%2C%20contributing%20to%20the%20development%20of%0Aresource-efficient%20models%20for%20the%20Malay%20language%20with%20the%20unique%20orchestration%0Abetween%20SLiM-34M%20and%20MANYAK-1.3B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06973v1&entry.124074799=Read"},
{"title": "LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and\n  Captioning", "author": "Zhe Li and Weihao Yuan and Yisheng He and Lingteng Qiu and Shenhao Zhu and Xiaodong Gu and Weichao Shen and Yuan Dong and Zilong Dong and Laurence T. Yang", "abstract": "  Language plays a vital role in the realm of human motion. Existing methods\nhave largely depended on CLIP text embeddings for motion generation, yet they\nfall short in effectively aligning language and motion due to CLIP's\npretraining on static image-text pairs. This work introduces LaMP, a novel\nLanguage-Motion Pretraining model, which transitions from a language-vision to\na more suitable language-motion latent space. It addresses key limitations by\ngenerating motion-informative text embeddings, significantly enhancing the\nrelevance and semantics of generated motion sequences. With LaMP, we advance\nthree key tasks: text-to-motion generation, motion-text retrieval, and motion\ncaptioning through aligned language-motion representation learning. For\ngeneration, we utilize LaMP to provide the text condition instead of CLIP, and\nan autoregressive masked prediction is designed to achieve mask modeling\nwithout rank collapse in transformers. For retrieval, motion features from\nLaMP's motion transformer interact with query tokens to retrieve text features\nfrom the text transformer, and vice versa. For captioning, we finetune a large\nlanguage model with the language-informative motion features to develop a\nstrong motion captioning model. In addition, we introduce the LaMP-BertScore\nmetric to assess the alignment of generated motions with textual descriptions.\nExtensive experimental results on multiple datasets demonstrate substantial\nimprovements over previous methods across all three tasks. The code of our\nmethod will be made public.\n", "link": "http://arxiv.org/abs/2410.07093v1", "date": "2024-10-09", "relevancy": 2.2363, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5823}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5703}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaMP%3A%20Language-Motion%20Pretraining%20for%20Motion%20Generation%2C%20Retrieval%2C%20and%0A%20%20Captioning&body=Title%3A%20LaMP%3A%20Language-Motion%20Pretraining%20for%20Motion%20Generation%2C%20Retrieval%2C%20and%0A%20%20Captioning%0AAuthor%3A%20Zhe%20Li%20and%20Weihao%20Yuan%20and%20Yisheng%20He%20and%20Lingteng%20Qiu%20and%20Shenhao%20Zhu%20and%20Xiaodong%20Gu%20and%20Weichao%20Shen%20and%20Yuan%20Dong%20and%20Zilong%20Dong%20and%20Laurence%20T.%20Yang%0AAbstract%3A%20%20%20Language%20plays%20a%20vital%20role%20in%20the%20realm%20of%20human%20motion.%20Existing%20methods%0Ahave%20largely%20depended%20on%20CLIP%20text%20embeddings%20for%20motion%20generation%2C%20yet%20they%0Afall%20short%20in%20effectively%20aligning%20language%20and%20motion%20due%20to%20CLIP%27s%0Apretraining%20on%20static%20image-text%20pairs.%20This%20work%20introduces%20LaMP%2C%20a%20novel%0ALanguage-Motion%20Pretraining%20model%2C%20which%20transitions%20from%20a%20language-vision%20to%0Aa%20more%20suitable%20language-motion%20latent%20space.%20It%20addresses%20key%20limitations%20by%0Agenerating%20motion-informative%20text%20embeddings%2C%20significantly%20enhancing%20the%0Arelevance%20and%20semantics%20of%20generated%20motion%20sequences.%20With%20LaMP%2C%20we%20advance%0Athree%20key%20tasks%3A%20text-to-motion%20generation%2C%20motion-text%20retrieval%2C%20and%20motion%0Acaptioning%20through%20aligned%20language-motion%20representation%20learning.%20For%0Ageneration%2C%20we%20utilize%20LaMP%20to%20provide%20the%20text%20condition%20instead%20of%20CLIP%2C%20and%0Aan%20autoregressive%20masked%20prediction%20is%20designed%20to%20achieve%20mask%20modeling%0Awithout%20rank%20collapse%20in%20transformers.%20For%20retrieval%2C%20motion%20features%20from%0ALaMP%27s%20motion%20transformer%20interact%20with%20query%20tokens%20to%20retrieve%20text%20features%0Afrom%20the%20text%20transformer%2C%20and%20vice%20versa.%20For%20captioning%2C%20we%20finetune%20a%20large%0Alanguage%20model%20with%20the%20language-informative%20motion%20features%20to%20develop%20a%0Astrong%20motion%20captioning%20model.%20In%20addition%2C%20we%20introduce%20the%20LaMP-BertScore%0Ametric%20to%20assess%20the%20alignment%20of%20generated%20motions%20with%20textual%20descriptions.%0AExtensive%20experimental%20results%20on%20multiple%20datasets%20demonstrate%20substantial%0Aimprovements%20over%20previous%20methods%20across%20all%20three%20tasks.%20The%20code%20of%20our%0Amethod%20will%20be%20made%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaMP%253A%2520Language-Motion%2520Pretraining%2520for%2520Motion%2520Generation%252C%2520Retrieval%252C%2520and%250A%2520%2520Captioning%26entry.906535625%3DZhe%2520Li%2520and%2520Weihao%2520Yuan%2520and%2520Yisheng%2520He%2520and%2520Lingteng%2520Qiu%2520and%2520Shenhao%2520Zhu%2520and%2520Xiaodong%2520Gu%2520and%2520Weichao%2520Shen%2520and%2520Yuan%2520Dong%2520and%2520Zilong%2520Dong%2520and%2520Laurence%2520T.%2520Yang%26entry.1292438233%3D%2520%2520Language%2520plays%2520a%2520vital%2520role%2520in%2520the%2520realm%2520of%2520human%2520motion.%2520Existing%2520methods%250Ahave%2520largely%2520depended%2520on%2520CLIP%2520text%2520embeddings%2520for%2520motion%2520generation%252C%2520yet%2520they%250Afall%2520short%2520in%2520effectively%2520aligning%2520language%2520and%2520motion%2520due%2520to%2520CLIP%2527s%250Apretraining%2520on%2520static%2520image-text%2520pairs.%2520This%2520work%2520introduces%2520LaMP%252C%2520a%2520novel%250ALanguage-Motion%2520Pretraining%2520model%252C%2520which%2520transitions%2520from%2520a%2520language-vision%2520to%250Aa%2520more%2520suitable%2520language-motion%2520latent%2520space.%2520It%2520addresses%2520key%2520limitations%2520by%250Agenerating%2520motion-informative%2520text%2520embeddings%252C%2520significantly%2520enhancing%2520the%250Arelevance%2520and%2520semantics%2520of%2520generated%2520motion%2520sequences.%2520With%2520LaMP%252C%2520we%2520advance%250Athree%2520key%2520tasks%253A%2520text-to-motion%2520generation%252C%2520motion-text%2520retrieval%252C%2520and%2520motion%250Acaptioning%2520through%2520aligned%2520language-motion%2520representation%2520learning.%2520For%250Ageneration%252C%2520we%2520utilize%2520LaMP%2520to%2520provide%2520the%2520text%2520condition%2520instead%2520of%2520CLIP%252C%2520and%250Aan%2520autoregressive%2520masked%2520prediction%2520is%2520designed%2520to%2520achieve%2520mask%2520modeling%250Awithout%2520rank%2520collapse%2520in%2520transformers.%2520For%2520retrieval%252C%2520motion%2520features%2520from%250ALaMP%2527s%2520motion%2520transformer%2520interact%2520with%2520query%2520tokens%2520to%2520retrieve%2520text%2520features%250Afrom%2520the%2520text%2520transformer%252C%2520and%2520vice%2520versa.%2520For%2520captioning%252C%2520we%2520finetune%2520a%2520large%250Alanguage%2520model%2520with%2520the%2520language-informative%2520motion%2520features%2520to%2520develop%2520a%250Astrong%2520motion%2520captioning%2520model.%2520In%2520addition%252C%2520we%2520introduce%2520the%2520LaMP-BertScore%250Ametric%2520to%2520assess%2520the%2520alignment%2520of%2520generated%2520motions%2520with%2520textual%2520descriptions.%250AExtensive%2520experimental%2520results%2520on%2520multiple%2520datasets%2520demonstrate%2520substantial%250Aimprovements%2520over%2520previous%2520methods%2520across%2520all%2520three%2520tasks.%2520The%2520code%2520of%2520our%250Amethod%2520will%2520be%2520made%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaMP%3A%20Language-Motion%20Pretraining%20for%20Motion%20Generation%2C%20Retrieval%2C%20and%0A%20%20Captioning&entry.906535625=Zhe%20Li%20and%20Weihao%20Yuan%20and%20Yisheng%20He%20and%20Lingteng%20Qiu%20and%20Shenhao%20Zhu%20and%20Xiaodong%20Gu%20and%20Weichao%20Shen%20and%20Yuan%20Dong%20and%20Zilong%20Dong%20and%20Laurence%20T.%20Yang&entry.1292438233=%20%20Language%20plays%20a%20vital%20role%20in%20the%20realm%20of%20human%20motion.%20Existing%20methods%0Ahave%20largely%20depended%20on%20CLIP%20text%20embeddings%20for%20motion%20generation%2C%20yet%20they%0Afall%20short%20in%20effectively%20aligning%20language%20and%20motion%20due%20to%20CLIP%27s%0Apretraining%20on%20static%20image-text%20pairs.%20This%20work%20introduces%20LaMP%2C%20a%20novel%0ALanguage-Motion%20Pretraining%20model%2C%20which%20transitions%20from%20a%20language-vision%20to%0Aa%20more%20suitable%20language-motion%20latent%20space.%20It%20addresses%20key%20limitations%20by%0Agenerating%20motion-informative%20text%20embeddings%2C%20significantly%20enhancing%20the%0Arelevance%20and%20semantics%20of%20generated%20motion%20sequences.%20With%20LaMP%2C%20we%20advance%0Athree%20key%20tasks%3A%20text-to-motion%20generation%2C%20motion-text%20retrieval%2C%20and%20motion%0Acaptioning%20through%20aligned%20language-motion%20representation%20learning.%20For%0Ageneration%2C%20we%20utilize%20LaMP%20to%20provide%20the%20text%20condition%20instead%20of%20CLIP%2C%20and%0Aan%20autoregressive%20masked%20prediction%20is%20designed%20to%20achieve%20mask%20modeling%0Awithout%20rank%20collapse%20in%20transformers.%20For%20retrieval%2C%20motion%20features%20from%0ALaMP%27s%20motion%20transformer%20interact%20with%20query%20tokens%20to%20retrieve%20text%20features%0Afrom%20the%20text%20transformer%2C%20and%20vice%20versa.%20For%20captioning%2C%20we%20finetune%20a%20large%0Alanguage%20model%20with%20the%20language-informative%20motion%20features%20to%20develop%20a%0Astrong%20motion%20captioning%20model.%20In%20addition%2C%20we%20introduce%20the%20LaMP-BertScore%0Ametric%20to%20assess%20the%20alignment%20of%20generated%20motions%20with%20textual%20descriptions.%0AExtensive%20experimental%20results%20on%20multiple%20datasets%20demonstrate%20substantial%0Aimprovements%20over%20previous%20methods%20across%20all%20three%20tasks.%20The%20code%20of%20our%0Amethod%20will%20be%20made%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07093v1&entry.124074799=Read"},
{"title": "Noise is All You Need: Private Second-Order Convergence of Noisy SGD", "author": "Dmitrii Avdiukhin and Michael Dinitz and Chenglin Fan and Grigory Yaroslavtsev", "abstract": "  Private optimization is a topic of major interest in machine learning, with\ndifferentially private stochastic gradient descent (DP-SGD) playing a key role\nin both theory and practice. Furthermore, DP-SGD is known to be a powerful tool\nin contexts beyond privacy, including robustness, machine unlearning, etc.\nExisting analyses of DP-SGD either make relatively strong assumptions (e.g.,\nLipschitz continuity of the loss function, or even convexity) or prove only\nfirst-order convergence (and thus might end at a saddle point in the non-convex\nsetting). At the same time, there has been progress in proving second-order\nconvergence of the non-private version of ``noisy SGD'', as well as progress in\ndesigning algorithms that are more complex than DP-SGD and do guarantee\nsecond-order convergence. We revisit DP-SGD and show that ``noise is all you\nneed'': the noise necessary for privacy already implies second-order\nconvergence under the standard smoothness assumptions, even for non-Lipschitz\nloss functions. Hence, we get second-order convergence essentially for free:\nDP-SGD, the workhorse of modern private optimization, under minimal assumptions\ncan be used to find a second-order stationary point.\n", "link": "http://arxiv.org/abs/2410.06878v1", "date": "2024-10-09", "relevancy": 2.235, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4572}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4448}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise%20is%20All%20You%20Need%3A%20Private%20Second-Order%20Convergence%20of%20Noisy%20SGD&body=Title%3A%20Noise%20is%20All%20You%20Need%3A%20Private%20Second-Order%20Convergence%20of%20Noisy%20SGD%0AAuthor%3A%20Dmitrii%20Avdiukhin%20and%20Michael%20Dinitz%20and%20Chenglin%20Fan%20and%20Grigory%20Yaroslavtsev%0AAbstract%3A%20%20%20Private%20optimization%20is%20a%20topic%20of%20major%20interest%20in%20machine%20learning%2C%20with%0Adifferentially%20private%20stochastic%20gradient%20descent%20%28DP-SGD%29%20playing%20a%20key%20role%0Ain%20both%20theory%20and%20practice.%20Furthermore%2C%20DP-SGD%20is%20known%20to%20be%20a%20powerful%20tool%0Ain%20contexts%20beyond%20privacy%2C%20including%20robustness%2C%20machine%20unlearning%2C%20etc.%0AExisting%20analyses%20of%20DP-SGD%20either%20make%20relatively%20strong%20assumptions%20%28e.g.%2C%0ALipschitz%20continuity%20of%20the%20loss%20function%2C%20or%20even%20convexity%29%20or%20prove%20only%0Afirst-order%20convergence%20%28and%20thus%20might%20end%20at%20a%20saddle%20point%20in%20the%20non-convex%0Asetting%29.%20At%20the%20same%20time%2C%20there%20has%20been%20progress%20in%20proving%20second-order%0Aconvergence%20of%20the%20non-private%20version%20of%20%60%60noisy%20SGD%27%27%2C%20as%20well%20as%20progress%20in%0Adesigning%20algorithms%20that%20are%20more%20complex%20than%20DP-SGD%20and%20do%20guarantee%0Asecond-order%20convergence.%20We%20revisit%20DP-SGD%20and%20show%20that%20%60%60noise%20is%20all%20you%0Aneed%27%27%3A%20the%20noise%20necessary%20for%20privacy%20already%20implies%20second-order%0Aconvergence%20under%20the%20standard%20smoothness%20assumptions%2C%20even%20for%20non-Lipschitz%0Aloss%20functions.%20Hence%2C%20we%20get%20second-order%20convergence%20essentially%20for%20free%3A%0ADP-SGD%2C%20the%20workhorse%20of%20modern%20private%20optimization%2C%20under%20minimal%20assumptions%0Acan%20be%20used%20to%20find%20a%20second-order%20stationary%20point.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise%2520is%2520All%2520You%2520Need%253A%2520Private%2520Second-Order%2520Convergence%2520of%2520Noisy%2520SGD%26entry.906535625%3DDmitrii%2520Avdiukhin%2520and%2520Michael%2520Dinitz%2520and%2520Chenglin%2520Fan%2520and%2520Grigory%2520Yaroslavtsev%26entry.1292438233%3D%2520%2520Private%2520optimization%2520is%2520a%2520topic%2520of%2520major%2520interest%2520in%2520machine%2520learning%252C%2520with%250Adifferentially%2520private%2520stochastic%2520gradient%2520descent%2520%2528DP-SGD%2529%2520playing%2520a%2520key%2520role%250Ain%2520both%2520theory%2520and%2520practice.%2520Furthermore%252C%2520DP-SGD%2520is%2520known%2520to%2520be%2520a%2520powerful%2520tool%250Ain%2520contexts%2520beyond%2520privacy%252C%2520including%2520robustness%252C%2520machine%2520unlearning%252C%2520etc.%250AExisting%2520analyses%2520of%2520DP-SGD%2520either%2520make%2520relatively%2520strong%2520assumptions%2520%2528e.g.%252C%250ALipschitz%2520continuity%2520of%2520the%2520loss%2520function%252C%2520or%2520even%2520convexity%2529%2520or%2520prove%2520only%250Afirst-order%2520convergence%2520%2528and%2520thus%2520might%2520end%2520at%2520a%2520saddle%2520point%2520in%2520the%2520non-convex%250Asetting%2529.%2520At%2520the%2520same%2520time%252C%2520there%2520has%2520been%2520progress%2520in%2520proving%2520second-order%250Aconvergence%2520of%2520the%2520non-private%2520version%2520of%2520%2560%2560noisy%2520SGD%2527%2527%252C%2520as%2520well%2520as%2520progress%2520in%250Adesigning%2520algorithms%2520that%2520are%2520more%2520complex%2520than%2520DP-SGD%2520and%2520do%2520guarantee%250Asecond-order%2520convergence.%2520We%2520revisit%2520DP-SGD%2520and%2520show%2520that%2520%2560%2560noise%2520is%2520all%2520you%250Aneed%2527%2527%253A%2520the%2520noise%2520necessary%2520for%2520privacy%2520already%2520implies%2520second-order%250Aconvergence%2520under%2520the%2520standard%2520smoothness%2520assumptions%252C%2520even%2520for%2520non-Lipschitz%250Aloss%2520functions.%2520Hence%252C%2520we%2520get%2520second-order%2520convergence%2520essentially%2520for%2520free%253A%250ADP-SGD%252C%2520the%2520workhorse%2520of%2520modern%2520private%2520optimization%252C%2520under%2520minimal%2520assumptions%250Acan%2520be%2520used%2520to%2520find%2520a%2520second-order%2520stationary%2520point.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise%20is%20All%20You%20Need%3A%20Private%20Second-Order%20Convergence%20of%20Noisy%20SGD&entry.906535625=Dmitrii%20Avdiukhin%20and%20Michael%20Dinitz%20and%20Chenglin%20Fan%20and%20Grigory%20Yaroslavtsev&entry.1292438233=%20%20Private%20optimization%20is%20a%20topic%20of%20major%20interest%20in%20machine%20learning%2C%20with%0Adifferentially%20private%20stochastic%20gradient%20descent%20%28DP-SGD%29%20playing%20a%20key%20role%0Ain%20both%20theory%20and%20practice.%20Furthermore%2C%20DP-SGD%20is%20known%20to%20be%20a%20powerful%20tool%0Ain%20contexts%20beyond%20privacy%2C%20including%20robustness%2C%20machine%20unlearning%2C%20etc.%0AExisting%20analyses%20of%20DP-SGD%20either%20make%20relatively%20strong%20assumptions%20%28e.g.%2C%0ALipschitz%20continuity%20of%20the%20loss%20function%2C%20or%20even%20convexity%29%20or%20prove%20only%0Afirst-order%20convergence%20%28and%20thus%20might%20end%20at%20a%20saddle%20point%20in%20the%20non-convex%0Asetting%29.%20At%20the%20same%20time%2C%20there%20has%20been%20progress%20in%20proving%20second-order%0Aconvergence%20of%20the%20non-private%20version%20of%20%60%60noisy%20SGD%27%27%2C%20as%20well%20as%20progress%20in%0Adesigning%20algorithms%20that%20are%20more%20complex%20than%20DP-SGD%20and%20do%20guarantee%0Asecond-order%20convergence.%20We%20revisit%20DP-SGD%20and%20show%20that%20%60%60noise%20is%20all%20you%0Aneed%27%27%3A%20the%20noise%20necessary%20for%20privacy%20already%20implies%20second-order%0Aconvergence%20under%20the%20standard%20smoothness%20assumptions%2C%20even%20for%20non-Lipschitz%0Aloss%20functions.%20Hence%2C%20we%20get%20second-order%20convergence%20essentially%20for%20free%3A%0ADP-SGD%2C%20the%20workhorse%20of%20modern%20private%20optimization%2C%20under%20minimal%20assumptions%0Acan%20be%20used%20to%20find%20a%20second-order%20stationary%20point.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06878v1&entry.124074799=Read"},
{"title": "A Poincar\u00e9 Inequality and Consistency Results for Signal Sampling on\n  Large Graphs", "author": "Thien Le and Luana Ruiz and Stefanie Jegelka", "abstract": "  Large-scale graph machine learning is challenging as the complexity of\nlearning models scales with the graph size. Subsampling the graph is a viable\nalternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.\nExisting graph sampling techniques require not only computing the spectra of\nlarge matrices but also repeating these computations when the graph changes,\ne.g., grows. In this paper, we introduce a signal sampling theory for a type of\ngraph limit -- the graphon. We prove a Poincar\\'e inequality for graphon\nsignals and show that complements of node subsets satisfying this inequality\nare unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting\nconnections with spectral clustering and Gaussian elimination, we prove that\nsuch sampling sets are consistent in the sense that unique sampling sets on a\nconvergent graph sequence converge to unique sampling sets on the graphon. We\nthen propose a related graphon signal sampling algorithm for large graphs, and\ndemonstrate its good empirical performance on graph machine learning tasks.\n", "link": "http://arxiv.org/abs/2311.10610v3", "date": "2024-10-09", "relevancy": 2.2285, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4568}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4509}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Poincar%C3%A9%20Inequality%20and%20Consistency%20Results%20for%20Signal%20Sampling%20on%0A%20%20Large%20Graphs&body=Title%3A%20A%20Poincar%C3%A9%20Inequality%20and%20Consistency%20Results%20for%20Signal%20Sampling%20on%0A%20%20Large%20Graphs%0AAuthor%3A%20Thien%20Le%20and%20Luana%20Ruiz%20and%20Stefanie%20Jegelka%0AAbstract%3A%20%20%20Large-scale%20graph%20machine%20learning%20is%20challenging%20as%20the%20complexity%20of%0Alearning%20models%20scales%20with%20the%20graph%20size.%20Subsampling%20the%20graph%20is%20a%20viable%0Aalternative%2C%20but%20sampling%20on%20graphs%20is%20nontrivial%20as%20graphs%20are%20non-Euclidean.%0AExisting%20graph%20sampling%20techniques%20require%20not%20only%20computing%20the%20spectra%20of%0Alarge%20matrices%20but%20also%20repeating%20these%20computations%20when%20the%20graph%20changes%2C%0Ae.g.%2C%20grows.%20In%20this%20paper%2C%20we%20introduce%20a%20signal%20sampling%20theory%20for%20a%20type%20of%0Agraph%20limit%20--%20the%20graphon.%20We%20prove%20a%20Poincar%5C%27e%20inequality%20for%20graphon%0Asignals%20and%20show%20that%20complements%20of%20node%20subsets%20satisfying%20this%20inequality%0Aare%20unique%20sampling%20sets%20for%20Paley-Wiener%20spaces%20of%20graphon%20signals.%20Exploiting%0Aconnections%20with%20spectral%20clustering%20and%20Gaussian%20elimination%2C%20we%20prove%20that%0Asuch%20sampling%20sets%20are%20consistent%20in%20the%20sense%20that%20unique%20sampling%20sets%20on%20a%0Aconvergent%20graph%20sequence%20converge%20to%20unique%20sampling%20sets%20on%20the%20graphon.%20We%0Athen%20propose%20a%20related%20graphon%20signal%20sampling%20algorithm%20for%20large%20graphs%2C%20and%0Ademonstrate%20its%20good%20empirical%20performance%20on%20graph%20machine%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10610v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Poincar%25C3%25A9%2520Inequality%2520and%2520Consistency%2520Results%2520for%2520Signal%2520Sampling%2520on%250A%2520%2520Large%2520Graphs%26entry.906535625%3DThien%2520Le%2520and%2520Luana%2520Ruiz%2520and%2520Stefanie%2520Jegelka%26entry.1292438233%3D%2520%2520Large-scale%2520graph%2520machine%2520learning%2520is%2520challenging%2520as%2520the%2520complexity%2520of%250Alearning%2520models%2520scales%2520with%2520the%2520graph%2520size.%2520Subsampling%2520the%2520graph%2520is%2520a%2520viable%250Aalternative%252C%2520but%2520sampling%2520on%2520graphs%2520is%2520nontrivial%2520as%2520graphs%2520are%2520non-Euclidean.%250AExisting%2520graph%2520sampling%2520techniques%2520require%2520not%2520only%2520computing%2520the%2520spectra%2520of%250Alarge%2520matrices%2520but%2520also%2520repeating%2520these%2520computations%2520when%2520the%2520graph%2520changes%252C%250Ae.g.%252C%2520grows.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520signal%2520sampling%2520theory%2520for%2520a%2520type%2520of%250Agraph%2520limit%2520--%2520the%2520graphon.%2520We%2520prove%2520a%2520Poincar%255C%2527e%2520inequality%2520for%2520graphon%250Asignals%2520and%2520show%2520that%2520complements%2520of%2520node%2520subsets%2520satisfying%2520this%2520inequality%250Aare%2520unique%2520sampling%2520sets%2520for%2520Paley-Wiener%2520spaces%2520of%2520graphon%2520signals.%2520Exploiting%250Aconnections%2520with%2520spectral%2520clustering%2520and%2520Gaussian%2520elimination%252C%2520we%2520prove%2520that%250Asuch%2520sampling%2520sets%2520are%2520consistent%2520in%2520the%2520sense%2520that%2520unique%2520sampling%2520sets%2520on%2520a%250Aconvergent%2520graph%2520sequence%2520converge%2520to%2520unique%2520sampling%2520sets%2520on%2520the%2520graphon.%2520We%250Athen%2520propose%2520a%2520related%2520graphon%2520signal%2520sampling%2520algorithm%2520for%2520large%2520graphs%252C%2520and%250Ademonstrate%2520its%2520good%2520empirical%2520performance%2520on%2520graph%2520machine%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10610v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Poincar%C3%A9%20Inequality%20and%20Consistency%20Results%20for%20Signal%20Sampling%20on%0A%20%20Large%20Graphs&entry.906535625=Thien%20Le%20and%20Luana%20Ruiz%20and%20Stefanie%20Jegelka&entry.1292438233=%20%20Large-scale%20graph%20machine%20learning%20is%20challenging%20as%20the%20complexity%20of%0Alearning%20models%20scales%20with%20the%20graph%20size.%20Subsampling%20the%20graph%20is%20a%20viable%0Aalternative%2C%20but%20sampling%20on%20graphs%20is%20nontrivial%20as%20graphs%20are%20non-Euclidean.%0AExisting%20graph%20sampling%20techniques%20require%20not%20only%20computing%20the%20spectra%20of%0Alarge%20matrices%20but%20also%20repeating%20these%20computations%20when%20the%20graph%20changes%2C%0Ae.g.%2C%20grows.%20In%20this%20paper%2C%20we%20introduce%20a%20signal%20sampling%20theory%20for%20a%20type%20of%0Agraph%20limit%20--%20the%20graphon.%20We%20prove%20a%20Poincar%5C%27e%20inequality%20for%20graphon%0Asignals%20and%20show%20that%20complements%20of%20node%20subsets%20satisfying%20this%20inequality%0Aare%20unique%20sampling%20sets%20for%20Paley-Wiener%20spaces%20of%20graphon%20signals.%20Exploiting%0Aconnections%20with%20spectral%20clustering%20and%20Gaussian%20elimination%2C%20we%20prove%20that%0Asuch%20sampling%20sets%20are%20consistent%20in%20the%20sense%20that%20unique%20sampling%20sets%20on%20a%0Aconvergent%20graph%20sequence%20converge%20to%20unique%20sampling%20sets%20on%20the%20graphon.%20We%0Athen%20propose%20a%20related%20graphon%20signal%20sampling%20algorithm%20for%20large%20graphs%2C%20and%0Ademonstrate%20its%20good%20empirical%20performance%20on%20graph%20machine%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10610v3&entry.124074799=Read"},
{"title": "ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models", "author": "Yaswanth Narsupalli and Abhranil Chandra and Sreevatsa Muppirala and Manish Gupta and Pawan Goyal", "abstract": "  Assessing the quality of outputs generated by generative models, such as\nlarge language models and vision language models, presents notable challenges.\nTraditional methods for evaluation typically rely on either human assessments,\nwhich are resource-intensive, or automatic metrics that often show a low\ncorrelation with human judgment. Another common approach is to use deep\nlearning systems, which not only consume a substantial amount of compute and\ntime but also require extensive training data. In this study, we introduce a\ntuning-free framework called ReFeR, designed to evaluate generative outputs,\nincluding both text and images, by leveraging a 2-level hierarchy of LLMs and\nVLMs themselves. We rigorously evaluate our framework, ReFeR, across four\ndiverse evaluation tasks. The framework not only improves the accuracy of these\nevaluations, surpassing previous benchmarks but also generates constructive\nfeedback. Interestingly, the framework is also applicable to reasoning tasks.\nExperiments on four reasoning tasks demonstrate superior collective reasoning\nabilities of the framework. We present two variants of the framework:\nReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a\nmore cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient\nwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIP\npackage publicly available. See this PIP URL\nhttps://pypi.org/project/refer-agents/ and this Git URL\nhttps://github.com/yaswanth-iitkgp/ReFeR_Code .\n", "link": "http://arxiv.org/abs/2407.12877v2", "date": "2024-10-09", "relevancy": 2.2228, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.558}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReFeR%3A%20Improving%20Evaluation%20and%20Reasoning%20through%20Hierarchy%20of%20Models&body=Title%3A%20ReFeR%3A%20Improving%20Evaluation%20and%20Reasoning%20through%20Hierarchy%20of%20Models%0AAuthor%3A%20Yaswanth%20Narsupalli%20and%20Abhranil%20Chandra%20and%20Sreevatsa%20Muppirala%20and%20Manish%20Gupta%20and%20Pawan%20Goyal%0AAbstract%3A%20%20%20Assessing%20the%20quality%20of%20outputs%20generated%20by%20generative%20models%2C%20such%20as%0Alarge%20language%20models%20and%20vision%20language%20models%2C%20presents%20notable%20challenges.%0ATraditional%20methods%20for%20evaluation%20typically%20rely%20on%20either%20human%20assessments%2C%0Awhich%20are%20resource-intensive%2C%20or%20automatic%20metrics%20that%20often%20show%20a%20low%0Acorrelation%20with%20human%20judgment.%20Another%20common%20approach%20is%20to%20use%20deep%0Alearning%20systems%2C%20which%20not%20only%20consume%20a%20substantial%20amount%20of%20compute%20and%0Atime%20but%20also%20require%20extensive%20training%20data.%20In%20this%20study%2C%20we%20introduce%20a%0Atuning-free%20framework%20called%20ReFeR%2C%20designed%20to%20evaluate%20generative%20outputs%2C%0Aincluding%20both%20text%20and%20images%2C%20by%20leveraging%20a%202-level%20hierarchy%20of%20LLMs%20and%0AVLMs%20themselves.%20We%20rigorously%20evaluate%20our%20framework%2C%20ReFeR%2C%20across%20four%0Adiverse%20evaluation%20tasks.%20The%20framework%20not%20only%20improves%20the%20accuracy%20of%20these%0Aevaluations%2C%20surpassing%20previous%20benchmarks%20but%20also%20generates%20constructive%0Afeedback.%20Interestingly%2C%20the%20framework%20is%20also%20applicable%20to%20reasoning%20tasks.%0AExperiments%20on%20four%20reasoning%20tasks%20demonstrate%20superior%20collective%20reasoning%0Aabilities%20of%20the%20framework.%20We%20present%20two%20variants%20of%20the%20framework%3A%0AReFeR-Turbo%2C%20optimized%20for%20accelerated%20performance%2C%20and%20ReFeR-Lite%2C%20offering%20a%0Amore%20cost-effective%20solution.%20ReFeR-Lite%20is%20%24%5Csim7.7%5Ctimes%24%20more%20efficient%0Awhile%20being%20comparably%20accurate%20to%20ReFeR-Turbo.%20We%20make%20code%2C%20data%20and%20PIP%0Apackage%20publicly%20available.%20See%20this%20PIP%20URL%0Ahttps%3A//pypi.org/project/refer-agents/%20and%20this%20Git%20URL%0Ahttps%3A//github.com/yaswanth-iitkgp/ReFeR_Code%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReFeR%253A%2520Improving%2520Evaluation%2520and%2520Reasoning%2520through%2520Hierarchy%2520of%2520Models%26entry.906535625%3DYaswanth%2520Narsupalli%2520and%2520Abhranil%2520Chandra%2520and%2520Sreevatsa%2520Muppirala%2520and%2520Manish%2520Gupta%2520and%2520Pawan%2520Goyal%26entry.1292438233%3D%2520%2520Assessing%2520the%2520quality%2520of%2520outputs%2520generated%2520by%2520generative%2520models%252C%2520such%2520as%250Alarge%2520language%2520models%2520and%2520vision%2520language%2520models%252C%2520presents%2520notable%2520challenges.%250ATraditional%2520methods%2520for%2520evaluation%2520typically%2520rely%2520on%2520either%2520human%2520assessments%252C%250Awhich%2520are%2520resource-intensive%252C%2520or%2520automatic%2520metrics%2520that%2520often%2520show%2520a%2520low%250Acorrelation%2520with%2520human%2520judgment.%2520Another%2520common%2520approach%2520is%2520to%2520use%2520deep%250Alearning%2520systems%252C%2520which%2520not%2520only%2520consume%2520a%2520substantial%2520amount%2520of%2520compute%2520and%250Atime%2520but%2520also%2520require%2520extensive%2520training%2520data.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%250Atuning-free%2520framework%2520called%2520ReFeR%252C%2520designed%2520to%2520evaluate%2520generative%2520outputs%252C%250Aincluding%2520both%2520text%2520and%2520images%252C%2520by%2520leveraging%2520a%25202-level%2520hierarchy%2520of%2520LLMs%2520and%250AVLMs%2520themselves.%2520We%2520rigorously%2520evaluate%2520our%2520framework%252C%2520ReFeR%252C%2520across%2520four%250Adiverse%2520evaluation%2520tasks.%2520The%2520framework%2520not%2520only%2520improves%2520the%2520accuracy%2520of%2520these%250Aevaluations%252C%2520surpassing%2520previous%2520benchmarks%2520but%2520also%2520generates%2520constructive%250Afeedback.%2520Interestingly%252C%2520the%2520framework%2520is%2520also%2520applicable%2520to%2520reasoning%2520tasks.%250AExperiments%2520on%2520four%2520reasoning%2520tasks%2520demonstrate%2520superior%2520collective%2520reasoning%250Aabilities%2520of%2520the%2520framework.%2520We%2520present%2520two%2520variants%2520of%2520the%2520framework%253A%250AReFeR-Turbo%252C%2520optimized%2520for%2520accelerated%2520performance%252C%2520and%2520ReFeR-Lite%252C%2520offering%2520a%250Amore%2520cost-effective%2520solution.%2520ReFeR-Lite%2520is%2520%2524%255Csim7.7%255Ctimes%2524%2520more%2520efficient%250Awhile%2520being%2520comparably%2520accurate%2520to%2520ReFeR-Turbo.%2520We%2520make%2520code%252C%2520data%2520and%2520PIP%250Apackage%2520publicly%2520available.%2520See%2520this%2520PIP%2520URL%250Ahttps%253A//pypi.org/project/refer-agents/%2520and%2520this%2520Git%2520URL%250Ahttps%253A//github.com/yaswanth-iitkgp/ReFeR_Code%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReFeR%3A%20Improving%20Evaluation%20and%20Reasoning%20through%20Hierarchy%20of%20Models&entry.906535625=Yaswanth%20Narsupalli%20and%20Abhranil%20Chandra%20and%20Sreevatsa%20Muppirala%20and%20Manish%20Gupta%20and%20Pawan%20Goyal&entry.1292438233=%20%20Assessing%20the%20quality%20of%20outputs%20generated%20by%20generative%20models%2C%20such%20as%0Alarge%20language%20models%20and%20vision%20language%20models%2C%20presents%20notable%20challenges.%0ATraditional%20methods%20for%20evaluation%20typically%20rely%20on%20either%20human%20assessments%2C%0Awhich%20are%20resource-intensive%2C%20or%20automatic%20metrics%20that%20often%20show%20a%20low%0Acorrelation%20with%20human%20judgment.%20Another%20common%20approach%20is%20to%20use%20deep%0Alearning%20systems%2C%20which%20not%20only%20consume%20a%20substantial%20amount%20of%20compute%20and%0Atime%20but%20also%20require%20extensive%20training%20data.%20In%20this%20study%2C%20we%20introduce%20a%0Atuning-free%20framework%20called%20ReFeR%2C%20designed%20to%20evaluate%20generative%20outputs%2C%0Aincluding%20both%20text%20and%20images%2C%20by%20leveraging%20a%202-level%20hierarchy%20of%20LLMs%20and%0AVLMs%20themselves.%20We%20rigorously%20evaluate%20our%20framework%2C%20ReFeR%2C%20across%20four%0Adiverse%20evaluation%20tasks.%20The%20framework%20not%20only%20improves%20the%20accuracy%20of%20these%0Aevaluations%2C%20surpassing%20previous%20benchmarks%20but%20also%20generates%20constructive%0Afeedback.%20Interestingly%2C%20the%20framework%20is%20also%20applicable%20to%20reasoning%20tasks.%0AExperiments%20on%20four%20reasoning%20tasks%20demonstrate%20superior%20collective%20reasoning%0Aabilities%20of%20the%20framework.%20We%20present%20two%20variants%20of%20the%20framework%3A%0AReFeR-Turbo%2C%20optimized%20for%20accelerated%20performance%2C%20and%20ReFeR-Lite%2C%20offering%20a%0Amore%20cost-effective%20solution.%20ReFeR-Lite%20is%20%24%5Csim7.7%5Ctimes%24%20more%20efficient%0Awhile%20being%20comparably%20accurate%20to%20ReFeR-Turbo.%20We%20make%20code%2C%20data%20and%20PIP%0Apackage%20publicly%20available.%20See%20this%20PIP%20URL%0Ahttps%3A//pypi.org/project/refer-agents/%20and%20this%20Git%20URL%0Ahttps%3A//github.com/yaswanth-iitkgp/ReFeR_Code%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12877v2&entry.124074799=Read"},
{"title": "A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one\n  Single X-ray", "author": "Zhe Wang and Rachid Jennane and Aladine Chetouani and Mohamed Jarraya", "abstract": "  Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays\nare commonly used for its diagnosis due to their cost-effectiveness. Magnetic\nResonance Imaging (MRI), on the other hand, offers detailed soft tissue\nvisualization and has become a valuable supplementary diagnostic tool for KOA.\nUnfortunately, the high cost and limited accessibility of MRI hinder its\nwidespread use, leaving many patients with KOA reliant solely on X-ray imaging.\nIn this study, we introduce a novel diffusion-based Xray2MRI model capable of\ngenerating pseudo-MRI volumes from one single X-ray image. In addition to using\nX-rays as conditional input, our model integrates target depth, KOA probability\ndistribution, and image intensity distribution modules to guide the synthesis\nprocess, ensuring that the generated corresponding slices accurately correspond\nto the anatomical structures. Experimental results demonstrate that by\nintegrating information from X-rays with additional input data, our proposed\napproach is capable of generating pseudo-MRI sequences that approximate real\nMRI scans. Moreover, by increasing the inference times, the model achieves\neffective interpolation, further improving the continuity and smoothness of the\ngenerated MRI sequences, representing one promising initial attempt for\ncost-effective medical imaging solutions.\n", "link": "http://arxiv.org/abs/2410.06997v1", "date": "2024-10-09", "relevancy": 2.2218, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5589}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5589}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Diffusion-based%20Xray2MRI%20Model%3A%20Generating%20Pseudo-MRI%20Volumes%20From%20one%0A%20%20Single%20X-ray&body=Title%3A%20A%20Diffusion-based%20Xray2MRI%20Model%3A%20Generating%20Pseudo-MRI%20Volumes%20From%20one%0A%20%20Single%20X-ray%0AAuthor%3A%20Zhe%20Wang%20and%20Rachid%20Jennane%20and%20Aladine%20Chetouani%20and%20Mohamed%20Jarraya%0AAbstract%3A%20%20%20Knee%20osteoarthritis%20%28KOA%29%20is%20a%20prevalent%20musculoskeletal%20disorder%2C%20and%20X-rays%0Aare%20commonly%20used%20for%20its%20diagnosis%20due%20to%20their%20cost-effectiveness.%20Magnetic%0AResonance%20Imaging%20%28MRI%29%2C%20on%20the%20other%20hand%2C%20offers%20detailed%20soft%20tissue%0Avisualization%20and%20has%20become%20a%20valuable%20supplementary%20diagnostic%20tool%20for%20KOA.%0AUnfortunately%2C%20the%20high%20cost%20and%20limited%20accessibility%20of%20MRI%20hinder%20its%0Awidespread%20use%2C%20leaving%20many%20patients%20with%20KOA%20reliant%20solely%20on%20X-ray%20imaging.%0AIn%20this%20study%2C%20we%20introduce%20a%20novel%20diffusion-based%20Xray2MRI%20model%20capable%20of%0Agenerating%20pseudo-MRI%20volumes%20from%20one%20single%20X-ray%20image.%20In%20addition%20to%20using%0AX-rays%20as%20conditional%20input%2C%20our%20model%20integrates%20target%20depth%2C%20KOA%20probability%0Adistribution%2C%20and%20image%20intensity%20distribution%20modules%20to%20guide%20the%20synthesis%0Aprocess%2C%20ensuring%20that%20the%20generated%20corresponding%20slices%20accurately%20correspond%0Ato%20the%20anatomical%20structures.%20Experimental%20results%20demonstrate%20that%20by%0Aintegrating%20information%20from%20X-rays%20with%20additional%20input%20data%2C%20our%20proposed%0Aapproach%20is%20capable%20of%20generating%20pseudo-MRI%20sequences%20that%20approximate%20real%0AMRI%20scans.%20Moreover%2C%20by%20increasing%20the%20inference%20times%2C%20the%20model%20achieves%0Aeffective%20interpolation%2C%20further%20improving%20the%20continuity%20and%20smoothness%20of%20the%0Agenerated%20MRI%20sequences%2C%20representing%20one%20promising%20initial%20attempt%20for%0Acost-effective%20medical%20imaging%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Diffusion-based%2520Xray2MRI%2520Model%253A%2520Generating%2520Pseudo-MRI%2520Volumes%2520From%2520one%250A%2520%2520Single%2520X-ray%26entry.906535625%3DZhe%2520Wang%2520and%2520Rachid%2520Jennane%2520and%2520Aladine%2520Chetouani%2520and%2520Mohamed%2520Jarraya%26entry.1292438233%3D%2520%2520Knee%2520osteoarthritis%2520%2528KOA%2529%2520is%2520a%2520prevalent%2520musculoskeletal%2520disorder%252C%2520and%2520X-rays%250Aare%2520commonly%2520used%2520for%2520its%2520diagnosis%2520due%2520to%2520their%2520cost-effectiveness.%2520Magnetic%250AResonance%2520Imaging%2520%2528MRI%2529%252C%2520on%2520the%2520other%2520hand%252C%2520offers%2520detailed%2520soft%2520tissue%250Avisualization%2520and%2520has%2520become%2520a%2520valuable%2520supplementary%2520diagnostic%2520tool%2520for%2520KOA.%250AUnfortunately%252C%2520the%2520high%2520cost%2520and%2520limited%2520accessibility%2520of%2520MRI%2520hinder%2520its%250Awidespread%2520use%252C%2520leaving%2520many%2520patients%2520with%2520KOA%2520reliant%2520solely%2520on%2520X-ray%2520imaging.%250AIn%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520diffusion-based%2520Xray2MRI%2520model%2520capable%2520of%250Agenerating%2520pseudo-MRI%2520volumes%2520from%2520one%2520single%2520X-ray%2520image.%2520In%2520addition%2520to%2520using%250AX-rays%2520as%2520conditional%2520input%252C%2520our%2520model%2520integrates%2520target%2520depth%252C%2520KOA%2520probability%250Adistribution%252C%2520and%2520image%2520intensity%2520distribution%2520modules%2520to%2520guide%2520the%2520synthesis%250Aprocess%252C%2520ensuring%2520that%2520the%2520generated%2520corresponding%2520slices%2520accurately%2520correspond%250Ato%2520the%2520anatomical%2520structures.%2520Experimental%2520results%2520demonstrate%2520that%2520by%250Aintegrating%2520information%2520from%2520X-rays%2520with%2520additional%2520input%2520data%252C%2520our%2520proposed%250Aapproach%2520is%2520capable%2520of%2520generating%2520pseudo-MRI%2520sequences%2520that%2520approximate%2520real%250AMRI%2520scans.%2520Moreover%252C%2520by%2520increasing%2520the%2520inference%2520times%252C%2520the%2520model%2520achieves%250Aeffective%2520interpolation%252C%2520further%2520improving%2520the%2520continuity%2520and%2520smoothness%2520of%2520the%250Agenerated%2520MRI%2520sequences%252C%2520representing%2520one%2520promising%2520initial%2520attempt%2520for%250Acost-effective%2520medical%2520imaging%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Diffusion-based%20Xray2MRI%20Model%3A%20Generating%20Pseudo-MRI%20Volumes%20From%20one%0A%20%20Single%20X-ray&entry.906535625=Zhe%20Wang%20and%20Rachid%20Jennane%20and%20Aladine%20Chetouani%20and%20Mohamed%20Jarraya&entry.1292438233=%20%20Knee%20osteoarthritis%20%28KOA%29%20is%20a%20prevalent%20musculoskeletal%20disorder%2C%20and%20X-rays%0Aare%20commonly%20used%20for%20its%20diagnosis%20due%20to%20their%20cost-effectiveness.%20Magnetic%0AResonance%20Imaging%20%28MRI%29%2C%20on%20the%20other%20hand%2C%20offers%20detailed%20soft%20tissue%0Avisualization%20and%20has%20become%20a%20valuable%20supplementary%20diagnostic%20tool%20for%20KOA.%0AUnfortunately%2C%20the%20high%20cost%20and%20limited%20accessibility%20of%20MRI%20hinder%20its%0Awidespread%20use%2C%20leaving%20many%20patients%20with%20KOA%20reliant%20solely%20on%20X-ray%20imaging.%0AIn%20this%20study%2C%20we%20introduce%20a%20novel%20diffusion-based%20Xray2MRI%20model%20capable%20of%0Agenerating%20pseudo-MRI%20volumes%20from%20one%20single%20X-ray%20image.%20In%20addition%20to%20using%0AX-rays%20as%20conditional%20input%2C%20our%20model%20integrates%20target%20depth%2C%20KOA%20probability%0Adistribution%2C%20and%20image%20intensity%20distribution%20modules%20to%20guide%20the%20synthesis%0Aprocess%2C%20ensuring%20that%20the%20generated%20corresponding%20slices%20accurately%20correspond%0Ato%20the%20anatomical%20structures.%20Experimental%20results%20demonstrate%20that%20by%0Aintegrating%20information%20from%20X-rays%20with%20additional%20input%20data%2C%20our%20proposed%0Aapproach%20is%20capable%20of%20generating%20pseudo-MRI%20sequences%20that%20approximate%20real%0AMRI%20scans.%20Moreover%2C%20by%20increasing%20the%20inference%20times%2C%20the%20model%20achieves%0Aeffective%20interpolation%2C%20further%20improving%20the%20continuity%20and%20smoothness%20of%20the%0Agenerated%20MRI%20sequences%2C%20representing%20one%20promising%20initial%20attempt%20for%0Acost-effective%20medical%20imaging%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06997v1&entry.124074799=Read"},
{"title": "QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space\n  Model", "author": "Fei Xie and Weijia Zhang and Zhongdao Wang and Chao Ma", "abstract": "  Recent advancements in State Space Models, notably Mamba, have demonstrated\nsuperior performance over the dominant Transformer models, particularly in\nreducing the computational complexity from quadratic to linear. Yet,\ndifficulties in adapting Mamba from language to vision tasks arise due to the\ndistinct characteristics of visual data, such as the spatial locality and\nadjacency within images and large variations in information granularity across\nvisual tokens. Existing vision Mamba approaches either flatten tokens into\nsequences in a raster scan fashion, which breaks the local adjacency of images,\nor manually partition tokens into windows, which limits their long-range\nmodeling and generalization capabilities. To address these limitations, we\npresent a new vision Mamba model, coined QuadMamba, that effectively captures\nlocal dependencies of varying granularities via quadtree-based image partition\nand scan. Concretely, our lightweight quadtree-based scan module learns to\npreserve the 2D locality of spatial regions within learned window quadrants.\nThe module estimates the locality score of each token from their features,\nbefore adaptively partitioning tokens into window quadrants. An omnidirectional\nwindow shifting scheme is also introduced to capture more intact and\ninformative features across different local regions. To make the discretized\nquadtree partition end-to-end trainable, we further devise a sequence masking\nstrategy based on Gumbel-Softmax and its straight-through gradient estimator.\nExtensive experiments demonstrate that QuadMamba achieves state-of-the-art\nperformance in various vision tasks, including image classification, object\ndetection, instance segmentation, and semantic segmentation. The code is in\nhttps://github.com/VISIONSJTU/QuadMamba.\n", "link": "http://arxiv.org/abs/2410.06806v1", "date": "2024-10-09", "relevancy": 2.2209, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5703}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuadMamba%3A%20Learning%20Quadtree-based%20Selective%20Scan%20for%20Visual%20State%20Space%0A%20%20Model&body=Title%3A%20QuadMamba%3A%20Learning%20Quadtree-based%20Selective%20Scan%20for%20Visual%20State%20Space%0A%20%20Model%0AAuthor%3A%20Fei%20Xie%20and%20Weijia%20Zhang%20and%20Zhongdao%20Wang%20and%20Chao%20Ma%0AAbstract%3A%20%20%20Recent%20advancements%20in%20State%20Space%20Models%2C%20notably%20Mamba%2C%20have%20demonstrated%0Asuperior%20performance%20over%20the%20dominant%20Transformer%20models%2C%20particularly%20in%0Areducing%20the%20computational%20complexity%20from%20quadratic%20to%20linear.%20Yet%2C%0Adifficulties%20in%20adapting%20Mamba%20from%20language%20to%20vision%20tasks%20arise%20due%20to%20the%0Adistinct%20characteristics%20of%20visual%20data%2C%20such%20as%20the%20spatial%20locality%20and%0Aadjacency%20within%20images%20and%20large%20variations%20in%20information%20granularity%20across%0Avisual%20tokens.%20Existing%20vision%20Mamba%20approaches%20either%20flatten%20tokens%20into%0Asequences%20in%20a%20raster%20scan%20fashion%2C%20which%20breaks%20the%20local%20adjacency%20of%20images%2C%0Aor%20manually%20partition%20tokens%20into%20windows%2C%20which%20limits%20their%20long-range%0Amodeling%20and%20generalization%20capabilities.%20To%20address%20these%20limitations%2C%20we%0Apresent%20a%20new%20vision%20Mamba%20model%2C%20coined%20QuadMamba%2C%20that%20effectively%20captures%0Alocal%20dependencies%20of%20varying%20granularities%20via%20quadtree-based%20image%20partition%0Aand%20scan.%20Concretely%2C%20our%20lightweight%20quadtree-based%20scan%20module%20learns%20to%0Apreserve%20the%202D%20locality%20of%20spatial%20regions%20within%20learned%20window%20quadrants.%0AThe%20module%20estimates%20the%20locality%20score%20of%20each%20token%20from%20their%20features%2C%0Abefore%20adaptively%20partitioning%20tokens%20into%20window%20quadrants.%20An%20omnidirectional%0Awindow%20shifting%20scheme%20is%20also%20introduced%20to%20capture%20more%20intact%20and%0Ainformative%20features%20across%20different%20local%20regions.%20To%20make%20the%20discretized%0Aquadtree%20partition%20end-to-end%20trainable%2C%20we%20further%20devise%20a%20sequence%20masking%0Astrategy%20based%20on%20Gumbel-Softmax%20and%20its%20straight-through%20gradient%20estimator.%0AExtensive%20experiments%20demonstrate%20that%20QuadMamba%20achieves%20state-of-the-art%0Aperformance%20in%20various%20vision%20tasks%2C%20including%20image%20classification%2C%20object%0Adetection%2C%20instance%20segmentation%2C%20and%20semantic%20segmentation.%20The%20code%20is%20in%0Ahttps%3A//github.com/VISIONSJTU/QuadMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuadMamba%253A%2520Learning%2520Quadtree-based%2520Selective%2520Scan%2520for%2520Visual%2520State%2520Space%250A%2520%2520Model%26entry.906535625%3DFei%2520Xie%2520and%2520Weijia%2520Zhang%2520and%2520Zhongdao%2520Wang%2520and%2520Chao%2520Ma%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520State%2520Space%2520Models%252C%2520notably%2520Mamba%252C%2520have%2520demonstrated%250Asuperior%2520performance%2520over%2520the%2520dominant%2520Transformer%2520models%252C%2520particularly%2520in%250Areducing%2520the%2520computational%2520complexity%2520from%2520quadratic%2520to%2520linear.%2520Yet%252C%250Adifficulties%2520in%2520adapting%2520Mamba%2520from%2520language%2520to%2520vision%2520tasks%2520arise%2520due%2520to%2520the%250Adistinct%2520characteristics%2520of%2520visual%2520data%252C%2520such%2520as%2520the%2520spatial%2520locality%2520and%250Aadjacency%2520within%2520images%2520and%2520large%2520variations%2520in%2520information%2520granularity%2520across%250Avisual%2520tokens.%2520Existing%2520vision%2520Mamba%2520approaches%2520either%2520flatten%2520tokens%2520into%250Asequences%2520in%2520a%2520raster%2520scan%2520fashion%252C%2520which%2520breaks%2520the%2520local%2520adjacency%2520of%2520images%252C%250Aor%2520manually%2520partition%2520tokens%2520into%2520windows%252C%2520which%2520limits%2520their%2520long-range%250Amodeling%2520and%2520generalization%2520capabilities.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apresent%2520a%2520new%2520vision%2520Mamba%2520model%252C%2520coined%2520QuadMamba%252C%2520that%2520effectively%2520captures%250Alocal%2520dependencies%2520of%2520varying%2520granularities%2520via%2520quadtree-based%2520image%2520partition%250Aand%2520scan.%2520Concretely%252C%2520our%2520lightweight%2520quadtree-based%2520scan%2520module%2520learns%2520to%250Apreserve%2520the%25202D%2520locality%2520of%2520spatial%2520regions%2520within%2520learned%2520window%2520quadrants.%250AThe%2520module%2520estimates%2520the%2520locality%2520score%2520of%2520each%2520token%2520from%2520their%2520features%252C%250Abefore%2520adaptively%2520partitioning%2520tokens%2520into%2520window%2520quadrants.%2520An%2520omnidirectional%250Awindow%2520shifting%2520scheme%2520is%2520also%2520introduced%2520to%2520capture%2520more%2520intact%2520and%250Ainformative%2520features%2520across%2520different%2520local%2520regions.%2520To%2520make%2520the%2520discretized%250Aquadtree%2520partition%2520end-to-end%2520trainable%252C%2520we%2520further%2520devise%2520a%2520sequence%2520masking%250Astrategy%2520based%2520on%2520Gumbel-Softmax%2520and%2520its%2520straight-through%2520gradient%2520estimator.%250AExtensive%2520experiments%2520demonstrate%2520that%2520QuadMamba%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520various%2520vision%2520tasks%252C%2520including%2520image%2520classification%252C%2520object%250Adetection%252C%2520instance%2520segmentation%252C%2520and%2520semantic%2520segmentation.%2520The%2520code%2520is%2520in%250Ahttps%253A//github.com/VISIONSJTU/QuadMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuadMamba%3A%20Learning%20Quadtree-based%20Selective%20Scan%20for%20Visual%20State%20Space%0A%20%20Model&entry.906535625=Fei%20Xie%20and%20Weijia%20Zhang%20and%20Zhongdao%20Wang%20and%20Chao%20Ma&entry.1292438233=%20%20Recent%20advancements%20in%20State%20Space%20Models%2C%20notably%20Mamba%2C%20have%20demonstrated%0Asuperior%20performance%20over%20the%20dominant%20Transformer%20models%2C%20particularly%20in%0Areducing%20the%20computational%20complexity%20from%20quadratic%20to%20linear.%20Yet%2C%0Adifficulties%20in%20adapting%20Mamba%20from%20language%20to%20vision%20tasks%20arise%20due%20to%20the%0Adistinct%20characteristics%20of%20visual%20data%2C%20such%20as%20the%20spatial%20locality%20and%0Aadjacency%20within%20images%20and%20large%20variations%20in%20information%20granularity%20across%0Avisual%20tokens.%20Existing%20vision%20Mamba%20approaches%20either%20flatten%20tokens%20into%0Asequences%20in%20a%20raster%20scan%20fashion%2C%20which%20breaks%20the%20local%20adjacency%20of%20images%2C%0Aor%20manually%20partition%20tokens%20into%20windows%2C%20which%20limits%20their%20long-range%0Amodeling%20and%20generalization%20capabilities.%20To%20address%20these%20limitations%2C%20we%0Apresent%20a%20new%20vision%20Mamba%20model%2C%20coined%20QuadMamba%2C%20that%20effectively%20captures%0Alocal%20dependencies%20of%20varying%20granularities%20via%20quadtree-based%20image%20partition%0Aand%20scan.%20Concretely%2C%20our%20lightweight%20quadtree-based%20scan%20module%20learns%20to%0Apreserve%20the%202D%20locality%20of%20spatial%20regions%20within%20learned%20window%20quadrants.%0AThe%20module%20estimates%20the%20locality%20score%20of%20each%20token%20from%20their%20features%2C%0Abefore%20adaptively%20partitioning%20tokens%20into%20window%20quadrants.%20An%20omnidirectional%0Awindow%20shifting%20scheme%20is%20also%20introduced%20to%20capture%20more%20intact%20and%0Ainformative%20features%20across%20different%20local%20regions.%20To%20make%20the%20discretized%0Aquadtree%20partition%20end-to-end%20trainable%2C%20we%20further%20devise%20a%20sequence%20masking%0Astrategy%20based%20on%20Gumbel-Softmax%20and%20its%20straight-through%20gradient%20estimator.%0AExtensive%20experiments%20demonstrate%20that%20QuadMamba%20achieves%20state-of-the-art%0Aperformance%20in%20various%20vision%20tasks%2C%20including%20image%20classification%2C%20object%0Adetection%2C%20instance%20segmentation%2C%20and%20semantic%20segmentation.%20The%20code%20is%20in%0Ahttps%3A//github.com/VISIONSJTU/QuadMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06806v1&entry.124074799=Read"},
{"title": "Motion and Structure from Event-based Normal Flow", "author": "Zhongyang Ren and Bangyan Liao and Delei Kong and Jinghang Li and Peidong Liu and Laurent Kneip and Guillermo Gallego and Yi Zhou", "abstract": "  Recovering the camera motion and scene geometry from visual data is a\nfundamental problem in the field of computer vision. Its success in standard\nvision is attributed to the maturity of feature extraction, data association\nand multi-view geometry. The recent emergence of neuromorphic event-based\ncameras places great demands on approaches that use raw event data as input to\nsolve this fundamental problem. Existing state-of-the-art solutions typically\ninfer implicitly data association by iteratively reversing the event data\ngeneration process. However, the nonlinear nature of these methods limits their\napplicability in real-time tasks, and the constant-motion assumption leads to\nunstable results under agile motion. To this end, we rethink the problem\nformulation in a way that aligns better with the differential working principle\nof event cameras. We show that the event-based normal flow can be used, via the\nproposed geometric error term, as an alternative to the full flow in solving a\nfamily of geometric problems that involve instantaneous first-order kinematics\nand scene geometry. Furthermore, we develop a fast linear solver and a\ncontinuous-time nonlinear solver on top of the proposed geometric error term.\nExperiments on both synthetic and real data show the superiority of our linear\nsolver in terms of accuracy and efficiency, and indicate its complementary\nfeature as an initialization method for existing nonlinear solvers. Besides,\nour continuous-time non-linear solver exhibits exceptional capability in\naccommodating sudden variations in motion since it does not rely on the\nconstant-motion assumption.\n", "link": "http://arxiv.org/abs/2407.12239v3", "date": "2024-10-09", "relevancy": 2.2153, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5653}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20and%20Structure%20from%20Event-based%20Normal%20Flow&body=Title%3A%20Motion%20and%20Structure%20from%20Event-based%20Normal%20Flow%0AAuthor%3A%20Zhongyang%20Ren%20and%20Bangyan%20Liao%20and%20Delei%20Kong%20and%20Jinghang%20Li%20and%20Peidong%20Liu%20and%20Laurent%20Kneip%20and%20Guillermo%20Gallego%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Recovering%20the%20camera%20motion%20and%20scene%20geometry%20from%20visual%20data%20is%20a%0Afundamental%20problem%20in%20the%20field%20of%20computer%20vision.%20Its%20success%20in%20standard%0Avision%20is%20attributed%20to%20the%20maturity%20of%20feature%20extraction%2C%20data%20association%0Aand%20multi-view%20geometry.%20The%20recent%20emergence%20of%20neuromorphic%20event-based%0Acameras%20places%20great%20demands%20on%20approaches%20that%20use%20raw%20event%20data%20as%20input%20to%0Asolve%20this%20fundamental%20problem.%20Existing%20state-of-the-art%20solutions%20typically%0Ainfer%20implicitly%20data%20association%20by%20iteratively%20reversing%20the%20event%20data%0Ageneration%20process.%20However%2C%20the%20nonlinear%20nature%20of%20these%20methods%20limits%20their%0Aapplicability%20in%20real-time%20tasks%2C%20and%20the%20constant-motion%20assumption%20leads%20to%0Aunstable%20results%20under%20agile%20motion.%20To%20this%20end%2C%20we%20rethink%20the%20problem%0Aformulation%20in%20a%20way%20that%20aligns%20better%20with%20the%20differential%20working%20principle%0Aof%20event%20cameras.%20We%20show%20that%20the%20event-based%20normal%20flow%20can%20be%20used%2C%20via%20the%0Aproposed%20geometric%20error%20term%2C%20as%20an%20alternative%20to%20the%20full%20flow%20in%20solving%20a%0Afamily%20of%20geometric%20problems%20that%20involve%20instantaneous%20first-order%20kinematics%0Aand%20scene%20geometry.%20Furthermore%2C%20we%20develop%20a%20fast%20linear%20solver%20and%20a%0Acontinuous-time%20nonlinear%20solver%20on%20top%20of%20the%20proposed%20geometric%20error%20term.%0AExperiments%20on%20both%20synthetic%20and%20real%20data%20show%20the%20superiority%20of%20our%20linear%0Asolver%20in%20terms%20of%20accuracy%20and%20efficiency%2C%20and%20indicate%20its%20complementary%0Afeature%20as%20an%20initialization%20method%20for%20existing%20nonlinear%20solvers.%20Besides%2C%0Aour%20continuous-time%20non-linear%20solver%20exhibits%20exceptional%20capability%20in%0Aaccommodating%20sudden%20variations%20in%20motion%20since%20it%20does%20not%20rely%20on%20the%0Aconstant-motion%20assumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12239v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520and%2520Structure%2520from%2520Event-based%2520Normal%2520Flow%26entry.906535625%3DZhongyang%2520Ren%2520and%2520Bangyan%2520Liao%2520and%2520Delei%2520Kong%2520and%2520Jinghang%2520Li%2520and%2520Peidong%2520Liu%2520and%2520Laurent%2520Kneip%2520and%2520Guillermo%2520Gallego%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Recovering%2520the%2520camera%2520motion%2520and%2520scene%2520geometry%2520from%2520visual%2520data%2520is%2520a%250Afundamental%2520problem%2520in%2520the%2520field%2520of%2520computer%2520vision.%2520Its%2520success%2520in%2520standard%250Avision%2520is%2520attributed%2520to%2520the%2520maturity%2520of%2520feature%2520extraction%252C%2520data%2520association%250Aand%2520multi-view%2520geometry.%2520The%2520recent%2520emergence%2520of%2520neuromorphic%2520event-based%250Acameras%2520places%2520great%2520demands%2520on%2520approaches%2520that%2520use%2520raw%2520event%2520data%2520as%2520input%2520to%250Asolve%2520this%2520fundamental%2520problem.%2520Existing%2520state-of-the-art%2520solutions%2520typically%250Ainfer%2520implicitly%2520data%2520association%2520by%2520iteratively%2520reversing%2520the%2520event%2520data%250Ageneration%2520process.%2520However%252C%2520the%2520nonlinear%2520nature%2520of%2520these%2520methods%2520limits%2520their%250Aapplicability%2520in%2520real-time%2520tasks%252C%2520and%2520the%2520constant-motion%2520assumption%2520leads%2520to%250Aunstable%2520results%2520under%2520agile%2520motion.%2520To%2520this%2520end%252C%2520we%2520rethink%2520the%2520problem%250Aformulation%2520in%2520a%2520way%2520that%2520aligns%2520better%2520with%2520the%2520differential%2520working%2520principle%250Aof%2520event%2520cameras.%2520We%2520show%2520that%2520the%2520event-based%2520normal%2520flow%2520can%2520be%2520used%252C%2520via%2520the%250Aproposed%2520geometric%2520error%2520term%252C%2520as%2520an%2520alternative%2520to%2520the%2520full%2520flow%2520in%2520solving%2520a%250Afamily%2520of%2520geometric%2520problems%2520that%2520involve%2520instantaneous%2520first-order%2520kinematics%250Aand%2520scene%2520geometry.%2520Furthermore%252C%2520we%2520develop%2520a%2520fast%2520linear%2520solver%2520and%2520a%250Acontinuous-time%2520nonlinear%2520solver%2520on%2520top%2520of%2520the%2520proposed%2520geometric%2520error%2520term.%250AExperiments%2520on%2520both%2520synthetic%2520and%2520real%2520data%2520show%2520the%2520superiority%2520of%2520our%2520linear%250Asolver%2520in%2520terms%2520of%2520accuracy%2520and%2520efficiency%252C%2520and%2520indicate%2520its%2520complementary%250Afeature%2520as%2520an%2520initialization%2520method%2520for%2520existing%2520nonlinear%2520solvers.%2520Besides%252C%250Aour%2520continuous-time%2520non-linear%2520solver%2520exhibits%2520exceptional%2520capability%2520in%250Aaccommodating%2520sudden%2520variations%2520in%2520motion%2520since%2520it%2520does%2520not%2520rely%2520on%2520the%250Aconstant-motion%2520assumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12239v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20and%20Structure%20from%20Event-based%20Normal%20Flow&entry.906535625=Zhongyang%20Ren%20and%20Bangyan%20Liao%20and%20Delei%20Kong%20and%20Jinghang%20Li%20and%20Peidong%20Liu%20and%20Laurent%20Kneip%20and%20Guillermo%20Gallego%20and%20Yi%20Zhou&entry.1292438233=%20%20Recovering%20the%20camera%20motion%20and%20scene%20geometry%20from%20visual%20data%20is%20a%0Afundamental%20problem%20in%20the%20field%20of%20computer%20vision.%20Its%20success%20in%20standard%0Avision%20is%20attributed%20to%20the%20maturity%20of%20feature%20extraction%2C%20data%20association%0Aand%20multi-view%20geometry.%20The%20recent%20emergence%20of%20neuromorphic%20event-based%0Acameras%20places%20great%20demands%20on%20approaches%20that%20use%20raw%20event%20data%20as%20input%20to%0Asolve%20this%20fundamental%20problem.%20Existing%20state-of-the-art%20solutions%20typically%0Ainfer%20implicitly%20data%20association%20by%20iteratively%20reversing%20the%20event%20data%0Ageneration%20process.%20However%2C%20the%20nonlinear%20nature%20of%20these%20methods%20limits%20their%0Aapplicability%20in%20real-time%20tasks%2C%20and%20the%20constant-motion%20assumption%20leads%20to%0Aunstable%20results%20under%20agile%20motion.%20To%20this%20end%2C%20we%20rethink%20the%20problem%0Aformulation%20in%20a%20way%20that%20aligns%20better%20with%20the%20differential%20working%20principle%0Aof%20event%20cameras.%20We%20show%20that%20the%20event-based%20normal%20flow%20can%20be%20used%2C%20via%20the%0Aproposed%20geometric%20error%20term%2C%20as%20an%20alternative%20to%20the%20full%20flow%20in%20solving%20a%0Afamily%20of%20geometric%20problems%20that%20involve%20instantaneous%20first-order%20kinematics%0Aand%20scene%20geometry.%20Furthermore%2C%20we%20develop%20a%20fast%20linear%20solver%20and%20a%0Acontinuous-time%20nonlinear%20solver%20on%20top%20of%20the%20proposed%20geometric%20error%20term.%0AExperiments%20on%20both%20synthetic%20and%20real%20data%20show%20the%20superiority%20of%20our%20linear%0Asolver%20in%20terms%20of%20accuracy%20and%20efficiency%2C%20and%20indicate%20its%20complementary%0Afeature%20as%20an%20initialization%20method%20for%20existing%20nonlinear%20solvers.%20Besides%2C%0Aour%20continuous-time%20non-linear%20solver%20exhibits%20exceptional%20capability%20in%0Aaccommodating%20sudden%20variations%20in%20motion%20since%20it%20does%20not%20rely%20on%20the%0Aconstant-motion%20assumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12239v3&entry.124074799=Read"},
{"title": "Z-upscaling: Optical Flow Guided Frame Interpolation for Isotropic\n  Reconstruction of 3D EM Volumes", "author": "Fisseha A. Ferede and Ali Khalighifar and Jaison John and Krishnan Venkataraman and Khaled Khairy", "abstract": "  We propose a novel optical flow based approach to enhance the axial\nresolution of anisotropic 3D EM volumes to achieve isotropic 3D reconstruction.\nAssuming spatial continuity of 3D biological structures in well aligned EM\nvolumes, we reasoned that optical flow estimation techniques, often applied for\ntemporal resolution enhancement in videos, can be utilized. Pixel level motion\nis estimated between neighboring 2D slices along z, using spatial gradient flow\nestimates to interpolate and generate new 2D slices resulting in isotropic\nvoxels. We leverage recent state-of-the-art learning methods for video frame\ninterpolation and transfer learning techniques, and demonstrate the success of\nour approach on publicly available ultrastructure EM volumes.\n", "link": "http://arxiv.org/abs/2410.07043v1", "date": "2024-10-09", "relevancy": 2.1984, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5646}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5496}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Z-upscaling%3A%20Optical%20Flow%20Guided%20Frame%20Interpolation%20for%20Isotropic%0A%20%20Reconstruction%20of%203D%20EM%20Volumes&body=Title%3A%20Z-upscaling%3A%20Optical%20Flow%20Guided%20Frame%20Interpolation%20for%20Isotropic%0A%20%20Reconstruction%20of%203D%20EM%20Volumes%0AAuthor%3A%20Fisseha%20A.%20Ferede%20and%20Ali%20Khalighifar%20and%20Jaison%20John%20and%20Krishnan%20Venkataraman%20and%20Khaled%20Khairy%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20optical%20flow%20based%20approach%20to%20enhance%20the%20axial%0Aresolution%20of%20anisotropic%203D%20EM%20volumes%20to%20achieve%20isotropic%203D%20reconstruction.%0AAssuming%20spatial%20continuity%20of%203D%20biological%20structures%20in%20well%20aligned%20EM%0Avolumes%2C%20we%20reasoned%20that%20optical%20flow%20estimation%20techniques%2C%20often%20applied%20for%0Atemporal%20resolution%20enhancement%20in%20videos%2C%20can%20be%20utilized.%20Pixel%20level%20motion%0Ais%20estimated%20between%20neighboring%202D%20slices%20along%20z%2C%20using%20spatial%20gradient%20flow%0Aestimates%20to%20interpolate%20and%20generate%20new%202D%20slices%20resulting%20in%20isotropic%0Avoxels.%20We%20leverage%20recent%20state-of-the-art%20learning%20methods%20for%20video%20frame%0Ainterpolation%20and%20transfer%20learning%20techniques%2C%20and%20demonstrate%20the%20success%20of%0Aour%20approach%20on%20publicly%20available%20ultrastructure%20EM%20volumes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZ-upscaling%253A%2520Optical%2520Flow%2520Guided%2520Frame%2520Interpolation%2520for%2520Isotropic%250A%2520%2520Reconstruction%2520of%25203D%2520EM%2520Volumes%26entry.906535625%3DFisseha%2520A.%2520Ferede%2520and%2520Ali%2520Khalighifar%2520and%2520Jaison%2520John%2520and%2520Krishnan%2520Venkataraman%2520and%2520Khaled%2520Khairy%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520optical%2520flow%2520based%2520approach%2520to%2520enhance%2520the%2520axial%250Aresolution%2520of%2520anisotropic%25203D%2520EM%2520volumes%2520to%2520achieve%2520isotropic%25203D%2520reconstruction.%250AAssuming%2520spatial%2520continuity%2520of%25203D%2520biological%2520structures%2520in%2520well%2520aligned%2520EM%250Avolumes%252C%2520we%2520reasoned%2520that%2520optical%2520flow%2520estimation%2520techniques%252C%2520often%2520applied%2520for%250Atemporal%2520resolution%2520enhancement%2520in%2520videos%252C%2520can%2520be%2520utilized.%2520Pixel%2520level%2520motion%250Ais%2520estimated%2520between%2520neighboring%25202D%2520slices%2520along%2520z%252C%2520using%2520spatial%2520gradient%2520flow%250Aestimates%2520to%2520interpolate%2520and%2520generate%2520new%25202D%2520slices%2520resulting%2520in%2520isotropic%250Avoxels.%2520We%2520leverage%2520recent%2520state-of-the-art%2520learning%2520methods%2520for%2520video%2520frame%250Ainterpolation%2520and%2520transfer%2520learning%2520techniques%252C%2520and%2520demonstrate%2520the%2520success%2520of%250Aour%2520approach%2520on%2520publicly%2520available%2520ultrastructure%2520EM%2520volumes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Z-upscaling%3A%20Optical%20Flow%20Guided%20Frame%20Interpolation%20for%20Isotropic%0A%20%20Reconstruction%20of%203D%20EM%20Volumes&entry.906535625=Fisseha%20A.%20Ferede%20and%20Ali%20Khalighifar%20and%20Jaison%20John%20and%20Krishnan%20Venkataraman%20and%20Khaled%20Khairy&entry.1292438233=%20%20We%20propose%20a%20novel%20optical%20flow%20based%20approach%20to%20enhance%20the%20axial%0Aresolution%20of%20anisotropic%203D%20EM%20volumes%20to%20achieve%20isotropic%203D%20reconstruction.%0AAssuming%20spatial%20continuity%20of%203D%20biological%20structures%20in%20well%20aligned%20EM%0Avolumes%2C%20we%20reasoned%20that%20optical%20flow%20estimation%20techniques%2C%20often%20applied%20for%0Atemporal%20resolution%20enhancement%20in%20videos%2C%20can%20be%20utilized.%20Pixel%20level%20motion%0Ais%20estimated%20between%20neighboring%202D%20slices%20along%20z%2C%20using%20spatial%20gradient%20flow%0Aestimates%20to%20interpolate%20and%20generate%20new%202D%20slices%20resulting%20in%20isotropic%0Avoxels.%20We%20leverage%20recent%20state-of-the-art%20learning%20methods%20for%20video%20frame%0Ainterpolation%20and%20transfer%20learning%20techniques%2C%20and%20demonstrate%20the%20success%20of%0Aour%20approach%20on%20publicly%20available%20ultrastructure%20EM%20volumes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07043v1&entry.124074799=Read"},
{"title": "Enforcing 3D Topological Constraints in Composite Objects via Implicit\n  Functions", "author": "Hieu Le and Jingyi Xu and Nicolas Talabot and Jiancheng Yang and Pascal Fua", "abstract": "  Medical applications often require accurate 3D representations of complex\norgans with multiple parts, such as the heart and spine. Their individual parts\nmust adhere to specific topological constraints to ensure proper functionality.\nYet, there are very few mechanisms in the deep learning literature to achieve\nthis goal.\n  This paper introduces a novel approach to enforce topological constraints in\n3D object reconstruction using deep implicit signed distance functions. Our\nmethod focuses on heart and spine reconstruction but is generalizable to other\napplications. We propose a sampling-based technique that effectively checks and\nenforces topological constraints between 3D shapes by evaluating signed\ndistances at randomly sampled points throughout the volume. We demonstrate it\nby refining 3D segmentations obtained from the nn-UNet architecture.\n", "link": "http://arxiv.org/abs/2307.08716v2", "date": "2024-10-09", "relevancy": 2.1878, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5477}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5472}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enforcing%203D%20Topological%20Constraints%20in%20Composite%20Objects%20via%20Implicit%0A%20%20Functions&body=Title%3A%20Enforcing%203D%20Topological%20Constraints%20in%20Composite%20Objects%20via%20Implicit%0A%20%20Functions%0AAuthor%3A%20Hieu%20Le%20and%20Jingyi%20Xu%20and%20Nicolas%20Talabot%20and%20Jiancheng%20Yang%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Medical%20applications%20often%20require%20accurate%203D%20representations%20of%20complex%0Aorgans%20with%20multiple%20parts%2C%20such%20as%20the%20heart%20and%20spine.%20Their%20individual%20parts%0Amust%20adhere%20to%20specific%20topological%20constraints%20to%20ensure%20proper%20functionality.%0AYet%2C%20there%20are%20very%20few%20mechanisms%20in%20the%20deep%20learning%20literature%20to%20achieve%0Athis%20goal.%0A%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20enforce%20topological%20constraints%20in%0A3D%20object%20reconstruction%20using%20deep%20implicit%20signed%20distance%20functions.%20Our%0Amethod%20focuses%20on%20heart%20and%20spine%20reconstruction%20but%20is%20generalizable%20to%20other%0Aapplications.%20We%20propose%20a%20sampling-based%20technique%20that%20effectively%20checks%20and%0Aenforces%20topological%20constraints%20between%203D%20shapes%20by%20evaluating%20signed%0Adistances%20at%20randomly%20sampled%20points%20throughout%20the%20volume.%20We%20demonstrate%20it%0Aby%20refining%203D%20segmentations%20obtained%20from%20the%20nn-UNet%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnforcing%25203D%2520Topological%2520Constraints%2520in%2520Composite%2520Objects%2520via%2520Implicit%250A%2520%2520Functions%26entry.906535625%3DHieu%2520Le%2520and%2520Jingyi%2520Xu%2520and%2520Nicolas%2520Talabot%2520and%2520Jiancheng%2520Yang%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520Medical%2520applications%2520often%2520require%2520accurate%25203D%2520representations%2520of%2520complex%250Aorgans%2520with%2520multiple%2520parts%252C%2520such%2520as%2520the%2520heart%2520and%2520spine.%2520Their%2520individual%2520parts%250Amust%2520adhere%2520to%2520specific%2520topological%2520constraints%2520to%2520ensure%2520proper%2520functionality.%250AYet%252C%2520there%2520are%2520very%2520few%2520mechanisms%2520in%2520the%2520deep%2520learning%2520literature%2520to%2520achieve%250Athis%2520goal.%250A%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520enforce%2520topological%2520constraints%2520in%250A3D%2520object%2520reconstruction%2520using%2520deep%2520implicit%2520signed%2520distance%2520functions.%2520Our%250Amethod%2520focuses%2520on%2520heart%2520and%2520spine%2520reconstruction%2520but%2520is%2520generalizable%2520to%2520other%250Aapplications.%2520We%2520propose%2520a%2520sampling-based%2520technique%2520that%2520effectively%2520checks%2520and%250Aenforces%2520topological%2520constraints%2520between%25203D%2520shapes%2520by%2520evaluating%2520signed%250Adistances%2520at%2520randomly%2520sampled%2520points%2520throughout%2520the%2520volume.%2520We%2520demonstrate%2520it%250Aby%2520refining%25203D%2520segmentations%2520obtained%2520from%2520the%2520nn-UNet%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.08716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enforcing%203D%20Topological%20Constraints%20in%20Composite%20Objects%20via%20Implicit%0A%20%20Functions&entry.906535625=Hieu%20Le%20and%20Jingyi%20Xu%20and%20Nicolas%20Talabot%20and%20Jiancheng%20Yang%20and%20Pascal%20Fua&entry.1292438233=%20%20Medical%20applications%20often%20require%20accurate%203D%20representations%20of%20complex%0Aorgans%20with%20multiple%20parts%2C%20such%20as%20the%20heart%20and%20spine.%20Their%20individual%20parts%0Amust%20adhere%20to%20specific%20topological%20constraints%20to%20ensure%20proper%20functionality.%0AYet%2C%20there%20are%20very%20few%20mechanisms%20in%20the%20deep%20learning%20literature%20to%20achieve%0Athis%20goal.%0A%20%20This%20paper%20introduces%20a%20novel%20approach%20to%20enforce%20topological%20constraints%20in%0A3D%20object%20reconstruction%20using%20deep%20implicit%20signed%20distance%20functions.%20Our%0Amethod%20focuses%20on%20heart%20and%20spine%20reconstruction%20but%20is%20generalizable%20to%20other%0Aapplications.%20We%20propose%20a%20sampling-based%20technique%20that%20effectively%20checks%20and%0Aenforces%20topological%20constraints%20between%203D%20shapes%20by%20evaluating%20signed%0Adistances%20at%20randomly%20sampled%20points%20throughout%20the%20volume.%20We%20demonstrate%20it%0Aby%20refining%203D%20segmentations%20obtained%20from%20the%20nn-UNet%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08716v2&entry.124074799=Read"},
{"title": "Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and\n  YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments", "author": "Ranjan Sapkota and Zhichao Meng and Martin Churuvija and Xiaoqiang Du and Zenghong Ma and Manoj Karkee", "abstract": "  This study extensively evaluated You Only Look Once (YOLO) object detection\nalgorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, and\nYOLO11 for green fruit detection in commercial orchards. The research also\nvalidated in-field fruitlet counting using an iPhone and machine vision sensors\nacross four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.\nAmong the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-base\noutperformed others with mAP@50 scores of 0.933 and 0.935 respectively. In\nterms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9\nconfigurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11n\nemerged as the fastest model, achieving fastest inference speed of only 2.4 ms,\nsignificantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,\nand YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. This\ncomparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,\noffering researchers essential insights to choose the best-suited model for\nfruitlet detection and possible automation in commercial orchards. For\nreal-time automation related work in relevant datasets, we recommend using\nYOLO11n due to its high detection and image processing speed. Keywords: YOLO11,\nYOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, Fruitlet\nDetection, Greenfruit Detection, Green Apple Detection, Agricultural\nAutomation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shot\nDetection\n", "link": "http://arxiv.org/abs/2407.12040v4", "date": "2024-10-09", "relevancy": 2.1816, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4417}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20Performance%20Evaluation%20of%20YOLO11%2C%20YOLOv10%2C%20YOLOv9%20and%0A%20%20YOLOv8%20on%20Detecting%20and%20Counting%20Fruitlet%20in%20Complex%20Orchard%20Environments&body=Title%3A%20Comprehensive%20Performance%20Evaluation%20of%20YOLO11%2C%20YOLOv10%2C%20YOLOv9%20and%0A%20%20YOLOv8%20on%20Detecting%20and%20Counting%20Fruitlet%20in%20Complex%20Orchard%20Environments%0AAuthor%3A%20Ranjan%20Sapkota%20and%20Zhichao%20Meng%20and%20Martin%20Churuvija%20and%20Xiaoqiang%20Du%20and%20Zenghong%20Ma%20and%20Manoj%20Karkee%0AAbstract%3A%20%20%20This%20study%20extensively%20evaluated%20You%20Only%20Look%20Once%20%28YOLO%29%20object%20detection%0Aalgorithms%20across%20all%20configurations%20%28total%2022%29%20of%20YOLOv8%2C%20YOLOv9%2C%20YOLOv10%2C%20and%0AYOLO11%20for%20green%20fruit%20detection%20in%20commercial%20orchards.%20The%20research%20also%0Avalidated%20in-field%20fruitlet%20counting%20using%20an%20iPhone%20and%20machine%20vision%20sensors%0Aacross%20four%20apple%20varieties%3A%20Scifresh%2C%20Scilate%2C%20Honeycrisp%20and%20Cosmic%20Crisp.%0AAmong%20the%2022%20configurations%20evaluated%2C%20YOLO11s%20and%20YOLOv9%20gelan-base%0Aoutperformed%20others%20with%20mAP%4050%20scores%20of%200.933%20and%200.935%20respectively.%20In%0Aterms%20of%20recall%2C%20YOLOv9%20gelan-base%20achieved%20the%20highest%20value%20among%20YOLOv9%0Aconfigurations%20at%200.899%2C%20while%20YOLO11m%20led%20YOLO11%20variants%20with%200.897.%20YOLO11n%0Aemerged%20as%20the%20fastest%20model%2C%20achieving%20fastest%20inference%20speed%20of%20only%202.4%20ms%2C%0Asignificantly%20outpacing%20the%20leading%20configurations%20of%20YOLOv10n%2C%20YOLOv9%20gelan-s%2C%0Aand%20YOLOv8n%2C%20with%20speeds%20of%205.5%2C%2011.5%2C%20and%204.1%20ms%2C%20respectively.%20This%0Acomparative%20evaluation%20highlights%20the%20strengths%20of%20YOLO11%2C%20YOLOv9%2C%20and%20YOLOv10%2C%0Aoffering%20researchers%20essential%20insights%20to%20choose%20the%20best-suited%20model%20for%0Afruitlet%20detection%20and%20possible%20automation%20in%20commercial%20orchards.%20For%0Areal-time%20automation%20related%20work%20in%20relevant%20datasets%2C%20we%20recommend%20using%0AYOLO11n%20due%20to%20its%20high%20detection%20and%20image%20processing%20speed.%20Keywords%3A%20YOLO11%2C%0AYOLO11%20Object%20Detection%2C%20YOLOv10%2C%20YOLOv9%2C%20YOLOv8%2C%20You%20Only%20Look%20Once%2C%20Fruitlet%0ADetection%2C%20Greenfruit%20Detection%2C%20Green%20Apple%20Detection%2C%20Agricultural%0AAutomation%2C%20Artificial%20Intelligence%2C%20Deep%20Learning%2C%20Machine%20Learning%2C%20Zero-shot%0ADetection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12040v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520Performance%2520Evaluation%2520of%2520YOLO11%252C%2520YOLOv10%252C%2520YOLOv9%2520and%250A%2520%2520YOLOv8%2520on%2520Detecting%2520and%2520Counting%2520Fruitlet%2520in%2520Complex%2520Orchard%2520Environments%26entry.906535625%3DRanjan%2520Sapkota%2520and%2520Zhichao%2520Meng%2520and%2520Martin%2520Churuvija%2520and%2520Xiaoqiang%2520Du%2520and%2520Zenghong%2520Ma%2520and%2520Manoj%2520Karkee%26entry.1292438233%3D%2520%2520This%2520study%2520extensively%2520evaluated%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520object%2520detection%250Aalgorithms%2520across%2520all%2520configurations%2520%2528total%252022%2529%2520of%2520YOLOv8%252C%2520YOLOv9%252C%2520YOLOv10%252C%2520and%250AYOLO11%2520for%2520green%2520fruit%2520detection%2520in%2520commercial%2520orchards.%2520The%2520research%2520also%250Avalidated%2520in-field%2520fruitlet%2520counting%2520using%2520an%2520iPhone%2520and%2520machine%2520vision%2520sensors%250Aacross%2520four%2520apple%2520varieties%253A%2520Scifresh%252C%2520Scilate%252C%2520Honeycrisp%2520and%2520Cosmic%2520Crisp.%250AAmong%2520the%252022%2520configurations%2520evaluated%252C%2520YOLO11s%2520and%2520YOLOv9%2520gelan-base%250Aoutperformed%2520others%2520with%2520mAP%254050%2520scores%2520of%25200.933%2520and%25200.935%2520respectively.%2520In%250Aterms%2520of%2520recall%252C%2520YOLOv9%2520gelan-base%2520achieved%2520the%2520highest%2520value%2520among%2520YOLOv9%250Aconfigurations%2520at%25200.899%252C%2520while%2520YOLO11m%2520led%2520YOLO11%2520variants%2520with%25200.897.%2520YOLO11n%250Aemerged%2520as%2520the%2520fastest%2520model%252C%2520achieving%2520fastest%2520inference%2520speed%2520of%2520only%25202.4%2520ms%252C%250Asignificantly%2520outpacing%2520the%2520leading%2520configurations%2520of%2520YOLOv10n%252C%2520YOLOv9%2520gelan-s%252C%250Aand%2520YOLOv8n%252C%2520with%2520speeds%2520of%25205.5%252C%252011.5%252C%2520and%25204.1%2520ms%252C%2520respectively.%2520This%250Acomparative%2520evaluation%2520highlights%2520the%2520strengths%2520of%2520YOLO11%252C%2520YOLOv9%252C%2520and%2520YOLOv10%252C%250Aoffering%2520researchers%2520essential%2520insights%2520to%2520choose%2520the%2520best-suited%2520model%2520for%250Afruitlet%2520detection%2520and%2520possible%2520automation%2520in%2520commercial%2520orchards.%2520For%250Areal-time%2520automation%2520related%2520work%2520in%2520relevant%2520datasets%252C%2520we%2520recommend%2520using%250AYOLO11n%2520due%2520to%2520its%2520high%2520detection%2520and%2520image%2520processing%2520speed.%2520Keywords%253A%2520YOLO11%252C%250AYOLO11%2520Object%2520Detection%252C%2520YOLOv10%252C%2520YOLOv9%252C%2520YOLOv8%252C%2520You%2520Only%2520Look%2520Once%252C%2520Fruitlet%250ADetection%252C%2520Greenfruit%2520Detection%252C%2520Green%2520Apple%2520Detection%252C%2520Agricultural%250AAutomation%252C%2520Artificial%2520Intelligence%252C%2520Deep%2520Learning%252C%2520Machine%2520Learning%252C%2520Zero-shot%250ADetection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12040v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20Performance%20Evaluation%20of%20YOLO11%2C%20YOLOv10%2C%20YOLOv9%20and%0A%20%20YOLOv8%20on%20Detecting%20and%20Counting%20Fruitlet%20in%20Complex%20Orchard%20Environments&entry.906535625=Ranjan%20Sapkota%20and%20Zhichao%20Meng%20and%20Martin%20Churuvija%20and%20Xiaoqiang%20Du%20and%20Zenghong%20Ma%20and%20Manoj%20Karkee&entry.1292438233=%20%20This%20study%20extensively%20evaluated%20You%20Only%20Look%20Once%20%28YOLO%29%20object%20detection%0Aalgorithms%20across%20all%20configurations%20%28total%2022%29%20of%20YOLOv8%2C%20YOLOv9%2C%20YOLOv10%2C%20and%0AYOLO11%20for%20green%20fruit%20detection%20in%20commercial%20orchards.%20The%20research%20also%0Avalidated%20in-field%20fruitlet%20counting%20using%20an%20iPhone%20and%20machine%20vision%20sensors%0Aacross%20four%20apple%20varieties%3A%20Scifresh%2C%20Scilate%2C%20Honeycrisp%20and%20Cosmic%20Crisp.%0AAmong%20the%2022%20configurations%20evaluated%2C%20YOLO11s%20and%20YOLOv9%20gelan-base%0Aoutperformed%20others%20with%20mAP%4050%20scores%20of%200.933%20and%200.935%20respectively.%20In%0Aterms%20of%20recall%2C%20YOLOv9%20gelan-base%20achieved%20the%20highest%20value%20among%20YOLOv9%0Aconfigurations%20at%200.899%2C%20while%20YOLO11m%20led%20YOLO11%20variants%20with%200.897.%20YOLO11n%0Aemerged%20as%20the%20fastest%20model%2C%20achieving%20fastest%20inference%20speed%20of%20only%202.4%20ms%2C%0Asignificantly%20outpacing%20the%20leading%20configurations%20of%20YOLOv10n%2C%20YOLOv9%20gelan-s%2C%0Aand%20YOLOv8n%2C%20with%20speeds%20of%205.5%2C%2011.5%2C%20and%204.1%20ms%2C%20respectively.%20This%0Acomparative%20evaluation%20highlights%20the%20strengths%20of%20YOLO11%2C%20YOLOv9%2C%20and%20YOLOv10%2C%0Aoffering%20researchers%20essential%20insights%20to%20choose%20the%20best-suited%20model%20for%0Afruitlet%20detection%20and%20possible%20automation%20in%20commercial%20orchards.%20For%0Areal-time%20automation%20related%20work%20in%20relevant%20datasets%2C%20we%20recommend%20using%0AYOLO11n%20due%20to%20its%20high%20detection%20and%20image%20processing%20speed.%20Keywords%3A%20YOLO11%2C%0AYOLO11%20Object%20Detection%2C%20YOLOv10%2C%20YOLOv9%2C%20YOLOv8%2C%20You%20Only%20Look%20Once%2C%20Fruitlet%0ADetection%2C%20Greenfruit%20Detection%2C%20Green%20Apple%20Detection%2C%20Agricultural%0AAutomation%2C%20Artificial%20Intelligence%2C%20Deep%20Learning%2C%20Machine%20Learning%2C%20Zero-shot%0ADetection%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12040v4&entry.124074799=Read"},
{"title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs", "author": "Azmain Kabir and Shaowei Wang and Yuan Tian and Tse-Hsun Chen and Muhammad Asaduzzaman and Wenbin Zhang", "abstract": "  Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1.\n", "link": "http://arxiv.org/abs/2401.14279v2", "date": "2024-10-09", "relevancy": 2.1791, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4435}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZS4C%3A%20Zero-Shot%20Synthesis%20of%20Compilable%20Code%20for%20Incomplete%20Code%0A%20%20Snippets%20using%20LLMs&body=Title%3A%20ZS4C%3A%20Zero-Shot%20Synthesis%20of%20Compilable%20Code%20for%20Incomplete%20Code%0A%20%20Snippets%20using%20LLMs%0AAuthor%3A%20Azmain%20Kabir%20and%20Shaowei%20Wang%20and%20Yuan%20Tian%20and%20Tse-Hsun%20Chen%20and%20Muhammad%20Asaduzzaman%20and%20Wenbin%20Zhang%0AAbstract%3A%20%20%20Technical%20Q%26A%20sites%20are%20valuable%20for%20software%20developers%20seeking%20knowledge%2C%0Abut%20the%20code%20snippets%20they%20provide%20are%20often%20uncompilable%20and%20incomplete%20due%20to%0Aunresolved%20types%20and%20missing%20libraries.%20This%20poses%20a%20challenge%20for%20users%20who%0Awish%20to%20reuse%20or%20analyze%20these%20snippets.%20Existing%20methods%20either%20do%20not%20focus%0Aon%20creating%20compilable%20code%20or%20have%20low%20success%20rates.%20To%20address%20this%2C%20we%0Apropose%20ZS4C%2C%20a%20lightweight%20approach%20for%20zero-shot%20synthesis%20of%20compilable%20code%0Afrom%20incomplete%20snippets%20using%20Large%20Language%20Models%20%28LLMs%29.%20ZS4C%20operates%20in%0Atwo%20stages%3A%20first%2C%20it%20uses%20an%20LLM%2C%20like%20GPT-3.5%2C%20to%20identify%20missing%20import%0Astatements%20in%20a%20snippet%3B%20second%2C%20it%20collaborates%20with%20a%20validator%20%28e.g.%2C%0Acompiler%29%20to%20fix%20compilation%20errors%20caused%20by%20incorrect%20imports%20and%20syntax%0Aissues.%20We%20evaluated%20ZS4C%20on%20the%20StatType-SO%20benchmark%20and%20a%20new%20dataset%2C%0APython-SO%2C%20which%20includes%20539%20Python%20snippets%20from%20Stack%20Overflow%20across%20the%2020%0Amost%20popular%20Python%20libraries.%20ZS4C%20significantly%20outperforms%20existing%20methods%2C%0Aimproving%20the%20compilation%20rate%20from%2063%25%20to%2095.1%25%20compared%20to%20the%0Astate-of-the-art%20SnR%2C%20marking%20a%2050.1%25%20improvement.%20On%20average%2C%20ZS4C%20can%20infer%0Amore%20accurate%20import%20statements%20%28with%20an%20F1%20score%20of%200.98%29%20than%20SnR%2C%20with%20an%0Aimprovement%20of%208.5%25%20in%20the%20F1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZS4C%253A%2520Zero-Shot%2520Synthesis%2520of%2520Compilable%2520Code%2520for%2520Incomplete%2520Code%250A%2520%2520Snippets%2520using%2520LLMs%26entry.906535625%3DAzmain%2520Kabir%2520and%2520Shaowei%2520Wang%2520and%2520Yuan%2520Tian%2520and%2520Tse-Hsun%2520Chen%2520and%2520Muhammad%2520Asaduzzaman%2520and%2520Wenbin%2520Zhang%26entry.1292438233%3D%2520%2520Technical%2520Q%2526A%2520sites%2520are%2520valuable%2520for%2520software%2520developers%2520seeking%2520knowledge%252C%250Abut%2520the%2520code%2520snippets%2520they%2520provide%2520are%2520often%2520uncompilable%2520and%2520incomplete%2520due%2520to%250Aunresolved%2520types%2520and%2520missing%2520libraries.%2520This%2520poses%2520a%2520challenge%2520for%2520users%2520who%250Awish%2520to%2520reuse%2520or%2520analyze%2520these%2520snippets.%2520Existing%2520methods%2520either%2520do%2520not%2520focus%250Aon%2520creating%2520compilable%2520code%2520or%2520have%2520low%2520success%2520rates.%2520To%2520address%2520this%252C%2520we%250Apropose%2520ZS4C%252C%2520a%2520lightweight%2520approach%2520for%2520zero-shot%2520synthesis%2520of%2520compilable%2520code%250Afrom%2520incomplete%2520snippets%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520ZS4C%2520operates%2520in%250Atwo%2520stages%253A%2520first%252C%2520it%2520uses%2520an%2520LLM%252C%2520like%2520GPT-3.5%252C%2520to%2520identify%2520missing%2520import%250Astatements%2520in%2520a%2520snippet%253B%2520second%252C%2520it%2520collaborates%2520with%2520a%2520validator%2520%2528e.g.%252C%250Acompiler%2529%2520to%2520fix%2520compilation%2520errors%2520caused%2520by%2520incorrect%2520imports%2520and%2520syntax%250Aissues.%2520We%2520evaluated%2520ZS4C%2520on%2520the%2520StatType-SO%2520benchmark%2520and%2520a%2520new%2520dataset%252C%250APython-SO%252C%2520which%2520includes%2520539%2520Python%2520snippets%2520from%2520Stack%2520Overflow%2520across%2520the%252020%250Amost%2520popular%2520Python%2520libraries.%2520ZS4C%2520significantly%2520outperforms%2520existing%2520methods%252C%250Aimproving%2520the%2520compilation%2520rate%2520from%252063%2525%2520to%252095.1%2525%2520compared%2520to%2520the%250Astate-of-the-art%2520SnR%252C%2520marking%2520a%252050.1%2525%2520improvement.%2520On%2520average%252C%2520ZS4C%2520can%2520infer%250Amore%2520accurate%2520import%2520statements%2520%2528with%2520an%2520F1%2520score%2520of%25200.98%2529%2520than%2520SnR%252C%2520with%2520an%250Aimprovement%2520of%25208.5%2525%2520in%2520the%2520F1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZS4C%3A%20Zero-Shot%20Synthesis%20of%20Compilable%20Code%20for%20Incomplete%20Code%0A%20%20Snippets%20using%20LLMs&entry.906535625=Azmain%20Kabir%20and%20Shaowei%20Wang%20and%20Yuan%20Tian%20and%20Tse-Hsun%20Chen%20and%20Muhammad%20Asaduzzaman%20and%20Wenbin%20Zhang&entry.1292438233=%20%20Technical%20Q%26A%20sites%20are%20valuable%20for%20software%20developers%20seeking%20knowledge%2C%0Abut%20the%20code%20snippets%20they%20provide%20are%20often%20uncompilable%20and%20incomplete%20due%20to%0Aunresolved%20types%20and%20missing%20libraries.%20This%20poses%20a%20challenge%20for%20users%20who%0Awish%20to%20reuse%20or%20analyze%20these%20snippets.%20Existing%20methods%20either%20do%20not%20focus%0Aon%20creating%20compilable%20code%20or%20have%20low%20success%20rates.%20To%20address%20this%2C%20we%0Apropose%20ZS4C%2C%20a%20lightweight%20approach%20for%20zero-shot%20synthesis%20of%20compilable%20code%0Afrom%20incomplete%20snippets%20using%20Large%20Language%20Models%20%28LLMs%29.%20ZS4C%20operates%20in%0Atwo%20stages%3A%20first%2C%20it%20uses%20an%20LLM%2C%20like%20GPT-3.5%2C%20to%20identify%20missing%20import%0Astatements%20in%20a%20snippet%3B%20second%2C%20it%20collaborates%20with%20a%20validator%20%28e.g.%2C%0Acompiler%29%20to%20fix%20compilation%20errors%20caused%20by%20incorrect%20imports%20and%20syntax%0Aissues.%20We%20evaluated%20ZS4C%20on%20the%20StatType-SO%20benchmark%20and%20a%20new%20dataset%2C%0APython-SO%2C%20which%20includes%20539%20Python%20snippets%20from%20Stack%20Overflow%20across%20the%2020%0Amost%20popular%20Python%20libraries.%20ZS4C%20significantly%20outperforms%20existing%20methods%2C%0Aimproving%20the%20compilation%20rate%20from%2063%25%20to%2095.1%25%20compared%20to%20the%0Astate-of-the-art%20SnR%2C%20marking%20a%2050.1%25%20improvement.%20On%20average%2C%20ZS4C%20can%20infer%0Amore%20accurate%20import%20statements%20%28with%20an%20F1%20score%20of%200.98%29%20than%20SnR%2C%20with%20an%0Aimprovement%20of%208.5%25%20in%20the%20F1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14279v2&entry.124074799=Read"},
{"title": "Applying Quantum Autoencoders for Time Series Anomaly Detection", "author": "Robin Frehner and Kurt Stockinger", "abstract": "  Anomaly detection is an important problem with applications in various\ndomains such as fraud detection, pattern recognition or medical diagnosis.\nSeveral algorithms have been introduced using classical computing approaches.\nHowever, using quantum computing for solving anomaly detection problems in time\nseries data is a widely unexplored research field.\n  This paper explores the application of quantum autoencoders to time series\nanomaly detection. We investigate two primary techniques for classifying\nanomalies: (1) Analyzing the reconstruction error generated by the quantum\nautoencoder and (2) latent representation analysis. Our simulated experimental\nresults, conducted across various ansaetze, demonstrate that quantum\nautoencoders consistently outperform classical deep learning-based autoencoders\nacross multiple datasets. Specifically, quantum autoencoders achieve superior\nanomaly detection performance while utilizing 60-230 times fewer parameters and\nrequiring five times fewer training iterations. In addition, we implement our\nquantum encoder on real quantum hardware. Our experimental results demonstrate\nthat quantum autoencoders achieve anomaly detection performance on par with\ntheir simulated counterparts.\n", "link": "http://arxiv.org/abs/2410.04154v2", "date": "2024-10-09", "relevancy": 2.1614, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4492}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.427}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20Quantum%20Autoencoders%20for%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20Applying%20Quantum%20Autoencoders%20for%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Robin%20Frehner%20and%20Kurt%20Stockinger%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20an%20important%20problem%20with%20applications%20in%20various%0Adomains%20such%20as%20fraud%20detection%2C%20pattern%20recognition%20or%20medical%20diagnosis.%0ASeveral%20algorithms%20have%20been%20introduced%20using%20classical%20computing%20approaches.%0AHowever%2C%20using%20quantum%20computing%20for%20solving%20anomaly%20detection%20problems%20in%20time%0Aseries%20data%20is%20a%20widely%20unexplored%20research%20field.%0A%20%20This%20paper%20explores%20the%20application%20of%20quantum%20autoencoders%20to%20time%20series%0Aanomaly%20detection.%20We%20investigate%20two%20primary%20techniques%20for%20classifying%0Aanomalies%3A%20%281%29%20Analyzing%20the%20reconstruction%20error%20generated%20by%20the%20quantum%0Aautoencoder%20and%20%282%29%20latent%20representation%20analysis.%20Our%20simulated%20experimental%0Aresults%2C%20conducted%20across%20various%20ansaetze%2C%20demonstrate%20that%20quantum%0Aautoencoders%20consistently%20outperform%20classical%20deep%20learning-based%20autoencoders%0Aacross%20multiple%20datasets.%20Specifically%2C%20quantum%20autoencoders%20achieve%20superior%0Aanomaly%20detection%20performance%20while%20utilizing%2060-230%20times%20fewer%20parameters%20and%0Arequiring%20five%20times%20fewer%20training%20iterations.%20In%20addition%2C%20we%20implement%20our%0Aquantum%20encoder%20on%20real%20quantum%20hardware.%20Our%20experimental%20results%20demonstrate%0Athat%20quantum%20autoencoders%20achieve%20anomaly%20detection%20performance%20on%20par%20with%0Atheir%20simulated%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520Quantum%2520Autoencoders%2520for%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DRobin%2520Frehner%2520and%2520Kurt%2520Stockinger%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520an%2520important%2520problem%2520with%2520applications%2520in%2520various%250Adomains%2520such%2520as%2520fraud%2520detection%252C%2520pattern%2520recognition%2520or%2520medical%2520diagnosis.%250ASeveral%2520algorithms%2520have%2520been%2520introduced%2520using%2520classical%2520computing%2520approaches.%250AHowever%252C%2520using%2520quantum%2520computing%2520for%2520solving%2520anomaly%2520detection%2520problems%2520in%2520time%250Aseries%2520data%2520is%2520a%2520widely%2520unexplored%2520research%2520field.%250A%2520%2520This%2520paper%2520explores%2520the%2520application%2520of%2520quantum%2520autoencoders%2520to%2520time%2520series%250Aanomaly%2520detection.%2520We%2520investigate%2520two%2520primary%2520techniques%2520for%2520classifying%250Aanomalies%253A%2520%25281%2529%2520Analyzing%2520the%2520reconstruction%2520error%2520generated%2520by%2520the%2520quantum%250Aautoencoder%2520and%2520%25282%2529%2520latent%2520representation%2520analysis.%2520Our%2520simulated%2520experimental%250Aresults%252C%2520conducted%2520across%2520various%2520ansaetze%252C%2520demonstrate%2520that%2520quantum%250Aautoencoders%2520consistently%2520outperform%2520classical%2520deep%2520learning-based%2520autoencoders%250Aacross%2520multiple%2520datasets.%2520Specifically%252C%2520quantum%2520autoencoders%2520achieve%2520superior%250Aanomaly%2520detection%2520performance%2520while%2520utilizing%252060-230%2520times%2520fewer%2520parameters%2520and%250Arequiring%2520five%2520times%2520fewer%2520training%2520iterations.%2520In%2520addition%252C%2520we%2520implement%2520our%250Aquantum%2520encoder%2520on%2520real%2520quantum%2520hardware.%2520Our%2520experimental%2520results%2520demonstrate%250Athat%2520quantum%2520autoencoders%2520achieve%2520anomaly%2520detection%2520performance%2520on%2520par%2520with%250Atheir%2520simulated%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20Quantum%20Autoencoders%20for%20Time%20Series%20Anomaly%20Detection&entry.906535625=Robin%20Frehner%20and%20Kurt%20Stockinger&entry.1292438233=%20%20Anomaly%20detection%20is%20an%20important%20problem%20with%20applications%20in%20various%0Adomains%20such%20as%20fraud%20detection%2C%20pattern%20recognition%20or%20medical%20diagnosis.%0ASeveral%20algorithms%20have%20been%20introduced%20using%20classical%20computing%20approaches.%0AHowever%2C%20using%20quantum%20computing%20for%20solving%20anomaly%20detection%20problems%20in%20time%0Aseries%20data%20is%20a%20widely%20unexplored%20research%20field.%0A%20%20This%20paper%20explores%20the%20application%20of%20quantum%20autoencoders%20to%20time%20series%0Aanomaly%20detection.%20We%20investigate%20two%20primary%20techniques%20for%20classifying%0Aanomalies%3A%20%281%29%20Analyzing%20the%20reconstruction%20error%20generated%20by%20the%20quantum%0Aautoencoder%20and%20%282%29%20latent%20representation%20analysis.%20Our%20simulated%20experimental%0Aresults%2C%20conducted%20across%20various%20ansaetze%2C%20demonstrate%20that%20quantum%0Aautoencoders%20consistently%20outperform%20classical%20deep%20learning-based%20autoencoders%0Aacross%20multiple%20datasets.%20Specifically%2C%20quantum%20autoencoders%20achieve%20superior%0Aanomaly%20detection%20performance%20while%20utilizing%2060-230%20times%20fewer%20parameters%20and%0Arequiring%20five%20times%20fewer%20training%20iterations.%20In%20addition%2C%20we%20implement%20our%0Aquantum%20encoder%20on%20real%20quantum%20hardware.%20Our%20experimental%20results%20demonstrate%0Athat%20quantum%20autoencoders%20achieve%20anomaly%20detection%20performance%20on%20par%20with%0Atheir%20simulated%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04154v2&entry.124074799=Read"},
{"title": "JPEG Inspired Deep Learning", "author": "Ahmed H. Salamah and Kaixiang Zheng and Yiwen Liu and En-Hui Yang", "abstract": "  Although it is traditionally believed that lossy image compression, such as\nJPEG compression, has a negative impact on the performance of deep neural\nnetworks (DNNs), it is shown by recent works that well-crafted JPEG compression\ncan actually improve the performance of deep learning (DL). Inspired by this,\nwe propose JPEG-DL, a novel DL framework that prepends any underlying DNN\narchitecture with a trainable JPEG compression layer. To make the quantization\noperation in JPEG compression trainable, a new differentiable soft quantizer is\nemployed at the JPEG layer, and then the quantization operation and underlying\nDNN are jointly trained. Extensive experiments show that in comparison with the\nstandard DL, JPEG-DL delivers significant accuracy improvements across various\ndatasets and model architectures while enhancing robustness against adversarial\nattacks. Particularly, on some fine-grained image classification datasets,\nJPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is\navailable on https://github.com/JpegInspiredDl/JPEG-Inspired-DL.git.\n", "link": "http://arxiv.org/abs/2410.07081v1", "date": "2024-10-09", "relevancy": 2.1558, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5551}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5439}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JPEG%20Inspired%20Deep%20Learning&body=Title%3A%20JPEG%20Inspired%20Deep%20Learning%0AAuthor%3A%20Ahmed%20H.%20Salamah%20and%20Kaixiang%20Zheng%20and%20Yiwen%20Liu%20and%20En-Hui%20Yang%0AAbstract%3A%20%20%20Although%20it%20is%20traditionally%20believed%20that%20lossy%20image%20compression%2C%20such%20as%0AJPEG%20compression%2C%20has%20a%20negative%20impact%20on%20the%20performance%20of%20deep%20neural%0Anetworks%20%28DNNs%29%2C%20it%20is%20shown%20by%20recent%20works%20that%20well-crafted%20JPEG%20compression%0Acan%20actually%20improve%20the%20performance%20of%20deep%20learning%20%28DL%29.%20Inspired%20by%20this%2C%0Awe%20propose%20JPEG-DL%2C%20a%20novel%20DL%20framework%20that%20prepends%20any%20underlying%20DNN%0Aarchitecture%20with%20a%20trainable%20JPEG%20compression%20layer.%20To%20make%20the%20quantization%0Aoperation%20in%20JPEG%20compression%20trainable%2C%20a%20new%20differentiable%20soft%20quantizer%20is%0Aemployed%20at%20the%20JPEG%20layer%2C%20and%20then%20the%20quantization%20operation%20and%20underlying%0ADNN%20are%20jointly%20trained.%20Extensive%20experiments%20show%20that%20in%20comparison%20with%20the%0Astandard%20DL%2C%20JPEG-DL%20delivers%20significant%20accuracy%20improvements%20across%20various%0Adatasets%20and%20model%20architectures%20while%20enhancing%20robustness%20against%20adversarial%0Aattacks.%20Particularly%2C%20on%20some%20fine-grained%20image%20classification%20datasets%2C%0AJPEG-DL%20can%20increase%20prediction%20accuracy%20by%20as%20much%20as%2020.9%25.%20Our%20code%20is%0Aavailable%20on%20https%3A//github.com/JpegInspiredDl/JPEG-Inspired-DL.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJPEG%2520Inspired%2520Deep%2520Learning%26entry.906535625%3DAhmed%2520H.%2520Salamah%2520and%2520Kaixiang%2520Zheng%2520and%2520Yiwen%2520Liu%2520and%2520En-Hui%2520Yang%26entry.1292438233%3D%2520%2520Although%2520it%2520is%2520traditionally%2520believed%2520that%2520lossy%2520image%2520compression%252C%2520such%2520as%250AJPEG%2520compression%252C%2520has%2520a%2520negative%2520impact%2520on%2520the%2520performance%2520of%2520deep%2520neural%250Anetworks%2520%2528DNNs%2529%252C%2520it%2520is%2520shown%2520by%2520recent%2520works%2520that%2520well-crafted%2520JPEG%2520compression%250Acan%2520actually%2520improve%2520the%2520performance%2520of%2520deep%2520learning%2520%2528DL%2529.%2520Inspired%2520by%2520this%252C%250Awe%2520propose%2520JPEG-DL%252C%2520a%2520novel%2520DL%2520framework%2520that%2520prepends%2520any%2520underlying%2520DNN%250Aarchitecture%2520with%2520a%2520trainable%2520JPEG%2520compression%2520layer.%2520To%2520make%2520the%2520quantization%250Aoperation%2520in%2520JPEG%2520compression%2520trainable%252C%2520a%2520new%2520differentiable%2520soft%2520quantizer%2520is%250Aemployed%2520at%2520the%2520JPEG%2520layer%252C%2520and%2520then%2520the%2520quantization%2520operation%2520and%2520underlying%250ADNN%2520are%2520jointly%2520trained.%2520Extensive%2520experiments%2520show%2520that%2520in%2520comparison%2520with%2520the%250Astandard%2520DL%252C%2520JPEG-DL%2520delivers%2520significant%2520accuracy%2520improvements%2520across%2520various%250Adatasets%2520and%2520model%2520architectures%2520while%2520enhancing%2520robustness%2520against%2520adversarial%250Aattacks.%2520Particularly%252C%2520on%2520some%2520fine-grained%2520image%2520classification%2520datasets%252C%250AJPEG-DL%2520can%2520increase%2520prediction%2520accuracy%2520by%2520as%2520much%2520as%252020.9%2525.%2520Our%2520code%2520is%250Aavailable%2520on%2520https%253A//github.com/JpegInspiredDl/JPEG-Inspired-DL.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JPEG%20Inspired%20Deep%20Learning&entry.906535625=Ahmed%20H.%20Salamah%20and%20Kaixiang%20Zheng%20and%20Yiwen%20Liu%20and%20En-Hui%20Yang&entry.1292438233=%20%20Although%20it%20is%20traditionally%20believed%20that%20lossy%20image%20compression%2C%20such%20as%0AJPEG%20compression%2C%20has%20a%20negative%20impact%20on%20the%20performance%20of%20deep%20neural%0Anetworks%20%28DNNs%29%2C%20it%20is%20shown%20by%20recent%20works%20that%20well-crafted%20JPEG%20compression%0Acan%20actually%20improve%20the%20performance%20of%20deep%20learning%20%28DL%29.%20Inspired%20by%20this%2C%0Awe%20propose%20JPEG-DL%2C%20a%20novel%20DL%20framework%20that%20prepends%20any%20underlying%20DNN%0Aarchitecture%20with%20a%20trainable%20JPEG%20compression%20layer.%20To%20make%20the%20quantization%0Aoperation%20in%20JPEG%20compression%20trainable%2C%20a%20new%20differentiable%20soft%20quantizer%20is%0Aemployed%20at%20the%20JPEG%20layer%2C%20and%20then%20the%20quantization%20operation%20and%20underlying%0ADNN%20are%20jointly%20trained.%20Extensive%20experiments%20show%20that%20in%20comparison%20with%20the%0Astandard%20DL%2C%20JPEG-DL%20delivers%20significant%20accuracy%20improvements%20across%20various%0Adatasets%20and%20model%20architectures%20while%20enhancing%20robustness%20against%20adversarial%0Aattacks.%20Particularly%2C%20on%20some%20fine-grained%20image%20classification%20datasets%2C%0AJPEG-DL%20can%20increase%20prediction%20accuracy%20by%20as%20much%20as%2020.9%25.%20Our%20code%20is%0Aavailable%20on%20https%3A//github.com/JpegInspiredDl/JPEG-Inspired-DL.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07081v1&entry.124074799=Read"},
{"title": "Clean Evaluations on Contaminated Visual Language Models", "author": "Hongyuan Lu and Shujie Miao and Wai Lam", "abstract": "  How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication.\n", "link": "http://arxiv.org/abs/2410.07030v1", "date": "2024-10-09", "relevancy": 2.1463, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clean%20Evaluations%20on%20Contaminated%20Visual%20Language%20Models&body=Title%3A%20Clean%20Evaluations%20on%20Contaminated%20Visual%20Language%20Models%0AAuthor%3A%20Hongyuan%20Lu%20and%20Shujie%20Miao%20and%20Wai%20Lam%0AAbstract%3A%20%20%20How%20to%20evaluate%20large%20language%20models%20%28LLMs%29%20cleanly%20has%20been%20established%20as%0Aan%20important%20research%20era%20to%20genuinely%20report%20the%20performance%20of%20possibly%0Acontaminated%20LLMs.%20Yet%2C%20how%20to%20cleanly%20evaluate%20the%20visual%20language%20models%0A%28VLMs%29%20is%20an%20under-studied%20problem.%20We%20propose%20a%20novel%20approach%20to%20achieve%20such%0Agoals%20through%20data%20augmentation%20methods%20on%20the%20visual%20input%20information.%20We%0Athen%20craft%20a%20new%20visual%20clean%20evaluation%20benchmark%20with%20thousands%20of%20data%0Ainstances.%20Through%20extensive%20experiments%2C%20we%20found%20that%20the%20traditional%20visual%0Adata%20augmentation%20methods%20are%20useful%2C%20but%20they%20are%20at%20risk%20of%20being%20used%20as%20a%0Apart%20of%20the%20training%20data%20as%20a%20workaround.%20We%20further%20propose%20using%20BGR%0Aaugmentation%20to%20switch%20the%20colour%20channel%20of%20the%20visual%20information.%20We%20found%0Athat%20it%20is%20a%20simple%20yet%20effective%20method%20for%20reducing%20the%20effect%20of%20data%0Acontamination%20and%20fortunately%2C%20it%20is%20also%20harmful%20to%20be%20used%20as%20a%20data%0Aaugmentation%20method%20during%20training.%20It%20means%20that%20it%20is%20hard%20to%20integrate%20such%0Adata%20augmentation%20into%20training%20by%20malicious%20trainers%20and%20it%20could%20be%20a%0Apromising%20technique%20to%20cleanly%20evaluate%20visual%20LLMs.%20Our%20code%2C%20data%2C%20and%20model%0Aweights%20will%20be%20released%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClean%2520Evaluations%2520on%2520Contaminated%2520Visual%2520Language%2520Models%26entry.906535625%3DHongyuan%2520Lu%2520and%2520Shujie%2520Miao%2520and%2520Wai%2520Lam%26entry.1292438233%3D%2520%2520How%2520to%2520evaluate%2520large%2520language%2520models%2520%2528LLMs%2529%2520cleanly%2520has%2520been%2520established%2520as%250Aan%2520important%2520research%2520era%2520to%2520genuinely%2520report%2520the%2520performance%2520of%2520possibly%250Acontaminated%2520LLMs.%2520Yet%252C%2520how%2520to%2520cleanly%2520evaluate%2520the%2520visual%2520language%2520models%250A%2528VLMs%2529%2520is%2520an%2520under-studied%2520problem.%2520We%2520propose%2520a%2520novel%2520approach%2520to%2520achieve%2520such%250Agoals%2520through%2520data%2520augmentation%2520methods%2520on%2520the%2520visual%2520input%2520information.%2520We%250Athen%2520craft%2520a%2520new%2520visual%2520clean%2520evaluation%2520benchmark%2520with%2520thousands%2520of%2520data%250Ainstances.%2520Through%2520extensive%2520experiments%252C%2520we%2520found%2520that%2520the%2520traditional%2520visual%250Adata%2520augmentation%2520methods%2520are%2520useful%252C%2520but%2520they%2520are%2520at%2520risk%2520of%2520being%2520used%2520as%2520a%250Apart%2520of%2520the%2520training%2520data%2520as%2520a%2520workaround.%2520We%2520further%2520propose%2520using%2520BGR%250Aaugmentation%2520to%2520switch%2520the%2520colour%2520channel%2520of%2520the%2520visual%2520information.%2520We%2520found%250Athat%2520it%2520is%2520a%2520simple%2520yet%2520effective%2520method%2520for%2520reducing%2520the%2520effect%2520of%2520data%250Acontamination%2520and%2520fortunately%252C%2520it%2520is%2520also%2520harmful%2520to%2520be%2520used%2520as%2520a%2520data%250Aaugmentation%2520method%2520during%2520training.%2520It%2520means%2520that%2520it%2520is%2520hard%2520to%2520integrate%2520such%250Adata%2520augmentation%2520into%2520training%2520by%2520malicious%2520trainers%2520and%2520it%2520could%2520be%2520a%250Apromising%2520technique%2520to%2520cleanly%2520evaluate%2520visual%2520LLMs.%2520Our%2520code%252C%2520data%252C%2520and%2520model%250Aweights%2520will%2520be%2520released%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clean%20Evaluations%20on%20Contaminated%20Visual%20Language%20Models&entry.906535625=Hongyuan%20Lu%20and%20Shujie%20Miao%20and%20Wai%20Lam&entry.1292438233=%20%20How%20to%20evaluate%20large%20language%20models%20%28LLMs%29%20cleanly%20has%20been%20established%20as%0Aan%20important%20research%20era%20to%20genuinely%20report%20the%20performance%20of%20possibly%0Acontaminated%20LLMs.%20Yet%2C%20how%20to%20cleanly%20evaluate%20the%20visual%20language%20models%0A%28VLMs%29%20is%20an%20under-studied%20problem.%20We%20propose%20a%20novel%20approach%20to%20achieve%20such%0Agoals%20through%20data%20augmentation%20methods%20on%20the%20visual%20input%20information.%20We%0Athen%20craft%20a%20new%20visual%20clean%20evaluation%20benchmark%20with%20thousands%20of%20data%0Ainstances.%20Through%20extensive%20experiments%2C%20we%20found%20that%20the%20traditional%20visual%0Adata%20augmentation%20methods%20are%20useful%2C%20but%20they%20are%20at%20risk%20of%20being%20used%20as%20a%0Apart%20of%20the%20training%20data%20as%20a%20workaround.%20We%20further%20propose%20using%20BGR%0Aaugmentation%20to%20switch%20the%20colour%20channel%20of%20the%20visual%20information.%20We%20found%0Athat%20it%20is%20a%20simple%20yet%20effective%20method%20for%20reducing%20the%20effect%20of%20data%0Acontamination%20and%20fortunately%2C%20it%20is%20also%20harmful%20to%20be%20used%20as%20a%20data%0Aaugmentation%20method%20during%20training.%20It%20means%20that%20it%20is%20hard%20to%20integrate%20such%0Adata%20augmentation%20into%20training%20by%20malicious%20trainers%20and%20it%20could%20be%20a%0Apromising%20technique%20to%20cleanly%20evaluate%20visual%20LLMs.%20Our%20code%2C%20data%2C%20and%20model%0Aweights%20will%20be%20released%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07030v1&entry.124074799=Read"},
{"title": "CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based\n  Multi-Entity Action Recognition", "author": "Yuhang Wen and Mengyuan Liu and Songtao Wu and Beichen Ding", "abstract": "  Skeleton-based multi-entity action recognition is a challenging task aiming\nto identify interactive actions or group activities involving multiple diverse\nentities. Existing models for individuals often fall short in this task due to\nthe inherent distribution discrepancies among entity skeletons, leading to\nsuboptimal backbone optimization. To this end, we introduce a Convex Hull\nAdaptive Shift based multi-Entity action recognition method (CHASE), which\nmitigates inter-entity distribution gaps and unbiases subsequent backbones.\nSpecifically, CHASE comprises a learnable parameterized network and an\nauxiliary objective. The parameterized network achieves plausible,\nsample-adaptive repositioning of skeleton sequences through two key components.\nFirst, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new\norigin of the coordinate system is within the skeleton convex hull. Second, the\nCoefficient Learning Block provides a lightweight parameterization of the\nmapping from skeleton sequences to their specific coefficients in convex\ncombinations. Moreover, to guide the optimization of this network for\ndiscrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean\nDiscrepancy as the additional objective. CHASE operates as a sample-adaptive\nnormalization method to mitigate inter-entity distribution discrepancies,\nthereby reducing data bias and improving the subsequent classifier's\nmulti-entity action recognition performance. Extensive experiments on six\ndatasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and\nVolleyball, consistently verify our approach by seamlessly adapting to\nsingle-entity backbones and boosting their performance in multi-entity\nscenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .\n", "link": "http://arxiv.org/abs/2410.07153v1", "date": "2024-10-09", "relevancy": 2.141, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5464}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHASE%3A%20Learning%20Convex%20Hull%20Adaptive%20Shift%20for%20Skeleton-based%0A%20%20Multi-Entity%20Action%20Recognition&body=Title%3A%20CHASE%3A%20Learning%20Convex%20Hull%20Adaptive%20Shift%20for%20Skeleton-based%0A%20%20Multi-Entity%20Action%20Recognition%0AAuthor%3A%20Yuhang%20Wen%20and%20Mengyuan%20Liu%20and%20Songtao%20Wu%20and%20Beichen%20Ding%0AAbstract%3A%20%20%20Skeleton-based%20multi-entity%20action%20recognition%20is%20a%20challenging%20task%20aiming%0Ato%20identify%20interactive%20actions%20or%20group%20activities%20involving%20multiple%20diverse%0Aentities.%20Existing%20models%20for%20individuals%20often%20fall%20short%20in%20this%20task%20due%20to%0Athe%20inherent%20distribution%20discrepancies%20among%20entity%20skeletons%2C%20leading%20to%0Asuboptimal%20backbone%20optimization.%20To%20this%20end%2C%20we%20introduce%20a%20Convex%20Hull%0AAdaptive%20Shift%20based%20multi-Entity%20action%20recognition%20method%20%28CHASE%29%2C%20which%0Amitigates%20inter-entity%20distribution%20gaps%20and%20unbiases%20subsequent%20backbones.%0ASpecifically%2C%20CHASE%20comprises%20a%20learnable%20parameterized%20network%20and%20an%0Aauxiliary%20objective.%20The%20parameterized%20network%20achieves%20plausible%2C%0Asample-adaptive%20repositioning%20of%20skeleton%20sequences%20through%20two%20key%20components.%0AFirst%2C%20the%20Implicit%20Convex%20Hull%20Constrained%20Adaptive%20Shift%20ensures%20that%20the%20new%0Aorigin%20of%20the%20coordinate%20system%20is%20within%20the%20skeleton%20convex%20hull.%20Second%2C%20the%0ACoefficient%20Learning%20Block%20provides%20a%20lightweight%20parameterization%20of%20the%0Amapping%20from%20skeleton%20sequences%20to%20their%20specific%20coefficients%20in%20convex%0Acombinations.%20Moreover%2C%20to%20guide%20the%20optimization%20of%20this%20network%20for%0Adiscrepancy%20minimization%2C%20we%20propose%20the%20Mini-batch%20Pair-wise%20Maximum%20Mean%0ADiscrepancy%20as%20the%20additional%20objective.%20CHASE%20operates%20as%20a%20sample-adaptive%0Anormalization%20method%20to%20mitigate%20inter-entity%20distribution%20discrepancies%2C%0Athereby%20reducing%20data%20bias%20and%20improving%20the%20subsequent%20classifier%27s%0Amulti-entity%20action%20recognition%20performance.%20Extensive%20experiments%20on%20six%0Adatasets%2C%20including%20NTU%20Mutual%2011/26%2C%20H2O%2C%20Assembly101%2C%20Collective%20Activity%20and%0AVolleyball%2C%20consistently%20verify%20our%20approach%20by%20seamlessly%20adapting%20to%0Asingle-entity%20backbones%20and%20boosting%20their%20performance%20in%20multi-entity%0Ascenarios.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Necolizer/CHASE%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHASE%253A%2520Learning%2520Convex%2520Hull%2520Adaptive%2520Shift%2520for%2520Skeleton-based%250A%2520%2520Multi-Entity%2520Action%2520Recognition%26entry.906535625%3DYuhang%2520Wen%2520and%2520Mengyuan%2520Liu%2520and%2520Songtao%2520Wu%2520and%2520Beichen%2520Ding%26entry.1292438233%3D%2520%2520Skeleton-based%2520multi-entity%2520action%2520recognition%2520is%2520a%2520challenging%2520task%2520aiming%250Ato%2520identify%2520interactive%2520actions%2520or%2520group%2520activities%2520involving%2520multiple%2520diverse%250Aentities.%2520Existing%2520models%2520for%2520individuals%2520often%2520fall%2520short%2520in%2520this%2520task%2520due%2520to%250Athe%2520inherent%2520distribution%2520discrepancies%2520among%2520entity%2520skeletons%252C%2520leading%2520to%250Asuboptimal%2520backbone%2520optimization.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520Convex%2520Hull%250AAdaptive%2520Shift%2520based%2520multi-Entity%2520action%2520recognition%2520method%2520%2528CHASE%2529%252C%2520which%250Amitigates%2520inter-entity%2520distribution%2520gaps%2520and%2520unbiases%2520subsequent%2520backbones.%250ASpecifically%252C%2520CHASE%2520comprises%2520a%2520learnable%2520parameterized%2520network%2520and%2520an%250Aauxiliary%2520objective.%2520The%2520parameterized%2520network%2520achieves%2520plausible%252C%250Asample-adaptive%2520repositioning%2520of%2520skeleton%2520sequences%2520through%2520two%2520key%2520components.%250AFirst%252C%2520the%2520Implicit%2520Convex%2520Hull%2520Constrained%2520Adaptive%2520Shift%2520ensures%2520that%2520the%2520new%250Aorigin%2520of%2520the%2520coordinate%2520system%2520is%2520within%2520the%2520skeleton%2520convex%2520hull.%2520Second%252C%2520the%250ACoefficient%2520Learning%2520Block%2520provides%2520a%2520lightweight%2520parameterization%2520of%2520the%250Amapping%2520from%2520skeleton%2520sequences%2520to%2520their%2520specific%2520coefficients%2520in%2520convex%250Acombinations.%2520Moreover%252C%2520to%2520guide%2520the%2520optimization%2520of%2520this%2520network%2520for%250Adiscrepancy%2520minimization%252C%2520we%2520propose%2520the%2520Mini-batch%2520Pair-wise%2520Maximum%2520Mean%250ADiscrepancy%2520as%2520the%2520additional%2520objective.%2520CHASE%2520operates%2520as%2520a%2520sample-adaptive%250Anormalization%2520method%2520to%2520mitigate%2520inter-entity%2520distribution%2520discrepancies%252C%250Athereby%2520reducing%2520data%2520bias%2520and%2520improving%2520the%2520subsequent%2520classifier%2527s%250Amulti-entity%2520action%2520recognition%2520performance.%2520Extensive%2520experiments%2520on%2520six%250Adatasets%252C%2520including%2520NTU%2520Mutual%252011/26%252C%2520H2O%252C%2520Assembly101%252C%2520Collective%2520Activity%2520and%250AVolleyball%252C%2520consistently%2520verify%2520our%2520approach%2520by%2520seamlessly%2520adapting%2520to%250Asingle-entity%2520backbones%2520and%2520boosting%2520their%2520performance%2520in%2520multi-entity%250Ascenarios.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Necolizer/CHASE%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHASE%3A%20Learning%20Convex%20Hull%20Adaptive%20Shift%20for%20Skeleton-based%0A%20%20Multi-Entity%20Action%20Recognition&entry.906535625=Yuhang%20Wen%20and%20Mengyuan%20Liu%20and%20Songtao%20Wu%20and%20Beichen%20Ding&entry.1292438233=%20%20Skeleton-based%20multi-entity%20action%20recognition%20is%20a%20challenging%20task%20aiming%0Ato%20identify%20interactive%20actions%20or%20group%20activities%20involving%20multiple%20diverse%0Aentities.%20Existing%20models%20for%20individuals%20often%20fall%20short%20in%20this%20task%20due%20to%0Athe%20inherent%20distribution%20discrepancies%20among%20entity%20skeletons%2C%20leading%20to%0Asuboptimal%20backbone%20optimization.%20To%20this%20end%2C%20we%20introduce%20a%20Convex%20Hull%0AAdaptive%20Shift%20based%20multi-Entity%20action%20recognition%20method%20%28CHASE%29%2C%20which%0Amitigates%20inter-entity%20distribution%20gaps%20and%20unbiases%20subsequent%20backbones.%0ASpecifically%2C%20CHASE%20comprises%20a%20learnable%20parameterized%20network%20and%20an%0Aauxiliary%20objective.%20The%20parameterized%20network%20achieves%20plausible%2C%0Asample-adaptive%20repositioning%20of%20skeleton%20sequences%20through%20two%20key%20components.%0AFirst%2C%20the%20Implicit%20Convex%20Hull%20Constrained%20Adaptive%20Shift%20ensures%20that%20the%20new%0Aorigin%20of%20the%20coordinate%20system%20is%20within%20the%20skeleton%20convex%20hull.%20Second%2C%20the%0ACoefficient%20Learning%20Block%20provides%20a%20lightweight%20parameterization%20of%20the%0Amapping%20from%20skeleton%20sequences%20to%20their%20specific%20coefficients%20in%20convex%0Acombinations.%20Moreover%2C%20to%20guide%20the%20optimization%20of%20this%20network%20for%0Adiscrepancy%20minimization%2C%20we%20propose%20the%20Mini-batch%20Pair-wise%20Maximum%20Mean%0ADiscrepancy%20as%20the%20additional%20objective.%20CHASE%20operates%20as%20a%20sample-adaptive%0Anormalization%20method%20to%20mitigate%20inter-entity%20distribution%20discrepancies%2C%0Athereby%20reducing%20data%20bias%20and%20improving%20the%20subsequent%20classifier%27s%0Amulti-entity%20action%20recognition%20performance.%20Extensive%20experiments%20on%20six%0Adatasets%2C%20including%20NTU%20Mutual%2011/26%2C%20H2O%2C%20Assembly101%2C%20Collective%20Activity%20and%0AVolleyball%2C%20consistently%20verify%20our%20approach%20by%20seamlessly%20adapting%20to%0Asingle-entity%20backbones%20and%20boosting%20their%20performance%20in%20multi-entity%0Ascenarios.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/Necolizer/CHASE%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07153v1&entry.124074799=Read"},
{"title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model", "author": "Chaochen Gao and Xing Wu and Qi Fu and Songlin Hu", "abstract": "  Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.\n", "link": "http://arxiv.org/abs/2405.19846v5", "date": "2024-10-09", "relevancy": 2.1281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5423}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quest%3A%20Query-centric%20Data%20Synthesis%20Approach%20for%20Long-context%20Scaling%20of%0A%20%20Large%20Language%20Model&body=Title%3A%20Quest%3A%20Query-centric%20Data%20Synthesis%20Approach%20for%20Long-context%20Scaling%20of%0A%20%20Large%20Language%20Model%0AAuthor%3A%20Chaochen%20Gao%20and%20Xing%20Wu%20and%20Qi%20Fu%20and%20Songlin%20Hu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20highlighted%20the%0Aimportance%20of%20extending%20context%20lengths%20for%20handling%20complex%20tasks.%20While%0Atraditional%20methods%20for%20training%20on%20long%20contexts%20often%20use%20filtered%20long%0Adocuments%2C%20these%20approaches%20lead%20to%20domain%20imbalances%2C%20limiting%20model%0Aperformance.%20To%20address%20this%2C%20techniques%20like%20random%20document%20concatenation%0A%28Standard%29%20and%20similarity-based%20methods%20%28KNN%2C%20ICLM%29%20have%20been%20developed.%0AHowever%2C%20they%20either%20sacrifice%20semantic%20coherence%20or%20diversity.%20To%20balance%20both%0Aaspects%2C%20we%20introduce%20Quest%2C%20a%20query-centric%20data%20synthesis%20method%20aggregating%0Asemantically%20relevant%20yet%20diverse%20documents.%20Quest%20uses%20a%20generative%20model%20to%0Apredict%20potential%20queries%20for%20each%20document%2C%20grouping%20documents%20with%20similar%0Aqueries%20and%20keywords.%20Extensive%20experiments%20demonstrate%20Quest%27s%20superior%0Aperformance%20on%20long-context%20tasks%2C%20achieving%20remarkable%20results%20with%20context%0Alengths%20of%20up%20to%201M%20tokens%20and%20confirming%20its%20scalability%20across%20various%20model%0Asizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19846v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuest%253A%2520Query-centric%2520Data%2520Synthesis%2520Approach%2520for%2520Long-context%2520Scaling%2520of%250A%2520%2520Large%2520Language%2520Model%26entry.906535625%3DChaochen%2520Gao%2520and%2520Xing%2520Wu%2520and%2520Qi%2520Fu%2520and%2520Songlin%2520Hu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520highlighted%2520the%250Aimportance%2520of%2520extending%2520context%2520lengths%2520for%2520handling%2520complex%2520tasks.%2520While%250Atraditional%2520methods%2520for%2520training%2520on%2520long%2520contexts%2520often%2520use%2520filtered%2520long%250Adocuments%252C%2520these%2520approaches%2520lead%2520to%2520domain%2520imbalances%252C%2520limiting%2520model%250Aperformance.%2520To%2520address%2520this%252C%2520techniques%2520like%2520random%2520document%2520concatenation%250A%2528Standard%2529%2520and%2520similarity-based%2520methods%2520%2528KNN%252C%2520ICLM%2529%2520have%2520been%2520developed.%250AHowever%252C%2520they%2520either%2520sacrifice%2520semantic%2520coherence%2520or%2520diversity.%2520To%2520balance%2520both%250Aaspects%252C%2520we%2520introduce%2520Quest%252C%2520a%2520query-centric%2520data%2520synthesis%2520method%2520aggregating%250Asemantically%2520relevant%2520yet%2520diverse%2520documents.%2520Quest%2520uses%2520a%2520generative%2520model%2520to%250Apredict%2520potential%2520queries%2520for%2520each%2520document%252C%2520grouping%2520documents%2520with%2520similar%250Aqueries%2520and%2520keywords.%2520Extensive%2520experiments%2520demonstrate%2520Quest%2527s%2520superior%250Aperformance%2520on%2520long-context%2520tasks%252C%2520achieving%2520remarkable%2520results%2520with%2520context%250Alengths%2520of%2520up%2520to%25201M%2520tokens%2520and%2520confirming%2520its%2520scalability%2520across%2520various%2520model%250Asizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19846v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quest%3A%20Query-centric%20Data%20Synthesis%20Approach%20for%20Long-context%20Scaling%20of%0A%20%20Large%20Language%20Model&entry.906535625=Chaochen%20Gao%20and%20Xing%20Wu%20and%20Qi%20Fu%20and%20Songlin%20Hu&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20highlighted%20the%0Aimportance%20of%20extending%20context%20lengths%20for%20handling%20complex%20tasks.%20While%0Atraditional%20methods%20for%20training%20on%20long%20contexts%20often%20use%20filtered%20long%0Adocuments%2C%20these%20approaches%20lead%20to%20domain%20imbalances%2C%20limiting%20model%0Aperformance.%20To%20address%20this%2C%20techniques%20like%20random%20document%20concatenation%0A%28Standard%29%20and%20similarity-based%20methods%20%28KNN%2C%20ICLM%29%20have%20been%20developed.%0AHowever%2C%20they%20either%20sacrifice%20semantic%20coherence%20or%20diversity.%20To%20balance%20both%0Aaspects%2C%20we%20introduce%20Quest%2C%20a%20query-centric%20data%20synthesis%20method%20aggregating%0Asemantically%20relevant%20yet%20diverse%20documents.%20Quest%20uses%20a%20generative%20model%20to%0Apredict%20potential%20queries%20for%20each%20document%2C%20grouping%20documents%20with%20similar%0Aqueries%20and%20keywords.%20Extensive%20experiments%20demonstrate%20Quest%27s%20superior%0Aperformance%20on%20long-context%20tasks%2C%20achieving%20remarkable%20results%20with%20context%0Alengths%20of%20up%20to%201M%20tokens%20and%20confirming%20its%20scalability%20across%20various%20model%0Asizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19846v5&entry.124074799=Read"},
{"title": "Gridded Transformer Neural Processes for Large Unstructured\n  Spatio-Temporal Data", "author": "Matthew Ashman and Cristiana Diaconu and Eric Langezaal and Adrian Weller and Richard E. Turner", "abstract": "  Many important problems require modelling large-scale spatio-temporal\ndatasets, with one prevalent example being weather forecasting. Recently,\ntransformer-based approaches have shown great promise in a range of weather\nforecasting problems. However, these have mostly focused on gridded data\nsources, neglecting the wealth of unstructured, off-the-grid data from\nobservational measurements such as those at weather stations. A promising\nfamily of models suitable for such tasks are neural processes (NPs), notably\nthe family of transformer neural processes (TNPs). Although TNPs have shown\npromise on small spatio-temporal datasets, they are unable to scale to the\nquantities of data used by state-of-the-art weather and climate models. This\nlimitation stems from their lack of efficient attention mechanisms. We address\nthis shortcoming through the introduction of gridded pseudo-token TNPs which\nemploy specialised encoders and decoders to handle unstructured observations\nand utilise a processor containing gridded pseudo-tokens that leverage\nefficient attention mechanisms. Our method consistently outperforms a range of\nstrong baselines on various synthetic and real-world regression tasks involving\nlarge-scale data, while maintaining competitive computational efficiency. The\nreal-life experiments are performed on weather data, demonstrating the\npotential of our approach to bring performance and computational benefits when\napplied at scale in a weather modelling pipeline.\n", "link": "http://arxiv.org/abs/2410.06731v1", "date": "2024-10-09", "relevancy": 2.1244, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5885}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5225}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gridded%20Transformer%20Neural%20Processes%20for%20Large%20Unstructured%0A%20%20Spatio-Temporal%20Data&body=Title%3A%20Gridded%20Transformer%20Neural%20Processes%20for%20Large%20Unstructured%0A%20%20Spatio-Temporal%20Data%0AAuthor%3A%20Matthew%20Ashman%20and%20Cristiana%20Diaconu%20and%20Eric%20Langezaal%20and%20Adrian%20Weller%20and%20Richard%20E.%20Turner%0AAbstract%3A%20%20%20Many%20important%20problems%20require%20modelling%20large-scale%20spatio-temporal%0Adatasets%2C%20with%20one%20prevalent%20example%20being%20weather%20forecasting.%20Recently%2C%0Atransformer-based%20approaches%20have%20shown%20great%20promise%20in%20a%20range%20of%20weather%0Aforecasting%20problems.%20However%2C%20these%20have%20mostly%20focused%20on%20gridded%20data%0Asources%2C%20neglecting%20the%20wealth%20of%20unstructured%2C%20off-the-grid%20data%20from%0Aobservational%20measurements%20such%20as%20those%20at%20weather%20stations.%20A%20promising%0Afamily%20of%20models%20suitable%20for%20such%20tasks%20are%20neural%20processes%20%28NPs%29%2C%20notably%0Athe%20family%20of%20transformer%20neural%20processes%20%28TNPs%29.%20Although%20TNPs%20have%20shown%0Apromise%20on%20small%20spatio-temporal%20datasets%2C%20they%20are%20unable%20to%20scale%20to%20the%0Aquantities%20of%20data%20used%20by%20state-of-the-art%20weather%20and%20climate%20models.%20This%0Alimitation%20stems%20from%20their%20lack%20of%20efficient%20attention%20mechanisms.%20We%20address%0Athis%20shortcoming%20through%20the%20introduction%20of%20gridded%20pseudo-token%20TNPs%20which%0Aemploy%20specialised%20encoders%20and%20decoders%20to%20handle%20unstructured%20observations%0Aand%20utilise%20a%20processor%20containing%20gridded%20pseudo-tokens%20that%20leverage%0Aefficient%20attention%20mechanisms.%20Our%20method%20consistently%20outperforms%20a%20range%20of%0Astrong%20baselines%20on%20various%20synthetic%20and%20real-world%20regression%20tasks%20involving%0Alarge-scale%20data%2C%20while%20maintaining%20competitive%20computational%20efficiency.%20The%0Areal-life%20experiments%20are%20performed%20on%20weather%20data%2C%20demonstrating%20the%0Apotential%20of%20our%20approach%20to%20bring%20performance%20and%20computational%20benefits%20when%0Aapplied%20at%20scale%20in%20a%20weather%20modelling%20pipeline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGridded%2520Transformer%2520Neural%2520Processes%2520for%2520Large%2520Unstructured%250A%2520%2520Spatio-Temporal%2520Data%26entry.906535625%3DMatthew%2520Ashman%2520and%2520Cristiana%2520Diaconu%2520and%2520Eric%2520Langezaal%2520and%2520Adrian%2520Weller%2520and%2520Richard%2520E.%2520Turner%26entry.1292438233%3D%2520%2520Many%2520important%2520problems%2520require%2520modelling%2520large-scale%2520spatio-temporal%250Adatasets%252C%2520with%2520one%2520prevalent%2520example%2520being%2520weather%2520forecasting.%2520Recently%252C%250Atransformer-based%2520approaches%2520have%2520shown%2520great%2520promise%2520in%2520a%2520range%2520of%2520weather%250Aforecasting%2520problems.%2520However%252C%2520these%2520have%2520mostly%2520focused%2520on%2520gridded%2520data%250Asources%252C%2520neglecting%2520the%2520wealth%2520of%2520unstructured%252C%2520off-the-grid%2520data%2520from%250Aobservational%2520measurements%2520such%2520as%2520those%2520at%2520weather%2520stations.%2520A%2520promising%250Afamily%2520of%2520models%2520suitable%2520for%2520such%2520tasks%2520are%2520neural%2520processes%2520%2528NPs%2529%252C%2520notably%250Athe%2520family%2520of%2520transformer%2520neural%2520processes%2520%2528TNPs%2529.%2520Although%2520TNPs%2520have%2520shown%250Apromise%2520on%2520small%2520spatio-temporal%2520datasets%252C%2520they%2520are%2520unable%2520to%2520scale%2520to%2520the%250Aquantities%2520of%2520data%2520used%2520by%2520state-of-the-art%2520weather%2520and%2520climate%2520models.%2520This%250Alimitation%2520stems%2520from%2520their%2520lack%2520of%2520efficient%2520attention%2520mechanisms.%2520We%2520address%250Athis%2520shortcoming%2520through%2520the%2520introduction%2520of%2520gridded%2520pseudo-token%2520TNPs%2520which%250Aemploy%2520specialised%2520encoders%2520and%2520decoders%2520to%2520handle%2520unstructured%2520observations%250Aand%2520utilise%2520a%2520processor%2520containing%2520gridded%2520pseudo-tokens%2520that%2520leverage%250Aefficient%2520attention%2520mechanisms.%2520Our%2520method%2520consistently%2520outperforms%2520a%2520range%2520of%250Astrong%2520baselines%2520on%2520various%2520synthetic%2520and%2520real-world%2520regression%2520tasks%2520involving%250Alarge-scale%2520data%252C%2520while%2520maintaining%2520competitive%2520computational%2520efficiency.%2520The%250Areal-life%2520experiments%2520are%2520performed%2520on%2520weather%2520data%252C%2520demonstrating%2520the%250Apotential%2520of%2520our%2520approach%2520to%2520bring%2520performance%2520and%2520computational%2520benefits%2520when%250Aapplied%2520at%2520scale%2520in%2520a%2520weather%2520modelling%2520pipeline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gridded%20Transformer%20Neural%20Processes%20for%20Large%20Unstructured%0A%20%20Spatio-Temporal%20Data&entry.906535625=Matthew%20Ashman%20and%20Cristiana%20Diaconu%20and%20Eric%20Langezaal%20and%20Adrian%20Weller%20and%20Richard%20E.%20Turner&entry.1292438233=%20%20Many%20important%20problems%20require%20modelling%20large-scale%20spatio-temporal%0Adatasets%2C%20with%20one%20prevalent%20example%20being%20weather%20forecasting.%20Recently%2C%0Atransformer-based%20approaches%20have%20shown%20great%20promise%20in%20a%20range%20of%20weather%0Aforecasting%20problems.%20However%2C%20these%20have%20mostly%20focused%20on%20gridded%20data%0Asources%2C%20neglecting%20the%20wealth%20of%20unstructured%2C%20off-the-grid%20data%20from%0Aobservational%20measurements%20such%20as%20those%20at%20weather%20stations.%20A%20promising%0Afamily%20of%20models%20suitable%20for%20such%20tasks%20are%20neural%20processes%20%28NPs%29%2C%20notably%0Athe%20family%20of%20transformer%20neural%20processes%20%28TNPs%29.%20Although%20TNPs%20have%20shown%0Apromise%20on%20small%20spatio-temporal%20datasets%2C%20they%20are%20unable%20to%20scale%20to%20the%0Aquantities%20of%20data%20used%20by%20state-of-the-art%20weather%20and%20climate%20models.%20This%0Alimitation%20stems%20from%20their%20lack%20of%20efficient%20attention%20mechanisms.%20We%20address%0Athis%20shortcoming%20through%20the%20introduction%20of%20gridded%20pseudo-token%20TNPs%20which%0Aemploy%20specialised%20encoders%20and%20decoders%20to%20handle%20unstructured%20observations%0Aand%20utilise%20a%20processor%20containing%20gridded%20pseudo-tokens%20that%20leverage%0Aefficient%20attention%20mechanisms.%20Our%20method%20consistently%20outperforms%20a%20range%20of%0Astrong%20baselines%20on%20various%20synthetic%20and%20real-world%20regression%20tasks%20involving%0Alarge-scale%20data%2C%20while%20maintaining%20competitive%20computational%20efficiency.%20The%0Areal-life%20experiments%20are%20performed%20on%20weather%20data%2C%20demonstrating%20the%0Apotential%20of%20our%20approach%20to%20bring%20performance%20and%20computational%20benefits%20when%0Aapplied%20at%20scale%20in%20a%20weather%20modelling%20pipeline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06731v1&entry.124074799=Read"},
{"title": "AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed\n  and Low Tolerance", "author": "Joao P. C. Bertoldo and Dick Ameln and Ashwin Vaidya and Samet Ak\u00e7ay", "abstract": "  Recent advances in visual anomaly detection research have seen AUROC and\nAUPRO scores on public benchmark datasets such as MVTec and VisA converge\ntowards perfect recall, giving the impression that these benchmarks are\nnear-solved. However, high AUROC and AUPRO scores do not always reflect\nqualitative performance, which limits the validity of these metrics in\nreal-world applications. We argue that the artificial ceiling imposed by the\nlack of an adequate evaluation metric restrains progression of the field, and\nit is crucial that we revisit the evaluation metrics used to rate our\nalgorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric\nthat addresses the shortcomings of AUROC and AUPRO. PIMO retains the\nrecall-based nature of the existing metrics but introduces two distinctions:\nthe assignment of curves (and respective area under the curve) is per-image,\nand its X-axis relies solely on normal images. Measuring recall per image\nsimplifies instance score indexing and is more robust to noisy annotations. As\nwe show, it also accelerates computation and enables the usage of statistical\ntests to compare models. By imposing low tolerance for false positives on\nnormal images, PIMO provides an enhanced model validation procedure and\nhighlights performance variations across datasets. Our experiments demonstrate\nthat PIMO offers practical advantages and nuanced performance insights that\nredefine anomaly detection benchmarks -- notably challenging the perception\nthat MVTec AD and VisA datasets have been solved by contemporary models.\nAvailable on GitHub: https://github.com/jpcbertoldo/aupimo.\n", "link": "http://arxiv.org/abs/2401.01984v4", "date": "2024-10-09", "relevancy": 2.1172, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5563}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5345}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AUPIMO%3A%20Redefining%20Visual%20Anomaly%20Detection%20Benchmarks%20with%20High%20Speed%0A%20%20and%20Low%20Tolerance&body=Title%3A%20AUPIMO%3A%20Redefining%20Visual%20Anomaly%20Detection%20Benchmarks%20with%20High%20Speed%0A%20%20and%20Low%20Tolerance%0AAuthor%3A%20Joao%20P.%20C.%20Bertoldo%20and%20Dick%20Ameln%20and%20Ashwin%20Vaidya%20and%20Samet%20Ak%C3%A7ay%0AAbstract%3A%20%20%20Recent%20advances%20in%20visual%20anomaly%20detection%20research%20have%20seen%20AUROC%20and%0AAUPRO%20scores%20on%20public%20benchmark%20datasets%20such%20as%20MVTec%20and%20VisA%20converge%0Atowards%20perfect%20recall%2C%20giving%20the%20impression%20that%20these%20benchmarks%20are%0Anear-solved.%20However%2C%20high%20AUROC%20and%20AUPRO%20scores%20do%20not%20always%20reflect%0Aqualitative%20performance%2C%20which%20limits%20the%20validity%20of%20these%20metrics%20in%0Areal-world%20applications.%20We%20argue%20that%20the%20artificial%20ceiling%20imposed%20by%20the%0Alack%20of%20an%20adequate%20evaluation%20metric%20restrains%20progression%20of%20the%20field%2C%20and%0Ait%20is%20crucial%20that%20we%20revisit%20the%20evaluation%20metrics%20used%20to%20rate%20our%0Aalgorithms.%20In%20response%2C%20we%20introduce%20Per-IMage%20Overlap%20%28PIMO%29%2C%20a%20novel%20metric%0Athat%20addresses%20the%20shortcomings%20of%20AUROC%20and%20AUPRO.%20PIMO%20retains%20the%0Arecall-based%20nature%20of%20the%20existing%20metrics%20but%20introduces%20two%20distinctions%3A%0Athe%20assignment%20of%20curves%20%28and%20respective%20area%20under%20the%20curve%29%20is%20per-image%2C%0Aand%20its%20X-axis%20relies%20solely%20on%20normal%20images.%20Measuring%20recall%20per%20image%0Asimplifies%20instance%20score%20indexing%20and%20is%20more%20robust%20to%20noisy%20annotations.%20As%0Awe%20show%2C%20it%20also%20accelerates%20computation%20and%20enables%20the%20usage%20of%20statistical%0Atests%20to%20compare%20models.%20By%20imposing%20low%20tolerance%20for%20false%20positives%20on%0Anormal%20images%2C%20PIMO%20provides%20an%20enhanced%20model%20validation%20procedure%20and%0Ahighlights%20performance%20variations%20across%20datasets.%20Our%20experiments%20demonstrate%0Athat%20PIMO%20offers%20practical%20advantages%20and%20nuanced%20performance%20insights%20that%0Aredefine%20anomaly%20detection%20benchmarks%20--%20notably%20challenging%20the%20perception%0Athat%20MVTec%20AD%20and%20VisA%20datasets%20have%20been%20solved%20by%20contemporary%20models.%0AAvailable%20on%20GitHub%3A%20https%3A//github.com/jpcbertoldo/aupimo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01984v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAUPIMO%253A%2520Redefining%2520Visual%2520Anomaly%2520Detection%2520Benchmarks%2520with%2520High%2520Speed%250A%2520%2520and%2520Low%2520Tolerance%26entry.906535625%3DJoao%2520P.%2520C.%2520Bertoldo%2520and%2520Dick%2520Ameln%2520and%2520Ashwin%2520Vaidya%2520and%2520Samet%2520Ak%25C3%25A7ay%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520visual%2520anomaly%2520detection%2520research%2520have%2520seen%2520AUROC%2520and%250AAUPRO%2520scores%2520on%2520public%2520benchmark%2520datasets%2520such%2520as%2520MVTec%2520and%2520VisA%2520converge%250Atowards%2520perfect%2520recall%252C%2520giving%2520the%2520impression%2520that%2520these%2520benchmarks%2520are%250Anear-solved.%2520However%252C%2520high%2520AUROC%2520and%2520AUPRO%2520scores%2520do%2520not%2520always%2520reflect%250Aqualitative%2520performance%252C%2520which%2520limits%2520the%2520validity%2520of%2520these%2520metrics%2520in%250Areal-world%2520applications.%2520We%2520argue%2520that%2520the%2520artificial%2520ceiling%2520imposed%2520by%2520the%250Alack%2520of%2520an%2520adequate%2520evaluation%2520metric%2520restrains%2520progression%2520of%2520the%2520field%252C%2520and%250Ait%2520is%2520crucial%2520that%2520we%2520revisit%2520the%2520evaluation%2520metrics%2520used%2520to%2520rate%2520our%250Aalgorithms.%2520In%2520response%252C%2520we%2520introduce%2520Per-IMage%2520Overlap%2520%2528PIMO%2529%252C%2520a%2520novel%2520metric%250Athat%2520addresses%2520the%2520shortcomings%2520of%2520AUROC%2520and%2520AUPRO.%2520PIMO%2520retains%2520the%250Arecall-based%2520nature%2520of%2520the%2520existing%2520metrics%2520but%2520introduces%2520two%2520distinctions%253A%250Athe%2520assignment%2520of%2520curves%2520%2528and%2520respective%2520area%2520under%2520the%2520curve%2529%2520is%2520per-image%252C%250Aand%2520its%2520X-axis%2520relies%2520solely%2520on%2520normal%2520images.%2520Measuring%2520recall%2520per%2520image%250Asimplifies%2520instance%2520score%2520indexing%2520and%2520is%2520more%2520robust%2520to%2520noisy%2520annotations.%2520As%250Awe%2520show%252C%2520it%2520also%2520accelerates%2520computation%2520and%2520enables%2520the%2520usage%2520of%2520statistical%250Atests%2520to%2520compare%2520models.%2520By%2520imposing%2520low%2520tolerance%2520for%2520false%2520positives%2520on%250Anormal%2520images%252C%2520PIMO%2520provides%2520an%2520enhanced%2520model%2520validation%2520procedure%2520and%250Ahighlights%2520performance%2520variations%2520across%2520datasets.%2520Our%2520experiments%2520demonstrate%250Athat%2520PIMO%2520offers%2520practical%2520advantages%2520and%2520nuanced%2520performance%2520insights%2520that%250Aredefine%2520anomaly%2520detection%2520benchmarks%2520--%2520notably%2520challenging%2520the%2520perception%250Athat%2520MVTec%2520AD%2520and%2520VisA%2520datasets%2520have%2520been%2520solved%2520by%2520contemporary%2520models.%250AAvailable%2520on%2520GitHub%253A%2520https%253A//github.com/jpcbertoldo/aupimo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01984v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AUPIMO%3A%20Redefining%20Visual%20Anomaly%20Detection%20Benchmarks%20with%20High%20Speed%0A%20%20and%20Low%20Tolerance&entry.906535625=Joao%20P.%20C.%20Bertoldo%20and%20Dick%20Ameln%20and%20Ashwin%20Vaidya%20and%20Samet%20Ak%C3%A7ay&entry.1292438233=%20%20Recent%20advances%20in%20visual%20anomaly%20detection%20research%20have%20seen%20AUROC%20and%0AAUPRO%20scores%20on%20public%20benchmark%20datasets%20such%20as%20MVTec%20and%20VisA%20converge%0Atowards%20perfect%20recall%2C%20giving%20the%20impression%20that%20these%20benchmarks%20are%0Anear-solved.%20However%2C%20high%20AUROC%20and%20AUPRO%20scores%20do%20not%20always%20reflect%0Aqualitative%20performance%2C%20which%20limits%20the%20validity%20of%20these%20metrics%20in%0Areal-world%20applications.%20We%20argue%20that%20the%20artificial%20ceiling%20imposed%20by%20the%0Alack%20of%20an%20adequate%20evaluation%20metric%20restrains%20progression%20of%20the%20field%2C%20and%0Ait%20is%20crucial%20that%20we%20revisit%20the%20evaluation%20metrics%20used%20to%20rate%20our%0Aalgorithms.%20In%20response%2C%20we%20introduce%20Per-IMage%20Overlap%20%28PIMO%29%2C%20a%20novel%20metric%0Athat%20addresses%20the%20shortcomings%20of%20AUROC%20and%20AUPRO.%20PIMO%20retains%20the%0Arecall-based%20nature%20of%20the%20existing%20metrics%20but%20introduces%20two%20distinctions%3A%0Athe%20assignment%20of%20curves%20%28and%20respective%20area%20under%20the%20curve%29%20is%20per-image%2C%0Aand%20its%20X-axis%20relies%20solely%20on%20normal%20images.%20Measuring%20recall%20per%20image%0Asimplifies%20instance%20score%20indexing%20and%20is%20more%20robust%20to%20noisy%20annotations.%20As%0Awe%20show%2C%20it%20also%20accelerates%20computation%20and%20enables%20the%20usage%20of%20statistical%0Atests%20to%20compare%20models.%20By%20imposing%20low%20tolerance%20for%20false%20positives%20on%0Anormal%20images%2C%20PIMO%20provides%20an%20enhanced%20model%20validation%20procedure%20and%0Ahighlights%20performance%20variations%20across%20datasets.%20Our%20experiments%20demonstrate%0Athat%20PIMO%20offers%20practical%20advantages%20and%20nuanced%20performance%20insights%20that%0Aredefine%20anomaly%20detection%20benchmarks%20--%20notably%20challenging%20the%20perception%0Athat%20MVTec%20AD%20and%20VisA%20datasets%20have%20been%20solved%20by%20contemporary%20models.%0AAvailable%20on%20GitHub%3A%20https%3A//github.com/jpcbertoldo/aupimo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01984v4&entry.124074799=Read"},
{"title": "Bridge the Points: Graph-based Few-shot Segment Anything Semantically", "author": "Anqi Zhang and Guangyu Gao and Jianbo Jiao and Chi Harold Liu and Yunchao Wei", "abstract": "  The recent advancements in large-scale pre-training techniques have\nsignificantly enhanced the capabilities of vision foundation models, notably\nthe Segment Anything Model (SAM), which can generate precise masks based on\npoint and box prompts. Recent studies extend SAM to Few-shot Semantic\nSegmentation (FSS), focusing on prompt generation for SAM-based automatic\nsemantic segmentation. However, these methods struggle with selecting suitable\nprompts, require specific hyperparameter settings for different scenarios, and\nexperience prolonged one-shot inference times due to the overuse of SAM,\nresulting in low efficiency and limited automation ability. To address these\nissues, we propose a simple yet effective approach based on graph analysis. In\nparticular, a Positive-Negative Alignment module dynamically selects the point\nprompts for generating masks, especially uncovering the potential of the\nbackground context as the negative reference. Another subsequent Point-Mask\nClustering module aligns the granularity of masks and selected points as a\ndirected graph, based on mask coverage over points. These points are then\naggregated by decomposing the weakly connected components of the directed graph\nin an efficient manner, constructing distinct natural clusters. Finally, the\npositive and overshooting gating, benefiting from graph-based granularity\nalignment, aggregate high-confident masks and filter out the false-positive\nmasks for final prediction, reducing the usage of additional hyperparameters\nand redundant mask generation. Extensive experimental analysis across standard\nFSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the\neffectiveness and efficiency of the proposed approach, surpassing\nstate-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2%\non LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/.\n", "link": "http://arxiv.org/abs/2410.06964v1", "date": "2024-10-09", "relevancy": 2.1136, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5435}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridge%20the%20Points%3A%20Graph-based%20Few-shot%20Segment%20Anything%20Semantically&body=Title%3A%20Bridge%20the%20Points%3A%20Graph-based%20Few-shot%20Segment%20Anything%20Semantically%0AAuthor%3A%20Anqi%20Zhang%20and%20Guangyu%20Gao%20and%20Jianbo%20Jiao%20and%20Chi%20Harold%20Liu%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20large-scale%20pre-training%20techniques%20have%0Asignificantly%20enhanced%20the%20capabilities%20of%20vision%20foundation%20models%2C%20notably%0Athe%20Segment%20Anything%20Model%20%28SAM%29%2C%20which%20can%20generate%20precise%20masks%20based%20on%0Apoint%20and%20box%20prompts.%20Recent%20studies%20extend%20SAM%20to%20Few-shot%20Semantic%0ASegmentation%20%28FSS%29%2C%20focusing%20on%20prompt%20generation%20for%20SAM-based%20automatic%0Asemantic%20segmentation.%20However%2C%20these%20methods%20struggle%20with%20selecting%20suitable%0Aprompts%2C%20require%20specific%20hyperparameter%20settings%20for%20different%20scenarios%2C%20and%0Aexperience%20prolonged%20one-shot%20inference%20times%20due%20to%20the%20overuse%20of%20SAM%2C%0Aresulting%20in%20low%20efficiency%20and%20limited%20automation%20ability.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20simple%20yet%20effective%20approach%20based%20on%20graph%20analysis.%20In%0Aparticular%2C%20a%20Positive-Negative%20Alignment%20module%20dynamically%20selects%20the%20point%0Aprompts%20for%20generating%20masks%2C%20especially%20uncovering%20the%20potential%20of%20the%0Abackground%20context%20as%20the%20negative%20reference.%20Another%20subsequent%20Point-Mask%0AClustering%20module%20aligns%20the%20granularity%20of%20masks%20and%20selected%20points%20as%20a%0Adirected%20graph%2C%20based%20on%20mask%20coverage%20over%20points.%20These%20points%20are%20then%0Aaggregated%20by%20decomposing%20the%20weakly%20connected%20components%20of%20the%20directed%20graph%0Ain%20an%20efficient%20manner%2C%20constructing%20distinct%20natural%20clusters.%20Finally%2C%20the%0Apositive%20and%20overshooting%20gating%2C%20benefiting%20from%20graph-based%20granularity%0Aalignment%2C%20aggregate%20high-confident%20masks%20and%20filter%20out%20the%20false-positive%0Amasks%20for%20final%20prediction%2C%20reducing%20the%20usage%20of%20additional%20hyperparameters%0Aand%20redundant%20mask%20generation.%20Extensive%20experimental%20analysis%20across%20standard%0AFSS%2C%20One-shot%20Part%20Segmentation%2C%20and%20Cross%20Domain%20FSS%20datasets%20validate%20the%0Aeffectiveness%20and%20efficiency%20of%20the%20proposed%20approach%2C%20surpassing%0Astate-of-the-art%20generalist%20models%20with%20a%20mIoU%20of%2058.7%25%20on%20COCO-20i%20and%2035.2%25%0Aon%20LVIS-92i.%20The%20code%20is%20available%20in%20https%3A//andyzaq.github.io/GF-SAM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridge%2520the%2520Points%253A%2520Graph-based%2520Few-shot%2520Segment%2520Anything%2520Semantically%26entry.906535625%3DAnqi%2520Zhang%2520and%2520Guangyu%2520Gao%2520and%2520Jianbo%2520Jiao%2520and%2520Chi%2520Harold%2520Liu%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520large-scale%2520pre-training%2520techniques%2520have%250Asignificantly%2520enhanced%2520the%2520capabilities%2520of%2520vision%2520foundation%2520models%252C%2520notably%250Athe%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520which%2520can%2520generate%2520precise%2520masks%2520based%2520on%250Apoint%2520and%2520box%2520prompts.%2520Recent%2520studies%2520extend%2520SAM%2520to%2520Few-shot%2520Semantic%250ASegmentation%2520%2528FSS%2529%252C%2520focusing%2520on%2520prompt%2520generation%2520for%2520SAM-based%2520automatic%250Asemantic%2520segmentation.%2520However%252C%2520these%2520methods%2520struggle%2520with%2520selecting%2520suitable%250Aprompts%252C%2520require%2520specific%2520hyperparameter%2520settings%2520for%2520different%2520scenarios%252C%2520and%250Aexperience%2520prolonged%2520one-shot%2520inference%2520times%2520due%2520to%2520the%2520overuse%2520of%2520SAM%252C%250Aresulting%2520in%2520low%2520efficiency%2520and%2520limited%2520automation%2520ability.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520approach%2520based%2520on%2520graph%2520analysis.%2520In%250Aparticular%252C%2520a%2520Positive-Negative%2520Alignment%2520module%2520dynamically%2520selects%2520the%2520point%250Aprompts%2520for%2520generating%2520masks%252C%2520especially%2520uncovering%2520the%2520potential%2520of%2520the%250Abackground%2520context%2520as%2520the%2520negative%2520reference.%2520Another%2520subsequent%2520Point-Mask%250AClustering%2520module%2520aligns%2520the%2520granularity%2520of%2520masks%2520and%2520selected%2520points%2520as%2520a%250Adirected%2520graph%252C%2520based%2520on%2520mask%2520coverage%2520over%2520points.%2520These%2520points%2520are%2520then%250Aaggregated%2520by%2520decomposing%2520the%2520weakly%2520connected%2520components%2520of%2520the%2520directed%2520graph%250Ain%2520an%2520efficient%2520manner%252C%2520constructing%2520distinct%2520natural%2520clusters.%2520Finally%252C%2520the%250Apositive%2520and%2520overshooting%2520gating%252C%2520benefiting%2520from%2520graph-based%2520granularity%250Aalignment%252C%2520aggregate%2520high-confident%2520masks%2520and%2520filter%2520out%2520the%2520false-positive%250Amasks%2520for%2520final%2520prediction%252C%2520reducing%2520the%2520usage%2520of%2520additional%2520hyperparameters%250Aand%2520redundant%2520mask%2520generation.%2520Extensive%2520experimental%2520analysis%2520across%2520standard%250AFSS%252C%2520One-shot%2520Part%2520Segmentation%252C%2520and%2520Cross%2520Domain%2520FSS%2520datasets%2520validate%2520the%250Aeffectiveness%2520and%2520efficiency%2520of%2520the%2520proposed%2520approach%252C%2520surpassing%250Astate-of-the-art%2520generalist%2520models%2520with%2520a%2520mIoU%2520of%252058.7%2525%2520on%2520COCO-20i%2520and%252035.2%2525%250Aon%2520LVIS-92i.%2520The%2520code%2520is%2520available%2520in%2520https%253A//andyzaq.github.io/GF-SAM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridge%20the%20Points%3A%20Graph-based%20Few-shot%20Segment%20Anything%20Semantically&entry.906535625=Anqi%20Zhang%20and%20Guangyu%20Gao%20and%20Jianbo%20Jiao%20and%20Chi%20Harold%20Liu%20and%20Yunchao%20Wei&entry.1292438233=%20%20The%20recent%20advancements%20in%20large-scale%20pre-training%20techniques%20have%0Asignificantly%20enhanced%20the%20capabilities%20of%20vision%20foundation%20models%2C%20notably%0Athe%20Segment%20Anything%20Model%20%28SAM%29%2C%20which%20can%20generate%20precise%20masks%20based%20on%0Apoint%20and%20box%20prompts.%20Recent%20studies%20extend%20SAM%20to%20Few-shot%20Semantic%0ASegmentation%20%28FSS%29%2C%20focusing%20on%20prompt%20generation%20for%20SAM-based%20automatic%0Asemantic%20segmentation.%20However%2C%20these%20methods%20struggle%20with%20selecting%20suitable%0Aprompts%2C%20require%20specific%20hyperparameter%20settings%20for%20different%20scenarios%2C%20and%0Aexperience%20prolonged%20one-shot%20inference%20times%20due%20to%20the%20overuse%20of%20SAM%2C%0Aresulting%20in%20low%20efficiency%20and%20limited%20automation%20ability.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20simple%20yet%20effective%20approach%20based%20on%20graph%20analysis.%20In%0Aparticular%2C%20a%20Positive-Negative%20Alignment%20module%20dynamically%20selects%20the%20point%0Aprompts%20for%20generating%20masks%2C%20especially%20uncovering%20the%20potential%20of%20the%0Abackground%20context%20as%20the%20negative%20reference.%20Another%20subsequent%20Point-Mask%0AClustering%20module%20aligns%20the%20granularity%20of%20masks%20and%20selected%20points%20as%20a%0Adirected%20graph%2C%20based%20on%20mask%20coverage%20over%20points.%20These%20points%20are%20then%0Aaggregated%20by%20decomposing%20the%20weakly%20connected%20components%20of%20the%20directed%20graph%0Ain%20an%20efficient%20manner%2C%20constructing%20distinct%20natural%20clusters.%20Finally%2C%20the%0Apositive%20and%20overshooting%20gating%2C%20benefiting%20from%20graph-based%20granularity%0Aalignment%2C%20aggregate%20high-confident%20masks%20and%20filter%20out%20the%20false-positive%0Amasks%20for%20final%20prediction%2C%20reducing%20the%20usage%20of%20additional%20hyperparameters%0Aand%20redundant%20mask%20generation.%20Extensive%20experimental%20analysis%20across%20standard%0AFSS%2C%20One-shot%20Part%20Segmentation%2C%20and%20Cross%20Domain%20FSS%20datasets%20validate%20the%0Aeffectiveness%20and%20efficiency%20of%20the%20proposed%20approach%2C%20surpassing%0Astate-of-the-art%20generalist%20models%20with%20a%20mIoU%20of%2058.7%25%20on%20COCO-20i%20and%2035.2%25%0Aon%20LVIS-92i.%20The%20code%20is%20available%20in%20https%3A//andyzaq.github.io/GF-SAM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06964v1&entry.124074799=Read"},
{"title": "InAttention: Linear Context Scaling for Transformers", "author": "Joseph Eisner", "abstract": "  VRAM requirements for transformer models scale quadratically with context\nlength due to the self-attention mechanism. In this paper we modify the\ndecoder-only transformer, replacing self-attention with InAttention, which\nscales linearly with context length during inference by having tokens attend\nonly to initial states. Benchmarking shows that InAttention significantly\nreduces VRAM usage during inference, enabling handling of long sequences on\nconsumer GPUs. We corroborate that fine-tuning extends context length\nefficiently, improving performance on long sequences without high training\ncosts. InAttention offers a scalable solution for long-range dependencies in\ntransformer models, paving the way for further optimization.\n", "link": "http://arxiv.org/abs/2410.07063v1", "date": "2024-10-09", "relevancy": 2.1024, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5792}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InAttention%3A%20Linear%20Context%20Scaling%20for%20Transformers&body=Title%3A%20InAttention%3A%20Linear%20Context%20Scaling%20for%20Transformers%0AAuthor%3A%20Joseph%20Eisner%0AAbstract%3A%20%20%20VRAM%20requirements%20for%20transformer%20models%20scale%20quadratically%20with%20context%0Alength%20due%20to%20the%20self-attention%20mechanism.%20In%20this%20paper%20we%20modify%20the%0Adecoder-only%20transformer%2C%20replacing%20self-attention%20with%20InAttention%2C%20which%0Ascales%20linearly%20with%20context%20length%20during%20inference%20by%20having%20tokens%20attend%0Aonly%20to%20initial%20states.%20Benchmarking%20shows%20that%20InAttention%20significantly%0Areduces%20VRAM%20usage%20during%20inference%2C%20enabling%20handling%20of%20long%20sequences%20on%0Aconsumer%20GPUs.%20We%20corroborate%20that%20fine-tuning%20extends%20context%20length%0Aefficiently%2C%20improving%20performance%20on%20long%20sequences%20without%20high%20training%0Acosts.%20InAttention%20offers%20a%20scalable%20solution%20for%20long-range%20dependencies%20in%0Atransformer%20models%2C%20paving%20the%20way%20for%20further%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInAttention%253A%2520Linear%2520Context%2520Scaling%2520for%2520Transformers%26entry.906535625%3DJoseph%2520Eisner%26entry.1292438233%3D%2520%2520VRAM%2520requirements%2520for%2520transformer%2520models%2520scale%2520quadratically%2520with%2520context%250Alength%2520due%2520to%2520the%2520self-attention%2520mechanism.%2520In%2520this%2520paper%2520we%2520modify%2520the%250Adecoder-only%2520transformer%252C%2520replacing%2520self-attention%2520with%2520InAttention%252C%2520which%250Ascales%2520linearly%2520with%2520context%2520length%2520during%2520inference%2520by%2520having%2520tokens%2520attend%250Aonly%2520to%2520initial%2520states.%2520Benchmarking%2520shows%2520that%2520InAttention%2520significantly%250Areduces%2520VRAM%2520usage%2520during%2520inference%252C%2520enabling%2520handling%2520of%2520long%2520sequences%2520on%250Aconsumer%2520GPUs.%2520We%2520corroborate%2520that%2520fine-tuning%2520extends%2520context%2520length%250Aefficiently%252C%2520improving%2520performance%2520on%2520long%2520sequences%2520without%2520high%2520training%250Acosts.%2520InAttention%2520offers%2520a%2520scalable%2520solution%2520for%2520long-range%2520dependencies%2520in%250Atransformer%2520models%252C%2520paving%2520the%2520way%2520for%2520further%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InAttention%3A%20Linear%20Context%20Scaling%20for%20Transformers&entry.906535625=Joseph%20Eisner&entry.1292438233=%20%20VRAM%20requirements%20for%20transformer%20models%20scale%20quadratically%20with%20context%0Alength%20due%20to%20the%20self-attention%20mechanism.%20In%20this%20paper%20we%20modify%20the%0Adecoder-only%20transformer%2C%20replacing%20self-attention%20with%20InAttention%2C%20which%0Ascales%20linearly%20with%20context%20length%20during%20inference%20by%20having%20tokens%20attend%0Aonly%20to%20initial%20states.%20Benchmarking%20shows%20that%20InAttention%20significantly%0Areduces%20VRAM%20usage%20during%20inference%2C%20enabling%20handling%20of%20long%20sequences%20on%0Aconsumer%20GPUs.%20We%20corroborate%20that%20fine-tuning%20extends%20context%20length%0Aefficiently%2C%20improving%20performance%20on%20long%20sequences%20without%20high%20training%0Acosts.%20InAttention%20offers%20a%20scalable%20solution%20for%20long-range%20dependencies%20in%0Atransformer%20models%2C%20paving%20the%20way%20for%20further%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07063v1&entry.124074799=Read"},
{"title": "Discrete time model predictive control for humanoid walking with step\n  adjustment", "author": "Vishnu Joshi and Suraj Kumar and Nithin V and Shishir Kolathaya", "abstract": "  This paper presents a Discrete-Time Model Predictive Controller (MPC) for\nhumanoid walking with online footstep adjustment. The proposed controller\nutilizes a hierarchical control approach. The high-level controller uses a\nlow-dimensional Linear Inverted Pendulum Model (LIPM) to determine desired foot\nplacement and Center of Mass (CoM) motion, to prevent falls while maintaining\nthe desired velocity. A Task Space Controller (TSC) then tracks the desired\nmotion obtained from the high-level controller, exploiting the whole-body\ndynamics of the humanoid. Our approach differs from existing MPC methods for\nwalking pattern generation by not relying on a predefined foot-plan or a\nreference center of pressure (CoP) trajectory. The overall approach is tested\nin simulation on a torque-controlled Humanoid Robot. Results show that proposed\ncontrol approach generates stable walking and prevents fall against push\ndisturbances.\n", "link": "http://arxiv.org/abs/2410.06790v1", "date": "2024-10-09", "relevancy": 2.0961, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.579}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5346}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20time%20model%20predictive%20control%20for%20humanoid%20walking%20with%20step%0A%20%20adjustment&body=Title%3A%20Discrete%20time%20model%20predictive%20control%20for%20humanoid%20walking%20with%20step%0A%20%20adjustment%0AAuthor%3A%20Vishnu%20Joshi%20and%20Suraj%20Kumar%20and%20Nithin%20V%20and%20Shishir%20Kolathaya%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20Discrete-Time%20Model%20Predictive%20Controller%20%28MPC%29%20for%0Ahumanoid%20walking%20with%20online%20footstep%20adjustment.%20The%20proposed%20controller%0Autilizes%20a%20hierarchical%20control%20approach.%20The%20high-level%20controller%20uses%20a%0Alow-dimensional%20Linear%20Inverted%20Pendulum%20Model%20%28LIPM%29%20to%20determine%20desired%20foot%0Aplacement%20and%20Center%20of%20Mass%20%28CoM%29%20motion%2C%20to%20prevent%20falls%20while%20maintaining%0Athe%20desired%20velocity.%20A%20Task%20Space%20Controller%20%28TSC%29%20then%20tracks%20the%20desired%0Amotion%20obtained%20from%20the%20high-level%20controller%2C%20exploiting%20the%20whole-body%0Adynamics%20of%20the%20humanoid.%20Our%20approach%20differs%20from%20existing%20MPC%20methods%20for%0Awalking%20pattern%20generation%20by%20not%20relying%20on%20a%20predefined%20foot-plan%20or%20a%0Areference%20center%20of%20pressure%20%28CoP%29%20trajectory.%20The%20overall%20approach%20is%20tested%0Ain%20simulation%20on%20a%20torque-controlled%20Humanoid%20Robot.%20Results%20show%20that%20proposed%0Acontrol%20approach%20generates%20stable%20walking%20and%20prevents%20fall%20against%20push%0Adisturbances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520time%2520model%2520predictive%2520control%2520for%2520humanoid%2520walking%2520with%2520step%250A%2520%2520adjustment%26entry.906535625%3DVishnu%2520Joshi%2520and%2520Suraj%2520Kumar%2520and%2520Nithin%2520V%2520and%2520Shishir%2520Kolathaya%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520Discrete-Time%2520Model%2520Predictive%2520Controller%2520%2528MPC%2529%2520for%250Ahumanoid%2520walking%2520with%2520online%2520footstep%2520adjustment.%2520The%2520proposed%2520controller%250Autilizes%2520a%2520hierarchical%2520control%2520approach.%2520The%2520high-level%2520controller%2520uses%2520a%250Alow-dimensional%2520Linear%2520Inverted%2520Pendulum%2520Model%2520%2528LIPM%2529%2520to%2520determine%2520desired%2520foot%250Aplacement%2520and%2520Center%2520of%2520Mass%2520%2528CoM%2529%2520motion%252C%2520to%2520prevent%2520falls%2520while%2520maintaining%250Athe%2520desired%2520velocity.%2520A%2520Task%2520Space%2520Controller%2520%2528TSC%2529%2520then%2520tracks%2520the%2520desired%250Amotion%2520obtained%2520from%2520the%2520high-level%2520controller%252C%2520exploiting%2520the%2520whole-body%250Adynamics%2520of%2520the%2520humanoid.%2520Our%2520approach%2520differs%2520from%2520existing%2520MPC%2520methods%2520for%250Awalking%2520pattern%2520generation%2520by%2520not%2520relying%2520on%2520a%2520predefined%2520foot-plan%2520or%2520a%250Areference%2520center%2520of%2520pressure%2520%2528CoP%2529%2520trajectory.%2520The%2520overall%2520approach%2520is%2520tested%250Ain%2520simulation%2520on%2520a%2520torque-controlled%2520Humanoid%2520Robot.%2520Results%2520show%2520that%2520proposed%250Acontrol%2520approach%2520generates%2520stable%2520walking%2520and%2520prevents%2520fall%2520against%2520push%250Adisturbances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20time%20model%20predictive%20control%20for%20humanoid%20walking%20with%20step%0A%20%20adjustment&entry.906535625=Vishnu%20Joshi%20and%20Suraj%20Kumar%20and%20Nithin%20V%20and%20Shishir%20Kolathaya&entry.1292438233=%20%20This%20paper%20presents%20a%20Discrete-Time%20Model%20Predictive%20Controller%20%28MPC%29%20for%0Ahumanoid%20walking%20with%20online%20footstep%20adjustment.%20The%20proposed%20controller%0Autilizes%20a%20hierarchical%20control%20approach.%20The%20high-level%20controller%20uses%20a%0Alow-dimensional%20Linear%20Inverted%20Pendulum%20Model%20%28LIPM%29%20to%20determine%20desired%20foot%0Aplacement%20and%20Center%20of%20Mass%20%28CoM%29%20motion%2C%20to%20prevent%20falls%20while%20maintaining%0Athe%20desired%20velocity.%20A%20Task%20Space%20Controller%20%28TSC%29%20then%20tracks%20the%20desired%0Amotion%20obtained%20from%20the%20high-level%20controller%2C%20exploiting%20the%20whole-body%0Adynamics%20of%20the%20humanoid.%20Our%20approach%20differs%20from%20existing%20MPC%20methods%20for%0Awalking%20pattern%20generation%20by%20not%20relying%20on%20a%20predefined%20foot-plan%20or%20a%0Areference%20center%20of%20pressure%20%28CoP%29%20trajectory.%20The%20overall%20approach%20is%20tested%0Ain%20simulation%20on%20a%20torque-controlled%20Humanoid%20Robot.%20Results%20show%20that%20proposed%0Acontrol%20approach%20generates%20stable%20walking%20and%20prevents%20fall%20against%20push%0Adisturbances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06790v1&entry.124074799=Read"},
{"title": "SurANet: Surrounding-Aware Network for Concealed Object Detection via\n  Highly-Efficient Interactive Contrastive Learning Strategy", "author": "Yuhan Kang and Qingpeng Li and Leyuan Fang and Jian Zhao and Xuelong Li", "abstract": "  Concealed object detection (COD) in cluttered scenes is significant for\nvarious image processing applications. However, due to that concealed objects\nare always similar to their background, it is extremely hard to distinguish\nthem. Here, the major obstacle is the tiny feature differences between the\ninside and outside object boundary region, which makes it trouble for existing\nCOD methods to achieve accurate results. In this paper, considering that the\nsurrounding environment information can be well utilized to identify the\nconcealed objects, and thus, we propose a novel deep Surrounding-Aware Network,\nnamely SurANet, for COD tasks, which introduces surrounding information into\nfeature extraction and loss function to improve the discrimination. First, we\nenhance the semantics of feature maps using differential fusion of surrounding\nfeatures to highlight concealed objects. Next, a Surrounding-Aware Contrastive\nLoss is applied to identify the concealed object via learning surrounding\nfeature maps contrastively. Then, SurANet can be trained end-to-end with high\nefficiency via our proposed Spatial-Compressed Correlation Transmission\nstrategy after our investigation of feature dynamics, and extensive experiments\nimprove that such features can be well reserved respectively. Finally,\nexperimental results demonstrate that the proposed SurANet outperforms\nstate-of-the-art COD methods on multiple real datasets. Our source code will be\navailable at https://github.com/kyh433/SurANet.\n", "link": "http://arxiv.org/abs/2410.06842v1", "date": "2024-10-09", "relevancy": 2.0953, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5277}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurANet%3A%20Surrounding-Aware%20Network%20for%20Concealed%20Object%20Detection%20via%0A%20%20Highly-Efficient%20Interactive%20Contrastive%20Learning%20Strategy&body=Title%3A%20SurANet%3A%20Surrounding-Aware%20Network%20for%20Concealed%20Object%20Detection%20via%0A%20%20Highly-Efficient%20Interactive%20Contrastive%20Learning%20Strategy%0AAuthor%3A%20Yuhan%20Kang%20and%20Qingpeng%20Li%20and%20Leyuan%20Fang%20and%20Jian%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Concealed%20object%20detection%20%28COD%29%20in%20cluttered%20scenes%20is%20significant%20for%0Avarious%20image%20processing%20applications.%20However%2C%20due%20to%20that%20concealed%20objects%0Aare%20always%20similar%20to%20their%20background%2C%20it%20is%20extremely%20hard%20to%20distinguish%0Athem.%20Here%2C%20the%20major%20obstacle%20is%20the%20tiny%20feature%20differences%20between%20the%0Ainside%20and%20outside%20object%20boundary%20region%2C%20which%20makes%20it%20trouble%20for%20existing%0ACOD%20methods%20to%20achieve%20accurate%20results.%20In%20this%20paper%2C%20considering%20that%20the%0Asurrounding%20environment%20information%20can%20be%20well%20utilized%20to%20identify%20the%0Aconcealed%20objects%2C%20and%20thus%2C%20we%20propose%20a%20novel%20deep%20Surrounding-Aware%20Network%2C%0Anamely%20SurANet%2C%20for%20COD%20tasks%2C%20which%20introduces%20surrounding%20information%20into%0Afeature%20extraction%20and%20loss%20function%20to%20improve%20the%20discrimination.%20First%2C%20we%0Aenhance%20the%20semantics%20of%20feature%20maps%20using%20differential%20fusion%20of%20surrounding%0Afeatures%20to%20highlight%20concealed%20objects.%20Next%2C%20a%20Surrounding-Aware%20Contrastive%0ALoss%20is%20applied%20to%20identify%20the%20concealed%20object%20via%20learning%20surrounding%0Afeature%20maps%20contrastively.%20Then%2C%20SurANet%20can%20be%20trained%20end-to-end%20with%20high%0Aefficiency%20via%20our%20proposed%20Spatial-Compressed%20Correlation%20Transmission%0Astrategy%20after%20our%20investigation%20of%20feature%20dynamics%2C%20and%20extensive%20experiments%0Aimprove%20that%20such%20features%20can%20be%20well%20reserved%20respectively.%20Finally%2C%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20SurANet%20outperforms%0Astate-of-the-art%20COD%20methods%20on%20multiple%20real%20datasets.%20Our%20source%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/kyh433/SurANet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurANet%253A%2520Surrounding-Aware%2520Network%2520for%2520Concealed%2520Object%2520Detection%2520via%250A%2520%2520Highly-Efficient%2520Interactive%2520Contrastive%2520Learning%2520Strategy%26entry.906535625%3DYuhan%2520Kang%2520and%2520Qingpeng%2520Li%2520and%2520Leyuan%2520Fang%2520and%2520Jian%2520Zhao%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Concealed%2520object%2520detection%2520%2528COD%2529%2520in%2520cluttered%2520scenes%2520is%2520significant%2520for%250Avarious%2520image%2520processing%2520applications.%2520However%252C%2520due%2520to%2520that%2520concealed%2520objects%250Aare%2520always%2520similar%2520to%2520their%2520background%252C%2520it%2520is%2520extremely%2520hard%2520to%2520distinguish%250Athem.%2520Here%252C%2520the%2520major%2520obstacle%2520is%2520the%2520tiny%2520feature%2520differences%2520between%2520the%250Ainside%2520and%2520outside%2520object%2520boundary%2520region%252C%2520which%2520makes%2520it%2520trouble%2520for%2520existing%250ACOD%2520methods%2520to%2520achieve%2520accurate%2520results.%2520In%2520this%2520paper%252C%2520considering%2520that%2520the%250Asurrounding%2520environment%2520information%2520can%2520be%2520well%2520utilized%2520to%2520identify%2520the%250Aconcealed%2520objects%252C%2520and%2520thus%252C%2520we%2520propose%2520a%2520novel%2520deep%2520Surrounding-Aware%2520Network%252C%250Anamely%2520SurANet%252C%2520for%2520COD%2520tasks%252C%2520which%2520introduces%2520surrounding%2520information%2520into%250Afeature%2520extraction%2520and%2520loss%2520function%2520to%2520improve%2520the%2520discrimination.%2520First%252C%2520we%250Aenhance%2520the%2520semantics%2520of%2520feature%2520maps%2520using%2520differential%2520fusion%2520of%2520surrounding%250Afeatures%2520to%2520highlight%2520concealed%2520objects.%2520Next%252C%2520a%2520Surrounding-Aware%2520Contrastive%250ALoss%2520is%2520applied%2520to%2520identify%2520the%2520concealed%2520object%2520via%2520learning%2520surrounding%250Afeature%2520maps%2520contrastively.%2520Then%252C%2520SurANet%2520can%2520be%2520trained%2520end-to-end%2520with%2520high%250Aefficiency%2520via%2520our%2520proposed%2520Spatial-Compressed%2520Correlation%2520Transmission%250Astrategy%2520after%2520our%2520investigation%2520of%2520feature%2520dynamics%252C%2520and%2520extensive%2520experiments%250Aimprove%2520that%2520such%2520features%2520can%2520be%2520well%2520reserved%2520respectively.%2520Finally%252C%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520SurANet%2520outperforms%250Astate-of-the-art%2520COD%2520methods%2520on%2520multiple%2520real%2520datasets.%2520Our%2520source%2520code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/kyh433/SurANet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurANet%3A%20Surrounding-Aware%20Network%20for%20Concealed%20Object%20Detection%20via%0A%20%20Highly-Efficient%20Interactive%20Contrastive%20Learning%20Strategy&entry.906535625=Yuhan%20Kang%20and%20Qingpeng%20Li%20and%20Leyuan%20Fang%20and%20Jian%20Zhao%20and%20Xuelong%20Li&entry.1292438233=%20%20Concealed%20object%20detection%20%28COD%29%20in%20cluttered%20scenes%20is%20significant%20for%0Avarious%20image%20processing%20applications.%20However%2C%20due%20to%20that%20concealed%20objects%0Aare%20always%20similar%20to%20their%20background%2C%20it%20is%20extremely%20hard%20to%20distinguish%0Athem.%20Here%2C%20the%20major%20obstacle%20is%20the%20tiny%20feature%20differences%20between%20the%0Ainside%20and%20outside%20object%20boundary%20region%2C%20which%20makes%20it%20trouble%20for%20existing%0ACOD%20methods%20to%20achieve%20accurate%20results.%20In%20this%20paper%2C%20considering%20that%20the%0Asurrounding%20environment%20information%20can%20be%20well%20utilized%20to%20identify%20the%0Aconcealed%20objects%2C%20and%20thus%2C%20we%20propose%20a%20novel%20deep%20Surrounding-Aware%20Network%2C%0Anamely%20SurANet%2C%20for%20COD%20tasks%2C%20which%20introduces%20surrounding%20information%20into%0Afeature%20extraction%20and%20loss%20function%20to%20improve%20the%20discrimination.%20First%2C%20we%0Aenhance%20the%20semantics%20of%20feature%20maps%20using%20differential%20fusion%20of%20surrounding%0Afeatures%20to%20highlight%20concealed%20objects.%20Next%2C%20a%20Surrounding-Aware%20Contrastive%0ALoss%20is%20applied%20to%20identify%20the%20concealed%20object%20via%20learning%20surrounding%0Afeature%20maps%20contrastively.%20Then%2C%20SurANet%20can%20be%20trained%20end-to-end%20with%20high%0Aefficiency%20via%20our%20proposed%20Spatial-Compressed%20Correlation%20Transmission%0Astrategy%20after%20our%20investigation%20of%20feature%20dynamics%2C%20and%20extensive%20experiments%0Aimprove%20that%20such%20features%20can%20be%20well%20reserved%20respectively.%20Finally%2C%0Aexperimental%20results%20demonstrate%20that%20the%20proposed%20SurANet%20outperforms%0Astate-of-the-art%20COD%20methods%20on%20multiple%20real%20datasets.%20Our%20source%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/kyh433/SurANet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06842v1&entry.124074799=Read"},
{"title": "Secure Video Quality Assessment Resisting Adversarial Attacks", "author": "Ao-Xiang Zhang and Yu Ran and Weixuan Tang and Yuan-Gen Wang and Qingxiao Guan and Chunsheng Yang", "abstract": "  The exponential surge in video traffic has intensified the imperative for\nVideo Quality Assessment (VQA). Leveraging cutting-edge architectures, current\nVQA models have achieved human-comparable accuracy. However, recent studies\nhave revealed the vulnerability of existing VQA models against adversarial\nattacks. To establish a reliable and practical assessment system, a secure VQA\nmodel capable of resisting such malicious attacks is urgently demanded.\nUnfortunately, no attempt has been made to explore this issue. This paper first\nattempts to investigate general adversarial defense principles, aiming at\nendowing existing VQA models with security. Specifically, we first introduce\nrandom spatial grid sampling on the video frame for intra-frame defense. Then,\nwe design pixel-wise randomization through a guardian map, globally\nneutralizing adversarial perturbations. Meanwhile, we extract temporal\ninformation from the video sequence as compensation for inter-frame defense.\nBuilding upon these principles, we present a novel VQA framework from the\nsecurity-oriented perspective, termed SecureVQA. Extensive experiments indicate\nthat SecureVQA sets a new benchmark in security while achieving competitive VQA\nperformance compared with state-of-the-art models. Ablation studies delve\ndeeper into analyzing the principles of SecureVQA, demonstrating their\ngeneralization and contributions to the security of leading VQA models.\n", "link": "http://arxiv.org/abs/2410.06866v1", "date": "2024-10-09", "relevancy": 2.0918, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5324}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5321}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Secure%20Video%20Quality%20Assessment%20Resisting%20Adversarial%20Attacks&body=Title%3A%20Secure%20Video%20Quality%20Assessment%20Resisting%20Adversarial%20Attacks%0AAuthor%3A%20Ao-Xiang%20Zhang%20and%20Yu%20Ran%20and%20Weixuan%20Tang%20and%20Yuan-Gen%20Wang%20and%20Qingxiao%20Guan%20and%20Chunsheng%20Yang%0AAbstract%3A%20%20%20The%20exponential%20surge%20in%20video%20traffic%20has%20intensified%20the%20imperative%20for%0AVideo%20Quality%20Assessment%20%28VQA%29.%20Leveraging%20cutting-edge%20architectures%2C%20current%0AVQA%20models%20have%20achieved%20human-comparable%20accuracy.%20However%2C%20recent%20studies%0Ahave%20revealed%20the%20vulnerability%20of%20existing%20VQA%20models%20against%20adversarial%0Aattacks.%20To%20establish%20a%20reliable%20and%20practical%20assessment%20system%2C%20a%20secure%20VQA%0Amodel%20capable%20of%20resisting%20such%20malicious%20attacks%20is%20urgently%20demanded.%0AUnfortunately%2C%20no%20attempt%20has%20been%20made%20to%20explore%20this%20issue.%20This%20paper%20first%0Aattempts%20to%20investigate%20general%20adversarial%20defense%20principles%2C%20aiming%20at%0Aendowing%20existing%20VQA%20models%20with%20security.%20Specifically%2C%20we%20first%20introduce%0Arandom%20spatial%20grid%20sampling%20on%20the%20video%20frame%20for%20intra-frame%20defense.%20Then%2C%0Awe%20design%20pixel-wise%20randomization%20through%20a%20guardian%20map%2C%20globally%0Aneutralizing%20adversarial%20perturbations.%20Meanwhile%2C%20we%20extract%20temporal%0Ainformation%20from%20the%20video%20sequence%20as%20compensation%20for%20inter-frame%20defense.%0ABuilding%20upon%20these%20principles%2C%20we%20present%20a%20novel%20VQA%20framework%20from%20the%0Asecurity-oriented%20perspective%2C%20termed%20SecureVQA.%20Extensive%20experiments%20indicate%0Athat%20SecureVQA%20sets%20a%20new%20benchmark%20in%20security%20while%20achieving%20competitive%20VQA%0Aperformance%20compared%20with%20state-of-the-art%20models.%20Ablation%20studies%20delve%0Adeeper%20into%20analyzing%20the%20principles%20of%20SecureVQA%2C%20demonstrating%20their%0Ageneralization%20and%20contributions%20to%20the%20security%20of%20leading%20VQA%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecure%2520Video%2520Quality%2520Assessment%2520Resisting%2520Adversarial%2520Attacks%26entry.906535625%3DAo-Xiang%2520Zhang%2520and%2520Yu%2520Ran%2520and%2520Weixuan%2520Tang%2520and%2520Yuan-Gen%2520Wang%2520and%2520Qingxiao%2520Guan%2520and%2520Chunsheng%2520Yang%26entry.1292438233%3D%2520%2520The%2520exponential%2520surge%2520in%2520video%2520traffic%2520has%2520intensified%2520the%2520imperative%2520for%250AVideo%2520Quality%2520Assessment%2520%2528VQA%2529.%2520Leveraging%2520cutting-edge%2520architectures%252C%2520current%250AVQA%2520models%2520have%2520achieved%2520human-comparable%2520accuracy.%2520However%252C%2520recent%2520studies%250Ahave%2520revealed%2520the%2520vulnerability%2520of%2520existing%2520VQA%2520models%2520against%2520adversarial%250Aattacks.%2520To%2520establish%2520a%2520reliable%2520and%2520practical%2520assessment%2520system%252C%2520a%2520secure%2520VQA%250Amodel%2520capable%2520of%2520resisting%2520such%2520malicious%2520attacks%2520is%2520urgently%2520demanded.%250AUnfortunately%252C%2520no%2520attempt%2520has%2520been%2520made%2520to%2520explore%2520this%2520issue.%2520This%2520paper%2520first%250Aattempts%2520to%2520investigate%2520general%2520adversarial%2520defense%2520principles%252C%2520aiming%2520at%250Aendowing%2520existing%2520VQA%2520models%2520with%2520security.%2520Specifically%252C%2520we%2520first%2520introduce%250Arandom%2520spatial%2520grid%2520sampling%2520on%2520the%2520video%2520frame%2520for%2520intra-frame%2520defense.%2520Then%252C%250Awe%2520design%2520pixel-wise%2520randomization%2520through%2520a%2520guardian%2520map%252C%2520globally%250Aneutralizing%2520adversarial%2520perturbations.%2520Meanwhile%252C%2520we%2520extract%2520temporal%250Ainformation%2520from%2520the%2520video%2520sequence%2520as%2520compensation%2520for%2520inter-frame%2520defense.%250ABuilding%2520upon%2520these%2520principles%252C%2520we%2520present%2520a%2520novel%2520VQA%2520framework%2520from%2520the%250Asecurity-oriented%2520perspective%252C%2520termed%2520SecureVQA.%2520Extensive%2520experiments%2520indicate%250Athat%2520SecureVQA%2520sets%2520a%2520new%2520benchmark%2520in%2520security%2520while%2520achieving%2520competitive%2520VQA%250Aperformance%2520compared%2520with%2520state-of-the-art%2520models.%2520Ablation%2520studies%2520delve%250Adeeper%2520into%2520analyzing%2520the%2520principles%2520of%2520SecureVQA%252C%2520demonstrating%2520their%250Ageneralization%2520and%2520contributions%2520to%2520the%2520security%2520of%2520leading%2520VQA%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secure%20Video%20Quality%20Assessment%20Resisting%20Adversarial%20Attacks&entry.906535625=Ao-Xiang%20Zhang%20and%20Yu%20Ran%20and%20Weixuan%20Tang%20and%20Yuan-Gen%20Wang%20and%20Qingxiao%20Guan%20and%20Chunsheng%20Yang&entry.1292438233=%20%20The%20exponential%20surge%20in%20video%20traffic%20has%20intensified%20the%20imperative%20for%0AVideo%20Quality%20Assessment%20%28VQA%29.%20Leveraging%20cutting-edge%20architectures%2C%20current%0AVQA%20models%20have%20achieved%20human-comparable%20accuracy.%20However%2C%20recent%20studies%0Ahave%20revealed%20the%20vulnerability%20of%20existing%20VQA%20models%20against%20adversarial%0Aattacks.%20To%20establish%20a%20reliable%20and%20practical%20assessment%20system%2C%20a%20secure%20VQA%0Amodel%20capable%20of%20resisting%20such%20malicious%20attacks%20is%20urgently%20demanded.%0AUnfortunately%2C%20no%20attempt%20has%20been%20made%20to%20explore%20this%20issue.%20This%20paper%20first%0Aattempts%20to%20investigate%20general%20adversarial%20defense%20principles%2C%20aiming%20at%0Aendowing%20existing%20VQA%20models%20with%20security.%20Specifically%2C%20we%20first%20introduce%0Arandom%20spatial%20grid%20sampling%20on%20the%20video%20frame%20for%20intra-frame%20defense.%20Then%2C%0Awe%20design%20pixel-wise%20randomization%20through%20a%20guardian%20map%2C%20globally%0Aneutralizing%20adversarial%20perturbations.%20Meanwhile%2C%20we%20extract%20temporal%0Ainformation%20from%20the%20video%20sequence%20as%20compensation%20for%20inter-frame%20defense.%0ABuilding%20upon%20these%20principles%2C%20we%20present%20a%20novel%20VQA%20framework%20from%20the%0Asecurity-oriented%20perspective%2C%20termed%20SecureVQA.%20Extensive%20experiments%20indicate%0Athat%20SecureVQA%20sets%20a%20new%20benchmark%20in%20security%20while%20achieving%20competitive%20VQA%0Aperformance%20compared%20with%20state-of-the-art%20models.%20Ablation%20studies%20delve%0Adeeper%20into%20analyzing%20the%20principles%20of%20SecureVQA%2C%20demonstrating%20their%0Ageneralization%20and%20contributions%20to%20the%20security%20of%20leading%20VQA%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06866v1&entry.124074799=Read"},
{"title": "Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks", "author": "Xin-Chun Li and Jin-Lin Tang and Bo Zhang and Lan Li and De-Chuan Zhan", "abstract": "  Exploring the loss landscape offers insights into the inherent principles of\ndeep neural networks (DNNs). Recent work suggests an additional asymmetry of\nthe valley beyond the flat and sharp ones, yet without thoroughly examining its\ncauses or implications. Our study methodically explores the factors affecting\nthe symmetry of DNN valleys, encompassing (1) the dataset, network\narchitecture, initialization, and hyperparameters that influence the\nconvergence point; and (2) the magnitude and direction of the noise for 1D\nvisualization. Our major observation shows that the {\\it degree of sign\nconsistency} between the noise and the convergence point is a critical\nindicator of valley symmetry. Theoretical insights from the aspects of ReLU\nactivation and softmax function could explain the interesting phenomenon. Our\ndiscovery propels novel understanding and applications in the scenario of Model\nFusion: (1) the efficacy of interpolating separate models significantly\ncorrelates with their sign consistency ratio, and (2) imposing sign alignment\nduring federated learning emerges as an innovative approach for model parameter\nalignment.\n", "link": "http://arxiv.org/abs/2405.12489v4", "date": "2024-10-09", "relevancy": 2.0895, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5372}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5356}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20and%20Exploiting%20the%20Asymmetric%20Valley%20of%20Deep%20Neural%20Networks&body=Title%3A%20Exploring%20and%20Exploiting%20the%20Asymmetric%20Valley%20of%20Deep%20Neural%20Networks%0AAuthor%3A%20Xin-Chun%20Li%20and%20Jin-Lin%20Tang%20and%20Bo%20Zhang%20and%20Lan%20Li%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20Exploring%20the%20loss%20landscape%20offers%20insights%20into%20the%20inherent%20principles%20of%0Adeep%20neural%20networks%20%28DNNs%29.%20Recent%20work%20suggests%20an%20additional%20asymmetry%20of%0Athe%20valley%20beyond%20the%20flat%20and%20sharp%20ones%2C%20yet%20without%20thoroughly%20examining%20its%0Acauses%20or%20implications.%20Our%20study%20methodically%20explores%20the%20factors%20affecting%0Athe%20symmetry%20of%20DNN%20valleys%2C%20encompassing%20%281%29%20the%20dataset%2C%20network%0Aarchitecture%2C%20initialization%2C%20and%20hyperparameters%20that%20influence%20the%0Aconvergence%20point%3B%20and%20%282%29%20the%20magnitude%20and%20direction%20of%20the%20noise%20for%201D%0Avisualization.%20Our%20major%20observation%20shows%20that%20the%20%7B%5Cit%20degree%20of%20sign%0Aconsistency%7D%20between%20the%20noise%20and%20the%20convergence%20point%20is%20a%20critical%0Aindicator%20of%20valley%20symmetry.%20Theoretical%20insights%20from%20the%20aspects%20of%20ReLU%0Aactivation%20and%20softmax%20function%20could%20explain%20the%20interesting%20phenomenon.%20Our%0Adiscovery%20propels%20novel%20understanding%20and%20applications%20in%20the%20scenario%20of%20Model%0AFusion%3A%20%281%29%20the%20efficacy%20of%20interpolating%20separate%20models%20significantly%0Acorrelates%20with%20their%20sign%20consistency%20ratio%2C%20and%20%282%29%20imposing%20sign%20alignment%0Aduring%20federated%20learning%20emerges%20as%20an%20innovative%20approach%20for%20model%20parameter%0Aalignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12489v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520and%2520Exploiting%2520the%2520Asymmetric%2520Valley%2520of%2520Deep%2520Neural%2520Networks%26entry.906535625%3DXin-Chun%2520Li%2520and%2520Jin-Lin%2520Tang%2520and%2520Bo%2520Zhang%2520and%2520Lan%2520Li%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3D%2520%2520Exploring%2520the%2520loss%2520landscape%2520offers%2520insights%2520into%2520the%2520inherent%2520principles%2520of%250Adeep%2520neural%2520networks%2520%2528DNNs%2529.%2520Recent%2520work%2520suggests%2520an%2520additional%2520asymmetry%2520of%250Athe%2520valley%2520beyond%2520the%2520flat%2520and%2520sharp%2520ones%252C%2520yet%2520without%2520thoroughly%2520examining%2520its%250Acauses%2520or%2520implications.%2520Our%2520study%2520methodically%2520explores%2520the%2520factors%2520affecting%250Athe%2520symmetry%2520of%2520DNN%2520valleys%252C%2520encompassing%2520%25281%2529%2520the%2520dataset%252C%2520network%250Aarchitecture%252C%2520initialization%252C%2520and%2520hyperparameters%2520that%2520influence%2520the%250Aconvergence%2520point%253B%2520and%2520%25282%2529%2520the%2520magnitude%2520and%2520direction%2520of%2520the%2520noise%2520for%25201D%250Avisualization.%2520Our%2520major%2520observation%2520shows%2520that%2520the%2520%257B%255Cit%2520degree%2520of%2520sign%250Aconsistency%257D%2520between%2520the%2520noise%2520and%2520the%2520convergence%2520point%2520is%2520a%2520critical%250Aindicator%2520of%2520valley%2520symmetry.%2520Theoretical%2520insights%2520from%2520the%2520aspects%2520of%2520ReLU%250Aactivation%2520and%2520softmax%2520function%2520could%2520explain%2520the%2520interesting%2520phenomenon.%2520Our%250Adiscovery%2520propels%2520novel%2520understanding%2520and%2520applications%2520in%2520the%2520scenario%2520of%2520Model%250AFusion%253A%2520%25281%2529%2520the%2520efficacy%2520of%2520interpolating%2520separate%2520models%2520significantly%250Acorrelates%2520with%2520their%2520sign%2520consistency%2520ratio%252C%2520and%2520%25282%2529%2520imposing%2520sign%2520alignment%250Aduring%2520federated%2520learning%2520emerges%2520as%2520an%2520innovative%2520approach%2520for%2520model%2520parameter%250Aalignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12489v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20and%20Exploiting%20the%20Asymmetric%20Valley%20of%20Deep%20Neural%20Networks&entry.906535625=Xin-Chun%20Li%20and%20Jin-Lin%20Tang%20and%20Bo%20Zhang%20and%20Lan%20Li%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20Exploring%20the%20loss%20landscape%20offers%20insights%20into%20the%20inherent%20principles%20of%0Adeep%20neural%20networks%20%28DNNs%29.%20Recent%20work%20suggests%20an%20additional%20asymmetry%20of%0Athe%20valley%20beyond%20the%20flat%20and%20sharp%20ones%2C%20yet%20without%20thoroughly%20examining%20its%0Acauses%20or%20implications.%20Our%20study%20methodically%20explores%20the%20factors%20affecting%0Athe%20symmetry%20of%20DNN%20valleys%2C%20encompassing%20%281%29%20the%20dataset%2C%20network%0Aarchitecture%2C%20initialization%2C%20and%20hyperparameters%20that%20influence%20the%0Aconvergence%20point%3B%20and%20%282%29%20the%20magnitude%20and%20direction%20of%20the%20noise%20for%201D%0Avisualization.%20Our%20major%20observation%20shows%20that%20the%20%7B%5Cit%20degree%20of%20sign%0Aconsistency%7D%20between%20the%20noise%20and%20the%20convergence%20point%20is%20a%20critical%0Aindicator%20of%20valley%20symmetry.%20Theoretical%20insights%20from%20the%20aspects%20of%20ReLU%0Aactivation%20and%20softmax%20function%20could%20explain%20the%20interesting%20phenomenon.%20Our%0Adiscovery%20propels%20novel%20understanding%20and%20applications%20in%20the%20scenario%20of%20Model%0AFusion%3A%20%281%29%20the%20efficacy%20of%20interpolating%20separate%20models%20significantly%0Acorrelates%20with%20their%20sign%20consistency%20ratio%2C%20and%20%282%29%20imposing%20sign%20alignment%0Aduring%20federated%20learning%20emerges%20as%20an%20innovative%20approach%20for%20model%20parameter%0Aalignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12489v4&entry.124074799=Read"},
{"title": "Topologically Faithful Multi-class Segmentation in Medical Images", "author": "Alexander H. Berger and Nico Stucki and Laurin Lux and Vincent Buergin and Suprosanna Shit and Anna Banaszak and Daniel Rueckert and Ulrich Bauer and Johannes C. Paetzold", "abstract": "  Topological accuracy in medical image segmentation is a highly important\nproperty for downstream applications such as network analysis and flow modeling\nin vessels or cell counting. Recently, significant methodological advancements\nhave brought well-founded concepts from algebraic topology to binary\nsegmentation. However, these approaches have been underexplored in multi-class\nsegmentation scenarios, where topological errors are common. We propose a\ngeneral loss function for topologically faithful multi-class segmentation\nextending the recent Betti matching concept, which is based on induced\nmatchings of persistence barcodes. We project the N-class segmentation problem\nto N single-class segmentation tasks, which allows us to use 1-parameter\npersistent homology, making training of neural networks computationally\nfeasible. We validate our method on a comprehensive set of four medical\ndatasets with highly variant topological characteristics. Our loss formulation\nsignificantly enhances topological correctness in cardiac, cell, artery-vein,\nand Circle of Willis segmentation.\n", "link": "http://arxiv.org/abs/2403.11001v2", "date": "2024-10-09", "relevancy": 2.0702, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5207}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5193}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topologically%20Faithful%20Multi-class%20Segmentation%20in%20Medical%20Images&body=Title%3A%20Topologically%20Faithful%20Multi-class%20Segmentation%20in%20Medical%20Images%0AAuthor%3A%20Alexander%20H.%20Berger%20and%20Nico%20Stucki%20and%20Laurin%20Lux%20and%20Vincent%20Buergin%20and%20Suprosanna%20Shit%20and%20Anna%20Banaszak%20and%20Daniel%20Rueckert%20and%20Ulrich%20Bauer%20and%20Johannes%20C.%20Paetzold%0AAbstract%3A%20%20%20Topological%20accuracy%20in%20medical%20image%20segmentation%20is%20a%20highly%20important%0Aproperty%20for%20downstream%20applications%20such%20as%20network%20analysis%20and%20flow%20modeling%0Ain%20vessels%20or%20cell%20counting.%20Recently%2C%20significant%20methodological%20advancements%0Ahave%20brought%20well-founded%20concepts%20from%20algebraic%20topology%20to%20binary%0Asegmentation.%20However%2C%20these%20approaches%20have%20been%20underexplored%20in%20multi-class%0Asegmentation%20scenarios%2C%20where%20topological%20errors%20are%20common.%20We%20propose%20a%0Ageneral%20loss%20function%20for%20topologically%20faithful%20multi-class%20segmentation%0Aextending%20the%20recent%20Betti%20matching%20concept%2C%20which%20is%20based%20on%20induced%0Amatchings%20of%20persistence%20barcodes.%20We%20project%20the%20N-class%20segmentation%20problem%0Ato%20N%20single-class%20segmentation%20tasks%2C%20which%20allows%20us%20to%20use%201-parameter%0Apersistent%20homology%2C%20making%20training%20of%20neural%20networks%20computationally%0Afeasible.%20We%20validate%20our%20method%20on%20a%20comprehensive%20set%20of%20four%20medical%0Adatasets%20with%20highly%20variant%20topological%20characteristics.%20Our%20loss%20formulation%0Asignificantly%20enhances%20topological%20correctness%20in%20cardiac%2C%20cell%2C%20artery-vein%2C%0Aand%20Circle%20of%20Willis%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopologically%2520Faithful%2520Multi-class%2520Segmentation%2520in%2520Medical%2520Images%26entry.906535625%3DAlexander%2520H.%2520Berger%2520and%2520Nico%2520Stucki%2520and%2520Laurin%2520Lux%2520and%2520Vincent%2520Buergin%2520and%2520Suprosanna%2520Shit%2520and%2520Anna%2520Banaszak%2520and%2520Daniel%2520Rueckert%2520and%2520Ulrich%2520Bauer%2520and%2520Johannes%2520C.%2520Paetzold%26entry.1292438233%3D%2520%2520Topological%2520accuracy%2520in%2520medical%2520image%2520segmentation%2520is%2520a%2520highly%2520important%250Aproperty%2520for%2520downstream%2520applications%2520such%2520as%2520network%2520analysis%2520and%2520flow%2520modeling%250Ain%2520vessels%2520or%2520cell%2520counting.%2520Recently%252C%2520significant%2520methodological%2520advancements%250Ahave%2520brought%2520well-founded%2520concepts%2520from%2520algebraic%2520topology%2520to%2520binary%250Asegmentation.%2520However%252C%2520these%2520approaches%2520have%2520been%2520underexplored%2520in%2520multi-class%250Asegmentation%2520scenarios%252C%2520where%2520topological%2520errors%2520are%2520common.%2520We%2520propose%2520a%250Ageneral%2520loss%2520function%2520for%2520topologically%2520faithful%2520multi-class%2520segmentation%250Aextending%2520the%2520recent%2520Betti%2520matching%2520concept%252C%2520which%2520is%2520based%2520on%2520induced%250Amatchings%2520of%2520persistence%2520barcodes.%2520We%2520project%2520the%2520N-class%2520segmentation%2520problem%250Ato%2520N%2520single-class%2520segmentation%2520tasks%252C%2520which%2520allows%2520us%2520to%2520use%25201-parameter%250Apersistent%2520homology%252C%2520making%2520training%2520of%2520neural%2520networks%2520computationally%250Afeasible.%2520We%2520validate%2520our%2520method%2520on%2520a%2520comprehensive%2520set%2520of%2520four%2520medical%250Adatasets%2520with%2520highly%2520variant%2520topological%2520characteristics.%2520Our%2520loss%2520formulation%250Asignificantly%2520enhances%2520topological%2520correctness%2520in%2520cardiac%252C%2520cell%252C%2520artery-vein%252C%250Aand%2520Circle%2520of%2520Willis%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topologically%20Faithful%20Multi-class%20Segmentation%20in%20Medical%20Images&entry.906535625=Alexander%20H.%20Berger%20and%20Nico%20Stucki%20and%20Laurin%20Lux%20and%20Vincent%20Buergin%20and%20Suprosanna%20Shit%20and%20Anna%20Banaszak%20and%20Daniel%20Rueckert%20and%20Ulrich%20Bauer%20and%20Johannes%20C.%20Paetzold&entry.1292438233=%20%20Topological%20accuracy%20in%20medical%20image%20segmentation%20is%20a%20highly%20important%0Aproperty%20for%20downstream%20applications%20such%20as%20network%20analysis%20and%20flow%20modeling%0Ain%20vessels%20or%20cell%20counting.%20Recently%2C%20significant%20methodological%20advancements%0Ahave%20brought%20well-founded%20concepts%20from%20algebraic%20topology%20to%20binary%0Asegmentation.%20However%2C%20these%20approaches%20have%20been%20underexplored%20in%20multi-class%0Asegmentation%20scenarios%2C%20where%20topological%20errors%20are%20common.%20We%20propose%20a%0Ageneral%20loss%20function%20for%20topologically%20faithful%20multi-class%20segmentation%0Aextending%20the%20recent%20Betti%20matching%20concept%2C%20which%20is%20based%20on%20induced%0Amatchings%20of%20persistence%20barcodes.%20We%20project%20the%20N-class%20segmentation%20problem%0Ato%20N%20single-class%20segmentation%20tasks%2C%20which%20allows%20us%20to%20use%201-parameter%0Apersistent%20homology%2C%20making%20training%20of%20neural%20networks%20computationally%0Afeasible.%20We%20validate%20our%20method%20on%20a%20comprehensive%20set%20of%20four%20medical%0Adatasets%20with%20highly%20variant%20topological%20characteristics.%20Our%20loss%20formulation%0Asignificantly%20enhances%20topological%20correctness%20in%20cardiac%2C%20cell%2C%20artery-vein%2C%0Aand%20Circle%20of%20Willis%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11001v2&entry.124074799=Read"},
{"title": "Do Contemporary CATE Models Capture Real-World Heterogeneity? Findings\n  from a Large-Scale Benchmark", "author": "Haining Yu and Yizhou Sun", "abstract": "  We present unexpected findings from a large-scale benchmark study evaluating\nConditional Average Treatment Effect (CATE) estimation algorithms. By running\n16 modern CATE models across 43,200 datasets, we find that: (a) 62\\% of CATE\nestimates have a higher Mean Squared Error (MSE) than a trivial zero-effect\npredictor, rendering them ineffective; (b) in datasets with at least one useful\nCATE estimate, 80\\% still have higher MSE than a constant-effect model; and (c)\nOrthogonality-based models outperform other models only 30\\% of the time,\ndespite widespread optimism about their performance. These findings expose\nsignificant limitations in current CATE models and suggest ample opportunities\nfor further research.\n  Our findings stem from a novel application of \\textit{observational\nsampling}, originally developed to evaluate Average Treatment Effect (ATE)\nestimates from observational methods with experiment data. To adapt\nobservational sampling for CATE evaluation, we introduce a statistical\nparameter, $Q$, equal to MSE minus a constant and preserves the ranking of\nmodels by their MSE. We then derive a family of sample statistics, collectively\ncalled $\\hat{Q}$, that can be computed from real-world data. We prove that\n$\\hat{Q}$ is a consistent estimator of $Q$ under mild technical conditions.\nWhen used in observational sampling, $\\hat{Q}$ is unbiased and asymptotically\nselects the model with the smallest MSE. To ensure the benchmark reflects\nreal-world heterogeneity, we handpick datasets where outcomes come from field\nrather than simulation. By combining the new observational sampling method, new\nstatistics, and real-world datasets, the benchmark provides a unique\nperspective on CATE estimator performance and uncover gaps in capturing\nreal-world heterogeneity.\n", "link": "http://arxiv.org/abs/2410.07021v1", "date": "2024-10-09", "relevancy": 1.8271, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4598}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4598}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Contemporary%20CATE%20Models%20Capture%20Real-World%20Heterogeneity%3F%20Findings%0A%20%20from%20a%20Large-Scale%20Benchmark&body=Title%3A%20Do%20Contemporary%20CATE%20Models%20Capture%20Real-World%20Heterogeneity%3F%20Findings%0A%20%20from%20a%20Large-Scale%20Benchmark%0AAuthor%3A%20Haining%20Yu%20and%20Yizhou%20Sun%0AAbstract%3A%20%20%20We%20present%20unexpected%20findings%20from%20a%20large-scale%20benchmark%20study%20evaluating%0AConditional%20Average%20Treatment%20Effect%20%28CATE%29%20estimation%20algorithms.%20By%20running%0A16%20modern%20CATE%20models%20across%2043%2C200%20datasets%2C%20we%20find%20that%3A%20%28a%29%2062%5C%25%20of%20CATE%0Aestimates%20have%20a%20higher%20Mean%20Squared%20Error%20%28MSE%29%20than%20a%20trivial%20zero-effect%0Apredictor%2C%20rendering%20them%20ineffective%3B%20%28b%29%20in%20datasets%20with%20at%20least%20one%20useful%0ACATE%20estimate%2C%2080%5C%25%20still%20have%20higher%20MSE%20than%20a%20constant-effect%20model%3B%20and%20%28c%29%0AOrthogonality-based%20models%20outperform%20other%20models%20only%2030%5C%25%20of%20the%20time%2C%0Adespite%20widespread%20optimism%20about%20their%20performance.%20These%20findings%20expose%0Asignificant%20limitations%20in%20current%20CATE%20models%20and%20suggest%20ample%20opportunities%0Afor%20further%20research.%0A%20%20Our%20findings%20stem%20from%20a%20novel%20application%20of%20%5Ctextit%7Bobservational%0Asampling%7D%2C%20originally%20developed%20to%20evaluate%20Average%20Treatment%20Effect%20%28ATE%29%0Aestimates%20from%20observational%20methods%20with%20experiment%20data.%20To%20adapt%0Aobservational%20sampling%20for%20CATE%20evaluation%2C%20we%20introduce%20a%20statistical%0Aparameter%2C%20%24Q%24%2C%20equal%20to%20MSE%20minus%20a%20constant%20and%20preserves%20the%20ranking%20of%0Amodels%20by%20their%20MSE.%20We%20then%20derive%20a%20family%20of%20sample%20statistics%2C%20collectively%0Acalled%20%24%5Chat%7BQ%7D%24%2C%20that%20can%20be%20computed%20from%20real-world%20data.%20We%20prove%20that%0A%24%5Chat%7BQ%7D%24%20is%20a%20consistent%20estimator%20of%20%24Q%24%20under%20mild%20technical%20conditions.%0AWhen%20used%20in%20observational%20sampling%2C%20%24%5Chat%7BQ%7D%24%20is%20unbiased%20and%20asymptotically%0Aselects%20the%20model%20with%20the%20smallest%20MSE.%20To%20ensure%20the%20benchmark%20reflects%0Areal-world%20heterogeneity%2C%20we%20handpick%20datasets%20where%20outcomes%20come%20from%20field%0Arather%20than%20simulation.%20By%20combining%20the%20new%20observational%20sampling%20method%2C%20new%0Astatistics%2C%20and%20real-world%20datasets%2C%20the%20benchmark%20provides%20a%20unique%0Aperspective%20on%20CATE%20estimator%20performance%20and%20uncover%20gaps%20in%20capturing%0Areal-world%20heterogeneity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Contemporary%2520CATE%2520Models%2520Capture%2520Real-World%2520Heterogeneity%253F%2520Findings%250A%2520%2520from%2520a%2520Large-Scale%2520Benchmark%26entry.906535625%3DHaining%2520Yu%2520and%2520Yizhou%2520Sun%26entry.1292438233%3D%2520%2520We%2520present%2520unexpected%2520findings%2520from%2520a%2520large-scale%2520benchmark%2520study%2520evaluating%250AConditional%2520Average%2520Treatment%2520Effect%2520%2528CATE%2529%2520estimation%2520algorithms.%2520By%2520running%250A16%2520modern%2520CATE%2520models%2520across%252043%252C200%2520datasets%252C%2520we%2520find%2520that%253A%2520%2528a%2529%252062%255C%2525%2520of%2520CATE%250Aestimates%2520have%2520a%2520higher%2520Mean%2520Squared%2520Error%2520%2528MSE%2529%2520than%2520a%2520trivial%2520zero-effect%250Apredictor%252C%2520rendering%2520them%2520ineffective%253B%2520%2528b%2529%2520in%2520datasets%2520with%2520at%2520least%2520one%2520useful%250ACATE%2520estimate%252C%252080%255C%2525%2520still%2520have%2520higher%2520MSE%2520than%2520a%2520constant-effect%2520model%253B%2520and%2520%2528c%2529%250AOrthogonality-based%2520models%2520outperform%2520other%2520models%2520only%252030%255C%2525%2520of%2520the%2520time%252C%250Adespite%2520widespread%2520optimism%2520about%2520their%2520performance.%2520These%2520findings%2520expose%250Asignificant%2520limitations%2520in%2520current%2520CATE%2520models%2520and%2520suggest%2520ample%2520opportunities%250Afor%2520further%2520research.%250A%2520%2520Our%2520findings%2520stem%2520from%2520a%2520novel%2520application%2520of%2520%255Ctextit%257Bobservational%250Asampling%257D%252C%2520originally%2520developed%2520to%2520evaluate%2520Average%2520Treatment%2520Effect%2520%2528ATE%2529%250Aestimates%2520from%2520observational%2520methods%2520with%2520experiment%2520data.%2520To%2520adapt%250Aobservational%2520sampling%2520for%2520CATE%2520evaluation%252C%2520we%2520introduce%2520a%2520statistical%250Aparameter%252C%2520%2524Q%2524%252C%2520equal%2520to%2520MSE%2520minus%2520a%2520constant%2520and%2520preserves%2520the%2520ranking%2520of%250Amodels%2520by%2520their%2520MSE.%2520We%2520then%2520derive%2520a%2520family%2520of%2520sample%2520statistics%252C%2520collectively%250Acalled%2520%2524%255Chat%257BQ%257D%2524%252C%2520that%2520can%2520be%2520computed%2520from%2520real-world%2520data.%2520We%2520prove%2520that%250A%2524%255Chat%257BQ%257D%2524%2520is%2520a%2520consistent%2520estimator%2520of%2520%2524Q%2524%2520under%2520mild%2520technical%2520conditions.%250AWhen%2520used%2520in%2520observational%2520sampling%252C%2520%2524%255Chat%257BQ%257D%2524%2520is%2520unbiased%2520and%2520asymptotically%250Aselects%2520the%2520model%2520with%2520the%2520smallest%2520MSE.%2520To%2520ensure%2520the%2520benchmark%2520reflects%250Areal-world%2520heterogeneity%252C%2520we%2520handpick%2520datasets%2520where%2520outcomes%2520come%2520from%2520field%250Arather%2520than%2520simulation.%2520By%2520combining%2520the%2520new%2520observational%2520sampling%2520method%252C%2520new%250Astatistics%252C%2520and%2520real-world%2520datasets%252C%2520the%2520benchmark%2520provides%2520a%2520unique%250Aperspective%2520on%2520CATE%2520estimator%2520performance%2520and%2520uncover%2520gaps%2520in%2520capturing%250Areal-world%2520heterogeneity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Contemporary%20CATE%20Models%20Capture%20Real-World%20Heterogeneity%3F%20Findings%0A%20%20from%20a%20Large-Scale%20Benchmark&entry.906535625=Haining%20Yu%20and%20Yizhou%20Sun&entry.1292438233=%20%20We%20present%20unexpected%20findings%20from%20a%20large-scale%20benchmark%20study%20evaluating%0AConditional%20Average%20Treatment%20Effect%20%28CATE%29%20estimation%20algorithms.%20By%20running%0A16%20modern%20CATE%20models%20across%2043%2C200%20datasets%2C%20we%20find%20that%3A%20%28a%29%2062%5C%25%20of%20CATE%0Aestimates%20have%20a%20higher%20Mean%20Squared%20Error%20%28MSE%29%20than%20a%20trivial%20zero-effect%0Apredictor%2C%20rendering%20them%20ineffective%3B%20%28b%29%20in%20datasets%20with%20at%20least%20one%20useful%0ACATE%20estimate%2C%2080%5C%25%20still%20have%20higher%20MSE%20than%20a%20constant-effect%20model%3B%20and%20%28c%29%0AOrthogonality-based%20models%20outperform%20other%20models%20only%2030%5C%25%20of%20the%20time%2C%0Adespite%20widespread%20optimism%20about%20their%20performance.%20These%20findings%20expose%0Asignificant%20limitations%20in%20current%20CATE%20models%20and%20suggest%20ample%20opportunities%0Afor%20further%20research.%0A%20%20Our%20findings%20stem%20from%20a%20novel%20application%20of%20%5Ctextit%7Bobservational%0Asampling%7D%2C%20originally%20developed%20to%20evaluate%20Average%20Treatment%20Effect%20%28ATE%29%0Aestimates%20from%20observational%20methods%20with%20experiment%20data.%20To%20adapt%0Aobservational%20sampling%20for%20CATE%20evaluation%2C%20we%20introduce%20a%20statistical%0Aparameter%2C%20%24Q%24%2C%20equal%20to%20MSE%20minus%20a%20constant%20and%20preserves%20the%20ranking%20of%0Amodels%20by%20their%20MSE.%20We%20then%20derive%20a%20family%20of%20sample%20statistics%2C%20collectively%0Acalled%20%24%5Chat%7BQ%7D%24%2C%20that%20can%20be%20computed%20from%20real-world%20data.%20We%20prove%20that%0A%24%5Chat%7BQ%7D%24%20is%20a%20consistent%20estimator%20of%20%24Q%24%20under%20mild%20technical%20conditions.%0AWhen%20used%20in%20observational%20sampling%2C%20%24%5Chat%7BQ%7D%24%20is%20unbiased%20and%20asymptotically%0Aselects%20the%20model%20with%20the%20smallest%20MSE.%20To%20ensure%20the%20benchmark%20reflects%0Areal-world%20heterogeneity%2C%20we%20handpick%20datasets%20where%20outcomes%20come%20from%20field%0Arather%20than%20simulation.%20By%20combining%20the%20new%20observational%20sampling%20method%2C%20new%0Astatistics%2C%20and%20real-world%20datasets%2C%20the%20benchmark%20provides%20a%20unique%0Aperspective%20on%20CATE%20estimator%20performance%20and%20uncover%20gaps%20in%20capturing%0Areal-world%20heterogeneity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07021v1&entry.124074799=Read"},
{"title": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making", "author": "Manling Li and Shiyu Zhao and Qineng Wang and Kangrui Wang and Yu Zhou and Sanjana Srivastava and Cem Gokmen and Tony Lee and Li Erran Li and Ruohan Zhang and Weiyu Liu and Percy Liang and Li Fei-Fei and Jiayuan Mao and Jiajun Wu", "abstract": "  We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making.\n", "link": "http://arxiv.org/abs/2410.07166v1", "date": "2024-10-09", "relevancy": 1.5442, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5198}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5194}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20Agent%20Interface%3A%20Benchmarking%20LLMs%20for%20Embodied%20Decision%20Making&body=Title%3A%20Embodied%20Agent%20Interface%3A%20Benchmarking%20LLMs%20for%20Embodied%20Decision%20Making%0AAuthor%3A%20Manling%20Li%20and%20Shiyu%20Zhao%20and%20Qineng%20Wang%20and%20Kangrui%20Wang%20and%20Yu%20Zhou%20and%20Sanjana%20Srivastava%20and%20Cem%20Gokmen%20and%20Tony%20Lee%20and%20Li%20Erran%20Li%20and%20Ruohan%20Zhang%20and%20Weiyu%20Liu%20and%20Percy%20Liang%20and%20Li%20Fei-Fei%20and%20Jiayuan%20Mao%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20We%20aim%20to%20evaluate%20Large%20Language%20Models%20%28LLMs%29%20for%20embodied%20decision%20making.%0AWhile%20a%20significant%20body%20of%20work%20has%20been%20leveraging%20LLMs%20for%20decision%20making%0Ain%20embodied%20environments%2C%20we%20still%20lack%20a%20systematic%20understanding%20of%20their%0Aperformance%20because%20they%20are%20usually%20applied%20in%20different%20domains%2C%20for%0Adifferent%20purposes%2C%20and%20built%20based%20on%20different%20inputs%20and%20outputs.%0AFurthermore%2C%20existing%20evaluations%20tend%20to%20rely%20solely%20on%20a%20final%20success%20rate%2C%0Amaking%20it%20difficult%20to%20pinpoint%20what%20ability%20is%20missing%20in%20LLMs%20and%20where%20the%0Aproblem%20lies%2C%20which%20in%20turn%20blocks%20embodied%20agents%20from%20leveraging%20LLMs%0Aeffectively%20and%20selectively.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Ageneralized%20interface%20%28Embodied%20Agent%20Interface%29%20that%20supports%20the%0Aformalization%20of%20various%20types%20of%20tasks%20and%20input-output%20specifications%20of%0ALLM-based%20modules.%20Specifically%2C%20it%20allows%20us%20to%20unify%201%29%20a%20broad%20set%20of%0Aembodied%20decision-making%20tasks%20involving%20both%20state%20and%20temporally%20extended%0Agoals%2C%202%29%20four%20commonly-used%20LLM-based%20modules%20for%20decision%20making%3A%20goal%0Ainterpretation%2C%20subgoal%20decomposition%2C%20action%20sequencing%2C%20and%20transition%0Amodeling%2C%20and%203%29%20a%20collection%20of%20fine-grained%20metrics%20which%20break%20down%0Aevaluation%20into%20various%20types%20of%20errors%2C%20such%20as%20hallucination%20errors%2C%0Aaffordance%20errors%2C%20various%20types%20of%20planning%20errors%2C%20etc.%20Overall%2C%20our%0Abenchmark%20offers%20a%20comprehensive%20assessment%20of%20LLMs%27%20performance%20for%20different%0Asubtasks%2C%20pinpointing%20the%20strengths%20and%20weaknesses%20in%20LLM-powered%20embodied%20AI%0Asystems%2C%20and%20providing%20insights%20for%20effective%20and%20selective%20use%20of%20LLMs%20in%0Aembodied%20decision%20making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520Agent%2520Interface%253A%2520Benchmarking%2520LLMs%2520for%2520Embodied%2520Decision%2520Making%26entry.906535625%3DManling%2520Li%2520and%2520Shiyu%2520Zhao%2520and%2520Qineng%2520Wang%2520and%2520Kangrui%2520Wang%2520and%2520Yu%2520Zhou%2520and%2520Sanjana%2520Srivastava%2520and%2520Cem%2520Gokmen%2520and%2520Tony%2520Lee%2520and%2520Li%2520Erran%2520Li%2520and%2520Ruohan%2520Zhang%2520and%2520Weiyu%2520Liu%2520and%2520Percy%2520Liang%2520and%2520Li%2520Fei-Fei%2520and%2520Jiayuan%2520Mao%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520evaluate%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520embodied%2520decision%2520making.%250AWhile%2520a%2520significant%2520body%2520of%2520work%2520has%2520been%2520leveraging%2520LLMs%2520for%2520decision%2520making%250Ain%2520embodied%2520environments%252C%2520we%2520still%2520lack%2520a%2520systematic%2520understanding%2520of%2520their%250Aperformance%2520because%2520they%2520are%2520usually%2520applied%2520in%2520different%2520domains%252C%2520for%250Adifferent%2520purposes%252C%2520and%2520built%2520based%2520on%2520different%2520inputs%2520and%2520outputs.%250AFurthermore%252C%2520existing%2520evaluations%2520tend%2520to%2520rely%2520solely%2520on%2520a%2520final%2520success%2520rate%252C%250Amaking%2520it%2520difficult%2520to%2520pinpoint%2520what%2520ability%2520is%2520missing%2520in%2520LLMs%2520and%2520where%2520the%250Aproblem%2520lies%252C%2520which%2520in%2520turn%2520blocks%2520embodied%2520agents%2520from%2520leveraging%2520LLMs%250Aeffectively%2520and%2520selectively.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Ageneralized%2520interface%2520%2528Embodied%2520Agent%2520Interface%2529%2520that%2520supports%2520the%250Aformalization%2520of%2520various%2520types%2520of%2520tasks%2520and%2520input-output%2520specifications%2520of%250ALLM-based%2520modules.%2520Specifically%252C%2520it%2520allows%2520us%2520to%2520unify%25201%2529%2520a%2520broad%2520set%2520of%250Aembodied%2520decision-making%2520tasks%2520involving%2520both%2520state%2520and%2520temporally%2520extended%250Agoals%252C%25202%2529%2520four%2520commonly-used%2520LLM-based%2520modules%2520for%2520decision%2520making%253A%2520goal%250Ainterpretation%252C%2520subgoal%2520decomposition%252C%2520action%2520sequencing%252C%2520and%2520transition%250Amodeling%252C%2520and%25203%2529%2520a%2520collection%2520of%2520fine-grained%2520metrics%2520which%2520break%2520down%250Aevaluation%2520into%2520various%2520types%2520of%2520errors%252C%2520such%2520as%2520hallucination%2520errors%252C%250Aaffordance%2520errors%252C%2520various%2520types%2520of%2520planning%2520errors%252C%2520etc.%2520Overall%252C%2520our%250Abenchmark%2520offers%2520a%2520comprehensive%2520assessment%2520of%2520LLMs%2527%2520performance%2520for%2520different%250Asubtasks%252C%2520pinpointing%2520the%2520strengths%2520and%2520weaknesses%2520in%2520LLM-powered%2520embodied%2520AI%250Asystems%252C%2520and%2520providing%2520insights%2520for%2520effective%2520and%2520selective%2520use%2520of%2520LLMs%2520in%250Aembodied%2520decision%2520making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20Agent%20Interface%3A%20Benchmarking%20LLMs%20for%20Embodied%20Decision%20Making&entry.906535625=Manling%20Li%20and%20Shiyu%20Zhao%20and%20Qineng%20Wang%20and%20Kangrui%20Wang%20and%20Yu%20Zhou%20and%20Sanjana%20Srivastava%20and%20Cem%20Gokmen%20and%20Tony%20Lee%20and%20Li%20Erran%20Li%20and%20Ruohan%20Zhang%20and%20Weiyu%20Liu%20and%20Percy%20Liang%20and%20Li%20Fei-Fei%20and%20Jiayuan%20Mao%20and%20Jiajun%20Wu&entry.1292438233=%20%20We%20aim%20to%20evaluate%20Large%20Language%20Models%20%28LLMs%29%20for%20embodied%20decision%20making.%0AWhile%20a%20significant%20body%20of%20work%20has%20been%20leveraging%20LLMs%20for%20decision%20making%0Ain%20embodied%20environments%2C%20we%20still%20lack%20a%20systematic%20understanding%20of%20their%0Aperformance%20because%20they%20are%20usually%20applied%20in%20different%20domains%2C%20for%0Adifferent%20purposes%2C%20and%20built%20based%20on%20different%20inputs%20and%20outputs.%0AFurthermore%2C%20existing%20evaluations%20tend%20to%20rely%20solely%20on%20a%20final%20success%20rate%2C%0Amaking%20it%20difficult%20to%20pinpoint%20what%20ability%20is%20missing%20in%20LLMs%20and%20where%20the%0Aproblem%20lies%2C%20which%20in%20turn%20blocks%20embodied%20agents%20from%20leveraging%20LLMs%0Aeffectively%20and%20selectively.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Ageneralized%20interface%20%28Embodied%20Agent%20Interface%29%20that%20supports%20the%0Aformalization%20of%20various%20types%20of%20tasks%20and%20input-output%20specifications%20of%0ALLM-based%20modules.%20Specifically%2C%20it%20allows%20us%20to%20unify%201%29%20a%20broad%20set%20of%0Aembodied%20decision-making%20tasks%20involving%20both%20state%20and%20temporally%20extended%0Agoals%2C%202%29%20four%20commonly-used%20LLM-based%20modules%20for%20decision%20making%3A%20goal%0Ainterpretation%2C%20subgoal%20decomposition%2C%20action%20sequencing%2C%20and%20transition%0Amodeling%2C%20and%203%29%20a%20collection%20of%20fine-grained%20metrics%20which%20break%20down%0Aevaluation%20into%20various%20types%20of%20errors%2C%20such%20as%20hallucination%20errors%2C%0Aaffordance%20errors%2C%20various%20types%20of%20planning%20errors%2C%20etc.%20Overall%2C%20our%0Abenchmark%20offers%20a%20comprehensive%20assessment%20of%20LLMs%27%20performance%20for%20different%0Asubtasks%2C%20pinpointing%20the%20strengths%20and%20weaknesses%20in%20LLM-powered%20embodied%20AI%0Asystems%2C%20and%20providing%20insights%20for%20effective%20and%20selective%20use%20of%20LLMs%20in%0Aembodied%20decision%20making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07166v1&entry.124074799=Read"},
{"title": "Control System Design and Experiments for Autonomous Underwater\n  Helicopter Docking Procedure Based on Acoustic-inertial-optical Guidance", "author": "Haoda Li and Xinyu An and Rendong Feng and Zhenwei Rong and Zhuoyu Zhang and Zhipeng Li and Liming Zhao and Ying Chen", "abstract": "  A control system structure for the underwater docking procedure of an\nAutonomous Underwater Helicopter (AUH) is proposed in this paper, which\nutilizes acoustic-inertial-optical guidance. Unlike conventional Autonomous\nUnderwater Vehicles (AUVs), the maneuverability requirements for AUHs are more\nstringent during the docking procedure, requiring it to remain stationary or\nhave minimal horizontal movement while moving vertically. The docking procedure\nis divided into two stages: Homing and Landing, each stage utilizing different\nguidance methods. Additionally, a segmented aligning strategy operating at\nvarious altitudes and a linear velocity decision are both adopted in Landing\nstage. Due to the unique structure of the Subsea Docking System (SDS), the AUH\nis required to dock onto the SDS in a fixed orientation with specific attitude\nand altitude. Therefore, a particular criterion is proposed to determine\nwhether the AUH has successfully docked onto the SDS. Furthermore, the\neffectiveness and robustness of the proposed control method in AUH's docking\nprocedure are demonstrated through pool experiments and sea trials.\n", "link": "http://arxiv.org/abs/2410.06953v1", "date": "2024-10-09", "relevancy": 1.8829, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4916}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4562}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Control%20System%20Design%20and%20Experiments%20for%20Autonomous%20Underwater%0A%20%20Helicopter%20Docking%20Procedure%20Based%20on%20Acoustic-inertial-optical%20Guidance&body=Title%3A%20Control%20System%20Design%20and%20Experiments%20for%20Autonomous%20Underwater%0A%20%20Helicopter%20Docking%20Procedure%20Based%20on%20Acoustic-inertial-optical%20Guidance%0AAuthor%3A%20Haoda%20Li%20and%20Xinyu%20An%20and%20Rendong%20Feng%20and%20Zhenwei%20Rong%20and%20Zhuoyu%20Zhang%20and%20Zhipeng%20Li%20and%20Liming%20Zhao%20and%20Ying%20Chen%0AAbstract%3A%20%20%20A%20control%20system%20structure%20for%20the%20underwater%20docking%20procedure%20of%20an%0AAutonomous%20Underwater%20Helicopter%20%28AUH%29%20is%20proposed%20in%20this%20paper%2C%20which%0Autilizes%20acoustic-inertial-optical%20guidance.%20Unlike%20conventional%20Autonomous%0AUnderwater%20Vehicles%20%28AUVs%29%2C%20the%20maneuverability%20requirements%20for%20AUHs%20are%20more%0Astringent%20during%20the%20docking%20procedure%2C%20requiring%20it%20to%20remain%20stationary%20or%0Ahave%20minimal%20horizontal%20movement%20while%20moving%20vertically.%20The%20docking%20procedure%0Ais%20divided%20into%20two%20stages%3A%20Homing%20and%20Landing%2C%20each%20stage%20utilizing%20different%0Aguidance%20methods.%20Additionally%2C%20a%20segmented%20aligning%20strategy%20operating%20at%0Avarious%20altitudes%20and%20a%20linear%20velocity%20decision%20are%20both%20adopted%20in%20Landing%0Astage.%20Due%20to%20the%20unique%20structure%20of%20the%20Subsea%20Docking%20System%20%28SDS%29%2C%20the%20AUH%0Ais%20required%20to%20dock%20onto%20the%20SDS%20in%20a%20fixed%20orientation%20with%20specific%20attitude%0Aand%20altitude.%20Therefore%2C%20a%20particular%20criterion%20is%20proposed%20to%20determine%0Awhether%20the%20AUH%20has%20successfully%20docked%20onto%20the%20SDS.%20Furthermore%2C%20the%0Aeffectiveness%20and%20robustness%20of%20the%20proposed%20control%20method%20in%20AUH%27s%20docking%0Aprocedure%20are%20demonstrated%20through%20pool%20experiments%20and%20sea%20trials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControl%2520System%2520Design%2520and%2520Experiments%2520for%2520Autonomous%2520Underwater%250A%2520%2520Helicopter%2520Docking%2520Procedure%2520Based%2520on%2520Acoustic-inertial-optical%2520Guidance%26entry.906535625%3DHaoda%2520Li%2520and%2520Xinyu%2520An%2520and%2520Rendong%2520Feng%2520and%2520Zhenwei%2520Rong%2520and%2520Zhuoyu%2520Zhang%2520and%2520Zhipeng%2520Li%2520and%2520Liming%2520Zhao%2520and%2520Ying%2520Chen%26entry.1292438233%3D%2520%2520A%2520control%2520system%2520structure%2520for%2520the%2520underwater%2520docking%2520procedure%2520of%2520an%250AAutonomous%2520Underwater%2520Helicopter%2520%2528AUH%2529%2520is%2520proposed%2520in%2520this%2520paper%252C%2520which%250Autilizes%2520acoustic-inertial-optical%2520guidance.%2520Unlike%2520conventional%2520Autonomous%250AUnderwater%2520Vehicles%2520%2528AUVs%2529%252C%2520the%2520maneuverability%2520requirements%2520for%2520AUHs%2520are%2520more%250Astringent%2520during%2520the%2520docking%2520procedure%252C%2520requiring%2520it%2520to%2520remain%2520stationary%2520or%250Ahave%2520minimal%2520horizontal%2520movement%2520while%2520moving%2520vertically.%2520The%2520docking%2520procedure%250Ais%2520divided%2520into%2520two%2520stages%253A%2520Homing%2520and%2520Landing%252C%2520each%2520stage%2520utilizing%2520different%250Aguidance%2520methods.%2520Additionally%252C%2520a%2520segmented%2520aligning%2520strategy%2520operating%2520at%250Avarious%2520altitudes%2520and%2520a%2520linear%2520velocity%2520decision%2520are%2520both%2520adopted%2520in%2520Landing%250Astage.%2520Due%2520to%2520the%2520unique%2520structure%2520of%2520the%2520Subsea%2520Docking%2520System%2520%2528SDS%2529%252C%2520the%2520AUH%250Ais%2520required%2520to%2520dock%2520onto%2520the%2520SDS%2520in%2520a%2520fixed%2520orientation%2520with%2520specific%2520attitude%250Aand%2520altitude.%2520Therefore%252C%2520a%2520particular%2520criterion%2520is%2520proposed%2520to%2520determine%250Awhether%2520the%2520AUH%2520has%2520successfully%2520docked%2520onto%2520the%2520SDS.%2520Furthermore%252C%2520the%250Aeffectiveness%2520and%2520robustness%2520of%2520the%2520proposed%2520control%2520method%2520in%2520AUH%2527s%2520docking%250Aprocedure%2520are%2520demonstrated%2520through%2520pool%2520experiments%2520and%2520sea%2520trials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control%20System%20Design%20and%20Experiments%20for%20Autonomous%20Underwater%0A%20%20Helicopter%20Docking%20Procedure%20Based%20on%20Acoustic-inertial-optical%20Guidance&entry.906535625=Haoda%20Li%20and%20Xinyu%20An%20and%20Rendong%20Feng%20and%20Zhenwei%20Rong%20and%20Zhuoyu%20Zhang%20and%20Zhipeng%20Li%20and%20Liming%20Zhao%20and%20Ying%20Chen&entry.1292438233=%20%20A%20control%20system%20structure%20for%20the%20underwater%20docking%20procedure%20of%20an%0AAutonomous%20Underwater%20Helicopter%20%28AUH%29%20is%20proposed%20in%20this%20paper%2C%20which%0Autilizes%20acoustic-inertial-optical%20guidance.%20Unlike%20conventional%20Autonomous%0AUnderwater%20Vehicles%20%28AUVs%29%2C%20the%20maneuverability%20requirements%20for%20AUHs%20are%20more%0Astringent%20during%20the%20docking%20procedure%2C%20requiring%20it%20to%20remain%20stationary%20or%0Ahave%20minimal%20horizontal%20movement%20while%20moving%20vertically.%20The%20docking%20procedure%0Ais%20divided%20into%20two%20stages%3A%20Homing%20and%20Landing%2C%20each%20stage%20utilizing%20different%0Aguidance%20methods.%20Additionally%2C%20a%20segmented%20aligning%20strategy%20operating%20at%0Avarious%20altitudes%20and%20a%20linear%20velocity%20decision%20are%20both%20adopted%20in%20Landing%0Astage.%20Due%20to%20the%20unique%20structure%20of%20the%20Subsea%20Docking%20System%20%28SDS%29%2C%20the%20AUH%0Ais%20required%20to%20dock%20onto%20the%20SDS%20in%20a%20fixed%20orientation%20with%20specific%20attitude%0Aand%20altitude.%20Therefore%2C%20a%20particular%20criterion%20is%20proposed%20to%20determine%0Awhether%20the%20AUH%20has%20successfully%20docked%20onto%20the%20SDS.%20Furthermore%2C%20the%0Aeffectiveness%20and%20robustness%20of%20the%20proposed%20control%20method%20in%20AUH%27s%20docking%0Aprocedure%20are%20demonstrated%20through%20pool%20experiments%20and%20sea%20trials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06953v1&entry.124074799=Read"},
{"title": "A Trilogy of AI Safety Frameworks: Paths from Facts and Knowledge Gaps\n  to Reliable Predictions and New Knowledge", "author": "Simon Kasif", "abstract": "  AI Safety has become a vital front-line concern of many scientists within and\noutside the AI community. There are many immediate and long term anticipated\nrisks that range from existential risk to human existence to deep fakes and\nbias in machine learning systems [1-5]. In this paper, we reduce the full scope\nand immense complexity of AI safety concerns to a trilogy of three important\nbut tractable opportunities for advances that have the short-term potential to\nimprove AI safety and reliability without reducing AI innovation in critical\ndomains. In this perspective, we discuss this vision based on several case\nstudies that already produced proofs of concept in critical ML applications in\nbiomedical science.\n", "link": "http://arxiv.org/abs/2410.06946v1", "date": "2024-10-09", "relevancy": 1.3703, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Trilogy%20of%20AI%20Safety%20Frameworks%3A%20Paths%20from%20Facts%20and%20Knowledge%20Gaps%0A%20%20to%20Reliable%20Predictions%20and%20New%20Knowledge&body=Title%3A%20A%20Trilogy%20of%20AI%20Safety%20Frameworks%3A%20Paths%20from%20Facts%20and%20Knowledge%20Gaps%0A%20%20to%20Reliable%20Predictions%20and%20New%20Knowledge%0AAuthor%3A%20Simon%20Kasif%0AAbstract%3A%20%20%20AI%20Safety%20has%20become%20a%20vital%20front-line%20concern%20of%20many%20scientists%20within%20and%0Aoutside%20the%20AI%20community.%20There%20are%20many%20immediate%20and%20long%20term%20anticipated%0Arisks%20that%20range%20from%20existential%20risk%20to%20human%20existence%20to%20deep%20fakes%20and%0Abias%20in%20machine%20learning%20systems%20%5B1-5%5D.%20In%20this%20paper%2C%20we%20reduce%20the%20full%20scope%0Aand%20immense%20complexity%20of%20AI%20safety%20concerns%20to%20a%20trilogy%20of%20three%20important%0Abut%20tractable%20opportunities%20for%20advances%20that%20have%20the%20short-term%20potential%20to%0Aimprove%20AI%20safety%20and%20reliability%20without%20reducing%20AI%20innovation%20in%20critical%0Adomains.%20In%20this%20perspective%2C%20we%20discuss%20this%20vision%20based%20on%20several%20case%0Astudies%20that%20already%20produced%20proofs%20of%20concept%20in%20critical%20ML%20applications%20in%0Abiomedical%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Trilogy%2520of%2520AI%2520Safety%2520Frameworks%253A%2520Paths%2520from%2520Facts%2520and%2520Knowledge%2520Gaps%250A%2520%2520to%2520Reliable%2520Predictions%2520and%2520New%2520Knowledge%26entry.906535625%3DSimon%2520Kasif%26entry.1292438233%3D%2520%2520AI%2520Safety%2520has%2520become%2520a%2520vital%2520front-line%2520concern%2520of%2520many%2520scientists%2520within%2520and%250Aoutside%2520the%2520AI%2520community.%2520There%2520are%2520many%2520immediate%2520and%2520long%2520term%2520anticipated%250Arisks%2520that%2520range%2520from%2520existential%2520risk%2520to%2520human%2520existence%2520to%2520deep%2520fakes%2520and%250Abias%2520in%2520machine%2520learning%2520systems%2520%255B1-5%255D.%2520In%2520this%2520paper%252C%2520we%2520reduce%2520the%2520full%2520scope%250Aand%2520immense%2520complexity%2520of%2520AI%2520safety%2520concerns%2520to%2520a%2520trilogy%2520of%2520three%2520important%250Abut%2520tractable%2520opportunities%2520for%2520advances%2520that%2520have%2520the%2520short-term%2520potential%2520to%250Aimprove%2520AI%2520safety%2520and%2520reliability%2520without%2520reducing%2520AI%2520innovation%2520in%2520critical%250Adomains.%2520In%2520this%2520perspective%252C%2520we%2520discuss%2520this%2520vision%2520based%2520on%2520several%2520case%250Astudies%2520that%2520already%2520produced%2520proofs%2520of%2520concept%2520in%2520critical%2520ML%2520applications%2520in%250Abiomedical%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Trilogy%20of%20AI%20Safety%20Frameworks%3A%20Paths%20from%20Facts%20and%20Knowledge%20Gaps%0A%20%20to%20Reliable%20Predictions%20and%20New%20Knowledge&entry.906535625=Simon%20Kasif&entry.1292438233=%20%20AI%20Safety%20has%20become%20a%20vital%20front-line%20concern%20of%20many%20scientists%20within%20and%0Aoutside%20the%20AI%20community.%20There%20are%20many%20immediate%20and%20long%20term%20anticipated%0Arisks%20that%20range%20from%20existential%20risk%20to%20human%20existence%20to%20deep%20fakes%20and%0Abias%20in%20machine%20learning%20systems%20%5B1-5%5D.%20In%20this%20paper%2C%20we%20reduce%20the%20full%20scope%0Aand%20immense%20complexity%20of%20AI%20safety%20concerns%20to%20a%20trilogy%20of%20three%20important%0Abut%20tractable%20opportunities%20for%20advances%20that%20have%20the%20short-term%20potential%20to%0Aimprove%20AI%20safety%20and%20reliability%20without%20reducing%20AI%20innovation%20in%20critical%0Adomains.%20In%20this%20perspective%2C%20we%20discuss%20this%20vision%20based%20on%20several%20case%0Astudies%20that%20already%20produced%20proofs%20of%20concept%20in%20critical%20ML%20applications%20in%0Abiomedical%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06946v1&entry.124074799=Read"},
{"title": "Evaluating the Quality of Hallucination Benchmarks for Large\n  Vision-Language Models", "author": "Bei Yan and Jie Zhang and Zheng Yuan and Shiguang Shan and Xilin Chen", "abstract": "  Despite the rapid progress and outstanding performance of Large\nVision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the\nissue of hallucination, i.e., LVLMs tend to generate responses that are\ninconsistent with the corresponding visual inputs. To evaluate the degree of\nhallucination in LVLMs, previous works have proposed a series of benchmarks\nfeaturing different types of tasks and evaluation metrics. However, we find\nthat the quality of the existing hallucination benchmarks varies, with some\nsuffering from problems, e.g., inconsistent evaluation results under repeated\ntests, and misalignment with human evaluation. To this end, we propose a\nHallucination benchmark Quality Measurement framework (HQM), which leverages\nvarious indicators to assess the reliability and validity of existing\nhallucination benchmarks separately. Specifically, for reliability we explore\ntest-retest reliability and parallel-forms reliability, while for validity we\nexamine criterion validity and coverage of hallucination types. Furthermore,\nbased on the results of our quality measurement, we construct a High-Quality\nHallucination Benchmark (HQH) for LVLMs, which demonstrates superior\nreliability and validity under our HQM framework. We conduct an extensive\nevaluation of over 10 representative LVLMs, including GPT-4o and\nGemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in\nexisting models. Our benchmark is publicly available at\nhttps://github.com/HQHBench/HQHBench.\n", "link": "http://arxiv.org/abs/2406.17115v2", "date": "2024-10-09", "relevancy": 2.0174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Quality%20of%20Hallucination%20Benchmarks%20for%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20Evaluating%20the%20Quality%20of%20Hallucination%20Benchmarks%20for%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Bei%20Yan%20and%20Jie%20Zhang%20and%20Zheng%20Yuan%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20Despite%20the%20rapid%20progress%20and%20outstanding%20performance%20of%20Large%0AVision-Language%20Models%20%28LVLMs%29%20in%20recent%20years%2C%20LVLMs%20have%20been%20plagued%20by%20the%0Aissue%20of%20hallucination%2C%20i.e.%2C%20LVLMs%20tend%20to%20generate%20responses%20that%20are%0Ainconsistent%20with%20the%20corresponding%20visual%20inputs.%20To%20evaluate%20the%20degree%20of%0Ahallucination%20in%20LVLMs%2C%20previous%20works%20have%20proposed%20a%20series%20of%20benchmarks%0Afeaturing%20different%20types%20of%20tasks%20and%20evaluation%20metrics.%20However%2C%20we%20find%0Athat%20the%20quality%20of%20the%20existing%20hallucination%20benchmarks%20varies%2C%20with%20some%0Asuffering%20from%20problems%2C%20e.g.%2C%20inconsistent%20evaluation%20results%20under%20repeated%0Atests%2C%20and%20misalignment%20with%20human%20evaluation.%20To%20this%20end%2C%20we%20propose%20a%0AHallucination%20benchmark%20Quality%20Measurement%20framework%20%28HQM%29%2C%20which%20leverages%0Avarious%20indicators%20to%20assess%20the%20reliability%20and%20validity%20of%20existing%0Ahallucination%20benchmarks%20separately.%20Specifically%2C%20for%20reliability%20we%20explore%0Atest-retest%20reliability%20and%20parallel-forms%20reliability%2C%20while%20for%20validity%20we%0Aexamine%20criterion%20validity%20and%20coverage%20of%20hallucination%20types.%20Furthermore%2C%0Abased%20on%20the%20results%20of%20our%20quality%20measurement%2C%20we%20construct%20a%20High-Quality%0AHallucination%20Benchmark%20%28HQH%29%20for%20LVLMs%2C%20which%20demonstrates%20superior%0Areliability%20and%20validity%20under%20our%20HQM%20framework.%20We%20conduct%20an%20extensive%0Aevaluation%20of%20over%2010%20representative%20LVLMs%2C%20including%20GPT-4o%20and%0AGemini-1.5-Pro%2C%20to%20provide%20an%20in-depth%20analysis%20of%20the%20hallucination%20issues%20in%0Aexisting%20models.%20Our%20benchmark%20is%20publicly%20available%20at%0Ahttps%3A//github.com/HQHBench/HQHBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Quality%2520of%2520Hallucination%2520Benchmarks%2520for%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DBei%2520Yan%2520and%2520Jie%2520Zhang%2520and%2520Zheng%2520Yuan%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3D%2520%2520Despite%2520the%2520rapid%2520progress%2520and%2520outstanding%2520performance%2520of%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520in%2520recent%2520years%252C%2520LVLMs%2520have%2520been%2520plagued%2520by%2520the%250Aissue%2520of%2520hallucination%252C%2520i.e.%252C%2520LVLMs%2520tend%2520to%2520generate%2520responses%2520that%2520are%250Ainconsistent%2520with%2520the%2520corresponding%2520visual%2520inputs.%2520To%2520evaluate%2520the%2520degree%2520of%250Ahallucination%2520in%2520LVLMs%252C%2520previous%2520works%2520have%2520proposed%2520a%2520series%2520of%2520benchmarks%250Afeaturing%2520different%2520types%2520of%2520tasks%2520and%2520evaluation%2520metrics.%2520However%252C%2520we%2520find%250Athat%2520the%2520quality%2520of%2520the%2520existing%2520hallucination%2520benchmarks%2520varies%252C%2520with%2520some%250Asuffering%2520from%2520problems%252C%2520e.g.%252C%2520inconsistent%2520evaluation%2520results%2520under%2520repeated%250Atests%252C%2520and%2520misalignment%2520with%2520human%2520evaluation.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250AHallucination%2520benchmark%2520Quality%2520Measurement%2520framework%2520%2528HQM%2529%252C%2520which%2520leverages%250Avarious%2520indicators%2520to%2520assess%2520the%2520reliability%2520and%2520validity%2520of%2520existing%250Ahallucination%2520benchmarks%2520separately.%2520Specifically%252C%2520for%2520reliability%2520we%2520explore%250Atest-retest%2520reliability%2520and%2520parallel-forms%2520reliability%252C%2520while%2520for%2520validity%2520we%250Aexamine%2520criterion%2520validity%2520and%2520coverage%2520of%2520hallucination%2520types.%2520Furthermore%252C%250Abased%2520on%2520the%2520results%2520of%2520our%2520quality%2520measurement%252C%2520we%2520construct%2520a%2520High-Quality%250AHallucination%2520Benchmark%2520%2528HQH%2529%2520for%2520LVLMs%252C%2520which%2520demonstrates%2520superior%250Areliability%2520and%2520validity%2520under%2520our%2520HQM%2520framework.%2520We%2520conduct%2520an%2520extensive%250Aevaluation%2520of%2520over%252010%2520representative%2520LVLMs%252C%2520including%2520GPT-4o%2520and%250AGemini-1.5-Pro%252C%2520to%2520provide%2520an%2520in-depth%2520analysis%2520of%2520the%2520hallucination%2520issues%2520in%250Aexisting%2520models.%2520Our%2520benchmark%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/HQHBench/HQHBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Quality%20of%20Hallucination%20Benchmarks%20for%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Bei%20Yan%20and%20Jie%20Zhang%20and%20Zheng%20Yuan%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=%20%20Despite%20the%20rapid%20progress%20and%20outstanding%20performance%20of%20Large%0AVision-Language%20Models%20%28LVLMs%29%20in%20recent%20years%2C%20LVLMs%20have%20been%20plagued%20by%20the%0Aissue%20of%20hallucination%2C%20i.e.%2C%20LVLMs%20tend%20to%20generate%20responses%20that%20are%0Ainconsistent%20with%20the%20corresponding%20visual%20inputs.%20To%20evaluate%20the%20degree%20of%0Ahallucination%20in%20LVLMs%2C%20previous%20works%20have%20proposed%20a%20series%20of%20benchmarks%0Afeaturing%20different%20types%20of%20tasks%20and%20evaluation%20metrics.%20However%2C%20we%20find%0Athat%20the%20quality%20of%20the%20existing%20hallucination%20benchmarks%20varies%2C%20with%20some%0Asuffering%20from%20problems%2C%20e.g.%2C%20inconsistent%20evaluation%20results%20under%20repeated%0Atests%2C%20and%20misalignment%20with%20human%20evaluation.%20To%20this%20end%2C%20we%20propose%20a%0AHallucination%20benchmark%20Quality%20Measurement%20framework%20%28HQM%29%2C%20which%20leverages%0Avarious%20indicators%20to%20assess%20the%20reliability%20and%20validity%20of%20existing%0Ahallucination%20benchmarks%20separately.%20Specifically%2C%20for%20reliability%20we%20explore%0Atest-retest%20reliability%20and%20parallel-forms%20reliability%2C%20while%20for%20validity%20we%0Aexamine%20criterion%20validity%20and%20coverage%20of%20hallucination%20types.%20Furthermore%2C%0Abased%20on%20the%20results%20of%20our%20quality%20measurement%2C%20we%20construct%20a%20High-Quality%0AHallucination%20Benchmark%20%28HQH%29%20for%20LVLMs%2C%20which%20demonstrates%20superior%0Areliability%20and%20validity%20under%20our%20HQM%20framework.%20We%20conduct%20an%20extensive%0Aevaluation%20of%20over%2010%20representative%20LVLMs%2C%20including%20GPT-4o%20and%0AGemini-1.5-Pro%2C%20to%20provide%20an%20in-depth%20analysis%20of%20the%20hallucination%20issues%20in%0Aexisting%20models.%20Our%20benchmark%20is%20publicly%20available%20at%0Ahttps%3A//github.com/HQHBench/HQHBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17115v2&entry.124074799=Read"},
{"title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection", "author": "Cristian Gutierrez", "abstract": "  This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems.\n  We release code, models, and dataset at https://github.com/ggcr/TinyEmo\n", "link": "http://arxiv.org/abs/2410.07062v1", "date": "2024-10-09", "relevancy": 1.4906, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4969}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TinyEmo%3A%20Scaling%20down%20Emotional%20Reasoning%20via%20Metric%20Projection&body=Title%3A%20TinyEmo%3A%20Scaling%20down%20Emotional%20Reasoning%20via%20Metric%20Projection%0AAuthor%3A%20Cristian%20Gutierrez%0AAbstract%3A%20%20%20This%20paper%20introduces%20TinyEmo%2C%20a%20family%20of%20small%20multi-modal%20language%20models%0Afor%20emotional%20reasoning%20and%20classification.%20Our%20approach%20features%3A%20%281%29%20a%0Asynthetic%20emotional%20instruct%20dataset%20for%20both%20pre-training%20and%20fine-tuning%0Astages%2C%20%282%29%20a%20Metric%20Projector%20that%20delegates%20classification%20from%20the%20language%0Amodel%20allowing%20for%20more%20efficient%20training%20and%20inference%2C%20%283%29%20a%20multi-modal%0Alarge%20language%20model%20%28MM-LLM%29%20for%20emotional%20reasoning%2C%20and%20%284%29%20a%20semi-automated%0Aframework%20for%20bias%20detection.%20TinyEmo%20is%20able%20to%20perform%20emotion%20classification%0Aand%20emotional%20reasoning%2C%20all%20while%20using%20substantially%20fewer%20parameters%20than%0Acomparable%20models.%20This%20efficiency%20allows%20us%20to%20freely%20incorporate%20more%20diverse%0Aemotional%20datasets%2C%20enabling%20strong%20performance%20on%20classification%20tasks%2C%20with%0Aour%20smallest%20model%20%28700M%20parameters%29%20outperforming%20larger%20state-of-the-art%0Amodels%20based%20on%20general-purpose%20MM-LLMs%20with%20over%207B%20parameters.%20Additionally%2C%0Athe%20Metric%20Projector%20allows%20for%20interpretability%20and%20indirect%20bias%20detection%20in%0Alarge%20models%20without%20additional%20training%2C%20offering%20an%20approach%20to%20understand%0Aand%20improve%20AI%20systems.%0A%20%20We%20release%20code%2C%20models%2C%20and%20dataset%20at%20https%3A//github.com/ggcr/TinyEmo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinyEmo%253A%2520Scaling%2520down%2520Emotional%2520Reasoning%2520via%2520Metric%2520Projection%26entry.906535625%3DCristian%2520Gutierrez%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520TinyEmo%252C%2520a%2520family%2520of%2520small%2520multi-modal%2520language%2520models%250Afor%2520emotional%2520reasoning%2520and%2520classification.%2520Our%2520approach%2520features%253A%2520%25281%2529%2520a%250Asynthetic%2520emotional%2520instruct%2520dataset%2520for%2520both%2520pre-training%2520and%2520fine-tuning%250Astages%252C%2520%25282%2529%2520a%2520Metric%2520Projector%2520that%2520delegates%2520classification%2520from%2520the%2520language%250Amodel%2520allowing%2520for%2520more%2520efficient%2520training%2520and%2520inference%252C%2520%25283%2529%2520a%2520multi-modal%250Alarge%2520language%2520model%2520%2528MM-LLM%2529%2520for%2520emotional%2520reasoning%252C%2520and%2520%25284%2529%2520a%2520semi-automated%250Aframework%2520for%2520bias%2520detection.%2520TinyEmo%2520is%2520able%2520to%2520perform%2520emotion%2520classification%250Aand%2520emotional%2520reasoning%252C%2520all%2520while%2520using%2520substantially%2520fewer%2520parameters%2520than%250Acomparable%2520models.%2520This%2520efficiency%2520allows%2520us%2520to%2520freely%2520incorporate%2520more%2520diverse%250Aemotional%2520datasets%252C%2520enabling%2520strong%2520performance%2520on%2520classification%2520tasks%252C%2520with%250Aour%2520smallest%2520model%2520%2528700M%2520parameters%2529%2520outperforming%2520larger%2520state-of-the-art%250Amodels%2520based%2520on%2520general-purpose%2520MM-LLMs%2520with%2520over%25207B%2520parameters.%2520Additionally%252C%250Athe%2520Metric%2520Projector%2520allows%2520for%2520interpretability%2520and%2520indirect%2520bias%2520detection%2520in%250Alarge%2520models%2520without%2520additional%2520training%252C%2520offering%2520an%2520approach%2520to%2520understand%250Aand%2520improve%2520AI%2520systems.%250A%2520%2520We%2520release%2520code%252C%2520models%252C%2520and%2520dataset%2520at%2520https%253A//github.com/ggcr/TinyEmo%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinyEmo%3A%20Scaling%20down%20Emotional%20Reasoning%20via%20Metric%20Projection&entry.906535625=Cristian%20Gutierrez&entry.1292438233=%20%20This%20paper%20introduces%20TinyEmo%2C%20a%20family%20of%20small%20multi-modal%20language%20models%0Afor%20emotional%20reasoning%20and%20classification.%20Our%20approach%20features%3A%20%281%29%20a%0Asynthetic%20emotional%20instruct%20dataset%20for%20both%20pre-training%20and%20fine-tuning%0Astages%2C%20%282%29%20a%20Metric%20Projector%20that%20delegates%20classification%20from%20the%20language%0Amodel%20allowing%20for%20more%20efficient%20training%20and%20inference%2C%20%283%29%20a%20multi-modal%0Alarge%20language%20model%20%28MM-LLM%29%20for%20emotional%20reasoning%2C%20and%20%284%29%20a%20semi-automated%0Aframework%20for%20bias%20detection.%20TinyEmo%20is%20able%20to%20perform%20emotion%20classification%0Aand%20emotional%20reasoning%2C%20all%20while%20using%20substantially%20fewer%20parameters%20than%0Acomparable%20models.%20This%20efficiency%20allows%20us%20to%20freely%20incorporate%20more%20diverse%0Aemotional%20datasets%2C%20enabling%20strong%20performance%20on%20classification%20tasks%2C%20with%0Aour%20smallest%20model%20%28700M%20parameters%29%20outperforming%20larger%20state-of-the-art%0Amodels%20based%20on%20general-purpose%20MM-LLMs%20with%20over%207B%20parameters.%20Additionally%2C%0Athe%20Metric%20Projector%20allows%20for%20interpretability%20and%20indirect%20bias%20detection%20in%0Alarge%20models%20without%20additional%20training%2C%20offering%20an%20approach%20to%20understand%0Aand%20improve%20AI%20systems.%0A%20%20We%20release%20code%2C%20models%2C%20and%20dataset%20at%20https%3A//github.com/ggcr/TinyEmo%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07062v1&entry.124074799=Read"},
{"title": "Efficient Weight-Space Laplace-Gaussian Filtering and Smoothing for\n  Sequential Deep Learning", "author": "Joanna Sliwa and Frank Schneider and Nathanael Bosch and Agustinus Kristiadi and Philipp Hennig", "abstract": "  Efficiently learning a sequence of related tasks, such as in continual\nlearning, poses a significant challenge for neural nets due to the delicate\ntrade-off between catastrophic forgetting and loss of plasticity. We address\nthis challenge with a grounded framework for sequentially learning related\ntasks based on Bayesian inference. Specifically, we treat the model's\nparameters as a nonlinear Gaussian state-space model and perform efficient\ninference using Gaussian filtering and smoothing. This general formalism\nsubsumes existing continual learning approaches, while also offering a clearer\nconceptual understanding of its components. Leveraging Laplace approximations\nduring filtering, we construct Gaussian posterior measures on the weight space\nof a neural network for each task. We use it as an efficient regularizer by\nexploiting the structure of the generalized Gauss-Newton matrix (GGN) to\nconstruct diagonal plus low-rank approximations. The dynamics model allows\ntargeted control of the learning process and the incorporation of\ndomain-specific knowledge, such as modeling the type of shift between tasks.\nAdditionally, using Bayesian approximate smoothing can enhance the performance\nof task-specific models without needing to re-access any data.\n", "link": "http://arxiv.org/abs/2410.06800v1", "date": "2024-10-09", "relevancy": 1.5912, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5348}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Weight-Space%20Laplace-Gaussian%20Filtering%20and%20Smoothing%20for%0A%20%20Sequential%20Deep%20Learning&body=Title%3A%20Efficient%20Weight-Space%20Laplace-Gaussian%20Filtering%20and%20Smoothing%20for%0A%20%20Sequential%20Deep%20Learning%0AAuthor%3A%20Joanna%20Sliwa%20and%20Frank%20Schneider%20and%20Nathanael%20Bosch%20and%20Agustinus%20Kristiadi%20and%20Philipp%20Hennig%0AAbstract%3A%20%20%20Efficiently%20learning%20a%20sequence%20of%20related%20tasks%2C%20such%20as%20in%20continual%0Alearning%2C%20poses%20a%20significant%20challenge%20for%20neural%20nets%20due%20to%20the%20delicate%0Atrade-off%20between%20catastrophic%20forgetting%20and%20loss%20of%20plasticity.%20We%20address%0Athis%20challenge%20with%20a%20grounded%20framework%20for%20sequentially%20learning%20related%0Atasks%20based%20on%20Bayesian%20inference.%20Specifically%2C%20we%20treat%20the%20model%27s%0Aparameters%20as%20a%20nonlinear%20Gaussian%20state-space%20model%20and%20perform%20efficient%0Ainference%20using%20Gaussian%20filtering%20and%20smoothing.%20This%20general%20formalism%0Asubsumes%20existing%20continual%20learning%20approaches%2C%20while%20also%20offering%20a%20clearer%0Aconceptual%20understanding%20of%20its%20components.%20Leveraging%20Laplace%20approximations%0Aduring%20filtering%2C%20we%20construct%20Gaussian%20posterior%20measures%20on%20the%20weight%20space%0Aof%20a%20neural%20network%20for%20each%20task.%20We%20use%20it%20as%20an%20efficient%20regularizer%20by%0Aexploiting%20the%20structure%20of%20the%20generalized%20Gauss-Newton%20matrix%20%28GGN%29%20to%0Aconstruct%20diagonal%20plus%20low-rank%20approximations.%20The%20dynamics%20model%20allows%0Atargeted%20control%20of%20the%20learning%20process%20and%20the%20incorporation%20of%0Adomain-specific%20knowledge%2C%20such%20as%20modeling%20the%20type%20of%20shift%20between%20tasks.%0AAdditionally%2C%20using%20Bayesian%20approximate%20smoothing%20can%20enhance%20the%20performance%0Aof%20task-specific%20models%20without%20needing%20to%20re-access%20any%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Weight-Space%2520Laplace-Gaussian%2520Filtering%2520and%2520Smoothing%2520for%250A%2520%2520Sequential%2520Deep%2520Learning%26entry.906535625%3DJoanna%2520Sliwa%2520and%2520Frank%2520Schneider%2520and%2520Nathanael%2520Bosch%2520and%2520Agustinus%2520Kristiadi%2520and%2520Philipp%2520Hennig%26entry.1292438233%3D%2520%2520Efficiently%2520learning%2520a%2520sequence%2520of%2520related%2520tasks%252C%2520such%2520as%2520in%2520continual%250Alearning%252C%2520poses%2520a%2520significant%2520challenge%2520for%2520neural%2520nets%2520due%2520to%2520the%2520delicate%250Atrade-off%2520between%2520catastrophic%2520forgetting%2520and%2520loss%2520of%2520plasticity.%2520We%2520address%250Athis%2520challenge%2520with%2520a%2520grounded%2520framework%2520for%2520sequentially%2520learning%2520related%250Atasks%2520based%2520on%2520Bayesian%2520inference.%2520Specifically%252C%2520we%2520treat%2520the%2520model%2527s%250Aparameters%2520as%2520a%2520nonlinear%2520Gaussian%2520state-space%2520model%2520and%2520perform%2520efficient%250Ainference%2520using%2520Gaussian%2520filtering%2520and%2520smoothing.%2520This%2520general%2520formalism%250Asubsumes%2520existing%2520continual%2520learning%2520approaches%252C%2520while%2520also%2520offering%2520a%2520clearer%250Aconceptual%2520understanding%2520of%2520its%2520components.%2520Leveraging%2520Laplace%2520approximations%250Aduring%2520filtering%252C%2520we%2520construct%2520Gaussian%2520posterior%2520measures%2520on%2520the%2520weight%2520space%250Aof%2520a%2520neural%2520network%2520for%2520each%2520task.%2520We%2520use%2520it%2520as%2520an%2520efficient%2520regularizer%2520by%250Aexploiting%2520the%2520structure%2520of%2520the%2520generalized%2520Gauss-Newton%2520matrix%2520%2528GGN%2529%2520to%250Aconstruct%2520diagonal%2520plus%2520low-rank%2520approximations.%2520The%2520dynamics%2520model%2520allows%250Atargeted%2520control%2520of%2520the%2520learning%2520process%2520and%2520the%2520incorporation%2520of%250Adomain-specific%2520knowledge%252C%2520such%2520as%2520modeling%2520the%2520type%2520of%2520shift%2520between%2520tasks.%250AAdditionally%252C%2520using%2520Bayesian%2520approximate%2520smoothing%2520can%2520enhance%2520the%2520performance%250Aof%2520task-specific%2520models%2520without%2520needing%2520to%2520re-access%2520any%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Weight-Space%20Laplace-Gaussian%20Filtering%20and%20Smoothing%20for%0A%20%20Sequential%20Deep%20Learning&entry.906535625=Joanna%20Sliwa%20and%20Frank%20Schneider%20and%20Nathanael%20Bosch%20and%20Agustinus%20Kristiadi%20and%20Philipp%20Hennig&entry.1292438233=%20%20Efficiently%20learning%20a%20sequence%20of%20related%20tasks%2C%20such%20as%20in%20continual%0Alearning%2C%20poses%20a%20significant%20challenge%20for%20neural%20nets%20due%20to%20the%20delicate%0Atrade-off%20between%20catastrophic%20forgetting%20and%20loss%20of%20plasticity.%20We%20address%0Athis%20challenge%20with%20a%20grounded%20framework%20for%20sequentially%20learning%20related%0Atasks%20based%20on%20Bayesian%20inference.%20Specifically%2C%20we%20treat%20the%20model%27s%0Aparameters%20as%20a%20nonlinear%20Gaussian%20state-space%20model%20and%20perform%20efficient%0Ainference%20using%20Gaussian%20filtering%20and%20smoothing.%20This%20general%20formalism%0Asubsumes%20existing%20continual%20learning%20approaches%2C%20while%20also%20offering%20a%20clearer%0Aconceptual%20understanding%20of%20its%20components.%20Leveraging%20Laplace%20approximations%0Aduring%20filtering%2C%20we%20construct%20Gaussian%20posterior%20measures%20on%20the%20weight%20space%0Aof%20a%20neural%20network%20for%20each%20task.%20We%20use%20it%20as%20an%20efficient%20regularizer%20by%0Aexploiting%20the%20structure%20of%20the%20generalized%20Gauss-Newton%20matrix%20%28GGN%29%20to%0Aconstruct%20diagonal%20plus%20low-rank%20approximations.%20The%20dynamics%20model%20allows%0Atargeted%20control%20of%20the%20learning%20process%20and%20the%20incorporation%20of%0Adomain-specific%20knowledge%2C%20such%20as%20modeling%20the%20type%20of%20shift%20between%20tasks.%0AAdditionally%2C%20using%20Bayesian%20approximate%20smoothing%20can%20enhance%20the%20performance%0Aof%20task-specific%20models%20without%20needing%20to%20re-access%20any%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06800v1&entry.124074799=Read"},
{"title": "Adversarial Vulnerability as a Consequence of On-Manifold Inseparibility", "author": "Rajdeep Haldar and Yue Xing and Qifan Song and Guang Lin", "abstract": "  Recent works have shown theoretically and empirically that redundant data\ndimensions are a source of adversarial vulnerability. However, the inverse\ndoesn't seem to hold in practice; employing dimension-reduction techniques\ndoesn't exhibit robustness as expected. In this work, we consider\nclassification tasks and characterize the data distribution as a\nlow-dimensional manifold, with high/low variance features defining the on/off\nmanifold direction. We argue that clean training experiences poor convergence\nin the off-manifold direction caused by the ill-conditioning in widely used\nfirst-order optimizers like gradient descent. The poor convergence then acts as\na source of adversarial vulnerability when the dataset is inseparable in the\non-manifold direction. We provide theoretical results for logistic regression\nand a 2-layer linear network on the considered data distribution. Furthermore,\nwe advocate using second-order methods that are immune to ill-conditioning and\nlead to better robustness. We perform experiments and exhibit tremendous\nrobustness improvements in clean training through long training and the\nemployment of second-order methods, corroborating our framework. Additionally,\nwe find the inclusion of batch-norm layers hinders such robustness gains. We\nattribute this to differing implicit biases between traditional and\nbatch-normalized neural networks.\n", "link": "http://arxiv.org/abs/2410.06921v1", "date": "2024-10-09", "relevancy": 1.9032, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4894}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4703}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Vulnerability%20as%20a%20Consequence%20of%20On-Manifold%20Inseparibility&body=Title%3A%20Adversarial%20Vulnerability%20as%20a%20Consequence%20of%20On-Manifold%20Inseparibility%0AAuthor%3A%20Rajdeep%20Haldar%20and%20Yue%20Xing%20and%20Qifan%20Song%20and%20Guang%20Lin%0AAbstract%3A%20%20%20Recent%20works%20have%20shown%20theoretically%20and%20empirically%20that%20redundant%20data%0Adimensions%20are%20a%20source%20of%20adversarial%20vulnerability.%20However%2C%20the%20inverse%0Adoesn%27t%20seem%20to%20hold%20in%20practice%3B%20employing%20dimension-reduction%20techniques%0Adoesn%27t%20exhibit%20robustness%20as%20expected.%20In%20this%20work%2C%20we%20consider%0Aclassification%20tasks%20and%20characterize%20the%20data%20distribution%20as%20a%0Alow-dimensional%20manifold%2C%20with%20high/low%20variance%20features%20defining%20the%20on/off%0Amanifold%20direction.%20We%20argue%20that%20clean%20training%20experiences%20poor%20convergence%0Ain%20the%20off-manifold%20direction%20caused%20by%20the%20ill-conditioning%20in%20widely%20used%0Afirst-order%20optimizers%20like%20gradient%20descent.%20The%20poor%20convergence%20then%20acts%20as%0Aa%20source%20of%20adversarial%20vulnerability%20when%20the%20dataset%20is%20inseparable%20in%20the%0Aon-manifold%20direction.%20We%20provide%20theoretical%20results%20for%20logistic%20regression%0Aand%20a%202-layer%20linear%20network%20on%20the%20considered%20data%20distribution.%20Furthermore%2C%0Awe%20advocate%20using%20second-order%20methods%20that%20are%20immune%20to%20ill-conditioning%20and%0Alead%20to%20better%20robustness.%20We%20perform%20experiments%20and%20exhibit%20tremendous%0Arobustness%20improvements%20in%20clean%20training%20through%20long%20training%20and%20the%0Aemployment%20of%20second-order%20methods%2C%20corroborating%20our%20framework.%20Additionally%2C%0Awe%20find%20the%20inclusion%20of%20batch-norm%20layers%20hinders%20such%20robustness%20gains.%20We%0Aattribute%20this%20to%20differing%20implicit%20biases%20between%20traditional%20and%0Abatch-normalized%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Vulnerability%2520as%2520a%2520Consequence%2520of%2520On-Manifold%2520Inseparibility%26entry.906535625%3DRajdeep%2520Haldar%2520and%2520Yue%2520Xing%2520and%2520Qifan%2520Song%2520and%2520Guang%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520shown%2520theoretically%2520and%2520empirically%2520that%2520redundant%2520data%250Adimensions%2520are%2520a%2520source%2520of%2520adversarial%2520vulnerability.%2520However%252C%2520the%2520inverse%250Adoesn%2527t%2520seem%2520to%2520hold%2520in%2520practice%253B%2520employing%2520dimension-reduction%2520techniques%250Adoesn%2527t%2520exhibit%2520robustness%2520as%2520expected.%2520In%2520this%2520work%252C%2520we%2520consider%250Aclassification%2520tasks%2520and%2520characterize%2520the%2520data%2520distribution%2520as%2520a%250Alow-dimensional%2520manifold%252C%2520with%2520high/low%2520variance%2520features%2520defining%2520the%2520on/off%250Amanifold%2520direction.%2520We%2520argue%2520that%2520clean%2520training%2520experiences%2520poor%2520convergence%250Ain%2520the%2520off-manifold%2520direction%2520caused%2520by%2520the%2520ill-conditioning%2520in%2520widely%2520used%250Afirst-order%2520optimizers%2520like%2520gradient%2520descent.%2520The%2520poor%2520convergence%2520then%2520acts%2520as%250Aa%2520source%2520of%2520adversarial%2520vulnerability%2520when%2520the%2520dataset%2520is%2520inseparable%2520in%2520the%250Aon-manifold%2520direction.%2520We%2520provide%2520theoretical%2520results%2520for%2520logistic%2520regression%250Aand%2520a%25202-layer%2520linear%2520network%2520on%2520the%2520considered%2520data%2520distribution.%2520Furthermore%252C%250Awe%2520advocate%2520using%2520second-order%2520methods%2520that%2520are%2520immune%2520to%2520ill-conditioning%2520and%250Alead%2520to%2520better%2520robustness.%2520We%2520perform%2520experiments%2520and%2520exhibit%2520tremendous%250Arobustness%2520improvements%2520in%2520clean%2520training%2520through%2520long%2520training%2520and%2520the%250Aemployment%2520of%2520second-order%2520methods%252C%2520corroborating%2520our%2520framework.%2520Additionally%252C%250Awe%2520find%2520the%2520inclusion%2520of%2520batch-norm%2520layers%2520hinders%2520such%2520robustness%2520gains.%2520We%250Aattribute%2520this%2520to%2520differing%2520implicit%2520biases%2520between%2520traditional%2520and%250Abatch-normalized%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Vulnerability%20as%20a%20Consequence%20of%20On-Manifold%20Inseparibility&entry.906535625=Rajdeep%20Haldar%20and%20Yue%20Xing%20and%20Qifan%20Song%20and%20Guang%20Lin&entry.1292438233=%20%20Recent%20works%20have%20shown%20theoretically%20and%20empirically%20that%20redundant%20data%0Adimensions%20are%20a%20source%20of%20adversarial%20vulnerability.%20However%2C%20the%20inverse%0Adoesn%27t%20seem%20to%20hold%20in%20practice%3B%20employing%20dimension-reduction%20techniques%0Adoesn%27t%20exhibit%20robustness%20as%20expected.%20In%20this%20work%2C%20we%20consider%0Aclassification%20tasks%20and%20characterize%20the%20data%20distribution%20as%20a%0Alow-dimensional%20manifold%2C%20with%20high/low%20variance%20features%20defining%20the%20on/off%0Amanifold%20direction.%20We%20argue%20that%20clean%20training%20experiences%20poor%20convergence%0Ain%20the%20off-manifold%20direction%20caused%20by%20the%20ill-conditioning%20in%20widely%20used%0Afirst-order%20optimizers%20like%20gradient%20descent.%20The%20poor%20convergence%20then%20acts%20as%0Aa%20source%20of%20adversarial%20vulnerability%20when%20the%20dataset%20is%20inseparable%20in%20the%0Aon-manifold%20direction.%20We%20provide%20theoretical%20results%20for%20logistic%20regression%0Aand%20a%202-layer%20linear%20network%20on%20the%20considered%20data%20distribution.%20Furthermore%2C%0Awe%20advocate%20using%20second-order%20methods%20that%20are%20immune%20to%20ill-conditioning%20and%0Alead%20to%20better%20robustness.%20We%20perform%20experiments%20and%20exhibit%20tremendous%0Arobustness%20improvements%20in%20clean%20training%20through%20long%20training%20and%20the%0Aemployment%20of%20second-order%20methods%2C%20corroborating%20our%20framework.%20Additionally%2C%0Awe%20find%20the%20inclusion%20of%20batch-norm%20layers%20hinders%20such%20robustness%20gains.%20We%0Aattribute%20this%20to%20differing%20implicit%20biases%20between%20traditional%20and%0Abatch-normalized%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06921v1&entry.124074799=Read"},
{"title": "Robust Regression over Averaged Uncertainty", "author": "Dimitris Bertsimas and Yu Ma", "abstract": "  We propose a new formulation of robust regression by integrating all\nrealizations of the uncertainty set and taking an averaged approach to obtain\nthe optimal solution for the ordinary least squares regression problem. We show\nthat this formulation recovers ridge regression exactly and establishes the\nmissing link between robust optimization and the mean squared error approaches\nfor existing regression problems. We further demonstrate that the condition of\nthis equivalence relies on the geometric properties of the defined uncertainty\nset. We provide exact, closed-form, in some cases, analytical solutions to the\nequivalent regularization strength under uncertainty sets induced by $\\ell_p$\nnorm, Schatten $p$-norm, and general polytopes. We then show in synthetic\ndatasets with different levels of uncertainties, a consistent improvement of\nthe averaged formulation over the existing worst-case formulation in\nout-of-sample performance. In real-world regression problems obtained from UCI\ndatasets, similar improvements are seen in the out-of-sample datasets.\n", "link": "http://arxiv.org/abs/2311.06960v2", "date": "2024-10-09", "relevancy": 1.473, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4994}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Regression%20over%20Averaged%20Uncertainty&body=Title%3A%20Robust%20Regression%20over%20Averaged%20Uncertainty%0AAuthor%3A%20Dimitris%20Bertsimas%20and%20Yu%20Ma%0AAbstract%3A%20%20%20We%20propose%20a%20new%20formulation%20of%20robust%20regression%20by%20integrating%20all%0Arealizations%20of%20the%20uncertainty%20set%20and%20taking%20an%20averaged%20approach%20to%20obtain%0Athe%20optimal%20solution%20for%20the%20ordinary%20least%20squares%20regression%20problem.%20We%20show%0Athat%20this%20formulation%20recovers%20ridge%20regression%20exactly%20and%20establishes%20the%0Amissing%20link%20between%20robust%20optimization%20and%20the%20mean%20squared%20error%20approaches%0Afor%20existing%20regression%20problems.%20We%20further%20demonstrate%20that%20the%20condition%20of%0Athis%20equivalence%20relies%20on%20the%20geometric%20properties%20of%20the%20defined%20uncertainty%0Aset.%20We%20provide%20exact%2C%20closed-form%2C%20in%20some%20cases%2C%20analytical%20solutions%20to%20the%0Aequivalent%20regularization%20strength%20under%20uncertainty%20sets%20induced%20by%20%24%5Cell_p%24%0Anorm%2C%20Schatten%20%24p%24-norm%2C%20and%20general%20polytopes.%20We%20then%20show%20in%20synthetic%0Adatasets%20with%20different%20levels%20of%20uncertainties%2C%20a%20consistent%20improvement%20of%0Athe%20averaged%20formulation%20over%20the%20existing%20worst-case%20formulation%20in%0Aout-of-sample%20performance.%20In%20real-world%20regression%20problems%20obtained%20from%20UCI%0Adatasets%2C%20similar%20improvements%20are%20seen%20in%20the%20out-of-sample%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Regression%2520over%2520Averaged%2520Uncertainty%26entry.906535625%3DDimitris%2520Bertsimas%2520and%2520Yu%2520Ma%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520formulation%2520of%2520robust%2520regression%2520by%2520integrating%2520all%250Arealizations%2520of%2520the%2520uncertainty%2520set%2520and%2520taking%2520an%2520averaged%2520approach%2520to%2520obtain%250Athe%2520optimal%2520solution%2520for%2520the%2520ordinary%2520least%2520squares%2520regression%2520problem.%2520We%2520show%250Athat%2520this%2520formulation%2520recovers%2520ridge%2520regression%2520exactly%2520and%2520establishes%2520the%250Amissing%2520link%2520between%2520robust%2520optimization%2520and%2520the%2520mean%2520squared%2520error%2520approaches%250Afor%2520existing%2520regression%2520problems.%2520We%2520further%2520demonstrate%2520that%2520the%2520condition%2520of%250Athis%2520equivalence%2520relies%2520on%2520the%2520geometric%2520properties%2520of%2520the%2520defined%2520uncertainty%250Aset.%2520We%2520provide%2520exact%252C%2520closed-form%252C%2520in%2520some%2520cases%252C%2520analytical%2520solutions%2520to%2520the%250Aequivalent%2520regularization%2520strength%2520under%2520uncertainty%2520sets%2520induced%2520by%2520%2524%255Cell_p%2524%250Anorm%252C%2520Schatten%2520%2524p%2524-norm%252C%2520and%2520general%2520polytopes.%2520We%2520then%2520show%2520in%2520synthetic%250Adatasets%2520with%2520different%2520levels%2520of%2520uncertainties%252C%2520a%2520consistent%2520improvement%2520of%250Athe%2520averaged%2520formulation%2520over%2520the%2520existing%2520worst-case%2520formulation%2520in%250Aout-of-sample%2520performance.%2520In%2520real-world%2520regression%2520problems%2520obtained%2520from%2520UCI%250Adatasets%252C%2520similar%2520improvements%2520are%2520seen%2520in%2520the%2520out-of-sample%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Regression%20over%20Averaged%20Uncertainty&entry.906535625=Dimitris%20Bertsimas%20and%20Yu%20Ma&entry.1292438233=%20%20We%20propose%20a%20new%20formulation%20of%20robust%20regression%20by%20integrating%20all%0Arealizations%20of%20the%20uncertainty%20set%20and%20taking%20an%20averaged%20approach%20to%20obtain%0Athe%20optimal%20solution%20for%20the%20ordinary%20least%20squares%20regression%20problem.%20We%20show%0Athat%20this%20formulation%20recovers%20ridge%20regression%20exactly%20and%20establishes%20the%0Amissing%20link%20between%20robust%20optimization%20and%20the%20mean%20squared%20error%20approaches%0Afor%20existing%20regression%20problems.%20We%20further%20demonstrate%20that%20the%20condition%20of%0Athis%20equivalence%20relies%20on%20the%20geometric%20properties%20of%20the%20defined%20uncertainty%0Aset.%20We%20provide%20exact%2C%20closed-form%2C%20in%20some%20cases%2C%20analytical%20solutions%20to%20the%0Aequivalent%20regularization%20strength%20under%20uncertainty%20sets%20induced%20by%20%24%5Cell_p%24%0Anorm%2C%20Schatten%20%24p%24-norm%2C%20and%20general%20polytopes.%20We%20then%20show%20in%20synthetic%0Adatasets%20with%20different%20levels%20of%20uncertainties%2C%20a%20consistent%20improvement%20of%0Athe%20averaged%20formulation%20over%20the%20existing%20worst-case%20formulation%20in%0Aout-of-sample%20performance.%20In%20real-world%20regression%20problems%20obtained%20from%20UCI%0Adatasets%2C%20similar%20improvements%20are%20seen%20in%20the%20out-of-sample%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06960v2&entry.124074799=Read"},
{"title": "Jointly Generating Multi-view Consistent PBR Textures using\n  Collaborative Control", "author": "Shimon Vainer and Konstantin Kutsy and Dante De Nigris and Ciara Rowles and Slava Elizarov and Simon Donn\u00e9", "abstract": "  Multi-view consistency remains a challenge for image diffusion models. Even\nwithin the Text-to-Texture problem, where perfect geometric correspondences are\nknown a priori, many methods fail to yield aligned predictions across views,\nnecessitating non-trivial fusion methods to incorporate the results onto the\noriginal mesh. We explore this issue for a Collaborative Control workflow\nspecifically in PBR Text-to-Texture. Collaborative Control directly models PBR\nimage probability distributions, including normal bump maps; to our knowledge,\nthe only diffusion model to directly output full PBR stacks. We discuss the\ndesign decisions involved in making this model multi-view consistent, and\ndemonstrate the effectiveness of our approach in ablation studies, as well as\npractical applications.\n", "link": "http://arxiv.org/abs/2410.06985v1", "date": "2024-10-09", "relevancy": 1.813, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6216}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5876}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jointly%20Generating%20Multi-view%20Consistent%20PBR%20Textures%20using%0A%20%20Collaborative%20Control&body=Title%3A%20Jointly%20Generating%20Multi-view%20Consistent%20PBR%20Textures%20using%0A%20%20Collaborative%20Control%0AAuthor%3A%20Shimon%20Vainer%20and%20Konstantin%20Kutsy%20and%20Dante%20De%20Nigris%20and%20Ciara%20Rowles%20and%20Slava%20Elizarov%20and%20Simon%20Donn%C3%A9%0AAbstract%3A%20%20%20Multi-view%20consistency%20remains%20a%20challenge%20for%20image%20diffusion%20models.%20Even%0Awithin%20the%20Text-to-Texture%20problem%2C%20where%20perfect%20geometric%20correspondences%20are%0Aknown%20a%20priori%2C%20many%20methods%20fail%20to%20yield%20aligned%20predictions%20across%20views%2C%0Anecessitating%20non-trivial%20fusion%20methods%20to%20incorporate%20the%20results%20onto%20the%0Aoriginal%20mesh.%20We%20explore%20this%20issue%20for%20a%20Collaborative%20Control%20workflow%0Aspecifically%20in%20PBR%20Text-to-Texture.%20Collaborative%20Control%20directly%20models%20PBR%0Aimage%20probability%20distributions%2C%20including%20normal%20bump%20maps%3B%20to%20our%20knowledge%2C%0Athe%20only%20diffusion%20model%20to%20directly%20output%20full%20PBR%20stacks.%20We%20discuss%20the%0Adesign%20decisions%20involved%20in%20making%20this%20model%20multi-view%20consistent%2C%20and%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20ablation%20studies%2C%20as%20well%20as%0Apractical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointly%2520Generating%2520Multi-view%2520Consistent%2520PBR%2520Textures%2520using%250A%2520%2520Collaborative%2520Control%26entry.906535625%3DShimon%2520Vainer%2520and%2520Konstantin%2520Kutsy%2520and%2520Dante%2520De%2520Nigris%2520and%2520Ciara%2520Rowles%2520and%2520Slava%2520Elizarov%2520and%2520Simon%2520Donn%25C3%25A9%26entry.1292438233%3D%2520%2520Multi-view%2520consistency%2520remains%2520a%2520challenge%2520for%2520image%2520diffusion%2520models.%2520Even%250Awithin%2520the%2520Text-to-Texture%2520problem%252C%2520where%2520perfect%2520geometric%2520correspondences%2520are%250Aknown%2520a%2520priori%252C%2520many%2520methods%2520fail%2520to%2520yield%2520aligned%2520predictions%2520across%2520views%252C%250Anecessitating%2520non-trivial%2520fusion%2520methods%2520to%2520incorporate%2520the%2520results%2520onto%2520the%250Aoriginal%2520mesh.%2520We%2520explore%2520this%2520issue%2520for%2520a%2520Collaborative%2520Control%2520workflow%250Aspecifically%2520in%2520PBR%2520Text-to-Texture.%2520Collaborative%2520Control%2520directly%2520models%2520PBR%250Aimage%2520probability%2520distributions%252C%2520including%2520normal%2520bump%2520maps%253B%2520to%2520our%2520knowledge%252C%250Athe%2520only%2520diffusion%2520model%2520to%2520directly%2520output%2520full%2520PBR%2520stacks.%2520We%2520discuss%2520the%250Adesign%2520decisions%2520involved%2520in%2520making%2520this%2520model%2520multi-view%2520consistent%252C%2520and%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520ablation%2520studies%252C%2520as%2520well%2520as%250Apractical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jointly%20Generating%20Multi-view%20Consistent%20PBR%20Textures%20using%0A%20%20Collaborative%20Control&entry.906535625=Shimon%20Vainer%20and%20Konstantin%20Kutsy%20and%20Dante%20De%20Nigris%20and%20Ciara%20Rowles%20and%20Slava%20Elizarov%20and%20Simon%20Donn%C3%A9&entry.1292438233=%20%20Multi-view%20consistency%20remains%20a%20challenge%20for%20image%20diffusion%20models.%20Even%0Awithin%20the%20Text-to-Texture%20problem%2C%20where%20perfect%20geometric%20correspondences%20are%0Aknown%20a%20priori%2C%20many%20methods%20fail%20to%20yield%20aligned%20predictions%20across%20views%2C%0Anecessitating%20non-trivial%20fusion%20methods%20to%20incorporate%20the%20results%20onto%20the%0Aoriginal%20mesh.%20We%20explore%20this%20issue%20for%20a%20Collaborative%20Control%20workflow%0Aspecifically%20in%20PBR%20Text-to-Texture.%20Collaborative%20Control%20directly%20models%20PBR%0Aimage%20probability%20distributions%2C%20including%20normal%20bump%20maps%3B%20to%20our%20knowledge%2C%0Athe%20only%20diffusion%20model%20to%20directly%20output%20full%20PBR%20stacks.%20We%20discuss%20the%0Adesign%20decisions%20involved%20in%20making%20this%20model%20multi-view%20consistent%2C%20and%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20ablation%20studies%2C%20as%20well%20as%0Apractical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06985v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


