<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251009.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic\n  Urban Scenes", "author": "Yikang Zhang and Rui Fan", "abstract": "  3D Gaussian splatting (3DGS) has demonstrated impressive performance in\nsynthesizing high-fidelity novel views. Nonetheless, its effectiveness\ncritically depends on the quality of the initialized point cloud. Specifically,\nachieving uniform and complete point coverage over the underlying scene\nstructure requires overlapping observation frustums, an assumption that is\noften violated in unbounded, dynamic urban environments. Training Gaussian\nmodels with partially initialized point clouds often leads to distortions and\nartifacts, as camera rays may fail to intersect valid surfaces, resulting in\nincorrect gradient propagation to Gaussian primitives associated with occluded\nor invisible geometry. Additionally, existing densification strategies simply\nclone and split Gaussian primitives from existing ones, incapable of\nreconstructing missing structures. To address these limitations, we propose\nVAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban\nscenes. Our method identifies unreliable geometry structures via voxel-based\nvisibility reasoning, selects informative supporting views through\ndiversity-aware view selection, and recovers missing structures via patch\nmatching-based multi-view stereo reconstruction. This design enables the\ngeneration of new Gaussian primitives guided by reliable geometric priors, even\nin regions lacking initial points. Extensive experiments on the Waymo and\nnuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS\napproaches and significantly improves the quality of reconstructed geometry for\nboth static and dynamic objects. Source code will be released upon publication.\n", "link": "http://arxiv.org/abs/2510.09364v1", "date": "2025-10-10", "relevancy": 3.4799, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7097}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7034}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visibility-Aware%20Densification%20for%203D%20Gaussian%20Splatting%20in%20Dynamic%0A%20%20Urban%20Scenes&body=Title%3A%20Visibility-Aware%20Densification%20for%203D%20Gaussian%20Splatting%20in%20Dynamic%0A%20%20Urban%20Scenes%0AAuthor%3A%20Yikang%20Zhang%20and%20Rui%20Fan%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20demonstrated%20impressive%20performance%20in%0Asynthesizing%20high-fidelity%20novel%20views.%20Nonetheless%2C%20its%20effectiveness%0Acritically%20depends%20on%20the%20quality%20of%20the%20initialized%20point%20cloud.%20Specifically%2C%0Aachieving%20uniform%20and%20complete%20point%20coverage%20over%20the%20underlying%20scene%0Astructure%20requires%20overlapping%20observation%20frustums%2C%20an%20assumption%20that%20is%0Aoften%20violated%20in%20unbounded%2C%20dynamic%20urban%20environments.%20Training%20Gaussian%0Amodels%20with%20partially%20initialized%20point%20clouds%20often%20leads%20to%20distortions%20and%0Aartifacts%2C%20as%20camera%20rays%20may%20fail%20to%20intersect%20valid%20surfaces%2C%20resulting%20in%0Aincorrect%20gradient%20propagation%20to%20Gaussian%20primitives%20associated%20with%20occluded%0Aor%20invisible%20geometry.%20Additionally%2C%20existing%20densification%20strategies%20simply%0Aclone%20and%20split%20Gaussian%20primitives%20from%20existing%20ones%2C%20incapable%20of%0Areconstructing%20missing%20structures.%20To%20address%20these%20limitations%2C%20we%20propose%0AVAD-GS%2C%20a%203DGS%20framework%20tailored%20for%20geometry%20recovery%20in%20challenging%20urban%0Ascenes.%20Our%20method%20identifies%20unreliable%20geometry%20structures%20via%20voxel-based%0Avisibility%20reasoning%2C%20selects%20informative%20supporting%20views%20through%0Adiversity-aware%20view%20selection%2C%20and%20recovers%20missing%20structures%20via%20patch%0Amatching-based%20multi-view%20stereo%20reconstruction.%20This%20design%20enables%20the%0Ageneration%20of%20new%20Gaussian%20primitives%20guided%20by%20reliable%20geometric%20priors%2C%20even%0Ain%20regions%20lacking%20initial%20points.%20Extensive%20experiments%20on%20the%20Waymo%20and%0AnuScenes%20datasets%20demonstrate%20that%20VAD-GS%20outperforms%20state-of-the-art%203DGS%0Aapproaches%20and%20significantly%20improves%20the%20quality%20of%20reconstructed%20geometry%20for%0Aboth%20static%20and%20dynamic%20objects.%20Source%20code%20will%20be%20released%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisibility-Aware%2520Densification%2520for%25203D%2520Gaussian%2520Splatting%2520in%2520Dynamic%250A%2520%2520Urban%2520Scenes%26entry.906535625%3DYikang%2520Zhang%2520and%2520Rui%2520Fan%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520impressive%2520performance%2520in%250Asynthesizing%2520high-fidelity%2520novel%2520views.%2520Nonetheless%252C%2520its%2520effectiveness%250Acritically%2520depends%2520on%2520the%2520quality%2520of%2520the%2520initialized%2520point%2520cloud.%2520Specifically%252C%250Aachieving%2520uniform%2520and%2520complete%2520point%2520coverage%2520over%2520the%2520underlying%2520scene%250Astructure%2520requires%2520overlapping%2520observation%2520frustums%252C%2520an%2520assumption%2520that%2520is%250Aoften%2520violated%2520in%2520unbounded%252C%2520dynamic%2520urban%2520environments.%2520Training%2520Gaussian%250Amodels%2520with%2520partially%2520initialized%2520point%2520clouds%2520often%2520leads%2520to%2520distortions%2520and%250Aartifacts%252C%2520as%2520camera%2520rays%2520may%2520fail%2520to%2520intersect%2520valid%2520surfaces%252C%2520resulting%2520in%250Aincorrect%2520gradient%2520propagation%2520to%2520Gaussian%2520primitives%2520associated%2520with%2520occluded%250Aor%2520invisible%2520geometry.%2520Additionally%252C%2520existing%2520densification%2520strategies%2520simply%250Aclone%2520and%2520split%2520Gaussian%2520primitives%2520from%2520existing%2520ones%252C%2520incapable%2520of%250Areconstructing%2520missing%2520structures.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250AVAD-GS%252C%2520a%25203DGS%2520framework%2520tailored%2520for%2520geometry%2520recovery%2520in%2520challenging%2520urban%250Ascenes.%2520Our%2520method%2520identifies%2520unreliable%2520geometry%2520structures%2520via%2520voxel-based%250Avisibility%2520reasoning%252C%2520selects%2520informative%2520supporting%2520views%2520through%250Adiversity-aware%2520view%2520selection%252C%2520and%2520recovers%2520missing%2520structures%2520via%2520patch%250Amatching-based%2520multi-view%2520stereo%2520reconstruction.%2520This%2520design%2520enables%2520the%250Ageneration%2520of%2520new%2520Gaussian%2520primitives%2520guided%2520by%2520reliable%2520geometric%2520priors%252C%2520even%250Ain%2520regions%2520lacking%2520initial%2520points.%2520Extensive%2520experiments%2520on%2520the%2520Waymo%2520and%250AnuScenes%2520datasets%2520demonstrate%2520that%2520VAD-GS%2520outperforms%2520state-of-the-art%25203DGS%250Aapproaches%2520and%2520significantly%2520improves%2520the%2520quality%2520of%2520reconstructed%2520geometry%2520for%250Aboth%2520static%2520and%2520dynamic%2520objects.%2520Source%2520code%2520will%2520be%2520released%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visibility-Aware%20Densification%20for%203D%20Gaussian%20Splatting%20in%20Dynamic%0A%20%20Urban%20Scenes&entry.906535625=Yikang%20Zhang%20and%20Rui%20Fan&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20demonstrated%20impressive%20performance%20in%0Asynthesizing%20high-fidelity%20novel%20views.%20Nonetheless%2C%20its%20effectiveness%0Acritically%20depends%20on%20the%20quality%20of%20the%20initialized%20point%20cloud.%20Specifically%2C%0Aachieving%20uniform%20and%20complete%20point%20coverage%20over%20the%20underlying%20scene%0Astructure%20requires%20overlapping%20observation%20frustums%2C%20an%20assumption%20that%20is%0Aoften%20violated%20in%20unbounded%2C%20dynamic%20urban%20environments.%20Training%20Gaussian%0Amodels%20with%20partially%20initialized%20point%20clouds%20often%20leads%20to%20distortions%20and%0Aartifacts%2C%20as%20camera%20rays%20may%20fail%20to%20intersect%20valid%20surfaces%2C%20resulting%20in%0Aincorrect%20gradient%20propagation%20to%20Gaussian%20primitives%20associated%20with%20occluded%0Aor%20invisible%20geometry.%20Additionally%2C%20existing%20densification%20strategies%20simply%0Aclone%20and%20split%20Gaussian%20primitives%20from%20existing%20ones%2C%20incapable%20of%0Areconstructing%20missing%20structures.%20To%20address%20these%20limitations%2C%20we%20propose%0AVAD-GS%2C%20a%203DGS%20framework%20tailored%20for%20geometry%20recovery%20in%20challenging%20urban%0Ascenes.%20Our%20method%20identifies%20unreliable%20geometry%20structures%20via%20voxel-based%0Avisibility%20reasoning%2C%20selects%20informative%20supporting%20views%20through%0Adiversity-aware%20view%20selection%2C%20and%20recovers%20missing%20structures%20via%20patch%0Amatching-based%20multi-view%20stereo%20reconstruction.%20This%20design%20enables%20the%0Ageneration%20of%20new%20Gaussian%20primitives%20guided%20by%20reliable%20geometric%20priors%2C%20even%0Ain%20regions%20lacking%20initial%20points.%20Extensive%20experiments%20on%20the%20Waymo%20and%0AnuScenes%20datasets%20demonstrate%20that%20VAD-GS%20outperforms%20state-of-the-art%203DGS%0Aapproaches%20and%20significantly%20improves%20the%20quality%20of%20reconstructed%20geometry%20for%0Aboth%20static%20and%20dynamic%20objects.%20Source%20code%20will%20be%20released%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09364v1&entry.124074799=Read"},
{"title": "SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images", "author": "Yu Sheng and Jiajun Deng and Xinran Zhang and Yu Zhang and Bei Hua and Yanyong Zhang and Jianmin Ji", "abstract": "  A major breakthrough in 3D reconstruction is the feedforward paradigm to\ngenerate pixel-wise 3D points or Gaussian primitives from sparse, unposed\nimages. To further incorporate semantics while avoiding the significant memory\nand storage costs of high-dimensional semantic features, existing methods\nextend this paradigm by associating each primitive with a compressed semantic\nfeature vector. However, these methods have two major limitations: (a) the\nnaively compressed feature compromises expressiveness, affecting the model's\nability to capture fine-grained semantics, and (b) the pixel-wise primitive\nprediction introduces redundancy in overlapping areas, causing unnecessary\nmemory overhead. To this end, we introduce \\textbf{SpatialSplat}, a feedforward\nframework that produces redundancy-aware Gaussians and capitalizes on a\ndual-field semantic representation. Particularly, with the insight that\nprimitives within the same instance exhibit high semantic consistency, we\ndecompose the semantic representation into a coarse feature field that encodes\nuncompressed semantics with minimal primitives, and a fine-grained yet\nlow-dimensional feature field that captures detailed inter-instance\nrelationships. Moreover, we propose a selective Gaussian mechanism, which\nretains only essential Gaussians in the scene, effectively eliminating\nredundant primitives. Our proposed Spatialsplat learns accurate semantic\ninformation and detailed instances prior with more compact 3D Gaussians, making\nsemantic 3D reconstruction more applicable. We conduct extensive experiments to\nevaluate our method, demonstrating a remarkable 60\\% reduction in scene\nrepresentation parameters while achieving superior performance over\nstate-of-the-art methods. The code is available at\nhttps://github.com/shengyuuu/SpatialSplat.git\n", "link": "http://arxiv.org/abs/2505.23044v2", "date": "2025-10-10", "relevancy": 3.3988, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7244}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6968}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialSplat%3A%20Efficient%20Semantic%203D%20from%20Sparse%20Unposed%20Images&body=Title%3A%20SpatialSplat%3A%20Efficient%20Semantic%203D%20from%20Sparse%20Unposed%20Images%0AAuthor%3A%20Yu%20Sheng%20and%20Jiajun%20Deng%20and%20Xinran%20Zhang%20and%20Yu%20Zhang%20and%20Bei%20Hua%20and%20Yanyong%20Zhang%20and%20Jianmin%20Ji%0AAbstract%3A%20%20%20A%20major%20breakthrough%20in%203D%20reconstruction%20is%20the%20feedforward%20paradigm%20to%0Agenerate%20pixel-wise%203D%20points%20or%20Gaussian%20primitives%20from%20sparse%2C%20unposed%0Aimages.%20To%20further%20incorporate%20semantics%20while%20avoiding%20the%20significant%20memory%0Aand%20storage%20costs%20of%20high-dimensional%20semantic%20features%2C%20existing%20methods%0Aextend%20this%20paradigm%20by%20associating%20each%20primitive%20with%20a%20compressed%20semantic%0Afeature%20vector.%20However%2C%20these%20methods%20have%20two%20major%20limitations%3A%20%28a%29%20the%0Anaively%20compressed%20feature%20compromises%20expressiveness%2C%20affecting%20the%20model%27s%0Aability%20to%20capture%20fine-grained%20semantics%2C%20and%20%28b%29%20the%20pixel-wise%20primitive%0Aprediction%20introduces%20redundancy%20in%20overlapping%20areas%2C%20causing%20unnecessary%0Amemory%20overhead.%20To%20this%20end%2C%20we%20introduce%20%5Ctextbf%7BSpatialSplat%7D%2C%20a%20feedforward%0Aframework%20that%20produces%20redundancy-aware%20Gaussians%20and%20capitalizes%20on%20a%0Adual-field%20semantic%20representation.%20Particularly%2C%20with%20the%20insight%20that%0Aprimitives%20within%20the%20same%20instance%20exhibit%20high%20semantic%20consistency%2C%20we%0Adecompose%20the%20semantic%20representation%20into%20a%20coarse%20feature%20field%20that%20encodes%0Auncompressed%20semantics%20with%20minimal%20primitives%2C%20and%20a%20fine-grained%20yet%0Alow-dimensional%20feature%20field%20that%20captures%20detailed%20inter-instance%0Arelationships.%20Moreover%2C%20we%20propose%20a%20selective%20Gaussian%20mechanism%2C%20which%0Aretains%20only%20essential%20Gaussians%20in%20the%20scene%2C%20effectively%20eliminating%0Aredundant%20primitives.%20Our%20proposed%20Spatialsplat%20learns%20accurate%20semantic%0Ainformation%20and%20detailed%20instances%20prior%20with%20more%20compact%203D%20Gaussians%2C%20making%0Asemantic%203D%20reconstruction%20more%20applicable.%20We%20conduct%20extensive%20experiments%20to%0Aevaluate%20our%20method%2C%20demonstrating%20a%20remarkable%2060%5C%25%20reduction%20in%20scene%0Arepresentation%20parameters%20while%20achieving%20superior%20performance%20over%0Astate-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shengyuuu/SpatialSplat.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23044v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialSplat%253A%2520Efficient%2520Semantic%25203D%2520from%2520Sparse%2520Unposed%2520Images%26entry.906535625%3DYu%2520Sheng%2520and%2520Jiajun%2520Deng%2520and%2520Xinran%2520Zhang%2520and%2520Yu%2520Zhang%2520and%2520Bei%2520Hua%2520and%2520Yanyong%2520Zhang%2520and%2520Jianmin%2520Ji%26entry.1292438233%3D%2520%2520A%2520major%2520breakthrough%2520in%25203D%2520reconstruction%2520is%2520the%2520feedforward%2520paradigm%2520to%250Agenerate%2520pixel-wise%25203D%2520points%2520or%2520Gaussian%2520primitives%2520from%2520sparse%252C%2520unposed%250Aimages.%2520To%2520further%2520incorporate%2520semantics%2520while%2520avoiding%2520the%2520significant%2520memory%250Aand%2520storage%2520costs%2520of%2520high-dimensional%2520semantic%2520features%252C%2520existing%2520methods%250Aextend%2520this%2520paradigm%2520by%2520associating%2520each%2520primitive%2520with%2520a%2520compressed%2520semantic%250Afeature%2520vector.%2520However%252C%2520these%2520methods%2520have%2520two%2520major%2520limitations%253A%2520%2528a%2529%2520the%250Anaively%2520compressed%2520feature%2520compromises%2520expressiveness%252C%2520affecting%2520the%2520model%2527s%250Aability%2520to%2520capture%2520fine-grained%2520semantics%252C%2520and%2520%2528b%2529%2520the%2520pixel-wise%2520primitive%250Aprediction%2520introduces%2520redundancy%2520in%2520overlapping%2520areas%252C%2520causing%2520unnecessary%250Amemory%2520overhead.%2520To%2520this%2520end%252C%2520we%2520introduce%2520%255Ctextbf%257BSpatialSplat%257D%252C%2520a%2520feedforward%250Aframework%2520that%2520produces%2520redundancy-aware%2520Gaussians%2520and%2520capitalizes%2520on%2520a%250Adual-field%2520semantic%2520representation.%2520Particularly%252C%2520with%2520the%2520insight%2520that%250Aprimitives%2520within%2520the%2520same%2520instance%2520exhibit%2520high%2520semantic%2520consistency%252C%2520we%250Adecompose%2520the%2520semantic%2520representation%2520into%2520a%2520coarse%2520feature%2520field%2520that%2520encodes%250Auncompressed%2520semantics%2520with%2520minimal%2520primitives%252C%2520and%2520a%2520fine-grained%2520yet%250Alow-dimensional%2520feature%2520field%2520that%2520captures%2520detailed%2520inter-instance%250Arelationships.%2520Moreover%252C%2520we%2520propose%2520a%2520selective%2520Gaussian%2520mechanism%252C%2520which%250Aretains%2520only%2520essential%2520Gaussians%2520in%2520the%2520scene%252C%2520effectively%2520eliminating%250Aredundant%2520primitives.%2520Our%2520proposed%2520Spatialsplat%2520learns%2520accurate%2520semantic%250Ainformation%2520and%2520detailed%2520instances%2520prior%2520with%2520more%2520compact%25203D%2520Gaussians%252C%2520making%250Asemantic%25203D%2520reconstruction%2520more%2520applicable.%2520We%2520conduct%2520extensive%2520experiments%2520to%250Aevaluate%2520our%2520method%252C%2520demonstrating%2520a%2520remarkable%252060%255C%2525%2520reduction%2520in%2520scene%250Arepresentation%2520parameters%2520while%2520achieving%2520superior%2520performance%2520over%250Astate-of-the-art%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shengyuuu/SpatialSplat.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23044v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialSplat%3A%20Efficient%20Semantic%203D%20from%20Sparse%20Unposed%20Images&entry.906535625=Yu%20Sheng%20and%20Jiajun%20Deng%20and%20Xinran%20Zhang%20and%20Yu%20Zhang%20and%20Bei%20Hua%20and%20Yanyong%20Zhang%20and%20Jianmin%20Ji&entry.1292438233=%20%20A%20major%20breakthrough%20in%203D%20reconstruction%20is%20the%20feedforward%20paradigm%20to%0Agenerate%20pixel-wise%203D%20points%20or%20Gaussian%20primitives%20from%20sparse%2C%20unposed%0Aimages.%20To%20further%20incorporate%20semantics%20while%20avoiding%20the%20significant%20memory%0Aand%20storage%20costs%20of%20high-dimensional%20semantic%20features%2C%20existing%20methods%0Aextend%20this%20paradigm%20by%20associating%20each%20primitive%20with%20a%20compressed%20semantic%0Afeature%20vector.%20However%2C%20these%20methods%20have%20two%20major%20limitations%3A%20%28a%29%20the%0Anaively%20compressed%20feature%20compromises%20expressiveness%2C%20affecting%20the%20model%27s%0Aability%20to%20capture%20fine-grained%20semantics%2C%20and%20%28b%29%20the%20pixel-wise%20primitive%0Aprediction%20introduces%20redundancy%20in%20overlapping%20areas%2C%20causing%20unnecessary%0Amemory%20overhead.%20To%20this%20end%2C%20we%20introduce%20%5Ctextbf%7BSpatialSplat%7D%2C%20a%20feedforward%0Aframework%20that%20produces%20redundancy-aware%20Gaussians%20and%20capitalizes%20on%20a%0Adual-field%20semantic%20representation.%20Particularly%2C%20with%20the%20insight%20that%0Aprimitives%20within%20the%20same%20instance%20exhibit%20high%20semantic%20consistency%2C%20we%0Adecompose%20the%20semantic%20representation%20into%20a%20coarse%20feature%20field%20that%20encodes%0Auncompressed%20semantics%20with%20minimal%20primitives%2C%20and%20a%20fine-grained%20yet%0Alow-dimensional%20feature%20field%20that%20captures%20detailed%20inter-instance%0Arelationships.%20Moreover%2C%20we%20propose%20a%20selective%20Gaussian%20mechanism%2C%20which%0Aretains%20only%20essential%20Gaussians%20in%20the%20scene%2C%20effectively%20eliminating%0Aredundant%20primitives.%20Our%20proposed%20Spatialsplat%20learns%20accurate%20semantic%0Ainformation%20and%20detailed%20instances%20prior%20with%20more%20compact%203D%20Gaussians%2C%20making%0Asemantic%203D%20reconstruction%20more%20applicable.%20We%20conduct%20extensive%20experiments%20to%0Aevaluate%20our%20method%2C%20demonstrating%20a%20remarkable%2060%5C%25%20reduction%20in%20scene%0Arepresentation%20parameters%20while%20achieving%20superior%20performance%20over%0Astate-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shengyuuu/SpatialSplat.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23044v2&entry.124074799=Read"},
{"title": "Gaussian Scenes: Pose-Free Sparse-View Scene Reconstruction using\n  Depth-Enhanced Diffusion Priors", "author": "Soumava Paul and Prakhar Kaushik and Alan Yuille", "abstract": "  In this work, we introduce a generative approach for pose-free (without\ncamera parameters) reconstruction of 360 scenes from a sparse set of 2D images.\nPose-free scene reconstruction from incomplete, pose-free observations is\nusually regularized with depth estimation or 3D foundational priors. While\nrecent advances have enabled sparse-view reconstruction of large complex scenes\n(with high degree of foreground and background detail) with known camera poses\nusing view-conditioned generative priors, these methods cannot be directly\nadapted for the pose-free setting when ground-truth poses are not available\nduring evaluation. To address this, we propose an image-to-image generative\nmodel designed to inpaint missing details and remove artifacts in novel view\nrenders and depth maps of a 3D scene. We introduce context and geometry\nconditioning using Feature-wise Linear Modulation (FiLM) modulation layers as a\nlightweight alternative to cross-attention and also propose a novel confidence\nmeasure for 3D Gaussian splat representations to allow for better detection of\nthese artifacts. By progressively integrating these novel views in a\nGaussian-SLAM-inspired process, we achieve a multi-view-consistent 3D\nrepresentation. Evaluations on the MipNeRF360 and DL3DV-10K benchmark dataset\ndemonstrate that our method surpasses existing pose-free techniques and\nperforms competitively with state-of-the-art posed (precomputed camera\nparameters are given) reconstruction methods in complex 360 scenes. Our project\npage provides additional results, videos, and code.\n", "link": "http://arxiv.org/abs/2411.15966v3", "date": "2025-10-10", "relevancy": 3.3945, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6949}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6751}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Scenes%3A%20Pose-Free%20Sparse-View%20Scene%20Reconstruction%20using%0A%20%20Depth-Enhanced%20Diffusion%20Priors&body=Title%3A%20Gaussian%20Scenes%3A%20Pose-Free%20Sparse-View%20Scene%20Reconstruction%20using%0A%20%20Depth-Enhanced%20Diffusion%20Priors%0AAuthor%3A%20Soumava%20Paul%20and%20Prakhar%20Kaushik%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20a%20generative%20approach%20for%20pose-free%20%28without%0Acamera%20parameters%29%20reconstruction%20of%20360%20scenes%20from%20a%20sparse%20set%20of%202D%20images.%0APose-free%20scene%20reconstruction%20from%20incomplete%2C%20pose-free%20observations%20is%0Ausually%20regularized%20with%20depth%20estimation%20or%203D%20foundational%20priors.%20While%0Arecent%20advances%20have%20enabled%20sparse-view%20reconstruction%20of%20large%20complex%20scenes%0A%28with%20high%20degree%20of%20foreground%20and%20background%20detail%29%20with%20known%20camera%20poses%0Ausing%20view-conditioned%20generative%20priors%2C%20these%20methods%20cannot%20be%20directly%0Aadapted%20for%20the%20pose-free%20setting%20when%20ground-truth%20poses%20are%20not%20available%0Aduring%20evaluation.%20To%20address%20this%2C%20we%20propose%20an%20image-to-image%20generative%0Amodel%20designed%20to%20inpaint%20missing%20details%20and%20remove%20artifacts%20in%20novel%20view%0Arenders%20and%20depth%20maps%20of%20a%203D%20scene.%20We%20introduce%20context%20and%20geometry%0Aconditioning%20using%20Feature-wise%20Linear%20Modulation%20%28FiLM%29%20modulation%20layers%20as%20a%0Alightweight%20alternative%20to%20cross-attention%20and%20also%20propose%20a%20novel%20confidence%0Ameasure%20for%203D%20Gaussian%20splat%20representations%20to%20allow%20for%20better%20detection%20of%0Athese%20artifacts.%20By%20progressively%20integrating%20these%20novel%20views%20in%20a%0AGaussian-SLAM-inspired%20process%2C%20we%20achieve%20a%20multi-view-consistent%203D%0Arepresentation.%20Evaluations%20on%20the%20MipNeRF360%20and%20DL3DV-10K%20benchmark%20dataset%0Ademonstrate%20that%20our%20method%20surpasses%20existing%20pose-free%20techniques%20and%0Aperforms%20competitively%20with%20state-of-the-art%20posed%20%28precomputed%20camera%0Aparameters%20are%20given%29%20reconstruction%20methods%20in%20complex%20360%20scenes.%20Our%20project%0Apage%20provides%20additional%20results%2C%20videos%2C%20and%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Scenes%253A%2520Pose-Free%2520Sparse-View%2520Scene%2520Reconstruction%2520using%250A%2520%2520Depth-Enhanced%2520Diffusion%2520Priors%26entry.906535625%3DSoumava%2520Paul%2520and%2520Prakhar%2520Kaushik%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520generative%2520approach%2520for%2520pose-free%2520%2528without%250Acamera%2520parameters%2529%2520reconstruction%2520of%2520360%2520scenes%2520from%2520a%2520sparse%2520set%2520of%25202D%2520images.%250APose-free%2520scene%2520reconstruction%2520from%2520incomplete%252C%2520pose-free%2520observations%2520is%250Ausually%2520regularized%2520with%2520depth%2520estimation%2520or%25203D%2520foundational%2520priors.%2520While%250Arecent%2520advances%2520have%2520enabled%2520sparse-view%2520reconstruction%2520of%2520large%2520complex%2520scenes%250A%2528with%2520high%2520degree%2520of%2520foreground%2520and%2520background%2520detail%2529%2520with%2520known%2520camera%2520poses%250Ausing%2520view-conditioned%2520generative%2520priors%252C%2520these%2520methods%2520cannot%2520be%2520directly%250Aadapted%2520for%2520the%2520pose-free%2520setting%2520when%2520ground-truth%2520poses%2520are%2520not%2520available%250Aduring%2520evaluation.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520image-to-image%2520generative%250Amodel%2520designed%2520to%2520inpaint%2520missing%2520details%2520and%2520remove%2520artifacts%2520in%2520novel%2520view%250Arenders%2520and%2520depth%2520maps%2520of%2520a%25203D%2520scene.%2520We%2520introduce%2520context%2520and%2520geometry%250Aconditioning%2520using%2520Feature-wise%2520Linear%2520Modulation%2520%2528FiLM%2529%2520modulation%2520layers%2520as%2520a%250Alightweight%2520alternative%2520to%2520cross-attention%2520and%2520also%2520propose%2520a%2520novel%2520confidence%250Ameasure%2520for%25203D%2520Gaussian%2520splat%2520representations%2520to%2520allow%2520for%2520better%2520detection%2520of%250Athese%2520artifacts.%2520By%2520progressively%2520integrating%2520these%2520novel%2520views%2520in%2520a%250AGaussian-SLAM-inspired%2520process%252C%2520we%2520achieve%2520a%2520multi-view-consistent%25203D%250Arepresentation.%2520Evaluations%2520on%2520the%2520MipNeRF360%2520and%2520DL3DV-10K%2520benchmark%2520dataset%250Ademonstrate%2520that%2520our%2520method%2520surpasses%2520existing%2520pose-free%2520techniques%2520and%250Aperforms%2520competitively%2520with%2520state-of-the-art%2520posed%2520%2528precomputed%2520camera%250Aparameters%2520are%2520given%2529%2520reconstruction%2520methods%2520in%2520complex%2520360%2520scenes.%2520Our%2520project%250Apage%2520provides%2520additional%2520results%252C%2520videos%252C%2520and%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Scenes%3A%20Pose-Free%20Sparse-View%20Scene%20Reconstruction%20using%0A%20%20Depth-Enhanced%20Diffusion%20Priors&entry.906535625=Soumava%20Paul%20and%20Prakhar%20Kaushik%20and%20Alan%20Yuille&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20a%20generative%20approach%20for%20pose-free%20%28without%0Acamera%20parameters%29%20reconstruction%20of%20360%20scenes%20from%20a%20sparse%20set%20of%202D%20images.%0APose-free%20scene%20reconstruction%20from%20incomplete%2C%20pose-free%20observations%20is%0Ausually%20regularized%20with%20depth%20estimation%20or%203D%20foundational%20priors.%20While%0Arecent%20advances%20have%20enabled%20sparse-view%20reconstruction%20of%20large%20complex%20scenes%0A%28with%20high%20degree%20of%20foreground%20and%20background%20detail%29%20with%20known%20camera%20poses%0Ausing%20view-conditioned%20generative%20priors%2C%20these%20methods%20cannot%20be%20directly%0Aadapted%20for%20the%20pose-free%20setting%20when%20ground-truth%20poses%20are%20not%20available%0Aduring%20evaluation.%20To%20address%20this%2C%20we%20propose%20an%20image-to-image%20generative%0Amodel%20designed%20to%20inpaint%20missing%20details%20and%20remove%20artifacts%20in%20novel%20view%0Arenders%20and%20depth%20maps%20of%20a%203D%20scene.%20We%20introduce%20context%20and%20geometry%0Aconditioning%20using%20Feature-wise%20Linear%20Modulation%20%28FiLM%29%20modulation%20layers%20as%20a%0Alightweight%20alternative%20to%20cross-attention%20and%20also%20propose%20a%20novel%20confidence%0Ameasure%20for%203D%20Gaussian%20splat%20representations%20to%20allow%20for%20better%20detection%20of%0Athese%20artifacts.%20By%20progressively%20integrating%20these%20novel%20views%20in%20a%0AGaussian-SLAM-inspired%20process%2C%20we%20achieve%20a%20multi-view-consistent%203D%0Arepresentation.%20Evaluations%20on%20the%20MipNeRF360%20and%20DL3DV-10K%20benchmark%20dataset%0Ademonstrate%20that%20our%20method%20surpasses%20existing%20pose-free%20techniques%20and%0Aperforms%20competitively%20with%20state-of-the-art%20posed%20%28precomputed%20camera%0Aparameters%20are%20given%29%20reconstruction%20methods%20in%20complex%20360%20scenes.%20Our%20project%0Apage%20provides%20additional%20results%2C%20videos%2C%20and%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15966v3&entry.124074799=Read"},
{"title": "PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from\n  One-shot Unposed Image", "author": "Peng Li and Yisheng He and Yingdong Hu and Yuan Dong and Weihao Yuan and Yuan Liu and Siyu Zhu and Gang Cheng and Zilong Dong and Yike Guo", "abstract": "  We present a feed-forward framework for Gaussian full-head synthesis from a\nsingle unposed image. Unlike previous work that relies on time-consuming GAN\ninversion and test-time optimization, our framework can reconstruct the\nGaussian full-head model given a single unposed image in a single forward pass.\nThis enables fast reconstruction and rendering during inference. To mitigate\nthe lack of large-scale 3D head assets, we propose a large-scale synthetic\ndataset from trained 3D GANs and train our framework using only synthetic data.\nFor efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian\nhead generation pipeline, where sparse points from the FLAME model interact\nwith the image features by transformer blocks for feature extraction and coarse\nshape reconstruction, which are then densified for high-fidelity\nreconstruction. To fully leverage the prior knowledge residing in pretrained 3D\nGANs for effective reconstruction, we propose a dual-branch framework that\neffectively aggregates the structured spherical triplane feature and\nunstructured point-based features for more effective Gaussian head\nreconstruction. Experimental results show the effectiveness of our framework\ntowards existing work. Project page at: https://panolam.github.io/.\n", "link": "http://arxiv.org/abs/2509.07552v2", "date": "2025-10-10", "relevancy": 3.3719, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6918}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6918}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanoLAM%3A%20Large%20Avatar%20Model%20for%20Gaussian%20Full-Head%20Synthesis%20from%0A%20%20One-shot%20Unposed%20Image&body=Title%3A%20PanoLAM%3A%20Large%20Avatar%20Model%20for%20Gaussian%20Full-Head%20Synthesis%20from%0A%20%20One-shot%20Unposed%20Image%0AAuthor%3A%20Peng%20Li%20and%20Yisheng%20He%20and%20Yingdong%20Hu%20and%20Yuan%20Dong%20and%20Weihao%20Yuan%20and%20Yuan%20Liu%20and%20Siyu%20Zhu%20and%20Gang%20Cheng%20and%20Zilong%20Dong%20and%20Yike%20Guo%0AAbstract%3A%20%20%20We%20present%20a%20feed-forward%20framework%20for%20Gaussian%20full-head%20synthesis%20from%20a%0Asingle%20unposed%20image.%20Unlike%20previous%20work%20that%20relies%20on%20time-consuming%20GAN%0Ainversion%20and%20test-time%20optimization%2C%20our%20framework%20can%20reconstruct%20the%0AGaussian%20full-head%20model%20given%20a%20single%20unposed%20image%20in%20a%20single%20forward%20pass.%0AThis%20enables%20fast%20reconstruction%20and%20rendering%20during%20inference.%20To%20mitigate%0Athe%20lack%20of%20large-scale%203D%20head%20assets%2C%20we%20propose%20a%20large-scale%20synthetic%0Adataset%20from%20trained%203D%20GANs%20and%20train%20our%20framework%20using%20only%20synthetic%20data.%0AFor%20efficient%20high-fidelity%20generation%2C%20we%20introduce%20a%20coarse-to-fine%20Gaussian%0Ahead%20generation%20pipeline%2C%20where%20sparse%20points%20from%20the%20FLAME%20model%20interact%0Awith%20the%20image%20features%20by%20transformer%20blocks%20for%20feature%20extraction%20and%20coarse%0Ashape%20reconstruction%2C%20which%20are%20then%20densified%20for%20high-fidelity%0Areconstruction.%20To%20fully%20leverage%20the%20prior%20knowledge%20residing%20in%20pretrained%203D%0AGANs%20for%20effective%20reconstruction%2C%20we%20propose%20a%20dual-branch%20framework%20that%0Aeffectively%20aggregates%20the%20structured%20spherical%20triplane%20feature%20and%0Aunstructured%20point-based%20features%20for%20more%20effective%20Gaussian%20head%0Areconstruction.%20Experimental%20results%20show%20the%20effectiveness%20of%20our%20framework%0Atowards%20existing%20work.%20Project%20page%20at%3A%20https%3A//panolam.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07552v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoLAM%253A%2520Large%2520Avatar%2520Model%2520for%2520Gaussian%2520Full-Head%2520Synthesis%2520from%250A%2520%2520One-shot%2520Unposed%2520Image%26entry.906535625%3DPeng%2520Li%2520and%2520Yisheng%2520He%2520and%2520Yingdong%2520Hu%2520and%2520Yuan%2520Dong%2520and%2520Weihao%2520Yuan%2520and%2520Yuan%2520Liu%2520and%2520Siyu%2520Zhu%2520and%2520Gang%2520Cheng%2520and%2520Zilong%2520Dong%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520feed-forward%2520framework%2520for%2520Gaussian%2520full-head%2520synthesis%2520from%2520a%250Asingle%2520unposed%2520image.%2520Unlike%2520previous%2520work%2520that%2520relies%2520on%2520time-consuming%2520GAN%250Ainversion%2520and%2520test-time%2520optimization%252C%2520our%2520framework%2520can%2520reconstruct%2520the%250AGaussian%2520full-head%2520model%2520given%2520a%2520single%2520unposed%2520image%2520in%2520a%2520single%2520forward%2520pass.%250AThis%2520enables%2520fast%2520reconstruction%2520and%2520rendering%2520during%2520inference.%2520To%2520mitigate%250Athe%2520lack%2520of%2520large-scale%25203D%2520head%2520assets%252C%2520we%2520propose%2520a%2520large-scale%2520synthetic%250Adataset%2520from%2520trained%25203D%2520GANs%2520and%2520train%2520our%2520framework%2520using%2520only%2520synthetic%2520data.%250AFor%2520efficient%2520high-fidelity%2520generation%252C%2520we%2520introduce%2520a%2520coarse-to-fine%2520Gaussian%250Ahead%2520generation%2520pipeline%252C%2520where%2520sparse%2520points%2520from%2520the%2520FLAME%2520model%2520interact%250Awith%2520the%2520image%2520features%2520by%2520transformer%2520blocks%2520for%2520feature%2520extraction%2520and%2520coarse%250Ashape%2520reconstruction%252C%2520which%2520are%2520then%2520densified%2520for%2520high-fidelity%250Areconstruction.%2520To%2520fully%2520leverage%2520the%2520prior%2520knowledge%2520residing%2520in%2520pretrained%25203D%250AGANs%2520for%2520effective%2520reconstruction%252C%2520we%2520propose%2520a%2520dual-branch%2520framework%2520that%250Aeffectively%2520aggregates%2520the%2520structured%2520spherical%2520triplane%2520feature%2520and%250Aunstructured%2520point-based%2520features%2520for%2520more%2520effective%2520Gaussian%2520head%250Areconstruction.%2520Experimental%2520results%2520show%2520the%2520effectiveness%2520of%2520our%2520framework%250Atowards%2520existing%2520work.%2520Project%2520page%2520at%253A%2520https%253A//panolam.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07552v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanoLAM%3A%20Large%20Avatar%20Model%20for%20Gaussian%20Full-Head%20Synthesis%20from%0A%20%20One-shot%20Unposed%20Image&entry.906535625=Peng%20Li%20and%20Yisheng%20He%20and%20Yingdong%20Hu%20and%20Yuan%20Dong%20and%20Weihao%20Yuan%20and%20Yuan%20Liu%20and%20Siyu%20Zhu%20and%20Gang%20Cheng%20and%20Zilong%20Dong%20and%20Yike%20Guo&entry.1292438233=%20%20We%20present%20a%20feed-forward%20framework%20for%20Gaussian%20full-head%20synthesis%20from%20a%0Asingle%20unposed%20image.%20Unlike%20previous%20work%20that%20relies%20on%20time-consuming%20GAN%0Ainversion%20and%20test-time%20optimization%2C%20our%20framework%20can%20reconstruct%20the%0AGaussian%20full-head%20model%20given%20a%20single%20unposed%20image%20in%20a%20single%20forward%20pass.%0AThis%20enables%20fast%20reconstruction%20and%20rendering%20during%20inference.%20To%20mitigate%0Athe%20lack%20of%20large-scale%203D%20head%20assets%2C%20we%20propose%20a%20large-scale%20synthetic%0Adataset%20from%20trained%203D%20GANs%20and%20train%20our%20framework%20using%20only%20synthetic%20data.%0AFor%20efficient%20high-fidelity%20generation%2C%20we%20introduce%20a%20coarse-to-fine%20Gaussian%0Ahead%20generation%20pipeline%2C%20where%20sparse%20points%20from%20the%20FLAME%20model%20interact%0Awith%20the%20image%20features%20by%20transformer%20blocks%20for%20feature%20extraction%20and%20coarse%0Ashape%20reconstruction%2C%20which%20are%20then%20densified%20for%20high-fidelity%0Areconstruction.%20To%20fully%20leverage%20the%20prior%20knowledge%20residing%20in%20pretrained%203D%0AGANs%20for%20effective%20reconstruction%2C%20we%20propose%20a%20dual-branch%20framework%20that%0Aeffectively%20aggregates%20the%20structured%20spherical%20triplane%20feature%20and%0Aunstructured%20point-based%20features%20for%20more%20effective%20Gaussian%20head%0Areconstruction.%20Experimental%20results%20show%20the%20effectiveness%20of%20our%20framework%0Atowards%20existing%20work.%20Project%20page%20at%3A%20https%3A//panolam.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07552v2&entry.124074799=Read"},
{"title": "Vision Language Models: A Survey of 26K Papers", "author": "Fengming Lin", "abstract": "  We present a transparent, reproducible measurement of research trends across\n26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles\nand abstracts are normalized, phrase-protected, and matched against a\nhand-crafted lexicon to assign up to 35 topical labels and mine fine-grained\ncues about tasks, architectures, training regimes, objectives, datasets, and\nco-mentioned modalities. The analysis quantifies three macro shifts: (1) a\nsharp rise of multimodal vision-language-LLM work, which increasingly reframes\nclassic perception as instruction following and multi-step reasoning; (2)\nsteady expansion of generative methods, with diffusion research consolidating\naround controllability, distillation, and speed; and (3) resilient 3D and video\nactivity, with composition moving from NeRFs to Gaussian splatting and a\ngrowing emphasis on human- and agent-centric understanding. Within VLMs,\nparameter-efficient adaptation like prompting/adapters/LoRA and lightweight\nvision-language bridges dominate; training practice shifts from building\nencoders from scratch to instruction tuning and finetuning strong backbones;\ncontrastive objectives recede relative to cross-entropy/ranking and\ndistillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and\nICLR the highest VLM share, while reliability themes such as efficiency or\nrobustness diffuse across areas. We release the lexicon and methodology to\nenable auditing and extension. Limitations include lexicon recall and\nabstract-only scope, but the longitudinal signals are consistent across venues\nand years.\n", "link": "http://arxiv.org/abs/2510.09586v1", "date": "2025-10-10", "relevancy": 3.0062, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6345}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Language%20Models%3A%20A%20Survey%20of%2026K%20Papers&body=Title%3A%20Vision%20Language%20Models%3A%20A%20Survey%20of%2026K%20Papers%0AAuthor%3A%20Fengming%20Lin%0AAbstract%3A%20%20%20We%20present%20a%20transparent%2C%20reproducible%20measurement%20of%20research%20trends%20across%0A26%2C104%20accepted%20papers%20from%20CVPR%2C%20ICLR%2C%20and%20NeurIPS%20spanning%202023-2025.%20Titles%0Aand%20abstracts%20are%20normalized%2C%20phrase-protected%2C%20and%20matched%20against%20a%0Ahand-crafted%20lexicon%20to%20assign%20up%20to%2035%20topical%20labels%20and%20mine%20fine-grained%0Acues%20about%20tasks%2C%20architectures%2C%20training%20regimes%2C%20objectives%2C%20datasets%2C%20and%0Aco-mentioned%20modalities.%20The%20analysis%20quantifies%20three%20macro%20shifts%3A%20%281%29%20a%0Asharp%20rise%20of%20multimodal%20vision-language-LLM%20work%2C%20which%20increasingly%20reframes%0Aclassic%20perception%20as%20instruction%20following%20and%20multi-step%20reasoning%3B%20%282%29%0Asteady%20expansion%20of%20generative%20methods%2C%20with%20diffusion%20research%20consolidating%0Aaround%20controllability%2C%20distillation%2C%20and%20speed%3B%20and%20%283%29%20resilient%203D%20and%20video%0Aactivity%2C%20with%20composition%20moving%20from%20NeRFs%20to%20Gaussian%20splatting%20and%20a%0Agrowing%20emphasis%20on%20human-%20and%20agent-centric%20understanding.%20Within%20VLMs%2C%0Aparameter-efficient%20adaptation%20like%20prompting/adapters/LoRA%20and%20lightweight%0Avision-language%20bridges%20dominate%3B%20training%20practice%20shifts%20from%20building%0Aencoders%20from%20scratch%20to%20instruction%20tuning%20and%20finetuning%20strong%20backbones%3B%0Acontrastive%20objectives%20recede%20relative%20to%20cross-entropy/ranking%20and%0Adistillation.%20Cross-venue%20comparisons%20show%20CVPR%20has%20a%20stronger%203D%20footprint%20and%0AICLR%20the%20highest%20VLM%20share%2C%20while%20reliability%20themes%20such%20as%20efficiency%20or%0Arobustness%20diffuse%20across%20areas.%20We%20release%20the%20lexicon%20and%20methodology%20to%0Aenable%20auditing%20and%20extension.%20Limitations%20include%20lexicon%20recall%20and%0Aabstract-only%20scope%2C%20but%20the%20longitudinal%20signals%20are%20consistent%20across%20venues%0Aand%20years.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Language%2520Models%253A%2520A%2520Survey%2520of%252026K%2520Papers%26entry.906535625%3DFengming%2520Lin%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520transparent%252C%2520reproducible%2520measurement%2520of%2520research%2520trends%2520across%250A26%252C104%2520accepted%2520papers%2520from%2520CVPR%252C%2520ICLR%252C%2520and%2520NeurIPS%2520spanning%25202023-2025.%2520Titles%250Aand%2520abstracts%2520are%2520normalized%252C%2520phrase-protected%252C%2520and%2520matched%2520against%2520a%250Ahand-crafted%2520lexicon%2520to%2520assign%2520up%2520to%252035%2520topical%2520labels%2520and%2520mine%2520fine-grained%250Acues%2520about%2520tasks%252C%2520architectures%252C%2520training%2520regimes%252C%2520objectives%252C%2520datasets%252C%2520and%250Aco-mentioned%2520modalities.%2520The%2520analysis%2520quantifies%2520three%2520macro%2520shifts%253A%2520%25281%2529%2520a%250Asharp%2520rise%2520of%2520multimodal%2520vision-language-LLM%2520work%252C%2520which%2520increasingly%2520reframes%250Aclassic%2520perception%2520as%2520instruction%2520following%2520and%2520multi-step%2520reasoning%253B%2520%25282%2529%250Asteady%2520expansion%2520of%2520generative%2520methods%252C%2520with%2520diffusion%2520research%2520consolidating%250Aaround%2520controllability%252C%2520distillation%252C%2520and%2520speed%253B%2520and%2520%25283%2529%2520resilient%25203D%2520and%2520video%250Aactivity%252C%2520with%2520composition%2520moving%2520from%2520NeRFs%2520to%2520Gaussian%2520splatting%2520and%2520a%250Agrowing%2520emphasis%2520on%2520human-%2520and%2520agent-centric%2520understanding.%2520Within%2520VLMs%252C%250Aparameter-efficient%2520adaptation%2520like%2520prompting/adapters/LoRA%2520and%2520lightweight%250Avision-language%2520bridges%2520dominate%253B%2520training%2520practice%2520shifts%2520from%2520building%250Aencoders%2520from%2520scratch%2520to%2520instruction%2520tuning%2520and%2520finetuning%2520strong%2520backbones%253B%250Acontrastive%2520objectives%2520recede%2520relative%2520to%2520cross-entropy/ranking%2520and%250Adistillation.%2520Cross-venue%2520comparisons%2520show%2520CVPR%2520has%2520a%2520stronger%25203D%2520footprint%2520and%250AICLR%2520the%2520highest%2520VLM%2520share%252C%2520while%2520reliability%2520themes%2520such%2520as%2520efficiency%2520or%250Arobustness%2520diffuse%2520across%2520areas.%2520We%2520release%2520the%2520lexicon%2520and%2520methodology%2520to%250Aenable%2520auditing%2520and%2520extension.%2520Limitations%2520include%2520lexicon%2520recall%2520and%250Aabstract-only%2520scope%252C%2520but%2520the%2520longitudinal%2520signals%2520are%2520consistent%2520across%2520venues%250Aand%2520years.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Language%20Models%3A%20A%20Survey%20of%2026K%20Papers&entry.906535625=Fengming%20Lin&entry.1292438233=%20%20We%20present%20a%20transparent%2C%20reproducible%20measurement%20of%20research%20trends%20across%0A26%2C104%20accepted%20papers%20from%20CVPR%2C%20ICLR%2C%20and%20NeurIPS%20spanning%202023-2025.%20Titles%0Aand%20abstracts%20are%20normalized%2C%20phrase-protected%2C%20and%20matched%20against%20a%0Ahand-crafted%20lexicon%20to%20assign%20up%20to%2035%20topical%20labels%20and%20mine%20fine-grained%0Acues%20about%20tasks%2C%20architectures%2C%20training%20regimes%2C%20objectives%2C%20datasets%2C%20and%0Aco-mentioned%20modalities.%20The%20analysis%20quantifies%20three%20macro%20shifts%3A%20%281%29%20a%0Asharp%20rise%20of%20multimodal%20vision-language-LLM%20work%2C%20which%20increasingly%20reframes%0Aclassic%20perception%20as%20instruction%20following%20and%20multi-step%20reasoning%3B%20%282%29%0Asteady%20expansion%20of%20generative%20methods%2C%20with%20diffusion%20research%20consolidating%0Aaround%20controllability%2C%20distillation%2C%20and%20speed%3B%20and%20%283%29%20resilient%203D%20and%20video%0Aactivity%2C%20with%20composition%20moving%20from%20NeRFs%20to%20Gaussian%20splatting%20and%20a%0Agrowing%20emphasis%20on%20human-%20and%20agent-centric%20understanding.%20Within%20VLMs%2C%0Aparameter-efficient%20adaptation%20like%20prompting/adapters/LoRA%20and%20lightweight%0Avision-language%20bridges%20dominate%3B%20training%20practice%20shifts%20from%20building%0Aencoders%20from%20scratch%20to%20instruction%20tuning%20and%20finetuning%20strong%20backbones%3B%0Acontrastive%20objectives%20recede%20relative%20to%20cross-entropy/ranking%20and%0Adistillation.%20Cross-venue%20comparisons%20show%20CVPR%20has%20a%20stronger%203D%20footprint%20and%0AICLR%20the%20highest%20VLM%20share%2C%20while%20reliability%20themes%20such%20as%20efficiency%20or%0Arobustness%20diffuse%20across%20areas.%20We%20release%20the%20lexicon%20and%20methodology%20to%0Aenable%20auditing%20and%20extension.%20Limitations%20include%20lexicon%20recall%20and%0Aabstract-only%20scope%2C%20but%20the%20longitudinal%20signals%20are%20consistent%20across%20venues%0Aand%20years.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09586v1&entry.124074799=Read"},
{"title": "ACD-CLIP: Decoupling Representation and Dynamic Fusion for Zero-Shot\n  Anomaly Detection", "author": "Ke Ma and Jun Long and Hongxiao Fei and Liujie Hua and Zhen Dai and Yueyi Luo", "abstract": "  Pre-trained Vision-Language Models (VLMs) struggle with Zero-Shot Anomaly\nDetection (ZSAD) due to a critical adaptation gap: they lack the local\ninductive biases required for dense prediction and employ inflexible feature\nfusion paradigms. We address these limitations through an Architectural\nCo-Design framework that jointly refines feature representation and cross-modal\nfusion. Our method proposes a parameter-efficient Convolutional Low-Rank\nAdaptation (Conv-LoRA) adapter to inject local inductive biases for\nfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that\nleverages visual context to adaptively modulate text prompts, enabling a\npowerful bidirectional fusion. Extensive experiments on diverse industrial and\nmedical benchmarks demonstrate superior accuracy and robustness, validating\nthat this synergistic co-design is critical for robustly adapting foundation\nmodels to dense perception tasks. The source code is available at\nhttps://github.com/cockmake/ACD-CLIP.\n", "link": "http://arxiv.org/abs/2508.07819v5", "date": "2025-10-10", "relevancy": 2.9915, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACD-CLIP%3A%20Decoupling%20Representation%20and%20Dynamic%20Fusion%20for%20Zero-Shot%0A%20%20Anomaly%20Detection&body=Title%3A%20ACD-CLIP%3A%20Decoupling%20Representation%20and%20Dynamic%20Fusion%20for%20Zero-Shot%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Ke%20Ma%20and%20Jun%20Long%20and%20Hongxiao%20Fei%20and%20Liujie%20Hua%20and%20Zhen%20Dai%20and%20Yueyi%20Luo%0AAbstract%3A%20%20%20Pre-trained%20Vision-Language%20Models%20%28VLMs%29%20struggle%20with%20Zero-Shot%20Anomaly%0ADetection%20%28ZSAD%29%20due%20to%20a%20critical%20adaptation%20gap%3A%20they%20lack%20the%20local%0Ainductive%20biases%20required%20for%20dense%20prediction%20and%20employ%20inflexible%20feature%0Afusion%20paradigms.%20We%20address%20these%20limitations%20through%20an%20Architectural%0ACo-Design%20framework%20that%20jointly%20refines%20feature%20representation%20and%20cross-modal%0Afusion.%20Our%20method%20proposes%20a%20parameter-efficient%20Convolutional%20Low-Rank%0AAdaptation%20%28Conv-LoRA%29%20adapter%20to%20inject%20local%20inductive%20biases%20for%0Afine-grained%20representation%2C%20and%20introduces%20a%20Dynamic%20Fusion%20Gateway%20%28DFG%29%20that%0Aleverages%20visual%20context%20to%20adaptively%20modulate%20text%20prompts%2C%20enabling%20a%0Apowerful%20bidirectional%20fusion.%20Extensive%20experiments%20on%20diverse%20industrial%20and%0Amedical%20benchmarks%20demonstrate%20superior%20accuracy%20and%20robustness%2C%20validating%0Athat%20this%20synergistic%20co-design%20is%20critical%20for%20robustly%20adapting%20foundation%0Amodels%20to%20dense%20perception%20tasks.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/cockmake/ACD-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07819v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACD-CLIP%253A%2520Decoupling%2520Representation%2520and%2520Dynamic%2520Fusion%2520for%2520Zero-Shot%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DKe%2520Ma%2520and%2520Jun%2520Long%2520and%2520Hongxiao%2520Fei%2520and%2520Liujie%2520Hua%2520and%2520Zhen%2520Dai%2520and%2520Yueyi%2520Luo%26entry.1292438233%3D%2520%2520Pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520struggle%2520with%2520Zero-Shot%2520Anomaly%250ADetection%2520%2528ZSAD%2529%2520due%2520to%2520a%2520critical%2520adaptation%2520gap%253A%2520they%2520lack%2520the%2520local%250Ainductive%2520biases%2520required%2520for%2520dense%2520prediction%2520and%2520employ%2520inflexible%2520feature%250Afusion%2520paradigms.%2520We%2520address%2520these%2520limitations%2520through%2520an%2520Architectural%250ACo-Design%2520framework%2520that%2520jointly%2520refines%2520feature%2520representation%2520and%2520cross-modal%250Afusion.%2520Our%2520method%2520proposes%2520a%2520parameter-efficient%2520Convolutional%2520Low-Rank%250AAdaptation%2520%2528Conv-LoRA%2529%2520adapter%2520to%2520inject%2520local%2520inductive%2520biases%2520for%250Afine-grained%2520representation%252C%2520and%2520introduces%2520a%2520Dynamic%2520Fusion%2520Gateway%2520%2528DFG%2529%2520that%250Aleverages%2520visual%2520context%2520to%2520adaptively%2520modulate%2520text%2520prompts%252C%2520enabling%2520a%250Apowerful%2520bidirectional%2520fusion.%2520Extensive%2520experiments%2520on%2520diverse%2520industrial%2520and%250Amedical%2520benchmarks%2520demonstrate%2520superior%2520accuracy%2520and%2520robustness%252C%2520validating%250Athat%2520this%2520synergistic%2520co-design%2520is%2520critical%2520for%2520robustly%2520adapting%2520foundation%250Amodels%2520to%2520dense%2520perception%2520tasks.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/cockmake/ACD-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07819v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACD-CLIP%3A%20Decoupling%20Representation%20and%20Dynamic%20Fusion%20for%20Zero-Shot%0A%20%20Anomaly%20Detection&entry.906535625=Ke%20Ma%20and%20Jun%20Long%20and%20Hongxiao%20Fei%20and%20Liujie%20Hua%20and%20Zhen%20Dai%20and%20Yueyi%20Luo&entry.1292438233=%20%20Pre-trained%20Vision-Language%20Models%20%28VLMs%29%20struggle%20with%20Zero-Shot%20Anomaly%0ADetection%20%28ZSAD%29%20due%20to%20a%20critical%20adaptation%20gap%3A%20they%20lack%20the%20local%0Ainductive%20biases%20required%20for%20dense%20prediction%20and%20employ%20inflexible%20feature%0Afusion%20paradigms.%20We%20address%20these%20limitations%20through%20an%20Architectural%0ACo-Design%20framework%20that%20jointly%20refines%20feature%20representation%20and%20cross-modal%0Afusion.%20Our%20method%20proposes%20a%20parameter-efficient%20Convolutional%20Low-Rank%0AAdaptation%20%28Conv-LoRA%29%20adapter%20to%20inject%20local%20inductive%20biases%20for%0Afine-grained%20representation%2C%20and%20introduces%20a%20Dynamic%20Fusion%20Gateway%20%28DFG%29%20that%0Aleverages%20visual%20context%20to%20adaptively%20modulate%20text%20prompts%2C%20enabling%20a%0Apowerful%20bidirectional%20fusion.%20Extensive%20experiments%20on%20diverse%20industrial%20and%0Amedical%20benchmarks%20demonstrate%20superior%20accuracy%20and%20robustness%2C%20validating%0Athat%20this%20synergistic%20co-design%20is%20critical%20for%20robustly%20adapting%20foundation%0Amodels%20to%20dense%20perception%20tasks.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/cockmake/ACD-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07819v5&entry.124074799=Read"},
{"title": "CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for\n  Embodied Reference Understanding", "author": "Fevziye Irem Eyiokur and Dogucan Yaman and Haz\u0131m Kemal Ekenel and Alexander Waibel", "abstract": "  We address the problem of Embodied Reference Understanding, which involves\npredicting the object that a person in the scene is referring to through both\npointing gesture and language. Accurately identifying the referent requires\nmultimodal understanding: integrating textual instructions, visual pointing,\nand scene context. However, existing methods often struggle to effectively\nleverage visual clues for disambiguation. We also observe that, while the\nreferent is often aligned with the head-to-fingertip line, it occasionally\naligns more closely with the wrist-to-fingertip line. Therefore, relying on a\nsingle line assumption can be overly simplistic and may lead to suboptimal\nperformance. To address this, we propose a dual-model framework, where one\nmodel learns from the head-to-fingertip direction and the other from the\nwrist-to-fingertip direction. We further introduce a Gaussian ray heatmap\nrepresentation of these lines and use them as input to provide a strong\nsupervisory signal that encourages the model to better attend to pointing cues.\nTo combine the strengths of both models, we present the CLIP-Aware Pointing\nEnsemble module, which performs a hybrid ensemble based on CLIP features.\nAdditionally, we propose an object center prediction head as an auxiliary task\nto further enhance referent localization. We validate our approach through\nextensive experiments and analysis on the benchmark YouRefIt dataset, achieving\nan improvement of approximately 4 mAP at the 0.25 IoU threshold. We further\nevaluate our approach on the CAESAR and ISL Pointing datasets.\n", "link": "http://arxiv.org/abs/2507.21888v2", "date": "2025-10-10", "relevancy": 2.9668, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6075}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5899}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%0A%20%20Embodied%20Reference%20Understanding&body=Title%3A%20CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%0A%20%20Embodied%20Reference%20Understanding%0AAuthor%3A%20Fevziye%20Irem%20Eyiokur%20and%20Dogucan%20Yaman%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20Embodied%20Reference%20Understanding%2C%20which%20involves%0Apredicting%20the%20object%20that%20a%20person%20in%20the%20scene%20is%20referring%20to%20through%20both%0Apointing%20gesture%20and%20language.%20Accurately%20identifying%20the%20referent%20requires%0Amultimodal%20understanding%3A%20integrating%20textual%20instructions%2C%20visual%20pointing%2C%0Aand%20scene%20context.%20However%2C%20existing%20methods%20often%20struggle%20to%20effectively%0Aleverage%20visual%20clues%20for%20disambiguation.%20We%20also%20observe%20that%2C%20while%20the%0Areferent%20is%20often%20aligned%20with%20the%20head-to-fingertip%20line%2C%20it%20occasionally%0Aaligns%20more%20closely%20with%20the%20wrist-to-fingertip%20line.%20Therefore%2C%20relying%20on%20a%0Asingle%20line%20assumption%20can%20be%20overly%20simplistic%20and%20may%20lead%20to%20suboptimal%0Aperformance.%20To%20address%20this%2C%20we%20propose%20a%20dual-model%20framework%2C%20where%20one%0Amodel%20learns%20from%20the%20head-to-fingertip%20direction%20and%20the%20other%20from%20the%0Awrist-to-fingertip%20direction.%20We%20further%20introduce%20a%20Gaussian%20ray%20heatmap%0Arepresentation%20of%20these%20lines%20and%20use%20them%20as%20input%20to%20provide%20a%20strong%0Asupervisory%20signal%20that%20encourages%20the%20model%20to%20better%20attend%20to%20pointing%20cues.%0ATo%20combine%20the%20strengths%20of%20both%20models%2C%20we%20present%20the%20CLIP-Aware%20Pointing%0AEnsemble%20module%2C%20which%20performs%20a%20hybrid%20ensemble%20based%20on%20CLIP%20features.%0AAdditionally%2C%20we%20propose%20an%20object%20center%20prediction%20head%20as%20an%20auxiliary%20task%0Ato%20further%20enhance%20referent%20localization.%20We%20validate%20our%20approach%20through%0Aextensive%20experiments%20and%20analysis%20on%20the%20benchmark%20YouRefIt%20dataset%2C%20achieving%0Aan%20improvement%20of%20approximately%204%20mAP%20at%20the%200.25%20IoU%20threshold.%20We%20further%0Aevaluate%20our%20approach%20on%20the%20CAESAR%20and%20ISL%20Pointing%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPE%253A%2520A%2520CLIP-Aware%2520Pointing%2520Ensemble%2520of%2520Complementary%2520Heatmap%2520Cues%2520for%250A%2520%2520Embodied%2520Reference%2520Understanding%26entry.906535625%3DFevziye%2520Irem%2520Eyiokur%2520and%2520Dogucan%2520Yaman%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520Embodied%2520Reference%2520Understanding%252C%2520which%2520involves%250Apredicting%2520the%2520object%2520that%2520a%2520person%2520in%2520the%2520scene%2520is%2520referring%2520to%2520through%2520both%250Apointing%2520gesture%2520and%2520language.%2520Accurately%2520identifying%2520the%2520referent%2520requires%250Amultimodal%2520understanding%253A%2520integrating%2520textual%2520instructions%252C%2520visual%2520pointing%252C%250Aand%2520scene%2520context.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520to%2520effectively%250Aleverage%2520visual%2520clues%2520for%2520disambiguation.%2520We%2520also%2520observe%2520that%252C%2520while%2520the%250Areferent%2520is%2520often%2520aligned%2520with%2520the%2520head-to-fingertip%2520line%252C%2520it%2520occasionally%250Aaligns%2520more%2520closely%2520with%2520the%2520wrist-to-fingertip%2520line.%2520Therefore%252C%2520relying%2520on%2520a%250Asingle%2520line%2520assumption%2520can%2520be%2520overly%2520simplistic%2520and%2520may%2520lead%2520to%2520suboptimal%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520dual-model%2520framework%252C%2520where%2520one%250Amodel%2520learns%2520from%2520the%2520head-to-fingertip%2520direction%2520and%2520the%2520other%2520from%2520the%250Awrist-to-fingertip%2520direction.%2520We%2520further%2520introduce%2520a%2520Gaussian%2520ray%2520heatmap%250Arepresentation%2520of%2520these%2520lines%2520and%2520use%2520them%2520as%2520input%2520to%2520provide%2520a%2520strong%250Asupervisory%2520signal%2520that%2520encourages%2520the%2520model%2520to%2520better%2520attend%2520to%2520pointing%2520cues.%250ATo%2520combine%2520the%2520strengths%2520of%2520both%2520models%252C%2520we%2520present%2520the%2520CLIP-Aware%2520Pointing%250AEnsemble%2520module%252C%2520which%2520performs%2520a%2520hybrid%2520ensemble%2520based%2520on%2520CLIP%2520features.%250AAdditionally%252C%2520we%2520propose%2520an%2520object%2520center%2520prediction%2520head%2520as%2520an%2520auxiliary%2520task%250Ato%2520further%2520enhance%2520referent%2520localization.%2520We%2520validate%2520our%2520approach%2520through%250Aextensive%2520experiments%2520and%2520analysis%2520on%2520the%2520benchmark%2520YouRefIt%2520dataset%252C%2520achieving%250Aan%2520improvement%2520of%2520approximately%25204%2520mAP%2520at%2520the%25200.25%2520IoU%2520threshold.%2520We%2520further%250Aevaluate%2520our%2520approach%2520on%2520the%2520CAESAR%2520and%2520ISL%2520Pointing%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%0A%20%20Embodied%20Reference%20Understanding&entry.906535625=Fevziye%20Irem%20Eyiokur%20and%20Dogucan%20Yaman%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel&entry.1292438233=%20%20We%20address%20the%20problem%20of%20Embodied%20Reference%20Understanding%2C%20which%20involves%0Apredicting%20the%20object%20that%20a%20person%20in%20the%20scene%20is%20referring%20to%20through%20both%0Apointing%20gesture%20and%20language.%20Accurately%20identifying%20the%20referent%20requires%0Amultimodal%20understanding%3A%20integrating%20textual%20instructions%2C%20visual%20pointing%2C%0Aand%20scene%20context.%20However%2C%20existing%20methods%20often%20struggle%20to%20effectively%0Aleverage%20visual%20clues%20for%20disambiguation.%20We%20also%20observe%20that%2C%20while%20the%0Areferent%20is%20often%20aligned%20with%20the%20head-to-fingertip%20line%2C%20it%20occasionally%0Aaligns%20more%20closely%20with%20the%20wrist-to-fingertip%20line.%20Therefore%2C%20relying%20on%20a%0Asingle%20line%20assumption%20can%20be%20overly%20simplistic%20and%20may%20lead%20to%20suboptimal%0Aperformance.%20To%20address%20this%2C%20we%20propose%20a%20dual-model%20framework%2C%20where%20one%0Amodel%20learns%20from%20the%20head-to-fingertip%20direction%20and%20the%20other%20from%20the%0Awrist-to-fingertip%20direction.%20We%20further%20introduce%20a%20Gaussian%20ray%20heatmap%0Arepresentation%20of%20these%20lines%20and%20use%20them%20as%20input%20to%20provide%20a%20strong%0Asupervisory%20signal%20that%20encourages%20the%20model%20to%20better%20attend%20to%20pointing%20cues.%0ATo%20combine%20the%20strengths%20of%20both%20models%2C%20we%20present%20the%20CLIP-Aware%20Pointing%0AEnsemble%20module%2C%20which%20performs%20a%20hybrid%20ensemble%20based%20on%20CLIP%20features.%0AAdditionally%2C%20we%20propose%20an%20object%20center%20prediction%20head%20as%20an%20auxiliary%20task%0Ato%20further%20enhance%20referent%20localization.%20We%20validate%20our%20approach%20through%0Aextensive%20experiments%20and%20analysis%20on%20the%20benchmark%20YouRefIt%20dataset%2C%20achieving%0Aan%20improvement%20of%20approximately%204%20mAP%20at%20the%200.25%20IoU%20threshold.%20We%20further%0Aevaluate%20our%20approach%20on%20the%20CAESAR%20and%20ISL%20Pointing%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21888v2&entry.124074799=Read"},
{"title": "Multimodal Language Models See Better When They Look Shallower", "author": "Haoran Chen and Junyan Lin and Xinghao Chen and Yue Fan and Jianfeng Dong and Xin Jin and Hui Su and Jinlan Fu and Xiaoyu Shen", "abstract": "  Multimodal large language models (MLLMs) typically extract visual features\nfrom the final layers of a pretrained Vision Transformer (ViT). This widespread\ndeep-layer bias, however, is largely driven by empirical convention rather than\nprincipled analysis. While prior studies suggest that different ViT layers\ncapture different types of information, with shallower layers focusing on fine\nvisual details and deeper layers aligning more closely with textual semantics,\nthe impact of this variation on MLLM performance remains underexplored. We\npresent the first comprehensive study of visual layer selection for MLLMs,\nanalyzing representation similarity across ViT layers to establish shallow,\nmiddle, and deep layer groupings. Through extensive evaluation of MLLMs\n(1.4B-7B parameters) across 10 benchmarks encompassing 60+ tasks, we find that\nwhile deep layers excel in semantic-rich tasks like OCR, shallow and middle\nlayers significantly outperform them on fine-grained visual tasks including\ncounting, positioning, and object localization. Building on these insights, we\npropose a lightweight feature fusion method that strategically incorporates\nshallower layers, achieving consistent improvements over both single-layer and\nspecialized fusion baselines. Our work offers the first principled study of\nvisual layer selection in MLLMs, showing that MLLMs can often see better when\nthey look shallower.\n", "link": "http://arxiv.org/abs/2504.21447v2", "date": "2025-10-10", "relevancy": 2.9519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Language%20Models%20See%20Better%20When%20They%20Look%20Shallower&body=Title%3A%20Multimodal%20Language%20Models%20See%20Better%20When%20They%20Look%20Shallower%0AAuthor%3A%20Haoran%20Chen%20and%20Junyan%20Lin%20and%20Xinghao%20Chen%20and%20Yue%20Fan%20and%20Jianfeng%20Dong%20and%20Xin%20Jin%20and%20Hui%20Su%20and%20Jinlan%20Fu%20and%20Xiaoyu%20Shen%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20typically%20extract%20visual%20features%0Afrom%20the%20final%20layers%20of%20a%20pretrained%20Vision%20Transformer%20%28ViT%29.%20This%20widespread%0Adeep-layer%20bias%2C%20however%2C%20is%20largely%20driven%20by%20empirical%20convention%20rather%20than%0Aprincipled%20analysis.%20While%20prior%20studies%20suggest%20that%20different%20ViT%20layers%0Acapture%20different%20types%20of%20information%2C%20with%20shallower%20layers%20focusing%20on%20fine%0Avisual%20details%20and%20deeper%20layers%20aligning%20more%20closely%20with%20textual%20semantics%2C%0Athe%20impact%20of%20this%20variation%20on%20MLLM%20performance%20remains%20underexplored.%20We%0Apresent%20the%20first%20comprehensive%20study%20of%20visual%20layer%20selection%20for%20MLLMs%2C%0Aanalyzing%20representation%20similarity%20across%20ViT%20layers%20to%20establish%20shallow%2C%0Amiddle%2C%20and%20deep%20layer%20groupings.%20Through%20extensive%20evaluation%20of%20MLLMs%0A%281.4B-7B%20parameters%29%20across%2010%20benchmarks%20encompassing%2060%2B%20tasks%2C%20we%20find%20that%0Awhile%20deep%20layers%20excel%20in%20semantic-rich%20tasks%20like%20OCR%2C%20shallow%20and%20middle%0Alayers%20significantly%20outperform%20them%20on%20fine-grained%20visual%20tasks%20including%0Acounting%2C%20positioning%2C%20and%20object%20localization.%20Building%20on%20these%20insights%2C%20we%0Apropose%20a%20lightweight%20feature%20fusion%20method%20that%20strategically%20incorporates%0Ashallower%20layers%2C%20achieving%20consistent%20improvements%20over%20both%20single-layer%20and%0Aspecialized%20fusion%20baselines.%20Our%20work%20offers%20the%20first%20principled%20study%20of%0Avisual%20layer%20selection%20in%20MLLMs%2C%20showing%20that%20MLLMs%20can%20often%20see%20better%20when%0Athey%20look%20shallower.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21447v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Language%2520Models%2520See%2520Better%2520When%2520They%2520Look%2520Shallower%26entry.906535625%3DHaoran%2520Chen%2520and%2520Junyan%2520Lin%2520and%2520Xinghao%2520Chen%2520and%2520Yue%2520Fan%2520and%2520Jianfeng%2520Dong%2520and%2520Xin%2520Jin%2520and%2520Hui%2520Su%2520and%2520Jinlan%2520Fu%2520and%2520Xiaoyu%2520Shen%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520typically%2520extract%2520visual%2520features%250Afrom%2520the%2520final%2520layers%2520of%2520a%2520pretrained%2520Vision%2520Transformer%2520%2528ViT%2529.%2520This%2520widespread%250Adeep-layer%2520bias%252C%2520however%252C%2520is%2520largely%2520driven%2520by%2520empirical%2520convention%2520rather%2520than%250Aprincipled%2520analysis.%2520While%2520prior%2520studies%2520suggest%2520that%2520different%2520ViT%2520layers%250Acapture%2520different%2520types%2520of%2520information%252C%2520with%2520shallower%2520layers%2520focusing%2520on%2520fine%250Avisual%2520details%2520and%2520deeper%2520layers%2520aligning%2520more%2520closely%2520with%2520textual%2520semantics%252C%250Athe%2520impact%2520of%2520this%2520variation%2520on%2520MLLM%2520performance%2520remains%2520underexplored.%2520We%250Apresent%2520the%2520first%2520comprehensive%2520study%2520of%2520visual%2520layer%2520selection%2520for%2520MLLMs%252C%250Aanalyzing%2520representation%2520similarity%2520across%2520ViT%2520layers%2520to%2520establish%2520shallow%252C%250Amiddle%252C%2520and%2520deep%2520layer%2520groupings.%2520Through%2520extensive%2520evaluation%2520of%2520MLLMs%250A%25281.4B-7B%2520parameters%2529%2520across%252010%2520benchmarks%2520encompassing%252060%252B%2520tasks%252C%2520we%2520find%2520that%250Awhile%2520deep%2520layers%2520excel%2520in%2520semantic-rich%2520tasks%2520like%2520OCR%252C%2520shallow%2520and%2520middle%250Alayers%2520significantly%2520outperform%2520them%2520on%2520fine-grained%2520visual%2520tasks%2520including%250Acounting%252C%2520positioning%252C%2520and%2520object%2520localization.%2520Building%2520on%2520these%2520insights%252C%2520we%250Apropose%2520a%2520lightweight%2520feature%2520fusion%2520method%2520that%2520strategically%2520incorporates%250Ashallower%2520layers%252C%2520achieving%2520consistent%2520improvements%2520over%2520both%2520single-layer%2520and%250Aspecialized%2520fusion%2520baselines.%2520Our%2520work%2520offers%2520the%2520first%2520principled%2520study%2520of%250Avisual%2520layer%2520selection%2520in%2520MLLMs%252C%2520showing%2520that%2520MLLMs%2520can%2520often%2520see%2520better%2520when%250Athey%2520look%2520shallower.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21447v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Language%20Models%20See%20Better%20When%20They%20Look%20Shallower&entry.906535625=Haoran%20Chen%20and%20Junyan%20Lin%20and%20Xinghao%20Chen%20and%20Yue%20Fan%20and%20Jianfeng%20Dong%20and%20Xin%20Jin%20and%20Hui%20Su%20and%20Jinlan%20Fu%20and%20Xiaoyu%20Shen&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20typically%20extract%20visual%20features%0Afrom%20the%20final%20layers%20of%20a%20pretrained%20Vision%20Transformer%20%28ViT%29.%20This%20widespread%0Adeep-layer%20bias%2C%20however%2C%20is%20largely%20driven%20by%20empirical%20convention%20rather%20than%0Aprincipled%20analysis.%20While%20prior%20studies%20suggest%20that%20different%20ViT%20layers%0Acapture%20different%20types%20of%20information%2C%20with%20shallower%20layers%20focusing%20on%20fine%0Avisual%20details%20and%20deeper%20layers%20aligning%20more%20closely%20with%20textual%20semantics%2C%0Athe%20impact%20of%20this%20variation%20on%20MLLM%20performance%20remains%20underexplored.%20We%0Apresent%20the%20first%20comprehensive%20study%20of%20visual%20layer%20selection%20for%20MLLMs%2C%0Aanalyzing%20representation%20similarity%20across%20ViT%20layers%20to%20establish%20shallow%2C%0Amiddle%2C%20and%20deep%20layer%20groupings.%20Through%20extensive%20evaluation%20of%20MLLMs%0A%281.4B-7B%20parameters%29%20across%2010%20benchmarks%20encompassing%2060%2B%20tasks%2C%20we%20find%20that%0Awhile%20deep%20layers%20excel%20in%20semantic-rich%20tasks%20like%20OCR%2C%20shallow%20and%20middle%0Alayers%20significantly%20outperform%20them%20on%20fine-grained%20visual%20tasks%20including%0Acounting%2C%20positioning%2C%20and%20object%20localization.%20Building%20on%20these%20insights%2C%20we%0Apropose%20a%20lightweight%20feature%20fusion%20method%20that%20strategically%20incorporates%0Ashallower%20layers%2C%20achieving%20consistent%20improvements%20over%20both%20single-layer%20and%0Aspecialized%20fusion%20baselines.%20Our%20work%20offers%20the%20first%20principled%20study%20of%0Avisual%20layer%20selection%20in%20MLLMs%2C%20showing%20that%20MLLMs%20can%20often%20see%20better%20when%0Athey%20look%20shallower.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21447v2&entry.124074799=Read"},
{"title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System", "author": "Lixuan He and Haoyu Dong and Zhenxing Chen and Yangcheng Yu and Jie Feng and Yong Li", "abstract": "  Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.\n", "link": "http://arxiv.org/abs/2506.19433v2", "date": "2025-10-10", "relevancy": 2.9244, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mem4Nav%3A%20Boosting%20Vision-and-Language%20Navigation%20in%20Urban%20Environments%0A%20%20with%20a%20Hierarchical%20Spatial-Cognition%20Long-Short%20Memory%20System&body=Title%3A%20Mem4Nav%3A%20Boosting%20Vision-and-Language%20Navigation%20in%20Urban%20Environments%0A%20%20with%20a%20Hierarchical%20Spatial-Cognition%20Long-Short%20Memory%20System%0AAuthor%3A%20Lixuan%20He%20and%20Haoyu%20Dong%20and%20Zhenxing%20Chen%20and%20Yangcheng%20Yu%20and%20Jie%20Feng%20and%20Yong%20Li%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20in%20large-scale%20urban%20environments%0Arequires%20embodied%20agents%20to%20ground%20linguistic%20instructions%20in%20complex%20scenes%0Aand%20recall%20relevant%20experiences%20over%20extended%20time%20horizons.%20Prior%20modular%0Apipelines%20offer%20interpretability%20but%20lack%20unified%20memory%2C%20while%20end-to-end%0A%28M%29LLM%20agents%20excel%20at%20fusing%20vision%20and%20language%20yet%20remain%20constrained%20by%0Afixed%20context%20windows%20and%20implicit%20spatial%20reasoning.%20We%20introduce%0A%5Ctextbf%7BMem4Nav%7D%2C%20a%20hierarchical%20spatial-cognition%20long-short%20memory%20system%0Athat%20can%20augment%20any%20VLN%20backbone.%20Mem4Nav%20fuses%20a%20sparse%20octree%20for%0Afine-grained%20voxel%20indexing%20with%20a%20semantic%20topology%20graph%20for%20high-level%0Alandmark%20connectivity%2C%20storing%20both%20in%20trainable%20memory%20tokens%20embedded%20via%20a%0Areversible%20Transformer.%20Long-term%20memory%20%28LTM%29%20compresses%20and%20retains%0Ahistorical%20observations%20at%20both%20octree%20and%20graph%20nodes%2C%20while%20short-term%20memory%0A%28STM%29%20caches%20recent%20multimodal%20entries%20in%20relative%20coordinates%20for%20real-time%0Aobstacle%20avoidance%20and%20local%20planning.%20At%20each%20step%2C%20STM%20retrieval%20sharply%0Aprunes%20dynamic%20context%2C%20and%2C%20when%20deeper%20history%20is%20needed%2C%20LTM%20tokens%20are%0Adecoded%20losslessly%20to%20reconstruct%20past%20embeddings.%20Evaluated%20on%20Touchdown%20and%0AMap2Seq%20across%20three%20backbones%20%28modular%2C%20state-of-the-art%20VLN%20with%20prompt-based%0ALLM%2C%20and%20state-of-the-art%20VLN%20with%20strided-attention%20MLLM%29%2C%20Mem4Nav%20yields%207-13%0App%20gains%20in%20Task%20Completion%2C%20sufficient%20SPD%20reduction%2C%20and%20%3E10%20pp%20nDTW%0Aimprovement.%20Ablations%20confirm%20the%20indispensability%20of%20both%20the%20hierarchical%0Amap%20and%20dual%20memory%20modules.%20Our%20codes%20are%20open-sourced%20via%0Ahttps%3A//github.com/tsinghua-fib-lab/Mem4Nav.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19433v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMem4Nav%253A%2520Boosting%2520Vision-and-Language%2520Navigation%2520in%2520Urban%2520Environments%250A%2520%2520with%2520a%2520Hierarchical%2520Spatial-Cognition%2520Long-Short%2520Memory%2520System%26entry.906535625%3DLixuan%2520He%2520and%2520Haoyu%2520Dong%2520and%2520Zhenxing%2520Chen%2520and%2520Yangcheng%2520Yu%2520and%2520Jie%2520Feng%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520in%2520large-scale%2520urban%2520environments%250Arequires%2520embodied%2520agents%2520to%2520ground%2520linguistic%2520instructions%2520in%2520complex%2520scenes%250Aand%2520recall%2520relevant%2520experiences%2520over%2520extended%2520time%2520horizons.%2520Prior%2520modular%250Apipelines%2520offer%2520interpretability%2520but%2520lack%2520unified%2520memory%252C%2520while%2520end-to-end%250A%2528M%2529LLM%2520agents%2520excel%2520at%2520fusing%2520vision%2520and%2520language%2520yet%2520remain%2520constrained%2520by%250Afixed%2520context%2520windows%2520and%2520implicit%2520spatial%2520reasoning.%2520We%2520introduce%250A%255Ctextbf%257BMem4Nav%257D%252C%2520a%2520hierarchical%2520spatial-cognition%2520long-short%2520memory%2520system%250Athat%2520can%2520augment%2520any%2520VLN%2520backbone.%2520Mem4Nav%2520fuses%2520a%2520sparse%2520octree%2520for%250Afine-grained%2520voxel%2520indexing%2520with%2520a%2520semantic%2520topology%2520graph%2520for%2520high-level%250Alandmark%2520connectivity%252C%2520storing%2520both%2520in%2520trainable%2520memory%2520tokens%2520embedded%2520via%2520a%250Areversible%2520Transformer.%2520Long-term%2520memory%2520%2528LTM%2529%2520compresses%2520and%2520retains%250Ahistorical%2520observations%2520at%2520both%2520octree%2520and%2520graph%2520nodes%252C%2520while%2520short-term%2520memory%250A%2528STM%2529%2520caches%2520recent%2520multimodal%2520entries%2520in%2520relative%2520coordinates%2520for%2520real-time%250Aobstacle%2520avoidance%2520and%2520local%2520planning.%2520At%2520each%2520step%252C%2520STM%2520retrieval%2520sharply%250Aprunes%2520dynamic%2520context%252C%2520and%252C%2520when%2520deeper%2520history%2520is%2520needed%252C%2520LTM%2520tokens%2520are%250Adecoded%2520losslessly%2520to%2520reconstruct%2520past%2520embeddings.%2520Evaluated%2520on%2520Touchdown%2520and%250AMap2Seq%2520across%2520three%2520backbones%2520%2528modular%252C%2520state-of-the-art%2520VLN%2520with%2520prompt-based%250ALLM%252C%2520and%2520state-of-the-art%2520VLN%2520with%2520strided-attention%2520MLLM%2529%252C%2520Mem4Nav%2520yields%25207-13%250App%2520gains%2520in%2520Task%2520Completion%252C%2520sufficient%2520SPD%2520reduction%252C%2520and%2520%253E10%2520pp%2520nDTW%250Aimprovement.%2520Ablations%2520confirm%2520the%2520indispensability%2520of%2520both%2520the%2520hierarchical%250Amap%2520and%2520dual%2520memory%2520modules.%2520Our%2520codes%2520are%2520open-sourced%2520via%250Ahttps%253A//github.com/tsinghua-fib-lab/Mem4Nav.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19433v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mem4Nav%3A%20Boosting%20Vision-and-Language%20Navigation%20in%20Urban%20Environments%0A%20%20with%20a%20Hierarchical%20Spatial-Cognition%20Long-Short%20Memory%20System&entry.906535625=Lixuan%20He%20and%20Haoyu%20Dong%20and%20Zhenxing%20Chen%20and%20Yangcheng%20Yu%20and%20Jie%20Feng%20and%20Yong%20Li&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20in%20large-scale%20urban%20environments%0Arequires%20embodied%20agents%20to%20ground%20linguistic%20instructions%20in%20complex%20scenes%0Aand%20recall%20relevant%20experiences%20over%20extended%20time%20horizons.%20Prior%20modular%0Apipelines%20offer%20interpretability%20but%20lack%20unified%20memory%2C%20while%20end-to-end%0A%28M%29LLM%20agents%20excel%20at%20fusing%20vision%20and%20language%20yet%20remain%20constrained%20by%0Afixed%20context%20windows%20and%20implicit%20spatial%20reasoning.%20We%20introduce%0A%5Ctextbf%7BMem4Nav%7D%2C%20a%20hierarchical%20spatial-cognition%20long-short%20memory%20system%0Athat%20can%20augment%20any%20VLN%20backbone.%20Mem4Nav%20fuses%20a%20sparse%20octree%20for%0Afine-grained%20voxel%20indexing%20with%20a%20semantic%20topology%20graph%20for%20high-level%0Alandmark%20connectivity%2C%20storing%20both%20in%20trainable%20memory%20tokens%20embedded%20via%20a%0Areversible%20Transformer.%20Long-term%20memory%20%28LTM%29%20compresses%20and%20retains%0Ahistorical%20observations%20at%20both%20octree%20and%20graph%20nodes%2C%20while%20short-term%20memory%0A%28STM%29%20caches%20recent%20multimodal%20entries%20in%20relative%20coordinates%20for%20real-time%0Aobstacle%20avoidance%20and%20local%20planning.%20At%20each%20step%2C%20STM%20retrieval%20sharply%0Aprunes%20dynamic%20context%2C%20and%2C%20when%20deeper%20history%20is%20needed%2C%20LTM%20tokens%20are%0Adecoded%20losslessly%20to%20reconstruct%20past%20embeddings.%20Evaluated%20on%20Touchdown%20and%0AMap2Seq%20across%20three%20backbones%20%28modular%2C%20state-of-the-art%20VLN%20with%20prompt-based%0ALLM%2C%20and%20state-of-the-art%20VLN%20with%20strided-attention%20MLLM%29%2C%20Mem4Nav%20yields%207-13%0App%20gains%20in%20Task%20Completion%2C%20sufficient%20SPD%20reduction%2C%20and%20%3E10%20pp%20nDTW%0Aimprovement.%20Ablations%20confirm%20the%20indispensability%20of%20both%20the%20hierarchical%0Amap%20and%20dual%20memory%20modules.%20Our%20codes%20are%20open-sourced%20via%0Ahttps%3A//github.com/tsinghua-fib-lab/Mem4Nav.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19433v2&entry.124074799=Read"},
{"title": "Few-shot multi-token DreamBooth with LoRa for style-consistent character\n  generation", "author": "Ruben Pascual and Mikel Sesma-Sara and Aranzazu Jurio and Daniel Paternain and Mikel Galar", "abstract": "  The audiovisual industry is undergoing a profound transformation as it is\nintegrating AI developments not only to automate routine tasks but also to\ninspire new forms of art. This paper addresses the problem of producing a\nvirtually unlimited number of novel characters that preserve the artistic style\nand shared visual traits of a small set of human-designed reference characters,\nthus broadening creative possibilities in animation, gaming, and related\ndomains. Our solution builds upon DreamBooth, a well-established fine-tuning\ntechnique for text-to-image diffusion models, and adapts it to tackle two core\nchallenges: capturing intricate character details beyond textual prompts and\nthe few-shot nature of the training data. To achieve this, we propose a\nmulti-token strategy, using clustering to assign separate tokens to individual\ncharacters and their collective style, combined with LoRA-based\nparameter-efficient fine-tuning. By removing the class-specific regularization\nset and introducing random tokens and embeddings during generation, our\napproach allows for unlimited character creation while preserving the learned\nstyle. We evaluate our method on five small specialized datasets, comparing it\nto relevant baselines using both quantitative metrics and a human evaluation\nstudy. Our results demonstrate that our approach produces high-quality, diverse\ncharacters while preserving the distinctive aesthetic features of the reference\ncharacters, with human evaluation further reinforcing its effectiveness and\nhighlighting the potential of our method.\n", "link": "http://arxiv.org/abs/2510.09475v1", "date": "2025-10-10", "relevancy": 2.8852, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.583}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5769}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20multi-token%20DreamBooth%20with%20LoRa%20for%20style-consistent%20character%0A%20%20generation&body=Title%3A%20Few-shot%20multi-token%20DreamBooth%20with%20LoRa%20for%20style-consistent%20character%0A%20%20generation%0AAuthor%3A%20Ruben%20Pascual%20and%20Mikel%20Sesma-Sara%20and%20Aranzazu%20Jurio%20and%20Daniel%20Paternain%20and%20Mikel%20Galar%0AAbstract%3A%20%20%20The%20audiovisual%20industry%20is%20undergoing%20a%20profound%20transformation%20as%20it%20is%0Aintegrating%20AI%20developments%20not%20only%20to%20automate%20routine%20tasks%20but%20also%20to%0Ainspire%20new%20forms%20of%20art.%20This%20paper%20addresses%20the%20problem%20of%20producing%20a%0Avirtually%20unlimited%20number%20of%20novel%20characters%20that%20preserve%20the%20artistic%20style%0Aand%20shared%20visual%20traits%20of%20a%20small%20set%20of%20human-designed%20reference%20characters%2C%0Athus%20broadening%20creative%20possibilities%20in%20animation%2C%20gaming%2C%20and%20related%0Adomains.%20Our%20solution%20builds%20upon%20DreamBooth%2C%20a%20well-established%20fine-tuning%0Atechnique%20for%20text-to-image%20diffusion%20models%2C%20and%20adapts%20it%20to%20tackle%20two%20core%0Achallenges%3A%20capturing%20intricate%20character%20details%20beyond%20textual%20prompts%20and%0Athe%20few-shot%20nature%20of%20the%20training%20data.%20To%20achieve%20this%2C%20we%20propose%20a%0Amulti-token%20strategy%2C%20using%20clustering%20to%20assign%20separate%20tokens%20to%20individual%0Acharacters%20and%20their%20collective%20style%2C%20combined%20with%20LoRA-based%0Aparameter-efficient%20fine-tuning.%20By%20removing%20the%20class-specific%20regularization%0Aset%20and%20introducing%20random%20tokens%20and%20embeddings%20during%20generation%2C%20our%0Aapproach%20allows%20for%20unlimited%20character%20creation%20while%20preserving%20the%20learned%0Astyle.%20We%20evaluate%20our%20method%20on%20five%20small%20specialized%20datasets%2C%20comparing%20it%0Ato%20relevant%20baselines%20using%20both%20quantitative%20metrics%20and%20a%20human%20evaluation%0Astudy.%20Our%20results%20demonstrate%20that%20our%20approach%20produces%20high-quality%2C%20diverse%0Acharacters%20while%20preserving%20the%20distinctive%20aesthetic%20features%20of%20the%20reference%0Acharacters%2C%20with%20human%20evaluation%20further%20reinforcing%20its%20effectiveness%20and%0Ahighlighting%20the%20potential%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520multi-token%2520DreamBooth%2520with%2520LoRa%2520for%2520style-consistent%2520character%250A%2520%2520generation%26entry.906535625%3DRuben%2520Pascual%2520and%2520Mikel%2520Sesma-Sara%2520and%2520Aranzazu%2520Jurio%2520and%2520Daniel%2520Paternain%2520and%2520Mikel%2520Galar%26entry.1292438233%3D%2520%2520The%2520audiovisual%2520industry%2520is%2520undergoing%2520a%2520profound%2520transformation%2520as%2520it%2520is%250Aintegrating%2520AI%2520developments%2520not%2520only%2520to%2520automate%2520routine%2520tasks%2520but%2520also%2520to%250Ainspire%2520new%2520forms%2520of%2520art.%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520producing%2520a%250Avirtually%2520unlimited%2520number%2520of%2520novel%2520characters%2520that%2520preserve%2520the%2520artistic%2520style%250Aand%2520shared%2520visual%2520traits%2520of%2520a%2520small%2520set%2520of%2520human-designed%2520reference%2520characters%252C%250Athus%2520broadening%2520creative%2520possibilities%2520in%2520animation%252C%2520gaming%252C%2520and%2520related%250Adomains.%2520Our%2520solution%2520builds%2520upon%2520DreamBooth%252C%2520a%2520well-established%2520fine-tuning%250Atechnique%2520for%2520text-to-image%2520diffusion%2520models%252C%2520and%2520adapts%2520it%2520to%2520tackle%2520two%2520core%250Achallenges%253A%2520capturing%2520intricate%2520character%2520details%2520beyond%2520textual%2520prompts%2520and%250Athe%2520few-shot%2520nature%2520of%2520the%2520training%2520data.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%250Amulti-token%2520strategy%252C%2520using%2520clustering%2520to%2520assign%2520separate%2520tokens%2520to%2520individual%250Acharacters%2520and%2520their%2520collective%2520style%252C%2520combined%2520with%2520LoRA-based%250Aparameter-efficient%2520fine-tuning.%2520By%2520removing%2520the%2520class-specific%2520regularization%250Aset%2520and%2520introducing%2520random%2520tokens%2520and%2520embeddings%2520during%2520generation%252C%2520our%250Aapproach%2520allows%2520for%2520unlimited%2520character%2520creation%2520while%2520preserving%2520the%2520learned%250Astyle.%2520We%2520evaluate%2520our%2520method%2520on%2520five%2520small%2520specialized%2520datasets%252C%2520comparing%2520it%250Ato%2520relevant%2520baselines%2520using%2520both%2520quantitative%2520metrics%2520and%2520a%2520human%2520evaluation%250Astudy.%2520Our%2520results%2520demonstrate%2520that%2520our%2520approach%2520produces%2520high-quality%252C%2520diverse%250Acharacters%2520while%2520preserving%2520the%2520distinctive%2520aesthetic%2520features%2520of%2520the%2520reference%250Acharacters%252C%2520with%2520human%2520evaluation%2520further%2520reinforcing%2520its%2520effectiveness%2520and%250Ahighlighting%2520the%2520potential%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20multi-token%20DreamBooth%20with%20LoRa%20for%20style-consistent%20character%0A%20%20generation&entry.906535625=Ruben%20Pascual%20and%20Mikel%20Sesma-Sara%20and%20Aranzazu%20Jurio%20and%20Daniel%20Paternain%20and%20Mikel%20Galar&entry.1292438233=%20%20The%20audiovisual%20industry%20is%20undergoing%20a%20profound%20transformation%20as%20it%20is%0Aintegrating%20AI%20developments%20not%20only%20to%20automate%20routine%20tasks%20but%20also%20to%0Ainspire%20new%20forms%20of%20art.%20This%20paper%20addresses%20the%20problem%20of%20producing%20a%0Avirtually%20unlimited%20number%20of%20novel%20characters%20that%20preserve%20the%20artistic%20style%0Aand%20shared%20visual%20traits%20of%20a%20small%20set%20of%20human-designed%20reference%20characters%2C%0Athus%20broadening%20creative%20possibilities%20in%20animation%2C%20gaming%2C%20and%20related%0Adomains.%20Our%20solution%20builds%20upon%20DreamBooth%2C%20a%20well-established%20fine-tuning%0Atechnique%20for%20text-to-image%20diffusion%20models%2C%20and%20adapts%20it%20to%20tackle%20two%20core%0Achallenges%3A%20capturing%20intricate%20character%20details%20beyond%20textual%20prompts%20and%0Athe%20few-shot%20nature%20of%20the%20training%20data.%20To%20achieve%20this%2C%20we%20propose%20a%0Amulti-token%20strategy%2C%20using%20clustering%20to%20assign%20separate%20tokens%20to%20individual%0Acharacters%20and%20their%20collective%20style%2C%20combined%20with%20LoRA-based%0Aparameter-efficient%20fine-tuning.%20By%20removing%20the%20class-specific%20regularization%0Aset%20and%20introducing%20random%20tokens%20and%20embeddings%20during%20generation%2C%20our%0Aapproach%20allows%20for%20unlimited%20character%20creation%20while%20preserving%20the%20learned%0Astyle.%20We%20evaluate%20our%20method%20on%20five%20small%20specialized%20datasets%2C%20comparing%20it%0Ato%20relevant%20baselines%20using%20both%20quantitative%20metrics%20and%20a%20human%20evaluation%0Astudy.%20Our%20results%20demonstrate%20that%20our%20approach%20produces%20high-quality%2C%20diverse%0Acharacters%20while%20preserving%20the%20distinctive%20aesthetic%20features%20of%20the%20reference%0Acharacters%2C%20with%20human%20evaluation%20further%20reinforcing%20its%20effectiveness%20and%0Ahighlighting%20the%20potential%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09475v1&entry.124074799=Read"},
{"title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning", "author": "Yuying Li and Siyi Qian and Hao Liang and Leqi Zheng and Ruichuan An and Yongzhen Guo and Wentao Zhang", "abstract": "  Geometric reasoning remains a core challenge for Multimodal Large Language\nModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3\nand Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite\nexhibiting strong textual reasoning abilities on tasks like the International\nMathematical Olympiad (IMO). This gap suggests that the bottleneck lies in\nunderstanding geometric diagrams rather than reasoning itself. Since geometric\nfigures can often be faithfully described in concise textual form, converting\nvisual content into captions offers a promising direction. Motivated by this\ninsight, we introduce CapGeo, a caption-assisted reasoning framework that\nbridges visual and textual modalities. Experiments show substantial\nimprovements when models are equipped with captions: Qwen2.5-VL-72B improves\nfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to\n73.0%. To systematically evaluate and identify high-quality geometric\ncaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated\nfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based\nevaluation metric that correlates strongly with downstream CapGeo performance,\nenabling reliable assessment of geometric captioning ability. Together, our\nframework and benchmark highlight a new pathway toward advancing geometric\nreasoning in MLLMs.\n", "link": "http://arxiv.org/abs/2510.09302v1", "date": "2025-10-10", "relevancy": 2.8129, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CapGeo%3A%20A%20Caption-Assisted%20Approach%20to%20Geometric%20Reasoning&body=Title%3A%20CapGeo%3A%20A%20Caption-Assisted%20Approach%20to%20Geometric%20Reasoning%0AAuthor%3A%20Yuying%20Li%20and%20Siyi%20Qian%20and%20Hao%20Liang%20and%20Leqi%20Zheng%20and%20Ruichuan%20An%20and%20Yongzhen%20Guo%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Geometric%20reasoning%20remains%20a%20core%20challenge%20for%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29.%20Even%20the%20most%20advanced%20closed-source%20systems%2C%20such%20as%20GPT-O3%0Aand%20Gemini-2.5-Pro%2C%20still%20struggle%20to%20solve%20geometry%20problems%20reliably%2C%20despite%0Aexhibiting%20strong%20textual%20reasoning%20abilities%20on%20tasks%20like%20the%20International%0AMathematical%20Olympiad%20%28IMO%29.%20This%20gap%20suggests%20that%20the%20bottleneck%20lies%20in%0Aunderstanding%20geometric%20diagrams%20rather%20than%20reasoning%20itself.%20Since%20geometric%0Afigures%20can%20often%20be%20faithfully%20described%20in%20concise%20textual%20form%2C%20converting%0Avisual%20content%20into%20captions%20offers%20a%20promising%20direction.%20Motivated%20by%20this%0Ainsight%2C%20we%20introduce%20CapGeo%2C%20a%20caption-assisted%20reasoning%20framework%20that%0Abridges%20visual%20and%20textual%20modalities.%20Experiments%20show%20substantial%0Aimprovements%20when%20models%20are%20equipped%20with%20captions%3A%20Qwen2.5-VL-72B%20improves%0Afrom%208.6%25%20%28vision-only%29%20to%2059.0%25%2C%20while%20Claude-Opus-4%20rises%20from%2044.8%25%20to%0A73.0%25.%20To%20systematically%20evaluate%20and%20identify%20high-quality%20geometric%0Acaptioning%20models%2C%20we%20further%20propose%20CapGeo-Bench%2C%20a%20dataset%20of%204%2C641%20curated%0Afigure-caption%20pairs.%20Crucially%2C%20CapGeo-Bench%20incorporates%20a%20keypoint-based%0Aevaluation%20metric%20that%20correlates%20strongly%20with%20downstream%20CapGeo%20performance%2C%0Aenabling%20reliable%20assessment%20of%20geometric%20captioning%20ability.%20Together%2C%20our%0Aframework%20and%20benchmark%20highlight%20a%20new%20pathway%20toward%20advancing%20geometric%0Areasoning%20in%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapGeo%253A%2520A%2520Caption-Assisted%2520Approach%2520to%2520Geometric%2520Reasoning%26entry.906535625%3DYuying%2520Li%2520and%2520Siyi%2520Qian%2520and%2520Hao%2520Liang%2520and%2520Leqi%2520Zheng%2520and%2520Ruichuan%2520An%2520and%2520Yongzhen%2520Guo%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Geometric%2520reasoning%2520remains%2520a%2520core%2520challenge%2520for%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529.%2520Even%2520the%2520most%2520advanced%2520closed-source%2520systems%252C%2520such%2520as%2520GPT-O3%250Aand%2520Gemini-2.5-Pro%252C%2520still%2520struggle%2520to%2520solve%2520geometry%2520problems%2520reliably%252C%2520despite%250Aexhibiting%2520strong%2520textual%2520reasoning%2520abilities%2520on%2520tasks%2520like%2520the%2520International%250AMathematical%2520Olympiad%2520%2528IMO%2529.%2520This%2520gap%2520suggests%2520that%2520the%2520bottleneck%2520lies%2520in%250Aunderstanding%2520geometric%2520diagrams%2520rather%2520than%2520reasoning%2520itself.%2520Since%2520geometric%250Afigures%2520can%2520often%2520be%2520faithfully%2520described%2520in%2520concise%2520textual%2520form%252C%2520converting%250Avisual%2520content%2520into%2520captions%2520offers%2520a%2520promising%2520direction.%2520Motivated%2520by%2520this%250Ainsight%252C%2520we%2520introduce%2520CapGeo%252C%2520a%2520caption-assisted%2520reasoning%2520framework%2520that%250Abridges%2520visual%2520and%2520textual%2520modalities.%2520Experiments%2520show%2520substantial%250Aimprovements%2520when%2520models%2520are%2520equipped%2520with%2520captions%253A%2520Qwen2.5-VL-72B%2520improves%250Afrom%25208.6%2525%2520%2528vision-only%2529%2520to%252059.0%2525%252C%2520while%2520Claude-Opus-4%2520rises%2520from%252044.8%2525%2520to%250A73.0%2525.%2520To%2520systematically%2520evaluate%2520and%2520identify%2520high-quality%2520geometric%250Acaptioning%2520models%252C%2520we%2520further%2520propose%2520CapGeo-Bench%252C%2520a%2520dataset%2520of%25204%252C641%2520curated%250Afigure-caption%2520pairs.%2520Crucially%252C%2520CapGeo-Bench%2520incorporates%2520a%2520keypoint-based%250Aevaluation%2520metric%2520that%2520correlates%2520strongly%2520with%2520downstream%2520CapGeo%2520performance%252C%250Aenabling%2520reliable%2520assessment%2520of%2520geometric%2520captioning%2520ability.%2520Together%252C%2520our%250Aframework%2520and%2520benchmark%2520highlight%2520a%2520new%2520pathway%2520toward%2520advancing%2520geometric%250Areasoning%2520in%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CapGeo%3A%20A%20Caption-Assisted%20Approach%20to%20Geometric%20Reasoning&entry.906535625=Yuying%20Li%20and%20Siyi%20Qian%20and%20Hao%20Liang%20and%20Leqi%20Zheng%20and%20Ruichuan%20An%20and%20Yongzhen%20Guo%20and%20Wentao%20Zhang&entry.1292438233=%20%20Geometric%20reasoning%20remains%20a%20core%20challenge%20for%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29.%20Even%20the%20most%20advanced%20closed-source%20systems%2C%20such%20as%20GPT-O3%0Aand%20Gemini-2.5-Pro%2C%20still%20struggle%20to%20solve%20geometry%20problems%20reliably%2C%20despite%0Aexhibiting%20strong%20textual%20reasoning%20abilities%20on%20tasks%20like%20the%20International%0AMathematical%20Olympiad%20%28IMO%29.%20This%20gap%20suggests%20that%20the%20bottleneck%20lies%20in%0Aunderstanding%20geometric%20diagrams%20rather%20than%20reasoning%20itself.%20Since%20geometric%0Afigures%20can%20often%20be%20faithfully%20described%20in%20concise%20textual%20form%2C%20converting%0Avisual%20content%20into%20captions%20offers%20a%20promising%20direction.%20Motivated%20by%20this%0Ainsight%2C%20we%20introduce%20CapGeo%2C%20a%20caption-assisted%20reasoning%20framework%20that%0Abridges%20visual%20and%20textual%20modalities.%20Experiments%20show%20substantial%0Aimprovements%20when%20models%20are%20equipped%20with%20captions%3A%20Qwen2.5-VL-72B%20improves%0Afrom%208.6%25%20%28vision-only%29%20to%2059.0%25%2C%20while%20Claude-Opus-4%20rises%20from%2044.8%25%20to%0A73.0%25.%20To%20systematically%20evaluate%20and%20identify%20high-quality%20geometric%0Acaptioning%20models%2C%20we%20further%20propose%20CapGeo-Bench%2C%20a%20dataset%20of%204%2C641%20curated%0Afigure-caption%20pairs.%20Crucially%2C%20CapGeo-Bench%20incorporates%20a%20keypoint-based%0Aevaluation%20metric%20that%20correlates%20strongly%20with%20downstream%20CapGeo%20performance%2C%0Aenabling%20reliable%20assessment%20of%20geometric%20captioning%20ability.%20Together%2C%20our%0Aframework%20and%20benchmark%20highlight%20a%20new%20pathway%20toward%20advancing%20geometric%0Areasoning%20in%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09302v1&entry.124074799=Read"},
{"title": "BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on\n  Visual Perception", "author": "Junyan Ye and Dongzhi Jiang and Jun He and Baichuan Zhou and Zilong Huang and Zhiyuan Yan and Hongsheng Li and Conghui He and Weijia Li", "abstract": "  Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,\nparticularly in enhancing their reasoning capabilities. However, existing\nreasoning benchmarks still primarily assess language-based reasoning, often\ntreating visual input as replaceable context. To address this gap, we introduce\nBLINK-Twice, a vision-centric reasoning benchmark grounded in challenging\nperceptual tasks. Instead of relying on external knowledge, our tasks require\nmodels to reason from visual content alone, shifting the focus from\nlanguage-based to image-grounded reasoning. Compared to prior perception\nbenchmarks, it moves beyond shallow perception (\"see\") and requires\nfine-grained observation and analytical reasoning (\"observe\"). BLINK-Twice\nintegrates three core components: seven types of visual challenges for testing\nvisual reasoning, natural adversarial image pairs that enforce reliance on\nvisual content, and annotated reasoning chains for fine-grained evaluation of\nthe reasoning process rather than final answers alone. We evaluate 20 leading\nMLLMs, including 12 foundation models and 8 reasoning-enhanced models.\nBLINK-Twice poses a significant challenge to current models. While existing\nreasoning strategies in the language space-such as chain-of-thought or\nself-criticism can improve performance, they often result in unstable and\nredundant reasoning. We observe that repeated image observation improves\nperformance across models, and active visual interaction, as demonstrated by\nmodels like o3, highlights the need for a new paradigm for vision reasoning.\nThe dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice\n", "link": "http://arxiv.org/abs/2510.09361v1", "date": "2025-10-10", "relevancy": 2.7935, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5814}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLINK-Twice%3A%20You%20see%2C%20but%20do%20you%20observe%3F%20A%20Reasoning%20Benchmark%20on%0A%20%20Visual%20Perception&body=Title%3A%20BLINK-Twice%3A%20You%20see%2C%20but%20do%20you%20observe%3F%20A%20Reasoning%20Benchmark%20on%0A%20%20Visual%20Perception%0AAuthor%3A%20Junyan%20Ye%20and%20Dongzhi%20Jiang%20and%20Jun%20He%20and%20Baichuan%20Zhou%20and%20Zilong%20Huang%20and%20Zhiyuan%20Yan%20and%20Hongsheng%20Li%20and%20Conghui%20He%20and%20Weijia%20Li%0AAbstract%3A%20%20%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20rapid%20progress%2C%0Aparticularly%20in%20enhancing%20their%20reasoning%20capabilities.%20However%2C%20existing%0Areasoning%20benchmarks%20still%20primarily%20assess%20language-based%20reasoning%2C%20often%0Atreating%20visual%20input%20as%20replaceable%20context.%20To%20address%20this%20gap%2C%20we%20introduce%0ABLINK-Twice%2C%20a%20vision-centric%20reasoning%20benchmark%20grounded%20in%20challenging%0Aperceptual%20tasks.%20Instead%20of%20relying%20on%20external%20knowledge%2C%20our%20tasks%20require%0Amodels%20to%20reason%20from%20visual%20content%20alone%2C%20shifting%20the%20focus%20from%0Alanguage-based%20to%20image-grounded%20reasoning.%20Compared%20to%20prior%20perception%0Abenchmarks%2C%20it%20moves%20beyond%20shallow%20perception%20%28%22see%22%29%20and%20requires%0Afine-grained%20observation%20and%20analytical%20reasoning%20%28%22observe%22%29.%20BLINK-Twice%0Aintegrates%20three%20core%20components%3A%20seven%20types%20of%20visual%20challenges%20for%20testing%0Avisual%20reasoning%2C%20natural%20adversarial%20image%20pairs%20that%20enforce%20reliance%20on%0Avisual%20content%2C%20and%20annotated%20reasoning%20chains%20for%20fine-grained%20evaluation%20of%0Athe%20reasoning%20process%20rather%20than%20final%20answers%20alone.%20We%20evaluate%2020%20leading%0AMLLMs%2C%20including%2012%20foundation%20models%20and%208%20reasoning-enhanced%20models.%0ABLINK-Twice%20poses%20a%20significant%20challenge%20to%20current%20models.%20While%20existing%0Areasoning%20strategies%20in%20the%20language%20space-such%20as%20chain-of-thought%20or%0Aself-criticism%20can%20improve%20performance%2C%20they%20often%20result%20in%20unstable%20and%0Aredundant%20reasoning.%20We%20observe%20that%20repeated%20image%20observation%20improves%0Aperformance%20across%20models%2C%20and%20active%20visual%20interaction%2C%20as%20demonstrated%20by%0Amodels%20like%20o3%2C%20highlights%20the%20need%20for%20a%20new%20paradigm%20for%20vision%20reasoning.%0AThe%20dataset%20is%20publicly%20available%20at%20https%3A//github.com/PicoTrex/BLINK-Twice%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLINK-Twice%253A%2520You%2520see%252C%2520but%2520do%2520you%2520observe%253F%2520A%2520Reasoning%2520Benchmark%2520on%250A%2520%2520Visual%2520Perception%26entry.906535625%3DJunyan%2520Ye%2520and%2520Dongzhi%2520Jiang%2520and%2520Jun%2520He%2520and%2520Baichuan%2520Zhou%2520and%2520Zilong%2520Huang%2520and%2520Zhiyuan%2520Yan%2520and%2520Hongsheng%2520Li%2520and%2520Conghui%2520He%2520and%2520Weijia%2520Li%26entry.1292438233%3D%2520%2520Recently%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520made%2520rapid%2520progress%252C%250Aparticularly%2520in%2520enhancing%2520their%2520reasoning%2520capabilities.%2520However%252C%2520existing%250Areasoning%2520benchmarks%2520still%2520primarily%2520assess%2520language-based%2520reasoning%252C%2520often%250Atreating%2520visual%2520input%2520as%2520replaceable%2520context.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250ABLINK-Twice%252C%2520a%2520vision-centric%2520reasoning%2520benchmark%2520grounded%2520in%2520challenging%250Aperceptual%2520tasks.%2520Instead%2520of%2520relying%2520on%2520external%2520knowledge%252C%2520our%2520tasks%2520require%250Amodels%2520to%2520reason%2520from%2520visual%2520content%2520alone%252C%2520shifting%2520the%2520focus%2520from%250Alanguage-based%2520to%2520image-grounded%2520reasoning.%2520Compared%2520to%2520prior%2520perception%250Abenchmarks%252C%2520it%2520moves%2520beyond%2520shallow%2520perception%2520%2528%2522see%2522%2529%2520and%2520requires%250Afine-grained%2520observation%2520and%2520analytical%2520reasoning%2520%2528%2522observe%2522%2529.%2520BLINK-Twice%250Aintegrates%2520three%2520core%2520components%253A%2520seven%2520types%2520of%2520visual%2520challenges%2520for%2520testing%250Avisual%2520reasoning%252C%2520natural%2520adversarial%2520image%2520pairs%2520that%2520enforce%2520reliance%2520on%250Avisual%2520content%252C%2520and%2520annotated%2520reasoning%2520chains%2520for%2520fine-grained%2520evaluation%2520of%250Athe%2520reasoning%2520process%2520rather%2520than%2520final%2520answers%2520alone.%2520We%2520evaluate%252020%2520leading%250AMLLMs%252C%2520including%252012%2520foundation%2520models%2520and%25208%2520reasoning-enhanced%2520models.%250ABLINK-Twice%2520poses%2520a%2520significant%2520challenge%2520to%2520current%2520models.%2520While%2520existing%250Areasoning%2520strategies%2520in%2520the%2520language%2520space-such%2520as%2520chain-of-thought%2520or%250Aself-criticism%2520can%2520improve%2520performance%252C%2520they%2520often%2520result%2520in%2520unstable%2520and%250Aredundant%2520reasoning.%2520We%2520observe%2520that%2520repeated%2520image%2520observation%2520improves%250Aperformance%2520across%2520models%252C%2520and%2520active%2520visual%2520interaction%252C%2520as%2520demonstrated%2520by%250Amodels%2520like%2520o3%252C%2520highlights%2520the%2520need%2520for%2520a%2520new%2520paradigm%2520for%2520vision%2520reasoning.%250AThe%2520dataset%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/PicoTrex/BLINK-Twice%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLINK-Twice%3A%20You%20see%2C%20but%20do%20you%20observe%3F%20A%20Reasoning%20Benchmark%20on%0A%20%20Visual%20Perception&entry.906535625=Junyan%20Ye%20and%20Dongzhi%20Jiang%20and%20Jun%20He%20and%20Baichuan%20Zhou%20and%20Zilong%20Huang%20and%20Zhiyuan%20Yan%20and%20Hongsheng%20Li%20and%20Conghui%20He%20and%20Weijia%20Li&entry.1292438233=%20%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20rapid%20progress%2C%0Aparticularly%20in%20enhancing%20their%20reasoning%20capabilities.%20However%2C%20existing%0Areasoning%20benchmarks%20still%20primarily%20assess%20language-based%20reasoning%2C%20often%0Atreating%20visual%20input%20as%20replaceable%20context.%20To%20address%20this%20gap%2C%20we%20introduce%0ABLINK-Twice%2C%20a%20vision-centric%20reasoning%20benchmark%20grounded%20in%20challenging%0Aperceptual%20tasks.%20Instead%20of%20relying%20on%20external%20knowledge%2C%20our%20tasks%20require%0Amodels%20to%20reason%20from%20visual%20content%20alone%2C%20shifting%20the%20focus%20from%0Alanguage-based%20to%20image-grounded%20reasoning.%20Compared%20to%20prior%20perception%0Abenchmarks%2C%20it%20moves%20beyond%20shallow%20perception%20%28%22see%22%29%20and%20requires%0Afine-grained%20observation%20and%20analytical%20reasoning%20%28%22observe%22%29.%20BLINK-Twice%0Aintegrates%20three%20core%20components%3A%20seven%20types%20of%20visual%20challenges%20for%20testing%0Avisual%20reasoning%2C%20natural%20adversarial%20image%20pairs%20that%20enforce%20reliance%20on%0Avisual%20content%2C%20and%20annotated%20reasoning%20chains%20for%20fine-grained%20evaluation%20of%0Athe%20reasoning%20process%20rather%20than%20final%20answers%20alone.%20We%20evaluate%2020%20leading%0AMLLMs%2C%20including%2012%20foundation%20models%20and%208%20reasoning-enhanced%20models.%0ABLINK-Twice%20poses%20a%20significant%20challenge%20to%20current%20models.%20While%20existing%0Areasoning%20strategies%20in%20the%20language%20space-such%20as%20chain-of-thought%20or%0Aself-criticism%20can%20improve%20performance%2C%20they%20often%20result%20in%20unstable%20and%0Aredundant%20reasoning.%20We%20observe%20that%20repeated%20image%20observation%20improves%0Aperformance%20across%20models%2C%20and%20active%20visual%20interaction%2C%20as%20demonstrated%20by%0Amodels%20like%20o3%2C%20highlights%20the%20need%20for%20a%20new%20paradigm%20for%20vision%20reasoning.%0AThe%20dataset%20is%20publicly%20available%20at%20https%3A//github.com/PicoTrex/BLINK-Twice%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09361v1&entry.124074799=Read"},
{"title": "The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using\n  4D Radar", "author": "William Muckelroy III and Mohammed Alsakabi and John Dolan and Ozan Tonguz", "abstract": "  LiDAR's dense, sharp point cloud (PC) representations of the surrounding\nenvironment enable accurate perception and significantly improve road safety by\noffering greater scene awareness and understanding. However, LiDAR's high cost\ncontinues to restrict the broad adoption of high-level Autonomous Driving (AD)\nsystems in commercially available vehicles. Prior research has shown progress\ntowards circumventing the need for LiDAR by training a neural network, using\nLiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds\nusing only 4D Radars. One of the best examples is a neural network created to\ntrain a more efficient radar target detector with a modular 2D convolutional\nneural network (CNN) backbone and a temporal coherence network at its core that\nuses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we\ninvestigate the impact of higher-capacity segmentation backbones on the quality\nof the produced point clouds. Our results show that while very high-capacity\nmodels may actually hurt performance, an optimal segmentation backbone can\nprovide a 23.7% improvement over the state-of-the-art (SOTA).\n", "link": "http://arxiv.org/abs/2509.19644v2", "date": "2025-10-10", "relevancy": 2.7774, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5695}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5554}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%202D%20Segmentation%20Backbones%20on%20Point%20Cloud%20Predictions%20Using%0A%20%204D%20Radar&body=Title%3A%20The%20Impact%20of%202D%20Segmentation%20Backbones%20on%20Point%20Cloud%20Predictions%20Using%0A%20%204D%20Radar%0AAuthor%3A%20William%20Muckelroy%20III%20and%20Mohammed%20Alsakabi%20and%20John%20Dolan%20and%20Ozan%20Tonguz%0AAbstract%3A%20%20%20LiDAR%27s%20dense%2C%20sharp%20point%20cloud%20%28PC%29%20representations%20of%20the%20surrounding%0Aenvironment%20enable%20accurate%20perception%20and%20significantly%20improve%20road%20safety%20by%0Aoffering%20greater%20scene%20awareness%20and%20understanding.%20However%2C%20LiDAR%27s%20high%20cost%0Acontinues%20to%20restrict%20the%20broad%20adoption%20of%20high-level%20Autonomous%20Driving%20%28AD%29%0Asystems%20in%20commercially%20available%20vehicles.%20Prior%20research%20has%20shown%20progress%0Atowards%20circumventing%20the%20need%20for%20LiDAR%20by%20training%20a%20neural%20network%2C%20using%0ALiDAR%20point%20clouds%20as%20ground%20truth%20%28GT%29%2C%20to%20produce%20LiDAR-like%203D%20point%20clouds%0Ausing%20only%204D%20Radars.%20One%20of%20the%20best%20examples%20is%20a%20neural%20network%20created%20to%0Atrain%20a%20more%20efficient%20radar%20target%20detector%20with%20a%20modular%202D%20convolutional%0Aneural%20network%20%28CNN%29%20backbone%20and%20a%20temporal%20coherence%20network%20at%20its%20core%20that%0Auses%20the%20RaDelft%20dataset%20for%20training%20%28see%20arXiv%3A2406.04723%29.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20impact%20of%20higher-capacity%20segmentation%20backbones%20on%20the%20quality%0Aof%20the%20produced%20point%20clouds.%20Our%20results%20show%20that%20while%20very%20high-capacity%0Amodels%20may%20actually%20hurt%20performance%2C%20an%20optimal%20segmentation%20backbone%20can%0Aprovide%20a%2023.7%25%20improvement%20over%20the%20state-of-the-art%20%28SOTA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19644v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%25202D%2520Segmentation%2520Backbones%2520on%2520Point%2520Cloud%2520Predictions%2520Using%250A%2520%25204D%2520Radar%26entry.906535625%3DWilliam%2520Muckelroy%2520III%2520and%2520Mohammed%2520Alsakabi%2520and%2520John%2520Dolan%2520and%2520Ozan%2520Tonguz%26entry.1292438233%3D%2520%2520LiDAR%2527s%2520dense%252C%2520sharp%2520point%2520cloud%2520%2528PC%2529%2520representations%2520of%2520the%2520surrounding%250Aenvironment%2520enable%2520accurate%2520perception%2520and%2520significantly%2520improve%2520road%2520safety%2520by%250Aoffering%2520greater%2520scene%2520awareness%2520and%2520understanding.%2520However%252C%2520LiDAR%2527s%2520high%2520cost%250Acontinues%2520to%2520restrict%2520the%2520broad%2520adoption%2520of%2520high-level%2520Autonomous%2520Driving%2520%2528AD%2529%250Asystems%2520in%2520commercially%2520available%2520vehicles.%2520Prior%2520research%2520has%2520shown%2520progress%250Atowards%2520circumventing%2520the%2520need%2520for%2520LiDAR%2520by%2520training%2520a%2520neural%2520network%252C%2520using%250ALiDAR%2520point%2520clouds%2520as%2520ground%2520truth%2520%2528GT%2529%252C%2520to%2520produce%2520LiDAR-like%25203D%2520point%2520clouds%250Ausing%2520only%25204D%2520Radars.%2520One%2520of%2520the%2520best%2520examples%2520is%2520a%2520neural%2520network%2520created%2520to%250Atrain%2520a%2520more%2520efficient%2520radar%2520target%2520detector%2520with%2520a%2520modular%25202D%2520convolutional%250Aneural%2520network%2520%2528CNN%2529%2520backbone%2520and%2520a%2520temporal%2520coherence%2520network%2520at%2520its%2520core%2520that%250Auses%2520the%2520RaDelft%2520dataset%2520for%2520training%2520%2528see%2520arXiv%253A2406.04723%2529.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520the%2520impact%2520of%2520higher-capacity%2520segmentation%2520backbones%2520on%2520the%2520quality%250Aof%2520the%2520produced%2520point%2520clouds.%2520Our%2520results%2520show%2520that%2520while%2520very%2520high-capacity%250Amodels%2520may%2520actually%2520hurt%2520performance%252C%2520an%2520optimal%2520segmentation%2520backbone%2520can%250Aprovide%2520a%252023.7%2525%2520improvement%2520over%2520the%2520state-of-the-art%2520%2528SOTA%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19644v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%202D%20Segmentation%20Backbones%20on%20Point%20Cloud%20Predictions%20Using%0A%20%204D%20Radar&entry.906535625=William%20Muckelroy%20III%20and%20Mohammed%20Alsakabi%20and%20John%20Dolan%20and%20Ozan%20Tonguz&entry.1292438233=%20%20LiDAR%27s%20dense%2C%20sharp%20point%20cloud%20%28PC%29%20representations%20of%20the%20surrounding%0Aenvironment%20enable%20accurate%20perception%20and%20significantly%20improve%20road%20safety%20by%0Aoffering%20greater%20scene%20awareness%20and%20understanding.%20However%2C%20LiDAR%27s%20high%20cost%0Acontinues%20to%20restrict%20the%20broad%20adoption%20of%20high-level%20Autonomous%20Driving%20%28AD%29%0Asystems%20in%20commercially%20available%20vehicles.%20Prior%20research%20has%20shown%20progress%0Atowards%20circumventing%20the%20need%20for%20LiDAR%20by%20training%20a%20neural%20network%2C%20using%0ALiDAR%20point%20clouds%20as%20ground%20truth%20%28GT%29%2C%20to%20produce%20LiDAR-like%203D%20point%20clouds%0Ausing%20only%204D%20Radars.%20One%20of%20the%20best%20examples%20is%20a%20neural%20network%20created%20to%0Atrain%20a%20more%20efficient%20radar%20target%20detector%20with%20a%20modular%202D%20convolutional%0Aneural%20network%20%28CNN%29%20backbone%20and%20a%20temporal%20coherence%20network%20at%20its%20core%20that%0Auses%20the%20RaDelft%20dataset%20for%20training%20%28see%20arXiv%3A2406.04723%29.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20impact%20of%20higher-capacity%20segmentation%20backbones%20on%20the%20quality%0Aof%20the%20produced%20point%20clouds.%20Our%20results%20show%20that%20while%20very%20high-capacity%0Amodels%20may%20actually%20hurt%20performance%2C%20an%20optimal%20segmentation%20backbone%20can%0Aprovide%20a%2023.7%25%20improvement%20over%20the%20state-of-the-art%20%28SOTA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19644v2&entry.124074799=Read"},
{"title": "Utilizing dynamic sparsity on pretrained DETR", "author": "Reza Sedghi and Anand Subramoney and David Kappel", "abstract": "  Efficient inference with transformer-based models remains a challenge,\nespecially in vision tasks like object detection. We analyze the inherent\nsparsity in the MLP layers of DETR and introduce two methods to exploit it\nwithout retraining. First, we propose Static Indicator-Based Sparsification\n(SIBS), a heuristic method that predicts neuron inactivity based on fixed\nactivation patterns. While simple, SIBS offers limited gains due to the\ninput-dependent nature of sparsity. To address this, we introduce Micro-Gated\nSparsification (MGS), a lightweight gating mechanism trained on top of a\npretrained DETR. MGS predicts dynamic sparsity using a small linear layer and\nachieves up to 85 to 95% activation sparsity. Experiments on the COCO dataset\nshow that MGS maintains or even improves performance while significantly\nreducing computation. Our method offers a practical, input-adaptive approach to\nsparsification, enabling efficient deployment of pretrained vision transformers\nwithout full model retraining.\n", "link": "http://arxiv.org/abs/2510.09380v1", "date": "2025-10-10", "relevancy": 2.7716, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5809}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.546}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20dynamic%20sparsity%20on%20pretrained%20DETR&body=Title%3A%20Utilizing%20dynamic%20sparsity%20on%20pretrained%20DETR%0AAuthor%3A%20Reza%20Sedghi%20and%20Anand%20Subramoney%20and%20David%20Kappel%0AAbstract%3A%20%20%20Efficient%20inference%20with%20transformer-based%20models%20remains%20a%20challenge%2C%0Aespecially%20in%20vision%20tasks%20like%20object%20detection.%20We%20analyze%20the%20inherent%0Asparsity%20in%20the%20MLP%20layers%20of%20DETR%20and%20introduce%20two%20methods%20to%20exploit%20it%0Awithout%20retraining.%20First%2C%20we%20propose%20Static%20Indicator-Based%20Sparsification%0A%28SIBS%29%2C%20a%20heuristic%20method%20that%20predicts%20neuron%20inactivity%20based%20on%20fixed%0Aactivation%20patterns.%20While%20simple%2C%20SIBS%20offers%20limited%20gains%20due%20to%20the%0Ainput-dependent%20nature%20of%20sparsity.%20To%20address%20this%2C%20we%20introduce%20Micro-Gated%0ASparsification%20%28MGS%29%2C%20a%20lightweight%20gating%20mechanism%20trained%20on%20top%20of%20a%0Apretrained%20DETR.%20MGS%20predicts%20dynamic%20sparsity%20using%20a%20small%20linear%20layer%20and%0Aachieves%20up%20to%2085%20to%2095%25%20activation%20sparsity.%20Experiments%20on%20the%20COCO%20dataset%0Ashow%20that%20MGS%20maintains%20or%20even%20improves%20performance%20while%20significantly%0Areducing%20computation.%20Our%20method%20offers%20a%20practical%2C%20input-adaptive%20approach%20to%0Asparsification%2C%20enabling%20efficient%20deployment%20of%20pretrained%20vision%20transformers%0Awithout%20full%20model%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520dynamic%2520sparsity%2520on%2520pretrained%2520DETR%26entry.906535625%3DReza%2520Sedghi%2520and%2520Anand%2520Subramoney%2520and%2520David%2520Kappel%26entry.1292438233%3D%2520%2520Efficient%2520inference%2520with%2520transformer-based%2520models%2520remains%2520a%2520challenge%252C%250Aespecially%2520in%2520vision%2520tasks%2520like%2520object%2520detection.%2520We%2520analyze%2520the%2520inherent%250Asparsity%2520in%2520the%2520MLP%2520layers%2520of%2520DETR%2520and%2520introduce%2520two%2520methods%2520to%2520exploit%2520it%250Awithout%2520retraining.%2520First%252C%2520we%2520propose%2520Static%2520Indicator-Based%2520Sparsification%250A%2528SIBS%2529%252C%2520a%2520heuristic%2520method%2520that%2520predicts%2520neuron%2520inactivity%2520based%2520on%2520fixed%250Aactivation%2520patterns.%2520While%2520simple%252C%2520SIBS%2520offers%2520limited%2520gains%2520due%2520to%2520the%250Ainput-dependent%2520nature%2520of%2520sparsity.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Micro-Gated%250ASparsification%2520%2528MGS%2529%252C%2520a%2520lightweight%2520gating%2520mechanism%2520trained%2520on%2520top%2520of%2520a%250Apretrained%2520DETR.%2520MGS%2520predicts%2520dynamic%2520sparsity%2520using%2520a%2520small%2520linear%2520layer%2520and%250Aachieves%2520up%2520to%252085%2520to%252095%2525%2520activation%2520sparsity.%2520Experiments%2520on%2520the%2520COCO%2520dataset%250Ashow%2520that%2520MGS%2520maintains%2520or%2520even%2520improves%2520performance%2520while%2520significantly%250Areducing%2520computation.%2520Our%2520method%2520offers%2520a%2520practical%252C%2520input-adaptive%2520approach%2520to%250Asparsification%252C%2520enabling%2520efficient%2520deployment%2520of%2520pretrained%2520vision%2520transformers%250Awithout%2520full%2520model%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20dynamic%20sparsity%20on%20pretrained%20DETR&entry.906535625=Reza%20Sedghi%20and%20Anand%20Subramoney%20and%20David%20Kappel&entry.1292438233=%20%20Efficient%20inference%20with%20transformer-based%20models%20remains%20a%20challenge%2C%0Aespecially%20in%20vision%20tasks%20like%20object%20detection.%20We%20analyze%20the%20inherent%0Asparsity%20in%20the%20MLP%20layers%20of%20DETR%20and%20introduce%20two%20methods%20to%20exploit%20it%0Awithout%20retraining.%20First%2C%20we%20propose%20Static%20Indicator-Based%20Sparsification%0A%28SIBS%29%2C%20a%20heuristic%20method%20that%20predicts%20neuron%20inactivity%20based%20on%20fixed%0Aactivation%20patterns.%20While%20simple%2C%20SIBS%20offers%20limited%20gains%20due%20to%20the%0Ainput-dependent%20nature%20of%20sparsity.%20To%20address%20this%2C%20we%20introduce%20Micro-Gated%0ASparsification%20%28MGS%29%2C%20a%20lightweight%20gating%20mechanism%20trained%20on%20top%20of%20a%0Apretrained%20DETR.%20MGS%20predicts%20dynamic%20sparsity%20using%20a%20small%20linear%20layer%20and%0Aachieves%20up%20to%2085%20to%2095%25%20activation%20sparsity.%20Experiments%20on%20the%20COCO%20dataset%0Ashow%20that%20MGS%20maintains%20or%20even%20improves%20performance%20while%20significantly%0Areducing%20computation.%20Our%20method%20offers%20a%20practical%2C%20input-adaptive%20approach%20to%0Asparsification%2C%20enabling%20efficient%20deployment%20of%20pretrained%20vision%20transformers%0Awithout%20full%20model%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09380v1&entry.124074799=Read"},
{"title": "Zero-shot image privacy classification with Vision-Language Models", "author": "Alina Elena Baia and Alessio Xompero and Andrea Cavallaro", "abstract": "  While specialized learning-based models have historically dominated image\nprivacy prediction, the current literature increasingly favours adopting large\nVision-Language Models (VLMs) designed for generic tasks. This trend risks\noverlooking the performance ceiling set by purpose-built models due to a lack\nof systematic evaluation. To address this problem, we establish a zero-shot\nbenchmark for image privacy classification, enabling a fair comparison. We\nevaluate the top-3 open-source VLMs, according to a privacy benchmark, using\ntask-aligned prompts and we contrast their performance, efficiency, and\nrobustness against established vision-only and multi-modal methods.\nCounter-intuitively, our results show that VLMs, despite their\nresource-intensive nature in terms of high parameter count and slower\ninference, currently lag behind specialized, smaller models in privacy\nprediction accuracy. We also find that VLMs exhibit higher robustness to image\nperturbations.\n", "link": "http://arxiv.org/abs/2510.09253v1", "date": "2025-10-10", "relevancy": 2.7289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20image%20privacy%20classification%20with%20Vision-Language%20Models&body=Title%3A%20Zero-shot%20image%20privacy%20classification%20with%20Vision-Language%20Models%0AAuthor%3A%20Alina%20Elena%20Baia%20and%20Alessio%20Xompero%20and%20Andrea%20Cavallaro%0AAbstract%3A%20%20%20While%20specialized%20learning-based%20models%20have%20historically%20dominated%20image%0Aprivacy%20prediction%2C%20the%20current%20literature%20increasingly%20favours%20adopting%20large%0AVision-Language%20Models%20%28VLMs%29%20designed%20for%20generic%20tasks.%20This%20trend%20risks%0Aoverlooking%20the%20performance%20ceiling%20set%20by%20purpose-built%20models%20due%20to%20a%20lack%0Aof%20systematic%20evaluation.%20To%20address%20this%20problem%2C%20we%20establish%20a%20zero-shot%0Abenchmark%20for%20image%20privacy%20classification%2C%20enabling%20a%20fair%20comparison.%20We%0Aevaluate%20the%20top-3%20open-source%20VLMs%2C%20according%20to%20a%20privacy%20benchmark%2C%20using%0Atask-aligned%20prompts%20and%20we%20contrast%20their%20performance%2C%20efficiency%2C%20and%0Arobustness%20against%20established%20vision-only%20and%20multi-modal%20methods.%0ACounter-intuitively%2C%20our%20results%20show%20that%20VLMs%2C%20despite%20their%0Aresource-intensive%20nature%20in%20terms%20of%20high%20parameter%20count%20and%20slower%0Ainference%2C%20currently%20lag%20behind%20specialized%2C%20smaller%20models%20in%20privacy%0Aprediction%20accuracy.%20We%20also%20find%20that%20VLMs%20exhibit%20higher%20robustness%20to%20image%0Aperturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520image%2520privacy%2520classification%2520with%2520Vision-Language%2520Models%26entry.906535625%3DAlina%2520Elena%2520Baia%2520and%2520Alessio%2520Xompero%2520and%2520Andrea%2520Cavallaro%26entry.1292438233%3D%2520%2520While%2520specialized%2520learning-based%2520models%2520have%2520historically%2520dominated%2520image%250Aprivacy%2520prediction%252C%2520the%2520current%2520literature%2520increasingly%2520favours%2520adopting%2520large%250AVision-Language%2520Models%2520%2528VLMs%2529%2520designed%2520for%2520generic%2520tasks.%2520This%2520trend%2520risks%250Aoverlooking%2520the%2520performance%2520ceiling%2520set%2520by%2520purpose-built%2520models%2520due%2520to%2520a%2520lack%250Aof%2520systematic%2520evaluation.%2520To%2520address%2520this%2520problem%252C%2520we%2520establish%2520a%2520zero-shot%250Abenchmark%2520for%2520image%2520privacy%2520classification%252C%2520enabling%2520a%2520fair%2520comparison.%2520We%250Aevaluate%2520the%2520top-3%2520open-source%2520VLMs%252C%2520according%2520to%2520a%2520privacy%2520benchmark%252C%2520using%250Atask-aligned%2520prompts%2520and%2520we%2520contrast%2520their%2520performance%252C%2520efficiency%252C%2520and%250Arobustness%2520against%2520established%2520vision-only%2520and%2520multi-modal%2520methods.%250ACounter-intuitively%252C%2520our%2520results%2520show%2520that%2520VLMs%252C%2520despite%2520their%250Aresource-intensive%2520nature%2520in%2520terms%2520of%2520high%2520parameter%2520count%2520and%2520slower%250Ainference%252C%2520currently%2520lag%2520behind%2520specialized%252C%2520smaller%2520models%2520in%2520privacy%250Aprediction%2520accuracy.%2520We%2520also%2520find%2520that%2520VLMs%2520exhibit%2520higher%2520robustness%2520to%2520image%250Aperturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20image%20privacy%20classification%20with%20Vision-Language%20Models&entry.906535625=Alina%20Elena%20Baia%20and%20Alessio%20Xompero%20and%20Andrea%20Cavallaro&entry.1292438233=%20%20While%20specialized%20learning-based%20models%20have%20historically%20dominated%20image%0Aprivacy%20prediction%2C%20the%20current%20literature%20increasingly%20favours%20adopting%20large%0AVision-Language%20Models%20%28VLMs%29%20designed%20for%20generic%20tasks.%20This%20trend%20risks%0Aoverlooking%20the%20performance%20ceiling%20set%20by%20purpose-built%20models%20due%20to%20a%20lack%0Aof%20systematic%20evaluation.%20To%20address%20this%20problem%2C%20we%20establish%20a%20zero-shot%0Abenchmark%20for%20image%20privacy%20classification%2C%20enabling%20a%20fair%20comparison.%20We%0Aevaluate%20the%20top-3%20open-source%20VLMs%2C%20according%20to%20a%20privacy%20benchmark%2C%20using%0Atask-aligned%20prompts%20and%20we%20contrast%20their%20performance%2C%20efficiency%2C%20and%0Arobustness%20against%20established%20vision-only%20and%20multi-modal%20methods.%0ACounter-intuitively%2C%20our%20results%20show%20that%20VLMs%2C%20despite%20their%0Aresource-intensive%20nature%20in%20terms%20of%20high%20parameter%20count%20and%20slower%0Ainference%2C%20currently%20lag%20behind%20specialized%2C%20smaller%20models%20in%20privacy%0Aprediction%20accuracy.%20We%20also%20find%20that%20VLMs%20exhibit%20higher%20robustness%20to%20image%0Aperturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09253v1&entry.124074799=Read"},
{"title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models", "author": "Qinghongbing Xie and Zhaoyuan Xia and Feng Zhu and Lijun Gong and Ziyue Li and Rui Zhao and Long Zeng", "abstract": "  Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has\nattracted much attention due to its importance for Autonomous Driving, Embodied\nAI and General Artificial Intelligence. Existing spatial-temporal benchmarks\nmainly focus on egocentric perspective reasoning with images/video context, or\ngeographic perspective reasoning with graphics context (eg. a map), thus fail\nto assess VLMs' geographic spatial-temporal intelligence with both images/video\nand graphics context, which is important for areas like traffic management and\nemergency response. To address the gaps, we introduce Geo-Temporal Reasoning\nbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of\nmoving targets in a large-scale camera network. GTR-Bench is more challenging\nas it requires multiple perspective switches between maps and videos, joint\nreasoning across multiple videos with non-overlapping fields of view, and\ninference over spatial-temporal regions that are unobserved by any video\ncontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that\neven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags\nbehind human performance (78.61%) on geo-temporal reasoning. Moreover, our\ncomprehensive analysis on GTR-Bench reveals three primary deficiencies of\ncurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by\nan imbalanced utilization of spatial-temporal context. (2) VLMs are weak in\ntemporal forecasting, which leads to worse performance on temporal-emphasized\ntasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to\ncomprehend or align the map data with multi-view video inputs. We believe\nGTR-Bench offers valuable insights and opens up new opportunities for research\nand applications in spatial-temporal intelligence. Benchmark and code will be\nreleased at https://github.com/X-Luffy/GTR-Bench.\n", "link": "http://arxiv.org/abs/2510.07791v2", "date": "2025-10-10", "relevancy": 2.7271, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTR-Bench%3A%20Evaluating%20Geo-Temporal%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20GTR-Bench%3A%20Evaluating%20Geo-Temporal%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Qinghongbing%20Xie%20and%20Zhaoyuan%20Xia%20and%20Feng%20Zhu%20and%20Lijun%20Gong%20and%20Ziyue%20Li%20and%20Rui%20Zhao%20and%20Long%20Zeng%0AAbstract%3A%20%20%20Recently%20spatial-temporal%20intelligence%20of%20Visual-Language%20Models%20%28VLMs%29%20has%0Aattracted%20much%20attention%20due%20to%20its%20importance%20for%20Autonomous%20Driving%2C%20Embodied%0AAI%20and%20General%20Artificial%20Intelligence.%20Existing%20spatial-temporal%20benchmarks%0Amainly%20focus%20on%20egocentric%20perspective%20reasoning%20with%20images/video%20context%2C%20or%0Ageographic%20perspective%20reasoning%20with%20graphics%20context%20%28eg.%20a%20map%29%2C%20thus%20fail%0Ato%20assess%20VLMs%27%20geographic%20spatial-temporal%20intelligence%20with%20both%20images/video%0Aand%20graphics%20context%2C%20which%20is%20important%20for%20areas%20like%20traffic%20management%20and%0Aemergency%20response.%20To%20address%20the%20gaps%2C%20we%20introduce%20Geo-Temporal%20Reasoning%0Abenchmark%20%28GTR-Bench%29%2C%20a%20novel%20challenge%20for%20geographic%20temporal%20reasoning%20of%0Amoving%20targets%20in%20a%20large-scale%20camera%20network.%20GTR-Bench%20is%20more%20challenging%0Aas%20it%20requires%20multiple%20perspective%20switches%20between%20maps%20and%20videos%2C%20joint%0Areasoning%20across%20multiple%20videos%20with%20non-overlapping%20fields%20of%20view%2C%20and%0Ainference%20over%20spatial-temporal%20regions%20that%20are%20unobserved%20by%20any%20video%0Acontext.%20Evaluations%20of%20more%20than%2010%20popular%20VLMs%20on%20GTR-Bench%20demonstrate%20that%0Aeven%20the%20best%20proprietary%20model%2C%20Gemini-2.5-Pro%20%2834.9%25%29%2C%20significantly%20lags%0Abehind%20human%20performance%20%2878.61%25%29%20on%20geo-temporal%20reasoning.%20Moreover%2C%20our%0Acomprehensive%20analysis%20on%20GTR-Bench%20reveals%20three%20primary%20deficiencies%20of%0Acurrent%20models%20for%20geo-temporal%20reasoning.%20%281%29%20VLMs%27%20reasoning%20is%20impaired%20by%0Aan%20imbalanced%20utilization%20of%20spatial-temporal%20context.%20%282%29%20VLMs%20are%20weak%20in%0Atemporal%20forecasting%2C%20which%20leads%20to%20worse%20performance%20on%20temporal-emphasized%0Atasks%20than%20on%20spatial-emphasized%20tasks.%20%283%29%20VLMs%20lack%20the%20proficiency%20to%0Acomprehend%20or%20align%20the%20map%20data%20with%20multi-view%20video%20inputs.%20We%20believe%0AGTR-Bench%20offers%20valuable%20insights%20and%20opens%20up%20new%20opportunities%20for%20research%0Aand%20applications%20in%20spatial-temporal%20intelligence.%20Benchmark%20and%20code%20will%20be%0Areleased%20at%20https%3A//github.com/X-Luffy/GTR-Bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTR-Bench%253A%2520Evaluating%2520Geo-Temporal%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DQinghongbing%2520Xie%2520and%2520Zhaoyuan%2520Xia%2520and%2520Feng%2520Zhu%2520and%2520Lijun%2520Gong%2520and%2520Ziyue%2520Li%2520and%2520Rui%2520Zhao%2520and%2520Long%2520Zeng%26entry.1292438233%3D%2520%2520Recently%2520spatial-temporal%2520intelligence%2520of%2520Visual-Language%2520Models%2520%2528VLMs%2529%2520has%250Aattracted%2520much%2520attention%2520due%2520to%2520its%2520importance%2520for%2520Autonomous%2520Driving%252C%2520Embodied%250AAI%2520and%2520General%2520Artificial%2520Intelligence.%2520Existing%2520spatial-temporal%2520benchmarks%250Amainly%2520focus%2520on%2520egocentric%2520perspective%2520reasoning%2520with%2520images/video%2520context%252C%2520or%250Ageographic%2520perspective%2520reasoning%2520with%2520graphics%2520context%2520%2528eg.%2520a%2520map%2529%252C%2520thus%2520fail%250Ato%2520assess%2520VLMs%2527%2520geographic%2520spatial-temporal%2520intelligence%2520with%2520both%2520images/video%250Aand%2520graphics%2520context%252C%2520which%2520is%2520important%2520for%2520areas%2520like%2520traffic%2520management%2520and%250Aemergency%2520response.%2520To%2520address%2520the%2520gaps%252C%2520we%2520introduce%2520Geo-Temporal%2520Reasoning%250Abenchmark%2520%2528GTR-Bench%2529%252C%2520a%2520novel%2520challenge%2520for%2520geographic%2520temporal%2520reasoning%2520of%250Amoving%2520targets%2520in%2520a%2520large-scale%2520camera%2520network.%2520GTR-Bench%2520is%2520more%2520challenging%250Aas%2520it%2520requires%2520multiple%2520perspective%2520switches%2520between%2520maps%2520and%2520videos%252C%2520joint%250Areasoning%2520across%2520multiple%2520videos%2520with%2520non-overlapping%2520fields%2520of%2520view%252C%2520and%250Ainference%2520over%2520spatial-temporal%2520regions%2520that%2520are%2520unobserved%2520by%2520any%2520video%250Acontext.%2520Evaluations%2520of%2520more%2520than%252010%2520popular%2520VLMs%2520on%2520GTR-Bench%2520demonstrate%2520that%250Aeven%2520the%2520best%2520proprietary%2520model%252C%2520Gemini-2.5-Pro%2520%252834.9%2525%2529%252C%2520significantly%2520lags%250Abehind%2520human%2520performance%2520%252878.61%2525%2529%2520on%2520geo-temporal%2520reasoning.%2520Moreover%252C%2520our%250Acomprehensive%2520analysis%2520on%2520GTR-Bench%2520reveals%2520three%2520primary%2520deficiencies%2520of%250Acurrent%2520models%2520for%2520geo-temporal%2520reasoning.%2520%25281%2529%2520VLMs%2527%2520reasoning%2520is%2520impaired%2520by%250Aan%2520imbalanced%2520utilization%2520of%2520spatial-temporal%2520context.%2520%25282%2529%2520VLMs%2520are%2520weak%2520in%250Atemporal%2520forecasting%252C%2520which%2520leads%2520to%2520worse%2520performance%2520on%2520temporal-emphasized%250Atasks%2520than%2520on%2520spatial-emphasized%2520tasks.%2520%25283%2529%2520VLMs%2520lack%2520the%2520proficiency%2520to%250Acomprehend%2520or%2520align%2520the%2520map%2520data%2520with%2520multi-view%2520video%2520inputs.%2520We%2520believe%250AGTR-Bench%2520offers%2520valuable%2520insights%2520and%2520opens%2520up%2520new%2520opportunities%2520for%2520research%250Aand%2520applications%2520in%2520spatial-temporal%2520intelligence.%2520Benchmark%2520and%2520code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/X-Luffy/GTR-Bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTR-Bench%3A%20Evaluating%20Geo-Temporal%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Qinghongbing%20Xie%20and%20Zhaoyuan%20Xia%20and%20Feng%20Zhu%20and%20Lijun%20Gong%20and%20Ziyue%20Li%20and%20Rui%20Zhao%20and%20Long%20Zeng&entry.1292438233=%20%20Recently%20spatial-temporal%20intelligence%20of%20Visual-Language%20Models%20%28VLMs%29%20has%0Aattracted%20much%20attention%20due%20to%20its%20importance%20for%20Autonomous%20Driving%2C%20Embodied%0AAI%20and%20General%20Artificial%20Intelligence.%20Existing%20spatial-temporal%20benchmarks%0Amainly%20focus%20on%20egocentric%20perspective%20reasoning%20with%20images/video%20context%2C%20or%0Ageographic%20perspective%20reasoning%20with%20graphics%20context%20%28eg.%20a%20map%29%2C%20thus%20fail%0Ato%20assess%20VLMs%27%20geographic%20spatial-temporal%20intelligence%20with%20both%20images/video%0Aand%20graphics%20context%2C%20which%20is%20important%20for%20areas%20like%20traffic%20management%20and%0Aemergency%20response.%20To%20address%20the%20gaps%2C%20we%20introduce%20Geo-Temporal%20Reasoning%0Abenchmark%20%28GTR-Bench%29%2C%20a%20novel%20challenge%20for%20geographic%20temporal%20reasoning%20of%0Amoving%20targets%20in%20a%20large-scale%20camera%20network.%20GTR-Bench%20is%20more%20challenging%0Aas%20it%20requires%20multiple%20perspective%20switches%20between%20maps%20and%20videos%2C%20joint%0Areasoning%20across%20multiple%20videos%20with%20non-overlapping%20fields%20of%20view%2C%20and%0Ainference%20over%20spatial-temporal%20regions%20that%20are%20unobserved%20by%20any%20video%0Acontext.%20Evaluations%20of%20more%20than%2010%20popular%20VLMs%20on%20GTR-Bench%20demonstrate%20that%0Aeven%20the%20best%20proprietary%20model%2C%20Gemini-2.5-Pro%20%2834.9%25%29%2C%20significantly%20lags%0Abehind%20human%20performance%20%2878.61%25%29%20on%20geo-temporal%20reasoning.%20Moreover%2C%20our%0Acomprehensive%20analysis%20on%20GTR-Bench%20reveals%20three%20primary%20deficiencies%20of%0Acurrent%20models%20for%20geo-temporal%20reasoning.%20%281%29%20VLMs%27%20reasoning%20is%20impaired%20by%0Aan%20imbalanced%20utilization%20of%20spatial-temporal%20context.%20%282%29%20VLMs%20are%20weak%20in%0Atemporal%20forecasting%2C%20which%20leads%20to%20worse%20performance%20on%20temporal-emphasized%0Atasks%20than%20on%20spatial-emphasized%20tasks.%20%283%29%20VLMs%20lack%20the%20proficiency%20to%0Acomprehend%20or%20align%20the%20map%20data%20with%20multi-view%20video%20inputs.%20We%20believe%0AGTR-Bench%20offers%20valuable%20insights%20and%20opens%20up%20new%20opportunities%20for%20research%0Aand%20applications%20in%20spatial-temporal%20intelligence.%20Benchmark%20and%20code%20will%20be%0Areleased%20at%20https%3A//github.com/X-Luffy/GTR-Bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07791v2&entry.124074799=Read"},
{"title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder", "author": "Elias Abad Rocamora and Christian Schlarmann and Naman Deep Singh and Yongtao Wu and Matthias Hein and Volkan Cevher", "abstract": "  Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. In multimodal\nretrieval tasks, LEAF improves the recall under adversarial noise over standard\nCLIP models. Finally, we show that robust text encoders facilitate better\nreconstruction of input text from its embedding via direct optimization. We\nopen-source our code ( https://github.com/LIONS-EPFL/LEAF ) and models (\nhttps://huggingface.co/LEAF-CLIP ).\n", "link": "http://arxiv.org/abs/2506.03355v2", "date": "2025-10-10", "relevancy": 2.7257, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness%20in%20Both%20Domains%3A%20CLIP%20Needs%20a%20Robust%20Text%20Encoder&body=Title%3A%20Robustness%20in%20Both%20Domains%3A%20CLIP%20Needs%20a%20Robust%20Text%20Encoder%0AAuthor%3A%20Elias%20Abad%20Rocamora%20and%20Christian%20Schlarmann%20and%20Naman%20Deep%20Singh%20and%20Yongtao%20Wu%20and%20Matthias%20Hein%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20Adversarial%20input%20attacks%20can%20cause%20a%20significant%20shift%20of%20CLIP%20embeddings.%0AThis%20can%20affect%20the%20downstream%20robustness%20of%20models%20incorporating%20CLIP%20in%20the%0Apipeline%2C%20such%20as%20text-to-image%20generative%20models%20or%20large%20vision%20language%0Amodels.%20While%20some%20efforts%20have%20been%20done%20towards%20making%20the%20CLIP%20image%0Aencoders%20robust%2C%20the%20robustness%20of%20text%20encoders%20remains%20unexplored.%20In%20this%0Awork%2C%20we%20cover%20this%20gap%20in%20the%20literature.%20We%20propose%20LEAF%3A%20an%20efficient%0Aadversarial%20finetuning%20method%20for%20the%20text%20domain%2C%20with%20the%20ability%20to%20scale%20to%0Alarge%20CLIP%20models.%20Our%20models%20significantly%20improve%20the%20zero-shot%20adversarial%0Aaccuracy%20in%20the%20text%20domain%2C%20while%20maintaining%20the%20vision%20performance%20provided%0Aby%20robust%20image%20encoders.%20When%20combined%20with%20text-to-image%20diffusion%20models%2C%20we%0Acan%20improve%20the%20generation%20quality%20under%20adversarial%20noise.%20In%20multimodal%0Aretrieval%20tasks%2C%20LEAF%20improves%20the%20recall%20under%20adversarial%20noise%20over%20standard%0ACLIP%20models.%20Finally%2C%20we%20show%20that%20robust%20text%20encoders%20facilitate%20better%0Areconstruction%20of%20input%20text%20from%20its%20embedding%20via%20direct%20optimization.%20We%0Aopen-source%20our%20code%20%28%20https%3A//github.com/LIONS-EPFL/LEAF%20%29%20and%20models%20%28%0Ahttps%3A//huggingface.co/LEAF-CLIP%20%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness%2520in%2520Both%2520Domains%253A%2520CLIP%2520Needs%2520a%2520Robust%2520Text%2520Encoder%26entry.906535625%3DElias%2520Abad%2520Rocamora%2520and%2520Christian%2520Schlarmann%2520and%2520Naman%2520Deep%2520Singh%2520and%2520Yongtao%2520Wu%2520and%2520Matthias%2520Hein%2520and%2520Volkan%2520Cevher%26entry.1292438233%3D%2520%2520Adversarial%2520input%2520attacks%2520can%2520cause%2520a%2520significant%2520shift%2520of%2520CLIP%2520embeddings.%250AThis%2520can%2520affect%2520the%2520downstream%2520robustness%2520of%2520models%2520incorporating%2520CLIP%2520in%2520the%250Apipeline%252C%2520such%2520as%2520text-to-image%2520generative%2520models%2520or%2520large%2520vision%2520language%250Amodels.%2520While%2520some%2520efforts%2520have%2520been%2520done%2520towards%2520making%2520the%2520CLIP%2520image%250Aencoders%2520robust%252C%2520the%2520robustness%2520of%2520text%2520encoders%2520remains%2520unexplored.%2520In%2520this%250Awork%252C%2520we%2520cover%2520this%2520gap%2520in%2520the%2520literature.%2520We%2520propose%2520LEAF%253A%2520an%2520efficient%250Aadversarial%2520finetuning%2520method%2520for%2520the%2520text%2520domain%252C%2520with%2520the%2520ability%2520to%2520scale%2520to%250Alarge%2520CLIP%2520models.%2520Our%2520models%2520significantly%2520improve%2520the%2520zero-shot%2520adversarial%250Aaccuracy%2520in%2520the%2520text%2520domain%252C%2520while%2520maintaining%2520the%2520vision%2520performance%2520provided%250Aby%2520robust%2520image%2520encoders.%2520When%2520combined%2520with%2520text-to-image%2520diffusion%2520models%252C%2520we%250Acan%2520improve%2520the%2520generation%2520quality%2520under%2520adversarial%2520noise.%2520In%2520multimodal%250Aretrieval%2520tasks%252C%2520LEAF%2520improves%2520the%2520recall%2520under%2520adversarial%2520noise%2520over%2520standard%250ACLIP%2520models.%2520Finally%252C%2520we%2520show%2520that%2520robust%2520text%2520encoders%2520facilitate%2520better%250Areconstruction%2520of%2520input%2520text%2520from%2520its%2520embedding%2520via%2520direct%2520optimization.%2520We%250Aopen-source%2520our%2520code%2520%2528%2520https%253A//github.com/LIONS-EPFL/LEAF%2520%2529%2520and%2520models%2520%2528%250Ahttps%253A//huggingface.co/LEAF-CLIP%2520%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness%20in%20Both%20Domains%3A%20CLIP%20Needs%20a%20Robust%20Text%20Encoder&entry.906535625=Elias%20Abad%20Rocamora%20and%20Christian%20Schlarmann%20and%20Naman%20Deep%20Singh%20and%20Yongtao%20Wu%20and%20Matthias%20Hein%20and%20Volkan%20Cevher&entry.1292438233=%20%20Adversarial%20input%20attacks%20can%20cause%20a%20significant%20shift%20of%20CLIP%20embeddings.%0AThis%20can%20affect%20the%20downstream%20robustness%20of%20models%20incorporating%20CLIP%20in%20the%0Apipeline%2C%20such%20as%20text-to-image%20generative%20models%20or%20large%20vision%20language%0Amodels.%20While%20some%20efforts%20have%20been%20done%20towards%20making%20the%20CLIP%20image%0Aencoders%20robust%2C%20the%20robustness%20of%20text%20encoders%20remains%20unexplored.%20In%20this%0Awork%2C%20we%20cover%20this%20gap%20in%20the%20literature.%20We%20propose%20LEAF%3A%20an%20efficient%0Aadversarial%20finetuning%20method%20for%20the%20text%20domain%2C%20with%20the%20ability%20to%20scale%20to%0Alarge%20CLIP%20models.%20Our%20models%20significantly%20improve%20the%20zero-shot%20adversarial%0Aaccuracy%20in%20the%20text%20domain%2C%20while%20maintaining%20the%20vision%20performance%20provided%0Aby%20robust%20image%20encoders.%20When%20combined%20with%20text-to-image%20diffusion%20models%2C%20we%0Acan%20improve%20the%20generation%20quality%20under%20adversarial%20noise.%20In%20multimodal%0Aretrieval%20tasks%2C%20LEAF%20improves%20the%20recall%20under%20adversarial%20noise%20over%20standard%0ACLIP%20models.%20Finally%2C%20we%20show%20that%20robust%20text%20encoders%20facilitate%20better%0Areconstruction%20of%20input%20text%20from%20its%20embedding%20via%20direct%20optimization.%20We%0Aopen-source%20our%20code%20%28%20https%3A//github.com/LIONS-EPFL/LEAF%20%29%20and%20models%20%28%0Ahttps%3A//huggingface.co/LEAF-CLIP%20%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03355v2&entry.124074799=Read"},
{"title": "D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt\n  Tuning in Vision-Language Models", "author": "Jisu Han and Wonjun Hwang", "abstract": "  Test-time adaptation paradigm provides flexibility towards domain shifts by\nperforming immediate adaptation on unlabeled target data from the source model.\nVision-Language Models (VLMs) leverage their generalization capabilities for\ndiverse downstream tasks, and test-time prompt tuning has emerged as a\nprominent solution for adapting VLMs. In this work, we explore contrastive VLMs\nand identify the modality gap caused by a single dominant feature dimension\nacross modalities. We observe that the dominant dimensions in both text and\nimage modalities exhibit high predictive sensitivity, and that constraining\ntheir influence can improve calibration error. Building on this insight, we\npropose dimensional entropy maximization that regularizes the distribution of\ntextual features toward uniformity to mitigate the dependency of dominant\ndimensions. Our method alleviates the degradation of calibration performance in\ntest-time prompt tuning, offering a simple yet effective solution to enhance\nthe reliability of VLMs in real-world deployment scenarios.\n", "link": "http://arxiv.org/abs/2510.09473v1", "date": "2025-10-10", "relevancy": 2.7245, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-TPT%3A%20Dimensional%20Entropy%20Maximization%20for%20Calibrating%20Test-Time%20Prompt%0A%20%20Tuning%20in%20Vision-Language%20Models&body=Title%3A%20D-TPT%3A%20Dimensional%20Entropy%20Maximization%20for%20Calibrating%20Test-Time%20Prompt%0A%20%20Tuning%20in%20Vision-Language%20Models%0AAuthor%3A%20Jisu%20Han%20and%20Wonjun%20Hwang%0AAbstract%3A%20%20%20Test-time%20adaptation%20paradigm%20provides%20flexibility%20towards%20domain%20shifts%20by%0Aperforming%20immediate%20adaptation%20on%20unlabeled%20target%20data%20from%20the%20source%20model.%0AVision-Language%20Models%20%28VLMs%29%20leverage%20their%20generalization%20capabilities%20for%0Adiverse%20downstream%20tasks%2C%20and%20test-time%20prompt%20tuning%20has%20emerged%20as%20a%0Aprominent%20solution%20for%20adapting%20VLMs.%20In%20this%20work%2C%20we%20explore%20contrastive%20VLMs%0Aand%20identify%20the%20modality%20gap%20caused%20by%20a%20single%20dominant%20feature%20dimension%0Aacross%20modalities.%20We%20observe%20that%20the%20dominant%20dimensions%20in%20both%20text%20and%0Aimage%20modalities%20exhibit%20high%20predictive%20sensitivity%2C%20and%20that%20constraining%0Atheir%20influence%20can%20improve%20calibration%20error.%20Building%20on%20this%20insight%2C%20we%0Apropose%20dimensional%20entropy%20maximization%20that%20regularizes%20the%20distribution%20of%0Atextual%20features%20toward%20uniformity%20to%20mitigate%20the%20dependency%20of%20dominant%0Adimensions.%20Our%20method%20alleviates%20the%20degradation%20of%20calibration%20performance%20in%0Atest-time%20prompt%20tuning%2C%20offering%20a%20simple%20yet%20effective%20solution%20to%20enhance%0Athe%20reliability%20of%20VLMs%20in%20real-world%20deployment%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-TPT%253A%2520Dimensional%2520Entropy%2520Maximization%2520for%2520Calibrating%2520Test-Time%2520Prompt%250A%2520%2520Tuning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DJisu%2520Han%2520and%2520Wonjun%2520Hwang%26entry.1292438233%3D%2520%2520Test-time%2520adaptation%2520paradigm%2520provides%2520flexibility%2520towards%2520domain%2520shifts%2520by%250Aperforming%2520immediate%2520adaptation%2520on%2520unlabeled%2520target%2520data%2520from%2520the%2520source%2520model.%250AVision-Language%2520Models%2520%2528VLMs%2529%2520leverage%2520their%2520generalization%2520capabilities%2520for%250Adiverse%2520downstream%2520tasks%252C%2520and%2520test-time%2520prompt%2520tuning%2520has%2520emerged%2520as%2520a%250Aprominent%2520solution%2520for%2520adapting%2520VLMs.%2520In%2520this%2520work%252C%2520we%2520explore%2520contrastive%2520VLMs%250Aand%2520identify%2520the%2520modality%2520gap%2520caused%2520by%2520a%2520single%2520dominant%2520feature%2520dimension%250Aacross%2520modalities.%2520We%2520observe%2520that%2520the%2520dominant%2520dimensions%2520in%2520both%2520text%2520and%250Aimage%2520modalities%2520exhibit%2520high%2520predictive%2520sensitivity%252C%2520and%2520that%2520constraining%250Atheir%2520influence%2520can%2520improve%2520calibration%2520error.%2520Building%2520on%2520this%2520insight%252C%2520we%250Apropose%2520dimensional%2520entropy%2520maximization%2520that%2520regularizes%2520the%2520distribution%2520of%250Atextual%2520features%2520toward%2520uniformity%2520to%2520mitigate%2520the%2520dependency%2520of%2520dominant%250Adimensions.%2520Our%2520method%2520alleviates%2520the%2520degradation%2520of%2520calibration%2520performance%2520in%250Atest-time%2520prompt%2520tuning%252C%2520offering%2520a%2520simple%2520yet%2520effective%2520solution%2520to%2520enhance%250Athe%2520reliability%2520of%2520VLMs%2520in%2520real-world%2520deployment%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-TPT%3A%20Dimensional%20Entropy%20Maximization%20for%20Calibrating%20Test-Time%20Prompt%0A%20%20Tuning%20in%20Vision-Language%20Models&entry.906535625=Jisu%20Han%20and%20Wonjun%20Hwang&entry.1292438233=%20%20Test-time%20adaptation%20paradigm%20provides%20flexibility%20towards%20domain%20shifts%20by%0Aperforming%20immediate%20adaptation%20on%20unlabeled%20target%20data%20from%20the%20source%20model.%0AVision-Language%20Models%20%28VLMs%29%20leverage%20their%20generalization%20capabilities%20for%0Adiverse%20downstream%20tasks%2C%20and%20test-time%20prompt%20tuning%20has%20emerged%20as%20a%0Aprominent%20solution%20for%20adapting%20VLMs.%20In%20this%20work%2C%20we%20explore%20contrastive%20VLMs%0Aand%20identify%20the%20modality%20gap%20caused%20by%20a%20single%20dominant%20feature%20dimension%0Aacross%20modalities.%20We%20observe%20that%20the%20dominant%20dimensions%20in%20both%20text%20and%0Aimage%20modalities%20exhibit%20high%20predictive%20sensitivity%2C%20and%20that%20constraining%0Atheir%20influence%20can%20improve%20calibration%20error.%20Building%20on%20this%20insight%2C%20we%0Apropose%20dimensional%20entropy%20maximization%20that%20regularizes%20the%20distribution%20of%0Atextual%20features%20toward%20uniformity%20to%20mitigate%20the%20dependency%20of%20dominant%0Adimensions.%20Our%20method%20alleviates%20the%20degradation%20of%20calibration%20performance%20in%0Atest-time%20prompt%20tuning%2C%20offering%20a%20simple%20yet%20effective%20solution%20to%20enhance%0Athe%20reliability%20of%20VLMs%20in%20real-world%20deployment%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09473v1&entry.124074799=Read"},
{"title": "PRNet: Original Information Is All You Have", "author": "PeiHuang Zheng and Yunlong Zhao and Zheng Cui and Yang Li", "abstract": "  Small object detection in aerial images suffers from severe information\ndegradation during feature extraction due to limited pixel representations,\nwhere shallow spatial details fail to align effectively with semantic\ninformation, leading to frequent misses and false positives. Existing FPN-based\nmethods attempt to mitigate these losses through post-processing enhancements,\nbut the reconstructed details often deviate from the original image\ninformation, impeding their fusion with semantic content. To address this\nlimitation, we propose PRNet, a real-time detection framework that prioritizes\nthe preservation and efficient utilization of primitive shallow spatial\nfeatures to enhance small object representations. PRNet achieves this via two\nmodules:the Progressive Refinement Neck (PRN) for spatial-semantic alignment\nthrough backbone reuse and iterative refinement, and the Enhanced SliceSamp\n(ESSamp) for preserving shallow information during downsampling via optimized\nrearrangement and convolution. Extensive experiments on the VisDrone, AI-TOD,\nand UAVDT datasets demonstrate that PRNet outperforms state-of-the-art methods\nunder comparable computational constraints, achieving superior\naccuracy-efficiency trade-offs.\n", "link": "http://arxiv.org/abs/2510.09531v1", "date": "2025-10-10", "relevancy": 2.7231, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5648}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRNet%3A%20Original%20Information%20Is%20All%20You%20Have&body=Title%3A%20PRNet%3A%20Original%20Information%20Is%20All%20You%20Have%0AAuthor%3A%20PeiHuang%20Zheng%20and%20Yunlong%20Zhao%20and%20Zheng%20Cui%20and%20Yang%20Li%0AAbstract%3A%20%20%20Small%20object%20detection%20in%20aerial%20images%20suffers%20from%20severe%20information%0Adegradation%20during%20feature%20extraction%20due%20to%20limited%20pixel%20representations%2C%0Awhere%20shallow%20spatial%20details%20fail%20to%20align%20effectively%20with%20semantic%0Ainformation%2C%20leading%20to%20frequent%20misses%20and%20false%20positives.%20Existing%20FPN-based%0Amethods%20attempt%20to%20mitigate%20these%20losses%20through%20post-processing%20enhancements%2C%0Abut%20the%20reconstructed%20details%20often%20deviate%20from%20the%20original%20image%0Ainformation%2C%20impeding%20their%20fusion%20with%20semantic%20content.%20To%20address%20this%0Alimitation%2C%20we%20propose%20PRNet%2C%20a%20real-time%20detection%20framework%20that%20prioritizes%0Athe%20preservation%20and%20efficient%20utilization%20of%20primitive%20shallow%20spatial%0Afeatures%20to%20enhance%20small%20object%20representations.%20PRNet%20achieves%20this%20via%20two%0Amodules%3Athe%20Progressive%20Refinement%20Neck%20%28PRN%29%20for%20spatial-semantic%20alignment%0Athrough%20backbone%20reuse%20and%20iterative%20refinement%2C%20and%20the%20Enhanced%20SliceSamp%0A%28ESSamp%29%20for%20preserving%20shallow%20information%20during%20downsampling%20via%20optimized%0Arearrangement%20and%20convolution.%20Extensive%20experiments%20on%20the%20VisDrone%2C%20AI-TOD%2C%0Aand%20UAVDT%20datasets%20demonstrate%20that%20PRNet%20outperforms%20state-of-the-art%20methods%0Aunder%20comparable%20computational%20constraints%2C%20achieving%20superior%0Aaccuracy-efficiency%20trade-offs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRNet%253A%2520Original%2520Information%2520Is%2520All%2520You%2520Have%26entry.906535625%3DPeiHuang%2520Zheng%2520and%2520Yunlong%2520Zhao%2520and%2520Zheng%2520Cui%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520Small%2520object%2520detection%2520in%2520aerial%2520images%2520suffers%2520from%2520severe%2520information%250Adegradation%2520during%2520feature%2520extraction%2520due%2520to%2520limited%2520pixel%2520representations%252C%250Awhere%2520shallow%2520spatial%2520details%2520fail%2520to%2520align%2520effectively%2520with%2520semantic%250Ainformation%252C%2520leading%2520to%2520frequent%2520misses%2520and%2520false%2520positives.%2520Existing%2520FPN-based%250Amethods%2520attempt%2520to%2520mitigate%2520these%2520losses%2520through%2520post-processing%2520enhancements%252C%250Abut%2520the%2520reconstructed%2520details%2520often%2520deviate%2520from%2520the%2520original%2520image%250Ainformation%252C%2520impeding%2520their%2520fusion%2520with%2520semantic%2520content.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520PRNet%252C%2520a%2520real-time%2520detection%2520framework%2520that%2520prioritizes%250Athe%2520preservation%2520and%2520efficient%2520utilization%2520of%2520primitive%2520shallow%2520spatial%250Afeatures%2520to%2520enhance%2520small%2520object%2520representations.%2520PRNet%2520achieves%2520this%2520via%2520two%250Amodules%253Athe%2520Progressive%2520Refinement%2520Neck%2520%2528PRN%2529%2520for%2520spatial-semantic%2520alignment%250Athrough%2520backbone%2520reuse%2520and%2520iterative%2520refinement%252C%2520and%2520the%2520Enhanced%2520SliceSamp%250A%2528ESSamp%2529%2520for%2520preserving%2520shallow%2520information%2520during%2520downsampling%2520via%2520optimized%250Arearrangement%2520and%2520convolution.%2520Extensive%2520experiments%2520on%2520the%2520VisDrone%252C%2520AI-TOD%252C%250Aand%2520UAVDT%2520datasets%2520demonstrate%2520that%2520PRNet%2520outperforms%2520state-of-the-art%2520methods%250Aunder%2520comparable%2520computational%2520constraints%252C%2520achieving%2520superior%250Aaccuracy-efficiency%2520trade-offs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRNet%3A%20Original%20Information%20Is%20All%20You%20Have&entry.906535625=PeiHuang%20Zheng%20and%20Yunlong%20Zhao%20and%20Zheng%20Cui%20and%20Yang%20Li&entry.1292438233=%20%20Small%20object%20detection%20in%20aerial%20images%20suffers%20from%20severe%20information%0Adegradation%20during%20feature%20extraction%20due%20to%20limited%20pixel%20representations%2C%0Awhere%20shallow%20spatial%20details%20fail%20to%20align%20effectively%20with%20semantic%0Ainformation%2C%20leading%20to%20frequent%20misses%20and%20false%20positives.%20Existing%20FPN-based%0Amethods%20attempt%20to%20mitigate%20these%20losses%20through%20post-processing%20enhancements%2C%0Abut%20the%20reconstructed%20details%20often%20deviate%20from%20the%20original%20image%0Ainformation%2C%20impeding%20their%20fusion%20with%20semantic%20content.%20To%20address%20this%0Alimitation%2C%20we%20propose%20PRNet%2C%20a%20real-time%20detection%20framework%20that%20prioritizes%0Athe%20preservation%20and%20efficient%20utilization%20of%20primitive%20shallow%20spatial%0Afeatures%20to%20enhance%20small%20object%20representations.%20PRNet%20achieves%20this%20via%20two%0Amodules%3Athe%20Progressive%20Refinement%20Neck%20%28PRN%29%20for%20spatial-semantic%20alignment%0Athrough%20backbone%20reuse%20and%20iterative%20refinement%2C%20and%20the%20Enhanced%20SliceSamp%0A%28ESSamp%29%20for%20preserving%20shallow%20information%20during%20downsampling%20via%20optimized%0Arearrangement%20and%20convolution.%20Extensive%20experiments%20on%20the%20VisDrone%2C%20AI-TOD%2C%0Aand%20UAVDT%20datasets%20demonstrate%20that%20PRNet%20outperforms%20state-of-the-art%20methods%0Aunder%20comparable%20computational%20constraints%2C%20achieving%20superior%0Aaccuracy-efficiency%20trade-offs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09531v1&entry.124074799=Read"},
{"title": "Geodesic Calculus on Latent Spaces", "author": "Florine Hartwig and Josua Sassen and Juliane Braunsmann and Martin Rumpf and Benedikt Wirth", "abstract": "  Latent manifolds of autoencoders provide low-dimensional representations of\ndata, which can be studied from a geometric perspective. We propose to describe\nthese latent manifolds as implicit submanifolds of some ambient latent space.\nBased on this, we develop tools for a discrete Riemannian calculus\napproximating classical geometric operators. These tools are robust against\ninaccuracies of the implicit representation often occurring in practical\nexamples. To obtain a suitable implicit representation, we propose to learn an\napproximate projection onto the latent manifold by minimizing a denoising\nobjective. This approach is independent of the underlying autoencoder and\nsupports the use of different Riemannian geometries on the latent manifolds.\nThe framework in particular enables the computation of geodesic paths\nconnecting given end points and shooting geodesics via the Riemannian\nexponential maps on latent manifolds. We evaluate our approach on various\nautoencoders trained on synthetic and real data.\n", "link": "http://arxiv.org/abs/2510.09468v1", "date": "2025-10-10", "relevancy": 2.7131, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5931}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5237}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geodesic%20Calculus%20on%20Latent%20Spaces&body=Title%3A%20Geodesic%20Calculus%20on%20Latent%20Spaces%0AAuthor%3A%20Florine%20Hartwig%20and%20Josua%20Sassen%20and%20Juliane%20Braunsmann%20and%20Martin%20Rumpf%20and%20Benedikt%20Wirth%0AAbstract%3A%20%20%20Latent%20manifolds%20of%20autoencoders%20provide%20low-dimensional%20representations%20of%0Adata%2C%20which%20can%20be%20studied%20from%20a%20geometric%20perspective.%20We%20propose%20to%20describe%0Athese%20latent%20manifolds%20as%20implicit%20submanifolds%20of%20some%20ambient%20latent%20space.%0ABased%20on%20this%2C%20we%20develop%20tools%20for%20a%20discrete%20Riemannian%20calculus%0Aapproximating%20classical%20geometric%20operators.%20These%20tools%20are%20robust%20against%0Ainaccuracies%20of%20the%20implicit%20representation%20often%20occurring%20in%20practical%0Aexamples.%20To%20obtain%20a%20suitable%20implicit%20representation%2C%20we%20propose%20to%20learn%20an%0Aapproximate%20projection%20onto%20the%20latent%20manifold%20by%20minimizing%20a%20denoising%0Aobjective.%20This%20approach%20is%20independent%20of%20the%20underlying%20autoencoder%20and%0Asupports%20the%20use%20of%20different%20Riemannian%20geometries%20on%20the%20latent%20manifolds.%0AThe%20framework%20in%20particular%20enables%20the%20computation%20of%20geodesic%20paths%0Aconnecting%20given%20end%20points%20and%20shooting%20geodesics%20via%20the%20Riemannian%0Aexponential%20maps%20on%20latent%20manifolds.%20We%20evaluate%20our%20approach%20on%20various%0Aautoencoders%20trained%20on%20synthetic%20and%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeodesic%2520Calculus%2520on%2520Latent%2520Spaces%26entry.906535625%3DFlorine%2520Hartwig%2520and%2520Josua%2520Sassen%2520and%2520Juliane%2520Braunsmann%2520and%2520Martin%2520Rumpf%2520and%2520Benedikt%2520Wirth%26entry.1292438233%3D%2520%2520Latent%2520manifolds%2520of%2520autoencoders%2520provide%2520low-dimensional%2520representations%2520of%250Adata%252C%2520which%2520can%2520be%2520studied%2520from%2520a%2520geometric%2520perspective.%2520We%2520propose%2520to%2520describe%250Athese%2520latent%2520manifolds%2520as%2520implicit%2520submanifolds%2520of%2520some%2520ambient%2520latent%2520space.%250ABased%2520on%2520this%252C%2520we%2520develop%2520tools%2520for%2520a%2520discrete%2520Riemannian%2520calculus%250Aapproximating%2520classical%2520geometric%2520operators.%2520These%2520tools%2520are%2520robust%2520against%250Ainaccuracies%2520of%2520the%2520implicit%2520representation%2520often%2520occurring%2520in%2520practical%250Aexamples.%2520To%2520obtain%2520a%2520suitable%2520implicit%2520representation%252C%2520we%2520propose%2520to%2520learn%2520an%250Aapproximate%2520projection%2520onto%2520the%2520latent%2520manifold%2520by%2520minimizing%2520a%2520denoising%250Aobjective.%2520This%2520approach%2520is%2520independent%2520of%2520the%2520underlying%2520autoencoder%2520and%250Asupports%2520the%2520use%2520of%2520different%2520Riemannian%2520geometries%2520on%2520the%2520latent%2520manifolds.%250AThe%2520framework%2520in%2520particular%2520enables%2520the%2520computation%2520of%2520geodesic%2520paths%250Aconnecting%2520given%2520end%2520points%2520and%2520shooting%2520geodesics%2520via%2520the%2520Riemannian%250Aexponential%2520maps%2520on%2520latent%2520manifolds.%2520We%2520evaluate%2520our%2520approach%2520on%2520various%250Aautoencoders%2520trained%2520on%2520synthetic%2520and%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geodesic%20Calculus%20on%20Latent%20Spaces&entry.906535625=Florine%20Hartwig%20and%20Josua%20Sassen%20and%20Juliane%20Braunsmann%20and%20Martin%20Rumpf%20and%20Benedikt%20Wirth&entry.1292438233=%20%20Latent%20manifolds%20of%20autoencoders%20provide%20low-dimensional%20representations%20of%0Adata%2C%20which%20can%20be%20studied%20from%20a%20geometric%20perspective.%20We%20propose%20to%20describe%0Athese%20latent%20manifolds%20as%20implicit%20submanifolds%20of%20some%20ambient%20latent%20space.%0ABased%20on%20this%2C%20we%20develop%20tools%20for%20a%20discrete%20Riemannian%20calculus%0Aapproximating%20classical%20geometric%20operators.%20These%20tools%20are%20robust%20against%0Ainaccuracies%20of%20the%20implicit%20representation%20often%20occurring%20in%20practical%0Aexamples.%20To%20obtain%20a%20suitable%20implicit%20representation%2C%20we%20propose%20to%20learn%20an%0Aapproximate%20projection%20onto%20the%20latent%20manifold%20by%20minimizing%20a%20denoising%0Aobjective.%20This%20approach%20is%20independent%20of%20the%20underlying%20autoencoder%20and%0Asupports%20the%20use%20of%20different%20Riemannian%20geometries%20on%20the%20latent%20manifolds.%0AThe%20framework%20in%20particular%20enables%20the%20computation%20of%20geodesic%20paths%0Aconnecting%20given%20end%20points%20and%20shooting%20geodesics%20via%20the%20Riemannian%0Aexponential%20maps%20on%20latent%20manifolds.%20We%20evaluate%20our%20approach%20on%20various%0Aautoencoders%20trained%20on%20synthetic%20and%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09468v1&entry.124074799=Read"},
{"title": "How a Bilingual LM Becomes Bilingual: Tracing Internal Representations\n  with Sparse Autoencoders", "author": "Tatsuro Inaba and Go Kamoda and Kentaro Inui and Masaru Isonuma and Yusuke Miyao and Yohei Oseki and Benjamin Heinzerling and Yu Takagi", "abstract": "  This study explores how bilingual language models develop complex internal\nrepresentations. We employ sparse autoencoders to analyze internal\nrepresentations of bilingual language models with a focus on the effects of\ntraining steps, layers, and model sizes. Our analysis shows that language\nmodels first learn languages separately, and then gradually form bilingual\nalignments, particularly in the mid layers. We also found that this bilingual\ntendency is stronger in larger models. Building on these findings, we\ndemonstrate the critical role of bilingual representations in model performance\nby employing a novel method that integrates decomposed representations from a\nfully trained model into a mid-training model. Our results provide insights\ninto how language models acquire bilingual capabilities.\n", "link": "http://arxiv.org/abs/2503.06394v2", "date": "2025-10-10", "relevancy": 2.7088, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5583}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20a%20Bilingual%20LM%20Becomes%20Bilingual%3A%20Tracing%20Internal%20Representations%0A%20%20with%20Sparse%20Autoencoders&body=Title%3A%20How%20a%20Bilingual%20LM%20Becomes%20Bilingual%3A%20Tracing%20Internal%20Representations%0A%20%20with%20Sparse%20Autoencoders%0AAuthor%3A%20Tatsuro%20Inaba%20and%20Go%20Kamoda%20and%20Kentaro%20Inui%20and%20Masaru%20Isonuma%20and%20Yusuke%20Miyao%20and%20Yohei%20Oseki%20and%20Benjamin%20Heinzerling%20and%20Yu%20Takagi%0AAbstract%3A%20%20%20This%20study%20explores%20how%20bilingual%20language%20models%20develop%20complex%20internal%0Arepresentations.%20We%20employ%20sparse%20autoencoders%20to%20analyze%20internal%0Arepresentations%20of%20bilingual%20language%20models%20with%20a%20focus%20on%20the%20effects%20of%0Atraining%20steps%2C%20layers%2C%20and%20model%20sizes.%20Our%20analysis%20shows%20that%20language%0Amodels%20first%20learn%20languages%20separately%2C%20and%20then%20gradually%20form%20bilingual%0Aalignments%2C%20particularly%20in%20the%20mid%20layers.%20We%20also%20found%20that%20this%20bilingual%0Atendency%20is%20stronger%20in%20larger%20models.%20Building%20on%20these%20findings%2C%20we%0Ademonstrate%20the%20critical%20role%20of%20bilingual%20representations%20in%20model%20performance%0Aby%20employing%20a%20novel%20method%20that%20integrates%20decomposed%20representations%20from%20a%0Afully%20trained%20model%20into%20a%20mid-training%20model.%20Our%20results%20provide%20insights%0Ainto%20how%20language%20models%20acquire%20bilingual%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520a%2520Bilingual%2520LM%2520Becomes%2520Bilingual%253A%2520Tracing%2520Internal%2520Representations%250A%2520%2520with%2520Sparse%2520Autoencoders%26entry.906535625%3DTatsuro%2520Inaba%2520and%2520Go%2520Kamoda%2520and%2520Kentaro%2520Inui%2520and%2520Masaru%2520Isonuma%2520and%2520Yusuke%2520Miyao%2520and%2520Yohei%2520Oseki%2520and%2520Benjamin%2520Heinzerling%2520and%2520Yu%2520Takagi%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520how%2520bilingual%2520language%2520models%2520develop%2520complex%2520internal%250Arepresentations.%2520We%2520employ%2520sparse%2520autoencoders%2520to%2520analyze%2520internal%250Arepresentations%2520of%2520bilingual%2520language%2520models%2520with%2520a%2520focus%2520on%2520the%2520effects%2520of%250Atraining%2520steps%252C%2520layers%252C%2520and%2520model%2520sizes.%2520Our%2520analysis%2520shows%2520that%2520language%250Amodels%2520first%2520learn%2520languages%2520separately%252C%2520and%2520then%2520gradually%2520form%2520bilingual%250Aalignments%252C%2520particularly%2520in%2520the%2520mid%2520layers.%2520We%2520also%2520found%2520that%2520this%2520bilingual%250Atendency%2520is%2520stronger%2520in%2520larger%2520models.%2520Building%2520on%2520these%2520findings%252C%2520we%250Ademonstrate%2520the%2520critical%2520role%2520of%2520bilingual%2520representations%2520in%2520model%2520performance%250Aby%2520employing%2520a%2520novel%2520method%2520that%2520integrates%2520decomposed%2520representations%2520from%2520a%250Afully%2520trained%2520model%2520into%2520a%2520mid-training%2520model.%2520Our%2520results%2520provide%2520insights%250Ainto%2520how%2520language%2520models%2520acquire%2520bilingual%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20a%20Bilingual%20LM%20Becomes%20Bilingual%3A%20Tracing%20Internal%20Representations%0A%20%20with%20Sparse%20Autoencoders&entry.906535625=Tatsuro%20Inaba%20and%20Go%20Kamoda%20and%20Kentaro%20Inui%20and%20Masaru%20Isonuma%20and%20Yusuke%20Miyao%20and%20Yohei%20Oseki%20and%20Benjamin%20Heinzerling%20and%20Yu%20Takagi&entry.1292438233=%20%20This%20study%20explores%20how%20bilingual%20language%20models%20develop%20complex%20internal%0Arepresentations.%20We%20employ%20sparse%20autoencoders%20to%20analyze%20internal%0Arepresentations%20of%20bilingual%20language%20models%20with%20a%20focus%20on%20the%20effects%20of%0Atraining%20steps%2C%20layers%2C%20and%20model%20sizes.%20Our%20analysis%20shows%20that%20language%0Amodels%20first%20learn%20languages%20separately%2C%20and%20then%20gradually%20form%20bilingual%0Aalignments%2C%20particularly%20in%20the%20mid%20layers.%20We%20also%20found%20that%20this%20bilingual%0Atendency%20is%20stronger%20in%20larger%20models.%20Building%20on%20these%20findings%2C%20we%0Ademonstrate%20the%20critical%20role%20of%20bilingual%20representations%20in%20model%20performance%0Aby%20employing%20a%20novel%20method%20that%20integrates%20decomposed%20representations%20from%20a%0Afully%20trained%20model%20into%20a%20mid-training%20model.%20Our%20results%20provide%20insights%0Ainto%20how%20language%20models%20acquire%20bilingual%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06394v2&entry.124074799=Read"},
{"title": "FREE: The Foundational Semantic Recognition for Modeling Environmental\n  Ecosystems", "author": "Shiyuan Luo and Juntong Ni and Shengyu Chen and Runlong Yu and Yiqun Xie and Licheng Liu and Zhenong Jin and Huaxiu Yao and Xiaowei Jia", "abstract": "  Modeling environmental ecosystems is critical for the sustainability of our\nplanet, but is extremely challenging due to the complex underlying processes\ndriven by interactions amongst a large number of physical variables. As many\nvariables are difficult to measure at large scales, existing works often\nutilize a combination of observable features and locally available measurements\nor modeled values as input to build models for a specific study region and time\nperiod. This raises a fundamental question in advancing the modeling of\nenvironmental ecosystems: how to build a general framework for modeling the\ncomplex relationships among diverse environmental variables over space and\ntime? In this paper, we introduce a framework, FREE, that enables the use of\nvarying features and available information to train a universal model. The core\nidea is to map available environmental data into a text space and then convert\nthe traditional predictive modeling task in environmental science to a semantic\nrecognition problem. Our evaluation on two societally important real-world\napplications, stream water temperature prediction and crop yield prediction,\ndemonstrates the superiority of FREE over multiple baselines, even in\ndata-sparse scenarios.\n", "link": "http://arxiv.org/abs/2311.10255v5", "date": "2025-10-10", "relevancy": 2.669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FREE%3A%20The%20Foundational%20Semantic%20Recognition%20for%20Modeling%20Environmental%0A%20%20Ecosystems&body=Title%3A%20FREE%3A%20The%20Foundational%20Semantic%20Recognition%20for%20Modeling%20Environmental%0A%20%20Ecosystems%0AAuthor%3A%20Shiyuan%20Luo%20and%20Juntong%20Ni%20and%20Shengyu%20Chen%20and%20Runlong%20Yu%20and%20Yiqun%20Xie%20and%20Licheng%20Liu%20and%20Zhenong%20Jin%20and%20Huaxiu%20Yao%20and%20Xiaowei%20Jia%0AAbstract%3A%20%20%20Modeling%20environmental%20ecosystems%20is%20critical%20for%20the%20sustainability%20of%20our%0Aplanet%2C%20but%20is%20extremely%20challenging%20due%20to%20the%20complex%20underlying%20processes%0Adriven%20by%20interactions%20amongst%20a%20large%20number%20of%20physical%20variables.%20As%20many%0Avariables%20are%20difficult%20to%20measure%20at%20large%20scales%2C%20existing%20works%20often%0Autilize%20a%20combination%20of%20observable%20features%20and%20locally%20available%20measurements%0Aor%20modeled%20values%20as%20input%20to%20build%20models%20for%20a%20specific%20study%20region%20and%20time%0Aperiod.%20This%20raises%20a%20fundamental%20question%20in%20advancing%20the%20modeling%20of%0Aenvironmental%20ecosystems%3A%20how%20to%20build%20a%20general%20framework%20for%20modeling%20the%0Acomplex%20relationships%20among%20diverse%20environmental%20variables%20over%20space%20and%0Atime%3F%20In%20this%20paper%2C%20we%20introduce%20a%20framework%2C%20FREE%2C%20that%20enables%20the%20use%20of%0Avarying%20features%20and%20available%20information%20to%20train%20a%20universal%20model.%20The%20core%0Aidea%20is%20to%20map%20available%20environmental%20data%20into%20a%20text%20space%20and%20then%20convert%0Athe%20traditional%20predictive%20modeling%20task%20in%20environmental%20science%20to%20a%20semantic%0Arecognition%20problem.%20Our%20evaluation%20on%20two%20societally%20important%20real-world%0Aapplications%2C%20stream%20water%20temperature%20prediction%20and%20crop%20yield%20prediction%2C%0Ademonstrates%20the%20superiority%20of%20FREE%20over%20multiple%20baselines%2C%20even%20in%0Adata-sparse%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10255v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFREE%253A%2520The%2520Foundational%2520Semantic%2520Recognition%2520for%2520Modeling%2520Environmental%250A%2520%2520Ecosystems%26entry.906535625%3DShiyuan%2520Luo%2520and%2520Juntong%2520Ni%2520and%2520Shengyu%2520Chen%2520and%2520Runlong%2520Yu%2520and%2520Yiqun%2520Xie%2520and%2520Licheng%2520Liu%2520and%2520Zhenong%2520Jin%2520and%2520Huaxiu%2520Yao%2520and%2520Xiaowei%2520Jia%26entry.1292438233%3D%2520%2520Modeling%2520environmental%2520ecosystems%2520is%2520critical%2520for%2520the%2520sustainability%2520of%2520our%250Aplanet%252C%2520but%2520is%2520extremely%2520challenging%2520due%2520to%2520the%2520complex%2520underlying%2520processes%250Adriven%2520by%2520interactions%2520amongst%2520a%2520large%2520number%2520of%2520physical%2520variables.%2520As%2520many%250Avariables%2520are%2520difficult%2520to%2520measure%2520at%2520large%2520scales%252C%2520existing%2520works%2520often%250Autilize%2520a%2520combination%2520of%2520observable%2520features%2520and%2520locally%2520available%2520measurements%250Aor%2520modeled%2520values%2520as%2520input%2520to%2520build%2520models%2520for%2520a%2520specific%2520study%2520region%2520and%2520time%250Aperiod.%2520This%2520raises%2520a%2520fundamental%2520question%2520in%2520advancing%2520the%2520modeling%2520of%250Aenvironmental%2520ecosystems%253A%2520how%2520to%2520build%2520a%2520general%2520framework%2520for%2520modeling%2520the%250Acomplex%2520relationships%2520among%2520diverse%2520environmental%2520variables%2520over%2520space%2520and%250Atime%253F%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520framework%252C%2520FREE%252C%2520that%2520enables%2520the%2520use%2520of%250Avarying%2520features%2520and%2520available%2520information%2520to%2520train%2520a%2520universal%2520model.%2520The%2520core%250Aidea%2520is%2520to%2520map%2520available%2520environmental%2520data%2520into%2520a%2520text%2520space%2520and%2520then%2520convert%250Athe%2520traditional%2520predictive%2520modeling%2520task%2520in%2520environmental%2520science%2520to%2520a%2520semantic%250Arecognition%2520problem.%2520Our%2520evaluation%2520on%2520two%2520societally%2520important%2520real-world%250Aapplications%252C%2520stream%2520water%2520temperature%2520prediction%2520and%2520crop%2520yield%2520prediction%252C%250Ademonstrates%2520the%2520superiority%2520of%2520FREE%2520over%2520multiple%2520baselines%252C%2520even%2520in%250Adata-sparse%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10255v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FREE%3A%20The%20Foundational%20Semantic%20Recognition%20for%20Modeling%20Environmental%0A%20%20Ecosystems&entry.906535625=Shiyuan%20Luo%20and%20Juntong%20Ni%20and%20Shengyu%20Chen%20and%20Runlong%20Yu%20and%20Yiqun%20Xie%20and%20Licheng%20Liu%20and%20Zhenong%20Jin%20and%20Huaxiu%20Yao%20and%20Xiaowei%20Jia&entry.1292438233=%20%20Modeling%20environmental%20ecosystems%20is%20critical%20for%20the%20sustainability%20of%20our%0Aplanet%2C%20but%20is%20extremely%20challenging%20due%20to%20the%20complex%20underlying%20processes%0Adriven%20by%20interactions%20amongst%20a%20large%20number%20of%20physical%20variables.%20As%20many%0Avariables%20are%20difficult%20to%20measure%20at%20large%20scales%2C%20existing%20works%20often%0Autilize%20a%20combination%20of%20observable%20features%20and%20locally%20available%20measurements%0Aor%20modeled%20values%20as%20input%20to%20build%20models%20for%20a%20specific%20study%20region%20and%20time%0Aperiod.%20This%20raises%20a%20fundamental%20question%20in%20advancing%20the%20modeling%20of%0Aenvironmental%20ecosystems%3A%20how%20to%20build%20a%20general%20framework%20for%20modeling%20the%0Acomplex%20relationships%20among%20diverse%20environmental%20variables%20over%20space%20and%0Atime%3F%20In%20this%20paper%2C%20we%20introduce%20a%20framework%2C%20FREE%2C%20that%20enables%20the%20use%20of%0Avarying%20features%20and%20available%20information%20to%20train%20a%20universal%20model.%20The%20core%0Aidea%20is%20to%20map%20available%20environmental%20data%20into%20a%20text%20space%20and%20then%20convert%0Athe%20traditional%20predictive%20modeling%20task%20in%20environmental%20science%20to%20a%20semantic%0Arecognition%20problem.%20Our%20evaluation%20on%20two%20societally%20important%20real-world%0Aapplications%2C%20stream%20water%20temperature%20prediction%20and%20crop%20yield%20prediction%2C%0Ademonstrates%20the%20superiority%20of%20FREE%20over%20multiple%20baselines%2C%20even%20in%0Adata-sparse%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10255v5&entry.124074799=Read"},
{"title": "Localist LLMs -- A Mathematical Framework for Dynamic Locality Control", "author": "Joachim Diederich", "abstract": "  We present a novel framework for training large language models with\ncontinuously adjustable internal representations that span the full spectrum\nfrom localist (interpretable, rule-based) to distributed (generalizable,\nefficient) encodings. The key innovation is a locality dial, a tunable\nparameter that dynamically controls the degree of localization during both\ntraining and inference without requiring model retraining. This is achieved\nthrough group sparsity penalties on attention mechanisms, information-theoretic\nanchor design, and dynamic rule injection. We provide rigorous mathematical\nproofs establishing explicit threshold conditions under which attention\nprovably concentrates on semantically relevant blocks, with exponential bounds\non attention entropy and pointer fidelity. Specifically, we prove that when\ngroup sparsity penalties exceed certain threshold values, the model's attention\nmechanisms concentrate on semantically relevant blocks, achieving low entropy\nand high fidelity with negligible error. This framework enables practitioners\nto continuously interpolate between interpretable and high-performance modes,\nsupporting applications in regulated domains requiring both transparency and\ncapability.\n", "link": "http://arxiv.org/abs/2510.09338v1", "date": "2025-10-10", "relevancy": 2.6562, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localist%20LLMs%20--%20A%20Mathematical%20Framework%20for%20Dynamic%20Locality%20Control&body=Title%3A%20Localist%20LLMs%20--%20A%20Mathematical%20Framework%20for%20Dynamic%20Locality%20Control%0AAuthor%3A%20Joachim%20Diederich%0AAbstract%3A%20%20%20We%20present%20a%20novel%20framework%20for%20training%20large%20language%20models%20with%0Acontinuously%20adjustable%20internal%20representations%20that%20span%20the%20full%20spectrum%0Afrom%20localist%20%28interpretable%2C%20rule-based%29%20to%20distributed%20%28generalizable%2C%0Aefficient%29%20encodings.%20The%20key%20innovation%20is%20a%20locality%20dial%2C%20a%20tunable%0Aparameter%20that%20dynamically%20controls%20the%20degree%20of%20localization%20during%20both%0Atraining%20and%20inference%20without%20requiring%20model%20retraining.%20This%20is%20achieved%0Athrough%20group%20sparsity%20penalties%20on%20attention%20mechanisms%2C%20information-theoretic%0Aanchor%20design%2C%20and%20dynamic%20rule%20injection.%20We%20provide%20rigorous%20mathematical%0Aproofs%20establishing%20explicit%20threshold%20conditions%20under%20which%20attention%0Aprovably%20concentrates%20on%20semantically%20relevant%20blocks%2C%20with%20exponential%20bounds%0Aon%20attention%20entropy%20and%20pointer%20fidelity.%20Specifically%2C%20we%20prove%20that%20when%0Agroup%20sparsity%20penalties%20exceed%20certain%20threshold%20values%2C%20the%20model%27s%20attention%0Amechanisms%20concentrate%20on%20semantically%20relevant%20blocks%2C%20achieving%20low%20entropy%0Aand%20high%20fidelity%20with%20negligible%20error.%20This%20framework%20enables%20practitioners%0Ato%20continuously%20interpolate%20between%20interpretable%20and%20high-performance%20modes%2C%0Asupporting%20applications%20in%20regulated%20domains%20requiring%20both%20transparency%20and%0Acapability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalist%2520LLMs%2520--%2520A%2520Mathematical%2520Framework%2520for%2520Dynamic%2520Locality%2520Control%26entry.906535625%3DJoachim%2520Diederich%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520framework%2520for%2520training%2520large%2520language%2520models%2520with%250Acontinuously%2520adjustable%2520internal%2520representations%2520that%2520span%2520the%2520full%2520spectrum%250Afrom%2520localist%2520%2528interpretable%252C%2520rule-based%2529%2520to%2520distributed%2520%2528generalizable%252C%250Aefficient%2529%2520encodings.%2520The%2520key%2520innovation%2520is%2520a%2520locality%2520dial%252C%2520a%2520tunable%250Aparameter%2520that%2520dynamically%2520controls%2520the%2520degree%2520of%2520localization%2520during%2520both%250Atraining%2520and%2520inference%2520without%2520requiring%2520model%2520retraining.%2520This%2520is%2520achieved%250Athrough%2520group%2520sparsity%2520penalties%2520on%2520attention%2520mechanisms%252C%2520information-theoretic%250Aanchor%2520design%252C%2520and%2520dynamic%2520rule%2520injection.%2520We%2520provide%2520rigorous%2520mathematical%250Aproofs%2520establishing%2520explicit%2520threshold%2520conditions%2520under%2520which%2520attention%250Aprovably%2520concentrates%2520on%2520semantically%2520relevant%2520blocks%252C%2520with%2520exponential%2520bounds%250Aon%2520attention%2520entropy%2520and%2520pointer%2520fidelity.%2520Specifically%252C%2520we%2520prove%2520that%2520when%250Agroup%2520sparsity%2520penalties%2520exceed%2520certain%2520threshold%2520values%252C%2520the%2520model%2527s%2520attention%250Amechanisms%2520concentrate%2520on%2520semantically%2520relevant%2520blocks%252C%2520achieving%2520low%2520entropy%250Aand%2520high%2520fidelity%2520with%2520negligible%2520error.%2520This%2520framework%2520enables%2520practitioners%250Ato%2520continuously%2520interpolate%2520between%2520interpretable%2520and%2520high-performance%2520modes%252C%250Asupporting%2520applications%2520in%2520regulated%2520domains%2520requiring%2520both%2520transparency%2520and%250Acapability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localist%20LLMs%20--%20A%20Mathematical%20Framework%20for%20Dynamic%20Locality%20Control&entry.906535625=Joachim%20Diederich&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20for%20training%20large%20language%20models%20with%0Acontinuously%20adjustable%20internal%20representations%20that%20span%20the%20full%20spectrum%0Afrom%20localist%20%28interpretable%2C%20rule-based%29%20to%20distributed%20%28generalizable%2C%0Aefficient%29%20encodings.%20The%20key%20innovation%20is%20a%20locality%20dial%2C%20a%20tunable%0Aparameter%20that%20dynamically%20controls%20the%20degree%20of%20localization%20during%20both%0Atraining%20and%20inference%20without%20requiring%20model%20retraining.%20This%20is%20achieved%0Athrough%20group%20sparsity%20penalties%20on%20attention%20mechanisms%2C%20information-theoretic%0Aanchor%20design%2C%20and%20dynamic%20rule%20injection.%20We%20provide%20rigorous%20mathematical%0Aproofs%20establishing%20explicit%20threshold%20conditions%20under%20which%20attention%0Aprovably%20concentrates%20on%20semantically%20relevant%20blocks%2C%20with%20exponential%20bounds%0Aon%20attention%20entropy%20and%20pointer%20fidelity.%20Specifically%2C%20we%20prove%20that%20when%0Agroup%20sparsity%20penalties%20exceed%20certain%20threshold%20values%2C%20the%20model%27s%20attention%0Amechanisms%20concentrate%20on%20semantically%20relevant%20blocks%2C%20achieving%20low%20entropy%0Aand%20high%20fidelity%20with%20negligible%20error.%20This%20framework%20enables%20practitioners%0Ato%20continuously%20interpolate%20between%20interpretable%20and%20high-performance%20modes%2C%0Asupporting%20applications%20in%20regulated%20domains%20requiring%20both%20transparency%20and%0Acapability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09338v1&entry.124074799=Read"},
{"title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models", "author": "Kele Shao and Keda Tao and Can Qin and Haoxuan You and Yang Sui and Huan Wang", "abstract": "  Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.\n", "link": "http://arxiv.org/abs/2505.21334v3", "date": "2025-10-10", "relevancy": 2.6119, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5322}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoliTom%3A%20Holistic%20Token%20Merging%20for%20Fast%20Video%20Large%20Language%20Models&body=Title%3A%20HoliTom%3A%20Holistic%20Token%20Merging%20for%20Fast%20Video%20Large%20Language%20Models%0AAuthor%3A%20Kele%20Shao%20and%20Keda%20Tao%20and%20Can%20Qin%20and%20Haoxuan%20You%20and%20Yang%20Sui%20and%20Huan%20Wang%0AAbstract%3A%20%20%20Video%20large%20language%20models%20%28video%20LLMs%29%20excel%20at%20video%20comprehension%20but%0Aface%20significant%20computational%20inefficiency%20due%20to%20redundant%20video%20tokens.%0AExisting%20token%20pruning%20methods%20offer%20solutions.%20However%2C%20approaches%20operating%0Awithin%20the%20LLM%20%28inner-LLM%20pruning%29%2C%20such%20as%20FastV%2C%20incur%20intrinsic%0Acomputational%20overhead%20in%20shallow%20layers.%20In%20contrast%2C%20methods%20performing%20token%0Apruning%20before%20the%20LLM%20%28outer-LLM%20pruning%29%20primarily%20address%20spatial%20redundancy%0Awithin%20individual%20frames%20or%20limited%20temporal%20windows%2C%20neglecting%20the%20crucial%0Aglobal%20temporal%20dynamics%20and%20correlations%20across%20longer%20video%20sequences.%20This%0Aleads%20to%20sub-optimal%20spatio-temporal%20reduction%20and%20does%20not%20leverage%20video%0Acompressibility%20fully.%20Crucially%2C%20the%20synergistic%20potential%20and%20mutual%0Ainfluence%20of%20combining%20these%20strategies%20remain%20unexplored.%20To%20further%20reduce%0Aredundancy%2C%20we%20introduce%20HoliTom%2C%20a%20novel%20training-free%20holistic%20token%20merging%0Aframework.%20HoliTom%20employs%20outer-LLM%20pruning%20through%20global%20redundancy-aware%0Atemporal%20segmentation%2C%20followed%20by%20spatial-temporal%20merging%20to%20reduce%20visual%0Atokens%20by%20over%2090%25%2C%20significantly%20alleviating%20the%20LLM%27s%20computational%20burden.%0AComplementing%20this%2C%20we%20introduce%20a%20robust%20inner-LLM%20token%20similarity-based%0Amerging%20approach%2C%20designed%20for%20superior%20performance%20and%20compatibility%20with%0Aouter-LLM%20pruning.%20Evaluations%20demonstrate%20our%20method%27s%20promising%0Aefficiency-performance%20trade-off%20on%20LLaVA-OneVision-7B%2C%20reducing%20computational%0Acosts%20to%206.9%25%20of%20FLOPs%20while%20maintaining%2099.1%25%20of%20the%20original%20performance.%0AFurthermore%2C%20we%20achieve%20a%202.28x%20reduction%20in%20Time-To-First-Token%20%28TTFT%29%20and%20a%0A1.32x%20acceleration%20in%20decoding%20throughput%2C%20highlighting%20the%20practical%20benefits%0Aof%20our%20integrated%20pruning%20approach%20for%20efficient%20video%20LLMs%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoliTom%253A%2520Holistic%2520Token%2520Merging%2520for%2520Fast%2520Video%2520Large%2520Language%2520Models%26entry.906535625%3DKele%2520Shao%2520and%2520Keda%2520Tao%2520and%2520Can%2520Qin%2520and%2520Haoxuan%2520You%2520and%2520Yang%2520Sui%2520and%2520Huan%2520Wang%26entry.1292438233%3D%2520%2520Video%2520large%2520language%2520models%2520%2528video%2520LLMs%2529%2520excel%2520at%2520video%2520comprehension%2520but%250Aface%2520significant%2520computational%2520inefficiency%2520due%2520to%2520redundant%2520video%2520tokens.%250AExisting%2520token%2520pruning%2520methods%2520offer%2520solutions.%2520However%252C%2520approaches%2520operating%250Awithin%2520the%2520LLM%2520%2528inner-LLM%2520pruning%2529%252C%2520such%2520as%2520FastV%252C%2520incur%2520intrinsic%250Acomputational%2520overhead%2520in%2520shallow%2520layers.%2520In%2520contrast%252C%2520methods%2520performing%2520token%250Apruning%2520before%2520the%2520LLM%2520%2528outer-LLM%2520pruning%2529%2520primarily%2520address%2520spatial%2520redundancy%250Awithin%2520individual%2520frames%2520or%2520limited%2520temporal%2520windows%252C%2520neglecting%2520the%2520crucial%250Aglobal%2520temporal%2520dynamics%2520and%2520correlations%2520across%2520longer%2520video%2520sequences.%2520This%250Aleads%2520to%2520sub-optimal%2520spatio-temporal%2520reduction%2520and%2520does%2520not%2520leverage%2520video%250Acompressibility%2520fully.%2520Crucially%252C%2520the%2520synergistic%2520potential%2520and%2520mutual%250Ainfluence%2520of%2520combining%2520these%2520strategies%2520remain%2520unexplored.%2520To%2520further%2520reduce%250Aredundancy%252C%2520we%2520introduce%2520HoliTom%252C%2520a%2520novel%2520training-free%2520holistic%2520token%2520merging%250Aframework.%2520HoliTom%2520employs%2520outer-LLM%2520pruning%2520through%2520global%2520redundancy-aware%250Atemporal%2520segmentation%252C%2520followed%2520by%2520spatial-temporal%2520merging%2520to%2520reduce%2520visual%250Atokens%2520by%2520over%252090%2525%252C%2520significantly%2520alleviating%2520the%2520LLM%2527s%2520computational%2520burden.%250AComplementing%2520this%252C%2520we%2520introduce%2520a%2520robust%2520inner-LLM%2520token%2520similarity-based%250Amerging%2520approach%252C%2520designed%2520for%2520superior%2520performance%2520and%2520compatibility%2520with%250Aouter-LLM%2520pruning.%2520Evaluations%2520demonstrate%2520our%2520method%2527s%2520promising%250Aefficiency-performance%2520trade-off%2520on%2520LLaVA-OneVision-7B%252C%2520reducing%2520computational%250Acosts%2520to%25206.9%2525%2520of%2520FLOPs%2520while%2520maintaining%252099.1%2525%2520of%2520the%2520original%2520performance.%250AFurthermore%252C%2520we%2520achieve%2520a%25202.28x%2520reduction%2520in%2520Time-To-First-Token%2520%2528TTFT%2529%2520and%2520a%250A1.32x%2520acceleration%2520in%2520decoding%2520throughput%252C%2520highlighting%2520the%2520practical%2520benefits%250Aof%2520our%2520integrated%2520pruning%2520approach%2520for%2520efficient%2520video%2520LLMs%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoliTom%3A%20Holistic%20Token%20Merging%20for%20Fast%20Video%20Large%20Language%20Models&entry.906535625=Kele%20Shao%20and%20Keda%20Tao%20and%20Can%20Qin%20and%20Haoxuan%20You%20and%20Yang%20Sui%20and%20Huan%20Wang&entry.1292438233=%20%20Video%20large%20language%20models%20%28video%20LLMs%29%20excel%20at%20video%20comprehension%20but%0Aface%20significant%20computational%20inefficiency%20due%20to%20redundant%20video%20tokens.%0AExisting%20token%20pruning%20methods%20offer%20solutions.%20However%2C%20approaches%20operating%0Awithin%20the%20LLM%20%28inner-LLM%20pruning%29%2C%20such%20as%20FastV%2C%20incur%20intrinsic%0Acomputational%20overhead%20in%20shallow%20layers.%20In%20contrast%2C%20methods%20performing%20token%0Apruning%20before%20the%20LLM%20%28outer-LLM%20pruning%29%20primarily%20address%20spatial%20redundancy%0Awithin%20individual%20frames%20or%20limited%20temporal%20windows%2C%20neglecting%20the%20crucial%0Aglobal%20temporal%20dynamics%20and%20correlations%20across%20longer%20video%20sequences.%20This%0Aleads%20to%20sub-optimal%20spatio-temporal%20reduction%20and%20does%20not%20leverage%20video%0Acompressibility%20fully.%20Crucially%2C%20the%20synergistic%20potential%20and%20mutual%0Ainfluence%20of%20combining%20these%20strategies%20remain%20unexplored.%20To%20further%20reduce%0Aredundancy%2C%20we%20introduce%20HoliTom%2C%20a%20novel%20training-free%20holistic%20token%20merging%0Aframework.%20HoliTom%20employs%20outer-LLM%20pruning%20through%20global%20redundancy-aware%0Atemporal%20segmentation%2C%20followed%20by%20spatial-temporal%20merging%20to%20reduce%20visual%0Atokens%20by%20over%2090%25%2C%20significantly%20alleviating%20the%20LLM%27s%20computational%20burden.%0AComplementing%20this%2C%20we%20introduce%20a%20robust%20inner-LLM%20token%20similarity-based%0Amerging%20approach%2C%20designed%20for%20superior%20performance%20and%20compatibility%20with%0Aouter-LLM%20pruning.%20Evaluations%20demonstrate%20our%20method%27s%20promising%0Aefficiency-performance%20trade-off%20on%20LLaVA-OneVision-7B%2C%20reducing%20computational%0Acosts%20to%206.9%25%20of%20FLOPs%20while%20maintaining%2099.1%25%20of%20the%20original%20performance.%0AFurthermore%2C%20we%20achieve%20a%202.28x%20reduction%20in%20Time-To-First-Token%20%28TTFT%29%20and%20a%0A1.32x%20acceleration%20in%20decoding%20throughput%2C%20highlighting%20the%20practical%20benefits%0Aof%20our%20integrated%20pruning%20approach%20for%20efficient%20video%20LLMs%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21334v3&entry.124074799=Read"},
{"title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity", "author": "Jiayi Zhang and Simon Yu and Derek Chong and Anthony Sicilia and Michael R. Tomz and Christopher D. Manning and Weiyan Shi", "abstract": "  Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n\"Generate 5 jokes about coffee and their corresponding probabilities\").\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.\n", "link": "http://arxiv.org/abs/2510.01171v3", "date": "2025-10-10", "relevancy": 2.6055, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5291}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verbalized%20Sampling%3A%20How%20to%20Mitigate%20Mode%20Collapse%20and%20Unlock%20LLM%0A%20%20Diversity&body=Title%3A%20Verbalized%20Sampling%3A%20How%20to%20Mitigate%20Mode%20Collapse%20and%20Unlock%20LLM%0A%20%20Diversity%0AAuthor%3A%20Jiayi%20Zhang%20and%20Simon%20Yu%20and%20Derek%20Chong%20and%20Anthony%20Sicilia%20and%20Michael%20R.%20Tomz%20and%20Christopher%20D.%20Manning%20and%20Weiyan%20Shi%0AAbstract%3A%20%20%20Post-training%20alignment%20often%20reduces%20LLM%20diversity%2C%20leading%20to%20a%20phenomenon%0Aknown%20as%20mode%20collapse.%20Unlike%20prior%20work%20that%20attributes%20this%20effect%20to%0Aalgorithmic%20limitations%2C%20we%20identify%20a%20fundamental%2C%20pervasive%20data-level%0Adriver%3A%20typicality%20bias%20in%20preference%20data%2C%20whereby%20annotators%20systematically%0Afavor%20familiar%20text%20as%20a%20result%20of%20well-established%20findings%20in%20cognitive%0Apsychology.%20We%20formalize%20this%20bias%20theoretically%2C%20verify%20it%20on%20preference%0Adatasets%20empirically%2C%20and%20show%20that%20it%20plays%20a%20central%20role%20in%20mode%20collapse.%0AMotivated%20by%20this%20analysis%2C%20we%20introduce%20Verbalized%20Sampling%2C%20a%20simple%2C%0Atraining-free%20prompting%20strategy%20to%20circumvent%20mode%20collapse.%20VS%20prompts%20the%0Amodel%20to%20verbalize%20a%20probability%20distribution%20over%20a%20set%20of%20responses%20%28e.g.%2C%0A%22Generate%205%20jokes%20about%20coffee%20and%20their%20corresponding%20probabilities%22%29.%0AComprehensive%20experiments%20show%20that%20VS%20significantly%20improves%20performance%0Aacross%20creative%20writing%20%28poems%2C%20stories%2C%20jokes%29%2C%20dialogue%20simulation%2C%0Aopen-ended%20QA%2C%20and%20synthetic%20data%20generation%2C%20without%20sacrificing%20factual%0Aaccuracy%20and%20safety.%20For%20instance%2C%20in%20creative%20writing%2C%20VS%20increases%20diversity%0Aby%201.6-2.1x%20over%20direct%20prompting.%20We%20further%20observe%20an%20emergent%20trend%20that%0Amore%20capable%20models%20benefit%20more%20from%20VS.%20In%20sum%2C%20our%20work%20provides%20a%20new%0Adata-centric%20perspective%20on%20mode%20collapse%20and%20a%20practical%20inference-time%20remedy%0Athat%20helps%20unlock%20pre-trained%20generative%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01171v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerbalized%2520Sampling%253A%2520How%2520to%2520Mitigate%2520Mode%2520Collapse%2520and%2520Unlock%2520LLM%250A%2520%2520Diversity%26entry.906535625%3DJiayi%2520Zhang%2520and%2520Simon%2520Yu%2520and%2520Derek%2520Chong%2520and%2520Anthony%2520Sicilia%2520and%2520Michael%2520R.%2520Tomz%2520and%2520Christopher%2520D.%2520Manning%2520and%2520Weiyan%2520Shi%26entry.1292438233%3D%2520%2520Post-training%2520alignment%2520often%2520reduces%2520LLM%2520diversity%252C%2520leading%2520to%2520a%2520phenomenon%250Aknown%2520as%2520mode%2520collapse.%2520Unlike%2520prior%2520work%2520that%2520attributes%2520this%2520effect%2520to%250Aalgorithmic%2520limitations%252C%2520we%2520identify%2520a%2520fundamental%252C%2520pervasive%2520data-level%250Adriver%253A%2520typicality%2520bias%2520in%2520preference%2520data%252C%2520whereby%2520annotators%2520systematically%250Afavor%2520familiar%2520text%2520as%2520a%2520result%2520of%2520well-established%2520findings%2520in%2520cognitive%250Apsychology.%2520We%2520formalize%2520this%2520bias%2520theoretically%252C%2520verify%2520it%2520on%2520preference%250Adatasets%2520empirically%252C%2520and%2520show%2520that%2520it%2520plays%2520a%2520central%2520role%2520in%2520mode%2520collapse.%250AMotivated%2520by%2520this%2520analysis%252C%2520we%2520introduce%2520Verbalized%2520Sampling%252C%2520a%2520simple%252C%250Atraining-free%2520prompting%2520strategy%2520to%2520circumvent%2520mode%2520collapse.%2520VS%2520prompts%2520the%250Amodel%2520to%2520verbalize%2520a%2520probability%2520distribution%2520over%2520a%2520set%2520of%2520responses%2520%2528e.g.%252C%250A%2522Generate%25205%2520jokes%2520about%2520coffee%2520and%2520their%2520corresponding%2520probabilities%2522%2529.%250AComprehensive%2520experiments%2520show%2520that%2520VS%2520significantly%2520improves%2520performance%250Aacross%2520creative%2520writing%2520%2528poems%252C%2520stories%252C%2520jokes%2529%252C%2520dialogue%2520simulation%252C%250Aopen-ended%2520QA%252C%2520and%2520synthetic%2520data%2520generation%252C%2520without%2520sacrificing%2520factual%250Aaccuracy%2520and%2520safety.%2520For%2520instance%252C%2520in%2520creative%2520writing%252C%2520VS%2520increases%2520diversity%250Aby%25201.6-2.1x%2520over%2520direct%2520prompting.%2520We%2520further%2520observe%2520an%2520emergent%2520trend%2520that%250Amore%2520capable%2520models%2520benefit%2520more%2520from%2520VS.%2520In%2520sum%252C%2520our%2520work%2520provides%2520a%2520new%250Adata-centric%2520perspective%2520on%2520mode%2520collapse%2520and%2520a%2520practical%2520inference-time%2520remedy%250Athat%2520helps%2520unlock%2520pre-trained%2520generative%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01171v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verbalized%20Sampling%3A%20How%20to%20Mitigate%20Mode%20Collapse%20and%20Unlock%20LLM%0A%20%20Diversity&entry.906535625=Jiayi%20Zhang%20and%20Simon%20Yu%20and%20Derek%20Chong%20and%20Anthony%20Sicilia%20and%20Michael%20R.%20Tomz%20and%20Christopher%20D.%20Manning%20and%20Weiyan%20Shi&entry.1292438233=%20%20Post-training%20alignment%20often%20reduces%20LLM%20diversity%2C%20leading%20to%20a%20phenomenon%0Aknown%20as%20mode%20collapse.%20Unlike%20prior%20work%20that%20attributes%20this%20effect%20to%0Aalgorithmic%20limitations%2C%20we%20identify%20a%20fundamental%2C%20pervasive%20data-level%0Adriver%3A%20typicality%20bias%20in%20preference%20data%2C%20whereby%20annotators%20systematically%0Afavor%20familiar%20text%20as%20a%20result%20of%20well-established%20findings%20in%20cognitive%0Apsychology.%20We%20formalize%20this%20bias%20theoretically%2C%20verify%20it%20on%20preference%0Adatasets%20empirically%2C%20and%20show%20that%20it%20plays%20a%20central%20role%20in%20mode%20collapse.%0AMotivated%20by%20this%20analysis%2C%20we%20introduce%20Verbalized%20Sampling%2C%20a%20simple%2C%0Atraining-free%20prompting%20strategy%20to%20circumvent%20mode%20collapse.%20VS%20prompts%20the%0Amodel%20to%20verbalize%20a%20probability%20distribution%20over%20a%20set%20of%20responses%20%28e.g.%2C%0A%22Generate%205%20jokes%20about%20coffee%20and%20their%20corresponding%20probabilities%22%29.%0AComprehensive%20experiments%20show%20that%20VS%20significantly%20improves%20performance%0Aacross%20creative%20writing%20%28poems%2C%20stories%2C%20jokes%29%2C%20dialogue%20simulation%2C%0Aopen-ended%20QA%2C%20and%20synthetic%20data%20generation%2C%20without%20sacrificing%20factual%0Aaccuracy%20and%20safety.%20For%20instance%2C%20in%20creative%20writing%2C%20VS%20increases%20diversity%0Aby%201.6-2.1x%20over%20direct%20prompting.%20We%20further%20observe%20an%20emergent%20trend%20that%0Amore%20capable%20models%20benefit%20more%20from%20VS.%20In%20sum%2C%20our%20work%20provides%20a%20new%0Adata-centric%20perspective%20on%20mode%20collapse%20and%20a%20practical%20inference-time%20remedy%0Athat%20helps%20unlock%20pre-trained%20generative%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01171v3&entry.124074799=Read"},
{"title": "Understanding Ice Crystal Habit Diversity with Self-Supervised Learning", "author": "Joseph Ko and Hariprasath Govindarajan and Fredrik Lindsten and Vanessa Przybylo and Kara Sulia and Marcus van Lier-Walqui and Kara Lamb", "abstract": "  Ice-containing clouds strongly impact climate, but they are hard to model due\nto ice crystal habit (i.e., shape) diversity. We use self-supervised learning\n(SSL) to learn latent representations of crystals from ice crystal imagery. By\npre-training a vision transformer with many cloud particle images, we learn\nrobust representations of crystal morphology, which can be used for various\nscience-driven tasks. Our key contributions include (1) validating that our SSL\napproach can be used to learn meaningful representations, and (2) presenting a\nrelevant application where we quantify ice crystal diversity with these latent\nrepresentations. Our results demonstrate the power of SSL-driven\nrepresentations to improve the characterization of ice crystals and\nsubsequently constrain their role in Earth's climate system.\n", "link": "http://arxiv.org/abs/2509.07688v2", "date": "2025-10-10", "relevancy": 2.6021, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Ice%20Crystal%20Habit%20Diversity%20with%20Self-Supervised%20Learning&body=Title%3A%20Understanding%20Ice%20Crystal%20Habit%20Diversity%20with%20Self-Supervised%20Learning%0AAuthor%3A%20Joseph%20Ko%20and%20Hariprasath%20Govindarajan%20and%20Fredrik%20Lindsten%20and%20Vanessa%20Przybylo%20and%20Kara%20Sulia%20and%20Marcus%20van%20Lier-Walqui%20and%20Kara%20Lamb%0AAbstract%3A%20%20%20Ice-containing%20clouds%20strongly%20impact%20climate%2C%20but%20they%20are%20hard%20to%20model%20due%0Ato%20ice%20crystal%20habit%20%28i.e.%2C%20shape%29%20diversity.%20We%20use%20self-supervised%20learning%0A%28SSL%29%20to%20learn%20latent%20representations%20of%20crystals%20from%20ice%20crystal%20imagery.%20By%0Apre-training%20a%20vision%20transformer%20with%20many%20cloud%20particle%20images%2C%20we%20learn%0Arobust%20representations%20of%20crystal%20morphology%2C%20which%20can%20be%20used%20for%20various%0Ascience-driven%20tasks.%20Our%20key%20contributions%20include%20%281%29%20validating%20that%20our%20SSL%0Aapproach%20can%20be%20used%20to%20learn%20meaningful%20representations%2C%20and%20%282%29%20presenting%20a%0Arelevant%20application%20where%20we%20quantify%20ice%20crystal%20diversity%20with%20these%20latent%0Arepresentations.%20Our%20results%20demonstrate%20the%20power%20of%20SSL-driven%0Arepresentations%20to%20improve%20the%20characterization%20of%20ice%20crystals%20and%0Asubsequently%20constrain%20their%20role%20in%20Earth%27s%20climate%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Ice%2520Crystal%2520Habit%2520Diversity%2520with%2520Self-Supervised%2520Learning%26entry.906535625%3DJoseph%2520Ko%2520and%2520Hariprasath%2520Govindarajan%2520and%2520Fredrik%2520Lindsten%2520and%2520Vanessa%2520Przybylo%2520and%2520Kara%2520Sulia%2520and%2520Marcus%2520van%2520Lier-Walqui%2520and%2520Kara%2520Lamb%26entry.1292438233%3D%2520%2520Ice-containing%2520clouds%2520strongly%2520impact%2520climate%252C%2520but%2520they%2520are%2520hard%2520to%2520model%2520due%250Ato%2520ice%2520crystal%2520habit%2520%2528i.e.%252C%2520shape%2529%2520diversity.%2520We%2520use%2520self-supervised%2520learning%250A%2528SSL%2529%2520to%2520learn%2520latent%2520representations%2520of%2520crystals%2520from%2520ice%2520crystal%2520imagery.%2520By%250Apre-training%2520a%2520vision%2520transformer%2520with%2520many%2520cloud%2520particle%2520images%252C%2520we%2520learn%250Arobust%2520representations%2520of%2520crystal%2520morphology%252C%2520which%2520can%2520be%2520used%2520for%2520various%250Ascience-driven%2520tasks.%2520Our%2520key%2520contributions%2520include%2520%25281%2529%2520validating%2520that%2520our%2520SSL%250Aapproach%2520can%2520be%2520used%2520to%2520learn%2520meaningful%2520representations%252C%2520and%2520%25282%2529%2520presenting%2520a%250Arelevant%2520application%2520where%2520we%2520quantify%2520ice%2520crystal%2520diversity%2520with%2520these%2520latent%250Arepresentations.%2520Our%2520results%2520demonstrate%2520the%2520power%2520of%2520SSL-driven%250Arepresentations%2520to%2520improve%2520the%2520characterization%2520of%2520ice%2520crystals%2520and%250Asubsequently%2520constrain%2520their%2520role%2520in%2520Earth%2527s%2520climate%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Ice%20Crystal%20Habit%20Diversity%20with%20Self-Supervised%20Learning&entry.906535625=Joseph%20Ko%20and%20Hariprasath%20Govindarajan%20and%20Fredrik%20Lindsten%20and%20Vanessa%20Przybylo%20and%20Kara%20Sulia%20and%20Marcus%20van%20Lier-Walqui%20and%20Kara%20Lamb&entry.1292438233=%20%20Ice-containing%20clouds%20strongly%20impact%20climate%2C%20but%20they%20are%20hard%20to%20model%20due%0Ato%20ice%20crystal%20habit%20%28i.e.%2C%20shape%29%20diversity.%20We%20use%20self-supervised%20learning%0A%28SSL%29%20to%20learn%20latent%20representations%20of%20crystals%20from%20ice%20crystal%20imagery.%20By%0Apre-training%20a%20vision%20transformer%20with%20many%20cloud%20particle%20images%2C%20we%20learn%0Arobust%20representations%20of%20crystal%20morphology%2C%20which%20can%20be%20used%20for%20various%0Ascience-driven%20tasks.%20Our%20key%20contributions%20include%20%281%29%20validating%20that%20our%20SSL%0Aapproach%20can%20be%20used%20to%20learn%20meaningful%20representations%2C%20and%20%282%29%20presenting%20a%0Arelevant%20application%20where%20we%20quantify%20ice%20crystal%20diversity%20with%20these%20latent%0Arepresentations.%20Our%20results%20demonstrate%20the%20power%20of%20SSL-driven%0Arepresentations%20to%20improve%20the%20characterization%20of%20ice%20crystals%20and%0Asubsequently%20constrain%20their%20role%20in%20Earth%27s%20climate%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07688v2&entry.124074799=Read"},
{"title": "Large Language Model Prompt Datasets: An In-depth Analysis and Insights", "author": "Yuanming Zhang and Yan Lin and Arijit Khan and Huaiyu Wan", "abstract": "  A prompt is a natural language instruction that defines a specific task for a\nlarge language model (LLM) and serves as the primary interface for human-LLM\ninteraction. With the growing deployment of LLMs, diverse prompt datasets are\nemerging from platforms such as GitHub and social media. These datasets span a\nwide array of applications and content types, facilitating both broader LLM\nutilization and improved prompt engineering. In this work, we--for the first\ntime--have compiled an extensive list of prompt datasets sourced from various\nchannels, representing a spectrum of downstream tasks, languages, engineering\ntechniques, attributes, and modalities. We select key representative datasets\nfor systematic analysis, revealing commonalities and differences in prompt\nconstruction across categories, distinguishing them from other text corpora\nlike literature and web. We further propose a prompt optimization approach that\nleverages syntactic embeddings of part-of-speech and dependency structures. By\nidentifying a centroid representation of prompts and guiding LLMs to rewrite\nprompts toward this centroid, our method improves the meaningfulness of model\noutputs. We have made our datasets and code available.\n", "link": "http://arxiv.org/abs/2510.09316v1", "date": "2025-10-10", "relevancy": 2.5818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Prompt%20Datasets%3A%20An%20In-depth%20Analysis%20and%20Insights&body=Title%3A%20Large%20Language%20Model%20Prompt%20Datasets%3A%20An%20In-depth%20Analysis%20and%20Insights%0AAuthor%3A%20Yuanming%20Zhang%20and%20Yan%20Lin%20and%20Arijit%20Khan%20and%20Huaiyu%20Wan%0AAbstract%3A%20%20%20A%20prompt%20is%20a%20natural%20language%20instruction%20that%20defines%20a%20specific%20task%20for%20a%0Alarge%20language%20model%20%28LLM%29%20and%20serves%20as%20the%20primary%20interface%20for%20human-LLM%0Ainteraction.%20With%20the%20growing%20deployment%20of%20LLMs%2C%20diverse%20prompt%20datasets%20are%0Aemerging%20from%20platforms%20such%20as%20GitHub%20and%20social%20media.%20These%20datasets%20span%20a%0Awide%20array%20of%20applications%20and%20content%20types%2C%20facilitating%20both%20broader%20LLM%0Autilization%20and%20improved%20prompt%20engineering.%20In%20this%20work%2C%20we--for%20the%20first%0Atime--have%20compiled%20an%20extensive%20list%20of%20prompt%20datasets%20sourced%20from%20various%0Achannels%2C%20representing%20a%20spectrum%20of%20downstream%20tasks%2C%20languages%2C%20engineering%0Atechniques%2C%20attributes%2C%20and%20modalities.%20We%20select%20key%20representative%20datasets%0Afor%20systematic%20analysis%2C%20revealing%20commonalities%20and%20differences%20in%20prompt%0Aconstruction%20across%20categories%2C%20distinguishing%20them%20from%20other%20text%20corpora%0Alike%20literature%20and%20web.%20We%20further%20propose%20a%20prompt%20optimization%20approach%20that%0Aleverages%20syntactic%20embeddings%20of%20part-of-speech%20and%20dependency%20structures.%20By%0Aidentifying%20a%20centroid%20representation%20of%20prompts%20and%20guiding%20LLMs%20to%20rewrite%0Aprompts%20toward%20this%20centroid%2C%20our%20method%20improves%20the%20meaningfulness%20of%20model%0Aoutputs.%20We%20have%20made%20our%20datasets%20and%20code%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Prompt%2520Datasets%253A%2520An%2520In-depth%2520Analysis%2520and%2520Insights%26entry.906535625%3DYuanming%2520Zhang%2520and%2520Yan%2520Lin%2520and%2520Arijit%2520Khan%2520and%2520Huaiyu%2520Wan%26entry.1292438233%3D%2520%2520A%2520prompt%2520is%2520a%2520natural%2520language%2520instruction%2520that%2520defines%2520a%2520specific%2520task%2520for%2520a%250Alarge%2520language%2520model%2520%2528LLM%2529%2520and%2520serves%2520as%2520the%2520primary%2520interface%2520for%2520human-LLM%250Ainteraction.%2520With%2520the%2520growing%2520deployment%2520of%2520LLMs%252C%2520diverse%2520prompt%2520datasets%2520are%250Aemerging%2520from%2520platforms%2520such%2520as%2520GitHub%2520and%2520social%2520media.%2520These%2520datasets%2520span%2520a%250Awide%2520array%2520of%2520applications%2520and%2520content%2520types%252C%2520facilitating%2520both%2520broader%2520LLM%250Autilization%2520and%2520improved%2520prompt%2520engineering.%2520In%2520this%2520work%252C%2520we--for%2520the%2520first%250Atime--have%2520compiled%2520an%2520extensive%2520list%2520of%2520prompt%2520datasets%2520sourced%2520from%2520various%250Achannels%252C%2520representing%2520a%2520spectrum%2520of%2520downstream%2520tasks%252C%2520languages%252C%2520engineering%250Atechniques%252C%2520attributes%252C%2520and%2520modalities.%2520We%2520select%2520key%2520representative%2520datasets%250Afor%2520systematic%2520analysis%252C%2520revealing%2520commonalities%2520and%2520differences%2520in%2520prompt%250Aconstruction%2520across%2520categories%252C%2520distinguishing%2520them%2520from%2520other%2520text%2520corpora%250Alike%2520literature%2520and%2520web.%2520We%2520further%2520propose%2520a%2520prompt%2520optimization%2520approach%2520that%250Aleverages%2520syntactic%2520embeddings%2520of%2520part-of-speech%2520and%2520dependency%2520structures.%2520By%250Aidentifying%2520a%2520centroid%2520representation%2520of%2520prompts%2520and%2520guiding%2520LLMs%2520to%2520rewrite%250Aprompts%2520toward%2520this%2520centroid%252C%2520our%2520method%2520improves%2520the%2520meaningfulness%2520of%2520model%250Aoutputs.%2520We%2520have%2520made%2520our%2520datasets%2520and%2520code%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Prompt%20Datasets%3A%20An%20In-depth%20Analysis%20and%20Insights&entry.906535625=Yuanming%20Zhang%20and%20Yan%20Lin%20and%20Arijit%20Khan%20and%20Huaiyu%20Wan&entry.1292438233=%20%20A%20prompt%20is%20a%20natural%20language%20instruction%20that%20defines%20a%20specific%20task%20for%20a%0Alarge%20language%20model%20%28LLM%29%20and%20serves%20as%20the%20primary%20interface%20for%20human-LLM%0Ainteraction.%20With%20the%20growing%20deployment%20of%20LLMs%2C%20diverse%20prompt%20datasets%20are%0Aemerging%20from%20platforms%20such%20as%20GitHub%20and%20social%20media.%20These%20datasets%20span%20a%0Awide%20array%20of%20applications%20and%20content%20types%2C%20facilitating%20both%20broader%20LLM%0Autilization%20and%20improved%20prompt%20engineering.%20In%20this%20work%2C%20we--for%20the%20first%0Atime--have%20compiled%20an%20extensive%20list%20of%20prompt%20datasets%20sourced%20from%20various%0Achannels%2C%20representing%20a%20spectrum%20of%20downstream%20tasks%2C%20languages%2C%20engineering%0Atechniques%2C%20attributes%2C%20and%20modalities.%20We%20select%20key%20representative%20datasets%0Afor%20systematic%20analysis%2C%20revealing%20commonalities%20and%20differences%20in%20prompt%0Aconstruction%20across%20categories%2C%20distinguishing%20them%20from%20other%20text%20corpora%0Alike%20literature%20and%20web.%20We%20further%20propose%20a%20prompt%20optimization%20approach%20that%0Aleverages%20syntactic%20embeddings%20of%20part-of-speech%20and%20dependency%20structures.%20By%0Aidentifying%20a%20centroid%20representation%20of%20prompts%20and%20guiding%20LLMs%20to%20rewrite%0Aprompts%20toward%20this%20centroid%2C%20our%20method%20improves%20the%20meaningfulness%20of%20model%0Aoutputs.%20We%20have%20made%20our%20datasets%20and%20code%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09316v1&entry.124074799=Read"},
{"title": "Rewiring Development in Brain Segmentation: Leveraging Adult Brain\n  Priors for Enhancing Infant MRI Segmentation", "author": "Alemu Sisay Nigru and Michele Svanera and Austin Dibble and Connor Dalby and Mattia Savardi and Sergio Benini", "abstract": "  Accurate segmentation of infant brain MRI is critical for studying early\nneurodevelopment and diagnosing neurological disorders. Yet, it remains a\nfundamental challenge due to continuously evolving anatomy of the subjects,\nmotion artifacts, and the scarcity of high-quality labeled data. In this work,\nwe present LODi, a novel framework that utilizes prior knowledge from an adult\nbrain MRI segmentation model to enhance the segmentation performance of infant\nscans. Given the abundance of publicly available adult brain MRI data, we\npre-train a segmentation model on a large adult dataset as a starting point.\nThrough transfer learning and domain adaptation strategies, we progressively\nadapt the model to the 0-2 year-old population, enabling it to account for the\nanatomical and imaging variability typical of infant scans. The adaptation of\nthe adult model is carried out using weakly supervised learning on infant brain\nscans, leveraging silver-standard ground truth labels obtained with FreeSurfer.\nBy introducing a novel training strategy that integrates hierarchical feature\nrefinement and multi-level consistency constraints, our method enables fast,\naccurate, age-adaptive segmentation, while mitigating scanner and site-specific\nbiases. Extensive experiments on both internal and external datasets\ndemonstrate the superiority of our approach over traditional supervised\nlearning and domain-specific models. Our findings highlight the advantage of\nleveraging adult brain priors as a foundation for age-flexible neuroimaging\nanalysis, paving the way for more reliable and generalizable brain MRI\nsegmentation across the lifespan.\n", "link": "http://arxiv.org/abs/2510.09306v1", "date": "2025-10-10", "relevancy": 2.5717, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rewiring%20Development%20in%20Brain%20Segmentation%3A%20Leveraging%20Adult%20Brain%0A%20%20Priors%20for%20Enhancing%20Infant%20MRI%20Segmentation&body=Title%3A%20Rewiring%20Development%20in%20Brain%20Segmentation%3A%20Leveraging%20Adult%20Brain%0A%20%20Priors%20for%20Enhancing%20Infant%20MRI%20Segmentation%0AAuthor%3A%20Alemu%20Sisay%20Nigru%20and%20Michele%20Svanera%20and%20Austin%20Dibble%20and%20Connor%20Dalby%20and%20Mattia%20Savardi%20and%20Sergio%20Benini%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20infant%20brain%20MRI%20is%20critical%20for%20studying%20early%0Aneurodevelopment%20and%20diagnosing%20neurological%20disorders.%20Yet%2C%20it%20remains%20a%0Afundamental%20challenge%20due%20to%20continuously%20evolving%20anatomy%20of%20the%20subjects%2C%0Amotion%20artifacts%2C%20and%20the%20scarcity%20of%20high-quality%20labeled%20data.%20In%20this%20work%2C%0Awe%20present%20LODi%2C%20a%20novel%20framework%20that%20utilizes%20prior%20knowledge%20from%20an%20adult%0Abrain%20MRI%20segmentation%20model%20to%20enhance%20the%20segmentation%20performance%20of%20infant%0Ascans.%20Given%20the%20abundance%20of%20publicly%20available%20adult%20brain%20MRI%20data%2C%20we%0Apre-train%20a%20segmentation%20model%20on%20a%20large%20adult%20dataset%20as%20a%20starting%20point.%0AThrough%20transfer%20learning%20and%20domain%20adaptation%20strategies%2C%20we%20progressively%0Aadapt%20the%20model%20to%20the%200-2%20year-old%20population%2C%20enabling%20it%20to%20account%20for%20the%0Aanatomical%20and%20imaging%20variability%20typical%20of%20infant%20scans.%20The%20adaptation%20of%0Athe%20adult%20model%20is%20carried%20out%20using%20weakly%20supervised%20learning%20on%20infant%20brain%0Ascans%2C%20leveraging%20silver-standard%20ground%20truth%20labels%20obtained%20with%20FreeSurfer.%0ABy%20introducing%20a%20novel%20training%20strategy%20that%20integrates%20hierarchical%20feature%0Arefinement%20and%20multi-level%20consistency%20constraints%2C%20our%20method%20enables%20fast%2C%0Aaccurate%2C%20age-adaptive%20segmentation%2C%20while%20mitigating%20scanner%20and%20site-specific%0Abiases.%20Extensive%20experiments%20on%20both%20internal%20and%20external%20datasets%0Ademonstrate%20the%20superiority%20of%20our%20approach%20over%20traditional%20supervised%0Alearning%20and%20domain-specific%20models.%20Our%20findings%20highlight%20the%20advantage%20of%0Aleveraging%20adult%20brain%20priors%20as%20a%20foundation%20for%20age-flexible%20neuroimaging%0Aanalysis%2C%20paving%20the%20way%20for%20more%20reliable%20and%20generalizable%20brain%20MRI%0Asegmentation%20across%20the%20lifespan.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRewiring%2520Development%2520in%2520Brain%2520Segmentation%253A%2520Leveraging%2520Adult%2520Brain%250A%2520%2520Priors%2520for%2520Enhancing%2520Infant%2520MRI%2520Segmentation%26entry.906535625%3DAlemu%2520Sisay%2520Nigru%2520and%2520Michele%2520Svanera%2520and%2520Austin%2520Dibble%2520and%2520Connor%2520Dalby%2520and%2520Mattia%2520Savardi%2520and%2520Sergio%2520Benini%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520infant%2520brain%2520MRI%2520is%2520critical%2520for%2520studying%2520early%250Aneurodevelopment%2520and%2520diagnosing%2520neurological%2520disorders.%2520Yet%252C%2520it%2520remains%2520a%250Afundamental%2520challenge%2520due%2520to%2520continuously%2520evolving%2520anatomy%2520of%2520the%2520subjects%252C%250Amotion%2520artifacts%252C%2520and%2520the%2520scarcity%2520of%2520high-quality%2520labeled%2520data.%2520In%2520this%2520work%252C%250Awe%2520present%2520LODi%252C%2520a%2520novel%2520framework%2520that%2520utilizes%2520prior%2520knowledge%2520from%2520an%2520adult%250Abrain%2520MRI%2520segmentation%2520model%2520to%2520enhance%2520the%2520segmentation%2520performance%2520of%2520infant%250Ascans.%2520Given%2520the%2520abundance%2520of%2520publicly%2520available%2520adult%2520brain%2520MRI%2520data%252C%2520we%250Apre-train%2520a%2520segmentation%2520model%2520on%2520a%2520large%2520adult%2520dataset%2520as%2520a%2520starting%2520point.%250AThrough%2520transfer%2520learning%2520and%2520domain%2520adaptation%2520strategies%252C%2520we%2520progressively%250Aadapt%2520the%2520model%2520to%2520the%25200-2%2520year-old%2520population%252C%2520enabling%2520it%2520to%2520account%2520for%2520the%250Aanatomical%2520and%2520imaging%2520variability%2520typical%2520of%2520infant%2520scans.%2520The%2520adaptation%2520of%250Athe%2520adult%2520model%2520is%2520carried%2520out%2520using%2520weakly%2520supervised%2520learning%2520on%2520infant%2520brain%250Ascans%252C%2520leveraging%2520silver-standard%2520ground%2520truth%2520labels%2520obtained%2520with%2520FreeSurfer.%250ABy%2520introducing%2520a%2520novel%2520training%2520strategy%2520that%2520integrates%2520hierarchical%2520feature%250Arefinement%2520and%2520multi-level%2520consistency%2520constraints%252C%2520our%2520method%2520enables%2520fast%252C%250Aaccurate%252C%2520age-adaptive%2520segmentation%252C%2520while%2520mitigating%2520scanner%2520and%2520site-specific%250Abiases.%2520Extensive%2520experiments%2520on%2520both%2520internal%2520and%2520external%2520datasets%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520traditional%2520supervised%250Alearning%2520and%2520domain-specific%2520models.%2520Our%2520findings%2520highlight%2520the%2520advantage%2520of%250Aleveraging%2520adult%2520brain%2520priors%2520as%2520a%2520foundation%2520for%2520age-flexible%2520neuroimaging%250Aanalysis%252C%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520and%2520generalizable%2520brain%2520MRI%250Asegmentation%2520across%2520the%2520lifespan.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rewiring%20Development%20in%20Brain%20Segmentation%3A%20Leveraging%20Adult%20Brain%0A%20%20Priors%20for%20Enhancing%20Infant%20MRI%20Segmentation&entry.906535625=Alemu%20Sisay%20Nigru%20and%20Michele%20Svanera%20and%20Austin%20Dibble%20and%20Connor%20Dalby%20and%20Mattia%20Savardi%20and%20Sergio%20Benini&entry.1292438233=%20%20Accurate%20segmentation%20of%20infant%20brain%20MRI%20is%20critical%20for%20studying%20early%0Aneurodevelopment%20and%20diagnosing%20neurological%20disorders.%20Yet%2C%20it%20remains%20a%0Afundamental%20challenge%20due%20to%20continuously%20evolving%20anatomy%20of%20the%20subjects%2C%0Amotion%20artifacts%2C%20and%20the%20scarcity%20of%20high-quality%20labeled%20data.%20In%20this%20work%2C%0Awe%20present%20LODi%2C%20a%20novel%20framework%20that%20utilizes%20prior%20knowledge%20from%20an%20adult%0Abrain%20MRI%20segmentation%20model%20to%20enhance%20the%20segmentation%20performance%20of%20infant%0Ascans.%20Given%20the%20abundance%20of%20publicly%20available%20adult%20brain%20MRI%20data%2C%20we%0Apre-train%20a%20segmentation%20model%20on%20a%20large%20adult%20dataset%20as%20a%20starting%20point.%0AThrough%20transfer%20learning%20and%20domain%20adaptation%20strategies%2C%20we%20progressively%0Aadapt%20the%20model%20to%20the%200-2%20year-old%20population%2C%20enabling%20it%20to%20account%20for%20the%0Aanatomical%20and%20imaging%20variability%20typical%20of%20infant%20scans.%20The%20adaptation%20of%0Athe%20adult%20model%20is%20carried%20out%20using%20weakly%20supervised%20learning%20on%20infant%20brain%0Ascans%2C%20leveraging%20silver-standard%20ground%20truth%20labels%20obtained%20with%20FreeSurfer.%0ABy%20introducing%20a%20novel%20training%20strategy%20that%20integrates%20hierarchical%20feature%0Arefinement%20and%20multi-level%20consistency%20constraints%2C%20our%20method%20enables%20fast%2C%0Aaccurate%2C%20age-adaptive%20segmentation%2C%20while%20mitigating%20scanner%20and%20site-specific%0Abiases.%20Extensive%20experiments%20on%20both%20internal%20and%20external%20datasets%0Ademonstrate%20the%20superiority%20of%20our%20approach%20over%20traditional%20supervised%0Alearning%20and%20domain-specific%20models.%20Our%20findings%20highlight%20the%20advantage%20of%0Aleveraging%20adult%20brain%20priors%20as%20a%20foundation%20for%20age-flexible%20neuroimaging%0Aanalysis%2C%20paving%20the%20way%20for%20more%20reliable%20and%20generalizable%20brain%20MRI%0Asegmentation%20across%20the%20lifespan.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09306v1&entry.124074799=Read"},
{"title": "Continual Adapter Tuning with Semantic Shift Compensation for\n  Class-Incremental Learning", "author": "Qinhao Zhou and Yuwen Tan and Boqing Gong and Xiang Xiang", "abstract": "  Class-incremental learning (CIL) aims to enable models to continuously learn\nnew classes while overcoming catastrophic forgetting. The introduction of\npre-trained models has brought new tuning paradigms to CIL. In this paper, we\nrevisit different parameter-efficient tuning (PET) methods within the context\nof continual learning. We observe that adapter tuning demonstrates superiority\nover prompt-based methods, even without parameter expansion in each learning\nsession. Motivated by this, we propose incrementally tuning the shared adapter\nwithout imposing parameter update constraints, enhancing the learning capacity\nof the backbone. Additionally, we employ feature sampling from stored\nprototypes to retrain a unified classifier, further improving its performance.\nWe estimate the semantic shift of old prototypes without access to past samples\nand update stored prototypes session by session. Our proposed method eliminates\nmodel expansion and avoids retaining any image samples. It surpasses previous\npre-trained model-based CIL methods and demonstrates remarkable continual\nlearning capabilities. Experimental results on five CIL benchmarks validate the\neffectiveness of our approach, achieving state-of-the-art (SOTA) performance.\n", "link": "http://arxiv.org/abs/2403.19979v2", "date": "2025-10-10", "relevancy": 2.548, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4938}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Adapter%20Tuning%20with%20Semantic%20Shift%20Compensation%20for%0A%20%20Class-Incremental%20Learning&body=Title%3A%20Continual%20Adapter%20Tuning%20with%20Semantic%20Shift%20Compensation%20for%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Qinhao%20Zhou%20and%20Yuwen%20Tan%20and%20Boqing%20Gong%20and%20Xiang%20Xiang%0AAbstract%3A%20%20%20Class-incremental%20learning%20%28CIL%29%20aims%20to%20enable%20models%20to%20continuously%20learn%0Anew%20classes%20while%20overcoming%20catastrophic%20forgetting.%20The%20introduction%20of%0Apre-trained%20models%20has%20brought%20new%20tuning%20paradigms%20to%20CIL.%20In%20this%20paper%2C%20we%0Arevisit%20different%20parameter-efficient%20tuning%20%28PET%29%20methods%20within%20the%20context%0Aof%20continual%20learning.%20We%20observe%20that%20adapter%20tuning%20demonstrates%20superiority%0Aover%20prompt-based%20methods%2C%20even%20without%20parameter%20expansion%20in%20each%20learning%0Asession.%20Motivated%20by%20this%2C%20we%20propose%20incrementally%20tuning%20the%20shared%20adapter%0Awithout%20imposing%20parameter%20update%20constraints%2C%20enhancing%20the%20learning%20capacity%0Aof%20the%20backbone.%20Additionally%2C%20we%20employ%20feature%20sampling%20from%20stored%0Aprototypes%20to%20retrain%20a%20unified%20classifier%2C%20further%20improving%20its%20performance.%0AWe%20estimate%20the%20semantic%20shift%20of%20old%20prototypes%20without%20access%20to%20past%20samples%0Aand%20update%20stored%20prototypes%20session%20by%20session.%20Our%20proposed%20method%20eliminates%0Amodel%20expansion%20and%20avoids%20retaining%20any%20image%20samples.%20It%20surpasses%20previous%0Apre-trained%20model-based%20CIL%20methods%20and%20demonstrates%20remarkable%20continual%0Alearning%20capabilities.%20Experimental%20results%20on%20five%20CIL%20benchmarks%20validate%20the%0Aeffectiveness%20of%20our%20approach%2C%20achieving%20state-of-the-art%20%28SOTA%29%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Adapter%2520Tuning%2520with%2520Semantic%2520Shift%2520Compensation%2520for%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DQinhao%2520Zhou%2520and%2520Yuwen%2520Tan%2520and%2520Boqing%2520Gong%2520and%2520Xiang%2520Xiang%26entry.1292438233%3D%2520%2520Class-incremental%2520learning%2520%2528CIL%2529%2520aims%2520to%2520enable%2520models%2520to%2520continuously%2520learn%250Anew%2520classes%2520while%2520overcoming%2520catastrophic%2520forgetting.%2520The%2520introduction%2520of%250Apre-trained%2520models%2520has%2520brought%2520new%2520tuning%2520paradigms%2520to%2520CIL.%2520In%2520this%2520paper%252C%2520we%250Arevisit%2520different%2520parameter-efficient%2520tuning%2520%2528PET%2529%2520methods%2520within%2520the%2520context%250Aof%2520continual%2520learning.%2520We%2520observe%2520that%2520adapter%2520tuning%2520demonstrates%2520superiority%250Aover%2520prompt-based%2520methods%252C%2520even%2520without%2520parameter%2520expansion%2520in%2520each%2520learning%250Asession.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520incrementally%2520tuning%2520the%2520shared%2520adapter%250Awithout%2520imposing%2520parameter%2520update%2520constraints%252C%2520enhancing%2520the%2520learning%2520capacity%250Aof%2520the%2520backbone.%2520Additionally%252C%2520we%2520employ%2520feature%2520sampling%2520from%2520stored%250Aprototypes%2520to%2520retrain%2520a%2520unified%2520classifier%252C%2520further%2520improving%2520its%2520performance.%250AWe%2520estimate%2520the%2520semantic%2520shift%2520of%2520old%2520prototypes%2520without%2520access%2520to%2520past%2520samples%250Aand%2520update%2520stored%2520prototypes%2520session%2520by%2520session.%2520Our%2520proposed%2520method%2520eliminates%250Amodel%2520expansion%2520and%2520avoids%2520retaining%2520any%2520image%2520samples.%2520It%2520surpasses%2520previous%250Apre-trained%2520model-based%2520CIL%2520methods%2520and%2520demonstrates%2520remarkable%2520continual%250Alearning%2520capabilities.%2520Experimental%2520results%2520on%2520five%2520CIL%2520benchmarks%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520approach%252C%2520achieving%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Adapter%20Tuning%20with%20Semantic%20Shift%20Compensation%20for%0A%20%20Class-Incremental%20Learning&entry.906535625=Qinhao%20Zhou%20and%20Yuwen%20Tan%20and%20Boqing%20Gong%20and%20Xiang%20Xiang&entry.1292438233=%20%20Class-incremental%20learning%20%28CIL%29%20aims%20to%20enable%20models%20to%20continuously%20learn%0Anew%20classes%20while%20overcoming%20catastrophic%20forgetting.%20The%20introduction%20of%0Apre-trained%20models%20has%20brought%20new%20tuning%20paradigms%20to%20CIL.%20In%20this%20paper%2C%20we%0Arevisit%20different%20parameter-efficient%20tuning%20%28PET%29%20methods%20within%20the%20context%0Aof%20continual%20learning.%20We%20observe%20that%20adapter%20tuning%20demonstrates%20superiority%0Aover%20prompt-based%20methods%2C%20even%20without%20parameter%20expansion%20in%20each%20learning%0Asession.%20Motivated%20by%20this%2C%20we%20propose%20incrementally%20tuning%20the%20shared%20adapter%0Awithout%20imposing%20parameter%20update%20constraints%2C%20enhancing%20the%20learning%20capacity%0Aof%20the%20backbone.%20Additionally%2C%20we%20employ%20feature%20sampling%20from%20stored%0Aprototypes%20to%20retrain%20a%20unified%20classifier%2C%20further%20improving%20its%20performance.%0AWe%20estimate%20the%20semantic%20shift%20of%20old%20prototypes%20without%20access%20to%20past%20samples%0Aand%20update%20stored%20prototypes%20session%20by%20session.%20Our%20proposed%20method%20eliminates%0Amodel%20expansion%20and%20avoids%20retaining%20any%20image%20samples.%20It%20surpasses%20previous%0Apre-trained%20model-based%20CIL%20methods%20and%20demonstrates%20remarkable%20continual%0Alearning%20capabilities.%20Experimental%20results%20on%20five%20CIL%20benchmarks%20validate%20the%0Aeffectiveness%20of%20our%20approach%2C%20achieving%20state-of-the-art%20%28SOTA%29%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19979v2&entry.124074799=Read"},
{"title": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from\n  RNA-Seq Data", "author": "Oussama Kharouiche and Aris Markogiannakis and Xiao Fei and Michail Chatzianastasis and Michalis Vazirgiannis", "abstract": "  Single-cell RNA sequencing has transformed biology by enabling the\nmeasurement of gene expression at cellular resolution, providing information\nfor cell types, states, and disease contexts. Recently, single-cell foundation\nmodels have emerged as powerful tools for learning transferable representations\ndirectly from expression profiles, improving performance on classification and\nclustering tasks. However, these models are limited to discrete prediction\nheads, which collapse cellular complexity into predefined labels that fail to\ncapture the richer, contextual explanations biologists need. We introduce\nCell2Text, a multimodal generative framework that translates scRNA-seq profiles\ninto structured natural language descriptions. By integrating gene-level\nembeddings from single-cell foundation models with pretrained large language\nmodels, Cell2Text generates coherent summaries that capture cellular identity,\ntissue origin, disease associations, and pathway activity, generalizing to\nunseen cells. Empirically, Cell2Text outperforms baselines on classification\naccuracy, demonstrates strong ontological consistency using PageRank-based\nsimilarity metrics, and achieves high semantic fidelity in text generation.\nThese results demonstrate that coupling expression data with natural language\noffers both stronger predictive performance and inherently interpretable\noutputs, pointing to a scalable path for label-efficient characterization of\nunseen cells.\n", "link": "http://arxiv.org/abs/2509.24840v2", "date": "2025-10-10", "relevancy": 2.547, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4976}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cell2Text%3A%20Multimodal%20LLM%20for%20Generating%20Single-Cell%20Descriptions%20from%0A%20%20RNA-Seq%20Data&body=Title%3A%20Cell2Text%3A%20Multimodal%20LLM%20for%20Generating%20Single-Cell%20Descriptions%20from%0A%20%20RNA-Seq%20Data%0AAuthor%3A%20Oussama%20Kharouiche%20and%20Aris%20Markogiannakis%20and%20Xiao%20Fei%20and%20Michail%20Chatzianastasis%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Single-cell%20RNA%20sequencing%20has%20transformed%20biology%20by%20enabling%20the%0Ameasurement%20of%20gene%20expression%20at%20cellular%20resolution%2C%20providing%20information%0Afor%20cell%20types%2C%20states%2C%20and%20disease%20contexts.%20Recently%2C%20single-cell%20foundation%0Amodels%20have%20emerged%20as%20powerful%20tools%20for%20learning%20transferable%20representations%0Adirectly%20from%20expression%20profiles%2C%20improving%20performance%20on%20classification%20and%0Aclustering%20tasks.%20However%2C%20these%20models%20are%20limited%20to%20discrete%20prediction%0Aheads%2C%20which%20collapse%20cellular%20complexity%20into%20predefined%20labels%20that%20fail%20to%0Acapture%20the%20richer%2C%20contextual%20explanations%20biologists%20need.%20We%20introduce%0ACell2Text%2C%20a%20multimodal%20generative%20framework%20that%20translates%20scRNA-seq%20profiles%0Ainto%20structured%20natural%20language%20descriptions.%20By%20integrating%20gene-level%0Aembeddings%20from%20single-cell%20foundation%20models%20with%20pretrained%20large%20language%0Amodels%2C%20Cell2Text%20generates%20coherent%20summaries%20that%20capture%20cellular%20identity%2C%0Atissue%20origin%2C%20disease%20associations%2C%20and%20pathway%20activity%2C%20generalizing%20to%0Aunseen%20cells.%20Empirically%2C%20Cell2Text%20outperforms%20baselines%20on%20classification%0Aaccuracy%2C%20demonstrates%20strong%20ontological%20consistency%20using%20PageRank-based%0Asimilarity%20metrics%2C%20and%20achieves%20high%20semantic%20fidelity%20in%20text%20generation.%0AThese%20results%20demonstrate%20that%20coupling%20expression%20data%20with%20natural%20language%0Aoffers%20both%20stronger%20predictive%20performance%20and%20inherently%20interpretable%0Aoutputs%2C%20pointing%20to%20a%20scalable%20path%20for%20label-efficient%20characterization%20of%0Aunseen%20cells.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24840v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCell2Text%253A%2520Multimodal%2520LLM%2520for%2520Generating%2520Single-Cell%2520Descriptions%2520from%250A%2520%2520RNA-Seq%2520Data%26entry.906535625%3DOussama%2520Kharouiche%2520and%2520Aris%2520Markogiannakis%2520and%2520Xiao%2520Fei%2520and%2520Michail%2520Chatzianastasis%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Single-cell%2520RNA%2520sequencing%2520has%2520transformed%2520biology%2520by%2520enabling%2520the%250Ameasurement%2520of%2520gene%2520expression%2520at%2520cellular%2520resolution%252C%2520providing%2520information%250Afor%2520cell%2520types%252C%2520states%252C%2520and%2520disease%2520contexts.%2520Recently%252C%2520single-cell%2520foundation%250Amodels%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520learning%2520transferable%2520representations%250Adirectly%2520from%2520expression%2520profiles%252C%2520improving%2520performance%2520on%2520classification%2520and%250Aclustering%2520tasks.%2520However%252C%2520these%2520models%2520are%2520limited%2520to%2520discrete%2520prediction%250Aheads%252C%2520which%2520collapse%2520cellular%2520complexity%2520into%2520predefined%2520labels%2520that%2520fail%2520to%250Acapture%2520the%2520richer%252C%2520contextual%2520explanations%2520biologists%2520need.%2520We%2520introduce%250ACell2Text%252C%2520a%2520multimodal%2520generative%2520framework%2520that%2520translates%2520scRNA-seq%2520profiles%250Ainto%2520structured%2520natural%2520language%2520descriptions.%2520By%2520integrating%2520gene-level%250Aembeddings%2520from%2520single-cell%2520foundation%2520models%2520with%2520pretrained%2520large%2520language%250Amodels%252C%2520Cell2Text%2520generates%2520coherent%2520summaries%2520that%2520capture%2520cellular%2520identity%252C%250Atissue%2520origin%252C%2520disease%2520associations%252C%2520and%2520pathway%2520activity%252C%2520generalizing%2520to%250Aunseen%2520cells.%2520Empirically%252C%2520Cell2Text%2520outperforms%2520baselines%2520on%2520classification%250Aaccuracy%252C%2520demonstrates%2520strong%2520ontological%2520consistency%2520using%2520PageRank-based%250Asimilarity%2520metrics%252C%2520and%2520achieves%2520high%2520semantic%2520fidelity%2520in%2520text%2520generation.%250AThese%2520results%2520demonstrate%2520that%2520coupling%2520expression%2520data%2520with%2520natural%2520language%250Aoffers%2520both%2520stronger%2520predictive%2520performance%2520and%2520inherently%2520interpretable%250Aoutputs%252C%2520pointing%2520to%2520a%2520scalable%2520path%2520for%2520label-efficient%2520characterization%2520of%250Aunseen%2520cells.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24840v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cell2Text%3A%20Multimodal%20LLM%20for%20Generating%20Single-Cell%20Descriptions%20from%0A%20%20RNA-Seq%20Data&entry.906535625=Oussama%20Kharouiche%20and%20Aris%20Markogiannakis%20and%20Xiao%20Fei%20and%20Michail%20Chatzianastasis%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Single-cell%20RNA%20sequencing%20has%20transformed%20biology%20by%20enabling%20the%0Ameasurement%20of%20gene%20expression%20at%20cellular%20resolution%2C%20providing%20information%0Afor%20cell%20types%2C%20states%2C%20and%20disease%20contexts.%20Recently%2C%20single-cell%20foundation%0Amodels%20have%20emerged%20as%20powerful%20tools%20for%20learning%20transferable%20representations%0Adirectly%20from%20expression%20profiles%2C%20improving%20performance%20on%20classification%20and%0Aclustering%20tasks.%20However%2C%20these%20models%20are%20limited%20to%20discrete%20prediction%0Aheads%2C%20which%20collapse%20cellular%20complexity%20into%20predefined%20labels%20that%20fail%20to%0Acapture%20the%20richer%2C%20contextual%20explanations%20biologists%20need.%20We%20introduce%0ACell2Text%2C%20a%20multimodal%20generative%20framework%20that%20translates%20scRNA-seq%20profiles%0Ainto%20structured%20natural%20language%20descriptions.%20By%20integrating%20gene-level%0Aembeddings%20from%20single-cell%20foundation%20models%20with%20pretrained%20large%20language%0Amodels%2C%20Cell2Text%20generates%20coherent%20summaries%20that%20capture%20cellular%20identity%2C%0Atissue%20origin%2C%20disease%20associations%2C%20and%20pathway%20activity%2C%20generalizing%20to%0Aunseen%20cells.%20Empirically%2C%20Cell2Text%20outperforms%20baselines%20on%20classification%0Aaccuracy%2C%20demonstrates%20strong%20ontological%20consistency%20using%20PageRank-based%0Asimilarity%20metrics%2C%20and%20achieves%20high%20semantic%20fidelity%20in%20text%20generation.%0AThese%20results%20demonstrate%20that%20coupling%20expression%20data%20with%20natural%20language%0Aoffers%20both%20stronger%20predictive%20performance%20and%20inherently%20interpretable%0Aoutputs%2C%20pointing%20to%20a%20scalable%20path%20for%20label-efficient%20characterization%20of%0Aunseen%20cells.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24840v2&entry.124074799=Read"},
{"title": "Brain2Text Decoding Model Reveals the Neural Mechanisms of Visual\n  Semantic Processing", "author": "Feihan Feng and Jingxin Nie", "abstract": "  Decoding sensory experiences from neural activity to reconstruct\nhuman-perceived visual stimuli and semantic content remains a challenge in\nneuroscience and artificial intelligence. Despite notable progress in current\nbrain decoding models, a critical gap still persists in their systematic\nintegration with established neuroscientific theories and the exploration of\nunderlying neural mechanisms. Here, we present a novel framework that directly\ndecodes fMRI signals into textual descriptions of viewed natural images. Our\nnovel deep learning model, trained without visual information, achieves\nstate-of-the-art semantic decoding performance, generating meaningful captions\nthat capture the core semantic content of complex scenes. Neuroanatomical\nanalysis reveals the critical role of higher-level visual cortices, including\nMT+ complex, ventral stream visual cortex, and inferior parietal cortex, in\nvisual semantic processing. Furthermore, category-specific analysis\ndemonstrates nuanced neural representations for semantic dimensions like\nanimacy and motion. This work provides a more direct and interpretable\nframework to the brain's semantic decoding, offering a powerful new methodology\nfor probing the neural basis of complex semantic processing, refining the\nunderstanding of the distributed semantic network, and potentially developing\nbrain-sinpired language models.\n", "link": "http://arxiv.org/abs/2503.22697v2", "date": "2025-10-10", "relevancy": 2.535, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6549}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain2Text%20Decoding%20Model%20Reveals%20the%20Neural%20Mechanisms%20of%20Visual%0A%20%20Semantic%20Processing&body=Title%3A%20Brain2Text%20Decoding%20Model%20Reveals%20the%20Neural%20Mechanisms%20of%20Visual%0A%20%20Semantic%20Processing%0AAuthor%3A%20Feihan%20Feng%20and%20Jingxin%20Nie%0AAbstract%3A%20%20%20Decoding%20sensory%20experiences%20from%20neural%20activity%20to%20reconstruct%0Ahuman-perceived%20visual%20stimuli%20and%20semantic%20content%20remains%20a%20challenge%20in%0Aneuroscience%20and%20artificial%20intelligence.%20Despite%20notable%20progress%20in%20current%0Abrain%20decoding%20models%2C%20a%20critical%20gap%20still%20persists%20in%20their%20systematic%0Aintegration%20with%20established%20neuroscientific%20theories%20and%20the%20exploration%20of%0Aunderlying%20neural%20mechanisms.%20Here%2C%20we%20present%20a%20novel%20framework%20that%20directly%0Adecodes%20fMRI%20signals%20into%20textual%20descriptions%20of%20viewed%20natural%20images.%20Our%0Anovel%20deep%20learning%20model%2C%20trained%20without%20visual%20information%2C%20achieves%0Astate-of-the-art%20semantic%20decoding%20performance%2C%20generating%20meaningful%20captions%0Athat%20capture%20the%20core%20semantic%20content%20of%20complex%20scenes.%20Neuroanatomical%0Aanalysis%20reveals%20the%20critical%20role%20of%20higher-level%20visual%20cortices%2C%20including%0AMT%2B%20complex%2C%20ventral%20stream%20visual%20cortex%2C%20and%20inferior%20parietal%20cortex%2C%20in%0Avisual%20semantic%20processing.%20Furthermore%2C%20category-specific%20analysis%0Ademonstrates%20nuanced%20neural%20representations%20for%20semantic%20dimensions%20like%0Aanimacy%20and%20motion.%20This%20work%20provides%20a%20more%20direct%20and%20interpretable%0Aframework%20to%20the%20brain%27s%20semantic%20decoding%2C%20offering%20a%20powerful%20new%20methodology%0Afor%20probing%20the%20neural%20basis%20of%20complex%20semantic%20processing%2C%20refining%20the%0Aunderstanding%20of%20the%20distributed%20semantic%20network%2C%20and%20potentially%20developing%0Abrain-sinpired%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain2Text%2520Decoding%2520Model%2520Reveals%2520the%2520Neural%2520Mechanisms%2520of%2520Visual%250A%2520%2520Semantic%2520Processing%26entry.906535625%3DFeihan%2520Feng%2520and%2520Jingxin%2520Nie%26entry.1292438233%3D%2520%2520Decoding%2520sensory%2520experiences%2520from%2520neural%2520activity%2520to%2520reconstruct%250Ahuman-perceived%2520visual%2520stimuli%2520and%2520semantic%2520content%2520remains%2520a%2520challenge%2520in%250Aneuroscience%2520and%2520artificial%2520intelligence.%2520Despite%2520notable%2520progress%2520in%2520current%250Abrain%2520decoding%2520models%252C%2520a%2520critical%2520gap%2520still%2520persists%2520in%2520their%2520systematic%250Aintegration%2520with%2520established%2520neuroscientific%2520theories%2520and%2520the%2520exploration%2520of%250Aunderlying%2520neural%2520mechanisms.%2520Here%252C%2520we%2520present%2520a%2520novel%2520framework%2520that%2520directly%250Adecodes%2520fMRI%2520signals%2520into%2520textual%2520descriptions%2520of%2520viewed%2520natural%2520images.%2520Our%250Anovel%2520deep%2520learning%2520model%252C%2520trained%2520without%2520visual%2520information%252C%2520achieves%250Astate-of-the-art%2520semantic%2520decoding%2520performance%252C%2520generating%2520meaningful%2520captions%250Athat%2520capture%2520the%2520core%2520semantic%2520content%2520of%2520complex%2520scenes.%2520Neuroanatomical%250Aanalysis%2520reveals%2520the%2520critical%2520role%2520of%2520higher-level%2520visual%2520cortices%252C%2520including%250AMT%252B%2520complex%252C%2520ventral%2520stream%2520visual%2520cortex%252C%2520and%2520inferior%2520parietal%2520cortex%252C%2520in%250Avisual%2520semantic%2520processing.%2520Furthermore%252C%2520category-specific%2520analysis%250Ademonstrates%2520nuanced%2520neural%2520representations%2520for%2520semantic%2520dimensions%2520like%250Aanimacy%2520and%2520motion.%2520This%2520work%2520provides%2520a%2520more%2520direct%2520and%2520interpretable%250Aframework%2520to%2520the%2520brain%2527s%2520semantic%2520decoding%252C%2520offering%2520a%2520powerful%2520new%2520methodology%250Afor%2520probing%2520the%2520neural%2520basis%2520of%2520complex%2520semantic%2520processing%252C%2520refining%2520the%250Aunderstanding%2520of%2520the%2520distributed%2520semantic%2520network%252C%2520and%2520potentially%2520developing%250Abrain-sinpired%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain2Text%20Decoding%20Model%20Reveals%20the%20Neural%20Mechanisms%20of%20Visual%0A%20%20Semantic%20Processing&entry.906535625=Feihan%20Feng%20and%20Jingxin%20Nie&entry.1292438233=%20%20Decoding%20sensory%20experiences%20from%20neural%20activity%20to%20reconstruct%0Ahuman-perceived%20visual%20stimuli%20and%20semantic%20content%20remains%20a%20challenge%20in%0Aneuroscience%20and%20artificial%20intelligence.%20Despite%20notable%20progress%20in%20current%0Abrain%20decoding%20models%2C%20a%20critical%20gap%20still%20persists%20in%20their%20systematic%0Aintegration%20with%20established%20neuroscientific%20theories%20and%20the%20exploration%20of%0Aunderlying%20neural%20mechanisms.%20Here%2C%20we%20present%20a%20novel%20framework%20that%20directly%0Adecodes%20fMRI%20signals%20into%20textual%20descriptions%20of%20viewed%20natural%20images.%20Our%0Anovel%20deep%20learning%20model%2C%20trained%20without%20visual%20information%2C%20achieves%0Astate-of-the-art%20semantic%20decoding%20performance%2C%20generating%20meaningful%20captions%0Athat%20capture%20the%20core%20semantic%20content%20of%20complex%20scenes.%20Neuroanatomical%0Aanalysis%20reveals%20the%20critical%20role%20of%20higher-level%20visual%20cortices%2C%20including%0AMT%2B%20complex%2C%20ventral%20stream%20visual%20cortex%2C%20and%20inferior%20parietal%20cortex%2C%20in%0Avisual%20semantic%20processing.%20Furthermore%2C%20category-specific%20analysis%0Ademonstrates%20nuanced%20neural%20representations%20for%20semantic%20dimensions%20like%0Aanimacy%20and%20motion.%20This%20work%20provides%20a%20more%20direct%20and%20interpretable%0Aframework%20to%20the%20brain%27s%20semantic%20decoding%2C%20offering%20a%20powerful%20new%20methodology%0Afor%20probing%20the%20neural%20basis%20of%20complex%20semantic%20processing%2C%20refining%20the%0Aunderstanding%20of%20the%20distributed%20semantic%20network%2C%20and%20potentially%20developing%0Abrain-sinpired%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22697v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Mamba Architectures for Medical Image\n  Analysis: Classification, Segmentation, Restoration and Beyond", "author": "Shubhi Bansal and Sreeharish A and Madhava Prasath J and Manikandan S and Sreekanth Madisetty and Mohammad Zia Ur Rehman and Chandravardhan Singh Raghaw and Gaurav Duggal and Nagendra Kumar", "abstract": "  Mamba, a special case of the State Space Model, is gaining popularity as an\nalternative to template-based deep learning approaches in medical image\nanalysis. While transformers are powerful architectures, they have drawbacks,\nincluding quadratic computational complexity and an inability to address\nlong-range dependencies efficiently. This limitation affects the analysis of\nlarge and complex datasets in medical imaging, where there are many spatial and\ntemporal relationships. In contrast, Mamba offers benefits that make it\nwell-suited for medical image analysis. It has linear time complexity, which is\na significant improvement over transformers. Mamba processes longer sequences\nwithout attention mechanisms, enabling faster inference and requiring less\nmemory. Mamba also demonstrates strong performance in merging multimodal data,\nimproving diagnosis accuracy and patient outcomes. The organization of this\npaper allows readers to appreciate the capabilities of Mamba in medical imaging\nstep by step. We begin by defining core concepts of SSMs and models, including\nS4, S5, and S6, followed by an exploration of Mamba architectures such as pure\nMamba, U-Net variants, and hybrid models with convolutional neural networks,\ntransformers, and Graph Neural Networks. We also cover Mamba optimizations,\ntechniques and adaptations, scanning, datasets, applications, experimental\nresults, and conclude with its challenges and future directions in medical\nimaging. This review aims to demonstrate the transformative potential of Mamba\nin overcoming existing barriers within medical imaging while paving the way for\ninnovative advancements in the field. A comprehensive list of Mamba\narchitectures applied in the medical field, reviewed in this work, is available\nat Github.\n", "link": "http://arxiv.org/abs/2410.02362v3", "date": "2025-10-10", "relevancy": 2.527, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5252}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20of%20Mamba%20Architectures%20for%20Medical%20Image%0A%20%20Analysis%3A%20Classification%2C%20Segmentation%2C%20Restoration%20and%20Beyond&body=Title%3A%20A%20Comprehensive%20Survey%20of%20Mamba%20Architectures%20for%20Medical%20Image%0A%20%20Analysis%3A%20Classification%2C%20Segmentation%2C%20Restoration%20and%20Beyond%0AAuthor%3A%20Shubhi%20Bansal%20and%20Sreeharish%20A%20and%20Madhava%20Prasath%20J%20and%20Manikandan%20S%20and%20Sreekanth%20Madisetty%20and%20Mohammad%20Zia%20Ur%20Rehman%20and%20Chandravardhan%20Singh%20Raghaw%20and%20Gaurav%20Duggal%20and%20Nagendra%20Kumar%0AAbstract%3A%20%20%20Mamba%2C%20a%20special%20case%20of%20the%20State%20Space%20Model%2C%20is%20gaining%20popularity%20as%20an%0Aalternative%20to%20template-based%20deep%20learning%20approaches%20in%20medical%20image%0Aanalysis.%20While%20transformers%20are%20powerful%20architectures%2C%20they%20have%20drawbacks%2C%0Aincluding%20quadratic%20computational%20complexity%20and%20an%20inability%20to%20address%0Along-range%20dependencies%20efficiently.%20This%20limitation%20affects%20the%20analysis%20of%0Alarge%20and%20complex%20datasets%20in%20medical%20imaging%2C%20where%20there%20are%20many%20spatial%20and%0Atemporal%20relationships.%20In%20contrast%2C%20Mamba%20offers%20benefits%20that%20make%20it%0Awell-suited%20for%20medical%20image%20analysis.%20It%20has%20linear%20time%20complexity%2C%20which%20is%0Aa%20significant%20improvement%20over%20transformers.%20Mamba%20processes%20longer%20sequences%0Awithout%20attention%20mechanisms%2C%20enabling%20faster%20inference%20and%20requiring%20less%0Amemory.%20Mamba%20also%20demonstrates%20strong%20performance%20in%20merging%20multimodal%20data%2C%0Aimproving%20diagnosis%20accuracy%20and%20patient%20outcomes.%20The%20organization%20of%20this%0Apaper%20allows%20readers%20to%20appreciate%20the%20capabilities%20of%20Mamba%20in%20medical%20imaging%0Astep%20by%20step.%20We%20begin%20by%20defining%20core%20concepts%20of%20SSMs%20and%20models%2C%20including%0AS4%2C%20S5%2C%20and%20S6%2C%20followed%20by%20an%20exploration%20of%20Mamba%20architectures%20such%20as%20pure%0AMamba%2C%20U-Net%20variants%2C%20and%20hybrid%20models%20with%20convolutional%20neural%20networks%2C%0Atransformers%2C%20and%20Graph%20Neural%20Networks.%20We%20also%20cover%20Mamba%20optimizations%2C%0Atechniques%20and%20adaptations%2C%20scanning%2C%20datasets%2C%20applications%2C%20experimental%0Aresults%2C%20and%20conclude%20with%20its%20challenges%20and%20future%20directions%20in%20medical%0Aimaging.%20This%20review%20aims%20to%20demonstrate%20the%20transformative%20potential%20of%20Mamba%0Ain%20overcoming%20existing%20barriers%20within%20medical%20imaging%20while%20paving%20the%20way%20for%0Ainnovative%20advancements%20in%20the%20field.%20A%20comprehensive%20list%20of%20Mamba%0Aarchitectures%20applied%20in%20the%20medical%20field%2C%20reviewed%20in%20this%20work%2C%20is%20available%0Aat%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02362v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520of%2520Mamba%2520Architectures%2520for%2520Medical%2520Image%250A%2520%2520Analysis%253A%2520Classification%252C%2520Segmentation%252C%2520Restoration%2520and%2520Beyond%26entry.906535625%3DShubhi%2520Bansal%2520and%2520Sreeharish%2520A%2520and%2520Madhava%2520Prasath%2520J%2520and%2520Manikandan%2520S%2520and%2520Sreekanth%2520Madisetty%2520and%2520Mohammad%2520Zia%2520Ur%2520Rehman%2520and%2520Chandravardhan%2520Singh%2520Raghaw%2520and%2520Gaurav%2520Duggal%2520and%2520Nagendra%2520Kumar%26entry.1292438233%3D%2520%2520Mamba%252C%2520a%2520special%2520case%2520of%2520the%2520State%2520Space%2520Model%252C%2520is%2520gaining%2520popularity%2520as%2520an%250Aalternative%2520to%2520template-based%2520deep%2520learning%2520approaches%2520in%2520medical%2520image%250Aanalysis.%2520While%2520transformers%2520are%2520powerful%2520architectures%252C%2520they%2520have%2520drawbacks%252C%250Aincluding%2520quadratic%2520computational%2520complexity%2520and%2520an%2520inability%2520to%2520address%250Along-range%2520dependencies%2520efficiently.%2520This%2520limitation%2520affects%2520the%2520analysis%2520of%250Alarge%2520and%2520complex%2520datasets%2520in%2520medical%2520imaging%252C%2520where%2520there%2520are%2520many%2520spatial%2520and%250Atemporal%2520relationships.%2520In%2520contrast%252C%2520Mamba%2520offers%2520benefits%2520that%2520make%2520it%250Awell-suited%2520for%2520medical%2520image%2520analysis.%2520It%2520has%2520linear%2520time%2520complexity%252C%2520which%2520is%250Aa%2520significant%2520improvement%2520over%2520transformers.%2520Mamba%2520processes%2520longer%2520sequences%250Awithout%2520attention%2520mechanisms%252C%2520enabling%2520faster%2520inference%2520and%2520requiring%2520less%250Amemory.%2520Mamba%2520also%2520demonstrates%2520strong%2520performance%2520in%2520merging%2520multimodal%2520data%252C%250Aimproving%2520diagnosis%2520accuracy%2520and%2520patient%2520outcomes.%2520The%2520organization%2520of%2520this%250Apaper%2520allows%2520readers%2520to%2520appreciate%2520the%2520capabilities%2520of%2520Mamba%2520in%2520medical%2520imaging%250Astep%2520by%2520step.%2520We%2520begin%2520by%2520defining%2520core%2520concepts%2520of%2520SSMs%2520and%2520models%252C%2520including%250AS4%252C%2520S5%252C%2520and%2520S6%252C%2520followed%2520by%2520an%2520exploration%2520of%2520Mamba%2520architectures%2520such%2520as%2520pure%250AMamba%252C%2520U-Net%2520variants%252C%2520and%2520hybrid%2520models%2520with%2520convolutional%2520neural%2520networks%252C%250Atransformers%252C%2520and%2520Graph%2520Neural%2520Networks.%2520We%2520also%2520cover%2520Mamba%2520optimizations%252C%250Atechniques%2520and%2520adaptations%252C%2520scanning%252C%2520datasets%252C%2520applications%252C%2520experimental%250Aresults%252C%2520and%2520conclude%2520with%2520its%2520challenges%2520and%2520future%2520directions%2520in%2520medical%250Aimaging.%2520This%2520review%2520aims%2520to%2520demonstrate%2520the%2520transformative%2520potential%2520of%2520Mamba%250Ain%2520overcoming%2520existing%2520barriers%2520within%2520medical%2520imaging%2520while%2520paving%2520the%2520way%2520for%250Ainnovative%2520advancements%2520in%2520the%2520field.%2520A%2520comprehensive%2520list%2520of%2520Mamba%250Aarchitectures%2520applied%2520in%2520the%2520medical%2520field%252C%2520reviewed%2520in%2520this%2520work%252C%2520is%2520available%250Aat%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02362v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Mamba%20Architectures%20for%20Medical%20Image%0A%20%20Analysis%3A%20Classification%2C%20Segmentation%2C%20Restoration%20and%20Beyond&entry.906535625=Shubhi%20Bansal%20and%20Sreeharish%20A%20and%20Madhava%20Prasath%20J%20and%20Manikandan%20S%20and%20Sreekanth%20Madisetty%20and%20Mohammad%20Zia%20Ur%20Rehman%20and%20Chandravardhan%20Singh%20Raghaw%20and%20Gaurav%20Duggal%20and%20Nagendra%20Kumar&entry.1292438233=%20%20Mamba%2C%20a%20special%20case%20of%20the%20State%20Space%20Model%2C%20is%20gaining%20popularity%20as%20an%0Aalternative%20to%20template-based%20deep%20learning%20approaches%20in%20medical%20image%0Aanalysis.%20While%20transformers%20are%20powerful%20architectures%2C%20they%20have%20drawbacks%2C%0Aincluding%20quadratic%20computational%20complexity%20and%20an%20inability%20to%20address%0Along-range%20dependencies%20efficiently.%20This%20limitation%20affects%20the%20analysis%20of%0Alarge%20and%20complex%20datasets%20in%20medical%20imaging%2C%20where%20there%20are%20many%20spatial%20and%0Atemporal%20relationships.%20In%20contrast%2C%20Mamba%20offers%20benefits%20that%20make%20it%0Awell-suited%20for%20medical%20image%20analysis.%20It%20has%20linear%20time%20complexity%2C%20which%20is%0Aa%20significant%20improvement%20over%20transformers.%20Mamba%20processes%20longer%20sequences%0Awithout%20attention%20mechanisms%2C%20enabling%20faster%20inference%20and%20requiring%20less%0Amemory.%20Mamba%20also%20demonstrates%20strong%20performance%20in%20merging%20multimodal%20data%2C%0Aimproving%20diagnosis%20accuracy%20and%20patient%20outcomes.%20The%20organization%20of%20this%0Apaper%20allows%20readers%20to%20appreciate%20the%20capabilities%20of%20Mamba%20in%20medical%20imaging%0Astep%20by%20step.%20We%20begin%20by%20defining%20core%20concepts%20of%20SSMs%20and%20models%2C%20including%0AS4%2C%20S5%2C%20and%20S6%2C%20followed%20by%20an%20exploration%20of%20Mamba%20architectures%20such%20as%20pure%0AMamba%2C%20U-Net%20variants%2C%20and%20hybrid%20models%20with%20convolutional%20neural%20networks%2C%0Atransformers%2C%20and%20Graph%20Neural%20Networks.%20We%20also%20cover%20Mamba%20optimizations%2C%0Atechniques%20and%20adaptations%2C%20scanning%2C%20datasets%2C%20applications%2C%20experimental%0Aresults%2C%20and%20conclude%20with%20its%20challenges%20and%20future%20directions%20in%20medical%0Aimaging.%20This%20review%20aims%20to%20demonstrate%20the%20transformative%20potential%20of%20Mamba%0Ain%20overcoming%20existing%20barriers%20within%20medical%20imaging%20while%20paving%20the%20way%20for%0Ainnovative%20advancements%20in%20the%20field.%20A%20comprehensive%20list%20of%20Mamba%0Aarchitectures%20applied%20in%20the%20medical%20field%2C%20reviewed%20in%20this%20work%2C%20is%20available%0Aat%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02362v3&entry.124074799=Read"},
{"title": "Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models:\n  Lessons Learned and Best Practices", "author": "Md Tahmid Rahman Laskar and Mohammed Saidul Islam and Ridwan Mahbub and Mizanur Rahman and Amran Bhuiyan and Israt Jahan and Mir Tafseer Nayeem and Shafiq Joty and Enamul Hoque and Jimmy Huang", "abstract": "  Large Vision-Language Models (LVLMs) with only 7B parameters have shown\npromise as automated judges in chart comprehension tasks. However, tiny models\n(<=2B parameters) still perform poorly as judges, limiting their real-world use\nin resource-constrained settings. To address this, we propose two approaches to\nensure cost-efficient evaluation: (i) multi-criteria prompting, which combines\nseparate evaluation criteria into a single query, and (ii) domain-adaptive\ntransfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic\njudgments in a chart dataset to create the ChartJudge. Experiments show that\nmulti-criteria prompting exposes robustness gaps, which led to a huge drop in\nperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.\nIn addition, we find that our tiny LVLM (ChartJudge) can effectively transfer\nknowledge from one dataset to another to make it a more specialized model. Our\nfine-grained analysis across chart types and query complexities offers\nactionable insights into trade-offs between model size, prompt design, and\ntransferability, enabling scalable, low-cost evaluation for chart reasoning\ntasks.\n", "link": "http://arxiv.org/abs/2510.07545v2", "date": "2025-10-10", "relevancy": 2.5087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deploying%20Tiny%20LVLM%20Judges%20for%20Real-World%20Evaluation%20of%20Chart%20Models%3A%0A%20%20Lessons%20Learned%20and%20Best%20Practices&body=Title%3A%20Deploying%20Tiny%20LVLM%20Judges%20for%20Real-World%20Evaluation%20of%20Chart%20Models%3A%0A%20%20Lessons%20Learned%20and%20Best%20Practices%0AAuthor%3A%20Md%20Tahmid%20Rahman%20Laskar%20and%20Mohammed%20Saidul%20Islam%20and%20Ridwan%20Mahbub%20and%20Mizanur%20Rahman%20and%20Amran%20Bhuiyan%20and%20Israt%20Jahan%20and%20Mir%20Tafseer%20Nayeem%20and%20Shafiq%20Joty%20and%20Enamul%20Hoque%20and%20Jimmy%20Huang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20with%20only%207B%20parameters%20have%20shown%0Apromise%20as%20automated%20judges%20in%20chart%20comprehension%20tasks.%20However%2C%20tiny%20models%0A%28%3C%3D2B%20parameters%29%20still%20perform%20poorly%20as%20judges%2C%20limiting%20their%20real-world%20use%0Ain%20resource-constrained%20settings.%20To%20address%20this%2C%20we%20propose%20two%20approaches%20to%0Aensure%20cost-efficient%20evaluation%3A%20%28i%29%20multi-criteria%20prompting%2C%20which%20combines%0Aseparate%20evaluation%20criteria%20into%20a%20single%20query%2C%20and%20%28ii%29%20domain-adaptive%0Atransfer%20learning%2C%20in%20which%20we%20fine-tune%20a%202B-parameter%20LVLM%20on%20synthetic%0Ajudgments%20in%20a%20chart%20dataset%20to%20create%20the%20ChartJudge.%20Experiments%20show%20that%0Amulti-criteria%20prompting%20exposes%20robustness%20gaps%2C%20which%20led%20to%20a%20huge%20drop%20in%0Aperformance%20for%207B%20models%2C%20including%20specialized%20LVLM%20judges%20like%20LLaVA-Critic.%0AIn%20addition%2C%20we%20find%20that%20our%20tiny%20LVLM%20%28ChartJudge%29%20can%20effectively%20transfer%0Aknowledge%20from%20one%20dataset%20to%20another%20to%20make%20it%20a%20more%20specialized%20model.%20Our%0Afine-grained%20analysis%20across%20chart%20types%20and%20query%20complexities%20offers%0Aactionable%20insights%20into%20trade-offs%20between%20model%20size%2C%20prompt%20design%2C%20and%0Atransferability%2C%20enabling%20scalable%2C%20low-cost%20evaluation%20for%20chart%20reasoning%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeploying%2520Tiny%2520LVLM%2520Judges%2520for%2520Real-World%2520Evaluation%2520of%2520Chart%2520Models%253A%250A%2520%2520Lessons%2520Learned%2520and%2520Best%2520Practices%26entry.906535625%3DMd%2520Tahmid%2520Rahman%2520Laskar%2520and%2520Mohammed%2520Saidul%2520Islam%2520and%2520Ridwan%2520Mahbub%2520and%2520Mizanur%2520Rahman%2520and%2520Amran%2520Bhuiyan%2520and%2520Israt%2520Jahan%2520and%2520Mir%2520Tafseer%2520Nayeem%2520and%2520Shafiq%2520Joty%2520and%2520Enamul%2520Hoque%2520and%2520Jimmy%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520with%2520only%25207B%2520parameters%2520have%2520shown%250Apromise%2520as%2520automated%2520judges%2520in%2520chart%2520comprehension%2520tasks.%2520However%252C%2520tiny%2520models%250A%2528%253C%253D2B%2520parameters%2529%2520still%2520perform%2520poorly%2520as%2520judges%252C%2520limiting%2520their%2520real-world%2520use%250Ain%2520resource-constrained%2520settings.%2520To%2520address%2520this%252C%2520we%2520propose%2520two%2520approaches%2520to%250Aensure%2520cost-efficient%2520evaluation%253A%2520%2528i%2529%2520multi-criteria%2520prompting%252C%2520which%2520combines%250Aseparate%2520evaluation%2520criteria%2520into%2520a%2520single%2520query%252C%2520and%2520%2528ii%2529%2520domain-adaptive%250Atransfer%2520learning%252C%2520in%2520which%2520we%2520fine-tune%2520a%25202B-parameter%2520LVLM%2520on%2520synthetic%250Ajudgments%2520in%2520a%2520chart%2520dataset%2520to%2520create%2520the%2520ChartJudge.%2520Experiments%2520show%2520that%250Amulti-criteria%2520prompting%2520exposes%2520robustness%2520gaps%252C%2520which%2520led%2520to%2520a%2520huge%2520drop%2520in%250Aperformance%2520for%25207B%2520models%252C%2520including%2520specialized%2520LVLM%2520judges%2520like%2520LLaVA-Critic.%250AIn%2520addition%252C%2520we%2520find%2520that%2520our%2520tiny%2520LVLM%2520%2528ChartJudge%2529%2520can%2520effectively%2520transfer%250Aknowledge%2520from%2520one%2520dataset%2520to%2520another%2520to%2520make%2520it%2520a%2520more%2520specialized%2520model.%2520Our%250Afine-grained%2520analysis%2520across%2520chart%2520types%2520and%2520query%2520complexities%2520offers%250Aactionable%2520insights%2520into%2520trade-offs%2520between%2520model%2520size%252C%2520prompt%2520design%252C%2520and%250Atransferability%252C%2520enabling%2520scalable%252C%2520low-cost%2520evaluation%2520for%2520chart%2520reasoning%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deploying%20Tiny%20LVLM%20Judges%20for%20Real-World%20Evaluation%20of%20Chart%20Models%3A%0A%20%20Lessons%20Learned%20and%20Best%20Practices&entry.906535625=Md%20Tahmid%20Rahman%20Laskar%20and%20Mohammed%20Saidul%20Islam%20and%20Ridwan%20Mahbub%20and%20Mizanur%20Rahman%20and%20Amran%20Bhuiyan%20and%20Israt%20Jahan%20and%20Mir%20Tafseer%20Nayeem%20and%20Shafiq%20Joty%20and%20Enamul%20Hoque%20and%20Jimmy%20Huang&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20with%20only%207B%20parameters%20have%20shown%0Apromise%20as%20automated%20judges%20in%20chart%20comprehension%20tasks.%20However%2C%20tiny%20models%0A%28%3C%3D2B%20parameters%29%20still%20perform%20poorly%20as%20judges%2C%20limiting%20their%20real-world%20use%0Ain%20resource-constrained%20settings.%20To%20address%20this%2C%20we%20propose%20two%20approaches%20to%0Aensure%20cost-efficient%20evaluation%3A%20%28i%29%20multi-criteria%20prompting%2C%20which%20combines%0Aseparate%20evaluation%20criteria%20into%20a%20single%20query%2C%20and%20%28ii%29%20domain-adaptive%0Atransfer%20learning%2C%20in%20which%20we%20fine-tune%20a%202B-parameter%20LVLM%20on%20synthetic%0Ajudgments%20in%20a%20chart%20dataset%20to%20create%20the%20ChartJudge.%20Experiments%20show%20that%0Amulti-criteria%20prompting%20exposes%20robustness%20gaps%2C%20which%20led%20to%20a%20huge%20drop%20in%0Aperformance%20for%207B%20models%2C%20including%20specialized%20LVLM%20judges%20like%20LLaVA-Critic.%0AIn%20addition%2C%20we%20find%20that%20our%20tiny%20LVLM%20%28ChartJudge%29%20can%20effectively%20transfer%0Aknowledge%20from%20one%20dataset%20to%20another%20to%20make%20it%20a%20more%20specialized%20model.%20Our%0Afine-grained%20analysis%20across%20chart%20types%20and%20query%20complexities%20offers%0Aactionable%20insights%20into%20trade-offs%20between%20model%20size%2C%20prompt%20design%2C%20and%0Atransferability%2C%20enabling%20scalable%2C%20low-cost%20evaluation%20for%20chart%20reasoning%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07545v2&entry.124074799=Read"},
{"title": "CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts", "author": "Jiuheng Lin and Cong Jiang and Zirui Wu and Jiarui Sun and Yansong Feng", "abstract": "  Training expert LLMs in domains with scarce data is difficult, often relying\non multiple-choice questions (MCQs). However, standard outcome-based\nreinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we\nobserve it often degrades reasoning quality such as logical consistency.\nExisting solutions to supervise reasoning, such as large-scale Process Reward\nModels (PRMs), are prohibitively expensive. To address this, we propose\nCLARity, a cost-effective RL framework that enhances reasoning quality using\nonly a small, general-purpose LLM. CLARity integrates a consistency-aware\nreward mechanism with a 2-stage refine-then-monitor training pipeline to\nenhance reasoning consistency, and a dynamic data reformulation strategy to to\nbetter exploit limited data. Experiments demonstrate that CLARity improves\nresponse consistency by 16.5% and accuracy by 7.5% over baselines. Human\nevaluations further confirm holistic improvements in coherence and\nprofessionalism. Thus, CLARity offers a generalizable solution that enables\nsmaller models to effectively guide expert models by reasoning consistency.Our\ncode is open sourced at: https://github.com/Infinite-set/CLARity\n", "link": "http://arxiv.org/abs/2510.09278v1", "date": "2025-10-10", "relevancy": 2.502, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5374}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLARity%3A%20Reasoning%20Consistency%20Alone%20Can%20Teach%20Reinforced%20Experts&body=Title%3A%20CLARity%3A%20Reasoning%20Consistency%20Alone%20Can%20Teach%20Reinforced%20Experts%0AAuthor%3A%20Jiuheng%20Lin%20and%20Cong%20Jiang%20and%20Zirui%20Wu%20and%20Jiarui%20Sun%20and%20Yansong%20Feng%0AAbstract%3A%20%20%20Training%20expert%20LLMs%20in%20domains%20with%20scarce%20data%20is%20difficult%2C%20often%20relying%0Aon%20multiple-choice%20questions%20%28MCQs%29.%20However%2C%20standard%20outcome-based%0Areinforcement%20learning%20%28RL%29%20on%20MCQs%20is%20risky.%20While%20it%20may%20improve%20accuracy%2C%20we%0Aobserve%20it%20often%20degrades%20reasoning%20quality%20such%20as%20logical%20consistency.%0AExisting%20solutions%20to%20supervise%20reasoning%2C%20such%20as%20large-scale%20Process%20Reward%0AModels%20%28PRMs%29%2C%20are%20prohibitively%20expensive.%20To%20address%20this%2C%20we%20propose%0ACLARity%2C%20a%20cost-effective%20RL%20framework%20that%20enhances%20reasoning%20quality%20using%0Aonly%20a%20small%2C%20general-purpose%20LLM.%20CLARity%20integrates%20a%20consistency-aware%0Areward%20mechanism%20with%20a%202-stage%20refine-then-monitor%20training%20pipeline%20to%0Aenhance%20reasoning%20consistency%2C%20and%20a%20dynamic%20data%20reformulation%20strategy%20to%20to%0Abetter%20exploit%20limited%20data.%20Experiments%20demonstrate%20that%20CLARity%20improves%0Aresponse%20consistency%20by%2016.5%25%20and%20accuracy%20by%207.5%25%20over%20baselines.%20Human%0Aevaluations%20further%20confirm%20holistic%20improvements%20in%20coherence%20and%0Aprofessionalism.%20Thus%2C%20CLARity%20offers%20a%20generalizable%20solution%20that%20enables%0Asmaller%20models%20to%20effectively%20guide%20expert%20models%20by%20reasoning%20consistency.Our%0Acode%20is%20open%20sourced%20at%3A%20https%3A//github.com/Infinite-set/CLARity%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLARity%253A%2520Reasoning%2520Consistency%2520Alone%2520Can%2520Teach%2520Reinforced%2520Experts%26entry.906535625%3DJiuheng%2520Lin%2520and%2520Cong%2520Jiang%2520and%2520Zirui%2520Wu%2520and%2520Jiarui%2520Sun%2520and%2520Yansong%2520Feng%26entry.1292438233%3D%2520%2520Training%2520expert%2520LLMs%2520in%2520domains%2520with%2520scarce%2520data%2520is%2520difficult%252C%2520often%2520relying%250Aon%2520multiple-choice%2520questions%2520%2528MCQs%2529.%2520However%252C%2520standard%2520outcome-based%250Areinforcement%2520learning%2520%2528RL%2529%2520on%2520MCQs%2520is%2520risky.%2520While%2520it%2520may%2520improve%2520accuracy%252C%2520we%250Aobserve%2520it%2520often%2520degrades%2520reasoning%2520quality%2520such%2520as%2520logical%2520consistency.%250AExisting%2520solutions%2520to%2520supervise%2520reasoning%252C%2520such%2520as%2520large-scale%2520Process%2520Reward%250AModels%2520%2528PRMs%2529%252C%2520are%2520prohibitively%2520expensive.%2520To%2520address%2520this%252C%2520we%2520propose%250ACLARity%252C%2520a%2520cost-effective%2520RL%2520framework%2520that%2520enhances%2520reasoning%2520quality%2520using%250Aonly%2520a%2520small%252C%2520general-purpose%2520LLM.%2520CLARity%2520integrates%2520a%2520consistency-aware%250Areward%2520mechanism%2520with%2520a%25202-stage%2520refine-then-monitor%2520training%2520pipeline%2520to%250Aenhance%2520reasoning%2520consistency%252C%2520and%2520a%2520dynamic%2520data%2520reformulation%2520strategy%2520to%2520to%250Abetter%2520exploit%2520limited%2520data.%2520Experiments%2520demonstrate%2520that%2520CLARity%2520improves%250Aresponse%2520consistency%2520by%252016.5%2525%2520and%2520accuracy%2520by%25207.5%2525%2520over%2520baselines.%2520Human%250Aevaluations%2520further%2520confirm%2520holistic%2520improvements%2520in%2520coherence%2520and%250Aprofessionalism.%2520Thus%252C%2520CLARity%2520offers%2520a%2520generalizable%2520solution%2520that%2520enables%250Asmaller%2520models%2520to%2520effectively%2520guide%2520expert%2520models%2520by%2520reasoning%2520consistency.Our%250Acode%2520is%2520open%2520sourced%2520at%253A%2520https%253A//github.com/Infinite-set/CLARity%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLARity%3A%20Reasoning%20Consistency%20Alone%20Can%20Teach%20Reinforced%20Experts&entry.906535625=Jiuheng%20Lin%20and%20Cong%20Jiang%20and%20Zirui%20Wu%20and%20Jiarui%20Sun%20and%20Yansong%20Feng&entry.1292438233=%20%20Training%20expert%20LLMs%20in%20domains%20with%20scarce%20data%20is%20difficult%2C%20often%20relying%0Aon%20multiple-choice%20questions%20%28MCQs%29.%20However%2C%20standard%20outcome-based%0Areinforcement%20learning%20%28RL%29%20on%20MCQs%20is%20risky.%20While%20it%20may%20improve%20accuracy%2C%20we%0Aobserve%20it%20often%20degrades%20reasoning%20quality%20such%20as%20logical%20consistency.%0AExisting%20solutions%20to%20supervise%20reasoning%2C%20such%20as%20large-scale%20Process%20Reward%0AModels%20%28PRMs%29%2C%20are%20prohibitively%20expensive.%20To%20address%20this%2C%20we%20propose%0ACLARity%2C%20a%20cost-effective%20RL%20framework%20that%20enhances%20reasoning%20quality%20using%0Aonly%20a%20small%2C%20general-purpose%20LLM.%20CLARity%20integrates%20a%20consistency-aware%0Areward%20mechanism%20with%20a%202-stage%20refine-then-monitor%20training%20pipeline%20to%0Aenhance%20reasoning%20consistency%2C%20and%20a%20dynamic%20data%20reformulation%20strategy%20to%20to%0Abetter%20exploit%20limited%20data.%20Experiments%20demonstrate%20that%20CLARity%20improves%0Aresponse%20consistency%20by%2016.5%25%20and%20accuracy%20by%207.5%25%20over%20baselines.%20Human%0Aevaluations%20further%20confirm%20holistic%20improvements%20in%20coherence%20and%0Aprofessionalism.%20Thus%2C%20CLARity%20offers%20a%20generalizable%20solution%20that%20enables%0Asmaller%20models%20to%20effectively%20guide%20expert%20models%20by%20reasoning%20consistency.Our%0Acode%20is%20open%20sourced%20at%3A%20https%3A//github.com/Infinite-set/CLARity%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09278v1&entry.124074799=Read"},
{"title": "Truncated Kernel Stochastic Gradient Descent with General Losses and\n  Spherical Radial Basis Functions", "author": "Jinhui Bai and Andreas Christmann and Lei Shi", "abstract": "  In this paper, we propose a novel kernel stochastic gradient descent (SGD)\nalgorithm for large-scale supervised learning with general losses. Compared to\ntraditional kernel SGD, our algorithm improves efficiency and scalability\nthrough an innovative regularization strategy. By leveraging the infinite\nseries expansion of spherical radial basis functions, this strategy projects\nthe stochastic gradient onto a finite-dimensional hypothesis space, which is\nadaptively scaled according to the bias-variance trade-off, thereby enhancing\ngeneralization performance. Based on a new estimation of the spectral structure\nof the kernel-induced covariance operator, we develop an analytical framework\nthat unifies optimization and generalization analyses. We prove that both the\nlast iterate and the suffix average converge at minimax-optimal rates, and we\nfurther establish optimal strong convergence in the reproducing kernel Hilbert\nspace. Our framework accommodates a broad class of classical loss functions,\nincluding least-squares, Huber, and logistic losses. Moreover, the proposed\nalgorithm significantly reduces computational complexity and achieves optimal\nstorage complexity by incorporating coordinate-wise updates from linear SGD,\nthereby avoiding the costly pairwise operations typical of kernel SGD and\nenabling efficient processing of streaming data. Finally, extensive numerical\nexperiments demonstrate the efficiency of our approach.\n", "link": "http://arxiv.org/abs/2510.04237v2", "date": "2025-10-10", "relevancy": 2.4948, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5107}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4942}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20with%20General%20Losses%20and%0A%20%20Spherical%20Radial%20Basis%20Functions&body=Title%3A%20Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20with%20General%20Losses%20and%0A%20%20Spherical%20Radial%20Basis%20Functions%0AAuthor%3A%20Jinhui%20Bai%20and%20Andreas%20Christmann%20and%20Lei%20Shi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20kernel%20stochastic%20gradient%20descent%20%28SGD%29%0Aalgorithm%20for%20large-scale%20supervised%20learning%20with%20general%20losses.%20Compared%20to%0Atraditional%20kernel%20SGD%2C%20our%20algorithm%20improves%20efficiency%20and%20scalability%0Athrough%20an%20innovative%20regularization%20strategy.%20By%20leveraging%20the%20infinite%0Aseries%20expansion%20of%20spherical%20radial%20basis%20functions%2C%20this%20strategy%20projects%0Athe%20stochastic%20gradient%20onto%20a%20finite-dimensional%20hypothesis%20space%2C%20which%20is%0Aadaptively%20scaled%20according%20to%20the%20bias-variance%20trade-off%2C%20thereby%20enhancing%0Ageneralization%20performance.%20Based%20on%20a%20new%20estimation%20of%20the%20spectral%20structure%0Aof%20the%20kernel-induced%20covariance%20operator%2C%20we%20develop%20an%20analytical%20framework%0Athat%20unifies%20optimization%20and%20generalization%20analyses.%20We%20prove%20that%20both%20the%0Alast%20iterate%20and%20the%20suffix%20average%20converge%20at%20minimax-optimal%20rates%2C%20and%20we%0Afurther%20establish%20optimal%20strong%20convergence%20in%20the%20reproducing%20kernel%20Hilbert%0Aspace.%20Our%20framework%20accommodates%20a%20broad%20class%20of%20classical%20loss%20functions%2C%0Aincluding%20least-squares%2C%20Huber%2C%20and%20logistic%20losses.%20Moreover%2C%20the%20proposed%0Aalgorithm%20significantly%20reduces%20computational%20complexity%20and%20achieves%20optimal%0Astorage%20complexity%20by%20incorporating%20coordinate-wise%20updates%20from%20linear%20SGD%2C%0Athereby%20avoiding%20the%20costly%20pairwise%20operations%20typical%20of%20kernel%20SGD%20and%0Aenabling%20efficient%20processing%20of%20streaming%20data.%20Finally%2C%20extensive%20numerical%0Aexperiments%20demonstrate%20the%20efficiency%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruncated%2520Kernel%2520Stochastic%2520Gradient%2520Descent%2520with%2520General%2520Losses%2520and%250A%2520%2520Spherical%2520Radial%2520Basis%2520Functions%26entry.906535625%3DJinhui%2520Bai%2520and%2520Andreas%2520Christmann%2520and%2520Lei%2520Shi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520kernel%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%250Aalgorithm%2520for%2520large-scale%2520supervised%2520learning%2520with%2520general%2520losses.%2520Compared%2520to%250Atraditional%2520kernel%2520SGD%252C%2520our%2520algorithm%2520improves%2520efficiency%2520and%2520scalability%250Athrough%2520an%2520innovative%2520regularization%2520strategy.%2520By%2520leveraging%2520the%2520infinite%250Aseries%2520expansion%2520of%2520spherical%2520radial%2520basis%2520functions%252C%2520this%2520strategy%2520projects%250Athe%2520stochastic%2520gradient%2520onto%2520a%2520finite-dimensional%2520hypothesis%2520space%252C%2520which%2520is%250Aadaptively%2520scaled%2520according%2520to%2520the%2520bias-variance%2520trade-off%252C%2520thereby%2520enhancing%250Ageneralization%2520performance.%2520Based%2520on%2520a%2520new%2520estimation%2520of%2520the%2520spectral%2520structure%250Aof%2520the%2520kernel-induced%2520covariance%2520operator%252C%2520we%2520develop%2520an%2520analytical%2520framework%250Athat%2520unifies%2520optimization%2520and%2520generalization%2520analyses.%2520We%2520prove%2520that%2520both%2520the%250Alast%2520iterate%2520and%2520the%2520suffix%2520average%2520converge%2520at%2520minimax-optimal%2520rates%252C%2520and%2520we%250Afurther%2520establish%2520optimal%2520strong%2520convergence%2520in%2520the%2520reproducing%2520kernel%2520Hilbert%250Aspace.%2520Our%2520framework%2520accommodates%2520a%2520broad%2520class%2520of%2520classical%2520loss%2520functions%252C%250Aincluding%2520least-squares%252C%2520Huber%252C%2520and%2520logistic%2520losses.%2520Moreover%252C%2520the%2520proposed%250Aalgorithm%2520significantly%2520reduces%2520computational%2520complexity%2520and%2520achieves%2520optimal%250Astorage%2520complexity%2520by%2520incorporating%2520coordinate-wise%2520updates%2520from%2520linear%2520SGD%252C%250Athereby%2520avoiding%2520the%2520costly%2520pairwise%2520operations%2520typical%2520of%2520kernel%2520SGD%2520and%250Aenabling%2520efficient%2520processing%2520of%2520streaming%2520data.%2520Finally%252C%2520extensive%2520numerical%250Aexperiments%2520demonstrate%2520the%2520efficiency%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20with%20General%20Losses%20and%0A%20%20Spherical%20Radial%20Basis%20Functions&entry.906535625=Jinhui%20Bai%20and%20Andreas%20Christmann%20and%20Lei%20Shi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20kernel%20stochastic%20gradient%20descent%20%28SGD%29%0Aalgorithm%20for%20large-scale%20supervised%20learning%20with%20general%20losses.%20Compared%20to%0Atraditional%20kernel%20SGD%2C%20our%20algorithm%20improves%20efficiency%20and%20scalability%0Athrough%20an%20innovative%20regularization%20strategy.%20By%20leveraging%20the%20infinite%0Aseries%20expansion%20of%20spherical%20radial%20basis%20functions%2C%20this%20strategy%20projects%0Athe%20stochastic%20gradient%20onto%20a%20finite-dimensional%20hypothesis%20space%2C%20which%20is%0Aadaptively%20scaled%20according%20to%20the%20bias-variance%20trade-off%2C%20thereby%20enhancing%0Ageneralization%20performance.%20Based%20on%20a%20new%20estimation%20of%20the%20spectral%20structure%0Aof%20the%20kernel-induced%20covariance%20operator%2C%20we%20develop%20an%20analytical%20framework%0Athat%20unifies%20optimization%20and%20generalization%20analyses.%20We%20prove%20that%20both%20the%0Alast%20iterate%20and%20the%20suffix%20average%20converge%20at%20minimax-optimal%20rates%2C%20and%20we%0Afurther%20establish%20optimal%20strong%20convergence%20in%20the%20reproducing%20kernel%20Hilbert%0Aspace.%20Our%20framework%20accommodates%20a%20broad%20class%20of%20classical%20loss%20functions%2C%0Aincluding%20least-squares%2C%20Huber%2C%20and%20logistic%20losses.%20Moreover%2C%20the%20proposed%0Aalgorithm%20significantly%20reduces%20computational%20complexity%20and%20achieves%20optimal%0Astorage%20complexity%20by%20incorporating%20coordinate-wise%20updates%20from%20linear%20SGD%2C%0Athereby%20avoiding%20the%20costly%20pairwise%20operations%20typical%20of%20kernel%20SGD%20and%0Aenabling%20efficient%20processing%20of%20streaming%20data.%20Finally%2C%20extensive%20numerical%0Aexperiments%20demonstrate%20the%20efficiency%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04237v2&entry.124074799=Read"},
{"title": "A Multimodal Depth-Aware Method For Embodied Reference Understanding", "author": "Fevziye Irem Eyiokur and Dogucan Yaman and Haz\u0131m Kemal Ekenel and Alexander Waibel", "abstract": "  Embodied Reference Understanding requires identifying a target object in a\nvisual scene based on both language instructions and pointing cues. While prior\nworks have shown progress in open-vocabulary object detection, they often fail\nin ambiguous scenarios where multiple candidate objects exist in the scene. To\naddress these challenges, we propose a novel ERU framework that jointly\nleverages LLM-based data augmentation, depth-map modality, and a depth-aware\ndecision module. This design enables robust integration of linguistic and\nembodied cues, improving disambiguation in complex or cluttered environments.\nExperimental results on two datasets demonstrate that our approach\nsignificantly outperforms existing baselines, achieving more accurate and\nreliable referent detection.\n", "link": "http://arxiv.org/abs/2510.08278v2", "date": "2025-10-10", "relevancy": 2.4867, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Depth-Aware%20Method%20For%20Embodied%20Reference%20Understanding&body=Title%3A%20A%20Multimodal%20Depth-Aware%20Method%20For%20Embodied%20Reference%20Understanding%0AAuthor%3A%20Fevziye%20Irem%20Eyiokur%20and%20Dogucan%20Yaman%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20Embodied%20Reference%20Understanding%20requires%20identifying%20a%20target%20object%20in%20a%0Avisual%20scene%20based%20on%20both%20language%20instructions%20and%20pointing%20cues.%20While%20prior%0Aworks%20have%20shown%20progress%20in%20open-vocabulary%20object%20detection%2C%20they%20often%20fail%0Ain%20ambiguous%20scenarios%20where%20multiple%20candidate%20objects%20exist%20in%20the%20scene.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20ERU%20framework%20that%20jointly%0Aleverages%20LLM-based%20data%20augmentation%2C%20depth-map%20modality%2C%20and%20a%20depth-aware%0Adecision%20module.%20This%20design%20enables%20robust%20integration%20of%20linguistic%20and%0Aembodied%20cues%2C%20improving%20disambiguation%20in%20complex%20or%20cluttered%20environments.%0AExperimental%20results%20on%20two%20datasets%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20existing%20baselines%2C%20achieving%20more%20accurate%20and%0Areliable%20referent%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Depth-Aware%2520Method%2520For%2520Embodied%2520Reference%2520Understanding%26entry.906535625%3DFevziye%2520Irem%2520Eyiokur%2520and%2520Dogucan%2520Yaman%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520Embodied%2520Reference%2520Understanding%2520requires%2520identifying%2520a%2520target%2520object%2520in%2520a%250Avisual%2520scene%2520based%2520on%2520both%2520language%2520instructions%2520and%2520pointing%2520cues.%2520While%2520prior%250Aworks%2520have%2520shown%2520progress%2520in%2520open-vocabulary%2520object%2520detection%252C%2520they%2520often%2520fail%250Ain%2520ambiguous%2520scenarios%2520where%2520multiple%2520candidate%2520objects%2520exist%2520in%2520the%2520scene.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520ERU%2520framework%2520that%2520jointly%250Aleverages%2520LLM-based%2520data%2520augmentation%252C%2520depth-map%2520modality%252C%2520and%2520a%2520depth-aware%250Adecision%2520module.%2520This%2520design%2520enables%2520robust%2520integration%2520of%2520linguistic%2520and%250Aembodied%2520cues%252C%2520improving%2520disambiguation%2520in%2520complex%2520or%2520cluttered%2520environments.%250AExperimental%2520results%2520on%2520two%2520datasets%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520existing%2520baselines%252C%2520achieving%2520more%2520accurate%2520and%250Areliable%2520referent%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Depth-Aware%20Method%20For%20Embodied%20Reference%20Understanding&entry.906535625=Fevziye%20Irem%20Eyiokur%20and%20Dogucan%20Yaman%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel&entry.1292438233=%20%20Embodied%20Reference%20Understanding%20requires%20identifying%20a%20target%20object%20in%20a%0Avisual%20scene%20based%20on%20both%20language%20instructions%20and%20pointing%20cues.%20While%20prior%0Aworks%20have%20shown%20progress%20in%20open-vocabulary%20object%20detection%2C%20they%20often%20fail%0Ain%20ambiguous%20scenarios%20where%20multiple%20candidate%20objects%20exist%20in%20the%20scene.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20ERU%20framework%20that%20jointly%0Aleverages%20LLM-based%20data%20augmentation%2C%20depth-map%20modality%2C%20and%20a%20depth-aware%0Adecision%20module.%20This%20design%20enables%20robust%20integration%20of%20linguistic%20and%0Aembodied%20cues%2C%20improving%20disambiguation%20in%20complex%20or%20cluttered%20environments.%0AExperimental%20results%20on%20two%20datasets%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20existing%20baselines%2C%20achieving%20more%20accurate%20and%0Areliable%20referent%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08278v2&entry.124074799=Read"},
{"title": "SMF: Template-free and Rig-free Animation Transfer using Kinetic Codes", "author": "Sanjeev Muralikrishnan and Niladri Shekhar Dutt and Niloy J. Mitra", "abstract": "  Animation retargetting applies sparse motion description (e.g., keypoint\nsequences) to a character mesh to produce a semantically plausible and\ntemporally coherent full-body mesh sequence. Existing approaches come with\nrestrictions -- they require access to template-based shape priors or\nartist-designed deformation rigs, suffer from limited generalization to unseen\nmotion and/or shapes, or exhibit motion jitter. We propose Self-supervised\nMotion Fields (SMF), a self-supervised framework that is trained with only\nsparse motion representations, without requiring dataset-specific annotations,\ntemplates, or rigs. At the heart of our method are Kinetic Codes, a novel\nautoencoder-based sparse motion encoding, that exposes a semantically rich\nlatent space, simplifying large-scale training. Our architecture comprises\ndedicated spatial and temporal gradient predictors, which are jointly trained\nin an end-to-end fashion. The combined network, regularized by the Kinetic\nCodes' latent space, has good generalization across both unseen shapes and new\nmotions. We evaluated our method on unseen motion sampled from AMASS, D4D,\nMixamo, and raw monocular video for animation transfer on various characters\nwith varying shapes and topology. We report a new SoTA on the AMASS dataset in\nthe context of generalization to unseen motion. Code, weights, and\nsupplementary are available on the project webpage at\nhttps://motionfields.github.io/\n", "link": "http://arxiv.org/abs/2504.04831v2", "date": "2025-10-10", "relevancy": 2.4655, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6409}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.611}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMF%3A%20Template-free%20and%20Rig-free%20Animation%20Transfer%20using%20Kinetic%20Codes&body=Title%3A%20SMF%3A%20Template-free%20and%20Rig-free%20Animation%20Transfer%20using%20Kinetic%20Codes%0AAuthor%3A%20Sanjeev%20Muralikrishnan%20and%20Niladri%20Shekhar%20Dutt%20and%20Niloy%20J.%20Mitra%0AAbstract%3A%20%20%20Animation%20retargetting%20applies%20sparse%20motion%20description%20%28e.g.%2C%20keypoint%0Asequences%29%20to%20a%20character%20mesh%20to%20produce%20a%20semantically%20plausible%20and%0Atemporally%20coherent%20full-body%20mesh%20sequence.%20Existing%20approaches%20come%20with%0Arestrictions%20--%20they%20require%20access%20to%20template-based%20shape%20priors%20or%0Aartist-designed%20deformation%20rigs%2C%20suffer%20from%20limited%20generalization%20to%20unseen%0Amotion%20and/or%20shapes%2C%20or%20exhibit%20motion%20jitter.%20We%20propose%20Self-supervised%0AMotion%20Fields%20%28SMF%29%2C%20a%20self-supervised%20framework%20that%20is%20trained%20with%20only%0Asparse%20motion%20representations%2C%20without%20requiring%20dataset-specific%20annotations%2C%0Atemplates%2C%20or%20rigs.%20At%20the%20heart%20of%20our%20method%20are%20Kinetic%20Codes%2C%20a%20novel%0Aautoencoder-based%20sparse%20motion%20encoding%2C%20that%20exposes%20a%20semantically%20rich%0Alatent%20space%2C%20simplifying%20large-scale%20training.%20Our%20architecture%20comprises%0Adedicated%20spatial%20and%20temporal%20gradient%20predictors%2C%20which%20are%20jointly%20trained%0Ain%20an%20end-to-end%20fashion.%20The%20combined%20network%2C%20regularized%20by%20the%20Kinetic%0ACodes%27%20latent%20space%2C%20has%20good%20generalization%20across%20both%20unseen%20shapes%20and%20new%0Amotions.%20We%20evaluated%20our%20method%20on%20unseen%20motion%20sampled%20from%20AMASS%2C%20D4D%2C%0AMixamo%2C%20and%20raw%20monocular%20video%20for%20animation%20transfer%20on%20various%20characters%0Awith%20varying%20shapes%20and%20topology.%20We%20report%20a%20new%20SoTA%20on%20the%20AMASS%20dataset%20in%0Athe%20context%20of%20generalization%20to%20unseen%20motion.%20Code%2C%20weights%2C%20and%0Asupplementary%20are%20available%20on%20the%20project%20webpage%20at%0Ahttps%3A//motionfields.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMF%253A%2520Template-free%2520and%2520Rig-free%2520Animation%2520Transfer%2520using%2520Kinetic%2520Codes%26entry.906535625%3DSanjeev%2520Muralikrishnan%2520and%2520Niladri%2520Shekhar%2520Dutt%2520and%2520Niloy%2520J.%2520Mitra%26entry.1292438233%3D%2520%2520Animation%2520retargetting%2520applies%2520sparse%2520motion%2520description%2520%2528e.g.%252C%2520keypoint%250Asequences%2529%2520to%2520a%2520character%2520mesh%2520to%2520produce%2520a%2520semantically%2520plausible%2520and%250Atemporally%2520coherent%2520full-body%2520mesh%2520sequence.%2520Existing%2520approaches%2520come%2520with%250Arestrictions%2520--%2520they%2520require%2520access%2520to%2520template-based%2520shape%2520priors%2520or%250Aartist-designed%2520deformation%2520rigs%252C%2520suffer%2520from%2520limited%2520generalization%2520to%2520unseen%250Amotion%2520and/or%2520shapes%252C%2520or%2520exhibit%2520motion%2520jitter.%2520We%2520propose%2520Self-supervised%250AMotion%2520Fields%2520%2528SMF%2529%252C%2520a%2520self-supervised%2520framework%2520that%2520is%2520trained%2520with%2520only%250Asparse%2520motion%2520representations%252C%2520without%2520requiring%2520dataset-specific%2520annotations%252C%250Atemplates%252C%2520or%2520rigs.%2520At%2520the%2520heart%2520of%2520our%2520method%2520are%2520Kinetic%2520Codes%252C%2520a%2520novel%250Aautoencoder-based%2520sparse%2520motion%2520encoding%252C%2520that%2520exposes%2520a%2520semantically%2520rich%250Alatent%2520space%252C%2520simplifying%2520large-scale%2520training.%2520Our%2520architecture%2520comprises%250Adedicated%2520spatial%2520and%2520temporal%2520gradient%2520predictors%252C%2520which%2520are%2520jointly%2520trained%250Ain%2520an%2520end-to-end%2520fashion.%2520The%2520combined%2520network%252C%2520regularized%2520by%2520the%2520Kinetic%250ACodes%2527%2520latent%2520space%252C%2520has%2520good%2520generalization%2520across%2520both%2520unseen%2520shapes%2520and%2520new%250Amotions.%2520We%2520evaluated%2520our%2520method%2520on%2520unseen%2520motion%2520sampled%2520from%2520AMASS%252C%2520D4D%252C%250AMixamo%252C%2520and%2520raw%2520monocular%2520video%2520for%2520animation%2520transfer%2520on%2520various%2520characters%250Awith%2520varying%2520shapes%2520and%2520topology.%2520We%2520report%2520a%2520new%2520SoTA%2520on%2520the%2520AMASS%2520dataset%2520in%250Athe%2520context%2520of%2520generalization%2520to%2520unseen%2520motion.%2520Code%252C%2520weights%252C%2520and%250Asupplementary%2520are%2520available%2520on%2520the%2520project%2520webpage%2520at%250Ahttps%253A//motionfields.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMF%3A%20Template-free%20and%20Rig-free%20Animation%20Transfer%20using%20Kinetic%20Codes&entry.906535625=Sanjeev%20Muralikrishnan%20and%20Niladri%20Shekhar%20Dutt%20and%20Niloy%20J.%20Mitra&entry.1292438233=%20%20Animation%20retargetting%20applies%20sparse%20motion%20description%20%28e.g.%2C%20keypoint%0Asequences%29%20to%20a%20character%20mesh%20to%20produce%20a%20semantically%20plausible%20and%0Atemporally%20coherent%20full-body%20mesh%20sequence.%20Existing%20approaches%20come%20with%0Arestrictions%20--%20they%20require%20access%20to%20template-based%20shape%20priors%20or%0Aartist-designed%20deformation%20rigs%2C%20suffer%20from%20limited%20generalization%20to%20unseen%0Amotion%20and/or%20shapes%2C%20or%20exhibit%20motion%20jitter.%20We%20propose%20Self-supervised%0AMotion%20Fields%20%28SMF%29%2C%20a%20self-supervised%20framework%20that%20is%20trained%20with%20only%0Asparse%20motion%20representations%2C%20without%20requiring%20dataset-specific%20annotations%2C%0Atemplates%2C%20or%20rigs.%20At%20the%20heart%20of%20our%20method%20are%20Kinetic%20Codes%2C%20a%20novel%0Aautoencoder-based%20sparse%20motion%20encoding%2C%20that%20exposes%20a%20semantically%20rich%0Alatent%20space%2C%20simplifying%20large-scale%20training.%20Our%20architecture%20comprises%0Adedicated%20spatial%20and%20temporal%20gradient%20predictors%2C%20which%20are%20jointly%20trained%0Ain%20an%20end-to-end%20fashion.%20The%20combined%20network%2C%20regularized%20by%20the%20Kinetic%0ACodes%27%20latent%20space%2C%20has%20good%20generalization%20across%20both%20unseen%20shapes%20and%20new%0Amotions.%20We%20evaluated%20our%20method%20on%20unseen%20motion%20sampled%20from%20AMASS%2C%20D4D%2C%0AMixamo%2C%20and%20raw%20monocular%20video%20for%20animation%20transfer%20on%20various%20characters%0Awith%20varying%20shapes%20and%20topology.%20We%20report%20a%20new%20SoTA%20on%20the%20AMASS%20dataset%20in%0Athe%20context%20of%20generalization%20to%20unseen%20motion.%20Code%2C%20weights%2C%20and%0Asupplementary%20are%20available%20on%20the%20project%20webpage%20at%0Ahttps%3A//motionfields.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04831v2&entry.124074799=Read"},
{"title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance\n  for Self-supervised Monocular Depth Estimation", "author": "Wenyao Zhang and Hongsi Liu and Bohan Li and Jiawei He and Zekun Qi and Yunnan Wang and Shengyang Zhao and Xinqiang Yu and Wenjun Zeng and Xin Jin", "abstract": "  Current self-supervised monocular depth estimation (MDE) approaches encounter\nperformance limitations due to insufficient semantic-spatial knowledge\nextraction. To address this challenge, we propose Hybrid-depth, a novel\nframework that systematically integrates foundation models (e.g., CLIP and\nDINO) to extract visual priors and acquire sufficient contextual information\nfor MDE. Our approach introduces a coarse-to-fine progressive learning\nframework: 1) Firstly, we aggregate multi-grained features from CLIP (global\nsemantics) and DINO (local spatial details) under contrastive language\nguidance. A proxy task comparing close-distant image patches is designed to\nenforce depth-aware feature alignment using text prompts; 2) Next, building on\nthe coarse features, we integrate camera pose information and pixel-wise\nlanguage alignment to refine depth predictions. This module seamlessly\nintegrates with existing self-supervised MDE pipelines (e.g., Monodepth2,\nManyDepth) as a plug-and-play depth encoder, enhancing continuous depth\nestimation. By aggregating CLIP's semantic context and DINO's spatial details\nthrough language guidance, our method effectively addresses feature granularity\nmismatches. Extensive experiments on the KITTI benchmark demonstrate that our\nmethod significantly outperforms SOTA methods across all metrics, which also\nindeed benefits downstream tasks like BEV perception. Code is available at\nhttps://github.com/Zhangwenyao1/Hybrid-depth.\n", "link": "http://arxiv.org/abs/2510.09320v1", "date": "2025-10-10", "relevancy": 2.4213, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6179}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.609}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid-grained%20Feature%20Aggregation%20with%20Coarse-to-fine%20Language%20Guidance%0A%20%20for%20Self-supervised%20Monocular%20Depth%20Estimation&body=Title%3A%20Hybrid-grained%20Feature%20Aggregation%20with%20Coarse-to-fine%20Language%20Guidance%0A%20%20for%20Self-supervised%20Monocular%20Depth%20Estimation%0AAuthor%3A%20Wenyao%20Zhang%20and%20Hongsi%20Liu%20and%20Bohan%20Li%20and%20Jiawei%20He%20and%20Zekun%20Qi%20and%20Yunnan%20Wang%20and%20Shengyang%20Zhao%20and%20Xinqiang%20Yu%20and%20Wenjun%20Zeng%20and%20Xin%20Jin%0AAbstract%3A%20%20%20Current%20self-supervised%20monocular%20depth%20estimation%20%28MDE%29%20approaches%20encounter%0Aperformance%20limitations%20due%20to%20insufficient%20semantic-spatial%20knowledge%0Aextraction.%20To%20address%20this%20challenge%2C%20we%20propose%20Hybrid-depth%2C%20a%20novel%0Aframework%20that%20systematically%20integrates%20foundation%20models%20%28e.g.%2C%20CLIP%20and%0ADINO%29%20to%20extract%20visual%20priors%20and%20acquire%20sufficient%20contextual%20information%0Afor%20MDE.%20Our%20approach%20introduces%20a%20coarse-to-fine%20progressive%20learning%0Aframework%3A%201%29%20Firstly%2C%20we%20aggregate%20multi-grained%20features%20from%20CLIP%20%28global%0Asemantics%29%20and%20DINO%20%28local%20spatial%20details%29%20under%20contrastive%20language%0Aguidance.%20A%20proxy%20task%20comparing%20close-distant%20image%20patches%20is%20designed%20to%0Aenforce%20depth-aware%20feature%20alignment%20using%20text%20prompts%3B%202%29%20Next%2C%20building%20on%0Athe%20coarse%20features%2C%20we%20integrate%20camera%20pose%20information%20and%20pixel-wise%0Alanguage%20alignment%20to%20refine%20depth%20predictions.%20This%20module%20seamlessly%0Aintegrates%20with%20existing%20self-supervised%20MDE%20pipelines%20%28e.g.%2C%20Monodepth2%2C%0AManyDepth%29%20as%20a%20plug-and-play%20depth%20encoder%2C%20enhancing%20continuous%20depth%0Aestimation.%20By%20aggregating%20CLIP%27s%20semantic%20context%20and%20DINO%27s%20spatial%20details%0Athrough%20language%20guidance%2C%20our%20method%20effectively%20addresses%20feature%20granularity%0Amismatches.%20Extensive%20experiments%20on%20the%20KITTI%20benchmark%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20SOTA%20methods%20across%20all%20metrics%2C%20which%20also%0Aindeed%20benefits%20downstream%20tasks%20like%20BEV%20perception.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Zhangwenyao1/Hybrid-depth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid-grained%2520Feature%2520Aggregation%2520with%2520Coarse-to-fine%2520Language%2520Guidance%250A%2520%2520for%2520Self-supervised%2520Monocular%2520Depth%2520Estimation%26entry.906535625%3DWenyao%2520Zhang%2520and%2520Hongsi%2520Liu%2520and%2520Bohan%2520Li%2520and%2520Jiawei%2520He%2520and%2520Zekun%2520Qi%2520and%2520Yunnan%2520Wang%2520and%2520Shengyang%2520Zhao%2520and%2520Xinqiang%2520Yu%2520and%2520Wenjun%2520Zeng%2520and%2520Xin%2520Jin%26entry.1292438233%3D%2520%2520Current%2520self-supervised%2520monocular%2520depth%2520estimation%2520%2528MDE%2529%2520approaches%2520encounter%250Aperformance%2520limitations%2520due%2520to%2520insufficient%2520semantic-spatial%2520knowledge%250Aextraction.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Hybrid-depth%252C%2520a%2520novel%250Aframework%2520that%2520systematically%2520integrates%2520foundation%2520models%2520%2528e.g.%252C%2520CLIP%2520and%250ADINO%2529%2520to%2520extract%2520visual%2520priors%2520and%2520acquire%2520sufficient%2520contextual%2520information%250Afor%2520MDE.%2520Our%2520approach%2520introduces%2520a%2520coarse-to-fine%2520progressive%2520learning%250Aframework%253A%25201%2529%2520Firstly%252C%2520we%2520aggregate%2520multi-grained%2520features%2520from%2520CLIP%2520%2528global%250Asemantics%2529%2520and%2520DINO%2520%2528local%2520spatial%2520details%2529%2520under%2520contrastive%2520language%250Aguidance.%2520A%2520proxy%2520task%2520comparing%2520close-distant%2520image%2520patches%2520is%2520designed%2520to%250Aenforce%2520depth-aware%2520feature%2520alignment%2520using%2520text%2520prompts%253B%25202%2529%2520Next%252C%2520building%2520on%250Athe%2520coarse%2520features%252C%2520we%2520integrate%2520camera%2520pose%2520information%2520and%2520pixel-wise%250Alanguage%2520alignment%2520to%2520refine%2520depth%2520predictions.%2520This%2520module%2520seamlessly%250Aintegrates%2520with%2520existing%2520self-supervised%2520MDE%2520pipelines%2520%2528e.g.%252C%2520Monodepth2%252C%250AManyDepth%2529%2520as%2520a%2520plug-and-play%2520depth%2520encoder%252C%2520enhancing%2520continuous%2520depth%250Aestimation.%2520By%2520aggregating%2520CLIP%2527s%2520semantic%2520context%2520and%2520DINO%2527s%2520spatial%2520details%250Athrough%2520language%2520guidance%252C%2520our%2520method%2520effectively%2520addresses%2520feature%2520granularity%250Amismatches.%2520Extensive%2520experiments%2520on%2520the%2520KITTI%2520benchmark%2520demonstrate%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520SOTA%2520methods%2520across%2520all%2520metrics%252C%2520which%2520also%250Aindeed%2520benefits%2520downstream%2520tasks%2520like%2520BEV%2520perception.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Zhangwenyao1/Hybrid-depth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid-grained%20Feature%20Aggregation%20with%20Coarse-to-fine%20Language%20Guidance%0A%20%20for%20Self-supervised%20Monocular%20Depth%20Estimation&entry.906535625=Wenyao%20Zhang%20and%20Hongsi%20Liu%20and%20Bohan%20Li%20and%20Jiawei%20He%20and%20Zekun%20Qi%20and%20Yunnan%20Wang%20and%20Shengyang%20Zhao%20and%20Xinqiang%20Yu%20and%20Wenjun%20Zeng%20and%20Xin%20Jin&entry.1292438233=%20%20Current%20self-supervised%20monocular%20depth%20estimation%20%28MDE%29%20approaches%20encounter%0Aperformance%20limitations%20due%20to%20insufficient%20semantic-spatial%20knowledge%0Aextraction.%20To%20address%20this%20challenge%2C%20we%20propose%20Hybrid-depth%2C%20a%20novel%0Aframework%20that%20systematically%20integrates%20foundation%20models%20%28e.g.%2C%20CLIP%20and%0ADINO%29%20to%20extract%20visual%20priors%20and%20acquire%20sufficient%20contextual%20information%0Afor%20MDE.%20Our%20approach%20introduces%20a%20coarse-to-fine%20progressive%20learning%0Aframework%3A%201%29%20Firstly%2C%20we%20aggregate%20multi-grained%20features%20from%20CLIP%20%28global%0Asemantics%29%20and%20DINO%20%28local%20spatial%20details%29%20under%20contrastive%20language%0Aguidance.%20A%20proxy%20task%20comparing%20close-distant%20image%20patches%20is%20designed%20to%0Aenforce%20depth-aware%20feature%20alignment%20using%20text%20prompts%3B%202%29%20Next%2C%20building%20on%0Athe%20coarse%20features%2C%20we%20integrate%20camera%20pose%20information%20and%20pixel-wise%0Alanguage%20alignment%20to%20refine%20depth%20predictions.%20This%20module%20seamlessly%0Aintegrates%20with%20existing%20self-supervised%20MDE%20pipelines%20%28e.g.%2C%20Monodepth2%2C%0AManyDepth%29%20as%20a%20plug-and-play%20depth%20encoder%2C%20enhancing%20continuous%20depth%0Aestimation.%20By%20aggregating%20CLIP%27s%20semantic%20context%20and%20DINO%27s%20spatial%20details%0Athrough%20language%20guidance%2C%20our%20method%20effectively%20addresses%20feature%20granularity%0Amismatches.%20Extensive%20experiments%20on%20the%20KITTI%20benchmark%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20SOTA%20methods%20across%20all%20metrics%2C%20which%20also%0Aindeed%20benefits%20downstream%20tasks%20like%20BEV%20perception.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Zhangwenyao1/Hybrid-depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09320v1&entry.124074799=Read"},
{"title": "K-ASTRO: Structure-Aware Adaptation of LLMs for Code Vulnerability\n  Detection", "author": "Yifan Zhang and Michael Sandborn and Stefan Larson and Yu Huang and Kevin Leach", "abstract": "  Large Language Models (LLMs) are transforming software engineering tasks,\nincluding code vulnerability detection-a critical area of software security.\nHowever, existing methods often rely on resource-intensive models or\ngraph-based techniques, limiting their accessibility and practicality. This\npaper introduces K-ASTRO, a lightweight Transformer model that combines\nsemantic embeddings from LLMs with structural features of Abstract Syntax Trees\n(ASTs) to improve both efficiency and accuracy in code vulnerability detection.\nOur approach introduces an AST-based augmentation technique inspired by\nmutation testing, a structure-aware attention mechanism that incorporates\naugmented AST features, and a joint adaptation pipeline to unify code semantics\nand syntax. Experimental results on three large-scale datasets, including\nBigVul, DiverseVul, and PrimeVul-demonstrate state-of-the-art performance while\nenabling rapid inference on CPUs with minimal training time. By offering a\nscalable, interpretable, and efficient solution, K-ASTRO bridges the gap\nbetween LLM advancements and practical software vulnerability detection,\nproviding open-sourced tools to foster further research.\n", "link": "http://arxiv.org/abs/2208.08067v2", "date": "2025-10-10", "relevancy": 2.4211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20K-ASTRO%3A%20Structure-Aware%20Adaptation%20of%20LLMs%20for%20Code%20Vulnerability%0A%20%20Detection&body=Title%3A%20K-ASTRO%3A%20Structure-Aware%20Adaptation%20of%20LLMs%20for%20Code%20Vulnerability%0A%20%20Detection%0AAuthor%3A%20Yifan%20Zhang%20and%20Michael%20Sandborn%20and%20Stefan%20Larson%20and%20Yu%20Huang%20and%20Kevin%20Leach%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20transforming%20software%20engineering%20tasks%2C%0Aincluding%20code%20vulnerability%20detection-a%20critical%20area%20of%20software%20security.%0AHowever%2C%20existing%20methods%20often%20rely%20on%20resource-intensive%20models%20or%0Agraph-based%20techniques%2C%20limiting%20their%20accessibility%20and%20practicality.%20This%0Apaper%20introduces%20K-ASTRO%2C%20a%20lightweight%20Transformer%20model%20that%20combines%0Asemantic%20embeddings%20from%20LLMs%20with%20structural%20features%20of%20Abstract%20Syntax%20Trees%0A%28ASTs%29%20to%20improve%20both%20efficiency%20and%20accuracy%20in%20code%20vulnerability%20detection.%0AOur%20approach%20introduces%20an%20AST-based%20augmentation%20technique%20inspired%20by%0Amutation%20testing%2C%20a%20structure-aware%20attention%20mechanism%20that%20incorporates%0Aaugmented%20AST%20features%2C%20and%20a%20joint%20adaptation%20pipeline%20to%20unify%20code%20semantics%0Aand%20syntax.%20Experimental%20results%20on%20three%20large-scale%20datasets%2C%20including%0ABigVul%2C%20DiverseVul%2C%20and%20PrimeVul-demonstrate%20state-of-the-art%20performance%20while%0Aenabling%20rapid%20inference%20on%20CPUs%20with%20minimal%20training%20time.%20By%20offering%20a%0Ascalable%2C%20interpretable%2C%20and%20efficient%20solution%2C%20K-ASTRO%20bridges%20the%20gap%0Abetween%20LLM%20advancements%20and%20practical%20software%20vulnerability%20detection%2C%0Aproviding%20open-sourced%20tools%20to%20foster%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.08067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DK-ASTRO%253A%2520Structure-Aware%2520Adaptation%2520of%2520LLMs%2520for%2520Code%2520Vulnerability%250A%2520%2520Detection%26entry.906535625%3DYifan%2520Zhang%2520and%2520Michael%2520Sandborn%2520and%2520Stefan%2520Larson%2520and%2520Yu%2520Huang%2520and%2520Kevin%2520Leach%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520transforming%2520software%2520engineering%2520tasks%252C%250Aincluding%2520code%2520vulnerability%2520detection-a%2520critical%2520area%2520of%2520software%2520security.%250AHowever%252C%2520existing%2520methods%2520often%2520rely%2520on%2520resource-intensive%2520models%2520or%250Agraph-based%2520techniques%252C%2520limiting%2520their%2520accessibility%2520and%2520practicality.%2520This%250Apaper%2520introduces%2520K-ASTRO%252C%2520a%2520lightweight%2520Transformer%2520model%2520that%2520combines%250Asemantic%2520embeddings%2520from%2520LLMs%2520with%2520structural%2520features%2520of%2520Abstract%2520Syntax%2520Trees%250A%2528ASTs%2529%2520to%2520improve%2520both%2520efficiency%2520and%2520accuracy%2520in%2520code%2520vulnerability%2520detection.%250AOur%2520approach%2520introduces%2520an%2520AST-based%2520augmentation%2520technique%2520inspired%2520by%250Amutation%2520testing%252C%2520a%2520structure-aware%2520attention%2520mechanism%2520that%2520incorporates%250Aaugmented%2520AST%2520features%252C%2520and%2520a%2520joint%2520adaptation%2520pipeline%2520to%2520unify%2520code%2520semantics%250Aand%2520syntax.%2520Experimental%2520results%2520on%2520three%2520large-scale%2520datasets%252C%2520including%250ABigVul%252C%2520DiverseVul%252C%2520and%2520PrimeVul-demonstrate%2520state-of-the-art%2520performance%2520while%250Aenabling%2520rapid%2520inference%2520on%2520CPUs%2520with%2520minimal%2520training%2520time.%2520By%2520offering%2520a%250Ascalable%252C%2520interpretable%252C%2520and%2520efficient%2520solution%252C%2520K-ASTRO%2520bridges%2520the%2520gap%250Abetween%2520LLM%2520advancements%2520and%2520practical%2520software%2520vulnerability%2520detection%252C%250Aproviding%2520open-sourced%2520tools%2520to%2520foster%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.08067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=K-ASTRO%3A%20Structure-Aware%20Adaptation%20of%20LLMs%20for%20Code%20Vulnerability%0A%20%20Detection&entry.906535625=Yifan%20Zhang%20and%20Michael%20Sandborn%20and%20Stefan%20Larson%20and%20Yu%20Huang%20and%20Kevin%20Leach&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20transforming%20software%20engineering%20tasks%2C%0Aincluding%20code%20vulnerability%20detection-a%20critical%20area%20of%20software%20security.%0AHowever%2C%20existing%20methods%20often%20rely%20on%20resource-intensive%20models%20or%0Agraph-based%20techniques%2C%20limiting%20their%20accessibility%20and%20practicality.%20This%0Apaper%20introduces%20K-ASTRO%2C%20a%20lightweight%20Transformer%20model%20that%20combines%0Asemantic%20embeddings%20from%20LLMs%20with%20structural%20features%20of%20Abstract%20Syntax%20Trees%0A%28ASTs%29%20to%20improve%20both%20efficiency%20and%20accuracy%20in%20code%20vulnerability%20detection.%0AOur%20approach%20introduces%20an%20AST-based%20augmentation%20technique%20inspired%20by%0Amutation%20testing%2C%20a%20structure-aware%20attention%20mechanism%20that%20incorporates%0Aaugmented%20AST%20features%2C%20and%20a%20joint%20adaptation%20pipeline%20to%20unify%20code%20semantics%0Aand%20syntax.%20Experimental%20results%20on%20three%20large-scale%20datasets%2C%20including%0ABigVul%2C%20DiverseVul%2C%20and%20PrimeVul-demonstrate%20state-of-the-art%20performance%20while%0Aenabling%20rapid%20inference%20on%20CPUs%20with%20minimal%20training%20time.%20By%20offering%20a%0Ascalable%2C%20interpretable%2C%20and%20efficient%20solution%2C%20K-ASTRO%20bridges%20the%20gap%0Abetween%20LLM%20advancements%20and%20practical%20software%20vulnerability%20detection%2C%0Aproviding%20open-sourced%20tools%20to%20foster%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.08067v2&entry.124074799=Read"},
{"title": "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue\n  State Tracking Approach", "author": "Nizar El Ghazal and Antoine Caubri\u00e8re and Valentin Vielzeuf", "abstract": "  This paper presents a comparative study of context management strategies for\nend-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically\nevaluate traditional multimodal context (combining text history and spoken\ncurrent turn), full spoken history, and compressed spoken history approaches.\nOur experiments on the SpokenWOZ corpus demonstrate that providing the full\nspoken conversation as input yields the highest performance among models of\nsimilar size, significantly surpassing prior methods. Furthermore, we show that\nattention-pooling-based compression of the spoken history offers a strong\ntrade-off, maintaining competitive accuracy with reduced context size. Detailed\nanalysis confirms that improvements stem from more effective context\nutilization.\n", "link": "http://arxiv.org/abs/2510.09424v1", "date": "2025-10-10", "relevancy": 2.4158, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Speech-LLM%20Takes%20It%20All%3A%20A%20Truly%20Fully%20End-to-End%20Spoken%20Dialogue%0A%20%20State%20Tracking%20Approach&body=Title%3A%20The%20Speech-LLM%20Takes%20It%20All%3A%20A%20Truly%20Fully%20End-to-End%20Spoken%20Dialogue%0A%20%20State%20Tracking%20Approach%0AAuthor%3A%20Nizar%20El%20Ghazal%20and%20Antoine%20Caubri%C3%A8re%20and%20Valentin%20Vielzeuf%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comparative%20study%20of%20context%20management%20strategies%20for%0Aend-to-end%20Spoken%20Dialog%20State%20Tracking%20using%20Speech-LLMs.%20We%20systematically%0Aevaluate%20traditional%20multimodal%20context%20%28combining%20text%20history%20and%20spoken%0Acurrent%20turn%29%2C%20full%20spoken%20history%2C%20and%20compressed%20spoken%20history%20approaches.%0AOur%20experiments%20on%20the%20SpokenWOZ%20corpus%20demonstrate%20that%20providing%20the%20full%0Aspoken%20conversation%20as%20input%20yields%20the%20highest%20performance%20among%20models%20of%0Asimilar%20size%2C%20significantly%20surpassing%20prior%20methods.%20Furthermore%2C%20we%20show%20that%0Aattention-pooling-based%20compression%20of%20the%20spoken%20history%20offers%20a%20strong%0Atrade-off%2C%20maintaining%20competitive%20accuracy%20with%20reduced%20context%20size.%20Detailed%0Aanalysis%20confirms%20that%20improvements%20stem%20from%20more%20effective%20context%0Autilization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Speech-LLM%2520Takes%2520It%2520All%253A%2520A%2520Truly%2520Fully%2520End-to-End%2520Spoken%2520Dialogue%250A%2520%2520State%2520Tracking%2520Approach%26entry.906535625%3DNizar%2520El%2520Ghazal%2520and%2520Antoine%2520Caubri%25C3%25A8re%2520and%2520Valentin%2520Vielzeuf%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comparative%2520study%2520of%2520context%2520management%2520strategies%2520for%250Aend-to-end%2520Spoken%2520Dialog%2520State%2520Tracking%2520using%2520Speech-LLMs.%2520We%2520systematically%250Aevaluate%2520traditional%2520multimodal%2520context%2520%2528combining%2520text%2520history%2520and%2520spoken%250Acurrent%2520turn%2529%252C%2520full%2520spoken%2520history%252C%2520and%2520compressed%2520spoken%2520history%2520approaches.%250AOur%2520experiments%2520on%2520the%2520SpokenWOZ%2520corpus%2520demonstrate%2520that%2520providing%2520the%2520full%250Aspoken%2520conversation%2520as%2520input%2520yields%2520the%2520highest%2520performance%2520among%2520models%2520of%250Asimilar%2520size%252C%2520significantly%2520surpassing%2520prior%2520methods.%2520Furthermore%252C%2520we%2520show%2520that%250Aattention-pooling-based%2520compression%2520of%2520the%2520spoken%2520history%2520offers%2520a%2520strong%250Atrade-off%252C%2520maintaining%2520competitive%2520accuracy%2520with%2520reduced%2520context%2520size.%2520Detailed%250Aanalysis%2520confirms%2520that%2520improvements%2520stem%2520from%2520more%2520effective%2520context%250Autilization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Speech-LLM%20Takes%20It%20All%3A%20A%20Truly%20Fully%20End-to-End%20Spoken%20Dialogue%0A%20%20State%20Tracking%20Approach&entry.906535625=Nizar%20El%20Ghazal%20and%20Antoine%20Caubri%C3%A8re%20and%20Valentin%20Vielzeuf&entry.1292438233=%20%20This%20paper%20presents%20a%20comparative%20study%20of%20context%20management%20strategies%20for%0Aend-to-end%20Spoken%20Dialog%20State%20Tracking%20using%20Speech-LLMs.%20We%20systematically%0Aevaluate%20traditional%20multimodal%20context%20%28combining%20text%20history%20and%20spoken%0Acurrent%20turn%29%2C%20full%20spoken%20history%2C%20and%20compressed%20spoken%20history%20approaches.%0AOur%20experiments%20on%20the%20SpokenWOZ%20corpus%20demonstrate%20that%20providing%20the%20full%0Aspoken%20conversation%20as%20input%20yields%20the%20highest%20performance%20among%20models%20of%0Asimilar%20size%2C%20significantly%20surpassing%20prior%20methods.%20Furthermore%2C%20we%20show%20that%0Aattention-pooling-based%20compression%20of%20the%20spoken%20history%20offers%20a%20strong%0Atrade-off%2C%20maintaining%20competitive%20accuracy%20with%20reduced%20context%20size.%20Detailed%0Aanalysis%20confirms%20that%20improvements%20stem%20from%20more%20effective%20context%0Autilization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09424v1&entry.124074799=Read"},
{"title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment", "author": "Elena Camuffo and Francesco Barbato and Mete Ozay and Simone Milani and Umberto Michieli", "abstract": "  We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.\n", "link": "http://arxiv.org/abs/2509.14001v3", "date": "2025-10-10", "relevancy": 2.415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6447}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment&body=Title%3A%20MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%0AAuthor%3A%20Elena%20Camuffo%20and%20Francesco%20Barbato%20and%20Mete%20Ozay%20and%20Simone%20Milani%20and%20Umberto%20Michieli%0AAbstract%3A%20%20%20We%20introduce%20MOCHA%20%28Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%29%2C%0Aa%20knowledge%20distillation%20approach%20that%20transfers%20region-level%20multimodal%0Asemantics%20from%20a%20large%20vision-language%20teacher%20%28e.g.%2C%20LLaVa%29%20into%20a%20lightweight%0Avision-only%20object%20detector%20student%20%28e.g.%2C%20YOLO%29.%20A%20translation%20module%20maps%0Astudent%20features%20into%20a%20joint%20space%2C%20where%20the%20training%20of%20the%20student%20and%0Atranslator%20is%20guided%20by%20a%20dual-objective%20loss%20that%20enforces%20both%20local%0Aalignment%20and%20global%20relational%20consistency.%20Unlike%20prior%20approaches%20focused%20on%0Adense%20or%20global%20alignment%2C%20MOCHA%20operates%20at%20the%20object%20level%2C%20enabling%0Aefficient%20transfer%20of%20semantics%20without%20modifying%20the%20teacher%20or%20requiring%0Atextual%20input%20at%20inference.%20We%20validate%20our%20method%20across%20four%20personalized%0Adetection%20benchmarks%20under%20few-shot%20regimes.%20Results%20show%20consistent%20gains%20over%0Abaselines%2C%20with%20a%20%2B10.1%20average%20score%20improvement.%20Despite%20its%20compact%0Aarchitecture%2C%20MOCHA%20reaches%20performance%20on%20par%20with%20larger%20multimodal%20models%2C%0Aproving%20its%20suitability%20for%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14001v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOCHA%253A%2520Multi-modal%2520Objects-aware%2520Cross-arcHitecture%2520Alignment%26entry.906535625%3DElena%2520Camuffo%2520and%2520Francesco%2520Barbato%2520and%2520Mete%2520Ozay%2520and%2520Simone%2520Milani%2520and%2520Umberto%2520Michieli%26entry.1292438233%3D%2520%2520We%2520introduce%2520MOCHA%2520%2528Multi-modal%2520Objects-aware%2520Cross-arcHitecture%2520Alignment%2529%252C%250Aa%2520knowledge%2520distillation%2520approach%2520that%2520transfers%2520region-level%2520multimodal%250Asemantics%2520from%2520a%2520large%2520vision-language%2520teacher%2520%2528e.g.%252C%2520LLaVa%2529%2520into%2520a%2520lightweight%250Avision-only%2520object%2520detector%2520student%2520%2528e.g.%252C%2520YOLO%2529.%2520A%2520translation%2520module%2520maps%250Astudent%2520features%2520into%2520a%2520joint%2520space%252C%2520where%2520the%2520training%2520of%2520the%2520student%2520and%250Atranslator%2520is%2520guided%2520by%2520a%2520dual-objective%2520loss%2520that%2520enforces%2520both%2520local%250Aalignment%2520and%2520global%2520relational%2520consistency.%2520Unlike%2520prior%2520approaches%2520focused%2520on%250Adense%2520or%2520global%2520alignment%252C%2520MOCHA%2520operates%2520at%2520the%2520object%2520level%252C%2520enabling%250Aefficient%2520transfer%2520of%2520semantics%2520without%2520modifying%2520the%2520teacher%2520or%2520requiring%250Atextual%2520input%2520at%2520inference.%2520We%2520validate%2520our%2520method%2520across%2520four%2520personalized%250Adetection%2520benchmarks%2520under%2520few-shot%2520regimes.%2520Results%2520show%2520consistent%2520gains%2520over%250Abaselines%252C%2520with%2520a%2520%252B10.1%2520average%2520score%2520improvement.%2520Despite%2520its%2520compact%250Aarchitecture%252C%2520MOCHA%2520reaches%2520performance%2520on%2520par%2520with%2520larger%2520multimodal%2520models%252C%250Aproving%2520its%2520suitability%2520for%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14001v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment&entry.906535625=Elena%20Camuffo%20and%20Francesco%20Barbato%20and%20Mete%20Ozay%20and%20Simone%20Milani%20and%20Umberto%20Michieli&entry.1292438233=%20%20We%20introduce%20MOCHA%20%28Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%29%2C%0Aa%20knowledge%20distillation%20approach%20that%20transfers%20region-level%20multimodal%0Asemantics%20from%20a%20large%20vision-language%20teacher%20%28e.g.%2C%20LLaVa%29%20into%20a%20lightweight%0Avision-only%20object%20detector%20student%20%28e.g.%2C%20YOLO%29.%20A%20translation%20module%20maps%0Astudent%20features%20into%20a%20joint%20space%2C%20where%20the%20training%20of%20the%20student%20and%0Atranslator%20is%20guided%20by%20a%20dual-objective%20loss%20that%20enforces%20both%20local%0Aalignment%20and%20global%20relational%20consistency.%20Unlike%20prior%20approaches%20focused%20on%0Adense%20or%20global%20alignment%2C%20MOCHA%20operates%20at%20the%20object%20level%2C%20enabling%0Aefficient%20transfer%20of%20semantics%20without%20modifying%20the%20teacher%20or%20requiring%0Atextual%20input%20at%20inference.%20We%20validate%20our%20method%20across%20four%20personalized%0Adetection%20benchmarks%20under%20few-shot%20regimes.%20Results%20show%20consistent%20gains%20over%0Abaselines%2C%20with%20a%20%2B10.1%20average%20score%20improvement.%20Despite%20its%20compact%0Aarchitecture%2C%20MOCHA%20reaches%20performance%20on%20par%20with%20larger%20multimodal%20models%2C%0Aproving%20its%20suitability%20for%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14001v3&entry.124074799=Read"},
{"title": "ClustRecNet: A Novel End-to-End Deep Learning Framework for Clustering\n  Algorithm Recommendation", "author": "Mohammadreza Bakhtyari and Bogdan Mazoure and Renato Cordeiro de Amorim and Guillaume Rabusseau and Vladimir Makarenkov", "abstract": "  We introduce ClustRecNet - a novel deep learning (DL)-based recommendation\nframework for determining the most suitable clustering algorithms for a given\ndataset, addressing the long-standing challenge of clustering algorithm\nselection in unsupervised learning. To enable supervised learning in this\ncontext, we construct a comprehensive data repository comprising 34,000\nsynthetic datasets with diverse structural properties. Each of them was\nprocessed using 10 popular clustering algorithms. The resulting clusterings\nwere assessed via the Adjusted Rand Index (ARI) to establish ground truth\nlabels, used for training and evaluation of our DL model. The proposed network\narchitecture integrates convolutional, residual, and attention mechanisms to\ncapture both local and global structural patterns from the input data. This\ndesign supports end-to-end training to learn compact representations of\ndatasets and enables direct recommendation of the most suitable clustering\nalgorithm, reducing reliance on handcrafted meta-features and traditional\nCluster Validity Indices (CVIs). Comprehensive experiments across synthetic and\nreal-world benchmarks demonstrate that our DL model consistently outperforms\nconventional CVIs (e.g. Silhouette, Calinski-Harabasz, Davies-Bouldin, and\nDunn) as well as state-of-the-art AutoML clustering recommendation approaches\n(e.g. ML2DAC, AutoCluster, and AutoML4Clust). Notably, the proposed model\nachieves a 0.497 ARI improvement over the Calinski-Harabasz index on synthetic\ndata and a 15.3% ARI gain over the best-performing AutoML approach on\nreal-world data.\n", "link": "http://arxiv.org/abs/2509.25289v2", "date": "2025-10-10", "relevancy": 2.4144, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5061}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4755}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClustRecNet%3A%20A%20Novel%20End-to-End%20Deep%20Learning%20Framework%20for%20Clustering%0A%20%20Algorithm%20Recommendation&body=Title%3A%20ClustRecNet%3A%20A%20Novel%20End-to-End%20Deep%20Learning%20Framework%20for%20Clustering%0A%20%20Algorithm%20Recommendation%0AAuthor%3A%20Mohammadreza%20Bakhtyari%20and%20Bogdan%20Mazoure%20and%20Renato%20Cordeiro%20de%20Amorim%20and%20Guillaume%20Rabusseau%20and%20Vladimir%20Makarenkov%0AAbstract%3A%20%20%20We%20introduce%20ClustRecNet%20-%20a%20novel%20deep%20learning%20%28DL%29-based%20recommendation%0Aframework%20for%20determining%20the%20most%20suitable%20clustering%20algorithms%20for%20a%20given%0Adataset%2C%20addressing%20the%20long-standing%20challenge%20of%20clustering%20algorithm%0Aselection%20in%20unsupervised%20learning.%20To%20enable%20supervised%20learning%20in%20this%0Acontext%2C%20we%20construct%20a%20comprehensive%20data%20repository%20comprising%2034%2C000%0Asynthetic%20datasets%20with%20diverse%20structural%20properties.%20Each%20of%20them%20was%0Aprocessed%20using%2010%20popular%20clustering%20algorithms.%20The%20resulting%20clusterings%0Awere%20assessed%20via%20the%20Adjusted%20Rand%20Index%20%28ARI%29%20to%20establish%20ground%20truth%0Alabels%2C%20used%20for%20training%20and%20evaluation%20of%20our%20DL%20model.%20The%20proposed%20network%0Aarchitecture%20integrates%20convolutional%2C%20residual%2C%20and%20attention%20mechanisms%20to%0Acapture%20both%20local%20and%20global%20structural%20patterns%20from%20the%20input%20data.%20This%0Adesign%20supports%20end-to-end%20training%20to%20learn%20compact%20representations%20of%0Adatasets%20and%20enables%20direct%20recommendation%20of%20the%20most%20suitable%20clustering%0Aalgorithm%2C%20reducing%20reliance%20on%20handcrafted%20meta-features%20and%20traditional%0ACluster%20Validity%20Indices%20%28CVIs%29.%20Comprehensive%20experiments%20across%20synthetic%20and%0Areal-world%20benchmarks%20demonstrate%20that%20our%20DL%20model%20consistently%20outperforms%0Aconventional%20CVIs%20%28e.g.%20Silhouette%2C%20Calinski-Harabasz%2C%20Davies-Bouldin%2C%20and%0ADunn%29%20as%20well%20as%20state-of-the-art%20AutoML%20clustering%20recommendation%20approaches%0A%28e.g.%20ML2DAC%2C%20AutoCluster%2C%20and%20AutoML4Clust%29.%20Notably%2C%20the%20proposed%20model%0Aachieves%20a%200.497%20ARI%20improvement%20over%20the%20Calinski-Harabasz%20index%20on%20synthetic%0Adata%20and%20a%2015.3%25%20ARI%20gain%20over%20the%20best-performing%20AutoML%20approach%20on%0Areal-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustRecNet%253A%2520A%2520Novel%2520End-to-End%2520Deep%2520Learning%2520Framework%2520for%2520Clustering%250A%2520%2520Algorithm%2520Recommendation%26entry.906535625%3DMohammadreza%2520Bakhtyari%2520and%2520Bogdan%2520Mazoure%2520and%2520Renato%2520Cordeiro%2520de%2520Amorim%2520and%2520Guillaume%2520Rabusseau%2520and%2520Vladimir%2520Makarenkov%26entry.1292438233%3D%2520%2520We%2520introduce%2520ClustRecNet%2520-%2520a%2520novel%2520deep%2520learning%2520%2528DL%2529-based%2520recommendation%250Aframework%2520for%2520determining%2520the%2520most%2520suitable%2520clustering%2520algorithms%2520for%2520a%2520given%250Adataset%252C%2520addressing%2520the%2520long-standing%2520challenge%2520of%2520clustering%2520algorithm%250Aselection%2520in%2520unsupervised%2520learning.%2520To%2520enable%2520supervised%2520learning%2520in%2520this%250Acontext%252C%2520we%2520construct%2520a%2520comprehensive%2520data%2520repository%2520comprising%252034%252C000%250Asynthetic%2520datasets%2520with%2520diverse%2520structural%2520properties.%2520Each%2520of%2520them%2520was%250Aprocessed%2520using%252010%2520popular%2520clustering%2520algorithms.%2520The%2520resulting%2520clusterings%250Awere%2520assessed%2520via%2520the%2520Adjusted%2520Rand%2520Index%2520%2528ARI%2529%2520to%2520establish%2520ground%2520truth%250Alabels%252C%2520used%2520for%2520training%2520and%2520evaluation%2520of%2520our%2520DL%2520model.%2520The%2520proposed%2520network%250Aarchitecture%2520integrates%2520convolutional%252C%2520residual%252C%2520and%2520attention%2520mechanisms%2520to%250Acapture%2520both%2520local%2520and%2520global%2520structural%2520patterns%2520from%2520the%2520input%2520data.%2520This%250Adesign%2520supports%2520end-to-end%2520training%2520to%2520learn%2520compact%2520representations%2520of%250Adatasets%2520and%2520enables%2520direct%2520recommendation%2520of%2520the%2520most%2520suitable%2520clustering%250Aalgorithm%252C%2520reducing%2520reliance%2520on%2520handcrafted%2520meta-features%2520and%2520traditional%250ACluster%2520Validity%2520Indices%2520%2528CVIs%2529.%2520Comprehensive%2520experiments%2520across%2520synthetic%2520and%250Areal-world%2520benchmarks%2520demonstrate%2520that%2520our%2520DL%2520model%2520consistently%2520outperforms%250Aconventional%2520CVIs%2520%2528e.g.%2520Silhouette%252C%2520Calinski-Harabasz%252C%2520Davies-Bouldin%252C%2520and%250ADunn%2529%2520as%2520well%2520as%2520state-of-the-art%2520AutoML%2520clustering%2520recommendation%2520approaches%250A%2528e.g.%2520ML2DAC%252C%2520AutoCluster%252C%2520and%2520AutoML4Clust%2529.%2520Notably%252C%2520the%2520proposed%2520model%250Aachieves%2520a%25200.497%2520ARI%2520improvement%2520over%2520the%2520Calinski-Harabasz%2520index%2520on%2520synthetic%250Adata%2520and%2520a%252015.3%2525%2520ARI%2520gain%2520over%2520the%2520best-performing%2520AutoML%2520approach%2520on%250Areal-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClustRecNet%3A%20A%20Novel%20End-to-End%20Deep%20Learning%20Framework%20for%20Clustering%0A%20%20Algorithm%20Recommendation&entry.906535625=Mohammadreza%20Bakhtyari%20and%20Bogdan%20Mazoure%20and%20Renato%20Cordeiro%20de%20Amorim%20and%20Guillaume%20Rabusseau%20and%20Vladimir%20Makarenkov&entry.1292438233=%20%20We%20introduce%20ClustRecNet%20-%20a%20novel%20deep%20learning%20%28DL%29-based%20recommendation%0Aframework%20for%20determining%20the%20most%20suitable%20clustering%20algorithms%20for%20a%20given%0Adataset%2C%20addressing%20the%20long-standing%20challenge%20of%20clustering%20algorithm%0Aselection%20in%20unsupervised%20learning.%20To%20enable%20supervised%20learning%20in%20this%0Acontext%2C%20we%20construct%20a%20comprehensive%20data%20repository%20comprising%2034%2C000%0Asynthetic%20datasets%20with%20diverse%20structural%20properties.%20Each%20of%20them%20was%0Aprocessed%20using%2010%20popular%20clustering%20algorithms.%20The%20resulting%20clusterings%0Awere%20assessed%20via%20the%20Adjusted%20Rand%20Index%20%28ARI%29%20to%20establish%20ground%20truth%0Alabels%2C%20used%20for%20training%20and%20evaluation%20of%20our%20DL%20model.%20The%20proposed%20network%0Aarchitecture%20integrates%20convolutional%2C%20residual%2C%20and%20attention%20mechanisms%20to%0Acapture%20both%20local%20and%20global%20structural%20patterns%20from%20the%20input%20data.%20This%0Adesign%20supports%20end-to-end%20training%20to%20learn%20compact%20representations%20of%0Adatasets%20and%20enables%20direct%20recommendation%20of%20the%20most%20suitable%20clustering%0Aalgorithm%2C%20reducing%20reliance%20on%20handcrafted%20meta-features%20and%20traditional%0ACluster%20Validity%20Indices%20%28CVIs%29.%20Comprehensive%20experiments%20across%20synthetic%20and%0Areal-world%20benchmarks%20demonstrate%20that%20our%20DL%20model%20consistently%20outperforms%0Aconventional%20CVIs%20%28e.g.%20Silhouette%2C%20Calinski-Harabasz%2C%20Davies-Bouldin%2C%20and%0ADunn%29%20as%20well%20as%20state-of-the-art%20AutoML%20clustering%20recommendation%20approaches%0A%28e.g.%20ML2DAC%2C%20AutoCluster%2C%20and%20AutoML4Clust%29.%20Notably%2C%20the%20proposed%20model%0Aachieves%20a%200.497%20ARI%20improvement%20over%20the%20Calinski-Harabasz%20index%20on%20synthetic%0Adata%20and%20a%2015.3%25%20ARI%20gain%20over%20the%20best-performing%20AutoML%20approach%20on%0Areal-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25289v2&entry.124074799=Read"},
{"title": "Understanding and Improving Information Preservation in Prompt\n  Compression for LLMs", "author": "Weronika \u0141ajewska and Momchil Hardalov and Laura Aina and Neha Anna John and Hang Su and Llu\u00eds M\u00e0rquez", "abstract": "  Recent advancements in large language models (LLMs) have enabled their\nsuccessful application to a broad range of tasks. However, in\ninformation-intensive tasks, the prompt length can grow fast, leading to\nincreased computational requirements, performance degradation, and induced\nbiases from irrelevant or redundant information. Recently, various prompt\ncompression techniques have been introduced to optimize the trade-off between\nreducing input length and retaining performance. We propose a holistic\nevaluation framework that allows for in-depth analysis of prompt compression\nmethods. We focus on three key aspects, besides compression ratio: (i)\ndownstream task performance, (ii) grounding in the input context, and (iii)\ninformation preservation. Using our framework, we analyze state-of-the-art soft\nand hard compression methods and show that some fail to preserve key details\nfrom the original prompt, limiting performance on complex tasks. By identifying\nthese limitations, we are able to improve one soft prompting method by\ncontrolling compression granularity, achieving up to +23% in downstream\nperformance, +8 BERTScore points in grounding, and 2.7x more entities preserved\nin compression. Ultimately, we find that the best effectiveness/compression\nrate trade-off is achieved with soft prompting combined with sequence-level\ntraining.The code is available at\nhttps://github.com/amazon-science/information-preservation-in-prompt-compression.\n", "link": "http://arxiv.org/abs/2503.19114v2", "date": "2025-10-10", "relevancy": 2.4094, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Improving%20Information%20Preservation%20in%20Prompt%0A%20%20Compression%20for%20LLMs&body=Title%3A%20Understanding%20and%20Improving%20Information%20Preservation%20in%20Prompt%0A%20%20Compression%20for%20LLMs%0AAuthor%3A%20Weronika%20%C5%81ajewska%20and%20Momchil%20Hardalov%20and%20Laura%20Aina%20and%20Neha%20Anna%20John%20and%20Hang%20Su%20and%20Llu%C3%ADs%20M%C3%A0rquez%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20their%0Asuccessful%20application%20to%20a%20broad%20range%20of%20tasks.%20However%2C%20in%0Ainformation-intensive%20tasks%2C%20the%20prompt%20length%20can%20grow%20fast%2C%20leading%20to%0Aincreased%20computational%20requirements%2C%20performance%20degradation%2C%20and%20induced%0Abiases%20from%20irrelevant%20or%20redundant%20information.%20Recently%2C%20various%20prompt%0Acompression%20techniques%20have%20been%20introduced%20to%20optimize%20the%20trade-off%20between%0Areducing%20input%20length%20and%20retaining%20performance.%20We%20propose%20a%20holistic%0Aevaluation%20framework%20that%20allows%20for%20in-depth%20analysis%20of%20prompt%20compression%0Amethods.%20We%20focus%20on%20three%20key%20aspects%2C%20besides%20compression%20ratio%3A%20%28i%29%0Adownstream%20task%20performance%2C%20%28ii%29%20grounding%20in%20the%20input%20context%2C%20and%20%28iii%29%0Ainformation%20preservation.%20Using%20our%20framework%2C%20we%20analyze%20state-of-the-art%20soft%0Aand%20hard%20compression%20methods%20and%20show%20that%20some%20fail%20to%20preserve%20key%20details%0Afrom%20the%20original%20prompt%2C%20limiting%20performance%20on%20complex%20tasks.%20By%20identifying%0Athese%20limitations%2C%20we%20are%20able%20to%20improve%20one%20soft%20prompting%20method%20by%0Acontrolling%20compression%20granularity%2C%20achieving%20up%20to%20%2B23%25%20in%20downstream%0Aperformance%2C%20%2B8%20BERTScore%20points%20in%20grounding%2C%20and%202.7x%20more%20entities%20preserved%0Ain%20compression.%20Ultimately%2C%20we%20find%20that%20the%20best%20effectiveness/compression%0Arate%20trade-off%20is%20achieved%20with%20soft%20prompting%20combined%20with%20sequence-level%0Atraining.The%20code%20is%20available%20at%0Ahttps%3A//github.com/amazon-science/information-preservation-in-prompt-compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Improving%2520Information%2520Preservation%2520in%2520Prompt%250A%2520%2520Compression%2520for%2520LLMs%26entry.906535625%3DWeronika%2520%25C5%2581ajewska%2520and%2520Momchil%2520Hardalov%2520and%2520Laura%2520Aina%2520and%2520Neha%2520Anna%2520John%2520and%2520Hang%2520Su%2520and%2520Llu%25C3%25ADs%2520M%25C3%25A0rquez%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520enabled%2520their%250Asuccessful%2520application%2520to%2520a%2520broad%2520range%2520of%2520tasks.%2520However%252C%2520in%250Ainformation-intensive%2520tasks%252C%2520the%2520prompt%2520length%2520can%2520grow%2520fast%252C%2520leading%2520to%250Aincreased%2520computational%2520requirements%252C%2520performance%2520degradation%252C%2520and%2520induced%250Abiases%2520from%2520irrelevant%2520or%2520redundant%2520information.%2520Recently%252C%2520various%2520prompt%250Acompression%2520techniques%2520have%2520been%2520introduced%2520to%2520optimize%2520the%2520trade-off%2520between%250Areducing%2520input%2520length%2520and%2520retaining%2520performance.%2520We%2520propose%2520a%2520holistic%250Aevaluation%2520framework%2520that%2520allows%2520for%2520in-depth%2520analysis%2520of%2520prompt%2520compression%250Amethods.%2520We%2520focus%2520on%2520three%2520key%2520aspects%252C%2520besides%2520compression%2520ratio%253A%2520%2528i%2529%250Adownstream%2520task%2520performance%252C%2520%2528ii%2529%2520grounding%2520in%2520the%2520input%2520context%252C%2520and%2520%2528iii%2529%250Ainformation%2520preservation.%2520Using%2520our%2520framework%252C%2520we%2520analyze%2520state-of-the-art%2520soft%250Aand%2520hard%2520compression%2520methods%2520and%2520show%2520that%2520some%2520fail%2520to%2520preserve%2520key%2520details%250Afrom%2520the%2520original%2520prompt%252C%2520limiting%2520performance%2520on%2520complex%2520tasks.%2520By%2520identifying%250Athese%2520limitations%252C%2520we%2520are%2520able%2520to%2520improve%2520one%2520soft%2520prompting%2520method%2520by%250Acontrolling%2520compression%2520granularity%252C%2520achieving%2520up%2520to%2520%252B23%2525%2520in%2520downstream%250Aperformance%252C%2520%252B8%2520BERTScore%2520points%2520in%2520grounding%252C%2520and%25202.7x%2520more%2520entities%2520preserved%250Ain%2520compression.%2520Ultimately%252C%2520we%2520find%2520that%2520the%2520best%2520effectiveness/compression%250Arate%2520trade-off%2520is%2520achieved%2520with%2520soft%2520prompting%2520combined%2520with%2520sequence-level%250Atraining.The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/amazon-science/information-preservation-in-prompt-compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Improving%20Information%20Preservation%20in%20Prompt%0A%20%20Compression%20for%20LLMs&entry.906535625=Weronika%20%C5%81ajewska%20and%20Momchil%20Hardalov%20and%20Laura%20Aina%20and%20Neha%20Anna%20John%20and%20Hang%20Su%20and%20Llu%C3%ADs%20M%C3%A0rquez&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20enabled%20their%0Asuccessful%20application%20to%20a%20broad%20range%20of%20tasks.%20However%2C%20in%0Ainformation-intensive%20tasks%2C%20the%20prompt%20length%20can%20grow%20fast%2C%20leading%20to%0Aincreased%20computational%20requirements%2C%20performance%20degradation%2C%20and%20induced%0Abiases%20from%20irrelevant%20or%20redundant%20information.%20Recently%2C%20various%20prompt%0Acompression%20techniques%20have%20been%20introduced%20to%20optimize%20the%20trade-off%20between%0Areducing%20input%20length%20and%20retaining%20performance.%20We%20propose%20a%20holistic%0Aevaluation%20framework%20that%20allows%20for%20in-depth%20analysis%20of%20prompt%20compression%0Amethods.%20We%20focus%20on%20three%20key%20aspects%2C%20besides%20compression%20ratio%3A%20%28i%29%0Adownstream%20task%20performance%2C%20%28ii%29%20grounding%20in%20the%20input%20context%2C%20and%20%28iii%29%0Ainformation%20preservation.%20Using%20our%20framework%2C%20we%20analyze%20state-of-the-art%20soft%0Aand%20hard%20compression%20methods%20and%20show%20that%20some%20fail%20to%20preserve%20key%20details%0Afrom%20the%20original%20prompt%2C%20limiting%20performance%20on%20complex%20tasks.%20By%20identifying%0Athese%20limitations%2C%20we%20are%20able%20to%20improve%20one%20soft%20prompting%20method%20by%0Acontrolling%20compression%20granularity%2C%20achieving%20up%20to%20%2B23%25%20in%20downstream%0Aperformance%2C%20%2B8%20BERTScore%20points%20in%20grounding%2C%20and%202.7x%20more%20entities%20preserved%0Ain%20compression.%20Ultimately%2C%20we%20find%20that%20the%20best%20effectiveness/compression%0Arate%20trade-off%20is%20achieved%20with%20soft%20prompting%20combined%20with%20sequence-level%0Atraining.The%20code%20is%20available%20at%0Ahttps%3A//github.com/amazon-science/information-preservation-in-prompt-compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19114v2&entry.124074799=Read"},
{"title": "STaTS: Structure-Aware Temporal Sequence Summarization via Statistical\n  Window Merging", "author": "Disharee Bhowmick and Ranjith Ramanathan and Sathyanarayanan N. Aakur", "abstract": "  Time series data often contain latent temporal structure, transitions between\nlocally stationary regimes, repeated motifs, and bursts of variability, that\nare rarely leveraged in standard representation learning pipelines. Existing\nmodels typically operate on raw or fixed-window sequences, treating all time\nsteps as equally informative, which leads to inefficiencies, poor robustness,\nand limited scalability in long or noisy sequences. We propose STaTS, a\nlightweight, unsupervised framework for Structure-Aware Temporal Summarization\nthat adaptively compresses both univariate and multivariate time series into\ncompact, information-preserving token sequences. STaTS detects change points\nacross multiple temporal resolutions using a BIC-based statistical divergence\ncriterion, then summarizes each segment using simple functions like the mean or\ngenerative models such as GMMs. This process achieves up to 30x sequence\ncompression while retaining core temporal dynamics. STaTS operates as a\nmodel-agnostic preprocessor and can be integrated with existing unsupervised\ntime series encoders without retraining. Extensive experiments on 150+\ndatasets, including classification tasks on the UCR-85, UCR-128, and UEA-30\narchives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,\ndemonstrate that STaTS enables 85-90\\% of the full-model performance while\noffering dramatic reductions in computational cost. Moreover, STaTS improves\nrobustness under noise and preserves discriminative structure, outperforming\nuniform and clustering-based compression baselines. These results position\nSTaTS as a principled, general-purpose solution for efficient, structure-aware\ntime series modeling.\n", "link": "http://arxiv.org/abs/2510.09593v1", "date": "2025-10-10", "relevancy": 2.4018, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4908}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4776}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STaTS%3A%20Structure-Aware%20Temporal%20Sequence%20Summarization%20via%20Statistical%0A%20%20Window%20Merging&body=Title%3A%20STaTS%3A%20Structure-Aware%20Temporal%20Sequence%20Summarization%20via%20Statistical%0A%20%20Window%20Merging%0AAuthor%3A%20Disharee%20Bhowmick%20and%20Ranjith%20Ramanathan%20and%20Sathyanarayanan%20N.%20Aakur%0AAbstract%3A%20%20%20Time%20series%20data%20often%20contain%20latent%20temporal%20structure%2C%20transitions%20between%0Alocally%20stationary%20regimes%2C%20repeated%20motifs%2C%20and%20bursts%20of%20variability%2C%20that%0Aare%20rarely%20leveraged%20in%20standard%20representation%20learning%20pipelines.%20Existing%0Amodels%20typically%20operate%20on%20raw%20or%20fixed-window%20sequences%2C%20treating%20all%20time%0Asteps%20as%20equally%20informative%2C%20which%20leads%20to%20inefficiencies%2C%20poor%20robustness%2C%0Aand%20limited%20scalability%20in%20long%20or%20noisy%20sequences.%20We%20propose%20STaTS%2C%20a%0Alightweight%2C%20unsupervised%20framework%20for%20Structure-Aware%20Temporal%20Summarization%0Athat%20adaptively%20compresses%20both%20univariate%20and%20multivariate%20time%20series%20into%0Acompact%2C%20information-preserving%20token%20sequences.%20STaTS%20detects%20change%20points%0Aacross%20multiple%20temporal%20resolutions%20using%20a%20BIC-based%20statistical%20divergence%0Acriterion%2C%20then%20summarizes%20each%20segment%20using%20simple%20functions%20like%20the%20mean%20or%0Agenerative%20models%20such%20as%20GMMs.%20This%20process%20achieves%20up%20to%2030x%20sequence%0Acompression%20while%20retaining%20core%20temporal%20dynamics.%20STaTS%20operates%20as%20a%0Amodel-agnostic%20preprocessor%20and%20can%20be%20integrated%20with%20existing%20unsupervised%0Atime%20series%20encoders%20without%20retraining.%20Extensive%20experiments%20on%20150%2B%0Adatasets%2C%20including%20classification%20tasks%20on%20the%20UCR-85%2C%20UCR-128%2C%20and%20UEA-30%0Aarchives%2C%20and%20forecasting%20on%20ETTh1%20and%20ETTh2%2C%20ETTm1%2C%20and%20Electricity%2C%0Ademonstrate%20that%20STaTS%20enables%2085-90%5C%25%20of%20the%20full-model%20performance%20while%0Aoffering%20dramatic%20reductions%20in%20computational%20cost.%20Moreover%2C%20STaTS%20improves%0Arobustness%20under%20noise%20and%20preserves%20discriminative%20structure%2C%20outperforming%0Auniform%20and%20clustering-based%20compression%20baselines.%20These%20results%20position%0ASTaTS%20as%20a%20principled%2C%20general-purpose%20solution%20for%20efficient%2C%20structure-aware%0Atime%20series%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTaTS%253A%2520Structure-Aware%2520Temporal%2520Sequence%2520Summarization%2520via%2520Statistical%250A%2520%2520Window%2520Merging%26entry.906535625%3DDisharee%2520Bhowmick%2520and%2520Ranjith%2520Ramanathan%2520and%2520Sathyanarayanan%2520N.%2520Aakur%26entry.1292438233%3D%2520%2520Time%2520series%2520data%2520often%2520contain%2520latent%2520temporal%2520structure%252C%2520transitions%2520between%250Alocally%2520stationary%2520regimes%252C%2520repeated%2520motifs%252C%2520and%2520bursts%2520of%2520variability%252C%2520that%250Aare%2520rarely%2520leveraged%2520in%2520standard%2520representation%2520learning%2520pipelines.%2520Existing%250Amodels%2520typically%2520operate%2520on%2520raw%2520or%2520fixed-window%2520sequences%252C%2520treating%2520all%2520time%250Asteps%2520as%2520equally%2520informative%252C%2520which%2520leads%2520to%2520inefficiencies%252C%2520poor%2520robustness%252C%250Aand%2520limited%2520scalability%2520in%2520long%2520or%2520noisy%2520sequences.%2520We%2520propose%2520STaTS%252C%2520a%250Alightweight%252C%2520unsupervised%2520framework%2520for%2520Structure-Aware%2520Temporal%2520Summarization%250Athat%2520adaptively%2520compresses%2520both%2520univariate%2520and%2520multivariate%2520time%2520series%2520into%250Acompact%252C%2520information-preserving%2520token%2520sequences.%2520STaTS%2520detects%2520change%2520points%250Aacross%2520multiple%2520temporal%2520resolutions%2520using%2520a%2520BIC-based%2520statistical%2520divergence%250Acriterion%252C%2520then%2520summarizes%2520each%2520segment%2520using%2520simple%2520functions%2520like%2520the%2520mean%2520or%250Agenerative%2520models%2520such%2520as%2520GMMs.%2520This%2520process%2520achieves%2520up%2520to%252030x%2520sequence%250Acompression%2520while%2520retaining%2520core%2520temporal%2520dynamics.%2520STaTS%2520operates%2520as%2520a%250Amodel-agnostic%2520preprocessor%2520and%2520can%2520be%2520integrated%2520with%2520existing%2520unsupervised%250Atime%2520series%2520encoders%2520without%2520retraining.%2520Extensive%2520experiments%2520on%2520150%252B%250Adatasets%252C%2520including%2520classification%2520tasks%2520on%2520the%2520UCR-85%252C%2520UCR-128%252C%2520and%2520UEA-30%250Aarchives%252C%2520and%2520forecasting%2520on%2520ETTh1%2520and%2520ETTh2%252C%2520ETTm1%252C%2520and%2520Electricity%252C%250Ademonstrate%2520that%2520STaTS%2520enables%252085-90%255C%2525%2520of%2520the%2520full-model%2520performance%2520while%250Aoffering%2520dramatic%2520reductions%2520in%2520computational%2520cost.%2520Moreover%252C%2520STaTS%2520improves%250Arobustness%2520under%2520noise%2520and%2520preserves%2520discriminative%2520structure%252C%2520outperforming%250Auniform%2520and%2520clustering-based%2520compression%2520baselines.%2520These%2520results%2520position%250ASTaTS%2520as%2520a%2520principled%252C%2520general-purpose%2520solution%2520for%2520efficient%252C%2520structure-aware%250Atime%2520series%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STaTS%3A%20Structure-Aware%20Temporal%20Sequence%20Summarization%20via%20Statistical%0A%20%20Window%20Merging&entry.906535625=Disharee%20Bhowmick%20and%20Ranjith%20Ramanathan%20and%20Sathyanarayanan%20N.%20Aakur&entry.1292438233=%20%20Time%20series%20data%20often%20contain%20latent%20temporal%20structure%2C%20transitions%20between%0Alocally%20stationary%20regimes%2C%20repeated%20motifs%2C%20and%20bursts%20of%20variability%2C%20that%0Aare%20rarely%20leveraged%20in%20standard%20representation%20learning%20pipelines.%20Existing%0Amodels%20typically%20operate%20on%20raw%20or%20fixed-window%20sequences%2C%20treating%20all%20time%0Asteps%20as%20equally%20informative%2C%20which%20leads%20to%20inefficiencies%2C%20poor%20robustness%2C%0Aand%20limited%20scalability%20in%20long%20or%20noisy%20sequences.%20We%20propose%20STaTS%2C%20a%0Alightweight%2C%20unsupervised%20framework%20for%20Structure-Aware%20Temporal%20Summarization%0Athat%20adaptively%20compresses%20both%20univariate%20and%20multivariate%20time%20series%20into%0Acompact%2C%20information-preserving%20token%20sequences.%20STaTS%20detects%20change%20points%0Aacross%20multiple%20temporal%20resolutions%20using%20a%20BIC-based%20statistical%20divergence%0Acriterion%2C%20then%20summarizes%20each%20segment%20using%20simple%20functions%20like%20the%20mean%20or%0Agenerative%20models%20such%20as%20GMMs.%20This%20process%20achieves%20up%20to%2030x%20sequence%0Acompression%20while%20retaining%20core%20temporal%20dynamics.%20STaTS%20operates%20as%20a%0Amodel-agnostic%20preprocessor%20and%20can%20be%20integrated%20with%20existing%20unsupervised%0Atime%20series%20encoders%20without%20retraining.%20Extensive%20experiments%20on%20150%2B%0Adatasets%2C%20including%20classification%20tasks%20on%20the%20UCR-85%2C%20UCR-128%2C%20and%20UEA-30%0Aarchives%2C%20and%20forecasting%20on%20ETTh1%20and%20ETTh2%2C%20ETTm1%2C%20and%20Electricity%2C%0Ademonstrate%20that%20STaTS%20enables%2085-90%5C%25%20of%20the%20full-model%20performance%20while%0Aoffering%20dramatic%20reductions%20in%20computational%20cost.%20Moreover%2C%20STaTS%20improves%0Arobustness%20under%20noise%20and%20preserves%20discriminative%20structure%2C%20outperforming%0Auniform%20and%20clustering-based%20compression%20baselines.%20These%20results%20position%0ASTaTS%20as%20a%20principled%2C%20general-purpose%20solution%20for%20efficient%2C%20structure-aware%0Atime%20series%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09593v1&entry.124074799=Read"},
{"title": "HANDO: Hierarchical Autonomous Navigation and Dexterous\n  Omni-loco-manipulation", "author": "Jingyuan Sun and Chaoran Wang and Mingyu Zhang and Cui Miao and Hongyu Ji and Zihan Qu and Han Sun and Bing Wang and Qingyi Si", "abstract": "  Seamless loco-manipulation in unstructured environments requires robots to\nleverage autonomous exploration alongside whole-body control for physical\ninteraction. In this work, we introduce HANDO (Hierarchical Autonomous\nNavigation and Dexterous Omni-loco-manipulation), a two-layer framework\ndesigned for legged robots equipped with manipulators to perform human-centered\nmobile manipulation tasks. The first layer utilizes a goal-conditioned\nautonomous exploration policy to guide the robot to semantically specified\ntargets, such as a black office chair in a dynamic environment. The second\nlayer employs a unified whole-body loco-manipulation policy to coordinate the\narm and legs for precise interaction tasks-for example, handing a drink to a\nperson seated on the chair. We have conducted an initial deployment of the\nnavigation module, and will continue to pursue finer-grained deployment of\nwhole-body loco-manipulation.\n", "link": "http://arxiv.org/abs/2510.09221v1", "date": "2025-10-10", "relevancy": 2.3993, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6467}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6133}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HANDO%3A%20Hierarchical%20Autonomous%20Navigation%20and%20Dexterous%0A%20%20Omni-loco-manipulation&body=Title%3A%20HANDO%3A%20Hierarchical%20Autonomous%20Navigation%20and%20Dexterous%0A%20%20Omni-loco-manipulation%0AAuthor%3A%20Jingyuan%20Sun%20and%20Chaoran%20Wang%20and%20Mingyu%20Zhang%20and%20Cui%20Miao%20and%20Hongyu%20Ji%20and%20Zihan%20Qu%20and%20Han%20Sun%20and%20Bing%20Wang%20and%20Qingyi%20Si%0AAbstract%3A%20%20%20Seamless%20loco-manipulation%20in%20unstructured%20environments%20requires%20robots%20to%0Aleverage%20autonomous%20exploration%20alongside%20whole-body%20control%20for%20physical%0Ainteraction.%20In%20this%20work%2C%20we%20introduce%20HANDO%20%28Hierarchical%20Autonomous%0ANavigation%20and%20Dexterous%20Omni-loco-manipulation%29%2C%20a%20two-layer%20framework%0Adesigned%20for%20legged%20robots%20equipped%20with%20manipulators%20to%20perform%20human-centered%0Amobile%20manipulation%20tasks.%20The%20first%20layer%20utilizes%20a%20goal-conditioned%0Aautonomous%20exploration%20policy%20to%20guide%20the%20robot%20to%20semantically%20specified%0Atargets%2C%20such%20as%20a%20black%20office%20chair%20in%20a%20dynamic%20environment.%20The%20second%0Alayer%20employs%20a%20unified%20whole-body%20loco-manipulation%20policy%20to%20coordinate%20the%0Aarm%20and%20legs%20for%20precise%20interaction%20tasks-for%20example%2C%20handing%20a%20drink%20to%20a%0Aperson%20seated%20on%20the%20chair.%20We%20have%20conducted%20an%20initial%20deployment%20of%20the%0Anavigation%20module%2C%20and%20will%20continue%20to%20pursue%20finer-grained%20deployment%20of%0Awhole-body%20loco-manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHANDO%253A%2520Hierarchical%2520Autonomous%2520Navigation%2520and%2520Dexterous%250A%2520%2520Omni-loco-manipulation%26entry.906535625%3DJingyuan%2520Sun%2520and%2520Chaoran%2520Wang%2520and%2520Mingyu%2520Zhang%2520and%2520Cui%2520Miao%2520and%2520Hongyu%2520Ji%2520and%2520Zihan%2520Qu%2520and%2520Han%2520Sun%2520and%2520Bing%2520Wang%2520and%2520Qingyi%2520Si%26entry.1292438233%3D%2520%2520Seamless%2520loco-manipulation%2520in%2520unstructured%2520environments%2520requires%2520robots%2520to%250Aleverage%2520autonomous%2520exploration%2520alongside%2520whole-body%2520control%2520for%2520physical%250Ainteraction.%2520In%2520this%2520work%252C%2520we%2520introduce%2520HANDO%2520%2528Hierarchical%2520Autonomous%250ANavigation%2520and%2520Dexterous%2520Omni-loco-manipulation%2529%252C%2520a%2520two-layer%2520framework%250Adesigned%2520for%2520legged%2520robots%2520equipped%2520with%2520manipulators%2520to%2520perform%2520human-centered%250Amobile%2520manipulation%2520tasks.%2520The%2520first%2520layer%2520utilizes%2520a%2520goal-conditioned%250Aautonomous%2520exploration%2520policy%2520to%2520guide%2520the%2520robot%2520to%2520semantically%2520specified%250Atargets%252C%2520such%2520as%2520a%2520black%2520office%2520chair%2520in%2520a%2520dynamic%2520environment.%2520The%2520second%250Alayer%2520employs%2520a%2520unified%2520whole-body%2520loco-manipulation%2520policy%2520to%2520coordinate%2520the%250Aarm%2520and%2520legs%2520for%2520precise%2520interaction%2520tasks-for%2520example%252C%2520handing%2520a%2520drink%2520to%2520a%250Aperson%2520seated%2520on%2520the%2520chair.%2520We%2520have%2520conducted%2520an%2520initial%2520deployment%2520of%2520the%250Anavigation%2520module%252C%2520and%2520will%2520continue%2520to%2520pursue%2520finer-grained%2520deployment%2520of%250Awhole-body%2520loco-manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HANDO%3A%20Hierarchical%20Autonomous%20Navigation%20and%20Dexterous%0A%20%20Omni-loco-manipulation&entry.906535625=Jingyuan%20Sun%20and%20Chaoran%20Wang%20and%20Mingyu%20Zhang%20and%20Cui%20Miao%20and%20Hongyu%20Ji%20and%20Zihan%20Qu%20and%20Han%20Sun%20and%20Bing%20Wang%20and%20Qingyi%20Si&entry.1292438233=%20%20Seamless%20loco-manipulation%20in%20unstructured%20environments%20requires%20robots%20to%0Aleverage%20autonomous%20exploration%20alongside%20whole-body%20control%20for%20physical%0Ainteraction.%20In%20this%20work%2C%20we%20introduce%20HANDO%20%28Hierarchical%20Autonomous%0ANavigation%20and%20Dexterous%20Omni-loco-manipulation%29%2C%20a%20two-layer%20framework%0Adesigned%20for%20legged%20robots%20equipped%20with%20manipulators%20to%20perform%20human-centered%0Amobile%20manipulation%20tasks.%20The%20first%20layer%20utilizes%20a%20goal-conditioned%0Aautonomous%20exploration%20policy%20to%20guide%20the%20robot%20to%20semantically%20specified%0Atargets%2C%20such%20as%20a%20black%20office%20chair%20in%20a%20dynamic%20environment.%20The%20second%0Alayer%20employs%20a%20unified%20whole-body%20loco-manipulation%20policy%20to%20coordinate%20the%0Aarm%20and%20legs%20for%20precise%20interaction%20tasks-for%20example%2C%20handing%20a%20drink%20to%20a%0Aperson%20seated%20on%20the%20chair.%20We%20have%20conducted%20an%20initial%20deployment%20of%20the%0Anavigation%20module%2C%20and%20will%20continue%20to%20pursue%20finer-grained%20deployment%20of%0Awhole-body%20loco-manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09221v1&entry.124074799=Read"},
{"title": "Near-Optimal Second-Order Guarantees for Model-Based Adversarial\n  Imitation Learning", "author": "Shangzhe Li and Dongruo Zhou and Weitong Zhang", "abstract": "  We study online adversarial imitation learning (AIL), where an agent learns\nfrom offline expert demonstrations and interacts with the environment online\nwithout access to rewards. Despite strong empirical results, the benefits of\nonline interaction and the impact of stochasticity remain poorly understood. We\naddress these gaps by introducing a model-based AIL algorithm (MB-AIL) and\nestablish its horizon-free, second-order sample-complexity guarantees under\ngeneral function approximations for both expert data and reward-free\ninteractions. These second-order bounds provide an instance-dependent result\nthat can scale with the variance of returns under the relevant policies and\ntherefore tighten as the system approaches determinism. Together with\nsecond-order, information-theoretic lower bounds on a newly constructed\nhard-instance family, we show that MB-AIL attains minimax-optimal sample\ncomplexity for online interaction (up to logarithmic factors) with limited\nexpert demonstrations and matches the lower bound for expert demonstrations in\nterms of the dependence on horizon $H$, precision $\\epsilon$ and the policy\nvariance $\\sigma^2$. Experiments further validate our theoretical findings and\ndemonstrate that a practical implementation of MB-AIL matches or surpasses the\nsample efficiency of existing methods.\n", "link": "http://arxiv.org/abs/2510.09487v1", "date": "2025-10-10", "relevancy": 2.3877, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4859}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4803}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-Optimal%20Second-Order%20Guarantees%20for%20Model-Based%20Adversarial%0A%20%20Imitation%20Learning&body=Title%3A%20Near-Optimal%20Second-Order%20Guarantees%20for%20Model-Based%20Adversarial%0A%20%20Imitation%20Learning%0AAuthor%3A%20Shangzhe%20Li%20and%20Dongruo%20Zhou%20and%20Weitong%20Zhang%0AAbstract%3A%20%20%20We%20study%20online%20adversarial%20imitation%20learning%20%28AIL%29%2C%20where%20an%20agent%20learns%0Afrom%20offline%20expert%20demonstrations%20and%20interacts%20with%20the%20environment%20online%0Awithout%20access%20to%20rewards.%20Despite%20strong%20empirical%20results%2C%20the%20benefits%20of%0Aonline%20interaction%20and%20the%20impact%20of%20stochasticity%20remain%20poorly%20understood.%20We%0Aaddress%20these%20gaps%20by%20introducing%20a%20model-based%20AIL%20algorithm%20%28MB-AIL%29%20and%0Aestablish%20its%20horizon-free%2C%20second-order%20sample-complexity%20guarantees%20under%0Ageneral%20function%20approximations%20for%20both%20expert%20data%20and%20reward-free%0Ainteractions.%20These%20second-order%20bounds%20provide%20an%20instance-dependent%20result%0Athat%20can%20scale%20with%20the%20variance%20of%20returns%20under%20the%20relevant%20policies%20and%0Atherefore%20tighten%20as%20the%20system%20approaches%20determinism.%20Together%20with%0Asecond-order%2C%20information-theoretic%20lower%20bounds%20on%20a%20newly%20constructed%0Ahard-instance%20family%2C%20we%20show%20that%20MB-AIL%20attains%20minimax-optimal%20sample%0Acomplexity%20for%20online%20interaction%20%28up%20to%20logarithmic%20factors%29%20with%20limited%0Aexpert%20demonstrations%20and%20matches%20the%20lower%20bound%20for%20expert%20demonstrations%20in%0Aterms%20of%20the%20dependence%20on%20horizon%20%24H%24%2C%20precision%20%24%5Cepsilon%24%20and%20the%20policy%0Avariance%20%24%5Csigma%5E2%24.%20Experiments%20further%20validate%20our%20theoretical%20findings%20and%0Ademonstrate%20that%20a%20practical%20implementation%20of%20MB-AIL%20matches%20or%20surpasses%20the%0Asample%20efficiency%20of%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-Optimal%2520Second-Order%2520Guarantees%2520for%2520Model-Based%2520Adversarial%250A%2520%2520Imitation%2520Learning%26entry.906535625%3DShangzhe%2520Li%2520and%2520Dongruo%2520Zhou%2520and%2520Weitong%2520Zhang%26entry.1292438233%3D%2520%2520We%2520study%2520online%2520adversarial%2520imitation%2520learning%2520%2528AIL%2529%252C%2520where%2520an%2520agent%2520learns%250Afrom%2520offline%2520expert%2520demonstrations%2520and%2520interacts%2520with%2520the%2520environment%2520online%250Awithout%2520access%2520to%2520rewards.%2520Despite%2520strong%2520empirical%2520results%252C%2520the%2520benefits%2520of%250Aonline%2520interaction%2520and%2520the%2520impact%2520of%2520stochasticity%2520remain%2520poorly%2520understood.%2520We%250Aaddress%2520these%2520gaps%2520by%2520introducing%2520a%2520model-based%2520AIL%2520algorithm%2520%2528MB-AIL%2529%2520and%250Aestablish%2520its%2520horizon-free%252C%2520second-order%2520sample-complexity%2520guarantees%2520under%250Ageneral%2520function%2520approximations%2520for%2520both%2520expert%2520data%2520and%2520reward-free%250Ainteractions.%2520These%2520second-order%2520bounds%2520provide%2520an%2520instance-dependent%2520result%250Athat%2520can%2520scale%2520with%2520the%2520variance%2520of%2520returns%2520under%2520the%2520relevant%2520policies%2520and%250Atherefore%2520tighten%2520as%2520the%2520system%2520approaches%2520determinism.%2520Together%2520with%250Asecond-order%252C%2520information-theoretic%2520lower%2520bounds%2520on%2520a%2520newly%2520constructed%250Ahard-instance%2520family%252C%2520we%2520show%2520that%2520MB-AIL%2520attains%2520minimax-optimal%2520sample%250Acomplexity%2520for%2520online%2520interaction%2520%2528up%2520to%2520logarithmic%2520factors%2529%2520with%2520limited%250Aexpert%2520demonstrations%2520and%2520matches%2520the%2520lower%2520bound%2520for%2520expert%2520demonstrations%2520in%250Aterms%2520of%2520the%2520dependence%2520on%2520horizon%2520%2524H%2524%252C%2520precision%2520%2524%255Cepsilon%2524%2520and%2520the%2520policy%250Avariance%2520%2524%255Csigma%255E2%2524.%2520Experiments%2520further%2520validate%2520our%2520theoretical%2520findings%2520and%250Ademonstrate%2520that%2520a%2520practical%2520implementation%2520of%2520MB-AIL%2520matches%2520or%2520surpasses%2520the%250Asample%2520efficiency%2520of%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-Optimal%20Second-Order%20Guarantees%20for%20Model-Based%20Adversarial%0A%20%20Imitation%20Learning&entry.906535625=Shangzhe%20Li%20and%20Dongruo%20Zhou%20and%20Weitong%20Zhang&entry.1292438233=%20%20We%20study%20online%20adversarial%20imitation%20learning%20%28AIL%29%2C%20where%20an%20agent%20learns%0Afrom%20offline%20expert%20demonstrations%20and%20interacts%20with%20the%20environment%20online%0Awithout%20access%20to%20rewards.%20Despite%20strong%20empirical%20results%2C%20the%20benefits%20of%0Aonline%20interaction%20and%20the%20impact%20of%20stochasticity%20remain%20poorly%20understood.%20We%0Aaddress%20these%20gaps%20by%20introducing%20a%20model-based%20AIL%20algorithm%20%28MB-AIL%29%20and%0Aestablish%20its%20horizon-free%2C%20second-order%20sample-complexity%20guarantees%20under%0Ageneral%20function%20approximations%20for%20both%20expert%20data%20and%20reward-free%0Ainteractions.%20These%20second-order%20bounds%20provide%20an%20instance-dependent%20result%0Athat%20can%20scale%20with%20the%20variance%20of%20returns%20under%20the%20relevant%20policies%20and%0Atherefore%20tighten%20as%20the%20system%20approaches%20determinism.%20Together%20with%0Asecond-order%2C%20information-theoretic%20lower%20bounds%20on%20a%20newly%20constructed%0Ahard-instance%20family%2C%20we%20show%20that%20MB-AIL%20attains%20minimax-optimal%20sample%0Acomplexity%20for%20online%20interaction%20%28up%20to%20logarithmic%20factors%29%20with%20limited%0Aexpert%20demonstrations%20and%20matches%20the%20lower%20bound%20for%20expert%20demonstrations%20in%0Aterms%20of%20the%20dependence%20on%20horizon%20%24H%24%2C%20precision%20%24%5Cepsilon%24%20and%20the%20policy%0Avariance%20%24%5Csigma%5E2%24.%20Experiments%20further%20validate%20our%20theoretical%20findings%20and%0Ademonstrate%20that%20a%20practical%20implementation%20of%20MB-AIL%20matches%20or%20surpasses%20the%0Asample%20efficiency%20of%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09487v1&entry.124074799=Read"},
{"title": "SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking", "author": "Pedro Miguel Bastos Soares and Ali Tourani and Miguel Fernandez-Cortizas and Asier Bikandi-Noya and Holger Voos and Jose Luis Sanchez-Lopez", "abstract": "  Advancing research in fields such as Simultaneous Localization and Mapping\n(SLAM) and autonomous navigation critically depends on the availability of\nreliable and reproducible multimodal datasets. While several influential\ndatasets have driven progress in these domains, they often suffer from\nlimitations in sensing modalities, environmental diversity, and the\nreproducibility of the underlying hardware setups. To address these challenges,\nthis paper introduces SMapper, a novel open-hardware, multi-sensor platform\ndesigned explicitly for, though not limited to, SLAM research. The device\nintegrates synchronized LiDAR, multi-camera, and inertial sensing, supported by\na robust calibration and synchronization pipeline that ensures precise\nspatio-temporal alignment across modalities. Its open and replicable design\nallows researchers to extend its capabilities and reproduce experiments across\nboth handheld and robot-mounted scenarios. To demonstrate its practicality, we\nadditionally release SMapper-light, a publicly available SLAM dataset\ncontaining representative indoor and outdoor sequences. The dataset includes\ntightly synchronized multimodal data and ground truth trajectories derived from\noffline LiDAR-based SLAM with sub-centimeter accuracy, alongside dense 3D\nreconstructions. Furthermore, the paper contains benchmarking results on\nstate-of-the-art LiDAR and visual SLAM frameworks using the SMapper-light\ndataset. By combining open-hardware design, reproducible data collection, and\ncomprehensive benchmarking, SMapper establishes a robust foundation for\nadvancing SLAM algorithm development, evaluation, and reproducibility. The\nproject's documentation, including source code, CAD models, and dataset links,\nis publicly available at https://snt-arg.github.io/smapper_docs.\n", "link": "http://arxiv.org/abs/2509.09509v2", "date": "2025-10-10", "relevancy": 2.3877, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6039}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5965}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.59}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMapper%3A%20A%20Multi-Modal%20Data%20Acquisition%20Platform%20for%20SLAM%20Benchmarking&body=Title%3A%20SMapper%3A%20A%20Multi-Modal%20Data%20Acquisition%20Platform%20for%20SLAM%20Benchmarking%0AAuthor%3A%20Pedro%20Miguel%20Bastos%20Soares%20and%20Ali%20Tourani%20and%20Miguel%20Fernandez-Cortizas%20and%20Asier%20Bikandi-Noya%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez%0AAbstract%3A%20%20%20Advancing%20research%20in%20fields%20such%20as%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20and%20autonomous%20navigation%20critically%20depends%20on%20the%20availability%20of%0Areliable%20and%20reproducible%20multimodal%20datasets.%20While%20several%20influential%0Adatasets%20have%20driven%20progress%20in%20these%20domains%2C%20they%20often%20suffer%20from%0Alimitations%20in%20sensing%20modalities%2C%20environmental%20diversity%2C%20and%20the%0Areproducibility%20of%20the%20underlying%20hardware%20setups.%20To%20address%20these%20challenges%2C%0Athis%20paper%20introduces%20SMapper%2C%20a%20novel%20open-hardware%2C%20multi-sensor%20platform%0Adesigned%20explicitly%20for%2C%20though%20not%20limited%20to%2C%20SLAM%20research.%20The%20device%0Aintegrates%20synchronized%20LiDAR%2C%20multi-camera%2C%20and%20inertial%20sensing%2C%20supported%20by%0Aa%20robust%20calibration%20and%20synchronization%20pipeline%20that%20ensures%20precise%0Aspatio-temporal%20alignment%20across%20modalities.%20Its%20open%20and%20replicable%20design%0Aallows%20researchers%20to%20extend%20its%20capabilities%20and%20reproduce%20experiments%20across%0Aboth%20handheld%20and%20robot-mounted%20scenarios.%20To%20demonstrate%20its%20practicality%2C%20we%0Aadditionally%20release%20SMapper-light%2C%20a%20publicly%20available%20SLAM%20dataset%0Acontaining%20representative%20indoor%20and%20outdoor%20sequences.%20The%20dataset%20includes%0Atightly%20synchronized%20multimodal%20data%20and%20ground%20truth%20trajectories%20derived%20from%0Aoffline%20LiDAR-based%20SLAM%20with%20sub-centimeter%20accuracy%2C%20alongside%20dense%203D%0Areconstructions.%20Furthermore%2C%20the%20paper%20contains%20benchmarking%20results%20on%0Astate-of-the-art%20LiDAR%20and%20visual%20SLAM%20frameworks%20using%20the%20SMapper-light%0Adataset.%20By%20combining%20open-hardware%20design%2C%20reproducible%20data%20collection%2C%20and%0Acomprehensive%20benchmarking%2C%20SMapper%20establishes%20a%20robust%20foundation%20for%0Aadvancing%20SLAM%20algorithm%20development%2C%20evaluation%2C%20and%20reproducibility.%20The%0Aproject%27s%20documentation%2C%20including%20source%20code%2C%20CAD%20models%2C%20and%20dataset%20links%2C%0Ais%20publicly%20available%20at%20https%3A//snt-arg.github.io/smapper_docs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMapper%253A%2520A%2520Multi-Modal%2520Data%2520Acquisition%2520Platform%2520for%2520SLAM%2520Benchmarking%26entry.906535625%3DPedro%2520Miguel%2520Bastos%2520Soares%2520and%2520Ali%2520Tourani%2520and%2520Miguel%2520Fernandez-Cortizas%2520and%2520Asier%2520Bikandi-Noya%2520and%2520Holger%2520Voos%2520and%2520Jose%2520Luis%2520Sanchez-Lopez%26entry.1292438233%3D%2520%2520Advancing%2520research%2520in%2520fields%2520such%2520as%2520Simultaneous%2520Localization%2520and%2520Mapping%250A%2528SLAM%2529%2520and%2520autonomous%2520navigation%2520critically%2520depends%2520on%2520the%2520availability%2520of%250Areliable%2520and%2520reproducible%2520multimodal%2520datasets.%2520While%2520several%2520influential%250Adatasets%2520have%2520driven%2520progress%2520in%2520these%2520domains%252C%2520they%2520often%2520suffer%2520from%250Alimitations%2520in%2520sensing%2520modalities%252C%2520environmental%2520diversity%252C%2520and%2520the%250Areproducibility%2520of%2520the%2520underlying%2520hardware%2520setups.%2520To%2520address%2520these%2520challenges%252C%250Athis%2520paper%2520introduces%2520SMapper%252C%2520a%2520novel%2520open-hardware%252C%2520multi-sensor%2520platform%250Adesigned%2520explicitly%2520for%252C%2520though%2520not%2520limited%2520to%252C%2520SLAM%2520research.%2520The%2520device%250Aintegrates%2520synchronized%2520LiDAR%252C%2520multi-camera%252C%2520and%2520inertial%2520sensing%252C%2520supported%2520by%250Aa%2520robust%2520calibration%2520and%2520synchronization%2520pipeline%2520that%2520ensures%2520precise%250Aspatio-temporal%2520alignment%2520across%2520modalities.%2520Its%2520open%2520and%2520replicable%2520design%250Aallows%2520researchers%2520to%2520extend%2520its%2520capabilities%2520and%2520reproduce%2520experiments%2520across%250Aboth%2520handheld%2520and%2520robot-mounted%2520scenarios.%2520To%2520demonstrate%2520its%2520practicality%252C%2520we%250Aadditionally%2520release%2520SMapper-light%252C%2520a%2520publicly%2520available%2520SLAM%2520dataset%250Acontaining%2520representative%2520indoor%2520and%2520outdoor%2520sequences.%2520The%2520dataset%2520includes%250Atightly%2520synchronized%2520multimodal%2520data%2520and%2520ground%2520truth%2520trajectories%2520derived%2520from%250Aoffline%2520LiDAR-based%2520SLAM%2520with%2520sub-centimeter%2520accuracy%252C%2520alongside%2520dense%25203D%250Areconstructions.%2520Furthermore%252C%2520the%2520paper%2520contains%2520benchmarking%2520results%2520on%250Astate-of-the-art%2520LiDAR%2520and%2520visual%2520SLAM%2520frameworks%2520using%2520the%2520SMapper-light%250Adataset.%2520By%2520combining%2520open-hardware%2520design%252C%2520reproducible%2520data%2520collection%252C%2520and%250Acomprehensive%2520benchmarking%252C%2520SMapper%2520establishes%2520a%2520robust%2520foundation%2520for%250Aadvancing%2520SLAM%2520algorithm%2520development%252C%2520evaluation%252C%2520and%2520reproducibility.%2520The%250Aproject%2527s%2520documentation%252C%2520including%2520source%2520code%252C%2520CAD%2520models%252C%2520and%2520dataset%2520links%252C%250Ais%2520publicly%2520available%2520at%2520https%253A//snt-arg.github.io/smapper_docs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMapper%3A%20A%20Multi-Modal%20Data%20Acquisition%20Platform%20for%20SLAM%20Benchmarking&entry.906535625=Pedro%20Miguel%20Bastos%20Soares%20and%20Ali%20Tourani%20and%20Miguel%20Fernandez-Cortizas%20and%20Asier%20Bikandi-Noya%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez&entry.1292438233=%20%20Advancing%20research%20in%20fields%20such%20as%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20and%20autonomous%20navigation%20critically%20depends%20on%20the%20availability%20of%0Areliable%20and%20reproducible%20multimodal%20datasets.%20While%20several%20influential%0Adatasets%20have%20driven%20progress%20in%20these%20domains%2C%20they%20often%20suffer%20from%0Alimitations%20in%20sensing%20modalities%2C%20environmental%20diversity%2C%20and%20the%0Areproducibility%20of%20the%20underlying%20hardware%20setups.%20To%20address%20these%20challenges%2C%0Athis%20paper%20introduces%20SMapper%2C%20a%20novel%20open-hardware%2C%20multi-sensor%20platform%0Adesigned%20explicitly%20for%2C%20though%20not%20limited%20to%2C%20SLAM%20research.%20The%20device%0Aintegrates%20synchronized%20LiDAR%2C%20multi-camera%2C%20and%20inertial%20sensing%2C%20supported%20by%0Aa%20robust%20calibration%20and%20synchronization%20pipeline%20that%20ensures%20precise%0Aspatio-temporal%20alignment%20across%20modalities.%20Its%20open%20and%20replicable%20design%0Aallows%20researchers%20to%20extend%20its%20capabilities%20and%20reproduce%20experiments%20across%0Aboth%20handheld%20and%20robot-mounted%20scenarios.%20To%20demonstrate%20its%20practicality%2C%20we%0Aadditionally%20release%20SMapper-light%2C%20a%20publicly%20available%20SLAM%20dataset%0Acontaining%20representative%20indoor%20and%20outdoor%20sequences.%20The%20dataset%20includes%0Atightly%20synchronized%20multimodal%20data%20and%20ground%20truth%20trajectories%20derived%20from%0Aoffline%20LiDAR-based%20SLAM%20with%20sub-centimeter%20accuracy%2C%20alongside%20dense%203D%0Areconstructions.%20Furthermore%2C%20the%20paper%20contains%20benchmarking%20results%20on%0Astate-of-the-art%20LiDAR%20and%20visual%20SLAM%20frameworks%20using%20the%20SMapper-light%0Adataset.%20By%20combining%20open-hardware%20design%2C%20reproducible%20data%20collection%2C%20and%0Acomprehensive%20benchmarking%2C%20SMapper%20establishes%20a%20robust%20foundation%20for%0Aadvancing%20SLAM%20algorithm%20development%2C%20evaluation%2C%20and%20reproducibility.%20The%0Aproject%27s%20documentation%2C%20including%20source%20code%2C%20CAD%20models%2C%20and%20dataset%20links%2C%0Ais%20publicly%20available%20at%20https%3A//snt-arg.github.io/smapper_docs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09509v2&entry.124074799=Read"},
{"title": "System Prompt Optimization with Meta-Learning", "author": "Yumin Choi and Jinheon Baek and Sung Ju Hwang", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.\n", "link": "http://arxiv.org/abs/2505.09666v2", "date": "2025-10-10", "relevancy": 2.3832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20System%20Prompt%20Optimization%20with%20Meta-Learning&body=Title%3A%20System%20Prompt%20Optimization%20with%20Meta-Learning%0AAuthor%3A%20Yumin%20Choi%20and%20Jinheon%20Baek%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%2C%20with%0Aoptimizing%20their%20input%20prompts%20playing%20a%20pivotal%20role%20in%20maximizing%20their%0Aperformance.%20However%2C%20while%20LLM%20prompts%20consist%20of%20both%20the%20task-agnostic%0Asystem%20prompts%20and%20task-specific%20user%20prompts%2C%20existing%20work%20on%20prompt%0Aoptimization%20has%20focused%20on%20user%20prompts%20specific%20to%20individual%20queries%20or%0Atasks%2C%20and%20largely%20overlooked%20the%20system%20prompt%20that%20is%2C%20once%20optimized%2C%0Aapplicable%20across%20different%20tasks%20and%20domains.%20Motivated%20by%20this%2C%20we%20introduce%0Athe%20novel%20problem%20of%20bilevel%20system%20prompt%20optimization%2C%20whose%20objective%20is%20to%0Adesign%20system%20prompts%20that%20are%20robust%20to%20diverse%20user%20prompts%20and%20transferable%0Ato%20unseen%20tasks.%20To%20tackle%20this%20problem%2C%20we%20then%20propose%20a%20meta-learning%0Aframework%2C%20which%20meta-learns%20the%20system%20prompt%20by%20optimizing%20it%20over%20various%0Auser%20prompts%20across%20multiple%20datasets%2C%20while%20simultaneously%20updating%20the%20user%0Aprompts%20in%20an%20iterative%20manner%20to%20ensure%20synergy%20between%20them.%20We%20conduct%0Aexperiments%20on%2014%20unseen%20datasets%20spanning%205%20different%20domains%2C%20on%20which%20we%0Ashow%20that%20our%20approach%20produces%20system%20prompts%20that%20generalize%20effectively%20to%0Adiverse%20user%20prompts.%20Also%2C%20our%20findings%20reveal%20that%20the%20optimized%20system%0Aprompt%20enables%20rapid%20adaptation%20even%20to%20unseen%20tasks%2C%20requiring%20fewer%0Aoptimization%20steps%20for%20test-time%20user%20prompts%20while%20achieving%20improved%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09666v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystem%2520Prompt%2520Optimization%2520with%2520Meta-Learning%26entry.906535625%3DYumin%2520Choi%2520and%2520Jinheon%2520Baek%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%252C%2520with%250Aoptimizing%2520their%2520input%2520prompts%2520playing%2520a%2520pivotal%2520role%2520in%2520maximizing%2520their%250Aperformance.%2520However%252C%2520while%2520LLM%2520prompts%2520consist%2520of%2520both%2520the%2520task-agnostic%250Asystem%2520prompts%2520and%2520task-specific%2520user%2520prompts%252C%2520existing%2520work%2520on%2520prompt%250Aoptimization%2520has%2520focused%2520on%2520user%2520prompts%2520specific%2520to%2520individual%2520queries%2520or%250Atasks%252C%2520and%2520largely%2520overlooked%2520the%2520system%2520prompt%2520that%2520is%252C%2520once%2520optimized%252C%250Aapplicable%2520across%2520different%2520tasks%2520and%2520domains.%2520Motivated%2520by%2520this%252C%2520we%2520introduce%250Athe%2520novel%2520problem%2520of%2520bilevel%2520system%2520prompt%2520optimization%252C%2520whose%2520objective%2520is%2520to%250Adesign%2520system%2520prompts%2520that%2520are%2520robust%2520to%2520diverse%2520user%2520prompts%2520and%2520transferable%250Ato%2520unseen%2520tasks.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520then%2520propose%2520a%2520meta-learning%250Aframework%252C%2520which%2520meta-learns%2520the%2520system%2520prompt%2520by%2520optimizing%2520it%2520over%2520various%250Auser%2520prompts%2520across%2520multiple%2520datasets%252C%2520while%2520simultaneously%2520updating%2520the%2520user%250Aprompts%2520in%2520an%2520iterative%2520manner%2520to%2520ensure%2520synergy%2520between%2520them.%2520We%2520conduct%250Aexperiments%2520on%252014%2520unseen%2520datasets%2520spanning%25205%2520different%2520domains%252C%2520on%2520which%2520we%250Ashow%2520that%2520our%2520approach%2520produces%2520system%2520prompts%2520that%2520generalize%2520effectively%2520to%250Adiverse%2520user%2520prompts.%2520Also%252C%2520our%2520findings%2520reveal%2520that%2520the%2520optimized%2520system%250Aprompt%2520enables%2520rapid%2520adaptation%2520even%2520to%2520unseen%2520tasks%252C%2520requiring%2520fewer%250Aoptimization%2520steps%2520for%2520test-time%2520user%2520prompts%2520while%2520achieving%2520improved%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09666v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=System%20Prompt%20Optimization%20with%20Meta-Learning&entry.906535625=Yumin%20Choi%20and%20Jinheon%20Baek%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%2C%20with%0Aoptimizing%20their%20input%20prompts%20playing%20a%20pivotal%20role%20in%20maximizing%20their%0Aperformance.%20However%2C%20while%20LLM%20prompts%20consist%20of%20both%20the%20task-agnostic%0Asystem%20prompts%20and%20task-specific%20user%20prompts%2C%20existing%20work%20on%20prompt%0Aoptimization%20has%20focused%20on%20user%20prompts%20specific%20to%20individual%20queries%20or%0Atasks%2C%20and%20largely%20overlooked%20the%20system%20prompt%20that%20is%2C%20once%20optimized%2C%0Aapplicable%20across%20different%20tasks%20and%20domains.%20Motivated%20by%20this%2C%20we%20introduce%0Athe%20novel%20problem%20of%20bilevel%20system%20prompt%20optimization%2C%20whose%20objective%20is%20to%0Adesign%20system%20prompts%20that%20are%20robust%20to%20diverse%20user%20prompts%20and%20transferable%0Ato%20unseen%20tasks.%20To%20tackle%20this%20problem%2C%20we%20then%20propose%20a%20meta-learning%0Aframework%2C%20which%20meta-learns%20the%20system%20prompt%20by%20optimizing%20it%20over%20various%0Auser%20prompts%20across%20multiple%20datasets%2C%20while%20simultaneously%20updating%20the%20user%0Aprompts%20in%20an%20iterative%20manner%20to%20ensure%20synergy%20between%20them.%20We%20conduct%0Aexperiments%20on%2014%20unseen%20datasets%20spanning%205%20different%20domains%2C%20on%20which%20we%0Ashow%20that%20our%20approach%20produces%20system%20prompts%20that%20generalize%20effectively%20to%0Adiverse%20user%20prompts.%20Also%2C%20our%20findings%20reveal%20that%20the%20optimized%20system%0Aprompt%20enables%20rapid%20adaptation%20even%20to%20unseen%20tasks%2C%20requiring%20fewer%0Aoptimization%20steps%20for%20test-time%20user%20prompts%20while%20achieving%20improved%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09666v2&entry.124074799=Read"},
{"title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km", "author": "Peiwen Sun and Shiqiang Lang and Dongming Wu and Yi Ding and Kaituo Feng and Huadai Liu and Zhen Ye and Rui Liu and Yun-Hui Liu and Jianan Wang and Xiangyu Yue", "abstract": "  With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .\n", "link": "http://arxiv.org/abs/2510.09606v1", "date": "2025-10-10", "relevancy": 2.3829, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5975}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaceVista%3A%20All-Scale%20Visual%20Spatial%20Reasoning%20from%20mm%20to%20km&body=Title%3A%20SpaceVista%3A%20All-Scale%20Visual%20Spatial%20Reasoning%20from%20mm%20to%20km%0AAuthor%3A%20Peiwen%20Sun%20and%20Shiqiang%20Lang%20and%20Dongming%20Wu%20and%20Yi%20Ding%20and%20Kaituo%20Feng%20and%20Huadai%20Liu%20and%20Zhen%20Ye%20and%20Rui%20Liu%20and%20Yun-Hui%20Liu%20and%20Jianan%20Wang%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20With%20the%20current%20surge%20in%20spatial%20reasoning%20explorations%2C%20researchers%20have%0Amade%20significant%20progress%20in%20understanding%20indoor%20scenes%2C%20but%20still%20struggle%0Awith%20diverse%20applications%20such%20as%20robotics%20and%20autonomous%20driving.%20This%20paper%0Aaims%20to%20advance%20all-scale%20spatial%20reasoning%20across%20diverse%20scenarios%20by%0Atackling%20two%20key%20challenges%3A%201%29%20the%20heavy%20reliance%20on%20indoor%203D%20scans%20and%0Alabor-intensive%20manual%20annotations%20for%20dataset%20curation%3B%202%29%20the%20absence%20of%0Aeffective%20all-scale%20scene%20modeling%2C%20which%20often%20leads%20to%20overfitting%20to%0Aindividual%20scenes.%20In%20this%20paper%2C%20we%20introduce%20a%20holistic%20solution%20that%0Aintegrates%20a%20structured%20spatial%20reasoning%20knowledge%20system%2C%20scale-aware%0Amodeling%2C%20and%20a%20progressive%20training%20paradigm%2C%20as%20the%20first%20attempt%20to%20broaden%0Athe%20all-scale%20spatial%20intelligence%20of%20MLLMs%20to%20the%20best%20of%20our%20knowledge.%20Using%0Aa%20task-specific%2C%20specialist-driven%20automated%20pipeline%2C%20we%20curate%20over%2038K%20video%0Ascenes%20across%205%20spatial%20scales%20to%20create%20SpaceVista-1M%2C%20a%20dataset%20comprising%0Aapproximately%201M%20spatial%20QA%20pairs%20spanning%2019%20diverse%20task%20types.%20While%0Aspecialist%20models%20can%20inject%20useful%20domain%20knowledge%2C%20they%20are%20not%20reliable%20for%0Aevaluation.%20We%20then%20build%20an%20all-scale%20benchmark%20with%20precise%20annotations%20by%0Amanually%20recording%2C%20retrieving%2C%20and%20assembling%20video-based%20data.%20However%2C%20naive%0Atraining%20with%20SpaceVista-1M%20often%20yields%20suboptimal%20results%20due%20to%20the%0Apotential%20knowledge%20conflict.%20Accordingly%2C%20we%20introduce%20SpaceVista-7B%2C%20a%0Aspatial%20reasoning%20model%20that%20accepts%20dense%20inputs%20beyond%20semantics%20and%20uses%0Ascale%20as%20an%20anchor%20for%20scale-aware%20experts%20and%20progressive%20rewards.%20Finally%2C%0Aextensive%20evaluations%20across%205%20benchmarks%2C%20including%20our%20SpaceVista-Bench%2C%0Ademonstrate%20competitive%20performance%2C%20showcasing%20strong%20generalization%20across%0Aall%20scales%20and%20scenarios.%20Our%20dataset%2C%20model%2C%20and%20benchmark%20will%20be%20released%20on%0Ahttps%3A//peiwensun2000.github.io/mm2km%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaceVista%253A%2520All-Scale%2520Visual%2520Spatial%2520Reasoning%2520from%2520mm%2520to%2520km%26entry.906535625%3DPeiwen%2520Sun%2520and%2520Shiqiang%2520Lang%2520and%2520Dongming%2520Wu%2520and%2520Yi%2520Ding%2520and%2520Kaituo%2520Feng%2520and%2520Huadai%2520Liu%2520and%2520Zhen%2520Ye%2520and%2520Rui%2520Liu%2520and%2520Yun-Hui%2520Liu%2520and%2520Jianan%2520Wang%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520With%2520the%2520current%2520surge%2520in%2520spatial%2520reasoning%2520explorations%252C%2520researchers%2520have%250Amade%2520significant%2520progress%2520in%2520understanding%2520indoor%2520scenes%252C%2520but%2520still%2520struggle%250Awith%2520diverse%2520applications%2520such%2520as%2520robotics%2520and%2520autonomous%2520driving.%2520This%2520paper%250Aaims%2520to%2520advance%2520all-scale%2520spatial%2520reasoning%2520across%2520diverse%2520scenarios%2520by%250Atackling%2520two%2520key%2520challenges%253A%25201%2529%2520the%2520heavy%2520reliance%2520on%2520indoor%25203D%2520scans%2520and%250Alabor-intensive%2520manual%2520annotations%2520for%2520dataset%2520curation%253B%25202%2529%2520the%2520absence%2520of%250Aeffective%2520all-scale%2520scene%2520modeling%252C%2520which%2520often%2520leads%2520to%2520overfitting%2520to%250Aindividual%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520holistic%2520solution%2520that%250Aintegrates%2520a%2520structured%2520spatial%2520reasoning%2520knowledge%2520system%252C%2520scale-aware%250Amodeling%252C%2520and%2520a%2520progressive%2520training%2520paradigm%252C%2520as%2520the%2520first%2520attempt%2520to%2520broaden%250Athe%2520all-scale%2520spatial%2520intelligence%2520of%2520MLLMs%2520to%2520the%2520best%2520of%2520our%2520knowledge.%2520Using%250Aa%2520task-specific%252C%2520specialist-driven%2520automated%2520pipeline%252C%2520we%2520curate%2520over%252038K%2520video%250Ascenes%2520across%25205%2520spatial%2520scales%2520to%2520create%2520SpaceVista-1M%252C%2520a%2520dataset%2520comprising%250Aapproximately%25201M%2520spatial%2520QA%2520pairs%2520spanning%252019%2520diverse%2520task%2520types.%2520While%250Aspecialist%2520models%2520can%2520inject%2520useful%2520domain%2520knowledge%252C%2520they%2520are%2520not%2520reliable%2520for%250Aevaluation.%2520We%2520then%2520build%2520an%2520all-scale%2520benchmark%2520with%2520precise%2520annotations%2520by%250Amanually%2520recording%252C%2520retrieving%252C%2520and%2520assembling%2520video-based%2520data.%2520However%252C%2520naive%250Atraining%2520with%2520SpaceVista-1M%2520often%2520yields%2520suboptimal%2520results%2520due%2520to%2520the%250Apotential%2520knowledge%2520conflict.%2520Accordingly%252C%2520we%2520introduce%2520SpaceVista-7B%252C%2520a%250Aspatial%2520reasoning%2520model%2520that%2520accepts%2520dense%2520inputs%2520beyond%2520semantics%2520and%2520uses%250Ascale%2520as%2520an%2520anchor%2520for%2520scale-aware%2520experts%2520and%2520progressive%2520rewards.%2520Finally%252C%250Aextensive%2520evaluations%2520across%25205%2520benchmarks%252C%2520including%2520our%2520SpaceVista-Bench%252C%250Ademonstrate%2520competitive%2520performance%252C%2520showcasing%2520strong%2520generalization%2520across%250Aall%2520scales%2520and%2520scenarios.%2520Our%2520dataset%252C%2520model%252C%2520and%2520benchmark%2520will%2520be%2520released%2520on%250Ahttps%253A//peiwensun2000.github.io/mm2km%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaceVista%3A%20All-Scale%20Visual%20Spatial%20Reasoning%20from%20mm%20to%20km&entry.906535625=Peiwen%20Sun%20and%20Shiqiang%20Lang%20and%20Dongming%20Wu%20and%20Yi%20Ding%20and%20Kaituo%20Feng%20and%20Huadai%20Liu%20and%20Zhen%20Ye%20and%20Rui%20Liu%20and%20Yun-Hui%20Liu%20and%20Jianan%20Wang%20and%20Xiangyu%20Yue&entry.1292438233=%20%20With%20the%20current%20surge%20in%20spatial%20reasoning%20explorations%2C%20researchers%20have%0Amade%20significant%20progress%20in%20understanding%20indoor%20scenes%2C%20but%20still%20struggle%0Awith%20diverse%20applications%20such%20as%20robotics%20and%20autonomous%20driving.%20This%20paper%0Aaims%20to%20advance%20all-scale%20spatial%20reasoning%20across%20diverse%20scenarios%20by%0Atackling%20two%20key%20challenges%3A%201%29%20the%20heavy%20reliance%20on%20indoor%203D%20scans%20and%0Alabor-intensive%20manual%20annotations%20for%20dataset%20curation%3B%202%29%20the%20absence%20of%0Aeffective%20all-scale%20scene%20modeling%2C%20which%20often%20leads%20to%20overfitting%20to%0Aindividual%20scenes.%20In%20this%20paper%2C%20we%20introduce%20a%20holistic%20solution%20that%0Aintegrates%20a%20structured%20spatial%20reasoning%20knowledge%20system%2C%20scale-aware%0Amodeling%2C%20and%20a%20progressive%20training%20paradigm%2C%20as%20the%20first%20attempt%20to%20broaden%0Athe%20all-scale%20spatial%20intelligence%20of%20MLLMs%20to%20the%20best%20of%20our%20knowledge.%20Using%0Aa%20task-specific%2C%20specialist-driven%20automated%20pipeline%2C%20we%20curate%20over%2038K%20video%0Ascenes%20across%205%20spatial%20scales%20to%20create%20SpaceVista-1M%2C%20a%20dataset%20comprising%0Aapproximately%201M%20spatial%20QA%20pairs%20spanning%2019%20diverse%20task%20types.%20While%0Aspecialist%20models%20can%20inject%20useful%20domain%20knowledge%2C%20they%20are%20not%20reliable%20for%0Aevaluation.%20We%20then%20build%20an%20all-scale%20benchmark%20with%20precise%20annotations%20by%0Amanually%20recording%2C%20retrieving%2C%20and%20assembling%20video-based%20data.%20However%2C%20naive%0Atraining%20with%20SpaceVista-1M%20often%20yields%20suboptimal%20results%20due%20to%20the%0Apotential%20knowledge%20conflict.%20Accordingly%2C%20we%20introduce%20SpaceVista-7B%2C%20a%0Aspatial%20reasoning%20model%20that%20accepts%20dense%20inputs%20beyond%20semantics%20and%20uses%0Ascale%20as%20an%20anchor%20for%20scale-aware%20experts%20and%20progressive%20rewards.%20Finally%2C%0Aextensive%20evaluations%20across%205%20benchmarks%2C%20including%20our%20SpaceVista-Bench%2C%0Ademonstrate%20competitive%20performance%2C%20showcasing%20strong%20generalization%20across%0Aall%20scales%20and%20scenarios.%20Our%20dataset%2C%20model%2C%20and%20benchmark%20will%20be%20released%20on%0Ahttps%3A//peiwensun2000.github.io/mm2km%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09606v1&entry.124074799=Read"},
{"title": "Editable Noise Map Inversion: Encoding Target-image into Noise For\n  High-Fidelity Image Manipulation", "author": "Mingyu Kang and Yong Suk Choi", "abstract": "  Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality and diverse images. Building on these advancements, diffusion\nmodels have also demonstrated exceptional performance in text-guided image\nediting. A key strategy for effective image editing involves inverting the\nsource image into editable noise maps associated with the target image.\nHowever, previous inversion methods face challenges in adhering closely to the\ntarget text prompt. The limitation arises because inverted noise maps, while\nenabling faithful reconstruction of the source image, restrict the flexibility\nneeded for desired edits. To overcome this issue, we propose Editable Noise Map\nInversion (ENM Inversion), a novel inversion technique that searches for\noptimal noise maps to ensure both content preservation and editability. We\nanalyze the properties of noise maps for enhanced editability. Based on this\nanalysis, our method introduces an editable noise refinement that aligns with\nthe desired edits by minimizing the difference between the reconstructed and\nedited noise maps. Extensive experiments demonstrate that ENM Inversion\noutperforms existing approaches across a wide range of image editing tasks in\nboth preservation and edit fidelity with target prompts. Our approach can also\nbe easily applied to video editing, enabling temporal consistency and content\nmanipulation across frames.\n", "link": "http://arxiv.org/abs/2509.25776v2", "date": "2025-10-10", "relevancy": 2.3706, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6174}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5769}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Editable%20Noise%20Map%20Inversion%3A%20Encoding%20Target-image%20into%20Noise%20For%0A%20%20High-Fidelity%20Image%20Manipulation&body=Title%3A%20Editable%20Noise%20Map%20Inversion%3A%20Encoding%20Target-image%20into%20Noise%20For%0A%20%20High-Fidelity%20Image%20Manipulation%0AAuthor%3A%20Mingyu%20Kang%20and%20Yong%20Suk%20Choi%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20achieved%20remarkable%20success%20in%20generating%0Ahigh-quality%20and%20diverse%20images.%20Building%20on%20these%20advancements%2C%20diffusion%0Amodels%20have%20also%20demonstrated%20exceptional%20performance%20in%20text-guided%20image%0Aediting.%20A%20key%20strategy%20for%20effective%20image%20editing%20involves%20inverting%20the%0Asource%20image%20into%20editable%20noise%20maps%20associated%20with%20the%20target%20image.%0AHowever%2C%20previous%20inversion%20methods%20face%20challenges%20in%20adhering%20closely%20to%20the%0Atarget%20text%20prompt.%20The%20limitation%20arises%20because%20inverted%20noise%20maps%2C%20while%0Aenabling%20faithful%20reconstruction%20of%20the%20source%20image%2C%20restrict%20the%20flexibility%0Aneeded%20for%20desired%20edits.%20To%20overcome%20this%20issue%2C%20we%20propose%20Editable%20Noise%20Map%0AInversion%20%28ENM%20Inversion%29%2C%20a%20novel%20inversion%20technique%20that%20searches%20for%0Aoptimal%20noise%20maps%20to%20ensure%20both%20content%20preservation%20and%20editability.%20We%0Aanalyze%20the%20properties%20of%20noise%20maps%20for%20enhanced%20editability.%20Based%20on%20this%0Aanalysis%2C%20our%20method%20introduces%20an%20editable%20noise%20refinement%20that%20aligns%20with%0Athe%20desired%20edits%20by%20minimizing%20the%20difference%20between%20the%20reconstructed%20and%0Aedited%20noise%20maps.%20Extensive%20experiments%20demonstrate%20that%20ENM%20Inversion%0Aoutperforms%20existing%20approaches%20across%20a%20wide%20range%20of%20image%20editing%20tasks%20in%0Aboth%20preservation%20and%20edit%20fidelity%20with%20target%20prompts.%20Our%20approach%20can%20also%0Abe%20easily%20applied%20to%20video%20editing%2C%20enabling%20temporal%20consistency%20and%20content%0Amanipulation%20across%20frames.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25776v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEditable%2520Noise%2520Map%2520Inversion%253A%2520Encoding%2520Target-image%2520into%2520Noise%2520For%250A%2520%2520High-Fidelity%2520Image%2520Manipulation%26entry.906535625%3DMingyu%2520Kang%2520and%2520Yong%2520Suk%2520Choi%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520generating%250Ahigh-quality%2520and%2520diverse%2520images.%2520Building%2520on%2520these%2520advancements%252C%2520diffusion%250Amodels%2520have%2520also%2520demonstrated%2520exceptional%2520performance%2520in%2520text-guided%2520image%250Aediting.%2520A%2520key%2520strategy%2520for%2520effective%2520image%2520editing%2520involves%2520inverting%2520the%250Asource%2520image%2520into%2520editable%2520noise%2520maps%2520associated%2520with%2520the%2520target%2520image.%250AHowever%252C%2520previous%2520inversion%2520methods%2520face%2520challenges%2520in%2520adhering%2520closely%2520to%2520the%250Atarget%2520text%2520prompt.%2520The%2520limitation%2520arises%2520because%2520inverted%2520noise%2520maps%252C%2520while%250Aenabling%2520faithful%2520reconstruction%2520of%2520the%2520source%2520image%252C%2520restrict%2520the%2520flexibility%250Aneeded%2520for%2520desired%2520edits.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520propose%2520Editable%2520Noise%2520Map%250AInversion%2520%2528ENM%2520Inversion%2529%252C%2520a%2520novel%2520inversion%2520technique%2520that%2520searches%2520for%250Aoptimal%2520noise%2520maps%2520to%2520ensure%2520both%2520content%2520preservation%2520and%2520editability.%2520We%250Aanalyze%2520the%2520properties%2520of%2520noise%2520maps%2520for%2520enhanced%2520editability.%2520Based%2520on%2520this%250Aanalysis%252C%2520our%2520method%2520introduces%2520an%2520editable%2520noise%2520refinement%2520that%2520aligns%2520with%250Athe%2520desired%2520edits%2520by%2520minimizing%2520the%2520difference%2520between%2520the%2520reconstructed%2520and%250Aedited%2520noise%2520maps.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ENM%2520Inversion%250Aoutperforms%2520existing%2520approaches%2520across%2520a%2520wide%2520range%2520of%2520image%2520editing%2520tasks%2520in%250Aboth%2520preservation%2520and%2520edit%2520fidelity%2520with%2520target%2520prompts.%2520Our%2520approach%2520can%2520also%250Abe%2520easily%2520applied%2520to%2520video%2520editing%252C%2520enabling%2520temporal%2520consistency%2520and%2520content%250Amanipulation%2520across%2520frames.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25776v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Editable%20Noise%20Map%20Inversion%3A%20Encoding%20Target-image%20into%20Noise%20For%0A%20%20High-Fidelity%20Image%20Manipulation&entry.906535625=Mingyu%20Kang%20and%20Yong%20Suk%20Choi&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20achieved%20remarkable%20success%20in%20generating%0Ahigh-quality%20and%20diverse%20images.%20Building%20on%20these%20advancements%2C%20diffusion%0Amodels%20have%20also%20demonstrated%20exceptional%20performance%20in%20text-guided%20image%0Aediting.%20A%20key%20strategy%20for%20effective%20image%20editing%20involves%20inverting%20the%0Asource%20image%20into%20editable%20noise%20maps%20associated%20with%20the%20target%20image.%0AHowever%2C%20previous%20inversion%20methods%20face%20challenges%20in%20adhering%20closely%20to%20the%0Atarget%20text%20prompt.%20The%20limitation%20arises%20because%20inverted%20noise%20maps%2C%20while%0Aenabling%20faithful%20reconstruction%20of%20the%20source%20image%2C%20restrict%20the%20flexibility%0Aneeded%20for%20desired%20edits.%20To%20overcome%20this%20issue%2C%20we%20propose%20Editable%20Noise%20Map%0AInversion%20%28ENM%20Inversion%29%2C%20a%20novel%20inversion%20technique%20that%20searches%20for%0Aoptimal%20noise%20maps%20to%20ensure%20both%20content%20preservation%20and%20editability.%20We%0Aanalyze%20the%20properties%20of%20noise%20maps%20for%20enhanced%20editability.%20Based%20on%20this%0Aanalysis%2C%20our%20method%20introduces%20an%20editable%20noise%20refinement%20that%20aligns%20with%0Athe%20desired%20edits%20by%20minimizing%20the%20difference%20between%20the%20reconstructed%20and%0Aedited%20noise%20maps.%20Extensive%20experiments%20demonstrate%20that%20ENM%20Inversion%0Aoutperforms%20existing%20approaches%20across%20a%20wide%20range%20of%20image%20editing%20tasks%20in%0Aboth%20preservation%20and%20edit%20fidelity%20with%20target%20prompts.%20Our%20approach%20can%20also%0Abe%20easily%20applied%20to%20video%20editing%2C%20enabling%20temporal%20consistency%20and%20content%0Amanipulation%20across%20frames.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25776v2&entry.124074799=Read"},
{"title": "Learning Neural Exposure Fields for View Synthesis", "author": "Michael Niemeyer and Fabian Manhardt and Marie-Julie Rakotosaona and Michael Oechsle and Christina Tsalicoglou and Keisuke Tateno and Jonathan T. Barron and Federico Tombari", "abstract": "  Recent advances in neural scene representations have led to unprecedented\nquality in 3D reconstruction and view synthesis. Despite achieving high-quality\nresults for common benchmarks with curated data, outputs often degrade for data\nthat contain per image variations such as strong exposure changes, present,\ne.g., in most scenes with indoor and outdoor areas or rooms with windows. In\nthis paper, we introduce Neural Exposure Fields (NExF), a novel technique for\nrobustly reconstructing 3D scenes with high quality and 3D-consistent\nappearance from challenging real-world captures. In the core, we propose to\nlearn a neural field predicting an optimal exposure value per 3D point,\nenabling us to optimize exposure along with the neural scene representation.\nWhile capture devices such as cameras select optimal exposure per image/pixel,\nwe generalize this concept and perform optimization in 3D instead. This enables\naccurate view synthesis in high dynamic range scenarios, bypassing the need of\npost-processing steps or multi-exposure captures. Our contributions include a\nnovel neural representation for exposure prediction, a system for joint\noptimization of the scene representation and the exposure field via a novel\nneural conditioning mechanism, and demonstrated superior performance on\nchallenging real-world data. We find that our approach trains faster than prior\nworks and produces state-of-the-art results on several benchmarks improving by\nover 55% over best-performing baselines.\n", "link": "http://arxiv.org/abs/2510.08279v2", "date": "2025-10-10", "relevancy": 2.3697, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Neural%20Exposure%20Fields%20for%20View%20Synthesis&body=Title%3A%20Learning%20Neural%20Exposure%20Fields%20for%20View%20Synthesis%0AAuthor%3A%20Michael%20Niemeyer%20and%20Fabian%20Manhardt%20and%20Marie-Julie%20Rakotosaona%20and%20Michael%20Oechsle%20and%20Christina%20Tsalicoglou%20and%20Keisuke%20Tateno%20and%20Jonathan%20T.%20Barron%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Recent%20advances%20in%20neural%20scene%20representations%20have%20led%20to%20unprecedented%0Aquality%20in%203D%20reconstruction%20and%20view%20synthesis.%20Despite%20achieving%20high-quality%0Aresults%20for%20common%20benchmarks%20with%20curated%20data%2C%20outputs%20often%20degrade%20for%20data%0Athat%20contain%20per%20image%20variations%20such%20as%20strong%20exposure%20changes%2C%20present%2C%0Ae.g.%2C%20in%20most%20scenes%20with%20indoor%20and%20outdoor%20areas%20or%20rooms%20with%20windows.%20In%0Athis%20paper%2C%20we%20introduce%20Neural%20Exposure%20Fields%20%28NExF%29%2C%20a%20novel%20technique%20for%0Arobustly%20reconstructing%203D%20scenes%20with%20high%20quality%20and%203D-consistent%0Aappearance%20from%20challenging%20real-world%20captures.%20In%20the%20core%2C%20we%20propose%20to%0Alearn%20a%20neural%20field%20predicting%20an%20optimal%20exposure%20value%20per%203D%20point%2C%0Aenabling%20us%20to%20optimize%20exposure%20along%20with%20the%20neural%20scene%20representation.%0AWhile%20capture%20devices%20such%20as%20cameras%20select%20optimal%20exposure%20per%20image/pixel%2C%0Awe%20generalize%20this%20concept%20and%20perform%20optimization%20in%203D%20instead.%20This%20enables%0Aaccurate%20view%20synthesis%20in%20high%20dynamic%20range%20scenarios%2C%20bypassing%20the%20need%20of%0Apost-processing%20steps%20or%20multi-exposure%20captures.%20Our%20contributions%20include%20a%0Anovel%20neural%20representation%20for%20exposure%20prediction%2C%20a%20system%20for%20joint%0Aoptimization%20of%20the%20scene%20representation%20and%20the%20exposure%20field%20via%20a%20novel%0Aneural%20conditioning%20mechanism%2C%20and%20demonstrated%20superior%20performance%20on%0Achallenging%20real-world%20data.%20We%20find%20that%20our%20approach%20trains%20faster%20than%20prior%0Aworks%20and%20produces%20state-of-the-art%20results%20on%20several%20benchmarks%20improving%20by%0Aover%2055%25%20over%20best-performing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Neural%2520Exposure%2520Fields%2520for%2520View%2520Synthesis%26entry.906535625%3DMichael%2520Niemeyer%2520and%2520Fabian%2520Manhardt%2520and%2520Marie-Julie%2520Rakotosaona%2520and%2520Michael%2520Oechsle%2520and%2520Christina%2520Tsalicoglou%2520and%2520Keisuke%2520Tateno%2520and%2520Jonathan%2520T.%2520Barron%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520neural%2520scene%2520representations%2520have%2520led%2520to%2520unprecedented%250Aquality%2520in%25203D%2520reconstruction%2520and%2520view%2520synthesis.%2520Despite%2520achieving%2520high-quality%250Aresults%2520for%2520common%2520benchmarks%2520with%2520curated%2520data%252C%2520outputs%2520often%2520degrade%2520for%2520data%250Athat%2520contain%2520per%2520image%2520variations%2520such%2520as%2520strong%2520exposure%2520changes%252C%2520present%252C%250Ae.g.%252C%2520in%2520most%2520scenes%2520with%2520indoor%2520and%2520outdoor%2520areas%2520or%2520rooms%2520with%2520windows.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520Neural%2520Exposure%2520Fields%2520%2528NExF%2529%252C%2520a%2520novel%2520technique%2520for%250Arobustly%2520reconstructing%25203D%2520scenes%2520with%2520high%2520quality%2520and%25203D-consistent%250Aappearance%2520from%2520challenging%2520real-world%2520captures.%2520In%2520the%2520core%252C%2520we%2520propose%2520to%250Alearn%2520a%2520neural%2520field%2520predicting%2520an%2520optimal%2520exposure%2520value%2520per%25203D%2520point%252C%250Aenabling%2520us%2520to%2520optimize%2520exposure%2520along%2520with%2520the%2520neural%2520scene%2520representation.%250AWhile%2520capture%2520devices%2520such%2520as%2520cameras%2520select%2520optimal%2520exposure%2520per%2520image/pixel%252C%250Awe%2520generalize%2520this%2520concept%2520and%2520perform%2520optimization%2520in%25203D%2520instead.%2520This%2520enables%250Aaccurate%2520view%2520synthesis%2520in%2520high%2520dynamic%2520range%2520scenarios%252C%2520bypassing%2520the%2520need%2520of%250Apost-processing%2520steps%2520or%2520multi-exposure%2520captures.%2520Our%2520contributions%2520include%2520a%250Anovel%2520neural%2520representation%2520for%2520exposure%2520prediction%252C%2520a%2520system%2520for%2520joint%250Aoptimization%2520of%2520the%2520scene%2520representation%2520and%2520the%2520exposure%2520field%2520via%2520a%2520novel%250Aneural%2520conditioning%2520mechanism%252C%2520and%2520demonstrated%2520superior%2520performance%2520on%250Achallenging%2520real-world%2520data.%2520We%2520find%2520that%2520our%2520approach%2520trains%2520faster%2520than%2520prior%250Aworks%2520and%2520produces%2520state-of-the-art%2520results%2520on%2520several%2520benchmarks%2520improving%2520by%250Aover%252055%2525%2520over%2520best-performing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Neural%20Exposure%20Fields%20for%20View%20Synthesis&entry.906535625=Michael%20Niemeyer%20and%20Fabian%20Manhardt%20and%20Marie-Julie%20Rakotosaona%20and%20Michael%20Oechsle%20and%20Christina%20Tsalicoglou%20and%20Keisuke%20Tateno%20and%20Jonathan%20T.%20Barron%20and%20Federico%20Tombari&entry.1292438233=%20%20Recent%20advances%20in%20neural%20scene%20representations%20have%20led%20to%20unprecedented%0Aquality%20in%203D%20reconstruction%20and%20view%20synthesis.%20Despite%20achieving%20high-quality%0Aresults%20for%20common%20benchmarks%20with%20curated%20data%2C%20outputs%20often%20degrade%20for%20data%0Athat%20contain%20per%20image%20variations%20such%20as%20strong%20exposure%20changes%2C%20present%2C%0Ae.g.%2C%20in%20most%20scenes%20with%20indoor%20and%20outdoor%20areas%20or%20rooms%20with%20windows.%20In%0Athis%20paper%2C%20we%20introduce%20Neural%20Exposure%20Fields%20%28NExF%29%2C%20a%20novel%20technique%20for%0Arobustly%20reconstructing%203D%20scenes%20with%20high%20quality%20and%203D-consistent%0Aappearance%20from%20challenging%20real-world%20captures.%20In%20the%20core%2C%20we%20propose%20to%0Alearn%20a%20neural%20field%20predicting%20an%20optimal%20exposure%20value%20per%203D%20point%2C%0Aenabling%20us%20to%20optimize%20exposure%20along%20with%20the%20neural%20scene%20representation.%0AWhile%20capture%20devices%20such%20as%20cameras%20select%20optimal%20exposure%20per%20image/pixel%2C%0Awe%20generalize%20this%20concept%20and%20perform%20optimization%20in%203D%20instead.%20This%20enables%0Aaccurate%20view%20synthesis%20in%20high%20dynamic%20range%20scenarios%2C%20bypassing%20the%20need%20of%0Apost-processing%20steps%20or%20multi-exposure%20captures.%20Our%20contributions%20include%20a%0Anovel%20neural%20representation%20for%20exposure%20prediction%2C%20a%20system%20for%20joint%0Aoptimization%20of%20the%20scene%20representation%20and%20the%20exposure%20field%20via%20a%20novel%0Aneural%20conditioning%20mechanism%2C%20and%20demonstrated%20superior%20performance%20on%0Achallenging%20real-world%20data.%20We%20find%20that%20our%20approach%20trains%20faster%20than%20prior%0Aworks%20and%20produces%20state-of-the-art%20results%20on%20several%20benchmarks%20improving%20by%0Aover%2055%25%20over%20best-performing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08279v2&entry.124074799=Read"},
{"title": "Zero-shot Structure Learning and Planning for Autonomous Robot\n  Navigation using Active Inference", "author": "Daria de tinguy and Tim Verbelen and Emilio Gamba and Bart Dhoedt", "abstract": "  Autonomous navigation in unfamiliar environments requires robots to\nsimultaneously explore, localise, and plan under uncertainty, without relying\non predefined maps or extensive training. We present a biologically inspired,\nActive Inference-based framework, Active Inference MAPping and Planning\n(AIMAPP). This model unifies mapping, localisation, and decision-making within\na single generative model. Inspired by hippocampal navigation, it uses\ntopological reasoning, place-cell encoding, and episodic memory to guide\nbehaviour. The agent builds and updates a sparse topological map online, learns\nstate transitions dynamically, and plans actions by minimising Expected Free\nEnergy. This allows it to balance goal-directed and exploratory behaviours. We\nimplemented a ROS-compatible navigation system that is sensor and\nrobot-agnostic, capable of integrating with diverse hardware configurations. It\noperates in a fully self-supervised manner, is resilient to drift, and supports\nboth exploration and goal-directed navigation without any pre-training. We\ndemonstrate robust performance in large-scale real and simulated environments\nagainst state-of-the-art planning models, highlighting the system's\nadaptability to ambiguous observations, environmental changes, and sensor\nnoise. The model offers a biologically inspired, modular solution to scalable,\nself-supervised navigation in unstructured settings. AIMAPP is available at\nhttps://github.com/decide-ugent/AIMAPP.\n", "link": "http://arxiv.org/abs/2510.09574v1", "date": "2025-10-10", "relevancy": 2.365, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6069}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5884}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Structure%20Learning%20and%20Planning%20for%20Autonomous%20Robot%0A%20%20Navigation%20using%20Active%20Inference&body=Title%3A%20Zero-shot%20Structure%20Learning%20and%20Planning%20for%20Autonomous%20Robot%0A%20%20Navigation%20using%20Active%20Inference%0AAuthor%3A%20Daria%20de%20tinguy%20and%20Tim%20Verbelen%20and%20Emilio%20Gamba%20and%20Bart%20Dhoedt%0AAbstract%3A%20%20%20Autonomous%20navigation%20in%20unfamiliar%20environments%20requires%20robots%20to%0Asimultaneously%20explore%2C%20localise%2C%20and%20plan%20under%20uncertainty%2C%20without%20relying%0Aon%20predefined%20maps%20or%20extensive%20training.%20We%20present%20a%20biologically%20inspired%2C%0AActive%20Inference-based%20framework%2C%20Active%20Inference%20MAPping%20and%20Planning%0A%28AIMAPP%29.%20This%20model%20unifies%20mapping%2C%20localisation%2C%20and%20decision-making%20within%0Aa%20single%20generative%20model.%20Inspired%20by%20hippocampal%20navigation%2C%20it%20uses%0Atopological%20reasoning%2C%20place-cell%20encoding%2C%20and%20episodic%20memory%20to%20guide%0Abehaviour.%20The%20agent%20builds%20and%20updates%20a%20sparse%20topological%20map%20online%2C%20learns%0Astate%20transitions%20dynamically%2C%20and%20plans%20actions%20by%20minimising%20Expected%20Free%0AEnergy.%20This%20allows%20it%20to%20balance%20goal-directed%20and%20exploratory%20behaviours.%20We%0Aimplemented%20a%20ROS-compatible%20navigation%20system%20that%20is%20sensor%20and%0Arobot-agnostic%2C%20capable%20of%20integrating%20with%20diverse%20hardware%20configurations.%20It%0Aoperates%20in%20a%20fully%20self-supervised%20manner%2C%20is%20resilient%20to%20drift%2C%20and%20supports%0Aboth%20exploration%20and%20goal-directed%20navigation%20without%20any%20pre-training.%20We%0Ademonstrate%20robust%20performance%20in%20large-scale%20real%20and%20simulated%20environments%0Aagainst%20state-of-the-art%20planning%20models%2C%20highlighting%20the%20system%27s%0Aadaptability%20to%20ambiguous%20observations%2C%20environmental%20changes%2C%20and%20sensor%0Anoise.%20The%20model%20offers%20a%20biologically%20inspired%2C%20modular%20solution%20to%20scalable%2C%0Aself-supervised%20navigation%20in%20unstructured%20settings.%20AIMAPP%20is%20available%20at%0Ahttps%3A//github.com/decide-ugent/AIMAPP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Structure%2520Learning%2520and%2520Planning%2520for%2520Autonomous%2520Robot%250A%2520%2520Navigation%2520using%2520Active%2520Inference%26entry.906535625%3DDaria%2520de%2520tinguy%2520and%2520Tim%2520Verbelen%2520and%2520Emilio%2520Gamba%2520and%2520Bart%2520Dhoedt%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520in%2520unfamiliar%2520environments%2520requires%2520robots%2520to%250Asimultaneously%2520explore%252C%2520localise%252C%2520and%2520plan%2520under%2520uncertainty%252C%2520without%2520relying%250Aon%2520predefined%2520maps%2520or%2520extensive%2520training.%2520We%2520present%2520a%2520biologically%2520inspired%252C%250AActive%2520Inference-based%2520framework%252C%2520Active%2520Inference%2520MAPping%2520and%2520Planning%250A%2528AIMAPP%2529.%2520This%2520model%2520unifies%2520mapping%252C%2520localisation%252C%2520and%2520decision-making%2520within%250Aa%2520single%2520generative%2520model.%2520Inspired%2520by%2520hippocampal%2520navigation%252C%2520it%2520uses%250Atopological%2520reasoning%252C%2520place-cell%2520encoding%252C%2520and%2520episodic%2520memory%2520to%2520guide%250Abehaviour.%2520The%2520agent%2520builds%2520and%2520updates%2520a%2520sparse%2520topological%2520map%2520online%252C%2520learns%250Astate%2520transitions%2520dynamically%252C%2520and%2520plans%2520actions%2520by%2520minimising%2520Expected%2520Free%250AEnergy.%2520This%2520allows%2520it%2520to%2520balance%2520goal-directed%2520and%2520exploratory%2520behaviours.%2520We%250Aimplemented%2520a%2520ROS-compatible%2520navigation%2520system%2520that%2520is%2520sensor%2520and%250Arobot-agnostic%252C%2520capable%2520of%2520integrating%2520with%2520diverse%2520hardware%2520configurations.%2520It%250Aoperates%2520in%2520a%2520fully%2520self-supervised%2520manner%252C%2520is%2520resilient%2520to%2520drift%252C%2520and%2520supports%250Aboth%2520exploration%2520and%2520goal-directed%2520navigation%2520without%2520any%2520pre-training.%2520We%250Ademonstrate%2520robust%2520performance%2520in%2520large-scale%2520real%2520and%2520simulated%2520environments%250Aagainst%2520state-of-the-art%2520planning%2520models%252C%2520highlighting%2520the%2520system%2527s%250Aadaptability%2520to%2520ambiguous%2520observations%252C%2520environmental%2520changes%252C%2520and%2520sensor%250Anoise.%2520The%2520model%2520offers%2520a%2520biologically%2520inspired%252C%2520modular%2520solution%2520to%2520scalable%252C%250Aself-supervised%2520navigation%2520in%2520unstructured%2520settings.%2520AIMAPP%2520is%2520available%2520at%250Ahttps%253A//github.com/decide-ugent/AIMAPP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Structure%20Learning%20and%20Planning%20for%20Autonomous%20Robot%0A%20%20Navigation%20using%20Active%20Inference&entry.906535625=Daria%20de%20tinguy%20and%20Tim%20Verbelen%20and%20Emilio%20Gamba%20and%20Bart%20Dhoedt&entry.1292438233=%20%20Autonomous%20navigation%20in%20unfamiliar%20environments%20requires%20robots%20to%0Asimultaneously%20explore%2C%20localise%2C%20and%20plan%20under%20uncertainty%2C%20without%20relying%0Aon%20predefined%20maps%20or%20extensive%20training.%20We%20present%20a%20biologically%20inspired%2C%0AActive%20Inference-based%20framework%2C%20Active%20Inference%20MAPping%20and%20Planning%0A%28AIMAPP%29.%20This%20model%20unifies%20mapping%2C%20localisation%2C%20and%20decision-making%20within%0Aa%20single%20generative%20model.%20Inspired%20by%20hippocampal%20navigation%2C%20it%20uses%0Atopological%20reasoning%2C%20place-cell%20encoding%2C%20and%20episodic%20memory%20to%20guide%0Abehaviour.%20The%20agent%20builds%20and%20updates%20a%20sparse%20topological%20map%20online%2C%20learns%0Astate%20transitions%20dynamically%2C%20and%20plans%20actions%20by%20minimising%20Expected%20Free%0AEnergy.%20This%20allows%20it%20to%20balance%20goal-directed%20and%20exploratory%20behaviours.%20We%0Aimplemented%20a%20ROS-compatible%20navigation%20system%20that%20is%20sensor%20and%0Arobot-agnostic%2C%20capable%20of%20integrating%20with%20diverse%20hardware%20configurations.%20It%0Aoperates%20in%20a%20fully%20self-supervised%20manner%2C%20is%20resilient%20to%20drift%2C%20and%20supports%0Aboth%20exploration%20and%20goal-directed%20navigation%20without%20any%20pre-training.%20We%0Ademonstrate%20robust%20performance%20in%20large-scale%20real%20and%20simulated%20environments%0Aagainst%20state-of-the-art%20planning%20models%2C%20highlighting%20the%20system%27s%0Aadaptability%20to%20ambiguous%20observations%2C%20environmental%20changes%2C%20and%20sensor%0Anoise.%20The%20model%20offers%20a%20biologically%20inspired%2C%20modular%20solution%20to%20scalable%2C%0Aself-supervised%20navigation%20in%20unstructured%20settings.%20AIMAPP%20is%20available%20at%0Ahttps%3A//github.com/decide-ugent/AIMAPP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09574v1&entry.124074799=Read"},
{"title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes", "author": "Chen Sun and Haiyang Sun and Zhiqing Guo and Yunfeng Diao and Liejun Wang and Dan Ma and Gaobo Yang and Keqin Li", "abstract": "  Deepfakes pose significant security and privacy threats through malicious\nfacial manipulations. While robust watermarking can aid in authenticity\nverification and source tracking, existing methods often lack the sufficient\nrobustness against Deepfake manipulations. Diffusion models have demonstrated\nremarkable performance in image generation, enabling the seamless fusion of\nwatermark with image during generation. In this study, we propose a novel\nrobust watermarking framework based on diffusion model, called DiffMark. By\nmodifying the training and sampling scheme, we take the facial image and\nwatermark as conditions to guide the diffusion model to progressively denoise\nand generate corresponding watermarked image. In the construction of facial\ncondition, we weight the facial image by a timestep-dependent factor that\ngradually reduces the guidance intensity with the decrease of noise, thus\nbetter adapting to the sampling process of diffusion model. To achieve the\nfusion of watermark condition, we introduce a cross information fusion (CIF)\nmodule that leverages a learnable embedding table to adaptively extract\nwatermark features and integrates them with image features via cross-attention.\nTo enhance the robustness of the watermark against Deepfake manipulations, we\nintegrate a frozen autoencoder during training phase to simulate Deepfake\nmanipulations. Additionally, we introduce Deepfake-resistant guidance that\nemploys specific Deepfake model to adversarially guide the diffusion sampling\nprocess to generate more robust watermarked images. Experimental results\ndemonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.\nOur code will be available at https://github.com/vpsg-research/DiffMark.\n", "link": "http://arxiv.org/abs/2507.01428v2", "date": "2025-10-10", "relevancy": 2.3431, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6034}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5742}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffMark%3A%20Diffusion-based%20Robust%20Watermark%20Against%20Deepfakes&body=Title%3A%20DiffMark%3A%20Diffusion-based%20Robust%20Watermark%20Against%20Deepfakes%0AAuthor%3A%20Chen%20Sun%20and%20Haiyang%20Sun%20and%20Zhiqing%20Guo%20and%20Yunfeng%20Diao%20and%20Liejun%20Wang%20and%20Dan%20Ma%20and%20Gaobo%20Yang%20and%20Keqin%20Li%0AAbstract%3A%20%20%20Deepfakes%20pose%20significant%20security%20and%20privacy%20threats%20through%20malicious%0Afacial%20manipulations.%20While%20robust%20watermarking%20can%20aid%20in%20authenticity%0Averification%20and%20source%20tracking%2C%20existing%20methods%20often%20lack%20the%20sufficient%0Arobustness%20against%20Deepfake%20manipulations.%20Diffusion%20models%20have%20demonstrated%0Aremarkable%20performance%20in%20image%20generation%2C%20enabling%20the%20seamless%20fusion%20of%0Awatermark%20with%20image%20during%20generation.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Arobust%20watermarking%20framework%20based%20on%20diffusion%20model%2C%20called%20DiffMark.%20By%0Amodifying%20the%20training%20and%20sampling%20scheme%2C%20we%20take%20the%20facial%20image%20and%0Awatermark%20as%20conditions%20to%20guide%20the%20diffusion%20model%20to%20progressively%20denoise%0Aand%20generate%20corresponding%20watermarked%20image.%20In%20the%20construction%20of%20facial%0Acondition%2C%20we%20weight%20the%20facial%20image%20by%20a%20timestep-dependent%20factor%20that%0Agradually%20reduces%20the%20guidance%20intensity%20with%20the%20decrease%20of%20noise%2C%20thus%0Abetter%20adapting%20to%20the%20sampling%20process%20of%20diffusion%20model.%20To%20achieve%20the%0Afusion%20of%20watermark%20condition%2C%20we%20introduce%20a%20cross%20information%20fusion%20%28CIF%29%0Amodule%20that%20leverages%20a%20learnable%20embedding%20table%20to%20adaptively%20extract%0Awatermark%20features%20and%20integrates%20them%20with%20image%20features%20via%20cross-attention.%0ATo%20enhance%20the%20robustness%20of%20the%20watermark%20against%20Deepfake%20manipulations%2C%20we%0Aintegrate%20a%20frozen%20autoencoder%20during%20training%20phase%20to%20simulate%20Deepfake%0Amanipulations.%20Additionally%2C%20we%20introduce%20Deepfake-resistant%20guidance%20that%0Aemploys%20specific%20Deepfake%20model%20to%20adversarially%20guide%20the%20diffusion%20sampling%0Aprocess%20to%20generate%20more%20robust%20watermarked%20images.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20DiffMark%20on%20typical%20Deepfakes.%0AOur%20code%20will%20be%20available%20at%20https%3A//github.com/vpsg-research/DiffMark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01428v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffMark%253A%2520Diffusion-based%2520Robust%2520Watermark%2520Against%2520Deepfakes%26entry.906535625%3DChen%2520Sun%2520and%2520Haiyang%2520Sun%2520and%2520Zhiqing%2520Guo%2520and%2520Yunfeng%2520Diao%2520and%2520Liejun%2520Wang%2520and%2520Dan%2520Ma%2520and%2520Gaobo%2520Yang%2520and%2520Keqin%2520Li%26entry.1292438233%3D%2520%2520Deepfakes%2520pose%2520significant%2520security%2520and%2520privacy%2520threats%2520through%2520malicious%250Afacial%2520manipulations.%2520While%2520robust%2520watermarking%2520can%2520aid%2520in%2520authenticity%250Averification%2520and%2520source%2520tracking%252C%2520existing%2520methods%2520often%2520lack%2520the%2520sufficient%250Arobustness%2520against%2520Deepfake%2520manipulations.%2520Diffusion%2520models%2520have%2520demonstrated%250Aremarkable%2520performance%2520in%2520image%2520generation%252C%2520enabling%2520the%2520seamless%2520fusion%2520of%250Awatermark%2520with%2520image%2520during%2520generation.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%250Arobust%2520watermarking%2520framework%2520based%2520on%2520diffusion%2520model%252C%2520called%2520DiffMark.%2520By%250Amodifying%2520the%2520training%2520and%2520sampling%2520scheme%252C%2520we%2520take%2520the%2520facial%2520image%2520and%250Awatermark%2520as%2520conditions%2520to%2520guide%2520the%2520diffusion%2520model%2520to%2520progressively%2520denoise%250Aand%2520generate%2520corresponding%2520watermarked%2520image.%2520In%2520the%2520construction%2520of%2520facial%250Acondition%252C%2520we%2520weight%2520the%2520facial%2520image%2520by%2520a%2520timestep-dependent%2520factor%2520that%250Agradually%2520reduces%2520the%2520guidance%2520intensity%2520with%2520the%2520decrease%2520of%2520noise%252C%2520thus%250Abetter%2520adapting%2520to%2520the%2520sampling%2520process%2520of%2520diffusion%2520model.%2520To%2520achieve%2520the%250Afusion%2520of%2520watermark%2520condition%252C%2520we%2520introduce%2520a%2520cross%2520information%2520fusion%2520%2528CIF%2529%250Amodule%2520that%2520leverages%2520a%2520learnable%2520embedding%2520table%2520to%2520adaptively%2520extract%250Awatermark%2520features%2520and%2520integrates%2520them%2520with%2520image%2520features%2520via%2520cross-attention.%250ATo%2520enhance%2520the%2520robustness%2520of%2520the%2520watermark%2520against%2520Deepfake%2520manipulations%252C%2520we%250Aintegrate%2520a%2520frozen%2520autoencoder%2520during%2520training%2520phase%2520to%2520simulate%2520Deepfake%250Amanipulations.%2520Additionally%252C%2520we%2520introduce%2520Deepfake-resistant%2520guidance%2520that%250Aemploys%2520specific%2520Deepfake%2520model%2520to%2520adversarially%2520guide%2520the%2520diffusion%2520sampling%250Aprocess%2520to%2520generate%2520more%2520robust%2520watermarked%2520images.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520DiffMark%2520on%2520typical%2520Deepfakes.%250AOur%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/vpsg-research/DiffMark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01428v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffMark%3A%20Diffusion-based%20Robust%20Watermark%20Against%20Deepfakes&entry.906535625=Chen%20Sun%20and%20Haiyang%20Sun%20and%20Zhiqing%20Guo%20and%20Yunfeng%20Diao%20and%20Liejun%20Wang%20and%20Dan%20Ma%20and%20Gaobo%20Yang%20and%20Keqin%20Li&entry.1292438233=%20%20Deepfakes%20pose%20significant%20security%20and%20privacy%20threats%20through%20malicious%0Afacial%20manipulations.%20While%20robust%20watermarking%20can%20aid%20in%20authenticity%0Averification%20and%20source%20tracking%2C%20existing%20methods%20often%20lack%20the%20sufficient%0Arobustness%20against%20Deepfake%20manipulations.%20Diffusion%20models%20have%20demonstrated%0Aremarkable%20performance%20in%20image%20generation%2C%20enabling%20the%20seamless%20fusion%20of%0Awatermark%20with%20image%20during%20generation.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Arobust%20watermarking%20framework%20based%20on%20diffusion%20model%2C%20called%20DiffMark.%20By%0Amodifying%20the%20training%20and%20sampling%20scheme%2C%20we%20take%20the%20facial%20image%20and%0Awatermark%20as%20conditions%20to%20guide%20the%20diffusion%20model%20to%20progressively%20denoise%0Aand%20generate%20corresponding%20watermarked%20image.%20In%20the%20construction%20of%20facial%0Acondition%2C%20we%20weight%20the%20facial%20image%20by%20a%20timestep-dependent%20factor%20that%0Agradually%20reduces%20the%20guidance%20intensity%20with%20the%20decrease%20of%20noise%2C%20thus%0Abetter%20adapting%20to%20the%20sampling%20process%20of%20diffusion%20model.%20To%20achieve%20the%0Afusion%20of%20watermark%20condition%2C%20we%20introduce%20a%20cross%20information%20fusion%20%28CIF%29%0Amodule%20that%20leverages%20a%20learnable%20embedding%20table%20to%20adaptively%20extract%0Awatermark%20features%20and%20integrates%20them%20with%20image%20features%20via%20cross-attention.%0ATo%20enhance%20the%20robustness%20of%20the%20watermark%20against%20Deepfake%20manipulations%2C%20we%0Aintegrate%20a%20frozen%20autoencoder%20during%20training%20phase%20to%20simulate%20Deepfake%0Amanipulations.%20Additionally%2C%20we%20introduce%20Deepfake-resistant%20guidance%20that%0Aemploys%20specific%20Deepfake%20model%20to%20adversarially%20guide%20the%20diffusion%20sampling%0Aprocess%20to%20generate%20more%20robust%20watermarked%20images.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20DiffMark%20on%20typical%20Deepfakes.%0AOur%20code%20will%20be%20available%20at%20https%3A//github.com/vpsg-research/DiffMark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01428v2&entry.124074799=Read"},
{"title": "Unified Multimodal Model as Auto-Encoder", "author": "Zhiyuan Yan and Kaiqing Lin and Zongjian Li and Junyan Ye and Hui Han and Zhendong Wang and Hao Liu and Bin Lin and Hao Li and Xue Xu and Xinyan Xiao and Jingdong Wang and Haifeng Wang and Li Yuan", "abstract": "  The pursuit of unified multimodal models (UMMs) has long been hindered by a\nfundamental schism between multimodal understanding and generation. Current\napproaches typically disentangle the two and treat them as separate endeavors\nwith disjoint objectives, missing the mutual benefits. We argue that true\nunification requires more than just merging two tasks. It requires a unified,\nfoundational objective that intrinsically links them. In this paper, we\nintroduce an insightful paradigm through the Auto-Encoder lens, i.e., regarding\nunderstanding as the encoder (I2T) that compresses images into text, and\ngeneration as the decoder (T2I) that reconstructs images from that text. To\nimplement this, we propose UAE, where we begin by pre-training the decoder with\nthe proposed 700k long-context image-caption pairs to direct it to \"understand\"\nthe fine-grained and complex semantics from the text. We then propose\nUnified-GRPO via reinforcement learning (RL) to unify the two, which covers two\ncomplementary stages: (1) Generation for Understanding, where the encoder is\ntrained to generate informative captions that maximize the decoder's\nreconstruction quality, enhancing its visual perception; (2) Understanding for\nGeneration, where the decoder is refined to reconstruct from these captions,\nforcing it to leverage every detail and improving its long-context instruction\nfollowing and generation fidelity. Our empirical results suggest that\nunderstanding can largely enhance generation (verified on GenEval), while\ngeneration, in turn, notably strengthens fine-grained visual perception like\nsmall object and color recognition (verified on MMT-Bench). This bidirectional\nimprovement reveals a deep synergy: under the unified reconstruction objective,\ngeneration and understanding can mutually benefit each other, moving closer to\ntruly unified multimodal intelligence.\n", "link": "http://arxiv.org/abs/2509.09666v3", "date": "2025-10-10", "relevancy": 2.322, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6005}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Multimodal%20Model%20as%20Auto-Encoder&body=Title%3A%20Unified%20Multimodal%20Model%20as%20Auto-Encoder%0AAuthor%3A%20Zhiyuan%20Yan%20and%20Kaiqing%20Lin%20and%20Zongjian%20Li%20and%20Junyan%20Ye%20and%20Hui%20Han%20and%20Zhendong%20Wang%20and%20Hao%20Liu%20and%20Bin%20Lin%20and%20Hao%20Li%20and%20Xue%20Xu%20and%20Xinyan%20Xiao%20and%20Jingdong%20Wang%20and%20Haifeng%20Wang%20and%20Li%20Yuan%0AAbstract%3A%20%20%20The%20pursuit%20of%20unified%20multimodal%20models%20%28UMMs%29%20has%20long%20been%20hindered%20by%20a%0Afundamental%20schism%20between%20multimodal%20understanding%20and%20generation.%20Current%0Aapproaches%20typically%20disentangle%20the%20two%20and%20treat%20them%20as%20separate%20endeavors%0Awith%20disjoint%20objectives%2C%20missing%20the%20mutual%20benefits.%20We%20argue%20that%20true%0Aunification%20requires%20more%20than%20just%20merging%20two%20tasks.%20It%20requires%20a%20unified%2C%0Afoundational%20objective%20that%20intrinsically%20links%20them.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20insightful%20paradigm%20through%20the%20Auto-Encoder%20lens%2C%20i.e.%2C%20regarding%0Aunderstanding%20as%20the%20encoder%20%28I2T%29%20that%20compresses%20images%20into%20text%2C%20and%0Ageneration%20as%20the%20decoder%20%28T2I%29%20that%20reconstructs%20images%20from%20that%20text.%20To%0Aimplement%20this%2C%20we%20propose%20UAE%2C%20where%20we%20begin%20by%20pre-training%20the%20decoder%20with%0Athe%20proposed%20700k%20long-context%20image-caption%20pairs%20to%20direct%20it%20to%20%22understand%22%0Athe%20fine-grained%20and%20complex%20semantics%20from%20the%20text.%20We%20then%20propose%0AUnified-GRPO%20via%20reinforcement%20learning%20%28RL%29%20to%20unify%20the%20two%2C%20which%20covers%20two%0Acomplementary%20stages%3A%20%281%29%20Generation%20for%20Understanding%2C%20where%20the%20encoder%20is%0Atrained%20to%20generate%20informative%20captions%20that%20maximize%20the%20decoder%27s%0Areconstruction%20quality%2C%20enhancing%20its%20visual%20perception%3B%20%282%29%20Understanding%20for%0AGeneration%2C%20where%20the%20decoder%20is%20refined%20to%20reconstruct%20from%20these%20captions%2C%0Aforcing%20it%20to%20leverage%20every%20detail%20and%20improving%20its%20long-context%20instruction%0Afollowing%20and%20generation%20fidelity.%20Our%20empirical%20results%20suggest%20that%0Aunderstanding%20can%20largely%20enhance%20generation%20%28verified%20on%20GenEval%29%2C%20while%0Ageneration%2C%20in%20turn%2C%20notably%20strengthens%20fine-grained%20visual%20perception%20like%0Asmall%20object%20and%20color%20recognition%20%28verified%20on%20MMT-Bench%29.%20This%20bidirectional%0Aimprovement%20reveals%20a%20deep%20synergy%3A%20under%20the%20unified%20reconstruction%20objective%2C%0Ageneration%20and%20understanding%20can%20mutually%20benefit%20each%20other%2C%20moving%20closer%20to%0Atruly%20unified%20multimodal%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09666v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Multimodal%2520Model%2520as%2520Auto-Encoder%26entry.906535625%3DZhiyuan%2520Yan%2520and%2520Kaiqing%2520Lin%2520and%2520Zongjian%2520Li%2520and%2520Junyan%2520Ye%2520and%2520Hui%2520Han%2520and%2520Zhendong%2520Wang%2520and%2520Hao%2520Liu%2520and%2520Bin%2520Lin%2520and%2520Hao%2520Li%2520and%2520Xue%2520Xu%2520and%2520Xinyan%2520Xiao%2520and%2520Jingdong%2520Wang%2520and%2520Haifeng%2520Wang%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520The%2520pursuit%2520of%2520unified%2520multimodal%2520models%2520%2528UMMs%2529%2520has%2520long%2520been%2520hindered%2520by%2520a%250Afundamental%2520schism%2520between%2520multimodal%2520understanding%2520and%2520generation.%2520Current%250Aapproaches%2520typically%2520disentangle%2520the%2520two%2520and%2520treat%2520them%2520as%2520separate%2520endeavors%250Awith%2520disjoint%2520objectives%252C%2520missing%2520the%2520mutual%2520benefits.%2520We%2520argue%2520that%2520true%250Aunification%2520requires%2520more%2520than%2520just%2520merging%2520two%2520tasks.%2520It%2520requires%2520a%2520unified%252C%250Afoundational%2520objective%2520that%2520intrinsically%2520links%2520them.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520an%2520insightful%2520paradigm%2520through%2520the%2520Auto-Encoder%2520lens%252C%2520i.e.%252C%2520regarding%250Aunderstanding%2520as%2520the%2520encoder%2520%2528I2T%2529%2520that%2520compresses%2520images%2520into%2520text%252C%2520and%250Ageneration%2520as%2520the%2520decoder%2520%2528T2I%2529%2520that%2520reconstructs%2520images%2520from%2520that%2520text.%2520To%250Aimplement%2520this%252C%2520we%2520propose%2520UAE%252C%2520where%2520we%2520begin%2520by%2520pre-training%2520the%2520decoder%2520with%250Athe%2520proposed%2520700k%2520long-context%2520image-caption%2520pairs%2520to%2520direct%2520it%2520to%2520%2522understand%2522%250Athe%2520fine-grained%2520and%2520complex%2520semantics%2520from%2520the%2520text.%2520We%2520then%2520propose%250AUnified-GRPO%2520via%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520unify%2520the%2520two%252C%2520which%2520covers%2520two%250Acomplementary%2520stages%253A%2520%25281%2529%2520Generation%2520for%2520Understanding%252C%2520where%2520the%2520encoder%2520is%250Atrained%2520to%2520generate%2520informative%2520captions%2520that%2520maximize%2520the%2520decoder%2527s%250Areconstruction%2520quality%252C%2520enhancing%2520its%2520visual%2520perception%253B%2520%25282%2529%2520Understanding%2520for%250AGeneration%252C%2520where%2520the%2520decoder%2520is%2520refined%2520to%2520reconstruct%2520from%2520these%2520captions%252C%250Aforcing%2520it%2520to%2520leverage%2520every%2520detail%2520and%2520improving%2520its%2520long-context%2520instruction%250Afollowing%2520and%2520generation%2520fidelity.%2520Our%2520empirical%2520results%2520suggest%2520that%250Aunderstanding%2520can%2520largely%2520enhance%2520generation%2520%2528verified%2520on%2520GenEval%2529%252C%2520while%250Ageneration%252C%2520in%2520turn%252C%2520notably%2520strengthens%2520fine-grained%2520visual%2520perception%2520like%250Asmall%2520object%2520and%2520color%2520recognition%2520%2528verified%2520on%2520MMT-Bench%2529.%2520This%2520bidirectional%250Aimprovement%2520reveals%2520a%2520deep%2520synergy%253A%2520under%2520the%2520unified%2520reconstruction%2520objective%252C%250Ageneration%2520and%2520understanding%2520can%2520mutually%2520benefit%2520each%2520other%252C%2520moving%2520closer%2520to%250Atruly%2520unified%2520multimodal%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09666v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Multimodal%20Model%20as%20Auto-Encoder&entry.906535625=Zhiyuan%20Yan%20and%20Kaiqing%20Lin%20and%20Zongjian%20Li%20and%20Junyan%20Ye%20and%20Hui%20Han%20and%20Zhendong%20Wang%20and%20Hao%20Liu%20and%20Bin%20Lin%20and%20Hao%20Li%20and%20Xue%20Xu%20and%20Xinyan%20Xiao%20and%20Jingdong%20Wang%20and%20Haifeng%20Wang%20and%20Li%20Yuan&entry.1292438233=%20%20The%20pursuit%20of%20unified%20multimodal%20models%20%28UMMs%29%20has%20long%20been%20hindered%20by%20a%0Afundamental%20schism%20between%20multimodal%20understanding%20and%20generation.%20Current%0Aapproaches%20typically%20disentangle%20the%20two%20and%20treat%20them%20as%20separate%20endeavors%0Awith%20disjoint%20objectives%2C%20missing%20the%20mutual%20benefits.%20We%20argue%20that%20true%0Aunification%20requires%20more%20than%20just%20merging%20two%20tasks.%20It%20requires%20a%20unified%2C%0Afoundational%20objective%20that%20intrinsically%20links%20them.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20insightful%20paradigm%20through%20the%20Auto-Encoder%20lens%2C%20i.e.%2C%20regarding%0Aunderstanding%20as%20the%20encoder%20%28I2T%29%20that%20compresses%20images%20into%20text%2C%20and%0Ageneration%20as%20the%20decoder%20%28T2I%29%20that%20reconstructs%20images%20from%20that%20text.%20To%0Aimplement%20this%2C%20we%20propose%20UAE%2C%20where%20we%20begin%20by%20pre-training%20the%20decoder%20with%0Athe%20proposed%20700k%20long-context%20image-caption%20pairs%20to%20direct%20it%20to%20%22understand%22%0Athe%20fine-grained%20and%20complex%20semantics%20from%20the%20text.%20We%20then%20propose%0AUnified-GRPO%20via%20reinforcement%20learning%20%28RL%29%20to%20unify%20the%20two%2C%20which%20covers%20two%0Acomplementary%20stages%3A%20%281%29%20Generation%20for%20Understanding%2C%20where%20the%20encoder%20is%0Atrained%20to%20generate%20informative%20captions%20that%20maximize%20the%20decoder%27s%0Areconstruction%20quality%2C%20enhancing%20its%20visual%20perception%3B%20%282%29%20Understanding%20for%0AGeneration%2C%20where%20the%20decoder%20is%20refined%20to%20reconstruct%20from%20these%20captions%2C%0Aforcing%20it%20to%20leverage%20every%20detail%20and%20improving%20its%20long-context%20instruction%0Afollowing%20and%20generation%20fidelity.%20Our%20empirical%20results%20suggest%20that%0Aunderstanding%20can%20largely%20enhance%20generation%20%28verified%20on%20GenEval%29%2C%20while%0Ageneration%2C%20in%20turn%2C%20notably%20strengthens%20fine-grained%20visual%20perception%20like%0Asmall%20object%20and%20color%20recognition%20%28verified%20on%20MMT-Bench%29.%20This%20bidirectional%0Aimprovement%20reveals%20a%20deep%20synergy%3A%20under%20the%20unified%20reconstruction%20objective%2C%0Ageneration%20and%20understanding%20can%20mutually%20benefit%20each%20other%2C%20moving%20closer%20to%0Atruly%20unified%20multimodal%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09666v3&entry.124074799=Read"},
{"title": "Mitigating Overthinking through Reasoning Shaping", "author": "Feifan Song and Shaohang Wei and Bofei Gao and Yejie Wang and Wen Luo and Wei Li and Linli Yao and Weimin Xiong and Liang Chen and Tianyu Liu and Houfeng Wang", "abstract": "  Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier\nReward (RLVR) have shown great power in problem solving, yet they often cause\noverthinking: excessive, meandering reasoning that inflates computational cost.\nPrior designs of penalization in RLVR manage to reduce token consumption while\noften harming model performance, which arises from the oversimplicity of\ntoken-level supervision. In this paper, we argue that the granularity of\nsupervision plays a crucial role in balancing efficiency and accuracy, and\npropose Group Relative Segment Penalization (GRSP), a step-level method to\nregularize reasoning. Since preliminary analyses show that reasoning segments\nare strongly correlated with token consumption and model performance, we design\na length-aware weighting mechanism across segment clusters. Extensive\nexperiments demonstrate that GRSP achieves superior token efficiency without\nheavily compromising accuracy, especially the advantages with harder problems.\nMoreover, GRSP stabilizes RL training and scales effectively across model\nsizes.\n", "link": "http://arxiv.org/abs/2510.09535v1", "date": "2025-10-10", "relevancy": 2.3217, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Overthinking%20through%20Reasoning%20Shaping&body=Title%3A%20Mitigating%20Overthinking%20through%20Reasoning%20Shaping%0AAuthor%3A%20Feifan%20Song%20and%20Shaohang%20Wei%20and%20Bofei%20Gao%20and%20Yejie%20Wang%20and%20Wen%20Luo%20and%20Wei%20Li%20and%20Linli%20Yao%20and%20Weimin%20Xiong%20and%20Liang%20Chen%20and%20Tianyu%20Liu%20and%20Houfeng%20Wang%0AAbstract%3A%20%20%20Large%20reasoning%20models%20%28LRMs%29%20boosted%20by%20Reinforcement%20Learning%20from%20Verifier%0AReward%20%28RLVR%29%20have%20shown%20great%20power%20in%20problem%20solving%2C%20yet%20they%20often%20cause%0Aoverthinking%3A%20excessive%2C%20meandering%20reasoning%20that%20inflates%20computational%20cost.%0APrior%20designs%20of%20penalization%20in%20RLVR%20manage%20to%20reduce%20token%20consumption%20while%0Aoften%20harming%20model%20performance%2C%20which%20arises%20from%20the%20oversimplicity%20of%0Atoken-level%20supervision.%20In%20this%20paper%2C%20we%20argue%20that%20the%20granularity%20of%0Asupervision%20plays%20a%20crucial%20role%20in%20balancing%20efficiency%20and%20accuracy%2C%20and%0Apropose%20Group%20Relative%20Segment%20Penalization%20%28GRSP%29%2C%20a%20step-level%20method%20to%0Aregularize%20reasoning.%20Since%20preliminary%20analyses%20show%20that%20reasoning%20segments%0Aare%20strongly%20correlated%20with%20token%20consumption%20and%20model%20performance%2C%20we%20design%0Aa%20length-aware%20weighting%20mechanism%20across%20segment%20clusters.%20Extensive%0Aexperiments%20demonstrate%20that%20GRSP%20achieves%20superior%20token%20efficiency%20without%0Aheavily%20compromising%20accuracy%2C%20especially%20the%20advantages%20with%20harder%20problems.%0AMoreover%2C%20GRSP%20stabilizes%20RL%20training%20and%20scales%20effectively%20across%20model%0Asizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Overthinking%2520through%2520Reasoning%2520Shaping%26entry.906535625%3DFeifan%2520Song%2520and%2520Shaohang%2520Wei%2520and%2520Bofei%2520Gao%2520and%2520Yejie%2520Wang%2520and%2520Wen%2520Luo%2520and%2520Wei%2520Li%2520and%2520Linli%2520Yao%2520and%2520Weimin%2520Xiong%2520and%2520Liang%2520Chen%2520and%2520Tianyu%2520Liu%2520and%2520Houfeng%2520Wang%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520%2528LRMs%2529%2520boosted%2520by%2520Reinforcement%2520Learning%2520from%2520Verifier%250AReward%2520%2528RLVR%2529%2520have%2520shown%2520great%2520power%2520in%2520problem%2520solving%252C%2520yet%2520they%2520often%2520cause%250Aoverthinking%253A%2520excessive%252C%2520meandering%2520reasoning%2520that%2520inflates%2520computational%2520cost.%250APrior%2520designs%2520of%2520penalization%2520in%2520RLVR%2520manage%2520to%2520reduce%2520token%2520consumption%2520while%250Aoften%2520harming%2520model%2520performance%252C%2520which%2520arises%2520from%2520the%2520oversimplicity%2520of%250Atoken-level%2520supervision.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520the%2520granularity%2520of%250Asupervision%2520plays%2520a%2520crucial%2520role%2520in%2520balancing%2520efficiency%2520and%2520accuracy%252C%2520and%250Apropose%2520Group%2520Relative%2520Segment%2520Penalization%2520%2528GRSP%2529%252C%2520a%2520step-level%2520method%2520to%250Aregularize%2520reasoning.%2520Since%2520preliminary%2520analyses%2520show%2520that%2520reasoning%2520segments%250Aare%2520strongly%2520correlated%2520with%2520token%2520consumption%2520and%2520model%2520performance%252C%2520we%2520design%250Aa%2520length-aware%2520weighting%2520mechanism%2520across%2520segment%2520clusters.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520GRSP%2520achieves%2520superior%2520token%2520efficiency%2520without%250Aheavily%2520compromising%2520accuracy%252C%2520especially%2520the%2520advantages%2520with%2520harder%2520problems.%250AMoreover%252C%2520GRSP%2520stabilizes%2520RL%2520training%2520and%2520scales%2520effectively%2520across%2520model%250Asizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Overthinking%20through%20Reasoning%20Shaping&entry.906535625=Feifan%20Song%20and%20Shaohang%20Wei%20and%20Bofei%20Gao%20and%20Yejie%20Wang%20and%20Wen%20Luo%20and%20Wei%20Li%20and%20Linli%20Yao%20and%20Weimin%20Xiong%20and%20Liang%20Chen%20and%20Tianyu%20Liu%20and%20Houfeng%20Wang&entry.1292438233=%20%20Large%20reasoning%20models%20%28LRMs%29%20boosted%20by%20Reinforcement%20Learning%20from%20Verifier%0AReward%20%28RLVR%29%20have%20shown%20great%20power%20in%20problem%20solving%2C%20yet%20they%20often%20cause%0Aoverthinking%3A%20excessive%2C%20meandering%20reasoning%20that%20inflates%20computational%20cost.%0APrior%20designs%20of%20penalization%20in%20RLVR%20manage%20to%20reduce%20token%20consumption%20while%0Aoften%20harming%20model%20performance%2C%20which%20arises%20from%20the%20oversimplicity%20of%0Atoken-level%20supervision.%20In%20this%20paper%2C%20we%20argue%20that%20the%20granularity%20of%0Asupervision%20plays%20a%20crucial%20role%20in%20balancing%20efficiency%20and%20accuracy%2C%20and%0Apropose%20Group%20Relative%20Segment%20Penalization%20%28GRSP%29%2C%20a%20step-level%20method%20to%0Aregularize%20reasoning.%20Since%20preliminary%20analyses%20show%20that%20reasoning%20segments%0Aare%20strongly%20correlated%20with%20token%20consumption%20and%20model%20performance%2C%20we%20design%0Aa%20length-aware%20weighting%20mechanism%20across%20segment%20clusters.%20Extensive%0Aexperiments%20demonstrate%20that%20GRSP%20achieves%20superior%20token%20efficiency%20without%0Aheavily%20compromising%20accuracy%2C%20especially%20the%20advantages%20with%20harder%20problems.%0AMoreover%2C%20GRSP%20stabilizes%20RL%20training%20and%20scales%20effectively%20across%20model%0Asizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09535v1&entry.124074799=Read"},
{"title": "Cross-Receiver Generalization for RF Fingerprint Identification via\n  Feature Disentanglement and Adversarial Training", "author": "Yuhao Pan and Xiucheng Wang and Nan Cheng and Wenchao Xu", "abstract": "  Radio frequency fingerprint identification (RFFI) is a critical technique for\nwireless network security, leveraging intrinsic hardware-level imperfections\nintroduced during device manufacturing to enable precise transmitter\nidentification. While deep neural networks have shown remarkable capability in\nextracting discriminative features, their real-world deployment is hindered by\nreceiver-induced variability. In practice, RF fingerprint signals comprise\ntransmitter-specific features as well as channel distortions and\nreceiver-induced biases. Although channel equalization can mitigate channel\nnoise, receiver-induced feature shifts remain largely unaddressed, causing the\nRFFI models to overfit to receiver-specific patterns. This limitation is\nparticularly problematic when training and evaluation share the same receiver,\nas replacing the receiver in deployment can cause substantial performance\ndegradation. To tackle this challenge, we propose an RFFI framework robust to\ncross-receiver variability, integrating adversarial training and style transfer\nto explicitly disentangle transmitter and receiver features. By enforcing\ndomain-invariant representation learning, our method isolates genuine hardware\nsignatures from receiver artifacts, ensuring robustness against receiver\nchanges. Extensive experiments on multi-receiver datasets demonstrate that our\napproach consistently outperforms state-of-the-art baselines, achieving up to a\n10% improvement in average accuracy across diverse receiver settings.\n", "link": "http://arxiv.org/abs/2510.09405v1", "date": "2025-10-10", "relevancy": 2.3199, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4869}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4583}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Receiver%20Generalization%20for%20RF%20Fingerprint%20Identification%20via%0A%20%20Feature%20Disentanglement%20and%20Adversarial%20Training&body=Title%3A%20Cross-Receiver%20Generalization%20for%20RF%20Fingerprint%20Identification%20via%0A%20%20Feature%20Disentanglement%20and%20Adversarial%20Training%0AAuthor%3A%20Yuhao%20Pan%20and%20Xiucheng%20Wang%20and%20Nan%20Cheng%20and%20Wenchao%20Xu%0AAbstract%3A%20%20%20Radio%20frequency%20fingerprint%20identification%20%28RFFI%29%20is%20a%20critical%20technique%20for%0Awireless%20network%20security%2C%20leveraging%20intrinsic%20hardware-level%20imperfections%0Aintroduced%20during%20device%20manufacturing%20to%20enable%20precise%20transmitter%0Aidentification.%20While%20deep%20neural%20networks%20have%20shown%20remarkable%20capability%20in%0Aextracting%20discriminative%20features%2C%20their%20real-world%20deployment%20is%20hindered%20by%0Areceiver-induced%20variability.%20In%20practice%2C%20RF%20fingerprint%20signals%20comprise%0Atransmitter-specific%20features%20as%20well%20as%20channel%20distortions%20and%0Areceiver-induced%20biases.%20Although%20channel%20equalization%20can%20mitigate%20channel%0Anoise%2C%20receiver-induced%20feature%20shifts%20remain%20largely%20unaddressed%2C%20causing%20the%0ARFFI%20models%20to%20overfit%20to%20receiver-specific%20patterns.%20This%20limitation%20is%0Aparticularly%20problematic%20when%20training%20and%20evaluation%20share%20the%20same%20receiver%2C%0Aas%20replacing%20the%20receiver%20in%20deployment%20can%20cause%20substantial%20performance%0Adegradation.%20To%20tackle%20this%20challenge%2C%20we%20propose%20an%20RFFI%20framework%20robust%20to%0Across-receiver%20variability%2C%20integrating%20adversarial%20training%20and%20style%20transfer%0Ato%20explicitly%20disentangle%20transmitter%20and%20receiver%20features.%20By%20enforcing%0Adomain-invariant%20representation%20learning%2C%20our%20method%20isolates%20genuine%20hardware%0Asignatures%20from%20receiver%20artifacts%2C%20ensuring%20robustness%20against%20receiver%0Achanges.%20Extensive%20experiments%20on%20multi-receiver%20datasets%20demonstrate%20that%20our%0Aapproach%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20up%20to%20a%0A10%25%20improvement%20in%20average%20accuracy%20across%20diverse%20receiver%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Receiver%2520Generalization%2520for%2520RF%2520Fingerprint%2520Identification%2520via%250A%2520%2520Feature%2520Disentanglement%2520and%2520Adversarial%2520Training%26entry.906535625%3DYuhao%2520Pan%2520and%2520Xiucheng%2520Wang%2520and%2520Nan%2520Cheng%2520and%2520Wenchao%2520Xu%26entry.1292438233%3D%2520%2520Radio%2520frequency%2520fingerprint%2520identification%2520%2528RFFI%2529%2520is%2520a%2520critical%2520technique%2520for%250Awireless%2520network%2520security%252C%2520leveraging%2520intrinsic%2520hardware-level%2520imperfections%250Aintroduced%2520during%2520device%2520manufacturing%2520to%2520enable%2520precise%2520transmitter%250Aidentification.%2520While%2520deep%2520neural%2520networks%2520have%2520shown%2520remarkable%2520capability%2520in%250Aextracting%2520discriminative%2520features%252C%2520their%2520real-world%2520deployment%2520is%2520hindered%2520by%250Areceiver-induced%2520variability.%2520In%2520practice%252C%2520RF%2520fingerprint%2520signals%2520comprise%250Atransmitter-specific%2520features%2520as%2520well%2520as%2520channel%2520distortions%2520and%250Areceiver-induced%2520biases.%2520Although%2520channel%2520equalization%2520can%2520mitigate%2520channel%250Anoise%252C%2520receiver-induced%2520feature%2520shifts%2520remain%2520largely%2520unaddressed%252C%2520causing%2520the%250ARFFI%2520models%2520to%2520overfit%2520to%2520receiver-specific%2520patterns.%2520This%2520limitation%2520is%250Aparticularly%2520problematic%2520when%2520training%2520and%2520evaluation%2520share%2520the%2520same%2520receiver%252C%250Aas%2520replacing%2520the%2520receiver%2520in%2520deployment%2520can%2520cause%2520substantial%2520performance%250Adegradation.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520an%2520RFFI%2520framework%2520robust%2520to%250Across-receiver%2520variability%252C%2520integrating%2520adversarial%2520training%2520and%2520style%2520transfer%250Ato%2520explicitly%2520disentangle%2520transmitter%2520and%2520receiver%2520features.%2520By%2520enforcing%250Adomain-invariant%2520representation%2520learning%252C%2520our%2520method%2520isolates%2520genuine%2520hardware%250Asignatures%2520from%2520receiver%2520artifacts%252C%2520ensuring%2520robustness%2520against%2520receiver%250Achanges.%2520Extensive%2520experiments%2520on%2520multi-receiver%2520datasets%2520demonstrate%2520that%2520our%250Aapproach%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%252C%2520achieving%2520up%2520to%2520a%250A10%2525%2520improvement%2520in%2520average%2520accuracy%2520across%2520diverse%2520receiver%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Receiver%20Generalization%20for%20RF%20Fingerprint%20Identification%20via%0A%20%20Feature%20Disentanglement%20and%20Adversarial%20Training&entry.906535625=Yuhao%20Pan%20and%20Xiucheng%20Wang%20and%20Nan%20Cheng%20and%20Wenchao%20Xu&entry.1292438233=%20%20Radio%20frequency%20fingerprint%20identification%20%28RFFI%29%20is%20a%20critical%20technique%20for%0Awireless%20network%20security%2C%20leveraging%20intrinsic%20hardware-level%20imperfections%0Aintroduced%20during%20device%20manufacturing%20to%20enable%20precise%20transmitter%0Aidentification.%20While%20deep%20neural%20networks%20have%20shown%20remarkable%20capability%20in%0Aextracting%20discriminative%20features%2C%20their%20real-world%20deployment%20is%20hindered%20by%0Areceiver-induced%20variability.%20In%20practice%2C%20RF%20fingerprint%20signals%20comprise%0Atransmitter-specific%20features%20as%20well%20as%20channel%20distortions%20and%0Areceiver-induced%20biases.%20Although%20channel%20equalization%20can%20mitigate%20channel%0Anoise%2C%20receiver-induced%20feature%20shifts%20remain%20largely%20unaddressed%2C%20causing%20the%0ARFFI%20models%20to%20overfit%20to%20receiver-specific%20patterns.%20This%20limitation%20is%0Aparticularly%20problematic%20when%20training%20and%20evaluation%20share%20the%20same%20receiver%2C%0Aas%20replacing%20the%20receiver%20in%20deployment%20can%20cause%20substantial%20performance%0Adegradation.%20To%20tackle%20this%20challenge%2C%20we%20propose%20an%20RFFI%20framework%20robust%20to%0Across-receiver%20variability%2C%20integrating%20adversarial%20training%20and%20style%20transfer%0Ato%20explicitly%20disentangle%20transmitter%20and%20receiver%20features.%20By%20enforcing%0Adomain-invariant%20representation%20learning%2C%20our%20method%20isolates%20genuine%20hardware%0Asignatures%20from%20receiver%20artifacts%2C%20ensuring%20robustness%20against%20receiver%0Achanges.%20Extensive%20experiments%20on%20multi-receiver%20datasets%20demonstrate%20that%20our%0Aapproach%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20up%20to%20a%0A10%25%20improvement%20in%20average%20accuracy%20across%20diverse%20receiver%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09405v1&entry.124074799=Read"},
{"title": "Cross-attention Secretly Performs Orthogonal Alignment in Recommendation\n  Models", "author": "Hyunin Lee and Yong Zhang and Hoang Vu Nguyen and Xiaoyi Liu and Namyong Park and Christopher Jung and Rong Jin and Yang Wang and Zhigang Wang and Somayeh Sojoudi and Xue Feng", "abstract": "  Cross-domain sequential recommendation (CDSR) aims to align heterogeneous\nuser behavior sequences collected from different domains. While cross-attention\nis widely used to enhance alignment and improve recommendation performance, its\nunderlying mechanism is not fully understood. Most researchers interpret\ncross-attention as residual alignment, where the output is generated by\nremoving redundant and preserving non-redundant information from the query\ninput by referencing another domain data which is input key and value. Beyond\nthe prevailing view, we introduce Orthogonal Alignment, a phenomenon in which\ncross-attention discovers novel information that is not present in the query\ninput, and further argue that those two contrasting alignment mechanisms can\nco-exist in recommendation models We find that when the query input and output\nof cross-attention are orthogonal, model performance improves over 300\nexperiments. Notably, Orthogonal Alignment emerges naturally, without any\nexplicit orthogonality constraints. Our key insight is that Orthogonal\nAlignment emerges naturally because it improves scaling law. We show that\nbaselines additionally incorporating cross-attention module outperform\nparameter-matched baselines, achieving a superior accuracy-per-model parameter.\nWe hope these findings offer new directions for parameter-efficient scaling in\nmulti-modal research.\n", "link": "http://arxiv.org/abs/2510.09435v1", "date": "2025-10-10", "relevancy": 2.3107, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4662}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-attention%20Secretly%20Performs%20Orthogonal%20Alignment%20in%20Recommendation%0A%20%20Models&body=Title%3A%20Cross-attention%20Secretly%20Performs%20Orthogonal%20Alignment%20in%20Recommendation%0A%20%20Models%0AAuthor%3A%20Hyunin%20Lee%20and%20Yong%20Zhang%20and%20Hoang%20Vu%20Nguyen%20and%20Xiaoyi%20Liu%20and%20Namyong%20Park%20and%20Christopher%20Jung%20and%20Rong%20Jin%20and%20Yang%20Wang%20and%20Zhigang%20Wang%20and%20Somayeh%20Sojoudi%20and%20Xue%20Feng%0AAbstract%3A%20%20%20Cross-domain%20sequential%20recommendation%20%28CDSR%29%20aims%20to%20align%20heterogeneous%0Auser%20behavior%20sequences%20collected%20from%20different%20domains.%20While%20cross-attention%0Ais%20widely%20used%20to%20enhance%20alignment%20and%20improve%20recommendation%20performance%2C%20its%0Aunderlying%20mechanism%20is%20not%20fully%20understood.%20Most%20researchers%20interpret%0Across-attention%20as%20residual%20alignment%2C%20where%20the%20output%20is%20generated%20by%0Aremoving%20redundant%20and%20preserving%20non-redundant%20information%20from%20the%20query%0Ainput%20by%20referencing%20another%20domain%20data%20which%20is%20input%20key%20and%20value.%20Beyond%0Athe%20prevailing%20view%2C%20we%20introduce%20Orthogonal%20Alignment%2C%20a%20phenomenon%20in%20which%0Across-attention%20discovers%20novel%20information%20that%20is%20not%20present%20in%20the%20query%0Ainput%2C%20and%20further%20argue%20that%20those%20two%20contrasting%20alignment%20mechanisms%20can%0Aco-exist%20in%20recommendation%20models%20We%20find%20that%20when%20the%20query%20input%20and%20output%0Aof%20cross-attention%20are%20orthogonal%2C%20model%20performance%20improves%20over%20300%0Aexperiments.%20Notably%2C%20Orthogonal%20Alignment%20emerges%20naturally%2C%20without%20any%0Aexplicit%20orthogonality%20constraints.%20Our%20key%20insight%20is%20that%20Orthogonal%0AAlignment%20emerges%20naturally%20because%20it%20improves%20scaling%20law.%20We%20show%20that%0Abaselines%20additionally%20incorporating%20cross-attention%20module%20outperform%0Aparameter-matched%20baselines%2C%20achieving%20a%20superior%20accuracy-per-model%20parameter.%0AWe%20hope%20these%20findings%20offer%20new%20directions%20for%20parameter-efficient%20scaling%20in%0Amulti-modal%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-attention%2520Secretly%2520Performs%2520Orthogonal%2520Alignment%2520in%2520Recommendation%250A%2520%2520Models%26entry.906535625%3DHyunin%2520Lee%2520and%2520Yong%2520Zhang%2520and%2520Hoang%2520Vu%2520Nguyen%2520and%2520Xiaoyi%2520Liu%2520and%2520Namyong%2520Park%2520and%2520Christopher%2520Jung%2520and%2520Rong%2520Jin%2520and%2520Yang%2520Wang%2520and%2520Zhigang%2520Wang%2520and%2520Somayeh%2520Sojoudi%2520and%2520Xue%2520Feng%26entry.1292438233%3D%2520%2520Cross-domain%2520sequential%2520recommendation%2520%2528CDSR%2529%2520aims%2520to%2520align%2520heterogeneous%250Auser%2520behavior%2520sequences%2520collected%2520from%2520different%2520domains.%2520While%2520cross-attention%250Ais%2520widely%2520used%2520to%2520enhance%2520alignment%2520and%2520improve%2520recommendation%2520performance%252C%2520its%250Aunderlying%2520mechanism%2520is%2520not%2520fully%2520understood.%2520Most%2520researchers%2520interpret%250Across-attention%2520as%2520residual%2520alignment%252C%2520where%2520the%2520output%2520is%2520generated%2520by%250Aremoving%2520redundant%2520and%2520preserving%2520non-redundant%2520information%2520from%2520the%2520query%250Ainput%2520by%2520referencing%2520another%2520domain%2520data%2520which%2520is%2520input%2520key%2520and%2520value.%2520Beyond%250Athe%2520prevailing%2520view%252C%2520we%2520introduce%2520Orthogonal%2520Alignment%252C%2520a%2520phenomenon%2520in%2520which%250Across-attention%2520discovers%2520novel%2520information%2520that%2520is%2520not%2520present%2520in%2520the%2520query%250Ainput%252C%2520and%2520further%2520argue%2520that%2520those%2520two%2520contrasting%2520alignment%2520mechanisms%2520can%250Aco-exist%2520in%2520recommendation%2520models%2520We%2520find%2520that%2520when%2520the%2520query%2520input%2520and%2520output%250Aof%2520cross-attention%2520are%2520orthogonal%252C%2520model%2520performance%2520improves%2520over%2520300%250Aexperiments.%2520Notably%252C%2520Orthogonal%2520Alignment%2520emerges%2520naturally%252C%2520without%2520any%250Aexplicit%2520orthogonality%2520constraints.%2520Our%2520key%2520insight%2520is%2520that%2520Orthogonal%250AAlignment%2520emerges%2520naturally%2520because%2520it%2520improves%2520scaling%2520law.%2520We%2520show%2520that%250Abaselines%2520additionally%2520incorporating%2520cross-attention%2520module%2520outperform%250Aparameter-matched%2520baselines%252C%2520achieving%2520a%2520superior%2520accuracy-per-model%2520parameter.%250AWe%2520hope%2520these%2520findings%2520offer%2520new%2520directions%2520for%2520parameter-efficient%2520scaling%2520in%250Amulti-modal%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-attention%20Secretly%20Performs%20Orthogonal%20Alignment%20in%20Recommendation%0A%20%20Models&entry.906535625=Hyunin%20Lee%20and%20Yong%20Zhang%20and%20Hoang%20Vu%20Nguyen%20and%20Xiaoyi%20Liu%20and%20Namyong%20Park%20and%20Christopher%20Jung%20and%20Rong%20Jin%20and%20Yang%20Wang%20and%20Zhigang%20Wang%20and%20Somayeh%20Sojoudi%20and%20Xue%20Feng&entry.1292438233=%20%20Cross-domain%20sequential%20recommendation%20%28CDSR%29%20aims%20to%20align%20heterogeneous%0Auser%20behavior%20sequences%20collected%20from%20different%20domains.%20While%20cross-attention%0Ais%20widely%20used%20to%20enhance%20alignment%20and%20improve%20recommendation%20performance%2C%20its%0Aunderlying%20mechanism%20is%20not%20fully%20understood.%20Most%20researchers%20interpret%0Across-attention%20as%20residual%20alignment%2C%20where%20the%20output%20is%20generated%20by%0Aremoving%20redundant%20and%20preserving%20non-redundant%20information%20from%20the%20query%0Ainput%20by%20referencing%20another%20domain%20data%20which%20is%20input%20key%20and%20value.%20Beyond%0Athe%20prevailing%20view%2C%20we%20introduce%20Orthogonal%20Alignment%2C%20a%20phenomenon%20in%20which%0Across-attention%20discovers%20novel%20information%20that%20is%20not%20present%20in%20the%20query%0Ainput%2C%20and%20further%20argue%20that%20those%20two%20contrasting%20alignment%20mechanisms%20can%0Aco-exist%20in%20recommendation%20models%20We%20find%20that%20when%20the%20query%20input%20and%20output%0Aof%20cross-attention%20are%20orthogonal%2C%20model%20performance%20improves%20over%20300%0Aexperiments.%20Notably%2C%20Orthogonal%20Alignment%20emerges%20naturally%2C%20without%20any%0Aexplicit%20orthogonality%20constraints.%20Our%20key%20insight%20is%20that%20Orthogonal%0AAlignment%20emerges%20naturally%20because%20it%20improves%20scaling%20law.%20We%20show%20that%0Abaselines%20additionally%20incorporating%20cross-attention%20module%20outperform%0Aparameter-matched%20baselines%2C%20achieving%20a%20superior%20accuracy-per-model%20parameter.%0AWe%20hope%20these%20findings%20offer%20new%20directions%20for%20parameter-efficient%20scaling%20in%0Amulti-modal%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09435v1&entry.124074799=Read"},
{"title": "MorphGen: Controllable and Morphologically Plausible Generative\n  Cell-Imaging", "author": "Berker Demirel and Marco Fumero and Theofanis Karaletsos and Francesco Locatello", "abstract": "  Simulating in silico cellular responses to interventions is a promising\ndirection to accelerate high-content image-based assays, critical for advancing\ndrug discovery and gene editing. To support this, we introduce MorphGen, a\nstate-of-the-art diffusion-based generative model for fluorescent microscopy\nthat enables controllable generation across multiple cell types and\nperturbations. To capture biologically meaningful patterns consistent with\nknown cellular morphologies, MorphGen is trained with an alignment loss to\nmatch its representations to the phenotypic embeddings of OpenPhenom, a\nstate-of-the-art biological foundation model. Unlike prior approaches that\ncompress multichannel stains into RGB images -- thus sacrificing\norganelle-specific detail -- MorphGen generates the complete set of fluorescent\nchannels jointly, preserving per-organelle structures and enabling a\nfine-grained morphological analysis that is essential for biological\ninterpretation. We demonstrate biological consistency with real images via\nCellProfiler features, and MorphGen attains an FID score over 35% lower than\nthe prior state-of-the-art MorphoDiff, which only generates RGB images for a\nsingle cell type. Code is available at https://github.com/czi-ai/MorphGen.\n", "link": "http://arxiv.org/abs/2510.01298v2", "date": "2025-10-10", "relevancy": 2.3099, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6072}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5771}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MorphGen%3A%20Controllable%20and%20Morphologically%20Plausible%20Generative%0A%20%20Cell-Imaging&body=Title%3A%20MorphGen%3A%20Controllable%20and%20Morphologically%20Plausible%20Generative%0A%20%20Cell-Imaging%0AAuthor%3A%20Berker%20Demirel%20and%20Marco%20Fumero%20and%20Theofanis%20Karaletsos%20and%20Francesco%20Locatello%0AAbstract%3A%20%20%20Simulating%20in%20silico%20cellular%20responses%20to%20interventions%20is%20a%20promising%0Adirection%20to%20accelerate%20high-content%20image-based%20assays%2C%20critical%20for%20advancing%0Adrug%20discovery%20and%20gene%20editing.%20To%20support%20this%2C%20we%20introduce%20MorphGen%2C%20a%0Astate-of-the-art%20diffusion-based%20generative%20model%20for%20fluorescent%20microscopy%0Athat%20enables%20controllable%20generation%20across%20multiple%20cell%20types%20and%0Aperturbations.%20To%20capture%20biologically%20meaningful%20patterns%20consistent%20with%0Aknown%20cellular%20morphologies%2C%20MorphGen%20is%20trained%20with%20an%20alignment%20loss%20to%0Amatch%20its%20representations%20to%20the%20phenotypic%20embeddings%20of%20OpenPhenom%2C%20a%0Astate-of-the-art%20biological%20foundation%20model.%20Unlike%20prior%20approaches%20that%0Acompress%20multichannel%20stains%20into%20RGB%20images%20--%20thus%20sacrificing%0Aorganelle-specific%20detail%20--%20MorphGen%20generates%20the%20complete%20set%20of%20fluorescent%0Achannels%20jointly%2C%20preserving%20per-organelle%20structures%20and%20enabling%20a%0Afine-grained%20morphological%20analysis%20that%20is%20essential%20for%20biological%0Ainterpretation.%20We%20demonstrate%20biological%20consistency%20with%20real%20images%20via%0ACellProfiler%20features%2C%20and%20MorphGen%20attains%20an%20FID%20score%20over%2035%25%20lower%20than%0Athe%20prior%20state-of-the-art%20MorphoDiff%2C%20which%20only%20generates%20RGB%20images%20for%20a%0Asingle%20cell%20type.%20Code%20is%20available%20at%20https%3A//github.com/czi-ai/MorphGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphGen%253A%2520Controllable%2520and%2520Morphologically%2520Plausible%2520Generative%250A%2520%2520Cell-Imaging%26entry.906535625%3DBerker%2520Demirel%2520and%2520Marco%2520Fumero%2520and%2520Theofanis%2520Karaletsos%2520and%2520Francesco%2520Locatello%26entry.1292438233%3D%2520%2520Simulating%2520in%2520silico%2520cellular%2520responses%2520to%2520interventions%2520is%2520a%2520promising%250Adirection%2520to%2520accelerate%2520high-content%2520image-based%2520assays%252C%2520critical%2520for%2520advancing%250Adrug%2520discovery%2520and%2520gene%2520editing.%2520To%2520support%2520this%252C%2520we%2520introduce%2520MorphGen%252C%2520a%250Astate-of-the-art%2520diffusion-based%2520generative%2520model%2520for%2520fluorescent%2520microscopy%250Athat%2520enables%2520controllable%2520generation%2520across%2520multiple%2520cell%2520types%2520and%250Aperturbations.%2520To%2520capture%2520biologically%2520meaningful%2520patterns%2520consistent%2520with%250Aknown%2520cellular%2520morphologies%252C%2520MorphGen%2520is%2520trained%2520with%2520an%2520alignment%2520loss%2520to%250Amatch%2520its%2520representations%2520to%2520the%2520phenotypic%2520embeddings%2520of%2520OpenPhenom%252C%2520a%250Astate-of-the-art%2520biological%2520foundation%2520model.%2520Unlike%2520prior%2520approaches%2520that%250Acompress%2520multichannel%2520stains%2520into%2520RGB%2520images%2520--%2520thus%2520sacrificing%250Aorganelle-specific%2520detail%2520--%2520MorphGen%2520generates%2520the%2520complete%2520set%2520of%2520fluorescent%250Achannels%2520jointly%252C%2520preserving%2520per-organelle%2520structures%2520and%2520enabling%2520a%250Afine-grained%2520morphological%2520analysis%2520that%2520is%2520essential%2520for%2520biological%250Ainterpretation.%2520We%2520demonstrate%2520biological%2520consistency%2520with%2520real%2520images%2520via%250ACellProfiler%2520features%252C%2520and%2520MorphGen%2520attains%2520an%2520FID%2520score%2520over%252035%2525%2520lower%2520than%250Athe%2520prior%2520state-of-the-art%2520MorphoDiff%252C%2520which%2520only%2520generates%2520RGB%2520images%2520for%2520a%250Asingle%2520cell%2520type.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/czi-ai/MorphGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MorphGen%3A%20Controllable%20and%20Morphologically%20Plausible%20Generative%0A%20%20Cell-Imaging&entry.906535625=Berker%20Demirel%20and%20Marco%20Fumero%20and%20Theofanis%20Karaletsos%20and%20Francesco%20Locatello&entry.1292438233=%20%20Simulating%20in%20silico%20cellular%20responses%20to%20interventions%20is%20a%20promising%0Adirection%20to%20accelerate%20high-content%20image-based%20assays%2C%20critical%20for%20advancing%0Adrug%20discovery%20and%20gene%20editing.%20To%20support%20this%2C%20we%20introduce%20MorphGen%2C%20a%0Astate-of-the-art%20diffusion-based%20generative%20model%20for%20fluorescent%20microscopy%0Athat%20enables%20controllable%20generation%20across%20multiple%20cell%20types%20and%0Aperturbations.%20To%20capture%20biologically%20meaningful%20patterns%20consistent%20with%0Aknown%20cellular%20morphologies%2C%20MorphGen%20is%20trained%20with%20an%20alignment%20loss%20to%0Amatch%20its%20representations%20to%20the%20phenotypic%20embeddings%20of%20OpenPhenom%2C%20a%0Astate-of-the-art%20biological%20foundation%20model.%20Unlike%20prior%20approaches%20that%0Acompress%20multichannel%20stains%20into%20RGB%20images%20--%20thus%20sacrificing%0Aorganelle-specific%20detail%20--%20MorphGen%20generates%20the%20complete%20set%20of%20fluorescent%0Achannels%20jointly%2C%20preserving%20per-organelle%20structures%20and%20enabling%20a%0Afine-grained%20morphological%20analysis%20that%20is%20essential%20for%20biological%0Ainterpretation.%20We%20demonstrate%20biological%20consistency%20with%20real%20images%20via%0ACellProfiler%20features%2C%20and%20MorphGen%20attains%20an%20FID%20score%20over%2035%25%20lower%20than%0Athe%20prior%20state-of-the-art%20MorphoDiff%2C%20which%20only%20generates%20RGB%20images%20for%20a%0Asingle%20cell%20type.%20Code%20is%20available%20at%20https%3A//github.com/czi-ai/MorphGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01298v2&entry.124074799=Read"},
{"title": "ProbRes: Probabilistic Jump Diffusion for Open-World Egocentric Activity\n  Recognition", "author": "Sanjoy Kundu and Shanmukha Vellamcheti and Sathyanarayanan N. Aakur", "abstract": "  Open-world egocentric activity recognition poses a fundamental challenge due\nto its unconstrained nature, requiring models to infer unseen activities from\nan expansive, partially observed search space. We introduce ProbRes, a\nProbabilistic Residual search framework based on jump-diffusion that\nefficiently navigates this space by balancing prior-guided exploration with\nlikelihood-driven exploitation. Our approach integrates structured commonsense\npriors to construct a semantically coherent search space, adaptively refines\npredictions using Vision-Language Models (VLMs) and employs a stochastic search\nmechanism to locate high-likelihood activity labels while minimizing exhaustive\nenumeration efficiently. We systematically evaluate ProbRes across multiple\nopenness levels (L0-L3), demonstrating its adaptability to increasing search\nspace complexity. In addition to achieving state-of-the-art performance on\nbenchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we\nestablish a clear taxonomy for open-world recognition, delineating the\nchallenges and methodological advancements necessary for egocentric activity\nunderstanding. Our results highlight the importance of structured search\nstrategies, paving the way for scalable and efficient open-world activity\nrecognition.\n", "link": "http://arxiv.org/abs/2504.03948v2", "date": "2025-10-10", "relevancy": 2.3031, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProbRes%3A%20Probabilistic%20Jump%20Diffusion%20for%20Open-World%20Egocentric%20Activity%0A%20%20Recognition&body=Title%3A%20ProbRes%3A%20Probabilistic%20Jump%20Diffusion%20for%20Open-World%20Egocentric%20Activity%0A%20%20Recognition%0AAuthor%3A%20Sanjoy%20Kundu%20and%20Shanmukha%20Vellamcheti%20and%20Sathyanarayanan%20N.%20Aakur%0AAbstract%3A%20%20%20Open-world%20egocentric%20activity%20recognition%20poses%20a%20fundamental%20challenge%20due%0Ato%20its%20unconstrained%20nature%2C%20requiring%20models%20to%20infer%20unseen%20activities%20from%0Aan%20expansive%2C%20partially%20observed%20search%20space.%20We%20introduce%20ProbRes%2C%20a%0AProbabilistic%20Residual%20search%20framework%20based%20on%20jump-diffusion%20that%0Aefficiently%20navigates%20this%20space%20by%20balancing%20prior-guided%20exploration%20with%0Alikelihood-driven%20exploitation.%20Our%20approach%20integrates%20structured%20commonsense%0Apriors%20to%20construct%20a%20semantically%20coherent%20search%20space%2C%20adaptively%20refines%0Apredictions%20using%20Vision-Language%20Models%20%28VLMs%29%20and%20employs%20a%20stochastic%20search%0Amechanism%20to%20locate%20high-likelihood%20activity%20labels%20while%20minimizing%20exhaustive%0Aenumeration%20efficiently.%20We%20systematically%20evaluate%20ProbRes%20across%20multiple%0Aopenness%20levels%20%28L0-L3%29%2C%20demonstrating%20its%20adaptability%20to%20increasing%20search%0Aspace%20complexity.%20In%20addition%20to%20achieving%20state-of-the-art%20performance%20on%0Abenchmark%20datasets%20%28GTEA%20Gaze%2C%20GTEA%20Gaze%2B%2C%20EPIC-Kitchens%2C%20and%20Charades-Ego%29%2C%20we%0Aestablish%20a%20clear%20taxonomy%20for%20open-world%20recognition%2C%20delineating%20the%0Achallenges%20and%20methodological%20advancements%20necessary%20for%20egocentric%20activity%0Aunderstanding.%20Our%20results%20highlight%20the%20importance%20of%20structured%20search%0Astrategies%2C%20paving%20the%20way%20for%20scalable%20and%20efficient%20open-world%20activity%0Arecognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03948v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbRes%253A%2520Probabilistic%2520Jump%2520Diffusion%2520for%2520Open-World%2520Egocentric%2520Activity%250A%2520%2520Recognition%26entry.906535625%3DSanjoy%2520Kundu%2520and%2520Shanmukha%2520Vellamcheti%2520and%2520Sathyanarayanan%2520N.%2520Aakur%26entry.1292438233%3D%2520%2520Open-world%2520egocentric%2520activity%2520recognition%2520poses%2520a%2520fundamental%2520challenge%2520due%250Ato%2520its%2520unconstrained%2520nature%252C%2520requiring%2520models%2520to%2520infer%2520unseen%2520activities%2520from%250Aan%2520expansive%252C%2520partially%2520observed%2520search%2520space.%2520We%2520introduce%2520ProbRes%252C%2520a%250AProbabilistic%2520Residual%2520search%2520framework%2520based%2520on%2520jump-diffusion%2520that%250Aefficiently%2520navigates%2520this%2520space%2520by%2520balancing%2520prior-guided%2520exploration%2520with%250Alikelihood-driven%2520exploitation.%2520Our%2520approach%2520integrates%2520structured%2520commonsense%250Apriors%2520to%2520construct%2520a%2520semantically%2520coherent%2520search%2520space%252C%2520adaptively%2520refines%250Apredictions%2520using%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520and%2520employs%2520a%2520stochastic%2520search%250Amechanism%2520to%2520locate%2520high-likelihood%2520activity%2520labels%2520while%2520minimizing%2520exhaustive%250Aenumeration%2520efficiently.%2520We%2520systematically%2520evaluate%2520ProbRes%2520across%2520multiple%250Aopenness%2520levels%2520%2528L0-L3%2529%252C%2520demonstrating%2520its%2520adaptability%2520to%2520increasing%2520search%250Aspace%2520complexity.%2520In%2520addition%2520to%2520achieving%2520state-of-the-art%2520performance%2520on%250Abenchmark%2520datasets%2520%2528GTEA%2520Gaze%252C%2520GTEA%2520Gaze%252B%252C%2520EPIC-Kitchens%252C%2520and%2520Charades-Ego%2529%252C%2520we%250Aestablish%2520a%2520clear%2520taxonomy%2520for%2520open-world%2520recognition%252C%2520delineating%2520the%250Achallenges%2520and%2520methodological%2520advancements%2520necessary%2520for%2520egocentric%2520activity%250Aunderstanding.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520structured%2520search%250Astrategies%252C%2520paving%2520the%2520way%2520for%2520scalable%2520and%2520efficient%2520open-world%2520activity%250Arecognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03948v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProbRes%3A%20Probabilistic%20Jump%20Diffusion%20for%20Open-World%20Egocentric%20Activity%0A%20%20Recognition&entry.906535625=Sanjoy%20Kundu%20and%20Shanmukha%20Vellamcheti%20and%20Sathyanarayanan%20N.%20Aakur&entry.1292438233=%20%20Open-world%20egocentric%20activity%20recognition%20poses%20a%20fundamental%20challenge%20due%0Ato%20its%20unconstrained%20nature%2C%20requiring%20models%20to%20infer%20unseen%20activities%20from%0Aan%20expansive%2C%20partially%20observed%20search%20space.%20We%20introduce%20ProbRes%2C%20a%0AProbabilistic%20Residual%20search%20framework%20based%20on%20jump-diffusion%20that%0Aefficiently%20navigates%20this%20space%20by%20balancing%20prior-guided%20exploration%20with%0Alikelihood-driven%20exploitation.%20Our%20approach%20integrates%20structured%20commonsense%0Apriors%20to%20construct%20a%20semantically%20coherent%20search%20space%2C%20adaptively%20refines%0Apredictions%20using%20Vision-Language%20Models%20%28VLMs%29%20and%20employs%20a%20stochastic%20search%0Amechanism%20to%20locate%20high-likelihood%20activity%20labels%20while%20minimizing%20exhaustive%0Aenumeration%20efficiently.%20We%20systematically%20evaluate%20ProbRes%20across%20multiple%0Aopenness%20levels%20%28L0-L3%29%2C%20demonstrating%20its%20adaptability%20to%20increasing%20search%0Aspace%20complexity.%20In%20addition%20to%20achieving%20state-of-the-art%20performance%20on%0Abenchmark%20datasets%20%28GTEA%20Gaze%2C%20GTEA%20Gaze%2B%2C%20EPIC-Kitchens%2C%20and%20Charades-Ego%29%2C%20we%0Aestablish%20a%20clear%20taxonomy%20for%20open-world%20recognition%2C%20delineating%20the%0Achallenges%20and%20methodological%20advancements%20necessary%20for%20egocentric%20activity%0Aunderstanding.%20Our%20results%20highlight%20the%20importance%20of%20structured%20search%0Astrategies%2C%20paving%20the%20way%20for%20scalable%20and%20efficient%20open-world%20activity%0Arecognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03948v2&entry.124074799=Read"},
{"title": "FLOWING: Implicit Neural Flows for Structure-Preserving Morphing", "author": "Arthur Bizzi and Matias Grynberg and Vitor Matias and Daniel Perazzo and Jo\u00e3o Paulo Lima and Luiz Velho and Nuno Gon\u00e7alves and Jo\u00e3o Pereira and Guilherme Schardong and Tiago Novello", "abstract": "  Morphing is a long-standing problem in vision and computer graphics,\nrequiring a time-dependent warping for feature alignment and a blending for\nsmooth interpolation. Recently, multilayer perceptrons (MLPs) have been\nexplored as implicit neural representations (INRs) for modeling such\ndeformations, due to their meshlessness and differentiability; however,\nextracting coherent and accurate morphings from standard MLPs typically relies\non costly regularizations, which often lead to unstable training and prevent\neffective feature alignment. To overcome these limitations, we propose FLOWING\n(FLOW morphING), a framework that recasts warping as the construction of a\ndifferential vector flow, naturally ensuring continuity, invertibility, and\ntemporal coherence by encoding structural flow properties directly into the\nnetwork architectures. This flow-centric approach yields principled and stable\ntransformations, enabling accurate and structure-preserving morphing of both 2D\nimages and 3D shapes. Extensive experiments across a range of applications -\nincluding face and image morphing, as well as Gaussian Splatting morphing -\nshow that FLOWING achieves state-of-the-art morphing quality with faster\nconvergence. Code and pretrained models are available at\nhttp://schardong.github.io/flowing.\n", "link": "http://arxiv.org/abs/2510.09537v1", "date": "2025-10-10", "relevancy": 2.2934, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6297}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5906}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLOWING%3A%20Implicit%20Neural%20Flows%20for%20Structure-Preserving%20Morphing&body=Title%3A%20FLOWING%3A%20Implicit%20Neural%20Flows%20for%20Structure-Preserving%20Morphing%0AAuthor%3A%20Arthur%20Bizzi%20and%20Matias%20Grynberg%20and%20Vitor%20Matias%20and%20Daniel%20Perazzo%20and%20Jo%C3%A3o%20Paulo%20Lima%20and%20Luiz%20Velho%20and%20Nuno%20Gon%C3%A7alves%20and%20Jo%C3%A3o%20Pereira%20and%20Guilherme%20Schardong%20and%20Tiago%20Novello%0AAbstract%3A%20%20%20Morphing%20is%20a%20long-standing%20problem%20in%20vision%20and%20computer%20graphics%2C%0Arequiring%20a%20time-dependent%20warping%20for%20feature%20alignment%20and%20a%20blending%20for%0Asmooth%20interpolation.%20Recently%2C%20multilayer%20perceptrons%20%28MLPs%29%20have%20been%0Aexplored%20as%20implicit%20neural%20representations%20%28INRs%29%20for%20modeling%20such%0Adeformations%2C%20due%20to%20their%20meshlessness%20and%20differentiability%3B%20however%2C%0Aextracting%20coherent%20and%20accurate%20morphings%20from%20standard%20MLPs%20typically%20relies%0Aon%20costly%20regularizations%2C%20which%20often%20lead%20to%20unstable%20training%20and%20prevent%0Aeffective%20feature%20alignment.%20To%20overcome%20these%20limitations%2C%20we%20propose%20FLOWING%0A%28FLOW%20morphING%29%2C%20a%20framework%20that%20recasts%20warping%20as%20the%20construction%20of%20a%0Adifferential%20vector%20flow%2C%20naturally%20ensuring%20continuity%2C%20invertibility%2C%20and%0Atemporal%20coherence%20by%20encoding%20structural%20flow%20properties%20directly%20into%20the%0Anetwork%20architectures.%20This%20flow-centric%20approach%20yields%20principled%20and%20stable%0Atransformations%2C%20enabling%20accurate%20and%20structure-preserving%20morphing%20of%20both%202D%0Aimages%20and%203D%20shapes.%20Extensive%20experiments%20across%20a%20range%20of%20applications%20-%0Aincluding%20face%20and%20image%20morphing%2C%20as%20well%20as%20Gaussian%20Splatting%20morphing%20-%0Ashow%20that%20FLOWING%20achieves%20state-of-the-art%20morphing%20quality%20with%20faster%0Aconvergence.%20Code%20and%20pretrained%20models%20are%20available%20at%0Ahttp%3A//schardong.github.io/flowing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLOWING%253A%2520Implicit%2520Neural%2520Flows%2520for%2520Structure-Preserving%2520Morphing%26entry.906535625%3DArthur%2520Bizzi%2520and%2520Matias%2520Grynberg%2520and%2520Vitor%2520Matias%2520and%2520Daniel%2520Perazzo%2520and%2520Jo%25C3%25A3o%2520Paulo%2520Lima%2520and%2520Luiz%2520Velho%2520and%2520Nuno%2520Gon%25C3%25A7alves%2520and%2520Jo%25C3%25A3o%2520Pereira%2520and%2520Guilherme%2520Schardong%2520and%2520Tiago%2520Novello%26entry.1292438233%3D%2520%2520Morphing%2520is%2520a%2520long-standing%2520problem%2520in%2520vision%2520and%2520computer%2520graphics%252C%250Arequiring%2520a%2520time-dependent%2520warping%2520for%2520feature%2520alignment%2520and%2520a%2520blending%2520for%250Asmooth%2520interpolation.%2520Recently%252C%2520multilayer%2520perceptrons%2520%2528MLPs%2529%2520have%2520been%250Aexplored%2520as%2520implicit%2520neural%2520representations%2520%2528INRs%2529%2520for%2520modeling%2520such%250Adeformations%252C%2520due%2520to%2520their%2520meshlessness%2520and%2520differentiability%253B%2520however%252C%250Aextracting%2520coherent%2520and%2520accurate%2520morphings%2520from%2520standard%2520MLPs%2520typically%2520relies%250Aon%2520costly%2520regularizations%252C%2520which%2520often%2520lead%2520to%2520unstable%2520training%2520and%2520prevent%250Aeffective%2520feature%2520alignment.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520FLOWING%250A%2528FLOW%2520morphING%2529%252C%2520a%2520framework%2520that%2520recasts%2520warping%2520as%2520the%2520construction%2520of%2520a%250Adifferential%2520vector%2520flow%252C%2520naturally%2520ensuring%2520continuity%252C%2520invertibility%252C%2520and%250Atemporal%2520coherence%2520by%2520encoding%2520structural%2520flow%2520properties%2520directly%2520into%2520the%250Anetwork%2520architectures.%2520This%2520flow-centric%2520approach%2520yields%2520principled%2520and%2520stable%250Atransformations%252C%2520enabling%2520accurate%2520and%2520structure-preserving%2520morphing%2520of%2520both%25202D%250Aimages%2520and%25203D%2520shapes.%2520Extensive%2520experiments%2520across%2520a%2520range%2520of%2520applications%2520-%250Aincluding%2520face%2520and%2520image%2520morphing%252C%2520as%2520well%2520as%2520Gaussian%2520Splatting%2520morphing%2520-%250Ashow%2520that%2520FLOWING%2520achieves%2520state-of-the-art%2520morphing%2520quality%2520with%2520faster%250Aconvergence.%2520Code%2520and%2520pretrained%2520models%2520are%2520available%2520at%250Ahttp%253A//schardong.github.io/flowing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLOWING%3A%20Implicit%20Neural%20Flows%20for%20Structure-Preserving%20Morphing&entry.906535625=Arthur%20Bizzi%20and%20Matias%20Grynberg%20and%20Vitor%20Matias%20and%20Daniel%20Perazzo%20and%20Jo%C3%A3o%20Paulo%20Lima%20and%20Luiz%20Velho%20and%20Nuno%20Gon%C3%A7alves%20and%20Jo%C3%A3o%20Pereira%20and%20Guilherme%20Schardong%20and%20Tiago%20Novello&entry.1292438233=%20%20Morphing%20is%20a%20long-standing%20problem%20in%20vision%20and%20computer%20graphics%2C%0Arequiring%20a%20time-dependent%20warping%20for%20feature%20alignment%20and%20a%20blending%20for%0Asmooth%20interpolation.%20Recently%2C%20multilayer%20perceptrons%20%28MLPs%29%20have%20been%0Aexplored%20as%20implicit%20neural%20representations%20%28INRs%29%20for%20modeling%20such%0Adeformations%2C%20due%20to%20their%20meshlessness%20and%20differentiability%3B%20however%2C%0Aextracting%20coherent%20and%20accurate%20morphings%20from%20standard%20MLPs%20typically%20relies%0Aon%20costly%20regularizations%2C%20which%20often%20lead%20to%20unstable%20training%20and%20prevent%0Aeffective%20feature%20alignment.%20To%20overcome%20these%20limitations%2C%20we%20propose%20FLOWING%0A%28FLOW%20morphING%29%2C%20a%20framework%20that%20recasts%20warping%20as%20the%20construction%20of%20a%0Adifferential%20vector%20flow%2C%20naturally%20ensuring%20continuity%2C%20invertibility%2C%20and%0Atemporal%20coherence%20by%20encoding%20structural%20flow%20properties%20directly%20into%20the%0Anetwork%20architectures.%20This%20flow-centric%20approach%20yields%20principled%20and%20stable%0Atransformations%2C%20enabling%20accurate%20and%20structure-preserving%20morphing%20of%20both%202D%0Aimages%20and%203D%20shapes.%20Extensive%20experiments%20across%20a%20range%20of%20applications%20-%0Aincluding%20face%20and%20image%20morphing%2C%20as%20well%20as%20Gaussian%20Splatting%20morphing%20-%0Ashow%20that%20FLOWING%20achieves%20state-of-the-art%20morphing%20quality%20with%20faster%0Aconvergence.%20Code%20and%20pretrained%20models%20are%20available%20at%0Ahttp%3A//schardong.github.io/flowing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09537v1&entry.124074799=Read"},
{"title": "Glovity: Learning Dexterous Contact-Rich Manipulation via Spatial Wrench\n  Feedback Teleoperation System", "author": "Yuyang Gao and Haofei Ma and Pai Zheng", "abstract": "  We present Glovity, a novel, low-cost wearable teleoperation system that\nintegrates a spatial wrench (force-torque) feedback device with a haptic glove\nfeaturing fingertip Hall sensor calibration, enabling feedback-rich dexterous\nmanipulation. Glovity addresses key challenges in contact-rich tasks by\nproviding intuitive wrench and tactile feedback, while overcoming embodiment\ngaps through precise retargeting. User studies demonstrate significant\nimprovements: wrench feedback boosts success rates in book-flipping tasks from\n48% to 78% and reduces completion time by 25%, while fingertip calibration\nenhances thin-object grasping success significantly compared to commercial\nglove. Furthermore, incorporating wrench signals into imitation learning (via\nDP-R3M) achieves high success rate in novel contact-rich scenarios, such as\nadaptive page flipping and force-aware handovers. All hardware designs,\nsoftware will be open-sourced. Project website: https://glovity.github.io/\n", "link": "http://arxiv.org/abs/2510.09229v1", "date": "2025-10-10", "relevancy": 2.2925, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6009}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5651}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glovity%3A%20Learning%20Dexterous%20Contact-Rich%20Manipulation%20via%20Spatial%20Wrench%0A%20%20Feedback%20Teleoperation%20System&body=Title%3A%20Glovity%3A%20Learning%20Dexterous%20Contact-Rich%20Manipulation%20via%20Spatial%20Wrench%0A%20%20Feedback%20Teleoperation%20System%0AAuthor%3A%20Yuyang%20Gao%20and%20Haofei%20Ma%20and%20Pai%20Zheng%0AAbstract%3A%20%20%20We%20present%20Glovity%2C%20a%20novel%2C%20low-cost%20wearable%20teleoperation%20system%20that%0Aintegrates%20a%20spatial%20wrench%20%28force-torque%29%20feedback%20device%20with%20a%20haptic%20glove%0Afeaturing%20fingertip%20Hall%20sensor%20calibration%2C%20enabling%20feedback-rich%20dexterous%0Amanipulation.%20Glovity%20addresses%20key%20challenges%20in%20contact-rich%20tasks%20by%0Aproviding%20intuitive%20wrench%20and%20tactile%20feedback%2C%20while%20overcoming%20embodiment%0Agaps%20through%20precise%20retargeting.%20User%20studies%20demonstrate%20significant%0Aimprovements%3A%20wrench%20feedback%20boosts%20success%20rates%20in%20book-flipping%20tasks%20from%0A48%25%20to%2078%25%20and%20reduces%20completion%20time%20by%2025%25%2C%20while%20fingertip%20calibration%0Aenhances%20thin-object%20grasping%20success%20significantly%20compared%20to%20commercial%0Aglove.%20Furthermore%2C%20incorporating%20wrench%20signals%20into%20imitation%20learning%20%28via%0ADP-R3M%29%20achieves%20high%20success%20rate%20in%20novel%20contact-rich%20scenarios%2C%20such%20as%0Aadaptive%20page%20flipping%20and%20force-aware%20handovers.%20All%20hardware%20designs%2C%0Asoftware%20will%20be%20open-sourced.%20Project%20website%3A%20https%3A//glovity.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlovity%253A%2520Learning%2520Dexterous%2520Contact-Rich%2520Manipulation%2520via%2520Spatial%2520Wrench%250A%2520%2520Feedback%2520Teleoperation%2520System%26entry.906535625%3DYuyang%2520Gao%2520and%2520Haofei%2520Ma%2520and%2520Pai%2520Zheng%26entry.1292438233%3D%2520%2520We%2520present%2520Glovity%252C%2520a%2520novel%252C%2520low-cost%2520wearable%2520teleoperation%2520system%2520that%250Aintegrates%2520a%2520spatial%2520wrench%2520%2528force-torque%2529%2520feedback%2520device%2520with%2520a%2520haptic%2520glove%250Afeaturing%2520fingertip%2520Hall%2520sensor%2520calibration%252C%2520enabling%2520feedback-rich%2520dexterous%250Amanipulation.%2520Glovity%2520addresses%2520key%2520challenges%2520in%2520contact-rich%2520tasks%2520by%250Aproviding%2520intuitive%2520wrench%2520and%2520tactile%2520feedback%252C%2520while%2520overcoming%2520embodiment%250Agaps%2520through%2520precise%2520retargeting.%2520User%2520studies%2520demonstrate%2520significant%250Aimprovements%253A%2520wrench%2520feedback%2520boosts%2520success%2520rates%2520in%2520book-flipping%2520tasks%2520from%250A48%2525%2520to%252078%2525%2520and%2520reduces%2520completion%2520time%2520by%252025%2525%252C%2520while%2520fingertip%2520calibration%250Aenhances%2520thin-object%2520grasping%2520success%2520significantly%2520compared%2520to%2520commercial%250Aglove.%2520Furthermore%252C%2520incorporating%2520wrench%2520signals%2520into%2520imitation%2520learning%2520%2528via%250ADP-R3M%2529%2520achieves%2520high%2520success%2520rate%2520in%2520novel%2520contact-rich%2520scenarios%252C%2520such%2520as%250Aadaptive%2520page%2520flipping%2520and%2520force-aware%2520handovers.%2520All%2520hardware%2520designs%252C%250Asoftware%2520will%2520be%2520open-sourced.%2520Project%2520website%253A%2520https%253A//glovity.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glovity%3A%20Learning%20Dexterous%20Contact-Rich%20Manipulation%20via%20Spatial%20Wrench%0A%20%20Feedback%20Teleoperation%20System&entry.906535625=Yuyang%20Gao%20and%20Haofei%20Ma%20and%20Pai%20Zheng&entry.1292438233=%20%20We%20present%20Glovity%2C%20a%20novel%2C%20low-cost%20wearable%20teleoperation%20system%20that%0Aintegrates%20a%20spatial%20wrench%20%28force-torque%29%20feedback%20device%20with%20a%20haptic%20glove%0Afeaturing%20fingertip%20Hall%20sensor%20calibration%2C%20enabling%20feedback-rich%20dexterous%0Amanipulation.%20Glovity%20addresses%20key%20challenges%20in%20contact-rich%20tasks%20by%0Aproviding%20intuitive%20wrench%20and%20tactile%20feedback%2C%20while%20overcoming%20embodiment%0Agaps%20through%20precise%20retargeting.%20User%20studies%20demonstrate%20significant%0Aimprovements%3A%20wrench%20feedback%20boosts%20success%20rates%20in%20book-flipping%20tasks%20from%0A48%25%20to%2078%25%20and%20reduces%20completion%20time%20by%2025%25%2C%20while%20fingertip%20calibration%0Aenhances%20thin-object%20grasping%20success%20significantly%20compared%20to%20commercial%0Aglove.%20Furthermore%2C%20incorporating%20wrench%20signals%20into%20imitation%20learning%20%28via%0ADP-R3M%29%20achieves%20high%20success%20rate%20in%20novel%20contact-rich%20scenarios%2C%20such%20as%0Aadaptive%20page%20flipping%20and%20force-aware%20handovers.%20All%20hardware%20designs%2C%0Asoftware%20will%20be%20open-sourced.%20Project%20website%3A%20https%3A//glovity.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09229v1&entry.124074799=Read"},
{"title": "Precoder Design in Multi-User FDD Systems with VQ-VAE and GNN", "author": "Srikar Allaparapu and Michael Baur and Benedikt B\u00f6ck and Michael Joham and Wolfgang Utschick", "abstract": "  Robust precoding is efficiently feasible in frequency division duplex (FDD)\nsystems by incorporating the learnt statistics of the propagation environment\nthrough a generative model. We build on previous work that successfully\ndesigned site-specific precoders based on a combination of Gaussian mixture\nmodels (GMMs) and graph neural networks (GNNs). In this paper, by utilizing a\nvector quantized-variational autoencoder (VQ-VAE), we circumvent one of the key\ndrawbacks of GMMs, i.e., the number of GMM components scales exponentially to\nthe feedback bits. In addition, the deep learning architecture of the VQ-VAE\nallows us to jointly train the GNN together with VQ-VAE along with pilot\noptimization forming an end-to-end (E2E) model, resulting in considerable\nperformance gains in sum rate for multi-user wireless systems. Simulations\ndemonstrate the superiority of the proposed frameworks over the conventional\nmethods involving the sub-discrete Fourier transform (DFT) pilot matrix and\niterative precoder algorithms enabling the deployment of systems characterized\nby fewer pilots or feedback bits.\n", "link": "http://arxiv.org/abs/2510.09495v1", "date": "2025-10-10", "relevancy": 2.2918, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4712}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.461}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Precoder%20Design%20in%20Multi-User%20FDD%20Systems%20with%20VQ-VAE%20and%20GNN&body=Title%3A%20Precoder%20Design%20in%20Multi-User%20FDD%20Systems%20with%20VQ-VAE%20and%20GNN%0AAuthor%3A%20Srikar%20Allaparapu%20and%20Michael%20Baur%20and%20Benedikt%20B%C3%B6ck%20and%20Michael%20Joham%20and%20Wolfgang%20Utschick%0AAbstract%3A%20%20%20Robust%20precoding%20is%20efficiently%20feasible%20in%20frequency%20division%20duplex%20%28FDD%29%0Asystems%20by%20incorporating%20the%20learnt%20statistics%20of%20the%20propagation%20environment%0Athrough%20a%20generative%20model.%20We%20build%20on%20previous%20work%20that%20successfully%0Adesigned%20site-specific%20precoders%20based%20on%20a%20combination%20of%20Gaussian%20mixture%0Amodels%20%28GMMs%29%20and%20graph%20neural%20networks%20%28GNNs%29.%20In%20this%20paper%2C%20by%20utilizing%20a%0Avector%20quantized-variational%20autoencoder%20%28VQ-VAE%29%2C%20we%20circumvent%20one%20of%20the%20key%0Adrawbacks%20of%20GMMs%2C%20i.e.%2C%20the%20number%20of%20GMM%20components%20scales%20exponentially%20to%0Athe%20feedback%20bits.%20In%20addition%2C%20the%20deep%20learning%20architecture%20of%20the%20VQ-VAE%0Aallows%20us%20to%20jointly%20train%20the%20GNN%20together%20with%20VQ-VAE%20along%20with%20pilot%0Aoptimization%20forming%20an%20end-to-end%20%28E2E%29%20model%2C%20resulting%20in%20considerable%0Aperformance%20gains%20in%20sum%20rate%20for%20multi-user%20wireless%20systems.%20Simulations%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20frameworks%20over%20the%20conventional%0Amethods%20involving%20the%20sub-discrete%20Fourier%20transform%20%28DFT%29%20pilot%20matrix%20and%0Aiterative%20precoder%20algorithms%20enabling%20the%20deployment%20of%20systems%20characterized%0Aby%20fewer%20pilots%20or%20feedback%20bits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrecoder%2520Design%2520in%2520Multi-User%2520FDD%2520Systems%2520with%2520VQ-VAE%2520and%2520GNN%26entry.906535625%3DSrikar%2520Allaparapu%2520and%2520Michael%2520Baur%2520and%2520Benedikt%2520B%25C3%25B6ck%2520and%2520Michael%2520Joham%2520and%2520Wolfgang%2520Utschick%26entry.1292438233%3D%2520%2520Robust%2520precoding%2520is%2520efficiently%2520feasible%2520in%2520frequency%2520division%2520duplex%2520%2528FDD%2529%250Asystems%2520by%2520incorporating%2520the%2520learnt%2520statistics%2520of%2520the%2520propagation%2520environment%250Athrough%2520a%2520generative%2520model.%2520We%2520build%2520on%2520previous%2520work%2520that%2520successfully%250Adesigned%2520site-specific%2520precoders%2520based%2520on%2520a%2520combination%2520of%2520Gaussian%2520mixture%250Amodels%2520%2528GMMs%2529%2520and%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520In%2520this%2520paper%252C%2520by%2520utilizing%2520a%250Avector%2520quantized-variational%2520autoencoder%2520%2528VQ-VAE%2529%252C%2520we%2520circumvent%2520one%2520of%2520the%2520key%250Adrawbacks%2520of%2520GMMs%252C%2520i.e.%252C%2520the%2520number%2520of%2520GMM%2520components%2520scales%2520exponentially%2520to%250Athe%2520feedback%2520bits.%2520In%2520addition%252C%2520the%2520deep%2520learning%2520architecture%2520of%2520the%2520VQ-VAE%250Aallows%2520us%2520to%2520jointly%2520train%2520the%2520GNN%2520together%2520with%2520VQ-VAE%2520along%2520with%2520pilot%250Aoptimization%2520forming%2520an%2520end-to-end%2520%2528E2E%2529%2520model%252C%2520resulting%2520in%2520considerable%250Aperformance%2520gains%2520in%2520sum%2520rate%2520for%2520multi-user%2520wireless%2520systems.%2520Simulations%250Ademonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520frameworks%2520over%2520the%2520conventional%250Amethods%2520involving%2520the%2520sub-discrete%2520Fourier%2520transform%2520%2528DFT%2529%2520pilot%2520matrix%2520and%250Aiterative%2520precoder%2520algorithms%2520enabling%2520the%2520deployment%2520of%2520systems%2520characterized%250Aby%2520fewer%2520pilots%2520or%2520feedback%2520bits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Precoder%20Design%20in%20Multi-User%20FDD%20Systems%20with%20VQ-VAE%20and%20GNN&entry.906535625=Srikar%20Allaparapu%20and%20Michael%20Baur%20and%20Benedikt%20B%C3%B6ck%20and%20Michael%20Joham%20and%20Wolfgang%20Utschick&entry.1292438233=%20%20Robust%20precoding%20is%20efficiently%20feasible%20in%20frequency%20division%20duplex%20%28FDD%29%0Asystems%20by%20incorporating%20the%20learnt%20statistics%20of%20the%20propagation%20environment%0Athrough%20a%20generative%20model.%20We%20build%20on%20previous%20work%20that%20successfully%0Adesigned%20site-specific%20precoders%20based%20on%20a%20combination%20of%20Gaussian%20mixture%0Amodels%20%28GMMs%29%20and%20graph%20neural%20networks%20%28GNNs%29.%20In%20this%20paper%2C%20by%20utilizing%20a%0Avector%20quantized-variational%20autoencoder%20%28VQ-VAE%29%2C%20we%20circumvent%20one%20of%20the%20key%0Adrawbacks%20of%20GMMs%2C%20i.e.%2C%20the%20number%20of%20GMM%20components%20scales%20exponentially%20to%0Athe%20feedback%20bits.%20In%20addition%2C%20the%20deep%20learning%20architecture%20of%20the%20VQ-VAE%0Aallows%20us%20to%20jointly%20train%20the%20GNN%20together%20with%20VQ-VAE%20along%20with%20pilot%0Aoptimization%20forming%20an%20end-to-end%20%28E2E%29%20model%2C%20resulting%20in%20considerable%0Aperformance%20gains%20in%20sum%20rate%20for%20multi-user%20wireless%20systems.%20Simulations%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20frameworks%20over%20the%20conventional%0Amethods%20involving%20the%20sub-discrete%20Fourier%20transform%20%28DFT%29%20pilot%20matrix%20and%0Aiterative%20precoder%20algorithms%20enabling%20the%20deployment%20of%20systems%20characterized%0Aby%20fewer%20pilots%20or%20feedback%20bits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09495v1&entry.124074799=Read"},
{"title": "CausalVLBench: Benchmarking Visual Causal Reasoning in Large\n  Vision-Language Models", "author": "Aneesh Komanduri and Karuna Bhaila and Xintao Wu", "abstract": "  Large language models (LLMs) have shown remarkable ability in various\nlanguage tasks, especially with their emergent in-context learning capability.\nExtending LLMs to incorporate visual inputs, large vision-language models\n(LVLMs) have shown impressive performance in tasks such as recognition and\nvisual question answering (VQA). Despite increasing interest in the utility of\nLLMs in causal reasoning tasks such as causal discovery and counterfactual\nreasoning, there has been relatively little work showcasing the abilities of\nLVLMs on visual causal reasoning tasks. We take this opportunity to formally\nintroduce a comprehensive causal reasoning benchmark for multi-modal in-context\nlearning from LVLMs. Our CausalVLBench encompasses three representative tasks:\ncausal structure inference, intervention target prediction, and counterfactual\nprediction. We evaluate the ability of state-of-the-art open-source LVLMs on\nour causal reasoning tasks across three causal representation learning datasets\nand demonstrate their fundamental strengths and weaknesses. We hope that our\nbenchmark elucidates the drawbacks of existing vision-language models and\nmotivates new directions and paradigms in improving the visual causal reasoning\nabilities of LVLMs.\n", "link": "http://arxiv.org/abs/2506.11034v2", "date": "2025-10-10", "relevancy": 2.2852, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalVLBench%3A%20Benchmarking%20Visual%20Causal%20Reasoning%20in%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20CausalVLBench%3A%20Benchmarking%20Visual%20Causal%20Reasoning%20in%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Aneesh%20Komanduri%20and%20Karuna%20Bhaila%20and%20Xintao%20Wu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20ability%20in%20various%0Alanguage%20tasks%2C%20especially%20with%20their%20emergent%20in-context%20learning%20capability.%0AExtending%20LLMs%20to%20incorporate%20visual%20inputs%2C%20large%20vision-language%20models%0A%28LVLMs%29%20have%20shown%20impressive%20performance%20in%20tasks%20such%20as%20recognition%20and%0Avisual%20question%20answering%20%28VQA%29.%20Despite%20increasing%20interest%20in%20the%20utility%20of%0ALLMs%20in%20causal%20reasoning%20tasks%20such%20as%20causal%20discovery%20and%20counterfactual%0Areasoning%2C%20there%20has%20been%20relatively%20little%20work%20showcasing%20the%20abilities%20of%0ALVLMs%20on%20visual%20causal%20reasoning%20tasks.%20We%20take%20this%20opportunity%20to%20formally%0Aintroduce%20a%20comprehensive%20causal%20reasoning%20benchmark%20for%20multi-modal%20in-context%0Alearning%20from%20LVLMs.%20Our%20CausalVLBench%20encompasses%20three%20representative%20tasks%3A%0Acausal%20structure%20inference%2C%20intervention%20target%20prediction%2C%20and%20counterfactual%0Aprediction.%20We%20evaluate%20the%20ability%20of%20state-of-the-art%20open-source%20LVLMs%20on%0Aour%20causal%20reasoning%20tasks%20across%20three%20causal%20representation%20learning%20datasets%0Aand%20demonstrate%20their%20fundamental%20strengths%20and%20weaknesses.%20We%20hope%20that%20our%0Abenchmark%20elucidates%20the%20drawbacks%20of%20existing%20vision-language%20models%20and%0Amotivates%20new%20directions%20and%20paradigms%20in%20improving%20the%20visual%20causal%20reasoning%0Aabilities%20of%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalVLBench%253A%2520Benchmarking%2520Visual%2520Causal%2520Reasoning%2520in%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DAneesh%2520Komanduri%2520and%2520Karuna%2520Bhaila%2520and%2520Xintao%2520Wu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520ability%2520in%2520various%250Alanguage%2520tasks%252C%2520especially%2520with%2520their%2520emergent%2520in-context%2520learning%2520capability.%250AExtending%2520LLMs%2520to%2520incorporate%2520visual%2520inputs%252C%2520large%2520vision-language%2520models%250A%2528LVLMs%2529%2520have%2520shown%2520impressive%2520performance%2520in%2520tasks%2520such%2520as%2520recognition%2520and%250Avisual%2520question%2520answering%2520%2528VQA%2529.%2520Despite%2520increasing%2520interest%2520in%2520the%2520utility%2520of%250ALLMs%2520in%2520causal%2520reasoning%2520tasks%2520such%2520as%2520causal%2520discovery%2520and%2520counterfactual%250Areasoning%252C%2520there%2520has%2520been%2520relatively%2520little%2520work%2520showcasing%2520the%2520abilities%2520of%250ALVLMs%2520on%2520visual%2520causal%2520reasoning%2520tasks.%2520We%2520take%2520this%2520opportunity%2520to%2520formally%250Aintroduce%2520a%2520comprehensive%2520causal%2520reasoning%2520benchmark%2520for%2520multi-modal%2520in-context%250Alearning%2520from%2520LVLMs.%2520Our%2520CausalVLBench%2520encompasses%2520three%2520representative%2520tasks%253A%250Acausal%2520structure%2520inference%252C%2520intervention%2520target%2520prediction%252C%2520and%2520counterfactual%250Aprediction.%2520We%2520evaluate%2520the%2520ability%2520of%2520state-of-the-art%2520open-source%2520LVLMs%2520on%250Aour%2520causal%2520reasoning%2520tasks%2520across%2520three%2520causal%2520representation%2520learning%2520datasets%250Aand%2520demonstrate%2520their%2520fundamental%2520strengths%2520and%2520weaknesses.%2520We%2520hope%2520that%2520our%250Abenchmark%2520elucidates%2520the%2520drawbacks%2520of%2520existing%2520vision-language%2520models%2520and%250Amotivates%2520new%2520directions%2520and%2520paradigms%2520in%2520improving%2520the%2520visual%2520causal%2520reasoning%250Aabilities%2520of%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalVLBench%3A%20Benchmarking%20Visual%20Causal%20Reasoning%20in%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Aneesh%20Komanduri%20and%20Karuna%20Bhaila%20and%20Xintao%20Wu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20ability%20in%20various%0Alanguage%20tasks%2C%20especially%20with%20their%20emergent%20in-context%20learning%20capability.%0AExtending%20LLMs%20to%20incorporate%20visual%20inputs%2C%20large%20vision-language%20models%0A%28LVLMs%29%20have%20shown%20impressive%20performance%20in%20tasks%20such%20as%20recognition%20and%0Avisual%20question%20answering%20%28VQA%29.%20Despite%20increasing%20interest%20in%20the%20utility%20of%0ALLMs%20in%20causal%20reasoning%20tasks%20such%20as%20causal%20discovery%20and%20counterfactual%0Areasoning%2C%20there%20has%20been%20relatively%20little%20work%20showcasing%20the%20abilities%20of%0ALVLMs%20on%20visual%20causal%20reasoning%20tasks.%20We%20take%20this%20opportunity%20to%20formally%0Aintroduce%20a%20comprehensive%20causal%20reasoning%20benchmark%20for%20multi-modal%20in-context%0Alearning%20from%20LVLMs.%20Our%20CausalVLBench%20encompasses%20three%20representative%20tasks%3A%0Acausal%20structure%20inference%2C%20intervention%20target%20prediction%2C%20and%20counterfactual%0Aprediction.%20We%20evaluate%20the%20ability%20of%20state-of-the-art%20open-source%20LVLMs%20on%0Aour%20causal%20reasoning%20tasks%20across%20three%20causal%20representation%20learning%20datasets%0Aand%20demonstrate%20their%20fundamental%20strengths%20and%20weaknesses.%20We%20hope%20that%20our%0Abenchmark%20elucidates%20the%20drawbacks%20of%20existing%20vision-language%20models%20and%0Amotivates%20new%20directions%20and%20paradigms%20in%20improving%20the%20visual%20causal%20reasoning%0Aabilities%20of%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11034v2&entry.124074799=Read"},
{"title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams", "author": "Ruyi Xu and Guangxuan Xiao and Yukang Chen and Liuning He and Kelly Peng and Yao Lu and Song Han", "abstract": "  Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.\n", "link": "http://arxiv.org/abs/2510.09608v1", "date": "2025-10-10", "relevancy": 2.2816, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamingVLM%3A%20Real-Time%20Understanding%20for%20Infinite%20Video%20Streams&body=Title%3A%20StreamingVLM%3A%20Real-Time%20Understanding%20for%20Infinite%20Video%20Streams%0AAuthor%3A%20Ruyi%20Xu%20and%20Guangxuan%20Xiao%20and%20Yukang%20Chen%20and%20Liuning%20He%20and%20Kelly%20Peng%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20could%20power%20real-time%20assistants%20and%20autonomous%0Aagents%2C%20but%20they%20face%20a%20critical%20challenge%3A%20understanding%20near-infinite%20video%0Astreams%20without%20escalating%20latency%20and%20memory%20usage.%20Processing%20entire%20videos%0Awith%20full%20attention%20leads%20to%20quadratic%20computational%20costs%20and%20poor%20performance%0Aon%20long%20videos.%20Meanwhile%2C%20simple%20sliding%20window%20methods%20are%20also%20flawed%2C%20as%0Athey%20either%20break%20coherence%20or%20suffer%20from%20high%20latency%20due%20to%20redundant%0Arecomputation.%20In%20this%20paper%2C%20we%20introduce%20StreamingVLM%2C%20a%20model%20designed%20for%0Areal-time%2C%20stable%20understanding%20of%20infinite%20visual%20input.%20Our%20approach%20is%20a%0Aunified%20framework%20that%20aligns%20training%20with%20streaming%20inference.%20During%0Ainference%2C%20we%20maintain%20a%20compact%20KV%20cache%20by%20reusing%20states%20of%20attention%20sinks%2C%0Aa%20short%20window%20of%20recent%20vision%20tokens%2C%20and%20a%20long%20window%20of%20recent%20text%0Atokens.%20This%20streaming%20ability%20is%20instilled%20via%20a%20simple%20supervised%20fine-tuning%0A%28SFT%29%20strategy%20that%20applies%20full%20attention%20on%20short%2C%20overlapped%20video%20chunks%2C%0Awhich%20effectively%20mimics%20the%20inference-time%20attention%20pattern%20without%20training%0Aon%20prohibitively%20long%20contexts.%20For%20evaluation%2C%20we%20build%20Inf-Streams-Eval%2C%20a%0Anew%20benchmark%20with%20videos%20averaging%20over%20two%20hours%20that%20requires%20dense%2C%0Aper-second%20alignment%20between%20frames%20and%20text.%20On%20Inf-Streams-Eval%2C%20StreamingVLM%0Aachieves%20a%2066.18%25%20win%20rate%20against%20GPT-4O%20mini%20and%20maintains%20stable%2C%20real-time%0Aperformance%20at%20up%20to%208%20FPS%20on%20a%20single%20NVIDIA%20H100.%20Notably%2C%20our%20SFT%20strategy%0Aalso%20enhances%20general%20VQA%20abilities%20without%20any%20VQA-specific%20fine-tuning%2C%0Aimproving%20performance%20on%20LongVideoBench%20by%20%2B4.30%20and%20OVOBench%20Realtime%20by%0A%2B5.96.%20Code%20is%20available%20at%20https%3A//github.com/mit-han-lab/streaming-vlm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamingVLM%253A%2520Real-Time%2520Understanding%2520for%2520Infinite%2520Video%2520Streams%26entry.906535625%3DRuyi%2520Xu%2520and%2520Guangxuan%2520Xiao%2520and%2520Yukang%2520Chen%2520and%2520Liuning%2520He%2520and%2520Kelly%2520Peng%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520could%2520power%2520real-time%2520assistants%2520and%2520autonomous%250Aagents%252C%2520but%2520they%2520face%2520a%2520critical%2520challenge%253A%2520understanding%2520near-infinite%2520video%250Astreams%2520without%2520escalating%2520latency%2520and%2520memory%2520usage.%2520Processing%2520entire%2520videos%250Awith%2520full%2520attention%2520leads%2520to%2520quadratic%2520computational%2520costs%2520and%2520poor%2520performance%250Aon%2520long%2520videos.%2520Meanwhile%252C%2520simple%2520sliding%2520window%2520methods%2520are%2520also%2520flawed%252C%2520as%250Athey%2520either%2520break%2520coherence%2520or%2520suffer%2520from%2520high%2520latency%2520due%2520to%2520redundant%250Arecomputation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520StreamingVLM%252C%2520a%2520model%2520designed%2520for%250Areal-time%252C%2520stable%2520understanding%2520of%2520infinite%2520visual%2520input.%2520Our%2520approach%2520is%2520a%250Aunified%2520framework%2520that%2520aligns%2520training%2520with%2520streaming%2520inference.%2520During%250Ainference%252C%2520we%2520maintain%2520a%2520compact%2520KV%2520cache%2520by%2520reusing%2520states%2520of%2520attention%2520sinks%252C%250Aa%2520short%2520window%2520of%2520recent%2520vision%2520tokens%252C%2520and%2520a%2520long%2520window%2520of%2520recent%2520text%250Atokens.%2520This%2520streaming%2520ability%2520is%2520instilled%2520via%2520a%2520simple%2520supervised%2520fine-tuning%250A%2528SFT%2529%2520strategy%2520that%2520applies%2520full%2520attention%2520on%2520short%252C%2520overlapped%2520video%2520chunks%252C%250Awhich%2520effectively%2520mimics%2520the%2520inference-time%2520attention%2520pattern%2520without%2520training%250Aon%2520prohibitively%2520long%2520contexts.%2520For%2520evaluation%252C%2520we%2520build%2520Inf-Streams-Eval%252C%2520a%250Anew%2520benchmark%2520with%2520videos%2520averaging%2520over%2520two%2520hours%2520that%2520requires%2520dense%252C%250Aper-second%2520alignment%2520between%2520frames%2520and%2520text.%2520On%2520Inf-Streams-Eval%252C%2520StreamingVLM%250Aachieves%2520a%252066.18%2525%2520win%2520rate%2520against%2520GPT-4O%2520mini%2520and%2520maintains%2520stable%252C%2520real-time%250Aperformance%2520at%2520up%2520to%25208%2520FPS%2520on%2520a%2520single%2520NVIDIA%2520H100.%2520Notably%252C%2520our%2520SFT%2520strategy%250Aalso%2520enhances%2520general%2520VQA%2520abilities%2520without%2520any%2520VQA-specific%2520fine-tuning%252C%250Aimproving%2520performance%2520on%2520LongVideoBench%2520by%2520%252B4.30%2520and%2520OVOBench%2520Realtime%2520by%250A%252B5.96.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/mit-han-lab/streaming-vlm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamingVLM%3A%20Real-Time%20Understanding%20for%20Infinite%20Video%20Streams&entry.906535625=Ruyi%20Xu%20and%20Guangxuan%20Xiao%20and%20Yukang%20Chen%20and%20Liuning%20He%20and%20Kelly%20Peng%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20could%20power%20real-time%20assistants%20and%20autonomous%0Aagents%2C%20but%20they%20face%20a%20critical%20challenge%3A%20understanding%20near-infinite%20video%0Astreams%20without%20escalating%20latency%20and%20memory%20usage.%20Processing%20entire%20videos%0Awith%20full%20attention%20leads%20to%20quadratic%20computational%20costs%20and%20poor%20performance%0Aon%20long%20videos.%20Meanwhile%2C%20simple%20sliding%20window%20methods%20are%20also%20flawed%2C%20as%0Athey%20either%20break%20coherence%20or%20suffer%20from%20high%20latency%20due%20to%20redundant%0Arecomputation.%20In%20this%20paper%2C%20we%20introduce%20StreamingVLM%2C%20a%20model%20designed%20for%0Areal-time%2C%20stable%20understanding%20of%20infinite%20visual%20input.%20Our%20approach%20is%20a%0Aunified%20framework%20that%20aligns%20training%20with%20streaming%20inference.%20During%0Ainference%2C%20we%20maintain%20a%20compact%20KV%20cache%20by%20reusing%20states%20of%20attention%20sinks%2C%0Aa%20short%20window%20of%20recent%20vision%20tokens%2C%20and%20a%20long%20window%20of%20recent%20text%0Atokens.%20This%20streaming%20ability%20is%20instilled%20via%20a%20simple%20supervised%20fine-tuning%0A%28SFT%29%20strategy%20that%20applies%20full%20attention%20on%20short%2C%20overlapped%20video%20chunks%2C%0Awhich%20effectively%20mimics%20the%20inference-time%20attention%20pattern%20without%20training%0Aon%20prohibitively%20long%20contexts.%20For%20evaluation%2C%20we%20build%20Inf-Streams-Eval%2C%20a%0Anew%20benchmark%20with%20videos%20averaging%20over%20two%20hours%20that%20requires%20dense%2C%0Aper-second%20alignment%20between%20frames%20and%20text.%20On%20Inf-Streams-Eval%2C%20StreamingVLM%0Aachieves%20a%2066.18%25%20win%20rate%20against%20GPT-4O%20mini%20and%20maintains%20stable%2C%20real-time%0Aperformance%20at%20up%20to%208%20FPS%20on%20a%20single%20NVIDIA%20H100.%20Notably%2C%20our%20SFT%20strategy%0Aalso%20enhances%20general%20VQA%20abilities%20without%20any%20VQA-specific%20fine-tuning%2C%0Aimproving%20performance%20on%20LongVideoBench%20by%20%2B4.30%20and%20OVOBench%20Realtime%20by%0A%2B5.96.%20Code%20is%20available%20at%20https%3A//github.com/mit-han-lab/streaming-vlm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09608v1&entry.124074799=Read"},
{"title": "Randomized HyperSteiner: A Stochastic Delaunay Triangulation Heuristic\n  for the Hyperbolic Steiner Minimal Tree", "author": "Aniss Aiman Medbouhi and Alejandro Garc\u00eda-Castellanos and Giovanni Luca Marchetti and Daniel Pelt and Erik J Bekkers and Danica Kragic", "abstract": "  We study the problem of constructing Steiner Minimal Trees (SMTs) in\nhyperbolic space. Exact SMT computation is NP-hard, and existing hyperbolic\nheuristics such as HyperSteiner are deterministic and often get trapped in\nlocally suboptimal configurations. We introduce Randomized HyperSteiner (RHS),\na stochastic Delaunay triangulation heuristic that incorporates randomness into\nthe expansion process and refines candidate trees via Riemannian gradient\ndescent optimization. Experiments on synthetic data sets and a real-world\nsingle-cell transcriptomic data show that RHS outperforms Minimum Spanning Tree\n(MST), Neighbour Joining, and vanilla HyperSteiner (HS). In near-boundary\nconfigurations, RHS can achieve a 32% reduction in total length over HS,\ndemonstrating its effectiveness and robustness in diverse data regimes.\n", "link": "http://arxiv.org/abs/2510.09328v1", "date": "2025-10-10", "relevancy": 2.2762, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4666}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.456}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Randomized%20HyperSteiner%3A%20A%20Stochastic%20Delaunay%20Triangulation%20Heuristic%0A%20%20for%20the%20Hyperbolic%20Steiner%20Minimal%20Tree&body=Title%3A%20Randomized%20HyperSteiner%3A%20A%20Stochastic%20Delaunay%20Triangulation%20Heuristic%0A%20%20for%20the%20Hyperbolic%20Steiner%20Minimal%20Tree%0AAuthor%3A%20Aniss%20Aiman%20Medbouhi%20and%20Alejandro%20Garc%C3%ADa-Castellanos%20and%20Giovanni%20Luca%20Marchetti%20and%20Daniel%20Pelt%20and%20Erik%20J%20Bekkers%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20constructing%20Steiner%20Minimal%20Trees%20%28SMTs%29%20in%0Ahyperbolic%20space.%20Exact%20SMT%20computation%20is%20NP-hard%2C%20and%20existing%20hyperbolic%0Aheuristics%20such%20as%20HyperSteiner%20are%20deterministic%20and%20often%20get%20trapped%20in%0Alocally%20suboptimal%20configurations.%20We%20introduce%20Randomized%20HyperSteiner%20%28RHS%29%2C%0Aa%20stochastic%20Delaunay%20triangulation%20heuristic%20that%20incorporates%20randomness%20into%0Athe%20expansion%20process%20and%20refines%20candidate%20trees%20via%20Riemannian%20gradient%0Adescent%20optimization.%20Experiments%20on%20synthetic%20data%20sets%20and%20a%20real-world%0Asingle-cell%20transcriptomic%20data%20show%20that%20RHS%20outperforms%20Minimum%20Spanning%20Tree%0A%28MST%29%2C%20Neighbour%20Joining%2C%20and%20vanilla%20HyperSteiner%20%28HS%29.%20In%20near-boundary%0Aconfigurations%2C%20RHS%20can%20achieve%20a%2032%25%20reduction%20in%20total%20length%20over%20HS%2C%0Ademonstrating%20its%20effectiveness%20and%20robustness%20in%20diverse%20data%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandomized%2520HyperSteiner%253A%2520A%2520Stochastic%2520Delaunay%2520Triangulation%2520Heuristic%250A%2520%2520for%2520the%2520Hyperbolic%2520Steiner%2520Minimal%2520Tree%26entry.906535625%3DAniss%2520Aiman%2520Medbouhi%2520and%2520Alejandro%2520Garc%25C3%25ADa-Castellanos%2520and%2520Giovanni%2520Luca%2520Marchetti%2520and%2520Daniel%2520Pelt%2520and%2520Erik%2520J%2520Bekkers%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520constructing%2520Steiner%2520Minimal%2520Trees%2520%2528SMTs%2529%2520in%250Ahyperbolic%2520space.%2520Exact%2520SMT%2520computation%2520is%2520NP-hard%252C%2520and%2520existing%2520hyperbolic%250Aheuristics%2520such%2520as%2520HyperSteiner%2520are%2520deterministic%2520and%2520often%2520get%2520trapped%2520in%250Alocally%2520suboptimal%2520configurations.%2520We%2520introduce%2520Randomized%2520HyperSteiner%2520%2528RHS%2529%252C%250Aa%2520stochastic%2520Delaunay%2520triangulation%2520heuristic%2520that%2520incorporates%2520randomness%2520into%250Athe%2520expansion%2520process%2520and%2520refines%2520candidate%2520trees%2520via%2520Riemannian%2520gradient%250Adescent%2520optimization.%2520Experiments%2520on%2520synthetic%2520data%2520sets%2520and%2520a%2520real-world%250Asingle-cell%2520transcriptomic%2520data%2520show%2520that%2520RHS%2520outperforms%2520Minimum%2520Spanning%2520Tree%250A%2528MST%2529%252C%2520Neighbour%2520Joining%252C%2520and%2520vanilla%2520HyperSteiner%2520%2528HS%2529.%2520In%2520near-boundary%250Aconfigurations%252C%2520RHS%2520can%2520achieve%2520a%252032%2525%2520reduction%2520in%2520total%2520length%2520over%2520HS%252C%250Ademonstrating%2520its%2520effectiveness%2520and%2520robustness%2520in%2520diverse%2520data%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomized%20HyperSteiner%3A%20A%20Stochastic%20Delaunay%20Triangulation%20Heuristic%0A%20%20for%20the%20Hyperbolic%20Steiner%20Minimal%20Tree&entry.906535625=Aniss%20Aiman%20Medbouhi%20and%20Alejandro%20Garc%C3%ADa-Castellanos%20and%20Giovanni%20Luca%20Marchetti%20and%20Daniel%20Pelt%20and%20Erik%20J%20Bekkers%20and%20Danica%20Kragic&entry.1292438233=%20%20We%20study%20the%20problem%20of%20constructing%20Steiner%20Minimal%20Trees%20%28SMTs%29%20in%0Ahyperbolic%20space.%20Exact%20SMT%20computation%20is%20NP-hard%2C%20and%20existing%20hyperbolic%0Aheuristics%20such%20as%20HyperSteiner%20are%20deterministic%20and%20often%20get%20trapped%20in%0Alocally%20suboptimal%20configurations.%20We%20introduce%20Randomized%20HyperSteiner%20%28RHS%29%2C%0Aa%20stochastic%20Delaunay%20triangulation%20heuristic%20that%20incorporates%20randomness%20into%0Athe%20expansion%20process%20and%20refines%20candidate%20trees%20via%20Riemannian%20gradient%0Adescent%20optimization.%20Experiments%20on%20synthetic%20data%20sets%20and%20a%20real-world%0Asingle-cell%20transcriptomic%20data%20show%20that%20RHS%20outperforms%20Minimum%20Spanning%20Tree%0A%28MST%29%2C%20Neighbour%20Joining%2C%20and%20vanilla%20HyperSteiner%20%28HS%29.%20In%20near-boundary%0Aconfigurations%2C%20RHS%20can%20achieve%20a%2032%25%20reduction%20in%20total%20length%20over%20HS%2C%0Ademonstrating%20its%20effectiveness%20and%20robustness%20in%20diverse%20data%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09328v1&entry.124074799=Read"},
{"title": "Locally Optimal Private Sampling: Beyond the Global Minimax", "author": "Hrad Ghoukasian and Bonwoo Lee and Shahab Asoodeh", "abstract": "  We study the problem of sampling from a distribution under local differential\nprivacy (LDP). Given a private distribution $P \\in \\mathcal{P}$, the goal is to\ngenerate a single sample from a distribution that remains close to $P$ in\n$f$-divergence while satisfying the constraints of LDP. This task captures the\nfundamental challenge of producing realistic-looking data under strong privacy\nguarantees. While prior work by Park et al. (NeurIPS'24) focuses on global\nminimax-optimality across a class of distributions, we take a local\nperspective. Specifically, we examine the minimax risk in a neighborhood around\na fixed distribution $P_0$, and characterize its exact value, which depends on\nboth $P_0$ and the privacy level. Our main result shows that the local minimax\nrisk is determined by the global minimax risk when the distribution class\n$\\mathcal{P}$ is restricted to a neighborhood around $P_0$. To establish this,\nwe (1) extend previous work from pure LDP to the more general functional LDP\nframework, and (2) prove that the globally optimal functional LDP sampler\nyields the optimal local sampler when constrained to distributions near $P_0$.\nBuilding on this, we also derive a simple closed-form expression for the\nlocally minimax-optimal samplers which does not depend on the choice of\n$f$-divergence. We further argue that this local framework naturally models\nprivate sampling with public data, where the public data distribution is\nrepresented by $P_0$. In this setting, we empirically compare our locally\noptimal sampler to existing global methods, and demonstrate that it\nconsistently outperforms global minimax samplers.\n", "link": "http://arxiv.org/abs/2510.09485v1", "date": "2025-10-10", "relevancy": 2.2621, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4639}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4622}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Optimal%20Private%20Sampling%3A%20Beyond%20the%20Global%20Minimax&body=Title%3A%20Locally%20Optimal%20Private%20Sampling%3A%20Beyond%20the%20Global%20Minimax%0AAuthor%3A%20Hrad%20Ghoukasian%20and%20Bonwoo%20Lee%20and%20Shahab%20Asoodeh%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20sampling%20from%20a%20distribution%20under%20local%20differential%0Aprivacy%20%28LDP%29.%20Given%20a%20private%20distribution%20%24P%20%5Cin%20%5Cmathcal%7BP%7D%24%2C%20the%20goal%20is%20to%0Agenerate%20a%20single%20sample%20from%20a%20distribution%20that%20remains%20close%20to%20%24P%24%20in%0A%24f%24-divergence%20while%20satisfying%20the%20constraints%20of%20LDP.%20This%20task%20captures%20the%0Afundamental%20challenge%20of%20producing%20realistic-looking%20data%20under%20strong%20privacy%0Aguarantees.%20While%20prior%20work%20by%20Park%20et%20al.%20%28NeurIPS%2724%29%20focuses%20on%20global%0Aminimax-optimality%20across%20a%20class%20of%20distributions%2C%20we%20take%20a%20local%0Aperspective.%20Specifically%2C%20we%20examine%20the%20minimax%20risk%20in%20a%20neighborhood%20around%0Aa%20fixed%20distribution%20%24P_0%24%2C%20and%20characterize%20its%20exact%20value%2C%20which%20depends%20on%0Aboth%20%24P_0%24%20and%20the%20privacy%20level.%20Our%20main%20result%20shows%20that%20the%20local%20minimax%0Arisk%20is%20determined%20by%20the%20global%20minimax%20risk%20when%20the%20distribution%20class%0A%24%5Cmathcal%7BP%7D%24%20is%20restricted%20to%20a%20neighborhood%20around%20%24P_0%24.%20To%20establish%20this%2C%0Awe%20%281%29%20extend%20previous%20work%20from%20pure%20LDP%20to%20the%20more%20general%20functional%20LDP%0Aframework%2C%20and%20%282%29%20prove%20that%20the%20globally%20optimal%20functional%20LDP%20sampler%0Ayields%20the%20optimal%20local%20sampler%20when%20constrained%20to%20distributions%20near%20%24P_0%24.%0ABuilding%20on%20this%2C%20we%20also%20derive%20a%20simple%20closed-form%20expression%20for%20the%0Alocally%20minimax-optimal%20samplers%20which%20does%20not%20depend%20on%20the%20choice%20of%0A%24f%24-divergence.%20We%20further%20argue%20that%20this%20local%20framework%20naturally%20models%0Aprivate%20sampling%20with%20public%20data%2C%20where%20the%20public%20data%20distribution%20is%0Arepresented%20by%20%24P_0%24.%20In%20this%20setting%2C%20we%20empirically%20compare%20our%20locally%0Aoptimal%20sampler%20to%20existing%20global%20methods%2C%20and%20demonstrate%20that%20it%0Aconsistently%20outperforms%20global%20minimax%20samplers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Optimal%2520Private%2520Sampling%253A%2520Beyond%2520the%2520Global%2520Minimax%26entry.906535625%3DHrad%2520Ghoukasian%2520and%2520Bonwoo%2520Lee%2520and%2520Shahab%2520Asoodeh%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520sampling%2520from%2520a%2520distribution%2520under%2520local%2520differential%250Aprivacy%2520%2528LDP%2529.%2520Given%2520a%2520private%2520distribution%2520%2524P%2520%255Cin%2520%255Cmathcal%257BP%257D%2524%252C%2520the%2520goal%2520is%2520to%250Agenerate%2520a%2520single%2520sample%2520from%2520a%2520distribution%2520that%2520remains%2520close%2520to%2520%2524P%2524%2520in%250A%2524f%2524-divergence%2520while%2520satisfying%2520the%2520constraints%2520of%2520LDP.%2520This%2520task%2520captures%2520the%250Afundamental%2520challenge%2520of%2520producing%2520realistic-looking%2520data%2520under%2520strong%2520privacy%250Aguarantees.%2520While%2520prior%2520work%2520by%2520Park%2520et%2520al.%2520%2528NeurIPS%252724%2529%2520focuses%2520on%2520global%250Aminimax-optimality%2520across%2520a%2520class%2520of%2520distributions%252C%2520we%2520take%2520a%2520local%250Aperspective.%2520Specifically%252C%2520we%2520examine%2520the%2520minimax%2520risk%2520in%2520a%2520neighborhood%2520around%250Aa%2520fixed%2520distribution%2520%2524P_0%2524%252C%2520and%2520characterize%2520its%2520exact%2520value%252C%2520which%2520depends%2520on%250Aboth%2520%2524P_0%2524%2520and%2520the%2520privacy%2520level.%2520Our%2520main%2520result%2520shows%2520that%2520the%2520local%2520minimax%250Arisk%2520is%2520determined%2520by%2520the%2520global%2520minimax%2520risk%2520when%2520the%2520distribution%2520class%250A%2524%255Cmathcal%257BP%257D%2524%2520is%2520restricted%2520to%2520a%2520neighborhood%2520around%2520%2524P_0%2524.%2520To%2520establish%2520this%252C%250Awe%2520%25281%2529%2520extend%2520previous%2520work%2520from%2520pure%2520LDP%2520to%2520the%2520more%2520general%2520functional%2520LDP%250Aframework%252C%2520and%2520%25282%2529%2520prove%2520that%2520the%2520globally%2520optimal%2520functional%2520LDP%2520sampler%250Ayields%2520the%2520optimal%2520local%2520sampler%2520when%2520constrained%2520to%2520distributions%2520near%2520%2524P_0%2524.%250ABuilding%2520on%2520this%252C%2520we%2520also%2520derive%2520a%2520simple%2520closed-form%2520expression%2520for%2520the%250Alocally%2520minimax-optimal%2520samplers%2520which%2520does%2520not%2520depend%2520on%2520the%2520choice%2520of%250A%2524f%2524-divergence.%2520We%2520further%2520argue%2520that%2520this%2520local%2520framework%2520naturally%2520models%250Aprivate%2520sampling%2520with%2520public%2520data%252C%2520where%2520the%2520public%2520data%2520distribution%2520is%250Arepresented%2520by%2520%2524P_0%2524.%2520In%2520this%2520setting%252C%2520we%2520empirically%2520compare%2520our%2520locally%250Aoptimal%2520sampler%2520to%2520existing%2520global%2520methods%252C%2520and%2520demonstrate%2520that%2520it%250Aconsistently%2520outperforms%2520global%2520minimax%2520samplers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Optimal%20Private%20Sampling%3A%20Beyond%20the%20Global%20Minimax&entry.906535625=Hrad%20Ghoukasian%20and%20Bonwoo%20Lee%20and%20Shahab%20Asoodeh&entry.1292438233=%20%20We%20study%20the%20problem%20of%20sampling%20from%20a%20distribution%20under%20local%20differential%0Aprivacy%20%28LDP%29.%20Given%20a%20private%20distribution%20%24P%20%5Cin%20%5Cmathcal%7BP%7D%24%2C%20the%20goal%20is%20to%0Agenerate%20a%20single%20sample%20from%20a%20distribution%20that%20remains%20close%20to%20%24P%24%20in%0A%24f%24-divergence%20while%20satisfying%20the%20constraints%20of%20LDP.%20This%20task%20captures%20the%0Afundamental%20challenge%20of%20producing%20realistic-looking%20data%20under%20strong%20privacy%0Aguarantees.%20While%20prior%20work%20by%20Park%20et%20al.%20%28NeurIPS%2724%29%20focuses%20on%20global%0Aminimax-optimality%20across%20a%20class%20of%20distributions%2C%20we%20take%20a%20local%0Aperspective.%20Specifically%2C%20we%20examine%20the%20minimax%20risk%20in%20a%20neighborhood%20around%0Aa%20fixed%20distribution%20%24P_0%24%2C%20and%20characterize%20its%20exact%20value%2C%20which%20depends%20on%0Aboth%20%24P_0%24%20and%20the%20privacy%20level.%20Our%20main%20result%20shows%20that%20the%20local%20minimax%0Arisk%20is%20determined%20by%20the%20global%20minimax%20risk%20when%20the%20distribution%20class%0A%24%5Cmathcal%7BP%7D%24%20is%20restricted%20to%20a%20neighborhood%20around%20%24P_0%24.%20To%20establish%20this%2C%0Awe%20%281%29%20extend%20previous%20work%20from%20pure%20LDP%20to%20the%20more%20general%20functional%20LDP%0Aframework%2C%20and%20%282%29%20prove%20that%20the%20globally%20optimal%20functional%20LDP%20sampler%0Ayields%20the%20optimal%20local%20sampler%20when%20constrained%20to%20distributions%20near%20%24P_0%24.%0ABuilding%20on%20this%2C%20we%20also%20derive%20a%20simple%20closed-form%20expression%20for%20the%0Alocally%20minimax-optimal%20samplers%20which%20does%20not%20depend%20on%20the%20choice%20of%0A%24f%24-divergence.%20We%20further%20argue%20that%20this%20local%20framework%20naturally%20models%0Aprivate%20sampling%20with%20public%20data%2C%20where%20the%20public%20data%20distribution%20is%0Arepresented%20by%20%24P_0%24.%20In%20this%20setting%2C%20we%20empirically%20compare%20our%20locally%0Aoptimal%20sampler%20to%20existing%20global%20methods%2C%20and%20demonstrate%20that%20it%0Aconsistently%20outperforms%20global%20minimax%20samplers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09485v1&entry.124074799=Read"},
{"title": "Placeit! A Framework for Learning Robot Object Placement Skills", "author": "Amina Ferrad and Johann Huber and Fran\u00e7ois H\u00e9l\u00e9non and Julien Gleyze and Mahdi Khoramshahi and St\u00e9phane Doncieux", "abstract": "  Robotics research has made significant strides in learning, yet mastering\nbasic skills like object placement remains a fundamental challenge. A key\nbottleneck is the acquisition of large-scale, high-quality data, which is often\na manual and laborious process. Inspired by Graspit!, a foundational work that\nused simulation to automatically generate dexterous grasp poses, we introduce\nPlaceit!, an evolutionary-computation framework for generating valid placement\npositions for rigid objects. Placeit! is highly versatile, supporting tasks\nfrom placing objects on tables to stacking and inserting them. Our experiments\nshow that by leveraging quality-diversity optimization, Placeit! significantly\noutperforms state-of-the-art methods across all scenarios for generating\ndiverse valid poses. A pick&place pipeline built on our framework achieved a\n90% success rate over 120 real-world deployments. This work positions Placeit!\nas a powerful tool for open-environment pick-and-place tasks and as a valuable\nengine for generating the data needed to train simulation-based foundation\nmodels in robotics.\n", "link": "http://arxiv.org/abs/2510.09267v1", "date": "2025-10-10", "relevancy": 2.2512, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5713}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5706}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Placeit%21%20A%20Framework%20for%20Learning%20Robot%20Object%20Placement%20Skills&body=Title%3A%20Placeit%21%20A%20Framework%20for%20Learning%20Robot%20Object%20Placement%20Skills%0AAuthor%3A%20Amina%20Ferrad%20and%20Johann%20Huber%20and%20Fran%C3%A7ois%20H%C3%A9l%C3%A9non%20and%20Julien%20Gleyze%20and%20Mahdi%20Khoramshahi%20and%20St%C3%A9phane%20Doncieux%0AAbstract%3A%20%20%20Robotics%20research%20has%20made%20significant%20strides%20in%20learning%2C%20yet%20mastering%0Abasic%20skills%20like%20object%20placement%20remains%20a%20fundamental%20challenge.%20A%20key%0Abottleneck%20is%20the%20acquisition%20of%20large-scale%2C%20high-quality%20data%2C%20which%20is%20often%0Aa%20manual%20and%20laborious%20process.%20Inspired%20by%20Graspit%21%2C%20a%20foundational%20work%20that%0Aused%20simulation%20to%20automatically%20generate%20dexterous%20grasp%20poses%2C%20we%20introduce%0APlaceit%21%2C%20an%20evolutionary-computation%20framework%20for%20generating%20valid%20placement%0Apositions%20for%20rigid%20objects.%20Placeit%21%20is%20highly%20versatile%2C%20supporting%20tasks%0Afrom%20placing%20objects%20on%20tables%20to%20stacking%20and%20inserting%20them.%20Our%20experiments%0Ashow%20that%20by%20leveraging%20quality-diversity%20optimization%2C%20Placeit%21%20significantly%0Aoutperforms%20state-of-the-art%20methods%20across%20all%20scenarios%20for%20generating%0Adiverse%20valid%20poses.%20A%20pick%26place%20pipeline%20built%20on%20our%20framework%20achieved%20a%0A90%25%20success%20rate%20over%20120%20real-world%20deployments.%20This%20work%20positions%20Placeit%21%0Aas%20a%20powerful%20tool%20for%20open-environment%20pick-and-place%20tasks%20and%20as%20a%20valuable%0Aengine%20for%20generating%20the%20data%20needed%20to%20train%20simulation-based%20foundation%0Amodels%20in%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlaceit%2521%2520A%2520Framework%2520for%2520Learning%2520Robot%2520Object%2520Placement%2520Skills%26entry.906535625%3DAmina%2520Ferrad%2520and%2520Johann%2520Huber%2520and%2520Fran%25C3%25A7ois%2520H%25C3%25A9l%25C3%25A9non%2520and%2520Julien%2520Gleyze%2520and%2520Mahdi%2520Khoramshahi%2520and%2520St%25C3%25A9phane%2520Doncieux%26entry.1292438233%3D%2520%2520Robotics%2520research%2520has%2520made%2520significant%2520strides%2520in%2520learning%252C%2520yet%2520mastering%250Abasic%2520skills%2520like%2520object%2520placement%2520remains%2520a%2520fundamental%2520challenge.%2520A%2520key%250Abottleneck%2520is%2520the%2520acquisition%2520of%2520large-scale%252C%2520high-quality%2520data%252C%2520which%2520is%2520often%250Aa%2520manual%2520and%2520laborious%2520process.%2520Inspired%2520by%2520Graspit%2521%252C%2520a%2520foundational%2520work%2520that%250Aused%2520simulation%2520to%2520automatically%2520generate%2520dexterous%2520grasp%2520poses%252C%2520we%2520introduce%250APlaceit%2521%252C%2520an%2520evolutionary-computation%2520framework%2520for%2520generating%2520valid%2520placement%250Apositions%2520for%2520rigid%2520objects.%2520Placeit%2521%2520is%2520highly%2520versatile%252C%2520supporting%2520tasks%250Afrom%2520placing%2520objects%2520on%2520tables%2520to%2520stacking%2520and%2520inserting%2520them.%2520Our%2520experiments%250Ashow%2520that%2520by%2520leveraging%2520quality-diversity%2520optimization%252C%2520Placeit%2521%2520significantly%250Aoutperforms%2520state-of-the-art%2520methods%2520across%2520all%2520scenarios%2520for%2520generating%250Adiverse%2520valid%2520poses.%2520A%2520pick%2526place%2520pipeline%2520built%2520on%2520our%2520framework%2520achieved%2520a%250A90%2525%2520success%2520rate%2520over%2520120%2520real-world%2520deployments.%2520This%2520work%2520positions%2520Placeit%2521%250Aas%2520a%2520powerful%2520tool%2520for%2520open-environment%2520pick-and-place%2520tasks%2520and%2520as%2520a%2520valuable%250Aengine%2520for%2520generating%2520the%2520data%2520needed%2520to%2520train%2520simulation-based%2520foundation%250Amodels%2520in%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Placeit%21%20A%20Framework%20for%20Learning%20Robot%20Object%20Placement%20Skills&entry.906535625=Amina%20Ferrad%20and%20Johann%20Huber%20and%20Fran%C3%A7ois%20H%C3%A9l%C3%A9non%20and%20Julien%20Gleyze%20and%20Mahdi%20Khoramshahi%20and%20St%C3%A9phane%20Doncieux&entry.1292438233=%20%20Robotics%20research%20has%20made%20significant%20strides%20in%20learning%2C%20yet%20mastering%0Abasic%20skills%20like%20object%20placement%20remains%20a%20fundamental%20challenge.%20A%20key%0Abottleneck%20is%20the%20acquisition%20of%20large-scale%2C%20high-quality%20data%2C%20which%20is%20often%0Aa%20manual%20and%20laborious%20process.%20Inspired%20by%20Graspit%21%2C%20a%20foundational%20work%20that%0Aused%20simulation%20to%20automatically%20generate%20dexterous%20grasp%20poses%2C%20we%20introduce%0APlaceit%21%2C%20an%20evolutionary-computation%20framework%20for%20generating%20valid%20placement%0Apositions%20for%20rigid%20objects.%20Placeit%21%20is%20highly%20versatile%2C%20supporting%20tasks%0Afrom%20placing%20objects%20on%20tables%20to%20stacking%20and%20inserting%20them.%20Our%20experiments%0Ashow%20that%20by%20leveraging%20quality-diversity%20optimization%2C%20Placeit%21%20significantly%0Aoutperforms%20state-of-the-art%20methods%20across%20all%20scenarios%20for%20generating%0Adiverse%20valid%20poses.%20A%20pick%26place%20pipeline%20built%20on%20our%20framework%20achieved%20a%0A90%25%20success%20rate%20over%20120%20real-world%20deployments.%20This%20work%20positions%20Placeit%21%0Aas%20a%20powerful%20tool%20for%20open-environment%20pick-and-place%20tasks%20and%20as%20a%20valuable%0Aengine%20for%20generating%20the%20data%20needed%20to%20train%20simulation-based%20foundation%0Amodels%20in%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09267v1&entry.124074799=Read"},
{"title": "Spotlight on Token Perception for Multimodal Reinforcement Learning", "author": "Siyuan Huang and Xiaoye Qu and Yafu Li and Yun Luo and Zefeng He and Daizong Liu and Yu Cheng", "abstract": "  While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.\n", "link": "http://arxiv.org/abs/2510.09285v1", "date": "2025-10-10", "relevancy": 2.2506, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spotlight%20on%20Token%20Perception%20for%20Multimodal%20Reinforcement%20Learning&body=Title%3A%20Spotlight%20on%20Token%20Perception%20for%20Multimodal%20Reinforcement%20Learning%0AAuthor%3A%20Siyuan%20Huang%20and%20Xiaoye%20Qu%20and%20Yafu%20Li%20and%20Yun%20Luo%20and%20Zefeng%20He%20and%20Daizong%20Liu%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20While%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20advanced%20the%0Areasoning%20capabilities%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20most%20existing%0Amethods%20in%20multimodal%20reasoning%20neglect%20the%20critical%20role%20of%20visual%20perception%0Awithin%20the%20RLVR%20optimization%20process.%20In%20this%20paper%2C%20we%20undertake%20a%20pioneering%0Aexploration%20of%20multimodal%20RLVR%20through%20the%20novel%20perspective%20of%20token%0Aperception%2C%20which%20measures%20the%20visual%20dependency%20of%20each%20generated%20token.%20With%0Aa%20granular%20analysis%20of%20Chain-of-Thought%20%28CoT%29%20processes%2C%20we%20uncover%20two%20key%0Ainsights%3A%20first%2C%20token%20perception%20in%20a%20rollout%20trajectory%20is%20sparsely%0Adistributed%2C%20where%20only%20a%20small%20fraction%20of%20tokens%20have%20high%20visual%20dependency%0Afor%20visually-grounded%20reasoning%3B%20second%2C%20different%20trajectories%20exhibit%0Asignificant%20divergence%20in%20their%20overall%20visual%20dependency.%20Based%20on%20these%0Aobservations%2C%20we%20propose%20Visually-Perceptive%20Policy%20Optimization%20%28VPPO%29%2C%20a%0Anovel%20policy%20gradient%20algorithm%20that%20explicitly%20leverages%20token%20perception%20to%0Arefine%20the%20learning%20signal.%20Specifically%2C%20VPPO%20achieves%20this%20through%20a%20dual%0Amechanism%3A%20it%20reweights%20a%20trajectory%27s%20advantage%20by%20its%20overall%20visual%0Adependency%2C%20and%20focuses%20policy%20updates%20exclusively%20on%20perceptually%20pivotal%0Atokens.%20On%20a%20comprehensive%20suite%20of%20eight%20perception%20and%20reasoning%20benchmarks%2C%0AVPPO%20demonstrates%20substantial%20gains%20over%20leading%20open-source%20RL-tuned%20models%2C%0Awith%20its%20effectiveness%20consistently%20validated%20across%207B%20and%2032B%20model%20scales.%0AOur%20findings%20not%20only%20establish%20a%20new%20token-level%20perceptual%20perspective%20for%0Aanalyzing%20multimodal%20RLVR%20but%20also%20present%20a%20novel%20and%20effective%20optimization%0Astrategy%20to%20significantly%20enhance%20the%20multimodal%20reasoning%20capabilities%20of%0ALVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpotlight%2520on%2520Token%2520Perception%2520for%2520Multimodal%2520Reinforcement%2520Learning%26entry.906535625%3DSiyuan%2520Huang%2520and%2520Xiaoye%2520Qu%2520and%2520Yafu%2520Li%2520and%2520Yun%2520Luo%2520and%2520Zefeng%2520He%2520and%2520Daizong%2520Liu%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520While%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520advanced%2520the%250Areasoning%2520capabilities%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%2520most%2520existing%250Amethods%2520in%2520multimodal%2520reasoning%2520neglect%2520the%2520critical%2520role%2520of%2520visual%2520perception%250Awithin%2520the%2520RLVR%2520optimization%2520process.%2520In%2520this%2520paper%252C%2520we%2520undertake%2520a%2520pioneering%250Aexploration%2520of%2520multimodal%2520RLVR%2520through%2520the%2520novel%2520perspective%2520of%2520token%250Aperception%252C%2520which%2520measures%2520the%2520visual%2520dependency%2520of%2520each%2520generated%2520token.%2520With%250Aa%2520granular%2520analysis%2520of%2520Chain-of-Thought%2520%2528CoT%2529%2520processes%252C%2520we%2520uncover%2520two%2520key%250Ainsights%253A%2520first%252C%2520token%2520perception%2520in%2520a%2520rollout%2520trajectory%2520is%2520sparsely%250Adistributed%252C%2520where%2520only%2520a%2520small%2520fraction%2520of%2520tokens%2520have%2520high%2520visual%2520dependency%250Afor%2520visually-grounded%2520reasoning%253B%2520second%252C%2520different%2520trajectories%2520exhibit%250Asignificant%2520divergence%2520in%2520their%2520overall%2520visual%2520dependency.%2520Based%2520on%2520these%250Aobservations%252C%2520we%2520propose%2520Visually-Perceptive%2520Policy%2520Optimization%2520%2528VPPO%2529%252C%2520a%250Anovel%2520policy%2520gradient%2520algorithm%2520that%2520explicitly%2520leverages%2520token%2520perception%2520to%250Arefine%2520the%2520learning%2520signal.%2520Specifically%252C%2520VPPO%2520achieves%2520this%2520through%2520a%2520dual%250Amechanism%253A%2520it%2520reweights%2520a%2520trajectory%2527s%2520advantage%2520by%2520its%2520overall%2520visual%250Adependency%252C%2520and%2520focuses%2520policy%2520updates%2520exclusively%2520on%2520perceptually%2520pivotal%250Atokens.%2520On%2520a%2520comprehensive%2520suite%2520of%2520eight%2520perception%2520and%2520reasoning%2520benchmarks%252C%250AVPPO%2520demonstrates%2520substantial%2520gains%2520over%2520leading%2520open-source%2520RL-tuned%2520models%252C%250Awith%2520its%2520effectiveness%2520consistently%2520validated%2520across%25207B%2520and%252032B%2520model%2520scales.%250AOur%2520findings%2520not%2520only%2520establish%2520a%2520new%2520token-level%2520perceptual%2520perspective%2520for%250Aanalyzing%2520multimodal%2520RLVR%2520but%2520also%2520present%2520a%2520novel%2520and%2520effective%2520optimization%250Astrategy%2520to%2520significantly%2520enhance%2520the%2520multimodal%2520reasoning%2520capabilities%2520of%250ALVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spotlight%20on%20Token%20Perception%20for%20Multimodal%20Reinforcement%20Learning&entry.906535625=Siyuan%20Huang%20and%20Xiaoye%20Qu%20and%20Yafu%20Li%20and%20Yun%20Luo%20and%20Zefeng%20He%20and%20Daizong%20Liu%20and%20Yu%20Cheng&entry.1292438233=%20%20While%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20advanced%20the%0Areasoning%20capabilities%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20most%20existing%0Amethods%20in%20multimodal%20reasoning%20neglect%20the%20critical%20role%20of%20visual%20perception%0Awithin%20the%20RLVR%20optimization%20process.%20In%20this%20paper%2C%20we%20undertake%20a%20pioneering%0Aexploration%20of%20multimodal%20RLVR%20through%20the%20novel%20perspective%20of%20token%0Aperception%2C%20which%20measures%20the%20visual%20dependency%20of%20each%20generated%20token.%20With%0Aa%20granular%20analysis%20of%20Chain-of-Thought%20%28CoT%29%20processes%2C%20we%20uncover%20two%20key%0Ainsights%3A%20first%2C%20token%20perception%20in%20a%20rollout%20trajectory%20is%20sparsely%0Adistributed%2C%20where%20only%20a%20small%20fraction%20of%20tokens%20have%20high%20visual%20dependency%0Afor%20visually-grounded%20reasoning%3B%20second%2C%20different%20trajectories%20exhibit%0Asignificant%20divergence%20in%20their%20overall%20visual%20dependency.%20Based%20on%20these%0Aobservations%2C%20we%20propose%20Visually-Perceptive%20Policy%20Optimization%20%28VPPO%29%2C%20a%0Anovel%20policy%20gradient%20algorithm%20that%20explicitly%20leverages%20token%20perception%20to%0Arefine%20the%20learning%20signal.%20Specifically%2C%20VPPO%20achieves%20this%20through%20a%20dual%0Amechanism%3A%20it%20reweights%20a%20trajectory%27s%20advantage%20by%20its%20overall%20visual%0Adependency%2C%20and%20focuses%20policy%20updates%20exclusively%20on%20perceptually%20pivotal%0Atokens.%20On%20a%20comprehensive%20suite%20of%20eight%20perception%20and%20reasoning%20benchmarks%2C%0AVPPO%20demonstrates%20substantial%20gains%20over%20leading%20open-source%20RL-tuned%20models%2C%0Awith%20its%20effectiveness%20consistently%20validated%20across%207B%20and%2032B%20model%20scales.%0AOur%20findings%20not%20only%20establish%20a%20new%20token-level%20perceptual%20perspective%20for%0Aanalyzing%20multimodal%20RLVR%20but%20also%20present%20a%20novel%20and%20effective%20optimization%0Astrategy%20to%20significantly%20enhance%20the%20multimodal%20reasoning%20capabilities%20of%0ALVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09285v1&entry.124074799=Read"},
{"title": "SynthID-Image: Image watermarking at internet scale", "author": "Sven Gowal and Rudy Bunel and Florian Stimberg and David Stutz and Guillermo Ortiz-Jimenez and Christina Kouridi and Mel Vecerik and Jamie Hayes and Sylvestre-Alvise Rebuffi and Paul Bernard and Chris Gamble and Mikl\u00f3s Z. Horv\u00e1th and Fabian Kaczmarczyck and Alex Kaskasoli and Aleksandar Petrov and Ilia Shumailov and Meghana Thotakuri and Olivia Wiles and Jessica Yung and Zahra Ahmed and Victor Martin and Simon Rosen and Christopher Sav\u010dak and Armin Senoner and Nidhi Vyas and Pushmeet Kohli", "abstract": "  We introduce SynthID-Image, a deep learning-based system for invisibly\nwatermarking AI-generated imagery. This paper documents the technical\ndesiderata, threat models, and practical challenges of deploying such a system\nat internet scale, addressing key requirements of effectiveness, fidelity,\nrobustness, and security. SynthID-Image has been used to watermark over ten\nbillion images and video frames across Google's services and its corresponding\nverification service is available to trusted testers. For completeness, we\npresent an experimental evaluation of an external model variant, SynthID-O,\nwhich is available through partnerships. We benchmark SynthID-O against other\npost-hoc watermarking methods from the literature, demonstrating\nstate-of-the-art performance in both visual quality and robustness to common\nimage perturbations. While this work centers on visual media, the conclusions\non deployment, constraints, and threat modeling generalize to other modalities,\nincluding audio. This paper provides a comprehensive documentation for the\nlarge-scale deployment of deep learning-based media provenance systems.\n", "link": "http://arxiv.org/abs/2510.09263v1", "date": "2025-10-10", "relevancy": 2.245, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5915}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5423}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthID-Image%3A%20Image%20watermarking%20at%20internet%20scale&body=Title%3A%20SynthID-Image%3A%20Image%20watermarking%20at%20internet%20scale%0AAuthor%3A%20Sven%20Gowal%20and%20Rudy%20Bunel%20and%20Florian%20Stimberg%20and%20David%20Stutz%20and%20Guillermo%20Ortiz-Jimenez%20and%20Christina%20Kouridi%20and%20Mel%20Vecerik%20and%20Jamie%20Hayes%20and%20Sylvestre-Alvise%20Rebuffi%20and%20Paul%20Bernard%20and%20Chris%20Gamble%20and%20Mikl%C3%B3s%20Z.%20Horv%C3%A1th%20and%20Fabian%20Kaczmarczyck%20and%20Alex%20Kaskasoli%20and%20Aleksandar%20Petrov%20and%20Ilia%20Shumailov%20and%20Meghana%20Thotakuri%20and%20Olivia%20Wiles%20and%20Jessica%20Yung%20and%20Zahra%20Ahmed%20and%20Victor%20Martin%20and%20Simon%20Rosen%20and%20Christopher%20Sav%C4%8Dak%20and%20Armin%20Senoner%20and%20Nidhi%20Vyas%20and%20Pushmeet%20Kohli%0AAbstract%3A%20%20%20We%20introduce%20SynthID-Image%2C%20a%20deep%20learning-based%20system%20for%20invisibly%0Awatermarking%20AI-generated%20imagery.%20This%20paper%20documents%20the%20technical%0Adesiderata%2C%20threat%20models%2C%20and%20practical%20challenges%20of%20deploying%20such%20a%20system%0Aat%20internet%20scale%2C%20addressing%20key%20requirements%20of%20effectiveness%2C%20fidelity%2C%0Arobustness%2C%20and%20security.%20SynthID-Image%20has%20been%20used%20to%20watermark%20over%20ten%0Abillion%20images%20and%20video%20frames%20across%20Google%27s%20services%20and%20its%20corresponding%0Averification%20service%20is%20available%20to%20trusted%20testers.%20For%20completeness%2C%20we%0Apresent%20an%20experimental%20evaluation%20of%20an%20external%20model%20variant%2C%20SynthID-O%2C%0Awhich%20is%20available%20through%20partnerships.%20We%20benchmark%20SynthID-O%20against%20other%0Apost-hoc%20watermarking%20methods%20from%20the%20literature%2C%20demonstrating%0Astate-of-the-art%20performance%20in%20both%20visual%20quality%20and%20robustness%20to%20common%0Aimage%20perturbations.%20While%20this%20work%20centers%20on%20visual%20media%2C%20the%20conclusions%0Aon%20deployment%2C%20constraints%2C%20and%20threat%20modeling%20generalize%20to%20other%20modalities%2C%0Aincluding%20audio.%20This%20paper%20provides%20a%20comprehensive%20documentation%20for%20the%0Alarge-scale%20deployment%20of%20deep%20learning-based%20media%20provenance%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthID-Image%253A%2520Image%2520watermarking%2520at%2520internet%2520scale%26entry.906535625%3DSven%2520Gowal%2520and%2520Rudy%2520Bunel%2520and%2520Florian%2520Stimberg%2520and%2520David%2520Stutz%2520and%2520Guillermo%2520Ortiz-Jimenez%2520and%2520Christina%2520Kouridi%2520and%2520Mel%2520Vecerik%2520and%2520Jamie%2520Hayes%2520and%2520Sylvestre-Alvise%2520Rebuffi%2520and%2520Paul%2520Bernard%2520and%2520Chris%2520Gamble%2520and%2520Mikl%25C3%25B3s%2520Z.%2520Horv%25C3%25A1th%2520and%2520Fabian%2520Kaczmarczyck%2520and%2520Alex%2520Kaskasoli%2520and%2520Aleksandar%2520Petrov%2520and%2520Ilia%2520Shumailov%2520and%2520Meghana%2520Thotakuri%2520and%2520Olivia%2520Wiles%2520and%2520Jessica%2520Yung%2520and%2520Zahra%2520Ahmed%2520and%2520Victor%2520Martin%2520and%2520Simon%2520Rosen%2520and%2520Christopher%2520Sav%25C4%258Dak%2520and%2520Armin%2520Senoner%2520and%2520Nidhi%2520Vyas%2520and%2520Pushmeet%2520Kohli%26entry.1292438233%3D%2520%2520We%2520introduce%2520SynthID-Image%252C%2520a%2520deep%2520learning-based%2520system%2520for%2520invisibly%250Awatermarking%2520AI-generated%2520imagery.%2520This%2520paper%2520documents%2520the%2520technical%250Adesiderata%252C%2520threat%2520models%252C%2520and%2520practical%2520challenges%2520of%2520deploying%2520such%2520a%2520system%250Aat%2520internet%2520scale%252C%2520addressing%2520key%2520requirements%2520of%2520effectiveness%252C%2520fidelity%252C%250Arobustness%252C%2520and%2520security.%2520SynthID-Image%2520has%2520been%2520used%2520to%2520watermark%2520over%2520ten%250Abillion%2520images%2520and%2520video%2520frames%2520across%2520Google%2527s%2520services%2520and%2520its%2520corresponding%250Averification%2520service%2520is%2520available%2520to%2520trusted%2520testers.%2520For%2520completeness%252C%2520we%250Apresent%2520an%2520experimental%2520evaluation%2520of%2520an%2520external%2520model%2520variant%252C%2520SynthID-O%252C%250Awhich%2520is%2520available%2520through%2520partnerships.%2520We%2520benchmark%2520SynthID-O%2520against%2520other%250Apost-hoc%2520watermarking%2520methods%2520from%2520the%2520literature%252C%2520demonstrating%250Astate-of-the-art%2520performance%2520in%2520both%2520visual%2520quality%2520and%2520robustness%2520to%2520common%250Aimage%2520perturbations.%2520While%2520this%2520work%2520centers%2520on%2520visual%2520media%252C%2520the%2520conclusions%250Aon%2520deployment%252C%2520constraints%252C%2520and%2520threat%2520modeling%2520generalize%2520to%2520other%2520modalities%252C%250Aincluding%2520audio.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520documentation%2520for%2520the%250Alarge-scale%2520deployment%2520of%2520deep%2520learning-based%2520media%2520provenance%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthID-Image%3A%20Image%20watermarking%20at%20internet%20scale&entry.906535625=Sven%20Gowal%20and%20Rudy%20Bunel%20and%20Florian%20Stimberg%20and%20David%20Stutz%20and%20Guillermo%20Ortiz-Jimenez%20and%20Christina%20Kouridi%20and%20Mel%20Vecerik%20and%20Jamie%20Hayes%20and%20Sylvestre-Alvise%20Rebuffi%20and%20Paul%20Bernard%20and%20Chris%20Gamble%20and%20Mikl%C3%B3s%20Z.%20Horv%C3%A1th%20and%20Fabian%20Kaczmarczyck%20and%20Alex%20Kaskasoli%20and%20Aleksandar%20Petrov%20and%20Ilia%20Shumailov%20and%20Meghana%20Thotakuri%20and%20Olivia%20Wiles%20and%20Jessica%20Yung%20and%20Zahra%20Ahmed%20and%20Victor%20Martin%20and%20Simon%20Rosen%20and%20Christopher%20Sav%C4%8Dak%20and%20Armin%20Senoner%20and%20Nidhi%20Vyas%20and%20Pushmeet%20Kohli&entry.1292438233=%20%20We%20introduce%20SynthID-Image%2C%20a%20deep%20learning-based%20system%20for%20invisibly%0Awatermarking%20AI-generated%20imagery.%20This%20paper%20documents%20the%20technical%0Adesiderata%2C%20threat%20models%2C%20and%20practical%20challenges%20of%20deploying%20such%20a%20system%0Aat%20internet%20scale%2C%20addressing%20key%20requirements%20of%20effectiveness%2C%20fidelity%2C%0Arobustness%2C%20and%20security.%20SynthID-Image%20has%20been%20used%20to%20watermark%20over%20ten%0Abillion%20images%20and%20video%20frames%20across%20Google%27s%20services%20and%20its%20corresponding%0Averification%20service%20is%20available%20to%20trusted%20testers.%20For%20completeness%2C%20we%0Apresent%20an%20experimental%20evaluation%20of%20an%20external%20model%20variant%2C%20SynthID-O%2C%0Awhich%20is%20available%20through%20partnerships.%20We%20benchmark%20SynthID-O%20against%20other%0Apost-hoc%20watermarking%20methods%20from%20the%20literature%2C%20demonstrating%0Astate-of-the-art%20performance%20in%20both%20visual%20quality%20and%20robustness%20to%20common%0Aimage%20perturbations.%20While%20this%20work%20centers%20on%20visual%20media%2C%20the%20conclusions%0Aon%20deployment%2C%20constraints%2C%20and%20threat%20modeling%20generalize%20to%20other%20modalities%2C%0Aincluding%20audio.%20This%20paper%20provides%20a%20comprehensive%20documentation%20for%20the%0Alarge-scale%20deployment%20of%20deep%20learning-based%20media%20provenance%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09263v1&entry.124074799=Read"},
{"title": "Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought\n  in Vision-Language Models", "author": "Qihang Ma and Shengyu Li and Jie Tang and Dingkang Yang and Shaodong Chen and Yingyi Zhang and Chao Feng and Jiao Ran", "abstract": "  Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only\nmethods by incorporating multiple modalities of input information to produce a\nset of conclusive phrases. Traditional multi-modal approaches have been proven\nto have significant limitations in handling the challenging absence and unseen\nscenarios. Additionally, we identify shortcomings in existing benchmarks that\noverestimate model capability due to significant overlap in training tests. In\nthis work, we propose leveraging vision-language models (VLMs) for the MMKP\ntask. Firstly, we use two widely-used strategies, e.g., zero-shot and\nsupervised fine-tuning (SFT) to assess the lower bound performance of VLMs.\nNext, to improve the complex reasoning capabilities of VLMs, we adopt\nFine-tune-CoT, which leverages high-quality CoT reasoning data generated by a\nteacher model to finetune smaller models. Finally, to address the\n\"overthinking\" phenomenon, we propose a dynamic CoT strategy which adaptively\ninjects CoT data during training, allowing the model to flexibly leverage its\nreasoning capabilities during the inference stage. We evaluate the proposed\nstrategies on various datasets and the experimental results demonstrate the\neffectiveness of the proposed approaches. The code is available at\nhttps://github.com/bytedance/DynamicCoT.\n", "link": "http://arxiv.org/abs/2510.09358v1", "date": "2025-10-10", "relevancy": 2.2378, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5714}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Multi-modal%20Keyphrase%20Prediction%20with%20Dynamic%20Chain-of-Thought%0A%20%20in%20Vision-Language%20Models&body=Title%3A%20Boosting%20Multi-modal%20Keyphrase%20Prediction%20with%20Dynamic%20Chain-of-Thought%0A%20%20in%20Vision-Language%20Models%0AAuthor%3A%20Qihang%20Ma%20and%20Shengyu%20Li%20and%20Jie%20Tang%20and%20Dingkang%20Yang%20and%20Shaodong%20Chen%20and%20Yingyi%20Zhang%20and%20Chao%20Feng%20and%20Jiao%20Ran%0AAbstract%3A%20%20%20Multi-modal%20keyphrase%20prediction%20%28MMKP%29%20aims%20to%20advance%20beyond%20text-only%0Amethods%20by%20incorporating%20multiple%20modalities%20of%20input%20information%20to%20produce%20a%0Aset%20of%20conclusive%20phrases.%20Traditional%20multi-modal%20approaches%20have%20been%20proven%0Ato%20have%20significant%20limitations%20in%20handling%20the%20challenging%20absence%20and%20unseen%0Ascenarios.%20Additionally%2C%20we%20identify%20shortcomings%20in%20existing%20benchmarks%20that%0Aoverestimate%20model%20capability%20due%20to%20significant%20overlap%20in%20training%20tests.%20In%0Athis%20work%2C%20we%20propose%20leveraging%20vision-language%20models%20%28VLMs%29%20for%20the%20MMKP%0Atask.%20Firstly%2C%20we%20use%20two%20widely-used%20strategies%2C%20e.g.%2C%20zero-shot%20and%0Asupervised%20fine-tuning%20%28SFT%29%20to%20assess%20the%20lower%20bound%20performance%20of%20VLMs.%0ANext%2C%20to%20improve%20the%20complex%20reasoning%20capabilities%20of%20VLMs%2C%20we%20adopt%0AFine-tune-CoT%2C%20which%20leverages%20high-quality%20CoT%20reasoning%20data%20generated%20by%20a%0Ateacher%20model%20to%20finetune%20smaller%20models.%20Finally%2C%20to%20address%20the%0A%22overthinking%22%20phenomenon%2C%20we%20propose%20a%20dynamic%20CoT%20strategy%20which%20adaptively%0Ainjects%20CoT%20data%20during%20training%2C%20allowing%20the%20model%20to%20flexibly%20leverage%20its%0Areasoning%20capabilities%20during%20the%20inference%20stage.%20We%20evaluate%20the%20proposed%0Astrategies%20on%20various%20datasets%20and%20the%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/bytedance/DynamicCoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Multi-modal%2520Keyphrase%2520Prediction%2520with%2520Dynamic%2520Chain-of-Thought%250A%2520%2520in%2520Vision-Language%2520Models%26entry.906535625%3DQihang%2520Ma%2520and%2520Shengyu%2520Li%2520and%2520Jie%2520Tang%2520and%2520Dingkang%2520Yang%2520and%2520Shaodong%2520Chen%2520and%2520Yingyi%2520Zhang%2520and%2520Chao%2520Feng%2520and%2520Jiao%2520Ran%26entry.1292438233%3D%2520%2520Multi-modal%2520keyphrase%2520prediction%2520%2528MMKP%2529%2520aims%2520to%2520advance%2520beyond%2520text-only%250Amethods%2520by%2520incorporating%2520multiple%2520modalities%2520of%2520input%2520information%2520to%2520produce%2520a%250Aset%2520of%2520conclusive%2520phrases.%2520Traditional%2520multi-modal%2520approaches%2520have%2520been%2520proven%250Ato%2520have%2520significant%2520limitations%2520in%2520handling%2520the%2520challenging%2520absence%2520and%2520unseen%250Ascenarios.%2520Additionally%252C%2520we%2520identify%2520shortcomings%2520in%2520existing%2520benchmarks%2520that%250Aoverestimate%2520model%2520capability%2520due%2520to%2520significant%2520overlap%2520in%2520training%2520tests.%2520In%250Athis%2520work%252C%2520we%2520propose%2520leveraging%2520vision-language%2520models%2520%2528VLMs%2529%2520for%2520the%2520MMKP%250Atask.%2520Firstly%252C%2520we%2520use%2520two%2520widely-used%2520strategies%252C%2520e.g.%252C%2520zero-shot%2520and%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520to%2520assess%2520the%2520lower%2520bound%2520performance%2520of%2520VLMs.%250ANext%252C%2520to%2520improve%2520the%2520complex%2520reasoning%2520capabilities%2520of%2520VLMs%252C%2520we%2520adopt%250AFine-tune-CoT%252C%2520which%2520leverages%2520high-quality%2520CoT%2520reasoning%2520data%2520generated%2520by%2520a%250Ateacher%2520model%2520to%2520finetune%2520smaller%2520models.%2520Finally%252C%2520to%2520address%2520the%250A%2522overthinking%2522%2520phenomenon%252C%2520we%2520propose%2520a%2520dynamic%2520CoT%2520strategy%2520which%2520adaptively%250Ainjects%2520CoT%2520data%2520during%2520training%252C%2520allowing%2520the%2520model%2520to%2520flexibly%2520leverage%2520its%250Areasoning%2520capabilities%2520during%2520the%2520inference%2520stage.%2520We%2520evaluate%2520the%2520proposed%250Astrategies%2520on%2520various%2520datasets%2520and%2520the%2520experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520approaches.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/bytedance/DynamicCoT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Multi-modal%20Keyphrase%20Prediction%20with%20Dynamic%20Chain-of-Thought%0A%20%20in%20Vision-Language%20Models&entry.906535625=Qihang%20Ma%20and%20Shengyu%20Li%20and%20Jie%20Tang%20and%20Dingkang%20Yang%20and%20Shaodong%20Chen%20and%20Yingyi%20Zhang%20and%20Chao%20Feng%20and%20Jiao%20Ran&entry.1292438233=%20%20Multi-modal%20keyphrase%20prediction%20%28MMKP%29%20aims%20to%20advance%20beyond%20text-only%0Amethods%20by%20incorporating%20multiple%20modalities%20of%20input%20information%20to%20produce%20a%0Aset%20of%20conclusive%20phrases.%20Traditional%20multi-modal%20approaches%20have%20been%20proven%0Ato%20have%20significant%20limitations%20in%20handling%20the%20challenging%20absence%20and%20unseen%0Ascenarios.%20Additionally%2C%20we%20identify%20shortcomings%20in%20existing%20benchmarks%20that%0Aoverestimate%20model%20capability%20due%20to%20significant%20overlap%20in%20training%20tests.%20In%0Athis%20work%2C%20we%20propose%20leveraging%20vision-language%20models%20%28VLMs%29%20for%20the%20MMKP%0Atask.%20Firstly%2C%20we%20use%20two%20widely-used%20strategies%2C%20e.g.%2C%20zero-shot%20and%0Asupervised%20fine-tuning%20%28SFT%29%20to%20assess%20the%20lower%20bound%20performance%20of%20VLMs.%0ANext%2C%20to%20improve%20the%20complex%20reasoning%20capabilities%20of%20VLMs%2C%20we%20adopt%0AFine-tune-CoT%2C%20which%20leverages%20high-quality%20CoT%20reasoning%20data%20generated%20by%20a%0Ateacher%20model%20to%20finetune%20smaller%20models.%20Finally%2C%20to%20address%20the%0A%22overthinking%22%20phenomenon%2C%20we%20propose%20a%20dynamic%20CoT%20strategy%20which%20adaptively%0Ainjects%20CoT%20data%20during%20training%2C%20allowing%20the%20model%20to%20flexibly%20leverage%20its%0Areasoning%20capabilities%20during%20the%20inference%20stage.%20We%20evaluate%20the%20proposed%0Astrategies%20on%20various%20datasets%20and%20the%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approaches.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/bytedance/DynamicCoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09358v1&entry.124074799=Read"},
{"title": "Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep\n  Predictive Modeling", "author": "Tejaswi V. Panchagnula", "abstract": "  Animals often forage via Levy walks stochastic trajectories with heavy tailed\nstep lengths optimized for sparse resource environments. We show that human\nvisual gaze follows similar dynamics when scanning images. While traditional\nmodels emphasize image based saliency, the underlying spatiotemporal statistics\nof eye movements remain underexplored. Understanding these dynamics has broad\napplications in attention modeling and vision-based interfaces. In this study,\nwe conducted a large scale human subject experiment involving 40 participants\nviewing 50 diverse images under unconstrained conditions, recording over 4\nmillion gaze points using a high speed eye tracker. Analysis of these data\nshows that the gaze trajectory of the human eye also follows a Levy walk akin\nto animal foraging. This suggests that the human eye forages for visual\ninformation in an optimally efficient manner. Further, we trained a\nconvolutional neural network (CNN) to predict fixation heatmaps from image\ninput alone. The model accurately reproduced salient fixation regions across\nnovel images, demonstrating that key components of gaze behavior are learnable\nfrom visual structure alone. Our findings present new evidence that human\nvisual exploration obeys statistical laws analogous to natural foraging and\nopen avenues for modeling gaze through generative and predictive frameworks.\n", "link": "http://arxiv.org/abs/2510.09299v1", "date": "2025-10-10", "relevancy": 2.2353, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5649}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5597}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foraging%20with%20the%20Eyes%3A%20Dynamics%20in%20Human%20Visual%20Gaze%20and%20Deep%0A%20%20Predictive%20Modeling&body=Title%3A%20Foraging%20with%20the%20Eyes%3A%20Dynamics%20in%20Human%20Visual%20Gaze%20and%20Deep%0A%20%20Predictive%20Modeling%0AAuthor%3A%20Tejaswi%20V.%20Panchagnula%0AAbstract%3A%20%20%20Animals%20often%20forage%20via%20Levy%20walks%20stochastic%20trajectories%20with%20heavy%20tailed%0Astep%20lengths%20optimized%20for%20sparse%20resource%20environments.%20We%20show%20that%20human%0Avisual%20gaze%20follows%20similar%20dynamics%20when%20scanning%20images.%20While%20traditional%0Amodels%20emphasize%20image%20based%20saliency%2C%20the%20underlying%20spatiotemporal%20statistics%0Aof%20eye%20movements%20remain%20underexplored.%20Understanding%20these%20dynamics%20has%20broad%0Aapplications%20in%20attention%20modeling%20and%20vision-based%20interfaces.%20In%20this%20study%2C%0Awe%20conducted%20a%20large%20scale%20human%20subject%20experiment%20involving%2040%20participants%0Aviewing%2050%20diverse%20images%20under%20unconstrained%20conditions%2C%20recording%20over%204%0Amillion%20gaze%20points%20using%20a%20high%20speed%20eye%20tracker.%20Analysis%20of%20these%20data%0Ashows%20that%20the%20gaze%20trajectory%20of%20the%20human%20eye%20also%20follows%20a%20Levy%20walk%20akin%0Ato%20animal%20foraging.%20This%20suggests%20that%20the%20human%20eye%20forages%20for%20visual%0Ainformation%20in%20an%20optimally%20efficient%20manner.%20Further%2C%20we%20trained%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20to%20predict%20fixation%20heatmaps%20from%20image%0Ainput%20alone.%20The%20model%20accurately%20reproduced%20salient%20fixation%20regions%20across%0Anovel%20images%2C%20demonstrating%20that%20key%20components%20of%20gaze%20behavior%20are%20learnable%0Afrom%20visual%20structure%20alone.%20Our%20findings%20present%20new%20evidence%20that%20human%0Avisual%20exploration%20obeys%20statistical%20laws%20analogous%20to%20natural%20foraging%20and%0Aopen%20avenues%20for%20modeling%20gaze%20through%20generative%20and%20predictive%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForaging%2520with%2520the%2520Eyes%253A%2520Dynamics%2520in%2520Human%2520Visual%2520Gaze%2520and%2520Deep%250A%2520%2520Predictive%2520Modeling%26entry.906535625%3DTejaswi%2520V.%2520Panchagnula%26entry.1292438233%3D%2520%2520Animals%2520often%2520forage%2520via%2520Levy%2520walks%2520stochastic%2520trajectories%2520with%2520heavy%2520tailed%250Astep%2520lengths%2520optimized%2520for%2520sparse%2520resource%2520environments.%2520We%2520show%2520that%2520human%250Avisual%2520gaze%2520follows%2520similar%2520dynamics%2520when%2520scanning%2520images.%2520While%2520traditional%250Amodels%2520emphasize%2520image%2520based%2520saliency%252C%2520the%2520underlying%2520spatiotemporal%2520statistics%250Aof%2520eye%2520movements%2520remain%2520underexplored.%2520Understanding%2520these%2520dynamics%2520has%2520broad%250Aapplications%2520in%2520attention%2520modeling%2520and%2520vision-based%2520interfaces.%2520In%2520this%2520study%252C%250Awe%2520conducted%2520a%2520large%2520scale%2520human%2520subject%2520experiment%2520involving%252040%2520participants%250Aviewing%252050%2520diverse%2520images%2520under%2520unconstrained%2520conditions%252C%2520recording%2520over%25204%250Amillion%2520gaze%2520points%2520using%2520a%2520high%2520speed%2520eye%2520tracker.%2520Analysis%2520of%2520these%2520data%250Ashows%2520that%2520the%2520gaze%2520trajectory%2520of%2520the%2520human%2520eye%2520also%2520follows%2520a%2520Levy%2520walk%2520akin%250Ato%2520animal%2520foraging.%2520This%2520suggests%2520that%2520the%2520human%2520eye%2520forages%2520for%2520visual%250Ainformation%2520in%2520an%2520optimally%2520efficient%2520manner.%2520Further%252C%2520we%2520trained%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520to%2520predict%2520fixation%2520heatmaps%2520from%2520image%250Ainput%2520alone.%2520The%2520model%2520accurately%2520reproduced%2520salient%2520fixation%2520regions%2520across%250Anovel%2520images%252C%2520demonstrating%2520that%2520key%2520components%2520of%2520gaze%2520behavior%2520are%2520learnable%250Afrom%2520visual%2520structure%2520alone.%2520Our%2520findings%2520present%2520new%2520evidence%2520that%2520human%250Avisual%2520exploration%2520obeys%2520statistical%2520laws%2520analogous%2520to%2520natural%2520foraging%2520and%250Aopen%2520avenues%2520for%2520modeling%2520gaze%2520through%2520generative%2520and%2520predictive%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foraging%20with%20the%20Eyes%3A%20Dynamics%20in%20Human%20Visual%20Gaze%20and%20Deep%0A%20%20Predictive%20Modeling&entry.906535625=Tejaswi%20V.%20Panchagnula&entry.1292438233=%20%20Animals%20often%20forage%20via%20Levy%20walks%20stochastic%20trajectories%20with%20heavy%20tailed%0Astep%20lengths%20optimized%20for%20sparse%20resource%20environments.%20We%20show%20that%20human%0Avisual%20gaze%20follows%20similar%20dynamics%20when%20scanning%20images.%20While%20traditional%0Amodels%20emphasize%20image%20based%20saliency%2C%20the%20underlying%20spatiotemporal%20statistics%0Aof%20eye%20movements%20remain%20underexplored.%20Understanding%20these%20dynamics%20has%20broad%0Aapplications%20in%20attention%20modeling%20and%20vision-based%20interfaces.%20In%20this%20study%2C%0Awe%20conducted%20a%20large%20scale%20human%20subject%20experiment%20involving%2040%20participants%0Aviewing%2050%20diverse%20images%20under%20unconstrained%20conditions%2C%20recording%20over%204%0Amillion%20gaze%20points%20using%20a%20high%20speed%20eye%20tracker.%20Analysis%20of%20these%20data%0Ashows%20that%20the%20gaze%20trajectory%20of%20the%20human%20eye%20also%20follows%20a%20Levy%20walk%20akin%0Ato%20animal%20foraging.%20This%20suggests%20that%20the%20human%20eye%20forages%20for%20visual%0Ainformation%20in%20an%20optimally%20efficient%20manner.%20Further%2C%20we%20trained%20a%0Aconvolutional%20neural%20network%20%28CNN%29%20to%20predict%20fixation%20heatmaps%20from%20image%0Ainput%20alone.%20The%20model%20accurately%20reproduced%20salient%20fixation%20regions%20across%0Anovel%20images%2C%20demonstrating%20that%20key%20components%20of%20gaze%20behavior%20are%20learnable%0Afrom%20visual%20structure%20alone.%20Our%20findings%20present%20new%20evidence%20that%20human%0Avisual%20exploration%20obeys%20statistical%20laws%20analogous%20to%20natural%20foraging%20and%0Aopen%20avenues%20for%20modeling%20gaze%20through%20generative%20and%20predictive%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09299v1&entry.124074799=Read"},
{"title": "Issue Localization via LLM-Driven Iterative Code Graph Searching", "author": "Zhonghao Jiang and Xiaoxue Ren and Meng Yan and Wei Jiang and Yong Li and Zhongxin Liu", "abstract": "  Issue solving aims to generate patches to fix reported issues in real-world\ncode repositories according to issue descriptions. Issue localization forms the\nbasis for accurate issue solving. Recently, LLM-based issue localization\nmethods have demonstrated state-of-the-art performance. However, these methods\neither search from files mentioned in issue descriptions or in the whole\nrepository and struggle to balance the breadth and depth of the search space to\nconverge on the target efficiently. Moreover, they allow LLM to explore whole\nrepositories freely, making it challenging to control the search direction to\nprevent the LLM from searching for incorrect targets. This paper introduces\nCoSIL, an LLM-driven, powerful function-level issue localization method without\ntraining or indexing. CoSIL employs a two-phase code graph search strategy. It\nfirst conducts broad exploration at the file level using dynamically\nconstructed module call graphs, and then performs in-depth analysis at the\nfunction level by expanding the module call graph into a function call graph\nand executing iterative searches. To precisely control the search direction,\nCoSIL designs a pruner to filter unrelated directions and irrelevant contexts.\nTo avoid incorrect interaction formats in long contexts, CoSIL introduces a\nreflection mechanism that uses additional independent queries in short contexts\nto enhance formatted abilities. Experiment results demonstrate that CoSIL\nachieves a Top-1 localization accuracy of 43.3\\% and 44.6\\% on SWE-bench Lite\nand SWE-bench Verified, respectively, with Qwen2.5-Coder-32B, average\noutperforming the state-of-the-art methods by 96.04\\%. When CoSIL is integrated\ninto an issue-solving method, Agentless, the issue resolution rate improves by\n2.98\\%--30.5\\%.\n", "link": "http://arxiv.org/abs/2503.22424v3", "date": "2025-10-10", "relevancy": 2.2252, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4459}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Issue%20Localization%20via%20LLM-Driven%20Iterative%20Code%20Graph%20Searching&body=Title%3A%20Issue%20Localization%20via%20LLM-Driven%20Iterative%20Code%20Graph%20Searching%0AAuthor%3A%20Zhonghao%20Jiang%20and%20Xiaoxue%20Ren%20and%20Meng%20Yan%20and%20Wei%20Jiang%20and%20Yong%20Li%20and%20Zhongxin%20Liu%0AAbstract%3A%20%20%20Issue%20solving%20aims%20to%20generate%20patches%20to%20fix%20reported%20issues%20in%20real-world%0Acode%20repositories%20according%20to%20issue%20descriptions.%20Issue%20localization%20forms%20the%0Abasis%20for%20accurate%20issue%20solving.%20Recently%2C%20LLM-based%20issue%20localization%0Amethods%20have%20demonstrated%20state-of-the-art%20performance.%20However%2C%20these%20methods%0Aeither%20search%20from%20files%20mentioned%20in%20issue%20descriptions%20or%20in%20the%20whole%0Arepository%20and%20struggle%20to%20balance%20the%20breadth%20and%20depth%20of%20the%20search%20space%20to%0Aconverge%20on%20the%20target%20efficiently.%20Moreover%2C%20they%20allow%20LLM%20to%20explore%20whole%0Arepositories%20freely%2C%20making%20it%20challenging%20to%20control%20the%20search%20direction%20to%0Aprevent%20the%20LLM%20from%20searching%20for%20incorrect%20targets.%20This%20paper%20introduces%0ACoSIL%2C%20an%20LLM-driven%2C%20powerful%20function-level%20issue%20localization%20method%20without%0Atraining%20or%20indexing.%20CoSIL%20employs%20a%20two-phase%20code%20graph%20search%20strategy.%20It%0Afirst%20conducts%20broad%20exploration%20at%20the%20file%20level%20using%20dynamically%0Aconstructed%20module%20call%20graphs%2C%20and%20then%20performs%20in-depth%20analysis%20at%20the%0Afunction%20level%20by%20expanding%20the%20module%20call%20graph%20into%20a%20function%20call%20graph%0Aand%20executing%20iterative%20searches.%20To%20precisely%20control%20the%20search%20direction%2C%0ACoSIL%20designs%20a%20pruner%20to%20filter%20unrelated%20directions%20and%20irrelevant%20contexts.%0ATo%20avoid%20incorrect%20interaction%20formats%20in%20long%20contexts%2C%20CoSIL%20introduces%20a%0Areflection%20mechanism%20that%20uses%20additional%20independent%20queries%20in%20short%20contexts%0Ato%20enhance%20formatted%20abilities.%20Experiment%20results%20demonstrate%20that%20CoSIL%0Aachieves%20a%20Top-1%20localization%20accuracy%20of%2043.3%5C%25%20and%2044.6%5C%25%20on%20SWE-bench%20Lite%0Aand%20SWE-bench%20Verified%2C%20respectively%2C%20with%20Qwen2.5-Coder-32B%2C%20average%0Aoutperforming%20the%20state-of-the-art%20methods%20by%2096.04%5C%25.%20When%20CoSIL%20is%20integrated%0Ainto%20an%20issue-solving%20method%2C%20Agentless%2C%20the%20issue%20resolution%20rate%20improves%20by%0A2.98%5C%25--30.5%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22424v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIssue%2520Localization%2520via%2520LLM-Driven%2520Iterative%2520Code%2520Graph%2520Searching%26entry.906535625%3DZhonghao%2520Jiang%2520and%2520Xiaoxue%2520Ren%2520and%2520Meng%2520Yan%2520and%2520Wei%2520Jiang%2520and%2520Yong%2520Li%2520and%2520Zhongxin%2520Liu%26entry.1292438233%3D%2520%2520Issue%2520solving%2520aims%2520to%2520generate%2520patches%2520to%2520fix%2520reported%2520issues%2520in%2520real-world%250Acode%2520repositories%2520according%2520to%2520issue%2520descriptions.%2520Issue%2520localization%2520forms%2520the%250Abasis%2520for%2520accurate%2520issue%2520solving.%2520Recently%252C%2520LLM-based%2520issue%2520localization%250Amethods%2520have%2520demonstrated%2520state-of-the-art%2520performance.%2520However%252C%2520these%2520methods%250Aeither%2520search%2520from%2520files%2520mentioned%2520in%2520issue%2520descriptions%2520or%2520in%2520the%2520whole%250Arepository%2520and%2520struggle%2520to%2520balance%2520the%2520breadth%2520and%2520depth%2520of%2520the%2520search%2520space%2520to%250Aconverge%2520on%2520the%2520target%2520efficiently.%2520Moreover%252C%2520they%2520allow%2520LLM%2520to%2520explore%2520whole%250Arepositories%2520freely%252C%2520making%2520it%2520challenging%2520to%2520control%2520the%2520search%2520direction%2520to%250Aprevent%2520the%2520LLM%2520from%2520searching%2520for%2520incorrect%2520targets.%2520This%2520paper%2520introduces%250ACoSIL%252C%2520an%2520LLM-driven%252C%2520powerful%2520function-level%2520issue%2520localization%2520method%2520without%250Atraining%2520or%2520indexing.%2520CoSIL%2520employs%2520a%2520two-phase%2520code%2520graph%2520search%2520strategy.%2520It%250Afirst%2520conducts%2520broad%2520exploration%2520at%2520the%2520file%2520level%2520using%2520dynamically%250Aconstructed%2520module%2520call%2520graphs%252C%2520and%2520then%2520performs%2520in-depth%2520analysis%2520at%2520the%250Afunction%2520level%2520by%2520expanding%2520the%2520module%2520call%2520graph%2520into%2520a%2520function%2520call%2520graph%250Aand%2520executing%2520iterative%2520searches.%2520To%2520precisely%2520control%2520the%2520search%2520direction%252C%250ACoSIL%2520designs%2520a%2520pruner%2520to%2520filter%2520unrelated%2520directions%2520and%2520irrelevant%2520contexts.%250ATo%2520avoid%2520incorrect%2520interaction%2520formats%2520in%2520long%2520contexts%252C%2520CoSIL%2520introduces%2520a%250Areflection%2520mechanism%2520that%2520uses%2520additional%2520independent%2520queries%2520in%2520short%2520contexts%250Ato%2520enhance%2520formatted%2520abilities.%2520Experiment%2520results%2520demonstrate%2520that%2520CoSIL%250Aachieves%2520a%2520Top-1%2520localization%2520accuracy%2520of%252043.3%255C%2525%2520and%252044.6%255C%2525%2520on%2520SWE-bench%2520Lite%250Aand%2520SWE-bench%2520Verified%252C%2520respectively%252C%2520with%2520Qwen2.5-Coder-32B%252C%2520average%250Aoutperforming%2520the%2520state-of-the-art%2520methods%2520by%252096.04%255C%2525.%2520When%2520CoSIL%2520is%2520integrated%250Ainto%2520an%2520issue-solving%2520method%252C%2520Agentless%252C%2520the%2520issue%2520resolution%2520rate%2520improves%2520by%250A2.98%255C%2525--30.5%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22424v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Issue%20Localization%20via%20LLM-Driven%20Iterative%20Code%20Graph%20Searching&entry.906535625=Zhonghao%20Jiang%20and%20Xiaoxue%20Ren%20and%20Meng%20Yan%20and%20Wei%20Jiang%20and%20Yong%20Li%20and%20Zhongxin%20Liu&entry.1292438233=%20%20Issue%20solving%20aims%20to%20generate%20patches%20to%20fix%20reported%20issues%20in%20real-world%0Acode%20repositories%20according%20to%20issue%20descriptions.%20Issue%20localization%20forms%20the%0Abasis%20for%20accurate%20issue%20solving.%20Recently%2C%20LLM-based%20issue%20localization%0Amethods%20have%20demonstrated%20state-of-the-art%20performance.%20However%2C%20these%20methods%0Aeither%20search%20from%20files%20mentioned%20in%20issue%20descriptions%20or%20in%20the%20whole%0Arepository%20and%20struggle%20to%20balance%20the%20breadth%20and%20depth%20of%20the%20search%20space%20to%0Aconverge%20on%20the%20target%20efficiently.%20Moreover%2C%20they%20allow%20LLM%20to%20explore%20whole%0Arepositories%20freely%2C%20making%20it%20challenging%20to%20control%20the%20search%20direction%20to%0Aprevent%20the%20LLM%20from%20searching%20for%20incorrect%20targets.%20This%20paper%20introduces%0ACoSIL%2C%20an%20LLM-driven%2C%20powerful%20function-level%20issue%20localization%20method%20without%0Atraining%20or%20indexing.%20CoSIL%20employs%20a%20two-phase%20code%20graph%20search%20strategy.%20It%0Afirst%20conducts%20broad%20exploration%20at%20the%20file%20level%20using%20dynamically%0Aconstructed%20module%20call%20graphs%2C%20and%20then%20performs%20in-depth%20analysis%20at%20the%0Afunction%20level%20by%20expanding%20the%20module%20call%20graph%20into%20a%20function%20call%20graph%0Aand%20executing%20iterative%20searches.%20To%20precisely%20control%20the%20search%20direction%2C%0ACoSIL%20designs%20a%20pruner%20to%20filter%20unrelated%20directions%20and%20irrelevant%20contexts.%0ATo%20avoid%20incorrect%20interaction%20formats%20in%20long%20contexts%2C%20CoSIL%20introduces%20a%0Areflection%20mechanism%20that%20uses%20additional%20independent%20queries%20in%20short%20contexts%0Ato%20enhance%20formatted%20abilities.%20Experiment%20results%20demonstrate%20that%20CoSIL%0Aachieves%20a%20Top-1%20localization%20accuracy%20of%2043.3%5C%25%20and%2044.6%5C%25%20on%20SWE-bench%20Lite%0Aand%20SWE-bench%20Verified%2C%20respectively%2C%20with%20Qwen2.5-Coder-32B%2C%20average%0Aoutperforming%20the%20state-of-the-art%20methods%20by%2096.04%5C%25.%20When%20CoSIL%20is%20integrated%0Ainto%20an%20issue-solving%20method%2C%20Agentless%2C%20the%20issue%20resolution%20rate%20improves%20by%0A2.98%5C%25--30.5%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22424v3&entry.124074799=Read"},
{"title": "MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel\n  Understanding", "author": "Ming Dai and Sen Yang and Boqiang Duan and Wankou Yang and Jingdong Wang", "abstract": "  Referring Video Object Segmentation (RefVOS) seeks to segment target objects\nin videos guided by natural language descriptions, demanding both temporal\nreasoning and fine-grained visual comprehension. Existing sampling strategies\nfor LLM-based approaches typically rely on either handcrafted heuristics or\nexternal keyframe models. The former often overlooks essential temporal cues,\nwhile the latter increases system complexity. To address this, we propose a\nunified framework that jointly optimizes Temporal Sentence Grounding (TSG) and\nRefVOS, naturally incorporating key moment grounding capability. During\ntraining, we introduce a novel TSG paradigm that employs a dedicated\n\\texttt{[FIND]} token for key moment identification through temporal token\nsimilarity matching, thereby avoiding the need for external timestamp\nencodings. For inference, we design a Moment-Centric Sampling (MCS) strategy\nthat densely samples informative moments while sparsely sampling non-essential\nframes, preserving both motion details and global context. To further enhance\ntracking stability, we develop Bidirectional Anchor-updated Propagation (BAP),\nwhich leverages the most relevant moment as start point for high-quality mask\ninitialization and dynamically updates at sampled points to mitigate\naccumulated errors. Code and model will be available at:\nhttps://github.com/Dmmm1997/MomentSeg\n", "link": "http://arxiv.org/abs/2510.09274v1", "date": "2025-10-10", "relevancy": 2.2064, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MomentSeg%3A%20Moment-Centric%20Sampling%20for%20Enhanced%20Video%20Pixel%0A%20%20Understanding&body=Title%3A%20MomentSeg%3A%20Moment-Centric%20Sampling%20for%20Enhanced%20Video%20Pixel%0A%20%20Understanding%0AAuthor%3A%20Ming%20Dai%20and%20Sen%20Yang%20and%20Boqiang%20Duan%20and%20Wankou%20Yang%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Referring%20Video%20Object%20Segmentation%20%28RefVOS%29%20seeks%20to%20segment%20target%20objects%0Ain%20videos%20guided%20by%20natural%20language%20descriptions%2C%20demanding%20both%20temporal%0Areasoning%20and%20fine-grained%20visual%20comprehension.%20Existing%20sampling%20strategies%0Afor%20LLM-based%20approaches%20typically%20rely%20on%20either%20handcrafted%20heuristics%20or%0Aexternal%20keyframe%20models.%20The%20former%20often%20overlooks%20essential%20temporal%20cues%2C%0Awhile%20the%20latter%20increases%20system%20complexity.%20To%20address%20this%2C%20we%20propose%20a%0Aunified%20framework%20that%20jointly%20optimizes%20Temporal%20Sentence%20Grounding%20%28TSG%29%20and%0ARefVOS%2C%20naturally%20incorporating%20key%20moment%20grounding%20capability.%20During%0Atraining%2C%20we%20introduce%20a%20novel%20TSG%20paradigm%20that%20employs%20a%20dedicated%0A%5Ctexttt%7B%5BFIND%5D%7D%20token%20for%20key%20moment%20identification%20through%20temporal%20token%0Asimilarity%20matching%2C%20thereby%20avoiding%20the%20need%20for%20external%20timestamp%0Aencodings.%20For%20inference%2C%20we%20design%20a%20Moment-Centric%20Sampling%20%28MCS%29%20strategy%0Athat%20densely%20samples%20informative%20moments%20while%20sparsely%20sampling%20non-essential%0Aframes%2C%20preserving%20both%20motion%20details%20and%20global%20context.%20To%20further%20enhance%0Atracking%20stability%2C%20we%20develop%20Bidirectional%20Anchor-updated%20Propagation%20%28BAP%29%2C%0Awhich%20leverages%20the%20most%20relevant%20moment%20as%20start%20point%20for%20high-quality%20mask%0Ainitialization%20and%20dynamically%20updates%20at%20sampled%20points%20to%20mitigate%0Aaccumulated%20errors.%20Code%20and%20model%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/Dmmm1997/MomentSeg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMomentSeg%253A%2520Moment-Centric%2520Sampling%2520for%2520Enhanced%2520Video%2520Pixel%250A%2520%2520Understanding%26entry.906535625%3DMing%2520Dai%2520and%2520Sen%2520Yang%2520and%2520Boqiang%2520Duan%2520and%2520Wankou%2520Yang%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520Referring%2520Video%2520Object%2520Segmentation%2520%2528RefVOS%2529%2520seeks%2520to%2520segment%2520target%2520objects%250Ain%2520videos%2520guided%2520by%2520natural%2520language%2520descriptions%252C%2520demanding%2520both%2520temporal%250Areasoning%2520and%2520fine-grained%2520visual%2520comprehension.%2520Existing%2520sampling%2520strategies%250Afor%2520LLM-based%2520approaches%2520typically%2520rely%2520on%2520either%2520handcrafted%2520heuristics%2520or%250Aexternal%2520keyframe%2520models.%2520The%2520former%2520often%2520overlooks%2520essential%2520temporal%2520cues%252C%250Awhile%2520the%2520latter%2520increases%2520system%2520complexity.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Aunified%2520framework%2520that%2520jointly%2520optimizes%2520Temporal%2520Sentence%2520Grounding%2520%2528TSG%2529%2520and%250ARefVOS%252C%2520naturally%2520incorporating%2520key%2520moment%2520grounding%2520capability.%2520During%250Atraining%252C%2520we%2520introduce%2520a%2520novel%2520TSG%2520paradigm%2520that%2520employs%2520a%2520dedicated%250A%255Ctexttt%257B%255BFIND%255D%257D%2520token%2520for%2520key%2520moment%2520identification%2520through%2520temporal%2520token%250Asimilarity%2520matching%252C%2520thereby%2520avoiding%2520the%2520need%2520for%2520external%2520timestamp%250Aencodings.%2520For%2520inference%252C%2520we%2520design%2520a%2520Moment-Centric%2520Sampling%2520%2528MCS%2529%2520strategy%250Athat%2520densely%2520samples%2520informative%2520moments%2520while%2520sparsely%2520sampling%2520non-essential%250Aframes%252C%2520preserving%2520both%2520motion%2520details%2520and%2520global%2520context.%2520To%2520further%2520enhance%250Atracking%2520stability%252C%2520we%2520develop%2520Bidirectional%2520Anchor-updated%2520Propagation%2520%2528BAP%2529%252C%250Awhich%2520leverages%2520the%2520most%2520relevant%2520moment%2520as%2520start%2520point%2520for%2520high-quality%2520mask%250Ainitialization%2520and%2520dynamically%2520updates%2520at%2520sampled%2520points%2520to%2520mitigate%250Aaccumulated%2520errors.%2520Code%2520and%2520model%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/Dmmm1997/MomentSeg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MomentSeg%3A%20Moment-Centric%20Sampling%20for%20Enhanced%20Video%20Pixel%0A%20%20Understanding&entry.906535625=Ming%20Dai%20and%20Sen%20Yang%20and%20Boqiang%20Duan%20and%20Wankou%20Yang%20and%20Jingdong%20Wang&entry.1292438233=%20%20Referring%20Video%20Object%20Segmentation%20%28RefVOS%29%20seeks%20to%20segment%20target%20objects%0Ain%20videos%20guided%20by%20natural%20language%20descriptions%2C%20demanding%20both%20temporal%0Areasoning%20and%20fine-grained%20visual%20comprehension.%20Existing%20sampling%20strategies%0Afor%20LLM-based%20approaches%20typically%20rely%20on%20either%20handcrafted%20heuristics%20or%0Aexternal%20keyframe%20models.%20The%20former%20often%20overlooks%20essential%20temporal%20cues%2C%0Awhile%20the%20latter%20increases%20system%20complexity.%20To%20address%20this%2C%20we%20propose%20a%0Aunified%20framework%20that%20jointly%20optimizes%20Temporal%20Sentence%20Grounding%20%28TSG%29%20and%0ARefVOS%2C%20naturally%20incorporating%20key%20moment%20grounding%20capability.%20During%0Atraining%2C%20we%20introduce%20a%20novel%20TSG%20paradigm%20that%20employs%20a%20dedicated%0A%5Ctexttt%7B%5BFIND%5D%7D%20token%20for%20key%20moment%20identification%20through%20temporal%20token%0Asimilarity%20matching%2C%20thereby%20avoiding%20the%20need%20for%20external%20timestamp%0Aencodings.%20For%20inference%2C%20we%20design%20a%20Moment-Centric%20Sampling%20%28MCS%29%20strategy%0Athat%20densely%20samples%20informative%20moments%20while%20sparsely%20sampling%20non-essential%0Aframes%2C%20preserving%20both%20motion%20details%20and%20global%20context.%20To%20further%20enhance%0Atracking%20stability%2C%20we%20develop%20Bidirectional%20Anchor-updated%20Propagation%20%28BAP%29%2C%0Awhich%20leverages%20the%20most%20relevant%20moment%20as%20start%20point%20for%20high-quality%20mask%0Ainitialization%20and%20dynamically%20updates%20at%20sampled%20points%20to%20mitigate%0Aaccumulated%20errors.%20Code%20and%20model%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/Dmmm1997/MomentSeg%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09274v1&entry.124074799=Read"},
{"title": "Differentially Private 2D Human Pose Estimation", "author": "Kaushik Bhargav Sivangi and Paul Henderson and Fani Deligianni", "abstract": "  Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first comprehensive framework for\ndifferentially private 2D human pose estimation (2D-HPE) by applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to this task. To\neffectively balance privacy with performance, we adopt Projected DP-SGD\n(PDP-SGD), which projects the noisy gradients to a low-dimensional subspace.\nNext, we incorporate Feature Differential Privacy(FDP) to selectively privatize\nonly sensitive features while retaining public visual cues. Finally, we propose\na hybrid feature-projective DP framework that combines both approaches to\nbalance privacy and accuracy for HPE. We evaluate our approach on the MPII\ndataset across varying privacy budgets, training strategies, and clipping\nnorms. Our combined feature-projective method consistently outperforms vanilla\nDP-SGD and individual baselines, achieving up to 82.61\\% mean PCKh@0.5 at\n$\\epsilon = 0.8$, substantially closing the gap to the non-private performance.\nThis work lays foundation for privacy-preserving human pose estimation in\nreal-world, sensitive applications.\n", "link": "http://arxiv.org/abs/2504.10190v3", "date": "2025-10-10", "relevancy": 2.2009, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5679}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5487}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentially%20Private%202D%20Human%20Pose%20Estimation&body=Title%3A%20Differentially%20Private%202D%20Human%20Pose%20Estimation%0AAuthor%3A%20Kaushik%20Bhargav%20Sivangi%20and%20Paul%20Henderson%20and%20Fani%20Deligianni%0AAbstract%3A%20%20%20Human%20pose%20estimation%20%28HPE%29%20has%20become%20essential%20in%20numerous%20applications%0Aincluding%20healthcare%2C%20activity%20recognition%2C%20and%20human-computer%20interaction.%0AHowever%2C%20the%20privacy%20implications%20of%20processing%20sensitive%20visual%20data%20present%0Asignificant%20deployment%20barriers%20in%20critical%20domains.%20While%20traditional%0Aanonymization%20techniques%20offer%20limited%20protection%20and%20often%20compromise%20data%0Autility%20for%20broader%20motion%20analysis%2C%20Differential%20Privacy%20%28DP%29%20provides%20formal%0Aprivacy%20guarantees%20but%20typically%20degrades%20model%20performance%20when%20applied%0Anaively.%20In%20this%20work%2C%20we%20present%20the%20first%20comprehensive%20framework%20for%0Adifferentially%20private%202D%20human%20pose%20estimation%20%282D-HPE%29%20by%20applying%0ADifferentially%20Private%20Stochastic%20Gradient%20Descent%20%28DP-SGD%29%20to%20this%20task.%20To%0Aeffectively%20balance%20privacy%20with%20performance%2C%20we%20adopt%20Projected%20DP-SGD%0A%28PDP-SGD%29%2C%20which%20projects%20the%20noisy%20gradients%20to%20a%20low-dimensional%20subspace.%0ANext%2C%20we%20incorporate%20Feature%20Differential%20Privacy%28FDP%29%20to%20selectively%20privatize%0Aonly%20sensitive%20features%20while%20retaining%20public%20visual%20cues.%20Finally%2C%20we%20propose%0Aa%20hybrid%20feature-projective%20DP%20framework%20that%20combines%20both%20approaches%20to%0Abalance%20privacy%20and%20accuracy%20for%20HPE.%20We%20evaluate%20our%20approach%20on%20the%20MPII%0Adataset%20across%20varying%20privacy%20budgets%2C%20training%20strategies%2C%20and%20clipping%0Anorms.%20Our%20combined%20feature-projective%20method%20consistently%20outperforms%20vanilla%0ADP-SGD%20and%20individual%20baselines%2C%20achieving%20up%20to%2082.61%5C%25%20mean%20PCKh%400.5%20at%0A%24%5Cepsilon%20%3D%200.8%24%2C%20substantially%20closing%20the%20gap%20to%20the%20non-private%20performance.%0AThis%20work%20lays%20foundation%20for%20privacy-preserving%20human%20pose%20estimation%20in%0Areal-world%2C%20sensitive%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10190v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentially%2520Private%25202D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DKaushik%2520Bhargav%2520Sivangi%2520and%2520Paul%2520Henderson%2520and%2520Fani%2520Deligianni%26entry.1292438233%3D%2520%2520Human%2520pose%2520estimation%2520%2528HPE%2529%2520has%2520become%2520essential%2520in%2520numerous%2520applications%250Aincluding%2520healthcare%252C%2520activity%2520recognition%252C%2520and%2520human-computer%2520interaction.%250AHowever%252C%2520the%2520privacy%2520implications%2520of%2520processing%2520sensitive%2520visual%2520data%2520present%250Asignificant%2520deployment%2520barriers%2520in%2520critical%2520domains.%2520While%2520traditional%250Aanonymization%2520techniques%2520offer%2520limited%2520protection%2520and%2520often%2520compromise%2520data%250Autility%2520for%2520broader%2520motion%2520analysis%252C%2520Differential%2520Privacy%2520%2528DP%2529%2520provides%2520formal%250Aprivacy%2520guarantees%2520but%2520typically%2520degrades%2520model%2520performance%2520when%2520applied%250Anaively.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520comprehensive%2520framework%2520for%250Adifferentially%2520private%25202D%2520human%2520pose%2520estimation%2520%25282D-HPE%2529%2520by%2520applying%250ADifferentially%2520Private%2520Stochastic%2520Gradient%2520Descent%2520%2528DP-SGD%2529%2520to%2520this%2520task.%2520To%250Aeffectively%2520balance%2520privacy%2520with%2520performance%252C%2520we%2520adopt%2520Projected%2520DP-SGD%250A%2528PDP-SGD%2529%252C%2520which%2520projects%2520the%2520noisy%2520gradients%2520to%2520a%2520low-dimensional%2520subspace.%250ANext%252C%2520we%2520incorporate%2520Feature%2520Differential%2520Privacy%2528FDP%2529%2520to%2520selectively%2520privatize%250Aonly%2520sensitive%2520features%2520while%2520retaining%2520public%2520visual%2520cues.%2520Finally%252C%2520we%2520propose%250Aa%2520hybrid%2520feature-projective%2520DP%2520framework%2520that%2520combines%2520both%2520approaches%2520to%250Abalance%2520privacy%2520and%2520accuracy%2520for%2520HPE.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520MPII%250Adataset%2520across%2520varying%2520privacy%2520budgets%252C%2520training%2520strategies%252C%2520and%2520clipping%250Anorms.%2520Our%2520combined%2520feature-projective%2520method%2520consistently%2520outperforms%2520vanilla%250ADP-SGD%2520and%2520individual%2520baselines%252C%2520achieving%2520up%2520to%252082.61%255C%2525%2520mean%2520PCKh%25400.5%2520at%250A%2524%255Cepsilon%2520%253D%25200.8%2524%252C%2520substantially%2520closing%2520the%2520gap%2520to%2520the%2520non-private%2520performance.%250AThis%2520work%2520lays%2520foundation%2520for%2520privacy-preserving%2520human%2520pose%2520estimation%2520in%250Areal-world%252C%2520sensitive%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10190v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20Private%202D%20Human%20Pose%20Estimation&entry.906535625=Kaushik%20Bhargav%20Sivangi%20and%20Paul%20Henderson%20and%20Fani%20Deligianni&entry.1292438233=%20%20Human%20pose%20estimation%20%28HPE%29%20has%20become%20essential%20in%20numerous%20applications%0Aincluding%20healthcare%2C%20activity%20recognition%2C%20and%20human-computer%20interaction.%0AHowever%2C%20the%20privacy%20implications%20of%20processing%20sensitive%20visual%20data%20present%0Asignificant%20deployment%20barriers%20in%20critical%20domains.%20While%20traditional%0Aanonymization%20techniques%20offer%20limited%20protection%20and%20often%20compromise%20data%0Autility%20for%20broader%20motion%20analysis%2C%20Differential%20Privacy%20%28DP%29%20provides%20formal%0Aprivacy%20guarantees%20but%20typically%20degrades%20model%20performance%20when%20applied%0Anaively.%20In%20this%20work%2C%20we%20present%20the%20first%20comprehensive%20framework%20for%0Adifferentially%20private%202D%20human%20pose%20estimation%20%282D-HPE%29%20by%20applying%0ADifferentially%20Private%20Stochastic%20Gradient%20Descent%20%28DP-SGD%29%20to%20this%20task.%20To%0Aeffectively%20balance%20privacy%20with%20performance%2C%20we%20adopt%20Projected%20DP-SGD%0A%28PDP-SGD%29%2C%20which%20projects%20the%20noisy%20gradients%20to%20a%20low-dimensional%20subspace.%0ANext%2C%20we%20incorporate%20Feature%20Differential%20Privacy%28FDP%29%20to%20selectively%20privatize%0Aonly%20sensitive%20features%20while%20retaining%20public%20visual%20cues.%20Finally%2C%20we%20propose%0Aa%20hybrid%20feature-projective%20DP%20framework%20that%20combines%20both%20approaches%20to%0Abalance%20privacy%20and%20accuracy%20for%20HPE.%20We%20evaluate%20our%20approach%20on%20the%20MPII%0Adataset%20across%20varying%20privacy%20budgets%2C%20training%20strategies%2C%20and%20clipping%0Anorms.%20Our%20combined%20feature-projective%20method%20consistently%20outperforms%20vanilla%0ADP-SGD%20and%20individual%20baselines%2C%20achieving%20up%20to%2082.61%5C%25%20mean%20PCKh%400.5%20at%0A%24%5Cepsilon%20%3D%200.8%24%2C%20substantially%20closing%20the%20gap%20to%20the%20non-private%20performance.%0AThis%20work%20lays%20foundation%20for%20privacy-preserving%20human%20pose%20estimation%20in%0Areal-world%2C%20sensitive%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10190v3&entry.124074799=Read"},
{"title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols", "author": "Mikhail Terekhov and Alexander Panfilov and Daniil Dzenhaliou and Caglar Gulcehre and Maksym Andriushchenko and Ameya Prabhu and Jonas Geiping", "abstract": "  AI control protocols serve as a defense mechanism to stop untrusted LLM\nagents from causing harm in autonomous settings. Prior work treats this as a\nsecurity problem, stress testing with exploits that use the deployment context\nto subtly complete harmful side tasks, such as backdoor insertion. In practice,\nmost AI control protocols are fundamentally based on LLM monitors, which can\nbecome a central point of failure. We study adaptive attacks by an untrusted\nmodel that knows the protocol and the monitor model, which is plausible if the\nuntrusted model was trained with a later knowledge cutoff or can search for\nthis information autonomously. We instantiate a simple adaptive attack vector\nby which the attacker embeds publicly known or zero-shot prompt injections in\nthe model outputs. Using this tactic, frontier models consistently evade\ndiverse monitors and complete malicious tasks on two main AI control\nbenchmarks. The attack works universally against current protocols that rely on\na monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,\nas its resampling amplifies the prompt injection and effectively reframes it as\na best-of-$n$ attack. In general, adaptive attacks on monitor models represent\na major blind spot in current control protocols and should become a standard\ncomponent of evaluations for future AI control mechanisms.\n", "link": "http://arxiv.org/abs/2510.09462v1", "date": "2025-10-10", "relevancy": 2.1961, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4794}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4205}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Attacks%20on%20Trusted%20Monitors%20Subvert%20AI%20Control%20Protocols&body=Title%3A%20Adaptive%20Attacks%20on%20Trusted%20Monitors%20Subvert%20AI%20Control%20Protocols%0AAuthor%3A%20Mikhail%20Terekhov%20and%20Alexander%20Panfilov%20and%20Daniil%20Dzenhaliou%20and%20Caglar%20Gulcehre%20and%20Maksym%20Andriushchenko%20and%20Ameya%20Prabhu%20and%20Jonas%20Geiping%0AAbstract%3A%20%20%20AI%20control%20protocols%20serve%20as%20a%20defense%20mechanism%20to%20stop%20untrusted%20LLM%0Aagents%20from%20causing%20harm%20in%20autonomous%20settings.%20Prior%20work%20treats%20this%20as%20a%0Asecurity%20problem%2C%20stress%20testing%20with%20exploits%20that%20use%20the%20deployment%20context%0Ato%20subtly%20complete%20harmful%20side%20tasks%2C%20such%20as%20backdoor%20insertion.%20In%20practice%2C%0Amost%20AI%20control%20protocols%20are%20fundamentally%20based%20on%20LLM%20monitors%2C%20which%20can%0Abecome%20a%20central%20point%20of%20failure.%20We%20study%20adaptive%20attacks%20by%20an%20untrusted%0Amodel%20that%20knows%20the%20protocol%20and%20the%20monitor%20model%2C%20which%20is%20plausible%20if%20the%0Auntrusted%20model%20was%20trained%20with%20a%20later%20knowledge%20cutoff%20or%20can%20search%20for%0Athis%20information%20autonomously.%20We%20instantiate%20a%20simple%20adaptive%20attack%20vector%0Aby%20which%20the%20attacker%20embeds%20publicly%20known%20or%20zero-shot%20prompt%20injections%20in%0Athe%20model%20outputs.%20Using%20this%20tactic%2C%20frontier%20models%20consistently%20evade%0Adiverse%20monitors%20and%20complete%20malicious%20tasks%20on%20two%20main%20AI%20control%0Abenchmarks.%20The%20attack%20works%20universally%20against%20current%20protocols%20that%20rely%20on%0Aa%20monitor.%20Furthermore%2C%20the%20recent%20Defer-to-Resample%20protocol%20even%20backfires%2C%0Aas%20its%20resampling%20amplifies%20the%20prompt%20injection%20and%20effectively%20reframes%20it%20as%0Aa%20best-of-%24n%24%20attack.%20In%20general%2C%20adaptive%20attacks%20on%20monitor%20models%20represent%0Aa%20major%20blind%20spot%20in%20current%20control%20protocols%20and%20should%20become%20a%20standard%0Acomponent%20of%20evaluations%20for%20future%20AI%20control%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Attacks%2520on%2520Trusted%2520Monitors%2520Subvert%2520AI%2520Control%2520Protocols%26entry.906535625%3DMikhail%2520Terekhov%2520and%2520Alexander%2520Panfilov%2520and%2520Daniil%2520Dzenhaliou%2520and%2520Caglar%2520Gulcehre%2520and%2520Maksym%2520Andriushchenko%2520and%2520Ameya%2520Prabhu%2520and%2520Jonas%2520Geiping%26entry.1292438233%3D%2520%2520AI%2520control%2520protocols%2520serve%2520as%2520a%2520defense%2520mechanism%2520to%2520stop%2520untrusted%2520LLM%250Aagents%2520from%2520causing%2520harm%2520in%2520autonomous%2520settings.%2520Prior%2520work%2520treats%2520this%2520as%2520a%250Asecurity%2520problem%252C%2520stress%2520testing%2520with%2520exploits%2520that%2520use%2520the%2520deployment%2520context%250Ato%2520subtly%2520complete%2520harmful%2520side%2520tasks%252C%2520such%2520as%2520backdoor%2520insertion.%2520In%2520practice%252C%250Amost%2520AI%2520control%2520protocols%2520are%2520fundamentally%2520based%2520on%2520LLM%2520monitors%252C%2520which%2520can%250Abecome%2520a%2520central%2520point%2520of%2520failure.%2520We%2520study%2520adaptive%2520attacks%2520by%2520an%2520untrusted%250Amodel%2520that%2520knows%2520the%2520protocol%2520and%2520the%2520monitor%2520model%252C%2520which%2520is%2520plausible%2520if%2520the%250Auntrusted%2520model%2520was%2520trained%2520with%2520a%2520later%2520knowledge%2520cutoff%2520or%2520can%2520search%2520for%250Athis%2520information%2520autonomously.%2520We%2520instantiate%2520a%2520simple%2520adaptive%2520attack%2520vector%250Aby%2520which%2520the%2520attacker%2520embeds%2520publicly%2520known%2520or%2520zero-shot%2520prompt%2520injections%2520in%250Athe%2520model%2520outputs.%2520Using%2520this%2520tactic%252C%2520frontier%2520models%2520consistently%2520evade%250Adiverse%2520monitors%2520and%2520complete%2520malicious%2520tasks%2520on%2520two%2520main%2520AI%2520control%250Abenchmarks.%2520The%2520attack%2520works%2520universally%2520against%2520current%2520protocols%2520that%2520rely%2520on%250Aa%2520monitor.%2520Furthermore%252C%2520the%2520recent%2520Defer-to-Resample%2520protocol%2520even%2520backfires%252C%250Aas%2520its%2520resampling%2520amplifies%2520the%2520prompt%2520injection%2520and%2520effectively%2520reframes%2520it%2520as%250Aa%2520best-of-%2524n%2524%2520attack.%2520In%2520general%252C%2520adaptive%2520attacks%2520on%2520monitor%2520models%2520represent%250Aa%2520major%2520blind%2520spot%2520in%2520current%2520control%2520protocols%2520and%2520should%2520become%2520a%2520standard%250Acomponent%2520of%2520evaluations%2520for%2520future%2520AI%2520control%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Attacks%20on%20Trusted%20Monitors%20Subvert%20AI%20Control%20Protocols&entry.906535625=Mikhail%20Terekhov%20and%20Alexander%20Panfilov%20and%20Daniil%20Dzenhaliou%20and%20Caglar%20Gulcehre%20and%20Maksym%20Andriushchenko%20and%20Ameya%20Prabhu%20and%20Jonas%20Geiping&entry.1292438233=%20%20AI%20control%20protocols%20serve%20as%20a%20defense%20mechanism%20to%20stop%20untrusted%20LLM%0Aagents%20from%20causing%20harm%20in%20autonomous%20settings.%20Prior%20work%20treats%20this%20as%20a%0Asecurity%20problem%2C%20stress%20testing%20with%20exploits%20that%20use%20the%20deployment%20context%0Ato%20subtly%20complete%20harmful%20side%20tasks%2C%20such%20as%20backdoor%20insertion.%20In%20practice%2C%0Amost%20AI%20control%20protocols%20are%20fundamentally%20based%20on%20LLM%20monitors%2C%20which%20can%0Abecome%20a%20central%20point%20of%20failure.%20We%20study%20adaptive%20attacks%20by%20an%20untrusted%0Amodel%20that%20knows%20the%20protocol%20and%20the%20monitor%20model%2C%20which%20is%20plausible%20if%20the%0Auntrusted%20model%20was%20trained%20with%20a%20later%20knowledge%20cutoff%20or%20can%20search%20for%0Athis%20information%20autonomously.%20We%20instantiate%20a%20simple%20adaptive%20attack%20vector%0Aby%20which%20the%20attacker%20embeds%20publicly%20known%20or%20zero-shot%20prompt%20injections%20in%0Athe%20model%20outputs.%20Using%20this%20tactic%2C%20frontier%20models%20consistently%20evade%0Adiverse%20monitors%20and%20complete%20malicious%20tasks%20on%20two%20main%20AI%20control%0Abenchmarks.%20The%20attack%20works%20universally%20against%20current%20protocols%20that%20rely%20on%0Aa%20monitor.%20Furthermore%2C%20the%20recent%20Defer-to-Resample%20protocol%20even%20backfires%2C%0Aas%20its%20resampling%20amplifies%20the%20prompt%20injection%20and%20effectively%20reframes%20it%20as%0Aa%20best-of-%24n%24%20attack.%20In%20general%2C%20adaptive%20attacks%20on%20monitor%20models%20represent%0Aa%20major%20blind%20spot%20in%20current%20control%20protocols%20and%20should%20become%20a%20standard%0Acomponent%20of%20evaluations%20for%20future%20AI%20control%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09462v1&entry.124074799=Read"},
{"title": "Hallucination Filtering in Radiology Vision-Language Models Using\n  Discrete Semantic Entropy", "author": "Patrick Wienholt and Sophie Caselitz and Robert Siepmann and Philipp Bruners and Keno Bressem and Christiane Kuhl and Jakob Nikolas Kather and Sven Nebelung and Daniel Truhn", "abstract": "  To determine whether using discrete semantic entropy (DSE) to reject\nquestions likely to generate hallucinations can improve the accuracy of\nblack-box vision-language models (VLMs) in radiologic image based visual\nquestion answering (VQA). This retrospective study evaluated DSE using two\npublicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500\nimages with clinical questions and short-text answers) and (ii) a diagnostic\nradiology dataset (206 cases: 60 computed tomography scans, 60 magnetic\nresonance images, 60 radiographs, 26 angiograms) with corresponding\nground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times\nusing a temperature of 1.0. Baseline accuracy was determined using\nlow-temperature answers (temperature 0.1). Meaning-equivalent responses were\ngrouped using bidirectional entailment checks, and DSE was computed from the\nrelative frequencies of the resulting semantic clusters. Accuracy was\nrecalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and\n95% confidence intervals were obtained using bootstrap resampling and a\nBonferroni-corrected threshold of p < .004 for statistical significance. Across\n706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for\nGPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on\nthe remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and\n63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains\nwere observed across both datasets and largely remained statistically\nsignificant after Bonferroni correction. DSE enables reliable hallucination\ndetection in black-box VLMs by quantifying semantic inconsistency. This method\nsignificantly improves diagnostic answer accuracy and offers a filtering\nstrategy for clinical VLM applications.\n", "link": "http://arxiv.org/abs/2510.09256v1", "date": "2025-10-10", "relevancy": 2.1926, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallucination%20Filtering%20in%20Radiology%20Vision-Language%20Models%20Using%0A%20%20Discrete%20Semantic%20Entropy&body=Title%3A%20Hallucination%20Filtering%20in%20Radiology%20Vision-Language%20Models%20Using%0A%20%20Discrete%20Semantic%20Entropy%0AAuthor%3A%20Patrick%20Wienholt%20and%20Sophie%20Caselitz%20and%20Robert%20Siepmann%20and%20Philipp%20Bruners%20and%20Keno%20Bressem%20and%20Christiane%20Kuhl%20and%20Jakob%20Nikolas%20Kather%20and%20Sven%20Nebelung%20and%20Daniel%20Truhn%0AAbstract%3A%20%20%20To%20determine%20whether%20using%20discrete%20semantic%20entropy%20%28DSE%29%20to%20reject%0Aquestions%20likely%20to%20generate%20hallucinations%20can%20improve%20the%20accuracy%20of%0Ablack-box%20vision-language%20models%20%28VLMs%29%20in%20radiologic%20image%20based%20visual%0Aquestion%20answering%20%28VQA%29.%20This%20retrospective%20study%20evaluated%20DSE%20using%20two%0Apublicly%20available%2C%20de-identified%20datasets%3A%20%28i%29%20the%20VQA-Med%202019%20benchmark%20%28500%0Aimages%20with%20clinical%20questions%20and%20short-text%20answers%29%20and%20%28ii%29%20a%20diagnostic%0Aradiology%20dataset%20%28206%20cases%3A%2060%20computed%20tomography%20scans%2C%2060%20magnetic%0Aresonance%20images%2C%2060%20radiographs%2C%2026%20angiograms%29%20with%20corresponding%0Aground-truth%20diagnoses.%20GPT-4o%20and%20GPT-4.1%20answered%20each%20question%2015%20times%0Ausing%20a%20temperature%20of%201.0.%20Baseline%20accuracy%20was%20determined%20using%0Alow-temperature%20answers%20%28temperature%200.1%29.%20Meaning-equivalent%20responses%20were%0Agrouped%20using%20bidirectional%20entailment%20checks%2C%20and%20DSE%20was%20computed%20from%20the%0Arelative%20frequencies%20of%20the%20resulting%20semantic%20clusters.%20Accuracy%20was%0Arecalculated%20after%20excluding%20questions%20with%20DSE%20%3E%200.6%20or%20%3E%200.3.%20p-values%20and%0A95%25%20confidence%20intervals%20were%20obtained%20using%20bootstrap%20resampling%20and%20a%0ABonferroni-corrected%20threshold%20of%20p%20%3C%20.004%20for%20statistical%20significance.%20Across%0A706%20image-question%20pairs%2C%20baseline%20accuracy%20was%2051.7%25%20for%20GPT-4o%20and%2054.8%25%20for%0AGPT-4.1.%20After%20filtering%20out%20high-entropy%20questions%20%28DSE%20%3E%200.3%29%2C%20accuracy%20on%0Athe%20remaining%20questions%20was%2076.3%25%20%28retained%20questions%3A%20334/706%29%20for%20GPT-4o%20and%0A63.8%25%20%28retained%20questions%3A%20499/706%29%20for%20GPT-4.1%20%28both%20p%20%3C%20.001%29.%20Accuracy%20gains%0Awere%20observed%20across%20both%20datasets%20and%20largely%20remained%20statistically%0Asignificant%20after%20Bonferroni%20correction.%20DSE%20enables%20reliable%20hallucination%0Adetection%20in%20black-box%20VLMs%20by%20quantifying%20semantic%20inconsistency.%20This%20method%0Asignificantly%20improves%20diagnostic%20answer%20accuracy%20and%20offers%20a%20filtering%0Astrategy%20for%20clinical%20VLM%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallucination%2520Filtering%2520in%2520Radiology%2520Vision-Language%2520Models%2520Using%250A%2520%2520Discrete%2520Semantic%2520Entropy%26entry.906535625%3DPatrick%2520Wienholt%2520and%2520Sophie%2520Caselitz%2520and%2520Robert%2520Siepmann%2520and%2520Philipp%2520Bruners%2520and%2520Keno%2520Bressem%2520and%2520Christiane%2520Kuhl%2520and%2520Jakob%2520Nikolas%2520Kather%2520and%2520Sven%2520Nebelung%2520and%2520Daniel%2520Truhn%26entry.1292438233%3D%2520%2520To%2520determine%2520whether%2520using%2520discrete%2520semantic%2520entropy%2520%2528DSE%2529%2520to%2520reject%250Aquestions%2520likely%2520to%2520generate%2520hallucinations%2520can%2520improve%2520the%2520accuracy%2520of%250Ablack-box%2520vision-language%2520models%2520%2528VLMs%2529%2520in%2520radiologic%2520image%2520based%2520visual%250Aquestion%2520answering%2520%2528VQA%2529.%2520This%2520retrospective%2520study%2520evaluated%2520DSE%2520using%2520two%250Apublicly%2520available%252C%2520de-identified%2520datasets%253A%2520%2528i%2529%2520the%2520VQA-Med%25202019%2520benchmark%2520%2528500%250Aimages%2520with%2520clinical%2520questions%2520and%2520short-text%2520answers%2529%2520and%2520%2528ii%2529%2520a%2520diagnostic%250Aradiology%2520dataset%2520%2528206%2520cases%253A%252060%2520computed%2520tomography%2520scans%252C%252060%2520magnetic%250Aresonance%2520images%252C%252060%2520radiographs%252C%252026%2520angiograms%2529%2520with%2520corresponding%250Aground-truth%2520diagnoses.%2520GPT-4o%2520and%2520GPT-4.1%2520answered%2520each%2520question%252015%2520times%250Ausing%2520a%2520temperature%2520of%25201.0.%2520Baseline%2520accuracy%2520was%2520determined%2520using%250Alow-temperature%2520answers%2520%2528temperature%25200.1%2529.%2520Meaning-equivalent%2520responses%2520were%250Agrouped%2520using%2520bidirectional%2520entailment%2520checks%252C%2520and%2520DSE%2520was%2520computed%2520from%2520the%250Arelative%2520frequencies%2520of%2520the%2520resulting%2520semantic%2520clusters.%2520Accuracy%2520was%250Arecalculated%2520after%2520excluding%2520questions%2520with%2520DSE%2520%253E%25200.6%2520or%2520%253E%25200.3.%2520p-values%2520and%250A95%2525%2520confidence%2520intervals%2520were%2520obtained%2520using%2520bootstrap%2520resampling%2520and%2520a%250ABonferroni-corrected%2520threshold%2520of%2520p%2520%253C%2520.004%2520for%2520statistical%2520significance.%2520Across%250A706%2520image-question%2520pairs%252C%2520baseline%2520accuracy%2520was%252051.7%2525%2520for%2520GPT-4o%2520and%252054.8%2525%2520for%250AGPT-4.1.%2520After%2520filtering%2520out%2520high-entropy%2520questions%2520%2528DSE%2520%253E%25200.3%2529%252C%2520accuracy%2520on%250Athe%2520remaining%2520questions%2520was%252076.3%2525%2520%2528retained%2520questions%253A%2520334/706%2529%2520for%2520GPT-4o%2520and%250A63.8%2525%2520%2528retained%2520questions%253A%2520499/706%2529%2520for%2520GPT-4.1%2520%2528both%2520p%2520%253C%2520.001%2529.%2520Accuracy%2520gains%250Awere%2520observed%2520across%2520both%2520datasets%2520and%2520largely%2520remained%2520statistically%250Asignificant%2520after%2520Bonferroni%2520correction.%2520DSE%2520enables%2520reliable%2520hallucination%250Adetection%2520in%2520black-box%2520VLMs%2520by%2520quantifying%2520semantic%2520inconsistency.%2520This%2520method%250Asignificantly%2520improves%2520diagnostic%2520answer%2520accuracy%2520and%2520offers%2520a%2520filtering%250Astrategy%2520for%2520clinical%2520VLM%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallucination%20Filtering%20in%20Radiology%20Vision-Language%20Models%20Using%0A%20%20Discrete%20Semantic%20Entropy&entry.906535625=Patrick%20Wienholt%20and%20Sophie%20Caselitz%20and%20Robert%20Siepmann%20and%20Philipp%20Bruners%20and%20Keno%20Bressem%20and%20Christiane%20Kuhl%20and%20Jakob%20Nikolas%20Kather%20and%20Sven%20Nebelung%20and%20Daniel%20Truhn&entry.1292438233=%20%20To%20determine%20whether%20using%20discrete%20semantic%20entropy%20%28DSE%29%20to%20reject%0Aquestions%20likely%20to%20generate%20hallucinations%20can%20improve%20the%20accuracy%20of%0Ablack-box%20vision-language%20models%20%28VLMs%29%20in%20radiologic%20image%20based%20visual%0Aquestion%20answering%20%28VQA%29.%20This%20retrospective%20study%20evaluated%20DSE%20using%20two%0Apublicly%20available%2C%20de-identified%20datasets%3A%20%28i%29%20the%20VQA-Med%202019%20benchmark%20%28500%0Aimages%20with%20clinical%20questions%20and%20short-text%20answers%29%20and%20%28ii%29%20a%20diagnostic%0Aradiology%20dataset%20%28206%20cases%3A%2060%20computed%20tomography%20scans%2C%2060%20magnetic%0Aresonance%20images%2C%2060%20radiographs%2C%2026%20angiograms%29%20with%20corresponding%0Aground-truth%20diagnoses.%20GPT-4o%20and%20GPT-4.1%20answered%20each%20question%2015%20times%0Ausing%20a%20temperature%20of%201.0.%20Baseline%20accuracy%20was%20determined%20using%0Alow-temperature%20answers%20%28temperature%200.1%29.%20Meaning-equivalent%20responses%20were%0Agrouped%20using%20bidirectional%20entailment%20checks%2C%20and%20DSE%20was%20computed%20from%20the%0Arelative%20frequencies%20of%20the%20resulting%20semantic%20clusters.%20Accuracy%20was%0Arecalculated%20after%20excluding%20questions%20with%20DSE%20%3E%200.6%20or%20%3E%200.3.%20p-values%20and%0A95%25%20confidence%20intervals%20were%20obtained%20using%20bootstrap%20resampling%20and%20a%0ABonferroni-corrected%20threshold%20of%20p%20%3C%20.004%20for%20statistical%20significance.%20Across%0A706%20image-question%20pairs%2C%20baseline%20accuracy%20was%2051.7%25%20for%20GPT-4o%20and%2054.8%25%20for%0AGPT-4.1.%20After%20filtering%20out%20high-entropy%20questions%20%28DSE%20%3E%200.3%29%2C%20accuracy%20on%0Athe%20remaining%20questions%20was%2076.3%25%20%28retained%20questions%3A%20334/706%29%20for%20GPT-4o%20and%0A63.8%25%20%28retained%20questions%3A%20499/706%29%20for%20GPT-4.1%20%28both%20p%20%3C%20.001%29.%20Accuracy%20gains%0Awere%20observed%20across%20both%20datasets%20and%20largely%20remained%20statistically%0Asignificant%20after%20Bonferroni%20correction.%20DSE%20enables%20reliable%20hallucination%0Adetection%20in%20black-box%20VLMs%20by%20quantifying%20semantic%20inconsistency.%20This%20method%0Asignificantly%20improves%20diagnostic%20answer%20accuracy%20and%20offers%20a%20filtering%0Astrategy%20for%20clinical%20VLM%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09256v1&entry.124074799=Read"},
{"title": "A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI\n  Synthesis", "author": "Valentin Biller and Lucas Zimmer and Can Erdur and Sandeep Nagar and Daniel R\u00fcckert and Niklas Bubeck and Jonas Weidner", "abstract": "  Magnetic resonance imaging (MRI) inpainting supports numerous clinical and\nresearch applications. We introduce the first generative model that conditions\non voxel-level, continuous tumor concentrations to synthesize high-fidelity\nbrain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this\narchitecture to the complementary task of healthy tissue restoration by setting\nthe tumor concentrations to zero. Our latent diffusion model conditioned on\nboth tissue segmentations and the tumor concentrations generates 3D spatially\ncoherent and anatomically consistent images for both tumor synthesis and\nhealthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5,\nand for tumor inpainting, we achieve 17.4. Our code is available at:\nhttps://github.com/valentin-biller/ldm.git\n", "link": "http://arxiv.org/abs/2510.09365v1", "date": "2025-10-10", "relevancy": 2.1904, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5575}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5457}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Biophysically-Conditioned%20Generative%20Framework%20for%203D%20Brain%20Tumor%20MRI%0A%20%20Synthesis&body=Title%3A%20A%20Biophysically-Conditioned%20Generative%20Framework%20for%203D%20Brain%20Tumor%20MRI%0A%20%20Synthesis%0AAuthor%3A%20Valentin%20Biller%20and%20Lucas%20Zimmer%20and%20Can%20Erdur%20and%20Sandeep%20Nagar%20and%20Daniel%20R%C3%BCckert%20and%20Niklas%20Bubeck%20and%20Jonas%20Weidner%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20inpainting%20supports%20numerous%20clinical%20and%0Aresearch%20applications.%20We%20introduce%20the%20first%20generative%20model%20that%20conditions%0Aon%20voxel-level%2C%20continuous%20tumor%20concentrations%20to%20synthesize%20high-fidelity%0Abrain%20tumor%20MRIs.%20For%20the%20BraTS%202025%20Inpainting%20Challenge%2C%20we%20adapt%20this%0Aarchitecture%20to%20the%20complementary%20task%20of%20healthy%20tissue%20restoration%20by%20setting%0Athe%20tumor%20concentrations%20to%20zero.%20Our%20latent%20diffusion%20model%20conditioned%20on%0Aboth%20tissue%20segmentations%20and%20the%20tumor%20concentrations%20generates%203D%20spatially%0Acoherent%20and%20anatomically%20consistent%20images%20for%20both%20tumor%20synthesis%20and%0Ahealthy%20tissue%20inpainting.%20For%20healthy%20inpainting%2C%20we%20achieve%20a%20PSNR%20of%2018.5%2C%0Aand%20for%20tumor%20inpainting%2C%20we%20achieve%2017.4.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/valentin-biller/ldm.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Biophysically-Conditioned%2520Generative%2520Framework%2520for%25203D%2520Brain%2520Tumor%2520MRI%250A%2520%2520Synthesis%26entry.906535625%3DValentin%2520Biller%2520and%2520Lucas%2520Zimmer%2520and%2520Can%2520Erdur%2520and%2520Sandeep%2520Nagar%2520and%2520Daniel%2520R%25C3%25BCckert%2520and%2520Niklas%2520Bubeck%2520and%2520Jonas%2520Weidner%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520inpainting%2520supports%2520numerous%2520clinical%2520and%250Aresearch%2520applications.%2520We%2520introduce%2520the%2520first%2520generative%2520model%2520that%2520conditions%250Aon%2520voxel-level%252C%2520continuous%2520tumor%2520concentrations%2520to%2520synthesize%2520high-fidelity%250Abrain%2520tumor%2520MRIs.%2520For%2520the%2520BraTS%25202025%2520Inpainting%2520Challenge%252C%2520we%2520adapt%2520this%250Aarchitecture%2520to%2520the%2520complementary%2520task%2520of%2520healthy%2520tissue%2520restoration%2520by%2520setting%250Athe%2520tumor%2520concentrations%2520to%2520zero.%2520Our%2520latent%2520diffusion%2520model%2520conditioned%2520on%250Aboth%2520tissue%2520segmentations%2520and%2520the%2520tumor%2520concentrations%2520generates%25203D%2520spatially%250Acoherent%2520and%2520anatomically%2520consistent%2520images%2520for%2520both%2520tumor%2520synthesis%2520and%250Ahealthy%2520tissue%2520inpainting.%2520For%2520healthy%2520inpainting%252C%2520we%2520achieve%2520a%2520PSNR%2520of%252018.5%252C%250Aand%2520for%2520tumor%2520inpainting%252C%2520we%2520achieve%252017.4.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/valentin-biller/ldm.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Biophysically-Conditioned%20Generative%20Framework%20for%203D%20Brain%20Tumor%20MRI%0A%20%20Synthesis&entry.906535625=Valentin%20Biller%20and%20Lucas%20Zimmer%20and%20Can%20Erdur%20and%20Sandeep%20Nagar%20and%20Daniel%20R%C3%BCckert%20and%20Niklas%20Bubeck%20and%20Jonas%20Weidner&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20inpainting%20supports%20numerous%20clinical%20and%0Aresearch%20applications.%20We%20introduce%20the%20first%20generative%20model%20that%20conditions%0Aon%20voxel-level%2C%20continuous%20tumor%20concentrations%20to%20synthesize%20high-fidelity%0Abrain%20tumor%20MRIs.%20For%20the%20BraTS%202025%20Inpainting%20Challenge%2C%20we%20adapt%20this%0Aarchitecture%20to%20the%20complementary%20task%20of%20healthy%20tissue%20restoration%20by%20setting%0Athe%20tumor%20concentrations%20to%20zero.%20Our%20latent%20diffusion%20model%20conditioned%20on%0Aboth%20tissue%20segmentations%20and%20the%20tumor%20concentrations%20generates%203D%20spatially%0Acoherent%20and%20anatomically%20consistent%20images%20for%20both%20tumor%20synthesis%20and%0Ahealthy%20tissue%20inpainting.%20For%20healthy%20inpainting%2C%20we%20achieve%20a%20PSNR%20of%2018.5%2C%0Aand%20for%20tumor%20inpainting%2C%20we%20achieve%2017.4.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/valentin-biller/ldm.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09365v1&entry.124074799=Read"},
{"title": "BaNEL: Exploration Posteriors for Generative Modeling Using Only\n  Negative Rewards", "author": "Sangyun Lee and Brandon Amos and Giulia Fanti", "abstract": "  Today's generative models thrive with large amounts of supervised data and\ninformative reward functions characterizing the quality of the generation. They\nwork under the assumptions that the supervised data provides knowledge to\npre-train the model, and the reward function provides dense information about\nhow to further improve the generation quality and correctness. However, in the\nhardest instances of important problems, two problems arise: (1) the base\ngenerative model attains a near-zero reward signal, and (2) calls to the reward\noracle are expensive. This setting poses a fundamentally different learning\nchallenge than standard reward-based post-training. To address this, we propose\nBaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the\nmodel using failed attempts only, while minimizing the number of reward\nevaluations (NREs). Our method is based on the idea that the problem of\nlearning regularities underlying failures can be cast as another, in-loop\ngenerative modeling problem. We then leverage this model to assess whether new\ndata resembles previously seen failures and steer the generation away from\nthem. We show that BaNEL can improve model performance without observing a\nsingle successful sample on several sparse-reward tasks, outperforming existing\nnovelty-bonus approaches by up to several orders of magnitude in success rate,\nwhile using fewer reward evaluations.\n", "link": "http://arxiv.org/abs/2510.09596v1", "date": "2025-10-10", "relevancy": 2.1896, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5698}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.548}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BaNEL%3A%20Exploration%20Posteriors%20for%20Generative%20Modeling%20Using%20Only%0A%20%20Negative%20Rewards&body=Title%3A%20BaNEL%3A%20Exploration%20Posteriors%20for%20Generative%20Modeling%20Using%20Only%0A%20%20Negative%20Rewards%0AAuthor%3A%20Sangyun%20Lee%20and%20Brandon%20Amos%20and%20Giulia%20Fanti%0AAbstract%3A%20%20%20Today%27s%20generative%20models%20thrive%20with%20large%20amounts%20of%20supervised%20data%20and%0Ainformative%20reward%20functions%20characterizing%20the%20quality%20of%20the%20generation.%20They%0Awork%20under%20the%20assumptions%20that%20the%20supervised%20data%20provides%20knowledge%20to%0Apre-train%20the%20model%2C%20and%20the%20reward%20function%20provides%20dense%20information%20about%0Ahow%20to%20further%20improve%20the%20generation%20quality%20and%20correctness.%20However%2C%20in%20the%0Ahardest%20instances%20of%20important%20problems%2C%20two%20problems%20arise%3A%20%281%29%20the%20base%0Agenerative%20model%20attains%20a%20near-zero%20reward%20signal%2C%20and%20%282%29%20calls%20to%20the%20reward%0Aoracle%20are%20expensive.%20This%20setting%20poses%20a%20fundamentally%20different%20learning%0Achallenge%20than%20standard%20reward-based%20post-training.%20To%20address%20this%2C%20we%20propose%0ABaNEL%20%28Bayesian%20Negative%20Evidence%20Learning%29%2C%20an%20algorithm%20that%20post-trains%20the%0Amodel%20using%20failed%20attempts%20only%2C%20while%20minimizing%20the%20number%20of%20reward%0Aevaluations%20%28NREs%29.%20Our%20method%20is%20based%20on%20the%20idea%20that%20the%20problem%20of%0Alearning%20regularities%20underlying%20failures%20can%20be%20cast%20as%20another%2C%20in-loop%0Agenerative%20modeling%20problem.%20We%20then%20leverage%20this%20model%20to%20assess%20whether%20new%0Adata%20resembles%20previously%20seen%20failures%20and%20steer%20the%20generation%20away%20from%0Athem.%20We%20show%20that%20BaNEL%20can%20improve%20model%20performance%20without%20observing%20a%0Asingle%20successful%20sample%20on%20several%20sparse-reward%20tasks%2C%20outperforming%20existing%0Anovelty-bonus%20approaches%20by%20up%20to%20several%20orders%20of%20magnitude%20in%20success%20rate%2C%0Awhile%20using%20fewer%20reward%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBaNEL%253A%2520Exploration%2520Posteriors%2520for%2520Generative%2520Modeling%2520Using%2520Only%250A%2520%2520Negative%2520Rewards%26entry.906535625%3DSangyun%2520Lee%2520and%2520Brandon%2520Amos%2520and%2520Giulia%2520Fanti%26entry.1292438233%3D%2520%2520Today%2527s%2520generative%2520models%2520thrive%2520with%2520large%2520amounts%2520of%2520supervised%2520data%2520and%250Ainformative%2520reward%2520functions%2520characterizing%2520the%2520quality%2520of%2520the%2520generation.%2520They%250Awork%2520under%2520the%2520assumptions%2520that%2520the%2520supervised%2520data%2520provides%2520knowledge%2520to%250Apre-train%2520the%2520model%252C%2520and%2520the%2520reward%2520function%2520provides%2520dense%2520information%2520about%250Ahow%2520to%2520further%2520improve%2520the%2520generation%2520quality%2520and%2520correctness.%2520However%252C%2520in%2520the%250Ahardest%2520instances%2520of%2520important%2520problems%252C%2520two%2520problems%2520arise%253A%2520%25281%2529%2520the%2520base%250Agenerative%2520model%2520attains%2520a%2520near-zero%2520reward%2520signal%252C%2520and%2520%25282%2529%2520calls%2520to%2520the%2520reward%250Aoracle%2520are%2520expensive.%2520This%2520setting%2520poses%2520a%2520fundamentally%2520different%2520learning%250Achallenge%2520than%2520standard%2520reward-based%2520post-training.%2520To%2520address%2520this%252C%2520we%2520propose%250ABaNEL%2520%2528Bayesian%2520Negative%2520Evidence%2520Learning%2529%252C%2520an%2520algorithm%2520that%2520post-trains%2520the%250Amodel%2520using%2520failed%2520attempts%2520only%252C%2520while%2520minimizing%2520the%2520number%2520of%2520reward%250Aevaluations%2520%2528NREs%2529.%2520Our%2520method%2520is%2520based%2520on%2520the%2520idea%2520that%2520the%2520problem%2520of%250Alearning%2520regularities%2520underlying%2520failures%2520can%2520be%2520cast%2520as%2520another%252C%2520in-loop%250Agenerative%2520modeling%2520problem.%2520We%2520then%2520leverage%2520this%2520model%2520to%2520assess%2520whether%2520new%250Adata%2520resembles%2520previously%2520seen%2520failures%2520and%2520steer%2520the%2520generation%2520away%2520from%250Athem.%2520We%2520show%2520that%2520BaNEL%2520can%2520improve%2520model%2520performance%2520without%2520observing%2520a%250Asingle%2520successful%2520sample%2520on%2520several%2520sparse-reward%2520tasks%252C%2520outperforming%2520existing%250Anovelty-bonus%2520approaches%2520by%2520up%2520to%2520several%2520orders%2520of%2520magnitude%2520in%2520success%2520rate%252C%250Awhile%2520using%2520fewer%2520reward%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BaNEL%3A%20Exploration%20Posteriors%20for%20Generative%20Modeling%20Using%20Only%0A%20%20Negative%20Rewards&entry.906535625=Sangyun%20Lee%20and%20Brandon%20Amos%20and%20Giulia%20Fanti&entry.1292438233=%20%20Today%27s%20generative%20models%20thrive%20with%20large%20amounts%20of%20supervised%20data%20and%0Ainformative%20reward%20functions%20characterizing%20the%20quality%20of%20the%20generation.%20They%0Awork%20under%20the%20assumptions%20that%20the%20supervised%20data%20provides%20knowledge%20to%0Apre-train%20the%20model%2C%20and%20the%20reward%20function%20provides%20dense%20information%20about%0Ahow%20to%20further%20improve%20the%20generation%20quality%20and%20correctness.%20However%2C%20in%20the%0Ahardest%20instances%20of%20important%20problems%2C%20two%20problems%20arise%3A%20%281%29%20the%20base%0Agenerative%20model%20attains%20a%20near-zero%20reward%20signal%2C%20and%20%282%29%20calls%20to%20the%20reward%0Aoracle%20are%20expensive.%20This%20setting%20poses%20a%20fundamentally%20different%20learning%0Achallenge%20than%20standard%20reward-based%20post-training.%20To%20address%20this%2C%20we%20propose%0ABaNEL%20%28Bayesian%20Negative%20Evidence%20Learning%29%2C%20an%20algorithm%20that%20post-trains%20the%0Amodel%20using%20failed%20attempts%20only%2C%20while%20minimizing%20the%20number%20of%20reward%0Aevaluations%20%28NREs%29.%20Our%20method%20is%20based%20on%20the%20idea%20that%20the%20problem%20of%0Alearning%20regularities%20underlying%20failures%20can%20be%20cast%20as%20another%2C%20in-loop%0Agenerative%20modeling%20problem.%20We%20then%20leverage%20this%20model%20to%20assess%20whether%20new%0Adata%20resembles%20previously%20seen%20failures%20and%20steer%20the%20generation%20away%20from%0Athem.%20We%20show%20that%20BaNEL%20can%20improve%20model%20performance%20without%20observing%20a%0Asingle%20successful%20sample%20on%20several%20sparse-reward%20tasks%2C%20outperforming%20existing%0Anovelty-bonus%20approaches%20by%20up%20to%20several%20orders%20of%20magnitude%20in%20success%20rate%2C%0Awhile%20using%20fewer%20reward%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09596v1&entry.124074799=Read"},
{"title": "The 1st Solution for CARE Liver Task Challenge 2025: Contrast-Aware\n  Semi-Supervised Segmentation with Domain Generalization and Test-Time\n  Adaptation", "author": "Jincan Lou and Jingkun Chen and Haoquan Li and Hang Li and Wenjian Huang and Weihua Chen and Fan Wang and Jianguo Zhang", "abstract": "  Accurate liver segmentation from contrast-enhanced MRI is essential for\ndiagnosis, treatment planning, and disease monitoring. However, it remains\nchallenging due to limited annotated data, heterogeneous enhancement protocols,\nand significant domain shifts across scanners and institutions. Traditional\nimage-to-image translation frameworks have made great progress in domain\ngeneralization, but their application is not straightforward. For example,\nPix2Pix requires image registration, and cycle-GAN cannot be integrated\nseamlessly into segmentation pipelines. Meanwhile, these methods are originally\nused to deal with cross-modality scenarios, and often introduce structural\ndistortions and suffer from unstable training, which may pose drawbacks in our\nsingle-modality scenario. To address these challenges, we propose CoSSeg-TTA, a\ncompact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary\nphase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised\nmean teacher scheme to exploit large amounts of unlabeled volumes. A domain\nadaptation module, incorporating a randomized histogram-based style appearance\ntransfer function and a trainable contrast-aware network, enriches domain\ndiversity and mitigates cross-center variability. Furthermore, a continual\ntest-time adaptation strategy is employed to improve robustness during\ninference. Extensive experiments demonstrate that our framework consistently\noutperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff\nDistance while exhibiting strong generalization to unseen domains under\nlow-annotation conditions.\n", "link": "http://arxiv.org/abs/2510.04243v2", "date": "2025-10-10", "relevancy": 2.1825, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5352}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%201st%20Solution%20for%20CARE%20Liver%20Task%20Challenge%202025%3A%20Contrast-Aware%0A%20%20Semi-Supervised%20Segmentation%20with%20Domain%20Generalization%20and%20Test-Time%0A%20%20Adaptation&body=Title%3A%20The%201st%20Solution%20for%20CARE%20Liver%20Task%20Challenge%202025%3A%20Contrast-Aware%0A%20%20Semi-Supervised%20Segmentation%20with%20Domain%20Generalization%20and%20Test-Time%0A%20%20Adaptation%0AAuthor%3A%20Jincan%20Lou%20and%20Jingkun%20Chen%20and%20Haoquan%20Li%20and%20Hang%20Li%20and%20Wenjian%20Huang%20and%20Weihua%20Chen%20and%20Fan%20Wang%20and%20Jianguo%20Zhang%0AAbstract%3A%20%20%20Accurate%20liver%20segmentation%20from%20contrast-enhanced%20MRI%20is%20essential%20for%0Adiagnosis%2C%20treatment%20planning%2C%20and%20disease%20monitoring.%20However%2C%20it%20remains%0Achallenging%20due%20to%20limited%20annotated%20data%2C%20heterogeneous%20enhancement%20protocols%2C%0Aand%20significant%20domain%20shifts%20across%20scanners%20and%20institutions.%20Traditional%0Aimage-to-image%20translation%20frameworks%20have%20made%20great%20progress%20in%20domain%0Ageneralization%2C%20but%20their%20application%20is%20not%20straightforward.%20For%20example%2C%0APix2Pix%20requires%20image%20registration%2C%20and%20cycle-GAN%20cannot%20be%20integrated%0Aseamlessly%20into%20segmentation%20pipelines.%20Meanwhile%2C%20these%20methods%20are%20originally%0Aused%20to%20deal%20with%20cross-modality%20scenarios%2C%20and%20often%20introduce%20structural%0Adistortions%20and%20suffer%20from%20unstable%20training%2C%20which%20may%20pose%20drawbacks%20in%20our%0Asingle-modality%20scenario.%20To%20address%20these%20challenges%2C%20we%20propose%20CoSSeg-TTA%2C%20a%0Acompact%20segmentation%20framework%20for%20the%20GED4%20%28Gd-EOB-DTPA%20enhanced%20hepatobiliary%0Aphase%20MRI%29%20modality%20built%20upon%20nnU-Netv2%20and%20enhanced%20with%20a%20semi-supervised%0Amean%20teacher%20scheme%20to%20exploit%20large%20amounts%20of%20unlabeled%20volumes.%20A%20domain%0Aadaptation%20module%2C%20incorporating%20a%20randomized%20histogram-based%20style%20appearance%0Atransfer%20function%20and%20a%20trainable%20contrast-aware%20network%2C%20enriches%20domain%0Adiversity%20and%20mitigates%20cross-center%20variability.%20Furthermore%2C%20a%20continual%0Atest-time%20adaptation%20strategy%20is%20employed%20to%20improve%20robustness%20during%0Ainference.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20consistently%0Aoutperforms%20the%20nnU-Netv2%20baseline%2C%20achieving%20superior%20Dice%20score%20and%20Hausdorff%0ADistance%20while%20exhibiting%20strong%20generalization%20to%20unseen%20domains%20under%0Alow-annotation%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04243v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%25201st%2520Solution%2520for%2520CARE%2520Liver%2520Task%2520Challenge%25202025%253A%2520Contrast-Aware%250A%2520%2520Semi-Supervised%2520Segmentation%2520with%2520Domain%2520Generalization%2520and%2520Test-Time%250A%2520%2520Adaptation%26entry.906535625%3DJincan%2520Lou%2520and%2520Jingkun%2520Chen%2520and%2520Haoquan%2520Li%2520and%2520Hang%2520Li%2520and%2520Wenjian%2520Huang%2520and%2520Weihua%2520Chen%2520and%2520Fan%2520Wang%2520and%2520Jianguo%2520Zhang%26entry.1292438233%3D%2520%2520Accurate%2520liver%2520segmentation%2520from%2520contrast-enhanced%2520MRI%2520is%2520essential%2520for%250Adiagnosis%252C%2520treatment%2520planning%252C%2520and%2520disease%2520monitoring.%2520However%252C%2520it%2520remains%250Achallenging%2520due%2520to%2520limited%2520annotated%2520data%252C%2520heterogeneous%2520enhancement%2520protocols%252C%250Aand%2520significant%2520domain%2520shifts%2520across%2520scanners%2520and%2520institutions.%2520Traditional%250Aimage-to-image%2520translation%2520frameworks%2520have%2520made%2520great%2520progress%2520in%2520domain%250Ageneralization%252C%2520but%2520their%2520application%2520is%2520not%2520straightforward.%2520For%2520example%252C%250APix2Pix%2520requires%2520image%2520registration%252C%2520and%2520cycle-GAN%2520cannot%2520be%2520integrated%250Aseamlessly%2520into%2520segmentation%2520pipelines.%2520Meanwhile%252C%2520these%2520methods%2520are%2520originally%250Aused%2520to%2520deal%2520with%2520cross-modality%2520scenarios%252C%2520and%2520often%2520introduce%2520structural%250Adistortions%2520and%2520suffer%2520from%2520unstable%2520training%252C%2520which%2520may%2520pose%2520drawbacks%2520in%2520our%250Asingle-modality%2520scenario.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520CoSSeg-TTA%252C%2520a%250Acompact%2520segmentation%2520framework%2520for%2520the%2520GED4%2520%2528Gd-EOB-DTPA%2520enhanced%2520hepatobiliary%250Aphase%2520MRI%2529%2520modality%2520built%2520upon%2520nnU-Netv2%2520and%2520enhanced%2520with%2520a%2520semi-supervised%250Amean%2520teacher%2520scheme%2520to%2520exploit%2520large%2520amounts%2520of%2520unlabeled%2520volumes.%2520A%2520domain%250Aadaptation%2520module%252C%2520incorporating%2520a%2520randomized%2520histogram-based%2520style%2520appearance%250Atransfer%2520function%2520and%2520a%2520trainable%2520contrast-aware%2520network%252C%2520enriches%2520domain%250Adiversity%2520and%2520mitigates%2520cross-center%2520variability.%2520Furthermore%252C%2520a%2520continual%250Atest-time%2520adaptation%2520strategy%2520is%2520employed%2520to%2520improve%2520robustness%2520during%250Ainference.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520consistently%250Aoutperforms%2520the%2520nnU-Netv2%2520baseline%252C%2520achieving%2520superior%2520Dice%2520score%2520and%2520Hausdorff%250ADistance%2520while%2520exhibiting%2520strong%2520generalization%2520to%2520unseen%2520domains%2520under%250Alow-annotation%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04243v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%201st%20Solution%20for%20CARE%20Liver%20Task%20Challenge%202025%3A%20Contrast-Aware%0A%20%20Semi-Supervised%20Segmentation%20with%20Domain%20Generalization%20and%20Test-Time%0A%20%20Adaptation&entry.906535625=Jincan%20Lou%20and%20Jingkun%20Chen%20and%20Haoquan%20Li%20and%20Hang%20Li%20and%20Wenjian%20Huang%20and%20Weihua%20Chen%20and%20Fan%20Wang%20and%20Jianguo%20Zhang&entry.1292438233=%20%20Accurate%20liver%20segmentation%20from%20contrast-enhanced%20MRI%20is%20essential%20for%0Adiagnosis%2C%20treatment%20planning%2C%20and%20disease%20monitoring.%20However%2C%20it%20remains%0Achallenging%20due%20to%20limited%20annotated%20data%2C%20heterogeneous%20enhancement%20protocols%2C%0Aand%20significant%20domain%20shifts%20across%20scanners%20and%20institutions.%20Traditional%0Aimage-to-image%20translation%20frameworks%20have%20made%20great%20progress%20in%20domain%0Ageneralization%2C%20but%20their%20application%20is%20not%20straightforward.%20For%20example%2C%0APix2Pix%20requires%20image%20registration%2C%20and%20cycle-GAN%20cannot%20be%20integrated%0Aseamlessly%20into%20segmentation%20pipelines.%20Meanwhile%2C%20these%20methods%20are%20originally%0Aused%20to%20deal%20with%20cross-modality%20scenarios%2C%20and%20often%20introduce%20structural%0Adistortions%20and%20suffer%20from%20unstable%20training%2C%20which%20may%20pose%20drawbacks%20in%20our%0Asingle-modality%20scenario.%20To%20address%20these%20challenges%2C%20we%20propose%20CoSSeg-TTA%2C%20a%0Acompact%20segmentation%20framework%20for%20the%20GED4%20%28Gd-EOB-DTPA%20enhanced%20hepatobiliary%0Aphase%20MRI%29%20modality%20built%20upon%20nnU-Netv2%20and%20enhanced%20with%20a%20semi-supervised%0Amean%20teacher%20scheme%20to%20exploit%20large%20amounts%20of%20unlabeled%20volumes.%20A%20domain%0Aadaptation%20module%2C%20incorporating%20a%20randomized%20histogram-based%20style%20appearance%0Atransfer%20function%20and%20a%20trainable%20contrast-aware%20network%2C%20enriches%20domain%0Adiversity%20and%20mitigates%20cross-center%20variability.%20Furthermore%2C%20a%20continual%0Atest-time%20adaptation%20strategy%20is%20employed%20to%20improve%20robustness%20during%0Ainference.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20consistently%0Aoutperforms%20the%20nnU-Netv2%20baseline%2C%20achieving%20superior%20Dice%20score%20and%20Hausdorff%0ADistance%20while%20exhibiting%20strong%20generalization%20to%20unseen%20domains%20under%0Alow-annotation%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04243v2&entry.124074799=Read"},
{"title": "TARO: Timestep-Adaptive Representation Alignment with Onset-Aware\n  Conditioning for Synchronized Video-to-Audio Synthesis", "author": "Tri Ton and Ji Woo Hong and Chang D. Yoo", "abstract": "  This paper introduces Timestep-Adaptive Representation Alignment with\nOnset-Aware Conditioning (TARO), a novel framework for high-fidelity and\ntemporally coherent video-to-audio synthesis. Built upon flow-based\ntransformers, which offer stable training and continuous transformations for\nenhanced synchronization and audio quality, TARO introduces two key\ninnovations: (1) Timestep-Adaptive Representation Alignment (TRA), which\ndynamically aligns latent representations by adjusting alignment strength based\non the noise schedule, ensuring smooth evolution and improved fidelity, and (2)\nOnset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp\nevent-driven markers of audio-relevant visual moments to enhance\nsynchronization with dynamic visual events. Extensive experiments on the\nVGGSound and Landscape datasets demonstrate that TARO outperforms prior\nmethods, achieving relatively 53% lower Frechet Distance (FD), 29% lower\nFrechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its\nsuperior audio quality and synchronization precision.\n", "link": "http://arxiv.org/abs/2504.05684v3", "date": "2025-10-10", "relevancy": 2.1722, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5538}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TARO%3A%20Timestep-Adaptive%20Representation%20Alignment%20with%20Onset-Aware%0A%20%20Conditioning%20for%20Synchronized%20Video-to-Audio%20Synthesis&body=Title%3A%20TARO%3A%20Timestep-Adaptive%20Representation%20Alignment%20with%20Onset-Aware%0A%20%20Conditioning%20for%20Synchronized%20Video-to-Audio%20Synthesis%0AAuthor%3A%20Tri%20Ton%20and%20Ji%20Woo%20Hong%20and%20Chang%20D.%20Yoo%0AAbstract%3A%20%20%20This%20paper%20introduces%20Timestep-Adaptive%20Representation%20Alignment%20with%0AOnset-Aware%20Conditioning%20%28TARO%29%2C%20a%20novel%20framework%20for%20high-fidelity%20and%0Atemporally%20coherent%20video-to-audio%20synthesis.%20Built%20upon%20flow-based%0Atransformers%2C%20which%20offer%20stable%20training%20and%20continuous%20transformations%20for%0Aenhanced%20synchronization%20and%20audio%20quality%2C%20TARO%20introduces%20two%20key%0Ainnovations%3A%20%281%29%20Timestep-Adaptive%20Representation%20Alignment%20%28TRA%29%2C%20which%0Adynamically%20aligns%20latent%20representations%20by%20adjusting%20alignment%20strength%20based%0Aon%20the%20noise%20schedule%2C%20ensuring%20smooth%20evolution%20and%20improved%20fidelity%2C%20and%20%282%29%0AOnset-Aware%20Conditioning%20%28OAC%29%2C%20which%20integrates%20onset%20cues%20that%20serve%20as%20sharp%0Aevent-driven%20markers%20of%20audio-relevant%20visual%20moments%20to%20enhance%0Asynchronization%20with%20dynamic%20visual%20events.%20Extensive%20experiments%20on%20the%0AVGGSound%20and%20Landscape%20datasets%20demonstrate%20that%20TARO%20outperforms%20prior%0Amethods%2C%20achieving%20relatively%2053%25%20lower%20Frechet%20Distance%20%28FD%29%2C%2029%25%20lower%0AFrechet%20Audio%20Distance%20%28FAD%29%2C%20and%20a%2097.19%25%20Alignment%20Accuracy%2C%20highlighting%20its%0Asuperior%20audio%20quality%20and%20synchronization%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05684v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTARO%253A%2520Timestep-Adaptive%2520Representation%2520Alignment%2520with%2520Onset-Aware%250A%2520%2520Conditioning%2520for%2520Synchronized%2520Video-to-Audio%2520Synthesis%26entry.906535625%3DTri%2520Ton%2520and%2520Ji%2520Woo%2520Hong%2520and%2520Chang%2520D.%2520Yoo%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Timestep-Adaptive%2520Representation%2520Alignment%2520with%250AOnset-Aware%2520Conditioning%2520%2528TARO%2529%252C%2520a%2520novel%2520framework%2520for%2520high-fidelity%2520and%250Atemporally%2520coherent%2520video-to-audio%2520synthesis.%2520Built%2520upon%2520flow-based%250Atransformers%252C%2520which%2520offer%2520stable%2520training%2520and%2520continuous%2520transformations%2520for%250Aenhanced%2520synchronization%2520and%2520audio%2520quality%252C%2520TARO%2520introduces%2520two%2520key%250Ainnovations%253A%2520%25281%2529%2520Timestep-Adaptive%2520Representation%2520Alignment%2520%2528TRA%2529%252C%2520which%250Adynamically%2520aligns%2520latent%2520representations%2520by%2520adjusting%2520alignment%2520strength%2520based%250Aon%2520the%2520noise%2520schedule%252C%2520ensuring%2520smooth%2520evolution%2520and%2520improved%2520fidelity%252C%2520and%2520%25282%2529%250AOnset-Aware%2520Conditioning%2520%2528OAC%2529%252C%2520which%2520integrates%2520onset%2520cues%2520that%2520serve%2520as%2520sharp%250Aevent-driven%2520markers%2520of%2520audio-relevant%2520visual%2520moments%2520to%2520enhance%250Asynchronization%2520with%2520dynamic%2520visual%2520events.%2520Extensive%2520experiments%2520on%2520the%250AVGGSound%2520and%2520Landscape%2520datasets%2520demonstrate%2520that%2520TARO%2520outperforms%2520prior%250Amethods%252C%2520achieving%2520relatively%252053%2525%2520lower%2520Frechet%2520Distance%2520%2528FD%2529%252C%252029%2525%2520lower%250AFrechet%2520Audio%2520Distance%2520%2528FAD%2529%252C%2520and%2520a%252097.19%2525%2520Alignment%2520Accuracy%252C%2520highlighting%2520its%250Asuperior%2520audio%2520quality%2520and%2520synchronization%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05684v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TARO%3A%20Timestep-Adaptive%20Representation%20Alignment%20with%20Onset-Aware%0A%20%20Conditioning%20for%20Synchronized%20Video-to-Audio%20Synthesis&entry.906535625=Tri%20Ton%20and%20Ji%20Woo%20Hong%20and%20Chang%20D.%20Yoo&entry.1292438233=%20%20This%20paper%20introduces%20Timestep-Adaptive%20Representation%20Alignment%20with%0AOnset-Aware%20Conditioning%20%28TARO%29%2C%20a%20novel%20framework%20for%20high-fidelity%20and%0Atemporally%20coherent%20video-to-audio%20synthesis.%20Built%20upon%20flow-based%0Atransformers%2C%20which%20offer%20stable%20training%20and%20continuous%20transformations%20for%0Aenhanced%20synchronization%20and%20audio%20quality%2C%20TARO%20introduces%20two%20key%0Ainnovations%3A%20%281%29%20Timestep-Adaptive%20Representation%20Alignment%20%28TRA%29%2C%20which%0Adynamically%20aligns%20latent%20representations%20by%20adjusting%20alignment%20strength%20based%0Aon%20the%20noise%20schedule%2C%20ensuring%20smooth%20evolution%20and%20improved%20fidelity%2C%20and%20%282%29%0AOnset-Aware%20Conditioning%20%28OAC%29%2C%20which%20integrates%20onset%20cues%20that%20serve%20as%20sharp%0Aevent-driven%20markers%20of%20audio-relevant%20visual%20moments%20to%20enhance%0Asynchronization%20with%20dynamic%20visual%20events.%20Extensive%20experiments%20on%20the%0AVGGSound%20and%20Landscape%20datasets%20demonstrate%20that%20TARO%20outperforms%20prior%0Amethods%2C%20achieving%20relatively%2053%25%20lower%20Frechet%20Distance%20%28FD%29%2C%2029%25%20lower%0AFrechet%20Audio%20Distance%20%28FAD%29%2C%20and%20a%2097.19%25%20Alignment%20Accuracy%2C%20highlighting%20its%0Asuperior%20audio%20quality%20and%20synchronization%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05684v3&entry.124074799=Read"},
{"title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs", "author": "Zixin Zhang and Kanghao Chen and Xingwang Lin and Lutao Jiang and Xu Zheng and Yuanhuiyi Lyu and Litao Guo and Yinchuan Li and Ying-Cong Chen", "abstract": "  The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.\n", "link": "http://arxiv.org/abs/2510.09507v1", "date": "2025-10-10", "relevancy": 2.1688, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysToolBench%3A%20Benchmarking%20Physical%20Tool%20Understanding%20for%20MLLMs&body=Title%3A%20PhysToolBench%3A%20Benchmarking%20Physical%20Tool%20Understanding%20for%20MLLMs%0AAuthor%3A%20Zixin%20Zhang%20and%20Kanghao%20Chen%20and%20Xingwang%20Lin%20and%20Lutao%20Jiang%20and%20Xu%20Zheng%20and%20Yuanhuiyi%20Lyu%20and%20Litao%20Guo%20and%20Yinchuan%20Li%20and%20Ying-Cong%20Chen%0AAbstract%3A%20%20%20The%20ability%20to%20use%2C%20understand%2C%20and%20create%20tools%20is%20a%20hallmark%20of%20human%0Aintelligence%2C%20enabling%20sophisticated%20interaction%20with%20the%20physical%20world.%20For%0Aany%20general-purpose%20intelligent%20agent%20to%20achieve%20true%20versatility%2C%20it%20must%20also%0Amaster%20these%20fundamental%20skills.%20While%20modern%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20leverage%20their%20extensive%20common%20knowledge%20for%20high-level%20planning%20in%0Aembodied%20AI%20and%20in%20downstream%20Vision-Language-Action%20%28VLA%29%20models%2C%20the%20extent%0Aof%20their%20true%20understanding%20of%20physical%20tools%20remains%20unquantified.%20To%20bridge%0Athis%20gap%2C%20we%20present%20PhysToolBench%2C%20the%20first%20benchmark%20dedicated%20to%20evaluating%0Athe%20comprehension%20of%20physical%20tools%20by%20MLLMs.%20Our%20benchmark%20is%20structured%20as%20a%0AVisual%20Question%20Answering%20%28VQA%29%20dataset%20comprising%20over%201%2C000%20image-text%20pairs.%0AIt%20assesses%20capabilities%20across%20three%20distinct%20difficulty%20levels%3A%20%281%29%20Tool%0ARecognition%3A%20Requiring%20the%20recognition%20of%20a%20tool%27s%20primary%20function.%20%282%29%20Tool%0AUnderstanding%3A%20Testing%20the%20ability%20to%20grasp%20the%20underlying%20principles%20of%20a%0Atool%27s%20operation.%20%283%29%20Tool%20Creation%3A%20Challenging%20the%20model%20to%20fashion%20a%20new%0Atool%20from%20surrounding%20objects%20when%20conventional%20options%20are%20unavailable.%20Our%0Acomprehensive%20evaluation%20of%2032%20MLLMs-spanning%20proprietary%2C%20open-source%2C%0Aspecialized%20embodied%2C%20and%20backbones%20in%20VLAs-reveals%20a%20significant%20deficiency%20in%0Atool%20understanding.%20Furthermore%2C%20we%20provide%20an%20in-depth%20analysis%20and%20propose%0Apreliminary%20solutions.%20Code%20and%20dataset%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysToolBench%253A%2520Benchmarking%2520Physical%2520Tool%2520Understanding%2520for%2520MLLMs%26entry.906535625%3DZixin%2520Zhang%2520and%2520Kanghao%2520Chen%2520and%2520Xingwang%2520Lin%2520and%2520Lutao%2520Jiang%2520and%2520Xu%2520Zheng%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Litao%2520Guo%2520and%2520Yinchuan%2520Li%2520and%2520Ying-Cong%2520Chen%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520use%252C%2520understand%252C%2520and%2520create%2520tools%2520is%2520a%2520hallmark%2520of%2520human%250Aintelligence%252C%2520enabling%2520sophisticated%2520interaction%2520with%2520the%2520physical%2520world.%2520For%250Aany%2520general-purpose%2520intelligent%2520agent%2520to%2520achieve%2520true%2520versatility%252C%2520it%2520must%2520also%250Amaster%2520these%2520fundamental%2520skills.%2520While%2520modern%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520leverage%2520their%2520extensive%2520common%2520knowledge%2520for%2520high-level%2520planning%2520in%250Aembodied%2520AI%2520and%2520in%2520downstream%2520Vision-Language-Action%2520%2528VLA%2529%2520models%252C%2520the%2520extent%250Aof%2520their%2520true%2520understanding%2520of%2520physical%2520tools%2520remains%2520unquantified.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520present%2520PhysToolBench%252C%2520the%2520first%2520benchmark%2520dedicated%2520to%2520evaluating%250Athe%2520comprehension%2520of%2520physical%2520tools%2520by%2520MLLMs.%2520Our%2520benchmark%2520is%2520structured%2520as%2520a%250AVisual%2520Question%2520Answering%2520%2528VQA%2529%2520dataset%2520comprising%2520over%25201%252C000%2520image-text%2520pairs.%250AIt%2520assesses%2520capabilities%2520across%2520three%2520distinct%2520difficulty%2520levels%253A%2520%25281%2529%2520Tool%250ARecognition%253A%2520Requiring%2520the%2520recognition%2520of%2520a%2520tool%2527s%2520primary%2520function.%2520%25282%2529%2520Tool%250AUnderstanding%253A%2520Testing%2520the%2520ability%2520to%2520grasp%2520the%2520underlying%2520principles%2520of%2520a%250Atool%2527s%2520operation.%2520%25283%2529%2520Tool%2520Creation%253A%2520Challenging%2520the%2520model%2520to%2520fashion%2520a%2520new%250Atool%2520from%2520surrounding%2520objects%2520when%2520conventional%2520options%2520are%2520unavailable.%2520Our%250Acomprehensive%2520evaluation%2520of%252032%2520MLLMs-spanning%2520proprietary%252C%2520open-source%252C%250Aspecialized%2520embodied%252C%2520and%2520backbones%2520in%2520VLAs-reveals%2520a%2520significant%2520deficiency%2520in%250Atool%2520understanding.%2520Furthermore%252C%2520we%2520provide%2520an%2520in-depth%2520analysis%2520and%2520propose%250Apreliminary%2520solutions.%2520Code%2520and%2520dataset%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysToolBench%3A%20Benchmarking%20Physical%20Tool%20Understanding%20for%20MLLMs&entry.906535625=Zixin%20Zhang%20and%20Kanghao%20Chen%20and%20Xingwang%20Lin%20and%20Lutao%20Jiang%20and%20Xu%20Zheng%20and%20Yuanhuiyi%20Lyu%20and%20Litao%20Guo%20and%20Yinchuan%20Li%20and%20Ying-Cong%20Chen&entry.1292438233=%20%20The%20ability%20to%20use%2C%20understand%2C%20and%20create%20tools%20is%20a%20hallmark%20of%20human%0Aintelligence%2C%20enabling%20sophisticated%20interaction%20with%20the%20physical%20world.%20For%0Aany%20general-purpose%20intelligent%20agent%20to%20achieve%20true%20versatility%2C%20it%20must%20also%0Amaster%20these%20fundamental%20skills.%20While%20modern%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20leverage%20their%20extensive%20common%20knowledge%20for%20high-level%20planning%20in%0Aembodied%20AI%20and%20in%20downstream%20Vision-Language-Action%20%28VLA%29%20models%2C%20the%20extent%0Aof%20their%20true%20understanding%20of%20physical%20tools%20remains%20unquantified.%20To%20bridge%0Athis%20gap%2C%20we%20present%20PhysToolBench%2C%20the%20first%20benchmark%20dedicated%20to%20evaluating%0Athe%20comprehension%20of%20physical%20tools%20by%20MLLMs.%20Our%20benchmark%20is%20structured%20as%20a%0AVisual%20Question%20Answering%20%28VQA%29%20dataset%20comprising%20over%201%2C000%20image-text%20pairs.%0AIt%20assesses%20capabilities%20across%20three%20distinct%20difficulty%20levels%3A%20%281%29%20Tool%0ARecognition%3A%20Requiring%20the%20recognition%20of%20a%20tool%27s%20primary%20function.%20%282%29%20Tool%0AUnderstanding%3A%20Testing%20the%20ability%20to%20grasp%20the%20underlying%20principles%20of%20a%0Atool%27s%20operation.%20%283%29%20Tool%20Creation%3A%20Challenging%20the%20model%20to%20fashion%20a%20new%0Atool%20from%20surrounding%20objects%20when%20conventional%20options%20are%20unavailable.%20Our%0Acomprehensive%20evaluation%20of%2032%20MLLMs-spanning%20proprietary%2C%20open-source%2C%0Aspecialized%20embodied%2C%20and%20backbones%20in%20VLAs-reveals%20a%20significant%20deficiency%20in%0Atool%20understanding.%20Furthermore%2C%20we%20provide%20an%20in-depth%20analysis%20and%20propose%0Apreliminary%20solutions.%20Code%20and%20dataset%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09507v1&entry.124074799=Read"},
{"title": "SilvaScenes: Tree Segmentation and Species Classification from\n  Under-Canopy Images in Natural Forests", "author": "David-Alexandre Duclos and William Guimont-Martin and Gabriel Jeanson and Arthur Larochelle-Tremblay and Th\u00e9o Defosse and Fr\u00e9d\u00e9ric Moore and Philippe Nolet and Fran\u00e7ois Pomerleau and Philippe Gigu\u00e8re", "abstract": "  Interest in robotics for forest management is growing, but perception in\ncomplex, natural environments remains a significant hurdle. Conditions such as\nheavy occlusion, variable lighting, and dense vegetation pose challenges to\nautomated systems, which are essential for precision forestry, biodiversity\nmonitoring, and the automation of forestry equipment. These tasks rely on\nadvanced perceptual capabilities, such as detection and fine-grained species\nclassification of individual trees. Yet, existing datasets are inadequate to\ndevelop such perception systems, as they often focus on urban settings or a\nlimited number of species. To address this, we present SilvaScenes, a new\ndataset for instance segmentation of tree species from under-canopy images.\nCollected across five bioclimatic domains in Quebec, Canada, SilvaScenes\nfeatures 1476 trees from 24 species with annotations from forestry experts. We\ndemonstrate the relevance and challenging nature of our dataset by benchmarking\nmodern deep learning approaches for instance segmentation. Our results show\nthat, while tree segmentation is easy, with a top mean average precision (mAP)\nof 67.65%, species classification remains a significant challenge with an mAP\nof only 35.69%. Our dataset and source code will be available at\nhttps://github.com/norlab-ulaval/SilvaScenes.\n", "link": "http://arxiv.org/abs/2510.09458v1", "date": "2025-10-10", "relevancy": 2.1664, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SilvaScenes%3A%20Tree%20Segmentation%20and%20Species%20Classification%20from%0A%20%20Under-Canopy%20Images%20in%20Natural%20Forests&body=Title%3A%20SilvaScenes%3A%20Tree%20Segmentation%20and%20Species%20Classification%20from%0A%20%20Under-Canopy%20Images%20in%20Natural%20Forests%0AAuthor%3A%20David-Alexandre%20Duclos%20and%20William%20Guimont-Martin%20and%20Gabriel%20Jeanson%20and%20Arthur%20Larochelle-Tremblay%20and%20Th%C3%A9o%20Defosse%20and%20Fr%C3%A9d%C3%A9ric%20Moore%20and%20Philippe%20Nolet%20and%20Fran%C3%A7ois%20Pomerleau%20and%20Philippe%20Gigu%C3%A8re%0AAbstract%3A%20%20%20Interest%20in%20robotics%20for%20forest%20management%20is%20growing%2C%20but%20perception%20in%0Acomplex%2C%20natural%20environments%20remains%20a%20significant%20hurdle.%20Conditions%20such%20as%0Aheavy%20occlusion%2C%20variable%20lighting%2C%20and%20dense%20vegetation%20pose%20challenges%20to%0Aautomated%20systems%2C%20which%20are%20essential%20for%20precision%20forestry%2C%20biodiversity%0Amonitoring%2C%20and%20the%20automation%20of%20forestry%20equipment.%20These%20tasks%20rely%20on%0Aadvanced%20perceptual%20capabilities%2C%20such%20as%20detection%20and%20fine-grained%20species%0Aclassification%20of%20individual%20trees.%20Yet%2C%20existing%20datasets%20are%20inadequate%20to%0Adevelop%20such%20perception%20systems%2C%20as%20they%20often%20focus%20on%20urban%20settings%20or%20a%0Alimited%20number%20of%20species.%20To%20address%20this%2C%20we%20present%20SilvaScenes%2C%20a%20new%0Adataset%20for%20instance%20segmentation%20of%20tree%20species%20from%20under-canopy%20images.%0ACollected%20across%20five%20bioclimatic%20domains%20in%20Quebec%2C%20Canada%2C%20SilvaScenes%0Afeatures%201476%20trees%20from%2024%20species%20with%20annotations%20from%20forestry%20experts.%20We%0Ademonstrate%20the%20relevance%20and%20challenging%20nature%20of%20our%20dataset%20by%20benchmarking%0Amodern%20deep%20learning%20approaches%20for%20instance%20segmentation.%20Our%20results%20show%0Athat%2C%20while%20tree%20segmentation%20is%20easy%2C%20with%20a%20top%20mean%20average%20precision%20%28mAP%29%0Aof%2067.65%25%2C%20species%20classification%20remains%20a%20significant%20challenge%20with%20an%20mAP%0Aof%20only%2035.69%25.%20Our%20dataset%20and%20source%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/norlab-ulaval/SilvaScenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSilvaScenes%253A%2520Tree%2520Segmentation%2520and%2520Species%2520Classification%2520from%250A%2520%2520Under-Canopy%2520Images%2520in%2520Natural%2520Forests%26entry.906535625%3DDavid-Alexandre%2520Duclos%2520and%2520William%2520Guimont-Martin%2520and%2520Gabriel%2520Jeanson%2520and%2520Arthur%2520Larochelle-Tremblay%2520and%2520Th%25C3%25A9o%2520Defosse%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Moore%2520and%2520Philippe%2520Nolet%2520and%2520Fran%25C3%25A7ois%2520Pomerleau%2520and%2520Philippe%2520Gigu%25C3%25A8re%26entry.1292438233%3D%2520%2520Interest%2520in%2520robotics%2520for%2520forest%2520management%2520is%2520growing%252C%2520but%2520perception%2520in%250Acomplex%252C%2520natural%2520environments%2520remains%2520a%2520significant%2520hurdle.%2520Conditions%2520such%2520as%250Aheavy%2520occlusion%252C%2520variable%2520lighting%252C%2520and%2520dense%2520vegetation%2520pose%2520challenges%2520to%250Aautomated%2520systems%252C%2520which%2520are%2520essential%2520for%2520precision%2520forestry%252C%2520biodiversity%250Amonitoring%252C%2520and%2520the%2520automation%2520of%2520forestry%2520equipment.%2520These%2520tasks%2520rely%2520on%250Aadvanced%2520perceptual%2520capabilities%252C%2520such%2520as%2520detection%2520and%2520fine-grained%2520species%250Aclassification%2520of%2520individual%2520trees.%2520Yet%252C%2520existing%2520datasets%2520are%2520inadequate%2520to%250Adevelop%2520such%2520perception%2520systems%252C%2520as%2520they%2520often%2520focus%2520on%2520urban%2520settings%2520or%2520a%250Alimited%2520number%2520of%2520species.%2520To%2520address%2520this%252C%2520we%2520present%2520SilvaScenes%252C%2520a%2520new%250Adataset%2520for%2520instance%2520segmentation%2520of%2520tree%2520species%2520from%2520under-canopy%2520images.%250ACollected%2520across%2520five%2520bioclimatic%2520domains%2520in%2520Quebec%252C%2520Canada%252C%2520SilvaScenes%250Afeatures%25201476%2520trees%2520from%252024%2520species%2520with%2520annotations%2520from%2520forestry%2520experts.%2520We%250Ademonstrate%2520the%2520relevance%2520and%2520challenging%2520nature%2520of%2520our%2520dataset%2520by%2520benchmarking%250Amodern%2520deep%2520learning%2520approaches%2520for%2520instance%2520segmentation.%2520Our%2520results%2520show%250Athat%252C%2520while%2520tree%2520segmentation%2520is%2520easy%252C%2520with%2520a%2520top%2520mean%2520average%2520precision%2520%2528mAP%2529%250Aof%252067.65%2525%252C%2520species%2520classification%2520remains%2520a%2520significant%2520challenge%2520with%2520an%2520mAP%250Aof%2520only%252035.69%2525.%2520Our%2520dataset%2520and%2520source%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/norlab-ulaval/SilvaScenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SilvaScenes%3A%20Tree%20Segmentation%20and%20Species%20Classification%20from%0A%20%20Under-Canopy%20Images%20in%20Natural%20Forests&entry.906535625=David-Alexandre%20Duclos%20and%20William%20Guimont-Martin%20and%20Gabriel%20Jeanson%20and%20Arthur%20Larochelle-Tremblay%20and%20Th%C3%A9o%20Defosse%20and%20Fr%C3%A9d%C3%A9ric%20Moore%20and%20Philippe%20Nolet%20and%20Fran%C3%A7ois%20Pomerleau%20and%20Philippe%20Gigu%C3%A8re&entry.1292438233=%20%20Interest%20in%20robotics%20for%20forest%20management%20is%20growing%2C%20but%20perception%20in%0Acomplex%2C%20natural%20environments%20remains%20a%20significant%20hurdle.%20Conditions%20such%20as%0Aheavy%20occlusion%2C%20variable%20lighting%2C%20and%20dense%20vegetation%20pose%20challenges%20to%0Aautomated%20systems%2C%20which%20are%20essential%20for%20precision%20forestry%2C%20biodiversity%0Amonitoring%2C%20and%20the%20automation%20of%20forestry%20equipment.%20These%20tasks%20rely%20on%0Aadvanced%20perceptual%20capabilities%2C%20such%20as%20detection%20and%20fine-grained%20species%0Aclassification%20of%20individual%20trees.%20Yet%2C%20existing%20datasets%20are%20inadequate%20to%0Adevelop%20such%20perception%20systems%2C%20as%20they%20often%20focus%20on%20urban%20settings%20or%20a%0Alimited%20number%20of%20species.%20To%20address%20this%2C%20we%20present%20SilvaScenes%2C%20a%20new%0Adataset%20for%20instance%20segmentation%20of%20tree%20species%20from%20under-canopy%20images.%0ACollected%20across%20five%20bioclimatic%20domains%20in%20Quebec%2C%20Canada%2C%20SilvaScenes%0Afeatures%201476%20trees%20from%2024%20species%20with%20annotations%20from%20forestry%20experts.%20We%0Ademonstrate%20the%20relevance%20and%20challenging%20nature%20of%20our%20dataset%20by%20benchmarking%0Amodern%20deep%20learning%20approaches%20for%20instance%20segmentation.%20Our%20results%20show%0Athat%2C%20while%20tree%20segmentation%20is%20easy%2C%20with%20a%20top%20mean%20average%20precision%20%28mAP%29%0Aof%2067.65%25%2C%20species%20classification%20remains%20a%20significant%20challenge%20with%20an%20mAP%0Aof%20only%2035.69%25.%20Our%20dataset%20and%20source%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/norlab-ulaval/SilvaScenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09458v1&entry.124074799=Read"},
{"title": "Estimating Brain Activity with High Spatial and Temporal Resolution\n  using a Naturalistic MEG-fMRI Encoding Model", "author": "Beige Jerry Jin and Leila Wehbe", "abstract": "  Current non-invasive neuroimaging techniques trade off between spatial\nresolution and temporal resolution. While magnetoencephalography (MEG) can\ncapture rapid neural dynamics and functional magnetic resonance imaging (fMRI)\ncan spatially localize brain activity, a unified picture that preserves both\nhigh resolutions remains an unsolved challenge with existing source\nlocalization or MEG-fMRI fusion methods, especially for single-trial\nnaturalistic data. We collected whole-head MEG when subjects listened passively\nto more than seven hours of narrative stories, using the same stimuli in an\nopen fMRI dataset (LeBel et al., 2023). We developed a transformer-based\nencoding model that combines the MEG and fMRI from these two naturalistic\nspeech comprehension experiments to estimate latent cortical source responses\nwith high spatiotemporal resolution. Our model is trained to predict MEG and\nfMRI from multiple subjects simultaneously, with a latent layer that represents\nour estimates of reconstructed cortical sources. Our model predicts MEG better\nthan the common standard of single-modality encoding models, and it also yields\nsource estimates with higher spatial and temporal fidelity than classic\nminimum-norm solutions in simulation experiments. We validated the estimated\nlatent sources by showing its strong generalizability across unseen subjects\nand modalities. Estimated activity in our source space predict\nelectrocorticography (ECoG) better than an ECoG-trained encoding model in an\nentirely new dataset. By integrating the power of large naturalistic\nexperiments, MEG, fMRI, and encoding models, we propose a practical route\ntowards millisecond-and-millimeter brain mapping.\n", "link": "http://arxiv.org/abs/2510.09415v1", "date": "2025-10-10", "relevancy": 2.1645, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Brain%20Activity%20with%20High%20Spatial%20and%20Temporal%20Resolution%0A%20%20using%20a%20Naturalistic%20MEG-fMRI%20Encoding%20Model&body=Title%3A%20Estimating%20Brain%20Activity%20with%20High%20Spatial%20and%20Temporal%20Resolution%0A%20%20using%20a%20Naturalistic%20MEG-fMRI%20Encoding%20Model%0AAuthor%3A%20Beige%20Jerry%20Jin%20and%20Leila%20Wehbe%0AAbstract%3A%20%20%20Current%20non-invasive%20neuroimaging%20techniques%20trade%20off%20between%20spatial%0Aresolution%20and%20temporal%20resolution.%20While%20magnetoencephalography%20%28MEG%29%20can%0Acapture%20rapid%20neural%20dynamics%20and%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%0Acan%20spatially%20localize%20brain%20activity%2C%20a%20unified%20picture%20that%20preserves%20both%0Ahigh%20resolutions%20remains%20an%20unsolved%20challenge%20with%20existing%20source%0Alocalization%20or%20MEG-fMRI%20fusion%20methods%2C%20especially%20for%20single-trial%0Anaturalistic%20data.%20We%20collected%20whole-head%20MEG%20when%20subjects%20listened%20passively%0Ato%20more%20than%20seven%20hours%20of%20narrative%20stories%2C%20using%20the%20same%20stimuli%20in%20an%0Aopen%20fMRI%20dataset%20%28LeBel%20et%20al.%2C%202023%29.%20We%20developed%20a%20transformer-based%0Aencoding%20model%20that%20combines%20the%20MEG%20and%20fMRI%20from%20these%20two%20naturalistic%0Aspeech%20comprehension%20experiments%20to%20estimate%20latent%20cortical%20source%20responses%0Awith%20high%20spatiotemporal%20resolution.%20Our%20model%20is%20trained%20to%20predict%20MEG%20and%0AfMRI%20from%20multiple%20subjects%20simultaneously%2C%20with%20a%20latent%20layer%20that%20represents%0Aour%20estimates%20of%20reconstructed%20cortical%20sources.%20Our%20model%20predicts%20MEG%20better%0Athan%20the%20common%20standard%20of%20single-modality%20encoding%20models%2C%20and%20it%20also%20yields%0Asource%20estimates%20with%20higher%20spatial%20and%20temporal%20fidelity%20than%20classic%0Aminimum-norm%20solutions%20in%20simulation%20experiments.%20We%20validated%20the%20estimated%0Alatent%20sources%20by%20showing%20its%20strong%20generalizability%20across%20unseen%20subjects%0Aand%20modalities.%20Estimated%20activity%20in%20our%20source%20space%20predict%0Aelectrocorticography%20%28ECoG%29%20better%20than%20an%20ECoG-trained%20encoding%20model%20in%20an%0Aentirely%20new%20dataset.%20By%20integrating%20the%20power%20of%20large%20naturalistic%0Aexperiments%2C%20MEG%2C%20fMRI%2C%20and%20encoding%20models%2C%20we%20propose%20a%20practical%20route%0Atowards%20millisecond-and-millimeter%20brain%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Brain%2520Activity%2520with%2520High%2520Spatial%2520and%2520Temporal%2520Resolution%250A%2520%2520using%2520a%2520Naturalistic%2520MEG-fMRI%2520Encoding%2520Model%26entry.906535625%3DBeige%2520Jerry%2520Jin%2520and%2520Leila%2520Wehbe%26entry.1292438233%3D%2520%2520Current%2520non-invasive%2520neuroimaging%2520techniques%2520trade%2520off%2520between%2520spatial%250Aresolution%2520and%2520temporal%2520resolution.%2520While%2520magnetoencephalography%2520%2528MEG%2529%2520can%250Acapture%2520rapid%2520neural%2520dynamics%2520and%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%250Acan%2520spatially%2520localize%2520brain%2520activity%252C%2520a%2520unified%2520picture%2520that%2520preserves%2520both%250Ahigh%2520resolutions%2520remains%2520an%2520unsolved%2520challenge%2520with%2520existing%2520source%250Alocalization%2520or%2520MEG-fMRI%2520fusion%2520methods%252C%2520especially%2520for%2520single-trial%250Anaturalistic%2520data.%2520We%2520collected%2520whole-head%2520MEG%2520when%2520subjects%2520listened%2520passively%250Ato%2520more%2520than%2520seven%2520hours%2520of%2520narrative%2520stories%252C%2520using%2520the%2520same%2520stimuli%2520in%2520an%250Aopen%2520fMRI%2520dataset%2520%2528LeBel%2520et%2520al.%252C%25202023%2529.%2520We%2520developed%2520a%2520transformer-based%250Aencoding%2520model%2520that%2520combines%2520the%2520MEG%2520and%2520fMRI%2520from%2520these%2520two%2520naturalistic%250Aspeech%2520comprehension%2520experiments%2520to%2520estimate%2520latent%2520cortical%2520source%2520responses%250Awith%2520high%2520spatiotemporal%2520resolution.%2520Our%2520model%2520is%2520trained%2520to%2520predict%2520MEG%2520and%250AfMRI%2520from%2520multiple%2520subjects%2520simultaneously%252C%2520with%2520a%2520latent%2520layer%2520that%2520represents%250Aour%2520estimates%2520of%2520reconstructed%2520cortical%2520sources.%2520Our%2520model%2520predicts%2520MEG%2520better%250Athan%2520the%2520common%2520standard%2520of%2520single-modality%2520encoding%2520models%252C%2520and%2520it%2520also%2520yields%250Asource%2520estimates%2520with%2520higher%2520spatial%2520and%2520temporal%2520fidelity%2520than%2520classic%250Aminimum-norm%2520solutions%2520in%2520simulation%2520experiments.%2520We%2520validated%2520the%2520estimated%250Alatent%2520sources%2520by%2520showing%2520its%2520strong%2520generalizability%2520across%2520unseen%2520subjects%250Aand%2520modalities.%2520Estimated%2520activity%2520in%2520our%2520source%2520space%2520predict%250Aelectrocorticography%2520%2528ECoG%2529%2520better%2520than%2520an%2520ECoG-trained%2520encoding%2520model%2520in%2520an%250Aentirely%2520new%2520dataset.%2520By%2520integrating%2520the%2520power%2520of%2520large%2520naturalistic%250Aexperiments%252C%2520MEG%252C%2520fMRI%252C%2520and%2520encoding%2520models%252C%2520we%2520propose%2520a%2520practical%2520route%250Atowards%2520millisecond-and-millimeter%2520brain%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Brain%20Activity%20with%20High%20Spatial%20and%20Temporal%20Resolution%0A%20%20using%20a%20Naturalistic%20MEG-fMRI%20Encoding%20Model&entry.906535625=Beige%20Jerry%20Jin%20and%20Leila%20Wehbe&entry.1292438233=%20%20Current%20non-invasive%20neuroimaging%20techniques%20trade%20off%20between%20spatial%0Aresolution%20and%20temporal%20resolution.%20While%20magnetoencephalography%20%28MEG%29%20can%0Acapture%20rapid%20neural%20dynamics%20and%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%0Acan%20spatially%20localize%20brain%20activity%2C%20a%20unified%20picture%20that%20preserves%20both%0Ahigh%20resolutions%20remains%20an%20unsolved%20challenge%20with%20existing%20source%0Alocalization%20or%20MEG-fMRI%20fusion%20methods%2C%20especially%20for%20single-trial%0Anaturalistic%20data.%20We%20collected%20whole-head%20MEG%20when%20subjects%20listened%20passively%0Ato%20more%20than%20seven%20hours%20of%20narrative%20stories%2C%20using%20the%20same%20stimuli%20in%20an%0Aopen%20fMRI%20dataset%20%28LeBel%20et%20al.%2C%202023%29.%20We%20developed%20a%20transformer-based%0Aencoding%20model%20that%20combines%20the%20MEG%20and%20fMRI%20from%20these%20two%20naturalistic%0Aspeech%20comprehension%20experiments%20to%20estimate%20latent%20cortical%20source%20responses%0Awith%20high%20spatiotemporal%20resolution.%20Our%20model%20is%20trained%20to%20predict%20MEG%20and%0AfMRI%20from%20multiple%20subjects%20simultaneously%2C%20with%20a%20latent%20layer%20that%20represents%0Aour%20estimates%20of%20reconstructed%20cortical%20sources.%20Our%20model%20predicts%20MEG%20better%0Athan%20the%20common%20standard%20of%20single-modality%20encoding%20models%2C%20and%20it%20also%20yields%0Asource%20estimates%20with%20higher%20spatial%20and%20temporal%20fidelity%20than%20classic%0Aminimum-norm%20solutions%20in%20simulation%20experiments.%20We%20validated%20the%20estimated%0Alatent%20sources%20by%20showing%20its%20strong%20generalizability%20across%20unseen%20subjects%0Aand%20modalities.%20Estimated%20activity%20in%20our%20source%20space%20predict%0Aelectrocorticography%20%28ECoG%29%20better%20than%20an%20ECoG-trained%20encoding%20model%20in%20an%0Aentirely%20new%20dataset.%20By%20integrating%20the%20power%20of%20large%20naturalistic%0Aexperiments%2C%20MEG%2C%20fMRI%2C%20and%20encoding%20models%2C%20we%20propose%20a%20practical%20route%0Atowards%20millisecond-and-millimeter%20brain%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09415v1&entry.124074799=Read"},
{"title": "On Developers' Self-Declaration of AI-Generated Code: An Analysis of\n  Practices", "author": "Syed Mohammad Kashif and Peng Liang and Amjed Tahir", "abstract": "  AI code generation tools have gained significant popularity among developers,\nwho use them to assist in software development due to their capability to\ngenerate code. Existing studies mainly explored the quality, e.g., correctness\nand security, of AI-generated code, while in real-world software development,\nthe prerequisite is to distinguish AI-generated code from human-written code,\nwhich emphasizes the need to explicitly declare AI-generated code by\ndevelopers. To this end, this study intends to understand the ways developers\nuse to self-declare AI-generated code and explore the reasons why developers\nchoose to self-declare or not. We conducted a mixed-methods study consisting of\ntwo phases. In the first phase, we mined GitHub repositories and collected 613\ninstances of AI-generated code snippets. In the second phase, we conducted a\nfollow-up practitioners' survey, which received 111 valid responses. Our\nresearch revealed the practices followed by developers to self-declare\nAI-generated code. Most practitioners (76.6%) always or sometimes self-declare\nAI-generated code. In contrast, other practitioners (23.4%) noted that they\nnever self-declare AI-generated code. The reasons for self-declaring\nAI-generated code include the need to track and monitor the code for future\nreview and debugging, and ethical considerations. The reasons for not\nself-declaring AI-generated code include extensive modifications to\nAI-generated code and the developers' perception that self-declaration is an\nunnecessary activity. We finally provided guidelines for practitioners to\nself-declare AI-generated code, addressing ethical and code quality concerns.\n", "link": "http://arxiv.org/abs/2504.16485v3", "date": "2025-10-10", "relevancy": 2.1558, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4606}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4212}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Developers%27%20Self-Declaration%20of%20AI-Generated%20Code%3A%20An%20Analysis%20of%0A%20%20Practices&body=Title%3A%20On%20Developers%27%20Self-Declaration%20of%20AI-Generated%20Code%3A%20An%20Analysis%20of%0A%20%20Practices%0AAuthor%3A%20Syed%20Mohammad%20Kashif%20and%20Peng%20Liang%20and%20Amjed%20Tahir%0AAbstract%3A%20%20%20AI%20code%20generation%20tools%20have%20gained%20significant%20popularity%20among%20developers%2C%0Awho%20use%20them%20to%20assist%20in%20software%20development%20due%20to%20their%20capability%20to%0Agenerate%20code.%20Existing%20studies%20mainly%20explored%20the%20quality%2C%20e.g.%2C%20correctness%0Aand%20security%2C%20of%20AI-generated%20code%2C%20while%20in%20real-world%20software%20development%2C%0Athe%20prerequisite%20is%20to%20distinguish%20AI-generated%20code%20from%20human-written%20code%2C%0Awhich%20emphasizes%20the%20need%20to%20explicitly%20declare%20AI-generated%20code%20by%0Adevelopers.%20To%20this%20end%2C%20this%20study%20intends%20to%20understand%20the%20ways%20developers%0Ause%20to%20self-declare%20AI-generated%20code%20and%20explore%20the%20reasons%20why%20developers%0Achoose%20to%20self-declare%20or%20not.%20We%20conducted%20a%20mixed-methods%20study%20consisting%20of%0Atwo%20phases.%20In%20the%20first%20phase%2C%20we%20mined%20GitHub%20repositories%20and%20collected%20613%0Ainstances%20of%20AI-generated%20code%20snippets.%20In%20the%20second%20phase%2C%20we%20conducted%20a%0Afollow-up%20practitioners%27%20survey%2C%20which%20received%20111%20valid%20responses.%20Our%0Aresearch%20revealed%20the%20practices%20followed%20by%20developers%20to%20self-declare%0AAI-generated%20code.%20Most%20practitioners%20%2876.6%25%29%20always%20or%20sometimes%20self-declare%0AAI-generated%20code.%20In%20contrast%2C%20other%20practitioners%20%2823.4%25%29%20noted%20that%20they%0Anever%20self-declare%20AI-generated%20code.%20The%20reasons%20for%20self-declaring%0AAI-generated%20code%20include%20the%20need%20to%20track%20and%20monitor%20the%20code%20for%20future%0Areview%20and%20debugging%2C%20and%20ethical%20considerations.%20The%20reasons%20for%20not%0Aself-declaring%20AI-generated%20code%20include%20extensive%20modifications%20to%0AAI-generated%20code%20and%20the%20developers%27%20perception%20that%20self-declaration%20is%20an%0Aunnecessary%20activity.%20We%20finally%20provided%20guidelines%20for%20practitioners%20to%0Aself-declare%20AI-generated%20code%2C%20addressing%20ethical%20and%20code%20quality%20concerns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Developers%2527%2520Self-Declaration%2520of%2520AI-Generated%2520Code%253A%2520An%2520Analysis%2520of%250A%2520%2520Practices%26entry.906535625%3DSyed%2520Mohammad%2520Kashif%2520and%2520Peng%2520Liang%2520and%2520Amjed%2520Tahir%26entry.1292438233%3D%2520%2520AI%2520code%2520generation%2520tools%2520have%2520gained%2520significant%2520popularity%2520among%2520developers%252C%250Awho%2520use%2520them%2520to%2520assist%2520in%2520software%2520development%2520due%2520to%2520their%2520capability%2520to%250Agenerate%2520code.%2520Existing%2520studies%2520mainly%2520explored%2520the%2520quality%252C%2520e.g.%252C%2520correctness%250Aand%2520security%252C%2520of%2520AI-generated%2520code%252C%2520while%2520in%2520real-world%2520software%2520development%252C%250Athe%2520prerequisite%2520is%2520to%2520distinguish%2520AI-generated%2520code%2520from%2520human-written%2520code%252C%250Awhich%2520emphasizes%2520the%2520need%2520to%2520explicitly%2520declare%2520AI-generated%2520code%2520by%250Adevelopers.%2520To%2520this%2520end%252C%2520this%2520study%2520intends%2520to%2520understand%2520the%2520ways%2520developers%250Ause%2520to%2520self-declare%2520AI-generated%2520code%2520and%2520explore%2520the%2520reasons%2520why%2520developers%250Achoose%2520to%2520self-declare%2520or%2520not.%2520We%2520conducted%2520a%2520mixed-methods%2520study%2520consisting%2520of%250Atwo%2520phases.%2520In%2520the%2520first%2520phase%252C%2520we%2520mined%2520GitHub%2520repositories%2520and%2520collected%2520613%250Ainstances%2520of%2520AI-generated%2520code%2520snippets.%2520In%2520the%2520second%2520phase%252C%2520we%2520conducted%2520a%250Afollow-up%2520practitioners%2527%2520survey%252C%2520which%2520received%2520111%2520valid%2520responses.%2520Our%250Aresearch%2520revealed%2520the%2520practices%2520followed%2520by%2520developers%2520to%2520self-declare%250AAI-generated%2520code.%2520Most%2520practitioners%2520%252876.6%2525%2529%2520always%2520or%2520sometimes%2520self-declare%250AAI-generated%2520code.%2520In%2520contrast%252C%2520other%2520practitioners%2520%252823.4%2525%2529%2520noted%2520that%2520they%250Anever%2520self-declare%2520AI-generated%2520code.%2520The%2520reasons%2520for%2520self-declaring%250AAI-generated%2520code%2520include%2520the%2520need%2520to%2520track%2520and%2520monitor%2520the%2520code%2520for%2520future%250Areview%2520and%2520debugging%252C%2520and%2520ethical%2520considerations.%2520The%2520reasons%2520for%2520not%250Aself-declaring%2520AI-generated%2520code%2520include%2520extensive%2520modifications%2520to%250AAI-generated%2520code%2520and%2520the%2520developers%2527%2520perception%2520that%2520self-declaration%2520is%2520an%250Aunnecessary%2520activity.%2520We%2520finally%2520provided%2520guidelines%2520for%2520practitioners%2520to%250Aself-declare%2520AI-generated%2520code%252C%2520addressing%2520ethical%2520and%2520code%2520quality%2520concerns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Developers%27%20Self-Declaration%20of%20AI-Generated%20Code%3A%20An%20Analysis%20of%0A%20%20Practices&entry.906535625=Syed%20Mohammad%20Kashif%20and%20Peng%20Liang%20and%20Amjed%20Tahir&entry.1292438233=%20%20AI%20code%20generation%20tools%20have%20gained%20significant%20popularity%20among%20developers%2C%0Awho%20use%20them%20to%20assist%20in%20software%20development%20due%20to%20their%20capability%20to%0Agenerate%20code.%20Existing%20studies%20mainly%20explored%20the%20quality%2C%20e.g.%2C%20correctness%0Aand%20security%2C%20of%20AI-generated%20code%2C%20while%20in%20real-world%20software%20development%2C%0Athe%20prerequisite%20is%20to%20distinguish%20AI-generated%20code%20from%20human-written%20code%2C%0Awhich%20emphasizes%20the%20need%20to%20explicitly%20declare%20AI-generated%20code%20by%0Adevelopers.%20To%20this%20end%2C%20this%20study%20intends%20to%20understand%20the%20ways%20developers%0Ause%20to%20self-declare%20AI-generated%20code%20and%20explore%20the%20reasons%20why%20developers%0Achoose%20to%20self-declare%20or%20not.%20We%20conducted%20a%20mixed-methods%20study%20consisting%20of%0Atwo%20phases.%20In%20the%20first%20phase%2C%20we%20mined%20GitHub%20repositories%20and%20collected%20613%0Ainstances%20of%20AI-generated%20code%20snippets.%20In%20the%20second%20phase%2C%20we%20conducted%20a%0Afollow-up%20practitioners%27%20survey%2C%20which%20received%20111%20valid%20responses.%20Our%0Aresearch%20revealed%20the%20practices%20followed%20by%20developers%20to%20self-declare%0AAI-generated%20code.%20Most%20practitioners%20%2876.6%25%29%20always%20or%20sometimes%20self-declare%0AAI-generated%20code.%20In%20contrast%2C%20other%20practitioners%20%2823.4%25%29%20noted%20that%20they%0Anever%20self-declare%20AI-generated%20code.%20The%20reasons%20for%20self-declaring%0AAI-generated%20code%20include%20the%20need%20to%20track%20and%20monitor%20the%20code%20for%20future%0Areview%20and%20debugging%2C%20and%20ethical%20considerations.%20The%20reasons%20for%20not%0Aself-declaring%20AI-generated%20code%20include%20extensive%20modifications%20to%0AAI-generated%20code%20and%20the%20developers%27%20perception%20that%20self-declaration%20is%20an%0Aunnecessary%20activity.%20We%20finally%20provided%20guidelines%20for%20practitioners%20to%0Aself-declare%20AI-generated%20code%2C%20addressing%20ethical%20and%20code%20quality%20concerns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16485v3&entry.124074799=Read"},
{"title": "Minkowski-MambaNet: A Point Cloud Framework with Selective State Space\n  Models for Forest Biomass Quantification", "author": "Jinxiang Tu and Dayong Ren and Fei Shi and Zhenhong Jia and Yahong Ren and Jiwei Qin and Fang He", "abstract": "  Accurate forest biomass quantification is vital for carbon cycle monitoring.\nWhile airborne LiDAR excels at capturing 3D forest structure, directly\nestimating woody volume and Aboveground Biomass (AGB) from point clouds is\nchallenging due to difficulties in modeling long-range dependencies needed to\ndistinguish trees.We propose Minkowski-MambaNet, a novel deep learning\nframework that directly estimates volume and AGB from raw LiDAR. Its key\ninnovation is integrating the Mamba model's Selective State Space Model (SSM)\ninto a Minkowski network, enabling effective encoding of global context and\nlong-range dependencies for improved tree differentiation. Skip connections are\nincorporated to enhance features and accelerate convergence.Evaluated on Danish\nNational Forest Inventory LiDAR data, Minkowski-MambaNet significantly\noutperforms state-of-the-art methods, providing more accurate and robust\nestimates. Crucially, it requires no Digital Terrain Model (DTM) and is robust\nto boundary artifacts. This work offers a powerful tool for large-scale forest\nbiomass analysis, advancing LiDAR-based forest inventories.\n", "link": "http://arxiv.org/abs/2510.09367v1", "date": "2025-10-10", "relevancy": 2.132, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5461}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minkowski-MambaNet%3A%20A%20Point%20Cloud%20Framework%20with%20Selective%20State%20Space%0A%20%20Models%20for%20Forest%20Biomass%20Quantification&body=Title%3A%20Minkowski-MambaNet%3A%20A%20Point%20Cloud%20Framework%20with%20Selective%20State%20Space%0A%20%20Models%20for%20Forest%20Biomass%20Quantification%0AAuthor%3A%20Jinxiang%20Tu%20and%20Dayong%20Ren%20and%20Fei%20Shi%20and%20Zhenhong%20Jia%20and%20Yahong%20Ren%20and%20Jiwei%20Qin%20and%20Fang%20He%0AAbstract%3A%20%20%20Accurate%20forest%20biomass%20quantification%20is%20vital%20for%20carbon%20cycle%20monitoring.%0AWhile%20airborne%20LiDAR%20excels%20at%20capturing%203D%20forest%20structure%2C%20directly%0Aestimating%20woody%20volume%20and%20Aboveground%20Biomass%20%28AGB%29%20from%20point%20clouds%20is%0Achallenging%20due%20to%20difficulties%20in%20modeling%20long-range%20dependencies%20needed%20to%0Adistinguish%20trees.We%20propose%20Minkowski-MambaNet%2C%20a%20novel%20deep%20learning%0Aframework%20that%20directly%20estimates%20volume%20and%20AGB%20from%20raw%20LiDAR.%20Its%20key%0Ainnovation%20is%20integrating%20the%20Mamba%20model%27s%20Selective%20State%20Space%20Model%20%28SSM%29%0Ainto%20a%20Minkowski%20network%2C%20enabling%20effective%20encoding%20of%20global%20context%20and%0Along-range%20dependencies%20for%20improved%20tree%20differentiation.%20Skip%20connections%20are%0Aincorporated%20to%20enhance%20features%20and%20accelerate%20convergence.Evaluated%20on%20Danish%0ANational%20Forest%20Inventory%20LiDAR%20data%2C%20Minkowski-MambaNet%20significantly%0Aoutperforms%20state-of-the-art%20methods%2C%20providing%20more%20accurate%20and%20robust%0Aestimates.%20Crucially%2C%20it%20requires%20no%20Digital%20Terrain%20Model%20%28DTM%29%20and%20is%20robust%0Ato%20boundary%20artifacts.%20This%20work%20offers%20a%20powerful%20tool%20for%20large-scale%20forest%0Abiomass%20analysis%2C%20advancing%20LiDAR-based%20forest%20inventories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinkowski-MambaNet%253A%2520A%2520Point%2520Cloud%2520Framework%2520with%2520Selective%2520State%2520Space%250A%2520%2520Models%2520for%2520Forest%2520Biomass%2520Quantification%26entry.906535625%3DJinxiang%2520Tu%2520and%2520Dayong%2520Ren%2520and%2520Fei%2520Shi%2520and%2520Zhenhong%2520Jia%2520and%2520Yahong%2520Ren%2520and%2520Jiwei%2520Qin%2520and%2520Fang%2520He%26entry.1292438233%3D%2520%2520Accurate%2520forest%2520biomass%2520quantification%2520is%2520vital%2520for%2520carbon%2520cycle%2520monitoring.%250AWhile%2520airborne%2520LiDAR%2520excels%2520at%2520capturing%25203D%2520forest%2520structure%252C%2520directly%250Aestimating%2520woody%2520volume%2520and%2520Aboveground%2520Biomass%2520%2528AGB%2529%2520from%2520point%2520clouds%2520is%250Achallenging%2520due%2520to%2520difficulties%2520in%2520modeling%2520long-range%2520dependencies%2520needed%2520to%250Adistinguish%2520trees.We%2520propose%2520Minkowski-MambaNet%252C%2520a%2520novel%2520deep%2520learning%250Aframework%2520that%2520directly%2520estimates%2520volume%2520and%2520AGB%2520from%2520raw%2520LiDAR.%2520Its%2520key%250Ainnovation%2520is%2520integrating%2520the%2520Mamba%2520model%2527s%2520Selective%2520State%2520Space%2520Model%2520%2528SSM%2529%250Ainto%2520a%2520Minkowski%2520network%252C%2520enabling%2520effective%2520encoding%2520of%2520global%2520context%2520and%250Along-range%2520dependencies%2520for%2520improved%2520tree%2520differentiation.%2520Skip%2520connections%2520are%250Aincorporated%2520to%2520enhance%2520features%2520and%2520accelerate%2520convergence.Evaluated%2520on%2520Danish%250ANational%2520Forest%2520Inventory%2520LiDAR%2520data%252C%2520Minkowski-MambaNet%2520significantly%250Aoutperforms%2520state-of-the-art%2520methods%252C%2520providing%2520more%2520accurate%2520and%2520robust%250Aestimates.%2520Crucially%252C%2520it%2520requires%2520no%2520Digital%2520Terrain%2520Model%2520%2528DTM%2529%2520and%2520is%2520robust%250Ato%2520boundary%2520artifacts.%2520This%2520work%2520offers%2520a%2520powerful%2520tool%2520for%2520large-scale%2520forest%250Abiomass%2520analysis%252C%2520advancing%2520LiDAR-based%2520forest%2520inventories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minkowski-MambaNet%3A%20A%20Point%20Cloud%20Framework%20with%20Selective%20State%20Space%0A%20%20Models%20for%20Forest%20Biomass%20Quantification&entry.906535625=Jinxiang%20Tu%20and%20Dayong%20Ren%20and%20Fei%20Shi%20and%20Zhenhong%20Jia%20and%20Yahong%20Ren%20and%20Jiwei%20Qin%20and%20Fang%20He&entry.1292438233=%20%20Accurate%20forest%20biomass%20quantification%20is%20vital%20for%20carbon%20cycle%20monitoring.%0AWhile%20airborne%20LiDAR%20excels%20at%20capturing%203D%20forest%20structure%2C%20directly%0Aestimating%20woody%20volume%20and%20Aboveground%20Biomass%20%28AGB%29%20from%20point%20clouds%20is%0Achallenging%20due%20to%20difficulties%20in%20modeling%20long-range%20dependencies%20needed%20to%0Adistinguish%20trees.We%20propose%20Minkowski-MambaNet%2C%20a%20novel%20deep%20learning%0Aframework%20that%20directly%20estimates%20volume%20and%20AGB%20from%20raw%20LiDAR.%20Its%20key%0Ainnovation%20is%20integrating%20the%20Mamba%20model%27s%20Selective%20State%20Space%20Model%20%28SSM%29%0Ainto%20a%20Minkowski%20network%2C%20enabling%20effective%20encoding%20of%20global%20context%20and%0Along-range%20dependencies%20for%20improved%20tree%20differentiation.%20Skip%20connections%20are%0Aincorporated%20to%20enhance%20features%20and%20accelerate%20convergence.Evaluated%20on%20Danish%0ANational%20Forest%20Inventory%20LiDAR%20data%2C%20Minkowski-MambaNet%20significantly%0Aoutperforms%20state-of-the-art%20methods%2C%20providing%20more%20accurate%20and%20robust%0Aestimates.%20Crucially%2C%20it%20requires%20no%20Digital%20Terrain%20Model%20%28DTM%29%20and%20is%20robust%0Ato%20boundary%20artifacts.%20This%20work%20offers%20a%20powerful%20tool%20for%20large-scale%20forest%0Abiomass%20analysis%2C%20advancing%20LiDAR-based%20forest%20inventories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09367v1&entry.124074799=Read"},
{"title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs", "author": "Wenrui Zhou and Mohamed Hendy and Shu Yang and Qingsong Yang and Zikun Guo and Yuyu Luo and Lijie Hu and Di Wang", "abstract": "  As video large language models (Video-LLMs) become increasingly integrated\ninto real-world applications that demand grounded multimodal reasoning,\nensuring their factual consistency and reliability is of critical importance.\nHowever, sycophancy, the tendency of these models to align with user input even\nwhen it contradicts the visual evidence, undermines their trustworthiness in\nsuch contexts. Current sycophancy research has largely overlooked its specific\nmanifestations in the video-language domain, resulting in a notable absence of\nsystematic benchmarks and targeted evaluations to understand how Video-LLMs\nrespond under misleading user input. To fill this gap, we propose VISE\n(Video-LLM Sycophancy Benchmarking and Evaluation), the first benchmark\ndesigned to evaluate sycophantic behavior in state-of-the-art Video-LLMs across\ndiverse question formats, prompt biases, and visual reasoning tasks.\nSpecifically, VISE pioneeringly brings linguistic perspectives on sycophancy\ninto the video domain, enabling fine-grained analysis across multiple\nsycophancy types and interaction patterns. Furthermore, we propose two\npotential training-free mitigation strategies, revealing potential paths for\nreducing sycophantic bias: (i) enhancing visual grounding through interpretable\nkey-frame selection and (ii) steering model behavior away from sycophancy via\ntargeted, inference-time intervention on its internal neural representations.\nOur code is available at https://github.com/William030422/Video-Sycophancy.\n", "link": "http://arxiv.org/abs/2506.07180v2", "date": "2025-10-10", "relevancy": 2.1251, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5347}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flattery%20in%20Motion%3A%20Benchmarking%20and%20Analyzing%20Sycophancy%20in%20Video-LLMs&body=Title%3A%20Flattery%20in%20Motion%3A%20Benchmarking%20and%20Analyzing%20Sycophancy%20in%20Video-LLMs%0AAuthor%3A%20Wenrui%20Zhou%20and%20Mohamed%20Hendy%20and%20Shu%20Yang%20and%20Qingsong%20Yang%20and%20Zikun%20Guo%20and%20Yuyu%20Luo%20and%20Lijie%20Hu%20and%20Di%20Wang%0AAbstract%3A%20%20%20As%20video%20large%20language%20models%20%28Video-LLMs%29%20become%20increasingly%20integrated%0Ainto%20real-world%20applications%20that%20demand%20grounded%20multimodal%20reasoning%2C%0Aensuring%20their%20factual%20consistency%20and%20reliability%20is%20of%20critical%20importance.%0AHowever%2C%20sycophancy%2C%20the%20tendency%20of%20these%20models%20to%20align%20with%20user%20input%20even%0Awhen%20it%20contradicts%20the%20visual%20evidence%2C%20undermines%20their%20trustworthiness%20in%0Asuch%20contexts.%20Current%20sycophancy%20research%20has%20largely%20overlooked%20its%20specific%0Amanifestations%20in%20the%20video-language%20domain%2C%20resulting%20in%20a%20notable%20absence%20of%0Asystematic%20benchmarks%20and%20targeted%20evaluations%20to%20understand%20how%20Video-LLMs%0Arespond%20under%20misleading%20user%20input.%20To%20fill%20this%20gap%2C%20we%20propose%20VISE%0A%28Video-LLM%20Sycophancy%20Benchmarking%20and%20Evaluation%29%2C%20the%20first%20benchmark%0Adesigned%20to%20evaluate%20sycophantic%20behavior%20in%20state-of-the-art%20Video-LLMs%20across%0Adiverse%20question%20formats%2C%20prompt%20biases%2C%20and%20visual%20reasoning%20tasks.%0ASpecifically%2C%20VISE%20pioneeringly%20brings%20linguistic%20perspectives%20on%20sycophancy%0Ainto%20the%20video%20domain%2C%20enabling%20fine-grained%20analysis%20across%20multiple%0Asycophancy%20types%20and%20interaction%20patterns.%20Furthermore%2C%20we%20propose%20two%0Apotential%20training-free%20mitigation%20strategies%2C%20revealing%20potential%20paths%20for%0Areducing%20sycophantic%20bias%3A%20%28i%29%20enhancing%20visual%20grounding%20through%20interpretable%0Akey-frame%20selection%20and%20%28ii%29%20steering%20model%20behavior%20away%20from%20sycophancy%20via%0Atargeted%2C%20inference-time%20intervention%20on%20its%20internal%20neural%20representations.%0AOur%20code%20is%20available%20at%20https%3A//github.com/William030422/Video-Sycophancy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlattery%2520in%2520Motion%253A%2520Benchmarking%2520and%2520Analyzing%2520Sycophancy%2520in%2520Video-LLMs%26entry.906535625%3DWenrui%2520Zhou%2520and%2520Mohamed%2520Hendy%2520and%2520Shu%2520Yang%2520and%2520Qingsong%2520Yang%2520and%2520Zikun%2520Guo%2520and%2520Yuyu%2520Luo%2520and%2520Lijie%2520Hu%2520and%2520Di%2520Wang%26entry.1292438233%3D%2520%2520As%2520video%2520large%2520language%2520models%2520%2528Video-LLMs%2529%2520become%2520increasingly%2520integrated%250Ainto%2520real-world%2520applications%2520that%2520demand%2520grounded%2520multimodal%2520reasoning%252C%250Aensuring%2520their%2520factual%2520consistency%2520and%2520reliability%2520is%2520of%2520critical%2520importance.%250AHowever%252C%2520sycophancy%252C%2520the%2520tendency%2520of%2520these%2520models%2520to%2520align%2520with%2520user%2520input%2520even%250Awhen%2520it%2520contradicts%2520the%2520visual%2520evidence%252C%2520undermines%2520their%2520trustworthiness%2520in%250Asuch%2520contexts.%2520Current%2520sycophancy%2520research%2520has%2520largely%2520overlooked%2520its%2520specific%250Amanifestations%2520in%2520the%2520video-language%2520domain%252C%2520resulting%2520in%2520a%2520notable%2520absence%2520of%250Asystematic%2520benchmarks%2520and%2520targeted%2520evaluations%2520to%2520understand%2520how%2520Video-LLMs%250Arespond%2520under%2520misleading%2520user%2520input.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520VISE%250A%2528Video-LLM%2520Sycophancy%2520Benchmarking%2520and%2520Evaluation%2529%252C%2520the%2520first%2520benchmark%250Adesigned%2520to%2520evaluate%2520sycophantic%2520behavior%2520in%2520state-of-the-art%2520Video-LLMs%2520across%250Adiverse%2520question%2520formats%252C%2520prompt%2520biases%252C%2520and%2520visual%2520reasoning%2520tasks.%250ASpecifically%252C%2520VISE%2520pioneeringly%2520brings%2520linguistic%2520perspectives%2520on%2520sycophancy%250Ainto%2520the%2520video%2520domain%252C%2520enabling%2520fine-grained%2520analysis%2520across%2520multiple%250Asycophancy%2520types%2520and%2520interaction%2520patterns.%2520Furthermore%252C%2520we%2520propose%2520two%250Apotential%2520training-free%2520mitigation%2520strategies%252C%2520revealing%2520potential%2520paths%2520for%250Areducing%2520sycophantic%2520bias%253A%2520%2528i%2529%2520enhancing%2520visual%2520grounding%2520through%2520interpretable%250Akey-frame%2520selection%2520and%2520%2528ii%2529%2520steering%2520model%2520behavior%2520away%2520from%2520sycophancy%2520via%250Atargeted%252C%2520inference-time%2520intervention%2520on%2520its%2520internal%2520neural%2520representations.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/William030422/Video-Sycophancy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flattery%20in%20Motion%3A%20Benchmarking%20and%20Analyzing%20Sycophancy%20in%20Video-LLMs&entry.906535625=Wenrui%20Zhou%20and%20Mohamed%20Hendy%20and%20Shu%20Yang%20and%20Qingsong%20Yang%20and%20Zikun%20Guo%20and%20Yuyu%20Luo%20and%20Lijie%20Hu%20and%20Di%20Wang&entry.1292438233=%20%20As%20video%20large%20language%20models%20%28Video-LLMs%29%20become%20increasingly%20integrated%0Ainto%20real-world%20applications%20that%20demand%20grounded%20multimodal%20reasoning%2C%0Aensuring%20their%20factual%20consistency%20and%20reliability%20is%20of%20critical%20importance.%0AHowever%2C%20sycophancy%2C%20the%20tendency%20of%20these%20models%20to%20align%20with%20user%20input%20even%0Awhen%20it%20contradicts%20the%20visual%20evidence%2C%20undermines%20their%20trustworthiness%20in%0Asuch%20contexts.%20Current%20sycophancy%20research%20has%20largely%20overlooked%20its%20specific%0Amanifestations%20in%20the%20video-language%20domain%2C%20resulting%20in%20a%20notable%20absence%20of%0Asystematic%20benchmarks%20and%20targeted%20evaluations%20to%20understand%20how%20Video-LLMs%0Arespond%20under%20misleading%20user%20input.%20To%20fill%20this%20gap%2C%20we%20propose%20VISE%0A%28Video-LLM%20Sycophancy%20Benchmarking%20and%20Evaluation%29%2C%20the%20first%20benchmark%0Adesigned%20to%20evaluate%20sycophantic%20behavior%20in%20state-of-the-art%20Video-LLMs%20across%0Adiverse%20question%20formats%2C%20prompt%20biases%2C%20and%20visual%20reasoning%20tasks.%0ASpecifically%2C%20VISE%20pioneeringly%20brings%20linguistic%20perspectives%20on%20sycophancy%0Ainto%20the%20video%20domain%2C%20enabling%20fine-grained%20analysis%20across%20multiple%0Asycophancy%20types%20and%20interaction%20patterns.%20Furthermore%2C%20we%20propose%20two%0Apotential%20training-free%20mitigation%20strategies%2C%20revealing%20potential%20paths%20for%0Areducing%20sycophantic%20bias%3A%20%28i%29%20enhancing%20visual%20grounding%20through%20interpretable%0Akey-frame%20selection%20and%20%28ii%29%20steering%20model%20behavior%20away%20from%20sycophancy%20via%0Atargeted%2C%20inference-time%20intervention%20on%20its%20internal%20neural%20representations.%0AOur%20code%20is%20available%20at%20https%3A//github.com/William030422/Video-Sycophancy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07180v2&entry.124074799=Read"},
{"title": "Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios", "author": "Zhongzhen Huang and Linjie Mu and Yakun Zhu and Xiangyu Zhao and Shaoting Zhang and Xiaofan Zhang", "abstract": "  Effective clinical decision-making depends on iterative, multimodal reasoning\nacross diverse sources of evidence. The recent emergence of multimodal\nreasoning models has significantly transformed the landscape of solving complex\ntasks. Although such models have achieved notable success in mathematics and\nscience, their application to medical domains remains underexplored. In this\nwork, we propose \\textit{MedE$^2$}, a two-stage post-training pipeline that\nelicits and then enhances multimodal reasoning for medical domains. In Stage-I,\nwe fine-tune models using 2,000 text-only data samples containing precisely\norchestrated reasoning demonstrations to elicit reasoning behaviors. In\nStage-II, we further enhance the model's reasoning capabilities using 1,500\nrigorously curated multimodal medical cases, aligning model reasoning outputs\nwith our proposed multimodal medical reasoning preference. Extensive\nexperiments demonstrate the efficacy and reliability of \\textit{MedE$^2$} in\nimproving the reasoning performance of medical multimodal models. Notably,\nmodels trained with \\textit{MedE$^2$} consistently outperform baselines across\nmultiple medical multimodal benchmarks. Additional validation on larger models\nand under inference-time scaling further confirms the robustness and practical\nutility of our approach.\n", "link": "http://arxiv.org/abs/2505.23118v2", "date": "2025-10-10", "relevancy": 2.1173, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Elicit%20and%20Enhance%3A%20Advancing%20Multimodal%20Reasoning%20in%20Medical%20Scenarios&body=Title%3A%20Elicit%20and%20Enhance%3A%20Advancing%20Multimodal%20Reasoning%20in%20Medical%20Scenarios%0AAuthor%3A%20Zhongzhen%20Huang%20and%20Linjie%20Mu%20and%20Yakun%20Zhu%20and%20Xiangyu%20Zhao%20and%20Shaoting%20Zhang%20and%20Xiaofan%20Zhang%0AAbstract%3A%20%20%20Effective%20clinical%20decision-making%20depends%20on%20iterative%2C%20multimodal%20reasoning%0Aacross%20diverse%20sources%20of%20evidence.%20The%20recent%20emergence%20of%20multimodal%0Areasoning%20models%20has%20significantly%20transformed%20the%20landscape%20of%20solving%20complex%0Atasks.%20Although%20such%20models%20have%20achieved%20notable%20success%20in%20mathematics%20and%0Ascience%2C%20their%20application%20to%20medical%20domains%20remains%20underexplored.%20In%20this%0Awork%2C%20we%20propose%20%5Ctextit%7BMedE%24%5E2%24%7D%2C%20a%20two-stage%20post-training%20pipeline%20that%0Aelicits%20and%20then%20enhances%20multimodal%20reasoning%20for%20medical%20domains.%20In%20Stage-I%2C%0Awe%20fine-tune%20models%20using%202%2C000%20text-only%20data%20samples%20containing%20precisely%0Aorchestrated%20reasoning%20demonstrations%20to%20elicit%20reasoning%20behaviors.%20In%0AStage-II%2C%20we%20further%20enhance%20the%20model%27s%20reasoning%20capabilities%20using%201%2C500%0Arigorously%20curated%20multimodal%20medical%20cases%2C%20aligning%20model%20reasoning%20outputs%0Awith%20our%20proposed%20multimodal%20medical%20reasoning%20preference.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20and%20reliability%20of%20%5Ctextit%7BMedE%24%5E2%24%7D%20in%0Aimproving%20the%20reasoning%20performance%20of%20medical%20multimodal%20models.%20Notably%2C%0Amodels%20trained%20with%20%5Ctextit%7BMedE%24%5E2%24%7D%20consistently%20outperform%20baselines%20across%0Amultiple%20medical%20multimodal%20benchmarks.%20Additional%20validation%20on%20larger%20models%0Aand%20under%20inference-time%20scaling%20further%20confirms%20the%20robustness%20and%20practical%0Autility%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElicit%2520and%2520Enhance%253A%2520Advancing%2520Multimodal%2520Reasoning%2520in%2520Medical%2520Scenarios%26entry.906535625%3DZhongzhen%2520Huang%2520and%2520Linjie%2520Mu%2520and%2520Yakun%2520Zhu%2520and%2520Xiangyu%2520Zhao%2520and%2520Shaoting%2520Zhang%2520and%2520Xiaofan%2520Zhang%26entry.1292438233%3D%2520%2520Effective%2520clinical%2520decision-making%2520depends%2520on%2520iterative%252C%2520multimodal%2520reasoning%250Aacross%2520diverse%2520sources%2520of%2520evidence.%2520The%2520recent%2520emergence%2520of%2520multimodal%250Areasoning%2520models%2520has%2520significantly%2520transformed%2520the%2520landscape%2520of%2520solving%2520complex%250Atasks.%2520Although%2520such%2520models%2520have%2520achieved%2520notable%2520success%2520in%2520mathematics%2520and%250Ascience%252C%2520their%2520application%2520to%2520medical%2520domains%2520remains%2520underexplored.%2520In%2520this%250Awork%252C%2520we%2520propose%2520%255Ctextit%257BMedE%2524%255E2%2524%257D%252C%2520a%2520two-stage%2520post-training%2520pipeline%2520that%250Aelicits%2520and%2520then%2520enhances%2520multimodal%2520reasoning%2520for%2520medical%2520domains.%2520In%2520Stage-I%252C%250Awe%2520fine-tune%2520models%2520using%25202%252C000%2520text-only%2520data%2520samples%2520containing%2520precisely%250Aorchestrated%2520reasoning%2520demonstrations%2520to%2520elicit%2520reasoning%2520behaviors.%2520In%250AStage-II%252C%2520we%2520further%2520enhance%2520the%2520model%2527s%2520reasoning%2520capabilities%2520using%25201%252C500%250Arigorously%2520curated%2520multimodal%2520medical%2520cases%252C%2520aligning%2520model%2520reasoning%2520outputs%250Awith%2520our%2520proposed%2520multimodal%2520medical%2520reasoning%2520preference.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520efficacy%2520and%2520reliability%2520of%2520%255Ctextit%257BMedE%2524%255E2%2524%257D%2520in%250Aimproving%2520the%2520reasoning%2520performance%2520of%2520medical%2520multimodal%2520models.%2520Notably%252C%250Amodels%2520trained%2520with%2520%255Ctextit%257BMedE%2524%255E2%2524%257D%2520consistently%2520outperform%2520baselines%2520across%250Amultiple%2520medical%2520multimodal%2520benchmarks.%2520Additional%2520validation%2520on%2520larger%2520models%250Aand%2520under%2520inference-time%2520scaling%2520further%2520confirms%2520the%2520robustness%2520and%2520practical%250Autility%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elicit%20and%20Enhance%3A%20Advancing%20Multimodal%20Reasoning%20in%20Medical%20Scenarios&entry.906535625=Zhongzhen%20Huang%20and%20Linjie%20Mu%20and%20Yakun%20Zhu%20and%20Xiangyu%20Zhao%20and%20Shaoting%20Zhang%20and%20Xiaofan%20Zhang&entry.1292438233=%20%20Effective%20clinical%20decision-making%20depends%20on%20iterative%2C%20multimodal%20reasoning%0Aacross%20diverse%20sources%20of%20evidence.%20The%20recent%20emergence%20of%20multimodal%0Areasoning%20models%20has%20significantly%20transformed%20the%20landscape%20of%20solving%20complex%0Atasks.%20Although%20such%20models%20have%20achieved%20notable%20success%20in%20mathematics%20and%0Ascience%2C%20their%20application%20to%20medical%20domains%20remains%20underexplored.%20In%20this%0Awork%2C%20we%20propose%20%5Ctextit%7BMedE%24%5E2%24%7D%2C%20a%20two-stage%20post-training%20pipeline%20that%0Aelicits%20and%20then%20enhances%20multimodal%20reasoning%20for%20medical%20domains.%20In%20Stage-I%2C%0Awe%20fine-tune%20models%20using%202%2C000%20text-only%20data%20samples%20containing%20precisely%0Aorchestrated%20reasoning%20demonstrations%20to%20elicit%20reasoning%20behaviors.%20In%0AStage-II%2C%20we%20further%20enhance%20the%20model%27s%20reasoning%20capabilities%20using%201%2C500%0Arigorously%20curated%20multimodal%20medical%20cases%2C%20aligning%20model%20reasoning%20outputs%0Awith%20our%20proposed%20multimodal%20medical%20reasoning%20preference.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20and%20reliability%20of%20%5Ctextit%7BMedE%24%5E2%24%7D%20in%0Aimproving%20the%20reasoning%20performance%20of%20medical%20multimodal%20models.%20Notably%2C%0Amodels%20trained%20with%20%5Ctextit%7BMedE%24%5E2%24%7D%20consistently%20outperform%20baselines%20across%0Amultiple%20medical%20multimodal%20benchmarks.%20Additional%20validation%20on%20larger%20models%0Aand%20under%20inference-time%20scaling%20further%20confirms%20the%20robustness%20and%20practical%0Autility%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23118v2&entry.124074799=Read"},
{"title": "Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for\n  Smart Transportation", "author": "Vijay M. Galshetwar and Praful Hambarde and Prashant W. Patil and Akshay Dudhane and Sachin Chaudhary and Santosh Kumar Vipparathi and Subrahmanyam Murala", "abstract": "  Adverse weather conditions such as haze, rain, and snow significantly degrade\nthe quality of images and videos, posing serious challenges to intelligent\ntransportation systems (ITS) that rely on visual input. These degradations\naffect critical applications including autonomous driving, traffic monitoring,\nand surveillance. This survey presents a comprehensive review of image and\nvideo restoration techniques developed to mitigate weather-induced visual\nimpairments. We categorize existing approaches into traditional prior-based\nmethods and modern data-driven models, including CNNs, transformers, diffusion\nmodels, and emerging vision-language models (VLMs). Restoration strategies are\nfurther classified based on their scope: single-task models,\nmulti-task/multi-weather systems, and all-in-one frameworks capable of handling\ndiverse degradations. In addition, we discuss day and night time restoration\nchallenges, benchmark datasets, and evaluation protocols. The survey concludes\nwith an in-depth discussion on limitations in current research and outlines\nfuture directions such as mixed/compound-degradation restoration, real-time\ndeployment, and agentic AI frameworks. This work aims to serve as a valuable\nreference for advancing weather-resilient vision systems in smart\ntransportation environments. Lastly, to stay current with rapid advancements in\nthis field, we will maintain regular updates of the latest relevant papers and\ntheir open-source implementations at\nhttps://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration\n", "link": "http://arxiv.org/abs/2510.09228v1", "date": "2025-10-10", "relevancy": 2.1007, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5447}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5306}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clear%20Roads%2C%20Clear%20Vision%3A%20Advancements%20in%20Multi-Weather%20Restoration%20for%0A%20%20Smart%20Transportation&body=Title%3A%20Clear%20Roads%2C%20Clear%20Vision%3A%20Advancements%20in%20Multi-Weather%20Restoration%20for%0A%20%20Smart%20Transportation%0AAuthor%3A%20Vijay%20M.%20Galshetwar%20and%20Praful%20Hambarde%20and%20Prashant%20W.%20Patil%20and%20Akshay%20Dudhane%20and%20Sachin%20Chaudhary%20and%20Santosh%20Kumar%20Vipparathi%20and%20Subrahmanyam%20Murala%0AAbstract%3A%20%20%20Adverse%20weather%20conditions%20such%20as%20haze%2C%20rain%2C%20and%20snow%20significantly%20degrade%0Athe%20quality%20of%20images%20and%20videos%2C%20posing%20serious%20challenges%20to%20intelligent%0Atransportation%20systems%20%28ITS%29%20that%20rely%20on%20visual%20input.%20These%20degradations%0Aaffect%20critical%20applications%20including%20autonomous%20driving%2C%20traffic%20monitoring%2C%0Aand%20surveillance.%20This%20survey%20presents%20a%20comprehensive%20review%20of%20image%20and%0Avideo%20restoration%20techniques%20developed%20to%20mitigate%20weather-induced%20visual%0Aimpairments.%20We%20categorize%20existing%20approaches%20into%20traditional%20prior-based%0Amethods%20and%20modern%20data-driven%20models%2C%20including%20CNNs%2C%20transformers%2C%20diffusion%0Amodels%2C%20and%20emerging%20vision-language%20models%20%28VLMs%29.%20Restoration%20strategies%20are%0Afurther%20classified%20based%20on%20their%20scope%3A%20single-task%20models%2C%0Amulti-task/multi-weather%20systems%2C%20and%20all-in-one%20frameworks%20capable%20of%20handling%0Adiverse%20degradations.%20In%20addition%2C%20we%20discuss%20day%20and%20night%20time%20restoration%0Achallenges%2C%20benchmark%20datasets%2C%20and%20evaluation%20protocols.%20The%20survey%20concludes%0Awith%20an%20in-depth%20discussion%20on%20limitations%20in%20current%20research%20and%20outlines%0Afuture%20directions%20such%20as%20mixed/compound-degradation%20restoration%2C%20real-time%0Adeployment%2C%20and%20agentic%20AI%20frameworks.%20This%20work%20aims%20to%20serve%20as%20a%20valuable%0Areference%20for%20advancing%20weather-resilient%20vision%20systems%20in%20smart%0Atransportation%20environments.%20Lastly%2C%20to%20stay%20current%20with%20rapid%20advancements%20in%0Athis%20field%2C%20we%20will%20maintain%20regular%20updates%20of%20the%20latest%20relevant%20papers%20and%0Atheir%20open-source%20implementations%20at%0Ahttps%3A//github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClear%2520Roads%252C%2520Clear%2520Vision%253A%2520Advancements%2520in%2520Multi-Weather%2520Restoration%2520for%250A%2520%2520Smart%2520Transportation%26entry.906535625%3DVijay%2520M.%2520Galshetwar%2520and%2520Praful%2520Hambarde%2520and%2520Prashant%2520W.%2520Patil%2520and%2520Akshay%2520Dudhane%2520and%2520Sachin%2520Chaudhary%2520and%2520Santosh%2520Kumar%2520Vipparathi%2520and%2520Subrahmanyam%2520Murala%26entry.1292438233%3D%2520%2520Adverse%2520weather%2520conditions%2520such%2520as%2520haze%252C%2520rain%252C%2520and%2520snow%2520significantly%2520degrade%250Athe%2520quality%2520of%2520images%2520and%2520videos%252C%2520posing%2520serious%2520challenges%2520to%2520intelligent%250Atransportation%2520systems%2520%2528ITS%2529%2520that%2520rely%2520on%2520visual%2520input.%2520These%2520degradations%250Aaffect%2520critical%2520applications%2520including%2520autonomous%2520driving%252C%2520traffic%2520monitoring%252C%250Aand%2520surveillance.%2520This%2520survey%2520presents%2520a%2520comprehensive%2520review%2520of%2520image%2520and%250Avideo%2520restoration%2520techniques%2520developed%2520to%2520mitigate%2520weather-induced%2520visual%250Aimpairments.%2520We%2520categorize%2520existing%2520approaches%2520into%2520traditional%2520prior-based%250Amethods%2520and%2520modern%2520data-driven%2520models%252C%2520including%2520CNNs%252C%2520transformers%252C%2520diffusion%250Amodels%252C%2520and%2520emerging%2520vision-language%2520models%2520%2528VLMs%2529.%2520Restoration%2520strategies%2520are%250Afurther%2520classified%2520based%2520on%2520their%2520scope%253A%2520single-task%2520models%252C%250Amulti-task/multi-weather%2520systems%252C%2520and%2520all-in-one%2520frameworks%2520capable%2520of%2520handling%250Adiverse%2520degradations.%2520In%2520addition%252C%2520we%2520discuss%2520day%2520and%2520night%2520time%2520restoration%250Achallenges%252C%2520benchmark%2520datasets%252C%2520and%2520evaluation%2520protocols.%2520The%2520survey%2520concludes%250Awith%2520an%2520in-depth%2520discussion%2520on%2520limitations%2520in%2520current%2520research%2520and%2520outlines%250Afuture%2520directions%2520such%2520as%2520mixed/compound-degradation%2520restoration%252C%2520real-time%250Adeployment%252C%2520and%2520agentic%2520AI%2520frameworks.%2520This%2520work%2520aims%2520to%2520serve%2520as%2520a%2520valuable%250Areference%2520for%2520advancing%2520weather-resilient%2520vision%2520systems%2520in%2520smart%250Atransportation%2520environments.%2520Lastly%252C%2520to%2520stay%2520current%2520with%2520rapid%2520advancements%2520in%250Athis%2520field%252C%2520we%2520will%2520maintain%2520regular%2520updates%2520of%2520the%2520latest%2520relevant%2520papers%2520and%250Atheir%2520open-source%2520implementations%2520at%250Ahttps%253A//github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clear%20Roads%2C%20Clear%20Vision%3A%20Advancements%20in%20Multi-Weather%20Restoration%20for%0A%20%20Smart%20Transportation&entry.906535625=Vijay%20M.%20Galshetwar%20and%20Praful%20Hambarde%20and%20Prashant%20W.%20Patil%20and%20Akshay%20Dudhane%20and%20Sachin%20Chaudhary%20and%20Santosh%20Kumar%20Vipparathi%20and%20Subrahmanyam%20Murala&entry.1292438233=%20%20Adverse%20weather%20conditions%20such%20as%20haze%2C%20rain%2C%20and%20snow%20significantly%20degrade%0Athe%20quality%20of%20images%20and%20videos%2C%20posing%20serious%20challenges%20to%20intelligent%0Atransportation%20systems%20%28ITS%29%20that%20rely%20on%20visual%20input.%20These%20degradations%0Aaffect%20critical%20applications%20including%20autonomous%20driving%2C%20traffic%20monitoring%2C%0Aand%20surveillance.%20This%20survey%20presents%20a%20comprehensive%20review%20of%20image%20and%0Avideo%20restoration%20techniques%20developed%20to%20mitigate%20weather-induced%20visual%0Aimpairments.%20We%20categorize%20existing%20approaches%20into%20traditional%20prior-based%0Amethods%20and%20modern%20data-driven%20models%2C%20including%20CNNs%2C%20transformers%2C%20diffusion%0Amodels%2C%20and%20emerging%20vision-language%20models%20%28VLMs%29.%20Restoration%20strategies%20are%0Afurther%20classified%20based%20on%20their%20scope%3A%20single-task%20models%2C%0Amulti-task/multi-weather%20systems%2C%20and%20all-in-one%20frameworks%20capable%20of%20handling%0Adiverse%20degradations.%20In%20addition%2C%20we%20discuss%20day%20and%20night%20time%20restoration%0Achallenges%2C%20benchmark%20datasets%2C%20and%20evaluation%20protocols.%20The%20survey%20concludes%0Awith%20an%20in-depth%20discussion%20on%20limitations%20in%20current%20research%20and%20outlines%0Afuture%20directions%20such%20as%20mixed/compound-degradation%20restoration%2C%20real-time%0Adeployment%2C%20and%20agentic%20AI%20frameworks.%20This%20work%20aims%20to%20serve%20as%20a%20valuable%0Areference%20for%20advancing%20weather-resilient%20vision%20systems%20in%20smart%0Atransportation%20environments.%20Lastly%2C%20to%20stay%20current%20with%20rapid%20advancements%20in%0Athis%20field%2C%20we%20will%20maintain%20regular%20updates%20of%20the%20latest%20relevant%20papers%20and%0Atheir%20open-source%20implementations%20at%0Ahttps%3A//github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09228v1&entry.124074799=Read"},
{"title": "A methodology for clinically driven interactive segmentation evaluation", "author": "Parhom Esmaeili and Virginia Fernandez and Pedro Borges and Eli Gibson and Sebastien Ourselin and M. Jorge Cardoso", "abstract": "  Interactive segmentation is a promising strategy for building robust,\ngeneralisable algorithms for volumetric medical image segmentation. However,\ninconsistent and clinically unrealistic evaluation hinders fair comparison and\nmisrepresents real-world performance. We propose a clinically grounded\nmethodology for defining evaluation tasks and metrics, and built a software\nframework for constructing standardised evaluation pipelines. We evaluate\nstate-of-the-art algorithms across heterogeneous and complex tasks and observe\nthat (i) minimising information loss when processing user interactions is\ncritical for model robustness, (ii) adaptive-zooming mechanisms boost\nrobustness and speed convergence, (iii) performance drops if validation\nprompting behaviour/budgets differ from training, (iv) 2D methods perform well\nwith slab-like images and coarse targets, but 3D context helps with large or\nirregularly shaped targets, (v) performance of non-medical-domain models (e.g.\nSAM2) degrades with poor contrast and complex shapes.\n", "link": "http://arxiv.org/abs/2510.09499v1", "date": "2025-10-10", "relevancy": 2.1002, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20methodology%20for%20clinically%20driven%20interactive%20segmentation%20evaluation&body=Title%3A%20A%20methodology%20for%20clinically%20driven%20interactive%20segmentation%20evaluation%0AAuthor%3A%20Parhom%20Esmaeili%20and%20Virginia%20Fernandez%20and%20Pedro%20Borges%20and%20Eli%20Gibson%20and%20Sebastien%20Ourselin%20and%20M.%20Jorge%20Cardoso%0AAbstract%3A%20%20%20Interactive%20segmentation%20is%20a%20promising%20strategy%20for%20building%20robust%2C%0Ageneralisable%20algorithms%20for%20volumetric%20medical%20image%20segmentation.%20However%2C%0Ainconsistent%20and%20clinically%20unrealistic%20evaluation%20hinders%20fair%20comparison%20and%0Amisrepresents%20real-world%20performance.%20We%20propose%20a%20clinically%20grounded%0Amethodology%20for%20defining%20evaluation%20tasks%20and%20metrics%2C%20and%20built%20a%20software%0Aframework%20for%20constructing%20standardised%20evaluation%20pipelines.%20We%20evaluate%0Astate-of-the-art%20algorithms%20across%20heterogeneous%20and%20complex%20tasks%20and%20observe%0Athat%20%28i%29%20minimising%20information%20loss%20when%20processing%20user%20interactions%20is%0Acritical%20for%20model%20robustness%2C%20%28ii%29%20adaptive-zooming%20mechanisms%20boost%0Arobustness%20and%20speed%20convergence%2C%20%28iii%29%20performance%20drops%20if%20validation%0Aprompting%20behaviour/budgets%20differ%20from%20training%2C%20%28iv%29%202D%20methods%20perform%20well%0Awith%20slab-like%20images%20and%20coarse%20targets%2C%20but%203D%20context%20helps%20with%20large%20or%0Airregularly%20shaped%20targets%2C%20%28v%29%20performance%20of%20non-medical-domain%20models%20%28e.g.%0ASAM2%29%20degrades%20with%20poor%20contrast%20and%20complex%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520methodology%2520for%2520clinically%2520driven%2520interactive%2520segmentation%2520evaluation%26entry.906535625%3DParhom%2520Esmaeili%2520and%2520Virginia%2520Fernandez%2520and%2520Pedro%2520Borges%2520and%2520Eli%2520Gibson%2520and%2520Sebastien%2520Ourselin%2520and%2520M.%2520Jorge%2520Cardoso%26entry.1292438233%3D%2520%2520Interactive%2520segmentation%2520is%2520a%2520promising%2520strategy%2520for%2520building%2520robust%252C%250Ageneralisable%2520algorithms%2520for%2520volumetric%2520medical%2520image%2520segmentation.%2520However%252C%250Ainconsistent%2520and%2520clinically%2520unrealistic%2520evaluation%2520hinders%2520fair%2520comparison%2520and%250Amisrepresents%2520real-world%2520performance.%2520We%2520propose%2520a%2520clinically%2520grounded%250Amethodology%2520for%2520defining%2520evaluation%2520tasks%2520and%2520metrics%252C%2520and%2520built%2520a%2520software%250Aframework%2520for%2520constructing%2520standardised%2520evaluation%2520pipelines.%2520We%2520evaluate%250Astate-of-the-art%2520algorithms%2520across%2520heterogeneous%2520and%2520complex%2520tasks%2520and%2520observe%250Athat%2520%2528i%2529%2520minimising%2520information%2520loss%2520when%2520processing%2520user%2520interactions%2520is%250Acritical%2520for%2520model%2520robustness%252C%2520%2528ii%2529%2520adaptive-zooming%2520mechanisms%2520boost%250Arobustness%2520and%2520speed%2520convergence%252C%2520%2528iii%2529%2520performance%2520drops%2520if%2520validation%250Aprompting%2520behaviour/budgets%2520differ%2520from%2520training%252C%2520%2528iv%2529%25202D%2520methods%2520perform%2520well%250Awith%2520slab-like%2520images%2520and%2520coarse%2520targets%252C%2520but%25203D%2520context%2520helps%2520with%2520large%2520or%250Airregularly%2520shaped%2520targets%252C%2520%2528v%2529%2520performance%2520of%2520non-medical-domain%2520models%2520%2528e.g.%250ASAM2%2529%2520degrades%2520with%2520poor%2520contrast%2520and%2520complex%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20methodology%20for%20clinically%20driven%20interactive%20segmentation%20evaluation&entry.906535625=Parhom%20Esmaeili%20and%20Virginia%20Fernandez%20and%20Pedro%20Borges%20and%20Eli%20Gibson%20and%20Sebastien%20Ourselin%20and%20M.%20Jorge%20Cardoso&entry.1292438233=%20%20Interactive%20segmentation%20is%20a%20promising%20strategy%20for%20building%20robust%2C%0Ageneralisable%20algorithms%20for%20volumetric%20medical%20image%20segmentation.%20However%2C%0Ainconsistent%20and%20clinically%20unrealistic%20evaluation%20hinders%20fair%20comparison%20and%0Amisrepresents%20real-world%20performance.%20We%20propose%20a%20clinically%20grounded%0Amethodology%20for%20defining%20evaluation%20tasks%20and%20metrics%2C%20and%20built%20a%20software%0Aframework%20for%20constructing%20standardised%20evaluation%20pipelines.%20We%20evaluate%0Astate-of-the-art%20algorithms%20across%20heterogeneous%20and%20complex%20tasks%20and%20observe%0Athat%20%28i%29%20minimising%20information%20loss%20when%20processing%20user%20interactions%20is%0Acritical%20for%20model%20robustness%2C%20%28ii%29%20adaptive-zooming%20mechanisms%20boost%0Arobustness%20and%20speed%20convergence%2C%20%28iii%29%20performance%20drops%20if%20validation%0Aprompting%20behaviour/budgets%20differ%20from%20training%2C%20%28iv%29%202D%20methods%20perform%20well%0Awith%20slab-like%20images%20and%20coarse%20targets%2C%20but%203D%20context%20helps%20with%20large%20or%0Airregularly%20shaped%20targets%2C%20%28v%29%20performance%20of%20non-medical-domain%20models%20%28e.g.%0ASAM2%29%20degrades%20with%20poor%20contrast%20and%20complex%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09499v1&entry.124074799=Read"},
{"title": "FFT-based Selection and Optimization of Statistics for Robust\n  Recognition of Severely Corrupted Images", "author": "Elena Camuffo and Umberto Michieli and Jijoong Moon and Daehyun Kim and Mete Ozay", "abstract": "  Improving model robustness in case of corrupted images is among the key\nchallenges to enable robust vision systems on smart devices, such as robotic\nagents. Particularly, robust test-time performance is imperative for most of\nthe applications. This paper presents a novel approach to improve robustness of\nany classification model, especially on severely corrupted images. Our method\n(FROST) employs high-frequency features to detect input image corruption type,\nand select layer-wise feature normalization statistics. FROST provides the\nstate-of-the-art results for different models and datasets, outperforming\ncompetitors on ImageNet-C by up to 37.1% relative gain, improving baseline of\n40.9% mCE on severe corruptions.\n", "link": "http://arxiv.org/abs/2403.14335v2", "date": "2025-10-10", "relevancy": 2.0884, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5353}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5165}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FFT-based%20Selection%20and%20Optimization%20of%20Statistics%20for%20Robust%0A%20%20Recognition%20of%20Severely%20Corrupted%20Images&body=Title%3A%20FFT-based%20Selection%20and%20Optimization%20of%20Statistics%20for%20Robust%0A%20%20Recognition%20of%20Severely%20Corrupted%20Images%0AAuthor%3A%20Elena%20Camuffo%20and%20Umberto%20Michieli%20and%20Jijoong%20Moon%20and%20Daehyun%20Kim%20and%20Mete%20Ozay%0AAbstract%3A%20%20%20Improving%20model%20robustness%20in%20case%20of%20corrupted%20images%20is%20among%20the%20key%0Achallenges%20to%20enable%20robust%20vision%20systems%20on%20smart%20devices%2C%20such%20as%20robotic%0Aagents.%20Particularly%2C%20robust%20test-time%20performance%20is%20imperative%20for%20most%20of%0Athe%20applications.%20This%20paper%20presents%20a%20novel%20approach%20to%20improve%20robustness%20of%0Aany%20classification%20model%2C%20especially%20on%20severely%20corrupted%20images.%20Our%20method%0A%28FROST%29%20employs%20high-frequency%20features%20to%20detect%20input%20image%20corruption%20type%2C%0Aand%20select%20layer-wise%20feature%20normalization%20statistics.%20FROST%20provides%20the%0Astate-of-the-art%20results%20for%20different%20models%20and%20datasets%2C%20outperforming%0Acompetitors%20on%20ImageNet-C%20by%20up%20to%2037.1%25%20relative%20gain%2C%20improving%20baseline%20of%0A40.9%25%20mCE%20on%20severe%20corruptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFFT-based%2520Selection%2520and%2520Optimization%2520of%2520Statistics%2520for%2520Robust%250A%2520%2520Recognition%2520of%2520Severely%2520Corrupted%2520Images%26entry.906535625%3DElena%2520Camuffo%2520and%2520Umberto%2520Michieli%2520and%2520Jijoong%2520Moon%2520and%2520Daehyun%2520Kim%2520and%2520Mete%2520Ozay%26entry.1292438233%3D%2520%2520Improving%2520model%2520robustness%2520in%2520case%2520of%2520corrupted%2520images%2520is%2520among%2520the%2520key%250Achallenges%2520to%2520enable%2520robust%2520vision%2520systems%2520on%2520smart%2520devices%252C%2520such%2520as%2520robotic%250Aagents.%2520Particularly%252C%2520robust%2520test-time%2520performance%2520is%2520imperative%2520for%2520most%2520of%250Athe%2520applications.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520improve%2520robustness%2520of%250Aany%2520classification%2520model%252C%2520especially%2520on%2520severely%2520corrupted%2520images.%2520Our%2520method%250A%2528FROST%2529%2520employs%2520high-frequency%2520features%2520to%2520detect%2520input%2520image%2520corruption%2520type%252C%250Aand%2520select%2520layer-wise%2520feature%2520normalization%2520statistics.%2520FROST%2520provides%2520the%250Astate-of-the-art%2520results%2520for%2520different%2520models%2520and%2520datasets%252C%2520outperforming%250Acompetitors%2520on%2520ImageNet-C%2520by%2520up%2520to%252037.1%2525%2520relative%2520gain%252C%2520improving%2520baseline%2520of%250A40.9%2525%2520mCE%2520on%2520severe%2520corruptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FFT-based%20Selection%20and%20Optimization%20of%20Statistics%20for%20Robust%0A%20%20Recognition%20of%20Severely%20Corrupted%20Images&entry.906535625=Elena%20Camuffo%20and%20Umberto%20Michieli%20and%20Jijoong%20Moon%20and%20Daehyun%20Kim%20and%20Mete%20Ozay&entry.1292438233=%20%20Improving%20model%20robustness%20in%20case%20of%20corrupted%20images%20is%20among%20the%20key%0Achallenges%20to%20enable%20robust%20vision%20systems%20on%20smart%20devices%2C%20such%20as%20robotic%0Aagents.%20Particularly%2C%20robust%20test-time%20performance%20is%20imperative%20for%20most%20of%0Athe%20applications.%20This%20paper%20presents%20a%20novel%20approach%20to%20improve%20robustness%20of%0Aany%20classification%20model%2C%20especially%20on%20severely%20corrupted%20images.%20Our%20method%0A%28FROST%29%20employs%20high-frequency%20features%20to%20detect%20input%20image%20corruption%20type%2C%0Aand%20select%20layer-wise%20feature%20normalization%20statistics.%20FROST%20provides%20the%0Astate-of-the-art%20results%20for%20different%20models%20and%20datasets%2C%20outperforming%0Acompetitors%20on%20ImageNet-C%20by%20up%20to%2037.1%25%20relative%20gain%2C%20improving%20baseline%20of%0A40.9%25%20mCE%20on%20severe%20corruptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14335v2&entry.124074799=Read"},
{"title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion\n  Control", "author": "Minkyoung Cho and Ruben Ohana and Christian Jacobsen and Adityan Jothi and Min-Hung Chen and Z. Morley Mao and Ethem Can", "abstract": "  Current controllable diffusion models typically rely on fixed architectures\nthat modify intermediate activations to inject guidance conditioned on a new\nmodality. This approach uses a static conditioning strategy for a dynamic,\nmulti-stage denoising process, limiting the model's ability to adapt its\nresponse as the generation evolves from coarse structure to fine detail. We\nintroduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that\nenables dynamic, context-aware control by conditioning the model's weights\ndirectly. Our framework uses a hypernetwork to generate LoRA adapters\non-the-fly, tailoring weight modifications for the frozen backbone at each\ndiffusion step based on time and the user's condition. This mechanism enables\nthe model to learn and execute an explicit, adaptive strategy for applying\nconditional guidance throughout the entire generation process. Through\nexperiments on various data domains, we demonstrate that this dynamic,\nparametric control significantly enhances generative fidelity and adherence to\nspatial conditions compared to static, activation-based methods. TC-LoRA\nestablishes an alternative approach in which the model's conditioning strategy\nis modified through a deeper functional adaptation of its weights, allowing\ncontrol to align with the dynamic demands of the task and generative stage.\n", "link": "http://arxiv.org/abs/2510.09561v1", "date": "2025-10-10", "relevancy": 1.7461, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6179}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5732}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TC-LoRA%3A%20Temporally%20Modulated%20Conditional%20LoRA%20for%20Adaptive%20Diffusion%0A%20%20Control&body=Title%3A%20TC-LoRA%3A%20Temporally%20Modulated%20Conditional%20LoRA%20for%20Adaptive%20Diffusion%0A%20%20Control%0AAuthor%3A%20Minkyoung%20Cho%20and%20Ruben%20Ohana%20and%20Christian%20Jacobsen%20and%20Adityan%20Jothi%20and%20Min-Hung%20Chen%20and%20Z.%20Morley%20Mao%20and%20Ethem%20Can%0AAbstract%3A%20%20%20Current%20controllable%20diffusion%20models%20typically%20rely%20on%20fixed%20architectures%0Athat%20modify%20intermediate%20activations%20to%20inject%20guidance%20conditioned%20on%20a%20new%0Amodality.%20This%20approach%20uses%20a%20static%20conditioning%20strategy%20for%20a%20dynamic%2C%0Amulti-stage%20denoising%20process%2C%20limiting%20the%20model%27s%20ability%20to%20adapt%20its%0Aresponse%20as%20the%20generation%20evolves%20from%20coarse%20structure%20to%20fine%20detail.%20We%0Aintroduce%20TC-LoRA%20%28Temporally%20Modulated%20Conditional%20LoRA%29%2C%20a%20new%20paradigm%20that%0Aenables%20dynamic%2C%20context-aware%20control%20by%20conditioning%20the%20model%27s%20weights%0Adirectly.%20Our%20framework%20uses%20a%20hypernetwork%20to%20generate%20LoRA%20adapters%0Aon-the-fly%2C%20tailoring%20weight%20modifications%20for%20the%20frozen%20backbone%20at%20each%0Adiffusion%20step%20based%20on%20time%20and%20the%20user%27s%20condition.%20This%20mechanism%20enables%0Athe%20model%20to%20learn%20and%20execute%20an%20explicit%2C%20adaptive%20strategy%20for%20applying%0Aconditional%20guidance%20throughout%20the%20entire%20generation%20process.%20Through%0Aexperiments%20on%20various%20data%20domains%2C%20we%20demonstrate%20that%20this%20dynamic%2C%0Aparametric%20control%20significantly%20enhances%20generative%20fidelity%20and%20adherence%20to%0Aspatial%20conditions%20compared%20to%20static%2C%20activation-based%20methods.%20TC-LoRA%0Aestablishes%20an%20alternative%20approach%20in%20which%20the%20model%27s%20conditioning%20strategy%0Ais%20modified%20through%20a%20deeper%20functional%20adaptation%20of%20its%20weights%2C%20allowing%0Acontrol%20to%20align%20with%20the%20dynamic%20demands%20of%20the%20task%20and%20generative%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTC-LoRA%253A%2520Temporally%2520Modulated%2520Conditional%2520LoRA%2520for%2520Adaptive%2520Diffusion%250A%2520%2520Control%26entry.906535625%3DMinkyoung%2520Cho%2520and%2520Ruben%2520Ohana%2520and%2520Christian%2520Jacobsen%2520and%2520Adityan%2520Jothi%2520and%2520Min-Hung%2520Chen%2520and%2520Z.%2520Morley%2520Mao%2520and%2520Ethem%2520Can%26entry.1292438233%3D%2520%2520Current%2520controllable%2520diffusion%2520models%2520typically%2520rely%2520on%2520fixed%2520architectures%250Athat%2520modify%2520intermediate%2520activations%2520to%2520inject%2520guidance%2520conditioned%2520on%2520a%2520new%250Amodality.%2520This%2520approach%2520uses%2520a%2520static%2520conditioning%2520strategy%2520for%2520a%2520dynamic%252C%250Amulti-stage%2520denoising%2520process%252C%2520limiting%2520the%2520model%2527s%2520ability%2520to%2520adapt%2520its%250Aresponse%2520as%2520the%2520generation%2520evolves%2520from%2520coarse%2520structure%2520to%2520fine%2520detail.%2520We%250Aintroduce%2520TC-LoRA%2520%2528Temporally%2520Modulated%2520Conditional%2520LoRA%2529%252C%2520a%2520new%2520paradigm%2520that%250Aenables%2520dynamic%252C%2520context-aware%2520control%2520by%2520conditioning%2520the%2520model%2527s%2520weights%250Adirectly.%2520Our%2520framework%2520uses%2520a%2520hypernetwork%2520to%2520generate%2520LoRA%2520adapters%250Aon-the-fly%252C%2520tailoring%2520weight%2520modifications%2520for%2520the%2520frozen%2520backbone%2520at%2520each%250Adiffusion%2520step%2520based%2520on%2520time%2520and%2520the%2520user%2527s%2520condition.%2520This%2520mechanism%2520enables%250Athe%2520model%2520to%2520learn%2520and%2520execute%2520an%2520explicit%252C%2520adaptive%2520strategy%2520for%2520applying%250Aconditional%2520guidance%2520throughout%2520the%2520entire%2520generation%2520process.%2520Through%250Aexperiments%2520on%2520various%2520data%2520domains%252C%2520we%2520demonstrate%2520that%2520this%2520dynamic%252C%250Aparametric%2520control%2520significantly%2520enhances%2520generative%2520fidelity%2520and%2520adherence%2520to%250Aspatial%2520conditions%2520compared%2520to%2520static%252C%2520activation-based%2520methods.%2520TC-LoRA%250Aestablishes%2520an%2520alternative%2520approach%2520in%2520which%2520the%2520model%2527s%2520conditioning%2520strategy%250Ais%2520modified%2520through%2520a%2520deeper%2520functional%2520adaptation%2520of%2520its%2520weights%252C%2520allowing%250Acontrol%2520to%2520align%2520with%2520the%2520dynamic%2520demands%2520of%2520the%2520task%2520and%2520generative%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TC-LoRA%3A%20Temporally%20Modulated%20Conditional%20LoRA%20for%20Adaptive%20Diffusion%0A%20%20Control&entry.906535625=Minkyoung%20Cho%20and%20Ruben%20Ohana%20and%20Christian%20Jacobsen%20and%20Adityan%20Jothi%20and%20Min-Hung%20Chen%20and%20Z.%20Morley%20Mao%20and%20Ethem%20Can&entry.1292438233=%20%20Current%20controllable%20diffusion%20models%20typically%20rely%20on%20fixed%20architectures%0Athat%20modify%20intermediate%20activations%20to%20inject%20guidance%20conditioned%20on%20a%20new%0Amodality.%20This%20approach%20uses%20a%20static%20conditioning%20strategy%20for%20a%20dynamic%2C%0Amulti-stage%20denoising%20process%2C%20limiting%20the%20model%27s%20ability%20to%20adapt%20its%0Aresponse%20as%20the%20generation%20evolves%20from%20coarse%20structure%20to%20fine%20detail.%20We%0Aintroduce%20TC-LoRA%20%28Temporally%20Modulated%20Conditional%20LoRA%29%2C%20a%20new%20paradigm%20that%0Aenables%20dynamic%2C%20context-aware%20control%20by%20conditioning%20the%20model%27s%20weights%0Adirectly.%20Our%20framework%20uses%20a%20hypernetwork%20to%20generate%20LoRA%20adapters%0Aon-the-fly%2C%20tailoring%20weight%20modifications%20for%20the%20frozen%20backbone%20at%20each%0Adiffusion%20step%20based%20on%20time%20and%20the%20user%27s%20condition.%20This%20mechanism%20enables%0Athe%20model%20to%20learn%20and%20execute%20an%20explicit%2C%20adaptive%20strategy%20for%20applying%0Aconditional%20guidance%20throughout%20the%20entire%20generation%20process.%20Through%0Aexperiments%20on%20various%20data%20domains%2C%20we%20demonstrate%20that%20this%20dynamic%2C%0Aparametric%20control%20significantly%20enhances%20generative%20fidelity%20and%20adherence%20to%0Aspatial%20conditions%20compared%20to%20static%2C%20activation-based%20methods.%20TC-LoRA%0Aestablishes%20an%20alternative%20approach%20in%20which%20the%20model%27s%20conditioning%20strategy%0Ais%20modified%20through%20a%20deeper%20functional%20adaptation%20of%20its%20weights%2C%20allowing%0Acontrol%20to%20align%20with%20the%20dynamic%20demands%20of%20the%20task%20and%20generative%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09561v1&entry.124074799=Read"},
{"title": "CausalDynamics: A large-scale benchmark for structural discovery of\n  dynamical causal models", "author": "Benjamin Herdeanu and Juan Nathaniel and Carla Roesch and Jatan Buch and Gregor Ramien and Johannes Haux and Pierre Gentine", "abstract": "  Causal discovery for dynamical systems poses a major challenge in fields\nwhere active interventions are infeasible. Most methods used to investigate\nthese systems and their associated benchmarks are tailored to deterministic,\nlow-dimensional and weakly nonlinear time-series data. To address these\nlimitations, we present CausalDynamics, a large-scale benchmark and extensible\ndata generation framework to advance the structural discovery of dynamical\ncausal models. Our benchmark consists of true causal graphs derived from\nthousands of both linearly and nonlinearly coupled ordinary and stochastic\ndifferential equations as well as two idealized climate models. We perform a\ncomprehensive evaluation of state-of-the-art causal discovery algorithms for\ngraph reconstruction on systems with noisy, confounded, and lagged dynamics.\nCausalDynamics consists of a plug-and-play, build-your-own coupling workflow\nthat enables the construction of a hierarchy of physical systems. We anticipate\nthat our framework will facilitate the development of robust causal discovery\nalgorithms that are broadly applicable across domains while addressing their\nunique challenges. We provide a user-friendly implementation and documentation\non https://kausable.github.io/CausalDynamics.\n", "link": "http://arxiv.org/abs/2505.16620v2", "date": "2025-10-10", "relevancy": 1.8681, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4886}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4673}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalDynamics%3A%20A%20large-scale%20benchmark%20for%20structural%20discovery%20of%0A%20%20dynamical%20causal%20models&body=Title%3A%20CausalDynamics%3A%20A%20large-scale%20benchmark%20for%20structural%20discovery%20of%0A%20%20dynamical%20causal%20models%0AAuthor%3A%20Benjamin%20Herdeanu%20and%20Juan%20Nathaniel%20and%20Carla%20Roesch%20and%20Jatan%20Buch%20and%20Gregor%20Ramien%20and%20Johannes%20Haux%20and%20Pierre%20Gentine%0AAbstract%3A%20%20%20Causal%20discovery%20for%20dynamical%20systems%20poses%20a%20major%20challenge%20in%20fields%0Awhere%20active%20interventions%20are%20infeasible.%20Most%20methods%20used%20to%20investigate%0Athese%20systems%20and%20their%20associated%20benchmarks%20are%20tailored%20to%20deterministic%2C%0Alow-dimensional%20and%20weakly%20nonlinear%20time-series%20data.%20To%20address%20these%0Alimitations%2C%20we%20present%20CausalDynamics%2C%20a%20large-scale%20benchmark%20and%20extensible%0Adata%20generation%20framework%20to%20advance%20the%20structural%20discovery%20of%20dynamical%0Acausal%20models.%20Our%20benchmark%20consists%20of%20true%20causal%20graphs%20derived%20from%0Athousands%20of%20both%20linearly%20and%20nonlinearly%20coupled%20ordinary%20and%20stochastic%0Adifferential%20equations%20as%20well%20as%20two%20idealized%20climate%20models.%20We%20perform%20a%0Acomprehensive%20evaluation%20of%20state-of-the-art%20causal%20discovery%20algorithms%20for%0Agraph%20reconstruction%20on%20systems%20with%20noisy%2C%20confounded%2C%20and%20lagged%20dynamics.%0ACausalDynamics%20consists%20of%20a%20plug-and-play%2C%20build-your-own%20coupling%20workflow%0Athat%20enables%20the%20construction%20of%20a%20hierarchy%20of%20physical%20systems.%20We%20anticipate%0Athat%20our%20framework%20will%20facilitate%20the%20development%20of%20robust%20causal%20discovery%0Aalgorithms%20that%20are%20broadly%20applicable%20across%20domains%20while%20addressing%20their%0Aunique%20challenges.%20We%20provide%20a%20user-friendly%20implementation%20and%20documentation%0Aon%20https%3A//kausable.github.io/CausalDynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalDynamics%253A%2520A%2520large-scale%2520benchmark%2520for%2520structural%2520discovery%2520of%250A%2520%2520dynamical%2520causal%2520models%26entry.906535625%3DBenjamin%2520Herdeanu%2520and%2520Juan%2520Nathaniel%2520and%2520Carla%2520Roesch%2520and%2520Jatan%2520Buch%2520and%2520Gregor%2520Ramien%2520and%2520Johannes%2520Haux%2520and%2520Pierre%2520Gentine%26entry.1292438233%3D%2520%2520Causal%2520discovery%2520for%2520dynamical%2520systems%2520poses%2520a%2520major%2520challenge%2520in%2520fields%250Awhere%2520active%2520interventions%2520are%2520infeasible.%2520Most%2520methods%2520used%2520to%2520investigate%250Athese%2520systems%2520and%2520their%2520associated%2520benchmarks%2520are%2520tailored%2520to%2520deterministic%252C%250Alow-dimensional%2520and%2520weakly%2520nonlinear%2520time-series%2520data.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520present%2520CausalDynamics%252C%2520a%2520large-scale%2520benchmark%2520and%2520extensible%250Adata%2520generation%2520framework%2520to%2520advance%2520the%2520structural%2520discovery%2520of%2520dynamical%250Acausal%2520models.%2520Our%2520benchmark%2520consists%2520of%2520true%2520causal%2520graphs%2520derived%2520from%250Athousands%2520of%2520both%2520linearly%2520and%2520nonlinearly%2520coupled%2520ordinary%2520and%2520stochastic%250Adifferential%2520equations%2520as%2520well%2520as%2520two%2520idealized%2520climate%2520models.%2520We%2520perform%2520a%250Acomprehensive%2520evaluation%2520of%2520state-of-the-art%2520causal%2520discovery%2520algorithms%2520for%250Agraph%2520reconstruction%2520on%2520systems%2520with%2520noisy%252C%2520confounded%252C%2520and%2520lagged%2520dynamics.%250ACausalDynamics%2520consists%2520of%2520a%2520plug-and-play%252C%2520build-your-own%2520coupling%2520workflow%250Athat%2520enables%2520the%2520construction%2520of%2520a%2520hierarchy%2520of%2520physical%2520systems.%2520We%2520anticipate%250Athat%2520our%2520framework%2520will%2520facilitate%2520the%2520development%2520of%2520robust%2520causal%2520discovery%250Aalgorithms%2520that%2520are%2520broadly%2520applicable%2520across%2520domains%2520while%2520addressing%2520their%250Aunique%2520challenges.%2520We%2520provide%2520a%2520user-friendly%2520implementation%2520and%2520documentation%250Aon%2520https%253A//kausable.github.io/CausalDynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalDynamics%3A%20A%20large-scale%20benchmark%20for%20structural%20discovery%20of%0A%20%20dynamical%20causal%20models&entry.906535625=Benjamin%20Herdeanu%20and%20Juan%20Nathaniel%20and%20Carla%20Roesch%20and%20Jatan%20Buch%20and%20Gregor%20Ramien%20and%20Johannes%20Haux%20and%20Pierre%20Gentine&entry.1292438233=%20%20Causal%20discovery%20for%20dynamical%20systems%20poses%20a%20major%20challenge%20in%20fields%0Awhere%20active%20interventions%20are%20infeasible.%20Most%20methods%20used%20to%20investigate%0Athese%20systems%20and%20their%20associated%20benchmarks%20are%20tailored%20to%20deterministic%2C%0Alow-dimensional%20and%20weakly%20nonlinear%20time-series%20data.%20To%20address%20these%0Alimitations%2C%20we%20present%20CausalDynamics%2C%20a%20large-scale%20benchmark%20and%20extensible%0Adata%20generation%20framework%20to%20advance%20the%20structural%20discovery%20of%20dynamical%0Acausal%20models.%20Our%20benchmark%20consists%20of%20true%20causal%20graphs%20derived%20from%0Athousands%20of%20both%20linearly%20and%20nonlinearly%20coupled%20ordinary%20and%20stochastic%0Adifferential%20equations%20as%20well%20as%20two%20idealized%20climate%20models.%20We%20perform%20a%0Acomprehensive%20evaluation%20of%20state-of-the-art%20causal%20discovery%20algorithms%20for%0Agraph%20reconstruction%20on%20systems%20with%20noisy%2C%20confounded%2C%20and%20lagged%20dynamics.%0ACausalDynamics%20consists%20of%20a%20plug-and-play%2C%20build-your-own%20coupling%20workflow%0Athat%20enables%20the%20construction%20of%20a%20hierarchy%20of%20physical%20systems.%20We%20anticipate%0Athat%20our%20framework%20will%20facilitate%20the%20development%20of%20robust%20causal%20discovery%0Aalgorithms%20that%20are%20broadly%20applicable%20across%20domains%20while%20addressing%20their%0Aunique%20challenges.%20We%20provide%20a%20user-friendly%20implementation%20and%20documentation%0Aon%20https%3A//kausable.github.io/CausalDynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16620v2&entry.124074799=Read"},
{"title": "deep-REMAP: Probabilistic Parameterization of Stellar Spectra Using\n  Regularized Multi-Task Learning", "author": "Sankalp Gilda", "abstract": "  In the era of exploding survey volumes, traditional methods of spectroscopic\nanalysis are being pushed to their limits. In response, we develop deep-REMAP,\na novel deep learning framework that utilizes a regularized, multi-task\napproach to predict stellar atmospheric parameters from observed spectra. We\ntrain a deep convolutional neural network on the PHOENIX synthetic spectral\nlibrary and use transfer learning to fine-tune the model on a small subset of\nobserved FGK dwarf spectra from the MARVELS survey. We then apply the model to\n732 uncharacterized FGK giant candidates from the same survey. When validated\non 30 MARVELS calibration stars, deep-REMAP accurately recovers the effective\ntemperature ($T_{\\rm{eff}}$), surface gravity ($\\log \\rm{g}$), and metallicity\n([Fe/H]), achieving a precision of, for instance, approximately 75 K in\n$T_{\\rm{eff}}$. By combining an asymmetric loss function with an embedding\nloss, our regression-as-classification framework is interpretable, robust to\nparameter imbalances, and capable of capturing non-Gaussian uncertainties.\nWhile developed for MARVELS, the deep-REMAP framework is extensible to other\nsurveys and synthetic libraries, demonstrating a powerful and automated pathway\nfor stellar characterization.\n", "link": "http://arxiv.org/abs/2510.09362v1", "date": "2025-10-10", "relevancy": 1.9545, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4982}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4887}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20deep-REMAP%3A%20Probabilistic%20Parameterization%20of%20Stellar%20Spectra%20Using%0A%20%20Regularized%20Multi-Task%20Learning&body=Title%3A%20deep-REMAP%3A%20Probabilistic%20Parameterization%20of%20Stellar%20Spectra%20Using%0A%20%20Regularized%20Multi-Task%20Learning%0AAuthor%3A%20Sankalp%20Gilda%0AAbstract%3A%20%20%20In%20the%20era%20of%20exploding%20survey%20volumes%2C%20traditional%20methods%20of%20spectroscopic%0Aanalysis%20are%20being%20pushed%20to%20their%20limits.%20In%20response%2C%20we%20develop%20deep-REMAP%2C%0Aa%20novel%20deep%20learning%20framework%20that%20utilizes%20a%20regularized%2C%20multi-task%0Aapproach%20to%20predict%20stellar%20atmospheric%20parameters%20from%20observed%20spectra.%20We%0Atrain%20a%20deep%20convolutional%20neural%20network%20on%20the%20PHOENIX%20synthetic%20spectral%0Alibrary%20and%20use%20transfer%20learning%20to%20fine-tune%20the%20model%20on%20a%20small%20subset%20of%0Aobserved%20FGK%20dwarf%20spectra%20from%20the%20MARVELS%20survey.%20We%20then%20apply%20the%20model%20to%0A732%20uncharacterized%20FGK%20giant%20candidates%20from%20the%20same%20survey.%20When%20validated%0Aon%2030%20MARVELS%20calibration%20stars%2C%20deep-REMAP%20accurately%20recovers%20the%20effective%0Atemperature%20%28%24T_%7B%5Crm%7Beff%7D%7D%24%29%2C%20surface%20gravity%20%28%24%5Clog%20%5Crm%7Bg%7D%24%29%2C%20and%20metallicity%0A%28%5BFe/H%5D%29%2C%20achieving%20a%20precision%20of%2C%20for%20instance%2C%20approximately%2075%20K%20in%0A%24T_%7B%5Crm%7Beff%7D%7D%24.%20By%20combining%20an%20asymmetric%20loss%20function%20with%20an%20embedding%0Aloss%2C%20our%20regression-as-classification%20framework%20is%20interpretable%2C%20robust%20to%0Aparameter%20imbalances%2C%20and%20capable%20of%20capturing%20non-Gaussian%20uncertainties.%0AWhile%20developed%20for%20MARVELS%2C%20the%20deep-REMAP%20framework%20is%20extensible%20to%20other%0Asurveys%20and%20synthetic%20libraries%2C%20demonstrating%20a%20powerful%20and%20automated%20pathway%0Afor%20stellar%20characterization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Ddeep-REMAP%253A%2520Probabilistic%2520Parameterization%2520of%2520Stellar%2520Spectra%2520Using%250A%2520%2520Regularized%2520Multi-Task%2520Learning%26entry.906535625%3DSankalp%2520Gilda%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520exploding%2520survey%2520volumes%252C%2520traditional%2520methods%2520of%2520spectroscopic%250Aanalysis%2520are%2520being%2520pushed%2520to%2520their%2520limits.%2520In%2520response%252C%2520we%2520develop%2520deep-REMAP%252C%250Aa%2520novel%2520deep%2520learning%2520framework%2520that%2520utilizes%2520a%2520regularized%252C%2520multi-task%250Aapproach%2520to%2520predict%2520stellar%2520atmospheric%2520parameters%2520from%2520observed%2520spectra.%2520We%250Atrain%2520a%2520deep%2520convolutional%2520neural%2520network%2520on%2520the%2520PHOENIX%2520synthetic%2520spectral%250Alibrary%2520and%2520use%2520transfer%2520learning%2520to%2520fine-tune%2520the%2520model%2520on%2520a%2520small%2520subset%2520of%250Aobserved%2520FGK%2520dwarf%2520spectra%2520from%2520the%2520MARVELS%2520survey.%2520We%2520then%2520apply%2520the%2520model%2520to%250A732%2520uncharacterized%2520FGK%2520giant%2520candidates%2520from%2520the%2520same%2520survey.%2520When%2520validated%250Aon%252030%2520MARVELS%2520calibration%2520stars%252C%2520deep-REMAP%2520accurately%2520recovers%2520the%2520effective%250Atemperature%2520%2528%2524T_%257B%255Crm%257Beff%257D%257D%2524%2529%252C%2520surface%2520gravity%2520%2528%2524%255Clog%2520%255Crm%257Bg%257D%2524%2529%252C%2520and%2520metallicity%250A%2528%255BFe/H%255D%2529%252C%2520achieving%2520a%2520precision%2520of%252C%2520for%2520instance%252C%2520approximately%252075%2520K%2520in%250A%2524T_%257B%255Crm%257Beff%257D%257D%2524.%2520By%2520combining%2520an%2520asymmetric%2520loss%2520function%2520with%2520an%2520embedding%250Aloss%252C%2520our%2520regression-as-classification%2520framework%2520is%2520interpretable%252C%2520robust%2520to%250Aparameter%2520imbalances%252C%2520and%2520capable%2520of%2520capturing%2520non-Gaussian%2520uncertainties.%250AWhile%2520developed%2520for%2520MARVELS%252C%2520the%2520deep-REMAP%2520framework%2520is%2520extensible%2520to%2520other%250Asurveys%2520and%2520synthetic%2520libraries%252C%2520demonstrating%2520a%2520powerful%2520and%2520automated%2520pathway%250Afor%2520stellar%2520characterization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=deep-REMAP%3A%20Probabilistic%20Parameterization%20of%20Stellar%20Spectra%20Using%0A%20%20Regularized%20Multi-Task%20Learning&entry.906535625=Sankalp%20Gilda&entry.1292438233=%20%20In%20the%20era%20of%20exploding%20survey%20volumes%2C%20traditional%20methods%20of%20spectroscopic%0Aanalysis%20are%20being%20pushed%20to%20their%20limits.%20In%20response%2C%20we%20develop%20deep-REMAP%2C%0Aa%20novel%20deep%20learning%20framework%20that%20utilizes%20a%20regularized%2C%20multi-task%0Aapproach%20to%20predict%20stellar%20atmospheric%20parameters%20from%20observed%20spectra.%20We%0Atrain%20a%20deep%20convolutional%20neural%20network%20on%20the%20PHOENIX%20synthetic%20spectral%0Alibrary%20and%20use%20transfer%20learning%20to%20fine-tune%20the%20model%20on%20a%20small%20subset%20of%0Aobserved%20FGK%20dwarf%20spectra%20from%20the%20MARVELS%20survey.%20We%20then%20apply%20the%20model%20to%0A732%20uncharacterized%20FGK%20giant%20candidates%20from%20the%20same%20survey.%20When%20validated%0Aon%2030%20MARVELS%20calibration%20stars%2C%20deep-REMAP%20accurately%20recovers%20the%20effective%0Atemperature%20%28%24T_%7B%5Crm%7Beff%7D%7D%24%29%2C%20surface%20gravity%20%28%24%5Clog%20%5Crm%7Bg%7D%24%29%2C%20and%20metallicity%0A%28%5BFe/H%5D%29%2C%20achieving%20a%20precision%20of%2C%20for%20instance%2C%20approximately%2075%20K%20in%0A%24T_%7B%5Crm%7Beff%7D%7D%24.%20By%20combining%20an%20asymmetric%20loss%20function%20with%20an%20embedding%0Aloss%2C%20our%20regression-as-classification%20framework%20is%20interpretable%2C%20robust%20to%0Aparameter%20imbalances%2C%20and%20capable%20of%20capturing%20non-Gaussian%20uncertainties.%0AWhile%20developed%20for%20MARVELS%2C%20the%20deep-REMAP%20framework%20is%20extensible%20to%20other%0Asurveys%20and%20synthetic%20libraries%2C%20demonstrating%20a%20powerful%20and%20automated%20pathway%0Afor%20stellar%20characterization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09362v1&entry.124074799=Read"},
{"title": "FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in\n  Reinforcement Learning", "author": "Zhenglin Wan and Jingxuan Wu and Xingrui Yu and Chubin Zhang and Mingcong Lei and Bo An and Ivor Tsang", "abstract": "  Flow Matching (FM) has shown remarkable ability in modeling complex\ndistributions and achieves strong performance in offline imitation learning for\ncloning expert behaviors. However, despite its behavioral cloning\nexpressiveness, FM-based policies are inherently limited by their lack of\nenvironmental interaction and exploration. This leads to poor generalization in\nunseen scenarios beyond the expert demonstrations, underscoring the necessity\nof online interaction with environment. Unfortunately, optimizing FM policies\nvia online interaction is challenging and inefficient due to instability in\ngradient computation and high inference costs. To address these issues, we\npropose to let a student policy with simple MLP structure explore the\nenvironment and be online updated via RL algorithm with a reward model. This\nreward model is associated with a teacher FM model, containing rich information\nof expert data distribution. Furthermore, the same teacher FM model is utilized\nto regularize the student policy's behavior to stabilize policy learning. Due\nto the student's simple architecture, we avoid the gradient instability of FM\npolicies and enable efficient online exploration, while still leveraging the\nexpressiveness of the teacher FM model. Extensive experiments show that our\napproach significantly enhances learning efficiency, generalization, and\nrobustness, especially when learning from suboptimal expert data.\n", "link": "http://arxiv.org/abs/2510.09222v1", "date": "2025-10-10", "relevancy": 1.4687, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5126}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5016}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FM-IRL%3A%20Flow-Matching%20for%20Reward%20Modeling%20and%20Policy%20Regularization%20in%0A%20%20Reinforcement%20Learning&body=Title%3A%20FM-IRL%3A%20Flow-Matching%20for%20Reward%20Modeling%20and%20Policy%20Regularization%20in%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Zhenglin%20Wan%20and%20Jingxuan%20Wu%20and%20Xingrui%20Yu%20and%20Chubin%20Zhang%20and%20Mingcong%20Lei%20and%20Bo%20An%20and%20Ivor%20Tsang%0AAbstract%3A%20%20%20Flow%20Matching%20%28FM%29%20has%20shown%20remarkable%20ability%20in%20modeling%20complex%0Adistributions%20and%20achieves%20strong%20performance%20in%20offline%20imitation%20learning%20for%0Acloning%20expert%20behaviors.%20However%2C%20despite%20its%20behavioral%20cloning%0Aexpressiveness%2C%20FM-based%20policies%20are%20inherently%20limited%20by%20their%20lack%20of%0Aenvironmental%20interaction%20and%20exploration.%20This%20leads%20to%20poor%20generalization%20in%0Aunseen%20scenarios%20beyond%20the%20expert%20demonstrations%2C%20underscoring%20the%20necessity%0Aof%20online%20interaction%20with%20environment.%20Unfortunately%2C%20optimizing%20FM%20policies%0Avia%20online%20interaction%20is%20challenging%20and%20inefficient%20due%20to%20instability%20in%0Agradient%20computation%20and%20high%20inference%20costs.%20To%20address%20these%20issues%2C%20we%0Apropose%20to%20let%20a%20student%20policy%20with%20simple%20MLP%20structure%20explore%20the%0Aenvironment%20and%20be%20online%20updated%20via%20RL%20algorithm%20with%20a%20reward%20model.%20This%0Areward%20model%20is%20associated%20with%20a%20teacher%20FM%20model%2C%20containing%20rich%20information%0Aof%20expert%20data%20distribution.%20Furthermore%2C%20the%20same%20teacher%20FM%20model%20is%20utilized%0Ato%20regularize%20the%20student%20policy%27s%20behavior%20to%20stabilize%20policy%20learning.%20Due%0Ato%20the%20student%27s%20simple%20architecture%2C%20we%20avoid%20the%20gradient%20instability%20of%20FM%0Apolicies%20and%20enable%20efficient%20online%20exploration%2C%20while%20still%20leveraging%20the%0Aexpressiveness%20of%20the%20teacher%20FM%20model.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20significantly%20enhances%20learning%20efficiency%2C%20generalization%2C%20and%0Arobustness%2C%20especially%20when%20learning%20from%20suboptimal%20expert%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFM-IRL%253A%2520Flow-Matching%2520for%2520Reward%2520Modeling%2520and%2520Policy%2520Regularization%2520in%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZhenglin%2520Wan%2520and%2520Jingxuan%2520Wu%2520and%2520Xingrui%2520Yu%2520and%2520Chubin%2520Zhang%2520and%2520Mingcong%2520Lei%2520and%2520Bo%2520An%2520and%2520Ivor%2520Tsang%26entry.1292438233%3D%2520%2520Flow%2520Matching%2520%2528FM%2529%2520has%2520shown%2520remarkable%2520ability%2520in%2520modeling%2520complex%250Adistributions%2520and%2520achieves%2520strong%2520performance%2520in%2520offline%2520imitation%2520learning%2520for%250Acloning%2520expert%2520behaviors.%2520However%252C%2520despite%2520its%2520behavioral%2520cloning%250Aexpressiveness%252C%2520FM-based%2520policies%2520are%2520inherently%2520limited%2520by%2520their%2520lack%2520of%250Aenvironmental%2520interaction%2520and%2520exploration.%2520This%2520leads%2520to%2520poor%2520generalization%2520in%250Aunseen%2520scenarios%2520beyond%2520the%2520expert%2520demonstrations%252C%2520underscoring%2520the%2520necessity%250Aof%2520online%2520interaction%2520with%2520environment.%2520Unfortunately%252C%2520optimizing%2520FM%2520policies%250Avia%2520online%2520interaction%2520is%2520challenging%2520and%2520inefficient%2520due%2520to%2520instability%2520in%250Agradient%2520computation%2520and%2520high%2520inference%2520costs.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520to%2520let%2520a%2520student%2520policy%2520with%2520simple%2520MLP%2520structure%2520explore%2520the%250Aenvironment%2520and%2520be%2520online%2520updated%2520via%2520RL%2520algorithm%2520with%2520a%2520reward%2520model.%2520This%250Areward%2520model%2520is%2520associated%2520with%2520a%2520teacher%2520FM%2520model%252C%2520containing%2520rich%2520information%250Aof%2520expert%2520data%2520distribution.%2520Furthermore%252C%2520the%2520same%2520teacher%2520FM%2520model%2520is%2520utilized%250Ato%2520regularize%2520the%2520student%2520policy%2527s%2520behavior%2520to%2520stabilize%2520policy%2520learning.%2520Due%250Ato%2520the%2520student%2527s%2520simple%2520architecture%252C%2520we%2520avoid%2520the%2520gradient%2520instability%2520of%2520FM%250Apolicies%2520and%2520enable%2520efficient%2520online%2520exploration%252C%2520while%2520still%2520leveraging%2520the%250Aexpressiveness%2520of%2520the%2520teacher%2520FM%2520model.%2520Extensive%2520experiments%2520show%2520that%2520our%250Aapproach%2520significantly%2520enhances%2520learning%2520efficiency%252C%2520generalization%252C%2520and%250Arobustness%252C%2520especially%2520when%2520learning%2520from%2520suboptimal%2520expert%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FM-IRL%3A%20Flow-Matching%20for%20Reward%20Modeling%20and%20Policy%20Regularization%20in%0A%20%20Reinforcement%20Learning&entry.906535625=Zhenglin%20Wan%20and%20Jingxuan%20Wu%20and%20Xingrui%20Yu%20and%20Chubin%20Zhang%20and%20Mingcong%20Lei%20and%20Bo%20An%20and%20Ivor%20Tsang&entry.1292438233=%20%20Flow%20Matching%20%28FM%29%20has%20shown%20remarkable%20ability%20in%20modeling%20complex%0Adistributions%20and%20achieves%20strong%20performance%20in%20offline%20imitation%20learning%20for%0Acloning%20expert%20behaviors.%20However%2C%20despite%20its%20behavioral%20cloning%0Aexpressiveness%2C%20FM-based%20policies%20are%20inherently%20limited%20by%20their%20lack%20of%0Aenvironmental%20interaction%20and%20exploration.%20This%20leads%20to%20poor%20generalization%20in%0Aunseen%20scenarios%20beyond%20the%20expert%20demonstrations%2C%20underscoring%20the%20necessity%0Aof%20online%20interaction%20with%20environment.%20Unfortunately%2C%20optimizing%20FM%20policies%0Avia%20online%20interaction%20is%20challenging%20and%20inefficient%20due%20to%20instability%20in%0Agradient%20computation%20and%20high%20inference%20costs.%20To%20address%20these%20issues%2C%20we%0Apropose%20to%20let%20a%20student%20policy%20with%20simple%20MLP%20structure%20explore%20the%0Aenvironment%20and%20be%20online%20updated%20via%20RL%20algorithm%20with%20a%20reward%20model.%20This%0Areward%20model%20is%20associated%20with%20a%20teacher%20FM%20model%2C%20containing%20rich%20information%0Aof%20expert%20data%20distribution.%20Furthermore%2C%20the%20same%20teacher%20FM%20model%20is%20utilized%0Ato%20regularize%20the%20student%20policy%27s%20behavior%20to%20stabilize%20policy%20learning.%20Due%0Ato%20the%20student%27s%20simple%20architecture%2C%20we%20avoid%20the%20gradient%20instability%20of%20FM%0Apolicies%20and%20enable%20efficient%20online%20exploration%2C%20while%20still%20leveraging%20the%0Aexpressiveness%20of%20the%20teacher%20FM%20model.%20Extensive%20experiments%20show%20that%20our%0Aapproach%20significantly%20enhances%20learning%20efficiency%2C%20generalization%2C%20and%0Arobustness%2C%20especially%20when%20learning%20from%20suboptimal%20expert%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09222v1&entry.124074799=Read"},
{"title": "ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning\n  Language Models", "author": "Dongqi Zheng", "abstract": "  Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable\ncapabilities in complex reasoning tasks, but suffer from significant\ncomputational inefficiencies due to overthinking phenomena. Existing efficient\nreasoning methods face the challenge of balancing reasoning quality with\ninference cost reduction. We propose \\textbf{Adaptive Reasoning Suppression\n(ARS)}, a novel training-free approach that dynamically suppresses redundant\nreasoning steps while preserving accuracy through adaptive certainty\nmonitoring. ARS introduces a multi-checkpoint certainty estimation mechanism\nwith progressive suppression thresholds, achieving superior efficiency compared\nto static suppression methods. Our extensive evaluation across mathematical\nreasoning benchmarks using multiple model architectures demonstrates that ARS\nachieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,\nwhile maintaining or improving accuracy.\n", "link": "http://arxiv.org/abs/2510.00071v2", "date": "2025-10-10", "relevancy": 1.8944, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4801}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARS%3A%20Adaptive%20Reasoning%20Suppression%20for%20Efficient%20Large%20Reasoning%0A%20%20Language%20Models&body=Title%3A%20ARS%3A%20Adaptive%20Reasoning%20Suppression%20for%20Efficient%20Large%20Reasoning%0A%20%20Language%20Models%0AAuthor%3A%20Dongqi%20Zheng%0AAbstract%3A%20%20%20Large%20Reasoning%20Language%20Models%20%28LRLMs%20or%20LRMs%29%20demonstrate%20remarkable%0Acapabilities%20in%20complex%20reasoning%20tasks%2C%20but%20suffer%20from%20significant%0Acomputational%20inefficiencies%20due%20to%20overthinking%20phenomena.%20Existing%20efficient%0Areasoning%20methods%20face%20the%20challenge%20of%20balancing%20reasoning%20quality%20with%0Ainference%20cost%20reduction.%20We%20propose%20%5Ctextbf%7BAdaptive%20Reasoning%20Suppression%0A%28ARS%29%7D%2C%20a%20novel%20training-free%20approach%20that%20dynamically%20suppresses%20redundant%0Areasoning%20steps%20while%20preserving%20accuracy%20through%20adaptive%20certainty%0Amonitoring.%20ARS%20introduces%20a%20multi-checkpoint%20certainty%20estimation%20mechanism%0Awith%20progressive%20suppression%20thresholds%2C%20achieving%20superior%20efficiency%20compared%0Ato%20static%20suppression%20methods.%20Our%20extensive%20evaluation%20across%20mathematical%0Areasoning%20benchmarks%20using%20multiple%20model%20architectures%20demonstrates%20that%20ARS%0Aachieves%20up%20to%2053%25%2C%2046.1%25%2C%20and%2057.9%25%20in%20token%2C%20latency%20and%20energy%20reduction%2C%0Awhile%20maintaining%20or%20improving%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00071v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARS%253A%2520Adaptive%2520Reasoning%2520Suppression%2520for%2520Efficient%2520Large%2520Reasoning%250A%2520%2520Language%2520Models%26entry.906535625%3DDongqi%2520Zheng%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Language%2520Models%2520%2528LRLMs%2520or%2520LRMs%2529%2520demonstrate%2520remarkable%250Acapabilities%2520in%2520complex%2520reasoning%2520tasks%252C%2520but%2520suffer%2520from%2520significant%250Acomputational%2520inefficiencies%2520due%2520to%2520overthinking%2520phenomena.%2520Existing%2520efficient%250Areasoning%2520methods%2520face%2520the%2520challenge%2520of%2520balancing%2520reasoning%2520quality%2520with%250Ainference%2520cost%2520reduction.%2520We%2520propose%2520%255Ctextbf%257BAdaptive%2520Reasoning%2520Suppression%250A%2528ARS%2529%257D%252C%2520a%2520novel%2520training-free%2520approach%2520that%2520dynamically%2520suppresses%2520redundant%250Areasoning%2520steps%2520while%2520preserving%2520accuracy%2520through%2520adaptive%2520certainty%250Amonitoring.%2520ARS%2520introduces%2520a%2520multi-checkpoint%2520certainty%2520estimation%2520mechanism%250Awith%2520progressive%2520suppression%2520thresholds%252C%2520achieving%2520superior%2520efficiency%2520compared%250Ato%2520static%2520suppression%2520methods.%2520Our%2520extensive%2520evaluation%2520across%2520mathematical%250Areasoning%2520benchmarks%2520using%2520multiple%2520model%2520architectures%2520demonstrates%2520that%2520ARS%250Aachieves%2520up%2520to%252053%2525%252C%252046.1%2525%252C%2520and%252057.9%2525%2520in%2520token%252C%2520latency%2520and%2520energy%2520reduction%252C%250Awhile%2520maintaining%2520or%2520improving%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00071v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARS%3A%20Adaptive%20Reasoning%20Suppression%20for%20Efficient%20Large%20Reasoning%0A%20%20Language%20Models&entry.906535625=Dongqi%20Zheng&entry.1292438233=%20%20Large%20Reasoning%20Language%20Models%20%28LRLMs%20or%20LRMs%29%20demonstrate%20remarkable%0Acapabilities%20in%20complex%20reasoning%20tasks%2C%20but%20suffer%20from%20significant%0Acomputational%20inefficiencies%20due%20to%20overthinking%20phenomena.%20Existing%20efficient%0Areasoning%20methods%20face%20the%20challenge%20of%20balancing%20reasoning%20quality%20with%0Ainference%20cost%20reduction.%20We%20propose%20%5Ctextbf%7BAdaptive%20Reasoning%20Suppression%0A%28ARS%29%7D%2C%20a%20novel%20training-free%20approach%20that%20dynamically%20suppresses%20redundant%0Areasoning%20steps%20while%20preserving%20accuracy%20through%20adaptive%20certainty%0Amonitoring.%20ARS%20introduces%20a%20multi-checkpoint%20certainty%20estimation%20mechanism%0Awith%20progressive%20suppression%20thresholds%2C%20achieving%20superior%20efficiency%20compared%0Ato%20static%20suppression%20methods.%20Our%20extensive%20evaluation%20across%20mathematical%0Areasoning%20benchmarks%20using%20multiple%20model%20architectures%20demonstrates%20that%20ARS%0Aachieves%20up%20to%2053%25%2C%2046.1%25%2C%20and%2057.9%25%20in%20token%2C%20latency%20and%20energy%20reduction%2C%0Awhile%20maintaining%20or%20improving%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00071v2&entry.124074799=Read"},
{"title": "Detecting and Filtering Unsafe Training Data via Data Attribution with\n  Denoised Representation", "author": "Yijun Pan and Taiwei Shi and Jieyu Zhao and Jiaqi W. Ma", "abstract": "  Large language models (LLMs) are highly sensitive to even small amounts of\nunsafe training data, making effective detection and filtering essential for\ntrustworthy model development. Current state-of-the-art (SOTA) detection\napproaches primarily rely on moderation classifiers, which require significant\ncomputation overhead for training and are limited to predefined taxonomies. In\nthis work, we explore data attribution approaches that measure the similarity\nbetween individual training samples and a small set of unsafe target examples,\nbased on data representations such as hidden states or gradients. We identify a\nkey limitation in existing methods: unsafe target texts contain both critical\ntokens that make them unsafe and neutral tokens (e.g., stop words or benign\nfacts) that are necessary to form fluent language, and the latter of which\nmakes the overall representations ``noisy'' for the purpose of detecting unsafe\ntraining data. To address this challenge, we propose Denoised Representation\nAttribution (DRA), a novel representation-based data attribution approach that\ndenoises training and target representations for unsafe data detection. Across\ntasks of filtering jailbreaks and detecting gender bias, the proposed approach\nleads to significant improvement for data attribution methods, outperforming\nSOTA methods that are mostly based on moderation classifiers.\n", "link": "http://arxiv.org/abs/2502.11411v2", "date": "2025-10-10", "relevancy": 2.0304, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5151}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5065}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Filtering%20Unsafe%20Training%20Data%20via%20Data%20Attribution%20with%0A%20%20Denoised%20Representation&body=Title%3A%20Detecting%20and%20Filtering%20Unsafe%20Training%20Data%20via%20Data%20Attribution%20with%0A%20%20Denoised%20Representation%0AAuthor%3A%20Yijun%20Pan%20and%20Taiwei%20Shi%20and%20Jieyu%20Zhao%20and%20Jiaqi%20W.%20Ma%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20highly%20sensitive%20to%20even%20small%20amounts%20of%0Aunsafe%20training%20data%2C%20making%20effective%20detection%20and%20filtering%20essential%20for%0Atrustworthy%20model%20development.%20Current%20state-of-the-art%20%28SOTA%29%20detection%0Aapproaches%20primarily%20rely%20on%20moderation%20classifiers%2C%20which%20require%20significant%0Acomputation%20overhead%20for%20training%20and%20are%20limited%20to%20predefined%20taxonomies.%20In%0Athis%20work%2C%20we%20explore%20data%20attribution%20approaches%20that%20measure%20the%20similarity%0Abetween%20individual%20training%20samples%20and%20a%20small%20set%20of%20unsafe%20target%20examples%2C%0Abased%20on%20data%20representations%20such%20as%20hidden%20states%20or%20gradients.%20We%20identify%20a%0Akey%20limitation%20in%20existing%20methods%3A%20unsafe%20target%20texts%20contain%20both%20critical%0Atokens%20that%20make%20them%20unsafe%20and%20neutral%20tokens%20%28e.g.%2C%20stop%20words%20or%20benign%0Afacts%29%20that%20are%20necessary%20to%20form%20fluent%20language%2C%20and%20the%20latter%20of%20which%0Amakes%20the%20overall%20representations%20%60%60noisy%27%27%20for%20the%20purpose%20of%20detecting%20unsafe%0Atraining%20data.%20To%20address%20this%20challenge%2C%20we%20propose%20Denoised%20Representation%0AAttribution%20%28DRA%29%2C%20a%20novel%20representation-based%20data%20attribution%20approach%20that%0Adenoises%20training%20and%20target%20representations%20for%20unsafe%20data%20detection.%20Across%0Atasks%20of%20filtering%20jailbreaks%20and%20detecting%20gender%20bias%2C%20the%20proposed%20approach%0Aleads%20to%20significant%20improvement%20for%20data%20attribution%20methods%2C%20outperforming%0ASOTA%20methods%20that%20are%20mostly%20based%20on%20moderation%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Filtering%2520Unsafe%2520Training%2520Data%2520via%2520Data%2520Attribution%2520with%250A%2520%2520Denoised%2520Representation%26entry.906535625%3DYijun%2520Pan%2520and%2520Taiwei%2520Shi%2520and%2520Jieyu%2520Zhao%2520and%2520Jiaqi%2520W.%2520Ma%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520highly%2520sensitive%2520to%2520even%2520small%2520amounts%2520of%250Aunsafe%2520training%2520data%252C%2520making%2520effective%2520detection%2520and%2520filtering%2520essential%2520for%250Atrustworthy%2520model%2520development.%2520Current%2520state-of-the-art%2520%2528SOTA%2529%2520detection%250Aapproaches%2520primarily%2520rely%2520on%2520moderation%2520classifiers%252C%2520which%2520require%2520significant%250Acomputation%2520overhead%2520for%2520training%2520and%2520are%2520limited%2520to%2520predefined%2520taxonomies.%2520In%250Athis%2520work%252C%2520we%2520explore%2520data%2520attribution%2520approaches%2520that%2520measure%2520the%2520similarity%250Abetween%2520individual%2520training%2520samples%2520and%2520a%2520small%2520set%2520of%2520unsafe%2520target%2520examples%252C%250Abased%2520on%2520data%2520representations%2520such%2520as%2520hidden%2520states%2520or%2520gradients.%2520We%2520identify%2520a%250Akey%2520limitation%2520in%2520existing%2520methods%253A%2520unsafe%2520target%2520texts%2520contain%2520both%2520critical%250Atokens%2520that%2520make%2520them%2520unsafe%2520and%2520neutral%2520tokens%2520%2528e.g.%252C%2520stop%2520words%2520or%2520benign%250Afacts%2529%2520that%2520are%2520necessary%2520to%2520form%2520fluent%2520language%252C%2520and%2520the%2520latter%2520of%2520which%250Amakes%2520the%2520overall%2520representations%2520%2560%2560noisy%2527%2527%2520for%2520the%2520purpose%2520of%2520detecting%2520unsafe%250Atraining%2520data.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Denoised%2520Representation%250AAttribution%2520%2528DRA%2529%252C%2520a%2520novel%2520representation-based%2520data%2520attribution%2520approach%2520that%250Adenoises%2520training%2520and%2520target%2520representations%2520for%2520unsafe%2520data%2520detection.%2520Across%250Atasks%2520of%2520filtering%2520jailbreaks%2520and%2520detecting%2520gender%2520bias%252C%2520the%2520proposed%2520approach%250Aleads%2520to%2520significant%2520improvement%2520for%2520data%2520attribution%2520methods%252C%2520outperforming%250ASOTA%2520methods%2520that%2520are%2520mostly%2520based%2520on%2520moderation%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Filtering%20Unsafe%20Training%20Data%20via%20Data%20Attribution%20with%0A%20%20Denoised%20Representation&entry.906535625=Yijun%20Pan%20and%20Taiwei%20Shi%20and%20Jieyu%20Zhao%20and%20Jiaqi%20W.%20Ma&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20highly%20sensitive%20to%20even%20small%20amounts%20of%0Aunsafe%20training%20data%2C%20making%20effective%20detection%20and%20filtering%20essential%20for%0Atrustworthy%20model%20development.%20Current%20state-of-the-art%20%28SOTA%29%20detection%0Aapproaches%20primarily%20rely%20on%20moderation%20classifiers%2C%20which%20require%20significant%0Acomputation%20overhead%20for%20training%20and%20are%20limited%20to%20predefined%20taxonomies.%20In%0Athis%20work%2C%20we%20explore%20data%20attribution%20approaches%20that%20measure%20the%20similarity%0Abetween%20individual%20training%20samples%20and%20a%20small%20set%20of%20unsafe%20target%20examples%2C%0Abased%20on%20data%20representations%20such%20as%20hidden%20states%20or%20gradients.%20We%20identify%20a%0Akey%20limitation%20in%20existing%20methods%3A%20unsafe%20target%20texts%20contain%20both%20critical%0Atokens%20that%20make%20them%20unsafe%20and%20neutral%20tokens%20%28e.g.%2C%20stop%20words%20or%20benign%0Afacts%29%20that%20are%20necessary%20to%20form%20fluent%20language%2C%20and%20the%20latter%20of%20which%0Amakes%20the%20overall%20representations%20%60%60noisy%27%27%20for%20the%20purpose%20of%20detecting%20unsafe%0Atraining%20data.%20To%20address%20this%20challenge%2C%20we%20propose%20Denoised%20Representation%0AAttribution%20%28DRA%29%2C%20a%20novel%20representation-based%20data%20attribution%20approach%20that%0Adenoises%20training%20and%20target%20representations%20for%20unsafe%20data%20detection.%20Across%0Atasks%20of%20filtering%20jailbreaks%20and%20detecting%20gender%20bias%2C%20the%20proposed%20approach%0Aleads%20to%20significant%20improvement%20for%20data%20attribution%20methods%2C%20outperforming%0ASOTA%20methods%20that%20are%20mostly%20based%20on%20moderation%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11411v2&entry.124074799=Read"},
{"title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models", "author": "Chengyu Wang and Paria Rashidinejad and DiJia Su and Song Jiang and Sid Wang and Siyan Zhao and Cai Zhou and Shannon Zejiang Shen and Feiyu Chen and Tommi Jaakkola and Yuandong Tian and Bo Liu", "abstract": "  Diffusion large language models (dLLMs) are emerging as an efficient\nalternative to autoregressive models due to their ability to decode multiple\ntokens in parallel. However, aligning dLLMs with human preferences or\ntask-specific rewards via reinforcement learning (RL) is challenging because\ntheir intractable log-likelihood precludes the direct application of standard\npolicy gradient methods. While prior work uses surrogates like the evidence\nlower bound (ELBO), these one-sided approximations can introduce significant\npolicy gradient bias. To address this, we propose the Sandwiched Policy\nGradient (SPG) that leverages both an upper and a lower bound of the true\nlog-likelihood. Experiments show that SPG significantly outperforms baselines\nbased on ELBO or one-step estimation. Specifically, SPG improves the accuracy\nover state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,\n18.4% in Countdown and 27.0% in Sudoku.\n", "link": "http://arxiv.org/abs/2510.09541v1", "date": "2025-10-10", "relevancy": 1.5464, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5698}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5027}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPG%3A%20Sandwiched%20Policy%20Gradient%20for%20Masked%20Diffusion%20Language%20Models&body=Title%3A%20SPG%3A%20Sandwiched%20Policy%20Gradient%20for%20Masked%20Diffusion%20Language%20Models%0AAuthor%3A%20Chengyu%20Wang%20and%20Paria%20Rashidinejad%20and%20DiJia%20Su%20and%20Song%20Jiang%20and%20Sid%20Wang%20and%20Siyan%20Zhao%20and%20Cai%20Zhou%20and%20Shannon%20Zejiang%20Shen%20and%20Feiyu%20Chen%20and%20Tommi%20Jaakkola%20and%20Yuandong%20Tian%20and%20Bo%20Liu%0AAbstract%3A%20%20%20Diffusion%20large%20language%20models%20%28dLLMs%29%20are%20emerging%20as%20an%20efficient%0Aalternative%20to%20autoregressive%20models%20due%20to%20their%20ability%20to%20decode%20multiple%0Atokens%20in%20parallel.%20However%2C%20aligning%20dLLMs%20with%20human%20preferences%20or%0Atask-specific%20rewards%20via%20reinforcement%20learning%20%28RL%29%20is%20challenging%20because%0Atheir%20intractable%20log-likelihood%20precludes%20the%20direct%20application%20of%20standard%0Apolicy%20gradient%20methods.%20While%20prior%20work%20uses%20surrogates%20like%20the%20evidence%0Alower%20bound%20%28ELBO%29%2C%20these%20one-sided%20approximations%20can%20introduce%20significant%0Apolicy%20gradient%20bias.%20To%20address%20this%2C%20we%20propose%20the%20Sandwiched%20Policy%0AGradient%20%28SPG%29%20that%20leverages%20both%20an%20upper%20and%20a%20lower%20bound%20of%20the%20true%0Alog-likelihood.%20Experiments%20show%20that%20SPG%20significantly%20outperforms%20baselines%0Abased%20on%20ELBO%20or%20one-step%20estimation.%20Specifically%2C%20SPG%20improves%20the%20accuracy%0Aover%20state-of-the-art%20RL%20methods%20for%20dLLMs%20by%203.6%25%20in%20GSM8K%2C%202.6%25%20in%20MATH500%2C%0A18.4%25%20in%20Countdown%20and%2027.0%25%20in%20Sudoku.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPG%253A%2520Sandwiched%2520Policy%2520Gradient%2520for%2520Masked%2520Diffusion%2520Language%2520Models%26entry.906535625%3DChengyu%2520Wang%2520and%2520Paria%2520Rashidinejad%2520and%2520DiJia%2520Su%2520and%2520Song%2520Jiang%2520and%2520Sid%2520Wang%2520and%2520Siyan%2520Zhao%2520and%2520Cai%2520Zhou%2520and%2520Shannon%2520Zejiang%2520Shen%2520and%2520Feiyu%2520Chen%2520and%2520Tommi%2520Jaakkola%2520and%2520Yuandong%2520Tian%2520and%2520Bo%2520Liu%26entry.1292438233%3D%2520%2520Diffusion%2520large%2520language%2520models%2520%2528dLLMs%2529%2520are%2520emerging%2520as%2520an%2520efficient%250Aalternative%2520to%2520autoregressive%2520models%2520due%2520to%2520their%2520ability%2520to%2520decode%2520multiple%250Atokens%2520in%2520parallel.%2520However%252C%2520aligning%2520dLLMs%2520with%2520human%2520preferences%2520or%250Atask-specific%2520rewards%2520via%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520challenging%2520because%250Atheir%2520intractable%2520log-likelihood%2520precludes%2520the%2520direct%2520application%2520of%2520standard%250Apolicy%2520gradient%2520methods.%2520While%2520prior%2520work%2520uses%2520surrogates%2520like%2520the%2520evidence%250Alower%2520bound%2520%2528ELBO%2529%252C%2520these%2520one-sided%2520approximations%2520can%2520introduce%2520significant%250Apolicy%2520gradient%2520bias.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Sandwiched%2520Policy%250AGradient%2520%2528SPG%2529%2520that%2520leverages%2520both%2520an%2520upper%2520and%2520a%2520lower%2520bound%2520of%2520the%2520true%250Alog-likelihood.%2520Experiments%2520show%2520that%2520SPG%2520significantly%2520outperforms%2520baselines%250Abased%2520on%2520ELBO%2520or%2520one-step%2520estimation.%2520Specifically%252C%2520SPG%2520improves%2520the%2520accuracy%250Aover%2520state-of-the-art%2520RL%2520methods%2520for%2520dLLMs%2520by%25203.6%2525%2520in%2520GSM8K%252C%25202.6%2525%2520in%2520MATH500%252C%250A18.4%2525%2520in%2520Countdown%2520and%252027.0%2525%2520in%2520Sudoku.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPG%3A%20Sandwiched%20Policy%20Gradient%20for%20Masked%20Diffusion%20Language%20Models&entry.906535625=Chengyu%20Wang%20and%20Paria%20Rashidinejad%20and%20DiJia%20Su%20and%20Song%20Jiang%20and%20Sid%20Wang%20and%20Siyan%20Zhao%20and%20Cai%20Zhou%20and%20Shannon%20Zejiang%20Shen%20and%20Feiyu%20Chen%20and%20Tommi%20Jaakkola%20and%20Yuandong%20Tian%20and%20Bo%20Liu&entry.1292438233=%20%20Diffusion%20large%20language%20models%20%28dLLMs%29%20are%20emerging%20as%20an%20efficient%0Aalternative%20to%20autoregressive%20models%20due%20to%20their%20ability%20to%20decode%20multiple%0Atokens%20in%20parallel.%20However%2C%20aligning%20dLLMs%20with%20human%20preferences%20or%0Atask-specific%20rewards%20via%20reinforcement%20learning%20%28RL%29%20is%20challenging%20because%0Atheir%20intractable%20log-likelihood%20precludes%20the%20direct%20application%20of%20standard%0Apolicy%20gradient%20methods.%20While%20prior%20work%20uses%20surrogates%20like%20the%20evidence%0Alower%20bound%20%28ELBO%29%2C%20these%20one-sided%20approximations%20can%20introduce%20significant%0Apolicy%20gradient%20bias.%20To%20address%20this%2C%20we%20propose%20the%20Sandwiched%20Policy%0AGradient%20%28SPG%29%20that%20leverages%20both%20an%20upper%20and%20a%20lower%20bound%20of%20the%20true%0Alog-likelihood.%20Experiments%20show%20that%20SPG%20significantly%20outperforms%20baselines%0Abased%20on%20ELBO%20or%20one-step%20estimation.%20Specifically%2C%20SPG%20improves%20the%20accuracy%0Aover%20state-of-the-art%20RL%20methods%20for%20dLLMs%20by%203.6%25%20in%20GSM8K%2C%202.6%25%20in%20MATH500%2C%0A18.4%25%20in%20Countdown%20and%2027.0%25%20in%20Sudoku.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09541v1&entry.124074799=Read"},
{"title": "Language Model Guided Reinforcement Learning in Quantitative Trading", "author": "Adam Darmanin and Vince Vella", "abstract": "  Algorithmic trading requires short-term tactical decisions consistent with\nlong-term financial objectives. Reinforcement Learning (RL) has been applied to\nsuch problems, but adoption is limited by myopic behaviour and opaque policies.\nLarge Language Models (LLMs) offer complementary strategic reasoning and\nmulti-modal signal interpretation when guided by well-structured prompts.\n  This paper proposes a hybrid framework in which LLMs generate high-level\ntrading strategies to guide RL agents. We evaluate (i) the economic rationale\nof LLM-generated strategies through expert review, and (ii) the performance of\nLLM-guided agents against unguided RL baselines using Sharpe Ratio (SR) and\nMaximum Drawdown (MDD).\n  Empirical results indicate that LLM guidance improves both return and risk\nmetrics relative to standard RL.\n", "link": "http://arxiv.org/abs/2508.02366v2", "date": "2025-10-10", "relevancy": 1.8488, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Model%20Guided%20Reinforcement%20Learning%20in%20Quantitative%20Trading&body=Title%3A%20Language%20Model%20Guided%20Reinforcement%20Learning%20in%20Quantitative%20Trading%0AAuthor%3A%20Adam%20Darmanin%20and%20Vince%20Vella%0AAbstract%3A%20%20%20Algorithmic%20trading%20requires%20short-term%20tactical%20decisions%20consistent%20with%0Along-term%20financial%20objectives.%20Reinforcement%20Learning%20%28RL%29%20has%20been%20applied%20to%0Asuch%20problems%2C%20but%20adoption%20is%20limited%20by%20myopic%20behaviour%20and%20opaque%20policies.%0ALarge%20Language%20Models%20%28LLMs%29%20offer%20complementary%20strategic%20reasoning%20and%0Amulti-modal%20signal%20interpretation%20when%20guided%20by%20well-structured%20prompts.%0A%20%20This%20paper%20proposes%20a%20hybrid%20framework%20in%20which%20LLMs%20generate%20high-level%0Atrading%20strategies%20to%20guide%20RL%20agents.%20We%20evaluate%20%28i%29%20the%20economic%20rationale%0Aof%20LLM-generated%20strategies%20through%20expert%20review%2C%20and%20%28ii%29%20the%20performance%20of%0ALLM-guided%20agents%20against%20unguided%20RL%20baselines%20using%20Sharpe%20Ratio%20%28SR%29%20and%0AMaximum%20Drawdown%20%28MDD%29.%0A%20%20Empirical%20results%20indicate%20that%20LLM%20guidance%20improves%20both%20return%20and%20risk%0Ametrics%20relative%20to%20standard%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02366v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Model%2520Guided%2520Reinforcement%2520Learning%2520in%2520Quantitative%2520Trading%26entry.906535625%3DAdam%2520Darmanin%2520and%2520Vince%2520Vella%26entry.1292438233%3D%2520%2520Algorithmic%2520trading%2520requires%2520short-term%2520tactical%2520decisions%2520consistent%2520with%250Along-term%2520financial%2520objectives.%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520been%2520applied%2520to%250Asuch%2520problems%252C%2520but%2520adoption%2520is%2520limited%2520by%2520myopic%2520behaviour%2520and%2520opaque%2520policies.%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520complementary%2520strategic%2520reasoning%2520and%250Amulti-modal%2520signal%2520interpretation%2520when%2520guided%2520by%2520well-structured%2520prompts.%250A%2520%2520This%2520paper%2520proposes%2520a%2520hybrid%2520framework%2520in%2520which%2520LLMs%2520generate%2520high-level%250Atrading%2520strategies%2520to%2520guide%2520RL%2520agents.%2520We%2520evaluate%2520%2528i%2529%2520the%2520economic%2520rationale%250Aof%2520LLM-generated%2520strategies%2520through%2520expert%2520review%252C%2520and%2520%2528ii%2529%2520the%2520performance%2520of%250ALLM-guided%2520agents%2520against%2520unguided%2520RL%2520baselines%2520using%2520Sharpe%2520Ratio%2520%2528SR%2529%2520and%250AMaximum%2520Drawdown%2520%2528MDD%2529.%250A%2520%2520Empirical%2520results%2520indicate%2520that%2520LLM%2520guidance%2520improves%2520both%2520return%2520and%2520risk%250Ametrics%2520relative%2520to%2520standard%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02366v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Model%20Guided%20Reinforcement%20Learning%20in%20Quantitative%20Trading&entry.906535625=Adam%20Darmanin%20and%20Vince%20Vella&entry.1292438233=%20%20Algorithmic%20trading%20requires%20short-term%20tactical%20decisions%20consistent%20with%0Along-term%20financial%20objectives.%20Reinforcement%20Learning%20%28RL%29%20has%20been%20applied%20to%0Asuch%20problems%2C%20but%20adoption%20is%20limited%20by%20myopic%20behaviour%20and%20opaque%20policies.%0ALarge%20Language%20Models%20%28LLMs%29%20offer%20complementary%20strategic%20reasoning%20and%0Amulti-modal%20signal%20interpretation%20when%20guided%20by%20well-structured%20prompts.%0A%20%20This%20paper%20proposes%20a%20hybrid%20framework%20in%20which%20LLMs%20generate%20high-level%0Atrading%20strategies%20to%20guide%20RL%20agents.%20We%20evaluate%20%28i%29%20the%20economic%20rationale%0Aof%20LLM-generated%20strategies%20through%20expert%20review%2C%20and%20%28ii%29%20the%20performance%20of%0ALLM-guided%20agents%20against%20unguided%20RL%20baselines%20using%20Sharpe%20Ratio%20%28SR%29%20and%0AMaximum%20Drawdown%20%28MDD%29.%0A%20%20Empirical%20results%20indicate%20that%20LLM%20guidance%20improves%20both%20return%20and%20risk%0Ametrics%20relative%20to%20standard%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02366v2&entry.124074799=Read"},
{"title": "Detecting Data Contamination from Reinforcement Learning Post-training\n  for Large Language Models", "author": "Yongding Tao and Tian Wang and Yihong Dong and Huanyu Liu and Kechi Zhang and Xiaolong Hu and Ge Li", "abstract": "  Data contamination poses a significant threat to the reliable evaluation of\nLarge Language Models (LLMs). This issue arises when benchmark samples may\ninadvertently appear in training sets, compromising the validity of reported\nperformance. While detection methods have been developed for the pre-training\nand Supervised Fine-Tuning stages, a critical research gap exists for the\nincreasingly significant phase of Reinforcement Learning (RL) post-training. As\nRL post-training becomes pivotal for advancing LLM reasoning, the absence of\nspecialized contamination detection methods in this paradigm presents a\ncritical vulnerability. To address this, we conduct the first systematic study\nof data detection within RL post-training scenario and propose Self-Critique.\nOur method is motivated by a key observation: after RL phase, the output\nentropy distribution of LLMs tends to collapse into highly specific and sparse\nmodes. Self-Critique probes for the underlying policy collapse, i.e., the\nmodel's convergence to a narrow reasoning path, which causes this entropy\nreduction. To facilitate this research, we also introduce RL-MIA, a benchmark\nconstructed to simulate this specific contamination scenario. Extensive\nexperiments show that Self-Critique significantly outperforms baseline methods\nacross multiple models and contamination tasks, achieving an AUC improvement of\nup to 30%. Whereas existing methods are close to a random guess for RL-phase\ncontamination, our method makes detection possible.\n", "link": "http://arxiv.org/abs/2510.09259v1", "date": "2025-10-10", "relevancy": 1.9845, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5374}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4936}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Data%20Contamination%20from%20Reinforcement%20Learning%20Post-training%0A%20%20for%20Large%20Language%20Models&body=Title%3A%20Detecting%20Data%20Contamination%20from%20Reinforcement%20Learning%20Post-training%0A%20%20for%20Large%20Language%20Models%0AAuthor%3A%20Yongding%20Tao%20and%20Tian%20Wang%20and%20Yihong%20Dong%20and%20Huanyu%20Liu%20and%20Kechi%20Zhang%20and%20Xiaolong%20Hu%20and%20Ge%20Li%0AAbstract%3A%20%20%20Data%20contamination%20poses%20a%20significant%20threat%20to%20the%20reliable%20evaluation%20of%0ALarge%20Language%20Models%20%28LLMs%29.%20This%20issue%20arises%20when%20benchmark%20samples%20may%0Ainadvertently%20appear%20in%20training%20sets%2C%20compromising%20the%20validity%20of%20reported%0Aperformance.%20While%20detection%20methods%20have%20been%20developed%20for%20the%20pre-training%0Aand%20Supervised%20Fine-Tuning%20stages%2C%20a%20critical%20research%20gap%20exists%20for%20the%0Aincreasingly%20significant%20phase%20of%20Reinforcement%20Learning%20%28RL%29%20post-training.%20As%0ARL%20post-training%20becomes%20pivotal%20for%20advancing%20LLM%20reasoning%2C%20the%20absence%20of%0Aspecialized%20contamination%20detection%20methods%20in%20this%20paradigm%20presents%20a%0Acritical%20vulnerability.%20To%20address%20this%2C%20we%20conduct%20the%20first%20systematic%20study%0Aof%20data%20detection%20within%20RL%20post-training%20scenario%20and%20propose%20Self-Critique.%0AOur%20method%20is%20motivated%20by%20a%20key%20observation%3A%20after%20RL%20phase%2C%20the%20output%0Aentropy%20distribution%20of%20LLMs%20tends%20to%20collapse%20into%20highly%20specific%20and%20sparse%0Amodes.%20Self-Critique%20probes%20for%20the%20underlying%20policy%20collapse%2C%20i.e.%2C%20the%0Amodel%27s%20convergence%20to%20a%20narrow%20reasoning%20path%2C%20which%20causes%20this%20entropy%0Areduction.%20To%20facilitate%20this%20research%2C%20we%20also%20introduce%20RL-MIA%2C%20a%20benchmark%0Aconstructed%20to%20simulate%20this%20specific%20contamination%20scenario.%20Extensive%0Aexperiments%20show%20that%20Self-Critique%20significantly%20outperforms%20baseline%20methods%0Aacross%20multiple%20models%20and%20contamination%20tasks%2C%20achieving%20an%20AUC%20improvement%20of%0Aup%20to%2030%25.%20Whereas%20existing%20methods%20are%20close%20to%20a%20random%20guess%20for%20RL-phase%0Acontamination%2C%20our%20method%20makes%20detection%20possible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Data%2520Contamination%2520from%2520Reinforcement%2520Learning%2520Post-training%250A%2520%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DYongding%2520Tao%2520and%2520Tian%2520Wang%2520and%2520Yihong%2520Dong%2520and%2520Huanyu%2520Liu%2520and%2520Kechi%2520Zhang%2520and%2520Xiaolong%2520Hu%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520Data%2520contamination%2520poses%2520a%2520significant%2520threat%2520to%2520the%2520reliable%2520evaluation%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529.%2520This%2520issue%2520arises%2520when%2520benchmark%2520samples%2520may%250Ainadvertently%2520appear%2520in%2520training%2520sets%252C%2520compromising%2520the%2520validity%2520of%2520reported%250Aperformance.%2520While%2520detection%2520methods%2520have%2520been%2520developed%2520for%2520the%2520pre-training%250Aand%2520Supervised%2520Fine-Tuning%2520stages%252C%2520a%2520critical%2520research%2520gap%2520exists%2520for%2520the%250Aincreasingly%2520significant%2520phase%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%2520post-training.%2520As%250ARL%2520post-training%2520becomes%2520pivotal%2520for%2520advancing%2520LLM%2520reasoning%252C%2520the%2520absence%2520of%250Aspecialized%2520contamination%2520detection%2520methods%2520in%2520this%2520paradigm%2520presents%2520a%250Acritical%2520vulnerability.%2520To%2520address%2520this%252C%2520we%2520conduct%2520the%2520first%2520systematic%2520study%250Aof%2520data%2520detection%2520within%2520RL%2520post-training%2520scenario%2520and%2520propose%2520Self-Critique.%250AOur%2520method%2520is%2520motivated%2520by%2520a%2520key%2520observation%253A%2520after%2520RL%2520phase%252C%2520the%2520output%250Aentropy%2520distribution%2520of%2520LLMs%2520tends%2520to%2520collapse%2520into%2520highly%2520specific%2520and%2520sparse%250Amodes.%2520Self-Critique%2520probes%2520for%2520the%2520underlying%2520policy%2520collapse%252C%2520i.e.%252C%2520the%250Amodel%2527s%2520convergence%2520to%2520a%2520narrow%2520reasoning%2520path%252C%2520which%2520causes%2520this%2520entropy%250Areduction.%2520To%2520facilitate%2520this%2520research%252C%2520we%2520also%2520introduce%2520RL-MIA%252C%2520a%2520benchmark%250Aconstructed%2520to%2520simulate%2520this%2520specific%2520contamination%2520scenario.%2520Extensive%250Aexperiments%2520show%2520that%2520Self-Critique%2520significantly%2520outperforms%2520baseline%2520methods%250Aacross%2520multiple%2520models%2520and%2520contamination%2520tasks%252C%2520achieving%2520an%2520AUC%2520improvement%2520of%250Aup%2520to%252030%2525.%2520Whereas%2520existing%2520methods%2520are%2520close%2520to%2520a%2520random%2520guess%2520for%2520RL-phase%250Acontamination%252C%2520our%2520method%2520makes%2520detection%2520possible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Data%20Contamination%20from%20Reinforcement%20Learning%20Post-training%0A%20%20for%20Large%20Language%20Models&entry.906535625=Yongding%20Tao%20and%20Tian%20Wang%20and%20Yihong%20Dong%20and%20Huanyu%20Liu%20and%20Kechi%20Zhang%20and%20Xiaolong%20Hu%20and%20Ge%20Li&entry.1292438233=%20%20Data%20contamination%20poses%20a%20significant%20threat%20to%20the%20reliable%20evaluation%20of%0ALarge%20Language%20Models%20%28LLMs%29.%20This%20issue%20arises%20when%20benchmark%20samples%20may%0Ainadvertently%20appear%20in%20training%20sets%2C%20compromising%20the%20validity%20of%20reported%0Aperformance.%20While%20detection%20methods%20have%20been%20developed%20for%20the%20pre-training%0Aand%20Supervised%20Fine-Tuning%20stages%2C%20a%20critical%20research%20gap%20exists%20for%20the%0Aincreasingly%20significant%20phase%20of%20Reinforcement%20Learning%20%28RL%29%20post-training.%20As%0ARL%20post-training%20becomes%20pivotal%20for%20advancing%20LLM%20reasoning%2C%20the%20absence%20of%0Aspecialized%20contamination%20detection%20methods%20in%20this%20paradigm%20presents%20a%0Acritical%20vulnerability.%20To%20address%20this%2C%20we%20conduct%20the%20first%20systematic%20study%0Aof%20data%20detection%20within%20RL%20post-training%20scenario%20and%20propose%20Self-Critique.%0AOur%20method%20is%20motivated%20by%20a%20key%20observation%3A%20after%20RL%20phase%2C%20the%20output%0Aentropy%20distribution%20of%20LLMs%20tends%20to%20collapse%20into%20highly%20specific%20and%20sparse%0Amodes.%20Self-Critique%20probes%20for%20the%20underlying%20policy%20collapse%2C%20i.e.%2C%20the%0Amodel%27s%20convergence%20to%20a%20narrow%20reasoning%20path%2C%20which%20causes%20this%20entropy%0Areduction.%20To%20facilitate%20this%20research%2C%20we%20also%20introduce%20RL-MIA%2C%20a%20benchmark%0Aconstructed%20to%20simulate%20this%20specific%20contamination%20scenario.%20Extensive%0Aexperiments%20show%20that%20Self-Critique%20significantly%20outperforms%20baseline%20methods%0Aacross%20multiple%20models%20and%20contamination%20tasks%2C%20achieving%20an%20AUC%20improvement%20of%0Aup%20to%2030%25.%20Whereas%20existing%20methods%20are%20close%20to%20a%20random%20guess%20for%20RL-phase%0Acontamination%2C%20our%20method%20makes%20detection%20possible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09259v1&entry.124074799=Read"},
{"title": "Safety Game: Balancing Safe and Informative Conversations with Blackbox\n  Agentic AI using LP Solvers", "author": "Tuan Nguyen and Long Tran-Thanh", "abstract": "  Ensuring that large language models (LLMs) comply with safety requirements is\na central challenge in AI deployment. Existing alignment approaches primarily\noperate during training, such as through fine-tuning or reinforcement learning\nfrom human feedback, but these methods are costly and inflexible, requiring\nretraining whenever new requirements arise. Recent efforts toward\ninference-time alignment mitigate some of these limitations but still assume\naccess to model internals, which is impractical, and not suitable for third\nparty stakeholders who do not have access to the models. In this work, we\npropose a model-independent, black-box framework for safety alignment that does\nnot require retraining or access to the underlying LLM architecture. As a proof\nof concept, we address the problem of trading off between generating safe but\nuninformative answers versus helpful yet potentially risky ones. We formulate\nthis dilemma as a two-player zero-sum game whose minimax equilibrium captures\nthe optimal balance between safety and helpfulness. LLM agents operationalize\nthis framework by leveraging a linear programming solver at inference time to\ncompute equilibrium strategies. Our results demonstrate the feasibility of\nblack-box safety alignment, offering a scalable and accessible pathway for\nstakeholders, including smaller organizations and entities in\nresource-constrained settings, to enforce safety across rapidly evolving LLM\necosystems.\n", "link": "http://arxiv.org/abs/2510.09330v1", "date": "2025-10-10", "relevancy": 1.5539, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5343}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4978}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20Game%3A%20Balancing%20Safe%20and%20Informative%20Conversations%20with%20Blackbox%0A%20%20Agentic%20AI%20using%20LP%20Solvers&body=Title%3A%20Safety%20Game%3A%20Balancing%20Safe%20and%20Informative%20Conversations%20with%20Blackbox%0A%20%20Agentic%20AI%20using%20LP%20Solvers%0AAuthor%3A%20Tuan%20Nguyen%20and%20Long%20Tran-Thanh%0AAbstract%3A%20%20%20Ensuring%20that%20large%20language%20models%20%28LLMs%29%20comply%20with%20safety%20requirements%20is%0Aa%20central%20challenge%20in%20AI%20deployment.%20Existing%20alignment%20approaches%20primarily%0Aoperate%20during%20training%2C%20such%20as%20through%20fine-tuning%20or%20reinforcement%20learning%0Afrom%20human%20feedback%2C%20but%20these%20methods%20are%20costly%20and%20inflexible%2C%20requiring%0Aretraining%20whenever%20new%20requirements%20arise.%20Recent%20efforts%20toward%0Ainference-time%20alignment%20mitigate%20some%20of%20these%20limitations%20but%20still%20assume%0Aaccess%20to%20model%20internals%2C%20which%20is%20impractical%2C%20and%20not%20suitable%20for%20third%0Aparty%20stakeholders%20who%20do%20not%20have%20access%20to%20the%20models.%20In%20this%20work%2C%20we%0Apropose%20a%20model-independent%2C%20black-box%20framework%20for%20safety%20alignment%20that%20does%0Anot%20require%20retraining%20or%20access%20to%20the%20underlying%20LLM%20architecture.%20As%20a%20proof%0Aof%20concept%2C%20we%20address%20the%20problem%20of%20trading%20off%20between%20generating%20safe%20but%0Auninformative%20answers%20versus%20helpful%20yet%20potentially%20risky%20ones.%20We%20formulate%0Athis%20dilemma%20as%20a%20two-player%20zero-sum%20game%20whose%20minimax%20equilibrium%20captures%0Athe%20optimal%20balance%20between%20safety%20and%20helpfulness.%20LLM%20agents%20operationalize%0Athis%20framework%20by%20leveraging%20a%20linear%20programming%20solver%20at%20inference%20time%20to%0Acompute%20equilibrium%20strategies.%20Our%20results%20demonstrate%20the%20feasibility%20of%0Ablack-box%20safety%20alignment%2C%20offering%20a%20scalable%20and%20accessible%20pathway%20for%0Astakeholders%2C%20including%20smaller%20organizations%20and%20entities%20in%0Aresource-constrained%20settings%2C%20to%20enforce%20safety%20across%20rapidly%20evolving%20LLM%0Aecosystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520Game%253A%2520Balancing%2520Safe%2520and%2520Informative%2520Conversations%2520with%2520Blackbox%250A%2520%2520Agentic%2520AI%2520using%2520LP%2520Solvers%26entry.906535625%3DTuan%2520Nguyen%2520and%2520Long%2520Tran-Thanh%26entry.1292438233%3D%2520%2520Ensuring%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520comply%2520with%2520safety%2520requirements%2520is%250Aa%2520central%2520challenge%2520in%2520AI%2520deployment.%2520Existing%2520alignment%2520approaches%2520primarily%250Aoperate%2520during%2520training%252C%2520such%2520as%2520through%2520fine-tuning%2520or%2520reinforcement%2520learning%250Afrom%2520human%2520feedback%252C%2520but%2520these%2520methods%2520are%2520costly%2520and%2520inflexible%252C%2520requiring%250Aretraining%2520whenever%2520new%2520requirements%2520arise.%2520Recent%2520efforts%2520toward%250Ainference-time%2520alignment%2520mitigate%2520some%2520of%2520these%2520limitations%2520but%2520still%2520assume%250Aaccess%2520to%2520model%2520internals%252C%2520which%2520is%2520impractical%252C%2520and%2520not%2520suitable%2520for%2520third%250Aparty%2520stakeholders%2520who%2520do%2520not%2520have%2520access%2520to%2520the%2520models.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520model-independent%252C%2520black-box%2520framework%2520for%2520safety%2520alignment%2520that%2520does%250Anot%2520require%2520retraining%2520or%2520access%2520to%2520the%2520underlying%2520LLM%2520architecture.%2520As%2520a%2520proof%250Aof%2520concept%252C%2520we%2520address%2520the%2520problem%2520of%2520trading%2520off%2520between%2520generating%2520safe%2520but%250Auninformative%2520answers%2520versus%2520helpful%2520yet%2520potentially%2520risky%2520ones.%2520We%2520formulate%250Athis%2520dilemma%2520as%2520a%2520two-player%2520zero-sum%2520game%2520whose%2520minimax%2520equilibrium%2520captures%250Athe%2520optimal%2520balance%2520between%2520safety%2520and%2520helpfulness.%2520LLM%2520agents%2520operationalize%250Athis%2520framework%2520by%2520leveraging%2520a%2520linear%2520programming%2520solver%2520at%2520inference%2520time%2520to%250Acompute%2520equilibrium%2520strategies.%2520Our%2520results%2520demonstrate%2520the%2520feasibility%2520of%250Ablack-box%2520safety%2520alignment%252C%2520offering%2520a%2520scalable%2520and%2520accessible%2520pathway%2520for%250Astakeholders%252C%2520including%2520smaller%2520organizations%2520and%2520entities%2520in%250Aresource-constrained%2520settings%252C%2520to%2520enforce%2520safety%2520across%2520rapidly%2520evolving%2520LLM%250Aecosystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20Game%3A%20Balancing%20Safe%20and%20Informative%20Conversations%20with%20Blackbox%0A%20%20Agentic%20AI%20using%20LP%20Solvers&entry.906535625=Tuan%20Nguyen%20and%20Long%20Tran-Thanh&entry.1292438233=%20%20Ensuring%20that%20large%20language%20models%20%28LLMs%29%20comply%20with%20safety%20requirements%20is%0Aa%20central%20challenge%20in%20AI%20deployment.%20Existing%20alignment%20approaches%20primarily%0Aoperate%20during%20training%2C%20such%20as%20through%20fine-tuning%20or%20reinforcement%20learning%0Afrom%20human%20feedback%2C%20but%20these%20methods%20are%20costly%20and%20inflexible%2C%20requiring%0Aretraining%20whenever%20new%20requirements%20arise.%20Recent%20efforts%20toward%0Ainference-time%20alignment%20mitigate%20some%20of%20these%20limitations%20but%20still%20assume%0Aaccess%20to%20model%20internals%2C%20which%20is%20impractical%2C%20and%20not%20suitable%20for%20third%0Aparty%20stakeholders%20who%20do%20not%20have%20access%20to%20the%20models.%20In%20this%20work%2C%20we%0Apropose%20a%20model-independent%2C%20black-box%20framework%20for%20safety%20alignment%20that%20does%0Anot%20require%20retraining%20or%20access%20to%20the%20underlying%20LLM%20architecture.%20As%20a%20proof%0Aof%20concept%2C%20we%20address%20the%20problem%20of%20trading%20off%20between%20generating%20safe%20but%0Auninformative%20answers%20versus%20helpful%20yet%20potentially%20risky%20ones.%20We%20formulate%0Athis%20dilemma%20as%20a%20two-player%20zero-sum%20game%20whose%20minimax%20equilibrium%20captures%0Athe%20optimal%20balance%20between%20safety%20and%20helpfulness.%20LLM%20agents%20operationalize%0Athis%20framework%20by%20leveraging%20a%20linear%20programming%20solver%20at%20inference%20time%20to%0Acompute%20equilibrium%20strategies.%20Our%20results%20demonstrate%20the%20feasibility%20of%0Ablack-box%20safety%20alignment%2C%20offering%20a%20scalable%20and%20accessible%20pathway%20for%0Astakeholders%2C%20including%20smaller%20organizations%20and%20entities%20in%0Aresource-constrained%20settings%2C%20to%20enforce%20safety%20across%20rapidly%20evolving%20LLM%0Aecosystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09330v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


